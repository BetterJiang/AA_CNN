introduction .
the starting point for this paper is an interesting pro - cedure called boosting , which is a way of combining the performance of many weak classiers to produce a powerful committee .
boosting was proposed in the computational learning theory literature ( schapire ( 123 ) , freund ( 123 ) , freund and schapire ( 123 ) ) and has since received much attention .
while boosting has evolved somewhat over the years , we describe the most commonly used version of the adaboost procedure ( freund and schapire
received august 123; revised december 123
123also at stanford linear accelerator center , stanford , ca 123
supported in part by dept .
of energy contract de - ac123 - 123 sf 123 and nsf grant dms - 123 - 123
123also at division of biostatistics , dept .
of health , research and policy , stanford university ,
stanford , ca 123
123supported in part by nsf grants dms - 123 - 123 , dms - 123 - 123 and nih grant
123supported in part by natural sciences and engineering research council of canada .
ams 123 subject classications .
123g123 , 123g123 , 123t123 , 123t123
key words and phrases .
classication , tree , nonparametric estimation , stagewise tting ,
friedman , t .
hastie and r .
tibshirani
( 123b ) ) , which we call discrete adaboost .
this is essentially the same as adaboost . m123 for binary data in freund and schapire .
here is a concise descrip - tion of adaboost in the two - class classication setting .
we have training data dene f ( cid : 123 ) x ( cid : 123 ) = ( cid : 123 ) m ( cid : 123 ) x123 ( cid : 123 ) y123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xn ( cid : 123 ) yn ( cid : 123 ) with xi a vector valued feature and yi = 123 or 123
we 123 cmfm ( cid : 123 ) x ( cid : 123 ) where each fm ( cid : 123 ) x ( cid : 123 ) is a classier producing val - ues plus or minus 123 and cm are constants; the corresponding prediction is sign ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) .
the adaboost procedure trains the classiers fm ( cid : 123 ) x ( cid : 123 ) on weighted versions of the training sample , giving higher weight to cases that are cur - rently misclassied .
this is done for a sequence of weighted samples , and then the nal classier is dened to be a linear combination of the classiers from each stage .
a detailed description of discrete adaboost is given in the boxed display titled algorithm 123
much has been written about the success of adaboost in producing accurate classiers .
many authors have explored the use of a tree - based classier for fm ( cid : 123 ) x ( cid : 123 ) and have demonstrated that it consistently produces signicantly lower error rates than a single decision tree .
in fact , breiman ( 123 ) ( referring to a nips workshop ) called adaboost with trees the best off - the - shelf classier in the world ( see also breiman ( 123b ) ) .
interestingly , in many examples the test error seems to consistently decrease and then level off as more classiers are added , rather than ultimately increase .
for some reason , it seems that adaboost is resistant to overtting .
figure 123 shows the performance of discrete adaboost on a synthetic clas - sication task , using an adaptation of carttm ( breiman , friedman , olshen and stone ( 123 ) ) as the base classier .
this adaptation grows xed - size trees in a best - rst manner ( see section 123 ) .
included in the gure is the bagged tree ( breiman ( 123 ) ) which averages trees grown on bootstrap resampled versions of the training data .
bagging is purely a variance - reduction tech - nique , and since trees tend to have high variance , bagging often produces
discrete adaboost ( freund and schapire ( 123b ) )
start with weights wi = 123 / n ( cid : 123 ) i = 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n .
repeat for m = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) m :
( a ) fit the classier fm ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) using weights wi on the training data .
( b ) compute errm = ew ( cid : 123 ) 123 ( cid : 123 ) y ( cid : 123 ) =fm ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , cm = log ( cid : 123 ) ( cid : 123 ) 123 errm ( cid : 123 ) / errm ( cid : 123 ) .
( c ) set wi wi exp ( cid : 123 ) cm123 ( cid : 123 ) yi ( cid : 123 ) =fm ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , i = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n , and renormalize so that 123
output the classier sign ( cid : 123 ) ( cid : 123 ) m
i wi = 123
algorithm 123
ew represents expectation over the training data with weights w = ( cid : 123 ) w123 ( cid : 123 ) w123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) wn ( cid : 123 ) , and 123 ( cid : 123 ) s ( cid : 123 ) is the indicator of the set s .
at each iteration , adaboost increases the weights of the observations misclassied by fm ( cid : 123 ) x ( cid : 123 ) by a factor that depends on the weighted training error .
additive logistic regression
test error for bagging , discrete adaboost and real adaboost on a simulated two - class nested spheres problem ( see section 123 ) .
there are 123 training data points in ten dimensions , and the bayes error rate is zero .
all trees are grown best - rst without pruning .
the leftmost iteration corresponds to a single tree .
early versions of adaboost used a resampling scheme to implement step 123 of algorithm 123 , by weighted sampling from the training data .
this suggested a connection with bagging and that a major component of the success of boosting has to do with variance reduction .
however , boosting performs comparably well when :
a weighted tree - growing algorithm is used in step 123 rather than weighted resampling , where each training observation is assigned its weight wi .
this removes the randomization component essential in bagging .
stumps are used for the weak learners .
stumps are single - split trees with only two terminal nodes .
these typically have low variance but high bias .
bagging performs very poorly with stumps ( figure 123 ( top right panel ) ) .
friedman , t .
hastie and r .
tibshirani
these observations suggest that boosting is capable of both bias and vari -
ance reduction , and thus differs fundamentally from bagging .
the base classier in discrete adaboost produces a classication rule fm ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) , where ( cid : 123 ) is the domain of the predictive features x .
freund and schapire ( 123b ) , breiman ( 123a ) and schapire and singer ( 123 ) have suggested various modications to improve the boosting algo - a generalization of discrete adaboost appeared in freund and schapire ( 123b ) , and was developed further in schapire and singer ( 123 ) , that uses real - valued condence - rated predictions rather than the ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) of discrete adaboost .
the weak learner for this generalized boosting produces a map - ping fm ( cid : 123 ) x ( cid : 123 ) : ( cid : 123 ) ( cid : 123 ) r; the sign of fm ( cid : 123 ) x ( cid : 123 ) gives the classication , and ( cid : 123 ) fm ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) a measure of the condence in the prediction .
this real - valued contribution is combined with the previous contributions with a multiplier cm as before , and a slightly different recipe for cm is provided .
we present a generalized version of adaboost , which we call real adaboost in algorithm 123 , in which the weak learner returns a class probability estimate pm ( cid : 123 ) x ( cid : 123 ) = pw ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) .
the contribution to the nal classier is half the logit - transform of this probability estimate .
one form of schapire and singers generalized adaboost coincides with real adaboost , in the special case where the weak learner is a decision tree .
real adaboost tends to perform the best in our simulated examples in figure 123 , especially with stumps , although we see with 123 node trees discrete adaboost overtakes real adaboost after 123
in this paper we analyze the adaboost procedures from a statistical per - spective .
the main result of our paper rederives adaboost as a method for m fm ( cid : 123 ) x ( cid : 123 ) in a forward stagewise manner .
this sim - tting an additive model ple fact largely explains why it tends to outperform a single base learner .
by tting an additive model of different and potentially simple functions , it expands the class of functions that can be approximated .
start with weights wi = 123 / n ( cid : 123 ) i = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n .
repeat for m = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) m :
( a ) fit the classier to obtain a class probability estimate pm ( cid : 123 ) x ( cid : 123 ) = pw ( cid : 123 ) y = ( b ) set fm ( cid : 123 ) x ( cid : 123 ) 123 ( c ) set wi wi exp ( cid : 123 ) yifm ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) , i = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n , and renormalize so that 123
output the classier sign ( cid : 123 ) ( cid : 123 ) m
123 ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) , using weights wi on the training data .
i wi = 123
123 log pm ( cid : 123 ) x ( cid : 123 ) / ( cid : 123 ) 123 pm ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) r .
algorithm 123
the real adaboost algorithm uses class probability esti -
mates pm ( cid : 123 ) x ( cid : 123 ) to construct real - valued contributions fm ( cid : 123 ) x ( cid : 123 ) .
additive logistic regression
given this fact , discrete and real adaboost appear unnecessarily compli - squared - error loss e ( cid : 123 ) y ( cid : 123 ) cated .
a much simpler way to t an additive model would be to minimize fm ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 in a forward stagewise manner .
at the mth stage we x f123 ( cid : 123 ) x ( cid : 123 ) fm123 ( cid : 123 ) x ( cid : 123 ) and minimize squared error to obtain fm ( cid : 123 ) x ( cid : 123 ) fj ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) .
this is just tting of residuals and is commonly used in linear regression and additive modeling ( hastie and tibshirani ( 123 ) ) .
however squared error loss is not a good choice for classication ( see figure 123 in section 123 ) and hence tting of residuals doesnt work very well in that case .
we show that adaboost ts an additive model using a bet - ter loss function for classication .
specically we show that adaboost ts an additive logistic regression model , using a criterion similar to , but not the same as , the binomial log - likelihood .
( if pm ( cid : 123 ) x ( cid : 123 ) are the class probabilities , an additive logistic regression approximates log pm ( cid : 123 ) x ( cid : 123 ) / ( cid : 123 ) 123 pm ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) by an addi - m fm ( cid : 123 ) x ( cid : 123 ) . ) we then go on to derive a new boosting procedure logitboost that directly optimizes the binomial log - likelihood .
the original boosting techniques ( schapire ( 123 ) , freund ( 123 ) ) prov - ably improved or boosted the performance of a single classier by produc - ing a majority vote of similar classiers .
these algorithms then evolved into more adaptive and practical versions such as adaboost , whose success was still explained in terms of boosting individual classiers by a weighted majority vote or weighted committee .
we believe that this view , along with the appealing name boosting inherited by adaboost , may have led to some of the mystery about how and why the method works .
as mentioned above , we instead view boosting as a technique for tting an additive model .
section 123 gives a short history of the boosting idea .
in section 123 we briey review additive modeling .
section 123 shows how boosting can be viewed as an additive model estimator and proposes some new boosting methods for the two - class case .
the multiclass problem is studied in section 123
simulated and real data experiments are discussed in sections 123 and 123
our tree - growing imple - mentation , using truncated best - rst trees , is described in section 123
weight trimming to speed up computation is discussed in section 123 , and we briey describe generalizations of boosting in section 123
we end with a discussion in section 123
a brief history of boosting .
schapire ( 123 ) developed the rst sim - ple boosting procedure in the pac - learning framework ( valiant ( 123 ) , kearns and vazirani ( 123 ) ) .
schapire showed that a weak learner could always improve its performance by training two additional classiers on ltered ver - sions of the input data stream .
a weak learner is an algorithm for producing a two - class classier with performance guaranteed ( with high probability ) to be signicantly better than a coinip .
after learning an initial classier h123 on the rst n training points : 123
h123 is learned on a new sample of n points , half of which are misclassied
h123 is learned on n points for which h123 and h123 disagree .
the boosted classier is hb = majority vote ( cid : 123 ) h123 ( cid : 123 ) h123 ( cid : 123 ) h123 ( cid : 123 ) .
friedman , t .
hastie and r .
tibshirani
schapires strength of weak learnability theorem proves that hb has improved performance over h123
freund ( 123 ) proposed a boost by majority variation which combined many weak learners simultaneously and improved the performance of the sim - ple boosting algorithm of schapire .
the theory supporting both of these algo - rithms requires the weak learner to produce a classier with a xed error rate .
this led to the more adaptive and realistic adaboost ( freund and schapire ( 123b ) ) and its offspring , where this assumption was dropped .
freund and schapire ( 123b ) and schapire and singer ( 123 ) provide some theory to support their algorithms , in the form of upper bounds on generaliza - tion error .
this theory has evolved in the computational learning community , initially based on the concepts of pac learning .
other theories attempting to explain boosting come from game theory ( freund and schapire ( 123a ) , breiman ( 123 ) ) and vc theory ( schapire , freund , bartlett and lee ( 123 ) ) .
the bounds and the theory associated with the adaboost algorithms are inter - esting , but tend to be too loose to be of practical importance .
in practice , boost - ing achieves results far more impressive than the bounds would imply .
additive model f ( cid : 123 ) x ( cid : 123 ) = ( cid : 123 ) m
additive models .
we show in the next section that adaboost ts an m=123 cmfm ( cid : 123 ) x ( cid : 123 ) .
we believe that viewing current boost - ing procedures as stagewise algorithms for tting additive models goes a long way toward understanding their performance .
additive models have a long history in statistics , and so we rst give some examples here .
additive regression models .
we initially focus on the regression prob - lem , where the response y is quantitative , x and y have some joint distribu - tion , and we are interested in modeling the mean e ( cid : 123 ) y ( cid : 123 ) x ( cid : 123 ) = f ( cid : 123 ) x ( cid : 123 ) .
the additive model has the form
f ( cid : 123 ) x ( cid : 123 ) = p ( cid : 123 )
there is a separate function fj ( cid : 123 ) xj ( cid : 123 ) for each of the p input variables xj .
more generally , each component fj is a function of a small , prespecied subset of the input variables .
the backtting algorithm ( friedman and stuetzle ( 123 ) , buja , hastie and tibshirani ( 123 ) ) is a convenient modular gaussseidel algorithm for tting additive models .
a backtting update is
for j = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) p ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
any method or algorithm for estimating a function of xj can be used to obtain an estimate of the conditional expectation in ( 123 ) .
in particular , this can include nonparametric smoothing algorithms , such as local regression or smoothing splines .
in the right - hand side , all the latest versions of the func - tions fk are used in forming the partial residuals .
the backtting cycles are repeated until convergence .
under fairly general conditions , backtting can
additive logistic regression
be shown to converge to the minimizer of e ( cid : 123 ) y f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 ( buja , hastie and
extended additive models .
more generally , one can consider additive models whose elements ( cid : 123 ) fm ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) m 123 are functions of potentially all of the input features x .
usually in this context the fm ( cid : 123 ) x ( cid : 123 ) are taken to be simple functions characterized by a set of parameters and a multiplier m ,
the additive model then becomes
fm ( cid : 123 ) x ( cid : 123 ) = mb ( cid : 123 ) x ( cid : 123 ) m ( cid : 123 ) ( cid : 123 ) fm ( cid : 123 ) x ( cid : 123 ) = m ( cid : 123 )
for example , in single hidden layer neural networks b ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) = ( cid : 123 ) tx ( cid : 123 ) where ( cid : 123 ) ( cid : 123 ) is a sigmoid function and parameterizes a linear combination of the input features .
in signal processing , wavelets are a popular choice with parameterizing the location and scale shifts of a mother wavelet b ( cid : 123 ) x ( cid : 123 ) .
in these applications ( cid : 123 ) b ( cid : 123 ) x ( cid : 123 ) m ( cid : 123 ) ( cid : 123 ) m 123 are generally called basis functions since they span a function subspace .
if least - squares is used as a tting criterion , one can solve for an optimal set of parameters through a generalized backtting algorithm with updates ,
( cid : 123 ) m ( cid : 123 ) m ( cid : 123 ) arg min
kb ( cid : 123 ) x ( cid : 123 ) k ( cid : 123 ) b ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
for m = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) m in cycles until convergence .
alternatively , one can use a greedy forward stepwise approach , ( cid : 123 ) m ( cid : 123 ) m ( cid : 123 ) arg min
y fm123 ( cid : 123 ) x ( cid : 123 ) b ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123
for m = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) m , where ( cid : 123 ) k ( cid : 123 ) k ( cid : 123 ) m123 are xed at their corresponding solu - tion values at earlier iterations .
this is the approach used by mallat and zhang ( 123 ) in matching pursuit , where the b ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) are selected from an over - complete dictionary of wavelet bases .
in the language of boosting , f ( cid : 123 ) x ( cid : 123 ) = b ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) would be called a weak learner and fm ( cid : 123 ) x ( cid : 123 ) ( 123 ) the committee .
if decision trees were used as the weak learner , the parameters would repre - sent the splitting variables , split points , the constants in each terminal node and number of terminal nodes of each tree .
note that the backtting procedure ( 123 ) or its greedy cousin ( 123 ) only require an algorithm for tting a single weak learner ( 123 ) to data .
this base algorithm is simply applied repeatedly to modied versions of the original data
ym y ( cid : 123 )
friedman , t .
hastie and r .
tibshirani
in the forward stepwise procedure ( 123 ) , the modied output ym at the mth iter - ation depends only on its value ym123 and the solution fm123 ( cid : 123 ) x ( cid : 123 ) at the previous
ym = ym123 fm123 ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
at each step m , the previous output values ym123 are modied ( 123 ) so that the previous model fm123 ( cid : 123 ) x ( cid : 123 ) has no explanatory power on the new outputs ym .
one can therefore view this as a procedure for boosting a weak learner f ( cid : 123 ) x ( cid : 123 ) = b ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) to form a powerful committee fm ( cid : 123 ) x ( cid : 123 ) ( 123 ) .
classication problems .
for the classication problem , we learn from bayes theorem that all we need is p ( cid : 123 ) y = j ( cid : 123 ) x ( cid : 123 ) , the posterior or conditional class probabilities .
one could transfer all the above regression machinery across to the classication domain by simply noting that e ( cid : 123 ) 123 ( cid : 123 ) y=j ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) = p ( cid : 123 ) y = j ( cid : 123 ) x ( cid : 123 ) , where 123 ( cid : 123 ) y=j ( cid : 123 ) is the 123 / 123 indicator variable representing class j .
while this works fairly well in general , several problems have been noted ( hastie , tibshirani and buja ( 123 ) ) for constrained regression methods .
the estimates are typ - ically not conned to ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) , and severe masking problems can occur when there are more than two classes .
a notable exception is when trees are used as the regression method , and in fact this is the approach used by breiman , friedman , olshen and stone ( 123 ) .
logistic regression is a popular approach used in statistics for overcom - ing these problems .
for a two - class problem , an additive logistic model has
p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 )
p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) = m ( cid : 123 )
of f ( cid : 123 ) x ( cid : 123 ) = ( cid : 123 ) m
the monotone logit transformation on the left guarantees that for any values m=123 fm ( cid : 123 ) x ( cid : 123 ) r , the probability estimates lie in ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) ; inverting
p ( cid : 123 ) x ( cid : 123 ) = p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) = ef ( cid : 123 ) x ( cid : 123 )
123 + ef ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
here we have given a general additive form for f ( cid : 123 ) x ( cid : 123 ) ; special cases exist that are well known in statistics .
in particular , linear logistic regression ( mccullagh and nelder ( 123 ) , e . g . ) and additive logistic regression ( hastie and tibshirani ( 123 ) ) are popular .
these models are usually t by maximizing the binomial log - likelihood and enjoy all the associated asymptotic optimality features of maximum likelihood estimation .
a generalized version of backtting ( 123 ) , called local scoring in hastie and tibshirani ( 123 ) , can be used to t the additive logistic model by maximum fk ( cid : 123 ) xk ( cid : 123 ) and p ( cid : 123 ) x ( cid : 123 )
likelihood .
starting with guesses f123 ( cid : 123 ) x123 ( cid : 123 ) fp ( cid : 123 ) xp ( cid : 123 ) , f ( cid : 123 ) x ( cid : 123 ) = ( cid : 123 )
dened in ( 123 ) , we form the working response :
z = f ( cid : 123 ) x ( cid : 123 ) + 123 ( cid : 123 ) y=123 ( cid : 123 ) p ( cid : 123 ) x ( cid : 123 ) p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
additive logistic regression
we then apply backtting to the response z with observation weights p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) to obtain new fk ( cid : 123 ) xk ( cid : 123 ) .
this process is repeated until convergence .
the forward stagewise version ( 123 ) of this procedure bears a close similarity to the logitboost algorithm described later in the paper .
adaboost : an additive logistic regression model .
in this section we show that the adaboost algorithms ( discrete and real ) can be interpreted as stagewise estimation procedures for tting an additive logistic regression model .
they optimize an exponential criterion which to second order is equiva - lent to the binomial log - likelihood criterion .
we then propose a more standard likelihood - based boosting procedure .
an exponential criterion .
consider minimizing the criterion
j ( cid : 123 ) f ( cid : 123 ) = e ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
for estimation of f ( cid : 123 ) x ( cid : 123 ) .
here e represents expectation; depending on the con - text , this may be a population expectation ( with respect to a probability dis - tribution ) or else a sample average .
ew indicates a weighted expectation .
lemma 123 shows that the function f ( cid : 123 ) x ( cid : 123 ) that minimizes j ( cid : 123 ) f ( cid : 123 ) is the symmetric logistic transform of p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) .
lemma 123
e ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) is minimized at
f ( cid : 123 ) x ( cid : 123 ) = 123
p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) =
p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) =
ef ( cid : 123 ) x ( cid : 123 ) + ef ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
ef ( cid : 123 ) x ( cid : 123 ) + ef ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
while e entails expectation over the joint distribution of y and x ,
it is sufcient to minimize the criterion conditional on x :
( cid : 123 ) = p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) ef ( cid : 123 ) x ( cid : 123 ) + p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) ef ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) = p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) ef ( cid : 123 ) x ( cid : 123 ) + p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) ef ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
the result follows by setting the derivative to zero .
this exponential criterion appeared in schapire and singer ( 123 ) , moti - vated as an upper bound on misclassication error .
breiman ( 123 ) also used this criterion in his results on adaboost and prediction games .
the usual
friedman , t .
hastie and r .
tibshirani
logistic transform does not have the factor 123 123 as in ( 123 ) ; by multiplying the numerator and denominator in ( 123 ) by ef ( cid : 123 ) x ( cid : 123 ) , we get the usual logistic model
p ( cid : 123 ) x ( cid : 123 ) = e123f ( cid : 123 ) x ( cid : 123 )
123 + e123f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
hence the two models are equivalent up to a factor 123
if e is replaced by averages over regions of x where f ( cid : 123 ) x ( cid : 123 ) is constant ( as in the terminal node of a decision tree ) , the same result applies to the sample proportions of y = 123 and y = 123
results 123 and 123 show that both discrete and real adaboost , as well as the generalized adaboost of freund and schapire ( 123b ) , can be motivated as iterative algorithms for optimizing the ( population based ) exponential crite - rion .
the results share the same format .
given an imperfect f ( cid : 123 ) x ( cid : 123 ) , an update f ( cid : 123 ) x ( cid : 123 ) + f ( cid : 123 ) x ( cid : 123 ) is proposed based on the
population version of the criterion .
the update , which involves population conditional expectations , is imper - fectly approximated for nite data sets by some restricted class of estima - tors , such as averages in terminal nodes of trees .
hastie and tibshirani ( 123 ) use a similar derivation of the local scoring algorithm used in tting generalized additive models .
many terms are typi - cally required in practice , since at each stage the approximation to conditional expectation is rather crude .
because of lemma 123 , the resulting algorithms can be interpreted as a stagewise estimation procedure for tting an additive logistic regression model .
the derivations are sufciently different to warrant
result 123
the discrete adaboost algorithm ( population version ) builds an additive logistic regression model via newton - like updates for minimizing
let j ( cid : 123 ) f ( cid : 123 ) = e ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) .
suppose we have a current estimate f ( cid : 123 ) x ( cid : 123 ) and seek an improved estimate f ( cid : 123 ) x ( cid : 123 ) + cf ( cid : 123 ) x ( cid : 123 ) .
for xed c ( and x ) , we expand j ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + cf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) to second order about f ( cid : 123 ) x ( cid : 123 ) = 123 ,
j ( cid : 123 ) f + cf ( cid : 123 ) = e ( cid : 123 ) ey ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) +cf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
e ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 ycf ( cid : 123 ) x ( cid : 123 ) + c123y123f ( cid : 123 ) x ( cid : 123 ) 123 / 123 ( cid : 123 ) ( cid : 123 ) = e ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 ycf ( cid : 123 ) x ( cid : 123 ) + c123 / 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
since y123 = 123 and f ( cid : 123 ) x ( cid : 123 ) 123 = 123
minimizing pointwise with respect to f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) , we write
ew ( cid : 123 ) 123 ycf ( cid : 123 ) x ( cid : 123 ) + c123 / 123 ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
f ( cid : 123 ) x ( cid : 123 ) = arg min
additive logistic regression
here the notation ew ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) refers to a weighted conditional expectation , where w = w ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) = eyf ( cid : 123 ) x ( cid : 123 ) , and
ew ( cid : 123 ) g ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) def= e ( cid : 123 ) w ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) g ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) x ( cid : 123 )
for c > 123 , minimizing ( 123 ) is equivalent to maximizing
the solution is
if ew ( cid : 123 ) y ( cid : 123 ) x ( cid : 123 ) = pw ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) pw ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) > 123 ,
ew ( cid : 123 ) yf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) = ew ( cid : 123 ) y f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 / 123 123
( again using f ( cid : 123 ) x ( cid : 123 ) 123 = y123 = 123 ) .
thus minimizing a quadratic approximation to the criterion leads to a weighted least - squares choice of f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) , and this constitutes the newton - like step .
given f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) , we can directly minimize j ( cid : 123 ) f + cf ( cid : 123 ) to determine c :
c = arg min
where err = ew ( cid : 123 ) 123 ( cid : 123 ) y ( cid : 123 ) =f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) .
note that c can be negative if the weak learner does worse than 123% , in which case it automatically reverses the polarity .
combining these steps we get the update for f ( cid : 123 ) x ( cid : 123 ) ,
f ( cid : 123 ) x ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + 123
in the next iteration the new contribution cf ( cid : 123 ) x ( cid : 123 ) to f ( cid : 123 ) x ( cid : 123 ) augments the weights
since yf ( cid : 123 ) x ( cid : 123 ) = 123 123 ( cid : 123 ) y ( cid : 123 ) =f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 , we see that the update is equivalent to
w ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) w ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ecf ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) 123 err
w ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) w ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) exp
thus the function and weight updates are of an identical form to those used
in discrete adaboost .
this population version of adaboost translates naturally to a data version using trees .
the weighted conditional expectation in ( 123 ) is approximated by the terminal - node weighted averages in a tree .
in particular , the weighted least - squares criterion is used to grow the tree - based classier f ( cid : 123 ) x ( cid : 123 ) , and given f ( cid : 123 ) x ( cid : 123 ) , the constant c is based on the weighted training error .
friedman , t .
hastie and r .
tibshirani
note that after each newton step , the weights change , and hence the tree conguration will change as well .
this adds an adaptive twist to the data version of a newton - like algorithm .
parts of this derivation for adaboost can be found in breiman ( 123 ) and schapire and singer ( 123 ) , but without making the connection to additive logistic regression models .
corollary 123
after each update to the weights , the weighted misclassi -
cation error of the most recent weak learner is 123% .
this follows by noting that the c that minimizes j ( cid : 123 ) f+ cf ( cid : 123 ) satises
j ( cid : 123 ) f + cf ( cid : 123 )
the result follows since yf ( cid : 123 ) x ( cid : 123 ) is 123 for a correct and 123 for an incorrect
= e ( cid : 123 ) ey ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) +cf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) yf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) = 123 ( cid : 123 )
schapire and singer ( 123 ) give the interpretation that the weights are updated to make the new weighted problem maximally difcult for the next the discrete adaboost algorithm expects the tree or other weak learner to deliver a classier f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) .
result 123 requires minor modications to accommodate f ( cid : 123 ) x ( cid : 123 ) r , as in the generalized adaboost algorithms ( freund and schapire ( 123b ) , schapire and singer ( 123 ) ) ; the estimate for cm differs .
fixing f , we see that the minimizer of ( 123 ) must satisfy if f is not discrete , this equation has no closed - form solution for c , and requires an iterative solution such as newtonraphson .
ew ( cid : 123 ) yf ( cid : 123 ) x ( cid : 123 ) ecyf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) = 123 ( cid : 123 )
we now derive the real adaboost algorithm , which uses weighted proba - bility estimates to update the additive logistic model , rather than the classi - cations themselves .
again we derive the population updates and then apply it to data by approximating conditional expectations by terminal - node averages
result 123
the real adaboost algorithm ts an additive logistic regression
model by stagewise and approximate optimization of j ( cid : 123 ) f ( cid : 123 ) = e ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) .
suppose we have a current estimate f ( cid : 123 ) x ( cid : 123 ) and seek an improved esti -
mate f ( cid : 123 ) x ( cid : 123 ) + f ( cid : 123 ) x ( cid : 123 ) by minimizing j ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) at each x .
j ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) = e ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) x ( cid : 123 )
= ef ( cid : 123 ) x ( cid : 123 ) e ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) 123 ( cid : 123 ) y=123 ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) + ef ( cid : 123 ) x ( cid : 123 ) e ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) 123 ( cid : 123 ) y=123 ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
dividing through by e ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) and setting the derivative w . r . t .
f ( cid : 123 ) x ( cid : 123 ) to zero
f ( cid : 123 ) x ( cid : 123 ) = 123
additive logistic regression
where w ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) = exp ( cid : 123 ) yf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) .
the weights get updated by
pw ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) pw ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
w ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) w ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
the algorithm as presented would stop after one iteration .
in practice we use crude approximations to conditional expectation , such as decision trees or other constrained models , and hence many steps are required .
corollary 123
at the optimal f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) the weighted conditional mean of y is 123
if f ( cid : 123 ) x ( cid : 123 ) is optimal , we have
f ( cid : 123 ) x ( cid : 123 ) = eeyf ( cid : 123 ) x ( cid : 123 ) y = 123 ( cid : 123 )
we can think of the weights as providing an alternative to residuals for the binary classication problem .
at the optimal function f , there is no further information about f in the weighted conditional distribution of y .
if there is , we use it to update f .
an iteration m in either the discrete or real adaboost algorithms , we have
composed an additive function of the form
f ( cid : 123 ) x ( cid : 123 ) = m ( cid : 123 )
where each of the components are found in a greedy forward stagewise fash - ion , xing the earlier components .
our term stagewise refers to a similar approach in statistics : 123
variables are included sequentially in a stepwise regression .
the coefcients of variables already included receive no further adjustment .
why eeyf ( cid : 123 ) x ( cid : 123 ) ? so far the only justication for this exponential crite - rion is that it has a sensible population minimizer , and the algorithm described above performs well on real data .
in addition : 123
schapire and singer ( 123 ) motivate eyf ( cid : 123 ) x ( cid : 123 ) as a differentiable upper bound
to misclassication error 123 ( cid : 123 ) yf<123 ( cid : 123 ) ( see figure 123 ) .
the adaboost algorithm that it generates is extremely modular , requir - ing at each iteration the retraining of a classier on a weighted training let y = ( cid : 123 ) y + 123 ( cid : 123 ) / 123 , taking values 123 , 123 , and parametrize the binomial prob -
ef ( cid : 123 ) x ( cid : 123 ) + ef ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
the binomial log - likelihood is
l ( cid : 123 ) y ( cid : 123 ) p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) = y log ( cid : 123 ) p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) + ( cid : 123 ) 123 y ( cid : 123 ) log ( cid : 123 ) 123 p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
= log ( cid : 123 ) 123 + e123yf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
friedman , t .
hastie and r .
tibshirani
a variety of loss functions for estimating a function f ( cid : 123 ) x ( cid : 123 ) for classication .
the horizontal axis is yf ( cid : 123 ) which is negative for errors and positive for correct classications .
all the loss functions are monotone in yf ( cid : 123 ) and are centered and scaled to match eyf at f = 123
the curve labeled log - likelihood is the binomial log - likelihood or cross - entropy y log p + ( cid : 123 ) 123 y ( cid : 123 ) log ( cid : 123 ) 123 p ( cid : 123 ) .
the curve labeled squared error ( p ) is ( cid : 123 ) y p ( cid : 123 ) 123
the curve labeled squared error ( f ) is ( cid : 123 ) y f ( cid : 123 ) 123 and increases once yf exceeds 123 ( cid : 123 ) thereby increasingly penalizing classications that are too correct .
hence we see that :
the population minimizers of el ( cid : 123 ) y ( cid : 123 ) p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) and eeyf ( cid : 123 ) x ( cid : 123 ) coincide .
this is easily seen because the expected log - likelihood is maximized at the true probabilities p ( cid : 123 ) x ( cid : 123 ) = p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) , which dene the logit f ( cid : 123 ) x ( cid : 123 ) .
by lemma 123 we see that this is exactly the minimizer of eeyf ( cid : 123 ) x ( cid : 123 ) .
in fact , the exponential criterion and the ( negative ) log - likelihood are equivalent to second order in a taylor series around f = 123 ,
l ( cid : 123 ) y ( cid : 123 ) p ( cid : 123 ) exp ( cid : 123 ) yf ( cid : 123 ) + log ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 )
graphs of exp ( cid : 123 ) yf ( cid : 123 ) and log ( cid : 123 ) 123 + e123yf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) are shown in figure 123 , as a function of yfpositive values of yf imply correct classication .
note that exp ( cid : 123 ) yf ( cid : 123 ) itself is not a proper log - likelihood , as it does not equal the log of any probability mass function on plus or minus 123
there is another way to view the criterion j ( cid : 123 ) f ( cid : 123 ) .
it is easy to show that
p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
123 log ( cid : 123 ) p ( cid : 123 ) x ( cid : 123 ) / ( cid : 123 ) 123 p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) .
the right - hand side is known as the with f ( cid : 123 ) x ( cid : 123 ) = 123 statistic in the statistical literature .
123 is a quadratic approximation to the log - likelihood , and so can be considered a gentler alternative .
additive logistic regression
one feature of both the exponential and log - likelihood criteria is that they are monotone and smooth .
even if the training error is zero , the criteria will drive the estimates towards purer solutions ( in terms of probability estimates ) .
if fm123 ( cid : 123 ) x ( cid : 123 ) = ( cid : 123 ) m123 why not estimate the fm by minimizing the squared error e ( cid : 123 ) y f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 ? fj ( cid : 123 ) x ( cid : 123 ) is the current prediction , this leads to a forward stagewise procedure that does an unweighted t to the response y fm123 ( cid : 123 ) x ( cid : 123 ) at step m as in ( 123 ) .
empirically we have found that this approach works quite well , but is dominated by those that use monotone loss criteria .
we believe that the nonmonotonicity of squared error loss ( figure 123 ) is the reason .
correct classications , but with yf ( cid : 123 ) x ( cid : 123 ) > 123 , incur increasing loss for increasing values of ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) .
this makes squared - error loss an especially poor approximation to misclassication error rate .
classications that are too correct are penalized as much as misclassication errors .
direct optimization of the binomial log - likelihood .
in this section we explore algorithms for tting additive logistic regression models by stagewise optimization of the bernoulli log - likelihood .
here we focus again on the two - class case and will use a 123 / 123 response y to represent the outcome .
we repre - sent the probability of y = 123 by p ( cid : 123 ) x ( cid : 123 ) , where
ef ( cid : 123 ) x ( cid : 123 ) + ef ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
algorithm 123 gives the details .
logitboost ( two classes )
start with weights wi = 123 / n i= 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n , f ( cid : 123 ) x ( cid : 123 ) = 123 and probability esti - 123
repeat for m = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) m :
mates p ( cid : 123 ) xi ( cid : 123 ) = 123
( a ) compute the working response and weights
p ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) 123 p ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) wi = p ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) 123 p ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
xi using weights wi .
( b ) fit the function fm ( cid : 123 ) x ( cid : 123 ) by a weighted least - squares regression of zi to ( c ) update f ( cid : 123 ) x ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + 123 123
output the classier sign ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) = sign ( cid : 123 ) ( cid : 123 ) m
123 fm ( cid : 123 ) x ( cid : 123 ) and p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ef ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) / ( cid : 123 ) ef ( cid : 123 ) x ( cid : 123 ) + ef ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) .
algorithm 123
an adaptive newton algorithm for tting an additive logis -
tic regression model .
friedman , t .
hastie and r .
tibshirani
result 123
the logitboost algorithm ( two classes , population version ) uses newton steps for tting an additive symmetric logistic model by maximum
derivation .
consider the update f ( cid : 123 ) x ( cid : 123 ) + f ( cid : 123 ) x ( cid : 123 ) and the expected log -
conditioning on x , we compute the rst and second derivative at f ( cid : 123 ) x ( cid : 123 ) = 123 ,
123y ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) log ( cid : 123 ) 123 + e123 ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) +f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
el ( cid : 123 ) f + f ( cid : 123 ) = e
s ( cid : 123 ) x ( cid : 123 ) = el ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) = 123e ( cid : 123 ) y p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) h ( cid : 123 ) x ( cid : 123 ) = 123el ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
= 123e ( cid : 123 ) p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
f ( cid : 123 ) x ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) h ( cid : 123 ) x ( cid : 123 ) 123s ( cid : 123 ) x ( cid : 123 )
= f ( cid : 123 ) x ( cid : 123 ) + 123 = f ( cid : 123 ) x ( cid : 123 ) + 123
where p ( cid : 123 ) x ( cid : 123 ) is dened in terms of f ( cid : 123 ) x ( cid : 123 ) .
the newton update is then
where w ( cid : 123 ) x ( cid : 123 ) = p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) .
equivalently , the newton update f ( cid : 123 ) x ( cid : 123 ) solves the weighted least - squares approximation ( about f ( cid : 123 ) x ( cid : 123 ) ) to the log - likelihood
f ( cid : 123 ) x ( cid : 123 ) + 123
p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
the population algorithm described here translates immediately to an implementation on data when e ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) is replaced by a regression method , such as regression trees ( breiman , friedman , olshen and stone ( 123 ) ) .
while the role of the weights are somewhat articial in the population case , they are not in any implementation; w ( cid : 123 ) x ( cid : 123 ) is constant when conditioned on x , but the w ( cid : 123 ) xi ( cid : 123 ) in a terminal node of a tree , for example , depend on the current values f ( cid : 123 ) xi ( cid : 123 ) , and will typically not be constant .
sometimes the w ( cid : 123 ) x ( cid : 123 ) get very small in regions of ( x ) perceived ( by f ( cid : 123 ) x ( cid : 123 ) ) to be purethat is , when p ( cid : 123 ) x ( cid : 123 ) is close to 123 or 123
this can cause numerical problems in the construction of z , and lead to the following crucial implementation 123
if y = 123 , then compute z = ( cid : 123 ) ( cid : 123 ) y p ( cid : 123 ) / p ( cid : 123 ) 123 p ( cid : 123 ) ( cid : 123 ) as 123 / p .
since this number can get large if p is small , threshold this ratio at z max .
the particu - lar value chosen for z max is not crucial; we have found empirically that
additive logistic regression
z max ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) works well .
likewise , if y = 123 , compute z = 123 / ( cid : 123 ) 123 p ( cid : 123 ) with a lower threshold of z max .
enforce a lower threshold on the weights : w = max ( cid : 123 ) w ( cid : 123 ) 123 machine - zero ( cid : 123 ) .
optimizing eeyf ( cid : 123 ) x ( cid : 123 ) by newton stepping .
the population version of the real adaboost procedure ( algorithm 123 ) optimizes e exp ( cid : 123 ) y ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) exactly with respect to f at each iteration .
in algorithm 123 we propose the gentle adaboost procedure that instead takes adaptive newton steps much like the logitboost algorithm just described .
result 123
the gentle adaboost algorithm ( population version ) uses
newton steps for minimizing eeyf ( cid : 123 ) x ( cid : 123 ) .
j ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
123j ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
hence the newton update is
= e ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) since y123 = 123 ( cid : 123 )
f ( cid : 123 ) x ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + e ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) x ( cid : 123 )
= f ( cid : 123 ) x ( cid : 123 ) + ew ( cid : 123 ) y ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
where w ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) = eyf ( cid : 123 ) x ( cid : 123 ) .
the main difference between this and the real adaboost algorithm is how it uses its estimates of the weighted class probabilities to update the functions .
here the update is fm ( cid : 123 ) x ( cid : 123 ) = pw ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) pw ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) , rather than half the
start with weights wi = 123 / n ( cid : 123 ) i = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) = 123 ( cid : 123 ) 123
repeat for m = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) m :
with weights wi .
( a ) fit the regression function fm ( cid : 123 ) x ( cid : 123 ) by weighted least - squares of yi to xi ( b ) update f ( cid : 123 ) x ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) + fm ( cid : 123 ) x ( cid : 123 ) .
( c ) update wi wi exp ( cid : 123 ) yifm ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) and renormalize .
output the classier sign ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) = sign ( cid : 123 ) ( cid : 123 ) m
algorithm 123
a modied version of the real adaboost algorithm , using
newton stepping rather than exact optimization at each step .
friedman , t .
hastie and r .
tibshirani
log - ratio as in ( 123 ) : fm ( cid : 123 ) x ( cid : 123 ) = 123 123 log ( cid : 123 ) pw ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) / ( cid : 123 ) pw ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) .
log - ratios can be numerically unstable , leading to very large updates in pure regions , while the update here lies in the range ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) .
empirical evidence suggests ( see section 123 ) that this more conservative algorithm has similar performance to both the real adaboost and logitboost algorithms , and often outperforms them both , especially when stability is an issue .
there is a strong similarity between the updates for the gentle adaboost algorithm and those for the logitboost algorithm .
let p = p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) , and p ( cid : 123 ) x ( cid : 123 ) = ef ( cid : 123 ) x ( cid : 123 ) / ( cid : 123 ) ef ( cid : 123 ) x ( cid : 123 ) + ef ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
e ( cid : 123 ) eyf ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) = ef ( cid : 123 ) x ( cid : 123 ) p ef ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 p ( cid : 123 ) ef ( cid : 123 ) x ( cid : 123 ) p + ef ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 p ( cid : 123 )
( cid : 123 ) 123 p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) p + p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 p ( cid : 123 ) ( cid : 123 )
the analogous expression for logitboost from ( 123 ) is
p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
123 these are nearly the same , but they differ as the p ( cid : 123 ) x ( cid : 123 ) become at p ( cid : 123 ) x ( cid : 123 ) 123 extreme .
for example , if p 123 and p ( cid : 123 ) x ( cid : 123 ) 123 , ( 123 ) blows up , while ( 123 ) is about 123 ( and always falls in ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) ) .
multiclass procedures .
here we explore extensions of boosting to classication with multiple classes .
we start off by proposing a natural gen - eralization of the two - class symmetric logistic transformation , and then con - sider specic algorithms .
in this context schapire and singer ( 123 ) dene j responses yj for a j class problem , each taking values in ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) .
simi - larly the indicator response vector with elements y j is more standard in the statistics literature .
assume the classes are mutually exclusive .
definition 123
for a j class problem let pj ( cid : 123 ) x ( cid : 123 ) = p ( cid : 123 ) yj = 123 ( cid : 123 ) x ( cid : 123 ) .
we dene
the symmetric multiple logistic transformation
fj ( cid : 123 ) x ( cid : 123 ) = log pj ( cid : 123 ) x ( cid : 123 ) 123
k=123 efk ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
fk ( cid : 123 ) x ( cid : 123 ) = 123 ( cid : 123 )
the centering condition in ( 123 ) is for numerical stability only; it simply pins the fj down , else we could add an arbitrary constant to each fj and the probabilities remain the same .
the equivalence of these two denitions is easily established , as well as the equivalence with the two - class case .
additive logistic regression
schapire and singer ( 123 ) provide several generalizations of adaboost for the multiclass case , and also refer to other proposals ( freund and schapire ( 123 ) , schapire ( 123 ) ) ; we describe their adaboost . mh algorithm ( see algo - rithm 123 ) , since it seemed to dominate the others in their empirical studies .
we then connect it to the models presented here .
we will refer to the augmented variable in algorithm 123 as the class variable c .
we make a few observations : j=123 eeyjfj ( cid : 123 ) x ( cid : 123 ) , which is equivalent to running separate population boosting algorithms on each of the j problems of size n obtained by partitioning the n j samples in the obvious fashion .
this is seen trivially by rst conditioning on c = j , and then x ( cid : 123 ) c = j , when computing conditional expectations .
the population version of this algorithm minimizes
the same is almost true for a tree - based algorithm .
we see this because : ( a ) if the rst split is on c , either a j - nary split if permitted , or else j 123 binary splits , then the subtrees are identical to separate trees grown to each of the j groups .
this will always be the case for the rst tree .
( b ) if a tree does not split on c anywhere on the path to a terminal node , then that node returns a function fm ( cid : 123 ) x ( cid : 123 ) j ( cid : 123 ) = gm ( cid : 123 ) x ( cid : 123 ) that contributes nothing to the classication decision .
however , as long as a tree includes a split on c at least once on every path to a terminal node , it will make a contribution to the classier for all input feature values .
the advantage or disadvantage of building one large tree using class label as an additional input feature is not clear .
no motivation is provided .
we therefore implement adaboost . mh using the more traditional direct j=123 e exp ( cid : 123 ) yjfj ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) approach of building j separate trees to minimize
we have thus shown
result 123
the adaboost . mh algorithm for a j - class problem ts j uncou -
pled additive logistic models , gj ( cid : 123 ) x ( cid : 123 ) = 123 against the rest .
123 log pj ( cid : 123 ) x ( cid : 123 ) / ( cid : 123 ) 123 pj ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) , each class
adaboost . mh ( schapire and singer ( 123 ) )
expand the original n observations into n j pairs ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) yi123 ( cid : 123 ) , is the ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) yi123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) j ( cid : 123 ) ( cid : 123 ) yij ( cid : 123 ) ( cid : 123 ) response for class j and observation i .
f ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) j ( cid : 123 ) ( cid : 123 ) r ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) j ( cid : 123 ) = ( cid : 123 ) 123
apply real adaboost to the augmented dataset , producing a function 123
output the classier arg maxj f ( cid : 123 ) x ( cid : 123 ) j ( cid : 123 ) .
i= 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n .
here yij
m fm ( cid : 123 ) x ( cid : 123 ) j ( cid : 123 ) .
algorithm 123
the adaboost . mh algorithm converts the j class problem into that of estimating a two class classier on a training set j times as large , with an additional feature dened by the set of class labels .
friedman , t .
hastie and r .
tibshirani
in principal this parametrization is ne , since gj ( cid : 123 ) x ( cid : 123 ) is monotone in pj ( cid : 123 ) x ( cid : 123 ) .
however , we are estimating the gj ( cid : 123 ) x ( cid : 123 ) in an uncoupled fashion , and there is no guarantee that the implied probabilities sum to 123
we give some examples where this makes a difference , and adaboost . mh performs more poorly than an alternative coupled likelihood procedure .
schapire and singers adaboost . mh was also intended to cover situations where observations can belong to more than one class .
the mh represents multi - label hamming , hamming loss being used to measure the errors in the space of 123j possible class labels .
in this context tting a separate classier for each label is a reasonable strategy .
however , schapire and singer also propose using adaboost . mh when the class labels are mutually exclusive , which is the focus in this paper .
algorithm 123 is a natural generalization of algorithm 123 for tting the j - class
logistic regression model ( 123 ) .
result 123
the logitboost algorithm ( j classes , population version ) uses quasi - newton steps for tting an additive symmetric logistic model by
logitboost ( j classes )
start with weights wij = 123 / n ( cid : 123 ) i = 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n ( cid : 123 ) j = 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) j ( cid : 123 ) fj ( cid : 123 ) x ( cid : 123 ) = 123 and
pj ( cid : 123 ) x ( cid : 123 ) = 123 / j j .
repeat for m = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) m : ( a ) repeat for j = 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) j :
( i ) compute working responses and weights in the jth class ,
( b ) set fmj ( cid : 123 ) x ( cid : 123 ) j123 ( c ) update pj ( cid : 123 ) x ( cid : 123 ) via ( 123 ) .
output the classier arg maxj fj ( cid : 123 ) x ( cid : 123 ) .
algorithm 123
an adaptive newton algorithm for tting an additive mul -
tiple logistic regression model .
pj ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) 123 pj ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) wij = pj ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) 123 pj ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
( ii ) fit the function fmj ( cid : 123 ) x ( cid : 123 ) by a weighted least - squares regression of zij
to xi with weights wij .
k=123 fmk ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) , and fj ( cid : 123 ) x ( cid : 123 ) fj ( cid : 123 ) x ( cid : 123 ) +
additive logistic regression
we rst give the score and hessian for the population newton algorithm
corresponding to a standard multilogit parametrization
gj ( cid : 123 ) x ( cid : 123 ) = log
j = 123 ( cid : 123 ) x ( cid : 123 ) j = 123 ( cid : 123 ) x ( cid : 123 )
with gj ( cid : 123 ) x ( cid : 123 ) = 123 ( and the choice of j for the base class is arbitrary ) .
the expected conditional log - likelihood is :
e ( cid : 123 ) l ( cid : 123 ) g + g ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) = j123 ( cid : 123 )
j ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) gj ( cid : 123 ) x ( cid : 123 ) + gj ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 + j123 ( cid : 123 ) j pj ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) j = 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) j 123 ( cid : 123 )
sj ( cid : 123 ) x ( cid : 123 ) = e ( cid : 123 ) y
hj ( cid : 123 ) k ( cid : 123 ) x ( cid : 123 ) = pj ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) jk pk ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) j ( cid : 123 ) k = 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) j 123 ( cid : 123 )
our quasi - newton update amounts to using a diagonal approximation to
the hessian , producing updates :
pj ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 pj ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) j = 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) j 123 ( cid : 123 )
and set fj ( cid : 123 ) x ( cid : 123 ) = gj ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 / j ( cid : 123 ) ( cid : 123 ) j 123
to convert to the symmetric parametrization , we would note that gj = 123 , k=123 gk ( cid : 123 ) x ( cid : 123 ) .
however , this procedure could be applied using any class as the base , not just the jth .
by averaging over all choices for the base class , we get the update
pj ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 pj ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123
for more rigid parametric models and full newton stepping , this sym - metrization would be redundant .
with quasi - newton steps and adaptive ( tree based ) models , the symmetrization removes the dependence on the choice of the base class .
simulation studies .
in this section the four avors of boosting out - lined above are applied to several articially constructed problems .
compar - isons based on real data are presented in section 123
an advantage of comparisons made in a simulation setting is that all as - pects of each example are known , including the bayes error rate and the complexity of the decision boundary .
in addition , the population expected error rates achieved by each of the respective methods can be estimated to arbitrary accuracy by averaging over a large number of different training and test data
friedman , t .
hastie and r .
tibshirani
sets drawn from the population .
the four boosting methods compared here
dab : discrete adaboostalgorithm 123
rab : real adaboostalgorithm 123
lb : logitboostalgorithms 123 and 123
gab : gentle adaboostalgorithm 123
dab , rab and gab handle multiple classes using the adaboost . mh
in an attempt to differentiate performance , all of the simulated examples involve fairly complex decision boundaries .
the ten input features for all exam - ples are randomly drawn from a ten - dimensional standard normal distribution x n123 ( cid : 123 ) 123 ( cid : 123 ) i ( cid : 123 ) .
for the rst three examples the decision boundaries separating successive classes are nested concentric ten - dimensional spheres constructed by thresholding the squared - radius from the origin
r123 = 123 ( cid : 123 )
i < tk ( cid : 123 )
ck = ( cid : 123 ) xi ( cid : 123 ) tk123 r123
each class ck ( cid : 123 ) 123 k k ( cid : 123 ) is dened as the subset of observations with t123 = 123 and tk = .
the ( cid : 123 ) tk ( cid : 123 ) k123 for each example were chosen so as to put approximately equal numbers of observations in each class .
the training sample size is n = k 123 so that approximately 123 training observations are in each class .
an independently drawn test set of 123 , 123 observations was used to estimate error rates for each training set .
averaged results over ten such independently drawn trainingtest set combinations were used for the nal error rate estimates .
the corresponding statistical uncertainties ( stan - dard errors ) of these nal estimates ( averages ) are approximately a line width on each plot .
figure 123 ( top left ) compares the four algorithms in the two - class ( cid : 123 ) k = 123 ( cid : 123 ) case using a two - terminal node decision tree ( stump ) as the base classier .
shown is error rate as a function of number of boosting iterations .
the upper ( black ) line represents dab and the other three nearly coincident lines are the other three methods ( dotted red = rab , short - dashed green = lb , and long - dashed blue = gab ) .
note that the somewhat erratic behavior of dab , especially for less than 123 iterations , is not due to statistical uncertainty .
for less than 123 iterations lb has a minuscule edge , after that it is a dead heat with rab and gab .
dab shows substantially inferior performance here with roughly twice the error rate at all iterations .
figure 123 ( lower left ) shows the corresponding results for three classes ( cid : 123 ) k = 123 ( cid : 123 ) again with two - terminal node trees .
here the problem is more difcult as represented by increased error rates for all four methods , but their relation - ship is roughly the same : the upper ( black ) line represents dab and the other
additive logistic regression
test error curves for the simulation experiment with an additive decision boundary , as described in ( cid : 123 ) 123 ( cid : 123 ) .
in all panels except the top right , the solid curve ( representing discrete adaboost ) lies alone above the other three curves .
three nearly coincident lines are the other three methods .
the situation is somewhat different for larger number of classes .
figure 123 ( lower right ) shows results for k = 123 which are typical for k 123
as before , dab incurs much higher error rates than all the others , and rab and gab have nearly iden - tical performance .
however , the performance of lb relative to rab and gab has changed .
up to about 123 iterations it has the same error rate .
from 123 to about 123 iterations lbs error rates are slightly higher than the other two .
after 123 iterations the error rate for lb continues to improve whereas that
friedman , t .
hastie and r .
tibshirani
for rab and gab level off , decreasing much more slowly .
by 123 iterations the error rate for lb is 123 whereas that for rab and gab is 123 .
speculation as to the reason for lbs performance gain in these situations is presented
in the above examples a stump was used as the base classier .
one might expect the use of larger trees would do better for these rather complex prob - lems .
figure 123 ( top right ) shows results for the two - class problem , here boost - ing trees with eight terminal nodes .
these results can be compared to those for stumps in figure 123 ( top left ) .
initially , error rates for boosting eight - node trees decrease much more rapidly than for stumps , with each successive iteration , for all methods .
however , the error rates quickly level off and improvement is very slow after about 123 iterations .
the overall performance of dab is much improved with the bigger trees , coming close to that of the other three methods .
as before rab , gab and lb exhibit nearly identical performance .
note that at each iteration the eight - node tree model consists of four times the number of additive terms as does the corresponding stump model .
this is why the error rates decrease so much more rapidly in the early iterations .
in terms of model complexity ( and training time ) , a 123 - iteration model using eight - terminal node trees is equivalent to a 123 - iteration stump model .
comparing the top two panels in figure 123 , one sees that for rab , gab and lb the error rate using the bigger trees ( 123 ) is in fact 123% higher than that for stumps ( 123 ) at 123 iterations , even though the former is four times more complex .
this seemingly mysterious behavior is easily understood by examining the nature of the decision boundary separating the classes .
the bayes decision boundary between two classes is the set
p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) p ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) = 123
or simply ( cid : 123 ) x ( cid : 123 ) b ( cid : 123 ) x ( cid : 123 ) = 123 ( cid : 123 ) .
to approximate this set it is sufcient to estimate the logit b ( cid : 123 ) x ( cid : 123 ) , or any monotone transformation of b ( cid : 123 ) x ( cid : 123 ) , as closely as possible .
as discussed above , boosting produces an additive logistic model whose com - ponent functions are represented by the base classier .
with stumps as the base classier , each component function has the form
m123 ( cid : 123 ) xjtm ( cid : 123 ) + cr
fm ( cid : 123 ) x ( cid : 123 ) = cl
if the mth stump chose to split on coordinate j .
here tm is the split - point , m are the weighted means of the response in the left and right terminal nodes .
thus the model produced by boosting stumps is additive in the original features ,
m and cr
f ( cid : 123 ) x ( cid : 123 ) = p ( cid : 123 )
where gj ( cid : 123 ) xj ( cid : 123 ) adds together all those stumps involving xj ( and is 123 if none
additive logistic regression
examination of ( 123 ) and ( 123 ) reveals that an optimal decision boundary for the above examples is also additive in the original features , with fj ( cid : 123 ) xj ( cid : 123 ) = x123 constant .
thus , in the context of decision trees , stumps are ideally matched to these problems; larger trees are not needed .
however boosting larger trees need not be counterproductive in this case if all of the splits in each individual tree are made on the same predictor variable .
this would also produce an additive model in the original features ( 123 ) .
however , due to the forward greedy stagewise strategy used by boosting , this is not likely to happen if the decision boundary function involves more than one predictor; each individual tree will try to do its best to involve all of the important predictors .
owing to the nature of decision trees , this will produce models with interaction effects; most terms in the model will involve products in more than one variable .
such nonadditive models are not as well suited for approximating truly additive decision boundaries such as ( 123 ) and ( 123 ) .
this is reected in increased error rate as observed in figure 123
the above discussion also suggests that if the decision boundary separating pairs of classes were inherently nonadditive in the predictors , then boosting stumps would be less advantageous than using larger trees .
a tree with m terminal nodes can produce basis functions with a maximum interaction order of min ( cid : 123 ) m 123 ( cid : 123 ) p ( cid : 123 ) where p is the number of predictor features .
these higher order basis functions provide the possibility to more accurately estimate those decision boundaries b ( cid : 123 ) x ( cid : 123 ) with high - order interactions .
the purpose of the next example is to verify this intuition .
there are two classes ( cid : 123 ) k = 123 ( cid : 123 ) and 123 training observations with the ( cid : 123 ) xi ( cid : 123 ) 123 drawn from a ten - dimensional normal distribution as in the previous examples .
class labels were randomly assigned to each observation with log - odds pr ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 ) pr ( cid : 123 ) y = 123 ( cid : 123 ) x ( cid : 123 )
123 + 123 ( cid : 123 )
approximately equal numbers of observations are assigned to each of the two classes , and the bayes error rate is 123 .
the decision boundary for this problem is a complicated function of the rst six predictor variables involving all of them in second - order interactions of equal strength .
as in the above examples , test sets of 123 , 123 observations was used to estimate error rates for each training set , and nal estimates were averages over ten replications .
figure 123 ( top left ) shows test - error rate as a function of iteration number for each of the four boosting methods using stumps .
as in the previous examples , rab and gab track each other very closely .
dab begins very slowly , being dominated by all of the others until around 123 iterations , where it passes below rab and gab .
lb mostly dominates , having the lowest error rate until about 123 iterations .
at that point dab catches up and by 123 iterations it may have a very slight edge .
however , none of these boosting methods perform well with stumps on this problem , the best error rate being 123 .
figure 123 ( top right ) shows the corresponding plot when four terminal node trees are boosted .
here there is a dramatic improvement with all of the four
friedman , t .
hastie and r .
tibshirani
test error curves for the simulation experiment with a nonadditive decision boundary , as described in ( 123 ) .
methods .
for the rst time there is some small differentiation between rab and gab .
at nearly all iterations the performance ranking is lb best , followed by gab , rab and dab in order .
at 123 iterations lb achieves an error rate of 123 .
figure 123 ( lower left ) shows results when eight terminal node trees are boosted .
here , error rates are generally further reduced with lb improving the least ( 123 ) , but still dominating .
the performance ranking among the other three methods changes with increasing iterations; dab overtakes rab
additive logistic regression
at around 123 iterations and gab at about 123 becoming fairly close to lb by 123 iterations with an error rate of 123 .
although limited in scope , these simulation studies suggest several trends .
they explain why boosting stumps can sometimes be superior to using larger trees , and suggest situations where this is likely to be the case; that is when decision boundaries b ( cid : 123 ) x ( cid : 123 ) can be closely approximated by functions that are additive in the original predictor features .
when higher order interactions are required , stumps exhibit poor performance .
these examples illustrate the close similarity between rab and gab .
in all cases the difference in perfor - mance between dab and the others decreases when larger trees and more iterations are used , sometimes overtaking the others .
more generally , relative performance of these four methods depends on the problem at hand in terms of the nature of the decision boundaries , the complexity of the base classier and the number of boosting iterations .
the superior performance of lb in figure 123 ( lower right ) appears to be a consequence of the multiclass logistic model ( algorithm 123 ) .
all of the other methods use the asymmetric adaboost . mh strategy ( algorithm 123 ) of building separate two - class models for each individual class against the pooled com - plement classes .
even if the decision boundaries separating all class pairs are relatively simple , pooling classes can produce complex decision bound - aries that are difcult to approximate ( friedman ( 123 ) ) .
by considering all of the classes simultaneously , the symmetric multiclass model is better able to take advantage of simple pairwise boundaries when they exist ( hastie and tibshirani ( 123 ) ) .
as noted above , the pairwise boundaries induced by ( 123 ) and ( 123 ) are simple when viewed in the context of additive modeling , whereas the pooled boundaries are more complex; they cannot be well approximated by functions that are additive in the original predictor variables .
the decision boundaries associated with these examples were deliberately chosen to be geometrically complex in an attempt to elicit performance differ - ences among the methods being tested .
such complicated boundaries are not likely to often occur in practice .
many practical problems involve compara - tively simple boundaries ( holte ( 123 ) ) ; in such cases performance differences will still be situation dependent , but correspondingly less pronounced .
some experiments with real world data .
in this section we show the results of running the four tting methods : logitboost , discrete adaboost , real adaboost and gentle adaboost on a collection of datasets from the uc - irvine machine learning archive , plus a popular simulated dataset .
the base learner is a tree in each case , with either two or eight terminal nodes .
for com - parison , a single decision tree was also t ( using the tree function in splus ) , with the tree size determined by 123 - fold cross - validation .
the datasets are summarized in table 123
the test error rates are shown in table 123 for the smaller datasets , and in table 123 for the larger ones .
the vowel , sonar , satimage and letter datasets come with a prespecied test set .
the waveform data is simulated , as described in breiman , friedman , olshen and
friedman , t .
hastie and r .
tibshirani
datasets used in the experiments
stone ( 123 ) .
for the others , 123 - fold cross - validation was used to estimate the
it is difcult to discern trends on the small datasets ( table 123 ) because all but quite large observed differences in performance could be attributed to sampling uctuations .
on the vowel , breast cancer , ionosphere , sonar and waveform data , purely additive stump models seem to perform comparably to the larger ( eight - node ) trees .
the glass data seems to benet a little from larger trees .
there is no clear differentiation in performance among the boost -
on the larger data sets ( table 123 ) clearer trends are discernible .
for the satimage data the eight - node tree models are only slightly , but signicantly , more accurate than the purely additive models .
for the letter data there is no contest .
boosting stumps is clearly inadequate .
there is no clear differ - entiation among the boosting methods for eight - node trees .
for the stumps , logitboost , real adaboost and gentle adaboost have comparable perfor - break mance , distinctly superior to discrete adaboost .
this is consistent with the results of the simulation study ( section 123 ) .
except perhaps for discrete adaboost , the real data examples fail to demon - strate performance differences between the various boosting methods .
this is in contrast to the simulated data sets of section 123
there logitboost gener - ally dominated , although often by a small margin .
the inability of the real data examples to discriminate may reect statistical difculties in estimat - ing subtle differences with small samples .
alternatively , it may be that their underlying decision boundaries are all relatively simple ( holte ( 123 ) ) so that all reasonable methods exhibit similar performance .
additive logistic trees .
in most applications of boosting the base clas - sier is considered to be a primitive , repeatedly called by the boosting proce - dure as iterations proceed .
the operations performed by the base classier are the same as they would be in any other context given the same data and weights .
the fact that the nal model is going to be a linear combination of a large number of such classiers is not taken into account .
in particular , when using decision trees , the same tree growing and pruning algorithms are
additive logistic regression
test error rates on small real examples
123 terminal nodes
123 terminal nodes
cart error = 123 ( cid : 123 ) 123
cart error = 123 ( cid : 123 ) 123
cart error = 123 ( cid : 123 ) 123
cart error = 123 ( cid : 123 ) 123
cart error = 123 ( cid : 123 ) 123
cart error = 123 ( cid : 123 ) 123
friedman , t .
hastie and r .
tibshirani
test error rates on larger data examples
cart error = 123 ( cid : 123 ) 123
cart error = 123 ( cid : 123 ) 123
generally employed .
sometimes alterations are made ( such as no pruning ) for programming convenience and speed .
when boosting is viewed in the light of additive modeling , however , this greedy approach can be seen to be far from optimal in many situations .
as discussed in section 123 the goal of the nal classier is to produce an accurate approximation to the decision boundary function b ( cid : 123 ) x ( cid : 123 ) .
in the context of boost - ing , this goal applies to the nal additive model , not to the individual terms ( base classiers ) at the time they were constructed .
for example , it was seen in section 123 that if b ( cid : 123 ) x ( cid : 123 ) was close to being additive in the original predictive features , then boosting stumps was optimal since it produced an approxima - tion with the same structure .
building larger trees increased the error rate of the nal model because the resulting approximation involved high - order interactions among the features .
the larger trees optimized error rates of the individual base classiers , given the weights at that step , and even produced lower unweighted error rates in the early stages .
however , after a sufcient number of boosts , the stump - based model achieved superior performance .
more generally , one can consider an expansion of the decision boundary
function in a functional anova decomposition ( friedman ( 123 ) )
j ( cid : 123 ) k ( cid : 123 ) l
b ( cid : 123 ) x ( cid : 123 ) = ( cid : 123 )
fj ( cid : 123 ) xj ( cid : 123 ) + ( cid : 123 )
fjk ( cid : 123 ) xj ( cid : 123 ) xk ( cid : 123 ) + ( cid : 123 )
fjkl ( cid : 123 ) xj ( cid : 123 ) xk ( cid : 123 ) xl ( cid : 123 ) + ( cid : 123 )
additive logistic regression
the rst sum represents the closest function to b ( cid : 123 ) x ( cid : 123 ) that is additive in the original features , the rst two represent the closest approximation involv - ing at most two - feature interactions , the rst three represent three - feature interactions , and so on .
if b ( cid : 123 ) x ( cid : 123 ) can be accurately approximated by such an expansion , truncated at low interaction order , then allowing the base classi - er to produce higher order interactions can reduce the accuracy of the nal boosted model .
in the context of decision trees , higher order interactions are produced by deeper trees .
in situations where the true underlying decision boundary function admits a low order anova decomposition , one can take advantage of this structure to improve accuracy by restricting the depth of the base decision trees to be not much larger than the actual interaction order of b ( cid : 123 ) x ( cid : 123 ) .
since this is not likely to be known in advance for any particular problem , this maximum depth becomes a meta - parameter of the procedure to be estimated by some model selection technique , such as cross - validation .
one can restrict the depth of an induced decision tree by using its stan - dard pruning procedure , starting from the largest possible tree , but requiring it to delete enough splits to achieve the desired maximum depth .
this can be computationally wasteful when this depth is small .
the time required to build the tree is proportional to the depth of the largest possible tree before pruning .
therefore , dramatic computational savings can be achieved by sim - ply stopping the growing process at the maximum depth , or alternatively at a maximum number of terminal nodes .
the standard heuristic arguments in favor of growing large trees and then pruning do not apply in the context of boosting .
shortcomings in any individual tree can be compensated by trees grown later in the boosting sequence .
if a truncation strategy based on number of terminal nodes is to be employed , it is necessary to dene an order in which splitting takes place .
we adopt a best - rst strategy .
an optimal split is computed for each currently terminal node .
the node whose split would achieve the greatest reduction in the tree building criterion is then actually split .
this increases the number of terminal nodes by one .
this continues until a maximum number m of ter - minal notes is induced .
standard computational tricks can be employed so that inducing trees in this order requires no more computation than other orderings commonly used in decision tree induction .
the truncation limit m is applied to all trees in the boosting sequence .
it is thus a meta - parameter of the entire boosting procedure .
an optimal value can be estimated through standard model selection techniques such as minimizing cross - validated error rate of the nal boosted model .
we refer to this combi - nation of truncated best - rst trees , with boosting , as additive logistic trees ( alt ) .
best - rst trees were used in all of the simulated and real examples .
one can compare results on the latter ( tables 123 and 123 ) to corresponding results reported by dietterich ( ( 123 ) , table 123 ) on common data sets .
error rates achieved by alt with very small truncation values are seen to compare quite favorably with other committee approaches using much larger trees at each boosting step .
even when error rates are the same , the computational savings
friedman , t .
hastie and r .
tibshirani
coordinate functions for the additive logistic tree obtained by boosting ( logitboost ) with stumps , for the two - class nested sphere example from section 123
associated with alt can be quite important in data mining contexts where large data sets cause computation time to become an issue .
another advantage of low order approximations is model visualization .
in particular , for models additive in the input features ( 123 ) , the contribution of each feature xj can be viewed as a graph of gj ( cid : 123 ) xj ( cid : 123 ) plotted against xj .
figure 123 shows such plots for the ten features of the two - class nested spheres example of figure 123
the functions are shown for the rst class concentrated near the origin; the corresponding functions for the other class are the negatives of
the plots in figure 123 clearly show that the contribution to the log - odds of each individual feature is approximately quadratic , which matches the gener - ating model ( 123 ) and ( 123 ) .
when there are more than two classes , plots similar to figure 123 can be made for each class and analogously interpreted .
higher order interaction models are more difcult to visualize .
if there are at most two - feature interactions , the two - variable contributions can be visualized using contour or perspective mesh plots .
beyond two - feature interactions , visualization techniques are even less effective .
even when noninteraction ( stump ) models do not achieve the highest accuracy , they can be very useful as descriptive statistics owing to the interpretability of the resulting model .
weight trimming .
in this section we propose a simple idea and show that it can dramatically reduce computation for boosted models without sacri - cing accuracy .
despite its apparent simplicity , this approach does not appear to be in common use ( although similar ideas have been proposed before : schapire ( 123 ) , freund ( 123 ) ) .
at each boosting iteration there is a distribution of weights over the training sample .
as iterations proceed , this distribution tends to become highly skewed towards smaller weight values .
a larger fraction of the training sample becomes correctly classied with increasing condence , thereby receiving smaller weights .
observations with very low relative weight have little impact on training of the base classier; only those that carry the dominant proportion of the weight mass are inuen -
additive logistic regression
the fraction of such high weight observations can become very small in later iterations .
this suggests that at any iteration one can simply delete from the training sample the large fraction of observations with very low weight without having much effect on the resulting induced classier .
however , com - putation is reduced since it tends to be proportional to the size of the training sample , regardless of weights .
at each boosting iteration , training observations with weight wi less than a threshold wi < t ( cid : 123 ) ( cid : 123 ) are not used to train the classier .
we take the value of t ( cid : 123 ) ( cid : 123 ) to be the th quantile of the weight distribution over the training data at the corresponding iteration .
that is , only those observations that carry the fraction 123 of the total weight mass are used for training .
typically ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) so that the data used for training carries from 123 to 123% of the total weight mass .
note that the weights for all training observations are recomputed at each iteration .
observations deleted at a particular iteration may therefore reenter at later iterations if their weights subsequently increase relative to other observations .
figure 123 ( left panel ) shows test - error rate as a function of iteration number for the letter recognition problem described in section 123 , here using gentle adaboost and eight - node trees as the base classier .
two error rate curves are shown .
the black solid one represents using the full training sample at each iteration ( cid : 123 ) = 123 ( cid : 123 ) , whereas the blue dashed curve represents the corresponding error rate for = 123 ( cid : 123 ) 123
the two curves track each other very closely , especially at the later iterations .
figure 123 ( right panel ) shows the corresponding frac - tion of observations used to train the base classier as a function of iteration number .
here the two curves are not similar .
with = 123 ( cid : 123 ) 123 the number of observations used for training drops very rapidly reaching roughly 123% of the total at 123 iterations .
by 123 iterations it is down to about 123% where it stays throughout the rest of the boosting procedure .
thus , computation is reduced
the left panel shows the test error for the letter recognition problem as a function of iteration number .
the black solid curve uses all the training data , the red dashed curve uses a subset based on weight thresholding .
the right panel shows the percent of training data used for both approaches .
the upper curve steps down , because training can stop for an entire class if it is t sufciently well ( see text ) .
friedman , t .
hastie and r .
tibshirani
by over a factor of 123 with no apparent loss in classication accuracy .
the reason why sample size in this case decreases for = 123 after 123 iterations is that if all of the observations in a particular class are classied correctly with very high condence ( cid : 123 ) fk > 123+ log ( cid : 123 ) n ( cid : 123 ) ( cid : 123 ) training for that class stops , and con - tinues only for the remaining classes .
at 123 iterations , 123 classes remained of the original 123 classes .
the last column labeled fraction in table 123 for the letter - recognition problem shows the average fraction of observations used in training the base classiers over the 123 iterations , for all boosting methods and tree sizes .
for eight - node trees , all methods behave as shown in figure 123
with stumps , logitboost uses considerably less data than the others and is thereby correspondingly faster .
this is a genuine property of logitboost that sometimes gives it an advan - tage with weight trimming .
unlike the other methods , the logitboost weights wi = pi ( cid : 123 ) 123 pi ( cid : 123 ) do not in any way involve the class outputs yi; they simply measure nearness to the currently estimated decision boundary fm ( cid : 123 ) x ( cid : 123 ) = 123
discarding small weights thus retains only those training observations that are estimated to be close to the boundary .
for the other three procedures the weight is monotone in yifm ( cid : 123 ) xi ( cid : 123 ) .
this gives highest weight to currently mis - classied training observations , especially those far from the boundary .
if after trimming , the fraction of observations remaining is less than the error rate , the subsample passed to the base learner will be highly unbalanced contain - ing very few correctly classied observations .
this imbalance seems to inhibit learning .
no such imbalance occurs with logitboost since near the decision boundary , correctly and misclassied observations appear in roughly equal
as this example illustrates , very large reductions in computation for boost - ing can be achieved by this simple trick .
a variety of other examples ( not shown ) exhibit similar behavior with all boosting methods .
note that other committee approaches to classication such as bagging ( breiman ( 123 ) ) and randomized trees ( dietterich ( 123 ) ) while admitting parallel implementa - tions , cannot take advantage of this approach to reduce computation .
further generalizations of boosting .
we have shown above that adaboost ts an additive model , optimizing a criterion similar to binomial log - likelihood , via an adaptive newton method .
this suggests ways in which the boosting paradigm may be generalized .
first , the newton step can be replaced by a gradient step , slowing down the tting procedure .
this can reduce susceptibility to overtting and lead to improved performance .
sec - ond , any smooth loss function can be used : for regression , squared error is natural , leading to the tting of residuals boosting algorithm mentioned in the introduction .
however , other loss functions might have benets , for example , tapered squared error based on hubers robust inuence function ( huber ( 123 ) ) .
the resulting procedure is a fast , convenient method for resis - tant tting of additive models .
details of these generalizations may be found in friedman ( 123 ) .
additive logistic regression
concluding remarks .
in order to understand a learning procedure statistically it is necessary to identify two important aspects : its structural model and its error model .
the former is most important since it determines the function space of the approximator , thereby characterizing the class of functions or hypotheses that can be accurately approximated with it .
the error model species the distribution of random departures of sampled data from the structural model .
it thereby denes the criterion to be optimized in the estimation of the structural model .
we have shown that the structural model for boosting is additive on the logistic scale with the base learner providing the additive components .
this understanding alone explains many of the properties of boosting .
it is no sur - prise that a large number of such ( jointly optimized ) components denes a much richer class of learners than one of them alone .
it reveals that in the context of boosting all base learners are not equivalent , and there is no uni - versally best choice over all situations .
as illustrated in section 123 , the base learners need to be chosen so that the resulting additive expansion matches the particular decision boundary encountered .
even in the limited context of boosting decision trees the interaction order , as characterized by the number of terminal nodes , needs to be chosen with care .
purely additive models induced by decision stumps are sometimes , but not always , the best .
however , we con - jecture that boundaries involving very high - order interactions will rarely be encountered in practice .
this motivates our additive logistic trees ( alt ) pro - cedure described in section 123
the error model for two - class boosting is the obvious one for binary vari - ables , namely the bernoulli disribution .
we show that the adaboost proce - dures maximize a criterion that is closely related to expected log - bernoulli likelihood , having the identical solution in the distributional ( l123 ) limit of innite data .
we derived a more direct procedure for maximizing this log - likelihood ( logitboost ) and show that it exhibits properties nearly identical to those of real adaboost .
in the multiclass case , the adaboost procedures maximize a separate bernoulli likelihood for each class versus the others .
this is a natural choice and is especially appropriate when observations can belong to more than one class ( schapire and singer ( 123 ) ) .
in the more usual setting of a unique class label for each observation , the symmetric multinomial distribution is a more appropriate error model .
we develop a multiclass logitboost proce - dure that maximizes the corresponding log - likelihood by quasi - newton step - ping .
we show through simulated examples that there exist settings where this approach leads to superior performance , although none of these situa - tions seems to have been encountered in the set of real data examples used for illustration; the performance of both approaches had quite similar perfor - mance over these examples .
the concepts developed in this paper suggest that there is very little , if any , connection between ( deterministic ) weighted boosting and other ( randomized ) ensemble methods such as bagging ( breiman ( 123 ) ) and randomized trees ( dietterich ( 123 ) ) .
in the language of least - squares regression , the latter are
friedman , t .
hastie and r .
tibshirani
purely variance reducing procedures intended to mitigate instability , espe - cially that associated with decision trees .
boosting on the other hand seems fundamentally different .
it appears to be mainly a bias reducing procedure , intended to increase the exibility of stable ( highly biased ) weak learners by incorporating them in a jointly tted additive expansion .
the distinction becomes less clear ( breiman ( 123a ) ) when boosting is implemented by nite weighted random sampling instead of weighted opti - mization .
the advantages or disadvantages of introducing randomization into boosting by drawing nite samples is not clear .
if there turns out to be an advantage with randomization in some situations , then the degree of random - ization , as reected by the sample size , is an open question .
it is not obvious that the common choice of using the size of the original training sample is optimal in all ( or any ) situations .
one fascinating issue not covered in this paper is the fact that boosting , whatever avor , seems resistant to overtting .
some possible explanations are :
as the logitboost iterations proceed , the overall impact of changes intro - duced by fm ( cid : 123 ) x ( cid : 123 ) reduces .
only observations with appreciable weight deter - mine the new functionsthose near the decision boundary .
by denition these observations have f ( cid : 123 ) x ( cid : 123 ) near zero and can be affected by changes , while those in pure regions have large values of ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) and are less likely to be modied .
the stagewise nature of the boosting algorithms does not allow the full col - lection of parameters to be jointly t , and thus has far lower variance than the full parameterization might suggest .
in the computational learning the - ory literature this is explained in terms of vc dimension of the ensemble compared to that of each weak learner .
real adaboost ( stumps ) on a noisy concentric - sphere problem , with 123 observations per class and bayes error 123% .
the test error ( upper curve ) increases after about fty iterations .
additive logistic regression
classiers are hurt less by overtting than other function estimators ( e . g . , the famous risk bound of the 123 - nearest - neighbor classier , cover and
figure 123 shows a case where boosting does overt .
the data are gener - ated from two ten - dimensional spherical gaussians with the same mean , and variances chosen so that the bayes error is 123% ( 123 samples per class ) .
we used real adaboost and stumps ( the results were similar for all the boosting algorithms ) .
after about 123 iterations the test error ( slowly ) increases .
schapire , freund , bartlett and lee ( 123 ) suggest that the properties of adaboost , including its resistance to overtting , can be understood in terms of classication margins .
however , breiman ( 123 ) presents evidence counter to this explanation .
whatever the explanation , the empirical evidence is strong; the introduction of boosting by schapire , freund and colleagues has brought an exciting and important set of new ideas to the table .
acknowledgments .
we thank andreas buja for alerting us to the recent work on text classication at at&t laboratories , bogdan popescu for illu - minating discussions on pac learning theory and leo breiman and robert schapire for useful comments on an earlier version of this paper .
we also thank two anonymous referees and an associate editor for detailed and use - ful comments on an earlier draft of the paper .
