summary .
the lasso penalizes a least squares regression by the sum of the absolute values ( l123 - norm ) of the coefcients .
the form of this penalty encourages sparse solutions ( with many coefcients equal to 123 ) .
we propose the fused lasso , a generalization that is designed for prob - lems with features that can be ordered in some meaningful way .
the fused lasso penalizes the l123 - norm of both the coefcients and their successive differences .
thus it encourages sparsity of the coefcients and also sparsity of their differencesi . e .
local constancy of the coefcient prole .
the fused lasso is especially useful when the number of features p is much greater than n , the sample size .
the technique is also extended to the hinge loss function that underlies the support vector classier . we illustrate the methods on examples from protein mass spectroscopy and gene expression data .
keywords : fused lasso; gene expression; lasso; least squares regression; protein mass spectroscopy; sparse solutions; support vector classier
we consider a prediction problem with n cases having outcomes y123 , y123 , .
, yn and features xij , i= 123 , 123 , .
, n , j= 123 , 123 , .
the outcome can be quantitative , or equal to 123 or 123 , representing two classes like healthy and diseased .
we also assume that the xij are realizations of features xj that can be ordered as x123 , x123 , .
, xp in some meaningful way .
our goal is to predict y from x123 , x123 , .
we are especially interested in problems for which p ( cid : 123 ) n .
a motivating example comes from protein mass spectroscopy , in which we observe , for each blood serum sample i , the intensity xij for many time - of - ight values tj .
time of ight is related to the mass over charge ratio m=z of the constituent proteins in the blood .
123 shows an example that is taken from adam et al .
( 123 ) : the average spectra for healthy patients and those with prostate cancer .
there are 123 m=z - sites in total .
the full data set consists of 123 healthy patients and 123 with cancer , and the goal is to nd m=z - sites that discriminate between the two groups .
address for correspondence : robert tibshirani , department of health research and policy , h r p redwood
building , stanford university , stanford , ca 123 - 123 , usa .
123 royal statistical society
tibshirani , m .
saunders , s .
rosset , j .
zhu and k .
knight
protein mass spectroscopy data : average proles from normal ( (
) and prostate cancer patients
there has been much interest in this problem in the past few years; see for example petricoin et al .
( 123 ) and adam et al .
( 123 ) .
in other examples , the order of the features may not be xed a priori but may instead be estimated from the data .
an example is gene expression data measured from a microarray .
hier - archical clustering can be used to estimate an ordering of the genes , putting correlated genes near one another in the list .
we illustrate our methods on both protein mass spectroscopy and microarray data in this paper .
in section 123 we dene the fused lasso and illustrate it on a simple example .
section 123 describes computation of the solutions .
section 123 explores asymptotic properties .
in section 123 we relate the fused lasso to soft threshold methods and wavelets .
degrees of freedom of the fused lasso t are discussed in section 123
a protein mass spectroscopy data set on prostate cancer is analysed in section 123 , whereas section 123 carries out a simulation study .
an application of the method to unordered features is discussed in section 123 and illustrated on a microarray data set in section 123 .
the hinge loss function and support vector classiers are addressed in section 123
the lasso and fusion
we begin with a standard linear model
xijj + " i
with the errors " i having mean 123 and constant variance .
we also assume that the predictors are standardized to have mean 123 and unit variance , and the outcome yi has mean 123
hence we do not need an intercept in model ( 123 ) .
we note that p may be larger then n , and typically it is much larger than n in the applications that we consider .
many methods have been proposed for regularized or penalized regression , including ridge regression ( hoerl and kennard , 123 ) , partial least squares ( wold , 123 ) and principal components regression .
subset selection is more discrete , either including or excluding predictors from the model .
the lasso ( tibshirani , 123 ) is similar to ridge regression but uses the absolute values of the coefcients rather than their squares .
the lasso nds the coefcients = .
123 , 123 , .
, p / satisfying = arg min
the bound s is a tuning parameter : for sufciently large s we obtain the least squares solu - tion , or one of the many possible least squares solutions if p > n .
for smaller values of s , the solutions are sparse , i . e .
some components are exactly 123
this is attractive from a data analysis viewpoint , as it selects the important predictors and discards the rest .
in addition , since the criterion and constraints in condition ( 123 ) are convex , the problem can be solved even for large p ( e . g .
p= 123 ) by quadratic programming methods .
we discuss computation in detail in
unlike the lasso , ridge regression , partial least squares and principal components regression do not produce sparse models .
subset selection does produce sparse models but is not a convex operation; best subsets selection is combinatorial and is not practical for p > 123 or so .
the lasso can be applied even if p > n , and it has a unique solution assuming that no two predictors are perfectly collinear .
an interesting property of the solution is the fact that the number of non - zero coefcients is at most min . n , p / .
thus , if p= 123 and n = 123 , at most 123 coefcients in the solution will be non - zero .
the basis pursuit signal estimation method of chen et al .
( 123 ) uses the same idea as the lasso , but applied in the wavelet or other domains .
one drawback of the lasso in the present context is the fact that it ignores ordering of the features , of the type that we are assuming in this paper .
for this purpose , we propose the fused lasso dened by = arg min
|j j123| ( cid : 123 ) s123 :
|j| ( cid : 123 ) s123 and
the rst constraint encourages sparsity in the coefcients; the second encourages sparsity in their differences , i . e .
atness of the coefcient proles j as a function of j .
the term fusion is borrowed from land and friedman ( 123 ) , who proposed the use of a penalty of the form j |j j123| ( cid : 123 ) s123 for various values of , especially = 123 , 123 , 123
they did not consider the use of penalties on both j |j j123| and j |j| as in condition ( 123 ) .
123 gives a schematic view .
123 illustrates these ideas on a simulated example .
there are p= 123 predictors and n = 123 samples .
the data were generated from a model yi = jxijj + " i where the xij are standard
schematic diagram of the fused lasso , for the case n > pd 123 : we seek the rst time that the contours of the sum - of - squares loss function (
) and j jj ( cid : 123 ) j ( cid : 123 ) 123jd s123 (
) satisfy j jj jd s123 (
tibshirani , m .
saunders , s .
rosset , j .
zhu and k .
knight
gaussian , " i n . 123 , 123 / with = 123 : 123 , and there are three blocks of consecutive non - zero js shown by the black points in each of the panels .
123 ( a ) shows the univariate regression coef - cients ( red ) and a soft thresholded version of them ( green ) .
123 ( b ) shows the lasso solution ( red ) , using s123 = 123 : 123 and s123 = , and fig .
123 ( c ) shows the fusion estimate ( using s123 = and s123 = 123 ) .
these values of s123 and s123 were the values that minimized the estimated test set error .
finally fig .
123 ( d ) shows the fused lasso , using s123 = j |j| and s123 = j |j j123| , where is the true set of coefcients .
the fused lasso does the best job in estimating the true underlying coefcients .
however , the fusion method ( fig .
123 ( c ) ) performs as well as the fused lasso does in fig .
123 shows another example , with the same set - up as in fig .
123 except that = 123 : 123 and has two non - zero areasa spike at m=z= 123 and a at plateau between 123 and 123
as in the previous example , the bounds s123 and s123 were chosen in each case to minimize the prediction error .
the lasso performs poorly; fusion captures the plateau but does not clearly isolate the peak at m=z= 123
the fused lasso does a good job overall .
an alternative formulation would use a second penalty of the form j . j j123 / 123 ( cid : 123 ) s123 in place of j|j j123| ( cid : 123 ) s123 ( which was also suggested by a referee ) .
however , this has the anal - j has compared with j|j| : it does not produce a sparse solution , ogous drawback that 123 where sparsity refers to the rst differences j j123
the penalty j . j j123 / 123 ( cid : 123 ) s123 does not produce a simple piecewise constant solution , but rather a wiggly solution that is less attractive for interpretation .
the penalty j|j j123| ( cid : 123 ) s123 gives a piecewise constant solution , and this corresponds to a simple averaging of the features .
computational approach 123 .
fixed s123 and s123 criterion ( 123 ) leads to a quadratic programming problem .
for large p , the problem is difcult to solve and special care must be taken to avoid the use of p123 storage elements .
we use the two - phase active set algorithm sqopt of gill et al .
( 123 ) , which is designed for quadratic programming problems with sparse linear constraints .
let j =
j ( cid : 123 ) 123
let l be a p p matrix with lii = 123 , li+123 , i =123 and lij = 123 otherwise so that let x be the n p matrix of features and y and be n - and p - vectors of outcomes and
let e be a column p - vector of 123s , and i be the p p identity matrix .
j ( cid : 123 ) 123
dene j = j j123 for j > 123 and 123= 123
let j =
coefcients respectively .
we can write problem ( 123 ) as
= arg min ( . y x / ts . y x / )
( cid : 123 ) 123
the big matrix is of dimen - in addition to the non - negativity constraints sion . 123p + 123 / 123p but has only 123p 123 non - zero elements .
here a123 = . , 123 , 123 , .
since 123 = 123 , setting its bounds at avoids a double penalty for |123| .
similarly e123 = e with the rst component set to 123
simulated example , with pd 123 predictors having coefcients shown by the black lines : ( a ) uni - variate regression coefcients ( red ) and a soft thresholded version of them ( green ) ; ( b ) lasso solution ( red ) , using s123 d 123 : 123 and s123 d; ( c ) fusion estimate , using s123 d and s123 d 123 ( these values of s123 and s123 mini - mized the estimated test set error ) ; ( d ) the fused lasso , using s123 d j jj j and s123 d j jj ( cid : 123 ) j ( cid : 123 ) 123j , where
is the true set of coefcients
the sqopt package requires the user to write a procedure that computes xtxv for p - vectors v that are under consideration .
for many choices of the bounds s123 and s123 , the vector v is very sparse and hence xt . xv / can be efciently computed .
the algorithm is also well suited for warm starts : starting at a solution for a given s123 and s123 , the solution for nearby values of these bounds can be found relatively quickly .
search strategy for moderate - sized problems ( p ( cid : 123 ) 123 and n ( cid : 123 ) 123 say ) , the above procedure is sufciently
tibshirani , m .
saunders , s .
rosset , j .
zhu and k .
knight
simulated example with only two areas of non - zero coefcients ( black points and lines; red points , estimated coefcients from each method ) : ( a ) lasso , s123 d 123 : 123; ( b ) fusion , s123 d 123 : 123; ( c ) fused lasso , s123 d 123 : 123 , s123 d 123
simulated example of fig .
123 : ( a ) attainable values of bounds s123 and s123; ( b ) schematic diagram of the search process for the fused lasso , described in the text
fast that it can be applied over a grid of s123 - and s123 - values .
for larger problems , a more restricted search is necessary .
we rst exploit the fact that the complete sequence of lasso and fusion problems can be solved efciently by using the least angle regression ( lar ) procedure ( efron et al . , 123 ) .
the fusion problem is solved by rst transforming x to z = xl123 with = l , applying lar and then transforming back .
for a given problem , only some values of the bounds . s123 , s123 / will be attainable , i . e .
the solution vector satises both j| j|= s123 and j| j j123|= s123
123 ( a ) shows the attainable values for our simulated data example .
123 ( b ) is a schematic diagram of the search strategy .
using the lar procedure as above , we obtain solutions for bounds . s123i / , / , where s123i / is the bound giving a solution with i degrees
table 123
timings for typical runs of the fused lasso program
of freedom .
( we discuss the degrees of freedom of the fused lasso t in section 123 ) we use the lasso sequence of solutions and cross - validation or a test set to estimate an optimal degrees of freedom i .
now let
| j ( s123i / ) j123 ( s123i / ) | :
this is the largest value of the bound s123 at which it affects the solution .
the point c123 in fig .
123 ( b ) is ( s123i / , s123max ( s123i / ) ) .
we start at c123 and fuse the solutions by moving in the direction . 123 , 123 / .
in the same way , we dene points c123 to be the solution with degrees of freedom i=123 and c123 to have degrees of freedom ( i+ min . n , p / ) =123 , and we fuse the solutions from those points .
the particular direction . 123 , 123 / was chosen by empirical experimentation .
we are typically not interested in solutions that are near the pure fusion model ( the lower right boundary ) , and this search strategy tries to cover ( roughly ) the potentially useful values of ( s123 , s123 / .
this strategy is used in the real examples and simulation study that are discussed later in the paper .
for real data sets , we apply this search strategy to a training set and then evaluate the pre - diction error over a validation set .
this can be done with a single trainingvalidation split , or through vefold or tenfold cross - validation .
these are illustrated in the examples later in the
table 123 shows some typical computation times for problems of various dimensions , on a 123 ghz xeon linux computer .
some further discussion of computational issues can be found in section 123
asymptotic properties
in this section we derive results for the fused lasso that are analogous to those for the lasso ( knight and fu , 123 ) .
the penalized least squares criterion is
i / 123 +
with = . 123 , 123 , .
, p / t and xi = . xi123 , xi123 , .
xip / t , and the lagrange multipliers are functions of the sample size n .
for simplicity , we assume that p is xed with n .
these are not particularly realistic asymptotic conditions : we would prefer to have p= pn as n .
a result along these lines is probably attainable .
however , the following theorem adequately illustrates the basic dynamics of the fused lasso .
tibshirani , m .
saunders , s .
rosset , j .
zhu and k .
knight
theorem 123
( cid : 123 ) 123 ( l= 123 , 123 ) and
is non - singular then
( uj sgn . j / i . j ( cid : 123 ) = 123 / +|uj| i . j = 123 / )
( . uj uj123 / sgn . j j123 / i . j ( cid : 123 ) = j123 / +|uj uj123| i . j = j123 / )
and w has an n . 123 , 123c / distribution .
dene vn . u / by
vn . u / = n ( cid : 123 )
. |j + uj=
( |j j123 + . uj uj123 / = with u= . u123 , u123 , .
, up / t , and note that vn is minimized at
first note that
with nite dimensional convergence holding trivially .
we also have
. |j + uj=
( uj sgn . j / i . j ( cid : 123 ) = 123 / +|uj| i . j = 123 / )
( |j j123 + . uj uj123 / =
( . uj uj123 / sgn . j j123 / i . j ( cid : 123 ) = j123 / ) +
( |uj uj123| i . j = j123 / ) :
thus vn . u / d v . u / ( as dened above ) , with nite dimensional convergence holding trivially .
since vn is convex and v has a unique minimum , it follows ( geyer , 123 ) that
arg min . vn / =
as a simple example , suppose that 123 = 123 ( cid : 123 ) = 123
then the joint limiting distribution of
123n 123 / / will have probability concentrated on the line u123 = u123 when > 123 , we would see a lasso - type effect on the univariate limiting distributions , which would result in a shift of
123n 123 / ,
probability to the negative side if 123 = 123 > 123 and a shift of probability to the positive side if 123 = 123 < 123
soft thresholding and wavelets
soft thresholding estimators consider rst the lasso problem with orthonormal features and n > p , i . e .
in the fused lasso problem ( 123 ) we take s123 = and we assume that xtx= i .
then , if j are the univariate least squares estimates , the lasso solutions are soft threshold estimates :
j . 123 / = sgn .
j / . | j| 123 / + ,
where 123 satises j| j . 123 / |= s123
corresponding to this , there is a special case of the fused problem that also has an explicit solution .
we take s123 = and let = l and z = xl123
note that l123 is a lower triangular matrix of 123s , and hence the components of z are the right cumulative sums of the xij across j .
this gives a lasso problem for . z , y / and the solutions are
j . 123 / = sgn . j / . |j| 123 / + ,
provided that ztz= i , or equivalently xtx= ltl .
here 123 satises j| j . 123 / |= s123
the matrix ltl is tridiagonal , with 123s on the diagonal and 123s on the off - diagonals .
of course we cannot have both xtx= i and xtx= ltl at the same time .
but we can con - struct a scenario for which the fused lasso problem has an explicit solution .
we take x= ul123 ( cid : 123 ) = . xtx / 123xty are non - decreas - with utu = i and assume that the full least squares estimates p .
finally , we set s123= s123= s .
then the fused lasso solution soft - thresholds the full least squares estimates
j + = s .
however , this set - up does not seem to be very useful in practice , as its
assumptions are quite unrealistic .
( cid : 123 ) from the right : 123 , .
k , , 123 , 123 ,
basis transformations a transform approach to the problem of this paper would go roughly as follows .
we model = w , where the columns of w are appropriate bases .
for example , in our simulated example we might use haar wavelets , and then we can write x = x . w / = . xw / .
operationally , we transform our features to z= xw and t y to z , either by soft thresholding or by lasso , giving .
finally we map back to obtain = w .
note that soft thresholding implicitly assumes that the z - basis is orthonormal : ztz= i .
this procedure seeks a sparse representation of the s in the transformed space .
in contrast , the lasso and simple soft thresholded estimates ( 123 ) seek a sparse representation of the s in the the fused lasso is more ambitious : it uses two basis representations x and z = xl123 and seeks a representation that is sparse in both spaces .
it does not assume orthogonality , since this cannot hold simultaneously in both representations .
the price for this ambition is an increased
123 shows the results of applying soft thresholding ( fig .
123 ( a ) ) or the lasso ( fig .
123 ( b ) ) in the space of haar wavelets coefcients , and then transforming back to the original space .
for soft
tibshirani , m .
saunders , s .
rosset , j .
zhu and k .
knight
simulated example of fig .
123 : ( a ) true coefcients ( black ) , and estimated coefcients ( red ) obtained from transforming to a haar wavelet basis , thresholding and transforming back; ( b ) same procedure , except that the lasso was applied to the haar coefcients ( rather than soft thresholding )
( 123 log . nj / ) , where nj is the number
thresholding , we used the level - dependent threshold of wavelet coefcients at the given scale and was chosen to minimize the test error ( see for example donoho and johnstone ( 123 ) ) .
for the lasso , we chose the bound s123 to minimize the test error .
the resulting estimates are not very accurate , especially that from the lasso .
this may be partly due to the fact that the wavelet basis is not translation invariant .
hence , if the non - zero coefcients are not situated near a power of 123 along the feature axis , the wavelet basis will have difculty representing it .
degrees of freedom of the fused lasso t it is useful to consider how many degrees of freedom are used in a fused lasso t y = x as s123 and s123 are varied .
efron et al .
( 123 ) considered a denition of degrees of freedom using the formula of stein ( 123 ) :
df . y / = 123
where 123 is the variance of yi with x xed and cov refers to covariance with x xed .
for a stan - dard multiple linear regression with p < n predictors , df . y / reduces to p .
now , in the special case of an orthonormal design . xtx= i / , the lasso estimators are simply the soft threshold estimates ( 123 ) , and efron et al .
( 123 ) showed that the degrees of freedom equal the number of non - zero coefcients .
they also proved this for the lar and lasso estimators under a positive cone condition , which implies that the estimates are monotone as a function of the l123 - bound s123
the proof in the orthonormal case is simple : it uses steins formula
cov . yi , gi / = e
where y= . y123 , y123 , .
, yn / is a multivariate normal vector with mean and covariance i , and g . y / is an estimator , an almost differentiable function from rn to rn .
for the lasso with orthonormal design , we rotate the basis so that x= i , and hence from equation ( 123 ) g . y / = sgn . yi / . |yi| 123 / .
the derivative @g . y / =@yi equals 123 if the ith component is non - zero and 123 otherwise .
hence the degrees of freedom are the number of non - zero coefcients .
estimated degrees of freedom
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
estimated degrees of freedom
simulated example : actual and estimated degrees of freedom for ( a ) the fused lasso and ( b ) the
, 123 - line; - - - - - - - , least squares regression t )
for the fused lasso , the natural estimate of the degrees of freedom is df . y / = # ( non - zero coefcient blocks in ) :
in other words , we count a sequence of one or more consecutive non - zero and equal j - values as 123 degree of freedom .
equivalently , we can dene
df . y / = p # ( j = 123 ) # ( j j123 = 123 , j , j123 ( cid : 123 ) = 123 ) :
it is easy to see that these two denitions are the same .
furthermore , the objective function can be made 123 when df . y / ( cid : 123 ) min . n , p / , and hence min . n , p / is an effective upper bound for the degrees of freedom .
we have no proof that df . y / is a good estimate in general , but it follows from the stein result ( 123 ) in scenarios ( 123 ) ( 123 ) .
123 compares the estimated and actual degrees of freedom for the fused lasso and the lasso .
the approximation for the fused lasso is fairly crude , but it is not much worse than that for the lasso .
we used this denition only for descriptive purposes , to obtain a rough idea of the complexity of the tted model .
sparsity of fused lasso solutions as was mentioned in section 123 , the lasso has a sparse solution in high dimensional modelling , i . e . , if p > n , lasso solutions will have at most n non - zero coefcients , under mild ( non - redun - dancy ) conditions .
this property extends to any convex loss function with a lasso penalty .
it is proven explicitly , and the required non - redundancy conditions are spelled out , in rosset et al .
( 123 ) , appendix a .
the fused lasso turns out to have a similar sparsity property .
instead of applying to the num - ber of non - zero coefcients , however , the sparsity property applies to the number of sequences of identical non - zero coefcients .
so , if we consider the prostate cancer example in section 123 and fig .
123 , sparsity of the lasso implies that we could have at most 123 red dots in fig .
spar - sity of the fused lasso implies that we could have at most 123 black sequences of consecutive m=z - values with the same coefcient .
the formal statement of the sparsity result for the fused lasso follows .
theorem 123
set 123 = 123
let nseq . / = p 123 ( j ( cid : 123 ) = j123 ) .
then , under non - redundancy con - ditions on the design matrix x , the fused lasso problem ( 123 ) has a unique solution with nseq .
/ ( cid : 123 ) n .
tibshirani , m .
saunders , s .
rosset , j .
zhu and k .
knight
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
results for the prostate cancer example :
, ( cid : 123 ) , fused lasso non - zero coefcients;
the proof is very similar to the sparsity proof for the lasso in rosset et al .
( 123 ) , and is based on examining the karushkuhntucker conditions for optimality of the solution to the constrained problem ( 123 ) .
the non - redundancy conditions mentioned can be qualitatively summarized as follows .
( a ) no n columns of the design matrix x are linearly dependent .
( b ) none of a nite set of n + 123 linear equations in n variables ( the coefcients of which
depend on the specic problem ) has a solution .
analysis of prostate cancer data
as mentioned in section 123 the prostate cancer data set consists of 123 measurements on 123 patients : 123 healthy and 123 with cancer .
the average proles ( centroids ) are shown in fig .
following the original researchers , we ignored m=z - sites below 123 , where chemical artefacts can occur .
we randomly created training and validation sets of size 123 and 123 patients respectively .
to make computations manageable , we average the data in consecutive blocks of 123 , giving a total of 123 sites .
( we did manage to run the lasso on the full set of sites , and it produced error rates that were about the same as those reported for the lasso here . ) the
table 123
prostate data results
validation degrees of
nearest shrunken centroids
results of various methods are shown in table 123
in this two - class setting , the nearest shrunken centroids method ( tibshirani et al . , 123 ) is essentially equivalent to soft thresholding of the univariate regression coefcients .
adam et al .
( 123 ) reported error rates around 123% for a four - class version of this problem , using a peak nding procedure followed by a decision tree algorithm .
however , we ( and at least one other group that we know of ) have had difculty replicating their results , even when using their extracted peaks .
123 shows the non - zero coefcients from the two methods .
we see that the fused lasso puts non - zero weights at more sites , spreading out the weights especially at higher m=z - values .
a more careful analysis would use cross - validation to choose the bounds , and then report the test error for these bounds .
we carry out such an analysis for the leukaemia data in section 123 .
a simulation study
we carried out a small simulation study to compare the performance of the lasso and the fused lasso .
to ensure that our feature set had a realistic correlation structure for protein mass spec - troscopy , we used the rst 123 features from the data set that was described in the previous section .
we also used a random subset of 123 of the patients , to keep the feature to sample size ratio near a realistic level .
we then generated coefcient vectors by choosing 123 non - overlapping m=z - sites at random and dening blocks of equal non - zero coefcients of lengths uniform between 123 and 123
the values of the coefcients were generated as n . 123 , 123 / .
finally , training and test sets were generated according to
y= x + z , 123 : 123z n . 123 , 123 / :
the set - up is such that the amount of test variance that is explained by the model is about
for each data set , we found the lasso solution with the minimum test error .
we then used the search strategy that was outlined in section 123 for the fused lasso .
table 123 summarizes the results of 123 simulations from this model .
sensitivity and specicity refer to the proportion of true non - zero coefcients and true zero coefcients that are detected by each method .
shown are the minimum test error solution from the fused lasso and also that for the true values of the bounds s123 and s123
we see that the fused lasso slightly improves on the test error of the lasso and detects a much large proportion of the true non - zero coefcients .
in the process , it has a lower specicity .
even with the true s123 - and s123 - bounds , the fused lasso detects less than half the true non - zero coefcients .
this demonstrates the inherent difculty of problems having p ( cid : 123 ) n .
tibshirani , m .
saunders , s .
rosset , j .
zhu and k .
knight
table 123
results of the simulation study
( true s123 , s123 )
standard errors are given in parentheses .
application to unordered features
the fused lasso denition ( 123 ) assumes that the features xij , and hence the corresponding param - eters j , have a natural order in j .
in some problems , however , the features have no prespecied order , e . g .
genes in a microarray experiment .
there are at least two ways to apply the fused lasso in this case .
first , we can estimate an order for the features , using for example multidimensional scaling or hierarchical clustering .
the latter is commonly used for creating heat map displays of microarray data .
alternatively , we notice that denition ( 123 ) does not require a complete ordering of the features but only specication of the nearest neighbour of each feature , i . e .
let k . j / be the index of the feature that is closest to feature j , in terms , for example , of the smallest euclidean distance or maximal correlation .
then we can use the fused lasso with difference constraint
|j k . j / | ( cid : 123 ) s123 :
computationally , this just changes the p linear constraints that are expressed in matrix l in expression ( 123 ) .
note that more complicated schemes , such as the use of more than one near neighbour , would increase the number of linear constraints , potentially up to p123
we illustrate the rst method in the example below .
leukaemia classication by using microarrays the leukaemia data were introduced in golub et al .
( 123 ) .
there are 123 genes and 123 samples : 123 in class 123 ( acute lymphocytic leukaemia ) and 123 in class 123 ( acute mylogenous leukaemia ) .
in addition there is a test sample of size 123
the prediction results are shown in table 123
the rst two rows are based on all 123 genes .
the procedure of golub et al .
( 123 ) is similar to nearest shrunken centroids , but it uses hard thresholding .
for the lasso and fusion meth - ods , we rst ltered down to the top 123 genes in terms of overall variance .
then we applied average linkage hierarchical clustering to the genes , to provide a gene order for the fusion
all lasso and fusion models were tted by optimizing the tuning parameters using cross - validation and then applying these values to the test set .
the pure fusion estimate method ( 123 ) did poorly in the test error : this error never dropped below 123 for any value of the bound s123
we see that in row ( 123 ) fusing the lasso solution gives about the same error rate , using about four times as many genes .
further fusion in row ( 123 ) seems to increase the test error rate .
table 123 shows a sample of the estimated coefcients for the lasso and fused lasso solution method ( 123 ) .
we see that in many cases the fusion process has spread out the coefcient of a non - zero lasso coefcient onto adjacent genes .
table 123
results for the leukaemia microarray example
( 123 ) golub et al .
( 123 ) ( 123 genes ) ( 123 ) nearest shrunken centroid
( 123 ) lasso , 123 degrees of freedom ( s123 = 123 : 123 , s123 = 123 : 123 ) ( 123 ) fused lasso , 123 degrees of freedom ( s123 = 123 : 123 , s123 = 123 : 123 ) ( 123 ) fused lasso , 123 degrees of freedom ( s123 = 123 : 123 , s123 = 123 : 123 ) ( 123 ) fusion , 123 degree of freedom
table 123
leukaemia data example : a sample of the non - zero coefcients for the lasso and fused lasso , with contiguous blocks delineated
the full table appears in tibshirani et al .
( 123 ) .
hinge loss
for two - class problems the maximum margin approach that is used in the support vector classi - er ( boser et al . , 123; vapnik , 123 ) is an attractive alternative to least squares .
the maximum margin method can be expressed in terms of the hinge loss function ( see for example hastie et al .
( 123 ) , chapter 123 ) .
we minimize
j . 123 , , / = n ( cid : 123 )
tibshirani , m .
saunders , s .
rosset , j .
zhu and k .
knight
table 123
signs of lasso coefcients ( rows ) versus signs of fused lasso support vector coefcients
yi . 123 + txi / ( cid : 123 ) 123 i ,
j ( cid : 123 ) s .
recently there has the original support vector classier includes an l123 - constraint p been interest in the l123 - constrained ( lasso ) support vector classier .
zhu et al .
( 123 ) developed an lar - like algorithm for solving the problem for all values of the bound s .
we can generalize to the fused lasso support vector classier by imposing constraints
i ( cid : 123 ) 123 , for all i :
|j j123| ( cid : 123 ) s123 :
the complete set of constraints can be written as
j ( cid : 123 ) 123
since the objective function ( 123 ) is linear , this in addition to the bounds i , optimization is a linear ( rather than quadratic ) programming problem .
our implementation uses the sqopt package again , as it handles both linear and quadratic programming problems .
we applied the fused lasso support vector classier to the microarray leukaemia data .
using s123 = 123 and s123 = 123 gave a solution with 123 non - zero coefcients and 123 degrees of freedom .
it produced one misclassication error in both tenfold cross - validation and the test set , making it competitive with the best classiers from table 123
table 123 compares the signs of the fused lasso coefcients ( rows ) and the fused lasso support vector coefcients ( columns ) .
the agreement is substantial , but far from perfect .
one advantage of the support vector formulation is its fairly easy extension to multiclass
problems : see for example lee et al .
( 123 ) .
the fused lasso seems a promising method for regression and classication , in settings where the features have a natural order .
one difculty in using the fused lasso is computational speed .
the timing results in table 123 show that , when p > 123 and n > 123 , speed could become a practical limitation .
this is espe - cially true if ve or tenfold cross - validation is carried out .
hot starts can help : starting with large values of . s123 , s123 / , we obtain solutions for smaller values in a constant ( short ) time .
( initially we used increasing values of . s123 , s123 / because each solution is sure to be a feasible starting - point for the next values .
however , with decreasing values of . s123 , s123 / , sqopt achieves feasibility quickly and has tended to be more efcient that way . )
the lar algorithm of efron et al .
( 123 ) solves efciently the entire sequence of lasso prob - lems , for all values of the l123 - bound s123
it does so by exploiting the fact that the solution proles are piecewise linear functions of the l123 - bound , and the set of active coefcients changes in a predictable way .
one can show that the fused lasso solutions are piecewise linear functions as we move in a straight line in the . 123 , 123 / plane ( see rosset and zhu ( 123 ) ) .
here . 123 , 123 / are the lagrange multipliers corresponding to the bounds s123 and s123
hence it might be possible to develop an lar - style algorithm for quickly solving the fused lasso problem along these straight lines .
however , such an algorithm would be considerably more complex than lar , because of the many possible ways that the active sets of constraints can change .
in lar we can only add or drop a variable at a given step .
in the fused lasso , we can add or drop a variable , or fuse or defuse a set of variables .
we have not yet succeeded in developing an efcient algorithm for this procedure , but it will be a topic of future research .
generalizations of the fused lasso to higher dimensional orderings may also be possible .
sup - pose that the features xj , j ( cid : 123 ) are arranged on a two - way gride . g .
in an image .
then we might constrain coefcients that are 123 unit apart in any direction , i . e .
constraints of the form
|j , k j , l|+ ( cid : 123 )
|k , j l , j| ( cid : 123 ) s123 :
this would present interesting computational challenges , as the number of constraints is of the
tibshirani was partially supported by national science foundation grant dms - 123 and national institutes of health contract n123 - hv - 123
saunders was partially supported by national science foundation grant ccr - 123 and ofce of naval research grant n123 - 123 - 123 - 123
philip gills continuing work on the quadratic programming solver sqopt is also
