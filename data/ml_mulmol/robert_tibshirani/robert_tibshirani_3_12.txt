- we consider the problem of summarizing a scatterplot with a smooth , monotone that combines local averaging and isotonic regression is proposed .
we curve .
a solution give some theoreticai some data from box and cox ( 123 ) and it is shown how this new procedure generalizes box and coxs well known family of transformations .
in the same example , the bootstrap is applied to get a
in the second example , the procedure is applied ,
for the procedure - and demonstrate
of the variability of the procedure .
its use with
in a regression sethng ,
keywords : scatterplot smoothing ,
* work supported by the department of energy under contracts deacqb123sf123 the off - ice of naval research under contract onr - no123 123f - - k - 123 , and the army research office under contract daag123 - 123 - k - 123
we consider the following problem .
given a set of n data points - - ( ( zl , yl ) , .
y , ) ) , how can we summarize the association of the response g on the predictor z by a smooth , moaotone function s ( z ) ? put another way , how can we pass a smooth , monotone curve through a scatterplot of f / vs z to capture the trend of y as a function of z ? this problem is related to both isotonic regression ( see e . g .
barlow et al 123 ) and scatterplot smoothing ( see e . g .
cleveland 123 ) .
. in this paper we propose a solution to the problem that uses ideas from both isotonic regression and scatterplot smoothing ( section 123 ) .
this procedure proves to be useful not only as a descriptive of the response in linear regression ( section 123 , example 123 ) , a method closely related to those of box and cox ( 123 ) and kruskal ( l123 ) .
we begin with a brief review of isotonic regression and scatterplot smoothing
tool but also as a method for determining optimal
in the next section .
a review of isotonic regression and scatterplot
123 isotonic regression
to the restriction
r%l _< ti123 123
the problem of isotonic
. . +a . , .
a unique solution from the pool adjacent violators algorithm is too complex to fully describe here , but
regression on an ordered set is as follows .
given real the problem is to find ( & ~ ;123 , &123 , .
123 ~ ) to minimize ct ( y; - hi , ; ) 123 numbers ( ~ 123 , ~ 123 , .
. . y . ) , to this problem exists ( see barlow et al , pg .
and can be obtained the basic idea is the 123 ) .
this algorithm imagine a scatterplot of y; vs i .
starting with yl , we move to the right and stop at the first place that yi > yi+l .
since yi+l violates the monotone assumption , we pool vi and vi+123 replacing them both by their average .
call this average vr = vi++123 = if not , we ( vi + yi+i ) / 123
we then move to the left their average .
we continue to the is satisfied , then proceed again to the right .
this is continued until we reach the , are then given by the last average assigned
process of pooling the first violator and back - averaging rig123t hand edge .
the solutions at each i , hi
to make sure that vi - 123 123 vt -
the monotone requirement
replacing all three with
vi - 123 with gz and $+l ,
to the point at i .
to find the solution
for the dual problem ( gal 123 +i ~ 123
the pool adjacent is applied , starting at yn and moving to the left .
and to find ais to minimize cy ( vi - hi ) 123 subject to his non - decreasing or non - increasing , we can choose the best set from the two solutions .
we will refer to this two step algorithm as the pool adjacent violators algorithm .
its not obvious
the pool adjacent violators algorithm - solves
the isotonic re - a proof appears in barlow et al ( pg .
there are , however , two
facts we can notice about the solution :
, are monotone ,
reproduces the data .
if the data ( yl , f123 , .
. . v . , ) each tii will be an average of gjs near i .
the average will span over the local
then tii = vi for all i; that
of the yis .
to the isotonic regression problem is not the solution
to the problem of monotone smoothing because the solution sequence tii is not necessarily smooth .
for example , as we noted , if the data are monotone , the pool adjacent violators simply reproduces the data; any jaggedness in the data will be passed on to the solution .
in the next subsection , we briefly review scatterplot smoothing .
given n pairs of data points ( ( ~ 123 , yi ) , .
. . ( z . , , vn ) ) , zl < 123 <
. . iz ~ , assumed to inde - th e goal of a scatterplot smoother pendent realizations of random variables ( x , y ) , is find a smooth function a ( ~ ) that summarizes the dependence of y on x .
we assume that 123 is some smooth function of x plus a random component :
where e ( i ) = 123 and var ( c ) = o123 < 123
one way to formulate cally is to require that s ( z ) minimize
the predictive squared error
the problem mathemati -
pse = e ( y - 123 ( x ) ) 123
is over the joint distribution
where the expectation averaging .
many techniques have been suggested for this - will make use of , is the running mean :
the solution would be i ( x ) = e ( yix is rarely known , so the conditional expectation
if this joint distribution = z ) for all z .
of course this the simplest and the one we
i123k ( xi ) = awe ( si - k , sa . zi , .
. . zi+k )
the windows are shrunken near the left and right endpoints - in a window i - k ) , .
. . min ( n .
i + k ) ) .
is , the set of indices
the width of the window over which the average is taken , 123k+ 123 , is called the span .
in order to choose the span ,
a criterion based on the notion of cross - validation can be used .
denote by iki ( zi ) running average at zi leaving out
the span is 123 to 123 percent of the observations .
ifi ( 123; ) = aue ( 123i - k f s**zi - * , zi+* , as* %+k )
xi , i . e .
123; =
the same endpoint convention as before .
let 123; be a new observation at f ( zi ) + ct where pi * is independent of the cis .
then it can be shown that
( *iif the z values are not random but fixed by design , we would assume that of x replaced by an appropriate sum .
the derivations are still valid , with expectations over the distribution
the yis are
by using the fact that gki ( zi ) etytimate of pse , a sensible procedure is to choose k to minimize - act we will denote this value of k by k .
is independent of vi .
since the right hand side of ( 123 ) is an ( vi - ~ l ( z; ) ) ~ .
note also that a e cy ( 123; -
also minimizes an estimate of the expected squared error
i , ( zi ) ) 123 = aec;
- &k ( si ) ) 123 + nu123 , so that l
for a discussion of running mean smoothers and more sophisticated smoothers , see
friedman and stuetzle ( 123 ) .
the running mean smoother produces a smooth function
is proposed in the next section .
pendence of y on x , but this function isotonic regression produces a monotone function y on x , but this function function , why not smooth the data first , this is exactly
is not necessarily smooth .
that summarizes the de - is not necessarily monotone .
on the other hand , that summarizes the dependence of if we want a smooth , monotone then apply isotonic regression to the smooth ?
monotone smoothing
123 the problem
and a proposed solution
suppose we have a set of n data points ( ( 123 , yl ) , .
vn ) ) , where ~ 123 < x123 . .
< zn
the dependence of y on z .
and our goal is to model , with a monotone function , break this problem down into 123 - steps
find a smooth function g ( m ) that summarizes the dependence of y on x find the monotone function
t ? ~ ( a ) closest to i ( a )
then using the tools of isotonic regression and scatterplot smoothing discussed in section 123 , the solution
smooth the ( x , y ) apply the pool adjacent violators algorithm
to the smooth
in the next subsection , this heuristic procedure is given some theoretical
assume the setup described in section 123 .
a reasonable property
to require of the
it should satisfy
= min - l exezw ( zx
- gj ( x ) ) ~ = min - 123 psem
in x , where 123x has the distribution
is the integrated prediction squared error in predicting
of 123 given x .
the response for a new tia ( e ) .
if we knew the true joint distribution of observation , using the monotone function x and y , or we had a infinite test sample of zis , we could minimize psem over h ( e ) .
of course , we dont know the joint distribution and we have only a training sample , so from the training
instead derive a approximate criterion
that we can calculate
as in section 123 , we can equivalently minimize
the expected squared error
since psem = esem + nu 123
it turns out to be more convenient to work with esem .
we can first replace the marginal distribution of x by the marginal empirical dis -
to that of finding an
= ese m , so we can simplify
f ( e ) , we could simply minimize an estimate of ( 123 ) , ac;
estimate of esel .
- +ia ( zi ) ) 123 , to f ( e ) .
since we dont over ti , ( a ) by applying know f ( s ) , the next best thing is to replace f ( e ) with our best estimate ( in terms of mean squared error ) of f ( a ) .
in the class of running mean estimates , the best estimate is ii ( . ) ( from section 123 ) .
hence our approximate criterion
the pool adjacent violators algorithm
( ii ( xi ) -
ta minimize eie* m over t ? a ( * ) , we simply apply the pool adjacent violators algorithm
how far off ( on the average ) will
that minimizes esem ? unfortunately ,
we can expand the expected value of e . $eh as follows :
the ti ( . ) obtained by minimizing e , ! ? eb be from to get a handle on this .
it is difficult
note that only the last two terms in ( 123 ) involve + ? a ( s ) .
if gg ( w ) is exactly equal to f ( e ) , then the expected value of ekea is equal to ese;m .
otherwise , we can just hope that since iii ( m ) - f ( . ) i should b e small , the cross product term will be small compared to the
123 a summary of the algorithm
we can summarize the monotone smoothing algorithm as foltows :
123 smoota y on x :
i ( xi ) + aue ( zi - $ , ss . zi , .
. . ~ i+k ) where e is chosen to minimize
cy ( vi - iii the closest monotone function violators algorithm applied to &i ( s )
result of pool adjacent
o as a slight refinement of the algorithm ,
linear smoother in the numerical examples that
the running mean smoother was replaced by a running ( least squares ) lines givve results very close to running means in the middle of the data , and eliminate some of the bias near the endpoints .
if the smooth g ( s ) is monotone , then h ( s ) =
this makes good in the class of monotone
is the best estimate over all functions ,
it just says that
the best estimate of e ( y ) x )
if the latter
in the next section , we give two examples of the use of this procedure .
_ 123 points were generated from 123 = e123 + error , where x was uniformly distributed on ( 123 , 123 ) and the errors had a normal distribution with mean 123 and variance 123
the result of applying the monotone smoother is shown in figure 123
a span of 123 points was chosen by the procedure .
for comparison , the isotonic regression sequence is also from the smooth ( not shown ) , since the smooth was almost monotone .
in this case , the monotone smooth differed only slightly
in this example , we use the monotone smoothing procedure to
find an optimal
by kruska123 ( 123 ) , is a non - parametric version of the box - cox procedure ( 123 ) .
a special case of the alternating conditional expectation and friedman ( 123 ) .
given a set of responses and covariates ( ( yi , xl ) , goal is to find a smooth , monotone function a ( . ) and estimate b to minimize
for the response in a regression .
this procedure , similar to that proposed it is also ( ace ) algorithm of breiman
subject to var ( 123 ( y ) ) = 123 where var denotes the sample variance .
the procedure is an alternating one , finding b for fixed i ( s ) and vice - versa :
i ( - ) 123 - *i
b 123 - least squares estimate of i ( e ) on x - i ( e ) + monotone smooth of x b on y
residual sum of squares ( l123 )
fails to decrease
i ( s ) belongs to the parametric
the kruskal and box - cox procedures are essentially variants of the above algorithm .
kruskal uses isotonic regression to estimate 123 ( e ) , while box and cox assume
family ( vx - 123 ) / x .
to data on strength of yarns taken from box and cox the response y being number of cycles ( 123 ) .
the data consists of a 123x123x123 experiment , to failure , and the factors length of test specimen ( xl ) ( 123 , 123 or 123 mm ) ; amplitude of loading cycle ( x123 ) ( 123 , 123 , or 123 mm ) , and load ( x123 ) ( 123 , 123 or 123 gm ) .
as in box and the factors as quantitive and allowed only a linear term for each .
box cox , we treated and cox found that a logarithmic transformation was appropriate , with their procedure producing a value of - . 123 for i with an estimated 123 percent confidence interval of
figure 123 shows the transformation
chose a span of 123 observations .
for comparison , on the same figure .
the similarity k&&als procedure plotted along with
the log function
selected by the above algorithm .
the procedure is plotted ( normalized ) remarkable ! figure 123 shows the result of the log function .
the monotone smooth gives
very persuasive evidence for a log transformation , while kruskals hampered by its lack of smoothness .
the advantage , of course , of the monotone smoothing algorithm
assume a parametric tion from a much larger class than the box and cox family .
for the transformations ,
and hence it selects a transforma -
in order to assess the variability of the monotone smooth , we applied the bootstrap is fixed by design , we resampled the ( x , y ) p airs .
the bootstrap procedure was the
the residuals instead of from
in this problem
of efron ( 123 ) .
s ince the x matrix
calculate residuals ri = 123 ( vi ) - xi b , i = 123 , 123 ,
choose a sample ri , calculate yf =
. . ri with replacement
from rl ,
i = 123 , 123 ,
- compute aj ( a ) = monotone smooth of vi , .
. . yi on xl ,
the number of bootstrap replications , was 123
it is important
to note that , via the ris , this procedure assumes that in estimating a common residual distribution the model 123 ( v ) = x b + r is correct ( see efron ( 123 ) ) .
the 123 monotone smooths , il ( e ) , .
&jo ( e ) , along with i ( a ) , are shown in figure 123
the tight clustering of the smooths indicate that the original smooth has low variability .
this agrees with the short confidence interval for x given by the box and cox procedure .
the original monotone smooth ,
further remarks
the monotone smoothing procedure that
tool as well as a primitive
is discussed here should prove to be useful for any procedure requiring estimation it already being used in the ace program of breiman
both as a descriptive of a smooth , monotone function .
and eriedman ( 123 ) .
some further points :
the use of running mean or running
linear fits in the algorithm
is not essential .
fit like that proposed
if the procedure
that smooths the data using both the global information provided by the - monotonicity lower error of estimation
any reasonable smooth ( e . g .
kernel smoother or cubic splines ) should perform if robustness to outlying y values is a concern , a resistant in friedman and stuetzle ( 123 ) might be used .
in any broad sense .
it may be pos - the procedure described here is not optimal sible to develop a one step procedure information and and sumption .
such a procedure might have slightly is to be used as the monotone smoother described here .
but either a data summary or as a method to suggest a response transformation , we another way to estimate a monotone smooth would be to apply the pool adjacent then smooth the monotone sequence .
this has a serious drawback : while it is true that a running mean smooth of a monotone sequence is monotone , the running linear smooth of a monotone sequence is not necessarily therefore , one would the final smooth have to apply the pool adjacent violators again to ensure that was monotone .
this non - monotonicity preserving property other popular smoothers .
we didnt try this procedure , partly because of this fact but mostly because we didnt see a sensible justification
the gain would be worthwhile .
is easy to construct a counter - example ) .
we would like to thank ltevor ha&
for his valuable comments .
this research was supported in part by the natural sciences and engineeering research council of
