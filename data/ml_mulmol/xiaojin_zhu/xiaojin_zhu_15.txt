empirical evidence shows that in favorable situations semi - supervised learning ( ssl ) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task , in the sense that fewer labeled train - ing data are needed to achieve a target error bound .
however , in other situations unlabeled data do not seem to help .
recent attempts at theoretically character - izing ssl gains only provide a partial and sometimes apparently conicting ex - planations of whether , and to what extent , unlabeled data can help .
in this paper , we attempt to bridge the gap between the practice and theory of semi - supervised learning .
we develop a nite sample analysis that characterizes the value of un - labeled data and quanties the performance improvement of ssl compared to supervised learning .
we show that there are large classes of problems for which ssl can signicantly outperform supervised learning , in nite sample regimes and sometimes also in terms of error convergence rates .
labeled data can be expensive , time - consuming and difcult to obtain in many applications .
semi - supervised learning ( ssl ) aims to capitalize on the abundance of unlabeled data to improve learning performance .
empirical evidence suggests that in certain favorable situations unlabeled data can help , while in other situations it does not .
as a result , there have been several recent attempts ( 123 , 123 , 123 , 123 , 123 , 123 ) at developing a theoretical understanding of semi - supervised learning .
it is well - accepted that unlabeled data can help only if there exists a link between the marginal data distribution and the target function to be learnt .
two common types of links considered are the cluster assump - tion ( 123 , 123 , 123 ) which states that the target function is locally smooth over subsets of the feature space delineated by some property of the marginal density ( but may not be globally smooth ) , and the man - ifold assumption ( 123 , 123 ) which assumes that the target function lies on a low - dimensional manifold .
knowledge of these sets , which can be gleaned from unlabeled data , simplify the learning task .
however , recent attempts at characterizing the amount of improvement possible under these links only provide a partial and sometimes apparently conicting ( for example , ( 123 ) vs .
( 123 ) ) explanations of whether or not , and to what extent semi - supervised learning helps .
in this paper , we bridge the gap between these seemingly conicting views and develop a minimax framework based on nite sample bounds to identify situations in which unlabeled data help to improve learning .
our results quantify both the amount of improvement possible using ssl as well as the the relative value of
we focus on learning under a cluster assumption that is formalized in the next section , and estab - lish that there exist nonparametric classes of distributions , denoted pxy , for which the decision sets ( over which the target function is smooth ) are discernable from unlabeled data .
moreover , we show that there exist clairvoyant supervised learners that , given perfect knowledge of the de - cision sets denoted by d , can signicantly outperform any generic supervised learner fn in these
supported in part by the nsf grants ccf - 123 , ccf - 123 , and cns - 123
supported in part by the wisconsin alumni research foundation .
figure 123 : ( a ) two separated high density sets with different labels that ( b ) cannot be discerned if the sample size is too small , but ( c ) can be estimated if sample density is high enough .
e ( r ( bfd , n ) ) < inf fn suppxy
classes .
that is , if r denotes a risk of interest , n denotes the labeled data sample size , bfd , n denotes the clairvoyant supervised learner , and e denotes expectation with respect to training data , then e ( r ( fn ) ) .
based on this , we establish that there also exist semi - supervised learners , denoted bfm , n , that use m unlabeled examples in addition to the n labeled examples in order to estimate the decision sets , which perform as well as bfd , n , provided that m grows appropriately relative to n .
specically , if the error bound for bfd , n decays polynomially ( ex -
ponentially ) in n , then the number of unlabeled data m needs to grow polynomially ( exponentially ) with the number of labeled data n .
we provide general results for a broad range of learning problems using nite sample error bounds .
then we examine a concrete instantiation of these general results in the regression setting by deriving minimax lower bounds on the performance of any supervised
learner and compare that to upper bounds on the errors of bfd , n and bfm , n .
in their seminal papers , castelli and cover ( 123 , 123 ) suggested that in the classication setting the marginal distribution can be viewed as a mixture of class conditional distributions .
if this mixture is identiable , then the classication problem may reduce to a simple hypothesis testing problem for which the error converges exponentially fast in the number of labeled examples .
the ideas in this paper are similar , except that we do not require identiability of the mixture component densities , and show that it sufces to only approximately learn the decision sets over which the label is smooth .
more recent attempts at theoretically characterizing ssl have been relatively pessimistic .
rigollet ( 123 ) establishes that for a xed collection of distributions satisfying a cluster assumption , unlabeled data do not provide an improvement in convergence rate .
a similar argument was made by lafferty and wasserman ( 123 ) , based on the work of bickel and li ( 123 ) , for the manifold case .
however , in a recent paper , niyogi ( 123 ) gives a constructive example of a class of distributions supported on a manifold whose complexity increases with the number of labeled examples , and he shows that the error of any supervised learner is bounded from below by a constant , whereas there exists a semi - supervised learner that can provide an error bound of o ( n123 / 123 ) , assuming innite unlabeled data .
in this paper , we bridge the gap between these seemingly conicting views .
our arguments can be understood by the simple example shown in fig .
123 , where the distribution is supported on two component sets separated by a margin and the target function is smooth over each component .
given a nite sample of data , these decision sets may or may not be discernable depending on the sampling density ( see fig .
123 ( b ) , ( c ) ) .
if is xed ( this is similar to xing the class of cluster - based distributions in ( 123 ) or the manifold in ( 123 , 123 ) ) , then given enough labeled data a supervised learner can achieve optimal performance ( since , eventually , it operates in regime ( c ) of fig .
thus , in this example , there is no improvement due to unlabeled data in terms of the rate of error convergence for a xed collection of distributions .
however , since the true separation between the component sets is unknown , given a nite sample of data , there always exists a distribution for which these sets are indiscernible ( e . g . , 123 ) .
this perspective is similar in spirit to the argument in ( 123 ) .
we claim that meaningful characterizations of ssl performance and quantications of the value of unlabeled data require nite sample error bounds , and that rates of convergence and asymptotic analysis may not capture the distinctions between ssl and supervised learning .
simply stated , if the component density sets are discernable from a nite sample size m of unlabeled data but not from a nite sample size n < m of labeled data , then ssl can provide better performance than supervised learning .
we also show that there are certain plausible situations in which ssl yields rates of convergence that cannot be achieved by any supervised learner .
g ( x )
g ( x )
g ( x )
g ( x )
g ( x )
g ( x )
g ( x )
g ( x )
figure 123 : margin measures the minimum width of a decision set or separation between the support sets of the component marginal mixture densities .
the margin is positive if the component support sets are disjoint , and negative otherwise .
123 characterization of model distributions under the cluster assumption based on the cluster assumption ( 123 , 123 , 123 ) , we dene the following collection of joint distributions pxy ( ) = px py |x indexed by a margin parameter .
let x , y be bounded random variables with marginal distribution px px and conditional label distribution py |x py |x , supported on the domain x = ( 123 , 123 ) d .
the marginal density p ( x ) = pk k=123 akpk ( x ) is the mixture of a nite , but unknown , number of k=123 , where k < .
the unknown mixing proportions ak a > 123 and component densities ( pk ) k k=123 ak = 123
in addition , we place the following assumptions on the mixture component densities : 123
pk is supported on a unique compact , connected set ck x with lipschitz boundaries .
speci - cally , we assume the following form for the component support sets : ( see fig .
123 for d=123 illustration . )
k ( x123 , .
, xd123 ) xd g ( 123 )
ck = ( x ( x123 , .
, xd ) x : g ( 123 ) k ( ) , g ( 123 )
k ( x123 , .
, xd123 ) ) , k ( ) are d 123 dimensional lipschitz functions with lipschitz constant l . 123
pk is bounded from above and below , 123 < b pk b .
pk is holder - smooth on ck with holder constant k123 ( 123 , 123 ) .
let the conditional label density on ck be denoted by pk ( y |x = x ) .
thus , a labeled training point ( x , y ) is obtained as follows .
with probability ak , x is drawn from pk and y is drawn from pk ( y |x = x ) .
in the supervised setting , we assume access to n labeled data l = ( xi , yi ) n drawn i . i . d according to pxy pxy ( ) , and in the semi - supervised setting , we assume access to m additional unlabeled data u = ( xi ) m k=123 or their let d denote the collection of all non - empty sets obtained as intersections of ( ck ) k k that does not lie in the support of the marginal density .
observe that |d| 123k , and in practical situations the cardinality of d is much smaller as only a few of the sets are non - empty .
the cluster assumption is that the target function will be smooth on each set d d , hence the sets in d are called decision sets .
at this point , we do not consider a specic target function .
the collection pxy is indexed by a margin parameter , which denotes the minimum width of a decision set or separation between the component support sets ck .
the margin is assigned a positive sign if there is no overlap between components , otherwise it is assigned a negative sign as illustrated in figure 123
formally , for j , k ( 123 , .
, k ) , let
i=123 drawn i . i . d according to px px .
k=123 , excluding the set k
djk : = min
k k j 123= k ,
dkk : = kg ( 123 )
then the margin is dened as
if cj ck = j 123= k
123this form is a slight generalization of the boundary fragment class of sets which is used as a common tool for analysis of learning problems ( 123 ) .
boundary fragment sets capture the salient characteristics of more general decision sets since , locally , the boundaries of general sets are like fragments in a certain orientation .
123 learning decision sets ideally , we would like to break a given learning task into separate subproblems on each d d since the target function is smooth on each decision set .
note that the marginal density p is also smooth within each decision set , but exhibits jumps at the boundaries since the component densities are bounded away from zero .
hence , the collection d can be learnt from unlabeled data as follows : 123 ) marginal density estimation the procedure is based on the sup - norm kernel density estimator proposed in ( 123 ) .
consider a uniform square grid over the domain x = ( 123 , 123 ) d with spacing 123hm , where hm = 123 ( ( log m ) 123 / m ) 123 / d and 123 > 123 is a constant .
for any point x x , let ( x ) denote the closest point on the grid .
let g denote the kernel and hm = hmi , then the estimator of p ( x ) is
m ( xi ( x ) ) ) .
kzjzj+123k 123dhm , and for all points that satisfy kzizjk hm log m , |bp ( zi ) bp ( zj ) | m : =
123 ) decision set estimation two points x123 , x123 x are said to be connected , denoted by x123 x123 , if there exists a sequence of points x123 = z123 , z123 , .
, zl123 , zl = x123 such that z123 , .
, zl123 u , ( log m ) 123 / 123
that is , there exists a sequence of 123dhm - dense unlabeled data points between x123 and x123 such that the marginal density varies smoothly along the sequence .
all points that are pairwise connected specify an empirical decision set .
this decision set estimation procedure is similar in spirit to the semi - supervised learning algorithm proposed in ( 123 ) .
in practice , these sequences only need to be evaluated for the test and labeled training points .
the following lemma shows that if the margin is large relative to the average spacing m123 / d between unlabeled data points , then with high probability , two points are connected if and only if they lie in the same decision set d d , provided the points are not too close to the decision boundaries .
the proof sketch of the lemma and all other results are deferred to section 123
lemma 123
let d denote the boundary of d and dene the set of boundary points as
b = ( x :
zdd d kx zk 123dhm ) .
if || > co ( m / ( log m ) 123 ) 123 / d , where co = 123d123 , then for all p px , all pairs of points x123 , x123 supp ( p ) \ b and all d d , with probability > 123 123 / m ,
x123 , x123 d if and only if
for large enough m m123 , where m123 depends only on the xed parameters of the class pxy ( ) .
123 ssl performance and the value of unlabeled data we now state our main result that characterizes the performance of ssl relative to supervised learn - ing and follows as a corollary to the lemma stated above .
let r denote a risk of interest and e ( bf ) = r ( bf ) r , where r is the inmum risk over all possible learners .
corollary 123
assume that the excess risk e is bounded .
suppose there exists a clairvoyant super - vised learner bfd , n , with perfect knowledge of the decision sets d , for which the following nite sample upper bound holds then there exists a semi - supervised learner bfm , n such that if || > co ( m / ( log m ) 123 ) 123 / d ,
+ n ( cid : 123 ) m
( log m ) 123 ( cid : 123 ) 123 / d ! .
e ( e ( bfm , n ) ) 123 ( n ) + o 123
this result captures the essence of the relative characterization of semi - supervised and supervised learning for the margin based model distributions .
it suggests that if the sets d are discernable using unlabeled data ( the margin is large enough compared to average spacing between unlabeled data points ) , then there exists a semi - supervised learner that can perform as well as a supervised learner with clairvoyant knowledge of the decision sets , provided m n so that ( n / 123 ( n ) ) d =
o ( m / ( log m ) 123 ) implying that the additional term in the performace bound for bfm , n is negligible
compared to 123 ( n ) .
this indicates that if 123 ( n ) decays polynomially ( exponentially ) in n , then m needs to grow polynomially ( exponentially ) in n .
further , suppose that the following nite sample lower bound holds for any supervised learner :
if 123 ( n ) < 123 ( n ) , then there exists a clairvoyant supervised learner with perfect knowledge of the decision sets that outperforms any supervised learner that does not have this knowledge .
hence , corollary 123 implies that ssl can provide better performance than any supervised learner provided ( i ) m n so that ( n / 123 ( n ) ) d = o ( m / ( log m ) 123 ) , and ( ii ) knowledge of the decision sets simplies the supervised learning task , so that 123 ( n ) < 123 ( n ) .
in the next section , we provide a concrete application of this result in the regression setting .
as a simple example in the binary classication setting , if p ( x ) is supported on two disjoint sets and if p ( y = 123|x = x ) is strictly greater than 123 / 123 on one set and strictly less than 123 / 123 on the other , then perfect knowledge of the decision sets reduces the problem to a hypothesis testing problem for which 123 ( n ) = o ( e n ) , for some constant > 123
however , if is small relative to the average spacing n123 / d between labeled data points , then 123 ( n ) = cn123 / d where c > 123 is a constant .
this lower bound follows from the minimax lower bound proofs for regression in the next section .
thus , an exponential improvement is possible using semi - supervised learning provided m grows exponentially in n .
123 density - adaptive regression let y denote a continuous and bounded random variable .
under squared error loss , the target
function is f ( x ) = e ( y |x = x ) , and e ( bf ) = e ( ( bf ( x ) f ( x ) ) 123 ) .
recall that pk ( y |x = x ) is the conditional density on the k - th component and let ek denote expectation with respect to the corresponding conditional distribution .
the regression function on each component is fk ( x ) = ek ( y |x = x ) and we assume that for k = 123 , .
fk is uniformly bounded , |fk| m .
fk is holder - smooth on ck with holder constant k123
this implies that the overall regression function f ( x ) is piecewise holder - smooth; i . e . , it is holder - smooth on each d d , except possibly at the component boundaries .
123 since a holder - smooth function can be locally well - approximated by a taylor polynomial , we propose the follow - ing semi - supervised learner that performs local polynomial ts within each empirical decision set , that is , using training data that are connected as per the denition in section 123
while a spatially uniform estimator sufces when the decision sets are discernable , we use the following spatially adaptive estimator proposed in section 123 of ( 123 ) .
this ensures that when the decision sets are indiscernible using unlabeled data , the semi - supervised learner still achieves an error bound that is , up to logarithmic factors , no worse than the minimax lower bound for supervised learners .
( yi f ( xi ) ) 123xxi + pen ( f )
bfm , n , x ( ) = arg min here 123xxi is the indicator of x xi and denotes a collection of piecewise polynomials of degree ( ) ( the maximal integer < ) dened over recursive dyadic partitions of the domain x = ( 123 , 123 ) d with cells of sidelength between 123log ( n / log n ) / ( 123+d ) and 123log ( n / log n ) / d .
the penalty term pen ( f ) is proportional to log ( pn 123xxi ) #f , where #f denotes the number of cells in the recursive dyadic partition on which f is dened .
it is shown in ( 123 ) that this estimator yields a nite sample error bound of n123 / ( 123+d ) for holder - smooth functions , and max ( n123 / ( 123+d ) , n123 / d ) for piecewise holder - functions , ignoring logarithmic factors .
using these results from ( 123 ) and corollary 123 , we now state nite sample upper bounds on the semi - supervised learner ( ssl ) described above .
also , we derive nite sample minimax lower bounds on the performance of any supervised learner ( sl ) .
our main results are summarized in the following table , for model distributions characterized by various values of the margin parameter .
a sketch
123if the component marginal densities and regression functions have different smoothnesses , say and ,
the same analysis holds except that f ( x ) is holder - min ( , ) smooth on each d d .
of the derivations of the results is provided in section 123 .
here we assume that dimension d if d < 123 / ( 123 123 ) , then the supervised learning error due to to not resolving the decision sets ( which behaves like n123 / d ) is smaller than error incurred in estimating the target function itself ( which behaves like n123 / ( 123+d ) ) .
thus , when d < 123 / ( 123 123 ) , the supervised regression error is dominated by the error in smooth regions and there appears to be no benet to using a semi - supervised learner .
in the table , we suppress constants and log factors in the bounds , and also assume that m n123d so that ( n / 123 ( n ) ) d = o ( m / ( log m ) 123 ) .
the constants co and co only depend on the xed parameters of the class pxy ( ) and do not depend on .
ssl upper bound
sl lower bound
( log m ) 123 ) 123 / d
( log m ) 123 ) 123 / d
con123 / d > co ( m ( log m ) 123 ) 123 / d > co ( m ( log m ) 123 ) 123 / d >
if is large relative to the average spacing between labeled data points n123 / d , then a supervised learner can discern the decision sets accurately and ssl provides no gain .
however , if > 123 is small relative to n123 / d , but large with respect to the spacing between unlabeled data points m123 / d , then the proposed semi - supervised learner provides improved error bounds compared to any supervised learner .
if || is smaller than m123 / d , the decision sets are not discernable with unlabeled data and ssl provides no gain .
however , notice that the performance of the semi - supervised learner is no worse than the minimax lower bound for supervised learners .
in the < 123 case , if larger than m123 / d , then the semi - supervised learner can discern the decision sets and achieves smaller error bounds , whereas these sets cannot be as accurately discerned by any supervised learner .
for the overlap case ( < 123 ) , supervised learners are always limited by the error incurred due to averaging across decision sets ( n123 / d ) .
in particular , for the collection of distributions with < 123 , a faster rate of error convergence is attained by ssl compared to sl , provided m n123d .
in this paper , we develop a framework for evaluating the performance gains possible with semi - supervised learning under a cluster assumption using nite sample error bounds .
the theoretical characterization we present explains why in certain situations unlabeled data can help to improve learning , while in other situations they may not .
we demonstrate that there exist general situations under which semi - supervised learning can be signicantly superior to supervised learning in terms of achieving smaller nite sample error bounds than any supervised learner , and sometimes in terms of a better rate of error convergence .
moreover , our results also provide a quantication of the rela - tive value of unlabeled to labeled data .
while we focus on the cluster assumption in this paper , we conjecture that similar techniques can be applied to quantify the performance of semi - supervised learning under the manifold assumption as well .
in particular , we believe that the use of minimax lower bounding techniques is essential because many of the interesting distinctions between super - vised and semi - supervised learning occur only in nite sample regimes , and rates of convergence and asymptotic analyses may not capture the complete picture .
we sketch the main ideas behind the proofs here , please refer to ( 123 ) for details .
since the component densities are bounded from below and above , dene pmin : = b mink ak p ( x ) b = : pmax .
123 proof of lemma 123 first , we state two relatively straightforward results about the proposed kernel density estimator .
theorem 123 ( sup - norm density estimation of non - boundary points ) .
consider the kernel density
estimator bp ( x ) proposed in section 123
if the kernel g satises supp ( g ) = ( 123 , 123 ) d , 123 < g gmax < , r ( 123 , 123 ) d g ( u ) du = 123 and r ( 123 , 123 ) d ujg ( u ) du = 123 for 123 j ( ) , then for all
p px , with probability at least 123 123 / m ,
xsupp ( p ) \b |p ( x ) bp ( x ) | = o ( cid : 123 ) hmin ( 123 , )
m ) ( cid : 123 ) = : m .
i=123 g ( h 123
notice that m decreases with increasing m .
a detailed proof can be found in ( 123 ) .
corollary 123 ( empirical density of unlabeled data ) .
under the conditions of theorem 123 , for all p px and large enough m , with probability > 123 123 / m , for all x supp ( p ) \ b , xi u s . t .
kxi xk dhm .
from theorem 123 , for all x supp ( p ) \ b , bp ( x ) p ( x ) m pmin m > 123 , for m m ( xi x ) ) > 123 , and xi u within dhm of x .
sufciently large .
this impliespm using the density estimation results , we now show that if || > 123dhm , then for all p px , all pairs of points x123 , x123 supp ( p ) \b and all d d , for large enough m , with probability > 123 / m , we have x123 , x123 d if and only if x123 x123
we establish this in two steps : 123
x123 d , x123 123 d x123 123 x123 : since x123 and x123 belong to different decision sets , all sequences connecting x123 and x123 through unlabeled data points pass through a region where either ( i ) the density is zero and since the region is at least || > 123dhm wide , there cannot exist a sequence as dened in section 123 such that kzj zj+123k 123dhm , or ( ii ) the density is positive .
in the latter case , the marginal density p ( x ) jumps by at least pmin one or more times along all sequences connecting x123 and x123
suppose the rst jump occurs where decision set d ends and another decision set 123= d begins ( in the sequence ) .
then since d is at least || > 123dhm wide , by corollary 123 for all sequences connecting x123 and x123 through unlabeled data points , there exist points zi , zj in the sequence that lie in d \ b , d \ b , respectively , and kzi zjk hm log m .
since the density on each decision set is holder - smooth , we have |p ( zi ) p ( zj ) | pmin o ( ( hm log m ) min ( 123 , ) ) .
since zi , zj 123 b , using theorem 123 , |bp ( zi ) bp ( zj ) | |p ( zi ) p ( zj ) | 123m > m for large enough m .
thus , x123 123 x123
x123 , x123 d x123 x123 : since d has width at least || > 123dhm , there exists a region of width > 123dhm contained in d \ b , and corollary 123 implies that with probability > 123 123 / m , there exist sequence ( s ) contained in d \ b connecting x123 and x123 through 123dhm - dense unlabeled data points .
since the sequence is contained in d and the density on d is holder - smooth , we have for all points zi , zj in the sequence that satisfy kzi zjk hm log m , |p ( zi ) p ( zj ) | o ( ( hm log m ) min ( 123 , ) ) .
since zi , zj 123 b , using theorem 123 , |bp ( zi ) bp ( zj ) | |p ( zi ) p ( zj ) | + 123m m for large enough m .
thus , x123 x123
123 proof of corollary 123 123 ) 123 / m .
let 123 denote the let 123 denote the event under which lemma 123 holds .
then p ( c event that the test point x and training data x123 , .
, xn l dont lie in b .
then p ( c ( n + 123 ) p ( b ) ( n + 123 ) pmaxvol ( b ) = o ( nhm ) .
the last step follows from the denition of the set b and since the boundaries of the support sets are lipschitz , k is nite , and hence vol ( b ) = o ( hm ) .
now observe that bfd , n essentially uses the clairvoyant knowledge of the decision sets d to discern which labeled points x123 , .
, xn are in the same decision set as x .
condition - ing on 123 , 123 , lemma 123 implies that x , xi d iff x xi .
thus , we can dene a semi - supervised learner bfm , n to be the same as bfd , n except that instead of using clairvoyant knowledge of whether x , xi d , bfm , n is based on whether x xi .
it follows that e ( e ( bfd , n ) ) , and since the excess risk is bounded : e ( e ( bfm , n ) |123 , 123 ) + o ( 123 / m + nhm ) .
e ( e ( bfm , n ) |123 , 123 ) = suppxy ( ) e ( e ( bfm , n ) ) suppxy ( )
123 density adaptive regression results
123 ) semi - supervised learning upper bound : the clairvoyant counterpart of bfm , n ( x ) is given as bfd , n ( x ) bfd , n , x ( x ) , where bfd , n , x ( ) = arg minf pn i=123 ( yi f ( xi ) ) 123x , xid +pen ( f ) , and is a standard supervised learner that performs piecewise polynomial t on each decision set , where the regression function is holder - smooth .
let nd = 123
it follows ( 123 ) that
e ( ( f ( x ) bfd , n ( x ) ) 123xd|nd ) c ( nd / log nd ) 123
