in traditional machine learning approaches to classication , one uses only a labeled set to train the classier .
labeled instances however are often difcult , expensive , or time consuming to obtain , as they require the efforts of experienced human annotators .
meanwhile unlabeled data may be relatively easy to collect , but there has been few ways to use them .
semi - supervised learning addresses this problem by using large amount of unlabeled data , together with the labeled data , to build better classiers .
because semi - supervised learning requires less human effort and gives higher accuracy , it is of great interest both in theory and in practice .
we present a series of novel semi - supervised learning approaches arising from a graph representation , where labeled and unlabeled instances are represented as vertices , and edges encode the similarity between instances .
they address the fol - lowing questions : how to use unlabeled data ? ( label propagation ) ; what is the probabilistic interpretation ? ( gaussian elds and harmonic functions ) ; what if we can choose labeled data ? ( active learning ) ; how to construct good graphs ? ( hyperparameter learning ) ; how to work with kernel machines like svm ? ( graph kernels ) ; how to handle complex data like sequences ? ( kernel conditional ran - dom elds ) ; how to handle scalability and induction ? ( harmonic mixtures ) .
an extensive literature review is included at the end .
first i would like to thank my thesis committee members .
roni rosenfeld brought me into the wonderful world of research .
he not only gave me valuable advices in academics , but also helped my transition into a different culture .
john lafferty guided me further into machine learning .
i am always impressed by his mathe - matical vigor and sharp thinking .
zoubin ghahramani has been a great mentor and collaborator , energetic and full of ideas .
i wish he could stay in pittsburgh more ! tommi jaakkola helped me by asking insightful questions , and giving me thoughtful comments on the thesis .
i enjoyed working with them , and beneted enormously from the interactions with them .
i spent nearly seven years in carnegie mellon university .
i thank the fol - lowing collaborators , faculties , staffs , fellow students and friends , who made my graduate life a very memorable experience : maria florina balcan , paul bennett , adam berger , michael bett , alan black , avrim blum , dan bohus , sharon burks , can cai , jamie callan , rich caruana , arthur chan , peng chang , shuchi chawla , lifei cheng , stanley chen , tao chen , pak yan choi , ananlada chotimongicol , tianjiao chu , debbie clement , william cohen , catherine copetas , derek dreyer , dannie durand , maxine eskenazi , christos faloutsos , li fan , zhaohui fan , marc fasnacht , stephen fienberg , robert frederking , rayid ghani , anna goldenberg , evandro gouvea , alexander gray , ralph gross , benjamin han , thomas harris , alexander hauptmann , rose hoberman , fei huang , pu huang , xiaoqiu huang , yi - fen huang , jianing hu , changhao jiang , qin jin , rong jin , rosie jones , szu - chen jou , jaz kandola , chris koch , john kominek , leonid kontorovich , chad langley , guy lebanon , lillian lee , kevin lenzo , hongliang liu , yan liu , xi - ang li , ariadna font llitjos , si luo , yong lu , matt mason , iain matthews , an - drew mccallum , uwe meier , tom minka , tom mitchell , andrew w moore , jack mostow , ravishankar mosur , jon nedel , kamal nigam , eric nyberg , alice oh , chris paciorek , brian pantano , yue pan , vasco calais pedro , francisco pereira , yanjun qi , bhiksha raj , radha rao , pradeep ravikumar , nadine reaves , max ritter , chuck rosenberg , steven rudich , alex rudnicky , mugizi robert rweban - gira , kenji sagae , barbara sandling , henry schneiderman , tanja schultz , teddy
seidenfeld , michael seltzer , kristie seymore , minglong shao , chen shimin , rita singh , jim skees , richard stern , diane stidle , yong sun , sebastian thrun , ste - fanie tomko , laura mayeld tomokiyo , arthur toth , yanghai tsin , alex waibel , lisha wang , mengzhi wang , larry wasserman , jeannette wing , weng - keen wong , sharon woodside , hao xu , mingxin xu , wei xu , jie yang , jun yang , ke yang , wei yang , yiming yang , rong yan , rong yan , stacey young , hua yu , klaus zechner , jian zhang , jieyuan zhang , li zhang , rong zhang , ying zhang , yi zhang , bing zhao , pei zheng , jie zhu .
i spent some serious effort nding ev - eryone from archival emails .
my apologies if i left your name out .
in particular , i thank you if you are reading this thesis .
finally i thank my family .
my parents yu and jingquan endowed me with the curiosity about the natural world .
my dear wife jing brings to life so much love and happiness , making thesis writing an enjoyable endeavor .
last but not least , my ten - month - old daughter amanda helped me ty
pe the , manuscr ihpt .
123 what is semi - supervised learning ? .
123 a short history .
123 structure of the thesis
123 label propagation
123 problem setup .
123 the algorithm
illustrative examples
123 what is a good graph ?
123 example one : handwritten digits .
123 example two : document categorization .
123 example three : the freefoodcam .
123 common ways to create graphs
123 gaussian random fields
123 gaussian random fields .
123 the graph laplacian .
123 harmonic functions .
interpretation and connections .
123 . 123 random walks .
123 . 123 electric networks .
123 . 123 graph mincut .
incorporating class proportion knowledge .
incorporating vertex potentials on unlabeled instances
123 experimental results
123 active learning
123 combining semi - supervised and active learning .
123 why not entropy minimization .
123 experiments
123 connection to gaussian processes
incorporating a noise model
123 a finite set gaussian process model .
123 experiments .
123 extending to unseen data
123 graph hyperparameter learning
123 evidence maximization .
123 entropy minimization .
123 minimum spanning tree .
123 discussion
123 kernels from the spectrum of laplacians
123 the spectrum of laplacians 123 from laplacians to kernels .
123 convex optimization using qcqp .
123 semi - supervised kernels with order constraints .
123 experiments
123 sequences and beyond
123 cliques and two graphs .
123 representer theorem for kcrfs .
123 sparse training : clique selection .
123 synthetic data experiments
123 harmonic mixtures
123 review of mixture models and the em algorithm .
123 label smoothness on the graph .
123 combining mixture model and graph .
123 . 123 the special case with = 123 .
123 . 123 the general case with > 123 .
123 experiments .
123 . 123 synthetic data .
123 . 123 image recognition : handwritten digits .
123 . 123 text categorization : pc vs
123 related work .
123 discussion
123 literature review
123 q&a .
123 generative mixture models and em .
123 . 123 identiability .
123 123 . 123 model correctness .
123 123 . 123 em local maxima .
123 123 . 123 cluster and label .
123 123 self - training .
123 123 co - training .
123 123 maximizing separation .
123 123 . 123 transductive svm .
123 123 . 123 gaussian processes .
123 123 . 123 information regularization .
123 123 . 123 entropy minimization .
123 123 . 123 regularization by graph .
123 123 . 123 graph construction .
123 . 123 induction .
123 123 . 123 consistency .
123 . 123 ranking .
123 123 . 123 directed graphs .
123 123 . 123 fast computation .
123 123 metric - based model selection .
123 related areas .
123 123 . 123 spectral clustering .
123 . 123 clustering with side information .
123 123 . 123 nonlinear dimensionality reduction .
123 123 . 123 learning a distance metric .
123 123 . 123 inferring label sampling mechanisms
123 graph - based methods
a update harmonic function
b matrix inverse
c laplace approximation for gaussian processes
d evidence maximization
e mean field approximation
f comparing iterative algorithms
f . 123 label propagation .
f . 123 conjugate gradient .
f . 123 loopy belief propagation on gaussian elds .
f . 123 empirical results
123 what is semi - supervised learning ?
the eld of machine learning has traditionally been divided into three sub - elds : unsupervised learning .
the learning system observes an unlabeled set of items , represented by their features ( x123 , .
the goal is to organize the items .
typical unsupervised learning tasks include clustering that groups items into clusters; outlier detection which determines if a new item x is sig - nicantly different from items seen so far; dimensionality reduction which maps x into a low dimensional space , while preserving certain properties of
supervised learning .
the learning system observes a labeled training set consisting of ( feature , label ) pairs , denoted by ( ( x123 , y123 ) , .
, ( xn , yn ) ) .
the goal is to predict the label y for any new input with feature x .
a supervised learning task is called regression when y r , and classication when y takes a set of discrete values .
reinforcement learning .
the learning system repeatedly observes the envi - ronment x , performs an action a , and receives a reward r .
the goal is to choose the actions that maximize the future rewards .
this thesis focuses on classication , which is traditionally a supervised learn -
ing task .
to train a classier one needs the labeled training set ( ( x123 , y123 ) , .
, ( xn , yn ) ) .
however the labels y are often hard , expensive , and slow to obtain , because it may require experienced human annotators .
for instance ,
speech recognition .
accurate transcription of speech utterance at phonetic level is extremely time consuming ( as slow as 123rt , i . e .
123 times longer
than the utterance duration ) , and requires linguistic expertise .
transcription at word level is still time consuming ( about 123rt ) , especially for conver - sational or spontaneous speech .
this problem is more prominent for foreign languages or dialects with less speakers , when linguistic experts of that lan - guage are hard to nd .
text categorization .
filtering out spam emails , categorizing user messages , recommending internet articles many such tasks need the user to label text document as interesting or not .
having to read and label thousands of documents is daunting for average users .
parsing .
to train a good parser one needs sentence / parse tree pairs , known as treebanks .
treebanks are very time consuming to construct by linguists .
it took the experts several years to create parse trees for only a few thousand
video surveillance .
manually labeling people in large amount of surveil -
lance camera images can be time consuming .
protein structure prediction .
it may take months of expensive lab work by
expert crystallographers to identify the 123d structure of a single protein .
on the other hand , unlabeled data x , without labels , is usually available in large quantity and costs little to collect .
utterances can be recorded from radio broad - cast; text documents can be crawled from the internet; sentences are everywhere; surveillance cameras run 123 hours a day; dna sequences of proteins are readily available from gene databases .
the problem with traditional classication methods is : they cannot use unlabeled data to train classiers .
the question semi - supervised learning addresses is : given a relatively small labeled dataset ( ( x , y ) ) and a large unlabeled dataset ( x ) , can one devise ways to learn from both for classication ? the name semi - supervised learning comes from the fact that the data used is between supervised and unsupervised learning .
semi - supervised learning promises higher accuracies with less annotating effort .
it is therefore of great theoretic and practical interest .
a broader denition of semi - supervised learning includes regression and clustering as well , but we will not pursued that direction here .
123 a short history of semi - supervised learning
there has been a whole spectrum of interesting ideas on how to learn from both labeled and unlabeled data .
we give a highly simplied history of semi - supervised
learning in this section .
interested readers can skip to chapter 123 for an extended literature review .
it should be pointed out that semi - supervised learning is a rapidly evolving eld , and the review is necessarily incomplete .
early work in semi - supervised learning assumes there are two classes , and each class has a gaussian distribution .
this amounts to assuming the complete data comes from a mixture model .
with large amount of unlabeled data , the mixture components can be identied with the expectation - maximization ( em ) algorithm .
one needs only a single labeled example per component to fully determine the mixture model .
this model has been successfully applied to text categorization .
a variant is self - training : a classier is rst trained with the labeled data .
it is then used to classify the unlabeled data .
the most condent unlabeled points , together with their predicted labels , are added to the training set .
the classier is re - trained and the procedure repeated .
note the classier uses its own predictions to teach itself .
this is a hard version of the mixture model and em algorithm .
the procedure is also called self - teaching , or bootstrapping123 in some research communities .
one can imagine that a classication mistake can reinforce itself .
both methods have been used since long time ago .
they remain popular be -
cause of their conceptual and algorithmic simplicity .
co - training reduces the mistake - reinforcing danger of self - training .
this recent method assumes that the features of an item can be split into two subsets .
each sub - feature set is sufcient to train a good classier; and the two sets are conditionally independent given the class .
initially two classiers are trained with the labeled data , one on each sub - feature set .
each classier then iteratively classies the unlabeled data , and teaches the other classier with its predictions .
with the rising popularity of support vector machines ( svms ) , transductive svms emerge as an extension to standard svms for semi - supervised learning .
transductive svms nd a labeling for all the unlabeled data , and a separating hyperplane , such that maximum margin is achieved on both the labeled data and the ( now labeled ) unlabeled data .
intuitively unlabeled data guides the decision boundary away from dense regions .
recently graph - based semi - supervised learning methods have attracted great attention .
graph - based methods start with a graph where the nodes are the labeled and unlabeled data points , and ( weighted ) edges reect the similarity of nodes .
the assumption is that nodes connected by a large - weight edge tend to have the same label , and labels can propagation throughout the graph .
graph - based meth - ods enjoy nice properties from spectral graph theory .
this thesis mainly discusses graph - based semi - supervised methods .
we summarize a few representative semi - supervised methods in table 123 .
123not to be confused with the resample procedure with the same name in statistics .
mixture model , em generative mixture model transductive svm low density region between classes
conditionally independent and redundant features splits labels smooth on graph
table 123 : some representative semi - supervised learning methods
123 structure of the thesis
the rest of the thesis is organized as follows :
chapter 123 starts with the simple label propagation algorithm , which propagates class labels on a graph .
this is the rst semi - supervised learning algorithm we will encounter .
it is also the basis for many variations later .
chapter 123 discusses how one constructs a graph .
the emphasis is on the intu - ition what graphs make sense for semi - supervised learning ? we will give several examples on various datasets .
chapter 123 formalizes label propagation in a probabilistic framework with gaus - sian random elds .
concepts like graph laplacian and harmonic function are intro - duced .
we will explore interesting connections to electric networks , random walk , and spectral clustering .
issues like the balance between classes , and inclusion of external classiers are also discussed here .
chapter 123 assumes that one can choose a data point and ask an oracle for the label .
this is the standard active learning scheme .
we show that active learning and semi - supervised learning can be naturally combined .
chapter 123 establishes the link to gaussian processes .
the kernel matrices are
shown to be the smoothed inverse graph laplacian .
chapter 123 no longer assumes the graph is given and xed .
instead , we pa - rameterize the graph weights , and learn the optimal hyperparameters .
we will discuss several methods : evidence maximization , entropy minimization , and mini - mum spanning tree .
chapter 123 turns semi - supervised learning problem into kernel learning .
we show a natural family of kernels derived from the graph laplacian , and nd the best kernel via convex optimization .
chapter 123 discusses kernel conditional random elds , and its potential applica -
tion in semi - supervised learning , for sequences and other complex structures .
chapter 123 explores scalability and induction for semi - supervised learning .
chapter 123 reviews the literatures on semi - supervised learning .
in this chapter we introduce our rst semi - supervised learning algorithm : label propagation .
we formulate the problem as a form of propagation on a graph , where a nodes label propagates to neighboring nodes according to their proximity .
in this process we x the labels on the labeled data .
thus labeled data act like sources that push out labels through unlabeled data .
123 problem setup
let ( ( x123 , y123 ) .
( xl , yl ) ) be the labeled data , y ( 123 .
c ) , and ( xl+123 .
xl+u ) the unlabeled data , usually l ( cid : 123 ) u .
let n = l + u .
we will often use l and u to denote labeled and unlabeled data respectively .
we assume the number of classes c is known , and all classes are present in the labeled data .
in most of the thesis we study the transductive problem of nding the labels for u .
the inductive problem of nding labels for points outside of l u will be discussed in chapter 123
intuitively we want data points that are similar to have the same label .
we create a graph where the nodes are all the data points , both labeled and unlabeled .
the edge between nodes i , j represents their similarity .
for the time being let us assume the graph is fully connected with the following weights :
wij = exp ( cid : 123 ) kxi xjk123
where is a bandwidth hyperparameter .
the construction of graphs will be dis - cussed in later chapters .
123 the algorithm
we propagate the labels through the edges .
larger edge weights allow labels to travel through more easily .
dene a n n probabilistic transition matrix p
pij = p ( i j ) =
where pij is the probability of transit from node i to j .
also dene a l c label matrix yl , whose ith row is an indicator vector for yi , i l : yic = ( yi , c ) .
we will compute soft labels f for the nodes .
f is a n c matrix , the rows can be interpreted as the probability distributions over labels .
the initialization of f is not important .
we are now ready to present the algorithm .
the label propagation algorithm is as follows :
propagate f p f 123
clamp the labeled data fl = yl .
repeat from step 123 until f converges .
in step 123 , all nodes propagate their labels to their neighbors for one step .
step 123 is critical : we want persistent label sources from labeled data .
so instead of letting the initially labels fade away , we clamp them at yl .
with this constant push from labeled nodes , the class boundaries will be pushed through high density regions and settle in low density gaps .
if this structure of data ts the classication goal , then the algorithm can use unlabeled data to help learning .
we now show the algorithm converges to a simple solution .
let f = ( cid : 123 ) fl
since fl is clamped to yl , we are solely interested in fu .
we split p into labeled and unlabeled sub - matrices
p = ( cid : 123 ) pll plu pu l pu u ( cid : 123 )
it can be shown that our algorithm is
fu pu u fu + pu lyl
which leads to
fu = lim
( pu u ) nf 123
u + nxi=123
( pu u ) ( i123 ) ! pu lyl
u is the initial value for fu .
we need to show ( pu u ) nf 123
where f 123 row normalized , and pu u is a sub - matrix of p , it follows
since p is
( pu u ) ij , i = 123
ij = xj xk
( pu u ) n
( pu u ) ( n123 )
ik ( pu u ) kj
( pu u ) ( n123 )
( pu u ) ( n123 )
( pu u ) kj
therefore the row sums of ( pu u ) n converges to zero , which means ( pu u ) nf 123 123
thus the initial value f 123
u is inconsequential .
obviously fu = ( i pu u ) 123pu lyl
is a xed point .
therefore it is the unique xed point and the solution to our iterative algorithm .
this gives us a way to solve the label propagation problem directly without iterative propagation .
note the solution is valid only when i pu u is invertible .
the condition is satised , intuitively , when every connected component in the graph has at least one labeled point in it .
123 illustrative examples
we demonstrate the properties of the label propagation algorithm on two synthetic datasets .
figure 123 ( a ) shows a synthetic dataset with three classes , each being a narrow horizontal band .
data points are uniformly drawn from the bands .
there are 123 labeled points and 123 unlabeled points .
123 - nearest - neighbor algorithm , one of the standard supervised learning methods , ignores the unlabeled data and thus the
( a ) the data
( c ) label propagation
figure 123 : the three bands dataset .
labeled data are marked with color symbols , and unlabeled data are black dots in ( a ) .
123nn ignores unlabeled data structure ( b ) , while label propagation takes advantage of it ( c ) .
band structure ( b ) .
on the other hand , the label propagation algorithm takes into account the unlabeled data ( c ) .
it propagates labels along the bands .
in this exam - ple , we used = 123 from the minimum spanning tree heuristic ( see chapter 123 ) .
figure 123 shows a synthetic dataset with two classes as intertwined three - dimensional spirals .
there are 123 labeled points and 123 unlabeled points .
again , 123nn fails to notice the structure of unlabeled data , while label propagation nds the spirals .
we used = 123 .
( a ) the data
( c ) label propagation
figure 123 : the springs dataset .
again 123nn ignores unlabeled data structure , while label propagation takes advantage of it .
what is a good graph ?
in label propagation we need a graph , represented by the weight matrix w .
how does one construct a graph ? what is a good graph ? in this chapter we give several examples on different datasets .
the goal is not to rigorously dene good graphs , but to illustrate the assumptions behind graph based semi - supervised learning .
a good graph should reect our prior knowledge about the domain .
at the present time , its design is more of an art than science .
it is the practitioners respon - sibility to feed a good graph to graph - based semi - supervised learning algorithms , in order to expect useful output .
the algorithms in this thesis do not deal directly with the design of graphs ( with the exception of chapter 123 ) .
123 example one : handwritten digits
our rst example is optical character recognition ( ocr ) for handwritten digits .
the handwritten digits dataset originates from the cedar buffalo binary digits database ( hull , 123 ) .
the digits were initially preprocessed to reduce the size of each image down to a 123 123 grid by down - sampling and gaussian smoothing , with pixel values in 123 to 123 ( le cun et al . , 123 ) .
figure 123 shows a random sam - ple of the digits .
in some of the experiments below they are further scaled down to 123 123 by averaging 123 123 pixel bins .
we show why graphs based on pixel - wise euclidean distance make sense for digits semi - supervised learning .
euclidean distance by itself is a bad similarity measure .
for example the two images in figure 123 ( a ) have a large euclidean distance although they are in the same class .
however euclidean distance is a good local similarity measure .
if it is small , we can expect the two images to be in the same class .
consider a k - nearest - neighbor graph based on euclidean distance .
neighboring images have small euclidean distance .
with large amount
figure 123 : some random samples of the handwritten digits dataset
( a ) two images of 123 with large euclidean distance
( b ) a path in an euclidean distance knn graph between them
figure 123 : locally similar images propagate labels to globally dissimilar ones .
of unlabeled images of 123s , there will be many paths connecting the two images in ( a ) .
one such path is shown in figure 123 ( b ) .
note adjacent pairs are similar to each other .
although the two images in ( a ) are not directly connected ( not similar in euclidean distance ) , label propagation can propagate along the paths , marking them with the same label .
figure 123 shows a symmetrized 123 123nn graph based on euclidean distance .
the small dataset has only a few 123s and 123s for clarity .
the actual graphs used in the ocr experiments are too large to show .
it should be mentioned that our focus is on semi - supervised learning methods , not ocr handwriting recognizers .
we could have normalized the image intensity , or used edge detection or other invariant features instead of euclidean distance .
these should be used for any real applications , as the graph should represent do - main knowledge .
the same is true for all other tasks described below .
123symmetrization means we connect nodes i , j if i is in js knn or vice versa , and therefore a
node can have more than k edges .
figure 123 : a symmetrized euclidean 123nn graph on some 123s and 123s .
label prop - agation on this graph works well .
123 example two : document categorization
our second example is document categorization on 123 newsgroups dataset 123 .
each document has no header except from and subject lines .
each document is minimally processed into a tf . idf vector , without frequency cutoff , stemming , or a stopword list .
the from and subject lines are included .
we measure the similarity between two documents u , v with the cosine similarity cs ( u , v ) = u>v like euclidean distance , cosine similarity is not a good global measure : documents from the same class can have few common words .
however it is a good
a graph based on cosine similarity in this domain makes good sense .
docu - ments from the same thread ( class ) tend to quote one another , giving them high cosine similarities .
many paths in the graph are quotations .
even though the rst and last documents in a thread share few common words , them can be classied in the same class via the graph .
the full graphs are again too large to visualize .
we show the few nearest neigh - bors of document 123 in comp . sys . ibm . pc . hardware vs .
comp . sys . mac . hardware sub - dataset in figure 123 .
the example is typical in the whole graph .
nevertheless we note that not all edges are due to quotation .
123 example three : the freefoodcam
the carnegie mellon university school of computer science has a lounge , where leftover pizza from various meetings converge , to the delight of students .
in fact a webcam ( the freefoodcam 123 ) was set up in the lounge , so that people can see whether food is available .
the freefoodcam provides interesting research oppor - tunities .
we collect webcam images of 123 people over a period of several months .
the data is used for 123 - way people recognition , i . e .
identify the name of person in freefoodcam images .
the dataset consists of 123 images with one and only one person in it .
figure 123 shows some random images in the dataset .
the task is not
the images of each person were captured on multiple days during a four month period .
people changed clothes , had hair cut , one person even grew a beard .
we simulate a video surveillance scenario where a person is manually labeled at rst , and needs to be recognized on later days .
therefore we choose labeled data within the rst day of a persons appearance , and test on
123http : / / www . ai . mit . edu / people / jrennie / 123newsgroups / , 123 version 123http : / / www - 123cs . cmu . edu / coke / , carnegie mellon internal access .
from : rash@access . digex . com ( wayne rash ) subject : re : 123 " monitors mikey@sgi . com ( mike yang ) writes : >in article <123qslfs$bm123@access . digex . net> rash@access . digex . com ( wayne rash ) writes : >>i also reviewed a new nanao , the f123iw , which has just >whats the difference between the f123i and the new f123iw ? im >about to buy a gateway system and was going to take the f123i >upgrade .
should i get the f123iw instead ? the f123iw is optimized for windows .
it powers down when the screen blanker appears , it powers down with you turn your computer off , and it meets all of the swedish standards .
its also protected against emi from personally , i think the f123i is more bang for the buck right now .
silicon graphics , inc .
( a ) document 123
its nearest neighbors are shown below .
from : mikey@eukanuba . wpd . sgi . com ( mike yang ) subject : re : 123 " monitors in article <123qulqa$hp123@access . digex . net> , rash@access . digex . com ( wayne rash ) writes : |> the f123iw is optimized for windows .
it powers down when the screen |> blanker appears , it powers down with you turn your computer off , and it |> meets all of the swedish standards .
its also protected against emi from |> adjacent monitors .
thanks for the info .
|> personally , i think the f123i is more bang for the buck right now .
how much more does the f123iw cost ?
silicon graphics , inc .
( b ) the nearest neighbor 123
it quotes a large portion of 123
from : rash@access . digex . com ( wayne rash ) subject : re : 123 " monitors mikey@eukanuba . wpd . sgi . com ( mike yang ) writes : >in article <123qulqa$hp123@access . digex . net> , rash@access . digex . com ( wayne rash ) writes : >|> the f123iw is optimized for windows .
it powers down when the screen >|> blanker appears , it powers down with you turn your computer off , and it >|> meets all of the swedish standards .
its also protected against emi from >|> adjacent monitors .
>thanks for the info .
>|> personally , i think the f123i is more bang for the buck right now .
>how much more does the f123iw cost ? i think the difference is about 123 dollars , but i could be wrong .
these things change between press time and publication .
silicon graphics , inc .
( c ) the 123nd nearest neighbor 123
it also quotes 123
figure 123 : ( continued on next page )
from : mikey@sgi . com ( mike yang ) subject : re : 123 " monitors in article <123qslfs$bm123@access . digex . net> rash@access . digex . com ( wayne rash ) writes : >i also reviewed a new nanao , the f123iw , which has just whats the difference between the f123i and the new f123iw ? im about to buy a gateway system and was going to take the f123i upgrade .
should i get the f123iw instead ?
silicon graphics , inc .
( d ) the 123rd nearest neighbor 123 , quoted by 123
from : goyal@utdallas . edu ( mohit k goyal ) subject : re : 123 " monitors >the mitsubishi .
i also reviewed a new nanao , the f123iw , which has just >been released .
last year for the may 123 issue of windows , i reviewed do you have the specs for this monitor ? what have they changed from the do you know if their is going to be a new t123i soon ? ( a t123iw ? )
( e ) the 123th nearest neighbor 123
it and 123 quote the same source .
from : mikey@eukanuba . wpd . sgi . com ( mike yang ) subject : gateway 123dx123 - 123v update i just ordered my 123dx123 - 123v system from gateway .
thanks for all the net discussions which helped me decide among all the vendors and options .
right now , the 123dx123 - 123v system includes 123mb of ram .
the 123mb upgrade used to cost an additional $123
silicon graphics , inc .
( f ) the 123th nearest neighbor 123
it has a different subject than 123 , but the
same author signature appears in both .
figure 123 : the nearest neighbors of document 123 in the 123newsgroups dataset , as measured by cosine similarity .
notice many neighbors either quote or are quoted by the document .
many also share the same subject line .
figure 123 : a few freefoodcam image examples
the remaining images of the day and all other days .
it is harder than testing only on the same day , or allowing labeled data to come from all days .
the freefoodcam is a low quality webcam .
each frame is 123 123 so faces of far away people are small; the frame rate is a little over 123 frame per second; lighting in the lounge is complex and changing .
the person could turn the back to the camera .
about one third of the images
have no face .
since only a few images are labeled , and we have all the test images , it is a natural task to apply semi - supervised learning techniques .
as computer vision is not the focus of the paper , we use only primitive image processing methods to extract the following features :
each image has a time stamp .
foreground color histogram .
a simple background subtraction algorithm is ap - plied to each image to nd the foreground area .
the foreground area is assumed to be the person ( head and body ) .
we compute the color histogram ( hue , saturation and brightness ) of the foreground pixels .
the histogram is a 123 dimensional vector .
face image .
we apply a face detector ( schneiderman , 123b ) ( schneiderman , 123a ) to each image .
note it is not a face recognizer ( we do not use a face recognizer for this task ) .
it simply detects the presence of frontal or prole faces .
the output is the estimated center and radius of the detected face .
we take a square area around the center as the face image .
if no face is detected , the face image is empty .
one theme throughout the thesis is that the graph should reect domain knowl - edge of similarity .
the freefoodcam is a good example .
the nodes in the graph are all the images .
an edge is put between two images by the following criteria :
time edges people normally move around in the lounge in moderate speed , thus adjacent frames are likely to contain the same person .
we represent this belief in the graph by putting an edge between images i , j whose time difference is less than a threshold t123 ( usually a few seconds ) .
color edges the color histogram is largely determined by a persons clothes .
we assume people change clothes on different days , so color histogram is unusable across multiple days .
however it is an informative feature during a shorter time period ( t123 ) like half a day .
in the graph for every image i , we nd the set of images having a time difference between ( t123 , t123 ) to i , and connect i with its kc - nearest - neighbors ( in terms of cosine similarity on histograms ) in the set .
kc is a small number , e . g
face edges we resort to face similarity over longer time spans .
for every image i with a face , we nd the set of images more than t123 apart from i , and connect i with its kf - nearest - neighbor in the set .
we use pixel - wise euclidean distance between face images ( the pair of face images are scaled to the same size ) .
the nal graph is the union of the three kinds of edges .
the edges are unweighted in the experiments ( one could also learn different weights for different kinds of edges .
for example it might be advantageous to give time edges higher weights ) .
we used t123 = 123 second , t123 = 123 hours , kc = 123 and kf = 123 below .
incidentally these parameters give a connected graph .
it is impossible to visualize the whole graph .
instead we show the neighbors of a random node in figure 123 .
123 common ways to create graphs
sometimes one faces a dataset with limited domain knowledge .
this section dis - cusses some common ways to create a graph as a starting point .
neighbor 123 : time edge
neighbor 123 : color edge
neighbor 123 : color edge
neighbor 123 : color edge
neighbor 123 : face edge
figure 123 : a random image and its neighbors in the graph
fully connected graphs one can create a fully connected graph with an edge be - tween all pairs of nodes .
the graph needs to be weighted so that similar nodes have large edge weight between them .
the advantage of a fully con - nected graph is in weight learning with a differentiable weight function , one can easily take the derivatives of the graph w . r . t .
weight hyperparam - eters .
the disadvantage is in computational cost as the graph is dense ( al - though sometimes one can apply fast approximate algorithms like n - body problems ) .
furthermore we have observed that empirically fully connect graphs performs worse than sparse graphs .
sparse graphs one can create knn or nn graphs as shown below , where each node connects to only a few nodes .
such sparse graphs are computationally fast .
they also tend to enjoy good empirical performance .
we surmise it is because spurious connections between dissimilar nodes ( which tend to be in different classes ) are removed .
with sparse graphs , the edges can be un - weighted or weighted .
one disadvantage is weight learning a change in weight hyperparameters will likely change the neighborhood , making opti -
knn graphs nodes i , j are connected by an edge if i is in js k - nearest - neighborhood
or vice versa .
k is a hyperparameter that controls the density of the graph .
knn has the nice property of adaptive scales , because the neighborhood radius is different in low and high data density regions .
small k may re - sult in disconnected graphs .
for label propagation this is not a problem if each connected component has some labeled points .
for other algorithms introduced later in the thesis , one can smooth the laplacian .
nn graphs nodes i , j are connected by an edge , if the distance d ( i , j ) .
the hyperparameter controls neighborhood radius .
although is continuous , the search for the optimal value is discrete , with at most o ( n123 ) values ( the edge lengths in the graph ) .
tanh - weighted graphs wij = ( tanh ( 123 ( d ( i , j ) 123 ) ) + 123 ) / 123
the hyperbolic tangent function is a soft step function that simulates nn in that when d ( i , j ) ( cid : 123 ) 123 , wij 123; d ( i , j ) ( cid : 123 ) 123 , wij 123
the hyperparameters 123 , 123 controls the slope and cutoff value respectively .
the intuition is to create a soft cutoff around distance 123 , so that close examples ( presumably from the same class ) are connected and examples from different classes ( presumably with large distance ) are nearly disconnected .
unlike nn , tanh - weighted graph is continuous with respect to 123 , 123 and is amenable to learning with
exp - weighted graphs wij = exp ( d ( i , j ) 123 / 123 ) .
again this is a continuous weight - ing scheme , but the cutoff is not as clear as tanh ( ) .
hyperparameter controls the decay rate .
if d is e . g .
euclidean distance , one can have one hyperparameter per feature dimension .
these weight functions are all potentially useful when we do not have enough do - main knowledge .
however we observed that weighted knn graphs with a small k tend to perform well empirically .
all the graph construction methods have hyper - parameters .
we will discuss graph hyperparameter learning in chapter 123
a graph is represented by the n n weight matrix w , wij = 123 if there is no edge between node i , j .
we point out that w does not have to be positive semi - denite .
nor need it satisfy metric conditions .
as long as w s entries are non - negative and symmetric , the graph laplacian , an important quantity dened in the next chapter , will be well dened and positive semi - denite .
gaussian random fields and
in this chapter we formalize label propagation with a probabilistic framework .
without loss of generality we assume binary classication y ( 123 , 123 ) .
we as - sume the n n weight matrix w is given , which denes the graph .
w has to be symmetric with non - negative entries , but otherwise need not to be positive semi - denite .
intuitively w species the local similarity between points .
our task is to assign labels to unlabeled nodes .
123 gaussian random fields
our strategy is to dene a continuous random eld on the graph .
first we dene a real function over the nodes f : l u r .
notice f can be negative or larger than 123
intuitively , we want unlabeled points that are similar ( as determined by edge weights ) to have similar labels .
this motivates the choice of the quadratic
e ( f ) =
wij ( f ( i ) f ( j ) ) 123
obviously e is minimized by constant functions .
but since we have observed some labeled data , we constrain f to take values f ( i ) = yi , i l on the labeled data .
we assign a probability distribution to functions f by a gaussian random eld
p ( f ) =
where is an inverse temperature parameter , and z is the partition function
exp ( e ( f ) ) df
terested in the inference problem p ( fi|yl ) , i u , or the meanr
which normalizes over functions constrained to yl on the labeled data .
we are in - the distribution p ( f ) is very similar to a standard markov random eld with discrete states ( the ising model , or boltzmann machines ( zhu & ghahramani , 123b ) ) .
in fact the only difference is the relaxation to real - valued states .
however this relaxation greatly simplify the inference problem .
because of the quadratic energy , p ( f ) and p ( fu|yl ) are both multivariate gaussian distributions .
this is why p is called a gaussian random eld .
the marginals p ( fi|yl ) are univariate gaussian too , and have closed form solutions .
123 the graph laplacian
we now introduce an important quantity : the combinatorial laplacian
be the diagonal degree matrix , where dii =pj wij is the degree of node i
laplacian is dened as
for the time being the laplacian is useful shorthand for the energy function : one can verify that
e ( f ) =
wij ( f ( i ) f ( j ) ) 123 = f >f
the gaussian random eld can be written as
p ( f ) =
where the quadratic form becomes obvious .
plays the role of the precision ( in - verse covariance ) matrix in a multivariate gaussian distribution .
it is always pos - itive semi - denite if w is symmetric and non - negative .
the laplacian will be further explored in later chapters .
123 harmonic functions
it is not difcult to show that the minimum energy function f = arg minfl=yl is harmonic; namely , it satises f = 123 on unlabeled data points u , and is equal to yl on the labeled data points l .
we use h to represent this harmonic function .
the harmonic solution h = 123 subject to hl = yl is given by
w = ( cid : 123 ) wll wlu wu l wu u ( cid : 123 )
hu = ( du u wu u ) 123wu lyl
= ( u u ) 123u lyl = ( i pu u ) 123pu lyl
the harmonic property means that the value of h ( i ) at each unlabeled data
point i is the average of its neighbors in the graph :
wijh ( j ) , for i u
which is consistent with our prior notion of smoothness with respect to the graph .
because of the maximum principle of harmonic functions ( doyle & snell , 123 ) , h is unique and satises 123 h ( i ) 123 for i u ( remember h ( i ) = 123 or 123 for to compute the harmonic solution , we partition the weight matrix w ( and similarly d , , etc . ) into 123 blocks for l and u :
the last representation is the same as equation ( 123 ) , where p = d123w is the transition matrix on the graph .
the label propagation algorithm in chapter 123 in fact computes the harmonic function .
the harmonic function minimizes the energy and is thus the mode of ( 123 ) .
since ( 123 ) denes a gaussian distribution which is symmetric and unimodal , the mode is also the mean .
123 interpretation and connections
the harmonic function can be viewed in several fundamentally different ways , and these different viewpoints provide a rich and complementary set of techniques for reasoning about this approach to the semi - supervised learning problem .
123 . 123 random walks
imagine a random walk on the graph .
starting from an unlabeled node i , we move to a node j with probability pij after one step .
the walk stops when we hit a labeled node .
then h ( i ) is the probability that the random walk , starting from node i , hits a labeled node with label 123
here the labeled data is viewed as an absorbing boundary for the random walk .
the random walk interpretation is shown in figure 123 .
figure 123 : harmonic function as random walk on the graph
figure 123 : harmonic function as electric network graph
123 . 123 electric networks
we can also view the framework as electrical networks .
imagine the edges of the graph to be resistors with conductance w .
equivalently the resistance between nodes i , j is 123 / wij .
we connect positive labeled nodes to a +123 volt source , and negative labeled nodes to the ground .
then hu is the voltage in the resulting elec - tric network on each of the unlabeled nodes ( figure 123 ) .
furthermore hu min - imizes the energy dissipation , in the form of heat , of the electric network .
the energy dissipation is exactly e ( h ) as in ( 123 ) .
the harmonic property here follows from kirchoffs and ohms laws , and the maximum principle then shows that this is precisely the same solution obtained in ( 123 ) .
123 . 123 graph mincut
the harmonic function can be viewed as a soft version of the graph mincut ap - proach by blum and chawla ( 123 ) .
in graph mincut the problem is cast as one
of nding a minimum st - cut .
the minimum st - cuts minimize the same energy function ( 123 ) but with discrete labels 123 , 123
therefore they are the modes of a stan - dard boltzmann machine .
it is difcult to compute the mean .
one often has to use monte carlo markov chain or use approximation methods .
furthermore , the min - imum st - cut is not necessarily unique .
for example , consider a linear chain graph with n nodes .
let wi , i+123 = 123 and other edges zero .
let node 123 be labeled positive , node n negative .
then a cut on any one edge is a minimum st - cut .
in contrast , the harmonic solution has a closed form , unique solution for the mean , which is also
the gaussian random elds and harmonic functions also have connection to graph spectral clustering , and kernel regularization .
these will be discussed later .
123 incorporating class proportion knowledge
to go from f to class labels , the obvious decision rule is to assign label 123 to node i if h ( i ) > 123 , and label 123 otherwise .
we call this rule 123 - threshold .
in terms of the random walk interpretation if h ( i ) > 123 , then starting at i , the random walk is more likely to reach a positively labeled point before a negatively labeled point .
this decision rule works well when the classes are well separated .
however in practice , 123 - threshold tends to produce unbalanced classication ( most points in one of the classes ) .
the problem stems from the fact that w , which species the data manifold , is often poorly estimated in practice and does not reect the classication goal .
in other words , we should not fully trust the graph structure .
often we have the knowledge of class proportions , i . e .
how many unlabeled data are from class 123 and 123 respectively .
this can either be estimated from the labeled set , or given by domain experts .
this is a valuable piece of complementary
we propose a heuristic method called class mass normalization ( cmn ) to in - corporate the information as follows .
lets assume the desirable proportions for classes 123 and 123 are q and 123 q respectively .
dene the mass of class 123 to be pi hu ( i ) , and the mass of class 123 to bepi ( 123 hu ( i ) ) .
class mass normalization scales these masses to match q and 123 q .
in particular an unlabeled point i is classied as class 123 iff
> ( 123 q )
pi hu ( i )
123 hu ( i )
pi ( 123 hu ( i ) )
cmn extends naturally to the general multi - label case .
it is interesting to note cmns potential connection to the procedures in ( belkin et al . , 123a ) .
further research is needed to study whether the heuristic ( or its variation ) can be justied
123 incorporating vertex potentials on unlabeled instances
we can incorporate the knowledge on individual class label of unlabeled instances too .
this is similar to using a assignment cost for each unlabeled instance .
for example , the external knowledge may come from an external classier which is constructed on labeled data alone ( it could come from domain expert too ) .
the external classier produces labels gu on the unlabeled data; g can be 123 / 123 or soft labels in ( 123 , 123 ) .
we combine g with the harmonic function h by a simple modi - cation of the graph .
for each unlabeled node i in the original graph , we attach a dongle node which is a labeled node with value gi .
let the transition probabil - ity from i to its dongle be , and discount other transitions from i by 123 .
we then compute the harmonic function on this augmented graph .
thus , the external classier introduces assignment costs to the energy function , which play the role of vertex potentials in the random eld .
it is not difcult to show that the harmonic solution on the augmented graph is , in the random walk view ,
hu = ( i ( 123 ) pu u ) 123 ( ( 123 ) pu lyl + gu )
we note that up to now we have assumed the labeled data to be noise free , and so clamping their values makes sense .
if there is reason to doubt this assumption , it would be reasonable to attach dongles to labeled nodes as well , and to move the labels to these dongles .
an alternative is to use gaussian process classiers with a noise model , which will be discussed in chapter 123
123 experimental results
we evaluate harmonic functions on the following tasks .
for each task , we gradually increase the labeled set size systematically .
for each labeled set size , we perform 123 random trials .
in each trial we randomly sample a labeled set with the specic size ( except for the freefoodcam task where we sample labeled set from the rst day only ) .
however if a class is missing from the sampled labeled set , we redo the random sampling .
we use the remaining data as the unlabeled set and report the classication accuracy with harmonic functions on them .
to compare the harmonic function solution against a standard supervised learn - ing method , we use a matlab implementation of svm ( gunn , 123 ) as the baseline .
notice the svms are not semi - supervised : the unlabeled data are merely used as test data .
for c - class multiclass problems , we use a one - against - all scheme which creates c binary subproblems , one for each class against the rest classes , and select the class with the largest margin .
we use 123 standard kernels for each task : linear k ( i , j ) = hxi , xji , quadratic k ( i , j ) = ( hxi , xji + 123 ) 123 , and radial basis function
( rbf ) k ( i , j ) = exp ( cid : 123 ) kxi xjk123 / 123 ( cid : 123 ) .
the slack variable upper bound ( usu -
ally denoted by c ) for each kernel , as well as the bandwidth for rbf , are tuned by 123 fold cross validation for each task .
binary classication for ocr handwritten digits 123 vs .
this is a subset of the handwritten digits dataset .
there are 123 images , half are 123s and the other half are 123s .
the graph ( or equivalently the weight matrix w ) is the single most important input to the harmonic algorithm .
to demonstrate its importance , we show the results of not one but six related graphs :
( a ) 123 123 full .
each digit image is 123 123 gray scale with pixel values between 123 and 123
the graph is fully connected , and the weights decrease exponentially with euclidean distance :
wij = exp
the parameter 123 is chosen by evidence maximization ( see section 123 ) .
this was the graph used in ( zhu et al . , 123a ) .
( b ) 123 123 123nn weighted .
same as 123 123 full , but i , j are connected only if i is in js 123 - nearest - neighbor or vice versa .
other edges are re - moved .
the weights on the surviving edges are unchanged .
therefore this is a much sparser graph .
the number 123 is chosen arbitrarily and not tuned for semi - supervised learning .
( c ) 123 123 123nn unweighted .
same as 123 123 123nn weighted except that the weights on the surviving edges are all set to 123
this represents a further simplication of prior knowledge .
( d ) 123 123 full .
all images are down sampled to 123 123 by averaging 123 123 pixel bins .
lowering resolution helps to make euclidean distance less sensitive to small spatial variations .
the graph is fully connected with
wij = exp
( e ) 123 123 123nn weighted .
similar to 123 123 123nn weighted .
( f ) 123 123 123nn unweighted .
ditto .
the classication accuracy with these graphs are shown in figure 123 ( a ) .
different graphs give very different accuracies .
this should be a reminder that the quality of the graph determines the performance of harmonic func - tion ( as well as semi - supervised learning methods based on graphs in gen - eral ) .
123 123 seems to be better than 123 123
sparser graphs are better than fully connected graphs .
the better graphs outperform svm baselines when labeled set size is not too small .
ten digits .
123 - class classication for 123 ocr handwritten digit images .
the class proportions are intentionally chosen to be skewed , with 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , and 123 images for digits 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 respectively .
we use 123 graphs constructed similarly as in 123 vs .
figure 123 ( b ) shows the result , which is similar to 123 vs .
123 except the overall accu - racy is lower .
odd vs .
binary classication for ocr handwritten digits 123 , 123 , 123 , 123 , 123 vs .
123 , 123 , 123 , 123 , 123
each digit has 123 images , i . e .
123 per class and 123 total .
we show only the 123 123 graphs in figure 123 ( c ) , which do not outperform
baseball vs .
hockey binary document classication for rec . sport . baseball vs .
rec . sport . hockey in the 123newsgroups dataset ( 123 version ) .
the pro - cessing of documents into tf . idf vectors has been described in section 123 .
the classes have 123 and 123 documents respectively .
we report the results of three graphs in figure 123 ( d ) :
( a ) full .
a fully connected graph with weights
wij = exp ( cid : 123 )
123 ( cid : 123 ) 123 hdi , dji
so that the weights decreases with the cosine similarity between docu - ment di , dj .
( b ) 123nn weighted .
only symmetrized 123 - nearest - neighbor edges are kept in the graph , with the same weights above .
this was the graph in ( zhu et al . , 123a ) .
( c ) 123nn unweighted .
same as above except all weights are set to 123
mac binary classication on comp . sys . ibm . pc . hardware ( number of documents 123 ) vs .
comp . sys . mac . hardware ( 123 ) in the 123 newsgroups dataset .
the three graphs are constructed in the same way as baseball vs .
hockey .
see figure 123 ( e ) .
religion vs .
atheism binary classication on talk . religion . misc ( 123 ) vs .
alt . atheism ( 123 ) .
see figure 123 ( f ) .
the three 123newsgroups tasks have
isolet this is the isolet dataset from the uci data repository ( blake & merz , 123 ) .
it is a 123 - class classication problem for isolated spoken en - glish letter recognition .
there are 123 instances .
we use the euclidean distance on raw features , and create a 123nn unweighted graph .
the result is in figure 123 ( g ) .
freefoodcam the details of the dataset and graph construction are discussed in section 123 .
the experiments need special treatment compared to other datasets .
since we want to recognize people across multiple days , we only sample the labeled set from the rst days of a persons appearance .
this is harder and more realistic than sampling labeled set from the whole dataset .
we show two graphs in figure 123 ( h ) , one with t123 = 123 seconds , t123 = 123 hours , kc = 123 , kf = 123 , the other the same except kc = 123
the kernel for svm baseline is optimized differently as well .
we use an interpolated linear kernel k ( i , j ) = wtkt ( i , j ) + wckc ( i , j ) + wf kf ( i , j ) , where kt , kc , kf are linear kernels ( inner products ) on time stamp , color histogram , and face sub - image ( normalized to 123 123 pixels ) respectively .
if an image i contains no face , we dene kf ( i , ) = 123
the interpolation weights wt , wc , wf are optimized with cross validation .
the experiments demonstrate that the performance of harmonic function varies considerably depending on the graphs .
with certain graphs , the semi - supervised learning method outperforms svm , a standard supervised learning method .
in par - ticular sparse nearest - neighbor graphs , even unweighted , tend to outperform fully connected graphs .
we believe the reason is that in fully connected graphs the edges between different classes , even with relatively small weights , create unwarrantedly strong connections across the classes .
this highlights the sensitivity to the graph in graph - based semi - supervised learning methods .
it is also apparent from the results that the benet of semi - supervised learn - ing deminishes as the labeled set size grows .
this suggests that semi - supervised learning is most helpful when the cost of getting labels is prohibitive .
cmn : incorporating class proportion knowledge
the harmonic function accuracy can be signicantly improved , if we incorporate class proportion knowledge with the simple cmn heuristic .
the class proportion is estimated from labeled data with laplace ( add one ) smoothing .
all the graphs and
123 , harmonic function
ten digits , harmonic function
123x123 123nn weighted 123x123 123nn unweighted 123x123 123nn unweighted 123x123 123nn weighted
labeled set size
( a ) 123 vs
ten digits , harmonic function
123x123 123nn weighted 123x123 123nn unweighted
labeled set size
( c ) odd vs
123x123 123nn weighted 123x123 123nn unweighted 123x123 123nn unweighted 123x123 123nn weighted
labeled set size
( b ) ten digits
baseball vs .
hockey , harmonic function
labeled set size
( d ) baseball vs .
hockey
figure 123 : harmonic function accuracy
mac , harmonic function
religion vs .
atheism , harmonic function
labeled set size
( e ) pc vs
isolet , harmonic function
labeled set size
( f ) religion vs .
atheism
freefoodcam , harmonic function
labeled set size
labeled set size
figure 123 : harmonic function accuracy ( continued )
other settings are the same as in section 123 .
the cmn results are shown in figure 123 .
compared to figure 123 we see that in most cases cmn helps to improve
for several tasks , cmn gives a huge improvement for the smallest labeled set size .
the improvement is so large that the curves become v shaped at the left hand side .
this is an artifact : we often use the number of classes as the smallest labeled set size .
because of our sampling method , there will be one instance from each class in the labeled set .
the cmn class proportion estimation is thus uniform .
incidentally , many datasets have close to uniform class proportions .
therefore the cmn class proportion estimation is close to the truth for the smallest labeled set size , and produces large improvement .
on the other hand , intermediate labeled set size tends to give the worst class proportion estimates and hence little improve -
in conclusion , it is important to incorporate class proportion knowledge to as - sist semi - supervised learning .
however for clarity , cmn is not used in the remain -
dongles : incorporating external classier
we use the odd vs .
even task , where the rbf svm baseline is sometimes better than the harmonic function with a 123nn unweighted graph .
we augment the graph with a dongle on each unlabeled node .
we use the hard ( 123 / 123 ) labels from the rbf svm ( figure 123 ) on the dongles .
the dongle transition probability is set to 123 by cross validation .
as before , we experiment on different labeled set sizes , and 123 random trials per size .
in figure 123 , we compare the average accuracy of incorporating the external classier ( dongle ) to the external classier ( svm ) or the harmonic function ( harmonic ) alone .
the combination results in higher accuracy than either method alone , suggesting there is complementary information used by
123 , harmonic function + cmn
ten digits , harmonic function + cmn
123x123 123nn weighted 123x123 123nn unweighted 123x123 123nn unweighted 123x123 123nn weighted
labeled set size
( a ) 123 vs
ten digits , harmonic function + cmn
123x123 123nn weighted 123x123 123nn unweighted
labeled set size
( c ) odd vs
123x123 123nn weighted 123x123 123nn unweighted 123x123 123nn unweighted 123x123 123nn weighted
labeled set size
( b ) ten digits
baseball vs .
hockey , harmonic function + cmn
labeled set size
( d ) baseball vs .
hockey
figure 123 : cmn accuracy
mac , harmonic function + cmn
religion vs .
atheism , harmonic function + cmn
labeled set size
( e ) pc vs
isolet , harmonic function + cmn
labeled set size
( f ) religion vs .
atheism
freefoodcam , harmonic function + cmn
labeled set size
labeled set size
figure 123 : cmn accuracy ( continued )
labeled set size
figure 123 : incorporating external classier with dongles
in this chapter , we take a brief detour to look at the active learning problem .
we combine semi - supervised learning and active learning naturally and efciently .
123 combining semi - supervised and active learning
so far , we assumed the labeled data set is given and xed .
in practice , it may make sense to utilize active learning in conjunction with semi - supervised learning .
that is , we might allow the learning algorithm to pick unlabeled instances to be labeled by a domain expert .
the expert returns the label , which will then be used as ( or to augment ) the labeled data set .
in other words , if we have to label a few instances for semi - supervised learning , it may be attractive to let the learning algorithm tell us which instances to label , rather than selecting them randomly .
we will limit the range of query selection to the unlabeled data set , a practice known as pool - based active learning or selective sampling .
there has been a great deal of research in active learning .
for example , tong and koller ( 123 ) select queries to minimize the version space size for support vector machines; cohn et al .
( 123 ) minimize the variance component of the esti - mated generalization error; freund et al .
( 123 ) employ a committee of classiers , and query a point whenever the committee members disagree .
most of the active learning methods do not take further advantage of the large amount of unlabeled data once the queries are selected .
the work by mccallum and nigam ( 123b ) is an exception , where em with unlabeled data is integrated into active learning .
another exception is ( muslea et al . , 123 ) , which uses a semi - supervised learning method during training .
in addition to this body of work from the machine learning community , there is a large literature on the closely related topic of experimental design in statistics; chaloner and verdinelli ( 123 ) give a survey of experimental
design from a bayesian perspective .
the gaussian random elds and harmonic functions framework allows a nat - ural combination of active learning and semi - supervised learning .
in brief , the framework allows one to efciently estimate the expected generalization error af - ter querying a point , which leads to a better query selection criterion than naively selecting the point with maximum label ambiguity .
then , once the queries are se - lected and added to the labeled data set , the classier can be trained using both the labeled and remaining unlabeled data .
minimizing the estimated generalization er - ror was rst proposed by roy and mccallum ( 123 ) .
we independently discovered the same idea ( zhu et al . , 123b ) , and the effective combination of semi - supervised learning and active learning is novel .
we perform active learning with the gaussian random eld model by greedily selecting queries from the unlabeled data to minimize the risk of the harmonic energy minimization function .
the risk is the estimated generalization error of the bayes classier , and can be computed with matrix methods .
we dene the true risk r ( h ) of the bayes classier based on the harmonic function h to be
( sgn ( hi ) 123= yi ) p ( yi )
where sgn ( hi ) is the bayes decision rule with threshold 123 , such that ( with a slight abuse of notation ) sgn ( hi ) = 123 if hi > 123 and sgn ( hi ) = 123 otherwise .
here p ( yi ) is the unknown true label distribution at node i , given the labeled data .
because of this , r ( h ) is not computable .
in order to proceed , it is necessary to make assump - tions .
we begin by assuming that we can estimate the unknown distribution p ( yi ) with the mean of the gaussian eld model :
p ( yi = 123 ) hi
intuitively , recalling hi is the probability of reaching 123 in a random walk on the graph , our assumption is that we can approximate the distribution using a biased coin at each node , whose probability of heads is hi .
with this assumption , we can
( sgn ( hi ) 123= 123 ) ( 123 hi ) + ( sgn ( hi ) 123= 123 ) hi
min ( hi , 123 hi )
compute the estimated risk br ( h ) as
if we perform active learning and query an unlabeled node k , we will receive an answer yk ( 123 or 123 ) .
adding this point to the training set and retraining , the gaussian
eld and its mean function will of course change .
we denote the new harmonic function by h+ ( xk , yk ) .
the estimated risk will also change :
, 123 h+ ( xk , yk )
since we do not know what answer yk we will receive , we again assume the proba - bility of receiving answer p ( yk = 123 ) is approximately hk .
the expected estimated risk after querying node k is therefore
br ( h+xk ) = ( 123 hk ) br ( h+ ( xk , 123 ) ) + hk br ( h+ ( xk , 123 ) )
the active learning criterion we use in this paper is the greedy procedure of choos - ing the next query k that minimizes the expected estimated risk :
k = arg mink123br ( h+xk123 )
to carry out this procedure , we need to compute the harmonic function h+ ( xk , yk ) after adding ( xk , yk ) to the current labeled training set .
this is the retraining prob - lem and is computationally intensive in general .
however for gaussian elds and harmonic functions , there is an efcient way to retrain .
recall that the harmonic function solution is
hu = 123
u u u lyl
what is the solution if we x the value yk for node k ? this is the same as nding the conditional distribution of all unlabeled nodes , given the value of yk .
in gaus - sian elds the conditional on unlabeled data is multivariate normal distributions n ( hu , 123 u u ) .
a standard result ( a derivation is given in appendix a ) gives the mean of the conditional once we x yk :
= hu + ( yk hk )
u u ) k u u ) kk
u u ) k is the k - th column of the inverse laplacian on unlabeled data , u u ) kk is the k - th diagonal element of the same matrix .
both are already computed when we compute the harmonic function h .
this is a linear computation and therefore can be carried out efciently .
to summarize , the active learning algorithm is shown in figure 123 .
the time complexity to nd the best query is o ( n123 ) .
as a nal word on computational efciency , we note that after adding query xk and its answer to l , in the next iteration we will need to compute ( ( u u ) k ) 123 , the inverse of the laplacian on unlabeled data , with the row / column for xk removed .
instead of naively taking the inverse , there are efcient algorithms to compute it from ( u u ) 123; a derivation is given in appendix b .
input : l , u , weight matrix w while more labeled data required :
compute harmonic h using ( 123 ) find best query k using ( 123 ) query point xk , receive answer yk add ( xk , yk ) to l , remove xk from u
output : l and classier h .
figure 123 : the active learning algorithm
figure 123 : entropy minimization selects the most uncertain point a as the next query .
our method will select a point in b , a better choice .
123 why not entropy minimization
we used the estimated generalization error to select queries .
a different query selection criterion , entropy minimization ( or selecting the most uncertain instance ) , has been suggested in some papers .
we next show why it is inappropriate when the loss function is based on individual instances .
such loss functions include the widely used accuracy for classication and mean squared error for regression .
to illustrate the idea , figure 123 shows a synthetic dataset with two labeled data ( marked 123 , 123 ) , an unlabeled point a in the center above and a cluster of 123 unlabeled points b below .
b is slighted shifted to the right .
the graph is fully connected with weights wij = exp ( d123 ij ) , where dij is the euclidean distance be - tween i , j .
in this conguration , we have the most uncertainty in a : the harmonic function at node a is h ( a ) = 123 .
points in b have their harmonic func -
tion values around 123 .
therefore entropy minimization will pick a as the query .
however , the risk minimization criterion picks the upper center point ( marked with
a star ) in b to query , instead of a .
in fact the estimated risk is br ( a ) = 123 , and br ( b b ) 123 .
intuitively knowing the label of one point in b let us know the
label of all points in b , which is a larger gain .
entropy minimization is worse than risk minimization in this example .
the root of the problem is that entropy does not account for the loss of mak - ing a large number of correlated mistakes .
in a pool - based incremental active learning setting , given the current unlabeled set u , entropy minimization nds the query q u such that the conditional entropy h ( u \ q|q ) is minimized .
as h ( u \ q|q ) = h ( u ) h ( q ) , it amounts to selecting q with the largest entropy , or the most ambiguous unlabeled point as the query .
consider another example where u = ( a , b123 , .
, b123 ) .
let p ( a = + ) = p ( a = ) = 123 and p ( bi = + ) = 123 , p ( bi = ) = 123 for i = 123 .
furthermore let b123 .
b123 be perfectly correlated so they always take the same value; let a and bis be inde - pendent .
entropy minimization will select a as the next query since h ( a ) = 123 > h ( bi ) = 123 .
if our goal were to reduce uncertainty about u , such query selec - tion is good : h ( b123 .
b123|a ) = 123 < h ( a , b123 , .
, bi123 , bi+123 , .
, b123|bi ) = h ( a|bi ) = 123
however if our loss function is the accuracy on the remaining instances in u , the picture is quite different .
after querying a , p ( bi = + ) re - mains at 123 , so that each bi incurs a bayes error of 123 by always predict bi = + .
the problem is that the individual error adds up , and the overall accuracy is 123 123 / 123 = 123 .
on the other hand if we query b123 , we know the labels of b123 .
b123 too because of their perfect correlation .
the only error we might make is on a with bayes error of 123 .
the overall accuracy is ( 123 + 123 123 ) / 123 = 123 .
the situation is analogous to speech recognition in which one can measure the word level accuracy or sentence level accuracy where a sentence is correct if all words in it are correct .
the sentence corresponds to the whole u in our example .
entropy minimization is more aligned with sentence level accuracy .
nevertheless since most active learning systems use instance level loss function , it can leads to suboptimal query choices as we show above .
figure 123 shows a check - board synthetic dataset with 123 points .
we expect active learning to discover the pattern and query a small number of representatives from each cluster .
on the other hand , we expect a much larger number of queries if queries are randomly selected .
we use a fully connected graph with weight wij = ij / 123 ) .
we perform 123 random trials .
at the beginning of each trial we
most uncertain query
labeled set size
most uncertain query
labeled set size
figure 123 : a check - board example .
left : dataset and true labels; center : esti - mated risk; right : classication accuracy .
randomly select a positive example and a negative example as the initial training set .
we then run active learning and compare it to two baselines : ( 123 ) random query : randomly selecting the next query from u; ( 123 ) most uncertain query : selecting the most uncertain instance in u , i . e .
the one with h closest to 123 .
in each case , we run for 123 iterations ( queries ) .
at each iteration , we plot the estimated risk ( 123 ) of the selected query ( center ) , and the classication accuracy on u ( right ) .
the error bars are 123 standard deviation , averaged over the random trials .
as expected , with risk minimization active learning we reduce the risk more quickly than random queries or the most uncertain queries .
in fact , risk minimization active learning with about 123 queries ( plus 123 initial random points ) learns the correct concept , which is nearly optimal given that there are 123 clusters .
looking at the queries , we nd that active learning mostly selects the central points within the
next , we ran the risk minimization active learning method on several tasks ( marked active learning in the plots ) .
we compare it with several alternative ways of picking queries :
random query .
randomly select the next query from the unlabeled set .
classication on the unlabeled set is based on the harmonic function .
there - fore , this method consists of no active learning , but only semi - supervised
most uncertain .
pick the most ambiguous point ( h closest to 123 for binary
problems ) as the query .
classication is based on the harmonic function .
svm random query .
randomly select the next query from the unlabeled set .
classication with svm .
this is neither active nor semi - supervised
svm most uncertain .
pick the query closest to the svm decision boundary .
one vs .
two , active learning
svm most uncertain svm random query
labeled set size
( b ) ten digits
svm most uncertain svm random query
labeled set size
svm most uncertain svm random query
labeled set size
( a ) 123 vs
labeled set size
svm most uncertain svm random query
( c ) odd vs
( d ) baseball vs .
hockey
figure 123 : active learning accuracy
classication with svm .
for each task , we use the best graph for harmonic functions , and the best kernel for svm , as in section 123 .
we run 123 trials and the plots are the average .
each trial , we start from a randomly selected labeled set , so that each class has exactly one labeled example .
the query selection methods mentioned above are used independently to grow the labeled set until a predetermined size .
we plot the classication accuracy on the remaining unlabeled data in figure 123 .
for the freefoodcam task , there are two experiments : 123
we allow the queries to come from all days; 123
from only the rst days of a persons rst appearance .
it is interesting to see what queries are selected by different methods .
figures 123 and 123 compare the rst few queries for the 123 vs .
123 and ten digits tasks .
in each case , the initial labeled set is the same .
the combined semi - supervised learning and risk minimization active learning method performs well on the tasks .
compared to the results reported in ( roy &
svm most uncertain svm random query
svm most uncertain svm random query
labeled set size
labeled set size
( e ) pc vs
active learning , queries from all u
( f ) religion vs .
atheism
active learning , queries from first days only
labeled set size
svm most uncertain svm random query
labeled set size
( g ) freefoodcam , query from all days
( h ) freefoodcam , query from the rst days
figure 123 : active learning accuracy ( continued )
initial labeled set svm most uncertain
figure 123 : the rst few queries selected by different active learning methods on the 123 vs .
123 task .
all methods start with the same initial labeled set .
initial labeled set svm most uncertain
figure 123 : the rst few queries selected by different active learning methods on the ten digits task .
all methods start with the same initial labeled set .
mccallum , 123 ) , we think that good semi - supervised learning algorithm is a key to the success of the active learning scheme .
connection to gaussian processes
a gaussian process dene a prior p ( f ( x ) ) over function values f ( x ) , where x ranges over an innite input space .
it is an extension to an n - dimensional gaus - sian distribution as n goes to innity .
a gaussian process is dened by its mean function ( x ) ( usually taken to be zero everywhere ) , and a covariance function c ( x , x123 ) .
for any nite set of points x123 , .
, xm , the gaussian process on the set reduces to an m - dimensional gaussian distribution with a covariance matrix cij = c ( xi , xj ) , for i , j = 123 .
more information can be found in chapter 123 of ( mackay , 123 ) .
gaussian random elds are equivalent to gaussian processes that are restricted to a nite set of points .
thus , the standard machineries for gaussian processes can be used for semi - supervised learning .
through this connection , we establish the link between the graph laplacian and kernel methods in general .
123 a finite set gaussian process model
recall for any real - valued function f on the graph , the energy is dened as
e ( f ) =
wij ( f ( i ) f ( j ) ) 123 = f >f
the corresponding gaussian random eld is
p ( f ) =
ee ( f ) =
the gaussian random eld is nothing but a multivariate gaussian distribution on the nodes .
meanwhile a gaussian process restricted to nite data is a multivariate gaussian distribution too ( mackay , 123 ) .
this indicates a connection between
gaussian random elds and nite set gaussian processes .
notice the nite set gaussian processes are not real gaussian processes , since the kernel matrix is only dened on l u , not the whole input space x .
equation ( 123 ) can be viewed as a gaussian process restricted to l u with covariance matrix ( 123 ) 123
however the covariance matrix is an improper prior .
the laplacian by denition has a zero eigenvalue with constant eigenvector 123
to see this note that the degree matrix d is the row sum of w .
this makes singular : we cannot invert to get the covariance matrix .
to make a proper prior out of the laplacian , we can smooth its spectrum to remove the zero eigenvalues , as suggested in ( smola & kondor , 123 ) .
in particular , we choose to transform the eigenvalues according to the function r ( ) = + 123 / 123 where 123 / 123 is a small smoothing parameter .
this gives the regularized laplacian
using the regularized laplacian , we dene a zero mean prior as
p ( f ) exp ( cid : 123 )
f > f ( cid : 123 )
which corresponds to a kernel with gram matrix ( i . e .
covariance matrix )
we note several important aspects of the resulting nite set gaussian process :
k = 123 = ( cid : 123 ) 123 ( + i / 123 ) ( cid : 123 ) 123
f n ( cid : 123 ) 123 , 123 ( cid : 123 ) ; unlike , gives a proper covariance matrix .
the parameter controls the overall sharpness of the distribution; large
means p ( f ) is more peaked around its mean .
the parameter 123 controls the amount of spectral smoothing; large smoothes
the kernel ( covariance ) matrix k = 123 is the inverse of a function of the laplacian .
therefore the covariance between any two point i , j in general depends on all the points .
this is how unlabeled data inuences the prior .
the last point warrants further explanation .
in many standard kernels , the entries are local .
for example , in a radial basis function ( rbf ) kernel k , the matrix entry
kij = exp ( cid : 123 ) d123
ij / 123 ( cid : 123 ) only depends on the distance between i , j and not any other
points .
in this case unlabeled data is useless because the inuence of unlabeled data in k is marginalized out .
in contrast , the entries in kernel ( 123 ) depends on all entries in , which in turn depends on all edge weights w .
thus , unlabeled data will inuence the kernel , which is desirable for semi - supervised learning .
another way to view the difference is that in rbf ( and many other ) kernels we parameterize the covariance matrix directly , while with graph laplacians we parameterize the inverse covariance matrix .
123 incorporating a noise model
in moving from gaussian elds to nite set gaussian processes , we no longer assume that the soft labels fl for the labeled data are xed at the observed labels yl .
instead we now assume the data generation process is x f y , where f y is a noisy label generation process .
we use a sigmoid noise model between the hidden soft labels fi and observed labels yi :
p ( yi|fi ) =
efiyi + efiyi
123 + e123fiyi
where is a hyperparameter which controls the steepness of the sigmoid .
this assumption allows us to handle noise in training labels , and is a common practice in gaussian process classication .
we are interested in p ( yu|yl ) , the labels for unlabeled data .
we rst need to
compute the posterior distribution p ( fl , fu|yl ) .
by bayes theorem ,
p ( fl , fu|yl ) = ql
i=123 p ( yi|fi ) p ( fl , fu )
because of the noise model , the posterior is not gaussian and has no closed form solution .
there are several ways to approximate the posterior .
for simplicity we use the laplace approximation to nd the approximate p ( fl , fu|yl ) .
a deriva - tion can be found in appendix c , which largely follows ( herbrich , 123 ) ( b . 123 ) .
bayesian classication is based on the posterior distribution p ( yu|yl ) .
since un - der the laplace approximation this distribution is also gaussian , the classication rule depends only on the sign of the mean ( which is also the mode ) of fu .
we compare the accuracy of gaussian process classication with the 123 - threshold harmonic function ( without cmn ) .
to simplify the plots , we use the same graphs
123 , gaussian field
gaussian field , 123x123 123nn weighted harmonic , 123x123 123nn weighted
labeled set size
( a ) 123 vs
ten digits , gaussian field
gaussian field , 123x123 123nn weighted harmonic , 123x123 123nn weighted
labeled set size
( c ) odd vs
ten digits , gaussian field
gaussian field , 123x123 123nn weighted harmonic , 123x123 123nn weighted
labeled set size
( b ) ten digits
baseball vs .
hockey , gaussian field
gaussian field , 123nn weighted harmonic , 123nn weighted
labeled set size
( d ) baseball vs .
hockey
figure 123 : gaussian process accuracy
that give the best harmonic function accuracy ( except freefoodcam ) .
to aid com - parison we also show svms with the best kernel among linear , quadratic or rbf .
in the experiments , the inverse temperature parameter , smoothing parameter and noise model parameter are tuned with cross validation for each task .
the results are in figure 123 .
for freefoodcam we also use two other graphs with no face edges at all ( kf = 123 ) .
the rst one limits color edges to within 123 hours ( t123 = 123 hour ) , thus the rst days that contain the labeled data is disconnected from the rest .
the second one allows color edges on far away images ( t123 = ) .
neither has good accuracy , indicating that face is an important feature to use .
mac , gaussian field
religion vs .
atheism , gaussian field
gaussian field , 123nn weighted harmonic , 123nn weighted
labeled set size
( e ) pc vs
isolet , gaussian process
gaussian process , 123nn unweighted harmonic , 123nn unweighted
gaussian field , 123nn weighted harmonic , 123nn weighted
labeled set size
( f ) religion vs .
atheism
freefoodcam , gaussian field
gaussian field , t123=123sec , t123=123hr , kc=123 , kf=123 gaussian field , t123=123sec , t123=123hr , kc=123 , kf=123 gaussian field , t123=123sec , t123=inf , kc=123 , kf=123
labeled set size
labeled set size
figure 123 : gaussian process accuracy ( continued )
123 extending to unseen data
we have so far restricted ourselves to the l u nodes in the graph .
in this nite case gaussian processes are nothing but n - dimensional multivariate normal distri - butions , and are equivalent to gaussian random elds .
however gaussian elds , by denition , cannot handle unseen instances .
any new data points need to be - come additional nodes in the graph .
the laplacian and kernel matrices need to be re - computed , which is expensive .
we would like to extend the framework to allow arbitrary new points .
equivalently , this is the problem of induction instead
the simplest strategy is to divide the input space into voronoi cells .
the voronoi cells are centered on instances in l u .
we classify any new instance x by the voronoi cell it falls into .
let x l u be the point closest to x :
x = arg maxzlu wxz
where closeness is measured by weights wxz .
from an algorithmic point of view , we classify x by its 123 - nearest - neighbor x .
when the unlabeled data size is large , the approximation is reasonable .
we will discuss more inductive methods in chapter 123
graph hyperparameter learning
previously we assumed that the weight matrix w is given and xed .
in this chapter we investigate learning the weights from both labeled and unlabeled data .
we present three methods .
the rst one is evidence maximization in the context of gaussian processes .
the second is entropy minimization , and the third one is based on minimum spanning trees .
the latter ones are heuristic but also practical .
123 evidence maximization
we assume the edge weights are parameterized with hyperparameters .
for in - stance the edge weights can be
wij = exp
and = ( 123 , .
to learn the weight hyperparameters in a gaussian pro - cess , one can choose the hyperparameters that maximize the log likelihood : = arg max log p ( yl| ) .
log p ( yl| ) is known as the evidence and the procedure is also called evidence maximization .
one can also assume a prior on and nd the maximum a posteriori ( map ) estimate = arg max log p ( yl| ) + log p ( ) .
the evidence can be multimodal and usually gradient methods are used to nd a mode in hyperparameter space .
this requires the derivatives log p ( yl| ) / .
a complete derivation is given in appendix d .
in a full bayesian setup , one would average over all hyperparameter values ( weighted by the posterior p ( |yl ) ) instead of using a point estimate .
this usually involves markov chain monte carlo techniques , and is not pursued in this
123 123 vs
table 123 : the regularized evidence and classication before and after learning s for the two digits recognition tasks
we use binary ocr handwritten digits recognition tasks as our example , since the results are more interpretable .
we choose two tasks : 123 vs .
123 which has been presented previously , and 123 vs .
123 which are the two most confusing digits in terms of euclidean distance .
we use fully connected graphs with weights
wij = exp
the hyperparameters are the 123 length scales d for each pixel dimension on 123 123 images .
intuitively they determine which pixel positions are salient for the classi - cation task : if d is close to zero , a difference at pixel position d will be magnied; if it is large , pixel position d will be essentially ignored .
the weight function is an extension to eq ( 123 ) by giving each dimension its own length scale .
for each task there are 123 images , and we run 123 trials , in each trial we randomly pick 123 images as the labeled set .
the rest is used as unlabeled set .
for each trial we start at i = 123 , i = 123 .
123 , which is the same as in eq ( 123 ) .
we compute the gradients for i for evidence maximization .
however since there are 123 hyperparameters and only 123 labeled points , regularization is important .
we use a normal prior on the hyperparameters which is centered at the initial value : p ( i ) n ( 123 , 123 ) , i = 123 .
we use a line search algorithm to nd a ( pos - sibly local ) optimum for the s .
table 123 shows the regularized evidence and classication before and after learning s for the two tasks .
figure 123 compares the learned hyperparameters with the mean images of the tasks .
smaller ( darker ) s correspond to feature dimensions in which the learning algorithm pays more attention .
it is obvious , for instance in the 123 vs .
123 task , that the learned hyperparameters focus on the gap on the neck of the image , which is the distinguishing feature between 123s and 123s .
figure 123 : graph hyperparameter learning .
the upper row is for the 123 vs .
123 task , and the lower row for 123 vs .
the four images are : ( a , b ) averaged digit images for the two classes; ( c ) the 123 initial length scale hyperparameters , shown as an 123 123 array; ( d ) learned hyperparameters .
123 entropy minimization
alternatively , we can use average label entropy as a heuristic criterion for parame - ter learning 123
this heuristic uses only the harmonic function and does not depend on the gaussian process setup .
the average label entropy h ( h ) of the harmonic function h is dened as
where hi ( h ( i ) ) = h ( i ) log h ( i ) ( 123h ( i ) ) log ( 123h ( i ) ) is the shannon entropy of individual unlabeled data point i .
here we use the random walk interpretation of h , relying on the maximum principle of harmonic functions which guarantees that 123 h ( i ) 123 for i u .
small entropy implies that h ( i ) is close to 123 or 123; this captures the intuition that a good w ( equivalently , a good set of hyperparameters ) should result in a condent labeling .
there are of course many arbitrary label - ings of the data that have low entropy , which might suggest that this criterion will not work .
however , it is important to point out that we are constraining h on the labeled datamost of these arbitrary low entropy labelings are inconsistent with this constraint .
in fact , we nd that the space of low entropy labelings achievable by harmonic function is small and lends itself well to tuning the hyperparameters .
123we could have used the estimated risk , cf .
chapter 123
the gradient will be more difcult because
of the min function .
as an example , let us consider the case where weights are parameterized as ( 123 ) .
we can apply entropy minimization but there is a complication , namely h has a minimum at 123 as d 123
as the length scale approaches zero , the tail of the weight function ( 123 ) is increasingly sensitive to the distance .
in the end , the label predicted for an unlabeled example is dominated by its nearest neighbors label , which results in the following equivalent labeling procedure : ( 123 ) starting from the labeled data set , nd the unlabeled point xu that is closest to some labeled point xl; ( 123 ) label xu with xls label , put xu in the labeled set and repeat .
since these are hard labels , the entropy is zero .
this solution is desirable only when the classes are well separated , and is inferior otherwise .
this complication can be avoided by smoothing the transition matrix .
inspired by analysis of the pagerank algorithm in ( ng et al . , 123b ) , we smooth the transition matrix p with the uniform matrix u : uij = 123 / n .
the smoothed transition matrix is p = u + ( 123 ) p .
gradient is computed as
we use gradient descent to nd the hyperparameters d that minimize h
log ( cid : 123 ) 123 h ( i )
h ( i ) ( cid : 123 ) h ( i )
where the values h ( i ) / d can be read off the vector hu / d , which is given
= ( i pu u ) 123 pu u
using the fact that dx 123 = x 123 ( dx ) x 123
both pu u / d and pu l / d are sub - matrices of p / d = ( 123 ) p .
since the original transition matrix p is obtained by normalizing the weight matrix w , we have that
= 123wij ( xdi xdj ) 123 / 123
in the above derivation we use hu as label probabilities directly; that is , p ( yi = 123 ) = hu ( i ) .
if we incorporate class proportion information , or combine the har - monic function with other classiers , it makes sense to minimize entropy on the combined probabilities .
for instance , if we incorporate class proportions using cmn , the probability is given by
q ( u p hu ) hu ( i )
q ( u p hu ) hu ( i ) + ( 123 q ) p hu ( 123 hu ( j ) )
figure 123 : the effect of parameter on the harmonic function .
( a ) if not smoothed , h 123 as 123 , and the algorithm performs poorly .
( b ) result at optimal = 123 , smoothed with = 123 ( c ) smoothing helps to remove the
and we use this probability in place of h ( i ) in ( 123 ) .
the derivation of the gradient descent rule is a straightforward extension of the above analysis .
we use a toy dataset in figure 123 as an example for entropy minimization .
the upper grid is slightly tighter than the lower grid , and they are connected by a few data points .
there are two labeled examples , marked with large symbols .
we learn the optimal length scales for this dataset by minimizing entropy on unlabeled
to simplify the problem , we rst tie the length scales in the two dimensions , so there is only a single parameter to learn .
as noted earlier , without smoothing , the entropy approaches the minimum at 123 as 123
under such conditions , the harmonic function is usually undesirable , and for this dataset the tighter grid invades the sparser one as shown in figure 123 ( a ) .
with smoothing , the nuisance minimum at 123 gradually disappears as the smoothing factor grows , as shown in figure 123 ( c ) .
when we set = 123 , the minimum entropy is 123 bits at = 123 .
the harmonic function under this length scale is shown in figure 123 ( b ) , which is able to distinguish the structure of the two grids .
if we allow separate s for each dimension , parameter learning is more dra - matic .
with the same smoothing of = 123 , x keeps growing toward innity ( we use x = 123 for computation ) while y stabilizes at 123 , and we reach a minimum entropy of 123 bits .
in this case x is legitimate; it means that the learning algorithm has identied the x - direction as irrelevant , based on both the labeled and unlabeled data .
the harmonic function under these hyperparameters gives the same classication as shown in figure 123 ( b ) .
123 minimum spanning tree
if the graph edges are exp - weighted with a single hyperparameter ( section 123 ) , we can set the hyperparameter with the following heuristic .
we construct a minimum spanning tree over all data points with kruskals algorithm ( kruskal , 123 ) .
in the beginning no node is connected .
during tree growth , the edges are examined one by one from short to long .
an edge is added to the tree if it connects two separate components .
the process repeats until the whole graph is connected .
we nd the rst tree edge that connects two components with different labeled points in them .
we regard the length of this edge d123 as a heuristic to the minimum distance between different class regions .
we then set = d123 / 123 following the 123 rule of normal distribution , so that the weight of this edge is close to 123 , with the hope that local propagation is then mostly within classes .
other ways to learn the weight hyperparameters are possible .
for example one can try to maximize the kernel alignment to labeled data .
this criterion will be used to learn a spectral transformation from the laplacian to a graph kernel in chapter 123
there the graph weights are xed , and the hyperparameters are the eigenvalues of the graph kernel .
it is possible that one can instead x a spectral transformation but learn the weight hyperparameters , or better yet jointly learn both .
the hope is the problem can be formulated as convex optimization .
this remains future research .
kernels from the spectrum of
we used the inverse of a smoothed laplacian as kernel matrix in chapter 123
in fact , one can construct a whole family of graph kernels from the spectral decom - position of graph laplacians .
these kernels combine labeled and unlabeled data in a systematic fashion .
in this chapter we devise the best one ( in a certain sense ) for
123 the spectrum of laplacians
let us denote the laplacian s eigen - decomposition by ( i , i ) , so that = i .
we assume the eigenvalues are sorted in non - decreasing order .
the laplacian has many interesting properties ( chung , 123 ) ; for example has exactly k zero eigenvalues 123 = = k = 123 , where k is the number of con - nected subgraphs .
the corresponding eigenvectors 123 , .
, k are constant over the individual subgraphs and zero elsewhere .
perhaps the most important property of the laplacian related to semi - supervised learning is the following : a smaller eigenvalue corresponds to a smoother eigenvector over the graph; that is , the
valuepij wij ( ( i ) ( j ) ) 123 is small .
informally , a smooth eigenvector has the
property that two elements of the vector have similar values if there are many large weight paths between the nodes in the graph .
in a physical system , the smoother eigenvectors correspond to the major vibration modes .
figure 123 ( top ) shows a simple graph consisting of two linear segments .
the edges have the same weight 123
its laplacian spectral decomposition is shown below , where the eigenvalues are sorted from small to large .
the rst two eigenvalues should be zero there are numerical errors in matlab eigen computation .
as the eigenvalues increase , the
figure 123 : a simple graph with two segments , and its laplacian spectral decom - position .
the numbers are the eigenvalues , and the zigzag shapes are the corre -
corresponding eigenvectors become less and less smooth .
123 from laplacians to kernels
kernel - based methods are increasingly being used for data modeling and predic - tion because of their conceptual simplicity and good performance on many tasks .
a promising family of semi - supervised learning methods can be viewed as con - structing kernels by transforming the spectrum ( i . e .
eigen - decomposition ) of the graph laplacian .
these kernels , when viewed as regularizers , penalize functions that are not smooth over the graph ( smola & kondor , 123 ) .
assuming the graph structure is correct , from a regularization perspective we
want to encourage smooth functions , to reect our belief that labels should vary slowly over the graph .
specically , chapelle et al .
( 123 ) and smola and kondor ( 123 ) suggest a general principle for creating a family of semi - supervised kernels k from the graph laplacian : transform the eigenvalues into r ( ) , where the spectral transformation r is a non - negative and usually decreasing function123
note it may be that r reverses the order of the eigenvalues , so that smooth is have
larger eigenvalues in k .
with such a kernel , a soft labeling function f =p cii in a kernel machine has a penalty term in the rkhs norm given by ( ||f||123 f corresponding to eigenfunctions that are less smooth .
i / r ( i ) ) .
if r is decreasing , a greater penalty is incurred for those terms of
in previous work r has often been chosen from a parametric family .
for exam -
ple , the diffusion kernel ( kondor & lafferty , 123 ) corresponds to
r ( ) = exp (
the regularized gaussian process kernel in chapter 123 corresponds to
figure 123 shows such a regularized gaussian process kernel , constructed from the laplacian in figure 123 with = 123 .
cross validation has been used to nd the hyperparameter for these spectral transformations .
although the general principle of equation ( 123 ) is appealing , it does not address the question of which parametric family to use for r .
moreover , the degree of freedom ( or the number of hyperparameters ) may not suit the task , resulting in overly constrained kernels .
we address these limitations with a nonparametric method .
instead of using a parametric transformation r ( ) , we allow the transformed eigenvalues i = r ( i ) , i = 123 .
n to be almost independent .
the only additional condition is that is have to be non - increasing , to encourage smooth functions over the graph .
un - der this condition , we nd the set of optimal spectral transformation that maxi - mizes the kernel alignment to the labeled data .
the main advantage of using kernel alignment is that it gives us a convex optimization problem , and does not suf - fer from poor convergence to local minima .
the optimization problem in general is solved using semi - denite programming ( sdp ) ( boyd & vandenberge , 123 ) ;
123we use a slightly different notation where r is the inverse of that in ( smola & kondor , 123 ) .
figure 123 : the kernel constructed from the laplacian in figure 123 , with spectrum transformation r ( ) = 123 / ( + 123 ) .
however , in our approach the problem can be formulated in terms of quadratically constrained quadratic programming ( qcqp ) , which can be solved more efciently than a general sdp .
we review qcqp next .
123 convex optimization using qcqp
let ki = i> vectors .
our kernel k is a linear combination
i , i = 123 n be the outer product matrices of the laplacians eigen -
where i 123
we formulate the problem of nding the optimal spectral transfor - mation as one that nds the interpolation coefcients ( r ( i ) = i ) by optimizing some convex objective function on k .
to maintain the positive semi - deniteness constraint on k , one in general needs to invoke sdps ( boyd & vandenberge , 123 ) .
semi - denite optimization can be described as the problem of optimizing a linear function of a symmetric matrix subject to linear equality constraints and the condition that the matrix be positive semi - denite .
the well known linear pro - gramming problem can be generalized to a semi - denite optimization by replacing the vector of variables with a symmetric matrix , and replacing the non - negativity constraints with a positive semi - denite constraints .
this generalization inherits several properties : it is convex , has a rich duality theory and allows theoretically efcient solution algorithms based on iterating interior point methods to either fol - low a central path or decrease a potential function .
however , a limitation of sdps is their computational complexity ( boyd & vandenberge , 123 ) , which has restricted their application to small - scale problems ( lanckriet et al . , 123 ) .
however , an important special case of sdps are quadratically constrained quadratic programs
( qcqp ) which are computationally more efcient .
here both the objective func - tion and the constraints are quadratic as illustrated below ,
x>p123x + q>
123 x + r123
x>pix + q>
i x + ri 123
i = 123 m
ax = b
+ , i = 123 , .
, m , where s n
where pi s n + denes the set of square symmetric positive semi - denite matrices .
in a qcqp , we minimize a convex quadratic func - tion over a feasible region that is the intersection of ellipsoids .
the number of iterations required to reach the solution is comparable to the number required for linear programs , making the approach feasible for large datasets .
however , as ob - served in ( boyd & vandenberge , 123 ) , not all sdps can be relaxed to qcqps .
for the semi - supervised kernel learning task presented here solving an sdp would be computationally infeasible .
recent work ( cristianini et al . , 123a; lanckriet et al . , 123 ) has proposed ker - nel target alignment that can be used not only to assess the relationship between the feature spaces generated by two different kernels , but also to assess the similar - ity between spaces induced by a kernel and that induced by the labels themselves .
desirable properties of the alignment measure can be found in ( cristianini et al . , 123a ) .
the crucial aspect of alignment for our purposes is that its optimization can be formulated as a qcqp .
the objective function is the empirical kernel alignment
a ( ktr , t ) =
phktr , ktrifht , tif
where ktr is the kernel matrix restricted to the training points , hm , nif denotes the frobenius product between two square matrices hm , nif = pij mijnij = trace ( m n > ) , and t is the target matrix on training data , with entry tij set to +123 if yi = yj and 123 otherwise .
note for binary ( +123 , 123 ) training labels yl this l .
k is guaranteed to be positive semi - is simply the rank one matrix t = yly > denite by constraining i 123
our kernel alignment problem is special in that the kis were derived from the graph laplacian with the goal of semi - supervised learning .
we require smoother eigenvectors to receive larger coefcients , as shown in the next section .
123 semi - supervised kernels with order constraints
as stated above , we would like to maintain a decreasing order on the spectral transformation i = r ( i ) to encourage smooth functions over the graph
motivates the set of order constraints
i = 123 n 123
we can specify the desired semi - supervised kernel as follows .
denition 123 anorder constrained semi - supervised kernel k isthesolutiontothe
a ( ktr , t )
trace ( k ) = 123
i = 123 n 123
where t isthetrainingtargetmatrix , ki = i>
i and isaretheeigenvectorsof
the formulation is an extension to ( lanckriet et al . , 123 ) with order constraints , and with special components kis from the graph laplacian .
since i 123 and kis are outer products , k will automatically be positive semi - denite and hence a valid kernel matrix .
the trace constraint is needed to x the scale invariance of kernel alignment .
it is important to notice the order constraints are convex , and as such the whole problem is convex .
this problem is equivalent to :
subject to hktr , ktrif 123
i i+123 , i
let vec ( a ) be the column vectorization of a matrix a .
dening a l123 m matrix
m = ( cid : 123 ) vec ( k123 , tr ) vec ( km , tr ) ( cid : 123 )
it is not hard to show that the problem can then be expressed as
||m || 123
i = 123 n 123
the objective function is linear in , and there is a simple cone constraint , making it a quadratically constrained quadratic program ( qcqp ) 123
an improvement of the above order constrained semi - supervised kernel can be obtained by taking a closer look at the laplacian eigenvectors with zero eigenval - ues .
as stated earlier , for a graph laplacian there will be k zero eigenvalues if the graph has k connected subgraphs .
the k eigenvectors are piecewise constant over individual subgraphs , and zero elsewhere .
this is desirable when k > 123 , with the hope that subgraphs correspond to different classes .
however if k = 123 , the graph is connected .
the rst eigenvector 123 is a constant vector over all nodes .
the corre - sponding k123 is a constant matrix , and acts as a bias term in ( 123 ) .
in this situation we do not want to impose the order constraint 123 123 on the constant bias term , rather we let 123 vary freely during optimization :
denition 123 an improved order constrained semi - supervised kernel k istheso - lution to the same problem in denition 123 , but the order constraints ( 123 ) apply
i = 123 n 123 , and i notconstant
in practice we do not need all n eigenvectors of the graph laplacian , or equiva - lently all n kis .
the rst m < n eigenvectors with the smallest eigenvalues work well empirically .
also note we could have used the fact that kis are from orthog - onal eigenvectors i to further simplify the expression .
however we neglect this observation , making it easier to incorporate other kernel components if necessary .
it is illustrative to compare and contrast the order constrained semi - supervised kernels to other semi - supervised kernels with different spectral transformation .
we call the original kernel alignment solution in ( lanckriet et al . , 123 ) a maximal - alignment kernel .
it is the solution to denition 123 without the order constraints ( 123 ) .
because it does not have the additional constraints , it maximizes kernel alignment among all spectral transformation .
the hyperparameters of the diffu - sion kernel and gaussian elds kernel ( described earlier ) can be learned by max - imizing the alignment score too , although the optimization problem is not neces - sarily convex .
these kernels use different information in the original laplacian eigenvalues i .
the maximal - alignment kernels ignore i altogether .
the order constrained semi - supervised kernels only use the order of i and ignore their ac - tual values .
the diffusion and gaussian eld kernels use the actual values .
terms of the degree of freedom in choosing the spectral transformation is , the maximal - alignment kernels are completely free .
the diffusion and gaussian eld
123an alternative formulation results in a quadratic program ( qp ) , which is faster than qcqp .
details can be found at http : / / www . cs . cmu . edu / zhuxj / pub / qp . pdf
kernels are restrictive since they have an implicit parametric form and only one free parameter .
the order constrained semi - supervised kernels incorporates desirable features from both approaches .
we evaluate the order constrained kernels on seven datasets .
baseball - hockey ( 123 instances / 123 classes ) , pc - mac ( 123 / 123 ) and religion - atheism ( 123 / 123 ) are document categorization tasks taken from the 123 - newsgroups dataset .
the distance measure is the standard cosine similarity between tf . idf vectors .
one - two ( 123 / 123 ) , odd - even ( 123 / 123 ) and ten digits ( 123 / 123 ) are handwritten digits recognition tasks .
one - two is digits 123 vs .
123; odd - even is the articial task of classify - ing odd 123 , 123 , 123 , 123 , 123 vs .
even 123 , 123 , 123 , 123 , 123 digits , such that each class has several well dened internal clusters; ten digits is 123 - way classication .
isolet ( 123 / 123 ) is isolated spoken english alphabet recognition from the uci repository .
for these datasets we use euclidean distance on raw features .
we use 123nn unweighted graphs on all datasets except isolet which is 123nn .
for all datasets , we use the smallest m = 123 eigenvalue and eigenvector pairs from the graph laplacian .
these values are set arbitrarily without optimizing and do not create a unfair ad - vantage to the proposed kernels .
for each dataset we test on ve different labeled set sizes .
for a given labeled set size , we perform 123 random trials in which a la - beled set is randomly sampled from the whole dataset .
all classes must be present in the labeled set .
the rest is used as unlabeled ( test ) set in that trial .
we compare 123 semi - supervised kernels ( improved order constrained kernel , order constrained kernel , gaussian eld kernel , diffusion kernel123 and maximal - alignment kernel ) , and 123 standard supervised kernels ( rbf ( bandwidth learned using 123 - fold cross val - idation ) , linear and quadratic ) .
we compute the spectral transformation for order constrained kernels and maximal - alignment kernels by solving the qcqp using standard solvers ( sedumi / yalmip ) .
to compute accuracy we use these kernels in a standard svm .
we choose the bound on slack variables c with cross validation for all tasks and kernels .
for multiclass classication we perform one - against - all and pick the class with the largest margin .
table 123 through table 123 list the results .
there are two rows for each cell : the upper row is the average test set accuracy with one standard deviation; the lower row is the average training set kernel alignment , and in parenthesis the av - erage run time in seconds for qcqp on a 123ghz linux computer .
each number is averaged over 123 random trials .
to assess the statistical signicance of the re -
123the hyperparameters are learned with the fminbnd ( ) function in matlab to maximize kernel
123 ( 123 ) 123 ( 123 )
123 ( 123 ) 123 ( 123 )
123 ( 123 ) 123 ( 123 )
table 123 : baseball vs .
hockey
123 ( 123 ) 123 ( 123 )
123 ( 123 ) 123 ( 123 )
123 ( 123 ) 123 ( 123 )
table 123 : pc vs
123 ( 123 ) 123 ( 123 )
123 ( 123 ) 123 ( 123 )
123 ( 123 ) 123 ( 123 )
table 123 : religion vs .
atheism
123 ( 123 ) 123 ( 123 ) 123 ( 123 )
123 ( 123 ) 123 ( 123 ) 123 ( 123 )
123 ( 123 ) 123 ( 123 ) 123 ( 123 )
table 123 : one vs
123 ( 123 ) 123 ( 123 )
123 ( 123 ) 123 ( 123 )
123 ( 123 ) 123 ( 123 )
table 123 : odd vs
table 123 : ten digits ( 123 classes )
table 123 : isolet ( 123 classes )
sults , we perform paired t - test on test accuracy .
we highlight the best accuracy in each row , and those that cannot be determined as different from the best , with paired t - test at signicance level 123 .
the semi - supervised kernels tend to out - perform standard supervised kernels .
the improved order constrained kernels are consistently among the best .
figure 123 shows the spectral transformation i of the semi - supervised kernels for different tasks .
these are for the 123 trials with the largest labeled set size in each task .
the x - axis is in increasing order of i ( the original eigenvalues of the laplacian ) .
the mean ( thick lines ) and 123 standard de - viation ( dotted lines ) of only the top 123 is are plotted for clarity .
the i values are scaled vertically for easy comparison among kernels .
as expected the maximal - alignment kernels spectral transformation is zigzagged , diffusion and gaussian elds are very smooth , while order constrained kernels are in between .
the or - der constrained kernels ( green ) have large 123 because of the order constraint .
this seems to be disadvantageous the spectral transformation tries to balance it out by increasing the value of other is so that the constant k123s relative inuence is smaller .
on the other hand the improved order constrained kernels ( black ) allow 123 to be small .
as a result the rest is decay fast , which is desirable .
in conclusion , the method is both computationally feasible and results in im - provements to classication performance when used with support vector machines .
baseball vs .
hockey
religion vs .
atheism
one vs
odd vs
ten digits ( 123 classes )
isolet ( 123 classes )
figure 123 : spectral transformation of the 123 semi - supervised kernels .
sequences and beyond
so far , we have treated each data point individually .
however in many problems the data has complex structures .
for example in speech recognition the data is se - quential .
most semi - supervised learning methods have not addressed this problem .
we use sequential data as an example in the following discussion because it is sim - ple .
nevertheless the discussion applies to other complex data structures like grids ,
it is important to clarify the setting .
by sequential data we do not mean each data item x is a sequence and we give a single label y to the whole sequence .
instead we want to give individual labels to the constituent data points in the se -
there are generative and discriminative methods that can be used for semi -
supervised learning on sequences .
the hidden markov model ( hmm ) is such a generative methods .
speci - cally the standard em training with forward - backward algorithm ( also known as baum - welch ( rabiner , 123 ) ) is a sequence semi - supervised learning algorithm , although it is usually not presented that way .
the training data typically consists of a small labeled set with l labeled sequences ( xl , yl ) = ( ( x123 , y123 ) .
( xl , yl ) ) , and a much larger unlabeled set of sequences xu = ( xl+123 .
we use bold font xi to represent the i - th sequence with length mi , whose elements are xi123 .
similarly yi is a sequence of labels yi123 .
the labeled set is used to estimate initial hmm parameters .
the unlabeled data is then used to run the em algorithm on , to improve the hmm likelihood p ( xu ) to a local maxi - mum .
the trained hmm parameters thus are determined by both the labeled and unlabeled sequences .
this parallels the mixture models and em algorithm in the i . i . d .
we will not discuss it further in the thesis .
for discriminative methods one strategy is to use a kernel machine for se -
quences , and introduce semi - supervised dependency via the kernels in chapter 123
recent kernel machines for sequences and other complex structures include ker - nel conditional random fields ( kcrfs ) ( lafferty et al . , 123 ) and max - margin markov networks ( taskar et al . , 123 ) , which are generalization of logistic re - gression and support vector machines respectively to structured data .
these kernel machines by themselves are not designed specically for semi - supervised learn - ing .
however we can use a semi - supervised kernel , for example the graph kernels in chapter 123 , with the kernel machines .
this results in semi - supervised learning methods on sequential data .
the idea is straightforward .
the remainder of the chapter focuses on kcrfs , describing the formalism and training issues , with a synthetic example on semi -
123 cliques and two graphs
before we start , it is useful to distinguish two kinds of graphs in kcrf for semi - supervised learning .
the rst graph ( gs ) represents the conditional random eld structure , for example a linear chain graph for sequences .
in this case the size of gs is the length of the sequence .
in general let x be the features on gss nodes and y the labels .
a clique c is a subset of the nodes which is fully connected , with any pair of nodes joined by an edge .
let yc be the labels on the clique .
we want mercer kernels k to compare cliques in different graphs ,
k ( ( gs , x , c , yc ) , ( g123
s , x123 , c123 , y123
intuitively , this assigns a measure of similarity between a labeled clique in one graph and a labeled clique in a ( possibly ) different graph .
we denote by hk the associated reproducing kernel hilbert space , and by kkk the associated norm .
in the context of semi - supervised learning , we are interested in kernels with the special form :
k ( ( gs , x , c , yc ) , ( g123
s , x123 , c123 , y123
c ) , gs , yc , g123
c123 ) ) = ( cid : 123 ) k123 ( xc , x123
some function of a kernel k123 , where k123 depends only on the features , not the labels .
this is where the second graph ( denoted gk ) comes in .
gk is the semi - supervised graph discussed in previous chapters .
its nodes are the cliques xc in both labeled and unlabeled data , and edges represent similarity between the cliques .
the size of gk is the total number of cliques in the whole dataset .
does not represent the sequence structure .
gk is used to derive the laplacian and ultimately the kernel matrix k123 ( xc , x123
c ) , as in chapter 123
123 representer theorem for kcrfs
we start from a function f which , looking at a clique ( c ) in graph ( gs , x ) and an arbitrary labeling of the clique ( yc ) , computes a compatibility score .
that is , f ( gs , x , c , yc ) r .
we dene a conditional random eld
f ( gs , x , c , yc ) !
the normalization factor is
p ( y|gs , x ) = z123 ( gs , x , f ) exp xc z ( gs , x , f ) =xy123
f ( gs , x , c , y123
notice we sum over all possible labelings of all cliques .
the conditional random eld induces a loss function , the negative log loss
( y|gs , x , f )
= log p ( y|gs , x )
f ( gs , x , c , yc ) + logxy123
f ( gs , x , c , y123
we now extend the standard representer theorem of kernel machines ( kimel - dorf & wahba , 123 ) to conditional graphical models .
consider a regularized loss function ( i . e .
risk ) of the form
r ( f ) =
s , x ( i ) , f ( cid : 123 ) + ( kfkk )
on a labeled training set of size l .
is a strictly increasing function .
it is important to note that the risk depends on all possible assignments yc of labels to each clique , not just those observed in the labeled data y ( i ) .
this is due to the normalization factor in the negative log loss .
we have the following representer theorem for
proposition ( representer theorem for crfs ) .
the minimizer f ? of the risk
f ? ( gs , x , c , yc ) =
c123 ( y123 ) k ( ( g ( i )
s , x ( i ) , c123 , y123 ) , ( gs , x , c , yc ) ) ( 123 )
where the sum y123 is over all labelings of clique c123
the key property distinguish - ing this result from the standard representer theorem is that the dual parameters c123 ( y123 ) now depend on all assignments of labels .
that is , for each training graph i , and each clique c123 within the graph , and each labeling y123 of the clique , not just the labeling in the training data , there is a dual parameter .
the difference between kcrfs and the earlier non - kernel version of crfs is the representation of f .
in a standard non - kernel crf , f is represented as a sum of weights times feature functions
f ( gs , x , c , yc ) = > ( gs , x , c , yc )
where is a vector of weights ( the primal parameters ) , and is a set of xed feature functions .
standard crf learning nds the optimal .
therefore one ad - vantage of kcrfs is the use of kernels which can correspond to innite features .
in addition if we plug in a semi - supervised learning kernel to kcrfs , we obtain a semi - supervised learning algorithm on structured data .
let us look at two special cases of kcrf .
in the rst case let the cliques be the
vertices v , and with a special kernel
k ( ( gs , x , v , yv ) , ( g123
s , x123 , v123 , y123
v123 ) ) = k123 ( xv , x123
the representer theorem states that
f ? ( x , y ) =
v ( y ) k123 ( x , x ( i )
under the probabilistic model 123 , this is simply kernel logistic regression .
it has no ability to model sequences .
in the second case let the cliques be edges connecting two vertices v123 , v123
the kernel be
and we have
k ( ( gs , x , v123v123 , yv123yv123 ) , ( g123
= k123 ( xv123 , x123
s , x123 , v123 v123 ) + ( yv123 , y123
f ? ( xv123 , yv123yv123 ) =
u ( yv123 ) k123 ( xv123 , x ( i )
u ) + ( yv123 , yv123 )
which is a simple type of semiparametric crf .
it has rudimentary ability to model sequences with ( yv123 , yv123 ) , similar to a transition matrix between states .
in both cases , we can use a graph kernel k123 on both labeled and unlabeled data for semi -
123 sparse training : clique selection
the representer theorem shows that the minimizing function f is supported by la - beled cliques over the training examples; however , this may result in an extremely large number of parameters .
we therefore pursue a strategy of incrementally select - ing cliques in order to greedily reduce the risk .
the resulting procedure is parallel to forward stepwise logistic regression , and to related methods for kernel logistic regression ( zhu & hastie , 123 ) .
s , x ( i ) , c , yc ) o , each item uniquely our algorithm will maintain an active setn ( g species a labeled clique .
again notice the labelings yc are not necessarily those appearing in the training data .
each labeled clique can be represented by a ba - s , x ( i ) , c , yc ) , ) hk , and is assigned a parameter sis function h ( ) = k ( ( g h = ( i )
c ( yc ) .
we work with the regularized risk
r ( f ) =
s , x ( i ) , f ( cid : 123 ) +
where is the negative log loss of equation ( 123 ) .
to evaluate a candidate h , one strategy is to compute the gain sup r ( f ) r ( f + h ) , and to choose the candidate h having the largest gain .
this presents an apparent difculty , since the optimal parameter cannot be computed in closed form , and must be evaluated nu - merically .
for sequence models this would involve forward - backward calculations for each candidate h , the cost of which is prohibitive .
as an alternative , we adopt the functional gradient descent approach , which evaluates a small change to the current function .
for a given candidate h , consider adding h to the current model with small weight ; thus f 123 f + h .
then r ( f + h ) = r ( f ) + dr ( f , h ) + o ( 123 ) , where the functional derivative of r at f in the direction h is computed as
dr ( f , h ) = ef ( h ) ee ( h ) + hf , hik
where ee ( h ) =pipc h ( g pipypc p ( y|x ( i ) , f ) h ( g
s , x ( i ) , c , y c ) is the empirical expectation and ef ( h ) = s , x ( i ) , c , yc ) is the model expectation conditioned on x .
the idea is that in directions h where the functional gradient dr ( f , h ) is large , the model is mismatched with the labeled data; this direction should be added to the model to make a correction .
this results in the greedy clique selection algorithm , as summarized in figure 123 .
an alternative to the functional gradient descent algorithm above is to estimate parameters h for each candidate .
when each candidate clique is a vertex , the
initialize with f = 123 , and iterate :
for each candidate h hk , supported by a single labeled
clique , calculate the functional derivative dr ( f , h ) .
select the candidate h = arg maxh|dr ( f , h ) | having the largest
gradient direction .
set f 123 f + hh .
estimate parameters f for each active f by minimizing r ( f ) .
figure 123 : greedy clique selection .
labeled cliques encode basis functions h which are greedily added to the model , using a form of functional gradient descent .
training set size
training set size
figure 123 : left : the galaxy data is comprised of two interlocking spirals together with a dense core of samples from both classes .
center : kernel logistic regres - sion comparing two kernels , rbf and a graph kernel using the unlabeled data .
right : kernel conditional random elds , which take into account the sequential structure of the data .
gain can be efciently approximated using a mean eld approximation .
under this approximation , a candidate is evaluated according to the approximate gain
r ( f ) r ( f + h )
v |x ( i ) , f ) exp ( h ( x ( i ) , y ( i )
v ) ) + hf , hi ( 123 )
which is a logistic approximation .
details can be found in appendix e .
123 synthetic data experiments
in the experiments reported below for sequences , the marginal probabilities p ( yv = 123|x ) and expected counts for the state transitions are required; these are computed
using the forward - backward algorithm , with log domain arithmetic to avoid un - derow .
a quasi - newton method ( bfgs , cubic - polynomial line search ) is used to estimate the parameters in step 123 of figure 123 .
to work with a data set that will distinguish a semi - supervised graph kernel from a standard kernel , and a sequence model from a non - sequence model , we prepared a synthetic data set ( galaxy ) that is a variant of spirals , see figure 123 ( left ) .
note data in the dense core come from both classes .
we sample 123 sequences of length 123 according to an hmm with two states , where each state emits instances uniformly from one of the classes .
there is a 123% chance of staying in the same state , and the initial state is uniformly chosen .
the idea is that under a sequence model we should be able to use the context to deter - mine the class of an example at the core .
however , under a non - sequence model without the context , the core region will be indistinguishable , and the dataset as a whole will have about 123% bayes error rate .
note the choice of semi - supervised vs .
standard kernels and sequence vs .
non - sequence models are orthogonal; the four combinations are all tested on .
we construct the semi - supervised graph kernel by rst building an unweighted 123 - nearest neighbor graph .
we compute the associated graph laplacian , and
then the graph kernel k = 123 ( cid : 123 ) + 123i ( cid : 123 ) 123
the standard kernel is the radial
basis function ( rbf ) kernel with an optimal bandwidth = 123 .
first we apply both kernels to a non - sequence model : kernel logistic regression ( 123 ) , see figure 123 ( center ) .
the sequence structure is ignored .
ten random trials were performed with each training set size , which ranges from 123 to 123 points .
the error intervals are one standard error .
as expected , when the labeled set size is small , the rbf kernel results in signicantly larger test error than the graph kernel .
furthermore , both kernels saturate at the 123% bayes error rate .
next we apply both kernels to a kcrf sequence model 123 .
experimental results are shown in figure 123 ( right ) .
note the x - axis is the number of train - ing sequences : since each sequence has 123 instances , the range is the same as figure 123 ( center ) .
the kernel crf is capable of getting below the 123% bayes error rate of the non - sequence model , with both kernels and sufcient labeled data .
however the graph kernel is able to learn the structure much faster than the rbf kernel .
evidently the high error rate for small label data sizes prevents the rbf model from effectively using the context .
finally we examine clique selection in kcrfs .
for this experiment we use 123 training sequences .
we use the mean eld approximation and only select vertex cliques .
at each iteration the selection is based on the estimated change in risk for each candidate vertex ( training position ) .
we plot the estimated change in risk for the rst four iterations of clique selection , with the graph kernel and rbf kernel re -
spectively in figure 123 .
smaller values ( lower on z - axis ) indicate good candidates with potentially large reduction in risk if selected .
for the graph kernel , the rst two selected vertices are sufcient to reduce the risk essentially to the minimum ( note in the third iteration the z - axis scale is already 123 ) .
such reduction does not happen with the rbf kernel .
123st position candidates
123nd position candidates
123rd position candidates
123th position candidates
123st position candidates
123nd position candidates
123rd position candidates
123th position candidates
figure 123 : mean eld estimate of the change in loss function with the graph kernel ( top ) and the rbf kernel ( bottom ) for the rst four iterations of clique selection on the galaxy dataset .
for the graph kernel the endpoints of the spirals are chosen as the rst two cliques .
harmonic mixtures : handling unseen data and reducing
there are two important questions to graph based semi - supervised learning meth -
the graph is constructed only on the labeled and unlabeled data .
many such methods are transductive in nature .
how can we handle unseen new data
they often involve expensive manipulation on large matrices , for example matrix inversion , which can be o ( n123 ) .
because unlabeled data is relatively easy to obtain in large quantity , the matrix could be too big to handle .
how can we reduce computation when the unlabeled dataset is large ?
in this chapter we address these questions by combining graph method with a mix -
mixture model has long been used for semi - supervised learning , e . g .
gaussian mixture model ( gmm ) ( castelli & cover , 123 ) ( ratsaby & venkatesh , 123 ) , and mixture of multinomial ( nigam et al . , 123 ) .
training is typically done with the em algorithm .
it has several advantages : the model is inductive and handles un - seen points naturally; it is a parametric model with a small number of parameters .
however when there is underlying manifold structure in the data , em may have difculty making the labels follow the manifold : an example is given in figure 123 .
the desired behavior is shown in figure 123 , which can be achieved by the harmonic mixture method discussed in this chapter .
mixture models and graph based semi - supervised learning methods make dif - ferent assumptions about the relation between unlabeled data and labels .
neverthe - less , they are not mutually exclusive .
it is possible that the data ts the component model ( e . g .
gaussian ) locally , while the manifold structure appears globally .
we combine the best from both .
from a graph method point of view , the resulting model is a much smaller ( thus computationally less expensive ) backbone graph with supernodes induced by the mixture components; from a mixture model point of view , it is still inductive and naturally handles new points , but also has the ability for labels to follow the data manifold .
our approach is related to graph reg - ularization in ( belkin et al . , 123b ) , and is an alternative to the induction method in ( delalleau et al . , 123 ) .
it should be noted that we are interested in mixture models with a large number ( possibly more than the number of labeled points ) of compo - nents , so that the manifold structure can appear , which is different from previous
123 review of mixture models and the em algorithm
in typical mixture models for classication , the generative process is the follow - ing .
one rst picks a class y , then chooses a mixture component m ( 123 .
m ) by p ( m|y ) , and nally generates a point x according to p ( x|m ) .
thus p ( x , y ) = m=123 p ( y ) p ( m|y ) p ( x|m ) .
in this paper we take a different but equivalent param -
p ( x , y ) =
we allow p ( y|m ) > 123 for all y , enabling classes to share a mixture component .
lihood of observed data :
the standard em algorithm learns these parameters to maximize the log like -
l ( ) = log p ( xl , xu , yl| )
log p ( xi , yi| ) +xiu
we introduce arbitrary distributions qi ( m|i ) on mixture membership , one for each
reviewofmixturemodelsandtheemalgorithm 123
by jensens inequality
l ( ) = xil
qi ( m|xi , yi ) log
the em algorithm works by iterating coordinate - wise ascend on q and to max - imize f ( q , ) .
the e step xes and nds the q that maximizes f ( q , ) .
we denote the xed at iteration t by p ( m ) ( t ) , p ( y|m ) ( t ) and p ( x|m ) ( t ) .
since the terms of f has the form of kl divergence , it is easy to see that the optimal q are the posterior on m :
i ( m|xi , yi ) = p ( m|xi , yi ) =
i ( m|xi ) = p ( m|xi ) =
, i u
, i l
the m step xes q ( t ) and nds ( t+123 ) to maximize f .
taking the partial deriva - tives and set to zero , we nd
m p ( y = 123|m ) ( t+123 ) = pil , yi=123 qi ( m ) ( t )
the last equation needs to be reduced further with the specic generative model
for x , e . g .
gaussian or multinomial .
for gaussian , we have
in practice one can smooth the ml estimate of covariance to avoid degeneracy :
m ) ( xi ( t )
m ) ( xi ( t )
= pilu qi ( m ) ( t ) xi = pilu qi ( m ) ( t ) ( xi ( t ) i +pilu qi ( m ) ( t ) ( xi ( t )
p ( y = 123|m ) p ( m|x )
m=123 p ( y = 123|m ) p ( x|m ) p ( m )
p ( y = 123|x ) =
after em converges , the classication of a new point x is done by
123 label smoothness on the graph
graph - based semi - supervised learning methods enforce label smoothness over a graph , so that neighboring labels tend to have the same label .
the graph has n nodes l u .
two nodes are connected by an edge with higher weights if they are more likely to be in the same class .
the graph is represented by the n n symmetric weight matrix w , and is assumed given .
label smoothness can be expressed in different ways .
we use the energy of the
label posterior as the measure ,
where f is the label posterior vector , dened as
e ( f ) =
wij ( fi fj ) 123 = f >f
p ( yi = 123|xi , )
that is , fi is the probability that point i having label 123 under the mixture model .
the energy is small when f varies smoothly on the graph .
= d w is the combinatorial laplacian matrix , and d is the diagonal degree matrix with dii = pj wij .
see chapter 123 for more details .
other smoothness measures are
possible too , for example those derived from the normalized laplacian ( zhou et al . , 123a ) or spectral transforms ( zhu et al . , 123 ) .
123 combining mixture model and graph
we want to train a mixture model that maximizes the data log likelihood ( 123 ) and minimizes the graph energy ( 123 ) at the same time .
one way of doing so is to learn the parameters p ( m ) , p ( x|m ) , p ( y|m ) to maximize the objective
o = l ( 123 ) e
where ( 123 , 123 ) is a coefcient that controls the relative strength of the two terms .
the e term may look like a prior ef >f on the parameters .
but it involves the observed labels yl , and is best described as a discriminative objective , while l is a generative objective .
this is closely related to , but different from , the graph regularization framework of ( belkin et al . , 123b ) .
learning all the parameters together however is difcult .
because of the e term , it is similar to conditional em training which is more complicated than the standard em algorithm .
instead we take a two - step approach :
step 123 : train all parameters p ( m ) , p ( x|m ) , p ( y|m ) with standard em , which
maximizes l only;
step 123 : fix p ( m ) and p ( x|m ) , and only learn p ( y|m ) to maximize ( 123 ) .
it is suboptimal in terms of optimizing the objective function .
however it has two advantages : we created a concave optimization problem in the second step ( see section 123 . 123 ) ; moreover , we can use standard em without modication .
we call the solution harmonic mixtures .
we focus on step 123
the free parameters are p ( y|m ) for m = 123 .
to sim - plify the notation , we use the shorthand m p ( y = 123|m ) , and ( 123 , .
, m ) > .
we rst look at the special case with = 123 in the objective function ( 123 ) , as it has a particularly simple closed form solution and interpretation .
notice although = 123 , the generative objective l still inuences through p ( m ) and p ( x|m ) learned in step 123
123 . 123 the special case with = 123
we need to nd the parameters that minimize e .
are constrained in ( 123 , 123 ) m .
however let us look at the unconstrained optimization problem rst .
applying the
the rst term is
( f >f )
l llfl + 123f >
l lu fu + f >
u u u fu )
= 123lu fl + 123u u fu
where we partitioned the laplacian matrix into labeled and unlabeled parts respec - tively .
the second term is
= ( p ( m|xl+123 ) , .
, p ( m|xl+u ) ) > rm
where we dened a u m responsibility matrix r such that rim = p ( m|xi ) , and rm is its m - th column .
we used the fact that for i u ,
fi = p ( yi = 123|xi , )
= pm p ( m ) p ( yi = 123|m ) p ( xi|m )
p ( m|xi ) p ( yi = 123|m )
notice we can write fu = r .
therefore
m ( 123u u fu + 123u lfl )
m ( 123u u r + 123u lfl )
when we put all m partial derivatives in a vector and set them to zero , we nd
= r> ( 123u u r + 123u lfl ) = 123
where 123 is the zero vector of length m .
this is a linear system and the solution is
= ( r>u u r ) 123 r>u lfl
notice this is the solution to the unconstrained problem , where some might be out of the bound ( 123 , 123 ) .
if it happens , we set out - of - bound s to their corresponding boundary values of 123 or 123 , and use them as starting point in a constrained convex
optimization ( the problem is convex , as shown in the next section ) to nd the global solution .
in practice however we found most of the time the closed form solution for the unconstrained problem is already within bounds .
even when some compo - nents are out of bounds , the solution is close enough to the constrained optimum to allow quick convergence .
with the component class membership , the soft labels for the unlabeled data
are given by
fu = r unseen new points can be classied similarly .
we can compare ( 123 ) with the ( completely graph based ) harmonic function solution ( zhu et al . , 123a ) .
the former is fu = r ( r>u u r ) 123 r>u lfl; the latter is fu = 123 u u u lfl .
computationally the former only needs to invert a m m matrix , which is much cheaper than the latter of u u because typically the number of mixture components is much smaller than the number of unlabeled points .
this reduction is possible because fu are now tied together by the mixture
in the special case where r corresponds to hard clustering , we just created a much smaller backbone graph with supernodes induced by the mixture compo - nents .
in this case rim = 123 for cluster m to which point i belongs , and 123 for all other m 123 clusters .
the backbone graph has the same l labeled nodes as in the original graph , but only m unlabeled supernodes .
let wij be the weight between nodes i , j in the original graph .
by rearranging the terms it is not hard to show that in the backbone graph , the equivalent weight between supernodes s , t ( 123
wst = xi , ju
and the equivalent weight between a supernode s and a labeled node l l is
is simply the harmonic function on the supernodes in the backbone graph .
for this reason ( 123 , 123 ) m is guaranteed .
let c ( m ) = ( i|rim = 123 ) be the cluster m .
the equivalent weight between supernodes s , t reduces to
wst = xic ( s ) , jc ( t )
the supernodes are the clusters themselves .
the equivalent weights are the sum of edges between the clusters ( or the cluster and a labeled node ) .
one can easily
input : initial mixture model p ( m ) , p ( x|m ) , p ( y|m ) , m = 123
data xl , yl , xu
run standard em on data and get converged model p ( m ) , p ( x|m ) , p ( y|m ) 123
fix p ( m ) , p ( x|m ) .
compute m p ( y = 123|m ) = ( r>u u r ) 123 r>u lfl 123
set out - of - bound s to 123 or 123 , run constrained convex optimization output : mixture model p ( m ) , p ( x|m ) , p ( y|m ) , m = 123
table 123 : the harmonic mixture algorithm for the special case = 123
create such a backbone graph by e . g .
k - means clustering .
in the general case when r is soft , the solution deviates from that of the backbone graph .
the above algorithm is listed in table 123 .
in practice some mixture compo - nents may have little or no responsibility ( p ( m ) 123 ) .
they should be excluded from ( 123 ) to avoid numerical problems .
in addition , if r is rank decient we use the pseudo inverse in ( 123 ) .
123 . 123 the general case with > 123 the objective ( 123 ) is concave in .
to see this , we rst write l as l ( ) = xil
p ( m ) p ( xi|m ) m + xil
p ( m ) p ( yi|m ) p ( xi|m ) + const
p ( m ) p ( xi|m ) ( 123 m ) + const
since we x p ( m ) and p ( x|m ) , the term within the rst sum has the form logpm amm .
we can directly verify the hessian
h = ( cid : 123 ) logpm amm
( pm amm ) 123 aa> ( cid : 123 ) 123
is negative semi - denite .
therefore the rst term ( i l and yi = 123 ) is concave .
similarly the hessian for the second term is
h = ( cid : 123 ) logpm am ( 123 m )
( pm am ( 123 m ) ) 123 ( cid : 123 ) 123 ( 123 )
l is the non - negative sum of concave terms and is concave .
recall fu = r , the graph energy can be written as
e = f >f
= f > = f >
l llfl + 123f > l llfl + 123f >
l lu fu + f > l lu r + >r>u u r
u u u fu
the hessian is 123r>u u r ( cid : 123 ) 123 because u u ( cid : 123 ) 123
therefore e is convex in .
putting them together , o is concave in .
as m is in ( 123 , 123 ) , we perform constrained convex optimization in the general case with > 123
the gradient of the objective is easily computed :
m ( 123 )
k=123 p ( k ) p ( xi|k ) k xil
k=123 p ( k ) p ( xi|k ) ( 123 k )
and e / was given in ( 123 ) .
one can also use the sigmoid function to trans - form it into an unconstrained optimization problem with
m = ( m ) =
em + 123
and optimize the s .
although the objective is concave , a good starting point for is still important to reduce the computation time until convergence .
we nd a good initial value for by solving an one - dimensional concave optimization problem rst .
we have two parameters at hand : em is the solution from the standard em algorithm in step 123 , and special is the special case solution in section 123 . 123
we nd the optimal interpolated coefcient ( 123 , 123 )
init = em + ( 123 ) special
that maximizes the objective ( the optimal in general will not be ) .
then we start from init and use a quasi - newton algorithm to nd the global optimum for .
initial random gmm settings
( a ) m = 123 gaussian components
( b ) m = 123 gaussian components
after em converges
figure 123 : gaussian mixture models learned with the standard em algorithm cannot make labels follow the manifold structure in an articial dataset .
small dots are unlabeled data .
the two labeled points are marked with red + and green ( cid : 123 ) .
the left panel has m = 123 and right m = 123 mixture components .
top plots show the initial settings of the gmm .
bottom plots show the gmm after em converges .
the ellipses are the contours of covariance matrices .
the colored central dots have sizes proportional to the component weight p ( m ) .
components with very small p ( m ) are not plotted .
the color stands for component class membership m p ( y = 123|m ) : red for = 123 , green for = 123 , and intermediate yellow for values in between which did not occur in the converged solutions .
notice in the bottom - right plot , although the density p ( x ) is estimated well by em , does not follow the manifold .
figure 123 : the gmm with the component class membership learned as in the special case = 123
, color coded from red to yellow and green , now follow the structure of the unlabeled data .
we test harmonic mixture on synthetic data , image and text classication .
the emphases are on how harmonic mixtures perform on unlabeled data compared to em or the harmonic function; how they handle unseen data; and whether they can reduce the problem size .
unless otherwise noted , the harmonic mixtures are computed with = 123
123 . 123 synthetic data
first we look at a synthetic dataset in figure 123 .
it has a swiss roll structure , and we hope the labels can follow the spiral arms .
there is one positive and one negative labeled point , at roughly the opposite ends .
we use u = 123 unlabeled points and an additional 123 points as unseen test data .
the mixture model and standard em .
we start with figure 123 ( a , top ) , the initial setting for a gaussian mixture model with m = 123 components .
the initial means are set by running a k - means algorithm .
the initial covariances are identity , thus the circles .
the initial are all set to 123 , represented by the yellow color .
( a , bottom ) shows the gmm after em converges .
obviously it is a bad model because m is too small .
next we consider a gaussian mixture model ( gmm ) with m = 123 compo -
nents , each with full covariance .
figure 123 ( b , top ) shows the initial gmm and ( b , bottom ) the converged gmm after running em .
the gmm models the manifold density p ( x ) well .
however the component class membership m p ( y = 123|m ) ( red and green colors ) does not follow the manifold .
in fact takes the extreme values of 123 or 123 along a somewhat linear boundary instead of following the spiral arms , which is undesirable .
the classication of data points will not follow the the graph and harmonic mixtures .
next we combine the mixture model with a graph to compute the harmonic mixtures , as in the special case = 123
we construct a fully connected graph on the l u data points with weighted edges wij = exp ( cid : 123 ) ||xi xj||123 / 123 ( cid : 123 ) .
we then reestimate , which are shown in figure 123 .
note now follow the manifold as it changes from 123 ( green ) to approximately 123 ( yellow ) and nally 123 ( red ) .
this is the desired behavior .
the particular graph - based method we use needs extra care .
the harmonic function solution f is known to sometimes skew toward 123 or 123
this problem is easily corrected if we know or have an estimate of the proportion of positive and negative points , with the class mass normalization heuristic ( zhu et al . , 123a ) .
in this paper we use a similar but simpler heuristic .
assuming the two classes are about equal in size , we simply set the decision boundary at the median .
that is , let f ( l + 123 ) , .
, f ( n ) be the soft label values on the unlabeled nodes .
let m ( f ) = median ( f ( l + 123 ) , .
, f ( n ) ) .
we classify point i as positive if f ( i ) > m ( f ) , and sensitivity to m .
if the number of mixture components m is too small , the gmm is unable to model p ( x ) well , let alone .
in other words , the harmonic mixture is sensitive to m .
m has to be larger than a certain threshold so that the man - ifold structure can appear .
in fact m may need to be larger than the number of labeled points l , which is unusual in traditional mixture model methods for semi - supervised learning .
however once m is over the threshold , further increase should not dramatically change the solution .
in the end the harmonic mixture may ap - proach the harmonic function solution when m = u .
figure 123 ( a ) shows the classication accuracy on u as we change m .
we nd that the threshold for harmonic mixtures is m = 123 , at which point the ac - curacy ( hm ) jumps up and stabilizes thereafter .
this is the number of mixture components needed for harmonic mixture to capture the manifold structure .
the harmonic function on the complete graph ( graph ) is not a mixture model and appears at .
the em algorithm ( em ) fails to discover the manifold structure regardless of the number of mixtures m .
computational savings .
the harmonic mixtures perform almost as well as the harmonic function on the complete graph , but with a much smaller problem size .
as figure 123 ( a ) shows , we only need to invert a 123 123 matrix instead of a
123 123 one as required by the harmonic function solution .
the difference can be signicant if the unlabeled set size is even larger .
there is of course the overhead of em training .
handling unseen data .
because the harmonic mixture model is a mixture model , it naturally handles unseen points .
on 123 new test points harmonic mixtures perform similarly to figure 123 ( a ) , with accuracies around 123% after m 123
image recognition : handwritten digits
we use the 123vs123 dataset which contains equal number of images of handwritten digit of 123s and 123s .
each gray scale image is 123 123 , which is represented by a 123 dimensional vector of pixel values .
we use l + u = 123 images as the labeled and unlabeled set , and 123 additional images as unseen new data to test induction .
the mixture model .
we use gaussian mixture models .
to avoid data sparse - ness problem , we model each gaussian component with a spherical covariance , i . e .
diagonal covariance matrix with the same variance in all dimensions .
different components may have different variances .
we set the initial means and variances of the gmm with k - means algorithm before running em .
the graph .
we use a symmetrized 123 - nearest - neighbor weighted graph on the 123 images .
that is , images i , j are connected if i is within js 123nn or vice
versa , as measured by euclidean distance .
the weights are wij = exp ( cid : 123 ) ||xi xj||123 / 123 ( cid : 123 ) .
sensitivity to m .
as illustrated in the synthetic data , the number of mixture com - ponents m needs to be large enough for harmonic mixture to work .
we vary m and observe the classication accuracies on the unlabeled data with different meth - ods .
for each m we perform 123 trials with random l / u split and plot the mean and standard deviation of classication accuracies in figure 123 ( b ) .
the exper - iments were performed with labeled set size xed at l = 123
we conclude that harmonic mixtures need only m 123 components to match the performance of the harmonic function method .
computational savings .
in terms of graph method computation , we invert a 123 123 matrix instead of the original 123 123 matrix for harmonic function .
this is good saving with little sacrice in accuracy .
we x m = 123 in the experiments handling unseen data .
we systematically vary labeled set size l .
for each l we run 123 random trials .
the classication accuracy on u ( with 123 - l points ) and unseen data ( 123 points ) are listed in table 123 .
on u , harmonic mixtures ( hm ) achieve the same accuracy as harmonic function ( graph ) .
both are not sensitive to l .
the gmm trained with em ( em ) also performs well when l is not too small , but suffers otherwise .
on the unseen test data , the harmonic mixtures maintain
the general case > 123
we also vary the parameter between 123 and 123 , which balances the generative and discriminative objectives .
in our experiments = 123 always gives the best accuracies .
123 . 123 text categorization : pc vs
we perform binary text classication on the two groups comp . sys . ibm . pc . hardware vs .
comp . sys . mac . hardware ( 123 and 123 documents respectively ) in the 123 version of the 123 - newsgroups data .
we use rainbow ( mccallum , 123 ) to prepro - cess the data , with the default stopword list , no stemming , and keep words that occur at least 123 times .
we represent documents by tf . idf vectors with the okapi tf formula ( zhai , 123 ) , which was also used in ( zhu et al . , 123a ) .
of the 123 documents , we use 123 as l u and the rest as unseen test data .
the mixture model .
we use multinomial mixture models ( bag - of - words naive bayes model ) , treating tf . idf as pseudo word counts of the documents .
we found this works better than using the raw word counts .
we use k - means to initialize the the graph .
we use a symmetrized 123nn weighted graph on the 123 docu - ments .
the weight between documents u , v is wuv = exp ( ( 123 cuv ) / 123 ) , where cuv = hu , vi / ( ||u|| ||v|| ) is the cosine between the tf . idf vectors u , v .
sensitivity to m .
the accuracy on u with different number of components m is shown in figure 123 ( c ) .
l is xed at 123
qualitatively the performance of harmonic mixtures increases when m > 123
from the plot it may look like the graph curve varies with m , but this is an artifact as we used different randomly sampled l , u splits for different m .
the error bars on harmonic mixtures are large .
we suspect the particular mixture model is bad for the task .
computational savings .
unlike the previous tasks , we need a much larger m around 123
we still have a smaller problem than the original u = 123 , but the saving is limited .
handling unseen data .
we x m = 123 and vary labeled set size l .
for each l we run 123 random trials .
the classication accuracy on u ( with 123 - l documents ) and unseen data ( 123 documents ) are listed in table 123 .
the harmonic mixture model has lower accuracies than the harmonic function on the l u graph .
the harmonic mixture model performs similarly on u and on unseen data .
123 related work
recently delalleau et al .
( 123 ) use a small random subset of the unlabeled data to create a small graph .
this is related to the nystrom method in spectral clustering
table 123 : image classication 123 vs .
123 : accuracy on u and unseen data .
m = 123
each number is the mean and standard deviation of 123 trials .
table 123 : text classication pc vs .
mac : accuracy on u and unseen data .
m = 123
each number is the mean and standard deviation of 123 trials .
( fowlkes et al . , 123 ) , and to the random landmarks in dimensionality reduction ( weinberger et al . , 123 ) .
our method is different in that
it incorporates a generative mixture model , which is a second knowledge
source besides the graph;
the backbone graph is not built on randomly selected points , but on mean -
ingful mixture components;
when classifying an unseen point x , it does not need graph edges from land - mark points to x .
this is less demanding on the graph because the burden is transferred to the mixture component models .
for example one can now use knn graphs .
in the other works one needs edges between x and the landmarks , which are non - existent or awkward for knn graphs .
in terms of handling unseen data , our approach is closely related to the regu - larization framework of ( belkin et al . , 123b; krishnapuram et al . , 123 ) as graph regularization on mixture models .
however instead of a regularization term we used a discriminative term , which allows for the closed form solution in the special
to summarize , the proposed harmonic mixture method reduces the graph prob - lem size , and handles unseen test points .
it achieves comparable accuracy as the harmonic function for semi - supervised learning .
there are several questions for further research .
first , the component model affects the performance of the harmonic mixtures .
for example the gaussian in the synthetic task and 123 vs .
123 task seem to be more amenable to harmonic mixtures than the multinomial in pc vs .
mac task .
how to quantify the inuence remains a question .
a second question is when > 123 is useful in practice .
finally , we want to nd a way to automatically select the appropriate number of mixture components
the backbone graph is certainly not the only way to speed up computation .
we list some other methods in literature review in chapter 123
in addition , we also performed an empirical study to compare several iterative methods , including label propagation , loopy belief propagation , and conjugate gradient , which all converge to the harmonic function .
the study is presented in appendix f .
( a ) synthetic data
( b ) 123 vs
( c ) pc vs
figure 123 : sensitivity to m in three datasets .
shown are the classication accu - racies on u as m changes .
graph is the harmonic function on the complete l u graph; hm is the harmonic mixture , and em is the standard em algorithm .
the intervals are 123 standard deviation with 123 random trials when applicable .
we review some of the literature on semi - supervised learning .
there has been a whole spectrum of interesting ideas on how to learn from both labeled and un - labeled data .
the review is by no means comprehensive and the eld of semi - supervised learning is evolving rapidly .
the author apologizes in advance for any inaccuracies in the descriptions , and welcomes corrections and comments .
please send corrections and suggest papers to zhuxj@cs . cmu . edu .
to make the review more useful , we maintain an online version at which will be updated indenitely .
q : what is semi - supervised learning ? a : its a special form of classication .
traditional classiers need labeled data ( feature / label pairs ) to train .
labeled instances however are often difcult , ex - pensive , or time consuming to obtain , as they require the efforts of experienced human annotators .
meanwhile unlabeled data may be relatively easy to collect , but there has been few ways to use them .
semi - supervised learning addresses this problem by using large amount of unlabeled data , together with the labeled data , to build better classiers .
because semi - supervised learning requires less human effort and gives higher accuracy , it is of great interest both in theory and in practice .
q : can we really learn anything from unlabeled data ? it looks like magic .
a : yes we can under certain assumptions .
its not magic , but good matching of problem structure with model assumption .
q : does unlabeled data always help ? a : no , theres no free lunch .
bad matching of problem structure with model as - sumption can lead to degradation in classier performance .
for example , quite a few semi - supervised learning methods assume that the decision boundary should avoid regions with high p ( x ) .
these methods include transductive support vector machines ( svms ) , information regularization , gaussian processes with null cate - gory noise model , graph - based methods if the graph weights is determined by pair - wise distance .
nonetheless if the data is generated from two heavily overlapping gaussian , the decision boundary would go right through the densest region , and these methods would perform badly .
on the other hand em with generative mix - ture models , another semi - supervised learning method , would have easily solved the problem .
detecting bad match in advance however is hard and remains an open
q : how many semi - supervised learning methods are there ? a : many .
some often - used methods include : em with generative mixture models , self - training , co - training , transductive support vector machines , and graph - based methods .
see the following sections for more methods .
q : which method should i use / is the best ? a : there is no direct answer to this question .
because labeled data is scarce , semi - supervised learning methods make strong model assumptions .
ideally one should use a method whose assumptions t the problem structure .
this may be difcult in reality .
nonetheless we can try the following checklist : do the classes produce well clustered data ? if yes , em with generative mixture models may be a good choice; do the features naturally split into two sets ? if yes , co - training may be appropriate; is it true that two points with similar features tend to be in the same class ? if yes , graph - based methods can be used; already using svm ? transductive svm is a natural extension; is the existing supervised classier complicated and hard to modify ? self - training is a practical wrapper method .
q : how do semi - supervised learning methods use unlabeled data ? a : semi - supervised learning methods use unlabeled data to either modify or re - prioritize hypotheses obtained from labeled data alone .
although not all methods are probabilistic , it is easier to look at methods that represent hypotheses by p ( y|x ) , and unlabeled data by p ( x ) .
generative models have common parameters for the joint distribution p ( x , y ) .
it is easy to see that p ( x ) inuences p ( y|x ) .
mixture models with em is in this category , and to some extent self - training .
many other methods are discriminative , including transductive svm , gaussian processes , in - formation regularization , and graph - based methods .
original discriminative train -
ing cannot be used for semi - supervised learning , since p ( y|x ) is estimated ignoring p ( x ) .
to solve the problem , p ( x ) dependent terms are often brought into the ob - jective function , which amounts to assuming p ( y|x ) and p ( x ) share parameters .
q : where can i learn more ? a : an existing survey can be found in ( seeger , 123 ) .
123 generative mixture models and em
this is perhaps the oldest semi - supervised learning method .
it assumes a genera - tive model p ( x , y ) = p ( y ) p ( x|y ) where p ( x|y ) is an identiable mixture distribu - tion , for example gaussian mixture models .
with large amount of unlabeled data , the mixture components can be identied; then ideally we only need one labeled example per component to fully determine the mixture distribution .
one can think of the mixture components as soft clusters .
nigam et al .
( 123 ) apply the em algorithm on mixture of multinomial for the task of text classication .
they showed the resulting classiers perform better than those trained only from l .
baluja ( 123 ) uses the same algorithm on a face orientation discrimination task .
one has to pay attention to a few things :
the mixture model ideally should be identiable .
in general let ( p ) be a family of distributions indexed by a parameter vector .
is identiable if 123 123= 123 p123 123= p123 , up to a permutation of mixture components .
if the model family is identiable , in theory with innite u one can learn up to a permutation of component indices .
here is an example showing the problem with unidentiable models .
the model p ( x|y ) is uniform for y ( +123 , 123 ) .
assuming with large amount of un - labeled data u we know p ( x ) is uniform in ( 123 , 123 ) .
we also have 123 labeled data points ( 123 , +123 ) , ( 123 , 123 ) .
can we determine the label for x = 123 ? no .
with our assumptions we cannot distinguish the following two models : p ( y = 123 ) = 123 , p ( x|y = 123 ) = unif ( 123 , 123 ) , p ( x|y = 123 ) = unif ( 123 , 123 ) ( 123 ) p ( y = 123 ) = 123 , p ( x|y = 123 ) = unif ( 123 , 123 ) , p ( x|y = 123 ) = unif ( 123 , 123 ) ( 123 ) which give opposite labels at x = 123 , see figure 123 .
it is known that a mixture of gaussian is identiable .
mixture of multivariate bernoulli ( mccallum & nigam , 123a ) is not identiable .
more discussions on identiability and semi - supervised learning can be found in e . g .
( ratsaby & venkatesh , 123 ) and ( corduneanu &
+ 123 *
= 123 *
= 123 *
+ 123 *
figure 123 : an example of unidentiable models .
even if we known p ( x ) ( top ) is a mixture of two uniform distributions , we cannot uniquely identify the two components .
for instance , the mixtures on the second and third line give the same p ( x ) , but they classify x = 123 differently .
( a ) horizontal class separation
( b ) high probability
( c ) low probability
figure 123 : if the model is wrong , higher likelihood may lead to lower classica - tion accuracy .
for example , ( a ) is clearly not generated from two gaussian .
if we insist that each class is a single gaussian , ( b ) will have higher probability than ( c ) .
but ( b ) has around 123% accuracy , while ( c ) s is much better .
123 . 123 model correctness
if the mixture model assumption is correct , unlabeled data is guaranteed to improve accuracy ( castelli & cover , 123 ) ( castelli & cover , 123 ) ( ratsaby & venkatesh , 123 ) .
however if the model is wrong , unlabeled data may actually hurt accuracy .
figure 123 shows an example .
this has been observed by multiple researchers .
cozman et al .
( 123 ) give a formal derivation on how this might happen .
it is thus important to carefully construct the mixture model to reect reality .
for example in text categorization a topic may contain several sub - topics , and will be better modeled by multiple multinomial instead of a single one ( nigam et al . , 123 ) .
some other examples are ( shahshahani & landgrebe , 123 ) ( miller & uyar , 123 ) .
another solution is to down - weighing unlabeled data ( corduneanu &
jaakkola , 123 ) , which is also used by nigam et al .
( 123 ) , and by callison - burch et al .
( 123 ) who estimate word alignment for machine translation .
123 . 123 em local maxima
even if the mixture model assumption is correct , in practice mixture components are identied by the expectation - maximization ( em ) algorithm ( dempster et al . , 123 ) .
em is prone to local maxima .
if a local maximum is far from the global maximum , unlabeled data may again hurt learning .
remedies include smart choice of starting point by active learning ( nigam , 123 ) .
123 . 123 cluster and label
we shall also mention that instead of using an probabilistic generative mixture model , some approaches employ various clustering algorithms to cluster the whole dataset , then label each cluster with labeled data , e . g .
( demiriz et al . , 123 ) ( dara et al . , 123 ) .
although they may perform well if the particular clustering algo - rithms match the true data distribution , these approaches are hard to analyze due to their algorithmic nature .
self - training is a commonly used technique for semi - supervised learning .
in self - training a classier is rst trained with the small amount of labeled data .
the classier is then used to classify the unlabeled data .
typically the most condent unlabeled points , together with their predicted labels , are added to the training set .
the classier is re - trained and the procedure repeated .
note the classier uses its own predictions to teach itself .
the procedure is also called self - teaching or bootstrapping ( not to be confused with the statistical procedure with the same name ) .
the generative model and em approach of section 123 can be viewed as a special case of soft self - training .
one can imagine that a classication mistake can reinforce itself .
some algorithms try to avoid this by unlearn unlabeled points if the prediction condence drops below a threshold .
self - training has been applied to several natural language processing tasks .
yarowsky ( 123 ) uses self - training for word sense disambiguation , e . g .
deciding whether the word plant means a living organism or a factory in a give context .
riloff et al .
( 123 ) uses it to identify subjective nouns .
maeireizo et al .
( 123 ) classify dialogues as emotional or non - emotional with a procedure involving two classiers . self - training has also been applied to parsing and machine transla - tion .
rosenberg et al .
( 123 ) apply self - training to object detection systems from
( a ) x123 view
( b ) x123 view
figure 123 : co - training : conditional independent assumption on feature split .
with this assumption the high condent data points in x123 view , represented by circled labels , will be randomly scattered in x123 view .
this is advantageous if they are to be used to teach the classier in x123 view .
images , and show the semi - supervised technique compares favorably with a state -
co - training ( blum & mitchell , 123 ) ( mitchell , 123 ) assumes that features can be split into two sets; each sub - feature set is sufcient to train a good classier; the two sets are conditionally independent given the class .
initially two separate classiers are trained with the labeled data , on the two sub - feature sets respectively .
each classier then classies the unlabeled data , and teaches the other classier with the few unlabeled examples ( and the predicted labels ) they feel most con - dent .
each classier is retrained with the additional training examples given by the other classier , and the process repeats .
in co - training , unlabeled data helps by reducing the version space size .
in other words , the two classiers ( or hypotheses ) must agree on the much larger unlabeled data as well as the labeled data .
we need the assumption that sub - features are sufciently good , so that we can trust the labels by each learner on u .
we need the sub - features to be conditionally independent so that one classiers high condent data points are iid samples for the other classier .
figure 123 visualizes the assumption .
nigam and ghani ( 123 ) perform extensive empirical experiments to compare co - training with generative mixture models and em .
their result shows co - training performs well if the conditional independence assumption indeed holds .
in addi - tion , it is better to probabilistically label the entire u , instead of a few most con - dent data points .
they name this paradigm co - em .
finally , if there is no natural feature split , the authors create articial split by randomly break the feature set into
two subsets .
they show co - training with articial feature split still helps , though not as much as before .
jones ( 123 ) used co - training , co - em and other related methods for information extraction from text .
co - training makes strong assumptions on the splitting of features .
one might wonder if these conditions can be relaxed .
goldman and zhou ( 123 ) use two learners of different type but both takes the whole feature set , and essentially use one learners high condence data points , identied with a set of statistical tests , in u to teach the other learning and vice versa .
recently balcan et al .
( 123 ) relax the conditional independence assumption with a much weaker expansion condition , and justify the iterative co - training procedure .
123 maximizing separation
123 . 123 transductive svm discriminative methods work on p ( y|x ) directly .
this brings up the danger of leaving p ( x ) outside of the parameter estimation loop , if p ( x ) and p ( y|x ) do not share parameters .
notice p ( x ) is usually all we can get from unlabeled data .
it is believed that if p ( x ) and p ( y|x ) do not share parameters , semi - supervised learning cannot help .
this point is emphasized in ( seeger , 123 ) .
zhang and oles ( 123 ) give both theoretical and experimental evidence of the same point specically on transductive support vector machines ( tsvm ) .
however this is controversial as empirically tsvms seem benecial .
tsvm is an extension of standard support vector machines with unlabeled data .
in a standard svm only the labeled data is used , and the goal is to nd a maximum margin linear boundary in the reproducing kernel hilbert space .
in a tsvm the unlabeled data is also used .
the goal is to nd a labeling of the unla - beled data , so that a linear boundary has the maximum margin on both the original labeled data and the ( now labeled ) unlabeled data .
the decision boundary has the smallest generalization error bound on unlabeled data ( vapnik , 123 ) .
intuitively , unlabeled data guides the linear boundary away from dense regions .
however nding the exact transductive svm solution is np - hard .
several approximation al - gorithms have been proposed and show positive results , see e . g .
( joachims , 123 ) ( bennett & demiriz , 123 ) ( demirez & bennettt , 123 ) ( fung & mangasarian , 123 ) ( chapelle & zien , 123 ) .
the maximum entropy discrimination approach ( jaakkola et al . , 123 ) also maximizes the margin , and is able to take into account unlabeled data , with svm as a special case .
the application of graph kernels ( zhu et al . , 123 ) to svms differs from tsvm .
the graph kernels are special semi - supervised kernels applied to a stan -
figure 123 : in tsvm , u helps to put the decision boundary in sparse regions .
with labeled data only , the maximum margin boundary is plotted with dotted lines .
with unlabeled data ( black dots ) , the maximum margin boundary would be the one with solid lines .
dard svm; tsvm is a special optimization criterion regardless of the kernel being
123 . 123 gaussian processes
lawrence and jordan ( 123 ) proposed a gaussian process approach , which can be viewed as the gaussian process parallel of tsvm .
the key difference to a standard gaussian process is in the noise model .
a null category noise model maps the hidden continuous variable f to three instead of two labels , specically to the never used label 123 when f is around zero .
on top of that , it is restricted that unlabeled data points cannot take the label 123
this pushes the posterior of f away from zero for the unlabeled points .
it achieves the similar effect of tsvm where the margin avoids dense unlabeled data region .
however nothing special is done on the process model .
therefore all the benet of unlabeled data comes from the noise model .
a very similar noise model is proposed in ( chu & ghahramani , 123 ) for ordinal
this is different from the gaussian processes in ( zhu et al . , 123c ) , where we have a semi - supervised gram matrix , and semi - supervised learning originates from the process model , not the noise model .
szummer and jaakkola ( 123 ) propose the information regularization framework to control the label conditionals p ( y|x ) by p ( x ) , where p ( x ) may be estimated from unlabeled data .
the idea is that labels shouldnt change too much in regions where p ( x ) is high .
the authors use the mutual information i ( x; y ) between x and y as a measure of label complexity .
i ( x; y ) is small when the labels are homogeneous ,
and large when labels vary .
this motives the minimization of the product of p ( x ) mass in a region with i ( x; y ) ( normalized by a variance term ) .
the minimization is carried out on multiple overlapping regions covering the data space .
the theory is developed further in ( corduneanu & jaakkola , 123 ) .
cor - duneanu and jaakkola ( 123 ) extend the work by formulating semi - supervised learning as a communication problem .
regularization is expressed as the rate of information , which again discourages complex conditionals p ( y|x ) in regions with high p ( x ) .
the problem becomes nding the unique p ( y|x ) that minimizes a regu - larized loss on labeled data .
the authors give a local propagation algorithm .
123 . 123 entropy minimization
the hyperparameter learning method in section 123 uses entropy minimization .
grandvalet and bengio ( 123 ) used the label entropy on unlabeled data as a reg - ularizer .
by minimizing the entropy , the method assumes a prior which prefers minimal class overlap .
123 graph - based methods
graph - based semi - supervised methods dene a graph where the nodes are labeled and unlabeled examples in the dataset , and edges ( may be weighted ) reect the similarity of examples .
these methods usually assume label smoothness over the graph .
graph methods are nonparametric , discriminative , and transductive in na - ture .
this thesis largely focuses on graph - based semi - supervised learning algo -
123 . 123 regularization by graph
many graph - based methods can be viewed as estimating a function f on the graph .
one wants f to satisfy two things at the same time : 123 ) it should be close to the given labels yl on the labeled nodes , and 123 ) it should be smooth on the whole graph .
this can be expressed in a regularization framework where the rst term is a loss function , and the second term is a regularizer .
several graph - based methods listed here are similar to each other .
they differ in the particular choice of the loss function and the regularizer .
are these differ - ences crucial ? probably not .
we believe it is much more important to construct a good graph than to choose among the methods .
however graph construction , as we will see later , is not a well studied area .
blum and chawla ( 123 ) pose semi - supervised learning as a graph mincut ( also known as st - cut ) problem .
in the binary case , positive labels act as sources and negative labels act as sinks .
the objective is to nd a minimum set of edges whose removal blocks all ow from the sources to the sinks .
the nodes connecting to the sources are then labeled positive , and those to the sinks are labeled negative .
equiv - alently mincut is the mode of a markov random eld with binary labels ( boltzmann machine ) .
the loss function can be viewed as a quadratic loss with innity weight :
pil ( yi yi|l ) 123 , so that the values on labeled data are in fact clamped
labeling y minimizes
wij|yi yj| =
which can be thought of as a regularizer on binary ( 123 and 123 ) labels .
one problem with mincut is that it only gives hard classication without con - dence .
blum et al .
( 123 ) perturb the graph by adding random noise to the edge weights .
mincut is applied to multiple perturbed graphs , and the labels are deter - mined by a majority vote .
the procedure is similar to bagging , and creates a soft
pang and lee ( 123 ) use mincut to improve the classication of a sentence into either objective or subjective , with the assumption that sentences close to each other tend to have the same class .
gaussian random fields and harmonic functions
the gaussian random elds and harmonic function methods in ( zhu et al . , 123a ) can be viewed as having a quadratic loss function with innity weight , so that the labeled data are clamped , and a regularizer based on the graph combinatorial
( fi yi ) 123 + 123 / 123xi , j ( fi yi ) 123 + f >f
recently grady and funka - lea ( 123 ) applied the harmonic function method to medical image segmentation tasks , where a user labels classes ( e . g .
different or - gans ) with a few strokes .
levin et al .
( 123 ) use essentially harmonic functions for colorization of gray - scale images .
again the user species the desired color with
only a few strokes on the image .
the rest of the image is used as unlabeled data , and the labels propagation through the image .
niu et al .
( 123 ) applied the label propagation algorithm ( which is equivalent to harmonic functions ) to word sense
local and global consistency
the local and global consistency method ( zhou et al . , 123a ) uses the loss function i=123 ( fiyi ) 123 , and the normalized laplacian d123 / 123d123 / 123 = id123 / 123w d123 / 123
in the regularizer ,
wij ( fi / pdii fj / pdjj ) 123 = f >d123 / 123d123 / 123f
the tikhonov regularization algorithm in ( belkin et al . , 123a ) uses the loss func - tion and regularizer :
( fi yi ) 123 + f >sf
where s = or p for some integer p .
for kernel methods , the regularizer is a ( typically monotonically increasing ) func - tion of the rkhs norm ||f||k = f >k123f with kernel k .
such kernels are derived from the graph , e . g .
the laplacian .
chapelle et al .
( 123 ) and smola and kondor ( 123 ) both show the spectral transformation of a laplacian results in kernels suitable for semi - supervised learn - ing .
the diffusion kernel ( kondor & lafferty , 123 ) corresponds to a spectrum transform of the laplacian with
r ( ) = exp (
the regularized gaussian process kernel + i / 123 in ( zhu et al . , 123c ) corre -
similarly the order constrained graph kernels in ( zhu et al . , 123 ) are con - structed from the spectrum of the laplacian , with non - parametric convex opti - mization .
learning the optimal eigenvalues for a graph kernel is in fact a way to
( at least partially ) correct an imprecise graph .
in this sense it is related to graph
spectral graph transducer
the spectral graph transducer ( joachims , 123 ) can be viewed with a loss function
c ( f ) >c ( f ) + f >lf
where i = pl / l+ for positive labeled data , pl+ / l for negative data , l
being the number of negative data and so on .
l can be the combinatorial or nor - malized graph laplacian , with a transformed spectrum .
kemp et al .
( 123 ) dene a probabilistic distribution p ( y |t ) on discrete ( e . g .
123 and 123 ) labelings y over an evolutionary tree t .
the tree t is constructed with the labeled and unlabeled data being the leaf nodes .
the labeled data is clamped .
the authors assume a mutation process , where a label at the root propagates down to the leaves .
the label mutates with a constant rate as it moves down along the edges .
as a result the tree t ( its structure and edge lengths ) uniquely denes the label prior p ( y |t ) .
under the prior if two leaf nodes are closer in the tree , they have a higher probability of sharing the same label .
one can also integrate over all
the tree - based bayes approach can be viewed as an interesting way to incor - porate structure of the domain .
notice the leaf nodes of the tree are the labeled and unlabeled data , while the internal nodes do not correspond to physical data .
this is in contrast with other graph - based methods where labeled and unlabeled data are all the nodes .
some other methods
szummer and jaakkola ( 123 ) perform a t - step markov random walk on the graph .
the inuence of one example to another example is proportional to how easy the random walk goes from one to the other .
it has certain resemblance to the diffusion kernel .
the parameter t is important .
chapelle and zien ( 123 ) use a density - sensitive connectivity distance between nodes i , j ( a given path between i , j consists of several segments , one of them is the longest; now consider all paths between i , j and nd the shortest longest segment ) .
exponentiating the negative distance gives a graph kernel .
bousquet et al .
( 123 ) consider the continuous counterpart of graph - based regularization .
they dene regularization based on a known p ( x ) and provide interesting theoretical analysis .
however there seem to be problems in applying the theoretical results to higher ( d > 123 ) dimensional tasks .
123 . 123 graph construction
although the graph is the heart and soul of graph - based semi - supervised learning methods , its construction has not been studied carefully .
the issue has been dis - cussed informally in chapter 123 , and graph hyperparameter learning discussed in chapter 123
there are relatively few literatures on graph construction .
for example carreira - perpinan and zemel ( 123 ) build robust graphs from multiple minimum spanning trees by perturbation and edge removal .
it is possible that graph construc - tion is domain specic because it encodes prior knowledge , and has thus far been treated on an individual basis .
most graph - based semi - supervised learning algorithms are transductive , i . e .
they cannot easily extend to new test points outside of l u .
recently induction has received increasing attention .
one common practice is to freeze the graph on l u .
new points do not ( although they should ) alter the graph structure .
this avoids expensive graph computation every time one encounters new points .
zhu et al .
( 123c ) propose that new test point be classied by its nearest neigh - bor in lu .
this is sensible when u is sufciently large .
in ( chapelle et al . , 123 ) the authors approximate a new point by a linear combination of labeled and unla - beled points .
similarly in ( delalleau et al . , 123 ) the authors proposes an induction scheme to classify a new point x by
f ( x ) = pilu wxif ( xi )
this can be viewed as an application of the nystrom method ( fowlkes et al . , 123 ) .
in the regularization framework of ( belkin et al . , 123b ) , the function f does not have to be restricted to the graph .
the graph is merely used to regularize f which can have a much larger support .
it is necessarily a combination of an in - ductive algorithm and graph regularization .
the authors give the graph - regularized version of least squares and svm .
note such an svm is different from the graph kernels in standard svm in ( zhu et al . , 123 ) .
the former is inductive with both a graph regularizer and an inductive kernel .
the latter is transductive with only the graph regularizer .
following the work , krishnapuram et al .
( 123 ) use graph
regularization on logistic regression .
these methods create inductive learners that naturally handle new test points .
the harmonic mixture model in chapter 123 naturally handles new points with
the help of a mixture model .
the consistency of graph - based semi - supervised learning algorithms has not been studied extensively according to the authors knowledge .
by consistency we mean whether the classication converges to the right solution as the number of labeled and unlabeled data grows to innity .
recently von luxburg et al .
( 123 ) ( von luxburg et al . , 123 ) study the consistency of spectral clustering methods .
the au - thors nd that the normalized laplacian is better than the unnormalized laplacian for spectral clustering .
the convergence of the eigenvectors of the unnormalized laplacian is not clear , while the normalized laplacian always converges under general conditions .
there are examples where the top eigenvectors of the unnor - malized laplacian do not yield a sensible clustering .
although these are valuable results , we feel the parallel problems in semi - supervised learning needs further study .
one reason is that in semi - supervised learning the whole laplacian ( nor - malized or not ) is often used for regularization , not only the top eigenvectors .
given a large collection of items , and a few query items , ranking orders the items according to their similarity to the queries .
it can be formulated as semi - supervised learning with positive data only ( zhou et al . , 123b ) , with the graph induced simi -
123 . 123 directed graphs
zhou et al .
( 123 ) take a hub / authority approach , and essentially convert a directed graph into an undirected one .
two hub nodes are connected by an undirected edge with appropriate weight if they co - link to authority nodes , and vice versa .
semi - supervised learning then proceeds on the undirected graph .
lu and getoor ( 123 ) convert the link structure in a directed graph into per - node features , and combines them with per - node object features in logistic regres - sion .
they also use an em - like iterative algorithm .
123 . 123 fast computation
fast computation with sparse graphs and iterative methods has been briey dis - cussed in chapter 123
recently numerical methods for fast n - body problems have been applied to dense graphs in semi - supervised learning , reducing the computa - tional cost from o ( n123 ) to o ( n ) ( mahdaviani et al . , 123 ) .
this is achieved with krylov subspace methods and the fast gauss transform .
123 metric - based model selection
metric - based model selection ( schuurmans & southey , 123 ) is a method to detect hypotheses inconsistency with unlabeled data .
we may have two hypotheses which are consistent on l , for example they all have zero training set error .
however they may be inconsistent on the much larger u .
if so we should reject at least one of them , e . g .
the more complex one if we employ occams razor .
the key observation is that a distance metric is dened in the hypothesis space h .
one such metric is the number of different classications two hypotheses make under the data distribution p ( x ) : dp ( h123 , h123 ) = ep ( h123 ( x ) 123= h123 ( x ) ) .
it is easy to verify that the metric satises the three metric properties .
now consider the true classication function h and two hypotheses h123 , h123
since the metric satises the triangle inequality ( the third property ) , we have
dp ( h123 , h123 ) dp ( h123 , h ) + dp ( h , h123 )
under the premise that labels in l is noiseless , lets assume we can approximate dp ( h123 , h ) and dp ( h , h123 ) by h123 and h123s training set error rates dl ( h123 , h ) and dl ( h123 , h ) , and approximate dp ( h123 , h123 ) by the difference h123 and h123 make on a large amount of unlabeled data u : du ( h123 , h123 ) .
we get
du ( h123 , h123 ) dl ( h123 , h ) + dl ( h , h123 )
which can be veried directly .
if the inequality does not hold , at least one of the assumptions is wrong .
if |u| is large enough and u iid p ( x ) , du ( h123 , h123 ) will be a good estimate of dp ( h123 , h123 ) .
this leaves us with the conclusion that at least one of the training errors does not reect its true error .
if both training errors are close to zero , we would know that at least one model is overtting .
an occams razor type of argument then can be used to select the model with less complexity .
such use of unlabeled data is very general and can be applied to almost any learning algorithms .
however it only selects among hypotheses; it does not generate new hypothesis based on unlabeled data .
the co - validation method ( madani et al . , 123 ) also uses unlabeled data for
model selection and active learning .
123 related areas
the focus of the thesis is on classication with semi - supervised methods .
there are some closely related areas with a rich literature .
123 . 123 spectral clustering
spectral clustering is unsupervised .
as such there is no labeled data to guide the process .
instead the clustering depends solely on the graph weights w .
on the other hand semi - supervised learning for classication has to maintain a balance between how good the clustering is , and how well the labeled data can be ex - plained by it .
such balance is expressed explicitly in the regularization framework .
as we have seen in section 123 and 123 . 123 , the top eigenvectors of the graph laplacian can unfold the data manifold to form meaningful clusters .
this is the intuition behind spectral clustering .
there are several criteria on what constitutes a good clustering ( weiss , 123 ) .
the normalized cut ( shi & malik , 123 ) seeks to minimize
n cut ( a , b ) =
assoc ( a , v )
assoc ( b , v )
the continuous relaxation of the cluster indicator vector can be derived from the normalized laplacian .
in fact it is derived from the second smallest eigenvector of the normalized laplacian .
the continuous vector is then discretized to obtain the
the data points are mapped into a new space spanned by the rst k eigenvec - tors of the normalized laplacian in ( ng et al . , 123a ) , with special normalization .
clustering is then performed with traditional methods ( like k - means ) in this new space .
this is very similar to kernel pca .
fowlkes et al .
( 123 ) use the nystrom method to reduce the computation cost for large spectral clustering problems .
this is related to our method in chapter 123
chung ( 123 ) presents the mathematical details of spectral graph theory .
123 . 123 clustering with side information
this is the opposite of semi - supervised classication .
the goal is clustering but there are some labeled data in the form of must - links ( two points must in the same cluster ) and cannot - links ( two points cannot in the same cluster ) .
there is a tension between satisfying these constraints and optimizing the original clustering criterion ( e . g .
minimizing the sum of squared distances within clusters ) .
procedurally one can modify the distance metric to try to accommodate the constraints , or one can
bias the search .
we refer readers to a recent short survey ( grira et al . , 123 ) for the
123 . 123 nonlinear dimensionality reduction
the goal of nonlinear dimensionality reduction is to nd a faithful low dimensional mapping of the high dimensional data .
as such it belongs to unsupervised learning .
however the way it discovers low dimensional manifold within a high dimensional space is closely related to spectral graph semi - supervised learning .
representative methods include isomap ( tenenbaum et al . , 123 ) , locally linear embedding ( lle ) ( roweis & saul , 123 ) ( saul & roweis , 123 ) , hessian lle ( donoho & grimes , 123 ) , laplacian eigenmaps ( belkin & niyogi , 123 ) , and semidenite embedding ( sde ) ( weinberger & saul , 123 ) ( weinberger et al . , 123 ) ( weinberger et al . ,
123 . 123 learning a distance metric
many learning algorithms depend , either explicitly or implicitly , on a distance met - ric on x .
we use the term metric here loosely to mean a measure of distance or ( dis ) similarity between two data points .
the default distance in the feature space may not be optimal , especially when the data forms a lower dimensional manifold in the feature vector space .
with a large amount of u , it is possible to detect such manifold structure and its associated metric .
the graph - based methods above are based on this principle .
we review some other methods next .
the simplest example in text classication might be latent semantic indexing ( lsi , a . k . a .
latent semantic analysis lsa , principal component analysis pca , or sometimes singular value decomposition svd ) .
this technique denes a lin - ear subspace , such that the variance of the data , when projected to the subspace , is maximumly preserved .
lsi is widely used in text classication , where the orig - inal space for x is usually tens of thousands dimensional , while people believe meaningful text documents reside in a much lower dimensional space .
zelikovitz and hirsh ( 123 ) and cristianini et al .
( 123b ) both use u , in this case unlabeled documents , to augment the term - by - document matrix of l .
lsi is performed on the augmented matrix .
this representation induces a new distance metric .
by the property of lsi , words that co - occur very often in the same documents are merged into a single dimension of the new space .
in the extreme this allows two docu - ments with no common words to be close to each other , via chains of co - occur word pairs in other documents .
probabilistic latent semantic analysis ( plsa ) ( hofmann , 123 ) is an impor - tant improvement over lsi .
each word in a document is generated by a topic ( a
multinomial , i . e .
unigram ) .
different words in the document may be generated by different topics .
each document in turn has a xed topic proportion ( a multino - mial on a higher level ) .
however there is no link between the topic proportions in
latent dirichlet allocation ( lda ) ( blei et al . , 123 ) is one step further .
assumes the topic proportion of each document is drawn from a dirichlet distribu - tion .
with variational approximation , each document is represented by a posterior dirichlet over the topics .
this is a much lower dimensional representation .
some algorithms derive a metric entirely from the density of u .
these are mo - tivated by unsupervised clustering and based on the intuition that data points in the same high density clump should be close in the new metric .
for instance , if u is generated from a single gaussian , then the mahalanobis distance induced by the covariance matrix is such a metric .
tipping ( 123 ) generalizes the mahalanobis distance by tting u with a mixture of gaussian , and dene a riemannian mani - fold with metric at x being the weighted average of individual component inverse covariance .
the distance between x123 and x123 is computed along the straight line ( in euclidean space ) between the two points .
rattray ( 123 ) further generalizes the metric so that it only depends on the change in log probabilities of the density , not on a particular gaussian mixture assumption .
and the distance is computed along a curve that minimizes the distance .
the new metric is invariate to linear transfor - mation of the features , and connected regions of relatively homogeneous density in u will be close to each other .
such metric is attractive , yet it depends on the homogeneity of the initial euclidean space .
their application in semi - supervised learning needs further investigation .
we caution the reader that the metrics proposed above are based on unsuper - vised techniques .
they all identify a lower dimensional manifold within which the data reside .
however the data manifold may or may not correlate with a particular classication task .
for example , in lsi the new metric emphasizes words with prominent count variances , but ignores words with small variances .
if the classi - cation task is subtle and depends on a few words with small counts , lsi might wipe out the salient words all together .
therefore the success of these methods is hard to guarantee without putting some restrictions on the kind of classication tasks .
it would be interesting to include l into the metric learning process .
in a separate line of work , baxter ( 123 ) proves that there is a unique optimal metric for classication if we use 123 - nearest - neighbor .
the metric , named canoni - cal distortion measure ( cdm ) , denes a distance d ( x123 , x123 ) as the expected loss if we classify x123 with x123s label .
the distance measure proposed in ( yianilos , 123 ) can be viewed as a special case .
yianilos assume a gaussian mixture model has been learned from u , such that a class correspond to a component , but the corre - spondence is unknown .
in this case cdm d ( x123 , x123 ) = p ( x123 , x123from same component )
and can be computed analytically .
now that a metric has been learned from u , we can nd within l the 123 - nearest - neighbor of a new data point x , and classify x with the nearest neighbors label .
it will be interesting to compare this scheme with em based semi - supervised learning , where l is used to label mixture components .
weston et al .
( 123 ) propose the neighborhood mismatch kernel and the bagged mismatch kernel .
more precisely both are kernel transformation that modies an input kernel .
in the neighborhood method , one denes the neighborhood of a point as points close enough according to certain similarity measure ( note this is not the measure induced by the input kernel ) .
the output kernel between point i , j is the average of pairwise kernel entries between is neighbors and js neighbors .
in bagged method , if a clustering algorithm thinks they tend to be in the same cluster ( note again this is a different measure than the input kernel ) , the corresponding entry in the input kernel is boosted .
inferring label sampling mechanisms
most semi - supervised learning methods assume l and u are both i . i . d .
from the underlying distribution .
however as ( rosset et al . , 123 ) points out that is not always the case .
for example y can be the binary label whether a customer is satised , obtained through a survey .
it is conceivable survey participation ( and thus labeled data ) depends on the satisfaction y .
let si be the binary missing indicator for yi .
the authors model p ( s|x , y ) with a parametric family .
the goal is to estimate p ( s|x , y ) which is the label sampling mechanism .
this is done by computing the expectation of an arbi - trary function g ( x ) in two ways : on l u as 123 / npn i=123 g ( xi ) , and on l only as 123 / npil g ( xi ) / p ( si = 123|xi , yi ) .
by equating the two p ( s|x , y ) can be estimated .
the intuition is that the expectation on l requires weighting the labeled samples inversely proportional to the labeling probability , to compensate for ignoring the
we have presented a series of semi - supervised learning algorithms , based on a graph representation of the data .
experiments show that they are able to take ad - vantage of the unlabeled data to improve classication .
contributions of the thesis
we proposed a harmonic function and gaussian eld formulations for semi - supervised problems .
this is not the rst graph - based semi - supervised method .
the rst one was graph mincut .
however our formulation is a continuous relaxation to the discrete labels , resulting in a more benign problem .
sev - eral variations of the formulation were proposed independently by different groups shortly after .
we addressed the problem of graph construction , by setting up parametric edge weights and performing edge hyperparameter learning .
since the graph is the input to all graph - based semi - supervised algorithms , it is important that we construct graphs that best suit the task .
we combined an active learning scheme that reduces expected error instead of ambiguity , with graph - based semi - supervised learning .
we believe that active learning and semi - supervised learning will be used together for prac - tical problems , because limited human annotation resources should be spent
we dened optimal semi - supervised kernels by spectral transformation of the graph laplacian .
such optimal kernels can be found with convex opti - mization .
we can use the kernels with any kernel machine , e . g .
support vec - tor machines , for semi - supervised learning .
the kernel machines in general can handle noisy labeled data , which is an improvement over the harmonic
we kernelized conditional random elds .
crfs were traditionally feature based .
we derived the dual problem and presented an algorithm for fast sparse kernel crf training .
with kernel crfs , it is possible to use a semi - supervised kernel on instances for semi - supervised learning on sequences and other structures .
we proposed to solve large - scale problems with harmonic mixtures .
har - monic mixtures reduce computation cost signicantly by grouping unlabeled data into soft clusters , then carrying out semi - supervised learning on the coarser data representation .
harmonic mixtures also handle new data points naturally , making the semi - supervised learning method inductive .
semi - supervised learning is a relatively new research area .
there are many
open questions and research opportunities :
the graph is the single most important quantity for graph - based semi - supervised
learning .
parameterizing graph edge weights , and learning weight hyperpa - rameters , should be the rst step of any graph - based semi - supervised learn - ing methods .
current methods in chapter 123 are not efcient enough .
can we nd better ways to learn the graph structure and parameters ?
real problems can have millions of unlabeled data points .
anecdotal sto - ries and experiments in appendix f indicate that conjugate gradient with a suitable pre - conditioner is one of the fastest algorithms in solving harmonic functions .
harmonic mixture works along an orthogonal direction by reduc - ing the problem size .
how large a dataset can we process if we combine conjugate gradient and harmonic mixture ? what can we do to handle even
semi - supervised learning on structured data , e . g .
sequences and trees , is largely unexplored .
we have proposed the use of kernel conditional ran - dom elds plus semi - supervised kernels .
much more work is needed in this
in this thesis we focused on classication problems .
the spirit of combining some human effort with large amount of data should be applicable to other problems .
examples include : regression with both labeled and unlabeled data; ranking with ordered pairs and unlabeled data; clustering with cluster membership knowledge .
what can we do beyond classication ?
because labeled data is scarce , semi - supervised learning methods depend more heavily on their assumptions ( see e . g .
table 123 ) .
can we develop novel semi - supervised learning algorithms with new assumptions ?
applications of semi - supervised learning are emerging rapidly .
these in - clude text categorization , natural language processing , bioinformatics , im - age processing , and computer vision .
many others are sure to come .
appli - cations are attractive because they solve important practical problems , and provide fertile test bed for new ideas in machine learning .
what problems can we apply semi - supervised learning ? what applications were too hard but are now feasible with semi - supervised learning ?
the theory of semi - supervised learning is almost absent in both the ma - chine learning literature and the statistics literature .
is graph - based semi - supervised learning consistent ? how many labeled and unlabeled points are needed to learn a concept with condence ?
we expect advances in research will address these questions .
we hope semi - supervised learning become a fruitful area for both machine learning theory and
the harmonic function after knowing one more label
uu ulfl = 123
construct the graph as usual .
we use f to denote the harmonic function .
the random walk solution is fu = 123 uu wulfl .
there are u unlabeled nodes .
we ask the question : what is the solution if we add a node with value f123 to the graph , and connect the new node to unlabeled node i with weight w123 ? the new node is a dongle attached to node i .
besides the usage here , dongle nodes can be useful for handling noisy labels where one would put the observed labels on the dongles , and infer the hidden true labels for the nodes attached to dongles .
note that when w123 , we effectively assign label f123 to node i .
since the dongle is a labeled node in the augmented graph ,
ul f +
l = ( d+
ul f + = ( w123ee> + duu wuu ) 123 ( w123f123e + wulfl ) = ( w123ee> + uu ) 123 ( w123f123e + wulfl )
uu wuu ) 123w +
where e is a column vector of length u with 123 in position i and 123 elsewhere .
note that we can use the matrix inversion lemma here , to obtain
( w123ee> + uu ) 123 = 123
123 + ( w123e ) >123
123 + w123gii
where we use the shorthand g = 123 uu ( the greens function ) ; gii is the i - th row , i - th column element in g; g|i is a square matrix with gs i - th column and 123 else -
some calculation gives
u = fu +
123 + w123gii
where fi is the unlabeled nodes original solution , and gi is the i - th column vector in g .
if we want to pin down the unlabeled node to value f123 , we can let w123
u = fu +
the inverse of a matrix with one
let a be an n n non - singular matrix .
given a123 , we would like a fast algorithm to compute a123 i , where ai is the ( n 123 ) ( n 123 ) matrix obtained by removing the i - th row and column from a .
let b = perm ( a , i ) be the matrix created by moving the i - th row in front of
the 123st row , and the i - th column in front of the 123st column of a
i = ( perm ( a , i ) 123 ) 123 = ( b123 ) 123
also note b123 = perm ( a123 , i ) .
so we only need to consider the special case of
where b123 = ( b123 .
b123n ) and b123 = ( b123 .
we will transform b into a
removing the rst row / column of a matrix .
write b out as b = ( cid : 123 ) b123 b123 b123 b123 ( cid : 123 ) , block diagonal form in two steps .
first , let b123 = ( cid : 123 ) 123 b123 b123 ( cid : 123 ) = b +uv> where u = ( 123 , 123 , .
, 123 ) > and v = ( b123 123 , b123 ) > .
we are interested in ( b123 ) 123 which will be used in the next step .
by the matrix inversion lemma ( sherman - morrison -
( b123 ) 123 = ( b + uv> ) 123 = b123 123 b123 ( cid : 123 ) = b123 + wu> where w = ( 123 , b123 ) > .
applying the
123 + v>b123u
next let b123 = ( cid : 123 ) 123
matrix inversion lemma again ,
( b123 ) 123 = ( b123 + wu> ) 123 = ( b123 ) 123
123 + u> ( b123 ) 123w
but since b123 is block diagonal , we know ( b123 ) 123 = ( cid : 123 ) 123
123 ( b123 ) 123 ( cid : 123 ) .
therefore
( b123 ) 123 = ( ( b123 ) 123 ) 123
laplace approximation for
this derivation largely follows ( herbrich , 123 ) ( b . 123 ) .
the gaussian process model , restricted to the labeled and unlabeled data , is
f n ( cid : 123 ) , 123 ( cid : 123 )
we will use g = 123 to denote the covariance matrix ( i . e .
the gram matrix ) .
let y ( 123 , +123 ) be the observed discrete class labels .
the hidden variable f and labels y are connected via a sigmoid noise model
p ( yi|fi ) =
efiyi + efiyi
123 + e123fiyi
where is a hyperparameter which controls the steepness of the sigmoid .
given the prior and the noise model , we are interested in the posterior p ( fl , fu|yl )
p ( fl , fu|yl ) = ql
i=123 p ( yi|fi ) p ( fl , fu )
because of the noise model , the posterior is not gaussian and has no closed form solution .
we use the laplace approximation .
first , we nd the mode of the posterior ( 123 ) :
( fl , fu ) = arg maxfl , fuql
= arg maxfl , fu
= arg maxfl , fu
q123 + q123
i=123 p ( yi|fi ) p ( fl , fu )
ln p ( yi|fi ) + ln p ( fl , fu )
note fu only appears in q123 , and we can maximize fu independently given fl .
q123 is the log likelihood of the gaussian ( c . 123 ) .
therefore given fl , fu follows the conditional distribution of gaussian :
p ( fu| fl ) = n ( cid : 123 ) gu lg123
fl , gu u gu lg123
moreover , the mode is the conditional mean
fu = gu lg123
its easy to see ( c . 123 ) has the same form as the solution for gaussian fields ( 123 ) : recall g = 123
from partitioned matrix inversion theorem ,
u u = s123
u l = s123
a gu lg123
where sa = gu u gu l ( gll ) 123glu is the schur complement of gll
( u u ) 123 u l = sas123
a gu lg123
ll = gu lg123
thus we have
fu = 123
u l fl u u wu l fl
which has the same form as the harmonic energy minimizing function in ( zhu et al . , 123a ) .
in fact the latter is the limiting case when 123 and there is no noise substitute ( c . 123 ) back to q123 , using partitioned inverse of a matrix , it can be
shown that ( not surprisingly )
llfl + c
now go back to q123
the noise model can be written as
p ( yi|fi ) =
efiyi + efiyi
efi + efi ( cid : 123 ) yi+123
efi + efi ( cid : 123 ) 123yi
123 ( 123 ( fi ) )
ln p ( yi|fi )
yi + 123
ln ( fi ) +
ln ( 123 + e123fi )
= ( yl 123 ) >fl
put it together ,
fl = arg maxq123 + q123
= arg max ( yl 123 ) >fl
to nd the mode , we take the derivative ,
ln ( 123 + e123fi )
( q123 + q123 )
= ( yl 123 ) + 123 ( 123 ( fl ) ) g123
because of the term ( fl ) it is not possible to nd the root directly .
we solve it with newton - raphson algorithm ,
where h is the hessian matrix ,
l h 123 ( q123 + q123 )
h = " 123 ( q123 + q123 )
( fi ) = 123 ( fi ) ( 123 ( fi ) ) , we can write h as
h = g123
where p is a diagonal matrix with elements pii = 123 ( fi ) ( 123 ( fi ) ) .
once newton - raphson converges we compute fu from fl with ( c . 123 ) .
classi - cation can be done with sgn ( fu ) noting this is the bayesian classication rule with gaussian distribution and sigmoid noise model .
to compute the covariance matrix of the laplace approximation , note by de -
nition the inverse covariance matrix of the laplace approximation is
from ( 123 ) it is straightforward to conrm
123 = " 123 ln p ( f|y )
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) fl , fu# 123 123 ( cid : 123 ) + g123 = ( cid : 123 ) p 123 123 123 ( cid : 123 ) +
123 = ( cid : 123 ) p 123
therefore the covariance matrix is
= ( cid : 123 ) ( cid : 123 ) p 123
123 123 ( cid : 123 ) + ( cid : 123 ) 123
where p is evaluated at the mode fl .
hyperparameter learning by
this derivation largely follows ( williams & barber , 123 ) .
we want to nd the map hyperparameters which maximize the posterior
the prior p ( ) is usually chosen to be simple , and so we focus on the term p ( yl| ) , known as the evidence .
the denition
p ( yl| ) =z p ( yl|fl ) p ( fl| ) dfl
is hard to compute analytically .
however notice
since it holds for all fl , it holds for the mode of the laplace approximation fl :
p ( yl| fl ) p ( fl| )
p ( fl|yl , )
the terms on the numerator are straightforward to compute; the denominator is tricky .
however we can use the laplace approximation , i . e .
the probability density at the mode : p ( fl|yl , ) n ( fl| fl , ll ) .
recall 123 123 ( cid : 123 ) + ( cid : 123 ) gll glu
gu l gu u ( cid : 123 ) 123 ! 123
= ( cid : 123 ) p 123
by applying schur complement in block matrix decomposition twice , we nd
ll = ( p + g123
therefore the evidence is
p ( yl| fl ) p ( fl| ) n ( fl| fl , ll ) p ( yl| fl ) p ( fl| ) 123 |ll| 123 p ( yl| fl ) p ( fl| ) 123 | ( p + g123
switching to log domain , we have
log p ( yl| ) ( fl ) + = ( fl ) +
log 123 +
log |p + g123
where ( fl ) = log p ( yl|fl ) + log p ( fl| ) .
since f n ( cid : 123 ) , 123 ( cid : 123 ) = n ( , g ) , we have fl n ( l , gll ) .
therefore
( fl ) = log p ( yl| fl ) + log p ( fl| )
log ( 123 + exp ( 123 fiyi ) )
( fl l ) >g123
ll ( fl l ) ( d . 123 )
put it together ,
log ( 123 + exp ( 123 fiyi ) )
log ( 123 + exp ( 123 fiyi ) )
( fl l ) >g123
ll ( fl l )
( fl l ) >g123
ll ( fl l )
log |p + g123
log |gllp + i|
this gives us a way to ( approximately ) compute the evidence .
to nd the map estimate of ( which can have multiple local maxima ) , we use gradient methods .
this involves the derivatives of the evidence log p ( yl| ) / , where is the hyperparameter , , or the ones controlling w .
we start from
( fi ) =
123 + e123 fi
= 123 ( fi ) ( 123 ( fi ) ) ( fi
to compute fl / , note the laplace approximation mode fl satises
fl ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) fl
= ( yl + 123 123 ( fl ) ) g123
ll ( fl l ) = 123
fl = gll ( yl + 123 123 ( fl ) ) + l
taking derivatives on both sides ,
gll ( yl + 123 123 ( fl ) )
( yl + 123 123 ( fl ) ) 123gll ( yl + 123 123 ( fl ) )
= ( i + gllp ) 123 ( cid : 123 ) gll
( yl + 123 123 ( fl ) )
now it is straightforward to compute the gradient with ( d . 123 ) :
( fl l ) >g123
log ( 123 + exp ( 123 fiyi ) )
123 + exp ( 123 fiyi ) ll ( fl l ) ) > fl + ( fl l ) > g123 tr ( cid : 123 ) ( gllp + i ) 123 gllp = tr ( cid : 123 ) a123 a
where we used the fact
ll ( fl l )
log |gllp + i|#
( fl l ) #
for example , if = , the gradient can be computed by noting gll
= 123 ( fi ) ( 123 ( fi ) ) +
= 123 , and gllp
= 123 , g123 123 ( 123 123 ( fi ) ) ( fi )
= 123 , g123
for = , we have gll = 123 ( fi ) ( 123 ( fi ) ) ( 123 ( fi ) ) fi for = , the computation is more intensive because the complex depen - ( cid : 123 ) ll .
using the fact a123 dency between g and .
we start from gll = / 123g123
note the computation in - volves the multiplication of the full matrix g and is thus more demanding
a123 and g = 123 , we get g
is computed the rest is easy .
if we parameterize the weights w in gaussian fields with radial basis func - tions ( for simplicity we assume a single length scale parameter for all dimen - sions .
extension to multiple length scales is simple ) ,
wij = exp
where dij is the euclidean distance between xi , xj in the original feature space , we can similarly learn the hyperparameter .
note wij
the rest is the same as for above .
similarly with a tanh ( ) - weighted weight function wij = ( tanh ( 123 ( dij
= ( 123 tanh123 ( 123 ( dij 123 ) ) ) ( dij 123 ) / 123 and wij
123 ) ) + 123 ) / 123 , we have wij ( 123 tanh123 ( 123 ( dij 123 ) ) ) 123 / 123 , and the rest follows .
mean field approximation for kernel crf training
in the basic kernel crf model , each clique c is associated with |y||c| parameters j ( yc ) .
even if we only consider vertex cliques , there would be hundreds of thou - sands of parameters for a typical protein dataset .
this seriously affects the training
to solve the problem , we adopt the notion of import vector machines by zhu and hastie ( 123 ) .
that is , we use a subset of the training examples instead of all of them .
the subset is constructed by greedily selecting training examples one at a time to minimize the loss function :
arg minkr ( fa ( k ) , ) r ( fa , ) fa ( x , y ) =xja
and a is the current active import vector set .
( e . 123 ) is hard to compute : we need to update all the parameters for fa ( k ) .
even if we keep old parameters in fa xed , we still need to use expensive forward - backward algorithm to train the new parameters k ( y ) and compute the loss .
fol - lowing mccallum ( 123 ) , we make a set of speed up approximations .
approximation 123 : mean eld approximation .
with the old fa we have an a ( x , y ) ) over a label sequence y
old distribution p ( y|x ) = 123 / z exp ( pc f c approximate p ( y|x ) by the mean eld
the mean eld approximation is the independent product of marginal distribu - tions at each position i .
it can be computed with the forward - backward algorithm on p ( y|x ) .
approximation 123 : consider only the vertex kernel .
in conjunction with the mean eld approximation , we only consider the vertex kernel k ( xi , xj ) and ignore edge or other higher order kernels .
the loss function becomes
log po ( yi|xi ) +
r ( fa , ) = xit where t = ( 123 , .
, m ) is the set of training positions on which to evaluate the loss function .
once we add a candidate import vector xk to the active set , the new
po ( yi|xi ) exp ( k ( yi ) k ( xi , xk ) )
py po ( y|xi ) exp ( k ( y ) k ( xi , xk ) )
log pn ( yi|xi ) +
the new loss function is
r ( fa ( k ) , ) = xit
and ( e . 123 ) can be written as
r ( fa ( k ) , ) r ( fa , ) = xit
po ( y|xi ) exp ( k ( y ) k ( xi , xk ) )
j ( y ) k ( y ) k ( xj , xk ) +
this change of loss is a convex function of the |y| parameters k ( y ) .
we can nd the best parameters with newtons method .
the rst order derivatives are
r ( fa ( k ) , ) r ( fa , )
k ( xi , xk ) ( yi , y )
and the second order derivatives are 123r ( fa ( k ) , ) r ( fa , )
= xit ( cid : 123 ) pn ( y|xi ) k123 ( xi , xk ) ( y , y123 ) pn ( y|xi ) k123 ( xi , xk ) pn ( y123|xi ) ( cid : 123 )
+k ( xk , xk ) ( y , y123 )
approximation 123 and 123 allow us to estimate the change in loss function inde - pendently for each position in t .
this avoids the need of dynamic programming .
although the time complexity to evaluate each candidate xk is still linear in |t| , we save by a ( potentially large ) constant factor .
further more , they allow a more dramatic approximation as shown next .
approximation 123 : sparse evaluation of likelihood .
a typical protein database has around 123 sequences , with hundreds of amino acid residuals per sequence .
therefore m , the total number of training positions , can easily be around 123 , 123
normally t = ( 123 , .
, m ) , i . e .
we need to sum over all training positions to evaluate the log - likelihood .
however we can speed up by reducing t .
there are
focus on errors : t = ( i|yi 123= arg maxypo ( y|xi ) ) 123
focus on low condence : t = ( i|po ( yi|xi ) < p123 ) 123
skip positions : t = ( ai|ai m ; a , i n ) 123
random sample : t = ( i|i unif orm ( 123 , m ) ) 123
error / condence guided sample : errors / low condence positions have higher
probability to be sampled .
we need to scale the log likelihood term to maintain the balance between it and the
r ( fa , ) =
log po ( yi|xi ) +
and scale the derivatives accordingly .
i ( y ) j ( y ) k ( xi , xj ) ( e . 123 )
other approximations : we may want to add more than one candidate import vector to a at a time .
however we need to eliminate redundant vectors , possibly by the kernel distance .
we may not want to fully train fa ( k ) once we selected k .
an empirical comparison of
the single most signicant bottleneck in computing the harmonic function is to invert a u u matrix , as in fu = 123 uu ulfl .
done naively the cost is close to o ( n123 ) , which is prohibitive for practical problems .
for example matlab inv ( ) function can only handle n in the range of several thousand .
clearly , we need to nd ways to avoid the expensive inversion .
one can go several directions :
one can approximate the inversion of a matrix by its top few eigenvalues and eigenvectors .
if a n n invertible matrix a has spectrum decomposition i .
the top m < n eigenvectors i with the smallest eigenvalues i is less expensive to compute than inverting the matrix .
this has been used in non - parametric transforms of graph kernels for semi - supervised learning in chapter 123
a similar approximation is used in ( joachims , 123 ) .
we will not pursue it
i , then a123 =pn
one can reduced the problem size .
instead of using all of the unlabeled data , we can use a subset ( or clusters ) to construct the graph .
the harmonic solution on the remaining data can be approximated with a computationally cheap method .
the backbone graph in chapter 123 is an example .
one can use iterative methods .
the hope is that each iteration is o ( n ) and convergence can be reached in relatively few iterations .
there is a rich set of iterative methods applicable .
we will compare the simple label propagation algorithm , loopy belief propagation and conjugate gradient next .
f . 123 label propagation
the original label propagation algorithm was proposed in ( zhu & ghahramani , 123a ) .
a slightly modied version is presented here .
let p = d123w be the transition matrix .
let fl be the vector for labeled set ( for multiclass problems it can be an l c matrix ) .
the label propagation algorithm consists of two steps : 123
f ( t+123 )
! = p f ( t )
clamp the labeled data f ( t+123 )
it can be shown fu converges to the harmonic solution regardless of initialization .
each iteration needs a matrix - vector multiplication , which can be o ( n ) for sparse graphs .
however the convergence may be slow .
f . 123 conjugate gradient
the harmonic function is the solution to the linear system
uufu = ulfl
standard conjugate gradient methods have been shown to perform well ( argyriou , 123 ) .
in particular , the use of jacobi preconditioner was shown to improve con - vergence .
the jacobi preconditioner is simply the diagonal of uu , and the pre - conditioned linear system is
diag ( uu ) 123uufu = diag ( uu ) 123ulfl
we note this is exactly
the alternative denition of harmonic function fu = ( ipuu ) 123pulfl , where p = d123w is the transition matrix .
( i puu ) fu = pulfl
f . 123 loopy belief propagation on gaussian elds
the harmonic solution
fu = 123
computes the mean of the marginals on unlabeled nodes u .
is the graph lapla - cian .
the computation involves inverting a u u matrix and is expensive for large
datasets .
we hope to use loopy belief propagation instead , as each iteration is o ( n ) if the graph is sparse , and loopy bp has a reputation of converging fast ( weiss & freeman , 123 ) ( sudderth et al . , 123 ) .
it has been proved that if loopy bp con - verges , the mean values are correct ( i . e .
the harmonic solution ) .
the gaussian eld is dened as
and fu = ep ( yu ) .
note the corresponding pairwise clique representation is
( yiyj ) ( cid : 123 ) a b
c d ( cid : 123 ) ( cid : 123 ) yi
where a = d = wij , b = c = wij , and wij is the weight of edge ij .
notice in this simple model we dont have n nodes for hidden variables and another n for observed ones; we only have n nodes with some of them observed .
in other words , there is no noise model .
the standard belief propagation messages are
mij ( yj ) = zyi
ij ( yi , yj ) ykn ( i ) \j
where mij is the message from i to j , n ( i ) \j is the neighbors of i except j , and a normalization factor .
initially the messages are arbitrary ( e . g .
uniform ) except for observed nodes yl = fl , whose messages to their neighbors are
after the messages converge , the marginals ( belief ) is computed as
mlj ( yj ) = ij ( yl , yj )
b ( yi ) = ykn ( i )
for gaussian elds with scalar - valued nodes , each message mij can be param - eterized similar to a gaussian distribution by its mean ij and inverse variance ( precision ) pij = 123 / 123
ij parameters .
that is ,
we derive the belief propagation iterations for this special case next .
ij ( yi , yj ) ykn ( i ) \j ( yiyj ) ( cid : 123 ) a b c d ( cid : 123 ) ( cid : 123 ) yi 123 ( yiyj ) ( cid : 123 ) a b c d ( cid : 123 ) ( cid : 123 ) yi = 123 exp ( cid : 123 ) 123a + xkn ( i ) \j
( xi ki ) 123pki dyi pkiki yi dyi where we use the fact b = c .
let a = a+pkn ( i ) \j pki , b = byjpkn ( i ) \j pkiki ,
yj ( cid : 123 ) ( cid : 123 ) ykn ( i ) \j yj ( cid : 123 ) + xkn ( i ) \j i + 123byj xkn ( i ) \j
= 123 exp ( cid : 123 ) = 123 exp ( cid : 123 ) = 123 exp ( cid : 123 )
i + 123byi ( cid : 123 ) ( cid : 123 ) dyi 123 ( cid : 123 ) ( ayi + b / a ) 123 b123 / a ( cid : 123 ) ( cid : 123 ) dyi 123 ( cid : 123 ) ( ayi + b / a ) 123 ( cid : 123 ) ( cid : 123 ) dyi
note the integral is gaussian whose value depends on a , not b .
however since a is constant w . r . t .
yj , the integral can be absorbed into the normalization factor ,
= 123 exp ( cid : 123 ) = 123 exp " = 123 exp "
j 123bpkn ( i ) \j pkikiyj + ( pkn ( i ) \j pkiki ) 123 a +pkn ( i ) \j pki ! y123
bpkn ( i ) \j pkiki a +pkn ( i ) \j pki
a +pkn ( i ) \j pki
let c = d
a+pkn ( i ) \j pki = 123 exp ( cid : 123 ) = 123 exp ( cid : 123 ) = 123 exp ( cid : 123 ) = 123 exp ( cid : 123 )
bpkn ( i ) \j pkiki a+pkn ( i ) \j pki j + 123dyj ( cid : 123 ) ( cid : 123 )
123 ( cid : 123 ) ( cid : 123 ) cyj + d / c ( cid : 123 ) 123 123 ( cid : 123 ) ( cid : 123 ) cyj + d / c ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) ( yj ( d / c ) ) 123 c ( cid : 123 ) ( cid : 123 )
thus we see the message mij has the form of a gaussian density with sufcient
pij = c
ij = d / c
a +pkn ( i ) \j pki bpkn ( i ) \j pkiki a +pkn ( i ) \j pki
for our special case of a = d = wij , b = c = wij , we get wij +pkn ( i ) \j pki
pij = wij ij = d / c
wijpkn ( i ) \j pkiki wij +pkn ( i ) \j pki
for observed nodes yl = fl , they ignore any messages sent to them , while sending out the following messages to their neighbors j :
lj = fl plj = wlj
the belief at node i is
= ykn ( i ) = 123 exp = 123 exp
123 xkn ( i ) 123 xkn ( i ) i 123 xkn ( i ) 123 yi pkn ( i ) pkiki pkn ( i ) pki ! 123 i = pkn ( i ) pkiki pkn ( i ) pki pi = xkn ( i )
this is a gaussian distribution with mean and inverse variance
f . 123 empirical results
we compare label propagation ( lp ) , loopy belief propagation ( loopy bp ) , conju - gate gradient ( cg ) and preconditioned conjugate gradient ( cg ( p ) ) on eight tasks .
the tasks are small because we want to be able to compute the closed form solution fu with matrix inversion .
lp is coded in matlab with sparse matrix .
loopy bp is implemented in c .
cg and cg ( p ) use matlab cgs ( ) function .
figure f . 123 compares the mean squared errorpiu ( cid : 123 ) f ( t ) ( i ) fu ( i ) ( cid : 123 ) 123 with dif -
ferent methods at iteration t .
we assume that with good implementation , the cost per iteration for different methods is similar .
for multiclass tasks , it shows the binary sub - task of the rst class vs .
the rest .
note the y - axis is in log scale .
we observe that loopy bp always converges reasonably fast; cg ( p ) can catch up and come closest to the closed form solution quickly , however sometimes it does not converge ( d , e , f ) ; cg is always worse than cg ( p ) ; lp converges very slowly .
for classication purpose we do not need to wait for f ( t )
quantity of interest is when does f ( t ) form solution fu .
for the binary case this means f ( t )
u to converge .
another u give the same classication as the closed u and fu are on the same side
( a ) 123 vs
( c ) odd vs
( e ) pc vs
( b ) ten digits
( d ) baseball vs .
hockey
( f ) religion vs .
atheism
figure f . 123 : mean squared error to the harmonic solution with various iterative methods : loopy belief propagation ( loopy bp ) , conjugate gradient ( cg ) , conjugate gradient with jacobi preconditioner ( cg ( p ) ) , and label propagation ( lp ) .
note the
baseball vs .
hockey
one vs .
two odd vs
religion vs .
atheism 123
lp closed form
table f . 123 : average run time per iteration for loopy belief propagation ( loopy bp ) , conjugate gradient ( cg ) , conjugate gradient with jacobi preconditioner ( cg ( p ) ) , and label propagation ( lp ) .
also listed is the run time for closed form solution .
time is in seconds .
loopy bp is implemented in c , others in matlab .
of 123 , if labels are 123 and 123
we dene classication agreement as the percentage of unlabeled data whose f ( t ) u and fu have the same label .
note this is not classication accuracy .
ideally agreement should reach 123% long before f ( t ) u converges .
figure f . 123 compares the agreement .
note x - axis is in log scale .
all methods quickly reach classication agreement with the closed form solution , except cg and cg ( p ) sometimes do not converge; task ( f ) has only 123% agreement .
since loopy bp code is implemented in c and others in matlab , their speed may not be directly comparable .
nonetheless we list the average per - iteration run time of different iterative methods in table f . 123
also listed are the run time of the closed form solution with matlab inv ( ) .
( a ) 123 vs
( c ) odd vs
( e ) pc vs
( b ) ten digits
( d ) baseball vs .
hockey
( f ) religion vs .
atheism
figure f . 123 : classication agreement to the closed form harmonic solution with various iterative methods : loopy belief propagation ( loopy bp ) , conjugate gradient ( cg ) , conjugate gradient with jacobi preconditioner ( cg ( p ) ) , and label propaga - tion ( lp ) .
note the log - scale x - axis .
