we study semi - supervised learning when the data consists of multiple intersecting mani - folds .
we give a nite sample analysis to quantify the potential gain of using unlabeled data in this multi - manifold setting .
we then propose a semi - supervised learning algorithm that separates dierent manifolds into deci - sion sets , and performs supervised learning within each set .
our algorithm involves a novel application of hellinger distance and size - constrained spectral clustering .
exper - iments demonstrate the benet of our multi - manifold semi - supervised learning approach .
the promising empirical success of semi - supervised learning algorithms in favorable situations has trig - gered several recent attempts ( balcan & blum 123 , ben - david , lu & pal 123 , kaariainen 123 , laf - ferty & wasserman 123 , niyogi 123 , rigollet 123 ) at developing a theoretical understanding of semi - supervised learning .
in a recent paper ( singh , nowak & zhu 123 ) , it was established using a nite sam - ple analysis that if the complexity of the distribu - tions under consideration is too high to be learnt us - ing n labeled data points , but is small enough to be learnt using m ( cid : 123 ) n unlabeled data points , then semi - supervised learning ( ssl ) can improve the per - formance of a supervised learning ( sl ) task .
there have also been many successful practical ssl algo - rithms as summarized in ( chapelle , zien & scholkopf 123 , zhu 123 ) .
these theoretical analyses and prac -
appearing in proceedings of the 123th international confe - rence on articial intelligence and statistics ( aistats ) 123 , clearwater beach , florida , usa .
volume 123 of jmlr : w&cp 123
copyright 123 by the authors .
tical algorithms often assume that the data forms clus - ters or resides on a single manifold .
however , both a theory and an algorithm are lacking when the data is supported on a mixture of manifolds .
such data occurs naturally in practice .
for instance , in handwritten digit recognition each digit forms its own manifold in the feature space; in computer vision motion segmentation , moving objects trace dierent trajectories which are low dimensional manifolds ( tron & vidal 123 ) .
these manifolds may intersect or par - tially overlap , while having dierent dimensionality , orientation , and density .
existing ssl approaches can - not be directly applied to multi - manifold data .
for instance , traditional graph - based ssl algorithms may create a graph that connects points on dierent mani - folds near a manifold intersection , thus diusing infor - mation across the wrong manifolds .
in this paper , we generalize the theoretical analysis of ( singh et al .
123 ) to the case where the data is sup - ported on a mixture of manifolds .
guided by the the - ory , we propose an ssl algorithm that handles multi - ple manifolds as well as clusters .
the algorithm builds upon novel hellinger - distance - based graphs and size - constrained manifold clustering .
experiments show that our algorithm can perform ssl on multiple in - tersecting , overlapping , and noisy manifolds .
123 theoretic perspectives
in this section , we rst review the conclusions of ( singh et al .
123 ) , which are based on the cluster assump - tion , and then give conjectured bounds in the single manifold and multi - manifold case .
the cluster assumption , as formulated in ( singh et al .
123 ) , states that the target regression function or class label is locally smooth over certain subsets of the d - dimensional feature space that are delineated by changes in the marginal densitythroughout this pa - per , we assume the marginal density is bounded above
123 multi - manifold semi - supervised learning
and below ( away from zero ) .
we refer to these delin - eated subsets as decision sets; i . e . , all non - empty sets formed by intersections between the cluster support sets and their complements .
if these decision sets , de - noted by c , can be learnt using unlabeled data , the learning task on each decision set is simplied .
the results of ( singh et al .
123 ) suggest that if the de - cision sets can be resolved using unlabeled data , but not using labeled data , then semi - supervised learning can help .
however , this simple argument , and hence the distinctions between ssl and sl , are not always captured by standard asymptotic arguments based on rates of convergence .
( singh et al .
123 ) used nite sample bounds to characterize both the ssl gains and the relative value of unlabeled data .
to derive the nite sample bounds , the rst step is to understand when the decision sets are resolvable using data .
this depends on the interplay between the complexity of the class of distributions under con - sideration and the number of unlabeled points m and labeled points n .
for the cluster case , the complex - ity of the distributions is determined by the margin , dened as the minimum separation between clus - ters or the minimum width of a decision set ( singh et al .
if the margin is larger than the typi - cal distance between the data points ( m123 / d if using unlabeled data , or n123 / d if using only labeled data ) , then with high probability the decision sets can be learnt up to a high accuracy ( which depends on m or n , respectively ) ( singh et al .
this implies that if > m123 / d ( margin exists with respect to density of unlabeled data ) , then the nite sample performance ( the expected excess error err ) of a semi - supervised
learner bfm , n relative to the performance of a clairvoy - ant supervised learner bfc , n , which has perfect knowl - err ( bfc , n ) + ( m , n )
edge of the decision sets c , can be characterized as
here pxy ( ) denotes the cluster - based class of distri - butions with complexity , and ( m , n ) is the error in - curred due to inaccuracies in learning the decision sets using unlabeled data .
comparing this upper bound on the semi - supervised learning performance to a - nite sample minimax lower bound on the performance of any supervised learner provides a sense of the rel - ative performance of sl vs .
thus , ssl helps if complexity of the class of distributions > m123 / d and both of the following conditions hold : ( i ) knowl - edge of decision sets simplies the supervised learn - ing task , that is , the error of the clairvoyant learner
suppxy ( ) err ( bfc , n ) < inf fn suppxy ( ) err ( fn ) , the
smallest error that can be achieved by any supervised learner based on n labeled data .
the dierence quan -
table 123 : conjectured nite sample performance of ssl and sl for regression of a holder - , > 123 , smooth function ( with respect to geodesic distance in the man - ifold cases ) .
these bounds hold for d 123 , d < d , m ( cid : 123 ) n , and suppress constants and log factors .
d > m 123 d > m 123
single manifold sm : = min ( r123 , s123 )
sm n 123 d > sm m 123 d > sm 123 multi - manifold mm : = sgn ( ) min ( || , r123 , s123 )
mm n 123 d > mm m 123 d > mm m 123
d > mm
suppxy ( ) err ( bfc , n )
ties the ssl performance gain .
( ii ) m is large enough so that the error incurred due to using a nite amount of unlabeled data to learn the decision sets is negligi - ble : ( m , n ) = o .
this quan - ties the relative value of labeled and unlabeled data .
the nite sample performance bounds on ssl and sl performance as derived in ( singh et al .
123 ) for the cluster assumption are summarized in table 123 for the regression setting , where the target function is a holder - smooth function on each decision set and > 123
we can see that ssl provides improved per - formance , by capitalizing on the local smoothness of the function on each decision set , when the separation between the clusters is large compared to the typical distance between unlabeled data m123 / d but less than the typical distance between labeled data n123 / d .
neg - ative refers to the case where the clusters are not separated , but can overlap and give rise to decision sets that are adjacent ( see ( singh et al .
123 ) ) .
in this case , ssl always outperforms sl provided the width of the resulting decision sets is detectable using unla - beled data .
thus , the interplay between the margin and the number of labeled and unlabeled data charac - terizes the relative performance of sl vs .
ssl under the cluster assumption .
similar results can be derived in the classication setting where an exponential im - provement ( from n123 / d to en ) is possible provided the number of unlabeled data m grows exponentially
123 goldberg , zhu , singh , xu , nowak
with n ( singh et al .
123 ) .
123 single manifold case
in the single manifold case , the assumption is that the target function lies on a lower d - dimensional man - ifold , where d < d , and is holder - smooth ( > 123 ) with respect to the geodesic distance on the manifold .
hence knowledge of the manifold , or equivalently the geodesic distances between all pairs of data points , can be gleaned using unlabeled data and reduces the di - mensionality of the learning task .
in the case of distributions supported on a single man - ifold , the ability to learn the geodesic distances well , and hence the complexity sm of the distributions , de - pends on two geometric properties of the manifold its minimum radius of curvature r123 and proximity to self - intersection s123 ( also known as branch separation ) ( bernstein , de silva , langford & tenenbaum 123 ) .
if sm : = min ( r123 , s123 ) is larger than the typical dis - tance between the data points ( m123 / d with unlabeled data , or n123 / d with only labeled data ) , then with high probability the manifold structure is resolvable and geodesic distances can be learnt up to a high accuracy ( which depends on m or n , respectively ) .
this can be achieved by using shortest distance paths on an - or k - nearest neighbor graph to approximate the geodesic distances ( bernstein et al .
the use of approx - imate geodesic distances to learn the target function gives rise to an error - in - variable problem .
though the overall learning problem is now reduced to a lower - dimensional problem , we are now faced with two types of errorsthe label noise and the error in the esti - mated distances .
however , the error incurred in the nal estimation due to errors in geodesic distances de - pends on m which is assumed to be much greater than n .
thus , the eect of the geodesic distance errors is negligible , compared to the error due to label noise , for m suciently large .
this suggests that for the manifold case , if sm > m123 / d , then nite sample performance of semi - supervised learning can again be related to the performance of a clairvoyant supervised
learner bfc , n as in ( 123 ) above , since ( m , n ) is negligible
for m suciently large .
comparing this ssl performance bound to a nite sample minimax lower bound on the performance of any supervised learner indicates ssls gain in the sin - gle manifold case and is summarized in table 123
these are conjectured bounds based on the arguments above and similar arguments in ( niyogi 123 ) .
the ssl up - per bound can be achieved using a learning procedure adaptive to both and d , such as the method proposed in ( bickel & li 123 ) 123
the sl lower bounds follow
123note , however , that the analysis in ( bickel & li 123 )
from the results in ( tsybakov 123 ) and ( niyogi 123 ) .
ssl provides improved performance by capitalizing on the lower - dimensional structure of the manifold when the minimum radius of curvature and branch separa - tion are large compared to the typical distance be - tween unlabeled data m123 / d , but small compared to the typical distance between labeled data n123 / d .
123 multi - manifold case
the multi - manifold case addresses the generic setting where the clusters are low - dimensional manifolds that possibly intersect or overlap .
in this case , the target function is supported on multiple manifolds and can be piecewise smooth on each manifold .
thus , it is of in - terest to resolve the manifolds , as well as the subsets of each manifold where the decision label varies smoothly ( that are characterized by changes in the marginal den - sity ) .
the analysis for this case is a combination of the cluster and single manifold case .
the complexity of the multi - manifold class of distributions , denoted mm , is governed by the minimum of the manifold curvatures , branch separations , and the separations and overlaps between distinct manifolds .
for the regression setting , the conjectured nite sample minimax analysis is pre - sented in table 123
these results indicate that when there is enough unla - beled data , but not enough labeled data , to handle the complexity of the class , then semi - supervised learning can help by adapting to both the intrinsic dimensional - ity and smoothness of the target function .
extensions of these results to the classication setting are straight - forward , as discussed under the cluster assumption .
123 an algorithm
guided by the theoretical analysis in the previous sec - tion , we propose a cluster - then - label type of ssl al - gorithm , see figure 123
it consists of three main steps : ( 123 ) it uses the unlabeled data to form a small num - ber of decision sets , on which the target function is assumed to be smooth .
the decision sets are dened in the ambient space , not just on the labeled and unla - beled points .
( 123 ) the target function within a partic - ular decision set is estimated using only labeled data that fall in that decision set , and using a supervised learner specied by the user .
( 123 ) a new test point is predicted by the target function in the decision set it there have been several cluster - then - label approaches in the ssl literature .
for example , the early work of demiriz et al .
modies the objective of standard
considers the asymptotic performance of sl , whereas here we are studying the nite - sample performance of ssl .
123 multi - manifold semi - supervised learning
k - means clustering algorithms to include a class impu - rity term ( demiriz , bennett & embrechts 123 ) .
el - yaniv and gerzon enumerate all spectral clusterings of the unlabeled data with varying number of clusters , which together with labeled data induce a hypothesis space .
they then select the best hypothesis based on an occams razor - type transductive bound ( el - yaniv & gerzon 123 ) .
some work in constrained cluster - ing is also closely related to cluster - then - label from an ssl perspective ( basu , davidson & wagsta 123 ) .
compared to these approaches , our algorithm has two i ) it is supported by our ssl minimax theory; ii ) it handles both overlapping clusters and in - tersecting manifolds by detecting changes in support , density , dimensionality or orientation .
our algorithm is also dierent from the family of graph - regularized ssl approaches , such as manifold regularization ( belkin , sindhwani & niyogi 123 ) and earlier variants ( joachims 123 , zhou , bousquet , lal , weston & scholkopf 123 , zhu , ghahramani & laerty 123 ) .
those approaches essentially add a graph - regularization term in the objective .
they also depend on the manifold assumption that the target function indeed varies smoothly on the manifold .
in contrast , i ) our algorithm is a wrapper method , which uses any user - specied supervised learner sl as a sub - routine .
this allows us to directly take advantage of advances in supervised learning without the need to derive new algorithms .
ii ) our theory ensures that , even when the manifold assumption is wrong , our ssl performance bound is the same as that of the super - vised learner ( up to a log factor ) .
finally , step 123 of our algorithm is an instance of man - ifold clustering .
recent advances on this topic include generalized principal component analysis ( vidal , ma & sastry 123 ) and lossy coding ( ma , derksen , hong & wright 123 ) for mixtures of linear subspaces , mul - tiscale manifold identication with algebraic multi - grid ( kushnir , galun & brandt 123 ) , tensor vot - ing ( mordohai & medioni 123 ) , spectral curvature clustering ( chen & lerman 123 ) , and translated pois - son mixture model ( haro , randall & sapiro 123 ) for mixtures of nonlinear manifolds .
our algorithm is unique in two ways : i ) its use of hellinger distance oers a new approach to detecting overlapping clus - ters and intersecting manifolds; ii ) our decision sets have minimum size constraints , which we enforce by
123 hellinger distance graph let the labeled data be ( ( xi , yi ) ) n i=123 , and the unla - beled data be ( xj ) m j=123 , where m ( cid : 123 ) n .
the build - ing block of our algorithm is a local sample covari - for a point x , dene n ( x ) to be
a small neighborhood around x in euclidean space .
let x be the local sample covariance matrix at x : x123n ( x ) ( x123x ) ( x123x ) > / ( |n ( x ) |123 ) , where x123n ( x ) x123 / |n ( x ) | is the neighborhood mean .
in our experiments , we let |n ( x ) | o ( log ( m ) ) so that the neighborhood size grows with unlabeled data size m .
the covariance x captures the local geometry our intuition is that points xi , xj on dierent man - ifolds or in regions with dierent density should be considered dissimilar .
this intuition is captured by the hellinger distance between their local sample co - variance matrices i , j .
the squared hellinger dis - tance is dened between two pdfs p , q : h 123 ( p , q ) = by setting p ( x ) = n ( x; 123 , i ) , i . e . , a gaussian with zero mean and co - variance i , and similarly q ( x ) = n ( x; 123 , j ) , we ex - tend the denition of hellinger distance to covariance matrices : h ( i , j ) h ( n ( x; 123 , i ) , n ( x; 123 , i ) ) =
p123 123d / 123|i|123 / 123|j|123 / 123 / |i + j|123 / 123 , where d is the
dimensionality of the ambient feature space .
we will also call h ( i , j ) the hellinger distance between the two points xi , xj .
when i +j is rank decient , h is computed in the subspace occupied by i + j .
the hellinger distance h is symmetric and in ( 123 , 123 ) .
h is small when the local geometry is similar , and large when there is signicant dierence in density , manifold dimensionality or orientation .
example 123d covariance matrices and their h values are shown in figure 123
comment h ( 123 , 123 )
figure 123 : hellinger distance
it would seem natural to compute all pairwise hellinger distances between our dataset of n + m points to form a graph , and apply a graph - cut algo - rithm to separate multiple manifolds or clusters .
how - ever , if xi and xj are very close to each other , their lo - cal neighborhoods n ( xi ) , n ( xj ) will strongly overlap .
then , even if the two points are on dierent manifolds the hellinger distance will be small , because their co - variance matrices i , j will be similar .
therefore , we select a subset of m o ( m / log ( m ) ) unlabeled points so that they are farther apart while still covering the whole dataset .
this is done using a greedy procedure ,
123 goldberg , zhu , singh , xu , nowak
given n labeled examples and m unlabeled examples , and a supervised learner sl ,
use the unlabeled data to infer k o ( log ( n ) ) decision setscci :
( a ) select a subset of m < m unlabeled points ( b ) form a graph on the n + m labeled and unlabeled points , where the edge weights are computed
from the hellinger distance between local sample covariance matrices
( c ) perform size - constrained spectral clustering to cut the graph into k parts , while keeping enough
labeled and unlabeled points in each part
use the labeled data incci and the supervised learning sl to train bfi 123
for test point x cci , predict bfi ( x ) .
figure 123 : the multi - manifold semi - supervised learning algorithm
where we rst select an arbitrary unlabeled point x ( 123 ) .
we then remove its unlabeled neighbors n ( x ( 123 ) ) , in - cluding itself .
we select x ( 123 ) to be the next nearest neighbor , and repeat .
this procedure thus approxi - mately selects a cover of the dataset .
we focus on the subset of m unlabeled and n labeled points .
each of these n + m points has its local covariance com - puted from the original full dataset .
we then discard the m m unselected unlabeled points .
notice , how - ever , that the number m of eective unlabeled data points is polynomially of the same order as the total number m of available unlabeled data points .
figure 123 : the graph on the dollar sign dataset .
we can now dene a sparse graph on the n+ m points .
each point x is connected by a weighted , undirected edge to o ( log ( n+m ) ) of its nearest mahalanobis neigh - bors chosen from the the set of n + m points too .
the choice of o ( log ( n + m ) ) allows neighborhood size to grow with dataset size .
since we know the local geom - etry around x ( captured by x ) , we follow the man - ifold by using the mahalanobis distance as the local distance metric at x : d123 for example , a somewhat at x will preferentially connect to neighbors in or near the same at sub - space .
the graph edges are weighted using the stan - dard rbf scheme , but with hellinger distance : wij =
exp ( cid : 123 ) h 123 ( i , j ) / ( 123 ) ( cid : 123 ) .
figure 123 ( a ) shows a small
m ( x , x123 ) = ( xx123 ) >123
part of a synthetic dollar sign dataset , consisting of two intersecting manifolds : s and | .
the green dots are the original unlabeled points , and the ellip - soids are the contours of covariance matrices around
the subset of selected unlabeled points within a small region .
figure 123 ( b ) shows the graph on the complete dollar sign dataset , where red edges have large weights and yellow edges have small weights .
thus the graph combines locality and geometry : an edge has large weight when the two nodes are close in mahalanobis distance , and have similar covariance structure .
123 size - constrained spectral
matrix , and degii =p
we perform spectral clustering on this graph of n + m nodes .
we hope each resulting cluster represents a separate manifold , from which we will dene a deci - sion set .
of the many spectral clustering algorithms , we chose ratio cut for its simplicity , though others can be similarly adapted for use here .
the standard ra - tio cut algorithm for k clusters has four steps ( von luxburg 123 ) : 123
compute the unnormalized graph laplacian l = degw , where w = ( wij ) is the weight j wij form the diagonal degree matrix .
compute the k eigenvectors v123 .
vk of l with the smallest eigenvalues .
form matrix v with v123 .
vk as columns .
use the ith row of v as the new representation of xi .
cluster all x under the new representation into k clusters using k - means .
our ultimate goal of semi - supervised learning poses new challenges; we want our ssl algorithm to de - grade gracefully , even when the manifold assumption does not hold .
the ssl algorithm should not break the problem into too many subproblems and increase the complexity of the supervised learning task .
this is achieved by requiring that the algorithm does not generate too many clusters and that each cluster con - tains enough labeled points .
because we will simply do supervised learning within each decision set , as long as the number of sets does not grow polynomially with n , the performance of our algorithm is guaranteed to be polynomially no worse than the performance of the supervised learner when the manifold assumption fails .
thus , we automatically revert to the supervised learn -
123 multi - manifold semi - supervised learning
ing performance .
one way to achieve this is to have i ) the number of clusters grows as k o ( log ( n ) ) ; ii ) each cluster must have at least a o ( n / log123 ( n ) ) labeled points; iii ) each spectral cluster must have at least b o ( m / log123 ( n ) ) unla - beled points .
the rst sets the number of clusters k , allowing more clusters and thus handling more com - plex problems as labeled data size grows , while suer - ing only a logarithmic performance loss compared to a supervised learner if the manifold assumption fails .
the second requirement ensures that each decision set has o ( n ) labeled points up to log factor123
the third is similar , and makes spectral clustering more robust .
spectral clustering with minimum size constraints a , b on each cluster is an open problem .
directly en - forcing these constraints in graph partitioning leads to dicult integer programs .
instead , we enforce the constraints in k - means ( step 123 ) of spectral clus - tering .
our approach is a straightforward extension to the constrained k - means algorithm of bradley et al .
( bradley , bennett & demiriz 123 ) .
for point xi , let ti123 .
tik r be its cluster indicators : tih = 123 if xi is in cluster h , and 123 otherwise .
let c123 .
ck rd denote the cluster centers .
constrained k - means is the iterative minimization over t and c of the following problem :
h=123 tihkxi chk123 i=123 tih a , pn+m h=123 tih = 123 , t 123
i=n+123 tih b , h = 123 .
k , ( 123 ) where we assume the points are ordered so that the rst n points are labeled .
fixing t , optimizing over c is trivial , and amounts to moving the centers to the bradley et al .
showed that xing c and optimizing t can be converted into a minimum cost flow problem , which can be exactly solved .
in a minimum cost flow problem , there is a directed graph where each node is either a supply node with a number r > 123 , or a demand node with r < 123
the arcs from i j is associated with cost sij , and ow tij .
the goal is to nd the ow t such that supply meets demand at all nodes , while the cost is minimized :
tji = ri , i .
for our problem ( 123 ) , the corresponding minimum cost flow problem is shown in figure 123
the supply nodes are x123 .
xn+m with r = 123
there are two sets of clus - k , each with demand ter center nodes .
one set c 123the square allows the size ratio between two clusters to be arbitrarily skewed as n grows .
we do not want to x the relative sizes of the decision sets a priori .
r = a , is due to the labeled data size constraint .
k , each with demand r = b , the other set cu is due to the unlabeled data size constraint .
finally , a sink demand node with r = ( n + m ak bk ) catches all the remaining ow .
the cost from xi to ch is sih = kxi chk123 , and from ch to the sink is 123
it is then clear that the minimum cost flow problem ( 123 ) is equivalent to ( 123 ) with tih = tih and c xed .
inter - estingly , ( 123 ) is proven to have integer solutions which correspond exactly to the desired cluster indicators .
figure 123 : the minimum cost flow problem
nearest neighbor among the n + m points has cluster
once size - constrained spectral clustering is completed , the n+m points will each have a cluster index in 123 .
i=123 by the voronoi cells
we dene k decision sets ( cci ) k around these points : cci = ( x rd | xs euclidean index i ) .
we train a separate predictor bfi for each test time , an unseen point x cci is predicted as bfi ( x ) .
therefore , the unlabeled data in our algorithm
decision set using the labeled points in that decision set , and a user - specied supervised learner .
during
is used merely to determine the decision sets .
data sets .
we experimented with ve synthetic ( fig - ure 123 ) and one real data sets .
data sets 123 are for regression , and 123 are for classication : ( 123 ) .
dol - lar sign contains two intersecting manifolds .
the s manifold has target y varying from 123 to 123
the | manifold has target function y = x123 + 123 , where x123 is the vertical dimension .
white noise n ( 123 , 123 ) is added to y .
( 123 ) surface - sphere slices a 123d sur - face through a solid ball .
the ball has target function y = ||x|| , and the surface has y = x123 123
( 123 ) den - sity change contains two overlapping rectangles .
one rectangle is wide and sparse with y = x123 , the other is narrow and ve times as dense with y = 123x123
to - gether they produce three decision sets .
( 123 ) surface - helix has a 123d toroidal helix intersecting a surface .
each manifold is a separate class .
( 123 ) martini is a composition of ve manifolds ( classes ) to form the shape of a martini glass with an olive on a toothpick , as shown in figure 123 ( e ) .
( 123 ) mnist digits .
we scaled
123 goldberg , zhu , singh , xu , nowak
( a ) dollar sign
( c ) density change
figure 123 : regression mse ( a - c ) and classication error ( d - e ) for synthetic data sets .
all curves are based on m = 123 , 123 - trial averages , and error bars plot 123 standard deviation .
clairvoyant classication error is 123
down the images to 123 x 123 pixels and used the ocial train / test split , with dierent numbers of labeled and unlabeled examples sampled from the training set .
methodology & implementation details .
in all experiments , we report results that are the average of 123 trials over random draws of m unlabeled and n la - beled points .
we compare three learners : ( global ) : supervised learner trained on all of the labeled data , ignoring unlabeled data .
( clairvoyant ) : with the knowledge of the true decision sets , trains one su - pervised learner per decision set .
( ssl ) : our semi - supervised learner that discovers the decision sets us - ing unlabeled data , then trains one supervised learner per decision set .
after training , each classier is eval - uated on a massive test set , also sampled from the un - derlying distribution , to estimate generalization error .
we implemented the algorithms in matlab , with minimum cost flow solved by the network simplex method in cplex .
we used the same set of param - eters for all experiments and all data sets : we chose the number of decision sets to be k = d123 log ( n ) e .
to obtain the subset of m unlabeled points , we let the neighborhood size |n ( x ) | = b123 log ( m ) c .
when creating the graph w , we used b123 log ( m + n ) c near - est mahalanobis neighbors , and an rbf bandwidth = 123 to convert hellinger distances to edge weights .
the size constraints were a = b123n / log123 ( n ) c , b = b123m / log123 ( n ) c .
finally , to avoid poor local optima in spectral clustering , we ran 123 random restarts for constrained k - means , and chose the result with the lowest objective .
for the regression tasks , we used kernel regression with an rbf kernel , and tuned the bandwidth parameter with 123 - fold cross validation us - ing only labeled data in each decision set ( or glob - ally for global ) .
for classication , we used a sup - port vector machine ( libsvm ) with an rbf kernel , and tuned its bandwidth and regularization parame - ter with 123 - fold cross validation .
we used euclidean distance in each decision region for the supervised
learner , but we expect performance with geodesic dis - tance would be even better .
results of large m : figure 123 reports the results for the ve synthetic data sets .
in all cases , we used m = 123 , n ( 123 , 123 , 123 , 123 , 123 , 123 ) , and the resulting regressors / classiers are evaluated in terms of mse or error rate using a test set of 123 points .
these results show that our ssl algorithm can dis - cover multiple manifolds and changes in density well enough to consistently outperform sl in both regres - sion and classication settings of varying complexity123
we also observed that even under - or over - partitioning into fewer or more decision sets than manifolds can still improve ssl performance123
we performed three experiments with the digit recog - nition data : binary classication of the digits 123 vs 123 , and three - way classication of 123 , 123 , 123 and 123 , 123 , 123
here , we xed n = 123 , m = 123 , 123 random training tri - als , each tested on the ocial test set .
table 123 con - tains results averaged over these trials .
ssl outper - forms global in all three digit tasks , and all dierences are statistically signicant ( = 123 ) .
note that we used the same parameters as the synthetic data exper - iments , which results in k = 123 decision sets for n = 123; again , the algorithm performs well even when there are fewer decision sets than classes .
close inspection re - vealed that our clustering step creates relatively pure decision sets .
for the binary task , this leads to two
123though not shown in figure 123 , we found that a standard graph - based ssl algorithm manifold regulariza - tion ( belkin et al .
123 ) , using euclidean knn graphs with rbf weights and all parameters tuned using cross valida - tion , performs worse than global on these datasets due to the strong connections across manifolds .
123we compared global and ssls 123 trials at each n using two - tailed paired t - tests .
ssl was statistically sig - nicantly better ( = 123 ) in the following cases : dollar sign at n = 123 , density at n = 123 , surface - helix at n = 123 , and martini at n = 123
the two methods were statistically indistinguishable in other cases .
123nmse globalsslclairvoyant123nmse globalsslclairvoyant123nmse globalsslclairvoyant123 . 123 . 123nerror rate globalssl123 . 123nerror rate globalssl multi - manifold semi - supervised learning
table 123 : 123 - trial average test set error rates one standard deviation for handwritten digit recognition .
123 vs 123
123 , 123 , 123
123 , 123 , 123
m = 123
m = 123
m = 123
figure 123 : eect of varying m for the surface - helix data set ( n = 123 ) .
see text for details .
trivial classication problems , and errors are due only to incorrect assignments of test points to decision sets .
for the 123 - way tasks , the algorithm creates 123+123 and 123 clusters , and 123+123 and 123 clusters .
eect of too small an m : finally , we examine our ssl algorithms performance with less unlabeled data .
for the surface - helix data set , we now x n = 123 ( which leads to k = 123 decision sets ) and reduce m .
figure 123 depicts example partitionings for three m values , along with 123 - trial average error rates ( one standard deviation ) in each setting .
note these are top - down views of the data in figure 123 ( d ) .
when m is small , the resulting subset of m unlabeled points is too small , and the partition boundaries cannot be reliably estimated .
segments of the helix shown in red and areas of the surface in blue or green correspond to such partitioning errors .
nevertheless , even when m is as small as 123 , ssls performance is no worse than global supervised learning , which achieves an error rate of 123 123 when n = 123 ( see figure 123 ( d ) ) .
conclusions : we have extended ssl theory and practice to multi - manifolds .
a detailed analysis of geodesic distances , automatic parameter selection , and large scale empirical study remains as future work .
thank the wisconsin alumni research foundation .
ag is supported in part by a yahoo ! key technical challenges grant .
