label dissimilarity species that a pair of ex - amples probably have dierent class labels .
we present a semi - supervised classication algorithm that learns from dissimilarity and similarity information on labeled and unla - beled data .
our approach uses a novel graph - based encoding of dissimilarity that results in a convex problem , and can handle both binary and multiclass classication .
experi - ments on several tasks are promising .
semi - supervised classication learns a classier from both labeled and unlabeled data by encoding domain knowledge on unlabeled data in the model ( 123 , 123 , 123 ) .
in this paper we focus on a particular form of domain knowledge : the label dissimilarity between examples .
we assume we are given a set of dissimilarity pairs d = ( ( i , j ) ) .
for ( i , j ) d , the two points xi , xj may be both unlabeled , or one labeled and the other unlabeled .
in either case we know they probably do not have the same label .
the dissimilarity knowledge can be noisy .
as an example , consider the problem of predicting a persons political view ( left , right ) from his / her post - ings to online blogs .
the fact that person b quotes person a and uses expletives near the quote is a strong indication that b disagrees with a ( 123 ) .
simple text processing thus allows us to create a dissimilarity pair ( a , b ) to reect our knowledge that a and b probably have dierent labels ( political views ) .
such dissimilarity knowledge has been extensively studied in semi - supervised clustering , where such pairs are known as cannot - links ( 123 , 123 , 123 , 123 , 123 ) , mean - ing they cannot be in the same cluster .
these meth - ods either directly modify the clustering algorithm , or
change the underlying distance metric .
our method is dierent in that it specically applies to classica - tion , and works on discriminant functions .
dissimilar - ity as negative correlation on discriminant functions has been discussed in relational learning with gaus - sian processes ( 123 ) , but their formulation is non - convex and applies only to binary classication .
in contrast our formulation is convex and applicable to multiple
our contribution is a convex method that incorporates both similarity and dissimilarity in semi - supervised learning .
we start with graph - based semi - supervised classication methods ( e . g . , ( 123 , 123 ) ) , which allows a natural combination of similarity and dissimilarity .
existing graph - based semi - supervised learning meth - ods encode label similarity knowledge , but they cannot handle dissimilarity easily , as we show in section 123
we dene a mixed graph to accommodate both , and dene the analog of graph laplacian .
we then adapt manifold regularization ( 123 ) to the mixed graph .
we extend our method to multiclass classication in sec - tion 123 , and present experimental results in section 123
123 dissimilarity in binary
there be n items , of which l are labeled : ( ( x123 , y123 ) , , ( xl , yl ) , xl+123 , , xn ) .
existing graph - based semi - supervised classication methods assume that a graph over the n items is given .
the graph is represented by an n n matrix w , where wij is the non - negative edge weight between items i , j .
similar items have large weights , reecting the domain knowl - edge ( or assumption ) that they tend to have similar la - bels .
such knowledge can be represented as a penalty term ( 123 ) on the discriminant function f : x 123 r :
wij ( f ( xi ) f ( xj ) ) 123
minimization of ( 123 ) tends to force f ( xi ) f ( xj ) when wij is large .
therefore existing graph - based meth - ods are able to encode label similarity domain knowl - edge .
the penalty ( 123 ) can be written in quadratic form f lf , where f = ( f ( x123 ) , , f ( xn ) ) and l is known as the combinatorial graph laplacian matrix , dened as l = d w where d is the diagonal degree matrix
with dii = pn
existing graph - based methods cannot easily handle dissimilarity , which is the requirement that two items have dierent labels .
a small or zero weight wij does not represent dissimilarity between xi and xj; in fact , a zero edge weight means no preference at all .
a neg - ative weight wij < 123 does encourage a large dier - ence between f ( xi ) , f ( xj ) , but this creates a num - ber of problems .
first f needs to be bounded or ( , ) will be a trivial minimizer .
second , any neg - ative weight in w will make ( 123 ) , and ultimately the whole semi - supervised problem , non - convex .
one has to resort to approximations ( 123 , 123 , 123 ) .
it is highly desirable to keep the optimization problem convex .
123 mixed graphs
let us assume y ( 123 , 123 ) for binary classication .
our key idea is to encode dissimilarity between i , j as wij ( f ( xi ) + f ( xj ) ) 123
note the summation .
this term is zero if f ( xi ) , f ( xj ) have the same absolute value but opposite signs , thus encouraging dierent labels .
the trivial case f ( xi ) = f ( xj ) = 123 is avoided by competing terms in a risk minimization framework ( section 123 ) .
the weight wij remains positive , and represents the strength of our belief in this dissimilarity edge .
denition 123 a mixed graph over n nodes has sim - ilarity and dissimilarity edges , and is represented by two n n matrices s and w .
s species the edge type : sij = 123 if there is a similarity edge between i , j; sij = 123 if there is a dissimilarity edge .
non - negative weights wij 123 represent the strength of the edge , re - gardless of its type .
the graphs in existing graph - based semi - supervised learning methods can be viewed as having an all - one s and the same w .
extending ( 123 ) to the mixed graph , we would like to minimize a new penalty term
wij ( f ( xi ) sijf ( xj ) ) 123
it handles both similarity and dissimilarity , and is clearly convex in f .
furthermore , we can re - write ( 123 ) in a quadratic form .
proposition 123 let m = l + ( 123 s ) w , where l is the combinatorial graph laplacian , 123 is the all - one
matrix , and is the hadamard ( elementwise ) prod - uct .
then m is positive semi - denite , and f mf =
123 pi , j wij ( f ( xi ) sijf ( xj ) ) 123
the matrix m is the mixed - graph analog of the graph laplacian l .
like the laplacian , m is positive semi - denite , as can be seen from ( 123 ) .
if the graph has no dissimilarity edges , then m = l .
123 manifold regularization
manifold regularization ( 123 ) generalizes graph - based semi - supervised learning with a regularized risk mini - mization framework .
let h be the reproducing ker - nel hilbert space ( rkhs ) of a kernel k .
manifold reg - ularization obtains the discriminant function by solv -
c ( yi , f ( xi ) ) + 123kf k123
h + 123f lf ,
where c ( ) is an arbitrary loss function , e . g . , the hinge loss for support vector machines ( svms ) , or squared loss for regularized least squares ( rls ) classiers .
as before , f is the vector of discriminant function val - ues on the n points .
the rst two terms in ( 123 ) are the same as in supervised learning , while the third term is the additional regularization term for graph - based semi - supervised learning .
because f is dened in h now , it naturally extends to new test points .
noisy labels are tolerated by the loss function .
the mixed - graph analog of ( 123 ) is
c ( yi , f ( xi ) ) + 123kf k123
h + 123f mf .
one can solve the optimization problem ( 123 ) directly .
alternatively one can view the second and third terms together as regularization by a warped kernel , as pro - posed in ( 123 ) .
in this view , one denes a second rkhs h , which has the same functions as h but a dierent inner product : hf , gih = hf , gih + f m g , where m is some positive semi - denite matrix on the n points .
h + f m f .
the super - it follows that kf k123 then equivalent to our semi - supervised learning prob - lem ( 123 ) , if we let m = 123 m .
importantly , it is shown in ( 123 ) that the kernel k for the warped rkhs h is related to the original k as follows :
vised problem minf h pl
i=123 c ( yi , f ( xi ) ) + 123kf k123
h = kf k123
( i + m k ) 123m kz ,
k ( x , z ) = k ( x , z ) kx
where kx = ( k ( x123 , x ) , , k ( xn , x ) ) .
this allows one to compute the warped kernel k from some original kernel ( e . g . , rbf ) k and the mixed - graph m .
there - fore , to solve ( 123 ) , we can use k in conjunction with standard supervised kernel machine software .
123 dissimilarity in
it is non - trivial to incorporate dissimilarity into mul - 123
one - vs - rest does not work with dissimilarity and semi - supervised learning .
suppose , for example , that there are three classes , and that xi , xj are two unla - beled points whose actual labels are 123 and 123 , respec - tively .
let ( i , j ) be specied as a dissimilarity edge .
in the binary sub - task of class 123 vs .
all other classes , how - ever , this dissimilarity edge should become a similarity edge , since xi , xj are both in the rest meta - class .
one - vs - one does not work either .
for any partic - ular one - vs - one sub - task ( say class 123 vs .
123 ) , it is not clear whether any unlabeled point ( say xj which actu - ally has class 123 ) should participate in the one - vs - one semi - supervised learning .
if an unlabeled point does not have one of the two labels , its inclusion will likely 123
using the warped kernel ( 123 ) in a standard multi - class kernel machine ( e . g . , multiclass svm ) does not work .
multiclass methods use k discriminant functions f123 , , fk , one for each class .
the warped kernel incor - rectly encourages all discriminant functions to honor f ( xi ) + f ( xj ) = 123 , which is unnecessary and poten - we found all the above approaches indeed hurt accu - racy .
these experiments are not reported here .
we therefore need to redesign the multiclass objective in order to incorporate dissimilarity .
for simplicity we focus on multiclass svms , but our method works for other loss functions too .
there are several for - mulations of multiclass svms , e . g . , ( 123 , 123 , 123 ) .
for our purpose it is important to anchor the discrimi - nant functions around zero .
for this reason we start with the formulation in ( 123 ) .
a k - class svm is de - ned as the optimization problem of nding functions f ( x ) = ( f123 ( x ) , , fk ( x ) ) that solve :
i=123 li ( f ( xi ) yi ) + + pk
j=123 fj ( xi ) = 123 ,
i = 123 l ,
where fj ( x ) = hj ( x ) +bj for j = 123 k; hj h , which is the rkhs of some kernel k; and bj r .
there are l labeled training points .
l is an l k matrix , with the i - th row li = ( 123 , , 123 , 123 , 123 , , 123 ) being an all - one vector except the yi - th element which is zero .
yi is the given label for xi .
the vector yi = ( 123 / ( k 123 ) , , 123 , 123 / ( k 123 ) , ) is an encoding of the label yi , where the number 123 occurs in the yi - th position .
the plus function is ( z ) + = max ( 123 , z ) .
( 123 ) means that f ( xi ) should have elements less than 123 / ( k 123 ) for all wrong classes .
it is important to note that the elements of yi and f ( xi ) sum to zero .
we exploit this sum - to - zero label encoding to repre - sent dissimilarity as a convex multiclass svm objec - tive .
to simplify the notation , we will restrict our - selves to dissimilarity edges with weight 123
similarity edges can be added to the formulation easily by us - ing terms like ( f ( xi ) f ( xj ) ) 123 as in ( 123 , 123 ) .
given a dissimilarity edge ( s , t ) d , the key idea behind our multiclass dissimilarity formula comes from comparing f ( xs ) , f ( xt ) for the good and bad cases .
the good case is when f takes the nominal encoding f ( xs ) = ys and f ( xt ) = yt , and ys 123= yt .
by denition ys and yt have the form ( 123 / ( k 123 ) , , 123 , 123 / ( k 123 ) , ) , where the elements with value 123 must be at dierent positions .
hence ys + yt is a vector with two kinds of elements : ( k 123 ) / ( k 123 ) and 123 / ( k 123 ) .
the bad case is when ys = yt , so the elements with value 123 coincide .
in this case the sum ys + yt has two kinds of elements : 123 and 123 / ( k 123 ) .
comparing good and bad , we do not want any element in f ( xs ) + f ( xt ) to be larger than ( k 123 ) / ( k 123 ) .
we are therefore led to the following dissimilarity objective :
( cid : 123 ) fj ( xs ) + fj ( xt )
which is a sum of plus functions raised to the p - th power .
the advantages of this denition are that it is convex and simple , and it reduces to our binary svm dissimilarity formulation when p = 123 , k = 123
in standard practice , one can combine ( 123 ) and ( 123 ) as
i=123 li ( f ( xi ) yi ) + + 123pk
j=123 ( cid : 123 ) fj ( xs ) + fj ( xt ) k123
j=123 fj ( xi ) = 123 ,
i = 123 n ,
where n is the sum of the number of unlabeled points that are involved in any dissimilarity edge , plus the number of labeled points l .
the representer theorem in ( 123 ) needs to be extended to include these unlabeled in particular , the minimizing functions for ( 123 ) have the form
cijk ( xi , x ) + bj for j = 123 , , k
the essential dierence to supervised learning is that we now have n rather than l representers in ( 123 ) .
using ( 123 ) , we formulate ( 123 ) as a quadratic program .
j kcj , where kst = k ( xs , xt ) is the n n gram matrix .
we let p = 123 in the dissimilarity
objective ( 123 ) .
this leads to the primal form
i=123 li ( f ( xi ) yi ) + + 123pk
j=123 ( cid : 123 ) fj ( xs ) + fj ( xt ) k123
j=123 fj ( xi ) = 123 ,
i = 123 n .
we dene an l k matrix y whose i - th row is y substituting ( 123 ) into ( 123 ) , we obtain
i=123l lij ( kicj + bj yij ) +
( s , t ) d ( cid : 123 ) ( ks + kt ) cj + 123bj k123 pj=123k ( kicj + bj ) = 123 ,
i = 123 n .
finally we introduce an l k matrix and a |d| k matrix as auxiliary variables .
with standard refor - mulation techniques , we rewrite ( 123 ) as
i=123l lijij + 123pk
kicj + bj yij ij ,
i = 123 l , j = 123 k
i = 123 l , j = 123 k
( ks + kt ) cj + 123bj k123
stj 123 , ( s , t ) d , j = 123 k
pj=123k ( kicj + bj ) = 123 ,
i = 123 n ,
where the minimization is over c , b , , .
the quadratic program has o ( nk ) variables and constraints .
in the following sections , we empirically demonstrate the benets of incorporating dissimilarity in several
123 standard binary datasets
we rst experimented using the standard binary datasets g123c and mac - windows used in ( 123 ) and avail - able with the authors code at http : / / people . cs .
uchicago . edu / vikass / research . html .
as in ( 123 ) , g123c contains 123 examples containing 123 dimensions , and we use l = 123 labeled samples .
mac - windows has 123 examples with 123 dimensions , also with l = 123
ideally , we would like to use dissimilarity information based on domain knowledge .
however , without such expertise available to us , we performed oracle exper - iments in which we introduce dissimilarity edges be - tween randomly sampled data points with dierent la - bels .
because the edges represent ground - truth dis - similarity , we disallow edges to touch labeled points ,
to prevent the true labels propagating throughout the unlabeled data .
note that the actual label values are not revealedjust the fact that the points should re - ceive dierent label classications .
simulating domain knowledge in this manner is common for cannot - link clustering and related work .
in section 123 , we present results involving real dissimilarity based on domain -
in this subsection , we introduce dissimilarity in the manifold regularization framework , discussed in sec - tion 123 .
following ( 123 ) , we start with a gaussian base kernel k and encode similarity using k - nearest - neighbor graphs with gaussian weights .
cally , the weight between knn points xi and xj is , while all other weights are zero .
we then add the above dissimilarity edges , and assign them a relatively large weight ( see below ) to form the mixed - graph matrix m .
our experiments used the resulting warped kernel k in both svm and rls classiers .
the methods were implemented using libsvm and a modied version of the code from ( 123 ) .
we used the same parameter values as ( 123 ) .
these had been tuned in that paper with 123 - fold cross validation using simi - larity only; our dissimilarity results could become even better with additional parameter tuning .
to compare error rate on unlabeled data used dur - ing semi - supervised training , and on new unseen test data , we divided each dataset into four disjoint folds .
we then performed 123 - fold cross validation , using each fold as a test set once .
the test set remains unseen throughout the learning process .
the remaining three folds comprised the training set ( labeled and unlabeled data ) .
for each train / test split , we trained 123 dierent classiers , each time using a dierent random choice of labeled examples and dissimilarity edges between un - labeled examples .
the same random choices are made in all experimental runs , so we can compare results using paired statistical tests .
we report classication error rate on the unlabeled training set ( in - sample per - formance ) and unseen test data ( out - of - sample perfor - mance ) .
each number is averaged over 123 folds with 123 random trials each .
we address two questions in these standard binary dataset experiments :
how does the number of dissimilarity edges in - uence mean error rate ? we experimented rst with varying the number of dissimilarity edges in the graph .
since we have high condence in the oracle edges , we assign each edge a weight equal to the maxi - mal similarity edge weight ( close to 123 for our datasets ) .
figure 123 shows the eect of changing the number of dissimilarity edges in the g123c and mac - windows datasets .
figures 123 ( a , b , e , f ) present mean in - sample and out - of - sample error rates using 123 dissimi -
larity edges , as compared to the baseline with 123 dissim - ilarity edges , using a hinge loss function for c ( ) in ( 123 ) .
they are similar to lapsvms , but with dissimilarity edges .
figures 123 ( c , d , g , h ) display comparable results using a squared error loss function for c ( ) in ( 123 ) .
these are similar to laprls , but with dissimilarity edges .
in all plots , we show one standard deviation above and below the error rate curve .
the baselines here use only similarity edges in graph - based semi - supervised learn - ing .
they are equivalent to lapsvm and laprls
figure 123 shows the positive impact of dissimilarity edges .
the eect is greater for in - sample performance; the in - sample points were directly involved in the ker - nel deformation , so this benet is to be expected .
our model also generalizes to out - of - sample test data .
to measure statistical signicance , we performed two - tailed , paired t - tests , comparing the results using each number of dissimilarity edges to the baseline in each of the subplots .
the circled settings are statistically signicant at the 123 level .
while out - of - sample performance steadily improves in the mac - windows dataset ( figures 123 ( f , h ) ) , the g123c out - of - sample error benets less with 123 or 123 dissimilarity edges ( figures 123 ( b , d ) ) .
the increase in error rate corresponds with near - zero in - sample error rates , suggesting that the learning algorithm is over - tting the dissimilarity edges .
for this small dataset , nearly all of the unlabeled points are touched by one or more of the 123 dissimilarity edges .
( mac - windows is roughly four times as large , so this is not the case . ) it seems the kernel becomes so warped that it ts the g123c unlabeled points perfectly , but becomes less eective in classifying unseen test points .
though we require only f ( xi ) f ( xj ) < 123 for xi and xj to be labeled dierently , the dissimilarity terms encourage f ( xi ) = f ( xj ) for ( i , j ) d .
we believe that this unnecessarily stringent requirement is at the root of the observed overtting when too many dissimilarity terms are included .
while the mechanics are still un - clear , the inappropriate demand appears to become overwhelming , and generalization error starts to in -
what is the eect of the weight assigned to dissimilarity edges ? in the preceding experiments , we varied the number of dissimilarity edges , but xed their weights to roughly 123
we next xed the number of edges at 123 , and experimented with varying this weight by a range of multiplicative factors ( figure 123 ) .
this eectively places more or less condence in the dissimilarity edges , compared to the similarity edges .
as before , the baseline is either lapsvm or laprls , and does not use any dissimilarity .
table 123 : mean error rate with varying numbers of dis - similarity edges in the usps dataset using the multi - class svm formulation .
we observe that in - sample performance tends to ben - et from stronger weights on dissimilarity edges ( fig - ures 123 ( a , c , e , g ) ) .
the maximal decrease in mean er - ror rate appears at a weight of approximately 123 , above which the error rate rises slightly .
datasets , above a weight of approximately 123 , the out - of - sample error rate ( figures 123 ( b , d , f , h ) ) dramat - ically rises above the baseline .
this appears to be an - other case of overttingthe kernel deformation relies too heavily on the dissimilarity edges , and much useful similarity information is being ignored .
this results in good in - sample performance , at the expense of correct classication of new examples .
123 standard multiclass dataset
we next experimented with dissimilarity in multiclass classication as described in section 123
we used the standard multiclass dataset usps test , which contains 123 examples with 123 dimensions , each belonging to one of 123 classes .
we used labeled set size l = 123
this dataset was also used in ( 123 ) and is available at the url cited above .
we solve the quadratic pro - gram in ( 123 ) using the cplex qp solver .
we exper - imented using varying numbers of oracle dissimilarity edges .
as before , our dissimilarity edges do not touch labeled points .
we consider those examples involved in dissimilarity to be the unlabeled set , and the remain - ing examples ( ignored during training ) the unseen test set .
we report mean error rates over 123 repeated tri - als using dierent random labeled sets and dierent random unlabeled - unlabeled dissimilarity edges .
the 123 parameter in ( 123 ) was optimized using mean test set performance without any dissimilarity .
thus , we are making the baseline as strong as possible .
we ar - bitrarily set 123 = 123
careful tuning of this parameter could potentially lead to even better results .
table 123 presents the overall , in - sample , and out - of - sample mean error rates using the 123 - norm svm for - mulation ( 123 ) with a varying number of dissimilarity edges .
statistically signicant reductions in error rate ,
( c ) se in - sample
( d ) se out - of - sample
( a ) hinge in - sample
( b ) hinge out - of - sample
( e ) hinge in - sample
( f ) hinge out - of - sample
( g ) se in - sample
( h ) se out - of - sample
figure 123 : varying the number of dissimilarity edges ( x - axis ) in the g123c dataset ( a - d ) and the mac - windows dataset ( e - h ) .
y - axis is mean error rate across 123 folds with 123 random trials each .
hinge stands for the hinge loss and se the squared error loss .
the baselines are lapsvm and laprls respectively , which have no dissimilarity edges .
circled settings are statistically signicantly better than the baseline .
compared to the baseline , are indicated in bold face .
the 123 - norm multiclass svm formulation uses the dis - similarity edges eectively to lower overall and out - of - sample mean error rate for all amounts of dissimilar - ity edges that we tested .
note that the baseline has a higher error rate than reported in ( 123 ) , and this is because we used the multiclass svm formulation of ( 123 ) to allow dissimilarities , not the code in ( 123 ) .
123 politics dataset
in our nal set of experiments , we create real ( instead of oracle ) dissimilarity edges based on domain knowl - edge .
we experimented with the politics . com dis - cussion board text data from ( 123 ) .
the task here is to predict the political aliation of the users post - ing messages on a political discussion board .
we re - strict ourselves to the 123 users with left ( 123 ) and right ( 123 ) political tendencies .
the dataset contains the text of several thousand posts .
quoting behavior is annotated in the dataset , so we know who quoted who .
since we are interested in classifying each user ( as opposed to each post ) , we concatenated together all posts ( excluding quoted text ) written by a user .
we removed punctuation and common english words , and applied stemming .
we then formed term frequency - inverse document frequency ( tf - idf ) vectors ( see ( 123 ) ) for each user using word types occurring 123 or more times , which resulted in 123 unique terms .
we created dissimilarity edges by the quoting behav - ior between users .
in political discussion boards , users tend to quote posts by users with diering political views ( 123 ) .
for example , users often debate a con -
troversial issue , quoting and disputing each others previous claims .
we declare disagreement between a and b if b quotes a , and the text adjacent to the quoted text contains two or more question marks or exclamation marks , or two or more consecutive words in all capital letters ( i . e . , internet shouting123 ) .
consider the following illustrative example taken from the current dataset , where the user dixie has quoted and responded to the user deshrubinator :
deshrubinator : you were the one who thought it should be investigated last week .
dixie : no i didnt , and i made it clear .
you are insane ! you are the one with no ****ing respect for
we create a dissimilarity edge ( a , b ) if they have ex - hibited such seemingly hostile behavior toward each other in more than 123 posts .
this thresholding ensures that we have seen multiple pieces of evidence for dis -
it is worth noting that our dissimilarity edges only need simple text processing , and can be easily de - ned over unlabeled data ( users with unknown politi - cal view ) .
for this experiment we do not include sim - ilarity edges , partly because the standard cosine sim - ilarity on text ( 123 ) measures similarity in topics ( note users from dierent parties do talk about the same topic ) , rather than sentiment , which is more relevant to the current task .
we will investigate high quality similarity edges in future work .
therefore we cannot use lapsvm or laprls as our baselines .
instead we
123we also require these words to be more than three char - acters long to avoid false positives from common internet abbreviations like lol ( laugh out loud ) .
( a ) hinge in - sample
( e ) hinge in - sample
( b ) hinge out - of - sample
( c ) se in - sample
( d ) se out - of - sample
( f ) hinge out - of - sample
( g ) se in - sample
( h ) se out - of - sample
figure 123 : changing the weight of dissimilarity edges ( x - axis ) in the g123c dataset ( a - d ) and the mac - windows dataset ( e - h ) .
y - axis is mean error rate across 123 folds with 123 random trials each .
circled settings are statistically signicantly better than the baseline .
use the standard supervised svm and rls as base - lines , respectively .
also note that , unlike our exper - iments with oracle edges , we are now including all such dissimilarity edges , some of which connect labeled and unlabeled examples .
the only edges discarded are those between two labeled examples .
our scheme is realistic with noisy , real edges .
we used a graph of these dissimilarity edges to warp a linear kernel used in svm and rls classication .
we set the labeled set size l = 123 ( out of 123 ) and ran 123 repeated trials with randomly selected labeled examples .
out of the possible 123 dissimilarity edges derived using the above heuristics , the trials included an average of 123 edges ( i . e . , 123 labeled - labeled edges are ignored ) .
on average , 123 examples are involved in the dissimilarity edges .
table 123 reports the mean error rate on all unlabeled examples for svm and rls classiers with ( ssl ) and without ( base ) dissimi - larity edges .
the baseline results use unwarped linear kernels .
in both classiers , we observe a statistically signicant reduction in error rate ( p < 123 using a two - tailed , paired t - test ) ; it appears that the real - world dissimilarity edges aid classication .
however upon closer inspection , we also notice the improvement comes mostly from in - sample error reduction , and it does not generalize as well to out - of - sample data like in previous experiments .
we suspect this could be due to the high initial error rate .
finally , as a post - experiment study , we investigated how many of our heuristically derived dissimilarity edges were actually consistent with the true labels .
it turns out that 123 out of the 123 edges ( 123% ) are in fact true dissimilarity edges .
thus , we have shown
table 123 : mean error rates for svm and rls with and without dissimilarity edges on the politics dataset .
dissimilarity is incorporated through warped kernels .
both dierences are statistically signicant .
classier base error rate
ssl error rate
that , even if 123% of the dissimilarity edges represent false domain knowledge , we can achieve a signicant improvement in overall error rate .
we presented a convex algorithm to encode dissimilar - ity in semi - supervised learning .
we demonstrated that when such dissimilarity domain knowledge is available , our algorithm can take advantage of it and improve classication .
the major advantage of our dissimi - larity encoding formulations ( 123 ) and ( 123 ) is convexity .
however , they probably specify the relation between the discriminant function f at dissimilarity samples xi and xj more than necessary .
for example in the binary case we prefer f ( xi ) = f ( xj ) , while ideally it is sucient to require f ( xi ) , f ( xj ) having opposite signs .
finding computationally ecient encodings for this sucient condition is a direction for future re -
( 123 ) jurgen van gael and xiaojin zhu .
correlation clus - tering for crosslingual link detection .
in international joint conference on articial intelligence ( ijcai ) ,
( 123 ) kiri wagsta , claire cardie , seth rogers , and stefan schrodl .
constrained k - means clustering with back - in international conference on machine learning ( icml ) , page 123 , 123
( 123 ) martin wainwright , tommi jaakkola , and alan will - sky .
map estimation via agreement on ( hyper ) trees : message passing and linear - programming approaches .
ieee transactions on information theory , 123 : 123
( 123 ) yair weiss and william freeman .
on the optimality of solutions of the max - product belief - propagation al - gorithm in arbitrary graphs .
ieee transactions on information theory , 123 , 123
( 123 ) j .
weston and c .
watkins .
multi - class support vector machines .
technical report csd - tr - 123 - 123 , depart - ment of computer science , royal holloway , univer - sity of london , 123
( 123 ) eric xing , andrew ng , michael jordan , and stuart russell .
distance metric learning with application to clustering with side - information .
in advances in neu - ral information processing systems ( nips ) , 123
( 123 ) xiaojin zhu .
semi - supervised learning litera - technical report 123 , computer sciences , university of wisconsin - madison , 123
( 123 ) xiaojin zhu , zoubin ghahramani , and john laerty .
semi - supervised learning using gaussian elds and harmonic functions .
in icml - 123 , 123th international conference on machine learning , 123
( 123 ) xiaojin zhu and andrew b .
goldberg .
