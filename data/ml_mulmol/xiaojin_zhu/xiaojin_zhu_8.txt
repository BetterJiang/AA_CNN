active and semi - supervised learning are impor - tant techniques when labeled data are scarce .
we combine the two under a gaussian random eld model .
labeled and unlabeled data are rep - resented as vertices in a weighted graph , with edge weights encoding the similarity between in - stances .
the semi - supervised learning problem is then formulated in terms of a gaussian random eld on this graph , the mean of which is char - acterized in terms of harmonic functions .
ac - tive learning is performed on top of the semi - supervised learning scheme by greedily select - ing queries from the unlabeled data to minimize the estimated expected classication error ( risk ) ; in the case of gaussian elds the risk is ef - ciently computed using matrix methods .
we present experimental results on synthetic data , handwritten digit recognition , and text classica - tion tasks .
the active learning scheme requires a much smaller number of queries to achieve high accuracy compared with random query selection .
semi - supervised learning targets the common situation where labeled data are scarce but unlabeled data are abun - dant .
under suitable assumptions , it uses unlabeled data to help supervised learning tasks .
various semi - supervised learning methods have been proposed and show promising results; seeger ( 123 ) gives a survey .
these methods typi - cally assume that the labeled data set is given and xed .
in practice , it may make sense to utilize active learning in conjunction with semi - supervised learning .
that is , we might allow the learning algorithm to pick a set of unla - beled instances to be labeled by a domain expert , which
will then be used as ( or to augment ) the labeled data set .
in other words , if we have to label a few instances for semi - supervised learning , it may be attractive to let the learning algorithm tell us which instances to label , rather than se - lecting them randomly .
we will limit the range of query se - lection to the unlabeled data set , a practice known as pool - based active learning or selective sampling .
there has been a great deal of research in active learning .
for example , tong and koller ( 123 ) select queries to min - imize the version space size for support vector machines; cohn et al .
( 123 ) minimize the variance component of the estimated generalization error; freund et al .
( 123 ) employ a committee of classiers , and query a point whenever the committee members disagree .
most of the active learning methods do not take further advantage of the large amount of unlabeled data once the queries are selected .
exceptions include mccallum and nigam ( 123 ) who use em with un - labeled data integrated into the active learning , and muslea et al .
( 123 ) who use a semi - supervised learning method during training .
in addition to this body of work from the machine learning community , there is a large literature on the closely related topic of experimental design in statistics; chaloner and verdinelli ( 123 ) give a survey of experimen - tal design from a bayesian perspective .
recently zhu et al .
( 123 ) introduced a semi - supervised learning framework which is based on gaussian random elds and harmonic functions .
in this paper we demon - strate how this framework allows a combination of active learning and semi - supervised learning .
in brief , the frame - work allows one to efciently estimate the expected gener - alization error after querying a point , which leads to a bet - ter query selection criterion than naively selecting the point with maximum label ambiguity .
then , once the queries are selected and added to the labeled data set , the classier can be trained using both the labeled and remaining unlabeled data .
we present results on synthetic data , text classica -
proceedings of the icml - 123 workshop on the continuum from labeled to unlabeled data , washington dc , 123
tion and image classication tasks that indicate the com - bination of these techniques can result in highly effective
gaussian random elds and harmonic
energy minimizing functions
can be the
radial basis function ( rbf ) :
are labeled with the corresponding
we begin by briey describing the semi - supervised learn -
total number of points .
we assume the labels are binary :
the semi - supervised algorithm in this paper is based on a relaxation of the requirement that labels are binary , re - sulting in a simple and tractable family of algorithms .
we the labels on labeled data are still constrained to be 123 or 123 :
so that nearby points in euclidean space are assigned large edge weights .
other weightings are possible , of course , is discrete or sym - fully species the data manifold structure .
we note that a method for learn - is proposed in ( zhu et al . , 123 ) .
ing framework of zhu et al .
( 123 ) .
there are
, and unlabeled points .
we will use note the labeled and unlabeled set , and " $#% ' & ( * ) , + . - 123 / 123 .
we assume a connected graph123#123 ' 123 : is given with nodes123 corresponding to the " data points , of which the nodes in are represented by an " <;= " weight matrix s .
the edges123 > which is given .
for example if ? +a@cb
and may be more appropriate when bolic .
for our purposes the matrix> ing the scale parameterq allow continuous labels on unlabeled nodesmxzy ( 123 ) \^@ _` c#a123 ) bc_ d+a - e / 123 for_p# ( 123f .
we also denote the over the graph .
dene the diagonal matrixq123#sr*_`t*u ( 123r dfemg are the weighted degrees of .
the combinatorial laplacian is the " x;y " the nodes in123 matrix form as123|c123 # ( ) z : is an inverse temperature parameter , and is the partition function
since we want unlabeled points that are nearby in the graph to have similar labels , we dene the energy to be
we can rewrite the energy function in .
we consider the gaussian
so that low energy corresponds to a slowly varying function
is also the mean of the eld .
corresponding to unlabeled data .
into blocks for labeled and unlabeled nodes ,
is connected and labeled nodes from both classes
puted with matrix methods .
we partition the laplacian ma -
is the mode of the gaussian random eld , but since the joint distribu -
the gaussian random eld differs from the standard dis - crete markov random eld in that the eld congurations are over a continuous state space .
moreover , for a gaus - sian eld the joint probability distribution over unlabeled
the harmonic property means that the value at each unlabeled node is the average of neighboring
which is consistent with our prior notion of smoothness with respect to the graph .
because of the maximum prin -
nodes is gaussian with covariance matrix the submatrix ofz the minimum energy function# the gaussian eld is harmonic; that is , it satisesza# / on unlabeled data points ! , and is equal to ) on the la - beled data points ciple of harmonic functions ( doyle & snell , 123 ) , are present at the boundary ( the usual case; otherwise takes on the extremum 123 or 123 ) .
by denition tion is gaussian , the harmonic energy minimizing function can be com - , and denotes the
labeled data , is a multivariate normal : *yafz that the harmonic energy minimizing function is to label node_ as class 123 in casebc_ dj / 123h , and to label node_ class 123 otherwise .
the harmonic function has many nice interpretations , of dene the transition matrix#q random walk on graph123 unlabeled node_ , it moves to a noden with probability a labeled node .
then starting from node_ , reaches a labeled node with label 123
which the random walk view is particularly relevant here .
consider the by a particle .
starting from an
to carry out classication with a gaussian eld , we note mean of the eld .
therefore the bayes classication rule
after one step .
the walk continues until the particle reaches is the probability that the particle ,
mean values on the unlabeled data points .
the solution is
it is not hard to show that the gaussian eld , conditioned on
the labeled data are the absorbing boundary for the random walk .
more on this semi - supervised learning framework can be found in ( zhu et al . , 123 ) .
active learning
is the unknown true
we dene the true risk
we propose to perform active learning with the gaussian random eld model by greedily selecting queries from the unlabeled data to minimize the risk of the harmonic energy minimization function .
the risk is the estimated general - ization error of the bayes classier , and can be efciently computed with matrix methods .
mean of the gaussian eld model :
tion we can compute the estimated risk
is the bayes decision rule , where ( with a
is not computable .
in order to proceed , it is necessary to make assumptions .
we begin by assuming that
is the probability of reaching 123 in a random walk on the graph , our assumption is that we can approximate the distribution using a biased coin at each
of the bayes classier based on the harmonic function
# , / otherwise .
here ~ slight abuse of notation ) sgn
label distribution at node_ , given the labeled data .
because we can estimate the unknown distribution ~ d with the e .
with this assump - node , whose probability of heads is
, we will receive an answer
harmonic function by
since we do not know what answer we again assume the answer is approximated with
the expected estimated risk after querying
if we perform active learning and query an unlabeled node ( 123 or 123 ) .
adding this point to the training set and retraining , the gaussian eld and its mean function will of course change .
we denote the new .
the estimated risk will
we will receive ,
while more labeled data required :
find best query using ( 123 )
, weight matrix> compute harmonic using ( 123 ) , receive answer
figure123
the active learning algorithm
the active learning criterion we use in this paper is the greedy procedure of choosing the next query imizes the expected estimated risk :
standard result ( a derivation is given in appendix a ) gives
for node ? this is the same as nding the conditional distribution of .
noting that
, a multivariate normal distribution , a
to carry out this procedure , we need to compute the har - to the cur - rent labeled training set .
this is the retraining problem and is computationally intensive in general .
however for gaus - sian elds and harmonic functions , there is an efcient way to retrain .
recall that the harmonic function solution is
what is the solution if we x the value all unlabeled nodes , given the value of ' the conditional once we x '
on unlabeled data , andz we compute the harmonic function .
as a nal word on computational efciency , we note that after adding query and its answer to the next iteration we will need to compute 123z ; a derivation is given in appendix b .
is the - th column of the inverse laplacian is the - th diagonal ele - ment of the same matrix .
both are already computed when .
this is a linear com -
the inverse of the laplacian on unlabeled data , with the removed .
instead of naively taking the inverse , there are efcient algorithms to compute it from
to summarize , the active learning algorithm is shown in figure 123
the time complexity to nd the best query is
putation and therefore can be carried out efciently .
figure 123 shows ( top left ) a synthetic dataset with two la - beled data ( marked 123 , 123 ) , an unlabeled point a in the
most uncertain query
labeled set size
most uncertain query
labeled set size
figure123synthetic data experiments .
top left : synthetic data 123; top right : synthetic data 123
bottom left : risk on synthetic data 123; bottom right : classication accuracy on synthetic data 123
standard errors are shown as dotted lines .
points in
center above and a cluster of 123 unlabeled points b be - low .
b is slighted shifted to the right .
the graph is fully
connected with weightd=emg n .
in this conguration , we euclidean distance between_v have the most uncertainty in a : btm # b have around 123 , so we are more certain about the 123 .
this shows that the
labels of b than a .
however , the risk minimization cri - terion picks the upper center point ( marked with a star ) in b to query , instead of a .
in fact the estimated risk is
active learning algorithm is not simply picking the most uncertain point for query , but rather it thinks globally .
figure 123 also shows ( top right ) another synthetic dataset , where the true labels for the 123 points form a chess - board pattern .
we expect active learning to discover the pattern and query a small number of representatives from each cluster .
on the other hand , we expect a much larger num - ber of queries if queries are randomly selected .
we use
we perform 123 random trials .
at the beginning of each trial we randomly select a positive example and a negative example as the initial training set .
we then run active learn - ing and compare it to two baselines : ( 123 ) random query : ; ( 123 ) most un -
a fully connected graph with weightdemg
randomly selecting the next query from ! certain query : selecting the most uncertain instance in ! i . e .
the one with closest to 123 .
in each case we run for 123
iterations ( queries ) .
at each iteration we plot the estimated
( lower right ) .
the error bars are
risk ( 123 ) of the selected query ( lower left ) , and the classi -
cation accuracy on !
standard deviation , averaged over the random trials .
as ex - pected , with active learning we reduce risk more quickly than random queries or the most uncertain queries .
in fact , active learning with about 123 queries ( plus 123 initial random points ) learns the correct concept , which is nearly optimal given that there are 123 clusters .
looking at the queries , we nd that active learning mostly selects the central points of
next we report the results of document categorization ex - periments using the 123 newsgroups dataset123
we evaluate the following binary classication tasks : rec . sport . baseball ( 123 documents ) vs .
rec . sport . hockey ( 123 ) ; comp . sys . - ibm . pc . hardware ( 123 ) vs .
comp . sys . mac . hardware ( 123 ) ; talk . religion . misc ( 123 ) vs .
alt . atheism ( 123 ) .
they repre - sent easy , moderate and hard problems respectively .
each document is minimally processed into a tf . idf vector , without applying header removal , frequency cutoff , stem -
ming , or a stopword list .
two documentsp by an edge if .
we use the following weight among s 123 nearest neighbors , as measured by their co -
function on the edges : as before , we perform 123 trials , and randomly pick two
s 123 nearest neighbors or if
labeled set size
labeled set size
labeled set size
labeled set size
labeled set size
labeled set size
figure123risk ( top ) and classication accuracy ( bottom ) on 123 newsgroups , compared to random queries; baseball vs .
hockey ( left ) , pc vs .
mac ( center ) , and religion vs .
atheism ( right ) .
initial training examples to start each trial .
the rest of the documents are treated as unlabeled data .
in each trial we answer 123 queries .
again we compare active learning with
figure 123 compares it with random queries .
active learning reduces risk faster , and achieves high clas - sication accuracy more rapidly than random queries .
for easy datasets only a handful of queries are neces - sary for active learning to achieve 123% accuracy .
we observe that active learning tends to query the same small set of documents in different trials .
we also trained a svm classier on the random queries , with the cosine similarity kernel and ( which was the best setting among several different kernels and a wide values ) .
note that the svm does not utilize unlabeled data; on these datasets the semi - supervised method outperforms the svm .
figure 123 compares it with the most uncertain queries .
most uncertain query selects the instance whose
value is closest to 123 as the next query .
svm most uncertain selects the instance whose margin is clos - est to 123 ( closest to the decision boundary ) .
as before the svm does not utilize unlabeled data .
the har - monic function classication is worse with the most uncertain queries than with random queries , while the svm improves with the most uncertain queries .
in all cases our proposed active learning scheme clearly outperforms both baselines for the 123 newsgroups datasets .
we then evaluate active learning on a handwritten digits dataset , originally from the cedar buffalo binary digits database ( hull , 123 ) .
the digits were preprocessed to re - down - sampling and gaussian smoothing , with pixel val - ues ranging from 123 to 123 ( le cun et al . , 123 ) .
they are further scaled down to each image is thus represented by a 123 - dimensional vector .
we consider the binary problem of classifying digits 123 vs .
123 , with 123 images in each class .
we create a graph
duce the size of each image down to a123 k pixel bins .
on the 123 images , with an edge between images_vn is inn s 123 nearest neighbors ( in euclidean distance ) or vice versa .
the weights on edges ared ages_vn .
in each of 123 trials we randomly pick one exam -
ple from each class to form the initial training set .
we then compare active learning with random queries and the most uncertain queries for 123 iterations .
figure 123 ( left ) shows the risk .
with active learning the risk decreases faster than with the baselines .
figure 123 ( center ) shows the classica - tion accuracy where active learning is seen to outperform the baselines .
again only a handful of examples are needed for active learning to reach high accuracy .
the svm uses
is the pixel - wise euclidean distance between im -
among polynomial kernels with order 123 to 123 , and a wide values .
we also observe that there are certain
123 , which is one of the best
most uncertain query
labeled set size
most uncertain query svm most uncertain
labeled set size
most uncertain query
labeled set size
most uncertain query svm most uncertain
labeled set size
most uncertain query
labeled set size
most uncertain query svm most uncertain
labeled set size
figure123risk ( top ) and classication accuracy ( bottom ) on 123 newsgroups , compared to the most uncertain queries; baseball vs .
hockey ( left ) , pc vs .
mac ( center ) , and religion vs .
atheism ( right ) .
images that active learning frequently queries in different trials .
figure 123 ( right ) shows some of the most frequently queried images; we believe these images are representative of the variations in the dataset .
we also evaluate on the difcult binary problem of classify - ing odd digits vs .
even digits .
that is , we group 123 , 123 , 123 , 123 , 123 and 123 , 123 , 123 , 123 , 123 into two classes .
there are 123 images per digit ( 123 per class ) .
it is a difcult dataset because the target concept is rather articial; on the other hand this dataset resembles synthetic data 123 ( figure 123 ) where each class has several internal clusters .
the experimental setup is the same as above except that we run for 123 iterations .
figure 123 shows the results .
again active learning is supe - rior than the baselines .
we also see that odd vs .
even is a harder concept which takes active learning about 123 queries to approximately learn .
the digits shown are the 123 most frequently queried instances in the rst 123 iterations across all trials .
with 123 instances this dataset is also the largest and slowest .
with a naive matlab implementation , the cal - culations take roughly 123 seconds per iteration on a 123ghz linux machine .
in contrast , the 123 vs .
123 dataset re - quires ve seconds per iteration , while all of the 123 news - groups datasets take less than two seconds per iteration .
we nally note that if we train an svm on the active queries chosen from the harmonic function risk minimiza - tion procedure , the accuracy is always worse than our pro - posed active learning method , and often even worse than
the svm on random queries .
we have proposed an approach to active learning which is tightly coupled with semi - supervised learning using gaus - sian elds and harmonic functions .
the algorithm selects queries to minimize an approximation to the expected gen - eralization error .
experiments on text categorization and handwritten digit recognition indicate that the active learn - ing algorithm can be highly effective .
