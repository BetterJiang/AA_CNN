we introduce time - sensitive dirichlet process mixture models for clustering .
the models allow innite mixture components just like standard dirichlet process mixture models .
however they also have the ability to model time correlations between in -
research supported in part by nsf grants nsf - ccr 123 , nsf - iis 123 , and nsf - iis 123
zoubin ghahramani was supported at cmu by darpa under the calo project .
report documentation page
omb no .
123 - 123
public reporting burden for the collection of information is estimated to average 123 hour per response , including the time for reviewing instructions , searching existing data sources , gathering and maintaining the data needed , and completing and reviewing the collection of information .
send comments regarding this burden estimate or any other aspect of this collection of information , including suggestions for reducing this burden , to washington headquarters services , directorate for information operations and reports , 123 jefferson davis highway , suite 123 , arlington va 123 - 123
respondents should be aware that notwithstanding any other provision of law , no person shall be subject to a penalty for failing to comply with a collection of information if it does not display a currently valid omb control number .
report date
report type
title and subtitle time - sensitive dirichlet process mixture models
performing organization name ( s ) and address ( es ) carnegie mellon university , school of computer
dates covered 123 - 123 - 123 to 123 - 123 - 123
contract number 123b .
grant number 123c .
program element number 123d .
project number 123e .
task number 123f .
work unit number
performing organization
sponsoring / monitoring agency name ( s ) and address ( es )
sponsor / monitors acronym ( s )
sponsor / monitors report
distribution / availability statement approved for public release; distribution unlimited
supplementary notes the original document contains color images .
subject terms 123
security classification of :
this page
limitation of
name of
standard form 123 ( rev .
123 - 123 ) prescribed by ansi std z123 - 123
keywords : i . 123 ( articial intelligence ) : learning; i . 123 ( pattern recognition ) : models
statistical , i . 123 ( pattern recognition ) : design methodologyclassier design and eval - uation; general terms : algorithms; additional key words : dirichlet process mixture models , mcmc , time
traditional clustering algorithms make two assumptions that are often false in practice : 123
that the number of clusters is known; 123
that the data points are independent .
we propose a model that allows innite number of clusters , and cluster members may have certain dependency in time .
consider emails received by a user over a period of time .
suppose we want to
cluster the emails by topic thread .
there are several ways to do this :
we can sort emails by the subject line .
however it is unreliable and we want a
more exible probabilistic model based on email content .
we can model each thread with a multinomial distribution over the vocabulary , and treat each email as a bag of words .
the whole email collection can be modeled as a mixture of multinomial .
the problem is that we do not know the number of threads ( mixing components ) .
fixing the number , which is a common practice , seems arbitrary .
we can model the collection as a dirichlet process mixture model ( dpm ) ( 123 ) .
dpms allow potentially innite number of components .
nonetheless dpms are exchangeable .
when applied to emails , this means that old threads never die down .
this is undesirable because we want the emails from years ago to have less inuence than those from this morning in predicting the next email .
we therefore would like to introduce the concept of time into dpms , while keeping the ability to model unlimited number of clusters .
this is achieved with the proposed time - sensitive dirichlet process mixture ( tdpm ) models .
123 the tdpm framework
consider a sequence of input d with time stamp t : ( d123 , t123 ) , .
, ( dn , tn ) , where the time monotonically increases .
for concreteness let us assume the ds are email documents , each represented as a bag - of - word vector .
let si ( 123 , 123 , .
. ) be the true cluster membership ( email thread ) of di .
notice we do not set the number of clusters a priori .
there could potentially be an unlimited number of clusters as the number of documents
without loss of generality we assume that each cluster j is represented by a multi - nomial distribution j over the vocabulary .
the probability for cluster j to generate document di is then
p ( di|j ) = yvvocabulary
since past email threads can inuence the current email , we want si to depend on the history s123 , .
we also want such dependency to vary with time : older emails should have less inuence .
we introduce a weight function w ( t , j ) which summarizes
figure 123 : ( a ) the time kernel with = 123 .
( b ) the weight functions with data from two clusters , marked as star or circle respectively .
the history at time t .
it gives the weight ( or inuence ) of cluster j at time t , given the history so far s123 , .
, si : ti < t ,
w ( t , j ) = x ( i|ti<t , si=j )
note the weight function is the sum of some time kernel k .
in the email example we can use a kernel like k ( t ) = exp ( t ) if t 123 , and k ( t ) = 123 if t < 123
this kernel stipulates that an email will boost the probability of the same thread in later emails , but the boost decreases exponentially as specied by the parameter .
figure 123 ( a ) shows an example time kernel with = 123 , while ( b ) shows two weight functions built upon the kernel .
in the example there are documents from cluster 123 at time 123 , 123 , 123 , and from cluster 123 at time 123 , 123
other forms of the time kernel are possible too .
we dene the prior probability of assigning cluster j to di , given the history s123 , .
, si123 ,
p ( si = j|s123 , .
, si123 )
= p ( si = j|w ( ti , ) )
pj 123 w ( ti , j 123 ) +
pj 123 w ( ti , j 123 ) +
if j is in history if j is new
where is a concentration parameter .
we call this a time - sensitive dirichlet process mixture ( tdpm ) model .
intuitively if there has been many recent emails from cluster j , the new email will have a large probability also from j .
in addition , there is always a possibility that the new email is from a new cluster not seen so far .
tdpm is very similar to the standard dirichlet process mixture ( dpm ) models .
in fact , it can be shown that if the time kernel k is a step function , then we recover
figure 123 : the graphical model for time - sensitive dirichlet process mixture models .
d is the feature ( e . g .
words of an email ) , t is the time stamp , s is the cluster label , and w is the sufcient statistic that summarizes the history .
shaded nodes are observed .
the standard dpms .
it is the decaying of k over time that allows us to include time information in to the process .
the graphical model representation of tdpm is given in
given d and t , we would like to infer s .
we use a markov chain monte carlo method .
notice w is a deterministic function of s and t and does not need to be sampled .
as shown later if we used conjugate priors , we do not need to actually sample but can analytically integrate it out .
therefore we only need to sample s .
in gibbs sampling , we need to sample si from the distribution
p ( si = j|si , d123 , .
, dn ) p ( si = j|si ) p ( di|di : si=j )
where di : si=j is the set of documents in cluster si = j , excluding di .
the prior p ( si = j|si ) in ( 123 ) involves all nodes before and after si :
p ( si = j|si )
p ( sm|s123 , .
, sm123 ) ! p ( si = j|s123 , .
, si123 ) n
p ( si = j|s123 , .
, si123 ) n
p ( sm|s123 , .
, sm123 ) !
p ( sm|s123 , .
, sm123 ) !
substituting in the denition ( 123 ) , it is easy to show that the denominators are the same for different values of j , and the only difference is in the numerator .
the likelihood term p ( di|di : si=j ) in ( 123 ) is domain - specic .
for the email task ,
a dirichlet - multinomial ( 123 ) is the natural choice :
p ( di|di : si=j ) = z p ( di| ) p ( |di : si=j ) d
where p ( |di : si=j ) is a posterior dirichlet distribution .
the posterior is derived from a prior ( base ) dirichlet distribution g123 , and the observed data di : si=j .
let the dirichlet prior g123 be parameterized by m , where m is a vector over the vocabulary and m sums to 123 , and is the strength of the prior :
treating the document collection di : si=j as a single , large document , the dirichlet posterior after observing counts fv for word v in di : si=j is
p ( |di : si=j ) = p ( |f , m ) =
and the dirichlet - multinomial is
( pv fv + ) qv ( fv + mv ) yv
p ( di|di : si=j ) = z p ( di| ) p ( |di : si=j ) d
( pv fv + ) qv ( fv + mv ) qv ( di ( v ) + fv + mv ) ( pv di ( v ) +pv fv + )
putting everything together for ( 123 ) , we can x all other s and sample for si .
a single gibbs sampling iteration consists of looping through i = 123 .
n and sample si in turn .
the algorithm is given in figure 123
the time complexity is o ( n123 ) for each iteration of the gibbs sampler .
if k has limited support , the complexity reduces o ( n ) but we lose the ability to model long range correlations .
finally we run the gibbs sampler for many iterations to get the marginals on s .
some readers may be disturbed by the apparent double counting in figure 123 when we assign u ( c ) = to not only the brand new state cnew , but also to states not in ( s<i ) but in ( s>i ) .
we assure the readers that it is merely an artifact of numbering .
if we were to renumber the states at each iteration , we can recover ( 123 ) exactly .
123 parameter learning
the parameters of the model include the base dirichlet distribution g123 , the concentra - tion parameter , and the time kernel parameter .
we x the base dirichlet g123
for the time being let us assume that all clusters share the same kernel parameter .
the free parameters are = ( , ) .
we learn the parameters by evidence maximization .
since our model is conditioned
on time , the evidence is dened as
p ( d|t , ) =xs
p ( d|s ) p ( s|t , )
for position i = 123 to n
/ * c is the candidate states for si , * / / * where ( si ) is the set of current states at positions other than i , * / / * and cnew / ( si ) is a new state , represented by an arbitrary new number .
* / c = ( si ) ( cnew )
/ * compute the unnormalized probability p ( si = c|si ) for all candidate c * / for c c
/ * evaluate candidate si = c * / / * prior : the history part .
( s<i ) is the set of states before position i * / if c ( s<i ) then u ( c ) = wc ( ti ) else u ( c ) = / * prior : the future part .
* / for j = i + 123 to n
if sj ( s<j ) then u ( c ) = u ( c ) wsj ( tj ) else u ( c ) = u ( c )
/ * likelihood .
* / u ( c ) = u ( c ) p ( di|di : si=c )
/ * pick the state si with probability proportional to u ( ) * /
figure 123 : a single gibbs sampling iteration for tdpm
where d is the set of all documents , t is the corresponding set of time stamps , and s is the set of cluster assignments .
we want to nd the best parameters that maximize
= arg max
p ( d|t , )
= arg max
p ( d|s ) p ( s|t , )
we nd the parameters with a stochastic em algorithm .
the cluster labels s are hidden variables .
let 123 be the current parameters .
we can sample s ( 123 ) .
s ( m ) from the posterior distribution p ( s|d , t , 123 ) , as detailed in section 123
in generalized em algorithm , we seek a new parameter which increases the expected log likelihood of the complete data
q ( 123 , ) = ep ( s|d , t , 123 ) ( log p ( s , d|t , ) )
= ep ( s|d , t , 123 ) ( log p ( d|s ) + log p ( s|t , ) )
notice log p ( d|s ) does not depend on , .
we approximate the expectation by sam -
q ( 123 , ) = const ( ) + ep ( s|d , t , 123 ) ( log p ( s|t , ) )
log p ( s ( m ) |t , )
and we nd the gradients w . r . t .
for parameter update
log p ( s ( m ) |t , )
log p ( s ( m )
i123 , t , )
where p ( s ( m )
i123 , t , ) is dened in ( 123 ) .
the gradients are :
log p ( si|s123 .
si123 , t , ) = (
if si in history if si new
log p ( si|s123 .
si123 , t , )
if si in history if si new
w ( t , c ) = xi : ti<t , si=c w ( t , c ) = xi : ti<t , si=c
k ( t ti ) =x e ( tti )
we then take a gradient step in the m - step of the generalized em algorithm to improve the log likelihood .
we create synthetic datasets which have explicit time dependency between instances , and use them to illustrate the time sensitivity of tdpm models .
all synthetic datasets have n = 123 instances .
we rst create the time stamps of each instances by sampling from a poisson process .
in particular , the interval between two consecutive time stamps has an exponential distribution with mean 123 / = 123 :
p ( ti+123 ti ) = e ( ti+123ti )
for the instance di at time ti , its state si is sampled from the conditional distribution ( 123 ) .
we use an exponential function as the kernel k ,
k ( t ) = e123t , t 123
and the concentration parameter is set to 123 .
this emulates the situation where new clusters are created from time to time , and a cluster stays alive better if many preceding instances are from the cluster .
if a new cluster c is created , we sample its multinomial distribution c from the base distribution g123
the base distribution g123 is a at dirichlet on a vocabulary of size three : g123 dir ( 123 , 123 , 123 ) , so that all multinomials are equally likely .
finally docu - ments are sampled from their corresponding multinomial , where all documents have the same length |d| .
we create two datasets with document length |d| equals 123 and 123 respectively , with everything else being the same .
given that the vocabulary size is 123 , they correspond to somewhat hard ( less words ) and easy ( more words ) datasets re - spectively .
figure 123 shows time vs .
cluster plots of the two datasets .
notice documents from the same cluster tend to group together in time , which ts our intuition on real world problems like emails .
during evaluation , the input to various algorithms are the documents di and their time stamps ti , and the goal is to infer the clustering si .
notice the true number of clusters is not given to the algorithms .
for the tdpm model , we assume we know the true base distribution g123 dir ( 123 , 123 , 123 ) ,
concentration parameter = 123 , and the kernel k ( t ) = e123t .
we run the gibbs sam - pler with initial states s123 = .
= sn = 123
each mcmc iteration updates s123 , .
, sn once , and thus consists of n gibbs steps .
we ignore the burn - in period of the rst 123 mcmc iterations , and then take out a sample of s123 , .
, sn every 123 iterations .
in this
experiment we take out 123 samples altogether .
we evaluate the performance of tdpm by three measures :
number of clusters discovered .
notice each sample s123 , .
, sn is a clustering of the data , and different samples may have different number of clusters .
in fact figure 123 ( a , b ) shows the distribution of number of clusters in the 123 samples , on the hard ( |d| = 123 ) and easy ( |d| = 123 ) synthetic datasets respectively .
the modes are at 123 and 123 , very close to the true values of 123 and 123 respectively .
confusion matrix .
one way to combine the samples with possibly different num - ber of clusters is to compute the n n confusion matrix m , where mij is the probability that i , j are in same cluster .
this can be easily estimated from the 123 samples by the frequency of i , j in the same cluster .
ideally m should be similar to the true confusion matrix m , dened by m ij = 123 if the true cluster has label si = sj , and 123 otherwise .
in figure 123 ( c , d ) we plot the true confusion matrices m .
notice we sort the instances by their true cluster for better visu - alization .
in figure 123 ( e , f ) we plot the tdpm confusion matrices , using the same order .
they are reasonably similar .
variation of information .
we compute the variation of information measure ( 123 ) between the true clustering and each sample clustering .
we list the mean and standard deviation for the two synthetic datasets : ( hard ) 123 123 , ( easy )
we compare tdpm to a standard dpm model , by using a step function as the ker - nel k .
again we assume we know the true base distribution g123 dir ( 123 , 123 , 123 ) , and concentration parameter = 123 .
the gibbs sampling is done exactly the same as in tdpm .
we nd that
number of clusters discovered .
figure 123 ( a , b ) shows the distribution of number of clusters with dpm .
dpm discovers fewer clusters than tdpm .
the modes are at 123 ( or 123 ) and 123 respectively , and the true values are 123 and 123
confusion matrix .
in figure 123 ( c , d ) we plot the dpm confusion matrices .
notice
they are much less similar to the true matrices .
variation of information .
with dpm we have ( hard ) 123 123 , ( easy ) 123 123 .
this means the sample clusterings are signicantly farther away from the true clustering , compared to tdpm .
to summarize , tdpm is better than the standard dpm model , when the instances
have a time dependency .
the tdpm model is a way to take time into consideration .
notice it is different than simply adding time as a new feature for cluster .
the tdpm is not time reversible nor exchangeable in general .
this is different from the standard dpm .
it is both a blessing and curse .
it allows for the modeling of time , but at the expense of computation .
there are many ways one can extend the tdpm model proposed here :
the time kernel k can have different forms .
for example , different clusters can have different decay rate .
more interestingly , k can even be periodic to model repetitive emails like weekly meeting announcements .
currently the models for each cluster are stationary and do not evolve over time .
this can potentially be relaxed .
one can have a generative model on time dependencies .
for example one can as - sume a poisson process on cluster , and then a non - homogeneous poisson process on the documents within the cluster .
