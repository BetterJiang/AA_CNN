we present a class of algorithms for independent component analysis ( ica ) which use contrast functions based on canonical correlations in a reproducing kernel hilbert space .
on the one hand , we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence .
on the other hand , building on recent developments in kernel methods , we show that these criteria and their derivatives can be computed eciently .
minimizing these criteria leads to ( cid : 123 ) exible and robust algorithms for ica .
we illustrate with simulations involving a wide variety of source distributions , showing that our algorithms outperform many of the presently known kernel methods , independent component analysis , blind source separation , mutual information , gram matrices , canonical correlations , semiparametric models , inte - gral equations , stiefel manifold , incomplete cholesky decomposition
independent component analysis ( ica ) is the problem of recovering a latent random vector x = ( x123; : : : ; xm ) > from observations of m unknown linear functions of that vector .
the components of x are assumed to be mutually independent .
thus , an observation y = ( y123; : : : ; ym ) > is modeled as :
y = ax;
where x is a latent random vector with independent components , and where a is an mm matrix of parameters .
given n independently , identically distributed observations of y , we hope to estimate a and thereby to recover the latent vector x corresponding to any particular y by solving a linear system .
by specifying distributions for the components xi , one obtains a parametric model that can be estimated via maximum likelihood ( bell and sejnowski , 123 , cardoso , 123 ) .
working with w = a123 as the parameterization , one readily obtains a gradient or xed - point algorithm that yields an estimate ^w and provides estimates of the latent components via ^x = ^w y ( hyvarinen et al . , 123 ) .
c ( cid : 123 ) 123 francis r .
bach and michael i .
jordan .
bach and jordan
in practical applications , however , one does not generally know the distributions of the components xi , and it is preferable to view the ica model in eq .
( 123 ) as a semiparamet - ric model in which the distributions of the components of x are left unspecied ( bickel et al . , 123 ) .
maximizing the likelihood in the semiparametric ica model is essentially equivalent to minimizing the mutual information between the components of the estimate ^x = ^w y ( cardoso , 123 ) .
thus it is natural to view mutual information as a contrast function to be minimized in estimating the ica model .
moreover , given that the mutual information of a random vector is nonnegative , and zero if and only if the components of the vector are independent , the use of mutual information as a function to be minimized is well motivated , quite apart from the link to maximum likelihood ( comon , 123 ) .
unfortunately , the mutual information is dicult to approximate and optimize on the basis of a nite sample , and much research on ica has focused on alternative contrast functions ( amari et al . , 123 , comon , 123 , hyvarinen and oja , 123 ) .
these have either been derived as expansion - based approximations to the mutual information , or have had a looser relationship to the mutual information , essentially borrowing its key property of being equal to zero if and only if the arguments to the function are independent .
the earliest ica algorithms were ( in retrospect ) based on contrast functions dened in terms of expectations of a single xed nonlinear function , chosen in an ad - hoc manner ( jut - ten and herault , 123 ) .
more sophisticated algorithms have been obtained by careful choice of a single xed nonlinear function , such that the expectations of this function yield a ro - bust approximation to the mutual information ( hyvarinen and oja , 123 ) .
an interesting feature of this approach is that links can be made to the parametric maximum likelihood formulation , in which the nonlinearities in the contrast function are related to the assumed densities of the independent components .
all of these developments have helped to focus attention on the choice of particular nonlinearities as the key to the ica problem .
in the current paper , we provide a new approach to the ica problem based not on a single nonlinear function , but on an entire function space of candidate nonlinearities .
in particular , we work with the functions in a reproducing kernel hilbert space , and make use of the \kernel trick " to search over this space eciently .
the use of a function space makes it possible to adapt to a variety of sources and thus makes our algorithms more robust to varying source distributions , as illustrated in section 123
we dene a contrast function in terms of a rather direct measure of the dependence of a set of random variables .
considering the case of two univariate random variables x123 and x123 , for simplicity , and letting f be a vector space of functions from r to r , dene the f - correlation f as the maximal correlation between the random variables f123 ( x123 ) and f123 ( x123 ) , where f123 and f123 range over f :
f = max
corr ( f123 ( x123 ) ; f123 ( x123 ) ) = max
( var f123 ( x123 ) ) 123=123 ( var f123 ( x123 ) ) 123=123
clearly , if the variables x123 and x123 are independent , then the f - correlation is equal to zero .
moreover , if the set f is large enough , the converse is also true .
for example , it is well known that if f contains the fourier basis ( all functions of the form x 123 ! ei ! x where ! 123 r ) , then f = 123 implies that x123 and x123 are independent .
to obtain a computationally tractable implementation of the f - correlation , we make use of reproducing kernel hilbert space ( rkhs ) ideas .
let f be an rkhs on r , let k ( x; y ) be
kernel independent component analysis
the associated kernel , and let ' ( x ) = k ( ; x ) be the feature map , where k ( ; x ) is a function in f for each x .
we then have the well - known reproducing property ( saitoh , 123 ) :
f ( x ) = h ' ( x ) ; fi;
123f 123 f; 123x 123 r :
corr ( f123 ( x123 ) ; f123 ( x123 ) ) = corr ( h ' ( x123 ) ; f123i;h ' ( x123 ) ; f123i ) :
consequently , the f - correlation is the maximal possible correlation between one - dimensional linear projections of ' ( x123 ) and ' ( x123 ) .
this is exactly the denition of the rst canonical correlation between ' ( x123 ) and ' ( x123 ) ( hotelling , 123 ) .
this suggests that we can base an ica contrast function on the computation of a canonical correlation in function space .
canonical correlation analysis ( cca ) is a multivariate statistical technique similar in spirit to principal component analysis ( pca ) .
while pca works with a single random vector and maximizes the variance of projections of the data , cca works with a pair of random vectors ( or in general with a set of m random vectors ) and maximizes correlation between sets of projections .
while pca leads to an eigenvector problem , cca leads to a generalized eigenvector problem .
finally , just as pca can be carried out eciently in an rkhs by making use of the \kernel trick " ( scholkopf et al . , 123 ) , so too can cca ( as we show in section 123 ) .
thus we can employ a \kernelized " version of cca to compute a ( cid : 123 ) exible contrast function for ica .
there are several issues that must be faced in order to turn this line of reasoning into an ica algorithm .
first , we must show that the f - correlation in fact has the properties that are required of a contrast function; we do this in section 123 .
second , we must show how to formulate the canonical correlation problem with m random variables , and show how to solve the problem eciently using kernel functions .
this is easily done , as we show in section 123 .
third , our method turns out to require the computation of generalized eigenvectors of matrices of size mnmn .
a naive implementation of our algorithm would therefore require o ( m123n 123 ) operations .
as we show in section 123 , however , by making use of incomplete cholesky decomposition we are able to solve the kernelized cca problem in time o ( n ( h ( n= ) ) 123 ) , where is a precision parameter and h ( t ) is a slowly growing function of t .
moreover , in computing the contrast function , the precision need only be linear in n ; consequently , we have a linear time algorithm .
finally , our goal is not simply that of computing the contrast function , but of optimizing it , and for this we require derivatives of the contrast function .
although incomplete cholesky factorization cannot be used directly for computing these derivatives , we are able to derive an algorithm for computing derivatives with similar linear complexity in n ( see section 123 ) .
there are a number of other interesting relationships between cca and ica that we explore in this paper .
in particular , for gaussian variables the cca spectrum ( i . e . , all of the eigenvalues of the generalized eigenvector problem ) can be used to compute the mutual information ( essentially as a product of these eigenvalues ) .
this suggests a general connection between our contrast function and the mutual information , and it also suggests an alternative contrast function for ica , one based on all of the eigenvalues and not simply the maximal eigenvalue .
we discuss this connection in section 123 .
the remainder of the paper is organized as follows .
in section 123 , we present background material on cca , rkhs methods , and ica .
section 123 provides a discussion of the contrast
bach and jordan
functions underlying our new approach to ica , as well as a high - level description of our ica algorithms .
we discuss the numerical linear algebra underlying our algorithms in section 123 , the optimization methods in section 123 , and the computational complexity in section 123
finally , comparative empirical results are presented in section 123 , and we conclude in section 123
in this section we provide enough basic background on canonical correlation , kernel methods and ica so as to make the paper self - contained .
for additional discussion of cca see borga et al .
( 123 ) , for kernel methods see scholkopf and smola ( 123 ) , and for ica see hyvarinen et al .
( 123 ) .
123 canonical correlation
given a random vector x , principal component analysis ( pca ) is concerned with nding a linear transformation such that the components of the transformed vector are uncorrelated .
thus pca diagonalizes the covariance matrix of x .
similarly , given two random vectors , x123 and x123 , of dimension p123 and p123 , canonical correlation analysis ( cca ) is concerned with nding a pair of linear transformations such that one component within each set of transformed variables is correlated with a single component in the other set .
thus , the correlation matrix between x123 and x123 is reduced to a block diagonal matrix with blocks of
size two , where each block is of the form ( cid : 123 ) 123
which are nonzero , are called the canonical correlations .
the i , at most p = minfp123; p123g of
as in the case of pca , cca can be dened recursively , component by component .
indeed , the rst canonical correlation can be dened as the maximum possible correlation between the two projections >123 x123 and >123 x123 of x123 and x123 :
( x123; x123 ) = max
corr ( >123 x123; >123 x123 )
cov ( >123 x123; >123 x123 )
var >123 x123=123var >123 x123=123 >123 c123=123>123 c123=123
where c = ( cid : 123 ) c123 c123
c123 c123 denotes the covariance matrix of ( x123; x123 ) .
taking derivatives
with respect to 123 and 123 , we obtain :
kernel independent component analysis
normalizing the vectors 123 and 123 by letting >123 c123 = 123 and >123 c123 = 123 , we see that cca reduces to the following generalized eigenvalue problem :
123 ( cid : 123 ) 123
123 = ( cid : 123 ) c123
c123 ( cid : 123 ) 123
this problem has p123 + p123 eigenvalues : f123;123; : : : ; p;p; 123; : : : ; 123g .
note that the generalized eigenvector problem in eq .
( 123 ) can also be written in following
c123 c123 ( cid : 123 ) 123 ( cid : 123 ) c123 c123
123 = ( 123 + ) ( cid : 123 ) c123
c123 ( cid : 123 ) 123
with eigenvalues f123 + 123; 123 123; : : : ; 123 + p; 123 p; 123; : : : ; 123g .
note , moreover , that the problem of nding the maximal generalized eigenvalue , max = 123 + max , where max is the rst canonical correlation , is equivalent to nding the minimal generalized eigenvalue , min = 123 max .
in fact , this latter quantity is bounded between zero and one , and turns out to provide a more natural upgrade path when we consider the generalization to more than two variables .
thus henceforth our computational task will be that of nding minimum generalized eigenvalues .
123 . 123 generalizing to more than two variables
there are several ways to generalize cca to more than two sets of variables ( kettenring , 123 ) .
the generalization that we consider in this paper , justied in appendix a , is the fol - lowing .
given m multivariate random variables , x123; : : : ; xm , we nd the smallest generalized eigenvalue ( x123; : : : ; xm ) of the following problem :
or , in short , c = d , where c is the covariance matrix of ( x123; x123; : : : ; xm ) and d is the block - diagonal matrix of covariances of the individual vectors xi .
as we discuss in appendix a , the minimal generalized eigenvalue has the xed range ( 123; 123 ) , whereas the maximal generalized eigenvalue has a range dependent on the dimen - sions of the variables .
thus the minimal generalized eigenvalue is more convenient for our
123 reproducing kernel hilbert spaces let k ( x; y ) be a mercer kernel ( saitoh , 123 ) on x = rp , that is , a function for which the gram matrix kij = k ( xi; xj ) is positive semidenite for any collection fxigi=123; : : : ;n in x .
corresponding to any such kernel k there is a map ' from x to a feature space f , such
that is , the kernel can be used to evaluate an inner product in the feature space .
this is often referred to as the \kernel trick . "
k ( x; y ) = h ' ( x ) ; ' ( y ) i :
bach and jordan
one particularly attractive instantiation of such a feature space is the reproducing kernel hilbert space ( rkhs ) associated with k .
consider the set of functions fk ( ; x ) : x 123 xg , where the dot represents the argument to a given function and x indexes the set of functions .
dene a linear function space as the span of such functions .
such a function space is unique and can always be completed into a hilbert space ( saitoh , 123 ) .
the crucial property of these hilbert spaces is the \reproducing property " of the kernel :
f ( x ) = hk ( ; x ) ; fi 123f 123 f :
note in particular that if we dene ' ( x ) = k ( ; x ) as a map from the input space into the rkhs , then we have :
h ' ( x ) ; ' ( y ) i = hk ( ; x ) ; k ( ; y ) i = k ( x; y ) ;
and thus ' ( x ) = k ( ; x ) is indeed an instantiation of the \kernel trick . " for concreteness we restrict ourselves mostly to translation - invariant kernels in this paper; that is , to kernel functions of the form k ( x; y ) = k ( x y ) , where k is a function from rp to r .
in this case , the feature space f has innite dimension and the rkhs can be described succinctly using fourier theory ( girosi et al . , 123 ) .
indeed , for a given function k , f is composed of functions f 123 l123 ( rp ) such that :
j ^f ( ! ) j123
d ! < 123;
where ^f ( ! ) is the fourier transform of f and ( ! ) is the fourier transform of k ( which must be real and positive to yield a mercer kernel ) .
this interpretation shows that functions in the rkhs f have a fourier transform that decays rapidly , implying that f is a space of
finally , consider the case of an isotropic gaussian kernel in p dimensions :
k ( x; y ) = g ( cid : 123 ) ( x y ) = exp ( cid : 123 )
123 ( cid : 123 ) 123jjx yjj123 : 123 jj ! jj123 , and the feature in this case the fourier transform is ( ! ) = ( 123 ( cid : 123 ) 123 ) p=123 exp ( cid : 123 ) 123 space f ( cid : 123 ) contains functions whose fourier transform decays very rapidly .
alternatively , functions in f ( cid : 123 ) can be seen as convolutions of functions of l123 ( rp ) with a gaussian kernel ( cid : 123 ) 123jjxjj123
note that , as ( cid : 123 ) increases from 123 to 123 , the functions g ( cid : 123 ) =p123 g ( cid : 123 ) =p123 ( x ) = exp 123 range from an impulse to a constant function , and the function spaces f ( cid : 123 ) decrease from l123 ( rp ) to ? .
123 independent component analysis
the independent component analysis ( ica ) problem that we consider in this paper is based on the following statistical model :
where x is a latent random vector with m independent components , a is an mm matrix of parameters , assumed invertible , and y is an observed vector with m components
y = ax;
kernel independent component analysis
on a set of n independent , identically distributed observations of the vector y , we wish to estimate the parameter matrix a . 123 from the estimate of a we can estimate the values of x corresponding to any observed y by solving a linear system of equations .
the distribution of x is assumed unknown , and we do not care to estimate this distribution .
thus we formulate ica as a semiparametric model ( bickel et al . , 123 ) .
our goal is to nd a maximum likelihood estimate of a .
let us rst consider the population version of ica , in which p ( y ) denotes the true distribution of y , and p ( y ) denotes the model .
we wish to minimize the kullback - leibler ( kl ) divergence between the distributions p and p : d ( p ( y ) jj p ( y ) ) .
dene w = a123 , so that x = w y .
since the kl divergence is invariant with respect to an invertible transformation , we can apply w to y in both arguments of the kl divergence , which implies our problem is equivalent to that of minimizing d ( p ( x ) jj p ( x ) ) .
let ~ p ( x ) denote the joint probability distribution obtained by taking the product of the marginals of p ( x ) .
we have the following decomposition of the kl divergence ( see cover and thomas , 123 ) :
d ( p ( x ) jj p ( x ) ) = d ( p ( x ) jj ~ p ( x ) ) + d ( ~ p ( x ) jj p ( x ) ) ;
for any distribution p ( x ) with independent components .
consequently , for a given a , the minimum over all possible p ( x ) is attained precisely at p ( x ) = ~ p ( x ) , and the minimal value is d ( p ( x ) jj ~ p ( x ) ) , which is exactly the mutual information between the components of x = w y .
thus , the problem of maximizing the likelihood with respect to w is equivalent to the problem of minimizing the mutual information between the components of x = w y .
ica can be viewed as a generalization of principal components analysis ( pca ) .
while pca yields uncorrelated components , and is based solely on second moments , ica yields independent components , and is based on the mutual information , which is in general a function of higher - order moments .
clearly an ica solution is also a pca solution , but the converse is not true .
in practice , ica algorithms often take advantage of this relationship , treating pca as a preprocessing phase .
thus one whitens the random variable y , multiply - ing y by a matrix p such that ~ y = p y has an identity covariance matrix ( p can be chosen as the inverse of the square root of the covariance matrix of y ) .
there is a computational advantage to this approach : once the data are whitened , the matrix w is necessarily or - thogonal ( hyvarinen et al . , 123 ) .
this reduces the number of parameters to be estimated , and , as we discuss in section 123 , enables the use of ecient optimization techniques based on the stiefel manifold of orthogonal matrices .
in practice we do not know p ( y ) and thus the estimation criteria|mutual informa - tion or kl divergence|must be replaced with empirical estimates .
while in principle one could form an empirical mutual information or empirical likelihood , which is subsequently optimized with respect to w , the more common approach to ica is to work with approx - imations to the mutual information ( amari et al . , 123 , comon , 123 , hyvarinen , 123 ) , or to use alternative contrast functions ( jutten and herault , 123 ) .
for example , by using edgeworth or gram - charlier expansions one can develop an approximation of the mutual
the identiability of the ica model has been discussed by comon ( 123 ) .
brie ( cid : 123 ) y , the matrix a is identiable , up to permutation and scaling of its columns , if and only if at most one of the component distributions p ( xi ) is gaussian .
bach and jordan
information in terms of skew and kurtosis .
forming an empirical estimate of the skew and kurtosis via the method of moments , one obtains a function of w that can be optimized .
we propose two new ica contrast functions in this paper .
the rst is based on the f - correlation , which , as we brie ( cid : 123 ) y discussed in section 123 , can be obtained by computing the rst canonical correlation in a reproducing kernel hilbert space .
the second is based on computing not only the rst canonical correlation , but the entire cca spectrum , a quantity known as the \generalized variance . " we describe both of these contrast functions , and their relationship to the mutual information , in the following section .
kernel independent component analysis
we refer to our general approach to ica , based on the optimization of canonical correla - tions in a reproducing kernel hilbert space , as kernelica .
in this section we describe two contrast functions that exemplify our general approach , and we present the resulting
123 the f - correlation we begin by studying the f - correlation in more detail .
we restrict ourselves to two random variables in this section and present the generalization to m variables in section 123 . 123
theorem 123 let x123 and x123 be random variables in x = rp .
let k123 and k123 be mercer kernels with feature maps ' 123 , ' 123 and feature spaces f123 , f123 rx .
then the canonical correlation f between ' 123 ( x123 ) and ' 123 ( x123 ) , which is dened as
corr ( h ' 123 ( x123 ) ; f123i;h ' 123 ( x123 ) ; f123i ) ;
is equal to
proof this is immediate from the reproducing property ( 123 ) .
the choice of kernels k123 and k123 species the sets f123 and f123 of functions that we use to characterize independence , via the correlation between f123 ( x123 ) and f123 ( x123 ) .
while in general we can use dierent kernels for x123 and x123 , for notational simplicity we restrict ourselves in the remainder of the paper to cases in which the two kernels and the two feature spaces are equal , denoting them as k and f , respectively .
note that the larger f is , the larger the value of the f - correlation .
in particular , for translation - invariant kernels , the f - correlation increases as ( cid : 123 ) decreases .
but for any value of ( cid : 123 ) , the f - correlation turns out to provide a sound basis for assessing independence , as the following theorem makes precise in the case of the univariate gaussian kernel :
theorem 123 ( independence and f - correlation ) if f is the rkhs corresponding to a gaussian kernel on x = r , f = 123 if and only if the variables y123 and y123 are independent .
kernel independent component analysis
proof we mentioned earlier that the rst implication is trivial .
let us now assume that f = 123
since f is a vector space , we have :
( f123;f123 ) 123ff jcorr ( f123 ( x123 ) ; f123 ( x123 ) ) j;
which implies cov ( f123 ( x123 ) ; f123 ( x123 ) ) = 123 , or , equivalently , e ( f123 ( x123 ) f123 ( x123 ) ) = e ( f123 ( x123 ) ) e ( f123 ( x123 ) ) , for all f123; f123 123 f .
for any given ! 123 123 r and > 123 , the function x 123 ! ex123=123 123 ei ! 123x has a fourier transform equal to p123 e 123 ( ! ! 123 ) 123=123 , and thus satises the condition in eq .
( 123 ) as long as > ( cid : 123 ) =p123
consequently , if > ( cid : 123 ) =p123 , the function belongs to f and we have ,
for all real ! 123 and ! 123 :
123 ) =123 123 = eei ! 123x123ex123
123=123 123 eei ! 123x123ex123
123=123 123 :
letting tend to innity , we nd that for all ! 123 and ! 123 :
which implies that x123 and x123 are independent ( durrett , 123 ) .
eei ! 123x123+i ! 123x123 = eei ! 123x123 eei ! 123x123
note that when the function space has nite dimension , theorem 123 does not hold .
in particular , for polynomial kernels ( e . g . , scholkopf and smola , 123 ) , f is a nite - dimensional space of polynomials , and thus the f - correlation does not characterize in - dependence .
nonetheless , as we show in section 123 , polynomial kernels can still be usefully employed in the ica setting to provide heuristic initialization procedures for optimizing a
123 estimation of the f - correlation to employ the f - correlation as a contrast function for ica , we need to be able to compute canonical correlations in feature space .
we also need to be able to optimize the canonical correlation , but for now our focus is simply that of computing the canonical correlations in an rkhs .
( we discuss the optimization problem in section 123 ) .
in fact our goal is not solely computational , it is also statistical .
while thus far we have dened f - correlation in terms of a population expectation , we generally do not have access to the population but rather to a nite sample .
thus we need to develop an empirical estimate of the f - correlation .
indeed , the problem of working in an rkhs and the issue of developing a statistical estimate of a functional dened in an rkhs are closely tied| by considering the empirical version of the f - correlation we work in a nite - dimensional subspace of the rkhs , and can exploit its useful geometric properties while avoiding issues having to do with its ( possible ) innite dimensionality .
thus we will develop a \kernelized " version of canonical correlation , which involves two aspects : working with an empirical sample , and working in a feature space .
we proceed by rst presenting a naive \kernelization " of the population f - correlation .
for reasons that we will discuss , this naive kernelization does not provide a generally useful estimator , but it does serve as a guide for developing a regularized kernelization that does provide a useful estimator .
it is this regularized , kernelized canonical correlation that provides the foundation for the algorithms that we present in the remainder of the paper .
bach and jordan
123 . 123 kernelization of cca
in the case of two variables the goal is to maximize the correlation between projections of the data in the feature space .
a direct implementation would simply map each data point to feature space and use cca in the feature space .
this is likely to be very inecient computationally , however , if not impossible , and we would prefer to perform all of our calculations in the input space . 123
123; : : : ; xn
123; : : : ; xn 123 ) ; : : : ; ' ( xn
123 g and fx123 123 g denote sets of n empirical observations of x123 and x123 , respectively , and let f ' ( x123 123 ) g and f ' ( x123 123 ) g denote the corresponding images in feature space .
suppose ( momentarily ) that the data are centered in feature space 123 ) = pn 123 ) = 123 ) .
we let ^f ( x123; x123 ) denote the empirical canonical correlation; that is , the canonical correlation based not on population covariances but on empirical covariances .
since , as we shall see , ^f ( x123; x123 ) depends only on the gram matri - ces k123 and k123 of these observations , we also use the notation ^f ( k123; k123 ) to denote this
123 ) ; : : : ; ' ( xn
as in kernel pca ( scholkopf et al . , 123 ) , the key point to notice is that we only need to consider the subspace of f that contains the span of the data .
for xed f123 and f123 , the empirical covariance of the projections in feature space can be written :
dcov ( h ' ( x123 ) ; f123i;h ' ( x123 ) ; f123i ) =
123 ) ; f123e :
let s123 and s123 represent the linear spaces spanned by the ' - images of the data points .
thus 123 ) + f ? 123 , where f ? 123 and f ? 123 are orthogonal to s123 and s123 , respectively .
we have :
we can write f123 =pn dcov ( h ' ( x123 ) ; f123i;h ' ( x123 ) ; f123i ) =
123 ) + f ? 123 and f123 =pn
where k123 and k123 are the gram matrices associated with the data sets fxi respectively .
we also obtain :
123g and fxi
cvar ( h ' ( x123 ) ; f123i ) = cvar ( h ' ( x123 ) ; f123i ) =
melzer et al .
( 123 ) and akaho ( 123 ) have independently derived the kernelized cca algorithm for two variables that we present in this section .
a similar but not identical algorithm , also restricted to two variables , has been described by fyfe and lai ( 123 ) .
kernel independent component analysis
putting these results together , our kernelized cca problem becomes that of performing
the following maximization :
^f ( k123; k123 ) = max
123 123 ) 123=123 ( >123 k123
but this is equivalent to performing cca on two vectors of dimension n , with covariance
thus we see that we can perform a kernelized version
of cca by solving the following generalized eigenvalue problem :
123 = ( cid : 123 ) k123
123 ( cid : 123 ) 123
matrix equal to ( cid : 123 ) k123
based on the gram matrices k123 and k123
if the points ' ( xk
i ) are not centered , then although it is impossible to actually cen - ter them in feature space , it is possible to nd the gram matrix of the centered data points ( scholkopf et al . , 123 ) .
that is , if k is the nn gram matrix of the non - centered data points , then the gram matrix ek of the centered data points is ek = n123kn123 where n123 = i 123 n 123 is a constant singular matrix . 123 whenever we use a gram matrix , we assume that it has been centered in this way .
unfortunately , the kernelized cca problem in eq .
( 123 ) does not provide a useful estimate of the population canonical correlation in general .
this can easily be seen by considering a geometric interpretation of eq .
in particular , if we let v123 and v123 denote the subspaces of rn generated by the columns of k123 and k123 , respectively , then we can rewrite eq
^f ( k123; k123 ) = max
( v>123 v123 ) 123=123 ( v>123 v123 ) 123=123
which is exactly the cosine of the angle between the two subspaces v123 and v123 ( golub and loan , 123 ) .
from this interpretation , it is obvious that if the matrices k123 and k123 were invertible , then the subspaces v123 and v123 would be equal to rn and thus the angle would always be equal to zero , whatever k123 and k123 are .
the matrices k123 and k123 do not have full rank , because they are centered gram matrices .
however , centering is equivalent to projecting the column spaces v123 and v123 onto the subspace orthogonal to the vector composed of all ones; therefore , if the non - centered gram matrices are invertible ( which occurs , for example , if a gaussian kernel is used , and the data points are distinct ) , the two column spaces are identical and the angle between them is still equal to zero , resulting in a canonical correlation estimate that is always equal to one .
thus the naive kernelization in eq .
( 123 ) does not provide a useful estimator for general kernels .
it does , however , provide a guide to the design of a useful regularized estimator , as we now discuss .
our regularization will penalize the rkhs norms of f123 and f123 , and thus
the matrix 123 is an n n matrix composed of ones .
note that 123 = n 123
bach and jordan
provide control over the statistical properties of kernelica . 123 in particular , we dene the
f = max
( var f123 ( x123 ) + jjf123jj123
) 123=123 ( var f123 ( x123 ) + jjf123jj123
where is a small positive constant .
note that the regularized f - correlation inherits the independence characterization property of the f - correlation .
in order to estimate it from a nite sample , we expand var f123 ( x123 ) + jjf123jj123
f up to second order in , to obtain :
var f123 ( x123 ) + jjf123jj123
123 123 + >123 k123
>123 ( k123 +
thus the regularized kernel cca problem becomes :
f ( k123; k123 ) = max
( >123 ( k123 + n
123 i ) 123 ) 123=123 ( >123 ( k123 + n
with its equivalent formulation as a generalized eigenvalue problem as follows :
123 = ( cid : 123 ) ( k123 + n
123 i ) 123 ( cid : 123 ) 123
( k123 + n
as we shall see in section 123 , the formulation of the regularized kcca problem in eq .
( 123 ) can be reduced to a ( simple ) eigenvalue problem that is well - behaved computationally .
in addition , the regularized rst canonical correlation has an important statistical property that the unregularized canonical correlation does not have|it turns out to be a consistent estimator of the regularized f - correlation .
( that is , when the number of samples n tends to innity , the estimate converges in probability to the population quantity .
the proof of this result is beyond the scope of the paper ) .
123 . 123 generalizing to more than two variables
the generalization of regularized kernelized canonical correlation to more than two sets of variables is straightforward , given our generalization of cca to more than two sets of variables .
we simply denote by k the mnmn matrix whose blocks are ( k ) ij = kikj , for i 123= j , and ( k ) ii = ( ki + n 123 i ) 123 , and we let d denote the mn mn block - diagonal
intuitively , without any restriction on jjf123jjf and jjf123jjf , it is possible to separate one data point xk123 from the other points fxk 123 g with a function f123
for those f123 and f123 , we get a correlation equal to one and thus obtain no information about the dependence of x123 and x123
123 g with a function f123 , while separating xk123
from the other fxk
( km + n
( k123 + n
( km + n
( k123 + n
kernel independent component analysis
matrix with blocks ( ki + n
we obtain the following generalized eigenvalue problem :
( k123 + n
( k123 + n
or k = d for short .
the minimal eigenvalue of this problem will be denoted f ( k123; : : : ; km ) and referred to as the rst kernel canonical correlation .
we also extend our earlier terminology and refer to this eigenvalue as an ( empirical ) f - correlation .
note that in the two - variable case we dened a function f ( x123; x123 ) that depends on the covariances of the random variables ' ( x123 ) and ' ( x123 ) , and we obtained an empirical contrast f ( x123; x123 ) from f ( x123; x123 ) by substituting empirical covariances for population co - variances and introducing regularization . 123 in the m - variable case , we have ( thus far ) dened only the empirical function ^ f ( k123; : : : ; km ) .
in appendix a . 123 , we study the properties of the population version of this quantity , f ( x123; : : : ; xm ) , by relating f ( x123; : : : ; xm ) to a generalized notion of \correlation " among m variables .
by using this denition , we show that f ( x123; : : : ; xm ) is always between zero and one , and is equal to one if and only if the variables x123; : : : ; xm are pairwise independent .
thus we obtain an analog of theorem 123 for the m - variable case .
for reasons that will become clear in section 123 , where we discuss a relationship be - tween canonical correlations and mutual information , it is convenient to dene our contrast functions in terms of the negative logarithm of canonical correlations .
thus we dene a con - trast function if ( x123; : : : ; xm ) = 123 123 log f ( x123; : : : ; xm ) and ask to minimize this function .
the result alluded to in the preceding paragraph shows that this quantity is nonnegative , and equal to zero if and only if the variables x123; : : : ; xm are pairwise independent .
for the empirical contrast function , we will use the notation ^if ( k123; : : : ; km ) = 123 log ^ f ( k123; : : : ; km ) , emphasizing the fact that this contrast function depends on the data only through the gram matrices .
123 . 123 relationship to ace
given a response variable y and predictor variables x123; : : : ; xp , the alternating conditional expectation ( ace ) algorithm ( breiman and friedman , 123 ) minimizes
in fact the word \contrast function " is generally reserved for a quantity that depends only on data and parameters , and is to be extremized in order to obtain parameter estimates .
thus ^ f ( x123; x123 ) is a contrast function .
by also refering to the population version f ( x123; x123 ) as a \contrast function , " we are abusing terminology .
but this is a standard abuse in the ica literature , where , for example , the mutual information is viewed as a \contrast function . "
bach and jordan
input : data vectors y123; y123; : : : ; yn
kernel k ( x; y )
whiten the data 123
minimize ( with respect to w ) the contrast function c ( w ) dened as :
compute the centered gram matrices k123; k123; : : : ; km of the estimated
dene ^
sources fx123; x123; : : : ; xng , where xi = w yi vector equation k = d
dene c ( w ) = ^if ( k123; : : : ; km ) = 123
f ( k123; : : : ; km ) as the minimal eigenvalue of the generalized eigen -
123 log ^
f ( k123; : : : ; km )
figure 123 : a high - level description of the kernelica - kcca algorithm for estimating the
parameter matrix w in the ica model .
with respect to the real - valued functions ( cid : 123 ) , `123; : : : ; `p .
the transformations obtained pro - duce the best - tting additive models .
for the bivariate case , p = 123 , minimization of e can be shown to be equivalent to maximization of the correlation corr ( ( cid : 123 ) ( y ) ; ` ( x ) ) .
it is thus equivalent to the f - correlation problem .
in the population case , the optimization can be done in l123 ( r ) , while in presence of a nite sample , the function spaces that are considered by breiman and friedman ( 123 ) are smooth function spaces similar to the rkhs we use in this paper .
a strong link exists between kernel cca and ace in this case : one step of the ace algorithm is equivalent to one step of a power method algorithm for computing the largest generalized eigenvalue in eq .
( 123 ) ( buja , 123 , hastie and tibshirani , 123 ) . 123
123 the kernelica - kcca algorithm
let us now apply the machinery that we have developed to the ica problem .
given a set of data vectors y123; y123; : : : ; yn , and given a parameter matrix w , we set xi = w yi , for each i , and thereby form a set of estimated source vectors fx123; x123; : : : ; xng .
the m components of these vectors yield a set of m gram matrices , k123; k123; : : : ; km , and these gram matrices ( which depend on w ) dene the contrast function c ( w ) = ^if ( k123; : : : ; km ) .
we obtain an ica algorithm by minimizing this function with respect to w .
a high - level description of the resulting algorithm , which we refer to as kernelica -
kcca , is provided in figure 123
we note in passing that the implementation of kernel cca using low - rank approximations of gram matrices that we present in section 123 transfers readily to the general setting of generalized additive models based on kernel smoothers ( hastie and tibshirani , 123 ) , thus enabling a fast implementation of the tting procedure for such models .
kernel independent component analysis
note that kernelica - kcca is not simply a \kernelization " of an extant ica algo - rithm .
instead , we use kernel ideas to characterize independence and thus to dene a new contrast function for ica .
as with alternative approaches to ica , we minimize this contrast function with respect to the demixing matrix w .
we still have a signicant amount of work to do to turn the high - level description in figure 123 into a practical algorithm .
the numerical linear algebra and optimization procedures that complete our description of the algorithm are presented in sections 123 and 123
before turning to those details , however , we turn to the presentation of an alternative contrast function based on generalized variance .
123 kernel generalized variance
as we have discussed , the mutual information provides a natural contrast function for ica , because of its property of being equal to zero if and only if the components are independent , and because of the link to the semiparametric likelihood .
as we show in this section , there is a natural generalization of the f - correlation that has a close relationship to the mutual information .
we develop this generalization in this section , and use it to dene a second ica contrast function .
our generalization is inspired by an interesting link that exists between canonical corre - lations and mutual information in the case of gaussian variables .
as we show in appendix a , for jointly - gaussian variables x123 and x123 , the mutual information , i ( x123; x123 ) , can be written
i ( x123; x123 ) =
where i are the canonical correlations .
thus cca can be used to compute the mutual information between a pair of gaussian variables .
moreover , appendix a also shows that this link can be extended to the mutual information between m variables .
thus , the m - way mutual information between m gaussian random variables , i ( x123; x123; : : : ; xm ) , can be obtained from the set of eigenvalues obtained from the generalized eigenvector problem c = d that we dened in section 123 . 123
in particular , appendix a shows the following :
i ( x123; x123; : : : ; xm ) =
where i are the generalized eigenvalues of c = d .
this result suggests that it may be worth considering a contrast function based on more than the rst canonical correlation , and holds open the possibility that such a contrast func - tion , if based on the nonlinearities provided by an rkhs , might provide an approximation to the mutual information between non - gaussian variables .
let us dene the generalized variance associated with the generalized eigenvector prob - lem c = d as the ratio ( det c ) = ( det d ) .
the result in eq .
( 123 ) shows that for gaussian variables the mutual information is equal to minus one - half the logarithm of the generalized
we make an analogous denition in the kernelized cca problem , dening the kernel generalized variance to be the product of the eigenvalues of the generalized eigenvector
bach and jordan
figure 123 : the mutual information i ( x123; x123 ) ( dashed ) , the approximation if ( x123; x123 ) for ( cid : 123 ) = : 123; : 123; 123; 123; 123 ( dotted ) , and the limit j ( x123; x123 ) as ( cid : 123 ) tends to zero ( solid ) .
the abscissa is the angle of the rst independent component in a two - source ica problem .
as ( cid : 123 ) decreases , if increases towards j .
see the text for details .
problem in eq .
( 123 ) , or equivalently the ratio of determinants of the matrices in this problem .
that is , given the generalized eigenvector problem k = d of eq .
( 123 ) , we dene :
f ( k123; : : : ; km ) =
as the kernel generalized variance .
we also dene a contrast function ^if ( k123; : : : ; km ) :
^if ( k123; : : : ; km ) =
f ( k123; : : : ; km ) ;
by analogy with the mutual information for the gaussian case .
although we have proceeded by analogy with the gaussian case , which is of little interest in the ica setting , it turns out that ^if ( k123; : : : ; km ) has as its population counterpart a function if ( x123; : : : ; xm ) that is actually closely related to the mutual information between the original non - gaussian variables in the input space .
the proof of this result is sketched in appendix b for the gaussian kernel .
in particular , in the case of two variables ( m = 123 ) , we show that , as ( cid : 123 ) tends to zero , if ( x123; : : : ; xm ) tends to a limit j ( x123; : : : ; xm ) that is equal to the mutual information up to second order , when we expand the mutual information around distributions that factorize .
our result is illustrated in figure 123
we compute the mutual information for a whitened ica problem with two known sources and two components , as the angle ( cid : 123 ) of the estimated rst component ranges from 123 to 123 degrees , with the independent component occurring at 123 degrees .
the graph plots the true mutual information i ( x123; x123 ) , the approximation if ( x123; x123 ) , for various values of ( cid : 123 ) , and the limit j ( x123; x123 ) .
the close match of the shape of j ( x123; x123 ) and the mutual information is noteworthy .
kernel independent component analysis
input : data vectors y123; y123; : : : ; yn
kernel k ( x; y )
whiten the data 123
minimize ( with respect to w ) the contrast function c ( w ) dened as :
compute the centered gram matrices k123; k123; : : : ; km of the estimated
sources fx123; x123; : : : ; xng , where xi = w yi b .
dene ^ f ( k123; : : : ; km ) = detk= detd c .
dene c ( w ) = ^if ( k123; : : : ; km ) = 123
123 log ^
f ( k123; : : : ; km )
figure 123 : a high - level description of the kernelica - kgv algorithm for estimating the
parameter matrix w in the ica model .
123 the kernelica - kgv algorithm in the previous section , we dened an alternative contrast function , ^if ( x123; : : : ; xm ) , in terms of the generalized variance associated with the generalized eigenvector problem k = d .
essentially , instead of computing only the rst eigenvalue of this problem , as in the case of the f - correlation contrast function , we compute the entire spectrum .
as we show in section 123 , this does not increase the practical running time complexity of the algorithm .
based on this contrast function , we dene a second kernelica algorithm , the kernelica - kgv algorithm outlined in figure 123
in summary , we have dened two kernelica algorithms , both based on contrast functions dened in terms of the eigenvalues of the generalized eigenvector problem k = d .
we now turn to a discussion of the computational methods by which we evaluate and optimize these contrast functions .
computational issues
the algorithms that we have presented involve nding generalized eigenvalues of matrices of dimension mn mn , where n is the number of data points and m the number of sources .
a naive implementation of these algorithms would therefore scale as o ( m123n 123 ) , a computational complexity whose cubic growth in the number of data points would be a serious liability in applications to large data sets .
as noted by several researchers , however , the spectrum of gram matrices tends to show rapid decay , and low - rank approximations of gram matrices can therefore often provide sucient delity for the needs of kernel - based algorithms ( smola and scholkopf , 123 , williams and seeger , 123 ) .
indeed , building on these observations , we describe an implementation of kernelica whose computational complexity is linear in the number of data points .
bach and jordan
we have two goals in this section .
the rst is to overview theoretical results that support the use of low - rank approximations to gram matrices .
our presentation of these results will be concise , with a detailed discussion deferred to appendix c .
second , we present a kernelica implementation based on low - rank approximations obtained from incomplete cholesky decomposition .
we show both how to compute the kernelica contrast functions , and how to compute derivatives of the contrast functions .
in appendix c , we present theoretical results that show that in order to achieve a given required precision , the rank m of an approximation to a gram matrix k can be chosen as m = h ( n= ) , where h ( t ) is a function that depends on the underlying distribution p ( x ) of the data .
moreover , the growth of h ( t ) as t tends to innity depends only on the decay of p ( x ) as jxj tends to innity .
in particular , in the univariate case with a gaussian kernel , when this decay is exponential ( gaussian - like ) , we have h ( t ) = o ( log t ) .
when the decay is polynomial , as xd , then h ( t ) = o ( t123=d+ " ) , for arbitrary " > 123
these results imply that if we require a constant precision , it suces to nd an approximation of rank m = o ( log n ) , for exponentially - decaying input distributions , and rank m = o ( n 123=d+ " ) for polynomially - decaying input distributions .
these results are applicable to any method based on gram matrices over input spaces of small dimension , and thus we can expect that kernel algorithms should generally be able to achieve a substantial reduction in complexity via approximations whose rank grows slowly with respect to n .
we will show separately in section 123 , however , that in the context of f - correlation and the kgv , the precision can be taken to be linear in n .
this implies that the rank of the approximation can be taken to be bounded by a constant in the ica setting , and provides an even stronger motivation for basing an implementation of kernelica on low - rank
123 incomplete cholesky decomposition we aim to nd low - rank approximations of gram matrices of rank m n .
note that even calculating a full gram matrix is to be avoided because it is already an o ( n 123 ) opera - tion .
fortunately , the fact that gram matrices are positive semidenite is a rather strong constraint , allowing approximations to gram matrices to be found in o ( m 123n ) operations .
following fine and scheinberg ( 123 ) and cristianini et al .
( 123 ) , the particular tool that we employ here is the incomplete cholesky decomposition , commonly used in implemen - tations of interior point methods for linear programming ( wright , 123 ) .
alternatives to incomplete cholesky decomposition are provided by methods based on the nystrom ap - proximation ( smola and scholkopf , 123 , williams and seeger , 123 ) .
a positive semidenite matrix k can always be factored as gg> , where g is an nn matrix .
this factorization can be found via cholesky decomposition ( which is essentially a variant of gaussian elimination ) .
our goal , however , is to nd a matrix eg of size n m , for small m , such that the dierence k egeg> has norm less than a given value
incomplete cholesky decomposition diers from standard cholesky decomposition in that all pivots that are below a certain threshold are simply skipped .
if m is the number
can be achieved via incomplete cholesky decomposition .
kernel independent component analysis
of non - skipped pivots , then we obtain a lower triangular matrix eg with only m nonzero
columns .
symmetric permutations of rows and columns are necessary during the factoriza - tion if we require the rank to be as small as possible ( golub and loan , 123 ) .
in that case , the stopping criterion involves the sum of remaining pivots .
an algorithm for incomplete cholesky decomposition is presented in figure 123
the algorithm involves picking one column of k at a time , choosing the column to be added by greedily maximizing a lower bound on the reduction in the error of the approximation .
overall complexity is o ( m 123n ) .
ranking of the n l vectors that might be added in the following step is done by comparing
after l steps , we have an approximation of the form ekl = egleg>l , where gl is n l .
the the diagonal elements of the remainder matrix k egleg>l .
each of these elements requires o ( l ) operations to compute .
moreover , the update of egl has a cost of o ( ln ) , so that the
the incomplete cholesky method has many attractive features .
not only is its time complexity o ( m 123n ) , but also the only elements of k that are needed in memory are the diagonal elements ( which are equal to one for gaussian kernels123 ) .
most of the other elements are never used and those that are needed can be computed on demand .
the storage requirement is thus o ( m n ) .
also , the number m can be chosen online such that the approximation is as tight as desired . 123
123 solving kernel cca
before turning to a discussion of how to use incomplete cholesky decomposition to com - pute the eigenstructure needed for our ica contrast functions , we discuss the generalized eigenvector problem of eq .
( 123 ) , k = d , in more detail .
owing to the regularization , we can apply a classical method for solving such a problem by nding a matrix c such that d = c>c , dening = c , and thereby transforming the problem into a standard eigenvector problem c>kc123 = .
our kernelized cca problem thus reduces to nding the smallest eigenvalue of the matrix :
ek = d123=123
123 i ) 123ki .
if we have an eigenvector ~ of ek ,
123 i ) 123 ~ i , with the same
where r ( ki ) = ki ( ki + n then we have a generalized eigenvector dened by i = ( ki + n
123 i ) 123 = ( ki + n
eigenvalue .
in the case of the kgv problem , we need to compute detek .
or one , via the function 123 ! = ( + n
our regularization scheme has the eect of shrinking each eigenvalue of ki towards zero 123 ) .
consequently , all eigenvalues less than a small
centering , which would make the diagonal elements dierent from one and make the other elements
harder to compute , can be done easily after the cholesky decomposition .
note that no theoretical bound is available concerning the relation between m and the optimal rank m for a given precision .
in our empirical work , however , we always obtained a rank very close to the optimal one .
we believe this is due to the fact that our gram matrices have a spectrum that decays very rapidly .
indeed , as pointed out by wright ( 123 ) , a signicant eigengap ensures that incomplete cholesky has small numerical error and yields a good approximation .
bach and jordan
input : nn semidenite positive matrix k
j=i gjj >
initialization : i = 123 , k123 = k , p = i , for j 123 ( 123; n ) , gjj = kjj
find best new element : j = arg maxj123 ( i;n ) gjj update permutation p :
set pii = 123 , pjj = 123 and pij = 123 , pji = 123
permute elements i and j in k123 :
column k123 : n;i $ k123 : n;j row k123i;123 : n $ k123j;123 : n
update ( due to new permutation ) the already calculated elements
of g : gi;123 : i $ gj;123 : i
gi+123 : n;i = 123
set gii =pk123ii calculate ith column of g :
update only diagonal elements :
for j 123 ( i + 123; n ) , gjj = kjj pi
i i + 123
output p , g and m = i 123
output : an nm lower triangular matrix g and a permutation matrix p such that
jjp kp > gg>jj 123
figure 123 : an algorithm for incomplete cholesky decomposition .
the notation ga : b;c : d refers
to the matrix extracted from g by taking the rows a to b and columns c to d .
kernel independent component analysis
fraction of n ( we use the fraction 123 in our simulations ) will numerically be discarded .
this implies that in our search for low - rank approximations , we need only keep eigenvalues greater than = 123 n 123 .
as detailed in appendix c , this has the numerical eect of making our gram matrices of constant numerical rank as n increases .
123 algorithms for kcca and kgv
we now show how to use incomplete cholesky decomposition to solve the kcca and kgv problems .
as we have seen , these problems reduce to eigenvalue computations involving
the regularized matrix ek in eq .
using the incomplete cholesky decomposition , for each matrix ki we obtain the fac - torization ki gig>i , where gi is an n mi matrix with rank mi , where mi n .
we perform a singular value decomposition of gi , in time o ( m 123 i n ) , to obtain an nmi matrix ui with orthogonal columns ( i . e . , such that u >i ui = i ) , and an mimi diagonal matrix i
ki gig>i = uiiu>i :
let m = 123
i=123 mi denote the average value of the ranks mi .
in order to study how to use these matrices to perform our calculations , let vi denote the orthogonal complement of ui , such that ( ui vi ) is an n n orthogonal matrix
if we now consider the regularized matrices r ( ki ) , we have :
ki uiiu>i = ( ui vi ) ( cid : 123 ) i 123 i ) 123ki = ( ui vi ) ( cid : 123 ) ri 123
123 ( ui vi ) > 123 ( ui vi ) > = uiriu>i ;
r ( ki ) = ( ki +
where ri is the diagonal matrix obtained from the diagonal matrix i by applying the +n =123 to its elements .
as seen before , this function softly thresholds the eigenvalues less than n
we now have the following decomposition :
where u is mnmm , v is mn ( mn mm ) , r is mmmm , and ( u v ) is orthogonal :
i ( u v ) >;
ek = ur u> + vv> = ( u v ) ( cid : 123 ) r 123
bach and jordan
this allows us to compute the kcca criterion .
for the kgv criterion , we trivially have
the mn ( nonnegative ) eigenvalues of ek sum to tr ( ek ) = mn .
if ek 123= i then at least one of these eigenvalues must be less than 123
consequently , since ek is similar to ( cid : 123 ) r 123 the smallest eigenvalue of ek ( with eigenvector 123 rmn ) is equal to the smallest eigenvalue of r ( with eigenvector 123 rmm ) , and the two eigenvectors are related through : detek = det ( cid : 123 ) r 123 reduced .
in the case of the rst canonical correlation ( the smallest eigenvalue of ek ) we simply need to nd the smallest eigenvalue of r , which has a cost of o ( m123m 123 ) .
in the case of the generalized variance , we need to compute detek = detr , which costs o ( ( mm ) ) , where ( s ) is the complexity of multiplying two ss matrices , which is less than o ( s123 ) when using strassens algorithm ( cormen et al . , 123 ) .
since in our situation we have m = o ( 123 ) , the complexities are reduced to o ( m123 ) and o ( ( m ) ) .
we thus have reduced the size of our matrices from mn mn to mm mm .
once we have borne the cost of such a low - rank decomposition , the further complexity is greatly
= u ) = u> :
i = detr .
123 free parameters
the kernelica algorithms have two free parameters : the regularization parameter and the width ( cid : 123 ) of the kernel ( assuming identical gaussian kernels for each source ) .
experimental work we found that the kernelica algorithms were reasonably robust to the settings of these parameters .
our choices were to set = 123 123 , ( cid : 123 ) = 123=123 for large samples ( n > 123 ) and = 123 123 , ( cid : 123 ) = 123 for smaller samples ( n 123 123 ) .
for nite n , a value of ( cid : 123 ) that is overly small leads to diagonal gram matrices and our criteria become trivial .
on the other hand , for large n the kgv approaches the mutual information as ( cid : 123 ) tends to zero , and this suggests choosing ( cid : 123 ) as small as possible .
still another consideration , however , is computational|for small ( cid : 123 ) the spectra of the gram matrices decay less rapidly and the computational complexity grows .
this can be mitigated by an appropriate choice of ; in particular , the algorithm could choose so that the number of retained eigenvalues for each gram matrix is held constant .
it is also possible to use cross - validation to set ( leurgans et al . , 123 ) .
clearly , there are several tradeos at play here , and the development of theoretical guidelines for the choice of the parameters is deferred to future work .
our approach to ica involves optimizing a contrast function dened in terms of a set of m gram matrices , where m is the number of components .
these matrices are functions of the weight matrix w , and thus our contrast functions are dened on a manifold of di - mension m ( m 123 ) =123 ( see section 123 ) .
for small m ( less than m = 123 in our simulations ) , the optimization can be based on simple techniques such as naive rst - dierence approx - imations of derivatives , or optimization methods that require only function evaluations .
such techniques are not viable for large problems , however , and in general we must turn to
kernel independent component analysis
derivative - based optimization techniques , where it is required that derivatives are computed
the derivatives of gram matrices are not semidenite matrices in general , and thus we cannot directly invoke the low - rank decomposition algorithms that we have discussed in previous sections .
fortunately , however , in the case of gaussian kernels , it is possible to express these matrix derivatives as a dierence between two low - rank positive semidenite matrices , and we can apply the incomplete cholesky decomposition to each of these matrices separately .
the details of this computation are provided in appendix d .
another possibility to compute derivatives eciently is to use rst - dierence approxi - mations of derivatives , as mentioned earlier , but with the help of the particular structure of the problem .
indeed , a naive computation would require m ( m 123 ) =123 computations of the objective function , and yield a complexity of o ( m123n ) .
as shown in appendix d , we man - age to reduce the complexity to o ( m123n ) by reducing the number of incomplete cholesky decompositions to be performed .
an ica contrast function is ultimately a function of the parameter matrix w .
estimating ica parameters and independent components means minimizing the contrast function with respect to w .
as noted by amari ( 123 ) , the fact that w is an orthogonal matrix in the ica problem ( once the data are whitened ) endows the parameter space with additional structure , and this structure can be exploited by optimization algorithms .
the particular formalism that we pursue here is that of a stiefel manifold .
in this section , we rst review the properties of this manifold .
we then discuss convergence properties of our algorithm .
finally , we present two ecient techniques to tame local minima problems .
123 the stiefel manifold the set of all m m matrices w such that w >w = i is an instance of a stiefel mani - fold ( edelman et al . , 123 ) .
our optimization problem is thus the minimization of a function c ( w ) on the stiefel manifold .
the familiar optimization algorithms of euclidean spaces| gradient descent , steepest descent and conjugate gradient|can all be performed on a stiefel manifold .
the basic underlying quantities needed for optimization are the following :
the gradient of a function c ( w ) is dened as
@w w ( cid : 123 ) @c
element ( i; j ) is @c
@w is the derivative of c with respect to w ; that is , an m m matrix whose
the tangent space is equal to the space of all matrices h such that w >h is skew - symmetric .
it is of dimension m ( m 123 ) =123 and equipped with the canonical metric jjhjjc = 123
bach and jordan
the geodesic starting from w in the direction h ( in the tangent space at w ) is deter - mined as gw;h ( t ) = w exp ( tw >h ) , where the matrix exponential can be calculated eciently after having diagonalized ( in the complex eld ) the skew - symmetric matrix
in the simulations that we report in section 123 , we used steepest descent , with line search along the geodesic .
note that in our case , the calculation of the gradient is more costly ( from 123 to 123 times ) than the evaluation of the function c .
consequently , conjugate gradient techniques are particularly appropriate , because they save on the number of computed derivatives by computing more values of the functions .
123 convergence issues
since our algorithm is simply steepest descent ( with line search ) on an almost - everywhere dierentiable function c ( w ) , the algorithm converges to a local minimum of c ( w ) , for any starting point .
however , the ica contrast functions have multiple local minima , and restarts are generally necessary if we are to nd the global optimum .
empirically , the number of restarts that were needed was found to be small when the number of samples is suciently large so as to make the problem well - dened .
we have also developed two initialization heuristics that have been found to be par - ticularly useful in practice for large - scale problems .
these heuristics nd a matrix w123 suciently close to the global optimum so that the direct optimization of c ( w ) gives the desired optimum without the need for restarts .
we describe these heuristics in the following
123 polynomial kernels
in the case of the kernelica - kcca algorithm , the contrast function c ( w ) that is minimized is itself obtained through a minimization of a functional over the feature space of functions f , as seen in the case of two variables in eq .
this space of function is dened through the kernel k ( x; y ) .
intuitively , the larger the space of functions f is , the less smooth the function c ( w ) should be with respect to variations of w , since a larger f can more closely adapt to any w .
thus , by using a nite - dimensional feature space , we might hope that the optimization function will be smoother and thus local minima will disappear .
as we noted in section 123 , objective functions based on kernels with nite - dimensional feature spaces do not charac - terize independence as well as kernels with innite - dimensional feature spaces , so we should expect inaccurate solutions .
nevertheless , we have found empirically that such solutions are often found in the basin of attraction of the global minima for the full kernelica - kcca or kernelica - kgv algorithm .
thus we can use polynomial kernels to initialize
the classical polynomial kernel is of the form k ( x; y ) = ( r + sxy ) d .
its rkhs fd is the space of polynomials of degree less or equal to d and the rkhs norm jjfjj123 is a weighted sum of squares of the coecients of the polynomials : when a polynomial is considered as function in l123 ( r ) , the k - th coecient is equal to k ! dkf =dxk ( 123 ) , so we can write for f 123 fd , jjfjj123
kernel independent component analysis
hermite polynomial kernel of order d , k ( x; y ) =pd
attractive computationally , we see that the traditional polynomial kernel does not lead to a natural norm for its rkhs .
instead , in our simulations , we use the hermite polynomial kernels ( vapnik , 123 ) , which correspond to a more natural norm in a space of polynomials : if we let hk ( x ) denote the k - th hermite polynomials ( szego , 123 ) , we dene the following classical properties of orthogonal polynomials , this kernel can be computed as eciently as the traditional polynomial kernel .
the rkhs associated with this kernel is the space of functions that can be written as ex123=123 ( cid : 123 ) 123 p ( x ) where p ( x ) is a polynomial of degree less or equal to d , and the norm in that space is the l123 norm .
in the simulations that we report in section 123 , when we use nite dimensional kernels as a rst step in the global optimization , we use hermite polynomial kernels with ( cid : 123 ) = 123 : 123 and d = 123
123 one - unit contrast functions
one - unit contrast functions|objective functions similar to projection pursuit criteria that are designed to discover single components|have been usefully employed in the ica set - ting ( hyvarinen et al . , 123 ) .
these functions involve optimization on the sphere ( of di - mension m 123 ) rather than the orthogonal group ( of dimension m ( m 123 ) =123 ) ; this helps tame problems of local minima .
in the kernelica setting , the kcca or kgv contrast between one univariate component and its orthogonal subspace provides a natural one - unit contrast function .
two issues still have to be resolved to implement such an approach : the choice of the kernel on the component of dimension m 123 , and the combination of components obtained from one - unit contrasts into a full ica solution .
in this paper , we assume that we are given the same kernel k ( x; y ) for all desired univariate components .
to dene a kernel on vectors of dimension m 123 , two natural choices arise .
the rst is to take the product of the m 123 univariate kernels .
this is equivalent to choosing the feature space of functions on rm123 to be the tensor product of the m 123 ( identical ) feature spaces of functions on r ( vapnik , 123 ) .
in the case of a gaussian kernel , this amounts to using an isotropic gaussian kernel k ( x; y ) = ejjxyjj123=123 ( cid : 123 ) 123 this choice has the advantage that the kernel is invariant to ( m123 ) - dimensional orthogonal transformations , so that the contrast function is invariant with respect to the choice of the basis of the orthogonal subspace .
however , the spectrum of the gram matrix decays more slowly than for the one - dimensional gaussian , making this method impractical for large m .
the other natural choice is to take the sum of the kernels; this is equivalent to letting each component of the m 123 vector have its own gram matrix .
here , the decay of each of the m123 gram matrices is fast , so that the computation of the contrast function scales well with m .
however , care must be taken in the choice of the representative of the orthogonal subspace : to obtain a valid function it is necessary to dene a unique representative of the orthogonal subspace which depends continuously on the univariate component w .
this is easily done by considering the image of the canonical basis ( e123; : : : ; em ) of rm by the rotation that transforms the rst vector e123 of this basis into w , and reduces to identity on the orthogonal complement of the span of fw; e123g .
to combine one - dimensional component into a full ica solution we use a \de ( cid : 123 ) ation " technique similar to the one used by hyvarinen and oja ( 123 ) .
we rst nd a m - dimensional component w123 , then project the data into the subspace ( of dimension m 123 )
bach and jordan
orthogonal to w123 and search for a component w123 in this subspace .
this process is iterated until m 123 components are found ( which implies that the m - th is also found by orthog - onality ) .
a full ica solution w is then obtained by combining the m one - dimensional
computational complexity
let n denote the number of samples , and let m denote the number of sources .
m is the maximal rank considered by our low - rank decomposition algorithms for the kernels .
we assume that m 123 n .
performing pca on the input variables is o ( m123n ) |calculating the covariance matrix is o ( m123n ) , diagonalizing the m m matrix is o ( m123 ) = o ( m123n ) , and scaling is
kcca using incomplete cholesky decomposition is o ( m123m 123n ) |calculating the de - composition m times is m o ( n m 123 ) , then forming the matrix r is m ( m123 )
o ( m 123n ) = o ( m123m 123n ) , and nding its smallest eigenvalue is o ( ( mm ) 123 ) .
how - ever , in practice , the incomplete cholesky decompositions are the bottleneck of these computations , so that the empirical complexity is in fact o ( mm 123n ) .
kgv using incomplete cholesky decomposition is o ( m123m 123n + m123m 123 ) , which is usu - ally o ( m123m 123n ) because n is generally greater than mm |calculating the decompo - sition m times is m o ( m 123n ) , then forming the matrix r is m ( m123 ) o ( m 123n ) = o ( m123m 123n ) , and computing its determinant is o ( ( mm ) 123 ) .
as for kcca , the empir - ical complexity is o ( mm 123n ) .
computation of the derivatives is o ( m123m 123n ) |at most 123m123 incomplete cholesky decompositions to perform , and then matrix multiplications with lower complexity , for the direct approach , while o ( m123 ) computations of the contrast functions are needed for the rst - order dierence approach .
simulation results
we have conducted an extensive set of simulation experiments using data obtained from a variety of source distributions .
the sources that we used ( see figure 123 ) included subgaussian and supergaussian distributions , as well as distributions that are nearly gaussian .
we stud - ied unimodal , multimodal , symmetric , and nonsymmetric distributions .
all distributions are scaled to have zero mean and unit variance .
we also varied the number of components , from 123 to 123 , the number of training samples , from 123 to 123 , and studied the robustness of the algorithms to varying numbers of
comparisons were made with three existing ica algorithms : the fastica algorithm ( hyvarinen and oja , 123 ) , the jade algorithm ( cardoso , 123 ) , and the extended infomax algorithm ( lee et al . , 123 ) .
kernel independent component analysis
( a ) k= inf
( b ) k= 123
( c ) k= 123
( d ) k= 123
( e ) k= 123
( f ) k= 123
( g ) k= 123
( h ) k= 123
( i ) k= 123
( j ) k= 123
( k ) k= 123
( l ) k= 123
( m ) k= 123
( n ) k= 123
( o ) k= 123
( p ) k= 123
( q ) k= 123
( r ) k= 123
figure 123 : probability density functions of sources with their kurtoses : ( a ) student with 123 degrees of freedom; ( b ) double exponential; ( c ) uniform; ( d ) student with 123 degrees of freedom; ( e ) exponential; ( f ) mixture of two double exponentials; ( g ) - ( h ) - ( i ) symmetric mixtures of two gaussians : multimodal , transitional and unimodal; ( j ) - ( k ) - ( l ) nonsymmetric mixtures of two gaussians , multimodal , tran - sitional and unimodal; ( m ) - ( n ) - ( o ) symmetric mixtures of four gaussians : mul - timodal , transitional and unimodal; ( p ) - ( q ) - ( r ) nonsymmetric mixtures of four gaussians : multimodal , transitional and unimodal .
bach and jordan
123 experimental setup
all of our experiments made use of the same basic procedure for generating data : ( 123 ) n samples of each of the m sources were generated according to their probability density functions ( pdfs ) and placed into an m n matrix x , ( 123 ) a random mixing matrix a was
chosen , with random but bounded condition number ( between 123 and 123 ) , ( 123 ) a matrix ey of dimension m n was formed as the mixture ey = ax , ( 123 ) the data were whitened by multiplying ey by the inverse p of the square root of the sample covariance matrix , yielding
an m n matrix of whitened data y .
this matrix was the input to the ica algorithms .
each of the ica algorithms outputs a demixing matrix w which can be applied to the matrix y to recover estimates of the independent components .
to evaluate the performance of an algorithm , we compared w to the known truth , w123 = a123p 123 , using the following
maxi jaijj 123 ;
d ( v; w ) =
maxj jaijj 123 ! +
where aij = ( v w 123 ) ij .
this metric , introduced by ( amari et al . , 123 ) , is invariant to permutation and scaling of the columns of v and w , is always between 123 and ( m 123 ) , and is equal to zero if and only if v and w represent the same components .
we thus measure the performance of an algorithm by the value d ( w; w123 ) , which we refer to in the following sections as the \amari error . "
123 ica algorithms
we brie ( cid : 123 ) y overview the other ica algorithms that we used in our simulations .
the fastica algorithm ( hyvarinen and oja , 123 ) uses a de ( cid : 123 ) ation scheme to compute components se - quentially .
for each component an one - unit contrast function , based on an approximation to the negentropy of a component , is maximized .
this function can be viewed as a mea - sure of nongaussianity .
the jade algorithm ( cardoso , 123 ) is a cumulant - based method that uses joint diagonalization of a set of fourth - order cumulant matrices .
it uses algebraic properties of fourth - order cumulants to dene a contrast function that is minimized using jacobi rotations .
the extended infomax algorithm ( lee et al . , 123 ) is a variation on the infomax algorithm ( bell and sejnowski , 123 ) that can deal with either subgaussian or supergaussian components , by adaptively switching between two nonlinearities .
the last three algorithms were used with their default settings .
thus , no ne tuning was performed to increase their performance according to the various sources that we tested .
we did the same for the kernelica algorithms123 , xing the gaussian kernel width to ( cid : 123 ) = 123 and the regularization parameter to = 123 123 for samples n less than 123 and using ( cid : 123 ) = 123=123 , = 123 123 for larger samples .
we used the two initialization techniques presented in section 123 in a rst stage ( optimization of an one - unit contrast with a de ( cid : 123 ) ation scheme , with a hermite polynomial kernel of degree d = 123 and width ( cid : 123 ) = 123 : 123 ) and initialized the second stage ( optimization of the full m - way contrast functions based on the gaussian kernel ) with the result of the rst stage .
for numbers of components m smaller than 123 , the desired solution is reached with a limited number of restarts for the rst stage ( m=123 on average ) .
for m = 123 , however , the global minimum is not found consistently after a few
a matlab implementation can be downloaded at http : / / www . cs . berkeley . edu / ~ fbach / .
kernel independent component analysis
restarts .
we obtained better results ( results that are reported in the last line of table 123 ) by using the three other algorithms to initialize our optimization procedure .
123 in ( cid : 123 ) uence of source distributions
in a rst series of experiments we tested the various algorithms on a two - component ica problem , with 123 and 123 samples , and with all 123 possible source distributions .
we studied two kinds of ica problem .
in the rst ica problem , the two source distributions were identical .
for each of the 123 sources ( a to r ) , we replicated the experiment 123 times and calculated the average amari error .
the results are reported in table 123
the table also shows the average across these 123 123 simulations ( the line denoted mean ) .
in the second ica problem , we chose two sources uniformly at random among the 123 possibilities .
a total of 123 replicates were performed , with the average over replications presented in the line denoted rand in table 123
the results for the kernelica algorithms show a consistent improvement , up to 123% , over the other algorithms .
comparing just between the two kernelica algo - rithms , kernelica - kgv shows small but consistent performance improvements over
in addition , the performance of kernelica is robust with respect to the source dis - tributions .
performance is similar across multimodal ( f , g , j , m , p ) , unimodal ( a , b , d , e , i , l , o , r ) and transitional ( c , h , k , n , q ) distributions .
the kernelica algorithms are particularly insensitive to asymmetry of the pdf when compared to the other algorithms ( see , e . g . , case q ) .
123 increasing number of components
in a second series of experiments , we tested the algorithms in simulations with 123 , 123 , 123 and 123 components .
source distributions were chosen at random from the 123 possible sources in figure 123
the results are presented in table 123 , where we see that kernelica yields a smaller amari error than the other ica algorithms in all cases .
123 robustness to gaussianity
ica algorithms are known to have diculties when the sources are nearly gaussian .
to address this issue , we studied two - component ica problems with identical source distribu - tions .
these distributions were chosen at random among a set of mixtures of gaussians which are at various distances from gaussianity .
this set includes both supergaussian ( pos - itive kurtosis ) and subgaussian distributions ( negative kurtosis ) .
figure 123 shows the average performance of the algorithms as the kurtosis approaches zero , from above and from be - low . 123 we see that the performance of all algorithms degrades as the kurtosis approaches zero , and that the kernelica algorithms are more robust to near - gaussianity than the
although zero kurtosis does not characterize gaussianity , kurtosis is often used to score the non -
gaussianity of distributions ( e . g . , hyvarinen et al . , 123 )
bach and jordan
imax kcca kgv
mean 123 123
imax kcca kgv
table 123 : the amari errors ( multiplied by 123 ) for two - component ica with 123 samples ( left ) and 123 samples ( right ) .
for each pdf ( from a to r ) , averages over 123 replicates are presented .
the overall mean is calculated in the row labeled mean .
the rand row presents the average over 123 replications when two ( generally dierent ) pdfs were chosen uniformly at random among the 123 possible pdfs .
n # repl f - ica
imax kcca kgv
table 123 : the amari errors ( multiplied by 123 ) for m components with n samples : m ( generally dierent ) pdfs were chosen uniformly at random among the 123 possible pdfs .
the results are averaged over the stated number of replications .
kernel independent component analysis
figure 123 : robustness to near - gaussianity .
the solid lines plot the amari error of the ker - nelica algorithms as the kurtosis approaches zero .
the dotted lines plot the performance of the other three algorithms .
figure 123 : robustness to outliers .
the abscissa displays the number of outliers and the
ordinate shows the amari error .
bach and jordan
123 robustness to outliers
outliers are also an important concern for ica algorithms , given that ica algorithms are based in one way or another on high - order statistics .
direct estimation of third and fourth degree polynomials can be particularly problematic in this regard , and many ica algorithms are based on nonlinearities that are more robust to outliers .
in particular , in the case of the fastica algorithm , the hyperbolic tangent and gaussian nonlinearities are recommended in place of the default polynomial when robustness is a concern ( hyvarinen and oja , 123 ) .
we simulated outliers by randomly choosing up to 123 data points to corrupt .
this was done by adding the value +123 or 123 ( chosen with probability 123 / 123 ) to a single component in each of the selected data points .
we performed 123 replications using source distributions chosen uniformly at random from the 123 possible sources .
the results are shown in figure 123
we see that the kernelica methods are signi - cantly more robust to outliers than the other ica algorithms , including fastica with the hyperbolic tangent and gaussian nonlinearities .
123 running time
the performance improvements that we have demonstrated in this section come at a com - putational cost|kernelica is slower than the other algorithms we studied .
the running time is , however , still quite reasonable in the examples that we studied .
for example , for n = 123 samples , and m = 123 components , it takes 123 : 123 seconds to evaluate our contrast functions , and 123 : 123 second to evaluate their derivatives ( using matlab with a pentium 123 mhz processor ) .
moreover , the expected scaling of o ( mn ) for the computations of kcca and kgv was observed empirically in our experiments .
we have presented a new approach to ica based on kernel methods .
while most current ica algorithms are based on using a single nonlinear function|or a small parameterized set of functions|to measure departures from independence , our approach is a more ( cid : 123 ) exible one in which candidate nonlinear functions are chosen adaptively from a reproducing kernel hilbert space .
our approach thus involves a search in this space , a problem which boils down to nding solutions to a generalized eigenvector problem .
such a search is not present in other ica algorithms , and our approach to ica is thus more demanding computationally than the alternative approaches .
but the problem of measuring ( and minimizing ) departure from independence over all possible non - gaussian source distributions is a dicult one , and we feel that the ( cid : 123 ) exibility provided by our approach is appropriately targeted .
moreover , our experimental results show that the approach is more robust than other ica algorithms with regards to variations in source densities , degree of non - gaussianity , and presence of outliers .
our algorithms are thus particularly appropriate in situations where little is known about the underlying sources .
it is also worth noting that current algorithms can provide fast approximate solutions that can be improved by kernelica .
a number of ideas that are closely related to our own have been presented in recent work by other authors .
related work has been presented by fyfe and lai ( 123 ) , who propose the use of a kernelized version of canonical correlation analysis as an ica algorithm
kernel independent component analysis
( for two - component problems ) .
canonical correlation analysis in and of itself , however , is simply a feature extraction technique|it can be viewed as an extension of pca to two variables .
cca does not dene an ica contrast function and it should not be expected to nd independent components in general .
indeed , in the experiments presented by fyfe and lai ( 123 ) , independent components were not always present among the rst canonical variates .
it is important to emphasize that in our approach , canonical correlation is used to dene an ica contrast function , and this contrast function is subsequently optimized with respect to the parameters of the model to derive an ica algorithm .
harmeling et al .
( 123 ) have recently described work on kernel - based ica methods whose focus is complementary to ours .
they show how linear ica methods in feature space can be used to solve nonlinear ica problems ( problems of the general form y = f ( x ) , for nonlinear f ) .
their method nds a certain number of candidate nonlinear functions of the data as purported independent components .
these candidates , however , do not have any optimizing property in terms of an ica contrast function that allows them to be ranked and evaluated , and in harmeling et al .
( 123 ) the authors simply pick those components that are closest to the known sources ( in simulations in which these sources are known ) .
a possible solution to this problem may lie in combining their approach with ours , using kernelica in the subspace of feature space identied by their method , and identifying
there are a number of other recent lines of work that are similar to kernelica in being based on a ( cid : 123 ) exible treatment of the underlying source densities .
these include meth - ods that represent estimated source densities via kernel density estimation ( vlassis and motomura , 123 , boscolo et al . , 123 ) and gaussian mixtures ( attias , 123 , welling and weber , 123 ) , and methods that minimize asymptotic variances of estimators ( pham and garat , 123 ) .
these methods dier from kernelica along various dimensions , including statistical foundations ( frequentist vs .
bayesian , semiparametric vs .
parametric ) , computa - tional requirements and extensibility , and further work will be needed to disentangle their relative advantages and disadvantages .
all of these approaches , however , have in common with kernelica the emphasis on source adaptivity as a key to the most challenging ica
the current paper provides a general , ( cid : 123 ) exible foundation for algorithms that measure and minimize departure from independence , and can serve as a basis for exploring various extensions of the basic ica methodology .
there are several directions which we are currently
first , our approach generalizes in a straightforward manner to multidimensional ica ( car -
doso , 123 ) , which is a variant of ica with multivariate components .
indeed , the gram matrices in our methods can be based on kernel functions computed on vector variables , and the rest of our approach goes through as before .
a possible diculty is that the spectrum of such gram matrices may not decay as fast as the univariate case , and this may impact the running time complexity .
second , we can take advantage of the versality of kernels to extend the current ica model .
although the model we have presented here is based on an assumption of exchange - ability ( that is , the samples can be randomly permuted without aecting the kernelica contrast function ) , in applications where the data are clearly not exchangeable , such as speech processing , it is possible to dene kernels on \frames " |short overlapping sequences .
bach and jordan
by using frames , the local temporal structure of speech or music is taken into account , and contrast functions that are much more discriminative can be designed .
also , kernels can be dened on data that are not necessarily numerical ( e . g . , the \string kernels " of lodhi et al . , 123 ) , and it is interesting to explore the possibility that our kernel - based approach may allow generalizations of ica to problems involving more general notions of \sources "
third , a more thoroughgoing study of the statistical properties of kernelica is needed .
in particular , while we have justied our contrast functions in terms of their math - ematical properties in the limiting case of an innite number of samples , we have not yet fully studied the nite - sample properties of these contrast functions , including the bias and variance of the resulting estimators of the parameters .
nor have we studied the statistical adaptivity of our method as a semiparametric estimation method , comparing its theoretical rate of convergence to that of a method which knows the exact source distributions .
such analyses are needed not only to provide deeper insight into the ica problem and our pro - posed solution , but also to give guidance for choosing the values of the free parameters ( cid : 123 ) and in our algorithm .
finally , in this paper , we establish a connection between kernel - based quantities and information - theoretic concepts , a connection that may extend beyond its utility for dening contrast functions for ica .
in particular , in recent work we have used the kernel generalized variance to provide a contrast function for a model in which the sources are no longer independent , but factorize according to a tree - structured graphical model ( bach and jordan ,
appendix a .
canonical correlation and its generalizations
in the following appendices , we expand on several of the topics discussed in the paper .
this material should be viewed as optional , complementing and amplifying the ideas presented in the paper , but not necessary for a basic understanding of the kernelica algorithms .
this rst section provides additional background on canonical correlation , complement - ing the material in section 123 .
in particular , we review the relationship between cca and mutual information for gaussian variables , and we motivate the generalization of cca to more than two variables .
a . 123 cca and mutual information
for gaussian random variables there is a simple relationship between canonical correla - tion analysis and the mutual information .
consider two multivariate gaussian random
variables x123 and x123 , of dimension p123 and p123 , with covariance matrix c = ( cid : 123 ) c123 c123 c123 c123 .
the mutual information , m ( x123; x123 ) =r p ( x123; x123 ) log ( p ( x123; x123 ) =p ( x123 ) p ( x123 ) ) dx123dx123 , is readily
computed ( kullback , 123 ) :
m ( x123; x123 ) =
det c123 det c123 :
the determinant ratio appearing in this expression , det c= ( det c123 det c123 ) , is known as the
kernel independent component analysis
as we discussed in section 123 , cca reduces to the computation of eigenvalues of the
following generalized eigenvector problem :
( cid : 123 ) c123 c123 c123 c123 ( cid : 123 ) 123
123 = ( 123 + ) ( cid : 123 ) c123
c123 ( cid : 123 ) 123
the eigenvalues appear in pairs : f123 123; 123 + 123; : : : ; 123 p; 123 + p; 123; : : : ; 123g , where p = minfp123; p123g and where ( 123; : : : ; p ) are the canonical correlations .
for invertible b , the eigenvalues of a generalized eigenvector problem ax = bx are the same as the eigenvalues of the eigenvector problem b123ax = x .
thus the ratio of determinants in eq .
( 123 ) is equal to the product of the generalized eigenvalues of eq
m ( x123; x123 ) =
( 123 i ) ( 123 + i ) =
thus we see that for gaussian variables , the canonical correlations i obtained from cca can be used to compute the mutual information .
while eq .
( 123 ) is an exact result ( for gaussian variables ) , it also motivates us to consider approximations to the mutual information .
noting that all of the terms in eq .
( 123 ) are positive , suppose that we retain only the largest term in that sum , corresponding to the rst canonical correlation .
the following theorem , which is easily proved , shows that this yields an approximation to the mutual information .
theorem 123 let x123 and x123 be gaussian random variables of dimension p123 and p123 , respec - tively .
letting ( x123; x123 ) denote the maximal canonical correlation between x123 and x123 , and dening m ( x123; x123 ) = 123
123 log ( 123 123 ( x123; x123 ) ) , we have :
m ( x123; x123 ) 123 m ( x123; x123 ) 123 minfp123; p123gm ( x123; x123 ) :
moreover , m ( x123; x123 ) is the maximal mutual information between one - dimensional linear projections of x123 and x123
also , these bounds are tight|for each of the inequalities , one can nd x123 and x123 such that the inequality is an equality .
a . 123 generalizing to more than two variables
we generalize cca to more than two variables by preserving the relationship that we have just discussed between mutual information and cca for gaussian variables .
alternative generalizations of cca , see kettenring , 123 ) .
consider m multivariate gaussian random variables , x123; : : : ; xm , where xi has dimension pi .
let cij denote the pi pj covariance matrix between xi and xj , and c the overall covariance matrix whose ( i; j ) th block is cij .
the mutual information , m ( x123; : : : ; xm ) , is readily computed in terms of c ( kullback , 123 ) :
m ( x123; : : : ; xm ) =
det c123 det cmm :
we again refer to the ratio appearing in this expression , det c= ( det c123 det cmm ) , as the
bach and jordan
as in the two - variable case , the generalized variance can be obtained as the product of the eigenvalues of a certain generalized eigenvector problem .
in particular , we dene the
which we also write as c = d , where d is the block - diagonal matrix whose diagonal blocks are the cii .
given the denition of c and d , the ratio of determinants of c and d is clearly equal to the generalized variance .
thus we have :
m ( x123; : : : ; xm ) =
where i are the generalized eigenvalues of c = d .
dening cca as the solution of the generalized eigenvector problem c = d , we again obtain the mutual information in terms of a sum of functions of generalized eigenvalues . 123
if we wish to obtain a single \maximal canonical correlation , " we can proceed by analogy to the two - variable case and take the largest ( positive ) term in the sum in eq .
thus , we dene the rst canonical correlation ( x123; : : : ; xm ) as the smallest generalized eigenvalue of c = d .
we dene m ( x123; : : : ; xm ) = 123 123 log ( x123; : : : ; xm ) as an approximation to the mutual information based on this eigenvalue .
the following theorem , proved by making use of jensens inequality , shows that this approximation yields upper and lower bounds on the mutual information , in the case of gaussian variables :
theorem 123 let ( x123; : : : ; xm ) be multivariate gaussian random variables , where xi has di - mension pi .
we have the following lower and upper bounds on the mutual information m = m ( x123; : : : ; xm ) :
123 m 123 p m;
where m = m ( x123; : : : ; xm ) , = ( x123; : : : ; xm ) , and p =pi pi .
which of the properties of the classical denition of the rst canonical correlation gen - eralize to the m - variable denition ? as we have already noted , the eigenvalues occur in pairs in the two - variable case , while they do not in the m - variable case .
this implies that the specialization of the m - variable denition to m = 123 , ( x123; x123 ) , does not reduce exactly to the classical denition , ( x123; x123 ) .
but the dierence is unimportant; indeed , we have ( x123; x123 ) = 123 ( x123; x123 ) .
a more important aspect of the two - variable case is the fact ( cf .
theorem 123 ) that there is a relationship between ( x123; x123 ) and one - dimensional projections of x123 and x123
this relationship is an important one , lying at the heart of the properties of f - correlation .
in the following section , we prove that such a relation exists in the m - way case as well .
note that the i are all nonnegative and sum to p =pi pi .
note also that the i do not occur in pairs
as they do in the two - variable case .
moreover , the terms in the sum in eq .
( 123 ) are not all positive .
kernel independent component analysis
a . 123 f - correlation and independence
note that a correlation matrix is symmetric positive semidenite with trace equal to m , and thus the eigenvalues are nonnegative and sum to m .
this implies that ( y123; : : : ; ym )
let y123; : : : ; ym be univariate random variables , with correlation matrix ec , a matrix whose ( i; j ) th element is corr ( yi; yj ) .
we dene ( y123; : : : ; ym ) to be the minimal eigenvalue of ec .
must always be between zero and one , and is equal to one if and only if ec = i .
that is ,
( y123; : : : ; ym ) = 123 if and only if the variables y123; : : : ; ym are uncorrelated .
the function , a function of m univariate random variables , plays a similar role as the correlation between two random variables , as shown in the following theorem :
theorem 123 let x123; : : : ; xm be m multivariate random variables .
let ( x123; : : : ; xm ) be the rst canonical correlation , dened as the smallest generalized eigenvalue of eq .
then ( x123; : : : ; xm ) is the minimal possible value of ( y123; : : : ; ym ) , where y123; : : : ; ym are one - dimensional projections of x123; : : : ; xm :
( x123; : : : ; xm ) = min
( >123 x123; : : : ; >mxm ) :
in addition , ( x123; : : : ; xm ) = 123 if and only if the variables x123; : : : ; xm are uncorrelated .
proof let ec ( >123 x123; : : : ; >mxm ) denote the correlation matrix between ( >123 x123; : : : ; >mxm ) .
if the vectors i have unit norm then the ( i; j ) th element of ec ( >123 x123; : : : ; >mxm ) is just >i ecijj , where ecij is the correlation matrix between xi and xj .
we then have :
( >123 x123; : : : ; >mxm ) =
( >123 x123; : : : ; >mxm )
>ec ( >123 x123; : : : ; >mxm )
minimizing over all possible 123; : : : ; m and of unit norm is the same as minimizing over i=123 jjijj123 = 123 , by simply mapping ( ( i ) ; ) to ( i ) by i = ii
and mapping ( i ) to ( ( i ) ; ) , by i = jjijj and i = i=jjijj .
consequently , we have :
all possible i such that pm
( >123 x123; : : : ; >mxm ) = min
>i ecijj = min
>ec = ( x123; : : : ; xm ) ;
which proves the rst part of theorem 123
let us now prove the second part .
if the variables x123; : : : ; xm are uncorrelated , then any linear projections will also be un - correlated , so ( >123 x123; : : : ; >mxm ) is constant equal to one , which implies by eq .
( 123 ) that ( x123; : : : ; xm ) = 123
conversely , if ( x123; : : : ; xm ) = 123 , then since is always between zero and one , using eq .
( 123 ) , for all i , ( >123 x123; : : : ; >mxm ) must be equal to one , and consequently , the univariate random variables >123 x123; : : : ; >mxm are uncorrelated .
since this is true for all one - dimensional linear projections , x123; : : : ; xm must be uncorrelated .
bach and jordan
applying this theorem to a reproducing kernel hilbert space f , we see that the f - correlation between m variables is equal to zero if and only if for all functions f123; : : : ; fm in f , the variables f123 ( x123 ) ; : : : ; fm ( xm ) are uncorrelated .
consequently , assuming a gaussian kernel , we can use the same line of reasoning as in theorem 123 to prove that the f - correlation is zero if and only if the variables x123; : : : ; xm are pairwise independent .
concerning our second contrast function , the kgv , theorem 123 shows that the kgv is always an upper bound of a constant function ( ) of the rst canonical correlation .
since is nonnegative and equal to zero if and only if = 123 , this shows that if the kgv is equal to zero , then the rst canonical correlation is also zero , and the variables x123; : : : ; xm are pairwise independent .
as in the kcca case , the converse is trivially true .
thus , the kgv also denes a valid contrast function .
appendix b .
kernel generalized variance and mutual information
in section 123 we noted that there is a relationship between the kernel generalized variance ( kgv ) and the mutual information in the bivariate case .
in particular , we claim that in the population case ( where no regularization is needed ) , the kgv approaches a limit as the kernel width approaches zero , and that in the bivariate case , this limit is equal to the mutual information , up to second order , expanding around independence .
a full proof of this result is beyond the scope of this paper , but in this section we provide a sketch of the proof .
we rst introduce a new information criterion for discrete multinomial random variables , show its link to the mutual information and then extend it to continuous variables .
b . 123 multinomial and gaussian variables
we begin by establishing a relationship between a pair of multinomial random variables and a pair of gaussian random variables with the same covariance structure .
let x and y be multinomial random variables of dimension p and q , respectively , and let p denote the p q joint probability matrix whose ( i; j ) th element , pij , is equal to p ( x = i; y = j ) .
as usual , we also represent these variables as unit basis vectors , x 123 rp and y 123 rq , such that p ( xi = 123; yj = 123 ) = pij .
let px denote a p - dimensional vector representing the marginal probability distribution of x , and let py denote a q - dimensional vector representing the marginal probability distri - bution of y .
in the following , if r is a r 123 vector , diag ( r ) denotes the diagonal r r matrix with diagonal r .
the covariance structure of ( x; y ) can be written as follows , where dpx = diag ( px ) and dpy = diag ( py ) :
e ( xy > ) = p; e ( x ) = px; e ( y ) = py; e ( xx> ) = dpx; e ( y y > ) = dpy ;
which implies cxy = p pxp>y , cxx = dpx pxp>x , and cy y = dpy pyp>y .
let x g and y g denote gaussian random variables that have the same covariance structure as it is easy to show that the mutual information between x g and y g , which x and y .
we denote as i g , is equal to i g = 123 123 log det ( i cc> ) , where c = d123=123
123 log det ( i c>c ) = 123
we now expand i g near independence , that is , when we assume pij = pxipyj ( 123 + " ij )
where " is a matrix with small norm .
in this case , the matrix c dened as c = d123=123
( p pxp>y ) d123=123
kernel independent component analysis
py = d123=123
py , has also a small norm and we can expand i g as follows :
log det ( i cc> )
indeed , if we let si denote the singular values of c ( which are close to zero because the matrix c has small norm ) , we have :
log det ( i cc> ) =
thus , we obtain :
tr ( dpx " dpy " > ) =
let us now expand the mutual information i = i ( x; y ) , using the taylor expansion ( 123 + " ) log ( 123 + " ) " + " 123=123 :
pxipyj ( 123 + " ij ) log ( 123 + " ij ) xij
usingpij " ijpxipyj =pij ( 123 + " ij ) pxipyj pij pxipyj =pij pij pij pxipyj = 123 123 = 123
in summary , for multinomial random variables x and y , we have dened the quantity i g ( x; y ) in terms of the mutual information between gaussian variables with the same covariance .
we have shown that this quantity is equal up to second order to the actual mutual information , i ( x; y ) , when we expand \near independence . " we now extend these results , dening the quantity i g , which we will refer to as the gaussian mutual information ( gmi ) , for continuous univariate variables .
b . 123 a new information measure for continuous random variables
let x and y be two continuous random variables .
their mutual information , i ( x; y ) =
r p ( x; y ) log ( p ( x; y ) =p ( x ) p ( y ) ) dxdy , can be dened as the upper bound of the mutual infor -
mation between all discretizations of x and y ( kolmogorov , 123 ) .
behind this denition lies the crucial fact that when rening the partitions of the sample space used to discretize x and y , the discrete mutual information must increase .
by analogy , we generalize the gmi to continuous variables : the gmi i g ( x; y ) is dened to be the supremum of i g ( xd; yd ) for discretizations ( xd; yd ) of x and y .
in order to have a proper denition , we need to check that when we rene the partitions , then the discrete gmi can only increase .
it is easy to check that the associated gaussian random variables before the renement are linear combinations of the associated gaussian random variables after the renement , which implies that the renement can only increase the mutual information between the associated gaussian random variables .
but this implies that i g can only increase during a renement .
another property of the gaussian mutual information that we will need in the following section , one that is also shared by the classical mutual information , is that it is equal to the limit of the discrete mutual information , when the discretization is based on a uniform mesh whose spacing tends to zero .
from this , it can be shown that the equality of i and i g up to
bach and jordan
second order around independence still holds for continuous variables .
using the singular value decomposition for bivariate distributions ( buja , 123 ) , an alternate justication of this expansion could be derived .
b . 123 relation with kernel generalized variance let us consider the feature space f ( cid : 123 ) associated with a gaussian kernel k ( x; y ) = 123p123 ( cid : 123 ) ( cid : 123 ) , such that r g ( cid : 123 ) ( x ) dx = 123
as where g ( x ) = ex123=123
let us denote g ( cid : 123 ) ( x ) = 123p123 ( cid : 123 ) we saw in section 123 , the space f ( cid : 123 ) can be viewed as the completion of the space of nite linear combinations of functions of the form g ( cid : 123 ) ( x xi ) where xi 123 r .
let fxig be a mesh of uniformly distributed points in r with spacing h .
using these xed points , we dene f ( cid : 123 ) fxig to be the ( nite - dimensional ) linear span of the functions fi = g ( cid : 123 ) ( x xi ) .
similarly we dene a mesh fyjg for the second random variable , and let f ( cid : 123 ) fyjg denote the linear span of the functions gj = g ( cid : 123 ) ( x yj ) .
the contrast function if ( ( cid : 123 ) ) based on the kgv is dened as the mutual information between gaussian random variables that have the same covariance structure as ' ( x ) and ' ( y ) .
let if ( h; ( cid : 123 ) ) be the mutual information between nite - dimensional gaussian random variables that have the same covariance structure as the projections of ' ( x ) and ' ( y ) onto f ( cid : 123 ) fxig and f ( cid : 123 ) fyjg .
as the spacing h tends to zero and as the number of points tends to innity , the spaces f ( cid : 123 ) fxig and f ( cid : 123 ) fyjg tend to the feature space f ( cid : 123 ) , so that if ( h; ( cid : 123 ) ) tends to if ( ( cid : 123 ) ) .
we now relate the quantity if ( h; ( cid : 123 ) ) to the gaussian mutual information i g ( x; y ) .
we have :
ehfi; ' ( x ) ihgj; ' ( y ) i = z g ( cid : 123 ) ( x xi ) g ( cid : 123 ) ( y yj ) p ( x; y ) dx dy
= ( g ( cid : 123 ) ( x ) g ( cid : 123 ) ( y ) p ( x; y ) ) ( xi; yj ) = pg ( cid : 123 ) ( xi; yj ) ;
where pg ( cid : 123 ) , a smoothed version of p , is well dened as a probability density function because g ( cid : 123 ) is normalized .
similar formulas can be obtained for the other expectations :
ehfi; ' ( x ) i = ( pg ( cid : 123 ) ) x ( xi ) ;
ehgj; ' ( x ) i = ( pg ( cid : 123 ) ) y ( yj )
ehfi; ' ( x ) ihfj; ' ( x ) i / ij ( pg ( cid : 123 ) ) x ( xi ) if ( cid : 123 ) h 123 :
these identities ensure that , as h and ( cid : 123 ) tends to zero , the covariance structure of the pro - jections of ' ( x ) and ' ( y ) onto f ( cid : 123 ) fxig and f ( cid : 123 ) fyjg is equivalent to the covariance obtained through the discretization on the mesh fxi; yjg of random variables having joint distribution pg ( cid : 123 ) .
this implies that , as h and ( cid : 123 ) tends to zero , if ( h; ( cid : 123 ) ) is equivalent to the gaussian mutual information of the variables x and y , smoothed by g ( cid : 123 ) .
moreover , as the smoothing parameter ( cid : 123 ) tends to zero , pg ( cid : 123 ) tends to p , and we see that if ( ( cid : 123 ) ) tends to i g .
thus as ( cid : 123 ) tends to zero , the kgv tends to the gaussian mutual information .
kernel independent component analysis
appendix c .
spectrum of gram matrices
the computational eciency of our algorithms relies on the approximation of gram matrices by matrices of very low rank . 123 in this section we present theoretical results from functional analysis that justify the use of such approximations .
for simplicity , we restrict ourselves to gaussian kernels , but many of these results can be generalized to other translation - invariant
the rank of approximations to gram matrices depends on the decay of the distribution of the eigenspectrum of these matrices .
as pointed out by williams and seeger ( 123 ) , for one - dimensional input spaces the eigenvalues decay geometrically if the input density is gaussian .
we discuss a generalization of this result in this section , showing that the decay of the spectrum depends in general on the decay of the tails of the underlying distribution p ( x ) of the data .
the study of the spectrum of gram matrices calculated from a kernel k ( x; y ) is usually carried out by studying the spectrum of an associated integral operator , and using the nystrom method to relate these spectra ( baker , 123 ) .
we brie ( cid : 123 ) y review the relevant
c . 123 integral operators and nystrom method let k 123 l123 ( rdrd ) denote a symmetric kernel and p ( x ) the probability density of a random variable on rd .
we assume that p is bounded and that the integralrrdrd jk ( x; y ) jp ( x ) dxdy is nite .
we dene the integral operator t , from l123 ( rd ) to l123 ( rd ) , as follows :
t : ` ( y ) 123 ! zrd
t is called a hilbert - schmidt operator ( brezis , 123 ) .
it is known that the spectrum of such an operator is a sequence of real numbers tending to zero , where the spectrum is dened as the set of i for which there exists `i 123 l123 ( rd ) such that t `i = i`i :
k ( x; y ) p ( x ) `i ( x ) dx = i`i ( y ) :
the eigenvalues i and eigenvectors `i are often approximated using the \nystrom method , " which relates them to the spectra of gram matrices of points sampled from p .
that is , the expectation in eq .
( 123 ) is approximated by the sample mean t ` ( y ) 123 k=123 k ( xk; y ) ` ( xk ) , where xk are n data points sampled from p .
substituting this into the denition of an eigen - vector in eq .
( 123 ) and evaluating at y = xj , we get :
k ( xk; xj ) `i ( xk ) i`i ( xj ) ;
and thus ' i = ( `i ( x123 ) ; : : : ; `i ( xn ) ) > is an eigenvector of the gram matrix k = ( k ( xi; xj ) ) with eigenvalue n i :
k ' i = i ' i :
note that a ( non - centered ) gram matrix is always invertible ( e . g . , scholkopf and smola , 123 ) , given dis - tinct sample points and a gaussian kernel , so any low - rank representation of such a matrix is necessarily
bach and jordan
decay of p ( x ) jxjd; d > 123
bound of h ( t ) decay of n
ean log n
table 123 : bounds for the number of eigenvalues greater than , as a function h ( t ) of t = 123= , and the n - th eigenvalue n of the integral operator t .
the number of eigenvalues greater than , for an n n gram matrix , is bounded by h ( t ) , where t = n= ( see text for details ) .
consequently , the eigenvalues of the gram matrix k are approximately equal to n , where ranges over eigenvalues of the integral operator .
it is also possible to approximate the eigenfunctions `i using this approach ( see baker , 123 ) .
two problems arise : how fast does the spectrum of the integral operator decay for various kernels k and densities p ? how close are the eigenvalues of the gram matrices to n times the eigenvalues of the integral operator ? in the following section , we overview some theoretical results that give asymptotic bounds for the decay of the spectra of integral operators , and we provide empirical results that relate the eigenvalues of gram matrices to the eigenvalues of the integral operator .
c . 123 spectra of integral operators
widom ( 123 , 123 ) provides some useful results regarding the spectra of the operator t dened in eq .
( 123 ) for translation - invariant kernels of the form k ( x y ) .
he shows that the rate of decay of the spectrum depends only on the rate of decay of the fourier tranform ( ! ) of k , and of the rate of decay of the probability density function of the underlying input variable x .
moreover , he provides asymptotic equivalents for many cases of interest .
most of the results can be generalized to multivariate kernels .
for the case of gaussian kernels , we summarize some of the pertinent results in table 123
note that except for heavy - tailed distributions ( those with polynomial decay ) , the spectrum vanishes at least geometrically .
c . 123 nystrom approximation
we now provide empirical results about how the spectra of gram matrices relate to the spectrum of the associated integral operator .
we study the gaussian distribution , where an exact result can be calculated , and the student distribution with three degrees of freedom , where a function of the form n = a ( b+n ) 123 can be t tightly to the spectrum . 123 in both cases , we used distributions with unit variance .
we sampled n data points from these distributions , for n ranging from 123 to 123 , and computed the spectra of the resulting gram matrices .
the results are plotted in figure 123
we see that the spectrum of an nn gram matrix , which we denote as k;n , is composed
note that this is consistent with the bounds in table 123 , since the student distribution with three degrees
of freedom has a density that decays as jxj123
kernel independent component analysis
of two regimes .
for eigenvalues k;n up to a given rank k123 ( n ) , the eigenvalues are very close to their limiting value k=n , where k is the k - th eigenvalue of the associated integral operator .
after k123 ( n ) , the spectrum decays very rapidly .
the important point is that the spectra of the gram matrices decay at least as rapidly as n times the eigenvalues of the integral operators .
consequently , we need only consider low - rank approximations of order m = h ( n= ) , where , as t = n= tends to innity , h ( t ) grows as described in table 123
given that we choose the precision to be proportional to n , i . e .
= 123n , the number of eigenvalues we need to consider is bounded by a constant that depends solely on the input distribution .
figure 123 : spectra for two dierent input densities ( top : gaussian , bottom : student dis - tribution with three degrees of freedom ) .
the dashed lines are the exact or tted ( see text for details ) logarithm of the spectra log k , plotted as a func - tion of the eigenvalue number k .
( left ) the solid lines represent log 123 n k;n , for n = 123; 123; 123; 123; 123; 123
( right ) for n = 123 = 123 , the solid line repre - sents log 123 n k;n , plotted as a function of k , while the lower and upper ends of the error bars represent the minimum and the maximum of log 123 n k;n across 123
appendix d .
derivatives
in this section we provide a discussion of the computation of the derivatives of our contrast functions .
the computation of these derivatives is a straightforward application of the chain rule , where the core subroutine is the computation of the derivatives of the gram matrices .
bach and jordan
this latter computation is not entirely straightforward , however , and it is our focus in this section .
note that the computation of the derivatives of a gram matrix arises outside of the ica setting , and this material may therefore have utility for other kernel - based methods .
the key problem is that although the gram matrix k is symmetric and positive semidef - inite , its derivative with respect to some underlying variable is symmetric but not in general positive or negative semidenite .
consequently , incomplete cholesky decomposition cannot be used directly to nd low - rank approximations of derivatives .
fortunately , for gaussian kernels , it is possible to express the derivatives as sum and / or dierence of positive semidenite matrices that themselves are gram matrices , and to which incomplete cholesky decomposition can be applied .
more precisely , if w 123 rm is a row of our parameter matrix w , then the gram matrix that we have to dierentiate has its ( a; b ) th element equal to exp ' 123 123 ( cid : 123 ) 123 ( w>xa w>xb ) 123
without loss of generality , let us dierentiate
this expression around w = ( 123; 123; : : : ; 123 ) > .
we obtain :
@wj kab =
( cid : 123 ) 123 ( xa123 xb123 ) ( xaj xbj ) e 123
this is not a gram matrix , because the fourier transform of x 123 ! x123xjex123 is not real - valued and nonnegative .
we instead proceed by decomposing the derivative as a dierence of gram matrices .
two cases arise :
if j = 123 , from eq .
( 123 ) , we have a matrix whose elements are of the form f ( xa123 xb123 ) .
let ^f be the fourier transform of f .
the fourier transform
where f ( x ) = x123ex123=123 ( cid : 123 ) 123 of g ( x ) = ex123=123 ( cid : 123 ) 123
is ( ! ) = p123 ( cid : 123 ) e ! 123 ( cid : 123 ) 123=123 , and we have :
^f ( ! ) =
d ! 123 ( ( ! ) ) =
= ( cid : 123 ) 123 ( 123 ( cid : 123 ) 123 ! 123 ) p123 ( cid : 123 ) e ! 123 ( cid : 123 ) 123=123 = ( cid : 123 ) 123 ( ! ) ( cid : 123 ) 123 ! 123p123 ( cid : 123 ) e ! 123 ( cid : 123 ) 123=123 = ( cid : 123 ) 123 ( ! ) ^h ( ! )
the function h = ( cid : 123 ) 123g f has a nonnegative fourier transform , which implies that the matrix whose elements are ( cid : 123 ) 123g ( xa123 xb123 ) f ( xa123 xb123 ) is positive semidenite .
since g ( x ) also induces a positive semidenite matrix , we have managed to decompose our
if j 123= 123 , from eq .
( 123 ) , we have a matrix induced by a function of the form f ( xa123 .
we use the following trick to reduce the
xb123; xaj xbj ) , where f ( x; y ) = xyex123=123 ( cid : 123 ) 123 problem to the previous case .
for a positive real number ( cid : 123 ) , we write :
( ( cid : 123 ) 123 x123 ) +
( ( cid : 123 ) 123 + ( cid : 123 ) 123 ( x + y ) 123 ) :
thus we can decompose the function f ( cid : 123 ) ( x; y ) = xyex123=123 ( cid : 123 ) 123 f ( cid : 123 ) ( x; y ) = h ( cid : 123 ) ( x; y ) + h ( cid : 123 ) ( x; y ) h ( cid : 123 ) ; ( cid : 123 ) ( x; y ) where h ( cid : 123 ) ( x; y ) = 123 h ( cid : 123 ) ( x; y ) = 123 all have real positive fourier transforms .
to approximate f based on f ( cid : 123 ) , we note that :
123 ( ( cid : 123 ) 123 x123 ) ex123=123 ( cid : 123 ) 123
as before , letting
and h ( cid : 123 ) ; ( cid : 123 ) ( x; y ) = 123
jf ( x; y ) f ( cid : 123 ) ( x; y ) j 123 jxyjex123=123 ( cid : 123 ) 123
kernel independent component analysis
given that our goal is to obtain a descent direction and not an exact value for the derivative , we can chose a large value of ( cid : 123 ) ( we used ( cid : 123 ) = 123 in our simulations ) and obtain satisfactory results . 123
in summary , we have managed to decompose the derivatives of gram matrices in terms of the dierence of two matrices to which we can apply our low - rank decomposition algorithm .
the nal time complexity is o ( m123m 123n ) for the derivatives of the contrast functions we
a similar reduced complexity can be achieved through rst - dierence approximations of the derivatives , when the particular structure of the optimization is used .
indeed , since the derivatives have m ( m123 ) =123 components , we would need as many evaluations of the contrast functions , which would take o ( m123m 123n ) .
nonetheless , each new evaluation requires to compute incomplete cholesky decompositions only for two components ( all others are held xed to compute a partial derivative ) , so the complexity can be reduced to the one of m evaluations of the constrast functions , that is o ( m123m 123n ) .
we would like to thank alexander smola for useful comments on kernel canonical correlation analysis and regularization .
we would like to acknowledge support for this project from the national science foundation ( nsf grant iis - 123 ) , and the multidisciplinary research program of the department of defense ( muri n123 - 123 - 123 - 123 ) .
