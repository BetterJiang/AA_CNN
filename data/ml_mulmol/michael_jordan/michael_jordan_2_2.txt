we report on the successful application of feature selection methods to a classi ( cid : 123 ) ca - tion problem in molecular biology involving only 123 data points in a 123 dimensional space .
our approach is a hybrid of ( cid : 123 ) lter and wrapper approaches to feature selec - tion .
we make use of a sequence of simple ( cid : 123 ) lters , culminating in koller and sahamis ( 123 ) markov blanket ( cid : 123 ) lter , to decide on particular feature subsets for each subset cardinality .
we compare between the re - sulting subset cardinalities using cross val - idation .
the paper also investigates regu - larization methods as an alternative to fea - ture selection , showing that feature selec - tion methods are preferable in this prob -
structural and functional data from analysis of the human genome have increased many fold in recent years , presenting enormous opportunities and chal - lenges for machine learning .
in particular , gene ex - pression microarrays are a rapidly maturing technol - ogy that provide the opportunity to assay the ex - pression levels of thousands or tens of thousands of genes in a single experiment ( shalon et al . , 123 ) .
these assays provide the input to a wide variety of statistical modeling e ( cid : 123 ) orts , including classi ( cid : 123 ) ca - tion , clustering , and density estimation .
for exam - ple , by measuring expression levels associated with two kinds of tissue , tumor or non - tumor , one obtains labeled data sets that can be used to build diagnostic classi ( cid : 123 ) ers .
the number of replicates in these exper - iments are often severely limited , however; indeed , in the data that we analyze here ( cf .
golub , et al . , 123 ) , there are only 123 observations of the expres - sion levels of each of 123 genes .
in this extreme of very few observations on very many features , it is natural|and perhaps essential|to investigate fea -
ture selection and regularization methods .
feature selection methods have received much atten - tion in the classi ( cid : 123 ) cation literature ( kohavi & john , 123; langley , 123 ) , where two kinds of meth - ods have generally been studied| ( cid : 123 ) lter methods and wrapper methods .
the essential di ( cid : 123 ) erence between these approaches is that a wrapper method makes use of the algorithm that will be used to build the ( cid : 123 ) nal classi ( cid : 123 ) er , while a ( cid : 123 ) lter method does not .
thus , given a classi ( cid : 123 ) er c , and given a set of features f , a wrapper method searches in the space of subsets of f , using cross validation to compare the perfor - mance of the trained classi ( cid : 123 ) er c on each tested sub - set .
a ( cid : 123 ) lter method , on the other hand , does not make use of c , but rather attempts to ( cid : 123 ) nd predic - tive subsets of the features by making use of simple statistics computed from the empirical distribution .
an example is an algorithm that ranks features in terms of the mutual information between the fea - tures and the class label .
wrapper algorithms can perform better than ( cid : 123 ) lter algorithms , but they can require orders of magnitude more computation time .
an additional problem with wrapper methods is that the repeated use of cross validation on a single data set can lead to uncontrolled growth in the proba - bility of ( cid : 123 ) nding a feature subset that performs well on the validation data by chance alone .
in essence , in hypothesis spaces that are extremely large , cross validation can over ( cid : 123 ) t .
while theoretical attempts to calculate complexity measures in the feature selection setting generally lead to the pessimistic conclusion that exponentially many data points are needed to provide guarantees of choosing good feature subsets , ng has recently described a generic feature selection methodology , referred to as fs - ordered , that leads to more optimistic conclusions ( ng , 123 ) .
in ngs approach , cross validation is used only to compare between fea - ture subsets of di ( cid : 123 ) erent cardinality .
ng proves that this approach yields a generalization error that is upper - bounded by the logarithm of the number of
in a problem with over 123 features , ( cid : 123 ) ltering meth - ods have the key advantage of signi ( cid : 123 ) cantly smaller computational complexity than wrapper methods , and for this reason these methods are the main fo - cus of this paper .
earlier papers that have ana - lyzed microarray data have also used ( cid : 123 ) ltering meth - ods ( golub et al . , 123; chow et al . , dudoit et al . , 123 ) .
we show , however , that it is also possible to exploit prediction - error - oriented wrapper methods in the context of a large feature in particular , we adopt the spirit of ngs fs - ordered approach and present a speci ( cid : 123 ) c al - gorithmic instantiation of his general approach in which ( cid : 123 ) ltering methods are used to choose best sub - sets for a given cardinality .
thus we use simple ( cid : 123 ) l - tering methods to carry out the major pruning of the hypothesis space , and use cross validation for ( cid : 123 ) nal while feature selection methods search in the com - binatorial space of feature subsets , regularization or shrinkage methods trim the hypothesis space by constraining the magnitudes of parameters ( bishop , 123 ) .
consider , for example , a linear regression problem in which the parameters ( cid : 123 ) i are ( cid : 123 ) t by least squares .
regularization adds a penalty term to the least squares cost function , typically either the squared l123 norm or the l123 norm .
these terms are multiplied by a parameter ( cid : 123 ) , the regularization pa - rameter .
choosing ( cid : 123 ) by cross validation , one ob - tains a ( cid : 123 ) t in which the parameters ( cid : 123 ) i are shrunk toward zero .
this approach e ( cid : 123 ) ectively restricts the hypothesis space , providing much of the protection against over ( cid : 123 ) tting that feature selection methods aim to provide .
if the goal is to obtain small feature sets for com - putational or interpretational reasons , then feature selection is an obligatory step .
if the goal is to ob - tain the best predictive classi ( cid : 123 ) er , however , then reg - ularization methods may perform better than fea - ture selection methods .
few papers in the machine learning literature have compared these approaches directly; we propose to do so in the current paper .
feature selection in this section we describe the feature selection methodology that we adopted .
briey , our approach proceeds in three phases .
the ( cid : 123 ) rst phase we use unconditional univariate mix - ture modeling to provide an initial assessment of the viability of a ( cid : 123 ) ltering approach , and to provide a discretization for the second phase .
in the second phase , we rank features according to an information gain measure , substantially reducing the number of features that are input to the third phase .
finally , in
the third phase we use the more computationally in - tense procedure of markov blanket ( cid : 123 ) ltering to choose candidate feature subsets that are then passed to a
123 unconditional mixture modeling a useful empirical assumption about the activity of genes , and hence their expression , is that they generally assume two distinct biological states ( ei - ther \on " or \o ( cid : 123 ) " ) .
the combination of such binary patterns from multiple genes determines the sample phenotype .
given this assumption , we expect that the marginal probability of a given expression level can be modeled as a univariate mixture with two components ( which includes the degenerate case of a single component ) .
representative samples of em - pirical marginals are shown in figure 123
figure 123
two representative histograms of gene expres - sion measurements .
the x - axes represents the normal - ized expression level .
if the underlying binary state of the gene does not vary between the two classes , then the gene is not discriminative for the classi ( cid : 123 ) cation problem and should be discarded .
this suggests a heuristic pro - cedure in which we measure the separability of the mixture components as an assay of the discriminabil - ity of the feature .
j ( cid : 123 ) i ) denote a two - component gaussian let p ( fi mixture model for feature fi , where ( cid : 123 ) i denotes the means , standard deviations and mixing proportions of the mixture model .
we ( cid : 123 ) t these parameters using the em algorithm .
note that each feature fi is ( cid : 123 ) t suppose that we denote the underlying state of gene i as a latent variable zi 123 f123; 123g .
suppose moreover that we de ( cid : 123 ) ne a decision d ( fi ) on feature fi to be 123 if the posterior probability of fzi = 123g is greater than 123 : 123 under the mixture model , and let d ( fi ) equal 123 otherwise .
we now de ( cid : 123 ) ne a mixture overlap proba -
( cid : 123 ) = p ( zi = 123 ) p ( d ( fi ) = 123jzi = 123 ) + p ( zi = 123 ) p ( d ( fi ) = 123jzi = 123 ) :
if the mixture model were a true representation of the probability of gene expression , then the mixture
overlap probability would represent the bayes error of classi ( cid : 123 ) cation under this model .
we use this proba - bility as a heuristic surrogate for the discriminating potential of the gene , as assessed via its uncondi - note that a mixture model can also be used as a quantizer , allowing us to discretize the measure - ments for a given feature .
we simply replace the continuous measurement fi with the associated bi - nary value d ( fi ) .
this is in fact the main use that we make of the mixture models in the remainder of the paper .
in particular , in the following section we use the quantized features to de ( cid : 123 ) ne an information
123 information gain ranking we now turn to methods that make use of the class labels .
the goal of these methods is to ( cid : 123 ) nd a good approximation of the conditional distribution , p ( c j f ) , where f is the overall feature vector and c is the the information gain is commonly used as a surro - gate for approximating a conditional distribution in the classi ( cid : 123 ) cation setting ( cover & thomas , 123 ) .
let the class labels induce a reference partition s123; : : : ; sc .
let the probability of this partition be the empirical proportions : p ( t ) = jtj=jsj for any subset t .
now suppose a test on feature fi induces a partition of the training set into e123; : : : ; ek .
let p ( scjek ) = p ( sc\ek ) =p ( ek ) .
we de ( cid : 123 ) ne the infor - mation gain due to this feature with respect to the reference partition as :
igain = h ( p ( s123 ) ; : : : ; p ( sc ) )
p ( ek ) h ( p ( s123jek ) ; : : : ; p ( scjek ) ) ; ( 123 )
where h is the entropy function .
the information gain provides a simple initial ( cid : 123 ) lter with which to screen features .
for example , one can rank all genes in the order of increasing information gain and select features conservatively via a statistical signi ( cid : 123 ) cance test ( ben - dor et al . , 123 ) .
to calculate the information gain , we need to quan - tize the values of the features .
this is achieved in our approach via the unconditional mixture model quantization discussed in the previous section .
markov blanket ( cid : 123 ) ltering aims to minimize the discrepancy between the conditional distributions p ( cjf = f ) and p ( cjg = fg ) , as measured by a
p ( f ) d ( p ( cjf = f ) k p ( cjg = fg ) )
where d ( pkq ) = p ( x ) log ( p ( x ) =q ( x ) ) is the kullback - leibler divergence .
the goal is to ( cid : 123 ) nd a small feature set g for which ( cid : 123 ) g is small .
intuitively , if a feature fi is conditionally indepen - dent of the class label given some small subset of the other features , then we should be able to omit fi without compromising the accuracy of class pre - diction .
koller and sahami formalize this idea using the notion of a markov blanket .
de ( cid : 123 ) nition 123 ( markov blanket ) for a feature set g and class label c , the set mi ( cid : 123 ) g ( fi =123 mi ) is a markov blanket of fi ( fi 123 g ) if
fi ? g mi ffig; c j mi
the following proposition due to koller and sahami establishes the relevance of the markov blanket con - cept to the measure ( cid : 123 ) g .
proposition 123 for a complete feature set f , let g be a subset of f , and g123 = g fi .
if 123mi ( cid : 123 ) g ( where mi is a markov blanket of fi ) , then ( cid : 123 ) g123 =
the proposition implies that once we ( cid : 123 ) nd a markov blanket of feature fi in a feature set g , we can safely remove fi from g without increasing the divergence to the desired distribution .
koller and sahami fur - ther prove that in a sequential ( cid : 123 ) ltering process in which unnecessary features are removed one by one , a feature tagged as unnecessary based on the ex - istence of a markov blanket mi remains unneces - sary in later stages when more features have been in most cases , however , few if any features will have a markov blanket of limited size , and we must in - stead look for features that have an \approximate markov blanket . " for this purpose we de ( cid : 123 ) ne
p ( m = fm; fi = fi )
123 markov blanket filtering features that pass the information gain ( cid : 123 ) lter are in - put to a more computationally intensive subset se - lection procedure known as markov blanket ( cid : 123 ) ltering , a technique due to koller and sahami ( 123 ) .
let g be a subset of the overall feature set f .
let fg denote the projection of f onto the variables in
d ( p ( cjm = fm ; fi = fi ) kp ( cjm = fm ) ) : ( 123 ) if m is a markov blanket for fi then ( cid : 123 ) ( fijm ) = 123
since an exact zero is unlikely to occur , we relax the condition and seek a set m such that ( cid : 123 ) ( fijm ) is small .
note that if m is really a markov blanket of fi , then we have p ( cjm; fi ) = p ( cjm )
suggests an easy heuristic way to to search for a feature with an approximate markov blanket .
since the goal is to ( cid : 123 ) nd a small non - redundant fea - ture subset , and those features that form an approx - imate markov blanket of feature di are most likely to be more strongly correlated to fi , we construct a candidate markov blanket for fi by collecting the k features that have the highest correlations ( de - ( cid : 123 ) ned by the pearson correlations between the origi - nal non - quantized feature vectors ) with fi , where k is a small integer .
we have the following algorithm as proposed in ( koller & sahami , 123 ) :
- g = f - for each feature fi 123 g , let mi be the set of k features fj 123 g ffig for which the cor - relations between fi and fj are the highest .
- compute ( cid : 123 ) ( fijmi ) for each i - choose the i that minimizes ( cid : 123 ) ( fijmi ) , and de ( cid : 123 ) ne g = g ffig
this heuristic sequential method is far more e ( cid : 123 ) - cient than methods that conduct an extensive com - binatorial search over subsets of the feature set .
the heuristic method only requires computation of quantities of the form p ( cjm = fm ; fi = fi ) and p ( cjm = fm ) , which can be easily computed using the discretization discussed in section 123 .
classi ( cid : 123 ) cation algorithms we used a gaussian classi ( cid : 123 ) er , a logistic regression classi ( cid : 123 ) er and a nearest neighbor classi ( cid : 123 ) er in our study .
in this section we provide a brief description of these classi ( cid : 123 ) ers .
123 gaussian classi ( cid : 123 ) er a gaussian classi ( cid : 123 ) er is a generative classi ( cid : 123 ) cation model .
the model consists of a prior probability ( cid : 123 ) c for each class c , as well as a gaussian class - conditional density n ( ( cid : 123 ) c; ( cid : 123 ) c ) for class c . 123 maximum likelihood estimates of the parameters are readily restricting ourselves to binary classi ( cid : 123 ) cation , the posterior probability associated with a gaussian classi ( cid : 123 ) er is the logistic function of a quadratic func - tion of the feature vector , which we denote here by
p ( y = 123jx; ( cid : 123 ) ) =
123 + expf 123
123 xt ( cid : 123 ) x ( cid : 123 ) t x g
123note that when the covariance matrix ( cid : 123 ) c is diagonal , then the features are independent given the class and we obtain a continuous - valued analog of the popular naive
where ( cid : 123 ) , ( cid : 123 ) and are functions of the underlying covariances , means and class priors .
if the classes have equal covariance then ( cid : 123 ) is equal to zero and the quadratic function reduces to a linear function .
123 logistic regression logistic regression is the discriminative counterpart of the gaussian classi ( cid : 123 ) er .
here we assume that the posterior probability is the logistic of a linear func - tion of the feature vector : p ( y = 123jx; ( cid : 123 ) ) =
123 + e ( cid : 123 ) t x
where ( cid : 123 ) is the parameter to be estimated .
geomet - rically , this classi ( cid : 123 ) er corresponds to a smooth ramp - like function increasing from zero to one around a decision hyperplane in the feature space .
maximum likelihood estimates of the parameter vec - tor ( cid : 123 ) can be found via iterative optimization algo - rithms .
given our high - dimensional setting , and given the small number of data points , we found that stochastic gradient ascent provided an e ( cid : 123 ) ective optimization procedure .
the stochastic gradient al - gorithm takes the following simple form :
( cid : 123 ) ( t+123 ) = ( cid : 123 ) ( t ) + ( cid : 123 ) ( yn ( cid : 123 ) ( t )
where ( cid : 123 ) ( t ) is the parameter vector at the tth itera - ( t ) t xn ) , and where ( cid : 123 ) is tion , where ( cid : 123 ) ( t ) a step size ( chosen empirically in our experiments ) .
n ( cid : 123 ) 123= ( 123 + e ( cid : 123 )
123 k nearest neighbor classi ( cid : 123 ) cation we also used a simple k nearest neighbor classi - ( cid : 123 ) cation algorithm , setting k equal to three .
the distance metric that we used was the pearson corre -
regularization methods regularization methods provide a popular strategy to cope with over ( cid : 123 ) tting problems ( bishop , 123 ) .
let l ( ( cid : 123 ) j d ) represent the log likelihood associated with a probabilistic model for a data set d .
rather than simply maximizing the log likelihood , we con - sider a \penalized likelihood , " and de ( cid : 123 ) ne the follow - ing \regularized estimate " of the parameters :
^ ( cid : 123 ) = arg max
fl ( ( cid : 123 ) j d ) ( cid : 123 ) k ( cid : 123 ) kg ;
where k ( cid : 123 ) k is an appropriate norm , typically the l123 or the l123 norm , and where ( cid : 123 ) is a free parameter known as the \regularization parameter . " the basic idea is that the penalty term often leads to a sig - ni ( cid : 123 ) cant decrease in the variance of the estimate , at the expense of a slight bias , yielding an overall de - crease in risk .
one can also take a bayesian point of
predictive power of the genes
information gain for each genes with respect to the given partition
kl of each removal gene w . r . t .
to its mb
figure 123
feature selection using using a 123 - stage procedure .
( a ) genes ranked by ( cid : 123 ) ( eq .
123 ) ; ( b ) genes ranked by igain ( eq .
123 ) ; ( c ) genes ranked by ( cid : 123 ) ( fijm ) ( eq
view and interpret the penalty term as a log prior , in which case regularization can be viewed as a max - imum a posteriori estimation method .
the regularization parameter ( cid : 123 ) is generally set via some form of cross validation .
an l123 penalty is a rotation - invariant penalty , and shrinks the parameters along a ray toward the ori - gin .
using an l123 penalty , on the other hand , shrinks the parameters toward the l123 ball , which is not rota - tion invariant .
some of the parameters shrink more quickly than others , and indeed parameters can be set to zero in the l123 case ( tibshirani , 123 ) .
thus an l123 penalty has something of the avor of a fea - ture selection method .
parameter estimation is straightforward in the regu - larization setting , with the penalty term simply con - tributing an additive term to the gradient .
for ex - ample , in the case of logistic regression with an l123 penalty , we obtain the following stochastic gradient :
n ) xn ( cid : 123 ) ( cid : 123 ) ( t )
( cid : 123 ) ( t+123 ) = ( cid : 123 ) ( t ) + ( cid : 123 )
where the shrinkage toward the origin is apparent .
in the case of gaussian classi ( cid : 123 ) er , the regularized ml estimate of ( cid : 123 ) can be easily solved in closed form .
experiments and results in this section , we report the results of analysis of the data from a microarray classi ( cid : 123 ) cation prob - lem .
our data is a collection of 123 samples from leukemia patients , with each sample giving the ex - pression levels of 123 genes ( golub et al . , 123 ) .
according to pathological / histological criteria , these samples include 123 type i leukemias ( called all ) and 123 type ii leukemias ( called aml ) .
the sam - ples are split into two sets by the provider , with 123 ( all / aml=123 / 123 ) serving as a training set and the remaining 123 ( 123 / 123 ) as a test set .
the goal is to learn a binary classi ( cid : 123 ) er ( for the two cancer subtypes ) based on the gene expression patterns .
123 filtering results figure 123 ( a ) shows the mixture overlap probability ( cid : 123 ) ( de ( cid : 123 ) ned by eq .
123 ) for each single gene in ascending order .
it can be seen that only a small percentage of
the genes have an overlap probability signi ( cid : 123 ) cantly smaller than ( cid : 123 ) ( cid : 123 ) 123 : 123 , where 123 : 123 would constitute random guessing under a gaussian model if the un - derlying mixture components were construed as class in figure 123 ( b ) we present the information gain that can be provided by each individual gene with re - spect to the reference partition ( the leukemia class labels ) , compared to the partition obtained from the mixture models .
only a very small fraction of the genes induce a signi ( cid : 123 ) cant information gain .
we take the top 123 genes from this list and proceed with ( approximate ) markov blanket ( cid : 123 ) ltering .
figure 123 ( c ) displays the values of ( cid : 123 ) ( fijmi ) ( cf .
123 ) for each fi , an assessment of the extent to which the approximate markov blanket mi sub - sumes information carried by fi and thus renders fi redundant .
genes are ordered in their removal sequence from right to left .
note that the re - dundancy measure ( cid : 123 ) ( fijmi ) increases until there are fewer than 123 genes remaining .
at this point ( cid : 123 ) ( fijmi ) decreases , presumably because of a com - positional change in the approximate markov blan - kets of these genes compared to the original contents before many genes were removed .
the increasing trend of ( cid : 123 ) ( fijmi ) then resumes .
the fact that in a real biological regulatory network the fan - in and fan - out will generally be small pro - vides some justi ( cid : 123 ) cation for enforcing small markov blankets .
in any case , we have to keep the markov blankets small to avoid fragmenting our small data
123 classi ( cid : 123 ) cation results figure 123 shows training set and test set errors for each of the three di ( cid : 123 ) erent classi ( cid : 123 ) ers .
for each feature subset cardinality ( the abscissa in these graphs ) , we chose a feature subset using markov blanket ( cid : 123 ) ltering .
this is a classi ( cid : 123 ) er - independent method , thus the feature subsets are the same in all three ( cid : 123 ) gures .
the ( cid : 123 ) gures show that for all classi ( cid : 123 ) ers , after an initial coevolving trend of the training and testing curves for low - dimensional feature spaces ( the di - mensionality di ( cid : 123 ) ers for the di ( cid : 123 ) erent classi ( cid : 123 ) ers ) , the
gaussian generative model
number of features
number of features
number of features
figure 123
classi ( cid : 123 ) cation in a sequence of di ( cid : 123 ) erent feature spaces with increasing dimensionality due to inclusion of gradually less quali ( cid : 123 ) ed features .
( a ) classi ( cid : 123 ) cation using knn classi ( cid : 123 ) er; ( b ) classi ( cid : 123 ) cation using a quadratic bayesian classi ( cid : 123 ) er given by a gaussian generative model; ( c ) a linear classi ( cid : 123 ) er obtained from logistic regression .
all three classi ( cid : 123 ) ers use the same 123 - 123 genes selected by the three stages of feature selection .
classi ( cid : 123 ) ers quickly over ( cid : 123 ) t the training data .
for the logistic linear classi ( cid : 123 ) er and knn , the test error tops out at approximately 123 percent when the entire fea - ture set of 123 genes is used .
the generative gaus - sian quadratic classi ( cid : 123 ) er over ( cid : 123 ) ts less severely in the full feature space .
for all three classi ( cid : 123 ) ers , the best performance is achieved only in a signi ( cid : 123 ) cantly lower dimensional feature space .
of the three classi ( cid : 123 ) ers , knn requires the most features to achieve its best figure 123 shows that by an optimal choice of the num - ber of features it is possible to achieve error rates of 123% , 123% , and 123% for the gaussian classi ( cid : 123 ) er , the logistic regression classi ( cid : 123 ) er , and knn , respectively .
of course , in actual diagnostic practice we do not have the test set available , so these numbers are op - timistic .
to choose the number of features in an automatic way , we make use of leave - one - out cross validation on the training data .
that is , for each cardinality of feature subset , given the feature sub - set chosen by our ( cid : 123 ) ltering method , we choose among cardinalities by cross validation .
thus we have in essence a hybrid of a ( cid : 123 ) lter method and a wrapper method|the ( cid : 123 ) lter method is used to choose feature subsets , and the wrapper method is used to compare between best subsets for di ( cid : 123 ) erent cardinalities .
the results of leave - one - out cross validation are shown in figure 123
note that we have several minima for each of the cross - validation curves .
breaking ties by choosing the minima having the smallest cardi - nality , and running the resulting classi ( cid : 123 ) er on the test set , we obtain error rates of 123% , 123% , and 123% for the gaussian classi ( cid : 123 ) er , the logistic regression classi - ( cid : 123 ) er , and knn , respectively .
we also compared prediction performance when us - ing the unconditional mixture modeling ( mm ) ( cid : 123 ) lter alone and the information gain ( ig ) ( cid : 123 ) lter alone ( in the latter case , using the discretization provided by
table 123
performance of classi ( cid : 123 ) cation based on randomly selected features ( 123 trials )
training error ( % )
test error ( % )
classi ( cid : 123 ) er max min average max min average
knn 123 123
123 123 123 123 123 123
the ( cid : 123 ) rst phase of mixture modeling ) .
the results for the logistic regression classi ( cid : 123 ) er are shown in figure 123
as can be seen , the number of features deter - mined by cross - validation using the mm ( cid : 123 ) lter is 123 ( compared to 123 using the full markov blanket ( cid : 123 ) l - tering ) and the resulting classi ( cid : 123 ) er also has a higher test set error ( 123% versus 123% ) .
for the ig ( cid : 123 ) lter , the selected number of features is 123 , and the test set error rate is signi ( cid : 123 ) cantly higher ( 123% ) .
the latter result in particular suggests that it is not suf - ( cid : 123 ) cient to simply performance a \relevance check " to select features , but rather that a redundancy re - duction method such as the markov blanket ( cid : 123 ) lter appears to be required .
note also that using the mm ( cid : 123 ) lter alone results in better performance than using the ig ( cid : 123 ) lter alone .
while neither approach performs as well as markov blanket ( cid : 123 ) ltering , the mm ( cid : 123 ) lter has the advantage that it does not require class labels .
this opens up the possibility of doing feature selection on this data set in the context of unsupervised clustering ( see xing & karp , 123 ) .
in some high - dimensional problems , it may be pos - sible to bypass feature selection algorithms and ob - tain reasonable classi ( cid : 123 ) cation performance by choos - ing random subsets of features .
that this is not the case in the leukemia data set is shown by the re - sults ( table 123 ) .
in the experiments reported in this table , we chose ten randomly selected features for each classi ( cid : 123 ) er .
the performance is poorer than in the case of explicit feature selection .
number of features
gaussian generative model
number of features
number of features
figure 123
plots of leave - one - out cross validation error for the three classi ( cid : 123 ) ers .
lr ( with only mm filter )
lr ( with only ig filter )
number of features
number of features
figure 123
plots of leave - one - out cross validation error for the logistic regression classi ( cid : 123 ) er with ( a ) only the uncon - ditional mixture modeling ( cid : 123 ) lter and ( b ) only the infor - mation gain ( cid : 123 ) lter .
123 regularization versus feature selection
l123 for gaussian generative model
l123 for gaussian generative model
figure 123
training set and test set error as a function of the regularization parameter ( cid : 123 ) .
the results are for the gaussian classi ( cid : 123 ) er using the ( a ) l123 and ( b ) l123 penalties .
figure 123 shows the results for the regularized gaus - sian classi ( cid : 123 ) er using the l123 and l123 penalties .
similar results were found for the logistic regression classi - by choosing an optimal value of ( cid : 123 ) based ( optimisti - cally ) on the test set , we obtain test set errors of 123% and 123% for the gaussian classi ( cid : 123 ) er for the l123 and l123 norm respectively .
for the logistic regres - sion classi ( cid : 123 ) er , we obtain test set errors of 123% and
these errors are higher than those obtained with ex - plicit feature selection .
indeed , a comparison of fig - ures 123 and 123 , which show the range of test set perfor - mance achievable from the feature selection and the regularization approaches , respectively , show that the feature selection curves are generally associated with smaller error .
given that the regularization ap - proach can , in the worst case , leave us with all 123 features , we feel that feature selection provides the better alternative for our problem .
discussion and conclusion we have shown that feature selection methods can be applied successfully to a classi ( cid : 123 ) cation problem in - volving only 123 training data points in a 123 dimen - sional space .
this problem exempli ( cid : 123 ) es a situation that will be increasingly common in applications of machine learning to molecular biology .
microarray technology makes it possible to put the probes for the genes of an entire genome onto a chip , such that each data point provided by an experimenter lies in the high - dimensional space de ( cid : 123 ) ned by the size of the genome under investigation .
in high - dimensional problems such as these , feature selection methods are essential if the investigator is to make sense of his or her data , particularly if the goal of the study is to identify genes whose expres - sion patterns have meaningful biological relation - ships to the classi ( cid : 123 ) cation problem .
computational reasons can also impose important constraints .
fi - nally , as demonstrated in smaller problems in the ex - tant literature on feature selection ( kohavi & john , 123; langley , 123 ) , and as we have seen in the high - dimensional problem studied here , feature se - lection can lead to improved classi ( cid : 123 ) cation .
all of the classi ( cid : 123 ) ers that we studied|a generative gaussian classi ( cid : 123 ) er , a discriminative logistic regression classi - ( cid : 123 ) er , and a k - nn classi ( cid : 123 ) er , performed signi ( cid : 123 ) cantly better in the reduced feature space than in the full we have not attempted to discuss the biological sig -
cancer : class discovery and class prediction by gene expression monitoring .
science , 123 , 123 (
kohavi , r . , & john , g .
( 123 ) .
wrapper for feature subset selection .
arti ( cid : 123 ) cial intelligence , 123 , 123 (
koller , d . , & sahami , m .
( 123 ) .
toward optimal feature selection .
proceedings of the thirteenth international conference on machine learning .
langley , p .
( 123 ) .
selection of relevant features in machine learning .
proceedings of the aaai fall symposium on relevance .
aaai press .
( 123 ) .
on feature selection : learning with exponentially many irrelevant features as training examples .
proceedings of the fifteenth interna - tional conference on machine learning .
shalon , d . , smith , s .
j . , & brown , p .
( 123 ) .
a dna microarray system for analyzing complex dna samples using two - color uorescent probe hybridization .
genome research , 123 ( 123 ) , 123 ( 123
tibshirani , r .
( 123 ) .
regression selection and shrinkage via the lasso .
journal of the royal sta - tistical society b , 123 , 123 ( 123
xing , e .
p . , & karp , r .
( 123 ) .
cli ( cid : 123 ) : clustering of high - dimensional microarray data via iterative feature ( cid : 123 ) ltering using normalized cuts .
proceed - ings of the nineteenth international conference on intelligent systems for molecular biology .
ni ( cid : 123 ) cance of the speci ( cid : 123 ) c features that our algorithm identi ( cid : 123 ) ed , but it is worth noting that seven out of the ( cid : 123 ) fteen best features identi ( cid : 123 ) ed by our algorithm are included in the set of 123 informative features used in ( golub et al . , 123 ) , and moreover there is a similar degree of overlap with another recent study on this data set ( chow et al . , in press ) .
the fact that the overlap is less than perfect is likely due to the redundancy of the features in this data set .
note in particular that our algorithm works ex - plicitly to eliminate redundant features , whereas the golub and chow methods do not .
we have compared feature selection to regulariza - tion methods , which leave the feature set intact , but shrink the numerical values of the parameters to - ward zero .
our results show that explicit feature selection yields classi ( cid : 123 ) ers that perform better than regularization methods .
given the other advantages associated with feature selection , including compu - tational and interpretational , we feel that feature selection provides the preferred alternative on these it is worth noting , however , that these ap - proaches are not mutually exclusive and it may be worthwhile to consider combinations .
we thank andrew ng for helpful comments .
this work is partially supported by onr muri n123 - 123 - 123 - 123 and nsf grant iis - 123
