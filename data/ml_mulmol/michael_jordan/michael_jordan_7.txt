many algorithms rely critically on being given a good metric over their inputs .
for instance , data can often be clustered in many plausible ways , and if a clustering algorithm such as k - means initially fails to nd one that is meaningful to a user , the only recourse may be for the user to manually tweak the metric until sufciently good clusters are found .
for these and other applications requiring good metrics , it is desirable that we provide a more systematic way for users to indicate what they con - sider similar .
for instance , we may ask them to provide examples .
in this paper , we present an algorithm that , given examples of similar ( and , , learns a distance metric over if desired , dissimilar ) pairs of points in that respects these relationships .
our method is based on posing met -
ric learning as a convex optimization problem , which allows us to give efcient , local - optima - free algorithms .
we also demonstrate empirically that the learned metrics can be used to signicantly improve clustering
the performance of many learning and datamining algorithms depend critically on their being given a good metric over the input space .
for instance , k - means , nearest - neighbors classiers and kernel algorithms such as svms all need to be given good metrics that reect reasonably well the important relationships between the data .
this problem is particularly acute in unsupervised settings such as clustering , and is related to the perennial problem of there often being no right answer for clustering : if three algorithms are used to cluster a set of documents , and one clusters according to the authorship , another clusters according to topic , and a third clusters according to writing style , who is to say which is the right answer ? worse , if an algorithm were to have clustered by topic , and if we instead wanted it to cluster by writing style , there are relatively few systematic mechanisms for us to convey this to a clustering algorithm , and we are often left tweaking distance metrics by hand .
in this paper , we are interested in the following problem : suppose a user indicates that ) are considered by them to be similar .
can we certain points in an input space ( say ,
automatically learn a distance metric over that respects these relationships , i . e . , one that
assigns small distances between the similar pairs ? for instance , in the documents example , we might hope that , by giving it pairs of documents judged to be written in similar styles , it would learn to recognize the critical features for determining style .
one important family of algorithms that ( implicitly ) learn metrics are the unsupervised ones that take an input dataset , and nd an embedding of it in some space .
this includes algorithms such as multidimensional scaling ( mds ) ( 123 ) , and locally linear embedding ( lle ) ( 123 ) .
one feature distinguishing our work from these is that we will learn a full metric over the input space , rather than focusing only on ( nding an embed - ding for ) the points in the training set .
our learned metric thus generalizes more easily to previously unseen data .
more importantly , methods such as lle and mds also suffer from the no right answer problem : for example , if mds nds an embedding that fails to cap - ture the structure important to a user , it is unclear what systematic corrective actions would be available .
( similar comments also apply to principal components analysis ( pca ) ( 123 ) . ) as in our motivating clustering example , the methods we propose can also be used in a pre - processing step to help any of these unsupervised algorithms to nd better solutions .
in the supervised learning setting , for instance nearest neighbor classication , numerous attempts have been made to dene or learn either local or global metrics for classication .
in these problems , a clear - cut , supervised criterionclassication erroris available and can be optimized for .
( see also ( 123 ) , for a different way of supervising clustering . ) this literature is too wide to survey here , but some relevant examples include ( 123 , 123 , 123 , 123 ) , and ( 123 ) also gives a good overview of some of this work .
while these methods often learn good metrics for classication , it is less clear whether they can be used to learn good , general metrics for other algorithms such as k - means , particularly if the information available is less structured than the traditional , homogeneous training sets expected by in the context of clustering , a promising approach was recently proposed by wagstaff et al .
( 123 ) for clustering with similarity information .
if told that certain pairs are similar or dissimilar , they search for a clustering that puts the similar pairs into the same , and dis - similar pairs into different , clusters .
this gives a way of using similarity side - information to nd clusters that reect a users notion of meaningful clusters .
but similar to mds and lle , the ( instance - level ) constraints that they use do not generalize to previously unseen data whose similarity / dissimilarity to the training set is not known .
we will later discuss this work in more detail , and also examine the effects of using the methods we propose in conjunction with these methods .
123 learning distance metrics
, and are given information that certain
that respects this;
pairs of them are similar :
suppose we have some set of points
how can we learn a distance metric
and ! are similar " # ! between points
specically , so that similar points end up close to each other ? consider learning a distance metric of the form
of mahalanobis distances over
to be diagonal , this corresponds to learning a metric in which
to ensure that this be a metricsatisfying non - negativity and the triangle inequality
we require that 123 be positive semi - denite , 123; : =< . 123 setting 123>$@ ? gives euclidean distance; if we restrict 123 the different axes are given different weights; more generally , 123 parameterizes a family to nding a rescaling of a data that replaces each point with 123 123technically , this also allows pseudometrics , wheredfehgjilknmpo / qsr does not implyitqum 123note that , but putting the original dataset through a non - linear basis functionv and considering gxv / gji ! o y123vzgjmpono\ ( l ) ^gxv / gji ! oy_vzgjmpono , non - linear distance metrics can also be learned .
. 123 learning such a distance metric is also equivalent and applying the
standard euclidean metric to the rescaled data; this will later be useful in visualizing the a simple way of dening a criterion for the desired metric is to demand that pairs of points small squared distance between them :
can be a set of pairs of points known to be dissimilar if such information is explicitly available; otherwise , we may take it to be all
this is trivially solved with 123
useful , and we add the constraint
collapse the dataset into a single point .
here , ! pairs not in
this gives the optimization problem :
< , which is not
to ensure that 123
an efcient algorithm using the newton - raphson method
always being rank 123 ( i . e . , the data are always projected onto a line )
the choice of the constant 123 in the right hand side of ( 123 ) is arbitrary but not important , and , and both of the constraints are also easily veried to be convex .
thus , the optimization problem is convex , which enables us to derive efcient , local - minima - free algorithms to solve it .
' would not be a good choice despite its giving a simple linear constraint .
it , we can derive
changing it to any other positive constant% results only in123 being replaced by% this problem has an objective that is linear in the parameters 123 we also note that , while one might consider various alternatives to ( 123 ) , would result in123 123 the case of diagonal123 in the case that we want to learn a diagonal 123
( subject to 123 it is straightforward to show that minimizing - multiplication of 123 thus use newton - raphson to efciently optimize - 123 the case of full123 in the case of learning a full matrix 123 to enforce , and newtons method often becomes prohibitively expensive ( requiring 123 time to invert the hessian over123 .
decomposing ) sible since ) ai r ) , this gives= j , which we recognize as a rayleigh - e for the principal eigenvector , and settingg 123to ensure that ) mir , which is true iff the diagonal elements ) jj are non - negative , we actually replace the newton updatenpo line - search to give the largest downhill step subject to ) zjjuv . r .
, the constraint that 123 < becomes slightly trickier b parameters ) .
using gradient descent and the idea of
123the proof is reminiscent of the derivation of fishers linear discriminant .
briey , consider max -
< ) is equivalent , up to a
iterative projections ( e . g . , ( 123 ) ) we derive a different algorithm for this setting .
quotient like quantity whose solution is given by ( say ) solving the generalized eigenvector problem
by a positive constant , to solving the original problem ( 123 ) .
we can
is a step - size parameter optimized via a
is the frobenius norm on
we pose the equivalent problem :
figure 123 : gradient ascent + iterative projection algorithm .
here , hbhhhbh
we will use a gradient ascent step on - repeatedly take a gradient step 123
a single linear constraint; the solution to this is easily found by solving ( in 123 a sparse system of linear equations .
the second projection step onto b , the space of all positive - semi denite matrices , is done by rst nding the diagonalization 123 is a diagonal matrix of 123 s corresponding eigenvectors , and taking 123+
( e . g . , see ( 123 ) . )
to optimize ( 123 ) , followed by the method of iterative projections to ensure that the constraints ( 123 ) and ( 123 ) hold .
specically , we will .
this gives the
b can be done inexpensively .
specically , the rst projection step 123
algorithm shown in figure 123 the motivation for the specic choice of the problem formulation ( 123 ) is that projecting
123 , and then repeatedly project 123
involves minimizing a quadratic objective subject to
s eigenvalues and the columns of
123 experiments and examples
we begin by giving some examples of distance metrics learned on articial data , and then show how our methods can be used to improve clustering performance .
, we obtain :
123 examples of learned distance metrics consider the data shown in figure 123 ( a ) , which is divided into two classes ( shown by the different symbols and , where available , colors ) .
suppose that points in each class are sim - reecting this . 123 depending on whether we learn a
ilar to each other , and we are given diagonal or a full123 to visualize this , we can use the fact discussed earlier that learning * , *cb* , * to nding a rescaling of the data direction of the projection ofq disrupt the constrainte e .
empirically , this modication often signicantly speeds up convergence .
123in the experiments with synthetic data , f was a randomly sampled 123% of all pairs of similar
, that hopefully moves the similar pairs 123the algorithm shown in the gure includes a small renement that the gradient step is taken the , so that it will minimally
r onto the orthogonal subspace ofq
123class data ( original )
123class data projection ( newton )
123class data projection ( ip )
figure 123 : ( a ) original data , with the different classes indicated by the different symbols ( and col - ors , where available ) .
( b ) rescaling of data corresponding to learned diagonal )
123class data projection ( newton )
123class data projection ( ip )
corresponding to full )
123class data ( original )
( c ) rescaling corre -
figure 123 : ( a ) original data .
( b ) rescaling corresponding to learned diagonal ) sponding to full ) together .
figure 123 ( b , c ) shows the result of plotting 123
successfully brought together the similar points , while keeping dissimilar ones apart .
figure 123 shows a similar result for a case of three clusters whose centroids differ only in the x and y directions .
as we see in figure 123 ( b ) , the learned diagonal metric correctly , the algorithm nds a surprising
ignores the z direction .
interestingly , in the case of a full 123
projection of the data onto a line that still maintains the separation of the clusters well .
as we see , the algorithm has
123 application to clustering
one application of our methods is clustering with side information , in which we learn a distance metric using similarity information , and cluster data using that metric .
speci -
k - means + metric : k - means but with distortion dened using the distance metric
b between points
assigned to the same cluster ( 123 ) . 123
to the same cluster .
we will consider four algorithms for clustering :
cally , suppose we are given , and told that each pair 123
k - means using the default euclidean metric * , * to dene distortion ( and ignoring ) .
constrained k - means : k - means but subject to points
more generally , if we imagine drawing an edge between each pair of points in
123this is implemented as the usual k - means , except ifgji points are assigned to cluster centroids , we assign bothikj andi the points in each resulting connected componente we pick to beu t
constrained k - means + metric : constrained k - means using the distance metric
, then all are constrained to lie in the same cluster , which
, then during the step in which
original 123class data
porjected 123class data
k - means : accuracy = 123 123
constrained k - means : accuracy = 123 123
k - means + metric : accuracy = 123 123
constrained k - means + metric : accuracy = 123
figure 123 : ( a ) original dataset ( b ) data scaled according to learned metric .
s match the%
s result is gave visually indistinguishable results . )
) be the cluster to which point
is assigned by an automatic clustering
be some correct or desired clustering of the data .
following ( ? ) , in algorithm , and let%
s according to the case of 123 - cluster data , we will measure how well the < ) .
this is equivalent to the probability that for two points l , ! drawn randomly from the dataset , our clustering % agrees with the true clustering % on whether z and ! belong to same or different their - coordinate , but where the data in its original space seems to cluster much better according to their # - coordinate .
as shown by the accuracy scores given in the gure , both
as a simple example , consider figure 123 , which shows a clustering problem in which the true clusters ( indicated by the different symbols / colors in the plot ) are distinguished by
is the indicator function (
k - means and constrained k - means failed to nd good clusterings .
but by rst learning a distance metric and then clustering according to that metric , we easily nd the correct clustering separating the true clusters from each other .
figure 123 gives another example showing similar results .
we also applied our methods to 123 datasets from the uc irvine repository .
here , the true clustering is given by the datas class labels .
in each , we ran one experiment using lit - , and one with much side - information .
the results are given in
almost any clustering will correctly predict that most pairs are in different clusters .
in this setting ,
we see that , in almost every problem , using a learned diagonal or full metric leads to signicantly improved performance over naive k - means .
in most of the problems , using , 123th bar for full 123 a learned metric with constrained k - means ( the 123th bar for diagonal 123 also outperforms using constrained k - means alone ( 123th bar ) , sometimes by a very large 123in the case of many ( ) clusters , this evaluation metric tends to give inated scores since we therefore modied the measure averaging not only ij , i l drawn uniformly at random , but from ! ) with chance 123 , and from different clusters with chance 123 , so the same cluster ( as determined by that matches and mis - matches are given the same weight .
all results reported here used k - means with multiple restarts , and are averages over at least 123 trials ( except for wine , 123 trials ) .
123f was generated by picking a random subset of all pairs of points sharing the same class ! j .
in resulting connected components " $#
the case of little side - information , the size of the subset was chosen so that the resulting number of ( see footnote 123 ) would be very roughly 123% of the size of the
original dataset .
in the case of much side - information , this was changed to 123% .
k - means : accuracy = 123 123
constrained k - means : accuracy = 123 123
k - means + metric : accuracy = 123 123
constrained k - means + metric : accuracy = 123
figure 123 : ( a ) original dataset ( b ) data scaled according to learned metric .
gave visually indistinguishable results . )
s result is
boston housing ( n=123 , c=123 , d=123 )
ionosphere ( n=123 , c=123 , d=123 )
iris plants ( n=123 , c=123 , d=123 )
wine ( n=123 , c=123 , d=123 )
soy bean ( n=123 , c=123 , d=123 )
balance ( n=123 , c=123 , d=123 )
protein ( n=123 , c=123 , d=123 )
breast cancer ( n=123 , c=123 , d=123 )
diabetes ( n=123 , c=123 , d=123 )
figure 123 : clustering accuracy on 123 uci datasets .
in each panel , the six bars on the left correspond to an experiment with little side - informationf , and the six on the right to much side - information .
from left to right , the six bars in each set are respectively k - means , k - means diagonal metric , and : number of classes / clusters;d : di - mensionality of data; " $# : mean number of connected components ( see footnotes 123 , 123 ) .
bars are also shown .
full metric , constrained k - means ( c - kmeans ) , c - kmeans full metric .
also shown are
: size of dataset; e
performance on protein dataset
performance on wine dataset
kmeans + metric ( diag a ) ckmeans + metric ( diag a ) kmeans + metric ( full a ) ckmeans + metric ( full a )
ratio of constraints
kmeans + metric ( diag a ) ckmeans + metric ( diag a ) kmeans + metric ( full a ) ckmeans + metric ( full a )
ratio of constraints
figure 123 : plots of accuracy vs .
amount of side - information .
here , thei - axis gives the fraction of all pairs of points in the same class that are randomly sampled to be included inf margin .
not surprisingly , we also see that having more side - information in
leads to metrics giving better clusterings .
figure 123 also shows two typical examples of how the quality of the clusterings found in - creases with the amount of side - information .
for some problems ( e . g . , wine ) , our algo - rithm learns good diagonal and full metrics quickly with only a very small amount of side - information; for some others ( e . g . , protein ) , the distance metric , particularly the full metric , appears harder to learn and provides less benet over constrained k - means .
we have presented an algorithm that , given examples of similar pairs of points in a distance metric that respects these relationships .
our method is based on posing metric learning as a convex optimization problem , which allowed us to derive efcient , local - optima free algorithms .
we also showed examples of diagonal and full metrics learned from simple articial examples , and demonstrated on articial and on uci datasets how our methods can be used to improve clustering performance .
