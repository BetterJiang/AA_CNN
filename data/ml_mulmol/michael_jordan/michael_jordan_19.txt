internal models of the environment have an important role to play in adap - tive systems in general and are of particular importance for the supervised learning paradigm .
in this paper we demonstrate that certain classical prob - lems associated with the notion of the teacher " in supervised learning can be solved by judicious use of learned internal models as components of the adap - tive system .
in particular , we show how supervised learning algorithms can be utilized in cases in which an unknown dynamical system intervenes between actions and desired outcomes .
our approach applies to any supervised learning algorithm that is capable of learning in multi - layer networks .
*this paper is a revised version of mit center for cognitive science occasional paper .
we wish to thank michael mozer , andrew barto , robert jacobs , eric loeb , and james mcclelland for helpful comments on the manuscript .
this project was supported in part by brsg s rr - awarded by the biomedical re - search support grant program , division of research resources , national institutes of health , by a grant from atr auditory and visual perception research labora - tories , by a grant from siemens corporation , by a grant from the human frontier science program , and by grant n - - j - awarded by the oce of naval
recent work on learning algorithms for connectionist networks has seen a pro - gressive weakening of the assumptions made about the relationship between the learner and the environment .
classical supervised learning algorithms such as the perceptron rosenblatt , and the lms algorithm widrow & ho , made two strong assumptions : the output units are the only adaptive units in the network , and there is a teacher " that provides desired states for all of the output units .
early in the development of such algorithms it was recognized that more powerful supervised learning algorithms could be realized by weakening the rst assumption and incorporating internal units that adaptively recode the input representation provided by the environment rosenblatt , .
the subsequent development of algorithms such as boltzmann learning hinton & sejnowski , and backpropagation lecun , ; parker , ; rumelhart , hinton , & williams , ; werbos , have provided the means for training networks with adaptive nonlinear internal units .
the second assumption has also been weakened|learning algorithms that require no explicit teacher have been developed becker & hinton , ; grossberg , ; kohonen , ; linsker , ; rumelhart & zipser , .
such unsupervised " learning algorithms generally perform some sort of clustering or feature extraction on the input data and are based on assumptions about the statistical or topological properties of the input ensemble .
in this paper we examine in some detail the notion of the teacher " in the supervised learning paradigm .
we argue that the teacher is less of a liability than has commonly been assumed and that the assumption that the environment provides desired states for the output of the network can be weakened signicantly without abandoning the supervised learning paradigm altogether .
indeed , we feel that an appropriate interpretation of the role of the teacher is crucial in appreciating the range of problems to which the paradigm can be applied .
the issue that we wish to address is best illustrated by way of an example .
consider a skill - learning task such as that faced by a basketball player learning to shoot baskets .
the problem for the learner is to nd the appropriate muscle commands to propel the ball toward the goal .
dierent commands are appropriate for dierent locations of the goal in the visual scene; thus , a mapping from visual scenes to muscle commands is required .
what learning algorithm might underly the acquisition of such a mapping ? clearly , clustering or feature extraction on the visual input is not sucient .
moreover , it is dicult to see how to apply classical supervised algorithms to this problem , because there is no teacher to provide muscle commands as targets to the learner .
the only target information provided to the learner is in terms of the outcome of the movement; that is , the sights and sounds of a ball passing through the goal .
the general scenario suggested by the example is shown in figure .
intentions are provided as inputs to the learning system .
the learner transforms intentions into actions , which are transformed by the environment into outcomes .
actions are proximal variables; that is , variables that the learner controls directly , while
figure : the distal supervised learning problem .
target values are available for the distal variables the outcomes " but not for the proximal variables the actions " .
outcomes are distal variables , variables that the learner controls indirectly through the intermediary of the proximal variables .
during the learning process , target values are assumed to be available for the distal variables but not for the proximal variables .
therefore , from a point of view outside the learning system , a distal supervised learning task " is a mapping from intentions to desired outcomes .
from the point of view of the learner , however , the problem is to nd a mapping from intentions to actions that can be composed with the environment to yield desired distal outcomes .
the learner must discover how to vary the components of the proximal action vector so as to minimize the components of the distal error .
the distal supervised learning problem also has a temporal component .
in many environments the eects of actions are not punctate and instantaneous , but rather linger on and mix with the eects of other actions .
thus the outcome at any point in time is inuenced by any of a number of previous actions .
even if there exists a set of variables that have a static relationship to desired outcomes , the learner often does not have direct control over those variables .
consider again the example of the basketball player .
although the ight of the ball depends only on the velocity of the arm at the moment of releasea static relationshipit is unlikely that the motor control system is able to control release velocity directly .
rather , the system outputs forces or torques , and these variables do not have a static relationship to the distal outcome .
in the remainder of the paper we describe a general approach to solving the dis - tal supervised learning problem .
the approach is based on the idea that supervised learning in its most general form is a two - phase procedure .
in the rst phase the learner forms a predictive internal model a forward model of the transformation from actions to distal outcomes .
because such transformations are often not known a priori , the internal model must generally be learned by exploring the outcomes associated with particular choices of actions .
this auxiliary learning problem is it - self a supervised learning problem , based on the error between internal , predicted outcomes and actual outcomes .
once the internal model has been at least par - tially learned , it can be used in an indirect manner to solve for the mapping from
intentions to actions .
the idea of using an internal model to augment the capabilities of supervised learning algorithms has also been proposed by werbos , although his perspec - tive diers in certain respects from our own .
there have been a number of further developments of the idea kawato , ; miyata , ; munro , ; nguyen & widrow , ; robinson & fallside , ; schmidhuber , , based either on the work of werbos or our own unpublished work jordan , ; rumelhart , .
there are also close ties between our approach and techniques in optimal control theory kirk , and adaptive control theory goodwin & sin , ; narendra & parthasarathy , .
we discuss several of these relationships in the remainder of the paper , although we do not attempt to be comprehensive .
distal supervised learning and forward models
this section and the following section present a general approach to solving distal supervised learning problems .
we begin by describing our assumptions about the environment and the learner .
we assume that the environment can be characterized by a next - state function f and an output function g .
at time step n ( cid : 123 ) the learner produces an action un ( cid : 123 ) .
in conjunction with the state of the environment xn ( cid : 123 ) the action determines the next state xn :
xn = f xn ( cid : 123 ) ; un ( cid : 123 ) :
corresponding to each state xn there is also a sensation yn :
yn = gxn :
note that sensations are output vectors in the current formalism|outcomes " in the language of the introductory section .
the next - state function and the output function together determine a state - dependent mapping from actions to sensations .
in the current paper we assume that the learner has access to the state of the environment; we do not address issues relating to state representation and state estimation .
state representations might involve delayed values of previous actions and sensations ljung & soderstrom , , or they might involve internal state variables that are induced as part of the learning procedure mozer & bachrach , .
given the state xn ( cid : 123 ) and given the input pn ( cid : 123 ) , the learner produces an action un ( cid : 123 ) :
un ( cid : 123 ) = hxn ( cid : 123 ) ; pn ( cid : 123 ) :
the choice of time indices in equations , , and is based on our focus on the output at time n .
in our framework a learning algorithm alters yn based on previous values of the states , inputs ,
figure : the composite performance system consisting of the learner and the environment .
this system is a mapping from inputs pn ( cid : 123 ) to sensations yn .
the training data fpin ( cid : 123 ) ; y i ng specify desired inputoutput behavior across the composite system .
note that there is an implicit loop within the environment such that the output at time n depends on the state at time n ( cid : 123 ) cf .
equation .
a distal supervised learning problem is a set of training pairs fpin ( cid : 123 ) ; y
the goal of the learning procedure is to make appropriate adjustments to the input - to - action mapping h based on data obtained from interacting with the environment .
where pin ( cid : 123 ) are the input vectors and y i n are the corresponding desired sen - sations .
for example , in the basketball problem , the input might be a high - level intention of shooting a basket , and a desired sensation would be the correspond - ing visual representation of a successful outcome .
note that the distal supervised learning problem makes no mention of the actions that the learner must acquire; only inputs and desired sensations are specied .
from a point of view outside the learning system the training data specify desired inputoutput behavior across the composite performance system consisting of the learner and the environment see figure .
from the point of view of the learner , however , the problem is to nd a mapping from inputs pn ( cid : 123 ) to actions un ( cid : 123 ) such that the resulting distal sensations yn are the target values yn .
that is , the learner must nd a mapping from inputs to actions that can be placed in series with the environment so as to yield the desired pairing of inputs and sensations .
note that there may be more than one action that yields a given desired sensation from any given state; that is , the distal supervised learning problem may be underdetermined .
thus , in the bas - ketball example , there may be a variety of patterns of motor commands that yield the same desired sensation of seeing of the ball pass through the goal .
the learner is assumed to be able to observe states , actions , and sensations and can therefore model the mapping between actions and sensations .
a forward model is an internal model that produces a predicted sensation ^yn based on the state xn ( cid : 123 ) and the action un ( cid : 123 ) .
that is , a forward model predicts the consequences of a
figure : learning the forward model using the prediction error yn ( cid : 123 ) ^yn .
given action in the context of a given state vector .
as shown in figure , the forward model can be learned by comparing predicted sensations to actual sensations and using the resulting prediction error to adjust the parameters of the model .
learning the forward model is a classical supervised learning problem in which the teacher provides target values directly in the output coordinate system of the learner .
distal supervised learning
we now describe a general approach to solving the distal supervised learning prob - lem .
consider the system shown in figure , in which the learner is placed in series with a forward model of the environment .
this composite learning system is a state - dependent mapping from inputs to predicted sensations .
suppose that the forward model has been trained previously and is a perfect model of the environment; that is , the predicted sensation equals the actual sensation for all actions and all states .
in the engineering literature , this learning process is referred to as system identication "
ljung & soderstrom , .
figure : the composite learning system .
this composite system maps from inputs pn ( cid : 123 ) to predicted sensations ^yn in the context of a given state vector .
we now treat the composite learning system as a single supervised learning system and train it to map from inputs to desired sensations according to the data in the training set .
that is , the desired sensations y i are treated as targets for the compos - ite system .
any supervised learning algorithm can be used for this training process; however , the algorithm must be constrained so that it does not alter the forward model while the composite system is being trained .
by xing the forward model , we require the system to nd an optimal composite mapping by varying only the mapping from inputs to actions .
if the forward model is perfect , and if the learning algorithm nds the globally optimal solution , then the resulting state - dependent input - to - action mapping must also be perfect in the sense that it yields the desired composite inputoutput behavior when placed in series with the environment .
consider now the case of an imperfect forward model .
clearly an imperfect forward model will yield an imperfect input - to - action map if the composite system is trained in the obvious way , using the dierence between the desired sensation and the predicted sensation as the error term .
this dierence , the predicted performance error y ( cid : 123 ) ^y , is readily available at the output of the composite system , but it is an unreliable guide to the true performance of the learner .
suppose instead that we ignore the output of the composite system and substitute the performance error y ( cid : 123 ) y as the error term for training the composite system see figure .
if the performance error goes to zero the system has found a correct input - to - action map , regardless of the inaccuracy of the forward model .
the inaccuracy in the forward model manifests itself as a bias during the learning process , but need not prevent the performance error from going to zero .
consider , for example , algorithms based on steepest descent .
if the forward model is not too inaccurate the system can still move downhill and thereby reach the solution region , even though the movement is not in the direction of steepest descent .
to summarize , we propose to solve the distal supervised learning problem by training a composite learning system consisting of the learner and a forward model of the environment .
this procedure solves implicitly for an input - to - action map by
y * ( n ) _
figure : the composite system is trained using the performance error .
the forward model is held xed while the composite system is being trained .
training the composite system to map from inputs to distal targets .
the training of the forward model must precede the training of the composite system , but the forward model need not be perfect , nor need it be pre - trained throughout all of state space .
the ability of the system to utilize an inaccurate forward model is important; it implies that it may be possible to interleave the training of the forward model and the composite system .
in the remainder of the paper , we discuss the issues of interleaved training , inaccuracy in the forward model , and the choice of the error term in more detail .
we rst turn to an interesting special case of the general distal supervised learning problem|that of learning an inverse model of the environment .
an inverse model is an internal model that produces an action un ( cid : 123 ) as a function of the current state xn ( cid : 123 ) and the desired sensation yn .
inverse models are dened by the condition that they yield the identity mapping when placed in series with the environment .
inverse models are important in a variety of domains .
for example , if the en - vironment is viewed as a communications channel over which a message is to be transmitted , then it may be desirable to undo the distorting eects of the envi - ronment by placing it in series with an inverse model carlson , .
a second example , shown in figure , arises in control system design .
a controller receives the desired sensation yn as input and must nd actions that cause actual sensations to be as close as possible to desired sensations; that is , the controller must invert
figure : an inverse model as a controller .
the transformation from actions to sensations .
one approach to achieving this objective is to utilize an explicit inverse model of the environment as a controller .
whereas forward models are uniquely determined by the environment , inverse models are generally not .
if the environment is characterized by a many - to - one mapping from actions to sensations then there are generally an innite number of possible inverse models .
it is also worth noting that inverses do not always exist| it is not always possible to achieve a particular desired sensation from any given state .
as we shall discuss , these issues of existence and uniqueness have important implications for the problem of learning an inverse model .
there are two general approaches to learning inverse models using supervised learning algorithms : the distal learning approach presented above and an alternative approach that we refer to as direct inverse modeling " cf .
jordan & rosenbaum , .
we begin by describing the latter approach .
direct inverse modeling
direct inverse modeling treats the problem of learning an inverse model as a classical supervised learning problem widrow & stearns , .
as shown in figure , the idea is to observe the inputoutput behavior of the environment and to train an inverse model directly by reversing the roles of the inputs and outputs .
data are provided to the algorithm by sampling in action space and observing the results in
although direct inverse modeling has been shown to be a viable technique in a number of domains atkeson & reinkensmeyer , ; kuperstein , ; miller , , it has two drawbacks that limit its usefulness .
first , if the environment is characterized by a many - to - one mapping from actions to sensations , then the di - rect inverse modeling technique may be unable to nd an inverse .
the diculty is that nonlinear many - to - one mappings can yield nonconvex inverse images , which are
control system design normally involves a number of additional constraints involving stability and robustness; thus , the goal is generally to invert the environment as nearly as possible subject to these additional constraints .
figure : the direct inverse modeling approach to learning an inverse model .
problematic for direct inverse modeling .
consider the situation shown in figure .
the nonconvex region on the left is the inverse image of a point in sensation space .
suppose that the points labelled by xs are sampled during the learning process .
three of these points correspond to the same sensation; thus , the training data as seen by the direct inverse modeling procedure are one - to - many|one input is paired with many targets .
supervised learning algorithms resolve one - to - many inconsisten - cies by averaging across the multiple targets the form of the averaging depends on the particular cost function that is used .
as is shown in the gure , however , the average of points lying in a nonconvex set does not necessarily lie in the set .
thus the globally optimal minimum - cost solution found by the direct inverse modeling approach is not necessarily a correct inverse model .
we present an example of such behavior in a following section .
the second drawback with direct inverse modeling is that it is not goal - directed . " the algorithm samples in action space without regard to particular targets or errors in sensation space .
that is , there is no direct way to nd an action that corresponds to a particular desired sensation .
to obtain particular so - lutions the learner must sample over a suciently wide range of actions and rely on
finally , it is also important to emphasize that direct inverse modeling is re - stricted to the learning of inverse models|it is not applicable to the general distal
a set is convex if for every pair of points in the set all points on the line between the points
also lie in the set .
figure : the convexity problem .
the region on the left is the inverse image of the point on the right .
the arrow represents the direction in which the mapping is learned by direct inverse modeling .
the three points lying inside the inverse image are averaged by the learning procedure , yielding the vector represented by the small circle .
this point is not a solution , because the inverse image is not convex .
supervised learning problem .
the distal learning approach to learning an inverse model
the methods described earlier in this section are directly applicable to the problem of learning an inverse model .
the problem of learning an inverse model can be treated as a special case of the distal supervised learning problem in which the input vector and the desired sensation are the same that is , pn ( cid : 123 ) is equal to yn in equation .
thus , an inverse model is learned by placing the learner and the forward model in series and learning an identity mapping across the composite
a fundamental dierence between the distal learning approach and direct in - verse modeling approach is that rather than averaging over regions in action space , the distal learning approach nds particular solutions in action space .
the globally optimal solution for distal learning is a set of vectors fuig such that the performance
an interesting analogy can be drawn between the distal learning approach and indirect tech - niques for solving systems of linear equations .
in numerical linear algebra , rather than solving explicitly for a generalized inverse of the coecient matrix , solutions are generally found indirectly e . g . , by applying gaussian elimination to both sides of the equation ga = i , where i is the identity
i ( cid : 123 ) yig are zero .
this is true irrespective of the shapes of the inverse im - i .
vectors lying outside of an inverse image , such as the average ages of the targets y vector shown in figure , do not yield zero performance error and are therefore not globally optimal .
thus nonconvex inverse images do not present the same funda - mental diculties for the distal learning framework as they do for direct inverse
it is also true that the distal learning approach is fundamentally goal - directed .
the system works to minimize the performance error; thus , it works directly to nd solutions that correspond to the particular goals at hand .
in cases in which the forward mapping is many - to - one , the distal learning pro - cedure nds a particular inverse model .
without additional information about the particular structure of the input - to - action mapping there is no way of predicting which of the possibly innite set of inverse models the procedure will nd .
as is discussed below , however , the procedure can also be constrained to nd particular inverse models with certain desired properties .
distal learning and backpropagation
in this section we describe an implementation of the distal learning approach that utilizes the machinery of the backpropagation algorithm .
it is important to empha - size at the outset , however , that backpropagation is not the only algorithm that can be used to implement the distal learning approach .
any supervised learning algo - rithm can be used as long as it is capable of learning a mapping across a composite network that includes a previously trained subnetwork; in particular , boltzmann learning is applicable jordan , .
we begin by introducing a useful shorthand for describing backpropagation in layered networks .
a layered network can be described as a parameterized mapping from an input vector x to an output vector y :
y = x; w;
where w is a vector of parameters weights .
in the classical paradigm , the procedure for changing the weights is based on the discrepancy between a target vector y and the actual output vector y .
the magnitude of this discrepancy is measured by a cost functional of the form :
y ( cid : 123 ) yt y ( cid : 123 ) y :
j is the sum of squared error at the output units of the network .
it is generally desired to minimize this cost .
backpropagation is an algorithm for computing gradients of the cost functional .
the details of the algorithm can be found elsewhere e . g . , rumelhart , et al . , ; our intention here is to develop a simple notation that hides the details .
this is
achieved formally by using the chain rule to dierentiate j with respect to the weight vector w :
rwj = ( cid : 123 )
y ( cid : 123 ) y :
this equation shows that any algorithm that computes the gradient of j eectively multiplies the error vector y ( cid : 123 ) y by the transpose jacobian matrix @y=@wt .
although the backpropagation algorithm never forms this matrix explicitly back - propagation is essentially a factorization of the matrix; jordan , , equation nonetheless describes the results of the computation performed by backpropagation .
backpropagation also computes the gradient of the cost functional with respect to the activations of the units in the network .
in particular , the cost functional j can be dierentiated with respect to the activations of the input units to yield :
rxj = ( cid : 123 )
y ( cid : 123 ) y :
we refer to equation as backpropagation - to - weights " and equation as back - propagation - to - activation . " both computations are carried out in one pass of the algorithm; indeed , backpropagation - to - activation is needed as an intermediate step in the backpropagation - to - weights computation .
in the remainder of this section we formulate two broad categories of learning problems that lie within the scope of the distal learning approach and derive ex - pressions for the gradients that arise .
for simplicity it is assumed in both of these derivations that the task is to learn an inverse model that is , the inputs and the distal targets are assumed to be identical .
the two formulations of the distal learn - ing framework focus on dierent aspects of the distal learning problem and have dierent strengths and weaknesses .
the rst approach , the local optimization " formulation , focuses on the local dynamical structure of the environment .
because it assumes that the learner is able to predict state transitions based on information that is available locally in time , it depends on prior knowledge of an adequate set of state variables for describing the environment .
it is most naturally applied to problems in which target values are provided at each moment in time , although it can be extended to problems in which target values are provided intermittently as we demonstrate in a following section .
all of the computations needed for the local
the jacobian matrix of a vector function is simply its rst derivative|it is a matrix of rst partial derivatives .
that is , the entries of the matrix @y=@w are the partial derivatives of the each of the output activations with respect to each of the weights in the network .
to gain some insight into why a transpose matrix arises in backpropagation , consider a single - layer linear network described by y = w x , where w is the weight matrix .
the rows of w are the incoming weight vectors for the output units of the network , and the columns of w are the outgoing weight vectors for the input units of the network .
passing a vector forward in the network involves taking the inner product of the vector with each of the incoming weight vectors .
this operation corresponds to multiplication by w .
passing a vector backward in the network corresponds to taking the inner product of the vector with each of the outgoing weight vectors .
this operation corresponds to multiplication by w t , because the rows of w t are the columns of w .
optimization formulation can be performed in feedforward networks , thus there is no problem with stability .
the second approach , the optimization - along - trajectories " formulation , focuses on global temporal dependencies along particular target tra - jectories .
the computation needed to obtain these dependencies is more complex than the computation needed for the local optimization formulation , but it is more exible .
it can be extended to cases in which a set of state variables is not known a priori and it is naturally applied to problems in which target values are provided in - termittently in time .
there is potentially a problem with stability , however , because the computations for obtaining the gradient involve a dynamical process .
the rst problem formulation that we discuss is a local optimization problem .
we assume that the process that generates target vectors is stationary and consider the following general cost functional :
efy ( cid : 123 ) yt y ( cid : 123 ) yg;
where y is an unknown function of the state x and the action u .
the action u is the output of a parameterized inverse model of the form :
u = hx; y
where w is a weight vector .
rather than optimizing j directly , by collecting statistics over the ensemble of states and actions , we utilize an online learning rule cf .
widrow & stearns , that makes incremental changes to the weights based on the instantaneous value of the cost functional :
yn ( cid : 123 ) ynt yn ( cid : 123 ) yn :
an online learning algorithm changes the weights at each time step based on the stochastic gradient of j; that is , the gradient of jn :
wn + = wn ( cid : 123 ) rwjn;
where is a step size .
to compute this gradient the chain rule is applied to equa -
rwjn = ( cid : 123 )
yn ( cid : 123 ) yn;
where the jacobian matrices @y=@u and @u=@w are evaluated at time n ( cid : 123 ) .
the rst and the third factors in this expression are easily computed : the rst factor describes the propagation of derivatives from the output units of the inverse model the action units " to the weights of the inverse model , and the third factor is the
distal error .
the origin of the second factor is problematic , however , because the dependence of y on u is assumed to be unknown a priori .
our approach to obtaining an estimate of this factor has two parts : first , the system acquires a parameterized forward model over an appropriate subdomain of the state space .
this model is of
^y = ^f x; u; v;
where v is a vector of weights and ^y is the predicted sensation .
second , the distal error is propagated backward through the forward model; this eectively multiplies the distal error by an estimate of the transpose jacobian matrix @y=@u .
putting these pieces together , the algorithm for learning the inverse model is
based on the following estimated stochastic gradient :
^rwjn = ( cid : 123 )
yn ( cid : 123 ) yn :
this expression describes the propagation of the distal error yn ( cid : 123 ) yn backward through the forward model and down into the inverse model where the weights are changed .
the network architecture in which these computations take place is shown in figure .
this network is a straightforward realization of the block diagram in figure .
it is composed of an inverse model , which links the state units and the input units to the action units , and a forward model , which links the state units and the action units to the predicted - sensation units .
learning the forward model
the learning of the forward model can itself be formulated as an optimization prob - lem , based on the following cost functional :
efy ( cid : 123 ) ^yt y ( cid : 123 ) ^yg;
where ^y is of the form given in equation .
although the choice of procedure for nding a set of weights v to minimize this cost is entirely independent of the choice of procedure for optimizing j in equation , it is convenient to base the learning of the forward model on a stochastic gradient as before :
rvln = ( cid : 123 )
yn ( cid : 123 ) ^yn;
where the jacobian matrix @ ^y=@v is evaluated at time n ( cid : 123 ) .
this gradient can be computed by the propagation of derivatives within the forward model and therefore requires no additional hardware beyond that already required for learning the inverse
note that the error term y
n ( cid : 123 ) yn is not a function of the output of the forward model; nonetheless , activation must ow forward in the model because the estimated jacobian matrix @ ^y=@u varies as a function of the activations of the hidden units and the output units of the
figure : a feedforward network that includes a forward model .
the action units are the output units of the system .
y ( cid : 123 ) y y ( cid : 123 ) ^y y ( cid : 123 ) ^y
predicted performance error
table : the error signals and their sources
the error signals
it is important to clarify the meanings of the error signals used in equations and .
as shown in table , there are three error signals that can be formed from the variables y , ^y , and y|the prediction error y ( cid : 123 ) ^y , the performance error y ( cid : 123 ) y , and the predicted performance error y ( cid : 123 ) ^y .
all three of these error signals are available to the learner because each of the signals y , y and ^y are available individually|the target y and the actual sensation y are provided by the environment , whereas the predicted sensation ^y is available internally .
for learning the forward model , the prediction error is clearly the appropriate error signal .
the learning of the inverse model , however , can be based on either the performance error or the predicted performance error .
using the performance error see equation has the advantage that the system can learn an exact inverse model even though the forward model is only approximate .
there are two reasons for this : rst , equation preserves the minima of the cost functional in equation |they are zeros of the estimated gradient .
that is , an inaccurate jacobian matrix cannot remove zeros of the estimated gradient points at which y ( cid : 123 ) y is zero , although it can introduce additional zeros spurious local minima .
second , if the estimated gradients obtained with the approximate forward model have positive inner product with the stochastic gradient in equation , then the expected step of the algorithm is downhill in the cost .
thus the algorithm can in principle nd an exact inverse model even though the forward model is only approximate .
there may also be advantages to using the predicted performance error .
particular , it may be easier in some situations to obtain learning trials using the internal model rather than the external environment rumelhart , smolensky , mc - clelland , & hinton , ; sutton , .
such internal trials can be thought of as a form of mental practice " in the case of backpropagation - to - weights or planning " in the case of backpropagation - to - activation .
these procedures lead to improved performance if the forward model is suciently accurate .
exact solutions cannot be found with such procedures , however , unless the forward model is exact .
in many cases the unknown mapping from actions to sensations can be decomposed into a series of simpler mappings , each of which can be modeled independently .
for example , it may often be preferable to model the next - state function and the output function separately rather than modeling them as a single composite function .
in such cases , the jacobian matrix @ ^y=@u can be factored using the chain rule to yield the following estimated stochastic gradient :
^rwjn = ( cid : 123 )
yn ( cid : 123 ) yn :
the estimated jacobian matrices in this expression are obtained by propagating derivatives backward through the corresponding forward models , each of which are
optimization along trajectories
a complete inverse model allows the learner to synthesize the actions that are needed to follow any desired trajectory .
in the local optimization formulation we eectively
this section is included for completeness and is not needed for the remainder of the paper .
assume that the learning of an inverse model is of primary concern and the learning of particular target trajectories is secondary .
the learning rule given by equation nds actions that invert the dynamics of the environment at the current point in state space , regardless of whether that point is on a desired trajectory or not .
in terms of network architectures , this approach leads to using feedforward networks to model the local forward and inverse state transition structure see figure .
in the current section we consider a more specialized problem formulation in which the focus is on particular classes of target trajectories .
this formulation is based on variational calculus and is closely allied with methods in optimal con - trol theory kirk , ; lecun , .
the algorithm that results is a form of backpropagation - in - time " rumelhart , hinton , & williams , in a recurrent network that incorporates a learned forward model .
the algorithm diers from the algorithm presented above in that it not only inverts the relationship between ac - tions and sensations at the current point in state space but also moves the current state toward the desired trajectory .
we consider an ensemble of target trajectories fy
ng and dene the following
n ( cid : 123 ) ynt y
n ( cid : 123 ) yng;
where is an index across target trajectories and y is an unknown function of the state x and the action u .
the action u is a parameterized function of the state x and the target y
u = hx; y
as in the previous formulation , we base the learning rule on the stochastic gradient of j , that is , the gradient evaluated along a particular sample trajectory y :
n ( cid : 123 ) ynt y
n ( cid : 123 ) yn :
the gradient of this cost functional can be obtained using the calculus of variations see also lecun , , narendra & parthasarathy , .
letting n represent the vector of partial derivatives of j with respect to xn , and letting n represent the vector of partial derivatives of j with respect to un , appendix a shows that the gradient of j is given by the following recurrence relations :
n ( cid : 123 ) yn
n ( cid : 123 ) =
figure : a recurrent network with a forward model .
the boxes labeled by ds are unit delay elements .
where the jacobian matrices are all evaluated at time step n and z stands for xn + thus , the jacobian matrices @z=@x and @z=@u are the deriva - tives of the next - state function .
this expression describes backpropagation - in - time in a recurrent network that incorporates a forward model of the next - state func - tion and the output function .
as shown in figure , the recurrent network is essentially the same as the network in figure , except that there are explicit con - nections with unit delay elements between the next - state and the current state .
backpropagation - in - time propagates derivatives backward through these recurrent connections as described by the recurrence relations in equations and .
as in the local optimization case , the equations for computing the gradient
alternatively , figure can be thought of as a special case of figure in which the backprop -
agated error signals stop at the state units cf .
jordan , .
involve the multiplication of the performance error y ( cid : 123 ) y by a series of transpose jacobian matrices , several of which are unknown a priori .
our approach to esti - mating the unknown factors is once again to learn forward models of the underlying mappings and to propagate signals backward through the models .
thus the jacobian matrices @z=@u , @z=@x , and @y=@x in equations , , and are all replaced by estimated quantities in computing the estimated stochastic gradient
in the following two sections , we pursue the presentation of the distal learning approach in the context of two problem domains .
the rst section describes learning in a static environment , whereas the second section describes learning in a dynamic environment .
in both sections , we utilize the local optimization formulation of distal
an environment is said to be static if the eect of any given action is independent of the history of previous actions .
in static environments the mapping from actions to sensations can be characterized without reference to a set of state variables .
such environments provide a simplied domain in which to study the learning of inverse mappings .
in this section , we present an illustrative static environment and focus on two issues : the eects of nonconvex inverse images in the transformation from sensations to actions and the problem of goal - directed learning .
the problem that we consider is that of learning the forward and inverse kine - matics of a three - joint planar arm .
as shown in figure and figure the con - guration of the arm is characterized by the three joint angles q; q; and q , and the corresponding pair of cartesian variables x and x .
the function that relates these variables is the forward kinematic function x = gq .
it is obtained in closed form using elementary trigonometry :
x = " lcosq + lcosq + q + lcosq + q + q
lsinq + lsinq + q + lsinq + q + q ;
where l; l; and l are the link lengths .
the forward kinematic function gq is a many - to - one mapping|for every cartesian position that is inside the boundary of the workspace , there are an in - nite number of joint angle congurations to achieve that position .
this implies that ( cid : 123 ) x is not a function; rather , there are an innite the inverse kinematic relation g number of inverse kinematic functions corresponding to particular choices of points q in the inverse images of each of the cartesian positions .
the problem of learning an inverse kinematic controller for the arm is that of nding a particular inverse among the many possible inverse mappings .
( x123 , x123 )
figure : a three - joint planar arm .
figure : the forward and inverse mappings associated with arm kinematics .
in the simulations reported below , the joint - angle congurations of the arm were represented using the vector cosq ( cid : 123 ) ; cosq; cosqt , rather than the vector of joint angles .
this eectively restricts the motion of the joints to the intervals
, ; , and ; , respectively , assuming that each component of the joint - angle conguration vector is allowed to range over the interval ( cid : 123 ) ; .
the cartesian variables x and x were represented as real numbers ranging over ( cid : 123 ) ; .
in all of the simulations , these variables were represented directly as real - valued activations of units in the network .
thus , three units were used to represent joint - angle cong - urations and two units were used to represent cartesian positions .
further details on the simulations are provided in appendix b .
the nonconvexity problem
one approach to learning an inverse mapping is to provide training pairs to the learner by observing the inputoutput behavior of the environment and reversing the role of the inputs and outputs .
this approach , which we referred to earlier as direct inverse modeling , " has been proposed in the domain of inverse kinematics by kuperstein .
kupersteins idea is to randomly sample points q in joint space and to use the real arm to evaluate the forward kinematic function x = gq , thereby obtaining training pairs x; q for learning the controller .
the controller is learned by optimization of the following cost functional :
efq ( cid : 123 ) qt q ( cid : 123 ) qg
where q = hx is the output of the controller .
as we discussed earlier , a diculty with the direct inverse modeling approach is that the optimization of the cost functional in equation does not necessarily yield an inverse kinematic function .
the problem arises because of the many - to - one nature of the forward kinematic function cf .
figure .
in particular , if two or more of the randomly sampled points q happen to map to the same endpoint , then the training data that is provided to the controller is one - to - many .
the particular manner in which the inconsistency is resolved depends on the form of the cost functional|use of the sum - of - squared error given in equation yields an arithmetic average over points that map to the same endpoint .
an average in joint space , however , does not necessarily yield a correct result in cartesian space , because the inverse images of nonlinear transformations are not necessarily convex .
this implies that the output of the controller may be in error even though the system has converged to the minimum of the cost functional .
in figure we demonstrate that the inverse kinematics of the three - joint arm is not convex .
to see if this nonconvexity has the expected eect on the direct inverse modeling procedure we conducted a simulation in which a feedforward network with one hidden layer was used to learn the inverse kinematics of the three - joint arm .
the simulation provided target vectors to the network by sampling randomly from a uniform distribution in joint space .
input vectors were obtained by mapping the target vectors into cartesian space according to equation .
the initial value of the root - mean - square rms joint - space error was : , ltered over the rst trials .
after ; learning trials the ltered error reached asymptote at a value of : .
a vector eld was then plotted by providing desired cartesian vectors as inputs to the network , obtaining the joint - angle outputs , and mapping these outputs into cartesian space using equation .
the resulting vector eld is shown in figure .
as can be seen , there is substantial error at many positions of the workspace , even though the learning algorithm has converged .
if training is continued , the loci of the errors continue to shift , but the rms error remains approximately constant .
al - though this error is partially due to the nite learning rate and the random sampling
figure : the nonconvexity of inverse kinematics .
the dotted conguration is an average in joint space of the two solid congurations .
procedure misadjustment , " see widrow & stearns , , the error remains above : even when the learning rate is taken to zero .
thus , misadjustment cannot ac - count for the error , which must be due to the nonconvexity of the inverse kinematic relation .
note , for example , that the error observed in figure is reproduced in the lower left portion of figure .
in figure , we demonstrate that the distal learning approach can nd a par - ticular inverse kinematic mapping .
we performed a simulation that was initialized with the incorrect controller obtained from direct inverse modeling .
the simula - tion utilized a forward model that had been trained previously the forward model was trained during the direct inverse modeling trials .
a grid of evenly spaced positions in cartesian space was used to provide targets during the second phase of the distal learning procedure .
on each trial the error in cartesian space was passed backward through the forward model and used to change the weights of the controller .
after ; such learning trials passes through the grid of tar - gets , the resulting vector eld was plotted .
as shown in the gure , the vector error decreases toward zero throughout the workspace; thus , the controller is converging toward a particular inverse kinematic function .
the use of a grid is not necessary; the procedure also works if cartesian positions are sampled
randomly on each trial .
figure : near - asymptotic performance of direct inverse modeling .
each vector represents the error at a particular position in the workspace .
a further virtue of the distal learning approach is the ease with which it is possible to incorporate additional constraints in the learning procedure and thereby bias the choice of a particular inverse function .
for example , a minimum - norm constraint can be realized by adding a penalty term of the form ( cid : 123 ) x to the propagated errors at the output of the controller .
temporal smoothness constraints can be realized by incorporating additional error terms of the form xn ( cid : 123 ) xn ( cid : 123 ) .
such constraints can be dened at other sites in the network as well , including the output units or hidden units of the forward model .
it is also possible to provide additional contextual inputs to the controller and thereby learn multiple , contextually - appropriate inverse functions .
these aspects of the distal learning approach are discussed in more detail in jordan , .
direct inverse modeling does not learn in a goal - directed manner .
to learn a specic cartesian target , the procedure must sample over a suciently large region of joint space and rely on interpolation .
heuristics may be available to restrict the search to certain regions of joint space , but such heuristics are essentially prior knowledge
figure : near - asymptotic performance of distal learning .
about the nature of the inverse mapping and can equally well be incorporated into the distal learning procedure .
distal learning is fundamentally goal - directed .
it is based on the performance error for a specic cartesian target and is capable of nding an exact solution for a particular target in a small number of trials .
this is demonstrated by the simulation shown in figure .
starting from the controller shown in figure , a particular cartesian target was presented for ten successive trials .
as shown in figure , the network reorganizes itself so that the error is small in the vicinity of the target .
after ten additional trials , the error at the target is zero within the oating - point resolution of the simulation .
approximate forward models
we conducted an additional simulation to study the eects of inaccuracy in the forward model .
the simulation varied the number of trials allocated to the learning of the forward model from to .
the controller was trained to an rms criterion of .
at the three target positions ( cid : 123 ) : ; : , : ; : , and : ; : .
as shown in figure , the results demonstrate that an accurate controller can be found with an inaccurate forward model .
fewer trials are needed to learn the target positions to criterion with the most accurate forward model; however , the dropo in learning rate with less accurate forward models is relatively slight .
reasonably
figure : goal - directed learning .
a cartesian target in the lower right portion of the gure was presented for ten successive trials .
the error vectors are close to zero in the vicinity of the target .
rapid learning is obtained even when the forward model is trained for only trials , even though the average rms error in the forward model is : m after trials , compared to : m after trials .
further comparisons with direct inverse modeling
in problems with many output variables it is often unrealistic to acquire an inverse model over the entire workspace .
in such cases the goal - directed nature of distal learning is particularly important because it allows the system to obtain inverse images for a restricted set of locations .
however , the forward model must also be learned over a restricted region of action space , and there is no general a priori method for determining the appropriate region of the space in which to sample .
that is , although distal learning is goal - directed in its acquisition of the inverse model , it is not inherently goal - directed in its acquisition of the forward model .
because neither direct inverse modeling nor distal
learning is entirely goal - directed , in any given problem it is important to consider whether it is more rea - sonable to acquire the inverse model or the forward model in a non - goal - directed
forward model training ( trials )
figure : number of trials required to train the controller to an rms criterion of .
as a function of the number of trials allocated to training the forward model .
each point is an average over three runs .
manner .
this issue is problem - dependent , depending on the nature of the function being learned , the nature of the class of functions that can be represented by the learner , and the nature of the learning algorithm .
it is worth noting , however , that there is an inherent tradeo in complexity between the inverse model and the for - ward model , due to the fact that their composition is the identity mapping .
this tradeo suggests a complementarity between the classes of problems for which direct inverse modeling and distal learning are appropriate .
we believe that distal learning is more generally useful , however , because an inaccurate forward model is generally acceptable whereas an inaccurate inverse model is not .
in many cases , it may be preferable to learn an inaccurate forward model that is specically inverted at a desired set of locations rather than learning an inaccurate inverse model directly and relying on interpolation .
dynamic environments : one - step dynamic models
to illustrate the application of distal learning to problems in which the environment has state , we consider the problem of learning to control a two - joint robot arm .
con - trolling a dynamic robot arm involves nding the appropriate torques to cause the arm to follow desired trajectories .
the problem is dicult because of the nonlinear couplings between the motions of the two links and because of the ctitious torques due to the rotating coordinate systems .
the arm that we consider is the two - link version of the arm shown previously in figure .
its conguration at each point in time is described by the joint angles qt and qt , and by the cartesian variables xt and xt .
the kinematic function xt = gqt that relates joint angles to cartesian variables can be obtained by letting l equal zero in equation :
xt = " lcosqt + lcosqt + qt
lsinqt + lsinqt + qt ;
where l and l are the link lengths .
the state space for the arm is the four - dimensional space of positions and velocities of the links .
the essence of robot arm dynamics is a mapping between the torques applied at the joints and the resulting angular accelerations of the links .
this mapping is _q , and q dependent on the state variables of angle and angular velocity .
let q , represent the vector of joint angles , angular velocities , and angular accelerations , respectively , and let represent the torques .
in the terminology of earlier sections , q and _q together constitute the state " and is the action . " for convenience , we take q to represent the next - state " see the discussion below .
to obtain an analog of the next - state function in equation , the following dierential equation can be derived for the angular motion of the links , using standard newtonian or lagrangian dynamical formulations craig , :
m qq + cq; _q _q + gq = ;
where m q is an inertia matrix , cq; _q is a matrix of coriolis and centripetal terms , and gq is the vector of torque due to gravity .
our interest is not in the physics behind these equations per se , but in the functional relationships that they dene .
in particular , to obtain a next - state function , " we rewrite equation by solving for the accelerations to yield :
( cid : 123 ) q ( cid : 123 ) cq; _q _q ( cid : 123 ) gq;
( cid : 123 ) q is always assured craig , .
equation ex - where the existence of m presses the state - dependent relationship between torques and accelerations at each moment in time : given the state variables qt and _qt , and given the torque t , the acceleration qt can be computed by substitution in equation .
we refer to this computation as the forward dynamics of the arm .
figure : the forward and inverse mappings associated with arm dynamics .
an inverse mapping between torques and accelerations can be obtained by in - terpreting equation in the proper manner .
given the state variables qt and _qt , and given the acceleration qt , substitution in equation yields the corre - sponding torques .
this algebraic computation is refered to as inverse dynamics .
it should be clear that inverse dynamics and forward dynamics are complemen - tary computations : substitution of from equation into equation yields the requisite identity mapping .
these relationships between torques , accelerations , and states are summarized in figure .
it is useful to compare this gure with the kinematic example shown in figure .
in both the kinematic case and the dynamic case , the forward and inverse mappings that must be learned are xed functions of the instantaneous values of the relevant variables .
in the dynamic case , this is due to the fact that the structural terms of the dynamical equations the terms m , c , and g are explicit functions of state rather than time .
the dynamic case can be thought of as a generalization of the kinematic case in which additional contextual state variables are needed to index the mappings that must be learned .
figure is an instantiation of figure , with the acceleration playing the role of the next - state . " in general , for systems described by dierential equations , it is convenient to dene the notion of next - state " in terms of the time derivative of one or more of the state variables e . g . , accelerations in the case of arm dynamics .
this denition is entirely consistent with the development in preceding sections; indeed , if the dierential equations in equation are simulated in discrete time on a computer , then the numerical algorithm must compute the accelerations dened by equation to convert the positions and velocities at the current time step into the positions and velocities at the next time step .
this perspective is essentially that underlying the local optimization formulation of distal
because of the amplication of noise in dierentiated signals , however , most realistic implemen - tations of forward dynamical models would utilize positions and velocities rather than accelerations .
in such cases the numerical integration of equation would be incorporated as part of the forward
learning a dynamic forward model
a forward model of arm dynamics is a network that learns a prediction ^q of the acceleration q , given the position q , the velocity _q , and the torque .
the appro - priate teaching signal for such a network is the actual acceleration q , yielding the following cost functional :
efq ( cid : 123 ) ^qt q ( cid : 123 ) ^qg :
the prediction ^q is a function of the position , the velocity , the torque and the
^q = ^f q; _q; ; w :
for an appropriate ensemble of control trajectories , this cost functional is minimized when a set of weights is found such that ^f ; w best approximates the forward dynamical function given by equation .
an important dierence between kinematic problems and dynamic problems is that it is generally infeasible to produce arbitrary random control signals in dy - namical environments , because of considerations of stability .
for example , if t in equation is allowed to be a stationary white - noise stochastic process , then the variance of qt approaches innity much like a random walk .
this yields data that is of little use for learning a model .
we have used two closely related approaches to overcome this problem .
the rst approach is to produce random equilibrium positions for the arm rather than random torques .
that is , we dene a new control signal ut such that the augmented arm dynamics are given by :
m qq + cq; _q _q + gq = kv _q ( cid : 123 ) _u + kpq ( cid : 123 ) u;
for xed constants kp and kv .
the random control signal u in this equation acts as a virtual " equilibrium position for the arm hogan , and the augmented dynamics can be used to generate training data for learning the forward model .
the second approach also utilizes equation and diers from the rst approach only in the choice of the control signal ut .
rather than using random controls , the target trajectories themselves are used as controls that is , the trajectories utilized in the second phase of learning are also used to train the forward model .
this approach is equivalent to using a simple xed - gain proportional - derivative pd feedback controller to stabilize the system along a set of reference trajectories and thereby generate training data .
such use of an auxiliary feedback controller is similar to its use in the feedback - error learning kawato , et al . , and direct inverse modeling atkeson & reinkensmeyer , ; miller , approaches .
as is discussed below , the second approach has the advantage that it does not require the forward model to be learned in a separate phase .
a pd controller is a device whose output is a weighted sum of position errors and velocity errors .
the position errors and the velocity errors are multiplied by xed numbers gains before
figure : the composite control system .
composite control system
the composite system for controlling the arm is shown in figure .
the control signal in this diagram is the torque , which is the sum of two components :
= f f + f b;
where f f is a feedforward torque and f b is the optional feedback torque produced by the auxiliary feedback controller .
the feedforward controller is the learning con - troller that converges toward a model of the inverse dynamics of the arm .
in the early phases of learning , the feedforward controller produces small random torques , thus the major source of control is provided by the error - correcting feedback controller .
when the feedforward controller begins to be learned it produces torques that al - low the system to follow desired trajectories with smaller error , thus the role of the feedback controller is diminished .
indeed , in the limit where the feedforward controller converges to a perfect inverse model , the feedforward torque causes the system to follow a desired trajectory without error and the feedback controller is
as is discussed below , this statement is not entirely accurate .
the learning algorithm itself
provides a form of error - correcting feedback control .
therefore silent assuming no disturbances .
thus the system shifts automatically from feedback - dominated control to feedforward - dominated control over the course of learning see also atkeson & reinkensmeyer , ; kawato , et al . , ; miller ,
there are two error signals utilized in learning inverse dynamics : the prediction error q ( cid : 123 ) ^q and the performance error q ( cid : 123 ) q .
the prediction error is used to train the forward model as discussed in the previous section .
once the forward model is at least partially learned , the performance error can be used in training the inverse model .
the error is propagated backward through the forward model and down into the feedforward controller where the weights are changed .
this process minimizes the distal cost functional :
efq ( cid : 123 ) qt q ( cid : 123 ) qg :
the arm was modeled using rigid body dynamics assuming the mass to be uniformly distributed along the links .
the links were modeled as thin cylinders .
details on the physical constants are provided in appendix c .
the simulation of the forward dynamics of the arm was carried out using a fourth - order runge - kutta algorithm with a sampling frequency of hz .
the control signals provided by the networks were sampled at hz .
standard feedforward connectionist networks were used in all of the simula - tions .
there were two feedforward networks in each simulation|a controller and a forward model|with overall connectivity as shown in figure with the box labelled arm " being replaced by a forward model .
both the controller and the forward model were feedforward networks with a single layer of logistic hidden units .
in all of the simulations , the state variables , torques , and accelerations were repre - sented directly as real - valued activations in the network .
details of the networks used in the simulations are provided in appendix b .
in all but the nal simulation reported below , the learning of the forward model and the learning of an inverse model were carried out in separate phases .
the forward model was learned in an initial phase by using a random process to drive the augmented dynamics given in equation .
the random process was a white noise position signal chosen uniformly within the workspace shown in figure .
the learning of the forward model was terminated when the ltered rms prediction error reached : rad=s .
as noted above , it is also possible to include the numerical integration of ^q as part of the forward model and learn a mapping whose output is the predicted next - state ^qn; ^_qn .
this approach may be preferred for systems in which dierentiation of noisy signals is a concern .
figure : the workspace the grey region and four target paths .
the trajectories move from left to right along the paths shown .
figure : performance on one of the four learned trajectories .
a before learning .
b after learning trials .
learning with an auxiliary feedback controller
after learning the forward model , the system learned to control the arm along the four paths shown in figure .
the target trajectories were minimum jerk trajectories of one second duration each .
an auxiliary proportional - derivative pd feedback controller was used , with position gains of : n m=rad and velocity gains of : n m s=rad .
figure shows the performance on a particular trajectory before learning with the pd controller alone and during the th learning trial .
the corresponding waveforms are shown in figure and figure .
the middle graphs in these gures show the feedback torques dashed lines and the feedforward torques solid lines .
as can be seen , in the early phases of learning the torques are
generated principally by the feedback controller and in later phases the torques are generated principally by the feedforward controller .
learning without an auxiliary feedback controller
an interesting consequence of the goal - directed nature of the forward modeling approach is that it is possible to learn an inverse dynamic model without using an auxiliary feedback controller .
to see why this is the case , rst note that minimum jerk reference trajectories and other smooth " reference trajectories change slowly in time .
this implies that successive time steps are essentially repeated learning trials on the same input vector; thus , the controller converges rapidly to a solution " for a local region of state space .
as the trajectory evolves , the solution tracks the input; thus , the controller produces reasonably good torques prior to any learning . " put another way , the distal learning approach is itself a form of error - correcting feedback control in the parameter space of the controller .
such error correction must eventually give way to convergence of the weights if the system is to learn an inverse model; nonetheless , it is a useful feature of the algorithm that it tends to stabilize the arm during learning .
this behavior is demonstrated by the simulations shown in figure .
the gure shows performance on the rst learning trial as a function of the learning rate .
the results demonstrate that changing the learning rate essentially changes the gain of the error - correcting behavior of the algorithm .
when the learning rate is set to . , the system produces nearly perfect performance on the rst learning trial .
this feature of the algorithm makes it important to clarify the meaning of the learning curves obtained with the distal learning approach .
figure shows two such learning curves .
the lower curve is the rms error that is obtained with a learning rate of . .
the upper curve is the rms error that is obtained when the learning rate is temporarily set to zero after each learning trial .
setting the learning rate to zero allows the eects of learning to be evaluated separately from the error - correcting behavior .
the curves clearly reveal that on the early trials the main contributor to performance is error correction rather than learning .
combining forward dynamics and forward kinematics
combining the forward dynamic models of this section with the forward kinematic models of the preceding section makes it possible to train the controller using carte - sian target trajectories .
given that the dynamic model and the kinematic model can be learned in parallel , there is essentially no performance decrement associated with using the combined system .
in our simulations , we nd that learning times increase by approximately eight percent when using cartesian targets rather than joint angle targets .
mu = 123
mu = 123
mu = 123
mu = 123
mu = 123
mu = 123
figure : performance on the rst learning trial as a function of the learning rate .
mu = 123 mu = 123
figure : rms error for zero and non - zero learning rates .
learning the forward model and the controller simultaneously
the distal learning approach involves using a forward model to train the controller; thus , learning of the forward model must precede the learning of the controller .
it is not necessary , however , to learn the forward model over the entire state space before learning the controller|a local forward model is generally sucient .
moreover , as we have discussed , the distal learning approach does not require an exact forward model|approximate forward models often suce .
these two facts , in conjunction with the use of smooth reference trajectories , imply that it should be possible to learn the forward model and the controller simultaneously .
an auxiliary feedback controller is needed to stabilize the system initially; however , once the forward model begins to be learned , the learning algorithm itself tends to stabilize the system .
moreover , as the controller begins to be learned , the errors decrease and the eects of the feedback controller diminish automatically .
thus the system bootstraps itself toward an inverse model .
the simulation shown in figure demonstrates the feasibility of this approach .
figure : learning the forward model and the controller simultaneously .
a per - formance before learning on two of the target trajectories .
b performance after
using the same architecture as in previous experiments the system learned four target trajectories starting with small random weights in both the controller and the forward model .
on each time step two passes of the backpropagation algorithm were required|one pass with the prediction error q ( cid : 123 ) ^q to change the weights of the forward model , and a second pass with the performance error q ( cid : 123 ) q to change the weights of the controller .
an auxiliary proportional - derivative pd feedback controller was used , with position gains of : n m=rad and velocity gains of : n m s=rad .
as shown in the gure , the system converges to an acceptable level of performance after learning trials .
although the simultaneous learning procedure requires more presentations of the target trajectories to achieve a level of performance comparable to that of the two - phase learning procedure , the simultaneous procedure is in fact more ecient than two - phase learning because it dispenses with the initial phase of learning the forward model .
this advantage must be weighed against certain disadvantages; in particular , the possibility of instability is enhanced because of the error in the gradients obtained from the partially - learned forward model .
in practice we nd that it is often necessary to use smaller step sizes in the simultaneous learning approach than in the two - phase learning approach .
preliminary experiments have also shown that is worthwhile to choose specialized representations that enhance the speed with which the forward model converges .
this can be done separately for the state variable input and the torque input .
dynamic environments : simplied models
in the previous section we demonstrated how the temporal component of the dis - tal supervised learning problem can be addressed by knowledge of a set of state variables for the environment .
assuming prior knowledge of a set of state variables is tantamount to assuming that the learner has prior knowledge of the maximum delay between the time at which an action is issued and the time at which an eect is observed in the sensation vector .
in the current section we present preliminary results that aim to broaden the scope of the distal learning approach to address problems in which the maximum delay is not known see also werbos , .
a simple example of such a problem is one in which a robot arm is required to be in a certain conguration at time t , where t is unknown , and where the trajectory in the open interval from to t is unconstrained .
one approach to solving such problems is to learn a one - step forward model of the arm dynamics and then to use backpropagation - in - time in a recurrent network that includes the forward model and a controller jordan , ; kawato , .
in many problems involving delayed temporal consequences , however , it is neither feasible nor desirable to learn a dynamic forward model of the environment , either because the environment is too complex or because solving the task at hand does not require knowledge of the evolution of all of the state variables .
consider for example the problem of predicting the height of a splash of water when stones of varying size are dropped into a pond .
it is unlikely that a useful one - step dynamic model could be learned for the uid dynamics of the pond .
moreover , if the control problem is to produce splashes of particular desired heights , it may not be necessary to model uid dynamics in detail .
a simple forward model that predicts an integrated quantity|splash height as a function of the size of the stone|may suce .
jordan and jacobs illustrated this approach by using distal learning to solve the problem of learning to balance an inverted pendulum on a moving cart .
this problem is generally posed as an avoidance control problem in which the only corrective information provided by the environment is a signal to indicate that failure has occured barto , sutton , & anderson , .
the delay between actions forces applied to the cart and the failure signal is unknown and indeed can be arbitrarily large .
in the spirit of the foregoing discussion , jordan and jacobs also assumed that it is undesirable to model the dynamics of the cart - pole system; thus , the controller cannot be learned by using backpropagation - in - time in a recurrent network that includes a one - step dynamic model of the plant .
a unique trajectory may be specied by enforcing additional constraints on the temporal evo - lution of the actions; however , the only explicit target information is assumed to be that provided at the nal time step .
in kawatos work , backpropagation - in - time is implemented in a spatially - unrolled network and the gradients are used to change activations rather than weights; however , the idea of using a one - step forward dynamic model is the same .
see also nguyen & widrow for an application to a kinematic problem .
the approach adopted by jordan and jacobs involves learning a forward model whose output is an integrated quantity|an estimate of the inverse of the time until failure .
this estimate is learned using temporal dierence techniques sutton , .
at time steps on which failure occurs , the target value for the forward model is unity :
et = ( cid : 123 ) ^zt;
where ^zt is the output of the forward model , and et is the error term used to change the weights .
on all other time steps , the following temporal dierence error term is used :
+ ^z ( cid : 123 ) t +
which yields an increasing arithmetic series along any trajectory that leads to failure .
once learned , the output of the forward model is used to provide a gradient for learning the controller .
in particular , because the desired outcome of balancing the pole can be described as the goal of maximizing the time until failure , the algorithm learns the controller by using zero minus the output of the forward model as the distal error signal .
the forward model used by jordan and jacobs diers in an important way from the other forward models described in this paper .
because the time - until - failure depends on future actions of the controller , the mapping that the forward model must learn depends not only on xed properties of the environment but also on the controller .
when the controller is changed by the learning algorithm , the mapping that the forward model must learn also changes .
thus the forward model must be updated continuously during the learning of the controller .
in general , for problems in which the forward model learns to estimate an integral of the closed - loop dynamics , the learning of the forward model and the controller must proceed
temporal dierence techniques provide the distal learning approach with en - hanced functionality .
they make it possible to learn to make long - term predictions and thereby adjust controllers on the basis on quantities that are distal in time .
they can also be used to learn multi - step forward models .
in conjunction with backpropagation - in - time , they provide a exible set of techniques for learning ac - tions on the basis of temporally - extended consequences .
in this paper we have argued that the supervised learning paradigm is broader than is commonly assumed .
the distal supervised learning framework extends super - vised learning to problems in which desired values are available only for the distal
this technique can be considered as an example of using supervised learning algorithms to solve
a reinforcement learning problem see below .
consequences of a learners actions and not for the actions themselves .
this is a sig - nicant weakening of the classical notion of the teacher " in the supervised learning in this section we provide further discussion of the class of problems that can be treated within the distal supervised learning framework .
we discuss possible sources of training data and we contrast distal supervised learning with
how is training data obtained ?
to provide support for our argument that distal supervised learning is more realistic than classical supervised learning it is necessary to consider possible sources of training data for distal supervised learning .
we discuss two such sources , which we refer to as imitation and envisioning .
one of the most common ways for humans to acquire skills is through imitation .
skills such as dance or athletics are often learned by observing another person performing the skill and attempting to replicate their behavior .
although in some cases a teacher may be available to suggest particular patterns of limb motion , such direct instruction does not appear to be a necessary component of skill acquisition .
a case in point is speech acquisition|children acquire speech by hearing speech sounds , not by receiving instruction on how to move their articulators .
our conception of a distal supervised learning problem involves a set of inten - tion , desired outcome training pairs .
learning by imitation clearly makes desired outcomes available to the learner .
with regard to intentions , there are three possi - bilities .
first , the learner may know or be able to infer the intentions of the person serving as a model .
alternatively , an idiosyncratic internal encoding of intentions is viable as long as the encoding is consistent .
for example , a child acquiring speech may have an intention to drink , may observe another person obtaining water by uttering the form water , " and may utilize the acoustic representation of water " as a distal target for learning the articulatory movements for expressing a desire to drink , even though the other person uses the water to douse a re .
finally , when the learner is acquiring an inverse model , as in the simulations reported in this paper , the intention is obviously available because it is the same as desired outcome .
our conception of distal supervised learning problem as a set of training pairs is of course an abstraction that must be elaborated when dealing with complex tasks .
in a complex task such as dance , it is presumably not easy to determine the choice of sensory data to be used as distal targets for the learning procedure .
indeed , the learner may alter the choice of targets once he or she has achieved a modicum of skill .
the learner may also need to decompose the task into simpler tasks and to set intermediate goals .
we suspect that the role of external teachers " is to help with these representational issues rather than to provide proximal targets directly to the
another source of data for the distal supervised learning paradigm is a process that we refer to as envisioning . " envisioning is a general process of converting
abstract goals into their corresponding sensory realization , without regard to the actions needed to achieve the goals .
envisioning involves deciding what it would look like " or feel like " to perform some task .
this process presumably involves general deductive and inductive reasoning abilities as well as experience with similar tasks .
the point that we want to emphasize is that envisioning need not refer to the actions that are needed to actually carry out a task; that is the problem solved by the distal learning procedure .
comparisons with reinforcement learning
an alternative approach to solving the class of problems that we have discussed in this paper is to use reinforcement learning algorithms barto , ; sutton , .
reinforcement learning algorithms are based on the assumption that the environ - ment provides an evaluation of the actions produced by the learner .
because the evaluation can be an arbitrary function , the approach is in principle applicable to the general problem of learning on the basis of distal signals .
reinforcement learning algorithms work by updating the probabilities of emit - ting particular actions .
the updating procedure is based on the evaluations received from the environment .
if the evaluation of an action is favorable then the probabil - ity associated with that action is increased and the probabilities associated with all other actions are decreased .
conversely , if the evaluation is unfavorable , then the probability of the given action is decreased and the probabilities associated with all other actions are increased .
these characteristic features of reinforcement learning algorithms dier in important ways from the corresponding features of supervised learning algorithms .
supervised learning algorithms are based on the existence of a signed error vector rather than an evaluation .
the signed error vector is generally , although not always , obtained by comparing the actual output vector to a target vector .
if the signed error vector is small , corresponding to a favorable evaluation , the algorithm initiates no changes .
if the signed error vector is large , corresponding to an unfavorable evaluation , the algorithm corrects the current action in favor of a particular alternative action .
supervised learning algorithms do not simply increase the probabilities of all alternative actions; rather , they choose particular alternatives based on the directionality of the signed error vector .
it is important to distinguish between learning paradigms and learning algo - rithms .
because the same learning algorithm can often be utilized in a variety of learning paradigms , a failure to distinguish between paradigms and algorithms can lead to misunderstanding .
this is particularly true of reinforcement learning tasks and supervised learning tasks because of the close relationships between evaluative signals and signed error vectors .
a signed error vector can always be converted into an evaluative signal any bounded monotonic function of the norm of the signed
as pointed out by barto , sutton & anderson , this distinction between reinforcement learning and supervised learning is signicant only if the learner has a repertoire of more than two
error vector suces; thus , reinforcement learning algorithms can always be used for supervised learning problems .
conversely , an evaluative signal can always be converted into a signed error vector using the machinery that we have discussed; see also munro , ; thus , supervised learning algorithms can always be used for reinforcement learning problems .
the denition of a learning paradigm , however , has more to do with the manner in which a problem is naturally posed than with the algorithm used to solve the problem .
in the case of the basketball player , for example , assuming that the environment provides directional information such as too far to the left , " too long , " or too short , " is very dierent from assuming that the environment provides evaluative information of the form good , " better , " or best " .
furthermore , learning algorithms dier in algorithmic complexity when applied across paradigms : using a reinforcement learning algorithm to solve a su - pervised learning problem is likely to be inecient because such algorithms do not take advantage of directional information .
conversely , using supervised learning al - gorithms to solve reinforcement learning problems is likely to be inecient because of the extra machinery that is required to induce a signed error vector .
in summary , although it has been suggested that the dierence between rein - forcement learning and supervised learning is the latters reliance on a teacher , " we feel that this argument is mistaken .
the distinction between the supervised learning paradigm and the reinforcement learning paradigm lies in the interpretation of envi - ronmental feedback as an error signal or as an evaluative signal , not the coordinate system in which such signals are provided .
many problems involving distal credit assignment may be better conceived of as supervised learning problems rather than reinforcement learning problems if the distal feedback signal can be interpreted as a performance error .
there are a number of diculties with the classical distinctions between unsuper - vised , " reinforcement , " and supervised " learning .
supervised learning is generally said to be dependent on a teacher " to provide target values for the output units of a network .
this is viewed as a limitation because in many domains there is no such teacher .
nevertheless , the environment often does provide sensory information about the consequences of an action which can be employed in making internal mod - ications just as if a teacher had provided the information to the learner directly .
the idea is that the learner rst acquires an internal model that allows prediction of the consequences of actions .
the internal model can be used as a mechanism for transforming distal sensory information about the consequences of actions into proximal information for making internal modications .
this two - phase procedure extends the scope of the supervised learning paradigm to include a broad range of problems in which actions are transformed by an unknown dynamical process before being compared to desired outcomes .
we rst illustrated this approach in the case of learning an inverse model of a simple static " environment .
we showed that our method of utilizing a forward model of the environment has a number of important advantages over the alternative method of building the inverse model directly .
these advantages are especially apparent in cases where there is no unique inverse model .
we also showed that this idea can be extended usefully to the case of a dynamic environment .
in this case , we simply elaborate both the forward model and the learner i . e . , controller so they take into account the current state of the environment .
finally , we showed how this approach can be combined with temporal dierence techniques to build a system capable of learning from sensory feedback that is subject to an unknown delay .
we also suggested that comparative work in the study of learning can be facili - tated by making a distinction between learning algorithms and learning paradigms .
a variety of learning algorithms can often be applied to a particular instance of a learning paradigm; thus , it is important to characterize not only the paradigmatic aspects of any given learning problem , such as the nature of the interaction between the learner and the environment and the nature of the quantities to be optimized , but also the tradeos in algorithmic complexity that arise when dierent classes of learning algorithms are applied to the problem .
further research is needed to delineate the natural classes at the levels of paradigms and algorithms and to clarify the relationships between levels .
we believe that such research will begin to pro - vide a theoretical basis for making distinctions between candidate hypotheses in the empirical study of human learning .
