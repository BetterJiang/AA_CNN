the formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables , and building large - scale multivariate statistical models .
graphical models have become a focus of research in many statisti - cal , computational and mathematical elds , including bioinformatics , communication theory , statistical physics , combinatorial optimiza - tion , signal and image processing , information retrieval and statistical machine learning .
many problems that arise in specic instances including the key problems of computing marginals and modes of probability distributions are best studied in the general setting .
working with exponential family representations , and exploiting the conjugate duality between the cumulant function and the entropy for exponential families , we develop general variational representa - tions of the problems of computing likelihoods , marginal probabili - ties and most probable congurations .
we describe how a wide variety
of algorithms among them sum - product , cluster variational meth - ods , expectation - propagation , mean eld methods , max - product and linear programming relaxation , as well as conic programming relax - ations can all be understood in terms of exact or approximate forms of these variational representations .
the variational approach provides a complementary alternative to markov chain monte carlo as a general source of approximation methods for inference in large - scale statistical
graphical models bring together graph theory and probability theory in a powerful formalism for multivariate statistical modeling .
in vari - ous applied elds including bioinformatics , speech processing , image processing and control theory , statistical models have long been for - mulated in terms of graphs , and algorithms for computing basic statis - tical quantities such as likelihoods and score functions have often been expressed in terms of recursions operating on these graphs; examples include phylogenies , pedigrees , hidden markov models , markov random elds , and kalman lters .
these ideas can be understood , unied , and generalized within the formalism of graphical models .
indeed , graphi - cal models provide a natural tool for formulating variations on these classical architectures , as well as for exploring entirely new families of statistical models .
accordingly , in elds that involve the study of large numbers of interacting variables , graphical models are increasingly in
graph theory plays an important role in many computationally ori - ented elds , including combinatorial optimization , statistical physics , and economics .
beyond its use as a language for formulating models , graph theory also plays a fundamental role in assessing computational
complexity and feasibility .
in particular , the running time of an algo - rithm or the magnitude of an error bound can often be characterized in terms of structural properties of a graph .
this statement is also true in the context of graphical models .
indeed , as we discuss , the com - putational complexity of a fundamental method known as the junction tree algorithm which generalizes many of the recursive algorithms on graphs cited above can be characterized in terms of a natural graph - theoretic measure of interaction among variables .
for suitably sparse graphs , the junction tree algorithm provides a systematic solution to the problem of computing likelihoods and other statistical quantities associated with a graphical model .
unfortunately , many graphical models of practical interest are not suitably sparse , so that the junction tree algorithm no longer provides a viable computational framework .
one popular source of methods for attempting to cope with such cases is the markov chain monte carlo ( mcmc ) framework , and indeed there is a signicant literature on the application of mcmc methods to graphical models ( e . g . , 123 , 123 , 123 ) .
our focus in this survey is rather dierent : we present an alternative computational methodology for statistical inference that is based on variational methods .
these techniques provide a general class of alter - natives to mcmc , and have applications outside of the graphical model framework .
as we will see , however , they are particularly natural in their application to graphical models , due to their relationships with the structural properties of graphs .
the phrase variational itself is an umbrella term that refers to var - ious mathematical tools for optimization - based formulations of prob - lems , as well as associated techniques for their solution .
the general idea is to express a quantity of interest as the solution of an opti - mization problem .
the optimization problem can then be relaxed in various ways , either by approximating the function to be optimized or by approximating the set over which the optimization takes place .
such relaxations , in turn , provide a means of approximating the original quantity of interest .
the roots of both mcmc methods and variational methods lie in statistical physics .
indeed , the successful deployment of mcmc methods in statistical physics motivated and predated their entry into
statistics .
however , the development of mcmc methodology specif - ically designed for statistical problems has played an important role in sparking widespread application of such methods in statistics ( 123 ) .
a similar development in the case of variational methodology would be of signicant interest .
in our view , the most promising avenue toward a variational methodology tuned to statistics is to build on existing links between variational analysis and the exponential family of distri - butions ( 123 , 123 , 123 , 123 ) .
indeed , the notions of convexity that lie at the heart of the statistical theory of the exponential family have immediate implications for the design of variational relaxations .
moreover , these variational relaxations have particularly interesting algorithmic conse - quences in the setting of graphical models , where they again lead to recursions on graphs .
thus , we present a story with three interrelated themes .
we begin in section 123 with a discussion of graphical models , providing both an overview of the general mathematical framework , and also presenting several specic examples .
all of these examples , as well as the majority of current applications of graphical models , involve distributions in the exponential family .
accordingly , section 123 is devoted to a discussion of exponential families , focusing on the mathematical links to convex analysis , and thus anticipating our development of variational meth - ods .
in particular , the principal object of interest in our exposition is a certain conjugate dual relation associated with exponential fam - ilies .
from this foundation of conjugate duality , we develop a gen - eral variational representation for computing likelihoods and marginal probabilities in exponential families .
subsequent sections are devoted to the exploration of various instantiations of this variational princi - ple , both in exact and approximate forms , which in turn yield various algorithms for computing exact and approximate marginal probabili - ties , respectively .
in section 123 , we discuss the connection between the bethe approximation and the sum - product algorithm , including both its exact form for trees and approximate form for graphs with cycles .
we also develop the connections between bethe - like approximations and other algorithms , including generalized sum - product , expectation - propagation and related moment - matching methods .
in section 123 , we discuss the class of mean eld methods , which arise from a qualitatively
dierent approximation to the exact variational principle , with the added benet of generating lower bounds on the likelihood .
in section 123 , we discuss the role of variational methods in parameter estimation , including both the fully observed and partially observed cases , as well as both frequentist and bayesian settings .
both bethe - type and mean eld methods are based on nonconvex optimization problems , which typically have multiple solutions .
in contrast , section 123 discusses vari - ational methods based on convex relaxations of the exact variational principle , many of which are also guaranteed to yield upper bounds on the log likelihood .
section 123 is devoted to the problem of mode compu - tation , with particular emphasis on the case of discrete random vari - ables , in which context computing the mode requires solving an integer programming problem .
we develop connections between ( reweighted ) max - product algorithms and hierarchies of linear programming relax - ations .
in section 123 , we discuss the broader class of conic programming relaxations , and show how they can be understood in terms of semidef - inite constraints imposed via moment matrices .
we conclude with a discussion in section 123
the scope of this survey is limited in the following sense : given a dis - tribution represented as a graphical model , we are concerned with the problem of computing marginal probabilities ( including likelihoods ) , as well as the problem of computing modes .
we refer to such computa - tional tasks as problems of probabilistic inference , or inference for short .
as with presentations of mcmc methods , such a limited focus may appear to aim most directly at applications in bayesian statis - tics .
while bayesian statistics is indeed a natural terrain for deploying many of the methods that we present here , we see these methods as having applications throughout statistics , within both the frequentist and bayesian paradigms , and we indicate some of these applications at various junctures in the survey .
we begin with background on graphical models .
the key idea is that of factorization : a graphical model consists of a collection of probability distributions that factorize according to the structure of an underly - ing graph .
here , we are using the terminology distribution loosely; our notation p should be understood as a mass function ( density with respect to counting measure ) in the discrete case , and a density with respect to lebesgue measure in the continuous case .
our focus in this section is the interplay between probabilistic notions such as conditional independence on one hand , and on the other hand , graph - theoretic notions such as cliques and separation .
123 probability distributions on graphs
we begin by describing the various types of graphical formalisms that are useful .
a graph g = ( v , e ) is formed by a collection of vertices v = ( 123 , 123 , .
, m ) , and a collection of edges e v v .
each edge con - sists of a pair of vertices s , t e , and may either be undirected , in which case there is no distinction between edge ( s , t ) and edge ( t , s ) , or directed , in which case we write ( s t ) to indicate the direction .
see appendix a . 123 for more background on graphs and their properties .
in order to dene a graphical model , we associate with each vertex s v a random variable xs taking values in some space xs .
depend - ing on the application , this state space xs may either be continuous , ( e . g . , xs = r ) or discrete ( e . g . , xs = ( 123 , 123 , .
, r 123 ) ) .
we use lower - case letters ( e . g . , xs xs ) to denote particular elements of xs , so that the notation ( xs = xs ) corresponds to the event that the random variable xs takes the value xs xs .
for any subset a of the vertex set v , we dene the subvector xa : = ( xs , s a ) , with the notation xa : = ( xs , s a ) corresponding to the analogous quantity for values of the random vector xa .
similarly , we dene saxs to be the carte - sian product of the state spaces for each of the elements of xa .
123 . 123 directed graphical models given a directed graph with edges ( s t ) , we say that t is a child of s , or conversely , that s is a parent of t .
for any vertex s v , let ( s ) denote the set of all parents of given node s v .
( if a vertex s has no parents , then the set ( s ) should be understood to be empty . ) a directed cycle is a sequence ( s123 , s123 , .
, sk ) such that ( si si+123 ) e for all i = 123 , .
, k 123 , and ( sk s123 ) e .
see figure 123 for an illustration of these concepts .
now suppose that g is a directed acyclic graph ( dag ) , meaning that every edge is directed , and that the graph contains no directed
123 ( a ) a simple directed graphical model with four variables ( x123 , x123 , x123 , x123 ) .
vertices ( 123 , 123 , 123 ) are all parents of vertex 123 , written ( 123 ) = ( 123 , 123 , 123 ) .
( b ) a more complicated directed acyclic graph ( dag ) that denes a partial order on its vertices .
note that vertex 123 is a child of vertex 123 , and vertex 123 is an ancestor of 123
( c ) a forbidden directed graph ( nonacyclic ) that includes the directed cycle ( 123 123 123 123 ) .
123 probability distributions on graphs
cycles .
for any such dag , we can dene a partial order on the vertex set v by the notion of ancestry : we say that node s is an ancestor of u if there is a directed path ( s , t123 , t123 , .
, tk , u ) ( see figure 123 ( b ) ) .
given a dag , for each vertex s and its parent set ( s ) , let ps ( xs | x ( s ) ) denote a nonnegative function over the variables ( xs , x ( s ) ) , normalized ps ( xs | x ( s ) ) dxs = 123
in terms of these local functions , a directed graphical model consists of a collection of probability distribu - tions ( densities or mass functions ) that factorize in the following way :
p ( x123 , x123 , .
, xm ) =
ps ( xs | x ( s ) ) .
it can be veried that our use of notation is consistent , in fact , the conditional probability of ( xs = xs ) ps ( xs | x ( s ) ) is , given ( x ( s ) = x ( s ) ) for the distribution p ( ) dened by the factor - ization ( 123 ) .
this follows by an inductive argument that makes use of the normalization condition imposed on ps ( ) , and the partial ordering induced by the ancestor relationship of the dag .
123 . 123 undirected graphical models
in the undirected case , the probability distribution factorizes according to functions dened on the cliques of the graph .
a clique c is a fully connected subset of the vertex set v , meaning that ( s , t ) e for all s , t c .
let us associate with each clique c a compatibility function c : ( scxs ) r+ .
recall that scxs denotes the cartesian prod - uct of the state spaces of the random vector xc .
consequently , the compatibility function c is a local quantity , dened only for elements xc within the clique .
with this notation , an undirected graphical model also known as a markov random eld ( mrf ) , or a gibbs distribution is a collection of distributions that factorize as
p ( x123 , x123 , .
, xm ) =
where z is a constant chosen to ensure that the distribution is nor - malized .
the set c is often taken to be the set of all maximal cliques of the graph , i . e . , the set of cliques that are not properly contained
within any other clique .
this condition can be imposed without loss of generality , because any representation based on nonmaximal cliques can always be converted to one based on maximal cliques by reden - ing the compatibility function on a maximal clique to be the product over the compatibility functions on the subsets of that clique .
how - ever , there may be computational benets to using a nonmaximal rep - resentation in particular , algorithms may be able to exploit specic features of the factorization special to the nonmaximal representation .
for this reason , we do not necessarily restrict compatibility functions to maximal cliques only , but instead dene the set c to contain all cliques .
( factor graphs , discussed in the following section , allow for a ner - grained specication of factorization properties . )
it is important to understand that for a general undirected graph the compatibility functions c need not have any obvious or direct relation to marginal or conditional distributions dened over the graph cliques .
this property should be contrasted with the directed factor - ization ( 123 ) , where the factors correspond to conditional probabilities over the child - parent sets .
123 . 123 factor graphs
for large graphs , the factorization properties of a graphical model , whether undirected or directed , may be dicult to visualize from the usual depictions of graphs .
the formalism of factor graphs provides an alternative graphical representation , one which emphasizes the factor - ization of the distribution ( 123 , 123 ) .
let f represent an index set for the set of factors dening a graph - ical model distribution .
in the undirected case , this set indexes the collection c of cliques , while in the directed case f indexes the set of parentchild neighborhoods .
we then consider a bipartite graph ( cid : 123 ) = ( v , f , e ( cid : 123 ) is a new ( cid : 123 ) ) , where v is the original set of vertices , and e edge set , joining only vertices s v to factors a f .
in particular , edge ( s , a ) e ( cid : 123 ) if and only if xs participates in the factor indexed by a f .
see figure 123 ( b ) for an illustration .
for undirected models , the factor graph representation is of partic - ular value when c consists of more than the maximal cliques .
indeed , the compatibility functions for the nonmaximal cliques do not have
123 conditional independence
123 illustration of undirected graphical models and factor graphs .
( a ) an undirected graph on m = 123 vertices , with maximal cliques ( 123 , 123 , 123 , 123 ) , ( 123 , 123 , 123 ) , and ( 123 , 123 ) .
( b ) equiv - alent representation of the undirected graph in ( a ) as a factor graph , assuming that we dene compatibility functions only on the maximal cliques in ( a ) .
the factor graph is a bipartite graph with vertex set v = ( 123 , .
, 123 ) and factor set f = ( a , b , c ) , one for each of the compatibility functions of the original undirected graph .
an explicit representation in the usual representation of an undirected graph however , the factor graph makes them explicit .
123 conditional independence
families of probability distributions as dened as in equations ( 123 ) or ( 123 ) also have a characterization in terms of conditional indepen - dencies among subsets of random variables the markov properties of the graphical model .
we only touch upon this characterization here , as it is not needed in the remainder of the survey; for a full treatment , we refer the interested reader to lauritzen ( 123 ) .
for undirected graphical models , conditional independence is iden - tied with the graph - theoretic notion of reachability .
in particular , let a , b , and c be an arbitrary triple of mutually disjoint subsets of ver - tices .
let us stipulate that xa be independent of xb given xc if there is no path from a vertex in a to a vertex in b when we remove the vertices c from the graph .
ranging over all possible choices of subsets a , b , and c gives rise to a list of conditional independence assertions .
it can be shown that this list is always consistent ( i . e . , there exist prob - ability distributions that satisfy all of these assertions ) ; moreover , the set of probability distributions that satisfy these assertions is exactly the set of distributions dened by ( 123 ) ranging over all possible choices of compatibility functions .
thus , there are two equivalent characterizations of the family of probability distributions associated with an undirected graph .
this equivalence is a fundamental mathematical result , linking an alge - braic concept ( factorization ) and a graph - theoretic concept ( reachabil - ity ) .
this result also has algorithmic consequences , in that it reduces the problem of assessing conditional independence to the problem of assessing reachability on a graph , which is readily solved using simple breadth - rst search algorithms ( 123 ) .
an analogous result holds in the case of directed graphical models , with the only alteration being a dierent notion of reachability ( 123 ) .
once again , it is possible to establish an equivalence between the set of probability distributions specied by the directed factoriza - tion ( 123 ) , and that dened in terms of a set of conditional independence
123 statistical inference and exact algorithms
given a probability distribution p dened by a graphical model , our focus will be solving one or more of the following computational infer -
( a ) computing the likelihood of observed data .
( b ) computing the marginal distribution p ( xa ) over a partic - ular subset a v of nodes .
( c ) computing the conditional distribution p ( xa | xb ) , for dis - joint subsets a and b , where a b is in general a proper ( d ) computing a mode of the density ( i . e . , an element ( cid : 123 ) x in the subset of v .
set arg maxxx m p ( x ) ) .
clearly problem ( a ) is a special case of problem ( b ) .
the computation of a conditional probability in ( c ) is similar in that it also requires marginalization steps , an initial one to obtain the numerator p ( xa , xb ) , and a further step to obtain the denominator p ( xb ) .
in contrast , the problem of computing modes stated in ( d ) is fundamentally dierent , since it entails maximization rather than integration .
nonetheless , our variational development in the sequel will highlight some important
connections between the problem of computing marginals and that of to understand the challenges inherent in these inference prob - lems , consider the case of a discrete random vector x x m , where xs = ( 123 , 123 , .
, r 123 ) for each vertex s v .
a naive approach to com - puting a marginal at a single node say p ( xs ) entails summing s = xs ) .
since this set ( cid : 123 ) x m | x over all congurations of the form ( x has rm123 elements , it is clear that a brute force approach will rapidly become intractable .
even with binary variables ( r = 123 ) and a graph with m 123 nodes ( a small size for many applications ) , this summa - tion is beyond the reach of brute - force computation .
similarly , in this discrete case , computing a mode entails solving an integer programming problem over an exponential number of congurations .
for continuous random vectors , the problems are no easier123 and typically harder , since they require computing a large number of integrals .
for undirected graphs without cycles or for directed graphs in which each node has a single parent known generically as trees in either case these inference problems can be solved exactly by recursive message - passing algorithms of a dynamic programming nature , with a computational complexity that scales only linearly in the number of nodes .
in particular , for the case of computing marginals , the dynamic programming solution takes the form of a general algorithm known as the sum - product algorithm , whereas for the problem of comput - ing modes it takes the form of an analogous algorithm known as the max - product algorithm .
we describe these algorithms in section 123 . 123
more generally , as we discuss in section 123 . 123 , the junction tree algo - rithm provides a solution to inference problems for arbitrary graphs .
the junction tree algorithm has a computational complexity that is exponential in a quantity known as the treewidth of the graph .
before turning to these algorithmic issues , however , it is helpful to ground the discussion by considering some examples of applications of
123 the gaussian case is an important exception to this statement .
graphical models .
we present examples of the use of graphical models in the general areas of bayesian hierarchical modeling , contingency table analysis , and combinatorial optimization and satisability , as well spe - cic examples in bioinformatics , speech and language processing , image processing , spatial statistics , communication and coding theory .
123 . 123 hierarchical bayesian models
the bayesian framework treats all model quantities observed data , latent variables , parameters , nuisance variables as random variables .
thus , in a graphical model representation of a bayesian model , all such variables appear explicitly as vertices in the graph .
the general com - putational machinery associated with graphical models applies directly to bayesian computations of quantities such as marginal likelihoods and posterior probabilities of parameters .
although bayesian models can be represented using either directed or undirected graphs , it is the directed formalism that is most commonly encountered in practice .
in particular , in hierarchical bayesian models , the specication of prior distributions generally involves additional parameters known as hyper - parameters , and the overall model is specied as a set of conditional probabilities linking hyperparameters , parameters and data .
taking the product of such conditional probability distributions denes the joint probability; this factorization is simply a particular instance of equa -
there are several advantages to treating a hierarchical bayesian model as a directed graphical model .
first , hierarchical models are often specied by making various assertions of conditional independence .
these assertions imply other conditional independence relationships , and the reachability algorithms ( mentioned in section 123 ) provide a systematic method for investigating all such relationships .
second , the visualization provided by the graph can be useful both for understand - ing the model ( including the basic step of verifying that the graph is acyclic ) , as well for exploring extensions .
finally , general computational methods such as mcmc and variational inference algorithms can be implemented for general graphical models , and hence apply to hier - archical models in graphical form .
these advantages and others have
led to the development of general software programs for specifying and manipulating hierarchical bayesian models via the directed graphical model formalism ( 123 ) .
123 . 123 contingency table analysis
contingency tables are a central tool in categorical data analysis ( 123 , 123 , 123 ) , dating back to the seminal work of pearson , yule , and fisher .
an m - dimensional contingency table with r levels describes a probability distribution over m random variables ( x123 , .
, xm ) , each of which takes one of r possible values .
for the special case m = 123 , a contingency table with r levels is simply a r r matrix of nonnegative elements summing to one , whereas for m > 123 , it is a multi - dimensional array with rm nonnegative entries summing to one .
thus , the table fully species a probability distribution over a random vector ( x123 , .
, xm ) , where each xs takes one of r possible values .
contingency tables can be modeled within the framework of graph - ical models , and graphs provide a useful visualization of many natural questions .
for instance , one central question in contingency table anal - ysis is distinguishing dierent orders of interaction in the data .
as a concrete example , given m = 123 variables , a simple question is testing whether or not the three variables are independent .
from a graph - ical model perspective , this test amounts to distinguishing whether the associated interaction graph is completely disconnected or not ( see panels ( a ) and ( b ) in figure 123 ) .
a more subtle test is to distinguish whether the random variables interact in only a pairwise manner ( i . e . , with factors 123 , 123 , and 123 in equation ( 123 ) ) , or if there is actually a three - way interaction ( i . e . , a term 123 in the factorization ( 123 ) ) .
interestingly , these two factorization choices cannot be distinguished
123 some simple graphical interactions in contingency table analysis .
( a ) independence model .
( b ) general dependent model .
( c ) pairwise interactions only .
( d ) triplet interactions .
using a standard undirected graph; as illustrated in figure 123 ( b ) , the fully connected graph on 123 vertices does not distinguish between pair - wise interactions without triplet interactions , versus triplet interaction ( possibly including pairwise interaction as well ) .
in this sense , the fac - tor graph formalism is more expressive , since it can distinguish between pairwise and triplet interactions , as shown in panels ( c ) and ( d ) of
123 . 123 constraint satisfaction and combinatorial
problems of constraint satisfaction and combinatorial optimization arise in a wide variety of areas , among them articial intelligence ( 123 , 123 ) , communication theory ( 123 ) , computational complexity theory ( 123 ) , statistical image processing ( 123 ) , and bioinformatics ( 123 ) .
many prob - lems in satisability and combinatorial optimization ( 123 ) are dened in graph - theoretic terms , and thus are naturally recast in the graphical
let us illustrate by considering what is perhaps the best - known example of satisability , namely the 123 - sat problem ( 123 ) .
it is naturally described in terms of a factor graph , and a collection of binary random variables ( x123 , x123 , .
, xm ) ( 123 , 123 ) m .
for a given triplet ( s , t , u ) of ver - tices , let us specify some forbidden conguration ( zs , zt , zu ) ( 123 , 123 ) 123 , and then dene a triplet compatibility function
stu ( xs , xt , xu ) =
if ( xs , xt , xu ) = ( zs , zt , zu ) ,
each such compatibility function is referred to as a clause in the satisability literature , where such compatibility functions are typically encoded in terms of ( zs , zt , zu ) = ( 123 , 123 , 123 ) , then the function ( 123 ) can be written compactly as stu ( xs , xt , xu ) = xs xt xu , where denotes the logical or operation between two boolean symbols .
logical operations .
for instance ,
as a graphical model , a single clause corresponds to the model in figure 123 ( d ) ; of greater interest are models over larger collections involving many clauses .
one basic problem in of binary variables ,
satisability is to determine whether a given set of clauses f are sat - isable , meaning that there exists some conguration x ( 123 , 123 ) m such
( s , t , u ) f stu ( xs , xt , xu ) = 123
in this case , the factorization
stu ( xs , xt , xu )
denes the uniform distribution over the set of satisfying assignments .
when instances of 123 - sat are randomly constructed for instance , by xing a clause density > 123 , and drawing ( cid : 123 ) m ( cid : 123 ) clauses over m vari - ables uniformly from all triplets the graphs tend to have a locally tree - like structure; see the factor graph in figure 123 ( b ) for an illus - at which this tration .
one major problem is determining the value ensemble is conjectured to undergo a phase transition from being sat - isable ( for small , hence with few constraints ) to being unsatisable ( for suciently large ) .
the survey propagation algorithm , developed in the statistical physics community ( 123 , 123 ) , is a promising method for solving random instances of satisability problems .
survey propa - gation turns out to be an instance of the sum - product or belief propa - gation algorithm , but as applied to an alternative graphical model for satisability problems ( 123 , 123 ) .
many classical models in bioinformatics are instances of graphical mod - els , and the associated framework is often exploited in designing new models .
in this section , we briey review some instances of graphical models in both bioinformatics , both classical and recent .
sequential data play a central role in bioinformatics , and the workhorse underly - ing the modeling of sequential data is the hidden markov model ( hmm ) shown in figure 123 ( a ) .
the hmm is in essence a dynamical version of a nite mixture model , in which observations are generated conditionally on a underlying latent ( hidden ) state variable .
the state variables , which are generally taken to be discrete random variables following a multinomial distribution , form a markov chain .
the graphical model in figure 123 ( a ) is also a representation of the state - space model under - lying kalman ltering and smoothing , where the state variable is a
123 ( a ) the graphical model representation of a generic hidden markov model .
the shaded nodes ( y123 , .
, y123 ) are the observations and the unshaded nodes ( x123 , .
, x123 ) are the hidden state variables .
the latter form a markov chain , in which xs is independent of xu conditional on xt , where s < t < u .
( b ) the graphical model representation of a phylogeny on four extant organisms and m sites .
the tree encodes the assumption that there is a rst speciation event and then two further speciation events that lead to the four extant organisms .
the box around the tree ( a plate ) is a graphical model representation of replication , here representing the assumption that the m sites evolve independently .
gaussian vector .
these models thus also have a right to be referred to as hidden markov models , but the terminology is most commonly used to refer to models in which the state variables are discrete .
applying the junction tree formalism to the hmm yields an algo - rithm that passes messages in both directions along the chain of state variables , and computes the marginal probabilities p ( xt , xt+123| y ) and p ( xt| y ) .
in the hmm context , this algorithm is often referred to as the forwardbackward algorithm ( 123 ) .
these marginal probabilities are often of interest in and of themselves , but are also important in their role as expected sucient statistics in an expectationmaximization ( em ) algorithm for estimating the parameters of the hmm .
similarly , the maximum a posteriori state sequence can also be computed by the junction tree algorithm ( with summation replaced by maximization ) in the hmm context the resulting algorithm is generally referred to as the viterbi algorithm ( 123 ) .
gene - nding provides an important example of the application of the hmm ( 123 ) .
to a rst order of approximation , the genomic sequence of an organism can be segmented into regions containing genes and intergenic regions ( that separate the genes ) , where a gene is dened as a sequence of nucleotides that can be further segmented into meaning - ful intragenic structures ( exons and introns ) .
the boundaries between these segments are highly stochastic and hence dicult to nd reliably .
hidden markov models have been the methodology of choice for attack - ing this problem , with designers choosing states and state transitions to reect biological knowledge concerning gene structure ( 123 ) .
hidden markov models are also used to model certain aspects of protein struc - ture .
for example , membrane proteins are specic kinds of proteins that embed themselves in the membranes of cells , and play impor - tant roles in the transmission of signals in and out of the cell .
these proteins loop in and out of the membrane many times , alternating between hydrophilic amino acids ( which prefer the environment of the membrane ) and hydrophobic amino acids ( which prefer the environ - ment inside or outside the membrane ) .
these and other biological facts are used to design the states and state transition matrix of the trans - membrane hidden markov model , an hmm for modeling membrane
tree - structured models also play an important role in bioinformat - ics and language processing .
for example , phylogenetic trees can be treated as graphical models .
as shown in figure 123 ( b ) , a phylogenetic tree is a tree - structured graphical model in which a set of observed nucleotides ( or other biological characters ) are assumed to have evolved from an underlying set of ancestral species .
the conditional probabil - ities in the tree are obtained from evolutionary substitution models , and the computation of likelihoods are achieved by a recursion on the tree known as pruning ( 123 ) .
this recursion is a special case of the junction tree algorithm .
figure 123 provides examples of more complex graphical models that have been explored in bioinformatics .
figure 123 ( a ) shows a hidden
123 variations on the hmm used in bioinformatics .
( a ) a phylogenetic hmm .
( b ) the
markov phylogeny , an hmm in which the observations are sets of nucleotides related by a phylogenetic tree ( 123 , 123 , 123 ) .
this model has proven useful for gene - nding in the human genome based on data for multiple primate species ( 123 ) .
figure 123 ( b ) shows a factorial hmm , in which multiple chains are coupled by their links to a common set of observed variables ( 123 ) .
this model captures the problem of multi - locus linkage analysis in genetics , where the state variables correspond to phase ( maternal or paternal ) along the chromosomes in meiosis ( 123 ) .
123 . 123 language and speech processing
in language problems , hmms also play a fundamental role .
an exam - ple is the part - of - speech problem , in which words in sentences are to be labeled as to their part of speech ( noun , verb , adjective , etc . ) .
here the state variables are the parts of speech , and the transition matrix is esti - mated from a corpus via the em algorithm ( 123 ) .
the result of running the viterbi algorithm on a new sentence is a tagging of the sentence according to the hypothesized parts of speech of the words in the sen - tence .
moreover , essentially all modern speech recognition systems are built on the foundation of hmms ( 123 ) .
in this case the observations are generally a sequence of short - range speech spectra , and the states correspond to longer - range units of speech such as phonemes or pairs of phonemes .
large - scale systems are built by composing elementary hmms into larger graphical models .
the graphical model shown in figure 123 ( a ) is a coupled hmm , in which two chains of state variables are coupled via links between the
123 extensions of hmms used in language and speech processing .
( a ) the coupled hhm .
( b ) an hmm with mixture - model emissions .
chains; this model is appropriate for fusing pairs of data streams such as audio and lip - reading data in speech recognition ( 123 ) .
figure 123 ( b ) shows an hmm variant in which the state - dependent observation dis - tribution is a nite mixture model .
this variant is widely used in speech recognition systems ( 123 ) .
another model class that is widely studied in language processing are so - called bag - of - words models , which are of particular interest for modeling large - scale corpora of text documents .
the terminology bag - of - words means that the order of words in a document is ignored i . e . , an assumption of exchangeability is made .
the goal of such models is often that of nding latent topics in the corpus , and using these topics to cluster or classify the documents .
an example bag - of - words model is the latent dirichlet allocation model ( 123 ) , in which a topic denes a probability distribution on words , and a document denes a probability distribution on topics .
in particular , as shown in figure 123 , each document in a corpus is assumed to be generated by sampling a dirichlet variable with hyperparameter , and then repeatedly selecting a topic according to these dirichlet probabilities , and choosing a word from the distribution associated with the selected topic . 123
image processing and spatial statistics
for several decades , undirected graphical models or markov ran - dom elds have played an important role in image processing
123 graphical illustration of the latent dirichlet allocation model .
the variable u , which is distributed as dirichlet with parameter , species the parameter for the multi - nomial topic variable z .
the word variable w is also multinomial conditioned on z , with specifying the word probabilities associated with each topic .
the rectangles , known as plates , denote conditionally independent replications of the random variables inside the
123 this model is discussed in more detail in example 123 of section 123 .
123 ( a ) the 123 - nearest neighbor lattice model in 123d is often used for image modeling .
( b ) a multiscale quad tree approximation to a 123d lattice model ( 123 ) .
nodes in the original lattice ( drawn in white ) lie at the nest scale of the tree .
the middle and top scales of the tree consist of auxiliary nodes ( drawn in gray ) , introduced to model the ne scale behavior .
( e . g . , 123 , 123 , 123 , 123 ) , as well as in spatial statistics more generally ( 123 , 123 , 123 , 123 ) .
for modeling an image , the simplest use of a markov random eld model is in the pixel domain , where each pixel in the image is associated with a vertex in an underlying graph .
more structured models are based on feature vectors at each spatial location , where each feature could be a linear multiscale lter ( e . g . , a wavelet ) , or a more complicated nonlinear operator .
for image modeling , one very natural choice of graphical struc - ture is a 123d lattice , such as the 123 - nearest neighbor variety shown in figure 123 ( a ) .
the potential functions on the edges between adjacent pixels ( or more generally , features ) are typically chosen to enforce local smoothness conditions .
various tasks in image processing , including denoising , segmentation , and super - resolution , require solving an infer - ence problem on such a markov random eld .
however , exact inference for large - scale lattice models is intractable , which necessitates the use of approximate methods .
markov chain monte carlo methods are often used ( 123 ) , but they can be too slow and computationally intensive for many applications .
more recently , message - passing algorithms such as sum - product and tree - reweighted max - product have become popular as an approximate inference method for image processing and computer vision problems ( e . g . , 123 , 123 , 123 , 123 , 123 ) .
an alternative strategy is to sidestep the intractability of the lattice model by replacing it with a simpler albeit approximate model .
for instance , multiscale quad trees , such as that illustrated in figure 123 ( b ) , can be used to approximate lattice models ( 123 ) .
the advantage of such a multiscale model is in permitting the application of ecient tree algorithms to perform exact inference .
the trade - o is that the model is imperfect , and can introduce artifacts into image
123 . 123 error - correcting coding
a central problem in communication theory is that of transmitting information , represented as a sequence of bits , from one point to another .
examples include transmission from a personal computer over a network , or from a satellite to a ground position .
if the communication channel is noisy , then some of the transmitted bits may be corrupted .
in order to combat this noisiness , a natural strategy is to add redundancy to the transmitted bits , thereby dening codewords .
in principle , this coding strategy allows the transmission to be decoded perfectly even in the presence of some number of errors .
many of the best codes in use today , including turbo codes and low - density parity check codes ( e . g . , 123 , 123 ) , are based on graphical models .
figure 123 ( a ) provides an illustration of a very small parity check code , represented here in the factor graph formalism ( 123 ) .
( a somewhat larger code is shown in figure 123 ( b ) ) .
on the left , the six white nodes represent the bits xi that comprise the codewords ( i . e . , binary sequences of length six ) , whereas the gray nodes represent noisy observations yi associated with these bits .
on the right , each of the black square nodes corresponds to a factor stu that represents the parity of the triple ( xs , xt , xu ) .
this parity relation , expressed mathematically as xs xt xu zstu in modulo two arithmetic , can be represented as an undirected graphical model using a compatibility function of the form :
stu ( xs , xt , xu ) : =
if xs xt xu = 123
for the code shown in figure 123 , the parity checks range over the set of triples ( 123 , 123 , 123 ) , ( 123 , 123 , 123 ) , ( 123 , 123 , 123 ) , and ( 123 , 123 , 123 ) .
the decoding problem entails estimating which codeword was trans - mitted on the basis of a vector y = ( y123 , y123 , .
, ym ) of noisy observations .
123 ( a ) a factor graph representation of a parity check code of length m = 123
on the left , circular white nodes ( cid : 123 ) represent the unobserved bits xi that dene the code , whereas circular gray nodes represent the observed values yi , received from the channel .
on the right , black squares ( cid : 123 ) represent the associated factors , or parity checks .
this particular code is a ( 123 , 123 ) code , since each bit is connected to two parity variables , and each parity relation involves three bits .
( b ) a large factor graph with a locally tree - like structure .
random constructions of factor graphs on m vertices with bounded degree have cycles of typical length ( cid : 123 ) log m; this tree - like property is essential to the success of the sum - product algorithm for approximate decoding ( 123 , 123 ) .
with the specication of a model for channel noise , this decoding prob - lem can be cast as an inference problem .
depending on the loss function , optimal decoding is based either on computing the marginal probabil - ity p ( xs = 123 | y ) at each node , or computing the most likely codeword ( i . e . , the mode of the posterior ) .
for the simple code of figure 123 ( a ) , optimal decoding is easily achievable via the junction tree algorithm .
of interest in many applications , however , are much larger codes in which the number of bits is easily several thousand .
the graphs under - lying these codes are not of low treewidth , so that the junction tree algorithm is not viable .
moreover , mcmc algorithms have not been deployed successfully in this domain .
for many graphical codes , the most successful decoder is based on applying the sum - product algorithm , described in section 123 .
since the graphical models dening good codes invariably have cycles , the sum - product algorithm is not guaranteed to compute the correct marginals , nor even to converge .
nonetheless , the behavior of this
123 exact inference algorithms
approximate decoding algorithm is remarkably good for a large class of codes .
the behavior of sum - product algorithm is well understood in the asymptotic limit ( as the code length m goes to innity ) , where martingale arguments can be used to prove concentration results ( 123 , 123 ) .
for intermediate code lengths , in contrast , its behavior is not as
123 exact inference algorithms
in this section , we turn to a description of the basic exact inference algorithms for graphical models .
in computing a marginal probability , we must sum or integrate the joint probability distribution over one or more variables .
we can perform this computation as a sequence of operations by choosing a specic ordering of the variables ( and mak - ing an appeal to fubinis theorem ) .
recall that for either directed or undirected graphical models , the joint probability is a factored expres - sion over subsets of the variables .
consequently , we can make use of the distributive law to move individual sums or integrals across fac - tors that do not involve the variables being summed or integrated over .
the phrase exact inference refers to the ( essentially symbolic ) prob - lem of organizing this sequential computation , including managing the intermediate factors that arise .
assuming that each individual sum or integral is performed exactly , then the overall algorithm yields an exact
in order to obtain the marginal probability distribution of a single variable xs , it suces to choose a specic ordering of the remaining variables and to eliminate that is , sum or integrate out variables according to that order .
repeating this operation for each individual variable would yield the full set of marginals .
however , this approach is wasteful because it neglects to share intermediate terms in the individ - ual computations .
the sum - product and junction tree algorithms are essentially dynamic programming algorithms based on a calculus for sharing intermediate terms .
the algorithms involve message - passing operations on graphs , where the messages are exactly these shared intermediate terms .
upon convergence of the algorithms , we obtain marginal probabilities for all cliques of the original graph .
both directed and undirected graphical models involve factorized expressions for joint probabilities , and it should come as no surprise that exact inference algorithms treat them in an essentially identical manner .
indeed , to permit a simple unied treatment of inference algo - rithms , it is convenient to convert directed models to undirected models and to work exclusively within the undirected formalism .
we do this by observing that the terms in the directed factorization ( 123 ) are not necessarily dened on cliques , since the parents of a given vertex are not necessarily connected .
we thus transform a directed graph to an undirected moral graph , in which all parents of each child are linked , and all edges are converted to undirected edges .
in this moral graph , the factors are all dened on cliques , so that the moralized version of any directed factorization ( 123 ) is a special case of the undirected fac - torization ( 123 ) .
throughout the rest of the survey , we assume that this transformation has been carried out .
123 . 123 message - passing on trees
we now turn to a description of message - passing algorithms for exact inference on trees .
our treatment is brief; further details can be found in various sources ( 123 , 123 , 123 , 123 , 123 ) .
we begin by observing that the cliques of a tree - structured graph t = ( v , e ( t ) ) are simply the individ - ual nodes and edges .
as a consequence , any tree - structured graphical model has the following factorization :
p ( x123 , x123 , .
, xm ) =
here , we describe how the sum - product algorithm computes the
( x ( cid : 123 ) | x ( cid : 123 )
for every node of a tree - structured graph .
we will focus in detail on the case of discrete random variables , with the understanding that the computations carry over ( at least in principle ) to the continuous case by replacing sums with integrals .
123 exact inference algorithms
sum - product algorithm : the sum - product algorithm is a form of nonserial dynamic programming ( 123 ) , which generalizes the usual serial form of deterministic dynamic programming ( 123 ) to arbitrary tree - structured graphs .
the essential principle underlying dynamic programming ( dp ) is that of divide and conquer : we solve a large problem by breaking it down into a sequence of simpler problems .
in the context of graphical models , the tree itself provides a natural way to break down the problem .
for an arbitrary s v , consider the set of its neighbors
n ( s ) : = ( u v | ( s , u ) e ) .
for each u n ( s ) , let tu = ( vu , eu ) be the subgraph formed by the set of nodes ( and edges joining them ) that can be reached from u by paths that do not pass through node s .
the key property of a tree is that each such subgraph tu is again a tree , and tu and tv are vertex - disjoint for u ( cid : 123 ) = v .
in this way , each vertex u n ( s ) can be viewed as the root of a subtree tu , as illustrated in figure 123 .
for each subtree tt , we dene the subvector xvt : = ( xu , u vt ) of variables associated with its vertex set .
now consider the collection of terms in equation ( 123 ) associated with vertices or edges in tt .
we collect all of these terms into the following product :
with this notation , the conditional independence properties of a tree allow the computation of the marginal at node s to be broken down
123 decomposition of a tree , rooted at node s , into subtrees .
each neighbor ( e . g . , u ) of node s is the root of a subtree ( e . g . , tu ) .
subtrees tu and tv , for u ( cid : 123 ) = v , are disconnected when node s is removed from the graph .
into a product of subproblems , one for each of the subtrees in the set ( tt , t n ( s ) ) , in the following way : s ( xs ) = s ( xs )
in these equations , denotes a positive constant chosen to ensure that
s normalizes properly .
for xed xs , the subproblem dening m again a tree - structured summation , albeit involving a subtree tt smaller than the original tree t .
therefore , it too can be broken down recur - sively in a similar fashion .
in this way , the marginal at node s can be computed by a series of recursive updates .
rather than applying the procedure described above to each node separately , the sum - product algorithm computes the marginals for all nodes simultaneously and in parallel .
at each iteration , each node t passes a message to each of its neighbors u n ( t ) .
this message , which we denote by mtu ( xu ) , is a function of the possible states xu xu ( i . e . , a vector of length |xu| for discrete random variables ) .
on the full graph , there are a total of 123|e| messages , one for each direction of each edge .
this full collection of messages is updated , typically in parallel , according to the recursion
where > 123 again denotes a normalization constant .
be shown ( 123 ) ated by the update ( 123 ) will converge to a unique xed point ts , ( s , t ) e ) after a nite number of iterations
ts of this xed point is precisely equal , up to a over , component m normalization constant , to the subproblem dened in equation ( 123b ) , which justies our abuse of notation post hoc .
since the xed point species the solution to all of the subproblems , the marginal s at every node s v can be computed easily via equation ( 123a ) .
max - product algorithm : suppose summation in the update ( 123 ) is replaced by a maximization .
the resulting max - product
123 exact inference algorithms
algorithm solves the problem of nding a mode of a tree - structured distribution .
in this sense , it represents a generalization of the viterbi algorithm ( 123 ) from chains to arbitrary tree - structured graphs .
more specically , the max - product updates will converge to another unique distinct , of course , from the sum - product xed point .
xed point m this xed point can be used to compute the max - marginal
s ( xs ) : = max
( x ( cid : 123 ) | x ( cid : 123 )
, x to compute a mode ( cid : 123 ) x arg maxx ( cid : 123 ) p ( x
at each node of the graph , via the analog of equation ( 123 ) .
given these max - marginals , a standard back - tracking procedure can be used m ) of the distribution; 123 , .
, x see the papers ( 123 , 123 ) for further details .
more generally , updates of this form apply to arbitrary commutative semirings on tree - structured graphs ( 123 , 123 , 123 , 123 ) .
the pairs sum - product and max - product are two particular examples of such an algebraic structure .
123 . 123 junction tree representation
we have seen that inference problems on trees can be solved exactly by recursive message - passing algorithms .
given a graph with cycles , a natural idea is to cluster its nodes so as to form a clique tree that is , an acyclic graph whose nodes are formed by the maximal cliques of g .
having done so , it is tempting to apply a standard algorithm for inference on trees .
however , the clique tree must satisfy an addi - tional restriction so as to ensure correctness of these computations .
in particular , since a given vertex s v may appear in multiple cliques ( say c123 and c123 ) , what is required is a mechanism for enforcing consis - tency among the dierent appearances of the variable xs .
it turns out that the following property is necessary and sucient to enforce such
denition 123 .
a clique tree has the running intersection property if for any two clique nodes c123 and c123 , all nodes on the unique path joining them contain the intersection c123 c123
a clique tree with this property is known as a junction tree .
for what type of graphs can one build junction trees ? an important result in graph theory establishes a correspondence between junction trees and triangulation .
we say that a graph is triangulated if every cycle of length four or longer has a chord , meaning an edge joining a pair of nodes that are not adjacent on the cycle .
a key theorem is that a graph g has a junction tree if and only if it is triangulated ( see lauritzen ( 123 ) for a proof ) .
this result underlies the junction tree algorithm ( 123 ) for exact inference on arbitrary graphs :
( 123 ) given a graph with cycles g , triangulate it by adding edges
( 123 ) form a junction tree associated with the triangulated
( 123 ) run a tree inference algorithm on the junction tree .
example 123 ( junction tree ) .
to illustrate the junction tree con - struction , consider the 123 123 grid shown in figure 123 ( a ) .
the rst
step is to form a triangulated version ( cid : 123 ) g , as shown in figure 123 ( b ) .
note that the graph would not be triangulated if the additional edge joining nodes 123 and 123 were not present .
without this edge , the 123 - cycle ( 123 123 123 123 123 ) would lack a chord .
as a result of this additional edge , the junction tree has two 123 - cliques in the middle , as shown in
123 illustration of junction tree construction .
( a ) original graph is a 123 123 grid .
( b ) triangulated version of original graph .
note the two 123 - cliques in the middle .
( c ) corre - sponding junction tree for triangulated graph in ( b ) , with maximal cliques depicted within ellipses , and separator sets within rectangles .
123 exact inference algorithms
in principle , the inference in the third step of the junction tree algorithm can be performed over an arbitrary commutative semiring ( as mentioned in our earlier discussion of tree algorithms ) .
we refer the reader to dawid ( 123 ) for an extensive discussion of the max - product version of the junction tree algorithm .
for concreteness , we limit our discussion here to the sum - product version of junction tree updates .
there is an elegant way to express the basic algebraic operations in a junction tree inference algorithm that involves introducing potential functions not only on the cliques in the junction tree , but also on the separators in the junction tree the intersections of cliques that are adjacent in the junction tree ( the rectangles in figure 123 ) .
let c ( ) denote a potential function dened over the subvector xc = ( xt , t c ) on clique c , and let s ( ) denote a potential on a separator s .
we initialize the clique potentials by assigning each compatibility function in the original graph to ( exactly ) one clique potential and taking the product over these compatibility functions .
the separator potentials are initialized to unity .
given this set - up , the basic message - passing step of the junction tree algorithm are given by
where in the continuous case the summation is replaced by a suitable integral .
we refer to this pair of operations as passing a message from clique b to clique c ( see figure 123 ) .
it can be veried that if a message is passed from b to c , and subsequently from c to b , then the resulting clique potentials are consistent with each other , meaning that they agree with respect to the vertices s .
123 a message - passing operation between cliques b and c via the separator set s .
after a round of message passing on the junction tree , it can be shown that the clique potentials are proportional to marginal probabil - ities throughout the junction tree .
specically , letting c ( xc ) denote
the marginal probability of xc , we have c ( xc ) ( cid : 123 ) c ( xc ) for all cliques
this equivalence can be established by a suitable generalization of the proof of correctness of the sum - product algorithm sketched previ - ously ( see also lauritzen ( 123 ) ) .
note that achieving local consistency between pairs of cliques is obviously a necessary condition if the clique potentials are to be proportional to marginal probabilities .
moreover , the signicance of the running intersection property is now apparent; namely , it ensures that local consistency implies global consistency .
an important by - product of the junction tree algorithm is an alter - native representation of a distribution p .
let c denote the set of all
maximal cliques in ( cid : 123 ) g ( i . e . , nodes in the junction tree ) , and let s rep -
resent the set of all separator sets ( i . e . , intersections between cliques that are adjacent in the junction tree ) .
note that a given separator set s may occur multiple times in the junction tree .
for each separator set s s , let d ( s ) denote the number of maximal cliques to which it is adjacent .
the junction tree framework guarantees that the distribution p factorizes in the form
p ( x123 , x123 , .
, xm ) =
where c and s are the marginal distributions over the cliques and separator sets , respectively .
observe that unlike the factorization ( 123 ) , the decomposition ( 123 ) is directly in terms of marginal distributions , and does not require a normalization constant ( i . e . , z = 123 ) .
example 123 ( markov chain ) .
consider the markov chain p ( x123 , x123 , x123 ) = p ( x123 ) p ( x123| x123 ) p ( x123| x123 ) .
the cliques in a graphical model representation are ( 123 , 123 ) and ( 123 , 123 ) , with separator ( 123 ) .
clearly the distribution cannot be written as the product of marginals involving only the cliques .
it can , however , be written in terms of marginals if we
123 exact inference algorithms
include the separator :
p ( x123 , x123 , x123 ) = p ( x123 , x123 ) p ( x123 , x123 )
moreover , it can be easily veried that these marginals result from a single application of the updates ( 123 ) , given the initialization ( 123 , 123 ) ( x123 , x123 ) = p ( x123 ) p ( x123| x123 ) and ( 123 , 123 ) ( x123 , x123 ) = p ( x123| x123 ) .
to anticipate part of our development in the sequel , it is helpful to consider the following inverse perspective on the junction tree repre - sentation .
suppose that we are given a set of functions ( c , c c ) and ( s , s s ) associated with the cliques and separator sets in the junc - tion tree .
what conditions are necessary to ensure that these functions are valid marginals for some distribution ? suppose that the functions are locally consistent in the following sense :
s ) = 123 ,
c ) = s ( xs ) .
c c , s c ,
the essence of the junction tree theory described above is that such local consistency is both necessary and sucient to ensure that these functions are valid marginals for some distribution .
for the sake of future reference , we state this result in the following :
proposition 123 .
a candidate set of marginal distribution ( c , c c ) and ( c , c s ) on the cliques and separator sets ( respectively ) of a junction tree is globally consistent if and only if the normalization con - dition ( 123a ) is satised for all separator sets s j , and the marginal - ization condition ( 123b ) is satised for all cliques c c , and separator sets s c .
moreover , any such locally consistent quantities are the marginals of the probability distribution dened by equation ( 123 ) .
this particular consequence of the junction tree representation will play a fundamental role in our development in the sequel .
finally , let us turn to the key issue of the computational complex - ity of the junction tree algorithm .
inspection of equation ( 123 ) reveals that the computational costs grow exponentially in the size of the max - imal clique in the junction tree .
clearly then , it is of interest to control the size of this clique .
the size of the maximal clique over all possi - ble triangulations of a graph is an important graph - theoretic quantity known as the treewidth of the graph . 123 thus , the complexity of the junction tree algorithm is exponential in the treewidth .
for certain classes of graphs ,
including chains and trees , the treewidth is small and the junction tree algorithm provides an eec - tive solution to inference problems .
such families include many well - known graphical model architectures , and the junction tree algorithm subsumes the classical recursive algorithms , including the pruning and peeling algorithms from computational genetics ( 123 ) , the forward backward algorithms for hidden markov models ( 123 ) , and the kalman ltering - smoothing algorithms for state - space models ( 123 ) .
on the other hand , there are many graphical models , including several of the examples treated in section 123 , for which the treewidth is infeasibly large .
coping with such models requires leaving behind the junction tree framework , and turning to approximate inference algorithms .
123 message - passing algorithms for approximate inference
it is the goal of the remainder of the survey to develop a general theoret - ical framework for understanding and analyzing variational methods for computing approximations to marginal distributions and likelihoods , as well as for solving integer programming problems .
doing so requires mathematical background on convex analysis and exponential families , which we provide starting in section 123
historically , many of these algo - rithms have been developed without this background , but rather via physical intuition or on the basis of analogies to exact or monte carlo algorithms .
in this section , we give a high - level description of this avor for two variational inference algorithms , with the goal of highlighting their simple and intuitive nature .
123 to be more precise , the treewidth is one less than the size of this largest clique ( 123 ) .
123 message - passing algorithms for approximate inference
the rst variational algorithm that we consider is sum - product message - passing applied to a graph with cycles , where it is known as the loopy sum - product or belief propagation algorithm .
recall that the sum - product algorithm is an exact inference algorithm for trees .
from an algorithmic point of view , however , there is nothing to pre - vent one from running the procedure on a graph with cycles .
more specically , the message updates ( 123 ) can be applied at a given node while ignoring the presence of cycles essentially pretending that any given node is embedded in a tree .
intuitively , such an algorithm might be expected to work well if the graph is sparse , such that the eect of messages propagating around cycles is appropriately diminished , or if suitable symmetries are present .
as discussed in section 123 , this algorithm is in fact successfully used in various applications .
also , an analogous form of the max - product algorithm can be used for comput - ing approximate modes in graphical models with cycles .
a second variational algorithm is the so - called naive mean eld algo - rithm .
for concreteness , we describe here this algorithm in application to the ising model , which is an undirected graphical model or markov random eld involving a binary random vector x ( 123 , 123 ) m , in which pairs of adjacent nodes are coupled with a weight st , and each node has an observation weight s .
( see example 123 of section 123 for a more detailed description of this model . ) to provide some intuition for the naive mean eld updates , we begin by describing the gibbs sampler , a particular type of monte carlo markov chain algorithm , for this partic - ular model .
the basic step of a gibbs sampler is to choose a node s v randomly , and then to update the state of the associated random vari - able according to the conditional probability with neighboring states xed .
more precisely , denoting by n ( s ) the neighbors of a node s v , n ( s ) denote the state of the neighbors of s at iteration n , and letting x the gibbs update for vertex s takes the form 123 if u ( 123 + exp ( ( s +
where u is a sample from a uniform distribution u ( 123 , 123 ) .
in a dense graph , such that the cardinality of the neighborhood set n ( s ) is large , we might attempt to invoke a law of large numbers
or some other concentration result for .
to the extent that such sums are concentrated , it might make sense to replace sample values with expectations .
that is , letting s denote an estimate of the marginal probability p ( xs = 123 ) at each vertex s v , we might consider the following averaged version of equation ( 123 ) :
( cid : 123 ) ( s +
123 + exp
thus , rather than ipping the random variable xs with a probability that depends on the state of its neighbors , we update a parameter s deterministically that depends on the corresponding parameters at its neighbors .
equation ( 123 ) denes the naive mean eld algorithm for the ising model .
as with the sum - product algorithm , the mean eld algorithm can be viewed as a message - passing algorithm , in which the right - hand side of ( 123 ) represents the message arriving at vertex s .
at rst sight , message - passing algorithms of this nature might seem rather mysterious , and do raise some questions .
do the updates have xed points ? do the updates converge ? what is the relation between the xed points and the exact quantities ? the goal of the rest of this survey is to shed some light on such issues .
ultimately , we will see that a broad class of message - passing algorithms , including the mean eld updates , the sum - product and max - product algorithms , as well as various extensions of these methods , can all be understood as solving either exact or approximate versions of variational problems .
exponen - tial families and convex analysis , which are the subject of the following section , provide the appropriate framework in which to develop these variational principles in an unied manner .
graphical models as exponential families
in this section , we describe how many graphical models are naturally viewed as exponential families , a broad class of distributions that have been extensively studied in the statistics literature ( 123 , 123 , 123 , 123 ) .
tak - ing the perspective of exponential families illuminates some fundamen - tal connections between inference algorithms and the theory of convex analysis ( 123 , 123 , 123 ) .
more specically , as we shall see , various types of inference problems in graphical models can be understood in terms of mappings between mean parameters and canonical parameters .
123 exponential representations via maximum entropy
one way in which to motivate exponential family representations of graphical models is through the principle of maximum entropy ( 123 , 123 ) .
here , so as to provide helpful intuition for our subsequent develop - ment , we describe a particularly simple version for a scalar random vari - able x .
suppose that given n independent and identically distributed ( i . i . d . ) observations x123 , .
, x n , we compute the empirical expectations of certain functions namely , the quantities
for all i ,
123 graphical models as exponential families where each in some set i indexes a function : x r .
for exam - ple , if we set 123 ( x ) = x and 123 ( x ) = x123 , then the observations ( 123 ) ical expectations ( cid : 123 ) = ( ( cid : 123 ) , i ) , our goal is to infer a full probability correspond to empirical versions of the rst and second moments of the random variable x .
based on the |i| - dimensional vector of empir -
distribution over the random variable x .
in particular , we represent the probability distribution as a density p absolutely continuous with respect to some measure .
this base measure might be the counting measure on ( 123 , 123 , .
, r 123 ) , in which case p is a probability mass func - tion; alternatively , for a continuous random vector , the base measure could be the ordinary lebesgue measure on r .
for a given distribution p ,
we say that the distribution p is consistent with the data if
let us consider the expectations x ( x ) p ( x ) ( dx ) for i , assuming that they exist .
ep ( ( x ) ) = ( cid : 123 )
for all i .
in words , the expectations ep ( ( x ) ) under the distribution p are matched to the expectations under the empirical distribution .
an important observation is that generically , this problem is under - determined , in that there are many distributions p that are consistent with the observations , so that we need a principle for choosing among
in order to develop such a principle , we begin by dening a func -
tional of the density p , known as the shannon entropy , via
( log p ( x ) ) p ( x ) ( dx ) .
the principle of maximum entropy is to choose , from among the dis - tributions consistent with the data , the distribution p entropy is maximal .
more formally , letting p be the set of all proba - bility distributions over the random variable x , the maximum entropy is given by the solution to the following constrained opti -
subject to ep ( ( x ) ) = ( cid : 123 )
: = arg max
for all i .
123 basics of exponential families
one interpretation of this principle is as choosing the distribution with maximal uncertainty , as measured by the entropy functional ( 123 ) , while remaining faithful to the data .
presuming that problem ( 123 ) is feasible ( and under certain technical conditions to be explored in the sequel ) , it can be shown by calculus of variations in the general continuous case , and by ordinary calculus in the discrete case that the optimal
takes the form
where rd represents a parameterization of the distribution in exponential family form .
from this maximum entropy perspective , the parameters have a very specic interpretation as the lagrange multipliers associated with the constraints specied by the empirical
moments ( cid : 123 ) .
we explore this connection in much more depth in the
123 basics of exponential families
with this motivating example in mind , let us now set up the framework of exponential families in more precise terms and greater generality .
at a high level , an exponential family is a parameterized family of densities , taken with respect to some underlying measure .
given a random vector ( x123 , x123 , .
, xm ) taking values in some space x m = m s=123xs , let = ( , i ) be a collection of functions : x m r , known either as potential functions or sucient statis - tics .
here i is an index set with d = |i| elements to be specied , so that can be viewed as a vector - valued mapping from x m to rd .
for a given vector of sucient statistics , let = ( , i ) be an asso - ciated vector of canonical or exponential parameters .
for each xed x x m , we use ( cid : 123 ) , ( x ) ( cid : 123 ) to denote the euclidean inner product in rd of the two vectors and ( x ) .
with this notation , the exponential family associated with consists
of the following parameterized collection of density functions
( cid : 123 ) ( cid : 123 ) , ( x ) ( cid : 123 ) a ( )
p ( x123 , x123 , .
, xm ) = exp
123 graphical models as exponential families
taken with respect123 to d .
the quantity a , known as the log partition function or cumulant function , is dened by the integral
exp ( cid : 123 ) , ( x ) ( cid : 123 ) ( dx ) .
a ( ) = log
presuming that the integral is nite , this denition ensures that p is x m p ( x ) ( dx ) = 123 ) .
with the set of poten - properly normalized ( i . e . , tials xed , each parameter vector indexes a particular member p of the family .
the canonical parameters of interest belong to the set we will see shortly that a is a convex function of , which in turn implies that must be a convex set .
the log partition function a plays a prominent role in this survey .
: = ( rd | a ( ) < + ) .
the following notions will be important in subsequent development : regular families : an exponential family for which the domain is an open set is known as a regular family .
although there do exist exponen - tial families for which is closed ( for instance , see brown ( 123 ) ) , herein we restrict our attention to regular exponential families .
minimal : it is typical to dene an exponential family with a vector of sucient statistics = ( , i ) for which there does not exist a nonzero vector a rd such that the linear combination
( cid : 123 ) a , ( x ) ( cid : 123 ) =
is equal to a constant ( - almost everywhere ) .
this condition gives rise to a so - called minimal representation , in which there is a unique param - eter vector associated with each distribution .
overcomplete : instead of a minimal representation , it can be conve - nient to use a nonminimal or overcomplete representation , in which there exist linear combinations ( cid : 123 ) a , ( x ) ( cid : 123 ) that are equal to a constant ( - a . e . ) .
in this case , there exists an entire ane subset of parameter vectors , each associated with the same distribution .
the reader might question the utility of an overcomplete representation .
indeed , from a statistical perspective , it can be undesirable since the identiability of 123 more precisely , for any measurable set s , we have p ( x s ) =
123 examples of graphical models in exponential form 123
the parameter vector is lost .
however , this notion of overcompleteness is useful in understanding the sum - product algorithm and its variants ( see section 123 ) .
table 123 provides some examples of well - known scalar exponential families .
observe that all of these families are both regular ( since is open ) , and minimal ( since the vector of sucient statistics does not satisfy any linear relations ) .
123 examples of graphical models in exponential form
the scalar examples in table 123 serve as building blocks for the con - struction of more complex exponential families for which graphical structure plays a role .
whereas earlier we described graphical models in terms of products of functions , as in equations ( 123 ) and ( 123 ) , these products become additive decompositions in the exponential family set - ting .
here , we discuss a few well - known examples of graphical models as exponential families .
those readers familiar with such formulations may skip directly to section 123 , where we continue our general discus - sion of exponential families .
example 123 ( ising model ) .
the ising model physics ( 123 , 123 ) is a classical example of a graphical model in expo - nential form .
consider a graph g = ( v , e ) and suppose that the ran - dom variable xs associated with node s v is bernoulli , say taking the spin values ( 123 , +123 ) .
in the context of statistical physics , these values might represent the orientations of magnets in a eld , or the presence / absence of particles in a gas .
the ising model and varia - tions on it have also been used in image processing and spatial statis - tics ( 123 , 123 , 123 ) , where xs might correspond to pixel values in a black - and - white image , and for modeling of social networks , for instance the voting patterns of politicians ( 123 ) .
components xs and xt of the full random vector x are allowed to interact directly only if s and t are joined by an edge in the graph .
this set - up leads to an exponential family consisting of the densities
p ( x ) = exp
123 graphical models as exponential families
123 examples of graphical models in exponential form 123 where the base measure is the counting measure123 restricted to ( 123 , 123 ) m .
here st r is the strength of edge ( s , t ) , and s r is a potential for node s , which models an external eld in statistical physics , or a noisy observation in spatial statistics .
strictly speaking , the family of densities ( 123 ) is more general than the classical ising model , in which st is constant for all edges .
as an exponential family , the ising index set i consists of the union v e , and the dimension of the family is d = m + |e| .
the log parti - tion function is given by the sum
a ( ) = log
since this sum is nite for all choices of rd , the domain is the full space rd , and the family is regular .
moreover , it is a minimal represen - tation , since there is no nontrivial linear combination of the potentials equal to a constant - a . e .
the standard ising model can be generalized in a number of dif - ferent ways .
although equation ( 123 ) includes only pairwise inter - actions , higher - order interactions among the random variables can also be included .
for example , in order to include coupling within the 123 - clique ( s , t , u ) , we add a monomial of the form xsxtxu , with corresponding canonical parameter stu , to equation ( 123 ) .
more gen - erally , to incorporate coupling in k - cliques , we can add monomi - als up to order k , which lead to so - called k - spin models in the statistical physics literature .
at the upper extreme , taking k = m amounts to connecting all nodes in the graphical model , which allows one to represent any distribution over a binary random vector
example 123 ( metric labeling and potts model ) .
here , we con - sider another generalization of the ising model : suppose that the ran - dom variable xs at each node s v takes values in the discrete space 123 explicitly , for each singleton set ( x ) , this counting measure is dened by ( ( x ) ) = 123 if x ( 123 , 123 ) m and ( ( x ) ) = 123 otherwise , and extended to arbitrary sets by sub - additivity .
123 graphical models as exponential families x : = ( 123 , 123 , .
, r 123 ) , for some integer r > 123
one interpretation of a state j x is as a label , for instance dening membership in an image segmentation problem .
each pairing of a node s v and a state j x yields a sucient statistic
if xs = j
i s;j ( xs ) =
with an associated vector s = ( s;j , j = 123 , .
, r 123 ) of canoni - for each edge ( s , t ) and pair of values cal parameters .
moreover , ( j , k ) x x , dene the sucient statistics
and xt = k ,
if xs = j
i st;jk ( xs , xt ) =
jx i s;j ( xs ) = 123 for all xs x .
as well as the associated parameter st;jk r .
viewed as an exponential family , the chosen collection of sucient statistics denes an exponen - tial family with dimension d = r|v | + r123|e| .
like the ising model , the log partition function is everywhere nite , so that the family is regular .
in contrast to the ising model , however , the family is overcomplete : indeed , the sucient statistics satisfy various linear relations for a special case of this model is the metric labeling problem , in which a metric : x x ( 123 , ) species the parameterization that is , st;jk = ( j , k ) for all ( j , k ) x x .
consequently , the canoni - cal parameters satisfy the relations st;kk = 123 for all k x , st;jk < 123 for all j ( cid : 123 ) = k , and satisfy the reversed triangle inequality ( that is , st;j ( cid : 123 ) st;jk + st;k ( cid : 123 ) for all triples ( j , k , ( cid : 123 ) ) ) .
another special case is the potts model from statistical physics , in which case st;kk = for all k x , and st;jk = for all j ( cid : 123 ) = k .
we now turn to an important class of graphical models based on con - tinuous random variables :
example 123 ( gaussian mrf ) .
given an undirected graph g with vertex set v = ( 123 , .
, m ) , a gaussian markov random eld ( 123 ) con - sists of a multivariate gaussian random vector ( x123 , .
, xm ) that
123 examples of graphical models in exponential form 123
123 ( a ) undirected graphical model on ve nodes .
( b ) for a gaussian markov random eld , the zero pattern of the inverse covariance or precision matrix respects the graph structure : for any pair i ( cid : 123 ) = j , if ( i , j ) / e , then ij = 123
respects the markov properties of g ( see section 123 ) .
it can be represented in exponential form using the collection of sucient statis - s , s v ; xsxt , ( s , t ) e ) .
we dene an m - vector of param - tics ( xs , x123 eters associated with the vector of sucient statistics x = ( x123 , .
, xm ) , and a symmetric matrix rmm associated with the matrix xxt .
concretely , the matrix is the negative of the inverse covariance or pre - cision matrix , and by the hammersleycliord theorem ( 123 , 123 , 123 ) , it has the property that st = 123 if ( s , t ) / e , as illustrated in fig - ure 123 .
consequently , the dimension of the resulting exponential family is d = 123m + |e| .
with this set - up , the multivariate gaussian is an exponential fam -
ily123 of the form :
( cid : 123 ) , x ( cid : 123 ) +
( cid : 123 ) ( cid : 123 ) , xxt ( cid : 123 ) ( cid : 123 ) a ( , )
p ( x ) = exp
( cid : 123 ) ( cid : 123 ) , xxt ( cid : 123 ) ( cid : 123 ) : = trace ( xxt ) =
where ( cid : 123 ) , x ( cid : 123 ) : =
i=123 ixi is the euclidean inner product on rm , and
is the frobenius inner product on symmetric matrices .
the integral dening a ( , ) is nite only if 123 , so that
= ( ( , ) rm rmm | 123 , = t ) .
123 our inclusion of the 123
123 - factor in the term 123
123 ( cid : 123 ) ( cid : 123 ) , xxt ( cid : 123 ) ( cid : 123 ) is for later technical convenience .
123 graphical models as exponential families
123 ( a ) graphical representation of a nite mixture of gaussians : xs is a multino - mial label for the mixture component , whereas ys is conditionally gaussian given xs = j , with gaussian density ps ( ys | xs ) in exponential form .
( b ) coupled mixture - of - gaussian graphical model , in which the vector x = ( x123 , .
, xm ) are a markov random eld on an undirected graph , and the elements of ( y123 , .
, ym ) are conditionally independent given x .
graphical models are not limited to cases in which the random variables at each node belong to the same exponential family .
more generally , we can consider heterogeneous combinations of exponential family members , as illustrated by the following examples .
example 123 ( mixture models ) .
as shown in figure 123 ( a ) , a scalar mixture model has a very simple graphical interpretation .
con - cretely , in order to form a nite mixture of gaussians , let xs be a multinomial variable , taking values in ( 123 , 123 , .
, r 123 ) .
the role of xs is to specify the choice of mixture component , so that our mixture model has r components in total .
as in the potts model described in example 123 , the distribution of this random variable is exponential family with sucient statistics ( i j ( xs ) , j = 123 , .
, r 123 ) , and associated canonical parameters ( s;123 , .
, s;r123 ) .
in order to form the nite mixture , conditioned on xs = j , we now let ys be conditionally gaussian with mean and variance ( j , 123 j ) .
each such conditional distribution can be written in exponential family form ) , with an associated pair of in terms of the sucient statistics ( ys , y123 ) .
overall , we obtain the canonical parameters ( s;j , exponential family with the canonical parameters ( cid : 123 ) s;123 , .
, s;r123
, s;r123 ,
s;j ) : = ( j
123 examples of graphical models in exponential form 123
and a density of the form :
ps ( ys , xs ) = p ( xs ) ps ( ys| xs )
s;ji j ( xs ) +
serves a basic block for building more sophisticated graphical models , suitable for describing multivariate data ( ( x123 , y123 ) , .
, ( xm , ym ) ) .
for instance , suppose that the vector x = ( x123 , .
, xm ) of multinomial variates is modeled as a markov ran - dom eld ( see example 123 on the potts model ) with respect to some underlying graph g = ( v , e ) , and the variables ys are conditionally independent given ( x = x ) .
these assumptions lead to an exponential family p ( y , x ) with parameters = ( , ) , and density p ( x ) p ( y | x ) exp
ps ( ys | xs ) ,
corresponding to a product of the potts - type distribution p ( x ) over x , and the local conditional distributions ps ( ys| xs ) .
here , we have used s ( xs ) as a shorthand for the exponential family representation j=123 s;ji j ( xs ) , and similarly for the quantity st ( xs , xt ) ; see exam -
ple 123 where this notation was used .
whereas the mixture model just described is a two - level hierarchy , the following example involves three distinct levels :
example 123 ( latent dirichlet allocation ) .
the latent dirichlet allocation model ( 123 ) is a particular type of hierarchical bayes model for capturing the statistical dependencies among words in a corpus of documents .
it involves three dierent types of random variables : docu - ments u , topics z , and words w .
words are multinomial random variables ranging over some vocabulary .
topics are also multinomial random variables .
associated to each value of the topic variable there is a distribution over words .
a document is a distribution over topics .
123 graphical models as exponential families
finally , a corpus is dened by placing a distribution on the documents .
for the latent dirichlet allocation model , this latter distribution is a more formally , words w are drawn from a multinomial distribu - tion , p ( w = j| z = i; ) = exp ( ij ) , for j = 123 , 123 , .
, k 123 , where ij is a parameter encoding the probability of the jth word under the ith topic .
this conditional distribution can be expressed as an exponential family in terms of indicator functions as follows :
p ( w | z ) exp
iji i ( z ) i j ( w )
where i i ( z ) is an ( 123 , 123 ) - valued indicator for the event ( z = i ) , and similarly for i j ( w ) .
at the next level of the hierarchy ( see figure 123 ) , the topic variable z also follows a multinomial distribution whose para - meters are determined by the dirichlet variable as follows :
p ( z | u ) exp
i i ( z ) log ui
the top level of
form p ( u ) exp ( ( cid : 123 )
the dirichlet vari - able u has a density with respect to lebesgue measure of the i=123 i log ui ) .
overall then , for a single triplet x : = ( u , z , w ) , the lda model is an exponential family with param - eter vector : = ( , ) , and an associated density of the form :
i log ui +
i i ( z ) log ui
iji i ( z ) i j ( w )
p ( u ) p ( z | u ) p ( w | z ) exp
the sucient statistics consist of the collections of functions ( log ui ) , ( i i ( z ) log ui ) , and ( i i ( z ) i j ( w ) ) .
as illustrated in figure 123 , the full lda model entails replicating these types of local structures many times .
many graphical models include what are known as hard - core constraints , meaning that a subset of congurations are forbidden .
123 examples of graphical models in exponential form 123
123 graphical illustration of the latent dirichlet allocation ( lda ) model .
the word variable w is multinomial conditioned on the underlying topic z , where species the topic distributions .
the topics z are also modeled as multinomial variables , with distributions parameterized by a probability vector u that follows a dirichlet distribution with parameter .
this model is an example of a hierarchical bayesian model .
the rectangles , known as plates , denote replication of the random variables .
instances of such problems include decoding of binary linear codes , and other combinatorial optimization problems ( e . g . , graph matching , cov - ering , packing , etc . ) .
at rst glance , it might seem that such families of distributions cannot be represented as exponential families , since the density p in any exponential family is strictly positive that is , p ( x ) > 123 for all x x m .
in the following example , we show that these hard - core constraints can be incorporated within the exponential family framework by making appropriate use of the underlying base
example 123 ( models with hard constraints ) .
one important domain in which hard - core constraints arise is communication theory , and in particular the problem of error - control coding ( 123 , 123 , 123 ) .
to motivate the coding problem , suppose that two people following an old convention , let us call them alice and bob wish to communicate .
we assume that they can communicate by transmitting a sequence of 123s and 123s , but make the problem interesting by assuming that the communication channel linking them behaves in a random manner .
concretely , in a bit - ipping channel , any binary digit transmitted by alice is received correctly by bob only with probability 123 .
as a probabilistic model , this channel can be modeled by the conditional
p ( y | x ) : =
if x = y if x ( cid : 123 ) = y ,
123 graphical models as exponential families where x ( 123 , 123 ) represents the bit transmitted by alice , and y ( 123 , 123 ) represents the bit received by bob .
in order to mitigate the noisiness of the channel , alice and bob agree on the following block coding scheme : instead of transmitting a single bit , they communicate using strings ( x123 , x123 , .
, xm ) of bits , and moreover , they agree to use only a subset of the total number 123m of length m binary strings .
these privileged strings , which are known as codewords , can be dened using a particular type of graphical model .
concretely , suppose that alice decides to choose from among strings x ( 123 , 123 ) m that satisfy a set of parity checks say of the form x123 x123 x123 = 123 , where denotes addition in modulo two arithmetic .
let us consider a collection f of such parity checks; each a f enforces a parity check constraint on some subset n ( a ) ( 123 , .
, m ) of bits .
if we dene the indicator function
in ( a ) xi = 123
( x123 , x123 , .
, xm ) ( 123 , 123 ) m for which af a ( xn ( a ) ) = 123
these constraints are known as hard - core , since they take the value 123 for some settings of x , and hence eliminate such congurations from the support of the distribution .
letting ( y123 , .
, ym ) ( 123 , 123 ) m denote the string of bits received by bob , his goal is to use these observations to infer which codeword was transmitted by alice .
depending on the error metric used , this decoding problem corresponds to either computing marginals or modes of the posterior distribution
p ( x123 , .
, xm | y123 , .
, ym ) m ( cid : 123 )
p ( yi | xi )
this distribution can be described by a factor graph , with the bits xi represented as unshaded circular variable nodes , the observed values yi as shaded circular variable nodes , and the parity check indicator functions represented at the square factor nodes ( see figure 123 ) .
the code can also be viewed as an exponential family on this graph , if we take densities with respect to an appropriately chosen base mea - sure .
in particular , let us use the parity checks to dene the counting
measure restricted to valid codewords
123 mean parameterization and inference problems
moreover , since the quantities yi are observed , they may be viewed as xed , so that the conditional distribution p ( yi | xi ) can be viewed as some function q ( ) of xi only .
a little calculation shows that we can write this function in exponential form as
q ( xi; i ) = exp ( ixi log ( 123 + exp ( i ) ) ) ,
is dened by the obser - where the exponential parameter i vation yi and conditional distribution p ( yi | xi ) via the relation i = log p ( yi | 123 ) / p ( yi | 123 ) .
in the particular case of the binary symmet - ric channel , these canonical parameters take the form
i = ( 123yi 123 ) log
note that we are using the fact that the vector ( y123 , y123 , .
, ym ) is observed , and hence can be viewed as xed .
with this set - up , the dis - tribution ( 123 ) can be cast as an exponential family , where the density is taken with respect to the restricted counting measure ( 123 ) , and has the form p ( x | y ) exp
123 mean parameterization and inference problems
thus far , we have characterized any exponential family member p by its vector of canonical parameters .
as we discuss in this section , it turns out that any exponential family has an alternative parame - terization in terms of a vector of mean parameters .
moreover , vari - ous statistical computations , among them marginalization and maxi - mum likelihood estimation , can be understood as transforming from one parameterization to the other .
we digress momentarily to make an important remark regarding the role of observed variables or conditioning for the marginalization problem .
as discussed earlier , applications of graphical models fre - quently involve conditioning on a subset of random variables say
123 graphical models as exponential families
y that represent observed quantities , and computing marginals under the posterior distribution p ( x | y ) .
the application to error - correcting coding , just discussed in example 123 , is one such example .
so as to simplify notation in our development to follow , it is convenient to no longer make direct reference to observed variables , but rather discuss marginalization and other computational problems only in reference to unconditional forms of exponential families .
for such computational purposes , there is no loss of generality in doing so , since the eects of observations can always be absorbed by modifying the canonical parameters and / or the sucient statistics appropriately .
( for instance , in example 123 , the noisy observation yi at node i con - tributes a new factor exp ( ixi ) to the exponential family factorization , as described in equation ( 123 ) . )
123 . 123 mean parameter spaces and marginal polytopes
let p be a given density dened with respect to the underlying base measure ; for the moment , we do not assume that p is a member of an exponential family dened with respect to .
the mean parameter associated with a sucient statistic : x m r is dened by the
= ep ( ( x ) ) =
in this way , we dene a vector of mean parameters ( 123 , .
, d ) , one for each of the |i| = d sucient statistics , with respect to an arbitrary density p .
an interesting object is the set of all such vectors rd traced out as the underlying density p is varied .
more formally , we dene the set
m : = ( rd | p s . t .
ep ( ( x ) ) = i ) ,
corresponding to all realizable mean parameters .
it is important to note that in this denition , we have not restricted the density p to the exponential family associated with the sucient statistics and base measure .
however , it turns out that under suitable technical conditions , this vector provides an alternative parameterization of this
123 mean parameterization and inference problems
we illustrate these concepts with a continuation of a previous
example 123 ( gaussian mrf mean parameters ) .
using the canonical parameterization of the gaussian markov random eld pro - vided in example 123 , the mean parameters for a gaussian markov ran - dom eld are the second - order moment matrix : = e ( xx t ) rmm , and the mean vector = e ( x ) rm .
for this particular model , it is straightforward to characterize the set m of globally realizable mean parameters ( , ) .
we begin by recognizing that if ( , ) are realized by some distribution ( not necessarily gaussian ) , then t must be a valid covariance matrix of the random vector x , implying that the positive semideniteness ( psd ) condition t ( cid : 123 ) 123 must hold .
conversely , any pair ( , ) for which the psd constraint holds , we may construct a multivariate gaussian distribution with mean , and ( pos - sibly degenerate ) covariance t , which by construction realizes ( , ) .
thus , we have established that for a gaussian markov random eld , the set m has the form
m = ( ( , ) rm s m
+ | t ( cid : 123 ) 123 ) ,
where s m + denotes the set of m m symmetric positive semidenite matrices .
figure 123 illustrates this set in the scalar case ( m = 123 ) ,
123 illustration of the set m for a scalar gaussian : the model has two mean parameters 123 123
note = e ( x ) and 123 = e ( x that the set m is convex , which is a general property .
123 ) , which must satisfy the quadratic constraint 123
123 graphical models as exponential families
where the mean parameters = e ( x ) and 123 = e ( x123 ) must satisfy the quadratic constraint 123 123 123
the set m is always a convex subset of rd .
indeed , if and both elements of m , then there must exist distributions p and p ( cid : 123 ) = ep ( cid : 123 ) ( ( x ) ) .
for any realize them , meaning that = ep ( ( x ) ) and ( 123 , 123 ) , the convex combination ( ) : = + ( 123 ) ( cid : 123 ) is realized by the mixture distribution p + ( 123 ) p ( cid : 123 ) , so that ( ) also belongs to m .
in appendix b . 123 , we summarize further properties of m that hold for general exponential families .
the case of discrete random variables yields a set m with some spe - cial properties .
more specically , for any random vector ( x123 , x123 , .
xm ) such that the associated state space x m is nite , we have the
rd | =
for some p ( x ) 123 ,
= conv ( ( x ) , x x m ) ,
p ( x ) = 123
where conv denotes the convex hull operation ( see appendix a . 123 ) .
con - sequently , when |x m| is nite , the set m is by denition a convex
the minkowskiweyl theorem ( 123 ) , stated in appendix a . 123 , pro - vides an alternative description of a convex polytope .
as opposed to the convex hull of a nite collection of vectors , any polytope m can also be characterized by a nite collection of linear inequality constraints .
explicitly , for any polytope m , there exists a collection ( ( aj , bj ) rd r | j j ) with |j | nite such that m = ( rd | ( cid : 123 ) aj , ( cid : 123 ) bj j j ) .
in geometric terms , this representation shows that m is equal to the intersection of a nite collection of half - spaces , as illustrated in fig - ure 123 .
let us show the distinction between the convex hull ( 123 ) and linear inequality ( 123 ) representations using the ising model .
123 mean parameterization and inference problems
123 generic illustration of m for a discrete random variable with |x m| nite .
in this case , the set m is a convex polytope , corresponding to the convex hull of ( ( x ) | x x m ) .
by the minkowskiweyl theorem , this polytope can also be written as the intersection of a nite number of half - spaces , each of the form ( rd | ( cid : 123 ) aj , ( cid : 123 ) bj ) for some pair ( aj , bj ) rd r .
example 123 ( ising mean parameters ) .
continuing from exam - ple 123 , the sucient statistics for the ising model are the singleton functions ( xs , s v ) and the pairwise functions ( xsxt , ( s , t ) e ) .
the vector of sucient statistics takes the form :
( cid : 123 ) r|v |+|e|
xs , s v ; xsxt , ( s , t ) e
the associated mean parameters correspond to particular marginal probabilities , associated with nodes and edges of the graph g as
for all s v , and
for all ( s , t ) e .
s = ep ( xs ) = p ( xs = 123 ) st = ep ( xsxt ) = p ( ( xs , xt ) = ( 123 , 123 ) )
consequently , the mean parameter vector r|v |+|e| consists of marginal probabilities over singletons ( s ) , and pairwise marginals over variable pairs on graph edges ( st ) .
the set m consists of the convex hull of ( ( x ) , x ( 123 , 123 ) m ) , where is given in equation ( 123 ) .
in probabilistic terms , the set m corresponds to the set of all singleton and pairwise marginal probabilities that can be realized by some distribution over ( x123 , .
, xm ) ( 123 , 123 ) m .
in the polyhedral combinatorics literature , this set is known as the correlation polytope , or the cut polytope ( 123 , 123 ) .
123 graphical models as exponential families
to make these ideas more concrete , consider the simplest nontrivial case : namely , a pair of variables ( x123 , x123 ) , and the graph consisting of the single edge joining them .
in this case , the set m is a polytope in three dimensions ( two nodes plus one edge ) : it is the convex hull of the vectors ( ( x123 , x123 , x123x123 ) | ( x123 , x123 ) ( 123 , 123 ) 123 ) , or more explicitly
as illustrated in figure 123 .
let us also consider the half - space representation ( 123 ) for this case .
elementary probability theory and a little calculation shows that the three mean parameters ( 123 , 123 , 123 ) must satisfy the constraints 123 123 i for i = 123 , 123 and 123 + 123 123 123 123
we can write these constraints in matrix - vector form as
these four constraints provide an alternative characterization of the 123d polytope illustrated in figure 123 .
123 illustration of m for the special case of an ising model with two variables ( x123 , x123 ) ( 123 , 123 ) 123
the four mean parameters 123 = e ( x123 ) , 123 = e ( x123 ) and 123 = e ( x123x123 ) must satisfy the constraints 123 123 i for i = 123 , 123 , and 123 + 123 123 123 123
these constraints carve out a polytope with four facets , contained within the unit hypercube
123 mean parameterization and inference problems
our next example deals with a interesting family of polytopes that
arises in error - control coding and binary matroid theory :
example 123 ( codeword polytopes and binary matroids ) .
recall the denition of a binary linear code from example 123 : it corresponds to the subset of binary strings x ( 123 , 123 ) m that satisfy a set of parity check relations
x ( 123 , 123 ) m |
a ( xn ( a ) ) = 123
n ( a ) ( 123 , 123 , .
, m ) of bits .
viewed as an exponential the sucient statistics are simply ( x ) = ( x123 , x123 , .
, xm ) , the base measure is counting measure restricted to the set c .
consequently , the set m for this problem corresponds to the codeword polytope namely , the convex hull of all possible codewords
m = conv
x ( 123 , 123 ) m |
= conv ( x c ) .
a ( xn ( a ) ) = 123
to provide a concrete illustration , consider the code on m = 123 bits , dened by the single parity check relation x123 x123 x123 = 123
figure 123 ( a ) shows the factor graph representation of this toy code .
this parity check eliminates half of the 123 = 123 possible binary sequences of length 123
the codeword polytope is simply the convex hull of ( ( 123 , 123 , 123 ) , ( 123 , 123 , 123 ) , ( 123 , 123 , 123 ) , ( 123 , 123 , 123 ) ) , as illustrated in figure 123 ( b ) , equivalently , we can represent this codeword polytope in terms of half - space constraints .
for this single parity check code , the codeword poly - tope is dened by the four inequalities
( 123 123 ) + ( 123 123 ) + ( 123 123 ) 123 , ( 123 123 ) + 123 + 123 123 , 123 + ( 123 123 ) + 123 123 , 123 + 123 + ( 123 123 ) 123
123 graphical models as exponential families
123 illustration of m for
the binary linear c = ( ( 123 , 123 , 123 ) , ( 123 , 123 , 123 ) , ( 123 , 123 , 123 ) , ( 123 , 123 , 123 ) ) .
this code is dened by a single parity check .
as we discuss at more length in section 123 . 123 , these inequalities can be understood as requiring that ( 123 , 123 , 123 ) is at least distance 123 from each of the forbidden odd - parity vertices of the hypercube ( 123 , 123 ) 123
of course , for larger code lengths m and many parity checks , the associated codeword polytope has a much more complicated structure .
it plays a key role in the error - control decoding problem ( 123 ) , studied intensively in the coding and information theory communities .
any binary linear code can be identied with a binary matroid ( 123 ) , in which context the codeword polytope is referred to as the cycle polytope of the binary matroid .
there is a rich literature in combinatorics on the structure of these codeword or cycle polytopes ( 123 , 123 , 123 , 123 ) .
examples 123 and 123 are specic instances of a more general object that we refer to as the marginal polytope for a discrete graphical model .
the marginal polytope is dened for any graphical model with multinomial random variables xs xs : = ( 123 , 123 , .
, rs 123 ) at each ver - tex s v ; note that the cardinality |xs| = rs can dier from node to node .
consider the exponential family dened in terms of ( 123 , 123 ) - valued
s v , j xs ,
i s;j ( xs ) : =
( s , t ) e , ( j , k )
i st;jk ( xs , xt ) : =
if xs = j
if ( xs , xt ) = ( j , k )
123 mean parameterization and inference problems
we refer to the sucient statistics ( 123 ) as the standard overcom - plete representation .
its overcompleteness was discussed previously in very intuitive form : in particular , for each node s v s;j = ep ( i j ( xs ) ) = p ( xs = j ) j xs ,
with this choice of sucient statistics , the mean parameters take a
and for each edge ( s , t ) e , we have st;jk = ep ( i st;jk ( xs , xt ) ) = p ( xs = j , xt = k ) ( j , k ) xs xt .
thus , the mean parameters correspond to singleton marginal distribu - tions s and pairwise marginal distributions st associated with the nodes and edges of the graph .
in this case , we refer to the set m as the marginal polytope associated with the graph , and denote it by m ( g ) .
explicitly , it is given by m ( g ) : = ( rd | p such that ( 123 ) holds ( s; j ) , and ( 123 ) holds ( st; jk
note that the correlation polytope for the ising model presented in example 123 is a special case of a marginal polytope , obtained for xs ( 123 , 123 ) for all nodes s .
the only dierence is we have dened marginal polytopes with respect to the standard overcomplete basis of indicator functions , whereas the ising model is usually parameterized as a minimal exponential family .
the codeword polytope of example 123 is another special case of a marginal polytope .
in this case , the reduction requires two steps : rst , we convert the factor graph representation of the code for instance , as shown in figure 123 ( a ) to an equiva - lent pairwise markov random eld , involving binary variables at each bit node , and higher - order discrete variables at each factor node .
( see appendix e . 123 for details of this procedure for converting from factor graphs to pairwise mrfs . ) the marginal polytope associated with this pairwise mrf is simply a lifted version of the codeword polytope .
we discuss these and other examples of marginal polytopes in more detail in later sections .
123 graphical models as exponential families
for the toy models considered explicitly in examples 123 and 123 , the number of half - space constraints |j | required to characterize the marginal polytopes was very small ( |j | = 123 in both cases ) .
it is natu - ral to ask how the number of constraints required grows as a function of the graph size .
interestingly , we will see later that this so - called facet complexity depends critically on the graph structure .
for trees , any marginal polytope is characterized by local constraints involving only pairs of random variables on edges with the total number grow - ing only linearly in the graph size .
in sharp contrast , for general graphs with cycles , the constraints are very nonlocal and the growth in their number is astonishingly fast .
for the special case of the ising model , the book by deza and laurent ( 123 ) contains a wealth of information about the correlation / cut polytope .
the intractability of representing marginal polytopes in a compact manner is one underlying cause of the complexity in performing statistical computation .
123 . 123 role of mean parameters in inference problems
the preceding examples suggest that mean parameters have a cen - tral role to play in the marginalization problem .
for the multivariate gaussian ( example 123 ) , an ecient algorithm for computing the mean parameterization provides us with both the gaussian mean vector , as well as the associated covariance matrix .
for the ising model ( see exam - ple 123 ) , the mean parameters completely specify the singleton and pairwise marginals of the probability distribution; the same statement holds more generally for the multinomial graphical model dened by the standard overcomplete parameterization ( 123 ) .
even more broadly , the computation of the forward mapping , from the canonical parameters to the mean parameters m , can be viewed as a fundamen - tal class of inference problems in exponential family models .
although computing the mapping is straightforward for most low - dimensional families , the computation of this forward mapping is extremely dicult for many high - dimensional exponential families .
the backward mapping , namely from mean parameters m to canonical parameters , also has a natural statistical interpretation .
in particular , suppose that we are given a set of samples 123 : = ( x123 , .
, x n ) , drawn independently from an exponential family
123 properties of a 123
log p ( x i ) = ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) a ( ) ,
member p ( x ) , where the parameter is unknown .
if the goal is to esti - mate , the classical principle of maximum likelihood dictates obtaining
an estimate ( cid : 123 ) by maximizing the likelihood of the data , or equivalently
( after taking logarithms and rescaling ) , maximizing the quantity
( cid : 123 ) ( ; x n
123 ) : =
parameters dened by the data x n
i=123 ( x i ) is the vector of empirical mean 123 .
the maximum likelihood estimate
where ( cid : 123 ) : = ( cid : 123 ) e ( ( x ) ) = 123 ( cid : 123 ) is chosen to achieve the maximum of this objective function .
note that computing ( cid : 123 ) is , in general , another challenging problem , since tionarity condition e ( cid : 123 ) ( ( x ) ) = ( cid : 123 ) .
finding the unique solution to this
the objective function involves the log partition function a .
as will be demonstrated by the development to follow , under suitable conditions , the maximum likelihood estimate is unique , and specied by the sta - equation is equivalent to computing the backward mapping ( cid : 123 ) , from mean parameters to canonical parameters .
in general , computing this inverse mapping is also computationally intensive .
123 properties of a
with this background in mind , we now turn to a deeper exploration of the properties of the cumulant function a .
perhaps the most important property of a is its convexity , which brings us into convex analysis , and more specically leads us to conjugate duality .
under suitable or more pre - conditions , the function a and its conjugate dual a cisely , their derivatives turn out to dene a one - to - one and sur - jective mapping between the canonical and mean parameters .
as dis - cussed above , the mapping between canonical and mean parameters is a core challenge that underlies various statistical computations in high - dimensional graphical models .
123 . 123 derivatives and convexity
recall that a real - valued function g is convex if , for any two x , y belong - ing to the domain of g and any scalar ( 123 , 123 ) , the inequality
g ( x + ( 123 ) y ) g ( x ) + ( 123 ) g ( y )
123 graphical models as exponential families
in geometric terms , this inequality means that the line con - necting the function values g ( x ) and g ( y ) lies above the graph of the function itself .
the function is strictly convex if the inequality ( 123 ) is strict for all x ( cid : 123 ) = y .
( see appendix a . 123 for some additional properties of convex functions . ) we begin by establishing that the log partition function is smooth , and convex in terms of .
proposition 123 .
the cumulant function
a ( ) : = log
exp ( cid : 123 ) , ( x ) ( cid : 123 ) ( dx )
associated with any regular exponential
family has the following
( a ) it has derivatives of all orders on its domain .
the rst two derivatives yield the cumulants of the random vector ( x ) as follows :
( ) = e ( ( x ) ) : = ( ) = e ( ( x ) ( x ) ) e ( ( x ) ) e ( ( x ) ) .
( b ) moreover , a is a convex function of on its domain , and
strictly so if the representation is minimal .
let us assume that dierentiating through the integral ( 123 ) dening a is permitted; verifying the validity of this assumption is a standard argument using the dominated convergence theorem ( e . g . , brown ( 123 ) ) .
under this assumption , we have
exp ( cid : 123 ) , ( x ) ( cid : 123 ) ( dx ) exp ( cid : 123 ) , ( x ) ( cid : 123 ) ( dx ) x m exp ( cid : 123 ) , ( u ) ( cid : 123 ) ( du )
123 properties of a 123
exp ( cid : 123 ) , ( x ) ( cid : 123 ) ( dx ) x m exp ( cid : 123 ) , ( u ) ( cid : 123 ) ( du )
which establishes equation ( 123a ) .
the formula for the higher - order derivatives can be proven in an entirely analogous manner .
observe from equation ( 123b ) that the second - order partial deriva - is equal to the covariance element cov ( ( x ) , ( x ) ) .
therefore , the full hessian 123a ( ) is the covariance matrix of the random vector ( x ) , and so is positive semidenite on the open set , which ensures convexity ( see theorem 123 . 123 of hiriart - urruty and lemarechal ( 123 ) ) .
if the representation is minimal , there is no nonzero vector a rd and constant b r such that ( cid : 123 ) a , ( x ) ( cid : 123 ) = b holds - a . e .
this condition implies var ( ( cid : 123 ) a , ( x ) ( cid : 123 ) ) = at123a ( ) a > 123 for all a rd and ; this strict positive deniteness of the hessian on the open set implies strict convexity ( 123 ) .
123 . 123 forward mapping to mean parameters
we now turn to an in - depth consideration of the forward mapping ( cid : 123 ) , from the canonical parameters dening a distribution p to its associated vector of mean parameters rd .
note that the gradi - ent a can be viewed as mapping from to rd .
indeed , proposition 123 demonstrates that the range of this mapping is contained within the set m of realizable mean parameters , dened previously as
m : = ( rd | p s . t .
ep ( ( x ) ) = ) .
we will see that a great deal hinges on the answers to the following
( a ) when does a dene a one - to - one mapping ? ( b ) when does the image of under the mapping a that is , the set a ( ) fully cover the set m ?
the answer to the rst question is relatively straightforward , essen - tially depending on whether or not the exponential family is minimal .
the second question is somewhat more delicate : to begin , note that our
123 graphical models as exponential families denition of m allows for mean parameters rd generated by any possible distribution , not just distributions p in the exponential family dened by the sucient statistics .
it turns out that this extra free - dom does not really enlarge the set m; as theorem 123 makes precise , under suitable conditions , all mean parameters in m can be realized by an exponential family distribution ( or , for boundary points , by a limiting sequence of such distributions ) .
we begin with a result addressing the rst question : proposition 123 .
the gradient mapping a : m is one - to - one if and only if the exponential representation is minimal .
if the representation is not minimal , then there must exist a nonzero vector rd for which ( cid : 123 ) , ( x ) ( cid : 123 ) is a constant ( almost surely with respect to ) .
given any parameter 123 , let us dene another parameter vector 123 = 123 + t , where t r .
since is open , choosing t suciently small ensures that 123 as well .
by the condition on the vector , the densities p123 and p123 induce the same probability distribution ( only their normalization constants dier ) .
for this pair , we have a ( 123 ) = a ( 123 ) , so that a is not one - to - one .
conversely , if the representation is minimal , then a is strictly con - vex by proposition 123 .
for any strictly convex and dierentiable func - tion , we have a ( 123 ) > a ( 123 ) + ( cid : 123 ) a ( 123 ) , 123 123 ( cid : 123 ) , for all 123 ( cid : 123 ) = 123 in the domain .
the same inequality also holds with the roles of 123 and 123 reversed; adding together these inequalities yields that
( cid : 123 ) a ( 123 ) a ( 123 ) , 123 123 ( cid : 123 ) > 123
for all distinct 123 , 123 , which shows that a is one - to - one .
in general , although the gradient mapping a is not one - to - one for an overcomplete representation , there is still a one - to - one correspon - dence between each element of a ( ) and an ane subset of .
in particular , this ane subset contains all those canonical parameters that are mapped to the same mean parameter .
for either a minimal or an overcomplete representation , we say that a pair ( , ) is dually
123 properties of a 123 coupled if = a ( ) .
this notion of dual coupling plays an important role in the sequel .
we now turn to the second issue regarding the image a ( ) of the domain of valid canonical parameters under the gradient map - ping a .
specically , the goal is to determine for which mean parame - ter vectors m does there exist a vector = ( ) such that e ( ( x ) ) = .
the solution turns out to be rather simple : the image a ( ) is simply the interior m .
this fact is remarkable : it means that ( disregarding boundary points ) all mean parameters m that are realizable by some distribution can be realized by a member of the exponential family .
to provide some intuition into this fact , consider the maximum entropy problem ( 123 ) for a given mean parameter in the interior of m .
as discussed earlier , when a solution to this problem exists , it necessarily takes the form of an exponential fam - ily member , say p ( ) .
moreover , from the optimality conditions for the maximum entropy problem , this exponential family member must sat - isfy the moment - matching conditions e ( ) ( ( x ) ) = .
note that these moment - matching conditions are identical to those dening the maxi - mum likelihood problem ( 123 ) as we discuss in the following section , this fact is not coincidental , but rather a consequence of the primal dual relationship between maximum entropy and maximum likelihood .
in a minimal exponential family , the gradient map a is onto the interior of m , denoted by m .
consequently , for each m , there exists some = ( ) such that e ( ( x ) ) = .
we provide the proof of this result in appendix b .
in conjunction with proposition 123 , theorem 123 guarantees that for minimal expo - nential families , each mean parameter m is uniquely realized by some density p ( ) in the exponential family .
however , a typical expo - nential family ( p | ) describes only a strict subset of all possible densities ( with respect to the given base measure ) .
in this case , there must exist at least some other density p albeit not a member of an exponential family that also realizes .
the distinguishing property of the exponential distribution p ( ) is that , among the set of all dis - tributions that realize , it has the maximum entropy .
the connection
123 graphical models as exponential families
between a and the maximum entropy principle is specied precisely in terms of the conjugate dual function a
, to which we now turn .
123 conjugate duality : maximum likelihood and
conjugate duality is a cornerstone of convex analysis ( 123 , 123 ) , and is a natural source for variational representations .
in this section , we explore the relationship between the log partition function a and its .
this conjugate relationship is dened by a conjugate dual function a variational principle that is central to the remainder of this survey , in that it underlies a wide variety of known algorithms , both of an exact nature ( e . g . , the junction tree algorithm and its special cases of kalman ltering , the forwardbackward algorithm , peeling algorithms ) and an approximate nature ( e . g . , sum - product on graphs with cycles , mean eld , expectation - propagation , kikuchi methods , linear programming , and semidenite relaxations ) .
123 . 123 general form of conjugate dual
given a function a , the conjugate dual function to a , which we denote
, is dened as follows :
( ) : = sup
( ( cid : 123 ) , ( cid : 123 ) a ( ) ) .
here rd is a xed vector of so - called dual variables of the same dimension as .
our choice of notation i . e . , using again is deliberately suggestive , in that these dual variables turn out to have a natural interpretation as mean parameters .
indeed , we have already mentioned one statistical interpretation of this variational prob - lem ( 123 ) ; in particular , the right - hand side is the optimized value of the rescaled log likelihood ( 123 ) .
of course , this maximum likelihood problem only makes sense when the vector belongs to the set m; an i=123 ( x i ) induced 123 = ( x123 , .
, x n ) .
in our development , we consider by a set of data x n the optimization problem ( 123 ) more broadly for any vector rd .
in as a function taking values in the this context , it is necessary to view a
example is the vector of empirical moments ( cid : 123 ) = 123
123 conjugate duality : maximum likelihood and maximum entropy
extended real line r = r ( + ) , as is standard in convex analysis ( see appendix a . 123 for more details ) .
as we have previously intimated , the conjugate dual function ( 123 ) is very closely connected to entropy .
recall the denition ( 123 ) of the shannon entropy .
the main result of the following theorem is that when m , the value of the dual function a ( ) is precisely the negative entropy of the exponential family distribution p ( ) , where ( ) is the unique vector of canonical parameters satisfying the relation
e ( ) ( ( x ) ) = a ( ( ) ) = .
we will also nd it essential to consider / m , in which case it is impossible to nd canonical parameters satisfying the relation ( 123 ) .
in ( ) requires a more this case , the behavior of the supremum dening a delicate analysis .
in fact , denoting by m the closure of m , it turns out ( ) = + .
this fact is essential in the that whenever / m , then a use of variational methods : it guarantees that any optimization problem involving the dual function can be reduced to an optimization problem over m .
accordingly , a great deal of our discussion in the sequel will be on the structure of m for various graphical models , and various approx - imations to m for models in which its structure is overly complex .
more formally , the following theorem , proved in appendix b . 123 , provides a precise characterization of the relation between a and its conjugate
( a ) for any m , denote by ( ) the unique canonical parameter satisfying the dual matching condition ( 123 ) .
the conjugate dual function a
takes the form if / m .
( ) = lim converging to .
( n ) taken over any sequence ( n ) m
123 graphical models as exponential families
( b ) in terms of this dual , the log partition function has the
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) a
a ( ) = sup
( c ) for all , the supremum in equation ( 123 ) is attained uniquely at the vector m specied by the moment -
( x ) p ( x ) ( dx ) = e ( ( x ) ) .
theorem 123 ( a ) provides a precise statement of the duality between the cumulant function a and entropy .
a few comments on this relation - is a slightly ship are in order .
first , it is important to recognize that a dierent object than the usual entropy ( 123 ) : whereas the entropy maps density functions to real numbers ( and so is a functional ) , the dual func - is an extended real - valued function on rd , nite only for valid mean parameters m .
second , the value a ( ) corresponds to the optimum of the maxi - mum entropy problem ( 123 ) , where rd parameterizes the constraint ( ) = + corresponds to infeasibility of the maxi - set .
the event a mum entropy problem .
this is an important point .
constrained opti - mization problems are dened both by the set being optimized over and the function being optimized .
given that the variational representation of the cumulant function in ( 123 ) takes the form of a maximization ( ) = can certainly problem , we see that vectors for which a not be optima .
thus , it suces to maximize over the set m instead of rd , as expressed in the variational representation ( 123 ) .
this fact implies that the nature of the set m plays a critical role in determining the complexity of computing the cumulant function .
third , theorem 123 also claries the precise nature of the bijection between the sets and m , which holds for any minimal exponential family .
in particular , the gradient mapping a maps in a one - to - one manner onto m , whereas the inverse mapping from m to is given by the gradient a of the dual function ( see appendix b . 123 for more details ) .
figure 123 provides an idealized illustration of this bijective correspondence based on the gradient mappings ( a , a
123 conjugate duality : maximum likelihood and maximum entropy
123 idealized illustration of the relation between the set of valid canonical param - eters , and the set m of valid mean parameters .
the gradient mappings a and a
) provide a bijective mapping between and associated with the conjugate dual pair ( a , a the interior m .
123 . 123 some simple examples
theorem 123 is best understood by working through some simple examples .
table 123 provides the conjugate dual pair ( a , a several well - known exponential families of scalar random variables .
for each family , the table also lists : = dom a , as well as the set m , , corresponding to the set of which contains the eective domain of a values for which a
in the rest of this section , we illustrate the basic ideas by work - ing through two simple scalar examples in detail .
to be clear , neither of these examples is interesting from a computational perspective indeed , for most scalar exponential families , it is trivial to compute the mapping between canonical and mean parameters by direct methods .
nonetheless , they are useful in building intuition for the consequences of theorem 123 .
the reader interested only in the main thread may skip ahead to section 123 , where we resume our discussion of the role of theorem 123 in the derivation of approximate inference algorithms for multivariate exponential families .
example 123 ( conjugate duality for bernoulli ) .
consider a bernoulli variable x ( 123 , 123 ) : its distribution can be written as an expo - nential family with ( x ) = x , a ( ) = log ( 123 + exp ( ) ) , and = r .
in order to verify the claim in theorem 123 ( a ) , let us compute the conju - by direct methods .
by the denition of conjugate gate dual function a
123 graphical models as exponential families
123 conjugate duality : maximum likelihood and maximum entropy
duality ( 123 ) , for any xed r , we have
( ) = sup
( log ( 123 + exp ( ) ) ) .
= exp ( ) / ( 123 + exp ( ) ) , which is simply the moment matching condition ( 123 ) specialized to the bernoulli model .
when is there a solution to this stationary condition ? if ( 123 , 123 ) , then some simple algebra shows that we may rearrange to nd the unique solution ( ) : = log ( / ( 123 ) ) .
since m = ( 123 , 123 ) for the bernoulli model , the existence and uniqueness of this solution are particular consequences of proposition 123 and theorem 123 .
since the objective function ( 123 ) is strictly convex , the solution ( ) species the optimum; substituting ( ) into the objective equation ( 123 ) and simplifying yields that
( ) = log ( / ( 123 ) ) log
= log + ( 123 ) log ( 123 ) ,
which is the negative entropy of bernoulli variate with mean para - meter .
we have thus veried theorem 123 ( a ) in the case that ( 123 , 123 ) = m .
now , let us consider the case / m = ( 123 , 123 ) ; concretely , let us sup - pose that > 123
in this case , there is no gradient stationary point in the optimization problem ( 123 ) .
therefore , the supremum is specied by the limiting behavior as .
for > 123 , we claim that the objec - tive function grows unboundedly as + .
indeed , by the convexity ( cid : 123 ) ( ) 123 for of a , we have a ( 123 ) = log 123 a ( ) + a all r , from which we obtain the upper bound a ( ) + log 123 , valid for > 123
consequently , for > 123 , we have
( cid : 123 ) ( ) ( ) .
moreover a
log ( 123 + exp ( ) ) ( 123 ) log 123 ,
showing that the objective function diverges as + , whenever > 123
a similar calculation establishes the same claim for < 123 , showing ( ) = + for / m and thus verifying the second part of theorem 123 ( a ) .
finally , for the boundary points = 123 and = 123 , it ( 123 ) = 123 for the bernoulli can be veried by taking limits that a
( 123 ) = a
123 graphical models as exponential families
turning to the verication of theorem 123 ( b ) , since a unless ( 123 , 123 ) , the optimization problem ( 123 ) reduces to
( log ( 123 ) log ( 123 ) ) .
( ) = +
explicitly solving this concave maximization problem yields that its optimal value is log ( 123 + exp ( ) ) , which veries the claim ( 123 ) .
more - over , this same calculation shows that the optimum is attained uniquely at ( ) = exp ( ) / ( 123 + exp ( ) ) , which veries theorem 123 ( c ) for the
example 123 ( conjugate duality for exponential ) .
consider the family of exponential distributions , represented as a regular expo - family with ( x ) = x , = ( , 123 ) , and a ( ) = log ( ) .
from the conjugate dual denition ( 123 ) , we have ( + log ( ) ) .
( ) = sup
for all m = ( 123 , ) , we have a
in order to nd the optimum , we take derivatives with respect to .
we thus obtain the stationary condition = 123 / , which corresponds to the moment matching condition ( 123 ) specialized to this family .
( ) = 123 log ( ) .
it can be veried that the entropy of an exponential distribution with mean > 123 is given by a ( ) .
this result conrms theorem 123 for m .
for / m that is , for < 123 we see by inspection that the objective function + log ( ) grows unboundedly as , ( ) = + for all / m .
the remaining thereby demonstrating that a ( 123 ) = + case is the boundary point = 123 , for which we have a from the denition ( 123 ) .
note also that the negative entropy of the exponential distribution 123 log ( n ) tends to innity for all sequence n 123+ , consistent with theorem 123 ( a ) .
having computed the dual , straightforward algebra shows that a ( ) = log ( ) = sup
( + 123 + log ( ) ) ,
with the optimum uniquely attained at = 123 / .
this calculation veries parts ( b ) and ( c ) of theorem 123 for the exponential variate .
123 computational challenges with high - dimensional models
123 computational challenges with high - dimensional
from a computational perspective , the essential features of theo - rem 123 are the representation ( 123 ) of the cumulant function , and the assertion ( 123 ) that the optimum is uniquely achieved at the mean parameters = e ( ( x ) ) .
it thus illuminates a key property of com - puting the cumulant function a , as well as the mean parameters : in principle , we can compute both of these quantities by solving the variational problem ( 123 ) .
even more encouragingly , at least from a supercial perspective , the optimization problem appears to be rather simple : it is a nite - dimensional optimization problem over a convex set , and the objective function is strictly concave and dierentiable .
thus , the optimization lacks local optima or other unpleasant fea - tures .
it is tempting , then , to assert that the problem of comput - ing the log partition function and the associated mean parameters is now solved , since we have reduced it to a convex optimization
in this context , the simple scalar examples of table 123 , for which the fundamental variational problem ( 123 ) had an explicit form and could be solved easily , are very misleading .
for general multivariate exponential families , in contrast , there are two primary challenges asso - ciated with the variational representation :
( a ) in many cases , the constraint set m of realizable mean parameters is extremely dicult to characterize in an is dened indirectly in a variational manner so that it too typically lacks an
( b ) the negative entropy function a
for instance , to illustrate issue ( a ) concerning the nature of m , for markov random elds involving discrete random variables x ( 123 , 123 , .
, r 123 ) m , the set m is always a polytope , which we have referred to as a marginal polytope .
in this case , at least in principle , the set m can be characterized by some nite number of linear inequali - ties .
however , for general graphs , the number of such inequalities grows
123 graphical models as exponential families
as the composition of two functions .
any fig .
123 a block diagram decomposition of a mean parameter m is rst mapped back to a canonical parameter ( ) in the inverse ( ) corresponds to the negative entropy h ( p ( ) ) of image ( a ) 123 ( ) .
the value of a the associated exponential family density p ( ) .
rapidly with the graph size .
indeed , unless fundamental conjectures in complexity theory turn out to be false , it is not even possible to opti - mize a linear function over m for a general discrete mrf .
in addition to the complexity of the constraint set , issue ( b ) highlights that even evaluating the cost function at a single point m , let alone optimiz - ing it over m , is extremely dicult .
to understand the complexity inherent in evaluating the dual value ( ) , note that theorem 123 provides only an implicit characteri - as the composition of mappings : rst , the inverse map - zation of a ping ( a ) 123 : m , in which maps to ( ) , corresponding to the exponential family member with mean parameters ; and second , the mapping from ( ) to the negative entropy h ( p ( ) ) of the associ - ated exponential family density .
this decomposition of the value a is illustrated in figure 123 .
consequently , computing the dual value ( ) at some point m requires computing the inverse map - ping ( a ) 123 ( ) , in itself a nontrivial problem , and then evaluating the entropy , which requires high - dimensional integration for general graphical models .
these diculties motivate the use of approximations to m and a .
indeed , as shown in the sections to follow , a broad class of methods for approximate marginalization are based on this strategy of nding an approximation to the exact variational principle , which is then often solved using some form of message - passing algorithm .
sum - product , bethekikuchi , and
in this section , we begin our exploration of the variational pretation of message - passing algorithms .
we rst discuss the sum - product algorithm , also known as the belief propagation algorithm .
as discussed in section 123 , sum - product is an exact algorithm for tree - structured graphs , for which it can be derived as a divide - and - conquer algorithm .
however , given the local form of the sum - product updates , there is no barrier to applying it to a graph with cycles , which yields the loopy form of the sum - product or belief propaga - tion algorithm .
in the presence of cycles , there are no general con - vergence or correctness guarantees associated with the sum - product algorithm , but it is nonetheless widely used to compute approximate marginals .
the rst part of this section describes the variational for - mulation of the sum - product updates in terms of the bethe approxi - mation .
although the approximation itself dates back to the work of bethe ( 123 ) , the connection to the sum - product algorithm was rst elu - cidated by yedidia et al .
( 123 , 123 ) .
we then describe various nat - ural generalizations of the bethe approximation , including kikuchi clustering and other hypergraph - based methods ( 123 , 123 ) .
finally , we describe expectation - propagation algorithms ( 123 , 123 ) and related
123 sum - product , bethekikuchi , and expectation - propagation
moment - matching methods ( 123 , 123 , 123 ) ; these are also variational methods based on bethe - like approximations .
123 sum - product and bethe approximation
the simplest instantiation of the bethe approximation applies to an undirected graphical model g = ( v , e ) with potential functions involv - ing at most pairs of variables; we refer to any such model as a pairwise markov random eld . 123 the sum - product algorithm is most widely used for the cases of discrete random variables , or gaussian random vari - ables .
the bulk of our discussion focuses on the former case , where for each node s v , the associated variable xs takes values in a discrete space xs = ( 123 , 123 , .
, rs 123 ) .
in the discrete case , the general variational principle ( 123 ) takes dierent forms , depending on the chosen form of sucient statistics .
recall from section 123 . 123 our denition of the canonical overcomplete representation ( 123 ) , using indicator functions for events ( xs = j ) and ( xs = j , xt = k ) .
using these sucient statistics , we dene an expo - nential family of the form
where we have introduced the convenient shorthand notation
st ( xs , xt ) : =
st;jki st;jk ( xs , xt ) .
as previously discussed in example 123 , this particular parameter - ization is overcomplete or nonidentiable , because there exist entire ane subsets of canonical parameters that induce the same probability
123 in principle , by selectively introducing auxiliary variables , any undirected graphical model can be converted into an equivalent pairwise form to which the bethe approximation can be applied; see appendix e . 123 for details of this procedure .
it can also be useful to treat higher order interactions directly , which can be done using the approximations discussed in section 123 .
st ( xs , xt ) : =
st;jki st;jk ( xs , xt ) .
123 sum - product and bethe approximation
distribution123 over the random vector x .
nonetheless , this overcom - pleteness is useful because the associated mean parameters are easily interpretable , since they correspond to singleton and pairwise marginal probabilities ( see equations ( 123 ) and ( 123 ) ) .
using these mean parameters , it is convenient for various calculations to dene the short -
note that s is a |xs| - dimensional marginal distribution over xs , whereas st is a |xs| |xt| matrix , representing a joint marginal over ( xs , xt ) .
the marginal polytope m ( g ) corresponds to the set of all singleton and pairwise marginals that are jointly realizable by some
m ( g ) : = ( rd | p with marginals s ( xs ) , st ( xs , xt ) ) .
in the case of discrete ( multinomial ) random variables , this set plays a central role in the general variational principle from theorem 123 .
123 . 123 a tree - based outer bound to m ( g )
as previously discussed in section 123 . 123 , the polytope m ( g ) can either be written as the convex hull of a nite number of vectors , one associ - ated with each conguration x x m , or alternatively , as the intersec - tion of a nite number of half - spaces ( see figure 123 for an illustration ) .
but how to provide an explicit listing of these half - space constraints , also known as facets ? in general , this problem is extremely dicult , so that we resort to listing only subsets of the constraints , thereby obtaining a polyhedral outer bound on m ( g ) .
sv rs +
123 a little we form a new canonical parameter c r is some xed constant , and to verify that p ( x ) = p ( cid : 123 ) ( x ) for all x x m , so that and
family ( 123 ) has dimension ( s , t ) e rs rt .
given some xed parameter vector rd , suppose that 123;j = 123;j + c for all j x , where = for all other indices .
it is then straightforward ( cid : 123 ) describe the same probability
( cid : 123 ) rd by setting
123 sum - product , bethekikuchi , and expectation - propagation
more specically , consider a trial set of single node functions ( s , s v ) and edge - based functions ( st , ( s , t ) e ) .
if these trial marginal distributions are to be globally realizable , they must of course the normalization condition ( cid : 123 ) be nonnegative , and in addition , each singleton quantity s must satisfy
moreover , for each edge ( s , t ) e , the singleton ( s , t ) and pairwise quantities st must satisfy the marginalization constraints
s ( xs ) = 123
t ) = s ( xs ) , xs xs ,
s , xt ) = t ( xt ) , xt xt .
these constraints dene the set l ( g ) : = ( 123 | condition ( 123 ) holds for s v ,
condition ( 123 ) holds for ( s , t ) e ) ,
of locally consistent marginal distributions .
note that l ( g ) is also a polytope , in fact a very simple one , since it is dened by o ( |v | + |e| ) constraints in total .
what is the relation between the set l ( g ) of locally consistent marginals and the set m ( g ) of globally realizable marginals ? on one hand , it is clear that m ( g ) is always a subset of l ( g ) , since any set of globally realizable marginals must satisfy the normalization ( 123 ) and marginalization ( 123 ) constraints .
apart from this inclusion relation , it turns out that for a graph with cycles , these two sets are rather dier - ent .
however , for a tree t , the junction tree theorem , in the form of proposition 123 , guarantees that they are equivalent , as summarized in the following proposition .
proposition 123 .
the inclusion m ( g ) l ( g ) holds for any graph .
for a tree - structured graph t , the marginal polytope m ( t ) is equal to l ( t ) .
123 sum - product and bethe approximation
consider an element of the full marginal polytope m ( g ) : clearly , any such vector must satisfy the normalization and pair - wise marginalization conditions dening the set l ( g ) , from which we conclude that m ( g ) l ( g ) .
in order to demonstrate the reverse inclusion for a tree - structured graph t , let be an arbitrary element of l ( t ) ; we need to show that m ( t ) .
by denition of l ( t ) , the vector species a set of locally consistent singleton marginals s for vertices s v and pairwise marginals st for edges ( s , t ) e .
by the junction tree theorem , we may use them to form a distribution , markov with respect to the tree , as follows :
( we take 123 / 123 : = 123 in cases of zeros in the elements of . ) it is a consequence of the junction tree theorem or can be veried directly via an inductive leaf - stripping argument that with this choice of p , we have ep ( i j ( xs ) ) = s ( xs ) for all s v and j xs , as well as ep ( i jk ( xs , xt ) ) = st ( xs , xt ) for all ( s , t ) e , and ( j , k ) xs xt .
therefore , the distribution ( 123 ) provides a constructive certicate of the membership m ( t ) , which establishes that l ( t ) = m ( t ) .
for a graph g with cycles , in sharp contrast to the tree case , the set l ( g ) is a strict outer bound on m ( g ) , in that there exist vectors l ( g ) that do not belong to m ( g ) , for which reason we refer to members of l ( g ) as pseudomarginals .
the following example illus - trates the distinction between globally realizable marginals and pseu -
example 123 ( l ( g ) versus m ( g ) ) .
let us explore the relation between the two sets on the simplest graph for which they fail to be equivalent namely , the single cycle on three vertices , denoted by c123
considering the binary random vector x ( 123 , 123 ) 123 , note that each singleton pseudomarginal s , for s = 123 , 123 , 123 , can be viewed as a 123 123 vector , whereas each pairwise pseudomarginal st , for edges ( s , t ) ( ( 123 ) , ( 123 ) , ( 123 ) ) can be viewed as a 123 123 matrix .
we dene
123 sum - product , bethekikuchi , and expectation - propagation
the family of pseudomarginals
st ( xs , xt ) : =
where for each edge ( s , t ) e , the quantity st r is a parameter to we rst observe that for any st ( 123 , 123 ) , these pseudomarginals satisfy the normalization ( 123 ) and marginalization constraints ( 123 ) , so the associated pseudomarginals ( 123 ) belong to l ( c123 ) .
as a partic - ular choice , consider the collection of pseudomarginals generated by setting 123 = 123 = 123 , and 123 = 123 , as illustrated in figure 123 ( a ) .
with these settings , the vector is an element of l ( c123 ) ; however , as a candidate set of global marginal distributions , certain features of the collection should be suspicious .
in particular , according to the puta - tive marginals , the events ( x123 = x123 ) and ( x123 = x123 ) should each hold with probability 123 , whereas the event ( x123 = x123 ) should only hold with probability 123 .
at least intuitively , this set - up appears likely to violate some type of global constraint .
in order to prove the global invalidity of , we rst specify the con - straints that actually dene the marginal polytope m ( g ) .
for ease of
123 ( a ) a set of pseudomarginals associated with the nodes and edges of the graph : setting 123 = 123 = 123 and 123 = 123 in equation ( 123 ) yields a pseudomarginal vector which , though locally consistent , is not globally consistent .
( b ) marginal polytope m ( c123 ) for the three node cycle; in a minimal exponential representation , it is a 123d object .
illus - 123 ) , as well as the outer bound l ( c123 ) , also for this trated here is the slice ( 123 = 123 = 123 = 123
123 sum - product and bethe approximation
illustration , let us consider the slice of the marginal polytope dened by the constraints 123 = 123 = 123 = 123 123
viewed in this slice , the constraints dening l ( g ) reduce to 123 st 123 123 for all edges ( s , t ) , so that the set is simply a 123d cube , as drawn with dotted lines in figure 123 ( b ) .
it can be shown that the sliced version of m ( g ) is dened by these box constraints , and in addition the following cycle inequalities
123 + 123 123 123 123 , 123 123 + 123 123 123 + 123 + 123 123
and 123 + 123 + 123 123
( see example 123 in the sequel for the derivation of these cycle inequali - ties . ) as illustrated by the shaded region in figure 123 ( b ) , the marginal polytope m ( c123 ) is strictly contained within the scaled cube ( 123 , 123
to conclude the discussion of our example , note that the pseudo - marginal vector specied by 123 = 123 = 123 and 123 = 123 fails to satisfy the cycle inequalities ( 123 ) , since in particular , we have the
123 + 123 123 = 123 + 123 123 >
therefore , the vector is an instance of a set of pseudomarginals valid under the bethe approximation , but which could never arise from a true probability distribution .
123 . 123 bethe entropy approximation
we now turn to the second variational ingredient that underlies the sum - product algorithm , namely an approximation to the dual function ( or negative entropy ) .
as with the outer bound l ( g ) on the marginal polytope , the bethe entropy approximation is also tree - based .
for a general mrf based on a graph with cycles , the negative as a function of only the mean parameters typi - cally lacks a closed form expression .
an important exception to this rule is the case of a tree - structured markov random eld , for which the entropy decomposes in terms of local entropies associated with the edges and nodes of the graph .
in order to derive this decomposition ,
123 sum - product , bethekikuchi , and expectation - propagation
recall from the proof of proposition 123 the factorization ( 123 ) of any tree - structured mrf distribution in terms of marginal distributions ( s , s v ) and ( st , ( s , t ) e ) on the node and edges , respectively , of the tree .
these marginal distributions correspond to the mean parame - ters under the canonical overcomplete sucient statistics ( 123 ) .
thus , for a tree - structured mrf , we can compute the ( negative ) dual value ( ) directly , simply by computing the entropy h ( p ) of the dis - tribution ( 123 ) .
denoting by e the expectation under the distribu - tion ( 123 ) , we obtain
h ( p ) = a
( ) = e ( log p ( x ) )
the dierent terms in this expansion are the singleton entropy
for each node s v , and the mutual information
st ( xs , xt ) log st ( xs , xt )
for each edge ( s , t ) e .
consequently , for a tree - structured graph , the can be expressed as an explicit and easily computable dual function a function of the mean parameters .
with this background , the bethe approximation to the entropy of an mrf with cycles is easily described : it simply assumes that decom - position ( 123 ) is approximately valid for a graph with cycles .
this assumption yields the bethe entropy approximation
( ) hbethe ( ) : =
an important fact , central in the derivation of the sum - product algo - rithm , is that this approximation ( 123 ) can be evaluated for any set of pseudomarginals ( s , s v ) and ( st , ( s , t ) e ) that belong to l ( g ) .
for this reason , our change in notation from for exact marginals to for pseudomarginals is deliberate .
123 sum - product and bethe approximation
we note in passing that yedidia et al .
( 123 , 123 ) used an alternative form of the bethe entropy approximation ( 123 ) , one which can be obtained via the relation ist ( st ) = hs ( s ) + ht ( t ) hst ( st ) , where hst is the joint entropy dened by the pseudomarginal st .
doing so and performing some algebraic manipulation yields
( ds 123 ) hs ( s ) +
where ds corresponds to the number of neighbors of node s ( i . e . , the degree of node s ) .
however , the symmetric form ( 123 ) turns out to be most natural for our development in the sequel .
123 . 123 bethe variational problem and sum - product
we now have the two ingredients needed to construct the bethe approx - imation to the exact variational principle ( 123 ) from theorem 123 : the set l ( g ) of locally consistent pseudomarginals ( 123 ) is a convex ( polyhedral ) outer bound on the marginal polytope the bethe entropy ( 123 ) is an approximation of the exact dual function a
by combining these two ingredients , we obtain the bethe variational
( cid : 123 ) , ( cid : 123 ) +
note that this problem has a very simple structure : the cost function is given in closed form , it is dierentiable , and the constraint set l ( g ) is a polytope specied by a small number of constraints .
given this special structure , one might suspect that there should exist a relatively simple algorithm for solving this optimization problem ( 123 ) .
indeed , the sum - product algorithm turns out to be exactly such a method .
in order to develop this connection between the variational pro - blem ( 123 ) and the sum - product algorithm , let ss be a lagrange
123 sum - product , bethekikuchi , and expectation - propagation
multiplier associated with the normalization constraint css ( ) = 123 ,
css ( ) : = 123
moreover , for each direction t s of each edge and each xs xs , dene the constraint function
cts ( xs; ) : = s ( xs )
and let ts ( xs ) be a lagrange multiplier associated with the con - straint cts ( xs; ) = 123
these lagrange multipliers turn out to be closely related to the sum - product messages , in particular via the relation
we then consider the lagrangian corresponding to the bethe vari -
ational problem ( 123 ) : l ( , ; ) : = ( cid : 123 ) , ( cid : 123 ) + hbethe ( ) +
ts ( xs ) cts ( xs; ) +
with these denitions , the connection between sum - product and the bvp is made precise by the following result , due to yedidia et al .
( 123 ) .
theorem 123 ( sum - product and the bethe problem ) .
the sum - product updates are a lagrangian method for attempting to solve the bethe variational problem :
) such that
( a ) for any graph g , any xed point of the sum - product updates
species a pair (
; ) = 123
; ) = 123 ,
( b ) for a tree - structured markov random eld ( mrf ) , the lagrangian equations ( 123 ) have a unique solution ( correspond to the exact single - where the elements of ton and pairwise marginal distributions of the mrf .
more - over , the optimal value of the bvp is equal to the cumulant
123 sum - product and bethe approximation
it should be noted that the lagrangian formula - tion ( 123 ) is a partial one , because it assigns lagrange multipliers to the normalization css and marginalization cts constraints , but deals with the nonnegativity constraints implicitly .
however , any optimum of the bethe variational principle with strictly positive elements
must satisfy the lagrangian conditions ( 123 ) from theorem 123 ( a ) .
to is lagrange multiplier vector for the bvp , see this fact , note that if must maximize the lagrangian l ( , then any optimal solution over the positive orthant 123 ( see bertsekas ( 123 ) ) .
if the optimal solu - has strictly positive elements , then a necessary condition for lagrangian optimality is the zero - gradient condition l ( ) = 123 , as claimed .
moreover , for graphical models where all congurations are given strictly positive mass ( such as graphical models in exponential form with nite ) , the sum - product messages stay bounded strictly away from zero ( 123 ) , so that there is always an optimum strictly positive elements .
in this case , theorem 123 ( a ) guarantees that any sum - product xed point satises the lagrangian conditions neces - sary to be an optimum of the bethe variational principle .
for graphical models in which some congurations are assigned zero mass , further care is required in connecting xed points to local optima; we refer the reader to yedidia et al .
( 123 ) for details .
proof of part ( a ) : computing the partial derivative l ( ; ) and setting it to zero yields the relations
log s ( xs ) = ss + s ( xs ) +
log st ( xs , xt )
= st ( xs , xt ) ts ( xs ) st ( xt ) ,
where we have used the shorthand ( cid : 123 ) s ( xs ) : = ( xs , xt ) .
the con - dition l ( ; ) = 123 is equivalent to cts ( xs; ) = 123 and css ( ) = 123
using equation ( 123a ) and the fact that the marginalization condition ( xs , xt ) = s ( xs ) ) , we cts ( xs; ) = 123 hold at the optimum ( so that
123 sum - product , bethekikuchi , and expectation - propagation
can rearrange equation ( 123b ) to obtain :
log st ( xs , xt ) = ss + tt + st ( xs , xt ) + s ( xs ) + t ( xt )
so as to make explicit the connection to the sum - product algorithm , let us dene , for each directed edge t s , an rs - vector of messages
mts ( xs ) = exp ( ts ( xs ) ) ,
for all xs ( 123 , 123 , .
, rs 123 ) .
with this notation , we can then write an equivalent form of equa - tion ( 123a ) as follows :
s ( xs ) = exp ( s ( xs ) )
similarly , we have an equivalent form of equation ( 123 ) :
st ( xs , xt ) =
st ( xs , xt ) + s ( xs ) + t ( xt )
( cid : 123 ) are positive constants dependent on ss and tt , chosen so that the pseudomarginals satisfy normalization conditions .
note that s and st so dened are nonnegative .
to conclude , we need to adjust the lagrange multipliers or messages st ( xs , xt ) = s ( xs ) is satised for every edge .
so that the constraint using the relations ( 123 ) and ( 123 ) and performing some algebra , the end result is
st ( xs , xt ) + t ( xt )
which is equivalent to the familiar sum - product update ( 123 ) .
by of these updates ( 123 ) species a construction , any xed point m
) that satises the stationary conditions ( 123 ) .
proof of part ( b ) : see appendix b . 123 for the proof of this claim .
123 sum - product and bethe approximation
the connection between the sum - product algorithm and the bethe variational principle has a number of important consequences .
first , it provides a principled basis for applying the sum - product algorithm for graphs with cycles , namely as a particular type of iterative method for attempting to satisfy lagrangian conditions .
it should be noted , how - ever , that this connection between sum - product and the bethe prob - lem in itself provides no guarantees on the convergence of the sum - product updates on graphs with cycles .
indeed , whether or not the algorithm converges depends both on the potential strengths and the topology of the graph .
in the standard scheduling of the messages , each node applies equation ( 123 ) in parallel .
other more global schemes for message - passing are possible , and commonly used in certain appli - cations like error - control coding ( e . g . , 123 ) ; some recent work has also studied adaptive schedules for message - passing ( 123 , 123 ) .
tatikonda and jordan ( 123 ) established an elegant connection between convergence of parallel updates and gibbs measures on the innitely unwrapped computation tree , thereby showing that sucient conditions for con - vergence can be obtained from classical conditions for uniqueness of gibbs measures ( e . g . , dobrushins condition or simons condition ( 123 ) ) .
in subsequent work , other researchers ( 123 , 123 , 123 , 123 ) have used various types of contraction arguments to obtain sharper conditions for convergence , and / or uniqueness of xed points .
for suitably weak potentials , dobrushin - type conditions and related contraction argu - ments guarantee both convergence of the updates , and as a conse - quence , uniqueness of the associated xed point .
a parallel line of work ( 123 , 123 , 123 ) has explored alternatives to sum - product that are guaranteed to converge , albeit at the price of increased compu - tational cost .
however , with the exception of trees and other special cases ( 123 , 123 , 123 ) , the bethe variational problem is usually a non - convex problem , in that hbethe fails to be concave .
as a consequence , there are frequently local optima , so that even when using a conver - gent algorithm , there are no guarantees that it will nd the global for each rd , let abethe ( ) denote the optimal value of the bethe variational problem ( 123 ) .
theorem 123 ( b ) states for any
123 sum - product , bethekikuchi , and expectation - propagation
tree - structured problem , we have the equality abethe ( ) = a ( ) for all rd .
given this equivalence , it is natural to consider the rela - tion between abethe ( ) and the cumulant function a ( ) for general graphs .
in general , the bethe value abethe ( ) is simply an approxi - mation to the cumulant function value a ( ) .
unlike the mean eld methods to be discussed in section 123 , it is not guaranteed to pro - vide a lower bound on the cumulant function .
as will be discussed at more length in section 123 , wainwright et al .
( 123 ) derived con - vexied forms of the bethe variational principle that are guaranteed to yield upper bounds on the cumulant function for any graphical model .
on the other hand , sudderth et al .
( 123 ) show that abethe ( ) is a lower bound on the cumulant function a ( ) for certain classes of attractive graphical models .
such models , in which the interactions encourage random variables to agree with one another , are common in computer vision and other applications in spatial statistics .
this lower - bounding property is closely related to the connection between the bethe approximation and loop series expansions ( 123 ) , discussed in
another important consequence of the bethe / sum - product connec - tion is in suggesting a number of avenues for improving upon the ordinary sum - product algorithm , via progressively better approxima - tions to the entropy function and outer bounds on the marginal poly - tope .
we turn to discussion of a class of such generalized sum - product algorithms beginning in section 123 .
inexactness of bethe and sum - product
in this section , we explore some aspects of the inexactness of the sum - product algorithm .
from a variational perspective , the inexact - ness stems from the two approximations made in setting up bethe
( a ) replacing the marginal polytope m ( g ) by the polyhedral
outer bound l ( g ) and
( b ) the bethe entropy hbethe as an approximation to the exact
entropy as a function of the mean parameters .
123 sum - product and bethe approximation
we begin by considering the bethe entropy approximation , and its
example 123 ( inexactness connected graph k123 on four vertices , and the collection of singleton and pairwise marginal distributions given by
of hbethe ) .
consider
st ( xs , xt ) =
for s = 123 , 123 , 123 , 123 ( s , t ) e .
it can be veried that these marginals are globally valid , generated in particular by the distribution that places mass 123 on each of the congurations ( 123 , 123 , 123 , 123 ) and ( 123 , 123 , 123 , 123 ) .
let us calculate the bethe entropy approximation .
each of the four singleton entropies are given by hs ( s ) = log 123 , and each of the six ( one for each edge ) mutual infor - mation terms are given by ist ( st ) = log 123 , so that the bethe entropy is given by
hbethe ( ) = 123log 123 123log 123 = 123log 123 < 123 ,
for this example , the which cannot be a true entropy .
in fact , true entropy ( or value of the negative dual function ) is given by
( ) = log 123 > 123
in addition to the inexactness of hbethe as an approximation to the negative dual function , the bethe variational principle also involves relaxing the marginal polytope m ( g ) to the rst - order constraint set l ( g ) .
as illustrated in example 123 , the inclusion m ( c123 ) l ( c123 ) holds strictly for the 123 - node cycle c123
the constructive procedure of exam - ple 123 can be substantially generalized to show that the inclusion m ( g ) l ( g ) holds strictly for any graph g with cycles .
figure 123 provides a highly idealized illustration123 of the relation between m ( g ) and l ( g ) : both sets are polytopes , and for a graph with cycles , m ( g ) is always strictly contained within the outer bound l ( g ) .
123 in particular , this picture is misleading in that it suggests that l ( g ) has more facets and more vertices than m ( g ) ; in fact , the polytope l ( g ) has fewer facets and more vertices , but this is dicult to convey in a 123d representation .
123 sum - product , bethekikuchi , and expectation - propagation
123 highly idealized illustration of the relation between the marginal polytope m ( g ) and the outer bound l ( g ) .
the set l ( g ) is always an outer bound on m ( g ) , and the inclusion m ( g ) l ( g ) is strict whenever g has cycles .
both sets are polytopes and so can be represented either as the convex hull of a nite number of extreme points , or as the intersection of a nite number of half - spaces , known as facets .
both sets are polytopes , and consequently can be represented either as the convex hull of a nite number of extreme points , or as the inter - section of a nite number of half - spaces , known as facets .
letting be a shorthand for the full vector of indicator functions in the stan - dard overcomplete representation ( 123 ) , the marginal polytope has the convex hull representation m ( g ) = conv ( ( x ) | x x ) .
since the indicator functions are ( 123 , 123 ) - valued , all of its extreme points consist of ( 123 , 123 ) elements , of the form x : = ( x ) for some x x m; there are a total of |x m| such extreme points .
however , with the exception of tree - structured graphs , the number of facets for m ( g ) is not known in general , even for relatively simple cases like the ising model; see the book ( 123 ) for background on the cut or correlation polytope , which is equivalent to the marginal polytope for an ising model .
however , the growth must be super - polynomial in the graph size , unless certain widely believed conjectures in computational complexity are false .
on the other hand , the polytope l ( g ) has a polynomial number of facets , upper bounded by any graph by o ( rm + r123|e| ) .
it has more extreme points than m ( g ) , since in addition to all the integral extreme points ( x , x x m ) , it includes other extreme points l ( g ) \m ( g ) that contain fractional elements; see section 123 for further discussion of integral versus fractional extreme points .
with the exception of trees and small instances , the total number of extreme points of l ( g ) is not known in general .
123 sum - product and bethe approximation
the strict inclusion of m ( g ) within l ( g ) and the fundamental role of the latter set in the bethe variational problem ( 123 ) leads to the following question : do solutions to the bethe variational problem ever fall into the gap m ( g ) \l ( g ) ? the optimistically inclined would hope that these points would somehow be excluded as optima of the bethe variational problem .
unfortunately , such hope turns out to be misguided .
in fact , for every element of l ( g ) , it is possible to con - struct a distribution p such that if the sum - product algorithm is run on the problem , then arises from the messages dened by a sum - product xed point .
in order to understand this fact , we need to describe the reparameterization interpretation of the sum - product algorithm ( 123 ) , to which we now turn .
123 . 123 bethe optima and reparameterization
one view of the junction tree algorithm , as described in section 123 . 123 , is as follows : taking as input a set of potential functions on the cliques of some graph , it returns as output an alternative factorization of the same distribution in terms of local marginal distributions on the cliques and separator sets of a junction tree .
in the special case of an ordinary tree , the alternative factorization is a product of local marginals at single nodes and edges of the tree , as in equation ( 123 ) .
indeed , the sum - product algorithm for trees can be understood as an ecient method for computing this alternative parameterization .
it turns out that the same interpretation applies to arbitrary graphs with cycles : more precisely , any xed point of the sum - product algo - rithm and even more generally , any local optimum of the bethe variational principle species a reparameterization of the original distribution p .
we summarize in the following proposition ( 123 ) :
proposition 123 ( reparameterization properties of bethe s , s v ; st , ( s , t ) e ) denote any opti -
mum of the bethe variational principle dened by the distribution p .
then the distribution dened by the xed point as
t ( xt ) ,
123 sum - product , bethekikuchi , and expectation - propagation
is a reparameterization of the original distribution p that is , p ( x ) = p ( x ) for all x x m .
note that this type of reparameterization is possible only because the exponential family is dened by an overcomplete set of sucient statistics , involving the indicator functions ( 123 ) .
the reparameteri - zation ( 123 ) is the analog of the tree - structured factorization ( 123 ) , but as applied to a graph with cycles .
in contrast to the tree case , the ) is not , in general , equal to one .
whereas normalization constant z ( the junction tree theorem guarantees this reparameterization for any tree , it is not immediately obvious that even one reparameterization need exist for a general graph .
in fact , the result establishes that every graph has at least one such reparameterization , and some graphs may have multiple reparameterizations of the form ( 123 ) ; in particular , this is the case for any problem for which the bvp has multiple optima .
moreover , the reparameterization viewpoint provides some insight into the approximation error : that is , the dierence between
the exact marginals s of p ( x ) and the approximations by the sum - product algorithm .
indeed , using equation ( 123 ) , it is possible to derive an exact expression for the error in the sum - product algorithm , as well as computable error bounds , as described in more detail in wainwright et al .
( 123 ) .
we now show how the reparameterization characterization ( 123 ) enables us to specify , for any pseudomarginal in the interior of l ( g ) , a distribution p for which is a xed point of the sum - product algo - rithm .
the following example illustrates this construction .
example 123 ( fooling the sum - product algorithm ) .
let us return to the simplest graph for which sum - product is not exact namely , a single cycle with three nodes ( see example 123 ) .
consider candidate marginal distributions ( s , s v ) and ( st , ( s , t ) e ) of the form illustrated in figure 123 ( a ) , with 123 = 123 = 123 and 123 = 123 .
as discussed in example 123 , this setting yields a set of pseudomarginals that lie in l ( g ) but not in m ( g ) , and therefore could not possibly arise from any global probability distribution .
123 sum - product and bethe approximation
let us now demonstrate how ,
for an appropriately chosen distribution p on the graph , the sum - product algorithm can be fooled into converging to this pseudomarginal vector .
using the canonical overcomplete representation ( 123 ) , consider a set of canoni - cal parameters of the form :
s ( xs ) : = log s ( xs ) = log st ( xs , xt ) : = log st ( xs , xt )
= log 123
( cid : 123 ) s v , and
( s , t ) e ,
message initialization mts ( xs ) ( cid : 123 )
where we have adopted the short - hand notation from equation ( 123 ) .
with these canonical parameters , suppose that we apply the sum - product algorithm to the markov random eld p , using the uniform .
a little bit of algebra using the sum - product update ( 123 ) shows that for this parameter choice , the uniform messages m already dene a xed point of the sum - product algorithm .
moreover , if we compute the associated pseudomarginals specied by m and , they are equal to the previously specied s , st .
in summary , the sum - product algorithm when applied to the distribution p dened by the canonical parameters ( 123 ) pro - duces as its output the pseudomarginal as its estimate of the true
the reader might object to the fact that the problem construc - tion ensured the sum - product algorithm was already at this particular xed point , and so obviates the possibility of the updates converging to some other xed point if initialized in a dierent way .
however , it is known ( 123 , 123 ) that for any discrete markov random eld in exponential family form with at most a single cycle , sum - product has a unique xed point , and always converges to it .
therefore , the sum - product xed point that we have constructed ( 123 ) is the unique xed point for this problem , and the algorithm converges to it from any initialization of the messages .
123 sum - product , bethekikuchi , and expectation - propagation
the constructive approach illustrated in the preceding example applies to an arbitrary member of the interior123 of l ( g ) .
therefore , for all pseudomarginals l ( g ) , including those that are not globally valid , there exists a distribution p for which arises from a sum - product xed point .
123 . 123 bethe and loop series expansions
in this section , we discuss the loop series expansions of chertkov and chernyak ( 123 ) .
these expansions provide exact representation of the cumulant function as a sum of terms , with the rst term corresponding to the bethe approximation abethe ( ) , and higher - order terms obtained by adding in so - called loop corrections .
they provided two derivations of their loop series : one applies a trigonometric identity to a fourier representation of binary variables , while the second is based upon a saddle point approximation obtained via an auxiliary eld of complex variables .
in this section , we describe a more direct derivation of the loop expansion based on the reparameterization characterization of sum - product xed points given in proposition 123 .
although the loop series expansion can be developed for general factor graphs , considering the case of a pairwise markov random eld with binary variables that is , the ising model from example 123 suces to illustrate the basic ideas .
( see sudderth et al .
( 123 ) for a derivation for more general factor graphs . )
given an undirected graph g = ( v , e ) and some subset ( cid : 123 ) e e of the edge set e , we let g ( ( cid : 123 ) e ) denote the induced subgraph associated with ( cid : 123 ) e that is , the graph with edge set ( cid : 123 ) e , and vertex set v ( ( cid : 123 ) e ) : = ( t v | ( t , u ) ( cid : 123 ) e for some u ) .
for any vertex s v , we dene its degree with respect to ( cid : 123 ) e as
before stating the result , we require a few preliminary denitions .
ds ( ( cid : 123 ) e ) : = ( t v | ( s , t ) ( cid : 123 ) e ) .
123 strictly speaking , it applies to members of the relative interior since , as described in the overcomplete representation ( 123 ) , the set l ( g ) is not full - dimensional and hence has an
123 sum - product and bethe approximation
123 illustration of generalized loops .
( a ) original graph .
( b ) ( d ) various generalized loops associated with the graph in ( a ) .
in this particular case , the original graph is a generalized loop for itself .
following chertkov and chernyak ( 123 ) , we dene a generalized loop to
be a subgraph g ( ( cid : 123 ) e ) for which all nodes s v have degree ds ( ( cid : 123 ) e ) ( cid : 123 ) = 123
otherwise stated , for every node s , it either does not belong to g ( ( cid : 123 ) e ) so that ds ( ( cid : 123 ) e ) = 123 , or it has degree ds ( ( cid : 123 ) e ) 123
see figure 123 for an
illustration of the concept of a generalized loop .
note also that a graph without cycles ( i . e . , a tree or forest graph ) does not have any generalized consider a bp xed point for a pairwise mrf with binary vari - ables that is , an ising model ( 123 ) .
for binary variables xs ( 123 , 123 ) , the singleton and edgewise pseudomarginals associated with a bp xed point can be parameterized as
123 s t + st
and st ( xs , xt ) =
123 s t + st 123 ,
some calculation shows that membership in the set l ( g ) is equivalent to imposing the following four inequalities and t st 123 , for each edge ( s , t ) e .
see also example 123 for some discussion of the mean parameters for an ising model .
using this parameterization , for each edge ( s , t ) e , we dene the
s st 123 ,
s ( 123 s ) t ( 123 t ) ,
123 sum - product , bethekikuchi , and expectation - propagation
which extends naturally to the subgraph weight ( cid : 123 ) e : = these denitions , we have ( 123 ) :
( s , t ) ( cid : 123 ) e st
( 123 ) , and let abethe ( ) be
a pairwise markov random eld a bp xed point function value a ( )
the cumulant
proposition 123 .
consider with binary variables is equal to the loop series expansion :
s , s v ; st , ( s , t ) e
a ( ) = abethe ( ) + log
es ( ( xs s ) ds ( ( cid : 123 ) e ) )
before proving proposition 123 , we pause to make some remarks .
by denition , we have
es ( ( xs s ) d ) = ( 123 s ) ( s ) d + s ( 123 s ) d
( 123 s ) d123 + ( 123 ) d ( s ) d123 = s ( 123 s )
parameter s ( 123 , 123 ) .
consequently , for any ( cid : 123 ) e e such that ds ( ( cid : 123 ) e ) = 123 vanishes .
for this reason , only generalized loops ( cid : 123 ) e lead to nonzero
corresponding to dth central moments of a bernoulli variable with for at least one s v , then the associated term in the expansion ( 123 )
terms in the expansion ( 123 ) .
the terms associated with these general - ized loops eectively dene corrections to the bethe estimate abethe ( ) of the cumulant function .
tree - structured graphs do not contain any nontrivial generalized loops , which provides an alternative proof of the exactness of the bethe approximation for trees .
the rst term abethe ( ) in the loop expansion is easily computed from any bp xed point , since it simply corresponds to the optimized value of the bethe free energy ( 123 ) .
however , explicit computation of the full sequence of loop corrections and hence exact calculation of the cumulant function is intractable for general ( nontree ) models .
for instance , any fully connected graph with n 123 nodes has more than 123n generalized loops .
in some cases , accounting for a small set of signicant loop corrections may lead to improved approximations to
123 sum - product and bethe approximation
the partition function ( 123 ) , or more accurate approximations of the marginals for ldpc codes ( 123 ) .
proof of proposition 123 : recall that the canonical overcomplete parameterization ( 123 ) using indicator functions is overcomplete , so that there exist many distinct parameter vectors ( cid : 123 ) = ( cid : 123 ) such that p = p ( cid : 123 ) .
however , the dierence between the left - hand and right - hand sides of the loop series expansion ( 123 ) is identical for any choice of , since moving from ( cid : 123 ) simply shifts both a ( ) and abethe ( ) by some constant .
consequently , it suces to prove the loop series expan - sion for a single parameterization; in particular , we prove it for the reparameterization given by the canonical parameters
and ( cid : 123 ) st ( xs , xt ) = log st ( xs , xt )
( cid : 123 ) s ( xs ) = log s ( xs ) , reparameterization , a little calculation shows that abethe ( ( cid : 123 ) ) = 123
as specied by any bp xed point ( see proposition 123 ) .
for this
sequently , it suces to show that for this particular parameteriza - tion ( 123 ) , we have the equality
a ( ( cid : 123 ) ) = log
es ( ( xs s ) ds ( ( cid : 123 ) e ) )
using the representation ( 123 ) , a little calculation shows that
= 123 + st ( xs s ) ( xt t ) .
by denition of ( cid : 123 ) , we have
let e denote expectation taken with respect distribution fact ( x ) : =
to the product s s ( xs ) .
with this notation , and applying the
123 sum - product , bethekikuchi , and expectation - propagation
identity ( 123 ) , we have
123 + st ( xs s ) ( xt t )
expanding this polynomial and using linearity of expectation , we
st ( xs s ) ( xt t )
recover one term for each nonempty subset ( cid : 123 ) e e of the graphs edges : exp ( a ( ( cid : 123 ) ) ) = 123 + variables .
to evaluate these terms , note that if ds ( ( cid : 123 ) e ) = 123 , it follows that loop ( cid : 123 ) e , in which all connected nodes have degree at least two .
the expression ( 123 ) then follows from the independence structure of fact ( x ) , and standard formulas for the moments of bernoulli random e ( xs s ) = 123
there is thus one loop correction for each generalized
proposition 123 has extensions to more general factor graphs; see the papers ( 123 , 123 ) for more details .
moreover , sudderth et al .
( 123 ) show that the loop series expansion can be exploited to show that for certain types of graphical models with attractive interactions , the bethe value abethe ( ) is actually a lower bound on the true cumulant function value a ( ) .
123 kikuchi and hypertree - based methods
from our development thus far , we have seen that there are two distinct ways in which the bethe variational principle ( 123 ) is an approximate version of the exact variational principle ( 123 ) .
first , for general graphs , the bethe entropy ( 123 ) is only an approxi - mation to the true entropy or negative dual function .
second , the constraint set l ( g ) outer bound on the marginal polytope m ( g ) , as illustrated in figure 123 .
in principle , the accuracy of the bethe variational principle could be strengthened by improving either one , or both , of these components .
this section is devoted to one natural
123 kikuchi and hypertree - based methods
generalization of the bethe approximation , rst proposed by yedidia ( 123 , 123 ) and further explored by various researchers ( 123 , 123 , 123 , 123 , 123 ) , that improves both components simultaneously .
the origins of these methods lie in the statistical physics litera - ture , where they were referred to as cluster variational methods ( 123 , 123 , 123 ) .
at the high level , the approximations in the bethe approach are based on trees , which represent a special case of the junction trees .
a natural strategy , then , is to strengthen the approximations by exploiting more complex junction trees .
these approximations are most easily understood in terms of hypertrees , which represent an alterna - tive way in which to describe junction trees .
accordingly , we begin with some necessary background on hypergraphs and hypertrees .
123 . 123 hypergraphs and hypertrees
a hypergraph g = ( v , e ) is a generalization of a graph , consisting of a vertex set v = ( 123 , .
, m ) , and a set of hyperedges e , where each hyperedge h is a particular subset of v .
the hyperedges form a partially ordered set or poset ( 123 ) , where the partial ordering is specied by inclusion .
we say that a hyperedge h is maximal if it is not contained within any other hyperedge .
with these denitions , we see that an ordinary graph is a special case of a hypergraph , in which each maximal hyperedge consists of a pair of vertices or equivalently , an ordinary edge of the graph . 123
a convenient graphical representation of a hypergraph is in terms of a diagram of its hyperedges , with directed edges representing the inclusion relations; such a representation is known as a poset dia - gram ( 123 , 123 , 123 ) .
figure 123 provides some simple graphical illus - trations of hypergraphs .
any ordinary graph , as a special case of a hypergraph , can be drawn in terms of a poset diagram; in particu - lar , panel ( a ) shows the hypergraph representation of a single cycle on
123 there is a minor inconsistency in our denition of the hyperedge set e and previous graph - theoretic terminology; for hypergraphs ( unlike graphs ) , the set of hyperedges can include a individual vertex ( s ) as an element .
123 sum - product , bethekikuchi , and expectation - propagation
four nodes .
panel ( b ) shows a hypergraph that is not equivalent to an ordinary graph , consisting of two hyperedges of size three joined by
123 graphical representations of hypergraphs .
subsets of nodes corresponding to hyper - edges are shown in rectangles , whereas the arrows represent inclusion relations among hyperedges .
( a ) an ordinary single cycle graph represented as a hypergraph .
( b ) a sim - ple hypertree of width two .
( c ) a more complex hypertree of width three .
their intersection of size two .
shown in panel ( c ) is a more complex hypertree , to which we will return in the sequel .
hypertrees or acyclic hypergraphs provide an alternative way to describe the concept of junction trees , as originally described in sec - tion 123 . 123
in particular , a hypergraph is acyclic if it is possible to spec - ify a junction tree using its maximal hyperedges and their intersections .
the width of an acyclic hypergraph is the size of the largest hyperedge minus one; we use the term k - hypertree to mean an acyclic hypergraph of width k consisting of a single connected component .
thus , for exam - ple , a spanning tree of an ordinary graph is a 123 - hypertree , because its maximal hyperedges , corresponding to ordinary edges in the original graph , all have size two .
as a second example , consider the hypergraph shown in figure 123 ( c ) .
it is clear that this hypergraph is equivalent to the junction tree with maximal cliques ( ( 123 ) , ( 123 ) , ( 123 ) ) and sep - arator sets ( ( 123 ) , ( 123 ) ) .
since the maximal hyperedges have size four , this hypergraph is a hypertree of width three .
123 . 123 hypertree - based factorization and entropy
with this background , we now specify an alternative form of the junc - tion tree factorization ( 123 ) , and show how it leads to a local decom - position of the entropy .
recall that we view the set e of hyperedges
123 kikuchi and hypertree - based methods
as a partially ordered set .
as described in appendix e . 123 , associated with any poset is a mobius function : e e r .
using the set of marginals = ( h , h e ) associated with the hyperedge set , we can dene a new set of functions : = ( h , h e ) as follows :
log h ( xh ) : =
( g , h ) log g ( xg ) .
as a consequence of this denition and the mobius inversion formula ( see lemma e . 123 in appendix e . 123 ) , the marginals can be represented as
log h ( xh ) =
this formula provides a useful recursive method for computing the functions g , as illustrated in the examples to follow .
the signicance of the functions ( 123 ) is that they yield a par - ticularly simple factorization for any hypertree - structured graph .
in particular , for a hypertree with an edge set containing all intersections between maximal hyperedges , the underlying distribution is guaranteed to factorize as follows :
here we use the notation h ( xh; ) to emphasize that h is a function of the marginals .
equation ( 123 ) is an alternative formulation of the well - known junction tree decomposition ( 123 ) .
let us consider some examples to gain some intuition .
example 123 ( hypertree factorization ) .
( a ) first , suppose that the hypertree is an ordinary tree , in which case the hyperedge set consists of the union of the vertex set with the ( ordinary ) edge set .
viewing this hyperedge set as a poset ordered by inclusion , it can be shown ( see appendix e . 123 ) that the mobius function takes the form ( g , g ) = 123 for all g e; ( ( s ) , ( s , t ) ) = 123 for all vertices s and ordinary edges ( s , t ) , and ( g , h ) = 123 for all g that are not contained within h .
consequently , for any ordinary edge ( s , t ) ,
st ( xs , xt ) = st ( xs , xt )
123 sum - product , bethekikuchi , and expectation - propagation
whereas for each vertex , we have s ( xs ) = s ( xs ) .
consequently , in this special case , equation ( 123 ) reduces to the tree factorization in equa -
( b ) turning to a more complex example , consider the acyclic hyper -
graph specied by the hyperedge set
as illustrated in figure 123 ( c ) .
rather than computing the mobius function for this poset , it is more convenient to do the computation via equation ( 123 ) .
in particular , omitting explicit dependence on x for notational simplicity , we rst calculate the singleton function 123 = 123 , and the pairwise function 123 = 123 / 123 , with analogous expressions for the other pairwise terms .
finally , applying the representation ( 123 ) to h = ( 123 ) , we have
123 = 123
= 123 123
similar reasoning yields analogous expressions for 123 and 123
putting the pieces together yields that the density p factorizes as
p = 123
= 123 123 123
which agrees with the expression from the junction tree formula ( 123 ) .
an immediate but important consequence of the factorization ( 123 ) is a local decomposition of the entropy .
this decomposition can be expressed either as a sum of multi - information terms over the hyper - edges , or as a weighted sum of entropy terms .
more precisely , for each hyperedge g e , let us dene the hyperedge entropy
as dened by the marginal h , and the multi - information
123 kikuchi and hypertree - based methods
dened by the marginal h , and function h .
in terms of these quantities , the entropy of any hypertree - structured
distribution has the additive decomposition
which follows immediately from the hypertree factorization ( 123 ) and the denition of ih .
it is also possible to derive an alternative decomposition in terms of the hyperedge entropies .
using the mobius inversion relation ( 123 ) ,
h ( xf ) log f ( xf )
c ( f ) hf ( f ) ,
where we have dened the overcounting numbers
thus , we have the alternative decomposition
we illustrate the decompositions ( 123 ) and ( 123 ) by continuing with
example 123 ( hypertree entropies ) .
( a ) for an ordinary tree , there are two types of multi - information : for an edge ( s , t ) , ist is equivalent to the ordi - nary mutual information , whereas for any vertex s v , the term is is equal to the negative entropy hs .
consequently , in this special case , equation ( 123 ) is equivalent to the tree
123 sum - product , bethekikuchi , and expectation - propagation
entropy given in equation ( 123 ) .
the overcounting num - bers for a tree are c ( ( ( s , t ) ) ) = 123 for any edge ( s , t ) , and c ( ( s ) ) = 123 d ( s ) for any vertex s , where d ( s ) denotes the number of neighbors of s .
consequently , the decomposi - tion ( 123 ) reduces to the alternative form ( 123 ) of the
late i123 = ( cid : 123 )
h123 h123 h123 + h123
( b ) consider again the hypertree in figure 123 ( c ) .
on the basis of our previous calculations in example 123 ( c ) , we calcu - .
the expressions for the other two maximal hyperedges ( i . e . , i123 and i123 ) are analogous .
similarly , we can compute i123 = h123 h123 , with analogous expressions for the other hyperedges of size two .
finally , we have i123 = h123
putting the pieces together and doing some algebra yields hhyper = h123 + h123 + h123 h123 h123
123 . 123 kikuchi and related approximations
recall that the core of the bethe approach of section 123 consists of a par - ticular tree - based ( bethe ) approximation to entropy , and a tree - based outer bound on the marginal polytope .
the kikuchi method and related approximations extend these tree - based approximations to ones based on more general hypertrees , as we now describe .
consider a markov ran - dom eld ( mrf ) dened by some ( non - acyclic ) hypergraph g = ( v , e ) , giving rise to an exponential family distribution p of the form :
note that this equation reduces to our earlier representation ( 123 ) of a pairwise mrf when the hypergraph is an ordinary graph .
let = ( h ) be a collection of local marginals associated with the hyperedges h e .
these marginals must satisfy the obvious normal -
h ) = 123
123 kikuchi and hypertree - based methods
similarly , these local marginals must be consistent with one another wherever they overlap; more precisely , for any pair of hyperedges g h , the marginalization condition ( cid : 123 )
h ) = g ( xg )
must hold .
imposing these normalization and marginalization condi - tions leads to the following constraint set :
123 | conditions ( 123 ) h , and ( 123 ) g h
note that this constraint set is a natural generalization of the tree - based constraint set dened in equation ( 123 ) .
in particular , denition ( 123 ) coincides with denition ( 123 ) when the hypergraph g is an ordinary graph .
as before , we refer to members lt ( g ) as pseudomarginals .
by the junction tree conditions in proposition 123 , the local constraints dening lt ( g ) are sucient to guarantee global consistency whenever g is a hypertree .
in analogy to the bethe entropy approximation , the entropy decom - position ( 123 ) motivates the following hypertree - based approximation to the entropy :
fg ( g , f ) is the overcounting number dened in equa - tion ( 123 ) .
this entropy approximation and the outer bound lt ( g ) on the marginal polytope , lead to the following hypertree - based approximation to the exact variational principle :
( ( cid : 123 ) , ( cid : 123 ) + happ ( ) ) .
this problem is the hypertree - based generalization of the bethe varia - tional problem ( 123 ) .
example 123 ( kikuchi approximation ) .
to illustrate the approx - imate variational principle ( 123 ) , consider the hypergraph shown in
123 sum - product , bethekikuchi , and expectation - propagation
123 123 123 123
123 123 123 123
123 123 123 123
123 ( a ) kikuchi clusters superimposed upon a 123 123 lattice graph .
( b ) hypergraph dened by this kikuchi clustering .
figure 123 ( b ) .
this hypergraph arises from applying the kikuchi clus - tering method ( 123 ) to the 123 123 lattice in panel ( a ) ; see appendix d for more details .
we determine the form of the entropy approximation happ for this hypergraph by rst calculating the overcounting numbers .
by denition , c ( h ) = 123 for each of the four maximal hyperedges ( e . g . , h = ( 123 ) ) .
since each of the 123 - hyperedges has two parents , a little calculation shows that c ( g ) = 123 for each 123 - hyperedge g .
a nal calcu - lation shows that c ( ( 123 ) ) = 123 , so that the overall entropy approximation takes the form :
happ = ( h123 + h123 + h123 + h123 ) ( h123 + h123 + h123 + h123 ) + h123
since the hypergraph in panel ( b ) has treewidth 123 , the appropriate constraint set is the polytope l123 ( g ) , dened over the pseudomarginals ( h , h e ) .
in particular , it imposes nonnegativity constraints , nor - malization constraints , and marginalization constraints of the form :
123 , x123 , x123 ) = 123 ( x123 , x123 ) ,
123 ) = 123 ( x123 ) .
123 . 123 generalized belief propagation
in principle , the variational problem ( 123 ) could be solved by a number of methods .
here we describe a lagrangian - based message - passing algo - rithm that is a natural generalization of the ordinary sum - product updates for the bethe approximation .
as indicated by its name , the
123 kikuchi and hypertree - based methods
dening feature of this scheme is that the only messages passed are from parents to children i . e . , along directed edges in the poset rep - resentation of a hypergraph .
in the hypertree - based variational problem ( 123 ) , the variables correspond to a pseudomarginal h for each hyperedge e e with the earlier derivation of the sum - product algorithm , a lagrangian formulation of this optimization problem leads to a specication of the optimizing pseudomarginals in terms of messages , which rep - resent lagrange multipliers associated with the constraints .
there are various lagrangian reformulations of the original problem ( e . g . , 123 , 123 , 123 ) , which lead to dierent message - passing algorithms .
here we describe the parent - to - child form of message - passing derived by yedidia et al .
( 123 ) .
in order to describe the message - passing updates , it is convenient to dene for a given hyperedge h , the sets of its descendants and ancestors in the following way :
d ( h ) : = ( g e | g h ) , a ( h ) : = ( g e | g h ) .
for example , given the hyperedge h = ( 123 ) in the hypergraph in fig - ure 123 ( c ) , we have a ( h ) = and d ( h ) = ( ( 123 ) , ( 123 ) , ( 123 ) ) .
we use the notation d+ ( h ) and a+ ( h ) as shorthand for the sets d ( h ) h and a ( h ) h , respectively .
given a pair of hyperedges ( f , g ) , we let mfg ( xg ) denote the mes - sage passed from hyperedge f to hyperedge g .
more precisely , this mes - sage is a function over the state space of xg ( e . g . , a multi - dimensional array in the case of discrete random variables ) .
in terms of these mes - sages , the pseudomarginal h in parent - to - child message - passing takes the following form :
% $ ( cid : 123 )
g ( xg; ) = exp ( ( xg ) ) .
the pseudomarginal h includes a compatibility function g for each hyperedge g in the set
in this equation ,
123 sum - product , bethekikuchi , and expectation - propagation d+ ( h ) : = d ( h ) h .
it also collects a message from each hyperedge f / d+ ( h ) that is a parent of some hyperedge g d+ ( h ) .
we illustrate this construction by following up on example 123 .
example 123 ( parent - to - child for kikuchi ) .
in order to illus - trate the parent - to - child message - passing , consider approximation for a 123 123 grid , illustrated in figure 123 .
focusing rst on the hyperedge ( 123 ) , the rst term in equation ( 123 ) species a product of compatibility functions g as g ranges over d+ ( 123 ) , which in this case yields the product 123
we then take the prod - uct over messages from hyperedges that are parents of hyperedges in d+ ( ( 123 ) ) , excluding hyperedges in d+ ( ( 123 ) ) itself .
figure 123 ( a ) provides an illustration; the set d+ ( ( 123 ) ) is given by the hyperedges within the dotted ellipses .
in this case , the set g par ( g ) \d+ ( h ) is given by ( 123 ) and ( 123 ) , corresponding to the parents of ( 123 ) and ( 123 ) , respectively , combined with hyperedges ( 123 ) and ( 123 ) , which are both parents of hyperedge ( 123 ) .
the overall result is an expression of the
by symmetry , the expressions for the pseudomarginals on the other 123 - hyperedges are analogous .
by similar arguments , it is straightforward
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
123 illustration of for parent - to - child message - passing in a kikuchi approximation .
( a ) message - passing for hyperedge ( 123 ) .
set of descendants d+ ( ( 123 ) ) is shown within a dotted ellipse .
relevant parents for 123 consists of the set ( ( 123 ) , ( 123 ) , ( 123 ) , ( 123 ) ) .
( b ) message - passing for hyperedge ( 123 ) .
dotted ellipse shows descendant set d+ ( ( 123 ) ) .
in this case , relevant parent hyperedges are
123 expectation - propagation algorithms
to compute the following expression for 123 and 123 :
123 m ( 123 ) 123 m ( 123 ) 123 m ( 123 ) 123 m ( 123 ) 123
123 m ( 123 ) ( 123 ) m ( 123 ) ( 123 ) m ( 123 ) 123 m ( 123 ) 123 m ( 123 ) 123
generalized forms of the sum - product updates follow by updating the messages so as to enforce the marginalization constraints den - ing membership in l ( g ) ; as in the proof of theorem 123 , xed points of these updates satisfy the necessary stationary conditions of the lagrangian formulation .
further details on dierent variants of gener - alized sum - product updates can be found in various papers ( 123 , 123 , 123 , 123 , 123 ) .
123 expectation - propagation algorithms
there are a variety of other algorithms in the literature that are message - passing algorithms in the spirit of the sum - product algorithm .
examples of such algorithms include the family of expectation - propagation algorithms due to minka ( 123 ) , the related class of assumed density ltering methods ( 123 , 123 , 123 ) , expectation - consistent infer - ence ( 123 ) , structured summary - propagation algorithms ( 123 , 123 ) , and the adaptive tap method of opper and winther ( 123 , 123 ) .
these algorithms are often dened operationally in terms of sequences of local moment - matching updates , with variational principles invoked only to characterize each individual update , not to characterize the overall approximation .
in this section , we show that these algorithms are in fact variational inference algorithms , involving a particular kind of approximation to the exact variational principle in theorem 123 .
the earliest forms of assumed density ltering ( 123 ) were developed for time series applications , in which the underlying graphical model is a hidden markov model ( hmm ) , as illustrated in figure 123 ( a ) .
as we have discussed , for discrete random variables , the marginal distri - butions for an hmm can be computed using the forwardbackward algorithm , corresponding to a particular instantiation of the sum - product algorithm .
similarly , for a gaussmarkov process , the kalman lter also computes the means and covariances at each node of an hmm .
however , given a hidden markov model
123 sum - product , bethekikuchi , and expectation - propagation
continuous random variables , the message mts passed from node t to s is a real - valued function , and hence dicult to store and transmit . 123 the purpose of assumed density ltering is to circumvent the compu - tational challenges associated with passing function - valued messages .
instead , assumed density ltering ( adf ) operates by passing approxi - mate forms of the messages , in which the true message is approximated by the closest member of some tractable class .
for instance , a general continuous message might be approximated with a gaussian message .
this procedure of computing the message approximations , if closeness is measured using the kullbackleibler divergence , can be expressed in terms of moment - matching operations .
minka ( 123 ) observed that the basic ideas underlying adf can be generalized beyond markov chains to arbitrary graphical models , an insight that forms the basis for the family of expectation - propagation ( ep ) algorithms .
as with assumed density ltering , expectation - propagation ( 123 , 123 ) and various related algorithms ( 123 , 123 , 123 ) are typically described in terms of moment - matching operations .
to date , the close link between these algorithms and the bethe approximation does not appear to have been widely appreciated .
in this section , we show that these algorithms are methods for solving certain relaxations of the exact variational principle from theorem 123 , using bethe - like entropy approximations and particular convex outer bounds on the set m .
more specically , we recover the moment - matching updates of expectation - propagation as one particular type of lagrangian method for solving the resulting optimization problem , thereby showing that expectation - propagation algorithms belong to the same class of vari - ational methods as belief propagation .
this section is a renement of results from the thesis ( 123 ) .
123 . 123 entropy approximations based on term decoupling
we begin by developing a general class of entropy approximations based on decoupling an intractable collection of terms .
given a collection of
123 the gaussian case is special , in that this functional message can always be parameterized in terms of its mean and variance , and the eciency of kalman ltering stems from
random variables ( x123 , .
, xm ) rm , consider a collection of sucient statistics that is partitioned as
123 expectation - propagation algorithms
123 , 123 ,
where i are univariate statistics and the i are generally multivariate .
as will be made explicit in the examples to follow , the partitioning sep - arates the tractable from the intractable components of the associated exponential family distribution .
let us set up various exponential families associated with sub - collections of ( , ) .
first addressing the tractable component , the vector - valued function : x m rdt has an associated vector of canon - ical parameters rdt .
turning to the intractable component , for each i = 123 , .
, di , the function i maps from x m to rb , and the all , the function = ( 123 , .
, di ) maps from x m to rbdi , and has
vector ( cid : 123 ) i rb is the associated set of canonical parameters .
over - the associated canonical parameter vector ( cid : 123 ) rbdi , partitioned as ( cid : 123 ) ( cid : 123 ) 123 , ( cid : 123 ) 123 , .
, ( cid : 123 ) di p ( x; , ( cid : 123 ) ) f123 ( x ) exp
these families of sucient statistics dene the
( cid : 123 ) ( cid : 123 ) , ( x ) ( cid : 123 ) ( cid : 123 ) di ( cid : 123 )
we say that any density p of the form ( 123 ) belongs to the ( , ) -
next , we dene the base model
p ( x; , ( cid : 123 ) 123 ) f123 ( x ) exp
set ( cid : 123 ) = ( cid : 123 ) 123
we say that any distribution p of the form ( 123 ) belongs to
in which the intractable sucient statistics play no role , since we have the - exponential family .
similarly , for each index i ( 123 , .
, di ) , we dene the i - augmented distribution
in which only a single term i has been introduced , and we say that any p of the form ( 123 ) belongs to the ( , i ) - exponential family .
p ( x; , ( cid : 123 ) i ) f123 ( x ) exp
123 sum - product , bethekikuchi , and expectation - propagation
the basic premises in the tractableintractable partitioning between
first , it is possible to compute marginals exactly in polyno - mial time for distributions of the base form ( 123 ) that is , for any member of the - exponential family .
second , for each index i = 123 , .
, di , exact polynomial - time computation is also possible for any distribution of the i - augmented form ( 123 ) that is , for any member of the third , it is intractable to perform exact computations in the full ( , ) - exponential family ( 123 ) , since it simultaneously incorporates all of the terms ( 123 , .
, di ) .
example 123 ( tractable / intractable partitioning for mixture models ) .
let us illustrate the partitioning scheme ( 123 ) with the example of a gaussian mixture model .
suppose that the random vec - tor x rm has a multivariate gaussian distribution , n ( 123 , ) .
letting ( y; , ) denote the density of a random vector with a n ( , ) distri - bution , consider the two - component gaussian mixture model
p ( y | x = x ) = ( 123 ) ( y;123 , 123
123i ) + ( y; x , 123
where ( 123 , 123 ) is the mixing weight , 123 i is the m m identity matrix .
123 and 123
123 are the variances , and
given n i . i . d .
samples y123 , .
, yn from the mixture density ( 123 ) , it is frequently of interest to compute marginals under the posterior distribution of x conditioned on ( y123 , .
assuming a multivariate gaussian prior x n ( 123 , ) , and using bayes theorem , we nd that the posterior takes the form :
p ( x | y123 .
, yn ) exp
p ( yi | x = x )
log p ( yi | x = x )
123 expectation - propagation algorithms
to cast this model as a special case of the partitioned exponen - 123 xt 123x tial family ( 123 ) , we rst observe that the term exp can be identied with the base term f123 ( x ) exp ( ( cid : 123 ) , ( x ) ( cid : 123 ) ) , so that we have dt = m .
on the other hand , suppose that we dene tion , the term exp ( ( cid : 123 ) i ( x ) : = log p ( yi | x = x ) for each i = 123 , .
since the observation i=123 exp ( ( cid : 123 ) ( cid : 123 ) i , i ( x ) ( cid : 123 ) in equation ( 123 ) .
note that we have di = n and yi is a xed quantity , this denition makes sense .
with this deni - i=123 log p ( yi | x = x ) ) corresponds to the product b = 123 , with ( cid : 123 ) i = 123 for all i = 123 ,
as a particular case of the ( , ) - exponential family in this setting , the base distribution p ( x; , ( cid : 123 ) 123 ) exp ( 123 123 xt 123x ) corresponds to a mul - tivariate gaussian , for which exact calculations of marginals is possible in o ( m123 ) time .
similarly , for each i = 123 , .
, n , the i - augmented dis - tribution ( 123 ) is proportional to
( ( 123 ) ( yi;123 , 123
123i ) + ( yi; x , 123
this distribution is a gaussian mixture with two components , so that it is also possible to compute marginals exactly in cubic time .
however , the full distribution ( 123 ) is a gaussian mixture with 123n components , so that the complexity of computing exact marginals is exponential in the problem size .
returning to the main thread , let us develop a few more features of the distribution of interest ( 123 ) .
since it is an exponential family , it
has a partitioned set of mean parameters ( , ( cid : 123 ) ) rdt rdib , where
and ( ( cid : 123 ) 123 , .
, ( cid : 123 ) di ) = e ( 123 ( x ) , .
, di ( x ) ) .
as an exponential family , the general variational principle from theo - rem 123 is applicable .
as usual , the relevant quantities in the variational principle ( 123 ) are the set
( , ( cid : 123 ) ) | ( , ( cid : 123 ) ) = ep ( ( ( x ) , ( x ) ) )
and the entropy , or negative dual function h ( , ( cid : 123 ) ) = a
for some p
our assumption that exact computation under the full distribu - tion ( 123 ) is intractable , there must be challenges associated with
123 sum - product , bethekikuchi , and expectation - propagation
characterizing the mean parameter space ( 123 ) and / or the entropy function .
accordingly , we now describe a natural approximation to these quantities , based on the partitioned structure of the distri - bution ( 123 ) , that leads to the class of expectation - propagation
associated with the base distribution ( 123 ) is the set
rdt | = ep ( ( x ) )
for some density p
corresponding to the globally realizable mean parameters for the base distribution viewed as a dt - dimensional exponential family .
by theo - rem 123 , for any m ( ) , there exists an exponential family mem - ber p ( ) that realizes it .
moreover , by our assumption that the base distribution is tractable , we can compute the entropy h ( ) of this distribution .
similarly , for each i = 123 , .
, di , the i - augmented distri - bution ( 123 ) is associated with the mean parameter space m ( , i ) ( , ( cid : 123 ) i ) rdt rb | ( , ( cid : 123 ) i ) = ep ( ( ( x ) , i ( x ) ) ) by similar reasoning as above , for any ( , ( cid : 123 ) i ) m ( , i ) , there is a moreover the entropy h ( , ( cid : 123 ) i ) can be computed easily .
( , ( cid : 123 ) ) rdt rdib , we dene for each i = 123 , 123 , .
, di the coordinate
with these basic ingredients , we can now dene an outer bound on the set m ( , ) .
given a candidate set of mean parameters projection operator i : rdt rdib rdt rb that operates as
member of the ( , i ) - exponential family with these mean parameters .
furthermore , since we have assumed that the i - distribution ( 123 ) is tractable , it is easy to determine membership in the set m ( ;i ) , and
for some density p
( , ( cid : 123 ) ) i ( , ( cid : 123 ) i ) rdt rb .
we then dene the set
l ( ; ) : = ( ( , ( cid : 123 ) ) | m ( ) , i ( , ( cid : 123 ) ) m ( , i ) i = 123 , .
, di ) .
note that l ( ; ) is a convex set , and moreover it is an outer bound on the true set m ( ; ) of mean parameters .
123 expectation - propagation algorithms
we now develop an entropy approximation that is tailored to the
structure of l ( ; ) .
we begin by observing that for any ( , ( cid : 123 ) ) l ( ; ) family with mean parameters ( , ( cid : 123 ) i ) .
this assertion follows because the projected mean parameters ( , ( cid : 123 ) i ) belong to m ( ;i ) , so that the - orem 123 can be applied .
we let h ( , ( cid : 123 ) i ) denote the entropy of this
and for each i = 123 , .
, di , there is a member of the ( , i ) - exponential
exponential family member .
similarly , since m ( ) by denition of l ( ; ) , there is a member of the - exponential family with mean parameter ; we denote its entropy by h ( ) .
with these ingredients , we dene the following term - by - term entropy approximation
hep ( , ( cid : 123 ) ) : = h ( ) +
h ( , ( cid : 123 ) ( cid : 123 ) ) h ( ) ( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) + ( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) + hep ( , ( cid : 123 ) )
combining this entropy approximation with the convex outer bound ( 123 ) yields the optimization problem
this optimization problem is an approximation to the exact variational principle from theorem 123 for the ( , ) - exponential family ( 123 ) , and it underlies the family of expectation - propagation algorithms .
it is closely related to the bethe variational principle , as the examples to follow should clarify .
example 123 ( sum - product and bethe approximation ) .
to provide some intuition , let us consider the bethe approximation from the point of view of the variational principle ( 123 ) .
more specically , we derive the bethe entropy approximation ( 123 ) as a particular case of the term - by - term entropy approximation ( 123 ) , and the tree - based outer bound l ( g ) from equation ( 123 ) as a particular case of the con - vex outer bound ( 123 ) on m ( ; ) .
consider a pairwise markov random eld based on an undirected graph g = ( v , e ) , involving a discrete variable xs ( 123 , 123 , .
, rs 123 ) at each vertex s v ; as we have seen , this can be expressed as an exponential family in the form ( 123 ) using the standard overcomplete parameterization ( 123 ) with indicator functions .
taking the point of
123 sum - product , bethekikuchi , and expectation - propagation
view of ( 123 ) , we partition the sucient statistics in the following way .
the sucient statistics associated with nodes are dened to be the tractable set , and those associated with edges are dened to be the intractable set .
thus , using the functions s and st dened in equation ( 123 ) , the base distribution ( 123 ) takes the form
p ( x; 123 , .
, m , ( cid : 123 ) 123 )
in this particular case , the terms i to be added correspond to the functions st that is , the index i runs over the edge set of the graph .
for edge ( u , v ) , the uv - augmented distribution ( 123 ) takes the form :
p ( x; 123 , .
, m , uv )
the mean parameters associated with the standard overcomplete representation are singleton and pairwise marginal distributions , which we denote by ( s , s v ) and uv respectively .
the entropy of the base distribution depends only on the singleton marginals , and given the product structure ( 123 ) , takes the simple form
, m ) =
where h ( s ) = ( cid : 123 )
s ( xs ) log s ( xs ) is the entropy of the marginal distribution .
similarly , since the augmented distribution ( 123 ) has only a single edge added and factorizes over a cycle - free graph , its entropy has the explicit form
h ( uv ) h ( u ) h ( v )
, m , uv ) =
where h ( uv ) : = ( cid : 123 )
uv ( xu , xv ) log uv ( xu , xv ) is the joint entropy , and i ( uv ) : = h ( u ) + h ( v ) h ( uv ) is the mutual putting together the pieces , we nd that the term - by - term entropy approximation ( 123 ) for this particular problem has the form :
123 expectation - propagation algorithms
which is precisely the bethe entropy approximation dened previ - next , we show how the outer bound l ( ; ) dened in equa - tion ( 123 ) specializes to l ( g ) .
consider a candidate set of local
s , s v ; st , ( s , t ) e
in the current setting , the set m ( ) corresponds to the set of all glob - ally realizable marginals ( s , s v ) under a factorized distribution , so that the inclusion ( s , s v ) m ( ) is equivalent to the nonnegativ - ity constraints s ( xs ) 123 for all xs xs , and the local normalization
s ( xs ) = 123
for all s v .
recall that the index i runs over edges of the graph; instance i = ( u , v ) , then the projected marginals uv ( ) are given by ( 123 , .
, m , uv ) .
the set m ( ;uv ) is traced out by all globally consis - tent marginals of this form , and is equivalent to the marginal polytope m ( guv ) , where guv denotes the graph with a single edge ( u , v ) .
since this graph is a tree , the inclusion uv ( ) m ( ;uv ) is equivalent to having uv ( ) satisfy in addition to the nonnegativity and local normalization constraints ( 123 ) the marginalization conditions
uv ( xu , xv ) = u ( xu ) ,
uv ( xu , xv ) = v ( xv ) .
therefore , the full collection of inclusions uv ( ) m ( ;uv ) , as ( u , v ) runs over the graph edge set e , specify the same conditions dening the rst - order relaxed constraint set l ( g ) from equation ( 123 ) .
123 . 123 optimality in terms of moment - matching
returning to the main thread , we now derive a lagrangian method for attempting to solve the expectation - propagation variational prin - ciple ( 123 ) .
as we will show , these lagrangian updates reduce to moment - matching , so that the usual expectation - propagation updates
123 sum - product , bethekikuchi , and expectation - propagation
our lagrangian formulation is based on the following two steps : first , we augment the space of pseudo - mean - parameters over which we optimize , so that the original constraints dening l ( ; ) are decoupled .
second , we add new constraints to be penalized with lagrange multipliers so as to enforce the remaining con - straints required for membership in l ( ; ) .
beginning with the augmentation step , let us duplicate the vector rdt a total of di times , dening thereby dening di new vec - tors i rdt and imposing the constraint that i = for each index i = 123 , .
this yields a large collection of pseudo - mean - parameters
( , ( i , ( cid : 123 ) i ) , i = 123 , .
, di ) rdt ( rdt rb ) di ,
and we recast the variational principle ( 123 ) using these pseudo - mean - parameters as follows :
h ( i , ( cid : 123 ) i ) h ( i ) f ( ; ( i , ( cid : 123 ) i ) ) subject to the constraints ( i , ( cid : 123 ) i ) m ( ;i ) for all i = 123 , .
, di , and
( cid : 123 ) ( cid : 123 ) i , ( cid : 123 ) i ( cid : 123 ) + h ( ) +
( cid : 123 ) , ( cid : 123 ) +
i = 123 , .
, di .
let us now consider a particular iterative scheme for solving the reformulated problem ( 123 ) .
in particular , for each i = 123 , .
, di , dene a vector of lagrange multipliers i rdt associated with the con - straints = i .
we then form the lagrangian function
l ( ; ) = ( cid : 123 ) , ( cid : 123 ) +
( cid : 123 ) ( cid : 123 ) i , ( cid : 123 ) i ( cid : 123 ) + f ( ; ( i , ( cid : 123 ) i ) ) +
m ( ) and ( i , ( cid : 123 ) i ) m ( ;i ) explicitly .
this lagrangian is a partial one , since we are enforcing the constraints
123 expectation - propagation algorithms
consider an optimum solution ( , ( i , ( cid : 123 ) i ) , i = 123 , .
, di ) of the opti - each i = 123 , .
, di , the vector ( i , ( cid : 123 ) i ) belongs to the relative interior
mization problem ( 123 ) that satises the following properties : ( a ) the vector belongs to the ( relative ) interior m ( ) , and ( b ) for m ( ;i ) .
any such solution must satisfy the zero - gradient conditions associated with the partial lagrangian ( 123 ) namely
for i = 123 , .
, di , and
l ( ; ) = 123 , ( i , ( cid : 123 ) i ) l ( ; ) = 123 l ( ; ) = 123
since the vector belongs to m ( ) , it species a distribution in the - exponential family .
by explicitly computing the lagrangian con - dition ( 123a ) and performing some algebra essentially the same steps as the proof of theorem 123 we nd that this exponential fam - ily member can be written , in terms of the original parameter vector and the lagrange multipliers , as follows :
q ( x; , ) f123 ( x ) exp
similarly , since for each i = 123 , .
, di , the vector ( , ( cid : 123 ) i ) belongs to qi ( x; , ( cid : 123 ) i , ) f123 ( x ) exp
m ( ;i ) , it also species a distribution in the ( , i ) - exponential fam - ily .
explicitly computing the condition ( 123b ) and performing some algebra shows that this distribution can be expressed as
+ ( cid : 123 ) ( cid : 123 ) i , i ( x ) ( cid : 123 )
the nal lagrangian condition ( 123c )
( 123 ) are satised .
noting that = eq ( ( x ) ) and qi ( ( x ) ) , these constraints reduce to the moment - matching
q ( x; , ) ( x ) ( dx ) =
qi ( x; , ( cid : 123 ) i , ) ( x ) ( dx ) ,
for i = 123 ,
on the basis of equations ( 123 ) , ( 123 ) , and ( 123 ) , we arrive at the expectation - propagation updates , as summarized in figure 123 .
123 sum - product , bethekikuchi , and expectation - propagation
expectation - propagation ( ep ) updates :
( 123 ) at iteration n = 123 ,
initialize the lagrange multiplier
( 123 ) at each iteration , n = 123 , 123 , .
, choose some index
vectors ( 123 , .
, di ) .
i ( n ) ( 123 , .
, di ) , and
( a ) using equation ( 123 ) , form the augmented distribution
qi ( n ) and compute the mean parameter
qi ( n ) ( x ) ( x ) ( dx ) = e
( b ) using equation ( 123 ) , form the base distribution q and adjust i ( n ) to satisfy the moment - matching condition
eq ( ( x ) ) = i ( n ) .
123 steps involved in the expectation - propagation updates .
it is a lagrangian algorithm for attempting to solve the bethe - like approximation ( 123 ) , or equivalently ( 123 ) .
we note that the algorithm is well dened , since for any realizable mean parameter , there is always a solution for i ( n ) in equation ( 123 ) .
this fact can be seen by considering the dt - dimensional exponential family dened by the pair ( i ( n ) , ) , and then applying theorem 123 .
moreover , it follows that any xed point of these ep updates satises the neces - sary lagrangian conditions for optimality in the program ( 123 ) .
from this fact , we make an important conclusion namely , xed points of the ep updates are guaranteed to exist , assuming that the optimiza - tion problem ( 123 ) has at least one optimum .
as with the sum - product algorithm and the bethe variational principle , there are no guarantees that the ep updates converge in general .
however , it would be rel - atively straightforward to develop convergent algorithms for nding at least a local optimum of the variational problem ( 123 ) , or equiva - lently ( 123 ) , possibly along the lines of convergent algorithms devel - oped for the ordinary bethe variational problem or convexied versions thereof ( 123 , 123 , 123 , 123 ) .
123 expectation - propagation algorithms
at a high level , the key points to take away are the following : within the variational framework , expectation - propagation algorithms are based on a bethe - like entropy approximation ( 123 ) , and a partic - ular convex outer bound on the set of mean parameters ( 123 ) .
more - over , the moment - matching steps in the ep algorithm arise from a lagrangian approach for attempting to solve the relaxed variational
we conclude by illustrating the specic forms taken by the algo -
rithm in figure 123 for some concrete examples :
example 123 ( sum - product as moment - matching ) .
as a con - tinuation of example 123 , we now show how the updates ( 123 ) and ( 123 ) of figure 123 reduce to the sum - product updates .
in this particular example , the index i ranges over edges of the graph .
associated with i = ( u , v ) are the pair of lagrange multi - plier vectors ( uv ( xv ) , vu ( xu ) ) .
in terms of these quantities , the base distribution ( 123 ) can be written as
q ( x; , )
uv ( xv ) + vu ( xu )
or more compactly as q ( x; , ) ( cid : 123 )
sv s ( xs ) , where we have dened
it is worthwhile noting the similarity to the sum - product expres - sion ( 123 ) for the singleton pseudomarginals s , obtained in the proof of theorem 123 .
similarly , when ( cid : 123 ) = ( u , v ) , the augmented distribution ( 123 ) is given by q ( u , v ) ( x; ; ) q ( x; , ) exp
uv ( xu , xv ) vu ( xu ) uv ( xv ) uv ( xu , xv ) vu ( xu ) uv ( xv )
123 sum - product , bethekikuchi , and expectation - propagation
now consider the updates ( 123 ) and ( 123 ) .
if index i = ( u , v ) is chosen , the update ( 123 ) is equivalent to computing the single - ton marginals of the distribution ( 123 ) .
the update ( 123 ) dictates that the lagrange multipliers uv ( xv ) and vu ( xu ) should be adjusted so that the marginals ( u , v ) of the distribution ( 123 ) match these marginals .
following a little bit of algebra , these xed point con - ditions reduce to the sum - product updates along edge ( u , v ) , where the messages are given as exponentiated lagrange multipliers by muv ( xv ) = exp ( uv ( xv ) ) .
from the perspective of term - by - term approximations , the sum - product algorithm uses a product base distribution ( see equa - tion ( 123 ) ) .
a natural extension , then , is to consider a base distribution with more structure .
example 123 ( tree - structured ep ) .
we illustrate this idea by deriving the tree - structured ep algorithm ( 123 ) , applied to a pairwise markov random eld on a graph g = ( v , e ) .
we use the same set - up and parameterization as in examples 123 and 123 .
given some xed spanning tree t = ( v , e ( t ) ) of the graph , the base distribution ( 123 ) is given by
in this case , the index i runs over edges ( u , v ) e\e ( t ) ; given such an edge ( u , v ) , the uv - augmented distribution ( 123 ) is obtained by adding in the term uv :
p ( x; , uv ) p ( x; , ( cid : 123 ) 123 ) exp
let us now describe the analogs of the sets m ( ; ) , m ( ) , and m ( ;i ) for this setting .
first , as we saw in example 123 , the subvectro of mean parameters associated with the full model ( 123 ) is given by
s , s v ; st , ( s , t ) e
accordingly , the analog of m ( ; ) is the marginal polytope m ( g ) .
second , if we use the tree - structured distribution ( 123 ) as the base
123 expectation - propagation algorithms
distribution , the relevant subvector of mean parameters is
s , s v ; st , ( s , t ) e ( t )
( t ) : =
the analog of the set m ( ) is the marginal polytope m ( t ) , or equiv - alently using proposition 123 the set l ( t ) .
finally , if we add in edge i = ( u , v ) / e ( t ) , then the analog of the set m ( ;i ) is the marginal polytope m ( t ( u , v ) ) .
the analog of the set l ( ; ) is an interesting object , which we refer to as the tree - ep outer bound on the marginal polytope m ( g ) .
it consists of all vectors =
s , s v ; st , ( s , t ) e
( a ) the inclusion ( t ) m ( t ) holds , and ( b ) for all ( u , v ) / t , the inclusion ( ( t ) , uv ) m ( t ( u , v ) )
observe that the set thus dened is contained within l ( g ) : in partic - ular , the condition ( t ) m ( t ) ensures nonnegativity , normalization , and marginalization for all mean parameters associated with the tree , and the inclusion ( ( t ) , uv ) m ( t ( u , v ) ) ensures that uv is non - negative , and satises the marginalization constraints associated with u and v .
for graphs with cycles , this tree - ep outer bound is strictly contained within l ( g ) ; for instance , when applied to the single cycle c123 on three nodes , the tree - ep outer bound is equivalent to the marginal polytope ( 123 ) .
but the set m ( c123 ) is strictly contained within l ( c123 ) , as shown in example 123 .
the entropy approximation hep associated with tree - ep is easy to dene .
given a candidate set of pseudo - mean - parameters in the tree - ep outer bound , we dene the tree - structured entropy associated with the base distribution ( 123 )
h ( ( t ) ) : =
( u , v ) / e ( t ) , we dene
h ( ( t ) , uv ) associated with the augmented distribution ( 123 ) ; unlike the base case , this entropy does not have an explicit form ( since the graph is not in junction tree form ) , but it can be computed easily .
123 sum - product , bethekikuchi , and expectation - propagation
the tree - ep entropy approximation then takes the form :
h ( ( t ) , uv ) h ( ( t ) )
hep ( ) = h ( ( t ) ) +
( u , v ) / e ( t )
let us derive the moment - matching updates
and ( 123 ) for tree - ep .
for each edge ( u , v ) / e ( t ) , let uv ( t ) denote a vector of lagrange multipliers , of the same dimension as ( t ) , and let ( x; t ) denote the subset of sucient statistics associated with t .
the optimized base distribution ( 123 ) can be represented in terms of the original base distribution ( 123 ) and these tree - structured lagrange
q ( x; , ) p ( x; , ( cid : 123 ) 123 )
( u , v ) / e ( t )
exp ( ( cid : 123 ) uv ( t ) , ( x; t ) ( cid : 123 ) ) .
if edge ( u , v ) is added , the augmented distribution ( 123 ) is given by quv ( x; , ) p ( x; , ( cid : 123 ) 123 ) exp ( uv ( xu , xv ) ) exp ( ( cid : 123 ) uv ( t ) , ( x; t ) ( cid : 123 ) ) .
for a given edge ( u , v ) / e ( t ) , the moment - matching step ( 123 ) cor - responds to computing singleton marginals and pairwise marginals for edges ( s , t ) e ( t ) under the distribution quv ( x; , ) .
the step ( 123 ) corresponds to updating the lagrange multiplier vector uv ( t ) so the marginals of q ( x; ; ) agree with those of quv ( x; ; ) for all nodes s v , and all edges ( s , t ) e ( t ) .
see the papers ( 123 , 123 , 123 ) for further details on tree ep .
the previous examples dealt with discrete random variables; we now return to the case of a mixture model with continuous variables , rst introduced in example 123 .
example 123 ( ep for gaussian mixture models ) .
recall from equation ( 123 ) the representation of the mixture distribution as an exponential family with the base potentials ( x ) = ( x; xxt ) and auxi - liary terms i ( x ) = log p ( yi | x ) , for i = 123 , .
turning to the analogs of the various mean parameter spaces , the set m ( ; ) is a subset of
123 expectation - propagation algorithms
rm s m parameters partitioned as
+ rn , corresponding the space of all globally realizable mean
e ( x ) ; e ( xx t ) ; e ( log p ( yi | x ) ) ,
i = 123 ,
recall that ( y123 , y123 , .
, yn ) are observed , and hence remain xed throughout the derivation .
the set m ( ) is a subset of rm s m
+ , corresponding to the mean parameter space for a multivariate gaussian , as characterized in exam - ple 123 .
for each i = 123 , .
, n , the constraint set m ( ;i ) is a subset of rm s m + r , corresponding to the collection of all globally realizable mean parameters with the partitioned form
e ( x ) , e ( xx t ) , e ( log p ( yi | x ) )
turning to the various entropies , the base term is the entropy h ( ) of a multivariate gaussian where denotes the pair ( e ( x ) , e ( xx t ) ) ; family member with sucient statistics ( ( x ) , log p ( yi | x ) ) , and mean
similarly , the augmented term h ( , ( cid : 123 ) i ) is the entropy of the exponential parameters ( , ( cid : 123 ) i ) , where ( cid : 123 ) i : = e ( log p ( yi | x ) ) ) .
using these quantities ,
we can dene the analog of the variational principle ( 123 ) .
turning to the moment - matching steps ( 123 ) and ( 123 ) , the lagrangian version of gaussian - mixture ep algorithm can be formu - lated in terms of a collection of n matrix - vector lagrange multiplier
( i , i ) rm rmm ,
i = 123 ,
one for each augmented distribution .
since x n ( 123 , ) by denition , the optimized base distribution ( 123 ) can be written in terms of 123 and the lagrange multipliers
i , x ( cid : 123 ) +
note that this is simply a multivariate gaussian distribution .
the aug - mented distribution ( 123 ) associated with any term i ( 123 , 123 , .
, di ) ,
123 sum - product , bethekikuchi , and expectation - propagation
denoted by qi , takes the form
( cid : 123 ) , x ( cid : 123 ) +
+ ( cid : 123 ) ( cid : 123 ) i , log p ( yi | x ) ( cid : 123 ) ( cid : 123 )
with this set - up , the step ( 123 ) corresponds to computing the mean qi ( xx t ) ) under the distribution ( 123 ) , and step ( 123 ) corresponds to adjusting the multivariate gaussian ( 123 ) so that it has these mean parameters .
fixed points of these moment - matching updates satisfy the necessary lagrangian conditions to be optima of the associated bethe - like approximation of the exact varia -
we refer the reader to minka ( 123 , 123 ) and seeger ( 123 ) for fur - ther details of the gaussian - mixture ep algorithm and some of its
mean field methods
this section is devoted to a discussion of mean eld methods , which originated in the statistical physics literature ( e . g . , 123 , 123 , 123 ) .
from the perspective of this survey , the mean eld approach is based on a specic type of approximation to the exact variational principle ( 123 ) .
more specically , as discussed in section 123 , there are two fundamental diculties associated with the variational principle ( 123 ) : the nature of the constraint set m , and the lack of an explicit form for the dual .
the core idea of mean eld approaches is simple : let us limit the optimization to a subset of distributions for which both m and are relatively easy to characterize; e . g . , perhaps they correspond to a graph with small treewidth .
throughout this section , we refer to any such distribution as tractable .
the simplest choice is the family of product distributions , which gives rise to the naive mean eld method .
higher - order mean eld methods are obtained by choosing tractable distributions with more structure .
123 tractable families
given a graphical model based on a graph g , we base our description of mean eld methods on the notion of a tractable subgraph , by which
123 mean field methods
we mean a subgraph f of the graph g over which it is feasible to per - form exact calculations .
the simplest example of a tractable subgraph is the fully disconnected subgraph f123 = ( v , ) , which contains all the vertices of g but none of the edges .
any distribution that is markov with respect to f is then a product distribution , for which exact com - putations are trivial .
a bit more generally , consider an exponential family with a collec - tion = ( , i ) of sucient statistics associated with the cliques of g = ( v , e ) .
given a subgraph f , let i ( f ) i be the subset of sucient statistics associated with cliques of f .
the set of all distributions that are markov with respect to f is a sub - family of the full - exponential family; it is parameterized by the subspace of canonical parameters
( f ) : = ( | = 123 i\i ( f ) ) .
we consider some examples to illustrate : example 123 ( tractable subgraphs ) .
suppose that para - meterizes a pairwise markov random eld , with potential tions associated with the vertices and edges of an undirected graph g = ( v , e ) .
for each edge ( s , t ) e , let ( s , t ) denote the subvector of parameters associated with sucient statistics that depend only on ( xs , xt ) .
consider the completely disconnected subgraph f123 = ( v , ) .
with respect to this subgraph , permissible parameters must belong to
| ( s , t ) = 123 ( s , t ) e
the densities in this sub - family are all of the fully factorized or product
where s refers to the subvector of canonical parameters associated with
to obtain a more structured approximation , one could choose a spanning tree t = ( v , e ( t ) ) .
in this case , we are free to choose the canonical parameters corresponding to vertices and edges in the tree
123 optimization and lower bounds
t , but we must set to zero any canonical parameters corresponding to edges not in the tree .
accordingly , the subspace of tree - structured distributions is specied by the subset of canonical parameters
| ( s , t ) = 123 ( s , t ) / e ( t )
( t ) : =
associated with the exponential family dened by and g is the set m ( g; ) of all mean parameters realizable by any distribution , as previously dened in equation ( 123 ) .
( whereas our previous nota - tion did not make explicit reference to g and , the denition of m does depend on these quantities and it is now useful to make this dependence explicit . ) for a given tractable subgraph f , mean eld methods are based on optimizing over the subset of mean parame - ters that can be obtained by the subset of exponential family densities ( p , ( f ) ) namely
mf ( g; ) : = ( rd | = e ( ( x ) )
for some ( f ) ) .
in terms of the moment mapping from theorem 123 , a more compact denition of the set mf ( g; ) is as the image a ( ( f ) ) .
by theo - rem 123 , we have m ( g; ) = a ( ) so that the inclusion
f ( g; ) m
holds for any subgraph f .
for this reason , we say that mf is an inner approximation to the set m of realizable mean parameters .
to lighten the notation in the remainder of this section , we gener - ally drop the term from m ( g; ) and mf ( g; ) , writing m ( g ) and mf ( g ) , respectively .
it is important to keep in mind , though , that these sets do depend on the choice of sucient statistics .
123 optimization and lower bounds
we now have the necessary ingredients to develop the mean eld approach to approximate inference .
suppose that we are interested in approximating some target distribution p , where .
mean eld methods generate lower bounds on the value a ( ) of the cumulant func - tion , as well as approximations to the mean parameters = e ( ( x ) ) of this target distribution p .
123 mean field methods
123 . 123 generic mean field procedure
the key property of any mean eld method is the following fact : any valid mean parameter species a lower bound on the log partition
proposition 123 ( mean field lower bound ) .
any mean parame - ter m yields a lower bound on the cumulant function :
a ( ) ( cid : 123 ) , ( cid : 123 ) a
moreover , equality holds if and only if and are dually coupled ( i . e . ,
in convex analysis , this lower bound is known as fenchels inequality .
it is an immediate consequence of the general variational principle ( 123 ) , since the supremum over is always greater than the value at any particular .
moreover , the supremum is attained when = a ( ) , which means that and are dually coupled ( by
an alternative proof via jensens inequality is also possible , and given its popularity in the literature on mean eld methods , we give a version of this proof here .
by denition of m , for any mean parameter m , there must exist some density q , dened with respect to the base measure underlying the exponential family , for which eq ( ( x ) ) = .
we then have
a ( ) = log
q ( x ) ( ( cid : 123 ) , ( x ) ( cid : 123 ) log q ( x ) ) ( dx )
( b ) = ( cid : 123 ) , ( cid : 123 ) + h ( q ) ,
where h ( q ) = eq ( log q ( x ) ) is the entropy of q .
in this argument , step ( a ) follows from jensens inequality ( see appendix a . 123 ) applied to the negative logarithm , whereas step ( b ) follows from the moment - matching condition eq ( ( x ) ) = .
since the lower bound ( 123 ) holds
123 optimization and lower bounds
for any density q satisfying this moment - matching condition , we may optimize over the choice of q : by theorem 123 , doing so yields the exponential family density q
( x ) = p ( ) ( x ) , for which h ( q
since the dual function a
typically lacks an explicit form , it is not possible , at least in general , to compute the lower bound ( 123 ) .
the mean eld approach circumvents this diculty by restricting the choice of to the tractable subset mf ( g ) , for which the dual function has an explicit form .
for compactness in notation , we dene a corresponding to the dual function restricted to the set mf ( g ) .
as long as belongs to mf ( g ) , then the lower bound ( 123 ) involves a
and hence can be computed easily .
the next step of the mean eld method is the natural one : nd the best approximation , as measured in terms of the tightness of the lower bound ( 123 ) .
more precisely , the best lower bound from within mf ( g ) is given by
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) a
the corresponding value of is dened to be the mean eld approxi - mation to the true mean parameters .
in section 123 , we illustrate the use of this generic procedure in obtaining lower bounds and approximate mean parameters for various types of graphical models .
123 . 123 mean field and kullbackleibler divergence
an important alternative interpretation of the mean eld optimization problem ( 123 ) is as minimizing the kullbackleibler ( kl ) divergence between the approximating ( tractable ) distribution and the target dis - tribution .
in order to make this connection clear , we rst digress to dis - cuss various forms of the kl divergence for exponential family models .
, as characterized in the - orem 123 , leads to several alternative forms of the kl divergence for exponential family members .
given two distributions with densities q and p with respect to a base measure , the standard denition of the
the conjugate duality between a and a
123 mean field methods
kl divergence ( 123 ) is
d ( q ( cid : 123 ) p ) : =
the key result that underlies alternative representations for exponential families is proposition 123 .
consider two canonical parameter vectors 123 , 123 ; with a slight abuse of notation , we use d ( 123 ( cid : 123 ) 123 ) to refer to the kl divergence between p123 and p123
we use 123 and 123 to denote the respective mean parameters ( i . e . , i = e i ( ( x ) ) for i = 123 , 123 ) .
a rst alternative form of the kl divergence is obtained by substituting the exponential represen - tations of pi into equation ( 123 ) , and then expanding and simplifying
d ( 123 ( cid : 123 ) 123 ) = e123
= a ( 123 ) a ( 123 ) ( cid : 123 ) 123 , 123 123 ( cid : 123 ) .
we refer to this representation as the primal form of the kl divergence .
as illustrated in figure 123 , this form of the kl divergence can be interpreted as the dierence between a ( 123 ) and the hyperplane tangent to a at 123 with normal a ( 123 ) = 123
this interpretation shows that the kl divergence is a particular example of a bregman distance ( 123 , 123 ) .
a second form of the kl divergence can be obtained by using the fact that equality holds in proposition 123 for dually coupled param - eters .
in particular , given a pair ( 123 , 123 ) for which 123 = e123 ( ( x ) ) ,
123 the hyperplane a ( kullbackleibler divergence d (
123 ) + ( cid : 123 ) a (
123 ( cid : 123 ) supports the epigraph of a at
123 ) is equal to the dierence between a (
123 ) and this
123 optimization and lower bounds
we can transform equation ( 123 ) into the following mixed form of the kl divergence :
d ( 123 ( cid : 123 ) 123 ) d ( 123 ( cid : 123 ) 123 ) = a ( 123 ) + a
( 123 ) ( cid : 123 ) 123 , 123 ( cid : 123 ) .
note that this mixed form of the divergence corresponds to the slack in the inequality ( 123 ) .
it also provides an alternative view of the vari - ational representation given in theorem 123 ( b ) .
in particular , equa - tion ( 123 ) can be rewritten as follows :
( ) ( cid : 123 ) , ( cid : 123 ) ( cid : 123 )
a ( ) + a
using equation ( 123 ) , the variational representation in theorem 123 ( b ) is seen to be equivalent to the assertion that minm d ( ( cid : 123 ) ) = 123
finally , by applying equation ( 123 ) as an equality once again , this time for the coupled pair ( 123 , 123 ) , the mixed form ( 123 ) can be trans - formed into a purely dual form of the kl divergence :
d ( 123 ( cid : 123 ) 123 ) d ( 123 ( cid : 123 ) 123 )
( 123 ) ( cid : 123 ) 123 , 123 123 ( cid : 123 ) .
note the symmetry between representations ( 123 ) and ( 123 ) .
this form of the kl divergence has an interpretation analogous to that of figure 123 , but with a replaced by the dual a
with this background on the kullback - leibler divergence , let us now return to the consequences for mean eld methods .
for a given mean parameter mf ( g ) , the dierence between the log partition function a ( ) and the quantity ( cid : 123 ) , ( cid : 123 ) a
f ( ) to be maximized is
d ( ( cid : 123 ) ) = a ( ) + a
f ( ) ( cid : 123 ) , ( cid : 123 ) ,
corresponding to the mixed form of the kl divergence dened in equa - tion ( 123 ) .
on the basis of this relation , it can be seen that solving the variational problem ( 123 ) is equivalent to minimizing the kl divergence d ( ( cid : 123 ) ) , subject to the constraint that mf ( g ) .
consequently , any mean eld method can be understood as obtaining the best approx - imation to p from a family of tractable models , where approximation quality is measured by the kl divergence .
123 mean field methods
123 naive mean field algorithms
in this section , we illustrate various instances of mean eld algorithms for particular graphical models .
our examples in this section focus on the simplest type of approximation , referred to as the naive mean eld approach .
it is based on choosing a product distribution
p ( x123 , x123 , .
, xm ) : =
as the tractable approximation .
the naive mean eld updates are a particular set of recursions for nding a stationary point of the resulting optimization problem .
in the case of the ising model , the naive mean eld updates are a classical set of recursions from statistical physics , typically justied in a heuristic manner in terms of self - consistency .
for the case of a gaussian markov random eld , the naive mean eld updates turn out to be equivalent to the gaussjacobi or gaussseidel algorithm for solving a linear system of equations .
example 123 ( naive mean field for ising model ) .
as an illus - tration , we derive the naive mean eld updates for the ising model , rst introduced in example 123 .
recall that the ising model is character - ized by the sucient statistics ( xs , s v ) and ( xsxt , ( s , t ) e ) .
the associated mean parameters are the singleton and pairwise marginal
s = e ( xs ) = p ( xs = 123 ) , and st = e ( xsxt ) = p ( xs = 123 , xt = 123 ) ,
respectively .
the full vector of mean parameters is an element of letting f123 denote the fully disconnected graph that is , without any edges the tractable set mf123 ( g ) consists of all mean parameter vectors r|v |+|e| that arise from the product distribution ( 123 ) .
explicitly , in this binary case , we have
mf123 ( g ) : = ( r|v |+|e| | 123 s 123 s v ,
st = s t ( s , t ) e . ) ,
where the constraints st = st arise from the product nature of any distribution that is markov with respect to f123
123 naive mean field algorithms
for any mf123 ( g ) , the value of the dual function that is , the negative entropy of a product distribution has an explicit form in terms of ( s , s v ) .
in particular , a straightforward computation shows that this entropy takes the form :
s log s + ( 123 s ) log ( 123 s )
with these two ingredients , we can derive the specic form of the mean eld optimization problem ( 123 ) for the product distribution and the ising model .
given the product structure of the tractable family , any mean parameter mf123 ( g ) satises the equality st = st for all ( s , t ) e .
combining the expression for the entropy in ( 123 ) with the characterization of mf123 ( g ) in ( 123 ) yields the naive mean eld
for any s v , this objective function is strictly concave as a scalar function of s with all other coordinates held xed .
moreover , the max - imum over s with the other variables t , t ( cid : 123 ) = s held xed is attained in the open interval ( 123 , 123 ) .
indeed , by taking the derivative with respect to , setting it to zero and performing some algebra , we obtain the
where ( z ) : = ( 123 + exp ( z ) ) 123 is the logistic function .
thus , we have derived from a variational perspective the naive mean eld updates presented earlier ( 123 ) .
what about convergence properties and the nature of the xed points ? applying equation ( 123 ) iteratively to each node in succession amounts to performing coordinate ascent of the mean eld variational
123 mean field methods
problem ( 123 ) .
since the maximum is uniquely attained for every coordinate update , known results on coordinate ascent methods ( 123 ) imply that any sequence ( 123 , 123 , .
. ) generated by the updates ( 123 ) is guaranteed to converge to a local optimum of the naive mean eld
unfortunately , the mean eld problem is nonconvex in general , so that there may be multiple local optima , and the limit point of the sequence ( 123 , 123 , .
. ) can depend strongly on the initialization 123
we discuss this nonconvexity and its consequences at greater depth in section 123 .
despite these issues , the naive mean eld approximation becomes asymptotically exact for certain types of models as the number of nodes m grows to innity ( 123 , 123 ) .
an example is the ferromagnetic ising model dened on the complete graph km with suitably rescaled parameters st > 123 for all ( s , t ) e; see baxter ( 123 ) for further discussion of such exact cases .
similarly , it is straightforward to apply the naive mean eld approx - imation to other types of graphical models , as we illustrate for a mul -
example 123 ( gaussian mean field ) .
recall the gaussian markov random eld , rst discussed as an exponential family in example 123 .
its mean parameters consist of the vector = e ( x ) rm , and the symmetric123 matrix = e ( xx t ) s m + .
suppose that we again use the completely disconnected graph f123 = ( v , ) as the tractable class .
any gaussian distribution that is markov with respect to f123 must have a diagonal covariance matrix , meaning that the set of tractable mean parameters takes the form :
+ | t = diag ( t ) ( cid : 123 ) 123
( , ) rm s m
123 strictly speaking , only elements ( ) st with ( s , t ) e are included in the mean parameterization , since the associated canonical parameter is zero for any pair ( u , v ) / e .
123 naive mean field algorithms
for any such product distribution , the entropy ( negative dual function ) has the form :
f123 ( , ) = m
log 123e +
log 123e +
combining this form of the dual function with the constraints ( 123 ) yields that for a multivariate gaussian , the value a ( ) of the cumulant function is lower bounded by
s ) + m
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) +
s > 123 , st = st ,
( cid : 123 ) , ( cid : 123 ) + such that ss 123 where ( , ) are the canonical parameters associated with the mul - tivariate gaussian ( see example 123 ) .
the optimization problem ( 123 ) yields the naive mean eld lower bound for the multivariate gaussian .
this optimization problem can be further simplied by substituting the constraints st = st directly into the term ( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) = that appears in the objective function .
doing so and then taking deriva - tives with respect to the remaining optimization variables ( namely , s and ss ) yields the stationary condition
which must be satised for every vertex s v .
one particular set of updates for solving these xed point equations is the iteration
are these updates convergent , and what is the nature of the xed points ? interestingly , the updates ( 123 ) are equivalent , depending on the particular ordering used , to either the gaussjacobi or the gauss seidel methods ( 123 ) for solving the normal equations = 123
123 mean field methods
therefore , if the gaussian mean eld updates ( 123 ) converge , they compute the correct mean vector .
moreover , the convergence behavior of such updates is well understood : for instance , the updates ( 123 ) are guaranteed to converge whenever is strictly diagonally dom - inant; see demmel ( 123 ) for further details on such gaussjacobi and gaussseidel iterations for solving matrix - vector equations .
123 nonconvexity of mean field
an important fact about the mean eld approach is that the variational problem ( 123 ) may be nonconvex , so that there may be local minima , and the mean eld updates can have multiple solutions .
the source of this nonconvexity can be understood in dierent ways , depending on the formulation of the problem .
as an illustration , let us return again to naive mean eld for the ising model .
example 123 .
( nonconvexity for naive mean field ) we now consider an example , drawn from jaakkola ( 123 ) , that illustrates the nonconvexity of naive mean eld for a simple model .
con - sider a pair ( x123 , x123 ) of binary variates , taking values in the space123 ( 123 , +123 ) 123 , thereby dening a 123d exponential family of the form p ( x ) exp ( 123x123 + 123x123 + 123x123x123 ) , with associated mean parameters i = e ( xi ) and 123 = e ( x123x123 ) .
if the constraint 123 = 123 is imposed directly , as in the formulation ( 123 ) , then the naive mean eld objec - tive function for this very special model takes the form :
f ( 123 , 123; ) = 123 + 123 + 123 + h ( 123 ) + h ( 123 ) ,
where h ( i ) = 123 the singleton entropies for the ( 123 , +123 ) - spin representation .
123 ( 123 + i ) 123
123 ( 123 + i ) log 123
123 ( 123 i ) are
now , let us consider a subfamily of such models , given by canonical
parameters of the form :
123 ( 123 i ) log 123
( 123 , 123 , 123 ) =
123 this model , known as a spin representation , is a simple transformation of the ( 123 , 123 ) 123 state space considered earlier .
123 nonconvexity of mean field
123 two dierent perspectives on the nonconvexity of naive mean eld for the ising model .
( a ) illustration of the naive mean eld objective function ( 123 ) for three dierent parameter values : q ( 123 , 123 , 123 ) .
for q = 123 and q = 123 , the global maximum is achieved at ( 123 , 123 ) = ( 123 , 123 ) , whereas for q = 123 , the point ( 123 , 123 ) is no longer a global maximum .
instead the global maximum is achieved at two nonsymmetric points , ( + , ) and ( , + ) .
( b ) nonconvexity can also be seen by examining the shape of the set of fully factorized marginals for a pair of binary variables .
the gray area shows the polytope dened by the inequality ( 123 ) , corresponding to the intersection of m ( g ) with the hyperplane 123 corresponds to the intersection of this 123 = 123
the nonconvex quadratic set 123 = projected polytope with the set mf123 ( g ) of fully factorized marginals .
where q ( 123 , 123 ) is a parameter .
by construction , this model symmetric in x123 and x123 , so that for any value of q ( 123 , 123 ) , we have e ( x123 ) = e ( x123 ) = 123
moreover , some calculation shows that q = p ( x123 = x123 ) .
for q = 123 , the objective function f ( 123 , 123; ( 123 ) ) achieves its global maximum at ( 123 , 123 ) = ( 123 , 123 ) , so that the mean eld approxima - tion is exact .
( this exactness is to be expected since ( 123 ) = ( 123 , 123 , 123 ) , corresponding to a completely decoupled model . ) as q decreases away from 123 , the objective function f starts to change , until for suitably small q , the point ( 123 , 123 ) = ( 123 , 123 ) is no longer the global maximum in fact , it is not even a local maximum .
to illustrate this behavior explicitly , we consider the cross - section of f obtained by setting 123 = and 123 = , and then plot the 123d func - tion f ( , ; ( q ) ) for dierent values of q .
as shown in figure 123 ( a ) , for q = 123 , this 123d objective function has a unique global maximum at = 123
as q decreases away from 123 , the objective function gradually attens out , as shown in the change between q = 123 and q = 123 .
for q suciently close to zero , the point = 123 is no longer a global
123 mean field methods
maximum; instead , as shown in the curve for q = 123 , the global max - imum is achieved at the two points on either side of = 123
thus , for suciently small q , the maximum of the objective function ( 123 )
123 , even though the original model is always sym - occurs at a pair metric .
this phenomenon , known in the physics literature as sponta - neous symmetry - breaking , is a manifestation of nonconvexity , since the optimum of any convex function will always respect symmetries in the underlying problem .
symmetry - breaking is not limited to this toy example , but also occurs with mean eld methods applied to larger and more realistic graphical models , for which there may be a large number of competing modes in the objective function .
alternatively , nonconvexity in naive mean eld can be understood in terms of the shape of the constraint set as an inner approximation to m .
for a pair of binary variates ( x123 , x123 ) ( 123 , 123 ) 123 , the set m is eas - ily characterized : the mean parameters i = e ( xi ) and 123 = e ( x123x123 ) are completely characterized by the four inequalities 123 + ab123 + a123 + b123 123 , where ( a , b ) ( 123 , 123 ) 123
so as to facilitate visualization , con - sider a particular projection of this polytope namely , that corre - sponding to intersection with the hyperplane 123 = 123
in this case , the four inequalities reduce to three simpler ones namely : 123 123 , 123 123 123 , 123 123 123
figure 123 ( b ) shows the resulting 123d polytope , shaded in gray .
now consider the intersection between this projected polytope and the set of factorized marginals mf123 ( g ) .
the factorization condition imposes an additional constraint 123 = 123 123 , yielding a quadratic curve lying within the 123d polytope described by the equations ( 123 ) , as illustrated in figure 123 ( b ) .
since this quadratic set is not convex , this establishes that mf123 ( g ) is not convex either .
indeed , if it were convex , then its intersection with any hyperplane would also be convex .
the geometric perspective on the set m ( g ) and its inner approxi - mation mf ( g ) reveals that more generally , mean eld optimization is always nonconvex for any exponential family in which the state space x m is nite .
indeed , for any such exponential family , the set m ( g ) is
123 nonconvexity of mean field
123 cartoon illustration of the set mf ( g ) of mean parameters that arise from tractable distributions is a nonconvex inner bound on m ( g ) .
illustrated here is the case of discrete random variables where m ( g ) is a polytope .
the circles correspond to mean parameters that arise from delta distributions , and belong to both m ( g ) and mf ( g ) .
a nite convex hull 123
m ( g ) = conv ( ( e ) , e x m )
in d - dimensional space , with extreme points of the form e : = ( e ) for some e x m .
figure 123 provides a highly idealized illustration of this polytope , and its relation to the mean eld inner bound mf ( g ) .
we now claim that mf ( g ) assuming that it is a strict subset of m ( g ) must be a nonconvex set .
to establish this claim , we rst observe that mf ( g ) contains all of the extreme points x = ( x ) of the polytope m ( g ) .
indeed , the extreme point x is realized by the distribution that places all its mass on x , and such a distribution is markov with respect to any graph .
therefore , if mf ( g ) were a con - vex set , then it would have to contain any convex combination of such extreme points .
but from the representation ( 123 ) , taking convex com - binations of all such extreme points generates the full polytope m ( g ) .
therefore , whenever mf ( g ) is a proper subset of m ( g ) , it cannot be a convex set .
consequently , nonconvexity is an intrinsic property of mean eld approximations .
as suggested by example 123 , this nonconvexity
123 for instance , in the discrete case when the sucient statistics are dened by indicator functions in the standard overcomplete basis ( 123 ) , we referred to m ( g ) as a marginal
123 mean field methods
can have signicant operational consequences , including symmetry - breaking , multiple local optima , and sensitivity to initialization .
nonetheless , mean - eld methods have been used successfully in a vari - ety of applications , and the lower bounding property of mean eld methods is attractive , for instance in the context of parameter estima - tion , as we discuss at more length in section 123
123 structured mean field
of course , the essential principles underlying the mean eld approach are not limited to fully factorized distributions .
more generally , we can consider classes of tractable distributions that incorporate additional structure .
this structured mean eld approach was rst proposed by saul and jordan ( 123 ) , and further developed by various researchers ( 123 , 123 , 123 ) .
here , we capture the structured mean eld idea by discussing a gen - eral form of the updates for an approximation based on an arbitrary subgraph f of the original graph g .
we do not claim that these specic updates are the best for practical purposes; the main goal here is the conceptual one of understanding the structure of the solution .
depend - ing on the particular context , various techniques from nonlinear pro - gramming might be suitable for solving the mean eld problem ( 123 ) .
let i ( f ) be the subset of indices corresponding to sucient statis - tics associated with f , and let ( f ) : = ( , i ( f ) ) be the associ - ated subvector of mean parameters .
we use m ( f ) to denote the set of realizable mean parameters dened by the subgraph f and by the sub - set of sucient statistics picked out by i ( f ) ; it is a subset of r|i ( f ) | .
it is important to note that m ( f ) diers from the previously dened ( 123 ) set mf ( g ) , which is based on the entire set of sucient statistics , and so is a subset of r|i| .
we then observe that the mean eld problem ( 123 ) has the following
( a ) the subvector ( f ) can be an arbitrary member of m ( f ) .
f actually depends only on ( f ) , and ( b ) the dual function a not on mean parameters for indices in the complement i c ( f ) : = i ( g ) \i ( f ) .
123 structured mean field
of course , the mean parameters for indices ic ( f ) do play a role in the problem; in particular , they arise within the linear term ( cid : 123 ) , ( cid : 123 ) .
moreover , each mean parameter is constrained in a non - linear way by the choice of ( f ) .
accordingly , for each ic ( f ) , we write = g ( ( f ) ) for some nonlinear function g , of which particular examples are given below .
based on these observations , the optimiza - tion problem ( 123 ) can be rewritten in the form :
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) a
( f ) m ( f )
g ( ( f ) ) a
f ( ( f ) )
on the left - hand side , the optimization takes place over the vec - tor mf ( g ) , which is of the same dimension as r|i| .
the objective function f for the optimization on the right - hand side , in contrast , is a function only of the lower - dimensional vector ( f ) m ( f ) r|i ( f ) | .
to illustrate this transformation , consider the case of naive mean eld for the ising model , where f f123 is the completely disconnected graph .
in this case , each edge ( s , t ) e corresponds to an index in the set i c ( f123 ) ; moreover , for any such edge , we have gst ( ( f123 ) ) = st .
since f123 is the completely disconnected graph , m ( f123 ) is simply the hypercube ( 123 , 123 ) m .
therefore , for this particular example , the right - hand side of equation ( 123 ) is equivalent to equation ( 123 ) .
respect to some with i ( f ) yields :
taking the partial derivatives of the objective function f with
( ( f ) ) a
( ( f ) ) = +
setting these partial derivatives to zero and rearranging yields the xed
f ( ( f ) ) = +
123 mean field methods
to obtain a more intuitive form of this xed point equation , recall from proposition 123 that the gradient a denes the forward mapping , from canonical parameters to mean parameters .
similarly , as we have shown in section 123 , the gradient a denes the backward map - ping from mean parameters to canonical parameters .
let ( f ) denote the canonical parameters that are dually coupled with ( f ) , we can rewrite the xed point update for component as
( f ) +
after any such update , it is then necessary to adjust all mean param - eters ( f ) that depend on ( f ) for instance , via junction tree updates so that global consistency is maintained for the tractable
alternatively , letting af be the conjugate dual of the function a previously dened , we have af ( ( f ) ) = ( f ) .
this fact follows from
f ) ( see appendix b . 123 for back - the legendre duality between ( af , a ground ) , and the usual cumulant properties summarized in proposi - tion 123 .
by exploiting this relation , we obtain an alternative form of the update ( 123 ) , one which involves only the mean parameters ( f ) :
( f ) af
we illustrate the updates ( 123 ) and ( 123 ) with some examples .
example 123 ( re - derivation of naive mf updates ) .
we rst check that equation ( 123 ) reduces to the naive mean eld updates ( 123 ) , when f = f123 is the completely disconnected graph .
for any product distribution on the ising model , the subset of relevant mean parameters is given by ( f123 ) = ( 123 , .
by the properties of the product distribution , we have gst ( ( f123 ) ) = st for all edges ( s , t ) ,
if = s if = t
thus , for a given node index s v , the right - hand side of ( 123 ) is given by the sum
123 structured mean field
f123 ( ( f123 ) = ( cid : 123 )
next , we observe that for a fully disconnected graph , the func -
f123 corresponds to a sum of negative entropies that is ,
sv h ( s ) and so is a separable function .
con - sequently , the cumulant function af123 is also separable , and of the form af123 ( ( f123 ) ) = sv log ( 123 + exp ( s ) ) .
the partial derivatives are
123 + exp ( ) ,
which is the logistic function .
combining the expression ( 123 ) with the logistic function form ( 123 ) of the partial derivative shows that equation ( 123 ) , when specialized to product distributions and the ising model , reduces to the naive mean eld update ( 123 ) .
example 123 ( structured mf for factorial hmms ) .
to provide a more interesting example of the updates ( 123 ) , consider a factorial hidden markov model , as described in ghahramani and jordan ( 123 ) .
figure 123 ( a ) shows the original model , which consists of a set of m markov chains ( m = 123 in this diagram ) , which share a common obser - vation at each time step ( shaded nodes ) .
although the separate chains are independent a priori , the common observation induces an eective coupling among the observations .
( note that the observation nodes are linked by the moralization process that converts the directed graph into an undirected representation . ) thus , an equivalent model is shown in panel ( b ) , where the dotted ellipses represent the induced coupling of each observation .
a natural choice of approximating distribution in this case is based on the subgraph f consisting of the decoupled set of m chains , as illustrated in panel ( c ) .
now consider the nature of the quantities g ( ( f ) ) , which arise in the cost function ( 123 ) .
in this case , any function g will be dened
123 mean field methods
123 structured mean eld approximation for a factorial hmm .
( a ) original model consists of a set of hidden markov models ( dened on chains ) , coupled at each time by a common observation .
( b ) an equivalent model , where the ellipses represent interactions among all nodes at a xed time , induced by the common observation .
( c ) approximating distribution formed by a product of chain - structured models .
here and are the sets of mean parameters associated with the indicated vertex and edge , respectively .
on some subset of m nodes that are coupled at a given time slice ( e . g . , see ellipse in panel ( c ) ) .
note that this subset of nodes is independent with respect to the approximating distribution .
therefore , the function g ( ( f ) ) will decouple into a product of terms of the form fi ( ( i ( f ) ) ) , where each fi is some function of the mean parameters ( i ) ( i ( f ) ) associated with node i = 123 , .
, m in the relevant cluster .
for instance , if the factorial hmm involved binary variables and m = 123 and = ( stu ) , then gstu ( ) = stu .
the decoupled nature of the approximation yields valuable savings on the computational side .
in particular , the junction tree updates nec - essary to maintain consistency of the approximation can be performed by applying the forwardbackward algorithm ( i . e . , the sum - product updates as an exact method ) to each chain separately .
this decoupling also has important consequences for the structure of any mean eld xed point .
in particular , it can be seen that no term g ( ( f ) ) will
123 structured mean field
ever depend on mean parameters associated with edges within any of the chains ( e . g . , in panel ( c ) ) .
otherwise stated , the partial derivative is equal to 123 for all i ( g ) \i ( f ) .
as an immediate consequence
of these derivatives vanishing , the mean eld canonical parameter ( f ) remains equal to for all iterations of the updates ( 123 ) .
any interme - diate junction tree steps to maintain consistency will not aect ( f ) either .
we conclude that it is , in fact , optimal to simply copy the edge potentials from the original distribution onto each of the edges in the structured mean eld approximation .
in this particular form of struc - tured mean eld , only the single node potentials will be altered from their original setting .
this conclusion is sensible , since the structured approximation ( c ) is a factorized approximation on a set of m chains , the internal structure of which is fully preserved in the approximation .
in addition to structured mean eld , there are various other exten - sions to naive mean eld , which we mention only in passing here .
jaakkola and jordan ( 123 ) explored the use of mixture distributions in improving the mean eld approximation .
a large class of tech - niques , including linear response theory and the tap method ( e . g . , 123 , 123 , 123 , 123 , 123 , 123 ) , seek to improve the mean eld approx - imation by introducing higher - order correction terms .
typically , the lower bound on the log partition function is not preserved by these higher - order methods .
leisinck and kappen ( 123 ) proposed a class of higher - order expansions that generate tighter lower bounds .
variational methods in parameter estimation
our focus in the previous sections has been on the problems of com - puting or approximating the cumulant function value a ( ) and the mean parameters = e ( ( x ) ) , assuming that the canonical param - eter specifying the density p was known .
in this section , we turn to the inverse problem of estimating on the basis of observed data .
we consider both the frequentist setting , in which the parameter is assumed to be xed but unknown ( sections 123 and 123 ) , and the bayesian setting , in which the parameter is viewed as a random vari - able ( section 123 ) .
123 estimation in fully observed models
the simplest case of parameter estimation corresponds to the case of 123 : = ( x123 , .
, x n ) of n independent fully observed data : a collection x n and identically distributed ( i . i . d . ) m - vectors , each sampled according to p .
suppose that our goal to estimate the unknown parameter , which we view as a deterministic but nonrandom quantity for the moment .
a classical approach to this estimation problem , dating back to fisher , is via the principle of maximum likelihood : estimate by maximizing
123 estimation in fully observed models
123 ) : = 123
i=123 log p ( x i ) .
the log likelihood of the data , given by ( cid : 123 ) ( ; x n it is convenient and has no eect on the maximization to rescale the log likelihood by 123 / n , as we have done here .
parameters ( cid : 123 ) : = ( cid : 123 ) e ( ( x ) ) = 123
for exponential families , the rescaled log likelihood takes a particu - larly simple and intuitive form .
in particular , we dene empirical mean i=123 ( x i ) associated with the sample 123 .
in terms of these quantities , the log likelihood for an exponential
family can be written as
123 ) = ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) a ( ) .
( cid : 123 ) ( ; x n
this expression highlights the close connection to maximum entropy and conjugate duality , as discussed in section 123 .
the maximum like -
lihood estimate is the vector ( cid : 123 ) rd maximizing this ( random ) objective function ( 123 ) .
as a corollary of theorem 123 , whenever ( cid : 123 ) m , there ditions e ( cid : 123 ) ( ( x ) ) = ( cid : 123 ) .
moreover , by standard results on asymptotics of m - estimators ( 123 , 123 ) , the mle ( cid : 123 ) in regular exponential families is
exists a unique maximum likelihood solution .
indeed , by taking deriva - tives with respect to of the objective function ( 123 ) , the maximum likelihood estimate ( mle ) is specied by the moment matching con -
consistent , in that it converges in probability to as the sample size n tends to innity .
123 . 123 maximum likelihood for triangulated graphs
our discussion up to this point has ignored the computational issue
of solving the moment matching equation so as to obtain the mle ( cid : 123 ) .
a closed - form function of the empirical marginals ( cid : 123 ) , which we describe
as with other statistical computations in exponential families , the dif - cultly of solving this problem turns out to depend strongly on the graph structure .
for triangulated123 graphs , the mle can be written as
to illustrate the basic idea with a minimum of notational overhead , let us consider the simplest instance of a triangulated graph : namely , a tree t = ( v , e ) , with discrete random variables xs taking values in
123 see section 123 . 123 for further discussion of triangulated graphs .
123 variational methods in parameter estimation xs : = ( 123 , 123 , .
, rs 123 ) .
as previously discussed in section 123 , this collec - tion of distributions can be represented as an exponential family ( 123 ) , using indicator functions i s;j ( xs ) for the event ( xs = j ) and i st;jk ( xs , xt ) for the event ( xs = j , xt = k ) .
moreover , the mean parameters in this exponential family are marginal probabilities explicitly ,
s;j = p ( xs = j )
for all s v , j xs , and
st;jk = p ( xs = j , xt = k )
for all ( s , t ) e and ( j , k ) xs xt .
123 : = ( x123 , .
, x n ) , the empirical mean param - given an i . i . d .
sample x n and ( cid : 123 ) st;jk = eters correspond to the singleton and pairwise marginal probabilities
i st;jk ( x i
i s;j ( x i
s , x i
induced by the data .
with this set - up , we can exhibit the closed - form expression for the mle in terms of the empirical marginals .
for this particular expo -
nential family , our assumption that ( cid : 123 ) m means that the empiri - cal marginals are all strictly positive .
consequently , the vector ( cid : 123 ) with elements ( cid : 123 ) s;j : = log ( cid : 123 ) s;j s v , ( cid : 123 ) st;jk : = log we claim that the vector ( cid : 123 ) is the maximum likelihood estimate for moment matching conditions e ( cid : 123 ) ( ( x ) ) = ( cid : 123 ) hold for the distribution p ( cid : 123 ) .
the problem .
in order to establish this claim , it suces to show that the
( s , t ) e , ( j , k ) xs xt
is well dened .
from the denition ( 123 ) , this exponential family member has the form
p ( cid : 123 ) ( x ) = exp
a ( ( cid : 123 ) ) = 123 in this parameterization .
moreover , the distribution p ( cid : 123 ) has
where we have used the fact ( veriable by some calculation ) that
as its marginal distributions the empirical quantities ( cid : 123 ) s and ( cid : 123 ) st
123 estimation in fully observed models
claim follows as a consequence of the junction tree framework ( see proposition 123 ) ; alternatively , they can be proved directly by an inductive leaf - stripping argument , based on a directed version of the factorization ( 123 ) .
consequently , the mle has an explicit closed - form solution in terms of the empirical marginals for a tree .
the same closed - form property holds more generally for any triangulated graph .
iterative methods for computing mles
for nontriangulated graphical models , there is no longer a closed - form solution to the mle problem , and iterative algorithms are needed .
as an illustration , here we describe the iterative proportional tting ( ipf ) algorithm ( 123 , 123 ) , a type of coordinate ascent method with particularly intuitive updates .
to describe it , let us consider a generalization of the sucient statistics ( i s;j ( xs ) ) and ( i st;jk ( xs , xt ) ) used for trees , in which we dene similar indicator functions over cliques of higher order .
more specically , for any clique c of size k in a given graph g , let us dene a set of clique indicator functions
i j ( xc ) =
one for each conguration j = ( j123 , .
, jk ) over xc .
each such su - cient statistic is associated with a canonical parameter c;j .
as before , the associated mean parameters c;j = e ( i c;j ( xc ) ) = p ( xc = j ) are simply marginal probabilities , but now dened over cliques
larly , the data denes a set ( cid : 123 ) c;j = ( cid : 123 ) p ( xc = j ) of empirical marginal
since the mle problem is dierentiable and jointly concave in the vector , coordinate ascent algorithms are guaranteed to converge to the global optimum .
using the cumulant generating properties of a ( see proposition 123 ) , we can take derivatives with respect to some coordinate c;j , thereby obtaining
= ( cid : 123 ) c;j a
( ) = ( cid : 123 ) c;j c;j ,
123 variational methods in parameter estimation
iterative proportional fitting : at iterations t = 123 , 123 , 123 ,
( i ) choose a clique c = c ( t ) and compute the local marg - ( t ) ( xc = j ) j x |c| .
c;j : = p
( ii ) update the canonical parameter vector
c;j + log
if = ( c; j ) for some j x |c| .
123 steps in the iterative proportional tting ( ipf ) procedure .
where c;j = e ( i c;j ( xc ) ) are the mean parameters dened by the current model .
as a consequence , zero gradient points are specied by matching the model marginals p ( xc = j ) to the empirical marginals estimates ( ( t ) ) via the recursion in figure 123 .
( cid : 123 ) p ( xc = j ) .
with this set - up , we may dene a sequence of parameter
these updates have two key properties : rst , the log partition func -
tion is never changed by the updates .
indeed , we have
a ( ( t+123 ) ) = log
note that this invariance in a arises due to the overcompleteness of this i c;j ( xc ) = 123 particular exponential family in particular , since for all cliques c .
second , the update in step ( ii ) enforces exactly the zero - gradient condition ( 123 ) .
indeed , after the parameter update ( t ) ( t+123 ) , we have
( t ) ( i c;j ( xc ) ) = ( cid : 123 ) c;j .
( t+123 ) ( i c;j ( xc ) ) =
) i c;j ( xc ) ! ( cid : 123 ) c;j + a ( ( t ) )
= log e
123 partially observed models and expectationmaximization
consequently , this ipf algorithm corresponds to as a blockwise coor - dinate ascent method for maximizing the objective function ( 123 ) .
at each iteration , the algorithm picks a particular block of coordinates that is , c;j , for all congurations j x |c| dened on the block and maximizes the objective function over this subset of coordinates .
consequently , standard theory on blockwise coordinate ascent meth - ods ( 123 ) can be used to establish convergence of the ipf procedure .
more generally , this iterative proportional tting procedure is a spe - cial case of a broader class of iterative scaling , or successive projection algorithms; see papers ( 123 , 123 , 123 ) or the book ( 123 ) for further details on such algorithms and their properties .
from a computational perspective , however , there are still some open issues associated with the ipf algorithm in application to graph - ical models .
note that step ( i ) in the updates assume a black box routine that , given the current canonical parameter vector ( t ) , returns the associated vector of mean parameters ( t ) .
for graphs of low treewidth , this computation can be carried out with the junction tree algorithm .
for more general graphs not amenable to the junction tree method , one could imagine using approximate sampling meth - ods or variational methods .
the use of such approximate methods and their impact on parameter estimation is still an active area of research ( 123 , 123 , 123 , 123 , 123 ) .
123 partially observed models and
a more challenging version of parameter estimation arises in the par - tially observed setting , in which the random vector x p is not observed directly , but indirectly via a noisy version y of x .
this formulation includes the special case in which some subset where the remaining variates are unobserved that is , latent or hidden .
the expectationmaximization ( em ) algorithm of dempster et al .
( 123 ) provides a general approach to computing mles in this partially observed setting .
although the em algorithm is often presented as an alternation between an expectation step ( e step ) and a maximization step ( m step ) , it is also possible to take a variational perspective on em ,
123 variational methods in parameter estimation
and view both steps as maximization steps ( 123 , 123 ) .
such a perspective illustrates how variational inference algorithms can be used in place of exact inference algorithms in the e step within the em framework , and in particular , how the mean eld approach is especially appropriate for
a brief outline of our presentation in this section is as follows .
we begin by deriving the em algorithm in the exponential family setting , showing how the e step reduces to the computation of expected suf - cient statistics i . e . , mean parameters .
as we have seen , the vari - ational framework provides a general class of methods for computing approximations of mean parameters .
this observation suggests a gen - eral class of variational em algorithms , in which the approximation provided by a variational inference algorithm is substituted for the mean parameters in the e step .
in general , as a consequence of mak - ing such a substitution , one loses the guarantees of exactness that are associated with the em algorithm .
in the specic case of mean eld algorithms , however , one is still guaranteed that the method performs coordinate ascent on a lower bound of the likelihood function .
123 . 123 exact em algorithm in exponential families
we begin by deriving the exact em algorithm for exponential families .
suppose that the set of random variables is partitioned into a vector y of observed variables , and a vector x of unobserved variables , and the probability model is a joint exponential family distribution for ( x , y ) : given an observation y = y , we can also form the conditional
( cid : 123 ) ( cid : 123 ) , ( x , y ) ( cid : 123 ) a ( )
p ( x , y ) = exp
p ( x | y ) =
exp ( ( cid : 123 ) , ( x , y ) ( cid : 123 ) )
x m exp ( ( cid : 123 ) , ( x , y ) ( cid : 123 ) ) ( dx ) : = exp ( ( cid : 123 ) , ( x , y ) ( cid : 123 ) ay ( )
exp ( ( cid : 123 ) , ( x , y ) ( cid : 123 ) ) ( dx ) .
where for each xed y , the log partition function ay associated with this conditional distribution is given by the integral
ay ( ) : = log
123 partially observed models and expectationmaximization
thus , we see that the conditional distribution p ( x | y ) belongs to an d - dimensional exponential family , with vector ( , y ) of sucient statis -
the maximum likelihood estimate ( cid : 123 ) is obtained by maximizing log
probability of the observed data y , which is referred to as the incomplete log likelihood in the setting of em .
this incomplete log likelihood is given by the integral
( cid : 123 ) ( cid : 123 ) , ( x , y ) ( cid : 123 ) a ( )
( dx ) = ay ( ) a ( ) , ( 123 )
( cid : 123 ) ( ; y ) : = log
where the second equality follows by denition ( 123 ) of ay .
we now describe how to obtain a variational lower bound on the incomplete log likelihood .
for each xed y , the set my of valid mean parameters in this exponential family takes the form
rd | = ep ( ( x , y ) )
for some p
where p is any density function over x , taken with respect to underlying base measure .
consequently , applying theorem 123 to this exponen - tial family provides the variational representation
( cid : 123 ) ( cid : 123 ) , y ( cid : 123 ) a
ay ( ) = sup
where the conjugate dual is also dened variationally as ( ( cid : 123 ) y , ( cid : 123 ) ay ( ) ) .
from the variational representation ( 123 ) , we obtain the lower bound ay ( ) ( cid : 123 ) y , ( cid : 123 ) a y ( y ) , valid for any y my .
using the representa -
tion ( 123 ) , we obtain the lower bound for the incomplete log likelihood :
( cid : 123 ) ( ; y ) ( cid : 123 ) y , ( cid : 123 ) a
y ( y ) a ( ) = : l ( y , ) .
with this set - up , the em algorithm is coordinate ascent on this func - tion l dening the lower bound ( 123 ) :
( t+123 ) = arg max
( t+123 ) = arg max
123 variational methods in parameter estimation
to see the correspondence with the traditional presentation of the em algorithm , note rst that the maximization underlying the e step
( ( cid : 123 ) y , ( t ) ( cid : 123 ) a
which by the variational representation ( 123 ) is equal to ay ( ( t ) ) , with the maximizing argument equal to the mean parameter that is dually coupled with ( t ) .
otherwise stated , the vector computed by maximization in the rst argument of l ( y , ) is exactly ( t ) ( ( x , y ) ) , a computation that is tradi - the expectation y tionally referred to as the e step , for obvious reasons .
moreover , the maximization underlying the m step reduces to ( t+123 ) , ( cid : 123 ) a ( ) ) ,
( t+123 ) = e
which is simply a maximum likelihood problem based on the expected sucient statistics y
( t+123 ) traditionally referred to as the m step .
moreover , given that the value achieved by the e step on the right - hand side of ( 123 ) is equal to ay ( ( t ) ) , the inequality in ( 123 ) becomes an equality by ( 123 ) .
thus , after the e step , the lower bound l ( y , ( t ) ) is actually equal to the incomplete log likelihood ( cid : 123 ) ( ( t ) ; y ) , and the sub - sequent maximization of l with respect to in the m step is guaranteed to increase the log likelihood as well .
example 123 ( em for gaussian mixtures ) .
we illustrate the em algorithm with the classical example of the gaussian mixture model , discussed earlier in example 123 .
let us focus on a scalar mix - ture model for simplicity , for which the observed vector y is gaus - sian mixture vector with r components , where the unobserved vector x ( 123 , 123 , .
, r 123 ) indexes the components .
the complete likelihood of this mixture model can be written as
j + jy + ( cid : 123 ) jy123 aj ( j , ( cid : 123 ) j )
p ( x , y ) = exp
123 partially observed models and expectationmaximization
where = ( , , ( cid : 123 ) ) , with the vector rr parameterizing the ( j , ( cid : 123 ) j ) r123 parameterizing the gaussian distribution of the jth mix - ture component .
the log normalization function aj ( j , ( cid : 123 ) j ) is for the
multinomial distribution over the hidden vector x , and the pair
( conditionally ) gaussian distribution of y given x = j , whereas the j=123 exp ( j ) ) normalizes the multinomial distri - function a ( ) = log ( bution .
when the complete likelihood is viewed as an exponential family , the sucient statistics are the collection of triplets
j ( x , y ) : = ( i j ( x ) , i j ( x ) y , i j ( x ) y123 )
for j = 123 , .
, r 123
consider a collection of i . i . d .
observations y123 , .
to each obser -
vation yi , we associate a triplet ( i , i , ( cid : 123 ) i ) rr rr rr , corre - j + jyi + ( cid : 123 ) j ( yi ) 123 aj ( j , ( cid : 123 ) j )
sponding to expectations of the triplet of sucient statistics j ( x , yi ) , j = 123 , .
, r 123
since the conditional distribution has the form
p ( x | yi , ) exp
some straightforward computation yields that the probability of the jth mixture component or equivalently , the mean parameter j = e ( i j ( x ) ) is given by
j = e ( i j ( x ) )
j + jyi + ( cid : 123 ) j ( yi ) 123 aj ( j , ( cid : 123 ) j ) k + kyi + ( cid : 123 ) k ( yi ) 123 ak ( k , ( cid : 123 ) k )
similarly , the remaining mean parameters are
j = e ( i j ( x ) yi ) = i j = e ( i j ( x ) yi ) = i
the computation of these expectations ( 123 ) and ( 123 ) correspond to the e step for this particular model .
over the triplet = ( , , ( cid : 123 ) ) , using the expected sucient statistics com -
the m step requires solving the mle optimization problem ( 123 )
puted in the e step .
some computation shows that this maximization
123 variational methods in parameter estimation
takes the form :
consequently , the optimization decouples into separate maximization problems : one for the vector parameterizing the mixture component
distribution , and one for each of the pairs ( j , ( cid : 123 ) j ) specifying the gaus -
sian mixture components .
by the moment - matching property of maxi - mum likelihood estimates , the optimum solution is to update such
for each j = 123 , .
, r 123 ,
e ( i j ( x ) ) =
and to update the canonical parameters ( j , ( cid : 123 ) j ) specifying the jth
gaussian mixture component ( y | x = j ) such that the associated mean parameters corresponding to the gaussian mean and second moment are matched to the data as follows :
ej , ( cid : 123 ) j ( y 123 | x = j ) = family distribution specied by ( j , ( cid : 123 ) j ) .
in these equations , the expectations are taken under the exponential
ej , ( cid : 123 ) j ( y | x = j ) =
123 . 123 variational em
what if it is infeasible to compute the expected sucient statistics ? one possible response to this problem is to make use of a mean eld approx - imation for the e step .
in particular , given some class of tractable subgraphs f , recall the set mf ( g ) , as dened in equation ( 123 ) , corresponding to those mean parameters that can be obtained by
distributions that factor according to f .
by using mf ( g ) as an inner approximation to the set my , we can compute an approximate version of the e step ( 123 ) , of the form
123 variational bayes
( cid : 123 ) ( cid : 123 ) , ( t ) ( cid : 123 ) a
( mean eld e step )
this variational e step thus involves replacing the exact mean parame - ( t ) ( ( x , y ) ) , under the current model ( t ) , with the approximate set of mean parameters computed by a mean eld algorithm .
despite this ( possibly crude ) approximation , the resulting variational em algo - rithm is a still coordinate ascent algorithm for l .
however , given that the e step no longer closes the gap between the auxiliary function l and the incomplete log likelihood , it is no longer the case that the algo - rithm necessarily goes uphill in the latter quantity .
nonetheless , the variational mean eld em algorithm still has the attractive interpreta - tion of maximizing a lower bound on the incomplete log likelihood .
in section 123 , we described a broad class of variational methods , including belief propagation and expectation - propagation , that also generate approximations to mean parameters .
it is tempting , and common in practice , to substitute the approximate mean parameters obtained from these relaxations in the place of the expected sucient statistics in dening a variational em algorithm .
such a substitu - tion is particularly tempting given that these methods can yield better approximations to mean parameters than the mean eld approach .
care must be taken in working with these algorithms , however , because the underlying relaxations do not ( in general ) guarantee lower bounds on the log partition function . 123 in the absence of guaranteed lower bounds , the connection to em is thus less clear than in the mean eld case; in particular , the algorithm is not guaranteed to maximize a lower bound on the incomplete log likelihood .
123 variational bayes
all of the variational approximations that we have discussed thus far in the survey , as well as those to be discussed in later sections ,
123 however , see sudderth et al .
( 123 ) for results on certain classes of attractive graphical models for which the bethe approximation does provide such a lower bound .
123 variational methods in parameter estimation
are applicable in principle to bayesian inference problems , where the parameters are endowed with probability distributions and viewed as random variables .
in the literature on the topic , however , the term variational bayes has been reserved thus far for the application of the mean - eld variational method to bayesian inference ( 123 , 123 ) .
in this section , we review this application and use the terminology of varia - tional bayes to refer to the application , but it should be borne in mind that this is but one potential application of variational methodology to
we consider a general set - up akin to that used in the preceding development of the em algorithm .
in particular , let the data be parti - tioned into an observed component y and an unobserved component x , and assume that the complete data likelihood lies in some exponential
p ( x , y | ) = exp ( ( cid : 123 ) ( ) , ( x , y ) ( cid : 123 ) a ( ( ) ) ) .
the function : rd rd provides some additional exibility in the parameterization of the exponential family; this is convenient in the bayesian setting . 123 we assume moreover that the prior distribution over also lies in some exponential family , of the conjugate prior form :
p , ( ) = exp ( ( cid : 123 ) , ( ) ( cid : 123 ) a ( ( ) ) b ( , ) ) .
note that this exponential family is specied by the sucient statis - tics ( ( ) , a ( ( ) ) ) rd r , with associated canonical parameters ( , ) rd r .
its cumulant function b is dened in the usual way
b ( , ) : = log
exp ( ( cid : 123 ) , ( ) ( cid : 123 ) a ( ( ) ) ) d ,
and expectations of the sucient statistics ( ) and a ( ( ) ) can be obtained in the usual way by taking derivatives of b ( , ) with respect to and .
123 up to this point , we have considered only exponential families in canonical form , for which ( ( ) ) i = i for all i = 123 , .
this is without loss of generality , because any regular , minimal exponential family can be reparameterized in such a canonical form ( 123 ) .
however , it can be convenient to express exponential families with nonidentity .
123 variational bayes
the class of models specied by the pair of equations ( 123 ) and ( 123 ) is broad .
it includes as special cases gaussian mixture mod - els with dirichlet priors , and the latent dirichlet allocation model from
we now consider a central problem in bayesian inference , that of computing the marginal likelihood p , ( y ) , where y is an observed ) are xed values of the hyperparameters .
data point and where ( this computation entails averaging over both the unobserved variates x and the random parameters .
working in the log domain , we have .
log p , ( y ) = log
p ( x , y | ) dx
p , ( ) p ( y | ) d .
we now multiply and divide by p , ( ) and use jensens inequality ( see proposition 123 ) , to obtain the following lower bound , valid for any choice of ( , ) in the domain of b :
log p , ( y ) e , ( log p ( y | ) ) + e ,
with equality for ( , ) = ( with respect to the distribution p , ( ) .
here e , denotes averaging over from equation ( 123 ) , we have log p ( y | ) = ay ( ( ) ) a ( ( ) ) ,
which when substituted into equation ( 123 ) yields log p , ( y ) e , ( ay ( ( ) ) a ( ( ) ) ) + e ,
where ay is the cumulant function of the conditional density p ( x | for each xed y , recall the set my of mean parameters of the form = e ( ( x , y ) ) .
for any realization of , we could in principle apply the mean eld lower bound ( 123 ) on the log likelihood ay ( ) , using a value ( ) my that varies with .
we would thereby obtain that the marginal log likelihood log p , ( y ) is lower bounded by
( cid : 123 ) ( cid : 123 ) ( ) , ( ) ( cid : 123 ) a
123 variational methods in parameter estimation
at this stage , even after this sequence of lower bounds , if we were to optimize over the set of joint distributions on ( x , ) or equivalently , over the choice ( ) and the hyperparameters ( , ) specifying the distribution of the optimized lower bound ( 123 ) would be equal to the original marginal log likelihood log p , ( y ) .
the variational bayes algorithm is based on optimizing this lower bound using only product distributions over the pair ( x , ) .
such opti - mization is often described as free - form , in that beyond the assump - tion of a product distribution , the factors composing this product dis - tribution are allowed to be arbitrary .
but as we have seen in section 123 , the maximum entropy principle underlying the variational framework yields solutions that necessarily have exponential form .
therefore , we can exploit conjugate duality to obtain compact forms for these solu - tions , working with dually coupled sets of parameters for the distribu - tions over both x and .
we now derive the variational bayes algorithm as a coordinate - ascent method for solving the mean eld variational problem over prod - uct distributions .
we begin by reformulating the objective function into a form that allows us to exploit conjugate duality .
since is indepen - dent of , the optimization problem ( 123 ) can be simplied to
where : = e , ( ( ) ) and a : = e , ( a ( ) ) .
using the exponential form ( 123 ) of p , , we have
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) a
( cid : 123 ) + ( cid : 123 ) a ,
) + b ( , ) .
) , and since ( , a ) are dually
by the conjugate duality between ( b , b coupled with ( , ) by construction , we have
( , a ) = ( cid : 123 ) , ( cid : 123 ) + ( cid : 123 ) a , ( cid : 123 ) b ( , ) .
by combining equations ( 123 ) through ( 123 ) and performing some algebra , we nd that the decoupled optimization problem is equivalent
123 variational bayes
to maximizing the function
+ 123 , a ( cid : 123 ) b
, ( cid : 123 ) a
over my and ( , a ) dom ( b ) .
performing coordinate ascent on this function amounts to rst maximizing rst over , and then maxi - mizing over the mean parameters123 ( , a ) .
doing so generates a sequence
, which are updated as follows :
y ( ) + ( cid : 123 )
( t ) , ( t ) , a ( t+123 ) = arg max
( cid : 123 ) ( cid : 123 ) , ( t ) ( cid : 123 ) a
, ( cid : 123 ) ( 123 +
) a b
= arg max
in fact , both of these coordinate - wise optimizations have explicit solu - tions .
the explicit form of equation ( 123 ) is given by
( t+123 ) = e
as can be veried by taking derivatives in equation ( 123 ) , and using
y ( see proposition 123 ) .
by analogy to the the cumulant properties of a em algorithm , this step is referred to as the vb - e step .
similarly , in the vb - m step we rst update the hyperparameters ( , ) as
( ( t+123 ) , ( t+123 ) ) = (
and then use these updated hyperparameters to compute the new mean
( t+123 ) = e ( ( t+123 ) , ( t+123 ) ) ( ( ) ) .
in summary , the variational bayes algorithm is an iterative algo - rithm that alternates between two steps .
the vb - e step ( 123 ) entails computing the conditional expectation of the sucient statistics given
( x , y ) p ( x | y , ( t ) ) dx ,
123 equivalently , by the one - to - one correspondence between exponential and mean parame - ters ( theorem 123 ) , this step corresponds to maximization over the associated canonical parameters ( , ) .
123 variational methods in parameter estimation
with the conditional distribution specied by the current parameter estimate ( t ) .
the vb - m step ( 123 ) entails updating the hyperpa - rameters to incorporate the data , and then recomputing the averaged
( ) p ( t+123 ) , ( t+123 ) ( ) d .
the ordinary variational em algorithm can be viewed as a degenerate form of these updates , in which the prior distribution over is always a delta function at a single point estimate of .
convex relaxations and upper bounds
up to this point , we have considered two broad classes of approxi - mate variational methods : bethe approximations and their extensions ( section 123 ) , and mean eld methods ( section 123 ) .
mean eld methods provide not only approximate mean parameters but also lower bounds on the log partition function .
in contrast , the bethe method and its extensions lead to neither upper or lower bounds on the log parti - tion function .
we have seen that both classes of methods are , at least in general , based on nonconvex variational problems .
for mean eld methods , the nonconvexity stems from the nature of the inner approx - imation to the set of mean parameters , as illustrated in figure 123 .
for the bethe - type approaches , the lack of convexity in the objective function in particular , the entropy approximation is the source .
regardless of the underlying cause , such nonconvexity can have unde - sirable consequences , including multiple optima and sensitivity to the problem parameters ( for the variational problem itself ) , and conver - gence issues and dependence on initialization ( for iterative algorithms used to solve the variational problem ) .
given that the underlying exact variational principle ( 123 ) is cer - tainly convex , it is natural to consider variational approximations that
123 convex relaxations and upper bounds
retain this convexity .
in this section , we describe a broad class of such convex variational approximations , based on approximating the set m with a convex with a convex set , and replacing the dual function a function .
these two steps lead to a new variational principle , which rep - resents a computationally tractable alternative to the original problem .
since this approximate variational principle is based on maximizing a concave and continuous function over a convex and compact set , the global optimum is unique , and is achieved .
if in addition , the negative dual function a , corresponding to the entropy function , is replaced by an upper bound , and the set m is approximated by a convex outer bound , then the global optimum of the approximate variational princi - ple also provides an upper bound on the log partition function .
these upper bounds are complementary to the lower bounds provided by mean eld procedures ( see section 123 ) .
we begin by describing a general class of methods based on approx - imating the entropy by a convex combination of easily computable entropies , thereby obtaining a tractable relaxation that is guaranteed to be convex .
as our rst example illustrates , following this proce - dure using tree - structured distributions as the tractable class leads to the family of convexied bethe variational problems , and asso - ciated reweighted sum - product algorithms ( 123 , 123 ) .
however , this basic procedure for convexication is quite broadly applicable; as we describe , it yields convex analogs of other known variational methods , including kikuchi and region - graph methods ( 123 , 123 , 123 ) , as well as expectation - propagation approximation .
it has also suggested novel variational methods , also based on the notion of convex combinations , including those based on planar graph decomposition ( 123 ) .
finally , there are other convex variational relaxations that are not directly based on convex combinations of tractable distributions , including the method of conditional entropy decompositions ( 123 ) , and methods based on semidenite constraints and log - determinant programming ( 123 ) .
apart from these known algorithms , there are a large number of novel methods based on convex variational relaxations that await
we conclude the section by discussing some benets of convexity in variational methods .
in addition to the obvious one of unique optima ,
123 generic convex combinations and convex surrogates
possible benets include guarantees of algorithmic stability and useful - ness as surrogate likelihoods in parameter estimation .
123 generic convex combinations and convex surrogates
we begin with a generic description of approximations based on convex combinations of tractable distributions .
consider a d - dimensional exponential family , dened by a vector of sucient statis - tics = ( , i ) , and an associated vector of canonical parameters = ( , i ) .
although computing mean parameters for an arbitrary might be intractable , it is frequently the case that such compu - tations are tractable for certain special choices of the canonical param - eters .
if the exponential family is viewed as a markov random eld dened by some underlying graph g , many such special choices can be indexed by particular subgraphs f , say belonging to some class d .
examples that we consider below include d corresponding to the set of all spanning trees of g , or planar subgraphs of g .
in terms of the expo - nential family , we can think of the subgraph f as extracting a subset of indices i ( f ) from the full index set i of potential functions , thereby dening a lower - dimensional exponential family based on d ( f ) < d suf - cient statistics .
we let m ( f ) denote the lower - dimensional set of mean parameters associated with this exponential family; concretely , this set has the form : m ( f ) : = for any mean parameter m , let ( cid : 123 ) ( f ) represent the coor - dinate projection mapping from the full space i to the subset i ( f ) of indices associated with f .
by denition of the sets m rd and m ( f ) rd ( f ) , we are guaranteed that the projected vector ( f ) is an element of m ( f ) .
let a ( ( f ) ) be the negative dual function value ( or entropy ) associated with the projected mean parameter ( f ) .
a simple consequence of theorem 123 is that this entropy is always an ( ) dened in the origi - upper bound on the negative dual function a nal d - dimensional exponential family .
with a slight abuse of notation , we use h ( ) to denote the negative dual function or entropy a and similarly h ( ( f ) ) to denote the entropy a
r|i ( f ) | | p s . t .
= ep ( ( x ) ) i ( f )
123 convex relaxations and upper bounds
proposition 123 ( maximum entropy bounds ) .
given any mean parameter m and its projection ( f ) onto any subgraph f , we have the bound
( ( f ) ) a
or alternatively stated , h ( ( f ) ) h ( ) .
from theorem 123 , the negative dual value a
to the entropy of the exponential family member with mean param - ( ( f ) ) .
alternatively eters , with a similar interpretation for a phrased , proposition 123 states the entropy of the exponential fam - ily member p ( f ) is always larger than the entropy of the distribu - tion p .
intuitively , the distribution p is obtained from a maximum entropy problem with more constraints corresponding to the indices i\i ( f ) and so has lower entropy .
our proof makes this intuition
from theorem 123 , the dual function is realized as the solution of the optimization problem
( ( cid : 123 ) , ( cid : 123 ) a ( ) ) .
( ) = sup
since the exponential family dened by f involves only a subset i ( f ) ( ( f ) ) is of the potential functions , the associated dual function a realized by the lower - dimensional optimization problem
( ( f ) ) =
( f ) rd ( f )
( ( cid : 123 ) ( f ) , ( f ) ( cid : 123 ) a ( ( f ) ) ) .
but this optimization problem can be recast as a restricted version of the original problem ( 123 ) :
( ( f ) ) =
=123 / i ( f )
( ( cid : 123 ) , ( cid : 123 ) a ( ) ) ,
from which the claim follows .
123 generic convex combinations and convex surrogates
given the bound ( 123 ) for each subgraph f , any convex combination of subgraph - structured entropies is also an upper bound on the original entropy .
in particular , if we consider a probability distribution over the set of tractable graphs , meaning a probability vector r|d| such that
( f ) 123 for all tractable f d , and
any such distribution generates the upper bound
( f ) = 123
h ( ) e ( h ( ( f ) ) ) : =
( f ) h ( ( f ) ) .
l ( g; d ) : =
having derived an upper bound on the entropy ,
it remains to obtain a convex outer bound on the set of mean parameters m .
an important restriction is that each of the subgraph - structured entropies h ( ( f ) ) = a ( ( f ) ) must be well - dened on this outer bounding set .
the simplest outer approximation with this property is dened by requiring that each projected mean parameter ( f ) belong to the projected set m ( f ) .
in particular , let us dene the constraint set
rd | ( f ) m ( f ) f d
our shift in notation , from to , is deliberate , since l ( g; d ) is an outer bound on m ( g ) .
( indeed , by denition of the sets m ( f ) , any mean parameter m ( g ) , when projected down to ( f ) , is also glob - ally realizable , so that l ( g; d ) . ) moreover , l ( g; d ) is a convex set , since each of the subsets m ( f ) in its denition are convex .
overall , these two ingredients the convex upper bound ( 123 ) and the convex outer bound ( 123 ) yield the following approximate varia -
( cid : 123 ) , ( cid : 123 ) +
bd ( ; ) : = sup
( f ) h ( ( f ) )
note that the objective function dening bd is concave; moreover , the constraint set fmf is convex , since it is the intersection of the sets mf , each of which are individually convex sets .
overall , then , the function bd is a new convex function that approximates the original cumulant function a .
we refer to such a function as a convex surrogate
123 convex relaxations and upper bounds
of course , in order for variational principles of the form ( 123 ) to be useful , it must be possible to evaluate and optimize the objective func - tion .
accordingly , in the following sections , we provide some specic examples of the tractable class d that lead to interesting and practical
123 variational methods from convex relaxations
in this section , we describe a number of versions of the convex surro - gate ( 123 ) , including convex versions of the bethe / kikuchi problems , as well as convex versions of expectation - propagation and other moment -
123 . 123 tree - reweighted sum - product and bethe
we begin by deriving the tree - reweighted sum - product algorithm and the tree - reweighted bethe variational principle ( 123 , 123 ) , correspond - ing to a convexied analog of the ordinary bethe variational principle described in section 123
given an undirected graph g = ( v , e ) , consider a pairwise markov random eld
where we are using the standard overcomplete representation based on indicator functions at single nodes and edges ( see equation ( 123 ) ) .
let the tractable class d be the set t of all spanning trees t = ( v , e ( t ) ) of the graph g .
( a spanning tree of a graph is a tree - structured sub - graph whose vertex set covers the original graph . ) letting denote a probability distribution over the set of spanning trees , equation ( 123 ) t ( t ) h ( ( t ) ) , based on a convex
yields the upper bound h ( ) ( cid : 123 )
combination of tree - structured entropies .
an attractive property of tree - structured entropies is that they decompose additively in terms of entropies associated with the vertices and edges of the tree .
more specically , these entropies are dened by the mean parameters , which ( in the canonical overcomplete repre - sentation ( 123 ) ) correspond to singleton marginal distributions s ( )
123 variational methods from convex relaxations
dened at each vertex s v , and a joint pairwise marginal distribu - tion st ( , ) dened for each edge ( s , t ) e ( t ) .
as discussed earlier in section 123 , the factorization ( 123 ) of any tree - structured probability distribution yields the entropy decomposition
h ( ( t ) ) =
now consider the averaged form of the bound ( 123 ) .
since the trees are all spanning , the entropy term hs for node s v receives a weight of one in this average .
on the other hand , the mutual information term ist for edge ( s , t ) receives the weight st = e i ( ( s , t ) e ( t ) ) is an indicator function for the event that edge ( s , t ) is included in the edge set e ( t ) of a given tree t .
overall , we obtain the following upper bound on the exact entropy :
i ( ( s , t ) e ( t ) )
we refer to the edge weight st as the edge appearance probability , since it reects the probability mass associated with edge ( s , t ) .
the vector = ( st , ( s , t ) e ) of edge appearance probabilities belong to a set called the spanning tree polytope , as discussed at more length in theorem 123 to follow .
let us now consider the form of the outer bound l ( g; t ) on the set m .
for the pairwise mrf with the overcomplete parameterization under consideration , the set m is simply the marginal polytope m ( g ) .
on the other hand , the set m ( t ) is simply the marginal polytope for the tree t , which from our earlier development ( see proposition 123 ) is equivalent to l ( t ) .
consequently , the constraint ( t ) m ( t ) is equiv - alent to enforcing nonnegativity constraints , normalization ( at each vertex ) and marginalization ( across each edge ) of the tree .
enforc - ing the inclusion ( t ) m ( t ) for all trees t t is equivalent to enforcing the marginalization on every edge of the full graph g .
we conclude that in this particular case , the set l ( g; t ) is equiva - lent to the set l ( g ) of locally consistent pseudomarginals , as dened
123 convex relaxations and upper bounds
overall , then , we obtain a variational problem that can be viewed as a convexied form of the bethe variational problem .
we summarize our ndings in the following result ( 123 , 123 ) :
theorem 123 ( tree - reweighted bethe and sum - product ) .
( a ) for any choice of edge appearance vector ( st , ( s , t ) e ) in the spanning tree polytope , the cumulant function a ( ) evaluated at is upper bounded by the solution of the tree - reweighted bethe variational problem ( bvp ) :
bt ( ; e ) : = max
( cid : 123 ) , ( cid : 123 ) +
for any edge appearance vector such that st > 123 for all edges ( s , t ) , this problem is strictly convex with a unique
( b ) the tree - reweighted bvp can be solved using the tree -
reweighted sum - product updates
t ) : = exp
updates ( 123 ) have a unique xed point under the assumptions of part ( a ) .
t ) + t ( x
we make a few comments on theorem 123 , before providing the proof .
valid edge weights : observe that the tree - reweighted bethe variational problem ( 123 ) is closely related to the ordinary bethe problem ( 123 ) .
in particular , if we set st = 123 for all edges ( s , t ) e , then the two for - mulations are equivalent .
however , the condition st = 123 implies that every edge appears in every spanning tree of the graph with proba - bility one , which can happen if and only if the graph is actually tree - structured .
more generally , the set of valid edge appearance vectors e
123 variational methods from convex relaxations
123 illustration of valid edge appearance probabilities .
original graph is shown in panel ( a ) .
probability 123 / 123 is assigned to each of the three spanning trees ( ti | i = 123 , 123 , 123 ) shown in panels ( b ) ( d ) .
edge b appears in all three trees so that b = 123
edges e and f appear in two and one of the spanning trees , respectively , which gives rise to edge appearance probabilities e = 123 / 123 and f = 123 / 123
must belong to the so - called spanning tree polytope ( 123 , 123 ) associated with g .
note that these edge appearance probabilities must satisfy various constraints , depending on the structure of the graph .
a simple example should help to provide intuition .
example 123 ( edge appearance probabilities ) .
figure 123 ( a ) shows a graph , and panels ( b ) through ( d ) show three of its spanning trees ( t 123 , t 123 , t 123 ) .
suppose that we form a uniform distribution over these trees by assigning probability ( t i ) = 123 / 123 to each t i , i = 123 , 123 , 123
consider the edge with label f; notice that it appears in t 123 , but in neither of t 123 and t 123
therefore , under the uniform distribution , the associated edge appearance probability is f = 123 / 123
since edge e appears in two of the three spanning trees , similar reasoning establishes that e = 123 / 123
finally , observe that edge b appears in any spanning tree ( i . e . , it is a bridge ) , so that it must have edge appearance probability b = 123
in their work on fractional belief propagation , wiegerinck and hes - kes ( 123 ) examined the class of reweighted bethe problems of the form ( 123 ) , but without the requirement that the weights st belong to the spanning tree polytope .
although loosening this requirement does yield a richer family of variational problems , in general one loses
123 convex relaxations and upper bounds
the guarantee of convexity , and ( hence ) that of a unique global opti - mum .
on the other hand , weiss et al .
( 123 ) have pointed out that other choices of weights st , not necessarily in the spanning tree polytope , can also lead to convex variational problems .
in general , convexity and the upper bounding property are not equivalent .
for instance , for any single cycle graph , setting st = 123 for all edges ( i . e . , the ordinary bvp choice ) yields a convex variational problem ( 123 ) , but the value of the bethe variational problem does not upper bound the cumulant function value .
various other researchers ( 123 , 123 , 123 , 123 ) also discuss the choice of edge / clique weights in bethe / kikuchi approximations , and its conse - quences for convexity .
properties of tree - reweighted sum - product : in analogy to the ordinary bethe problem and sum - product algorithm , the xed point of tree - reweighted sum - product ( trw ) message - passing ( 123 ) species the optimal solution of the variational problem ( 123 ) as follows :
s ( xs ) = exp
st ( xs , xt ) = st ( xs , xt )
st ( xs , xt ) + s ( xs ) + t ( xt ) ) .
in contrast where st ( xs , xt ) : = exp ( 123 to the ordinary sum - product algorithm , the xed point ( and associ -
st ) ) is unique for any valid vector of edge appear - ated optimum ( ances .
roosta et al .
( 123 ) provide sucient conditions for convergence , based on contraction arguments such as those used for ordinary sum - product ( 123 , 123 , 123 , 123 ) .
in practical terms , the updates ( 123 ) appear to always converge if damped forms of the updates are used ( i . e . , setting log m new = ( 123 ) log m old + log m , where m old is the previous vector of messages , and ( 123 , 123 ) is the damping parameter ) .
as an alternative , globerson and jaakkola ( 123 ) proposed a related message - passing algorithm based on oriented trees that is guaran - teed to converge , but appears to do so more slowly than damped trw - message passing .
another possibility would be to adapt other double - loop algorithms ( 123 , 123 , 123 , 123 ) , originally developed for
123 variational methods from convex relaxations
the ordinary bethe / kikuchi problems , to solve these convex minimiza - tion problems; see hazan and shashua ( 123 ) for some recent work along
optimal choice of edge appearance probabilities : note that equa - tion ( 123 ) actually describes a family of variational problems , one for each choice of probability distribution over the set of spanning trees t .
a natural question thus arises : what is the optimal choice of this probability distribution , or equivalently the choice of the edge appear - ance vector ( st , ( s , t ) e ) ? one notion of optimality is to choose the edge appearance probabilities that leads to the tightest upper bound on the cumulant function ( i . e . , for which the gap bt ( ; ) a ( ) is small - est ) .
in principle , it appears as if the computation of this optimum might be dicult , since it seems to entail searching over all probabil - ity distributions over the set t of all spanning trees of the graph .
the number |t| of spanning trees can be computed via the matrix - tree the - orem ( 123 ) , and for suciently dense graphs , it grows rapidly enough to preclude direct optimization over t .
however , wainwright et al .
( 123 ) show that it is possible to directly optimize the vector ( st , ( s , t ) e ) of edge appearance probabilities , subject to the constraint of mem - bership in the spanning tree polytope .
although this polytope has a prohibitively large number of inequalities , it is possible to maximize a linear function over it equivalently , to solve a maximum weight span - ning tree problem by a greedy algorithm ( 123 ) .
using this fact , it is possible to develop a version of the conditional gradient algorithm ( 123 ) for eciently computing the optimal choice of edge appearance proba - bilities ( see the paper ( 123 ) for details ) .
proof of theorem 123 :
we begin with the proof of part ( a ) .
our discussion preceding the statement of the result establishes that bt is an upper bound on a .
it remains to establish the strict convexity , and resulting uniqueness of the global optimum .
note that the cost function dening the func - tion bt consists of a linear term ( cid : 123 ) , ( cid : 123 ) and a convex combination ( ( t ) ) ) of tree entropies , and hence is concave .
moreover , the constraint set l ( g ) is a polytope , and hence convex .
therefore , the variational problem ( 123 ) is always convex .
we now establish unique -
123 convex relaxations and upper bounds
ness of the optimum when e > 123
to simplify details of the proof , we assume without loss of generality that we are working in a minimal representation .
( if the variational problem is formulated in an over - complete representation , it can be reduced to an equivalent minimal formulation . ) to establish uniqueness , it suces to establish that the ( ( t ) ) ) is strictly convex when e > 123
this function is a ( ( t ) ) , each of which is convex combination of functions of the form a strictly convex in the ( nonzero ) components of ( t ) , but independent of the other components in the full vector .
for any vector rd , dene t ( ) = if i ( t ) , and t ( ) = 123 otherwise .
we then
( ( t ) ) ( cid : 123 ) = ( cid : 123 ) t ( ) , 123a
( ( t ) ) t ( ) ( cid : 123 ) 123 ,
with strict inequality unless t ( ) = 123
now the condition e > 123 for all e e ensures that ( cid : 123 ) = 123 implies that t ( ) must be dierent from zero for at least one tree t
( cid : 123 ) .
therefore , for any ( cid : 123 ) = 123 , we have
( ( t ) ) ) ( cid : 123 ) ( cid : 123 ) t
( ) ( cid : 123 ) > 123 ,
which establishes the assertion of strict convexity .
we now turn to proof of part ( b ) .
derivation of the trw - s updates ( 123 ) is entirely analogous to the proof of theorem 123 : setting up the lagrangian , taking derivatives , and performing some algebra yields the result ( see the paper ( 123 ) for full details ) .
the uniqueness follows from the strict convexity established in part ( a ) .
123 . 123 reweighted kikuchi approximations
a natural extension of convex combinations of trees , entirely analogous to the transition from the bethe to kikuchi variational problems , is to take convex combinations of hypertrees .
this extension was sketched out in wainwright et al .
( 123 ) , and has been further studied by other researchers ( 123 , 123 ) .
here we describe the basic idea with one illustra - tive example .
for a given treewidth t , consider the set of all hypertrees of width t ( t ) of width less than or equal to t .
of course , the under - lying assumption is that t is suciently small that performing exact computations on hypertrees of this width is feasible .
applying propo - sition 123 to a convex combination based on a probability distribution
123 variational methods from convex relaxations
= ( ( t ) , t t ( t ) ) over the set of all hypertrees of width at most t , we obtain the following upper bound on the entropy :
h ( ) e ( h ( ( t ) ) ) =
( t ) h ( ( t ) ) .
for a xed distribution , our strategy is to optimize the right - hand side of this upper bound over all pseudomarginals that are consistent on each hypertree .
the resulting constraint set is precisely the poly - tope lt ( g ) dened in equation ( 123 ) .
overall , the hypertree analog of theorem 123 asserts that the log partition function a is upper bounded by the hypertree - based convex surrogate :
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) + e ( h ( ( t ) ) )
a ( ) bt ( t ) ( ; ) : = max
( again , we switch from to to reect the fact that elements of the constraint set l ( g ) need not be globally realizable . ) note that the cost function in this optimization problem is concave for all choices of distributions over the hypertrees .
the variational problem ( 123 ) is the hypertree analog of equation ( 123 ) ; indeed , it reduces to the latter equation in the special case t = 123
example 123 ( convex combinations of hypertrees ) .
let us derive an explicit form of equation ( 123 ) for a particular hypergraph and choice of hypertrees .
the original graph is the 123 123 grid , as illus - trated in the earlier figure d . 123 ( a ) .
based on this grid , we construct the augmented hypergraph shown in figure 123 ( a ) , which has the hyper - edge set e given by it is straightforward to verify that it satises the single counting crite - rion ( see appendix d for background ) .
now consider a convex combination of
four hypertrees , each obtained by removing one of the 123 - hyperedges from the edge set .
for instance , shown in figure 123 ( b ) is one particular acyclic substructure t 123 with hyperedge set e ( t 123 )
123 convex relaxations and upper bounds
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
123 hyperforests embedded within augmented hypergraphs .
( a ) an augmented hyper - graph for the 123 123 grid with maximal hyperedges of size four that satises the single counting criterion .
( b ) one hyperforest of width three embedded within ( a ) .
( c ) a second hyperforest of width three .
obtained by removing ( 123 ) from the full hyperedge set e .
to be precise , the structure t 123 so dened is a spanning hyperforest , since it consists of two connected components ( namely , the isolated hyper - edge ( 123 ) along with the larger hypertree ) .
this choice , as opposed to a spanning hypertree , turns out to simplify the development to follow .
figure 123 ( c ) shows the analogous spanning hyperforest t 123 obtained by removing hyperedge ( 123 ) ; the nal two hyperforests t 123 and t 123 are dened analogously .
to specify the associated hypertree factorization , we rst compute the form of h for the maximal hyperedges ( i . e . , of size four ) .
for instance , looking at the hyperedge h = ( 123 ) , we see that hyperedges ( 123 ) , ( 123 ) , ( 123 ) , and ( 123 ) are contained within it .
thus , using the denition in equation ( 123 ) , we write ( suppressing the functional
123 variational methods from convex relaxations
dependence on x ) :
123 123 123
= 123 123
having calculated all the functions h , we can combine them , using the hypertree factorization ( 123 ) , in order to obtain the following fac - torization for a distribution on t 123 :
p ( t 123 ) ( x ) =
here each term within square brackets corresponds to h for some hyperedge h e ( t 123 ) ; for instance , the rst three terms correspond to the three maximal 123 - hyperedges in t 123
although this factorization could be simplied , leaving it in its current form makes the connec - tion to kikuchi approximations more explicit; in particular , the factor - ization ( 123 ) leads immediately to a decomposition of the entropy .
in an analogous manner , it is straightforward to derive factoriza - tions and entropy decompositions for the remaining three hyperforests ( t i , i = 123 , 123 , 123 ) .
now let e123 = ( ( 123 ) , ( 123 ) , ( 123 ) , ( 123 ) ) denote the set of all 123 - hyperedges .
we then form the convex combination of the four ( nega - tive ) entropies with uniform weight 123 / 123 on each t i; this weighted sum
( ( t i ) ) takes the form :
h ( xh ) log h ( xh ) +
the weight 123 / 123 arises because each of the maximal hyperedges h e123 appears in three of the four hypertrees .
all of the ( nonmaximal ) hyper - edge terms receive a weight of one , because they appear in all four hypertrees .
overall , then , these weights represent hyperedge appear - ance probabilities for this particular example , in analogy to ordinary edge appearance probabilities in the tree case .
we now simplify the
123 convex relaxations and upper bounds
so yields that the sum ( cid : 123 ) 123
weighted combination of entropies :
expression in equation ( 123 ) by expanding and collecting terms; doing ( ( t i ) ) is equal to the following
( h123 + h123 + h123 + h123 ) 123
( h123 + h123 + h123 + h123 ) .
( h123 + h123 + h123 + h123 )
if , on the other hand , starting from equation ( 123 ) again , suppose that we included each maximal hyperedge with a weight of 123 , instead of 123 / 123
then , after some simplication , we would nd that ( the nega - tive of ) equation ( 123 ) is equal to the following combination of local ( h123 + h123 + h123 + h123 ) ( h123 + h123 + h123 + h123 ) + h123 ,
which is equivalent to the kikuchi approximation derived in exam - ple 123 .
however , the choice of all ones for the hyperedge appearance probabilities is invalid that is , it could never arise from taking a convex combination of hypertree entropies .
more generally , any entropy approximation formed by taking such convex combinations of hypertree entropies will necessarily be convex .
in contrast , with the exception of certain special cases ( 123 , 123 , 123 , 123 ) , kikuchi and other hypergraph - based entropy approximations are typically not convex .
in analogy to the tree - reweighted sum - product algorithm , it is possible to develop hypertree - reweighted forms of gen - eralized sum - product updates ( 123 , 123 ) .
with a suitable choice of con - vex combination , the underlying variational problem will be strictly convex , and so the associated hypertree - reweighted sum - product algo - rithms will have a unique xed point .
an important dierence from the case of ordinary trees , however , is that optimization of the hyper - edge weights e cannot be performed exactly using a greedy algorithm .
indeed , the hypertree analog of the maximum weight spanning tree problem is known to be np - hard ( 123 ) .
123 variational methods from convex relaxations
123 . 123 convex forms of expectation - propagation
as discussed in section 123 , the family of expectation - propagation algorithms ( 123 , 123 ) , as well as related moment - matching algo - rithms ( 123 , 123 , 123 ) , can be understood in terms of term - by - term entropy approximations .
from this perspective , it is clear how to derive convex analogs of the variational problem ( 123 ) that underlies
in particular , recall from equation ( 123 ) the form of the term - by -
term entropy approximation
hep ( , ( cid : 123 ) ) : = h ( ) +
h ( , ( cid : 123 ) ( cid : 123 ) ) h ( )
where h ( ) represents the entropy of the base distribution , and h ( , ( cid : 123 ) ( cid : 123 ) )
represents the entropy of the base plus ( cid : 123 ) th term .
implicitly , this entropy approximation is weighting each term i = 123 , .
, di with weight one .
suppose instead that we consider a family of nonnegative weights ( ( 123 ) , .
, ( di ) ) over the dierent terms , and use the reweighted term - by - term entropy approximation
hep ( , ( cid : 123 ) ; ) : = h ( ) +
h ( , ( cid : 123 ) ( cid : 123 ) ) h ( )
for suitable choice of weights for instance , if we enforce the con - ( cid : 123 ) =123 ( ( cid : 123 ) ) = 123 this reweighted entropy approximation is concave .
other choices of nonnegative weights could also produce con - cave entropy approximations .
the ep outer bound l ( ; ) on the set m ( ) , as dened in equation ( 123 ) , is a convex set by denition .
con - sequently , if we combine a convex entropy approximation ( 123 ) with this outer bound , we obtain convexied forms of the ep variational problem .
following the line of development in section 123 , it is also possible to derive lagrangian algorithms for solving these optimization problems .
due to underlying structure , the updates can again be cast in terms of moment - matching steps .
in contrast to standard forms of ep , a benet of these algorithms would be the existence of unique xed points , assured by the convexity of the underlying variational princi - ple .
with dierent motivations , not directly related to convexity and
123 convex relaxations and upper bounds
uniqueness , minka ( 123 ) has discussed reweighted forms of ep , referred to as power ep .
seeger et al .
( 123 ) reported that reweighted forms of ep appear to have better empirical convergence properties than stan - dard ep .
it would be interesting to study connections between the convexity of the ep entropy approximation , and the convergence and stability properties of the ep updates .
123 . 123 other entropy approximations
the form p ( x ) exp ( ( cid : 123 )
trees ( and the associated hierarchy of hypertrees ) represent the best known classes of tractable graphical models .
however , exact compu - tation is also tractable for certain other classes of graphical models .
unlike hypertrees , certain models may be intractable in general , but tractable for specic settings of the parameters .
for instance , consider the subclass of planar graphs ( which can be drawn in the 123d plane with - out any crossing of the edges , for instance 123d grid models ) .
although exact computation for an arbitrary markov random eld on a planar graph is known to be intractable , if one considers only binary ran - dom variables without any observation potentials ( i . e . , ising models of ( s , t ) e stxsxt ) ) , then it is possible to compute the cumulant function a ( ) exactly , using clever combinatorial reduc - tions due independently to fisher ( 123 ) and kastelyn ( 123 ) .
globerson and jaakkola ( 123 ) exploit these facts in order to derive a variational relaxation based on convex combination of planar subgraphs , as well as iterative algorithms for computing the optimal bound .
they provide experimental results for various types of graphs ( both grids and fully connected graphs ) , with results superior to the trw relaxation ( 123 ) for most coupling strengths , albeit with higher computational cost .
as they point out , it should also be possible to combine hypertrees with planar graphs to form composite approximations .
123 other convex variational methods
the previous section focused exclusively on variational methods based on the idea of convex combinations , as stated in equation ( 123 ) .
how - ever , not all convex variational methods need arise in this way; more generally , a convex variational method requires only ( a ) some type of
123 other convex variational methods
( negative entropy ) , and convex approximation to the dual function a ( b ) convex outer bound on the set m .
this basic two - step procedure can be used to derive many convex relaxations of the marginalization
123 . 123 semidenite constraints and log - determinant bound
we illustrate this two - step procedure by describing a log - determinant relaxation for discrete markov random elds ( 123 ) .
this method diers qualitatively from the previously described bethe / kikuchi methods , in that it uses a semidenite outer bound on the marginal polytope m ( g ) .
although the basic ideas are more generally applicable , let us focus for concreteness on the case of a binary random vector x ( 123 , 123 ) m , with a distribution in the ising form :
p ( x ) = exp
without loss of generality , 123 we assume that the underlying graph is the complete graph km , so that the marginal polytope of interest is m ( km ) .
with this set - up , the mean parameterization consists of the vector m ( km ) rm+ ( m 123 ) , with elements s = e ( xs ) for each node , and st = e ( xsxt ) for each edge .
first , we describe how to approximate the negative dual function , or entropy function .
an important characterization of the mul - tivariate gaussian is as the maximum entropy distribution subject to
covariance constraints ( 123 ) .
in particular , the dierential entropy h ( ( cid : 123 ) x ) of any continuous random vector ( cid : 123 ) x is upper bounded by the entropy where cov ( ( cid : 123 ) x ) is the covariance matrix of ( cid : 123 ) x .
the upper bound ( 123 )
of a gaussian with matched covariance .
in analytical terms , we have
log detcov ( ( cid : 123 ) x ) + m
is not directly applicable to a random vector taking values in a dis - crete space ( since dierential entropy in this case diverges to minus
123 a problem dened on an arbitrary g = ( v , e ) can be embedded into the complete graph by setting st = 123 for all ( s , t ) / e .
123 convex relaxations and upper bounds
innity ) .
therefore , in order to exploit this bound for the discrete random vector x ( 123 , 123 ) m of interest , it is necessary to construct a suitably matched continuous version of x .
one method to do so is by the addition of an independent random vector u , such that the atoms in the distribution of x are smoothed out .
in particular , let us
dene the continuous random vector ( cid : 123 ) x : = x + u , where u is inde - the discrete entropy of x with the dierential entropy of ( cid : 123 ) x that is , h ( ( cid : 123 ) x ) = h ( x ) ; see wainwright and jordan ( 123 ) for the proof .
since x yields cov ( ( cid : 123 ) x ) = cov ( x ) + 123
and u are continuous by construction , a straightforward computation 123 im , so equation ( 123 ) yields the upper
pendent of x , with independent components distributed uniformly as us u ( 123 123 , 123 123 ) .
a key property of this construction is that it matches
second , we need to provide an outer bound on the marginal polytope for which the entropy approximation above is well dened .
the simplest such outer bound is obtained by observing that the covariance matrix cov ( x ) must always be positive semidenite .
( indeed , for any vector a rm , we have ( cid : 123 ) a , cov ( x ) a ( cid : 123 ) = var ( ( cid : 123 ) a , x ( cid : 123 ) ) 123 ) note that elements of the matrix cov ( x ) can be expressed in terms of the mean parameters s and st .
in particular , consider the ( m + 123 ) ( m + 123 ) matrix
n123 ( ) = e
123 ( m123 )
by the schur complement formula ( 123 ) , the constraint n123 ( ) ( cid : 123 ) 123 is equivalent to the constraint cov ( x ) ( cid : 123 ) 123
consequently , the set
s123 ( km ) : = ( rm+ ( m
123 ) | n123 ( ) ( cid : 123 ) 123 )
123 other convex variational methods
is an outer bound on the marginal polytope m ( km ) .
it is not dicult to see that the set s123 ( km ) is closed , convex , and also bounded , since it is contained within the hypercube ( 123 , 123 ) m+ ( m 123 ) .
again , our switch in notation from to is deliberate , since vectors belonging to the semidenite constraint set s123 ( km ) need not be globally consistent ( see section 123 for more details ) .
we now have the two necessary ingredients to dene a convex varia - tional principle based on a log - determinant relaxation ( 123 ) .
in addition to the constraint s123 ( km ) , one might imagine enforcing other con - straints ( e . g . , the linear constraints dening the set l ( g ) ) .
accordingly , let b ( km ) denote any convex and compact outer bound on m ( km ) that is contained within s123 ( km ) .
with this notation , the cumulant func - tion a ( ) is upper bounded by the log - determinant convex surrogate bld ( ) , dened by the variational problem
( cid : 123 ) , ( cid : 123 ) +
where blkdiag ( 123 , im ) is an ( m + 123 ) ( m + 123 ) block - diagonal matrix .
by enforcing the inclusion b ( km ) s123 ( km ) we guarantee that the matrix n123 ( ) , and hence the matrix sum
will always be positive semidenite .
importantly , the optimization problem ( 123 ) is a standard determinant maximization problem , for which ecient interior point methods have been developed ( e . g . , 123 ) .
wainwright and jordan ( 123 ) derive an ecient algorithm for solving a slightly weakened form of the log - determinant problem ( 123 ) , and provide empirical results for various types of graphs .
banerjee et al .
( 123 ) exploit the log - determinant relaxation ( 123 ) for a sparse model selec - tion procedure , and develop coordinate - wise algorithms for solving a broader class of convex programs .
123 . 123 other entropy approximations and polytope bounds
there are many other approaches , using dierent approximations to the entropy and outer bounds on the marginal polytope , that lead
123 convex relaxations and upper bounds
to interesting convex variational principles .
for instance , globerson and jaakkola ( 123 ) exploit the notion of conditional entropy decompo - sition in order to construct whole families of upper bounds on the entropy , including as a special case the hypertree - based entropy bounds that underlie the reweighted bethe / kikuchi approaches .
sontag and jaakkola ( 123 ) examine the eect of tightened outer bounds on the marginal polytope , including the so - called cycle inequalities ( 123 , 123 ) .
among other issues , they examine the diering eects of changing the entropy approximation versus rening the outer bound on the marginal polytope .
these questions remain an active and ongoing area
123 algorithmic stability
as discussed earlier , an obvious benet of approximate marginaliza - tion methods based on convex variational principles is the resulting uniqueness of the minima , and hence lack of dependence on algorithmic details , or initial conditions .
in this section , we turn to a less obvious but equally important benet of convexity , of particular relevance in a statistical setting .
more specically , in typical applications of graphi - cal models , the model parameters or graph topologies themselves are uncertain .
( for instance , they may have been estimated from noisy or incomplete data . ) in such a setting , then , it is natural to ask whether the output of a given variational method remains stable under small perturbations to the model parameters .
not all variational algorithms are stable in this sense; for instance , both mean eld methods and the ordinary sum - product algorithm can be highly unstable , with small perturbations in the model parameters leading to very large changes in the xed point ( s ) and outputs of the algorithm .
in this section , we show that any variational method based on a suitable convex optimization problem in contrast to mean eld and ordinary sum - product has an associated guarantee of lipschitz sta - bility .
in particular , let ( ) denote the output when some variational method is applied to the model p .
given two norms ( cid : 123 ) ( cid : 123 ) a and ( cid : 123 ) ( cid : 123 ) b , we say that the method is globally lipschitz stable if there exists some
123 algorithmic stability
nite constant l such that
( cid : 123 ) is small , then the dierence ( cid : 123 ) ( ) (
( cid : 123 ) .
this denition is one way of formalizing the notion of for any , algorithmic stability : namely , that if the dierence ( cid : 123 ) models indexed by and between the algorithms outputs is correspondingly small .
as an illus - tration , figure 123 compares the behavior of the ordinary sum - product algorithm known to be instable in certain regimes to the tree - reweighted sum - product algorithm .
note how the sum - product algo - rithm degrades rapidly beyond a certain critical coupling strength , whereas the performance of the tree - reweighted algorithm varies smoothly as a function of the coupling strength .
let us now try to gain some theoretical insight into which variational methods satisfy this lipschitz property .
consider a generic variational
grid with attractive coupling
123 contrast between the instability of ordinary sum - product and stability of tree - reweighted sum - product ( 123 ) .
plots show the error between the true marginals and correct marginals versus the coupling strength in a binary pairwise markov random eld .
note that the ordinary sum - product algorithm is very accurate up to a critical coupling ( 123 ) , after which it degrades rapidly .
on the other hand , the performance of trw message - passing varies smoothly as a function of the coupling strength .
the plot shows two versions of the trw sum - product algorithm , either based on uniform edge weights st = 123 / 123 , or edge weights optimized to minimize the upper bound .
123 convex relaxations and upper bounds
method of the form :
( ( cid : 123 ) , ( cid : 123 ) b
b ( ) = sup
where l is a convex outer bound on the set m , and b .
let us consider the convex approximation to the dual function a properties of the surrogate function b that we have dened .
as mentioned previously , it is always a convex function of .
in terms of properties beyond convexity , they are determined by the nature of is strictly convex; then the function b the optimum ( 123 ) is uniquely attained , so that danskins theorem ( 123 ) ensures that b is dierentiable .
for instance , suppose that b
in terms of lipschitz stability , it turns out that a somewhat stronger notion of convexity is required; in particular , a dierentiable function f : rd r is strongly convex with parameter c if for all y , z rd ,
f ( z ) f ( y ) + ( cid : 123 ) f ( y ) , z y ( cid : 123 ) + c
note that any dierentiable convex function satises this inequal - ity ( 123 ) with c = 123; the essence of strong convexity is that the same ( cid : 123 ) z y ( cid : 123 ) 123
( in fact , an equivalent inequality is satised with slack c characterization of strong convexity is to require that the function ( cid : 123 ) z ( cid : 123 ) 123 be convex ( 123 ) . ) in the setting of the convex surro - gate ( 123 ) , suppose that the function b is strictly convex , and the ( neg - is strongly convex , say with parameter ative ) entropy approximation b 123 / l > 123
under this assumption , it can be shown ( 123 ) that the output ( ) = b ( ) of the variational method is lipschitz stable , in the sense of denition ( 123 ) , with parameter l and norms ( cid : 123 ) ( cid : 123 ) a = ( cid : 123 ) ( cid : 123 ) b = ( cid : 123 ) ( cid : 123 ) 123
thus , when using variational methods of the form ( 123 ) , the issue of algorithmic stability reduces to strong convexity of the neg - .
what existing variational methods ative entropy approximation b are based on strongly convex entropy approximations ? for instance , the tree - reweighted bethe entropy ( 123 ) is strongly convex ( 123 ) for any pairwise markov random eld , so that as a corollary , the tree - reweighted sum - product algorithm is guaranteed to be globally lipschitz stable .
this lipschitz stability provides a theoretical expla - nation of the behavior illustrated in figure 123 .
similarly , empirical
123 convex surrogates in parameter estimation
results due to wiegerinck ( 123 ) show that suitably reweighted forms of generalized belief propagation ( gbp ) also behave in a stable manner ( unlike standard gbp ) , and this stability can be conrmed theoreti - cally via strong convexity .
it can also be shown that the log - determinant relaxation ( 123 ) is lipschitz stable .
123 convex surrogates in parameter estimation
until now , we have discussed convex variational methods in the context of computing approximations to the cumulant function value a ( ) , as well as approximate mean parameters .
here , we discuss an alterna - tive use of convex surrogates of the form ( 123 ) , namely for approx - imate parameter estimation .
recall from section 123 our discussion of maximum likelihood ( ml ) estimation in exponential families : it entails maximizing the rescaled log likelihood , given by
123 ) = ( cid : 123 ) ( ) = ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) a ( ) ,
( cid : 123 ) ( ; x n
where ( cid : 123 ) : = 123 the derivative of the log likelihood takes the form ( cid : 123 ) ( ) = ( cid : 123 ) ( ) ,
i=123 ( x i ) is the empirical expectation of the su - 123 = ( x123 , .
, x n ) .
recall also that
cient statistics given the data x n
where ( ) = e ( ( x ) ) are the mean parameters under the current model parameter .
as discussed in section 123 , this fact motivates a natural approach to approximate parameter estimation , in which the true model parameters ( ) are approximated by the output ( ) of some variational method .
for instance , this approach has frequently been suggested , using sum - product to compute the approximate mean parameters ( ) .
such heuristics may yield good results in some set - tings; however , if the underlying variational method is nonconvex , such a heuristic cannot be interpreted as minimizing an approximation to the likelihood .
indeed , given multiple xed points , its behavior can be unstable and erratic; the papers ( 123 , 123 ) provide some cautionary instances of poor behavior with ordinary sum - product .
123 . 123 surrogate likelihoods
on the other hand , the perspective of convex surrogates to a , of the form ( 123 ) , suggests a related but more principled method for
123 convex relaxations and upper bounds
approximate ml estimation , based on maximizing a concave approx - imation to the likelihood .
let us assume that the outer bound l is is strictly convex , compact and the negative entropy approximation b so that the maximum ( 123 ) is uniquely attained , say at some ( ) l .
recall that danskins theorem ( 123 ) shows that the convex surrogate b is dierentiable , with b ( ) = ( ) .
( note the parallel with the gradi - ent a , and its link to the mean parameterization , as summarized in proposition 123 ) .
given a convex surrogate b , let us dene the associ - ated surrogate likelihood as
123 ) = ( cid : 123 ) b ( ) : = ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) b ( ) .
( cid : 123 ) b ( ; x n
assuming that b is strictly convex , then this surrogate likelihood is strictly concave , and so ( at least for compact parameter spaces ) , we have the well - dened surrogate ml estimate
( cid : 123 ) b : = arg max
( cid : 123 ) b ( ; x n
note that when b upper bounds the true cumulant function ( as in the convex variational methods described in section 123 ) , then the surro -
gate likelihood ( cid : 123 ) b lower bounds the true likelihood , and ( cid : 123 ) b is obtained
from maximizing this lower bound .
123 . 123 optimizing surrogate likelihoods
in general , it is relatively straightforward to compute the surrogate
estimate ( cid : 123 ) = ( cid : 123 ) b .
indeed , from our discussion above , the derivative of the surrogate likelihood is ( cid : 123 ) b ( ) = ( cid : 123 ) ( ) , where ( ) = b ( ) are used to compute ( cid : 123 ) b .
the approximate mean parameters computed by the variational method associated with b .
since the optimization problem is concave , a stan - dard gradient ascent algorithm ( among other possibilities ) could be
interestingly , in certain cases , the unregularized surrogate mle can be specied in closed form , without the need for any optimization .
as an example , consider the reweighted bethe surrogate ( 123 ) , and the associated surrogate likelihood .
the surrogate mle is dened by the
xed point relation ( cid : 123 ) b ( ( cid : 123 ) ) = 123 , which has a very intuitive interpre - tation .
namely , the model parameters ( cid : 123 ) are chosen such that , when
123 convex surrogates in parameter estimation
the trw sum - product updates ( 123 ) are applied to the model p ( cid : 123 ) , the
resulting pseudo mean parameters ( ( cid : 123 ) ) computed by the algorithm are equal to the empirical mean parameters ( cid : 123 ) .
assuming that the empiri - cal mean vector has strictly positive elements , the unique vector ( cid : 123 ) with
this property has the closed - form expression :
( cid : 123 ) s;j = log ( cid : 123 ) s;j , ( cid : 123 ) st;jk = st log
s v , j xs , ( s , t ) e , ( j , k ) xs xt , here ( cid : 123 ) s;j = ( cid : 123 ) e ( i j ( xs ) ) is the empirical probability of the event ( xs = j ) , and ( cid : 123 ) st;jk is the empirical probability of the event ( xs = j , xt = k ) .
by construction , the vector ( cid : 123 ) dened by equation ( 123 ) has the property that ( cid : 123 ) are the pseudomarginals associated with mode p ( cid : 123 ) .
the proof of
this fact relies on the tree - based reparameterization interpretation of the ordinary sum - product algorithm , as well as its reweighted exten - sions ( 123 , 123 ) ; see section 123 . 123 for details on reparameterization .
similarly , closed - form solutions can be obtained for reweighted forms of kikuchi and region graph approximations ( section 123 . 123 ) , and con - vexied forms of expectation - propagation and other moment - matching algorithms ( section 123 . 123 ) .
such a closed - form solution eliminates the need for any iterative algorithms in the complete data setting ( as described here ) , and can substantially reduce computational overhead if a variational method is used within an inner loop in the incomplete
123 . 123 penalized surrogate likelihoods
ter to maximize a penalized likelihood function ( cid : 123 ) ( cid : 123 ) ( ; ) : = ( cid : 123 ) ( ) r ( ) ,
rather than maximizing the log likelihood ( 123 ) itself , it is often bet -
where > 123 is a regularization constant and r is a penalty func - tion , assumed to be convex but not necessarily dierentiable ( e . g . , ( cid : 123 ) 123 penalization ) .
such procedures can be motivated either from a frequen - tist perspective ( and justied in terms of improved loss and risk ) , or from a bayesian perspective , where the quantity r ( ) might arise from a prior over the parameter space .
if maximization of the likeli - hood is intractable then regularized likelihood is still intractable , so let
123 convex relaxations and upper bounds
ized surrogate likelihood ( rsl ) ( cid : 123 ) ( cid : 123 ) b ( ; ) : = ( cid : 123 ) b ( ) r ( ) .
again , given
us consider approximate procedures based on maximizing a regular -
that ( cid : 123 ) b is convex and dierentiable , this optimization problem could be solved by a variety of standard methods .
here , we describe an alternative formulation of optimizing the penalized surrogate likelihood , which exploits the variational manner in which the surrogate b was dened ( 123 ) .
if we reformulate the problem as one of minimizing the negative rsl , substituting the de - nition ( 123 ) yields the equivalent saddlepoint problem :
( ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) + b ( ) + r ( ) ) ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) + sup ( ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) b
( ) ) + r ( )
( ( cid : 123 ) , ( cid : 123 ) b
( ) + r ( ) ) .
note that this saddlepoint problem involves convex minimization and concave maximization; therefore , under standard regularity assump - tions ( e . g . , either compactness of the constraint sets , or coercivity of the function; see section vii of hiriart - urruty and lemarechal ( 123 ) ) , we can exchange the sup and inf to obtain
( ( cid : 123 ) b ( ) + r ( ) ) = sup
( ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) b
( ) + r ( ) )
is the conjugate dual function of r ( ) + i ( ) .
consequently ,
assuming that it is feasible to compute the dual function r imizing the rsl reduces to an unconstrained optimization problem used to dene the surrogate function b .
we involving the function b illustrate with a particular family of examples :
example 123 ( regularized surrogate bethe likelihood ) .
sup - pose that we use the surrogate function bt ( ; e ) dened in
123 convex surrogates in parameter estimation
theorem 123 to form a surrogate likelihood ( cid : 123 ) b , and that for our reg - ularizing function , we use an ( cid : 123 ) q norm ( cid : 123 ) ( cid : 123 ) q with q 123
for the dis - crete markov random elds to which the reweighted bethe surrogate bt ( ; e ) applies , the parameter space is simply = rd , so that we can
( ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) q )
( ) = sup
using the fact that ( cid : 123 ) ( cid : 123 ) q = sup ( cid : 123 ) ( cid : 123 ) lem ( 123 ) takes the form :
if ( cid : 123 ) ( cid : 123 ) q ( cid : 123 ) 123 , where 123 / q
( cid : 123 ) = 123 123 / q ,
q ( cid : 123 ) 123 ( cid : 123 ) , ( cid : 123 ) .
thus , the dual rsl prob -
note that this problem has a very intuitive interpretation : it involves maximizing the bethe entropy , subject to an ( cid : 123 ) q ( cid : 123 ) - norm constraint on
the distance to the empirical mean parameters ( cid : 123 ) .
as pointed out by
several authors ( 123 , 123 ) , a similar dual interpretation also exists for reg - ularized exact maximum likelihood .
the choice of regularization trans - lates into a certain uncertainty or robustness constraint in the dual domain .
for instance , given ( cid : 123 ) 123 regularization ( q = 123 ) on , we have ( cid : 123 ) = 123 so that the dual regularization is also ( cid : 123 ) 123 - based .
on the other hand , for ( cid : 123 ) 123 - regularization , ( q = 123 ) the corresponding dual norm is ( cid : 123 ) ( cid : 123 ) = + ) , so that we have box constraints in the dual domain .
from a convex surrogate .
these properties are straightforward to ana -
there remain a large number of open research questions associ - ated with the use of convex surrogates for approximate estimation .
an immediate statistical issue is to understand the consistency , asymptotic
normality and other issues associated with an estimate ( cid : 123 ) b obtained lyze , since the estimate ( cid : 123 ) b is a particular case of an m - estimator ( 123 ) .
we are guaranteed that ( cid : 123 ) b converges in probability to the minimizer of
consequently , under mild conditions on the surrogate ( so as to ensure uniform convergence of the empirical version to the population version ) ,
123 convex relaxations and upper bounds
the population surrogate likelihood namely
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) b ( )
b = arg min
where = e ( ( x ) ) are the mean parameters under the true distribu - tion p .
when the convex surrogate is strictly convex , the unique min -
b such that imizer of the population version is the parameter vector b ) = .
consequently , unless the mapping b coincides with the
mapping a , the estimators based on convex surrogates of the type described here can be inconsistent , meaning that the random variable
( cid : 123 ) b may not converge to the true model parameter
of course , such inconsistency is undesirable if the true parameter is of primary interest .
however , in other settings , it may actually be
desirable to use an inconsistent estimator .
for instance , suppose that we are interested in constructing a model with good predictive power , and moreover that in constructing new predictions , we are constrained to use only tractable algorithms .
for instance , we might be prevented from computing the exact likelihoods and marginal distributions of a graphical model , but instead required to use only approximate algo - rithms , such as sum - product or expectation - propagation .
in such set - tings , one can imagine that it might be benecial to use an incorrect model , since we are applying an approximate algorithm to generate pre - dictions .
based on this intuition , wainwright ( 123 ) proves that when performing prediction using the reweighted sum - product algorithm , the parameter estimates from the tree - reweighted bethe surrogate , even though they are dierent from the true model , yield lower prediction error than an oracle given access to the true model , but also required to use the reweighted sum - product algorithm .
it is likely that such phenomena also arise in other settings where computation is limited .
integer programming , max - product , and linear
thus far , the bulk of our discussion has focused on variational methods that , when applied to a given model p , yield approximations to the mean parameters = e ( ( x ) ) , as well as to the log partition or cumu - lant function a ( ) .
in this section , we turn our attention to a related but distinct problem namely , that of computing a mode or most probable conguration associated with the distribution p .
it turns out that the mode problem has a variational formulation in which the set m of mean parameters once again plays a central role .
more specically , since mode computation is of signicant interest for discrete random vectors , the marginal polytopes m ( g ) play a central role through much of this section .
123 variational formulation of computing modes
given a member p of some exponential family , the mode computation x m that belongs to problem refers to computing a conguration x
p ( x ) : = ( y x m | p ( y ) p ( x ) x x m ) .
123 max - product and lp relaxations
we assume that at least one mode exists , so that this arg max set is nonempty .
moreover , it is convenient for development in the sequel to note the equivalence
p ( x ) = arg max
which follows from the exponential form of the density p , and the fact that the cumulant function a ( ) does not depend on x .
it is not immediately obvious how the map problem ( 123 ) is related to the variational principle from theorem 123 .
as it turns out , the link lies in the notion of the zero - temperature limit .
we begin by provid - ing intuition for the more formal result to follow .
from its denition and theorem 123 , for any parameter vector , the cumulant function has the following two representations :
a ( ) : = log
( ( cid : 123 ) , ( cid : 123 ) a
now suppose that we rescale the canonical parameter by some scalar > 123
for the sake of this argument , let us assume that for all > 123
such a rescaling will put more weight , in a relative sense , on regions of the sample space x m for which ( cid : 123 ) , ( x ) ( cid : 123 ) is large .
ultimately , as + , probability mass should remain only on congurations x
in the set arg maxxx m ( cid : 123 ) , ( x ) ( cid : 123 ) .
this intuition suggests that the behavior of the function a ( ) should have a close connection to the problem of computing maxxx m ( cid : 123 ) , ( x ) ( cid : 123 ) .
since a ( ) may diverge as + , it is most nat - ural to consider the limiting behavior of the scaled quantity a ( ) / .
more formally , we state the following : theorem 123 for all , the problem of mode computation has the following alternative representations : ( cid : 123 ) , ( x ) ( cid : 123 ) = max ( cid : 123 ) , ( x ) ( cid : 123 ) = lim
123 variational formulation of computing modes
we provide the proof of this result in appendix b . 123; here we elaborate on its connections to the general variational principle ( 123 ) , stated in theorem 123 .
as a particular consequence of the general variational principle , we have
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) a
( cid : 123 ) , ( cid : 123 ) 123
in this context , one implication of theorem 123 is that the order of the limit over and the supremum over can be exchanged .
it is important to note that such an exchange is not correct in general; in this case , justication is provided by the convexity of a appendix b . 123 for details ) .
as with the variational principle of theorem 123 , only certain special cases of the variational problem max m ( cid : 123 ) , ( cid : 123 ) can be solved exactly in a computationally ecient manner .
the case of tree - structured markov random elds on discrete random variables , discussed in section 123 , is one such case; another important exactly solvable case is the gauss markov random eld , discussed in section 123 and appendix c . 123
with reference to other ( typically intractable ) models , since the objective function itself is linear , the sole source of diculty is the set m , and in particular , the complexity of characterizing it with a small number of constraints .
in the particular case of a discrete markov random eld associated with a graph g , the set m corresponds to the marginal polytope m ( g ) , as discussed previously in sections 123 and 123
for a dis - crete random vector , the mode - nding problem maxxx m ( cid : 123 ) , ( x ) ( cid : 123 ) is an integer program ( ip ) , since it involves searching over a nite discrete space .
an implication of theorem 123 , when specialized to this setting , is that this ip is equivalent to a linear program over the marginal poly - tope .
since integer programming problems are np - hard in general , this equivalence underscores the inherent complexity of marginal polytopes .
from an integer pro - gram to a linear program over the convex hull of its solutions is a standard technique in integer programming and combinato - rial optimization ( e . g . , 123 , 123 ) .
the eld of polyhedral combinatorics
this type of transformation namely ,
123 max - product and lp relaxations
( 123 , 123 , 123 , 123 ) is devoted to understanding the structure of poly - topes arising from various classes of discrete problems .
as we describe in this section , the perspective of graphical models provides additional insight into the structure of these marginal polytopes .
123 max - product and linear programming on trees
as discussed in section 123 , an important class of exactly solvable mod - els are markov random elds dened on tree - structured graphs , say t = ( v , e ) .
our earlier discussion focused on the computation of mean parameters and the cumulant function , showing that the sum - product algorithm can be understood as an iterative algorithm for exactly solv - ing the bethe variational problem on trees ( 123 , 123 ) .
in this section , we discuss the parallel link between the ordinary max - product algorithm and linear programming ( 123 ) .
for a discrete mrf on the tree , the set m is given by the marginal polytope m ( t ) , whose elements consist of a marginal probability vector s ( ) for each node , and joint probability matrix st ( , ) for each edge ( s , t ) e .
recall from proposition 123 that for a tree , the polytope m ( t ) is equivalent to the constraint set l ( t ) , and so has a compact description in terms of nonnegativity , local normalization , and edge marginalization conditions .
using this fact , we now show how the mode - nding problem for a tree - structured problem can be reformulated as a simple linear program ( lp ) over the set l ( g ) .
using es ( s ( xs ) ) : =
s ( xs ) s ( xs ) to expectation under the marginal distribution s and with a similar denition for est , let us dene the cost function
( cid : 123 ) , ( cid : 123 ) : =
with this notation , theorem 123 implies that the map problem for a tree is equivalent to
the left - hand side is the standard representation of the map problem as an integer program , whereas the right - hand side an optimization
123 max - product and linear programming on trees
problem over the polytope l ( t ) with a linear objective function is a linear program .
we now have two links : rst , the equivalence between the origi - nal mode - nding integer program and the lp ( 123 ) , and second , via theorem 123 , a connection between this linear program and the zero - temperature limit of the bethe variational principle .
these links raise the intriguing possibility : is there a precise connection between the max - product algorithm and the linear program ( 123 ) ? the max - product algo - rithm like its relative the sum - product algorithm is a distributed graph - based algorithm , which operates by passing a message mts along each direction t s of each edge of the tree .
for discrete random vari - ables xs ( 123 , 123 , .
, r 123 ) , each message is a r - vector , and the mes - sages are updated according to recursion
st ( xs , xt ) + t ( xt )
the following result ( 123 ) gives an armative answer to the question above : for tree - structured graphs , the max - product updates ( 123 ) are a lagrangian method for solving the dual of the linear program ( 123 ) .
this result can be viewed as the max - product analog to the con - nection between sum - product and the bethe variational problem on trees , and its exactness for computing marginals , as summarized in to set up the lagrangian , for each xs xs , let st ( xs ) be a lagrange multiplier associated with the marginalization constraint cts ( xs ) = 123 ,
cts ( xs ) : = s ( xs )
let n rd be the set of that are nonnegative and appropriately
rd | 123 ,
s ( xs ) = 123 ,
st ( xs , xt ) = 123
with this notation , we have the following result :
123 max - product and lp relaxations
proposition 123 ( max - product and lp duality ) .
consider the dual function q dened by the following partial lagrangian formulation of the tree - structured lp ( 123 ) : l ( ; ) , where
q ( ) : = max l ( ; ) : = ( cid : 123 ) , ( cid : 123 ) +
of the max - product updates ( 123 ) , the vector , where the logarithm is taken elementwise , is an optimal
for any xed point m : = log m
solution of the dual problem minq ( ) .
we begin by deriving a convenient representation of the dual function q .
first , let us convert the tree to a directed version by rst designating some node , say node 123 without loss of generality , as the root , and then directing all the edges from parent to child t s .
with regard to this rooted tree , the objective function ( cid : 123 ) , ( cid : 123 ) has the alter -
st ( xs , xt ) + s ( xs )
with this form of the cost function , the dual function can be put into
q ( ) : = max
st ( xs , xt ) t ( xt )
where the quantities s and st are dened in terms of and as :
t ( xt ) = t ( xt ) +
st ( xs , xt ) = st ( xs , xt ) + s ( xs ) + t ( xt )
123 max - product and linear programming on trees
taking the maximum over n in equation ( 123 ) yields that the dual function has the explicit form :
q ( ) = max
st ( xs , xt ) t ( xt )
using the representation ( 123 ) , we now have the necessary ingre - dients to make the connection to the max - product algorithm .
given a vector of messages m in the max - product algorithm , we use it to dene a vector of lagrange multipliers via = log m , where the logarithm is taken elementwise .
with a bit of algebra , it can be seen that a message is a xed point of the max - product updates ( 123 ) if and , satisfy the
only if the associated edgewise consistency condition
st , as dened by
: = log m
st ( xs , xt ) =
t ( xt ) + st
for all xt xt , where st is a constant independent of x .
we now show that any such
is a dual optimal solution .
we rst claim that under the edgewise consistency condition on a tree - structured graph , we can always nd at least one conguration
m ) that satises the conditions : 123 ,
t ) arg max
s v , ( s , t ) e .
indeed , such a conguration can be constructed recursively as fol -
123 to achieve the maximum lows .
first , for the root node 123 , choose x 123 ( x123 ) .
second , for each child t n ( 123 ) , choose xt to maxi -
mize t , 123 ( xt , x 123 ) , and then iterate the process .
the edgewise consis -
tency ( 123 ) guarantees that any conguration ( x 123 , .
, x structed in this way satises the conditions ( 123a ) and ( 123b ) .
the edgewise consistency condition ( 123 ) also guarantees the fol -
t ( xt ) + st
t ( xt ) ) = max
123 max - product and lp relaxations
next , applying the denition ( 123 ) of
) = 123 ( x
combining these relations yields the following expression for the dual
and simplifying , we nd that
t ) + s ( x
s ( xs ) is an indicator function for is primal feasible; moreover , the
s ( xs ) : = i x now consider the primal solution dened by
( xt ) , where i x ) .
it is clear that
s ( xs ) i x
st ( xs , xt ) = i x
the event ( xs = x
primal cost is equal to
which is precisely equal to q (
programs ( 123 ) , the pair (
st ( xs , xt ) + s ( xs )
t ) + s ( x ) .
therefore , by strong duality for linear ) is primaldual optimal .
remark 123 a careful examination of the proof of proposition 123 shows that several steps rely heavily on the fact that the underlying graph is a tree .
in fact , the corresponding result for a graph with cycles fails to hold , as we discuss at length in section 123 .
123 max - product for gaussians and other convex
multivariate gaussians are another special class of markov random elds for which there are various guarantees of correctness associated with the max - product ( or min - sum ) algorithm .
given an undirected graph g = ( v , e ) , consider a gaussian mrf in exponential form :
( cid : 123 ) ( cid : 123 ) , x ( cid : 123 ) + ( cid : 123 ) ( cid : 123 ) , xxt ( cid : 123 ) ( cid : 123 ) a ( , )
p ( , ) ( x ) = exp
123 max - product for gaussians and other convex problems
recall that the graph structure is reected in the sparsity of the matrix , with uv = 123 whenever ( u , v ) / e .
in appendix c . 123 , we describe how taking the zero - temperature limit for a multivariate gaussian , according to theorem 123 , leads to either a quadratic program , or a semidenite program .
there are various ways in which these convex programs could be solved; here we discuss known results for the application of the max - product algorithm .
to describe the max - product as applied to a multivariate gaussian , we rst need to convert the model into pairwise form .
let us dene potential functions of the form
s ( xs ) = sxs + sx123
st ( xs , xt ) = ss;t x123
( s , t ) e ,
where the free parameters ( s , ss;t ) must satisfy the constraint
s + 123stxsxt + tt;s x123
s , s v ,
ss;t = ss .
under this condition , the multivariate gaussian density ( 123 ) can equivalently expressed as the pairwise markov random eld
with this set - up , we can now describe the max - product message - passing updates .
for graphical models involving continuous random variables , each message mts ( xs ) is a real - valued function of the inde - terminate xs .
these functional messages are updated according to the usual max - product recursion ( 123 ) , with the potentials s and st in equation ( 123 ) replaced by their s and st analogs from the pair - wise factorization ( 123 ) .
in general continuous models , it is computa - tionally challenging to represent and compute with these functions , as they are innite - dimensional quantities .
an attractive feature of the gaussian problems these messages can be compactly paramete - rized; in particular , assuming a scalar gaussian random variable xs at each node , any message must be an exponentiated - quadratic function of its argument , of the form mts ( xs ) exp ( axs + bx123
123 max - product and lp relaxations
max - product message - passing updates ( 123 ) can be eciently imple - mented with one recursion for the mean term ( number a ) , and a second recursion for the variance component ( see the papers ( 123 , 123 ) for fur -
s = e ( x123
s ) 123
for gaussian max - product applied to a tree - structured problem , the updates are guaranteed to converge , and compute both the correct means s = e ( xs ) and variances 123 s at each node ( 123 ) .
for general graphs with cycles , the max - product algorithm is no longer guaranteed to converge .
however , weiss and freeman ( 123 ) and inde - pendently rusmevichientong and van roy ( 123 ) showed that if gaus - sian max - product ( or equivalently , gaussian sum - product ) converges , then the xed point species the correct gaussian means s , but the estimates of the node variances 123 s need not be correct .
the cor - rectness of gaussian max - product for mean computation also follows as a consequence of the reparameterization properties of the sum - and max - product algorithms ( 123 , 123 ) .
weiss and freeman ( 123 ) also showed that gaussian max - product converges for arbitrary graphs if the precision matrix ( in our notation ) satises a certain diagonal dominance condition .
this sucient condition for convergence was sub - stantially tightened in later work by malioutov et al .
( 123 ) , using the notions of walk - summability and pairwise normalizability; see also the paper ( 123 ) for further renements .
moallemi and van roy ( 123 ) con - sider the more general problem of maximizing an arbitrary function of the form ( 123 ) , where the potentials ( s , st ) are required only to dene a convex function .
the quadratic functions ( 123 ) for the multi - variate gaussian are a special case of this general set - up .
for this fam - ily of pairwise separable convex programs , moallemi and van roy ( 123 ) established convergence of the max - product updates under a certain scaled diagonal dominance condition .
123 first - order lp relaxation and reweighted
in this section , we return to markov random elds with discrete random variables , for which the mode - nding problem is an integer program , as opposed to the quadratic program that arises in the gaussian case .
123 first - order lp relaxation and reweighted max - product
for a general graph with cycles , this discrete mode - nding problem is known to be computationally intractable , since it includes as spe - cial cases many problems known to be np - complete , among them max - cut and related satisability problems ( 123 ) .
given the com - putational diculties associated with exact solutions , it is appropriate to consider approximate algorithms .
an important class of approximate methods for solving integer and combinatorial optimization problems is based on linear programming relaxation .
the basic idea is to approximate the convex hull of the solution set by a set of linear constraints , and solve the resulting linear program .
if the obtained solution is integral , then the lp relaxation is tight , whereas in other cases , the obtained solution may be fractional , corresponding to looseness of the relaxation .
frequently , lp relaxations are developed in a manner tailored to specic subclasses of combinato - rial problems ( 123 , 123 ) .
in this section , we discuss the linear programming ( lp ) relax - ation that emerges from the bethe variational principle , or its zero - temperature limit .
this lp relaxation , when specialized to particular combinatorial problems among them node cover , independent set , matching , and satisability problems recovers various classical lp relaxations as special cases .
finally , we discuss connections between this lp relaxation and max - product message - passing .
for general mrfs , max - product itself is not a method for solving this lp , as we demon - strate with a concrete counterexample ( 123 ) .
however , it turns out that suitably reweighted versions of max - product are intimately linked to this lp relaxation , which provides an entry point to an ongoing line of research on lp relaxations and message - passing algorithms on graphs .
123 . 123 basic properties of first - order lp relaxation
for a general graph , the set l ( g ) no longer provides an exact charac - terization of the marginal polytope m ( g ) , but is always guaranteed to outer bound it .
consequently , the following inequality is valid for any
( cid : 123 ) , ( x ) ( cid : 123 ) = max
( cid : 123 ) , ( cid : 123 ) max
123 max - product and lp relaxations
since the relaxed constraint set l ( g ) is a polytope , the right - hand side of equation ( 123 ) is a linear program , which we refer to as the rst - order lp relaxation . 123 most importantly , the right - hand side is a linear program , where the number of constraints dening l ( g ) grows only linearly in the graph size , so that it can be solved in polynomial time for any graph ( 123 ) .
special cases of this lp relaxation ( 123 ) have studied in past and ongoing work by various authors , including the special cases of ( 123 , 123 ) - quadratic programs ( 123 ) , metric labeling with potts mod - els ( 123 , 123 ) , error - control coding problems ( 123 , 123 , 123 , 123 , 123 , 123 ) , independent set problems ( 123 , 123 ) , and various types of matching problems ( 123 , 123 , 123 ) .
we begin by discussing some generic proper - ties of the lp relaxation , before discussing some of these particular examples in more detail .
by standard properties of linear programs ( 123 ) , the optimal value of the relaxed lp must be attained by at least one an extreme point of the polytope l ( g ) .
( a element of a convex set is an extreme point if it cannot be expressed as a convex combination of two distinct elements of the set; see appendix a . 123 for more background ) .
we say that an extreme point of l ( g ) is integral if all of its components are zero or one , and fractional otherwise .
by its denition , any extreme point m ( g ) is of the form y : = ( y ) , where y x m is a xed conguration .
recall that in the canonical overcomplete representation ( 123 ) , the sucient statistic vector consists of ( 123 , 123 ) - valued indicator functions , so that y = ( y ) is a vector with ( 123 , 123 ) components .
the following result spec - ies the relation between extreme points of m ( g ) and those of l ( g ) :
proposition 123 the extreme points of l ( g ) and m ( g ) are related
( a ) all the extreme points of m ( g ) are integral , and each one
is also an extreme point of l ( g ) .
123 the term rst - order refers to its status as the rst in a natural hierarchy of relaxations , based on the treewidth of the underlying graph , as discussed at more length in section 123 .
123 first - order lp relaxation and reweighted max - product
( b ) for any graph with cycles , l ( g ) also includes additional extreme points with fractional elements that lie strictly out -
( a ) any extreme point of m ( g ) is of the form ( y ) , some conguration y x m .
each of these extreme points has 123 components , and so is integral .
since l ( g ) is a polytope in
dimensions , in order to show that ( y ) is also an extreme point of l ( g ) , it is equivalent ( 123 ) to show that there are d constraints of l ( g ) that are active at ( y ) and are also linearly independent .
for any y x m , we have i k ( xs ) = 123 for all k xs\ ( ys ) , and i k ( xs ) i l ( xt ) = 123 for all ( k , l ) ( xs xt ) \ ( ys , yt ) .
all of these active inequality constraints are linearly independent , and there are a total of
( rs 123 ) +
( rsrt 123 ) = d m |e|
such constraints .
all of the normalization and marginalization con - straints are also satised by the vector y = ( y ) , but not all of them are linearly independent ( when added to the active inequality constraints ) .
however , we can add a normalization constraint for each vertex s = 123 , .
, m , as well as a normalization constraint for each edge ( s , t ) e , while still preserving linear independence .
adding these m + |e| equality constraints to the d ( cid : 123 ) inequality constraints yields a total of d linearly independent constraints of l ( g ) that are satised by y , which implies that it is an extreme point ( 123 ) .
( b ) example 123 provides a constructive procedure for constructing fractional extreme points of the polytope l ( g ) for any graph g with
the distinction between fractional and integral extreme points is crucial , because it determines whether or not the lp relaxation ( 123 ) specied by l ( g ) is tight .
in particular , there are only two possible
123 max - product and lp relaxations
outcomes to solving the relaxation :
( a ) the optimum is attained at an extreme point of m ( g ) , in which case the upper bound in equation ( 123 ) is tight , and a mode can be obtained .
( b ) the optimum is attained only at one or more fractional extreme points of l ( g ) , which lie strictly outside m ( g ) .
in this case , the upper bound of equation ( 123 ) is loose , and the optimal solution to the lp relaxation does not specify the optimal conguration .
in this case , one can imagine various types of rounding procedures for producing near - optimal solutions ( 123 ) .
when the graph has cycles , it is possible to explicitly construct a frac - tional extreme point of the relaxed polytope l ( g ) .
example 123 ( fractional extreme points of l ( g ) ) .
as an illus - tration , let us explicitly construct a fractional extreme point for the simplest problem on which the rst - order relaxation ( 123 ) is not always exact : a binary random vector x ( 123 , 123 ) 123 following an ising model on the complete graph k123
consider the canonical parameter shown in matrix form in figure 123 ( a ) .
when st < 123 , then congurations with xs ( cid : 123 ) = xt are favored , so that the interaction is repulsive .
in contrast , when st > 123 , the interaction is attractive , because it favors congura - tions with xs = xt .
when st > 123 for all ( s , t ) e , it can be shown ( 123 ) that the rst - order lp relaxation ( 123 ) is tight , for any choice of the single node parameters ( s , s v ) .
in contrast , when st < 123 for all edges , then there are choices of s , s v for which the relaxation breaks
the following canonical parameter corresponds to a direction for which the relaxation ( 123 ) is not tight , and hence exposes a frac - tional extreme point .
first , choose s = ( 123 , 123 ) for s = 123 , 123 , 123 , and then set st = < 123 for all edges ( s , t ) , and use these values to dene the pairwise potentials st via the construction in figure 123 ( a ) .
observe that for any conguration x ( 123 , 123 ) 123 , we must have xs = xt for at least one edge ( s , t ) e .
therefore , any m ( g ) must place nonzero mass
123 first - order lp relaxation and reweighted max - product
123 the smallest graph g = ( v , e ) on which the relaxation ( 123 ) can fail to be tight .
for st 123 for all ( s , t ) e , the relaxation is tight for any choice of s , s v .
on the other hand , if st < 123 for all edges ( s , t ) , the relaxation will fail for certain choices of s , s v .
on at least one term of involving , whence maxm ( g ) ( cid : 123 ) , ( cid : 123 ) < 123
in fact , this optimal value is exactly equal to < 123
on the other hand , l ( g ) formed by the singleton and consider the pseudomarginal pairwise pseudomarginals dened as follows :
( cid : 123 ) = 123
since 123 for all elements , this value observe that ( cid : 123 ) , is the optimum of ( cid : 123 ) , ( cid : 123 ) over l ( g ) , thereby showing that the relax - ation ( 123 ) is not tight .
is an extreme point of the polytope .
if ( cid : 123 ) , ( cid : 123 ) = l ( g ) , it is sucient ( 123 ) to show that ( cid : 123 ) , ( cid : 123 ) < 123 for all ( cid : 123 ) = 123 , then for all ( s , t ) e the pairwise pseudomarginals must be of the
for s v , for ( s , t ) e .
finally , to establish that
for some st ( 123 , 123 ) .
enforcing the marginalization constraints on these pairwise pseudomarginals yields the constraints 123 = 123 = 123 and 123 123 = 123 , whence st = 123 is the only possibility .
therefore , we is a fractional extreme conclude that the pseudomarginal vector point of the polytope l ( g ) .
123 max - product and lp relaxations
given the possibility of fractional extreme points in the rst - order lp relaxation ( 123 ) , it is natural to ask the question : do fractional solutions yield partial information about the set of optimal solutions to the original integer program ? one way in which to formalize this question is through the notion of persistence ( 123 ) .
in particular , let - ting o : = arg maxxx m ( cid : 123 ) , ( x ) ( cid : 123 ) denote the set of optima to an integer program , we have :
denition 123 given a fractional solution to the lp relax - ation ( 123 ) , let i v represent the subset of vertices for which s has s for all s i .
the fractional
only integral elements , say xing xs = x solution is strongly persistent if any optimal integral solution y s for all s i .
the fractional solution is weakly persistent
s = x if there exists some y
o such that y
s for all s i .
thus , any persistent fractional solution ( whether weak or strong ) can be used to x a subset of elements in the integer program , while still being assured that there exists an optimal integral solution that is consistent .
strong persistency ensures that no candidate solutions are eliminated by the xing procedure .
hammer et al .
( 123 ) studied the roof - dual relaxation for binary quadratic programs , an lp relaxation which is equivalent to specializing the rst - order lp to binary variables , and proved the following result :
proposition 123 suppose that the rst - order lp relaxation ( 123 ) is applied to the binary quadratic program
then any fractional solution is strongly persistent .
as will be discussed at more length in example 123 , the class of binary qps includes as special cases various classical problems from the combinatorics literature , among them max - 123sat , independent set , max - cut , and vertex cover .
for all of these problems , then ,
123 first - order lp relaxation and reweighted max - product
the rst - order lp relaxation is strongly persistent .
unfortunately , this strong persistency fails to extend to the rst - order relaxation ( 123 ) with nonbinary variables; see the discussion following example 123 for a
123 . 123 connection to max - product message - passing
in analogy to the general connection between the bethe variational problem and the sum - product algorithm ( see section 123 ) , one might pos - tulate that proposition 123 could be extended to graphs with cycles specically , that the max - product algorithm solves the dual of the tree - based relaxation ( 123 ) .
in general , this conjecture is false , as shown by the following counterexample ( 123 ) .
example 123 ( max - product does not solve the lp ) .
consider the diamond graph gdia shown in figure 123 , and suppose that we wish to maximize a cost function of the form :
( x123 + x123 ) + ( x123 + x123 ) +
i ( xs ( cid : 123 ) = xt ) .
here the maximization is over all binary vectors x ( 123 , 123 ) 123 , and , , and are parameters to be specied .
by design , the cost function ( 123 ) is such that if we make suciently negative , then any optimal solu - tion will either be 123 : = ( 123 , 123 , 123 , 123 ) or 123 : = ( 123 , 123 , 123 , 123 ) .
as an extreme example , if we set = 123 , = 123 , and = , we see imme - diately that the optimal solution is 123
( note that setting = is equivalent to imposing the hard - core constraint that xs = xt for all ( s , t ) e . )
a classical way of studying the ordinary sum - and max - product algorithms , dating back to the work of gallager ( 123 ) and wiberg et al .
( 123 ) , is via the computation tree associated with the message - passing updates .
as illustrated in figure 123 ( b ) , the computation tree is rooted at a particular vertex ( 123 in this case ) , and it tracks the paths of messages that reach this root node .
in general , the ( n + 123 ) th level of tree includes all vertices t such that a path of length n joins t to the root .
for the particular example shown in figure 123 ( b ) , in the
123 max - product and lp relaxations
123 ( a ) simple diamond graph gdia .
( b ) associated computation tree after four rounds of message - passing .
max - product solves exactly the modied integer program dened by the computation tree .
rst iteration represented by the second level of the tree in panel ( b ) , the root node 123 receives messages from nodes 123 and 123 , and at the sec - ond iteration represented by the third level of the tree , it receives one message from node 123 ( relayed by node 123 ) , one message from node 123 ( relayed by node 123 ) , and two messages from node 123 , one via node 123 and the other via node 123 , and so on down the tree .
the signicance of this computation tree is based on the following observation : by denition , the decision of the max - product algorithm at the root node 123 after n iterations is optimal with respect to the modied integer program dened by the computation tree with n + 123
levels .
that is , the max - product decision will be ( cid : 123 ) x123 = 123 if and only if
the optimal conguration in the tree with x123 = 123 has higher probabil - ity than the optimal conguration with x123 = 123
for the diamond graph under consideration and given the hard - core constraints imposed by setting = , the only two possible congurations in any computa - tion tree are all - zeros , or all - ones .
thus , the max - product reduces to comparing the total weight on the computation tree associated with all - ones to that associated with all - zeros .
however , the computation tree in figure 123 ( b ) has a curious prop - erty : due to the inhomogeneous node degrees more specically , with nodes 123 and 123 having three neighbors , and 123 and 123 having only two neighbors nodes 123 and 123 receive a disproportionate representation in the computation tree .
this fact is clear by inspection , and can be
123 first - order lp relaxation and reweighted max - product
veried rigorously by setting up a simple recursion to compute the appearance fractions of nodes 123 and 123 versus nodes 123 and 123
doing so shows that nodes 123 and 123 appear roughly 123 more frequently than nodes 123 and 123
as a consequence , the ordinary max - product algo - rithm makes its decision according to the threshold rule
if + > 123
whereas the correct decision rule is based on thresholding + .
con - sequently , for any ( , ) r123 such that + > 123 but + < 123 , the max - product algorithm outputs an incorrect conguration .
for instance , setting = 123 and = 123 yields one such counterexam - ple , as discussed in wainwright et al .
kulesza and pereira ( 123 ) provide a detailed analysis of the max - product message - passing updates for this example , analytically deriving the updates and explicitly demonstrating convergence to incorrect congurations .
thus far , we have demonstrated a simple problem for which the max - product algorithm fails .
what is the connection to the lp relax - ation ( 123 ) ? it turns out that the problem instance that we have con - structed is an instance of a supermodular maximization problem .
as discussed at more length in example 123 to follow .
it can be shown ( 123 ) that the rst - order lp relaxation ( 123 ) is tight for maximizing any binary quadratic cost function with supermodular interactions .
the cost function ( 123 ) is supermodular for all weights 123 , so that in par - ticular , the rst - order lp relaxation is tight for the cost function spec - ied by ( , , ) = ( 123 , 123 , ) .
however , as we have just shown , ordinary max - product fails for this problem , so it is not solving the lp
as discussed at more length in section 123 . 123 , for some problems with special combinatorial structure , the max - product algorithm does solve the rst - order lp relaxation ( 123 ) .
these instances include the prob - lem of bipartite matching and weighted b - matching .
however , establish - ing a general connection between message - passing and the rst - order lp relaxation ( 123 ) requires developing related but dierent message - passing algorithms , the topic to which we now turn .
123 max - product and lp relaxations
123 . 123 reweighted max - product and other modied
in this section , we begin by presenting the tree - reweighted max - product updates ( 123 ) , and describing their connection to the rst - order lp relaxation ( 123 ) .
recall that in theorem 123 , we established a connec - tion between the tree - reweighted bethe variational problem ( 123 ) , and the tree - reweighted sum - product updates ( 123 ) .
note that the con - straint set in the tree - reweighted bethe variational problem is exactly the polytope l ( g ) that denes the rst - order lp relaxation ( 123 ) .
this fact suggests that there should be a connection between the zero - temperature limit of the tree - reweighted bethe variational problem and the rst - order lp relaxation .
in particular , recalling the convex surrogate b ( ) = bt ( ; e ) dened by the tree - reweighted bethe varia - tional problem from theorem 123 , let us consider the limit b ( ) / as + .
from the variational denition ( 123 ) , we have
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) 123
as discussed previously , convexity allows us to exchange the order of the limit and supremum , so that we conclude that the zero - temperature limit of the convex surrogate b is simply the rst - order lp relaxation ( 123 ) .
based on this intuition ,
it is natural to suspect that the tree - reweighted max - product algorithm should have a general connection to the rst - order lp relaxation .
in analogy to the trw - sum - product updates ( 123 ) , the reweighted max - product updates take the form :
t ) + t ( x
123 first - order lp relaxation and reweighted max - product
where ( st , ( s , t ) e ) is a vector of positive edge weights in the span - ning tree polytope . 123
as with the reweighted sum - product updates , these messages dene
a collection of pseudo - max - marginals of the form
s ( xs , xt ) exp ( st ( xs , xt ) )
where st ( xs , xt ) : = s ( xs ) + t ( xt ) + st ( xs , xt )
pseudo - max - marginals that satisfy certain conditions can be used to
the most general sucient condition is that there exists a congura -
specify an optimal conguration ( cid : 123 ) xtrw .
as detailed in the paper ( 123 ) , tion ( cid : 123 ) x = ( cid : 123 ) xtrw that is nodewise and edgewise optimal across the entire graph , meaning that ( cid : 123 ) xs arg max ( ( cid : 123 ) xs , ( cid : 123 ) xt ) arg max
s ( xs ) s v , st ( xs , xt ) ( s , t ) e .
in this case , we say that the pseudo - max - marginals satisfy the strong tree agreement condition ( sta ) .
under this condition , wainwright et al .
( 123 ) showed the following :
proposition 123 given a weight vector e in the spanning tree poly - ) of trw - max - product species an tope , any sta xed point ( m optimal dual solution for the rst - order tree lp relaxation ( 123 ) .
satisfying conditions ( 123 ) are the most prac - tically relevant , since in this case , the xed point can be used to
determine the conguration ( cid : 123 ) xtrw , which is guaranteed to be glob -
for the original problem that is , an element of
123 see theorem 123 on the reweighted sum - product algorithm and the accompanying discus - sion for more details on the choice of these edge weights .
123 max - product and lp relaxations arg maxxx m ( cid : 123 ) , ( x ) ( cid : 123 ) so that the lp relaxation solves the origi - nal map problem .
an interesting theoretical question is whether any ) , regardless of whether it satises the criteria ( 123 ) , xed point ( m species an optimal solution to the dual of the rst - order lp relax - ation ( 123 ) .
this question was left open by wainwright et al .
( 123 ) , and later resolved by kolmogorov ( 123 ) , who provided a counterex - ample , involving nonbinary variables , for which a trw max - product xed point does not correspond to a dual - optimal solution .
in subse - quent work , kolmogorov and wainwright ( 123 ) showed that for pairwise mrfs with binary variables , the equivalence between trw message - passing and the lp relaxation is exact in all cases : any xed point of trw max - product species a dual - optimal solution to the rst - order lp relaxation ( 123 ) .
examples of pairwise mrfs with binary variables for which trw message - passing always solves the lp relaxation include the ising ground state problem , as well as various combinatorial prob - lems such as the independent set problem and the vertex cover problem; see example 123 for further discussion of these examples .
kolmogorov ( 123 ) also established convergence guarantees for a cer - tain sequential scheduling of trw updates ( known as trw - s updates ) , and showed empirically that trw - s updates converge faster than standard parallel scheduling of the trw max - product updates ( 123 ) .
various forms of these reweighted max - product algorithms have been applied in problems such as segmentation and disparity problems in computer vision ( 123 , 123 , 123 , 123 , 123 , 123 ) , error - control coding ( 123 ) , side - chain prediction ( 123 , 123 ) , and sensor fusion ( 123 , 123 ) .
there also turn out to be a number of interesting connections between trw max - product and a line of research , due to schlesinger and collaborators , pre - viously published in the russian literature ( 123 , 123 ) .
the survey ( 123 ) provides a detailed overview of this line of work , and some connections to reweighted max - product and lp relaxation .
in addition to the basic trw algorithm ( 123 ) and the trw - s scheduling studied by kolmogorov ( 123 ) , other researchers have proposed distributed algorithms solving the tree - based lp relaxation ( 123 ) , including subgradient methods ( 123 , 123 ) , dual coor - dinate ascent methods ( 123 , 123 ) , annealing - type methods ( 123 , 123 ) , proximal optimization schemes ( 123 ) , and adaptive lp solvers ( 123 ) .
123 first - order lp relaxation and reweighted max - product
weiss et al .
( 123 ) discuss connections between the zero - temperature limits of convex free energy problems , including the tree - reweighted bethe problem from theorem 123 as a special case , and optima of the rst - order lp ( 123 ) .
ravikumar et al .
( 123 ) discuss various rounding schemes that can be used for nite termination of lp - solving algo - rithms , with guarantees of correctness for the rounded solutions .
123 . 123 examples of the first - order lp relaxation
in this section , we discuss various examples of the rst - order lp relax - ation ( 123 ) .
one line of ongoing work in communication and information theory studies the behavior of the lp relaxation for decoding in error - control coding .
when applied to combinatorial problems , the rst - order lp relaxation recovers various known methods from the integer pro - gramming and approximation literature ( 123 , 123 ) .
in the special case of binary variables , there are a number of links to the literature on pseudo - boolean optimization ( 123 , 123 ) .
example 123 ( lp relaxations for error - control coding ) .
we begin by discussing an instance of the rst - order lp relaxation ( 123 ) , introduced by feldman et al .
( 123 ) for decoding low - density parity check ( ldpc ) codes .
recall from example 123 the notion of an error - correcting code , and its denition as an exponential family .
the code c is a subset of the boolean hypercube ( 123 , 123 ) m , dened by a set of parity checks a .
the base measure is the counting measure restricted to the set c of all valid codewords , and the m - dimensional vector of su - cient statistics is given by ( x ) = ( x123 , .
from example 123 , the set m is given by the convex hull of all possible codewords , which is known as the codeword polytope .
figure 123 provides a toy example of an ldpc code over bits ( x123 , x123 , x123 , x123 ) ( 123 , 123 ) 123 , and with two parity checks a and b , corresponding to the constraints x123 x123 x123 = 123 and x123 x123 x123 = 123 , respectively .
a family of codes is said to be low - density if the degrees of the parity checks and variable nodes in this graphical representation remain bounded as the blocklength m is
123 max - product and lp relaxations
123 ( a ) the factor graph representation of a toy binary linear code on four bits ( x123 , x123 , x123 , x123 ) ( 123 , 123 ) 123
each codeword must satisfy the two parity check constraints x123 x123 x123 = 123 and x123 x123 x123 = 123 , as dened by the constraint functions a and b .
( b ) construction of a fractional vertex , also known as a pseudocodeword , for the code shown in panel ( a ) .
the pseudocodeword has elements ( cid : 123 ) = , which satisfy all the constraints ( 123 ) dening the lp relaxation .
however , the vector ( cid : 123 ) cannot be expressed as a convex combination of codewords , and so lies strictly outside the codeword polytope , which corresponds to the marginal polytope for this graphical model .
when expressed as a graphical model using the parity check func - tions ( see equation ( 123 ) ) , the code is not immediately recogniz - able as a pairwise mrf to which the rst - order lp relaxation ( 123 ) can be applied .
however , any markov random eld over discrete vari - ables can be converted into pairwise form by the selective introduc - tion of auxiliary variables .
in particular , suppose that for each check a f , we introduce an auxiliary variable za taking values in the space ( 123 , 123 ) |n ( a ) | .
dening pairwise interactions between za and each bit xi for nodes i n ( a ) , we can use za as a device to enforce constraints over the subvector ( xi , i n ( a ) ) .
see appendix e . 123 for further details on converting a general discrete graphical model to an equivalent pair - if we apply the rst - order lp relaxation ( 123 ) to this pair - wise mrf , the relevant variables consist of a pseudomarginal vector for each i v , and for each check a f , a set of pseu - domarginals ( a;j , j an even - sized subset of n ( a ) ) .
in this case , the pairwise marginalization conditions that dene the set l ( g ) reduce to
a;j = i ,
for each a , and i n ( a ) ,
123 first - order lp relaxation and reweighted max - product
along with the box constraints i , a;j ( 123 , 123 ) .
it is also possible to remove the variables a;j by fouriermotzkin elimination ( 123 ) , so as to obtain an equivalent lp relaxation that is described only in terms .
doing so shows that the rst - order of the vector relaxation ( 123 ) , when applied to the coding problem , is characterized by the box constraints i ( 123 , 123 ) for each i v , and for each a f , the forbidden set constraints
k 123 k n ( a ) with |k| odd .
( 123 k ) +
the interpretation of this inequality is very intuitive : it enforces that for each parity check a f , the subvector n ( a ) = ( i , i n ( a ) ) must be at hamming distance at least one from any odd - parity conguration over the bits n ( a ) .
in practice , a binary codeword x c is transmitted through a chan - nel , so that the user receives only a vector of noisy observations .
as described in example 123 , channel transmission can be modeled in terms of a conditional distribution p ( yi | xi ) .
given a particular received sequence ( y123 , .
, ym ) from the channel , dene the vector rm of log likelihoods , with components i = log p ( yi|xi=123 ) p ( yi|xi=123 ) .
the ml decoding prob - lem corresponds to the integer program of maximizing the likelihood iv ixi over the discrete set of codewords x c .
it is well known to be computationally intractable ( 123 ) in a worst case sense .
the lp decoding algorithm of feldman et al .
( 123 ) is based on iv ii subject to the box constraints maximizing the objective ( 123 , 123 , .
, m ) ( 123 , 123 ) m as well as the forbidden set constraints ( 123 ) .
since its introduction ( 123 , 123 ) , the performance of this lp relaxation has been extensively studied ( e . g . , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 ) .
not surprisingly , given the role of the constraint set l ( g ) in the bethe variational problem , there are close connections between lp decoding and standard iterative algorithms like sum - product decoding ( 123 , 123 , 123 ) .
among other connections , the fractional extreme points of the rst - order lp relaxation have a very specic interpretation as pseu - docodewords of the underlying code , studied in earlier work on iterative decoding ( 123 , 123 , 123 ) .
figure 123 ( b ) provides a concrete illustration of a pseudocodeword that arises when the relaxation is applied to the toy
123 max - product and lp relaxations
code shown in figure 123 ( b ) .
consider the vector ( cid : 123 ) : = ( 123 , 123 a little more work , it can be shown that ( cid : 123 ) is a vertex of the relaxed lp decoding polytope . ) however , we claim that ( cid : 123 ) does not belong to
123 , 123 ) ; it is easy to verify that it satises the box inequalities and the forbid - den set constraints ( 123 ) that dene the lp relaxation .
( in fact , with
the marginal polytope for this graphical model that is , it cannot be written as a convex combination of codewords .
to see this fact , note that by taking a modulo two sum of the parity check constraints x123 x123 x123 = 123 and x123 x123 x123 = 123 , we obtain that x123 x123 = 123 for we must have 123 = 123 , which implies that ( cid : 123 ) lies outside the codeword any codeword .
therefore , for any vector in the codeword polytope ,
apart from its interest in the context of error - control coding , the fractional vertex in figure 123 ( b ) also provides a counterexample regarding the persistency of fractional vertices of the polytope l ( g ) .
recall from our discussion at the end of section 123 . 123 that the rst - order lp relaxation , when applied to integer programs with binary variables , has the strong persistency property ( 123 ) , as summarized in proposition 123 .
it is natural to ask whether strong persistency also holds for higher - order variables as well .
note that after conversion to a pairwise markov random eld ( to which the rst - order lp relax - we now claim that the fractional vertex ( cid : 123 ) illustrated in figure 123 ation applies ) , the coding problem illustrated in figure 123 ( a ) includes nonbinary variables za and zb at each of the factor nodes a , b f .
123 , 123 ) , a little calculation shows that ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) = 123
in contrast , ( cid : 123 ) = ( 123 , 123 have x123 = 123 , even though ( cid : 123 ) 123 = 123 in the fractional vertex .
consequently ,
constitutes a failure of strong persistency for the rst - order lp relax - ation with higher - order variables .
consider applying the lp decoder to the cost function = ( 123 , 123 , 123 , 123 ) ; using the previously constructed = ( 123 , 123 , 123 , 123 ) , both with ( cid : 123 ) = 123
thus , neither of the optimal integral solutions
the optimal codewords are x value ( cid : 123 ) , x ( cid : 123 ) = ( cid : 123 ) , y
= ( 123 , 123 , 123 , 123 ) and y
strong persistency is violated for this instance .
we conclude that relaxation ( 123 ) is strongly persistent only for pairwise markov random elds over binary random variables , otherwise
123 first - order lp relaxation and reweighted max - product
known as binary quadratic programs .
we now turn to an in - depth con - sideration of this particular case :
example 123 ( binary quadratic programs and combinatorial problems ) .
recall the ising model , as rst introduced in example 123 : it is an exponential family over a vector x of binary random vari - ables , which may take either spin values ( 123 , +123 ) m , or zero - one values ( 123 , 123 ) m .
note that the mapping xs ( cid : 123 ) 123xs 123 and its inverse zs ( cid : 123 ) 123 123 ( zs + 123 ) may be used to convert freely back and forth from the ( 123 , 123 ) form to the ( 123 , +123 ) form .
let us consider the ( 123 , 123 ) - case and the mode - nding problem asso - ciated with the canonical overcomplete set of potential functions
( cid : 123 ) , ( x ) ( cid : 123 ) = max
and st ( xs , xt ) : =
st;jki j ( xs ) i k ( xt )
are weighted sums of indicator functions .
this problem is a binary quadratic program , and includes as special cases various types of classical problems in combinatorial optimization .
set and vertex cover : given an undirected graph g = ( v , e ) , an independent set i is a subset of vertices such that ( s , t ) / e for all s , t i .
given a set of vertex weights ws 123 , the max - imum weight independent set ( mwis ) problem is to nd the indepen - si ws .
this problem has dent set i with maximal weight , w ( i ) : = applications in scheduling for wireless sensor networks , where the inde - pendent set constraints are imposed to avoid interference caused by having neighboring nodes transmit simultaneously .
to model this combinatorial problem as an instance of the binary quadratic program , let x ( 123 , 123 ) m be an indicator vector for mem - bership in s , meaning that xs = 123 if and only if s s .
then dene
123 max - product and lp relaxations
, and the pairwise interaction
st ( xs , xt ) =
with these denitions , the binary quadratic program corresponds to the maximum weight independent set ( mwis ) problem .
a related problem is that of nding a vertex cover that is , a set c of vertices such that for any edge ( s , t ) e , at least one of s sc ws .
this or t belongs to c with minimum weight w ( c ) = minimum weight vertex cover ( mwvc ) problem can be recast as a maximization problem : it is another special case of the binary qp with
st ( xs , xt ) =
let us now consider how the rst - order lp relaxation ( 123 ) special - izes to these problems .
recall that the general lp relaxation is in terms , and an of the two - vector of singleton pseudomarginals s = analogous 123 123 matrix st of pairwise pseudomarginals .
in the inde - pendent set problem , the parameter setting st;123 = is tantamount to enforcing the constraint st;123 = 123 , so that the lp relaxation can be expressed purely in terms of a vector rm with elements s : = s;123
after simplication , the constraints dening l ( g ) ( see proposition 123 ) can be reduced to s 123 for all nodes s v , and s + t 123 for all edges ( s , t ) e .
thus , for the mwis problem , the rst - order relax - ation ( 123 ) reduces to the linear program
such that s + t 123 for all ( s , t ) e .
this lp relaxation is the classical one for the independent set prob - lem ( 123 , 123 ) .
sanghavi et al .
( 123 ) discuss some connections between the ordinary max - product algorithm and this lp relaxation , as well as to auction algorithms ( 123 ) .
in a similar way , specializing the rst - order lp relaxation ( 123 ) to
the mwvc problem yields the linear program
such that s + t 123 for all ( s , t ) e ,
123 first - order lp relaxation and reweighted max - product
which is another classical lp relaxation from the combinatorics max - cut : consider the following graph - theoretic problem : given a nonnegative weight wst 123 for each edge of an undirected graph g = ( v , e ) , nd a partition ( u , u c ) of the vertex set such that the asso -
w ( u , u c ) : =
( ( s , t ) | su , tu c )
of edges across the partition is maximized .
to model this max - cut problem as an instance of the binary quadratic program , let x ( 123 , 123 ) m be an indicator vector for membership in u , meaning that xs = 123 if and only if s u .
then dene s ( xs ) = 123 for all vertices , and dene the pairwise interaction
st ( xs , xt ) =
with these denitions , a little algebra shows that problem ( 123 ) is equivalent to the max - cut problem , a canonical example of an np - complete problem .
as before , the rst - order lp relaxation ( 123 ) can be specialized to this problem .
in section 123 , we also describe the celebrated semidenite program ( sdp ) relaxation for max - cut due to goemans and williamson ( 123 ) ( see example 123 ) .
supermodular and submodular interactions : an important subclass of binary quadratic programs are those based on supermodular potential functions ( 123 ) .
the interaction st is supermodular if st ( 123 , 123 ) + st ( 123 , 123 ) st ( 123 , 123 ) + st ( 123 , 123 ) ,
and it is submodular if the function gst ( xs , xt ) = st ( xs , xt ) is super - modular .
note that the max - cut problem and the independent set problem both involve submodular potential functions , whereas the ver - tex cover problem involves supermodular potentials .
it is well known that the class of regular binary qps meaning supermodular max - imization problems or submodular minimization problems can be solved in polynomial time .
as a particular instance , consider the prob - lem of nding the minimum st cut in a graph .
this can be formulated
123 max - product and lp relaxations
as a minimization problem in terms of the potential functions ( 123 ) , with additional nonzero singleton potentials s , yielding an instance of submodular minimization .
it is easily solved by conversion to a max - imum ow problem , using the classical fordfulkerson duality theo - rem ( 123 , 123 ) .
however , the max - cut , independent set , and vertex cover problems all fall outside the class of regular binary qps , and indeed are canonical instances of intractable problems .
among other results , kolmogorov and wainwright ( 123 ) establish that tree - reweighted max - product is exact for any regular binary qp .
the same statement fails to hold for the ordinary max - product updates , since it fails on the regular binary qp discussed in example 123 .
the exactness of tree - reweighted max - product stems from the tightness of the rst - order lp relaxation ( 123 ) for regular binary qps .
this tight - ness can be established via results due to hammer et al .
( 123 ) on the so - called roof dual relaxation in pseudo - boolean optimization , which turns out to be equivalent to the tree - based relaxation ( 123 ) for the special case of binary variables .
finally , we discuss a related class of combinatorial problems for which some recent work has studied message - passing and linear
example 123 ( maximum weight matching problems ) .
given an undirected graph g = ( v , e ) , the matching problem is to nd a subset f of edges , such that each vertex is adjacent to at most one edge e f .
in the weighted variant , each edge is assigned a weight we , and the goal is to nd the matching f that maximizes the weight ef we .
this maximum weight matching ( mwm ) problem is well known to be solvable in polynomial time for any graph ( 123 ) .
for bipartite graphs , the mwm problem can be reduced to an especially simple linear program , which we derive here as a special case of the rst - order relaxation ( 123 ) .
in order to apply the rst - order relaxation , it is convenient to rst reformulate the matching problem as a mode - nding problem in an mrf described by a factor graph , and then convert the factor graph to a pairwise form , as in example 123 .
we begin by associating with the
123 first - order lp relaxation and reweighted max - product
original graph g = ( v , e ) a hypergraph ( cid : 123 ) g , in which each edge e e corresponds to a vertex of ( cid : 123 ) g , and each vertex s v corresponds to a of ( cid : 123 ) g ( edges of the original graph ) in the set e ( s ) = ( e e | s e ) .
hyperedge .
the hyperedge indexed by s connects to all those vertices
finally , we dene a markov random eld over the hypergraph as follows .
first , let each e e be associated with a binary variable xe ( 123 , 123 ) , which acts as an indicator variable for whether edge e participates in the matching .
we dene the weight function e ( xe ) = wexe , where we is the weight specied for edge e in the matching problem .
second , given the subvector xe ( s ) = ( xe , e e ( s ) ) , we dene an associated interaction
with this denition , the mode - nding problem
e ( cid : 123 ) s xe 123
in the hypergraph is equivalent to the original matching problem .
this hypergraph - based mode - nding problem ( 123 ) can be con - verted to an equivalent mode - nding problem in a pairwise markov random eld by following the generic recipe described in appendix e . 123
doing so and applying the rst - order lp relaxation ( 123 ) to the result - ing pairwise mrf yields the following lp relaxation of the maximum weight matching m
xe 123 e e ,
xe 123 s v .
this lp relaxation is a classical one for the matching problem , known to be tight for any bipartite graph but loose for nonbipar - tite graphs ( 123 ) .
a line of recent research has established close links between the lp relaxation and the ordinary max - product algo - rithm , including the case of bipartite weighted matching ( 123 ) , bipartite
123 max - product and lp relaxations
weighted b - matching ( 123 ) , weighted matching on general graphs ( 123 ) , and weighted b - matching on general graphs ( 123 ) .
123 higher - order lp relaxations
the tree - based relaxation ( 123 ) can be extended to hypertrees of higher treewidth t , by using the hypertree - based outer bounds lt ( g ) on marginal polytopes described in section 123 . 123
this extension pro - duces a sequence of progressively tighter lp relaxations , which we describe here .
given a hypergraph g = ( v , e ) , we use the short - hand notation xh : = ( xi , i h ) to denote the subvector of variables associated with hyperedge h e .
we dene interaction potentials j h;j i ( xh = j ) and consider the mode - nding problem of
note that problem ( 123 ) generalizes the analogous problem for pair - wise mrfs , a special case in which the hyperedge set consists of only vertices and ordinary edges .
letting t + 123 denote the maximal cardinality of any hyperedge , the relaxation based on lt ( g ) involves the collection of pseudomarginals ( h | h e ) subject to local consistency constraints h ) = 123 h e ,
h ) = g ( xg ) g h
following the same reasoning as in section 123 . 123 , we have the upper
123 higher - order lp relaxations
notice that the relaxation ( 123 ) can be applied directly to a graphical model that involves higher - order interactions , obviating the need to convert higher - order interactions into pairwise form , as done in illustrating the rst - order lp relaxation ( see example 123 ) .
in fact , in certain cases , this direct application can yield a tighter relaxation than that based on conversion to the pairwise case , as illustrated in example 123 .
of course , the relaxation ( 123 ) can also be applied directly to a pairwise markov random eld , which can always be embedded into a hypergraph with higher - order interactions .
thus , equation ( 123 ) actually describes a sequence of relaxations , based on the nested constraint sets
l123 ( g ) l123 ( g ) l123 ( g ) .
lt ( g ) .
m ( g ) ,
with increasing accuracy as the interaction size t is increased .
the tradeo , of course , is that computational complexity of the relaxation also increases in the parameter t .
the following result is an immediate consequence of our development thus far :
proposition 123 ( hypertree tightness ) .
for any hypergraph g of treewidth t , the lp relaxation ( 123 ) based on the set lt ( g ) is exact .
this assertion is equivalent to establishing that lt ( g ) = m ( g ) for any hypergraph g of treewidth t .
here m ( g ) denotes the marginal polytope , corresponding to marginal probability distributions ( h , h e ) dened over the hyperedges that are globally consistent .
first , the inclusion lt ( g ) m ( g ) is immediate , since any set of glob - ally consistent marginals must satisfy the local consistency conditions in the reverse direction , consider a locally consistent set of pseudo - marginals lt ( g ) .
using these pseudomarginals , we may construct the functions h dened previously ( 123 ) in our discussion of hypertree factorization .
using these quantities , let us dene distribution
p ( x123 , x123 , .
, xm ) : =
123 max - product and lp relaxations
this distribution is constructed according to the factorization princi - ple ( 123 ) for hypertrees .
using the local normalization and marginal - ization conditions that ( h , h e ) satises by virtue of membership in lt ( g ) , it can be veried that this this distribution is properly nor - x p ( x ) = 123 ) .
moreover , for each hyperedge h e , this distribution has marginal distribution h , meaning that
p ( x123 , .
, xm ) = h ( xh )
for all h e .
again , this fact can be veried by direct calculation from the factoriza - tion , or can be derived as a consequence of the junction tree theorem .
consequently , the distribution ( 123 ) provides a certicate of the mem - bership of in the marginal polytope m ( g ) .
in the binary ( 123 , 123 ) case , the sequence of relaxations ( 123 ) has been proposed and studied previously by hammer et al .
( 123 ) , boros et al .
( 123 ) , and sherali and adams ( 123 ) , although without the connections to the underlying graphical structure provided by proposition 123 .
example 123 ( tighter relaxations for higher - order interac - tions ) .
we begin by illustrating how higher - order lp relaxations yield tighter constraints with a continuation of example 123 .
consider the factor graph shown in figure 123 ( a ) , corresponding to a hypergraph - structured mrf of the form
i ( xi ) + a ( x123 , x123 , x123 ) + b ( x123 , x123 , x123 )
for suitable potential functions ( i , i = 123 , .
. 123 ) , a , and b .
in this exam - ple , we consider two dierent procedures for obtaining an lp relaxation :
( a ) first convert the hypergraph mrf ( 123 ) into a pairwise
mrf and then apply the rst - order relaxation ( 123 ) ; or
( b ) apply the second - order relaxation based on l123 ( g ) directly
to the original hypergraph mrf .
after some algebraic manipulation , both relaxations can be expressed purely in terms of the triplet pseudomarginals 123 ( x123 , x123 , x123 ) and
123 higher - order lp relaxations
123 ( a ) a hypergraph - structured markov random eld with two sets of triplet interac - tions over a = ( 123 , 123 , 123 ) and b = ( 123 , 123 , 123 ) .
the rst - order lp relaxation ( 123 ) applied to this graph rst converts it into an equivalent pairwise mrf , and thus enforces only consistency only via the singleton marginals 123 and 123
( b ) the second - order lp relaxation enforces additional consistency over the shared pairwise pseudomarginal 123 , and is exact for this
123 ( x123 , x123 , x123 ) , and in particular their consistency on the overlap ( x123 , x123 ) .
the basic rst - order lp relaxation ( procedure ( a ) ) imposes only the two marginalization conditions
123 , x123 ) =
123 , x123 , x
123 , x123 , x
which amount to ensuring that the singleton pseudomarginals 123 and 123 induced by 123 and 123 agree .
note , however , that the rst - order relax - ation imposes no constraint on the pairwise marginal ( s ) 123 induced by
( x123 , x123 , x123 ) and ( x123 , x123 , x123 ) exactly , and so addition to the singleton conditions ( 123 ) , also requires agreement on the overlap viz .
123 , x123 , x123 ) =
123 ( x123 , x123 , x
as a special case of proposition 123 , this second - order relaxation is tight , since the hypergraph in figure 123 ( a ) has treewidth two .
in example 123 , we considered the rst - order lp relaxation applied to the mrf in figure 123 ( a ) for the special case of a linear code dened
123 max - product and lp relaxations over binary random variables x ( 123 , 123 ) 123
there we constructed the
123 , 123 ) ,
123 ( 123 ) , 123 ( 123 ) , 123 ( 123 ) , 123 ( 123 )
and showed that it was a fractional vertex for the relaxed polytope .
although the vector ( cid : 123 ) is permitted under the pairwise relaxation in ation in figure 123 ( b ) .
indeed , the singleton pseudomarginals ( cid : 123 ) are con -
figure 123 ( a ) , we claim that it is forbidden by the second - order relax -
sistent with triplet pseudomarginals 123 and 123 dened as follows : let 123 assign mass 123 123 to the congurations ( x123 , x123 , x123 ) = ( 123 ) and ( 123 ) , with zero mass elsewhere , and let 123 assign mass 123 123 to the congura - tions ( x123 , x123 , x123 ) = ( 123 ) and ( 123 ) , and zero elsewhere .
by computing the induced marginals , it can be veried that 123 and 123 marginal -
required by the rst - order relaxation .
however , the second - order relax - ation also requires agreement over the overlap ( x123 , x123 ) , as expressed by condition ( 123 ) .
on one hand , by denition of 123 , we have
ize down to ( cid : 123 ) , and satisfy the rst - order consistency condition ( 123 )
whereas on the other hand , by denition of 123 , we have
123 , x123 , x123 ) =
i ( ( x123 , x123 ) = ( 123 , 123 ) ) +
i ( ( x123 , x123 ) = ( 123 , 123 ) ) ,
i ( ( x123 , x123 ) = ( 123 , 123 ) ) +
i ( ( x123 , x123 ) = ( 123 , 123 ) ) .
123 ( x123 , x123 , x
consequently , condition ( 123 ) is violated .
more generally , the second - order lp relaxation is exact for fig - ure 123 ( b ) , meaning that it is impossible to nd any triplet pseudo -
marginals 123 and 123 that marginalize down to ( cid : 123 ) , and agree over the
overlap ( x123 , x123 ) .
of course , the treewidth - two relaxation can be applied directly to mrfs with cost functions already in pairwise form .
in explicit terms , the second - order lp relaxation applied to a pairwise mrf has the form
st ( xs , xt ) st ( xs , xt )
subject to the constraints
st ( xs , xt ) =
( s , t , u ) ! s , s v
stu ( xs , xt , x
( s , t , u ) ! ( s , t ) , ( s , t ) e .
123 higher - order lp relaxations
assuming that all edges and vertices are involved in the cost function , this relaxation involves m singleton marginals ,
note that even though the triplet pseudomarginals stu play no role in the cost function ( 123 ) itself , they nonetheless play a central role in the relaxation via the consistency conditions ( 123 ) that they impose .
among other implications , equations ( 123a ) and ( 123b ) imply that the pairwise marginals must be consistent with the singleton marginals t ) = s ( xs ) ) .
since these pairwise conditions dene the rst - order lp relaxation ( 123 ) , this fact conrms that the second - order lp relaxation is at least as good as the rst - order one .
in general , the triplet consistency also imposes additional constraints not ensured by pairwise consistency alone .
for instance , example 123 shows that the pairwise constraints are insucient to characterize the marginal poly - tope of a single cycle on three nodes , whereas proposition 123 implies that the triplet constraints ( 123 ) provide a complete characterization , since a single cycle on three nodes has treewidth two .
this technique namely , introducing additional parameters in order to tighten a relaxation , such as the triplet pseudomarginals stu is known as a lifting operation .
it is always possible , at least in principle , to project the lifted polytope down to the original space , thereby yielding an explicit representation of the tightened relaxation in the original space .
this combination is known as a lift - and - project method ( 123 , 123 , 123 ) .
in the case of the lifted relaxation based on the triplet consistency ( 123 ) dening l123 ( g ) , the projection is from the full space down to the set of pairwise and singleton marginals .
example 123 ( lift - and - project for binary mrfs ) .
here we illus - trate how to project the triplet consistency constraints ( 123 ) for any binary markov random eld , thereby deriving the so - called cycle
123 max - product and lp relaxations
inequalities for the binary marginal polytope .
( we presented these cycle inequalities earlier in example 123; see section 123 of deza and laurent ( 123 ) for a complete discussion . ) in order to do so , it is con - venient to work with a minimal set of pseudomarginal parameters : in particular , for a binary markov random eld , the seven numbers ( stu , st , su , tu , s , t , u ) suce to characterize the triplet pseudo - marginal over variables ( xs , xt , xu ) .
following a little algebra , it can be shown that the singleton ( 123a ) and pairwise consistency ( 123b ) conditions are equivalent to the inequalities :
stu s + st + su stu 123 s t u + st + su + tu stu st , su , tu .
note that following permutations of the triplet ( s , t , u ) , there are eight inequalities in total , since inequality ( 123b ) has three distinct versions .
the goal of projection is to eliminate the variable stu from the description , and obtain a set of constraints purely in terms of the singleton and pairwise pseudomarginal parameters .
if we consider sin - gleton , pairwise , and triplet pseudomarginals for all possible combina - tions , there are a total of t = m + parameters .
we would like to project this subset of rt down to a lower - dimensional subset of rl , where l = m + is the total number of singleton and pairwise parameters .
more specically , we would like to determine the set ( l123 ( g ) ) rl of pseudomarginal parameters of the ( s , t , u , st , su , tu ) | stu such that inequalities ( 123 ) hold
corresponding to the projection of l123 ( g ) down to rl .
a classical technique for computing such projections is fourier motzkin elimination ( 123 , 123 ) .
it is based on the following two steps : ( a ) rst express all the inequalities so that the variable stu to be eliminated appears on the left - hand side; and ( b ) then combine the ( ) constraints with the ( ) constraints in pairs , thereby yielding a new inequality in which the variable stu no longer plays a role .
note that stu appears
123 higher - order lp relaxations
combine ( 123a ) and ( 123d ) combine ( 123b ) and ( 123c ) combine ( 123b ) and ( 123d ) combine ( 123b ) and ( 123d ) combine ( 123a ) and ( 123c ) .
on the left - hand side of all the inequalities ( 123 ) ; hence , performing step ( b ) yields the following inequalities st , su , tu 123 , 123 + tu t u 123 , s su 123 , s + tu su st 123 , 123 s t u + st + su + tu 123 , the rst three sets of inequalities should be familiar; in particular , they correspond to the constraints dening the rst - order relaxation , in the special case of binary variables ( see example 123 ) .
finally , the last two inequalities , and all permutations thereof , correspond to a known set of inequalities on the binary marginal polytope , usually referred to as the cycle inequalites . 123 in recent work , sontag and jaakkola ( 123 ) exam - ined the use of these cycle inequalities , as well as their extensions to non - binary variables , in variational methods for approximate marginal - ization , and showed signicantly more accurate results in many cases .
123 to be clear , in the specied form , these inequalities are usually referred to as the triangle inequalities , for obvious reasons .
note that for a graph g = ( v , e ) with |v | variables , there are a total of such groups of triangle inequalities .
however , if the graph g is not fully connected , then some of these triangle inequalities involve mean parameters uv for pairs ( u , v ) not in the graph edge set .
in order to obtain inequalities that involve only mean parameters st for edges ( s , t ) e , one can again perform fouriermotzkin elimination , which leads to the so - called cycle inequalities .
see deza and laurent ( 123 ) for more details .
moment matrices , semidenite constraints , and
conic programming relaxation
although the linear constraints that we have considered thus far yield a broad class of relaxations , many problems require a more expressive framework for imposing constraints on parameters .
in particular , as we have seen at several points in the preceding sections , semidenite constraints on moment matrices arise naturally within the variational approach for instance , in our discussion of gaussian mean param - eters from example 123 .
this section is devoted to a more system - atic study of moment matrices , in particular their use in constructing hierarchies of relaxations based on conic programming .
moment matri - ces and conic programming provide a very expressive language for the design of variational relaxations .
in particular , we will see that the lp relaxations considered in earlier sections are special cases of conic relaxations in which the underlying cone is the positive orthant .
we will also see that moment matrices allow us to dene a broad class of additional conic relaxations based on semidenite programming ( sdp ) and second - order cone programming ( socp ) .
the study of moment matrices and their properties has an extremely rich history ( e . g . , ( 123 , 123 ) ) , particularly in the context of scalar ran - dom variables .
the basis of our presentation is more recent work
123 moment matrices and their properties
( e . g . , 123 , 123 , 123 , 123 ) that applies to multivariate moment problems .
while much of this work aims at general classes of problems in algebraic geometry , in our treatment we limit ourselves to considering marginal polytopes and we adopt the statistical perspective of imposing posi - tive semideniteness on covariance and other moment matrices .
the moment matrix perspective allows for a unied treatment of various relaxations of marginal polytopes .
see wainwright and jordan ( 123 ) for additional material on the ideas presented here .
123 moment matrices and their properties given a random vector y rd , consider the collection of its second - order moments : st = e ( ysyt ) , for s , t = 123 , .
using these moments , we can form the following symmetric d d matrix :
n ( ) : = e ( y y t ) =
at rst sight , this denition might seem limiting , because the matrix involves only second - order moments .
however , given some random vector x of interest , we can expose any of its moments by dening y = f ( x ) for a suitable choice of function f , and then considering the associated second - order moment matrix ( 123 ) for y .
for instance , by setting y : = both rst and second - order moments of x .
similarly , by including terms of the form xsxt in the denition of y , we can expose third moments of x .
the signicance of the moment matrix ( 123 ) lies in the following
( cid : 123 ) r rm , the moment matrix ( 123 ) will include
lemma 123 ( moment matrices ) .
any valid moment matrix n ( ) is positive semidenite .
we must show that at n ( ) a 123 for an arbitrary vector a rd .
if is a valid moment vector , then it arises by taking expectations
123 moment matrices and conic relaxations
under some distribution p .
accordingly , we can write at n ( ) a = ep ( at y y t a ) = ep ( ( cid : 123 ) at y ) ( cid : 123 ) 123 ) ,
which is clearly nonnegative .
= ( st , s , t = 123 , .
, d ) to dene balid second - order moments .
such a condition is both necessary and sucient for certain classical moment problems involving scalar random variables ( e . g . , 123 , 123 ) .
this condi - tion is also necessary and sucient for a multivariate gaussian random vector , as discussed in example 123 .
123 . 123 multinomial and indicator bases now , consider a discrete random vector x ( 123 , 123 , .
, r 123 ) m .
in order to study moments associated with x , it is convenient to dene some function bases .
our rst basis involves multinomial functions over ( x123 , .
each multinomial is associated with a multi - index , mean - ing a vector : = ( 123 , 123 , .
, m ) of nonnegative integers s .
for each such multi - index , we dene the multinomial function
following the convention that x123
t = 123 for any position t such that t = 123
for discrete random variables in x m : = ( 123 , 123 , .
, r 123 ) m , it suf - to consider multi - indices such that the maximum degree ( cid : 123 ) ( cid : 123 ) : = maxs=123 , . . . , m s is less than or equal to r 123
consequently , our multinomial basis involves a total of rm multinomial functions .
we dene the hamming norm ( cid : 123 ) ( cid : 123 ) 123 : = card ( i = 123 , .
, m | i ( cid : 123 ) = 123 ) , which counts the number of nonzero elements in the multi - index .
for each integer k = 123 , .
, m , we then dene the multi - index set
ik : = ( | | ( cid : 123 ) ( cid : 123 ) 123 k ) .
j=123 ( x j ) = 123 123 indeed , for any variable x x = ( 123 , 123 , .
, r 123 ) , note that there holds a simple rearrangement of this relation yields an expression for xr as a polynomial of degree r 123 , which implies that any monomial xi with i r can be expressed as a linear combination of lower - order monomials .
123 moment matrices and their properties
the nested sequence of multi - index sets
i123 i123
describes a hierarchy of models , which can be associated with hyper - graphs with increasing sizes of hyperedges .
to calculate the cardinality of ik , observe that for each i = 123 , .
, k , there are possible subsets of size i .
moreover , for each member of each such subset , there are ( r 123 ) possible choices of the index value , so that ik has elements in total .
the total number of all possible multi - indices ( with ( cid : 123 ) ( cid : 123 ) r 123 ) is given by |im| =
( r 123 ) i = rm .
our second basis is a generalization of the standard overcom - plete potentials ( 123 ) , based on indicator functions for events of the form ( xs = j ) as well as their higher - order analogs .
recall the ( 123 , 123 ) - valued indicator function i js ( xs ) for the event ( xs = js ) .
using these node - based functions , we dene , for each global conguration j = ( j123 , .
, jm ) ( 123 , 123 , .
, r 123 ) m , the indicator function if ( x123 , .
, xm ) = ( j123 ,
i js ( xs ) =
i j ( x ) : =
in our discussion thus far , we have dened these function bases over the full vector ( x123 , .
, xm ) ; more generally , we can also consider the same bases for subvectors ( x123 , .
overall , for any integer k m , we have two function bases : the rk vector of multinomial
| ( 123 , 123 , .
, r 123 ) k
mk ( x123 , .
, xk ) : =
and the rk vector of indicator functions
ik ( x123 , .
, xk ) : =
i j ( x ) | j ( 123 , 123 , .
, r 123 ) k
the following elementary lemma shows that these two bases are in lemma 123 for each k = 123 , 123 , .
, m , there is an invertible rk rk matrix b such that
mk ( x ) = b ik ( x )
for all x x k .
123 moment matrices and conic relaxations
our proof is based on explicitly constructing the matrix b .
for each s = 123 , .
, k and j ( 123 , 123 , 123 , .
, r 123 ) , consider the following iden - tities between the scalar indicator functions i j ( xs ) and monomials xj
i j ( xs ) =
( cid : 123 ) j i ( cid : 123 ) ( xs ) .
using the second identity multiple times ( once for each s ( 123 , .
, k ) ) , we obtain that for each ik ,
( cid : 123 ) s i ( cid : 123 ) ( xs )
by expanding the product on the right - hand side , we obtain an expres - sion for multinomial x as a linear combination of the indicator func - tions ( i j ( x ) , | j x k ) .
conversely , for each j ( 123 , 123 , .
, r 123 ) k , we
i j ( x ) : =
i js ( xs ) =
as before , by expanding the product on the right - hand side , we obtain an expression for the indicator function i j as a linear combination of the monomials ( x , ik ) .
thus , there is an invertible linear transfor - mation between the indicator functions ( i j ( x ) , j x k ) and the mono - mials ( x , ik ) .
we let b rrkrk denote the invertible matrix that carries out this transformation .
these bases are convenient for dierent purposes , and lemma 123 allows us to move freely between them .
the mean parameters associ - ated with the indicator basis i are readily interpretable as probabil - ities viz .
e ( i j ( x ) ) = p ( x = j ) .
however , the multinomial basis is convenient for dening hierarchies of semidenite relaxations .
123 . 123 marginal polytopes for hypergraphs
we now turn to the use of these function bases in dening marginal polytopes for general hypergraphs .
given a hypergraph g = ( v , e )
123 semidenite bounds on marginal polytopes
in which the maximal hyperedge has cardinality k , we may consider the multinomial markov random eld p ( x ) exp here i ( g ) ik corresponds to those multi - indices associated with the hyperedges of g .
for instance , if g includes the triplet hyperedge ( s , t , u ) , then i ( g ) must include the set of r123 multi - indices of the form for some ( s , t , u ) ( 123 , 123 , .
, r 123 ) 123
( 123 , 123 , .
, s , t , u , .
, 123 ) as an important special case , for each integer t = 123 , 123 , .
, m , we also dene the multi - index sets it : = i ( km , t ) , where km , t denotes the hypergraph on m nodes that includes all hypergraphs up to size t .
for instance , the hypergraph km , 123 is simply the ordinary complete graph on m nodes , including all let p ( x m ) denote the set of all distributions p supported on x m = ( 123 , 123 , .
, r 123 ) m .
for any multi - index zm + and distribution p p ( x m ) , let
: = ep ( x ) = ep
denote the associated mean parameter or moment .
( we simply write e ( x ) when the underlying distribution p is understood from the con - text . ) we use m ( g ) to denote the marginal polytope associated with hypergraph g that is , m ( g ) is the set r|i ( g ) | | p p ( x m ) such that = ep ( x ) i ( g )
to be precise , it should be noted that our choice of notation is slightly inconsistent with previous sections , where we dened marginal polytopes in terms of the indicator functions i j ( xs ) and i j ( xs ) i k ( xt ) .
however , by the one - to - one correspondence between these indicator functions and the multinomials ( x ) from lemma 123 , the correspond - ing marginal polytopes are isomorphic objects , regardless of the under - lying potential functions chosen .
123 semidenite bounds on marginal polytopes
we now describe the lasserre sequence ( 123 , 123 ) of semidenite outer bounds on the marginal polytope m ( g ) .
this sequence is dened in
123 moment matrices and conic relaxations
terms of a hierarchy of moment matrices which generate a nested sequence of semidenite outer bounds on any marginal polytope .
123 . 123 lasserre sequence
to dene the relevant moment matrices , for each t = 123 , 123 , .
, m , consider the |it| |it| matrix of moments nt ( ) dened by the |it| - dimensional random vector y : = ( x , it ) .
each row and col - umn of nt ( ) is associated with some multi - index it , and its entries are specied as follows :
: = + = e ( x x ) .
as a particular example , figure 123 ( a ) provides an illustration of the matrix n123 ( ) for the special case of binary variables ( r = 123 ) on three nodes ( m = 123 ) , so that the overall matrix is eight - dimensional .
123 moment matrices and minors dening the lasserre sequence of semidenite relax - ations for a triplet ( m = 123 ) of binary variables ( r = 123 ) .
( a ) full matrix n123 ( ) .
( b ) shaded region : 123 123 principal minor n123 ( ) constrained by the lasserre relaxation at order 123
123 semidenite bounds on marginal polytopes
the shaded region in figure 123 ( b ) shows the matrix n123 ( ) for this same example; note that it has |i123| = 123 rows and columns .
let us make some clarifying comments regarding these moment matrices .
first , so as to simplify notation in this binary case , we have used 123 as a shorthand for the rst - order moment 123 , 123 , 123 , with similar shorthand for the other rst - order moments 123 and 123
sim - ilarly , the quantity 123 is shorthand for the second - order moment 123 , 123 , 123 = e ( x123 123 ) , and the quantity 123 denotes the triplet moment 123 , 123 , 123 = e ( x123x123x123 ) .
second , in both matrices , the element in the upper left - hand corner is
= 123 , 123 , 123 = e ( x123 ) ,
which is always equal to one .
third , in calculating the form of these moment matrices , we have repeatedly used the fact that x123 i = xi for any binary variable to simplify moment calculations .
for instance , in computing the ( 123 , 123 ) element of n123 ( ) , we write
e ( ( x123x123x123 ) ( x123x123 ) ) = e ( x123x123x123 ) = 123
we now describe how the moment matrices nt ( ) induce outer bounds on the marginal polytope m ( g ) .
given a hypergraph g , let t ( g ) denote the smallest integer such that all moments associated with m ( g ) appear in the moment matrix nt ( g ) ( ) .
( for example , for an ordi - nary graph with maximal cliques of size two , such as a 123d lattice , we have t ( g ) = 123 , since all moments of orders one and two appear in the matrix n123 ( ) . ) for each integer t = t ( g ) , .
, m , we can dene the map - ping g : r|it| r|i ( g ) | that maps any vector rit to the indices ( i ( g ) ) .
then for each t = t ( g ) , 123 , .
. , dene the semidenite con -
st ( g ) : = g ( ( r|it| | nt ( ) ( cid : 123 ) 123 ) ) .
as a consequence of lemma 123 , each set st ( g ) is an outer bound on the marginal polytope m ( g ) , and by denition of the matrices nt ( ) , these outer bounds form a nested sequence
s123 ( g ) s123 ( g ) s123 ( g ) m ( g ) .
123 moment matrices and conic relaxations
this sequence is known as the lasserre sequence of relaxations ( 123 , 123 ) .
lovasz and schrijver ( 123 ) describe a related class of relaxations , also based on semidenite constraints .
example 123 ( first - order semidenite bound on m ( k123 ) ) .
as an illustration of the power of semidenite bounds , recall exam - in which we considered the fully connected graph k123 on three nodes .
in terms of the 123d vector of sucient statistics ( x123 , x123 , x123 , x123x123 , x123x123 , x123x123 ) , we considered the pseudomarginal
123 , 123 , 123 , 123 , 123 , 123
in example 123 , we used the cycle inequalities , as derived in exam - ple 123 , to show that does not belong to m ( k123 ) .
here we provide an alternative and arguably more direct proof of this fact based on semidenite constraints .
in particular , the moment matrix n123 ( ) asso - ciated with these putative mean parameters has the form :
a simple calculation shows that this matrix n123 ( ) is not positive de - nite , whence / s123 ( k123 ) , so that by the inclusions ( 123 ) , we conclude that / m ( k123 ) .
123 . 123 tightness of semidenite outer bounds
given the nested sequence ( 123 ) of outer bounds on the marginal poly - tope m ( g ) , we now turn to a natural question : what is the minimal required order ( if any ) for which st ( g ) provides an exact character - ization of the marginal polytope ? it turns out that for an arbitrary hypergraph , t = m is the minimal required ( although see section 123 and proposition 123 for sharper guarantees based on treewidth ) .
this property of nite termination for semidenite relaxations in a general
123 semidenite bounds on marginal polytopes
setting was proved by lasserre ( 123 ) , and also by laurent ( 123 , 123 ) using dierent methods .
we provide here one proof of this exactness :
proposition 123 ( tightness of semidenite constraints ) .
for any hypergraph g with m vertices , the semidenite constraint set sm ( g ) provides an exact description of the associated marginal poly -
although the result holds more generally ( 123 ) , we prove it here for the case of binary random variables .
the inclusion m ( g ) sm ( g ) is an immediate consequence of lemma 123 , so that it remains to estab - lish the reverse inclusion .
in order to do so , it suces to consider a vec - tor r123m , with elements indexed by indicator vectors ( 123 , 123 ) m of subsets of ( 123 , .
in particular , element represents a candidate moment associated with the multinomial x = s .
suppose that the matrix nm ( ) r123m123m is positive semidenite; we need to show that is a valid moment vector , meaning that there is some distribution p supported on ( 123 , 123 ) m such that = ep ( x ) for all im .
lemma 123 shows that the multinomial basis mm and the indicator basis im are related by an invertible linear transformation b r123m123m .
in the binary case , each multi - index can be associated with a subset of ( 123 , 123 , .
, m ) , and we have the partial order dened by inclusion of these subsets .
it is straightforward to verify that the matrix b arises from the inclusion - exclusion principle , and so has entries
b ( , ) =
123 has entries
moreover , the inverse matrix b 123 ( , ) =
see appendix e . 123 for more details on these matrices , which arise as a special case of the mobius inversion formula .
in appendix e . 123 , we prove that b diagonalizes nm ( ) , so that we have bnm ( ) bt = d for some diagonal matrix d with a nonnegative
123 moment matrices and conic relaxations
entries , and trace ( d ) = 123
thus , we can dene a probability distribu - tion p supported on ( 123 , 123 ) m with elements p ( ) = d 123
since b is t .
focusing on the th diago - invertible , we have nm ( ) = b nal entry , the denition of nm ( ) implies that ( nm ( ) ) = .
conse - quently , we have
t ( , )
t ( , ) ,
using the fact that d is diagonal .
using the form ( 123 ) of b
p ( ) = ep ( x ) ,
which provides an explicit demonstration of the global realizability
this result shows that imposing a semidenite constraint on the largest possible moment matrix nm ( ) is sucient to fully character - ize all marginal polytopes .
moreover , the t = m condition in proposi - tion 123 is not only sucient for exactness , but also necessary in a worst case setting that is , for any t < m , there exists some hypergraph g such that m ( g ) is strictly contained within st ( g ) .
the following exam - ple illustrates both the suciency and necessity of proposition 123 :
( ( in ) exactness of semidenite constraints ) .
consider a pair of binary random variables ( x123 , x123 ) ( 123 , 123 ) 123
with respect to the monomials ( x123 , x123 , x123x123 ) , the marginal polytope m ( k123 ) consists of three moments ( 123 , 123 , 123 ) .
figure 123 ( a ) provides an illustration of this 123d polytope; as derived previously in exam - ple 123 , this set is characterized by the four constraints and s 123 123
123 + 123 + 123 123 123 ,
for s = 123 , 123
123 semidenite bounds on marginal polytopes
the rst - order semidenite constraint set s123 ( k123 ) is dened by the semidenite moment matrix constraint
123 123 123 123 123 123
in order to deduce the various constraints implied by this semide - nite inequality , we make use of an extension of sylvesters criterion for assessing positive semideniteness : a square matrix is positive semidef - inite if and only if the determinant of any principal submatrix is non - negative ( see horn and johnson ( 123 ) , p .
so as to facilitate both our calculation and subsequent visualization , it is convenient to focus on the intersection of both the marginal polytope and the constraint set s123 ( k123 ) with the hyperplane 123 = 123
stepping through the require - ments of sylvesters criterion , positivity of the ( 123 , 123 ) element is trivial , and the remaining singleton principal submatrices imply that 123 123 and 123 123
with a little calculation , it can be seen the ( 123 , 123 ) and ( 123 , 123 ) principal submatrices yield the interval constraints 123 , 123 ( 123 , 123 ) .
the ( 123 , 123 ) principal submatrix yields 123 123 123 , which after setting 123 = 123 reduces to 123 |123| .
finally , we turn to nonnegativity of the full determinant : after setting 123 = 123 and some algebraic manipula - tion , we obtain the the constraint 123 ) = ( 123 123 ) ( 123 123 ( 123 123 ) ) 123
since 123 |123| from before , this quadratic inequality implies the pair
123 ) 123 + ( 123
and 123 123 ( 123 123 ) .
the gray area in figure 123 ( b ) shows the intersection of the 123d marginal polytope m ( k123 ) from panel ( a ) with the hyperplane 123 = 123
the intersection of the semidenite constraint set s123 ( k123 ) with this same hyperplane is characterized by the interval inclusion 123 ( 123 , 123 ) and the two inequalities in equation ( 123 ) .
note that the semidenite con - straint set is an outer bound on m ( k123 ) , but that it includes points that are clearly not valid marginals .
for instance , it can be veried that ( 123 , 123 , 123 ) = ( 123 123 ) corresponds to a positive semidenite n123 ( ) , but this vector certainly does not belong to m ( k123 ) .
123 , 123 123 , 123
123 moment matrices and conic relaxations
123 ( a ) the marginal polytope m ( k123 ) for a pair ( x123 , x123 ) ( 123 , 123 ) 123 of random variables; it is a a polytope with four facets contained within the cube ( 123 , 123 ) 123
( b ) nature of the semidenite outer bound s123 on the marginal polytope m ( k123 ) .
the gray area shows the cross - section of the binary marginal polytope m ( k123 ) corresponding to intersection with the hyperplane 123 = 123
the intersection of s123 with this same hyperplane is dened by the inclusion 123 ( 123 , 123 ) , the linear constraint 123 123 , and the quadratic constraint 123 123 123
consequently , there are points belonging to s123 that lie strictly outside m ( k123 ) .
in this case , if we move up one more step to the semidenite outer bound s123 ( k123 ) , then proposition 123 guarantees that the description should be exact .
to verify this fact , note that s123 ( k123 ) is based on imposing positive semideniteness of the moment matrix
123 123 123 123 123 123 123 123 123 123 123
positivity of the diagonal element ( 123 , 123 ) gives the constraint 123 123
the determinant of 123 123 submatrix formed by rows and columns 123 and 123 ( referred to as the ( 123 , 123 ) subminor for short ) is given by 123 ( 123 123 ) .
combined with the constraint 123 123 , the nonnegativity of this deter - minant yields the inequality 123 123 123
by symmetry , the ( 123 , 123 ) sub - minor gives 123 123 123
finally , calculating the determinant of n123 ( )
123 + 123 123 123
det n123 ( ) = 123
the constraint det n123 ( ) 123 , in conjunction with the previous con - implies the inequality 123 + 123 123 123 123
in fact , the
123 semidenite bounds on marginal polytopes
quantities ( 123 , 123 123 , 123 123 , 123 + 123 123 123 ) are the eigen - values of n123 ( ) , so positive semideniteness of n123 ( ) is equivalent to nonnegativity of these four quantities .
the positive semideniteness of s123 ( k123 ) thus recovers the four inequalities ( 123 ) that dene m ( k123 ) , thereby providing an explicit conrmation of proposition 123 .
from a practical point of view , however , the consequences of propo - sition 123 are limited , because nm ( ) is a |im| |im| matrix , where |im| = rm is exponentially large in the number of variables m .
appli - cations of sdp relaxations are typically based on solving a semide - nite program involving the constraint nt ( ) ( cid : 123 ) 123 for some t substantially smaller than the total number m of variables .
we illustrate with a con - tinuation of the max - cut problem , introduced earlier in section 123 . 123
example 123 ( first - order sdp relaxation of max - cut ) .
as described previously in example 123 , the max - cut problem arises from a graph - partitioning problem , and is computationally intractable ( np - complete ) for general graphs g = ( v , e ) .
here , we describe a natu - ral semidenite programming relaxation obtained by applying the rst - order semidenite relaxation s123 ( g ) .
in the terminology of graphical models , the max - cut problem is equivalent to computing the mode of a binary pairwise markov random eld ( i . e . , an ising model ) with a very specic choice of interactions .
more specically , it corresponds to solving the binary quadratic program
xsxt + ( 123 xs ) ( 123 xt )
where wst 123 is a weight associated with each edge ( s , t ) of the graph g .
by theorem 123 , this integer program can be represented as an equivalent linear program .
in particular , introducing the mean param - eters s = e ( xs ) for each node s v , and st = e ( xsxt ) for each edge ( s , t ) e , we can rewrite the binary quadratic program ( 123 ) as
123st + 123 s t
where m ( g ) is the marginal polytope associated with the ordinary graph .
although originally described in somewhat dierent terms , the
123 moment matrices and conic relaxations
relaxation proposed by goemans and williamson ( 123 ) amounts to replacing the marginal polytope m ( g ) with the rst - order semide - nite outer bound s123 ( g ) , thereby obtaining a semidenite programming relaxation .
solving this sdp relaxation yields a moment matrix , which can be interpreted as the covariance matrix of a gaussian random vec - tor in m - dimensions .
sampling from this gaussian distribution and then randomly rounding the resulting m - dimensions yields a random - ized algorithm for approximating the maximum cut .
remarkably , goe - mans and williamson ( 123 ) show that in the worst case , the expected cut value obtained in this way is at most 123 less than the optimal cut value .
related work by nesterov ( 123 ) provides similar worst - case guarantees for applications of the rst - order sdp relaxation to general classes of binary quadratic programs .
in addition to the max - cut problem , semidenite programming relaxations based on the sequence st ( g ) , t = 123 , 123 , .
have been stud - ied for various other classes of combinatorial optimization problems , including graph coloring , graph partitioning and satisability prob - lems .
determining when the additional computation required to impose higher - order semidenite constraints does ( or does not ) yield more accurate approximations is an active and lively area of research .
123 link to lp relaxations and graphical structure
recall from our previous discussion that the complexity of a given marginal polytope m ( g ) depends very strongly on the structure of the ( hyper ) graph g .
one consequence of the junction tree theorem is that marginal polytopes associated with hypertrees are straightforward to characterize ( see propositions 123 and 123 ) .
this simplicity is also apparent in the context of semidenite characterizations .
in fact , the lp relaxations discussed in section 123 , which are tight for hypertrees of appropriate widths , can be obtained by imposing semidenite con - straints on particular minors of the overall moment matrix nm ( ) , as we illustrate here .
given a hypergraph g = ( v , e ) and the associated subset of multi - indices i ( g ) , let m ( g ) be the |i ( g ) | - dimensional marginal polytope
123 link to lp relaxations and graphical structure
dened by the sucient statistics ( x , i ( g ) ) .
for each hyper - edge h e , let i ( h ) be the subset of multi - indices with nonzero com - ponents only in the elements of h , and let m ( ( h ) ) be the |i ( h ) | - dimensional marginal polytope associated with the subvector of multi - nomials ( x , i ( h ) ) .
letting the maximal hyperedge have cardinal - ity t + 123 , we can then rewrite the hypertree - based lp relaxation from section 123 in the form
( r|i ( g ) | | h ( ) m ( ( h ) ) ) ,
where h : r|i ( g ) | r|i ( h ) | is the standard coordinate projection .
imposing semideniteness on moment matrices pro - duces constraint sets that are convex but nonpolyhedral ( i . e . , with curved boundaries; see figure 123 ( b ) for an illustration ) .
in certain cases , however , a semidenite constraint actually reduces to a set of linear inequalities , so that the associated constraint set is a polytope .
we have already seen one instance of this phenomenon in proposi - tion 123 , where by a basis transformation ( between the indicator func - tions i j ( xs ) and the monomials xs s ) , we showed that a semidenite constraint on the full matrix nm ( ) is equivalent to a total of rm linear
in similar fashion , it turns out that the lp relaxation lt ( g ) can be described in terms of semideniteness constraints on a subset of minors from the full moment matrix nm ( ) .
in particular , for each hyperedge h e , let n ( h ) ( ) denote the |i ( h ) | |i ( h ) | minor of nm ( ) , and dene the semidenite constraint set
r|im| | n ( h ) ( ) ( cid : 123 ) 123
as a special case of proposition 123 for m = |h| , we conclude that the projection of this constraint set down to the mean parameters indexed by i ( h ) namely , the set h ( s ( ( h ) ) ) is equal to the local marginal polytope m ( ( h ) ) .
we have thus established the following :
proposition 123 given a hypergraph g with maximal hyperedge size |h| = t + 123 , an equivalent representation of the polytope lt ( g ) is in
123 moment matrices and conic relaxations
terms of the family of hyperedge - based semidenite constraints
obtained by imposing semideniteness on certain minors of the full moment matrix nm ( ) .
this relaxation is tight when g is a hypertree of width t .
figure 123 provides an illustration of the particular minors that are constrained by the relaxation lt ( g ) in the special case of the sin - gle cycle k123 on m = 123 nodes with binary variables ( see figure 123 ) .
each panel in figure 123 shows the full 123 123 moment matrix n123 ( ) associated with this graph; the shaded portions in each panel demar - cate the minors n ( 123 ) ( ) and n ( 123 ) ( ) constrained in the rst - order
123 shaded regions correspond to the 123 123 minors n ( 123 ) ( ) in panel ( a ) , and n ( 123 ) ( ) in panel ( b ) are constrained by the sheraliadams relaxation of order 123
also constrained is the minor n ( 123 ) ( ) ( not shown ) .
123 link to lp relaxations and graphical structure
lp relaxation l ( g ) .
in addition , this lp relaxation also constrains the minor n ( 123 ) ( ) , not shown here .
thus , the framework of moment matrices allows us to understand both the hierarchy of lp relaxations dened by the hypertree - based polytopes lt ( g ) , as well as the sdp hierarchy based on the nonpolyhe - dral sets st ( g ) .
it is worth noting that at least for general graphs with m 123 vertices , this hierarchy of lp and sdp relaxations are mutu - ally incomparable , in that neither one dominates the other at a xed level of the hierarchy .
we illustrate this mutual incomparability in the
example 123 ( mutual incomparability of lp / sdp relax - ations ) .
for the single cycle k123 on m = 123 vertices , neither the rst - order lp relaxation l123 ( g ) nor the rst - order semidenite relaxation s123 ( k123 ) are exact .
at the same time , these two relaxations are mutually incomparable , in that neither dominates the other .
in one direction , example 123 provides an instance of a pseudomarginal vector that belongs to l123 ( k123 ) , but violates the semidenite constraint dening s123 ( k123 ) .
in the other direction , we need to construct a pseudomarginal vector that satises the semidenite constraint n123 ( ) ( cid : 123 ) 123 , but vio - lates at least one of the linear inequalities dening l123 ( k123 ) .
consider the pseudomarginal with moment matrix n123 ( ) of the form :
123 123 123
123 123 123 123 123 123
calculation shows that n123 ( ) is positive denite , yet the inequality 123 + 123 123 123 123 from the conditions ( 123 ) , necessary for mem - bership in l123 ( k123 ) , is not satised .
hence , the constructed vector belongs to s123 ( k123 ) but does not belong to l123 ( k123 ) .
of course , for hypergraphs of low treewidth , the lp relaxation is denitely advantageous , in that it is tight once the order of relaxation t hits the hypergraph treewidth that is , the equality lt ( g ) = m ( g ) holds for any hypergraph of treewidth t , as shown in proposition 123 .
123 moment matrices and conic relaxations
in contrast , the semidenite relaxation st ( g ) is not tight for a general hypergraph of width t; as concrete examples , see example 123 for the failure of s123 ( g ) for the graph g = k123 , which has treewidth t = 123 , and example 123 for the failure of s123 ( g ) for the graph g = k123 , which has treewidth t = 123
however , if the order of semidenite relaxation is raised to one order higher than the treewidth , then the semidenite relaxation proposition 123 for a random vector x ( 123 , 123 , .
, r 123 ) m that is markov with respect to a hypergraph g , we always have
where the inclusion is strict unless g is a hypertree of width t .
for any such hypertree , the equality st+123 ( g ) = m ( g ) holds .
includes as minors
each maximal hyperedge h in a hypergraph g with treewidth t has cardinality |h| = t + 123
note that the moment matrix nt+123 ( ) constrained by the relaxation st+123 ( g ) |i ( h ) | |i ( h ) | matrix n ( h ) ( ) , for every hyperedge h e .
this fact implies that st+123 ( g ) s ( ( h ) ) for each hyperedge h e , where the set s ( ( h ) ) was dened in equation ( 123 ) .
hence , the asserted inclusion fol - lows from proposition 123 .
for hypergraphs of treewidth t , the equal - ity st+123 ( g ) = m ( g ) follows from this inclusion , and the equivalence between lt ( g ) and m ( g ) for hypergraphs of width t , as asserted in
123 second - order cone relaxations
in addition to the lp and sdp relaxations discussed thus far , another class of constraints on marginal polytopes are based on the second - order cone ( see appendix a . 123 ) .
the resulting second - order cone program - ming ( socp ) relaxations of the mode - nding problem as well as of general nonconvex optimization problems have been studied by various researchers ( 123 , 123 ) .
in the framework of moment matrices , second - order cone constraints can be understood as a particular weakening of a positive semideniteness constraint , as we describe here .
123 second - order cone relaxations
the semidenite relaxations that we have developed are based on enforcing that certain moment matrices are positive semidenite .
all of these constraints can be expressed in terms of moment matrices of
n ( ) : = e
where the random vector y = f ( x123 , .
, xm ) is a suitable function of the vector x .
the class of socp relaxations are based on the following
lemma 123 for any moment matrix ( 123 ) , the positive semidenite - ness constraint n ( ) ( cid : 123 ) 123 holds if and only if
( cid : 123 ) ( cid : 123 ) u t u , e ( y y t ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) u e ( y ) ( cid : 123 ) 123
for all u rdd .
using the schur complement formula ( 123 ) , a little calculation shows that the condition n ( ) ( cid : 123 ) 123 is equivalent to requiring that the covariance matrix : = e ( y y t ) e ( y ) e ( y ) be positive semidenite .
a symmetric matrix is positive semidenite if and only if its frobenius inner product ( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) : = trace ( ) with any other positive semidenite matrix s d + is nonnegative ( 123 ) .
( this fact corresponds to the self - duality of the positive semidenite cone ( 123 ) . ) by the singular value decomposition ( 123 ) , any s d + can be written123 as = u t u for some matrix u rdd , so that the constraint ( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) 123 can be rewritten as ( cid : 123 ) ( cid : 123 ) u t u , e ( y y t ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) u e ( y ) ( cid : 123 ) 123 , as claimed .
an inequality of the form ( 123 ) corresponds to a second - order cone constraint on the elements of the moment matrix n ( ) ; see appendix a . 123 for background on this cone .
the second - order cone pro - gramming approach is based on imposing the constraint ( 123 ) only for a subset of matrices u , so that socps are weaker than the associated sdp relaxation .
the benet of this weakening is that many socps
123 we can obtain rank - decient by allowing u to have some zero rows .
123 moment matrices and conic relaxations
can be solved with lower computational complexity than semide - nite programs ( 123 ) .
kim and kojima ( 123 ) study socp relaxations for fairly general classes of nonconvex quadratic programming prob - lems .
kumar et al .
( 123 ) study the use of socps for mode - nding in pairwise markov random elds , in particular using matrices u that are locally dened on the edges of the graph , as well as additional linear inequalities , such as the cycle inequalities in the binary case ( see exam - ple 123 ) .
in later work , kumar et al .
( 123 ) showed that one form of their socp relaxation is equivalent to a form of the quadratic programming ( qp ) relaxation proposed by laerty and ravikumar ( 123 ) .
kumar et al .
( 123 ) also provide a cautionary message , by demonstrating that cer - tain classes of socp constraints fail to improve upon the rst - order tree - based lp relaxation ( 123 ) .
an example of such redundant socp constraints are those in which the matrix u t u has nonzero entries only in pairs of elements , corresponding to a single edge , or more generally , is associated with a subtree of the original graph on which the mrf is dened .
this result can be established using moment matrices by the following reasoning : any soc constraint ( 123 ) that constrains only ele - ments associated with some subtree of the graph is redundant with the rst - order lp constraints ( 123 ) , since the lp constraints ensure that any moment vector is consistent on any subtree embedded within the
overall , we have seen that a wide range of conic programming relax - ations , including linear programming , second - order cone programming , and semidenite programming , can be understood in a unied manner in terms of multinomial moment matrices .
an active area of current research is devoted to understanding the properties of these types of relaxations when applied to particular classes of graphical models .
the core of this survey is a general set of variational principles for the problems of computing marginal probabilities and modes , applicable to multivariate statistical models in the exponential family .
a fundamen - tal object underlying these optimization problems is the set of realiz - able mean parameters associated with the exponential family; indeed , the structure of this set largely determines whether or not the asso - ciated variational problem can be solved exactly in an ecient man - ner .
moreover , a large class of well - known algorithms for both exact and approximate inference including belief propagation or the sum - product algorithm , expectation propagation , generalized belief prop - agation , mean eld methods , the max - product algorithm and linear programming relaxations , as well as generalizations thereof can be derived and understood as methods for solving various forms , either exact or approximate , of these variational problems .
the variational perspective also suggests convex relaxations of the exact principle , and so is a fertile source of new approximation algorithms .
many of the algorithms described in this survey are already stan - dard tools in various application domains , as described in more detail in section 123 .
while such empirical successes underscore the promise
of variational approaches , a variety of theoretical questions remain to be addressed .
one important direction to pursue is obtaining a priori guarantees on the accuracy of a given variational method for particular subclasses of problems .
past and ongoing work has studied the perfor - mance of linear programming relaxations of combinatorial optimiza - tion problems ( e . g . , ( 123 , 123 , 123 , 123 ) ) , many of them particular cases of the rst - order lp relaxation ( 123 ) .
it would also be interesting to study higher - order analogs of the sequence of lp relaxations described in section 123
for certain special classes of graphical models , particu - larly those based on locally tree - like graphs , recent and ongoing work ( e . g . , ( 123 , 123 , 123 , 123 ) ) has provided some encouraging results on the accuracy of the sum - product algorithm in computing approximations of the marginal distributions and the cumulant function .
similar ques - tions apply to other types of more sophisticated variational methods , including the various extensions of sum - product ( e . g . , kikuchi methods or expectation - propagation ) discussed in this survey .
we have focused in this survey on regular exponential families where the parameters lie in an open convex set .
this class covers a broad class of graphical models , particularly undirected graphical models , where clique potentials often factor into products over parameters and suf - cient statistics .
however , there are also examples in which nonlinear constraints are imposed on the parameters in a graphical model .
such constraints , which often arise in the setting of directed graphical mod - els , require the general machinery of curved exponential families ( 123 ) .
although there have been specialized examples of variational meth - ods applied to such families ( 123 ) , there does not yet exist a general treatment of variational methods for curved exponential families .
another direction that requires further exploration is the study of variational inference for distributions outside of the exponential family .
in particular , it is of interest to develop variational methods for the stochastic processes that underlie nonparametric bayesian modeling .
again , special cases of variational inference have been presented for such models see in particular the work of blei and jordan ( 123 ) on variational inference for dirichlet processes but there is as of yet no
still more broadly , there are many general issues in statistical infer - ence that remain to be investigated from a variational perspective .
in particular , the treatment of multivariate point estimation and interval estimation in frequentist statistical inference often requires treating nui - sance variables dierently from parameters .
moreover , it is often neces - sary to integrate over some variables ( e . g . , random eects ) while maxi - mizing over others .
our presentation has focused on treating marginal - ization separately from maximization; further research will be needed to nd ways to best combine these operations .
also , although marginal - ization is natural from a bayesian perspective , our focus on the expo - nential family is limiting from this perspective , where it is relatively rare for the joint probability distribution of all random variables ( data , latent variables and parameters ) to follow an exponential family dis - tribution .
further work on nding variational approximations for non - exponential - family contributions to joint distributions , including nonin - formative and robust prior distributions , will be needed for variational inference methods to make further inroads into bayesian inference .
finally , it should be emphasized that the variational approach pro - vides a set of techniques that are complementary to monte carlo methods .
one important program of research , then , is to characterize the classes of problems for which variational methods ( or conversely , monte carlo methods ) are best suited , and moreover to develop a the - oretical characterization of the trade - os in complexity versus accuracy inherent to each method .
indeed , there are various emerging connec - tions between the mixing properties of markov chains for sampling from graphical models , and the contraction or correlation decay conditions that are sucient to ensure convergence of the sum - product algorithm .
a large number of people contributed to the gestation of this survey , and it is a pleasure to acknowledge them here .
the intellectual contri - butions and support of alan willsky and tommi jaakkola were par - ticularly signicant in the development of the ideas presented here .
in addition , we thank the following individuals for their comments and insights along the way : david blei , constantine caramanis , laurent el ghaoui , jon feldman , g .
david forney jr . , david karger , john laerty , adrian lewis , jon mcaulie , kurt miller , guillaume obozin - ski , michal rosen - zvi , lawrence saul , nathan srebro , erik sudderth , sekhar tatikonda , romain thibaux , yee whye teh , lieven vanden - berghe , yair weiss , jonathan yedidia , and junming yin .
( our apolo - gies to those individual who we have inadvertently forgotten . )
in this appendix , we provide some basic background on graph theory , and convex sets and functions .
a . 123 background on graphs and hypergraphs
here , we collect some basic denitions and results on graphs and hyper - graphs; see the standard books ( 123 , 123 , 123 ) for further background .
a graph g = ( v , e ) consists of a set v = ( 123 , 123 , .
, m ) of vertices , and a set e v v of edges .
by denition , a graph does not contain self - loops ( i . e . , ( s , s ) / e for all vertices s v ) , nor does it contain multiple copies of the same edge .
( these features are permitted only in the extended notion of a multigraph . ) for a directed graph , the ordering of the edge matters , meaning that ( s , t ) is distinct from ( t , s ) , whereas for an undirected graph , the quantities ( s , t ) and ( t , s ) refer to the same edge .
a subgraph f of a given graph g = ( v , e ) is a graph ( v ( f ) , e ( f ) ) such that v ( f ) v and e ( f ) e .
given a subset s v of the ver - tex set of a graph g = ( v , e ) , the vertex - induced subgraph is the sub - graph f ( s ) = ( s , e ( s ) ) with edge - set e ( s ) : = ( ( s , t ) e | ( s , t ) e ) .
given a subset of edges e
( cid : 123 ) e , the edge - induced subgraph is f ( e
( cid : 123 ) ) = ( s v | ( s , t ) e
( cid : 123 ) ) , where v ( e
123 background material
a path p is a graph p = ( v ( p ) , e ( p ) ) with vertex set
v ( p ) : = ( v123 , v123 , v123 , .
, vk ) , and edge set
e ( p ) : = ( ( v123 , v123 ) , ( v123 , v123 ) , .
, ( vk123 , vk ) ) .
we say that the path p joins vertex v123 to vertex vk .
of parti - interest are paths that are subgraphs of a given graph g = ( v , e ) , meaning that v ( p ) v and e ( p ) e .
a cycle is a graph c = ( v ( c ) , e ( c ) ) with v ( c ) = ( v123 , v123 , .
, vk ) with k 123 and edge set e ( c ) = ( ( v123 , v123 ) , ( v123 , v123 ) , .
, ( vk123 , vk ) , ( vk , v123 ) ) .
an undirected graph is acyclic if it contains no cycles .
a graph is bipartite if its vertex set can be partitioned as a disjoint union v = va vb ( where denotes disjoint union ) , such that ( s , t ) e implies that s va and t vb ( or a clique of a graph is a subset s of vertices that are all joined by edges ( i . e . , ( s , t ) e for all s , t s ) .
a chord of a cycle c on the vertex set v ( c ) = ( v123 , v123 , .
, vk ) is an edge ( vi , vj ) that is not part of the cycle edge set e ( c ) = ( ( v123 , v123 ) , ( v123 , v123 ) , .
, ( vk123 , vk ) , ( vk , v123 ) ) .
a cycle c of length four or greater contained within a larger graph g is chordless if the graphs edge set contains no chords for the cycle c .
a graph is triangulated if it contains no cycles of length four or greater that are chordless .
a connected component of a graph is a subset of vertices s v such that for all s , t s , there exists a path contained within the graph g joining s to t .
we say that a graph g is singly connected , or simply connected , if it consists of a single connected component .
a tree is an acyclic graph with a single connected component; it can be shown by induction that any tree on m vertices must have m 123 edges .
more generally , a forest is an acyclic graph consisting of one or more a hypergraph is g = ( v , e ) is a natural generalization of a graph : it consists of a vertex set v = ( 123 , 123 , .
, m ) , and a set e of hyperedges , which each hyperedge h e is a particular subset of v .
( thus , an ordinary graph is the special case in which |h| = 123 for each hyperedge . ) the factor graph associated with a hypergraph g is a bipartite graph ( cid : 123 ) corresponding to the union v e of the hyperedge vertex set v , and the set of hyperedges e , and an edge set
( cid : 123 ) ) with vertex set v
( cid : 123 ) that includes the edge ( s , h ) , where s v and h e , if and only if the hyperedge h includes vertex s .
a . 123 basics of convex sets and functions
a . 123 basics of convex sets and functions
here , we collect some basic denitions and results on convex sets and functions .
rockafellar ( 123 ) is a standard reference on convex analysis; see also the books by hiriart - urruty and lemarechal ( 123 , 123 ) , boyd and vandenberghe ( 123 ) , and bertsekas ( 123 ) .
a . 123 convex sets and cones a set c rd is convex if for all x , y c and ( 123 , 123 ) , the set c also contains the point x + ( 123 ) y .
equivalently , the set c must contain the entire line segment joining any two of its elements .
note that convexity is preserved by various operations on sets :
the intersection jj cj of a family ( cj ) jj of convex sets the cartesian product c123 c123 ck of a family of con - vex sets is convex .
the image f ( c ) of a convex set c under any ane mapping f ( x ) = ax + b is a convex set .
the closure c and interior c of a convex set c are also
note that the union of convex sets is not convex , in general .
a cone k is a set such that for any x k , the ray ( x | > 123 ) also belongs to k .
a convex cone is a cone that is also convex .
( to appreciate the distinction , the union of two distinct rays is a cone , but not convex in general . ) important examples of convex cones
a subspace ( y rd | ay = 123 ) for some matrix a rmd .
the nonnegative orthant
( y rd | y123 123 , y123 123 , .
, yd 123 ) .
123 background material
y rd | y =
the conical hull of a collection of vectors ( x123 , .
, xn ) : ixi with i 123 , i = 123 , .
, n the second - order cone ( ( y , t ) rd r | ( cid : 123 ) y ( cid : 123 ) 123 t ) .
the cone s d + of symmetric positive semidenite matrices + = ( x rdd | x = x t , x ( cid : 123 ) 123 ) .
a . 123 convex and ane hulls
a linear combination of elements x123 , x123 , .
, xk from a set s is a sum i=123 ixi , for arbitrary scalars i r .
an ane combination is a lin - i=123 i = 123 , whereas ear combination with the further restriction that a convex combination is a linear combination with the further restric - i=123 i = 123 and i 123 for all i = 123 , .
the ane hull of a set s , denoted by a ( s ) , is the smallest set that contains all ane com - binations .
similarly , the convex hull of a set s , denoted by conv ( s ) , is the smallest set that contains all its convex combinations .
note that conv ( s ) is a convex set , by denition .
a . 123 ane hulls and relative interior for any > 123 and vector z rd , dene the euclidean ball
y rd | ( cid : 123 ) y z ( cid : 123 ) < the interior of a convex set c rd , denoted by c z c | > 123 s . t .
b ( z ) c
, is given by
the relative interior is dened similarly , except that the interior is taken with respect to the ane hull of c , denoted a ( c ) .
more formally , the relative interior of c , denoted ri ( c ) , is given by
ri ( c ) : = ( z c | > 123 s . t .
b ( z ) a ( c ) c
to illustrate the distinction , note that the interior of the convex set ( 123 , 123 ) , when viewed as a subset of r123 , is empty .
in contrast , the ane
a . 123 basics of convex sets and functions
hull of ( 123 , 123 ) is the real line , so that the relative interior of ( 123 , 123 ) is the open interval ( 123 , 123 ) .
a key property of any convex set c is that its relative interior is always nonempty ( 123 ) .
a convex set c rd is full - dimensional if its ane hull is equal to rd .
for instance , the interval ( 123 , 123 ) , when viewed as a subset of r123 , is not full - dimensional , since its ane hull is only the real line .
for a full - dimensional convex set , the notion of interior and relative interior coincide .
a . 123 polyhedra and their representations
a polyhedron p is a set that can be represented as the intersection of a nite number of half - spaces
p = ( x rd | ( cid : 123 ) aj , x ( cid : 123 ) bj j j ) ,
where for each j j , the pair ( aj , bj ) rd r parameterizes a par - we refer to a bounded polyhedron as a polytope .
given a polyhedron p , we say that x p is an extreme point if it is not possible to write x = y + ( 123 ) z for some ( 123 , 123 ) and y , z p .
we say that x is a vertex if there exists some c rd such that ( cid : 123 ) c , x ( cid : 123 ) > ( cid : 123 ) c , y ( cid : 123 ) for all y p with y ( cid : 123 ) = x .
for a polyhedron , x is a vertex if and only if it is an extreme point .
a consequence of the minkowskiweyl theorem is that any nonempty polytope can be written as the convex hull of its extreme points .
this convex hull representation is dual to the half - space representation .
conversely , the convex hull of any nite collection of vectors is a polytope , and so has a half - space representation ( a . 123 ) .
a . 123 convex functions it is convenient to allow convex functions to take the value + , par - ticularly in performing dual calculations .
more formally , an extended real - valued function f on rd takes values in the extended reals r : = r ( + ) .
a function f : rd r ( + ) is convex if for all x , y rd and ( 123 , 123 ) , we have
f ( x + ( 123 ) y ) f ( x ) + ( 123 ) f ( y ) .
123 background material
the function is strictly convex if the inequality ( a . 123 ) holds strictly for all x ( cid : 123 ) = y and ( 123 , 123 ) .
the domain of a extended real - valued convex function f is the set
dom ( f ) : = ( x rd | f ( x ) < + ) .
note that the domain is always a convex subset of rd .
throughout , we restrict our attention to proper convex functions , meaning that dom ( f )
a key consequence of the denition ( a . 123 ) is jensens inequality , i=123 ixi of points ( xi ) which says that for any convex combination in the domain of f , we have f ticular , given a collection of convex functions ( fj j j ) :
any linear combination jj jfj with i 123 is a convex the pointwise supremum f ( x ) : = supjj fj ( x ) is also a con -
convexity of functions is preserved by various operations .
in par -
the convexity of a function can also be dened in terms of its epigraph , namely the subset of rd r given by
( x , t ) rd r | f ( x ) t
as illustrated in figure a . 123
many properties of convex functions can be stated in terms of properties of this epigraph .
for instance , it is a straightforward exercise to show that the function f is convex if and only if its epigraph is a convex subset of rd r .
a convex function is lower semi - continuous if limyx f ( y ) f ( x ) for all x in its domain .
it can be shown that a convex function f is lower semi - continuous if and only if its epigraph is a closed set; in this case , f is said to be a closed
a . 123 conjugate duality given a convex function f : rd r ( + ) with non - empty domain , its conjugate dual is a new extended real - valued function
a . 123 basics of convex sets and functions
a . 123 the epigraph of a convex function is a subset of rd r , given by epi ( f ) = ( ( x , t ) rd r | f ( x ) t ) .
for a convex function , this set is always convex .
: rd r ( + ) , dened as ( y ) : = sup
( cid : 123 ) ( cid : 123 ) y , x ( cid : 123 ) f ( x ) ( cid : 123 ) ( cid : 123 ) y , x ( cid : 123 ) f ( x )
where the second equality follows by denition of the domain of f .
note is always a convex function , since it is the pointwise supre - mum of a family of convex functions .
geometrically , this operation can be interpreted as computing the intercept of the supporting hyper - plane to epi ( f ) with normal vector ( y , 123 ) rd r , as illustrated in
it is also possible to take the conjugate dual of this dual function
thereby yielding a new function known as the biconjugate
( cid : 123 ) ( cid : 123 ) z , y ( cid : 123 ) f
( z ) = sup
an important property of conjugacy is that whenever f is a closed is equal to the original convex function , then the biconjugate f
for closed , convex , and dierentiable functions f that satisfy in particular , we say
additional technical conditions , the conjugate pair ( f , f a one - to - one correspondence between dom ( f ) and dom ( f on the gradient mappings f and f
123 background material
a . 123 interpretation of the conjugate dual function in terms of supporting hyperplanes to the epigraph .
the negative value of the dual function f ( y ) corresponds to the intercept of the supporting hyperplane to epi ( f ) with normal vector ( y , 123 ) rd r .
is essentially smooth if
it has a that a convex function f nonempty domain c = dom ( f ) , f is dierentiable throughout c limif ( xi ) = + for any sequence ( xi ) contained in c , and con - verging to a boundary point of c .
note that a convex function with domain rd is always essentially smooth , since its domain has no bound - ary .
the property of essential smoothness is also referred to as steepness in some statistical treatments ( 123 ) .
) , it can be shown that ( f , c
a function f is of legendre type if it is strictly convex , dieren - of its domain , and essentially smooth .
letting tiable on the interior c ) is a convex function of d = dom ( f ) is a convex function of legendre legendre type if and only if ( f type .
in this case , the gradient mapping f is one - to - one from c , continuous in both directions , and moreover the open convex set d = ( f ) 123
see section 123 of rockafellar ( 123 ) for further details on these types of legendre correspondences between f and its dual f
proofs and auxiliary results : exponential
families and duality
in this section , we provide the proofs of theorems 123 and 123 from section 123 , as well as some auxiliary results of independent interest .
b . 123 proof of theorem 123
we prove the result rst for a minimal representation , and then dis - cuss its extension to the overcomplete case .
recall from appendix a . 123 that a convex subset of rd is full - dimensional if its ane hull is equal to rd .
we rst note that m is a full - dimensional convex set if and only if the exponential family representation is minimal .
indeed , the repre - sentation is not minimal if and only if there exists some vector a rd and constant b r such that ( cid : 123 ) a , ( x ) ( cid : 123 ) = b holds - a . e .
by denition of m , this equality holds if and only if ( cid : 123 ) a , ( cid : 123 ) = b for all m , which is equivalent to m not being full - dimensional .
thus , if we restrict attention to minimal exponential families for the time - being , the set m is full - dimensional .
our proof makes use of the following properties of a full - dimensional convex set ( see 123 , 123 ) : ( a ) its interior m is nonempty , and the interior of the closure ( m ) is equal to the interior m; and ( b ) the interior m contains the zero
123 proofs for exponential families and duality vector 123 if and only if for all nonzero rd , there exists some m with ( cid : 123 ) , ( cid : 123 ) > 123
we begin by establishing the inclusion a ( ) m .
by shifting the potential by a constant vector if necessary , it suces to consider the case 123 a ( ) .
let 123 be the associated canonical parameter satisfying a ( 123 ) = 123
we prove that for all nonzero directions rd , there is some m such that ( cid : 123 ) , ( cid : 123 ) > 123 , which implies 123 m by property ( b ) .
for any rd , the openness of ensures the existence of some > 123 such that ( 123 + ) .
using the strict convexity and dierentiability of a on and the fact that a ( 123 ) = 123 by assumption ,
a ( 123 + ) > a ( 123 ) + ( cid : 123 ) a ( 123 ) , ( cid : 123 ) = a ( 123 ) .
similarly , dening : = a ( 123 + ) , we can write a ( 123 ) > a ( 123 + ) + ( cid : 123 ) , ( cid : 123 ) .
these two inequalities in conjunction imply that
( cid : 123 ) , ( cid : 123 ) > a ( 123 + ) a ( 123 ) > 123
since a ( ) m and rd was arbitrary , this establishes that next we show that m a ( ) .
as in the preceding argument , we may take 123 m without loss of generality .
then , we must establish the existence of such that a ( ) = 123
by convexity , it is equivalent to show that inf a ( ) is attained .
to establish the attainment of this inmum , we prove that a has no directions of recession , meaning that limn+ a ( n ) = + for all sequences ( n ) such that ( cid : 123 ) n ( cid : 123 ) + .
for an arbitrary nonzero direction rd and > 123 , consider the set h , : = ( x x m | ( cid : 123 ) , ( x ) ( cid : 123 ) ) .
since 123 m , this set must have positive measure under for all suciently small > 123
otherwise , the inequality ( cid : 123 ) , ( x ) ( cid : 123 ) 123 would hold - a . e . , which implies that ( cid : 123 ) , ( cid : 123 ) 123 for all m .
by the convexity of m , this inequality would imply that 123 / ( m ) = m , which contradicts our starting assumption .
a ( 123 + t ) log
for an arbitrary 123 , we now write
( cid : 123 ) ( cid : 123 ) 123 + t , ( x ) ( cid : 123 ) ( cid : 123 ) note that we must have c ( 123 ) > , because
t + log
b . 123 proof of theorem 123
exp ( ( cid : 123 ) 123 , ( x ) ( cid : 123 ) ) > 123
for all x x m ,
and ( h , ) > 123
hence , we conclude that limt+ a ( 123 + t ) = + for all directions rd , showing that a has no directions of recession .
finally , we discuss the extension to the overcomplete case .
for any overcomplete vector of sucient statistics , let be a set of poten - tial functions in an equivalent minimal representation .
in particular , a collection can be specied by eliminating elements of until no ane dependencies remain .
let a and a be the respective mean parameter mappings associated with and , with the sets m and m similarly dened .
by the result just established , a is onto the interior of m .
by construction of , each member in the relative interior of m is associated with a unique element in the interior of m .
we conclude that the mean parameter mapping a is onto the relative interior of m .
b . 123 proof of theorem 123
we begin with proof of part ( a ) , which we divide into three cases .
case ( i ) m : in this case , theorem 123 guarantees that the inverse image ( a ) 123 ( ) is nonempty .
any point in this inverse image attains the supremum in equation ( 123 ) .
in a minimal rep - resentation , there is only one optimizing point , whereas there is an ane subset for an overcomplete representation .
nonetheless , for any ( ) ( a ) 123 ( ) , the value of the optimum is given by
( ) = ( cid : 123 ) ( ) , ( cid : 123 ) a ( ( ) ) .
123 proofs for exponential families and duality
by denition ( 123 ) of the entropy , we have
h ( p ( ) ) = e ( ) ( ( cid : 123 ) ( ) , ( x ) ( cid : 123 ) a ( ( ) ) )
= ( cid : 123 ) ( ) , ( cid : 123 ) a ( ( ) ) ,
= ( rd | a
case ( ii ) / m : let dom a
where the nal equality uses linearity of expectation , and the fact that e ( ) ( ( x ) ) = .
( ) < + ) denote the .
with this notation , we must prove that eective domain of a m dom a .
in order to do so , we use corollary 123 . 123 of rockafel - lar ( 123 ) which asserts that if a is essentially smooth and lower semi - ) a ( ) dom a continuous , then we have the inclusions ( dom a
m dom a
since a ( ) = m from theorem 123 .
since both m and dom a
are convex sets , taking closures in these inclusions yields that dom a = ( m ) = m , where the second equality follows by the convex - ( ) = + ity of m .
therefore , by denition of the eective domain , a for any / m .
it remains to verify that a is both lower semi - continuous , and essentially smooth ( see appendix a . 123 for the denition ) .
recall from proposition 123 that a is dierentiable; hence , it is lower semi - continuous on its domain .
to establish that a is essentially smooth , let b be a boundary point , and let 123 be arbitrary .
since the set is convex and open , the line t : = tb + ( 123 t ) 123 is contained in for all t ( 123 , 123 ) ( see theorem 123 of rockafellar ( 123 ) ) .
using the dieren - tiability of a on and its convexity ( see proposition 123 ( b ) ) , for any t < 123 , we can write
a ( 123 ) a ( t ) + ( cid : 123 ) a ( t ) , 123 t ( cid : 123 ) .
re - arranging and applying the cauchyschwartz inequality to the inner product term yields that
a ( t ) a ( 123 ) ( cid : 123 ) t 123 ( cid : 123 ) ( cid : 123 ) a ( t ) ( cid : 123 ) .
now as t 123 , the left - hand side tends to innity by the lower semi - continuity of a , and the regularity of the exponential family .
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) a ( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) a
a ( ) = sup
b . 123 general properties of m and a
case ( iii ) m\m : since a
consequently , the right - hand side must also tend to innity; since ( cid : 123 ) t 123 ( cid : 123 ) is bounded , we conclude that ( cid : 123 ) a ( t ) ( cid : 123 ) + , which shows that a is essentially smooth .
is dened as a conjugate function , ( ) for any it is lower semi - continuous .
therefore , the value of a boundary point m\m is determined by the limit over a sequence approaching from inside m , as claimed .
we now turn to the proof of part ( b ) .
from proposition 123 , a is ) = a , and
convex and lower semi - continuous , which ensures that ( a part ( a ) shows that dom a = m .
consequently , we can write
since it is inconsequential whether we take the supremum over m or its closure m .
last , we prove part ( c ) of the theorem .
for a minimal representation , proposition 123 and theorem 123 guarantee that the gradient mapping a is a bijection between and m .
on this basis , it follows that the gradient mapping a also exists and is bijective ( 123 ) , whence the supremum ( b . 123 ) is attained at a unique point whenever .
the analogous statement for an overcomplete representation can be proved via reduction to a minimal representation .
b . 123 general properties of m and a in this section , we state and prove some auxiliary results of indepen - , and the dent interest on general properties of the dual function a associated set m of valid mean parameters .
b . 123 properties of m from its denition , it is clear that m is always a convex set .
other more specic properties of m turn out to be determined by the properties of the exponential family .
recall from appendix a . 123 that a convex set
123 proofs for exponential families and duality m rd is full - dimensional if its ane hull is equal to rd .
with this notion , we have the following : proposition b . 123 the set m has the following properties :
( a ) m is full - dimensional if and only if the exponential family is ( b ) m is bounded if and only if = rd and a is globally lips -
chitz on rd .
( a ) the representation is not minimal if and only if there exists some vector a rd and constant b r such that ( cid : 123 ) a , ( x ) ( cid : 123 ) = b holds - a . e .
by denition of m , this equality holds if and only if ( cid : 123 ) a , ( cid : 123 ) = b for all m , which is equivalent to m not being full - dimensional .
( b ) the recession function a is the support function supm ( cid : 123 ) , ( cid : 123 ) .
therefore , the set m is bounded if and only if a ( ) is nite for all rd .
the recession function a is nite - valued if and only if a is lipschitz and hence nite on all of rd ( see proposition 123 . 123 in hiriart - urruty and lemarechal ( 123 ) ) .
the necessity of the condition = rd for m to be bounded is clear from the boundary behavior of a given in proposition 123 .
however , the additional global lipschitz condition is also necessary , as demon - strated by the poisson family ( see table 123 ) .
in this case , we have = r yet the set of mean parameters m = ( 123 , + ) is unbounded .
this unboundedness occurs because the function a ( ) = exp ( ) , while nite on r , is not globally lipschitz .
b . 123 properties of a is not given in closed form , a number of proper - despite the fact that a ties can be inferred from its variational denition ( 123 ) .
for instance , is always convex .
more specic an immediate consequence is that a depend on the nature of the exponential family .
recall properties of a from appendix a . 123 the denition of an essentially smooth convex
b . 123 proof of theorem 123 ( b )
proposition b . 123 the dual function a
is always convex and lower in a minimal and regular exponential
is dierentiable on m , and a is strictly convex and essentially smooth .
( ) = ( a ) 123 ( ) .
the convexity and lower semi - continuity follow because a the supremum of collection of functions linear in .
since both a and has these properties if and are lower semi - continuous , the dual a only if a is strictly convex and essentially smooth ( see our discussion of legendre duality in appendix a . 123 , and theorem 123 in rockafel - lar ( 123 ) ) .
for a minimal representation , a is strictly convex by propo - sition 123 , and from the proof of theorem 123 ( a ) , it is also essentially smooth , so that the stated result follows .
this result is analogous to proposition 123 , in that the conditions stated ensure the essential smoothness of the dual function a minimal representation .
the boundary behavior of a can be veri - ed explicitly for the examples shown in table 123 , for which we have .
for instance , in the bernoulli case , closed form expressions for a ( ) | = |log ( ( 123 ) / ) | , which tends to we have m = ( 123 , 123 ) and |a innity as 123+ or 123
similarly , in the poisson case , we have ( ) | = |log | , which tends to innity as tends m = ( 123 , + ) and |a to the boundary point 123
b . 123 proof of theorem 123 ( b )
for any mrf dened by a tree t , both components of the bethe vari - ational problem ( 123 ) are exact :
( a ) by proposition 123 , the set l ( t ) is equivalent to the marginal
( b ) the bethe entropy hbethe is equivalent to the negative dual
polytope m ( t ) , and
123 proofs for exponential families and duality
consequently , for the tree - structured problem , the bethe variational principle is equivalent to a special case of the conjugate duality between from theorem 123 .
therefore , the value of the optimized a and a bethe problem is equal to the cumulant function value a ( ) , as claimed .
finally , theorem 123 ( c ) implies that the optimum the exact marginal distributions of the tree - structured mrf .
strict convexity implies that the solution is unique .
b . 123 proof of theorem 123 we begin by proving assertion ( 123a ) .
let p be the space of all densities p , taken with respect to the base measure associated with the given exponential family .
on the one hand , for any p p , we have
( cid : 123 ) ( cid : 123 ) , ( x ) ( cid : 123 ) p ( x ) ( dx ) maxxx m ( cid : 123 ) , ( x ) ( cid : 123 ) , whence
( cid : 123 ) , ( x ) ( cid : 123 ) p ( x ) ( dx ) max
( cid : 123 ) ( cid : 123 ) , ( x ) ( cid : 123 ) p ( x ) ( dx ) = supm ( cid : 123 ) , ( cid : 123 ) , which establishes the
since the support of is x m , equality is achieved in the inequality ( b . 123 ) by taking a sequence of densities ( pn ) con - arg maxx ( cid : 123 ) , ( x ) ( cid : 123 ) .
verging to a delta function x ( x ) , where x finally , by linearity of expectation and the denition of m , we we now turn to the claim ( 123b ) .
by proposition 123 , the func - tion a is lower semi - continuous .
therefore , for all , the quantity lim+ a ( ) / is equivalent to the recession function of a , which we denote by a ( see corollary 123 . 123 of rockafellar ( 123 ) ) .
hence , it suces to establish that a ( ) is equal to supm ( cid : 123 ) , ( cid : 123 ) .
using the lower semi - continuity of a and theorem 123 of rockafellar ( 123 ) , the recession function of a corresponds to the support function of the eec - tive domain of its dual .
by theorem 123 , we have dom a = m , whence a ( ) = sup m ( cid : 123 ) , ( cid : 123 ) .
finally , the supremum is not aected by taking
variational principles for multivariate gaussians
in this appendix , we describe how the general variational representa - tion ( 123 ) specializes to the multivariate gaussian case .
for a gaus - sian markov random eld , as described in example 123 , computing the mapping ( , ) ( cid : 123 ) ( , ) amounts to computing the mean vector rd , and the covariance matrix t .
these computations are typically carried out through the so - called normal equations ( 123 ) .
in this section , we derive such normal equations for gaussian inference from the principle ( 123 ) .
c . 123 gaussian with known covariance
we rst consider the case of x = ( x123 , .
, xm ) with unknown mean rm , but known covariance matrix , which we assume to be strictly positive denite ( hence invertible ) .
under this assumption , we may alternatively use the precision matrix p , corresponding to the inverse covariance 123
this collection of models can be written as an m - dimensional exponential family with respect to the base measure
( 123 ) m|p 123| dx ,
123 xt p x
123 variational principles for multivariate gaussians
with densities of the form p ( x ) = exp .
with this notation , the well - known normal equations for computing the gaussian mean vector = e ( x ) are given by = p
let us now see how these normal equations are a consequence of the variational principle ( 123 ) .
we rst compute the cumulant function for the specied exponential family as follows :
i=123 ixi a ( )
( 123 ) m|p 123| dx
123 xt p x
a ( ) = log
123 t p 123 t ,
where have completed the square in order to compute the integral . 123 consequently , we have a ( ) < + for all rm , meaning that = rm .
moreover , the mean parameter = e ( x ) takes values in all of rm , so that m = rm .
next , a straightforward calculation yields that the ( ) = 123 123 t p .
consequently , the conjugate dual of a is given by a variational principle ( 123 ) takes the form :
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) 123
a ( ) = sup
which is an unconstrained quadratic program .
taking derivatives to nd the optimum yields that ( ) = p
thus , we have shown that the normal equations for gaussian infer - ence are a consequence of the variational principle ( 123 ) specialized to this particular exponential family .
moreover , by the hammersley cliord theorem ( 123 , 123 , 123 ) , the precision matrix p has the same sparsity pattern as the graph adjacency matrix ( i . e . , for all i ( cid : 123 ) = j , pij ( cid : 123 ) = 123 implies that ( i , j ) e ) .
consequently , the matrix - inverse - vector 123 can often be solved very quickly by specialized meth - ods that exploit the graph structure of p ( see , for instance , golub and van loan ( 123 ) ) .
perhaps the best - known example is the case of a tri - diagonal matrix p , in which case the original gaussian vector x is markov with respect to a chain - structured graph , with edges ( i , i + 123 ) 123 note that since ( 123 tion 123 ( see equation ( 123b ) ) .
a ( ) ) ij = cov ( xi , xj ) , this calculation is consistent with proposi -
c . 123 gaussian with unknown covariance
for i = 123 , .
, m 123
more generally , the kalman lter on trees ( 123 ) can be viewed as a fast algorithm for solving the matrix - inverse - vector sys - tem of normal equations .
for graphs without cycles , there are also local iterative methods for solving the normal equations .
in particular , as we discuss in example 123 in section 123 , gaussian mean eld theory leads to the gaussjacobi updates for solving the normal equations .
c . 123 gaussian with unknown covariance
we now turn to the more general case of a gaussian with unknown covariance ( see example 123 ) .
its mean parameterization is specied by the vector = e ( x ) rm and the second - order moment matrix = e ( xx t ) rmm .
the set m of valid mean parameters is com - pletely characterized by the positive semideniteness ( psd ) constraint t ( cid : 123 ) 123 ( see example 123 ) .
turning to the form of the dual func - tion , given a set of valid mean parameters ( , ) m , we can associate them with a gaussian random vector x with mean rm and covari - ance matrix t " 123
the entropy of such a multivariate gaussian takes the form ( 123 ) : h ( x ) = a
by the schur complement formula ( 123 ) , we may rewrite the negative dual function as
since the log - determinant function is strictly concave on the cone of positive semidenite matrices ( 123 ) , this representation demonstrates the convexity of a
in an explicit way .
these two ingredients allow us to write the variational princi - ple ( 123 ) specialized to the multivariate gaussian with unknown covariance as follows : the value of cumulant function a ( , ) is equal to
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) +
( cid : 123 ) , ( cid : 123 ) +
123 variational principles for multivariate gaussians
this problem is an instance of a well - known class of concave log - determinant problem ( 123 ) .
lems can be solved eciently using standard methods for convex programming , such as interior point methods .
however , this specic log - determinant problem ( c . 123 ) actually has a closed - form solution : in particular , it can be veried by a lagrangian reformulation that the
, ) is specied by the relations optimal pair (
note that these relations , which emerge as the optimality con - ditions for the log - determinant problem ( c . 123 ) , are actually familiar statements .
recall from our exponential representation of the gaussian density ( 123 ) that is the precision matrix .
consequently , the rst equality in equation ( c . 123 ) conrms that the covariance matrix is the inverse of the precision matrix , whereas the second equality corresponds to the normal equations for the mean of a gaussian .
thus , as a spe - cial case of the general variational principle ( 123 ) , we have re - derived the familiar equations for gaussian inference .
) t = ( )
c . 123 gaussian mode computation
in section c . 123 , we showed that for a multivariate gaussian with unknown covariance , the general variational principle ( 123 ) corre - sponds to a log - determinant optimization problem .
in this section , we show that the mode - nding problem for a gaussian is a semidenite program ( sdp ) , another well - known class of convex optimization prob - lems ( 123 , 123 ) .
consistent with our development in section 123 , this sdp arises as the zero - temperature limit of the log - determinant prob - lem .
recall that for the multivariate gaussian , the mean parameters are the mean vector = e ( x ) and second - moment matrix = e ( xx t ) , and the set m of valid mean parameters is characterized by the semidef - inite constraint t ( cid : 123 ) 123 , or equivalently ( via the schur comple - ment formula ( 123 ) ) by a linear matrix inequality
t ( cid : 123 ) 123
consequently , by applying theorem 123 , we conclude that
c . 123 gaussian mode computation
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) +
( cid : 123 ) ( cid : 123 ) , x ( cid : 123 ) +
the problem on the left - hand side is simply a quadratic program , with = ( ) 123 of the gaus - optimal solution corresponding to the mode x sian density .
the problem on the right - hand side is a semidenite pro - gram ( 123 ) , as it involves an objective function that is linear in the matrix - vector pair ( , ) , with a constraint dened by the linear matrix
in order to demonstrate how the optimum of this semidenite , we begin with the program ( sdp ) recovers the gaussian mode x fact ( 123 ) that a matrix b is positive semidenite if and only if ( cid : 123 ) ( cid : 123 ) b , c ( cid : 123 ) ( cid : 123 ) : = trace ( bc ) 123 for all other positive semidenite matrices c .
( this property corresponds to the self - duality of the cone s m + . ) apply - ing this fact to the matrices b = t ( cid : 123 ) 123 and c = " 123 yields the inequality ( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , t ( cid : 123 ) ( cid : 123 ) , which in turn implies that
( cid : 123 ) , ( cid : 123 ) +
( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) +
observe that the right - hand side is a quadratic program in the vector = ( ) 123
consequently , if rm , with its maximum attained at we maximize the left - hand side over the set t ( cid : 123 ) 123 , the bound ) t .
the interpretation is the will be achieved at optimal solution corresponds to a degenerate gaussian , specied by , ) , with a zero covariance matrix , so that all
mean parameters ( mass concentrates on its mean
it is not necessary to solve the semidenite pro - gram ( c . 123 ) in order to compute the mode of a gaussian problem .
for multivariate gaussians , the mode ( or mean ) can be computed by more direct methods , including kalman ltering ( 123 ) , corresponding to the sum - product algorithm on trees , by numerical methods such as conjugate gradient ( 123 ) , by the max - product / min - sum algorithm ( see section 123 ) , both of which solve the quadratic program in an iterative manner , or by iterative algorithms based on tractable sub - graphs ( 123 , 123 ) .
however , the sdp - based formulation ( c . 123 ) provides valuable perspective on the use of semidenite relaxations for integer programming problems , as discussed in section 123
clustering and augmented hypergraphs
in this appendix , we elaborate on various techniques use to transform hypergraphs so as to apply the kikuchi approximation and related clus - ter variational methods .
the most natural strategy is to construct vari - ational relaxations directly using the original hypergraph .
the bethe approximation is of this form , corresponding to the case when the origi - nal structure is an ordinary graph .
alternatively , it can be benecial to develop approximations based on an augmented hypergraph g = ( v , e ) built on top of the original hypergraph .
a natural way to dene new hypergraphs is by dening new hyperedges over clusters of the original vertices; a variety of such clustering methods have been discussed in the literature ( e . g . , 123 , 123 , 123 , 123 ) .
d . 123 covering augmented hypergraphs
( cid : 123 ) = ( v , e
( cid : 123 ) ) denote the original hypergraph , on which the undi - rected graphical model or markov random eld is dened .
we consider an augmented hypergraph g = ( v , e ) , dened over the same set of ver - tices but with a hyperedge set e that includes all the original hyper - ( cid : 123 ) is covered by the augmented hypergraph g .
a desirable feature of this requirement is
( cid : 123 ) .
we say that the original hypergraph g
d . 123 covering augmented hypergraphs
( cid : 123 ) can also be viewed that any markov random eld ( mrf ) dened by g as an mrf on a covering hypergraph g , simply by setting h = 123 for all h e\e
example d . 123 ( covering hypergraph ) .
to illustrate , suppose that ( cid : 123 ) is simply an ordinary graph namely , the original hypergraph g the 123 123 grid shown in figure d . 123 ( a ) .
as illustrated in panel ( b ) , we cluster the nodes into groups of four , which is known as kikuchi 123 - plaque clustering in statistical physics ( 123 , 123 ) .
we then form the augmented hypergraph g shown in panel ( c ) , with hyperedge set e : = v e .
the dark - ness of the boxes in this diagram reects the depth of the hyperedges in the poset diagram .
this hypergraph covers the original ( hyper ) graph , since it includes as hyperedges all edges and vertices of the original 123 123 grid .
as emphasized by yedidia et al .
( 123 ) , it turns out to be impor - tant to ensure that every hyperedge ( including vertices ) in the orig - ( cid : 123 ) is counted exactly once in the augmented hyper - inal hypergraph g graph g .
more specically , for a given hyperedge h e , consider the set a+ ( h ) : = ( f e | f h ) of hyperedges in e that contain h .
let us recall the denition ( 123 ) of the overcounting numbers associated with the hypergraph g .
in particular , these overcounting numbers are dened in terms of the mobius function associated with g , viewed as a poset , in the following way :
the single counting criterion requires that for all vertices or hyper - ( cid : 123 ) v there holds
edges of the original graph that is , for all h
c ( f ) = 123
in words , the sum of the overcounting numbers over all hyperedges f that contain h
( cid : 123 ) must be equal to one .
123 clustering and augmented hypergraphs
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
( cid : 123 ) is a 123 123 grid .
its hyperedge set e
d . 123 constructing new hypergraphs via clustering and the single counting criterion .
( cid : 123 ) consists of the union of ( a ) original ( hyper ) graph g the vertex set with the ( ordinary ) edge set .
( b ) nodes are clustered into groups of four .
( c ) a covering hypergraph g formed by adjoining these 123 - clusters to the original hyperedge ( cid : 123 ) .
darkness of the boxes indicates depth of the hyperedges in the poset representation .
( d ) an augmented hypergraph constructed by the kikuchi method .
( e ) a third augmented hypergraph that fails the single counting criterion for ( 123 ) .
example d . 123 ( single counting ) .
to illustrate the single counting criterion , we consider two additional hypergraphs that can be con - structed from the 123 123 grid of figure d . 123 ( a ) .
the vertex set and edge ( cid : 123 ) .
the hypergraph set of the grid form the original hyperedge set e
d . 123 covering augmented hypergraphs
in panel ( d ) is constructed by the kikuchi method described by yedidia et al .
( 123 , 123 ) .
in this construction , we include the four clusters , all of their pairwise intersections , and all the intersections of intersections ( only ( 123 ) in this case ) .
the hypergraph in panel ( e ) includes only the hyperedges of size four and two; that is , it omits the let us focus rst on the hypergraph ( e ) , and understand why it violates the single counting criterion for hyperedge ( 123 ) .
viewed as a poset , all of the maximal hyperedges ( of size four ) in this hypergraph have a counting number of c ( h ) = ( h , h ) = 123
any hyperedge f of size two has two parents , each with an overcounting number of 123 , so that c ( f ) = 123 ( 123 + 123 ) = 123
the hyperedge ( 123 ) is a member of the ( cid : 123 ) ( of the 123 123 grid ) , but not of the augmented original hyperedge set e hypergraph .
it is included in all the hyperedges , so that a+ ( ( 123 ) ) = e ha+ ( ( 123 ) ) c ( h ) = 123
thus , the single criterion condition fails to hold for hypergraph ( e ) .
in contrast , it can be veried that for the hypergraphs in panels ( c ) and ( d ) , the single counting condition holds for all hyperedges h
there is another interesting fact about hypergraphs ( c ) and ( d ) .
if we eliminate from hypergraph ( c ) all hyperedges that have zero overcounting numbers , the result is hypergraph ( d ) .
to understand this reduction , consider for instance the hyperedge ( 123 , 123 ) which appears in ( c ) but not in ( d ) .
since it has only one parent that is a maximal hyperedge , we have c ( ( 123 , 123 ) ) = 123
in a similar fash - ion , we see that c ( ( 123 , 123 ) ) = 123
these two equalities together imply that c ( ( 123 ) ) = 123 , so that we can eliminate hyperedges ( 123 , 123 ) , ( 123 , 123 ) , and ( 123 ) from hypergraph ( c ) .
by applying a similar argument to the remaining hyperedges , we can fully reduce hypergraph ( c ) to
it turns out that if the augmented hypergraph g covers the original ( cid : 123 ) , then the single counting criterion is always satised .
implicit in this denition of covering is that the hyperedge set e the original hypergraph includes the vertex set , so that equation ( d . 123 ) should hold for the vertices .
the proof is quite straightforward .
123 clustering and augmented hypergraphs
lemma d . 123 ( single counting ) .
for any h e , the associated over - ea+ ( h ) c ( e ) = 123 , which can be counting numbers satisfy the identity
written equivalently as c ( h ) = 123 ( cid : 123 )
from the denition of c ( h ) , we have the identity :
considering the double sum on the right - hand side , we see that for a xed d a+ ( g ) , there is a term ( d , e ) for each e such that g e d .
using this observation , we can write
( h , f ) =
here equality ( a ) follows from the denition of the mobius function ( see appendix e . 123 ) , and ( d , g ) is the kronecker delta function , from which equality ( b ) follows .
thus , the construction that we have described , in which the hyper - ( cid : 123 ) are covered edges ( including all vertices ) of the original hypergraph g by g and the partial ordering is set inclusion , ensures that the sin - gle counting criterion is always satised .
we emphasize that there is a broad range of other possible constructions ( e . g . , 123 , 123 , 123 , 123 ) .
d . 123 specication of compatibility functions
our nal task is to specify how to assign the compatibility func - ( cid : 123 ) ) with the tions associated with the original hypergraph g hyperedges of the augmented hypergraph g = ( v , e ) .
it is convenient g ( xg ) : = exp ( g ( xg ) ) for the compatibility func - to use the notation tions of the original hypergraph , corresponding to terms in the prod - uct ( 123 ) .
we can extend this denition to all hyperedges in e by
( cid : 123 ) = ( v , e
h ( xh ) 123 for any hyperedge h e\e
h e , we then dene a new compatibility function h as follows :
d . 123 specication of compatibility functions
( cid : 123 ) .
for each hyperedge
( cid : 123 ) \e | g h ) is the set of hyperedges in e
where s ( h ) : = ( g e are subsets of h .
to illustrate this denition , consider the kikuchi con - struction of figure d . 123 ( d ) , which is an augmented hypergraph for the 123 123 grid in figure d . 123 ( a ) .
for the hyperedge ( 123 ) , we have s ( 123 ) = ( ( 123 ) ) , so that 123 = 123
on the other hand , for the hyper - 123 123 ( since ( 123 ) appears in e but not edge ( 123 ) , we have ( cid : 123 ) ) , and s ( 123 ) = ( ( 123 ) , ( 123 ) , ( 123 ) ) .
accordingly , equation ( d . 123 ) yields 123 = 123
more generally , using the denition ( d . 123 ) , it is straightforward to verify that the equivalence
holds , so that we have preserved the structure of the original mrf .
this appendix collects together various miscellaneous results on graphi - cal models and posets .
e . 123 mobius inversion
this appendix provides brief overview of the mobius function associ - ated with a partially ordered set .
we refer the reader to chapter 123 of stanley ( 123 ) for a thorough treatment .
a partially ordered set or poset p consists of a set along with a binary relation on elements of the set , which we denote by , satisfying reexivity ( g g for all g p ) , anti - symmetry ( g h and h g implies g = h ) , and transitivity ( f g and g h implies f g ) .
although posets can be dened more generally , for our purposes it suces to restrict attention to nite posets .
the zeta function of a poset is a mapping : p p r dened
( g , h ) =
the mobius function : p p r arises as the multiplicative inverse of this zeta function .
it can be dened in a recursive fashion , by rst
if g h ,
e . 123 auxiliary result for proposition 123
specifying ( g , g ) = 123 for all g p , and ( g , h ) = 123 for all h ( cid : 123 ) g .
once ( g , f ) has been dened for all f such that g f h , we then dene :
( g , h ) =
( f | gfh )
with this denition , it can be seen that and are multiplicative inverses , in the sense that
( g , f ) ( f , h ) =
( g , f ) = ( g , h ) ,
( f | gfh )
where ( g , h ) is the kronecker delta .
an especially simple example of a poset is the set p of all subsets of ( 123 , .
, m ) , using set inclusion as the partial order .
in this case , a straightforward calculation shows that the mobius function is given by ( g , h ) = ( 123 ) |h\g|i ( g h ) .
the inverse formula ( e . 123 ) then corresponds to the assertion
= ( g , h ) .
( f | gfh )
we now state the mobius inversion formula for functions dened
on a poset :
lemma e . 123 ( mobius inversion formula ) .
given functions real - valued functions and dened on a poset p , then for all h p
if and only if
( g ) ( g , h )
for all h p .
e . 123 auxiliary result for proposition 123 in this appendix , we prove that the matrix b ( , ) = ( 123 ) |\|i ( ) diagonalizes the matrix nm ( ) , for any vector r123m .
for any pair
123 miscellaneous results
, , we calculate
, b ( , ) =
suppose that the event ( ) does not hold; then there is some ele - ment i \ .
consequently .
we can write
otherwise , if , then we have
therefore , we have
b ( , ) = ( )
where the nal inequality uses equation ( e . 123 ) .
thus , we have shown that the matrix bnm ( ) bt is diagonal with entries ( ) .
if nm ( ) ( cid : 123 ) 123 , then by the invertibility of b , we are guaranteed that ( ) 123
it ( ) = 123
recall that = 123 by denition .
remains to show that from the denition ( e . 123 ) , we have
where we have again used equation ( e . 123 ) in moving from the third to
e . 123 conversion to a pairwise markov random field
e . 123 pairwise markov random fields
in this appendix , we describe how any markov random eld with dis - crete random variables can be converted to an equivalent pairwise form ( i . e . , with interactions only between pairs of variables ) .
to illustrate the general principle , it suces to show how to convert a compatibility function 123 dened on a triplet ( x123 , x123 , x123 ) of random variables into a pairwise form .
to do so , we introduce an auxiliary node a , and associate with it random variable z that takes values in the cartesian product space x123 x123 x123
in this way , each conguration of z can be identi - ed with a triplet ( z123 , z123 , z123 ) .
for each s ( 123 , 123 , 123 ) , we dene a pairwise compatibility function as , corresponding to the interaction between z and xs , by as ( z , xs ) : = ( 123 ( z123 , z123 , z123 ) ) 123 / 123i ( zs = xs ) .
( the purpose of the 123 / 123 power is to incorporate 123 with the correct exponent . ) with this denition , it is straightforward to verify that the equivalence
123 ( x123 , x123 , x123 ) =
holds , so that our augmented model faithfully captures the interaction dened on the triplet ( x123 , x123 , x123 ) .
