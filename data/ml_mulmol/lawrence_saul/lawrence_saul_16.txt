we study the problem of parameter estimation in continuous density hidden markov models ( cd - hmms ) for automatic speech recognition ( asr ) .
as in sup - port vector machines , we propose a learning algorithm based on the goal of mar - gin maximization .
unlike earlier work on max - margin markov networks , our approach is specically geared to the modeling of real - valued observations ( such as acoustic feature vectors ) using gaussian mixture models .
unlike previous dis - criminative frameworks for asr , such as maximum mutual information and min - imum classication error , our framework leads to a convex optimization , without any spurious local minima .
the objective function for large margin training of cd - hmms is dened over a parameter space of positive semidenite matrices .
its optimization can be performed efciently with simple gradient - based meth - ods that scale well to large problems .
we obtain competitive results for phonetic recognition on the timit speech corpus .
as a result of many years of widespread use , continuous density hidden markov models ( cd - hmms ) are very well matched to current front and back ends for automatic speech recognition ( asr ) ( 123 ) .
typical front ends compute real - valued feature vectors from the short - time power spec - tra of speech signals .
the distributions of these acoustic feature vectors are modeled by gaussian mixture models ( gmms ) , which in turn appear as observation models in cd - hmms .
viterbi de - coding is used to solve the problem of sequential classication in asrnamely , the mapping of sequences of acoustic feature vectors to sequences of phonemes and / or words , which are modeled by state transitions in cd - hmms .
the simplest method for parameter estimation in cd - hmms is the expectation - maximization ( em ) algorithm .
the em algorithm is based on maximizing the joint likelihood of observed feature vectors and label sequences .
it is widely used due to its simplicity and scalability to large data sets , which are common in asr .
a weakness of this approach , however , is that the model parameters of cd - hmms are not optimized for sequential classication : in general , maximizing the joint likelihood does not minimize the phoneme or word error rates , which are more relevant metrics for asr .
noting this weakness , many researchers in asr have studied alternative frameworks for parame - ter estimation based on conditional maximum likelihood ( 123 ) , minimum classication error ( 123 ) and maximum mutual information ( 123 ) .
the learning algorithms in these frameworks optimize discrim - inative criteria that more closely track actual error rates , as opposed to the em algorithm for maxi - mum likelihood estimation .
these algorithms do not enjoy the simple update rules and relatively fast convergence of em , but carefully and skillfully implemented , they lead to lower error rates ( 123 , 123 ) .
recently , in a new approach to discriminative acoustic modeling , we proposed the use of large margin gmms for multiway classication ( 123 ) .
inspired by support vector machines ( svms ) , the learning algorithm in large margin gmms is designed to maximize the distance between labeled ex - amples and the decision boundaries that separate different classes ( 123 ) .
under mild assumptions , the required optimization is convex , without any spurious local minima .
in contrast to svms , however , large margin gmms are very naturally suited to problems in multiway ( as opposed to binary ) clas - sication; also , they do not require the kernel trick for nonlinear decision boundaries .
we showed how to train large margin gmms as segment - based phonetic classiers , yielding signicantly lower error rates than maximum likelihood gmms ( 123 ) .
the integrated large margin training of gmms and transition probabilities in cd - hmms , however , was left as an open problem .
we address that problem in this paper , showing how to train large margin cd - hmms in the more general setting of sequential ( as opposed to multiway ) classication .
in this setting , the gmms appear as acoustic models whose likelihoods are integrated over time by viterbi decoding .
experi - mentally , we nd that large margin training of hmms for sequential classication leads to signicant improvement beyond the frame - based and segment - based discriminative training in ( 123 ) .
our framework for large margin training of cd - hmms builds on ideas from many previous studies in machine learning and asr .
it has similar motivation as recent frameworks for sequential classi - cation in the machine learning community ( 123 , 123 , 123 ) , but differs in its focus on the real - valued acous - tic feature representations used in asr .
it has similar motivation as other discriminative paradigms in asr ( 123 , 123 , 123 , 123 , 123 , 123 ) , but differs in its goal of margin maximization and its formulation of the learning problem as a convex optimization over positive semidenite matrices .
the recent margin - based approach of ( 123 ) is closest in terms of its goals , but entirely different in its mechanics; moreover , its learning is limited to the mean parameters in gmms .
123 large margin gmms for multiway classication
before developing large margin hmms for asr , we briey review large margin gmms for multi - way classication ( 123 ) .
the problem of multiway classication is to map inputs x d to labels y ( 123 , 123 , .
, c ) , where c is the number of classes .
large margin gmms are trained from a set of labeled examples ( ( xn , yn ) ) n n=123
they have many parallels to svms , including the goal of margin maximization and the use of a convex surrogate to the zero - one loss ( 123 ) .
unlike svms , where classes are modeled by half - spaces , in large margin gmms the classes are modeled by collections of ellipsoids .
for this reason , they are more naturally suited to problems in multiway as opposed to binary classication .
sections 123 . 123 review the basic framework for large margin gmms : rst , the simplest setting in which each class is modeled by a single ellipsoid; second , the formulation of the learning problem as a convex optimization; third , the general setting in which each class is modeled by two or more ellipsoids .
section 123 presents results on handwritten digit recognition .
123 parameterization of the decision rule
the simplest large margin gmms model each class by a single ellipsoid in the input space .
the ellipsoid for class c is parameterized by a centroid vector c d and a positive semidenite matrix c dd that determines its orientation .
also associated with each class is a nonnegative scalar offset c 123
the decision rule labels an example x d by the class whose centroid yields the smallest mahalanobis distance :
y = argmin
( cid : 123 ) ( xc ) tc ( xc ) + c ( cid : 123 ) .
the decision rule in eq .
( 123 ) is merely an alternative way of parameterizing the maximum a posterior ( map ) label in traditional gmms with mean vectors c , covariance matrices 123 c , and prior class probabilities pc , given by y = argminc ( pc n ( c , 123 the argument on the right hand side of the decision rule in eq .
( 123 ) is nonlinear in the ellipsoid parameters c and c .
as shown in ( 123 ) , however , a useful reparameterization yields a simpler expression .
for each class c , the reparameterization collects the parameters ( c , c , c ) in a single enlarged matrix c ( d+123 ) ( d+123 ) :
c ) ) .
c = ( cid : 123 ) c
cc + c ( cid : 123 ) .
note that c is positive semidenite .
furthermore , if c is strictly positive denite , the parameters ( c , c , c ) can be uniquely recovered from c .
with this reparameterization , the decision rule in eq .
( 123 ) simplies to :
y = argmin
( cid : 123 ) ztc z ( cid : 123 ) where
z = ( cid : 123 ) x
the argument on the right hand side of the decision rule in eq .
( 123 ) is linear in the parameters c .
in what follows , we will adopt the representation in eq .
( 123 ) , implicitly constructing the augmented vector z for each input vector x .
note that eq .
( 123 ) still yields nonlinear ( piecewise quadratic ) deci - sion boundaries in the vector z .
123 margin maximization
analogous to learning in svms , we nd the parameters ( c ) that minimize the empirical risk on the training datai . e . , parameters that not only classify the training data correctly , but also place the decision boundaries as far away as possible .
the margin of a labeled example is dened as its distance to the nearest decision boundary .
if possible , each labeled example is constrained to lie at least one unit distance away from the decision boundary to each competing class :
c 123= yn , zt
n ( c yn ) zn 123
123 illustrates this idea .
note that in the realizable setting where these constraints can be simultaneously satised , they do not uniquely determine the parameters ( c ) , which can be scaled to yield arbitrarily large margins .
therefore , as in svms , we propose a convex optimization that selects the smallest parameters that satisfy the large margin constraints in eq .
in this case , the optimization is an instance of semidenite programming ( 123 ) :
min pc trace ( c )
123 + zt
n ( yn c ) zn 123 , c = 123 , 123 ,
c 123= yn , n = 123 , 123 ,
note that the trace of the matrix c appears in the above objective function , as opposed to the trace of the matrix c , as dened in eq .
( 123 ) ; minimizing the former imposes the scale regularization only on the inverse covariance matrices of the gmm , while the latter would improperly regularize the mean vectors as well .
the constraints c 123 restrict the matrices to be positive semidenite .
the objective function must be modied for training data that lead to infeasible constraints in eq .
as in svms , we introduce nonnegative slack variables nc to monitor the amount by which the margin constraints in eq .
( 123 ) are violated ( 123 ) .
the objective function in this setting balances the margin violations versus the scale regularization :
min pnc nc + pc trace ( c )
n ( yn c ) zn nc ,
123 + zt
c 123= yn , n = 123 , 123 , .
, n c = 123 , 123 ,
where the balancing hyperparameter > 123 is set by some form of cross - validation .
this optimization is also an instance of semidenite programming .
123 softmax margin maximization for multiple mixture components
lastly we review the extension to mixture modeling where each class is represented by multiple ellipsoids ( 123 ) .
let cm denote the matrix for the mth ellipsoid ( or mixture component ) in class c .
we imagine that each example xn has not only a class label yn , but also a mixture component label mn .
such labels are not provided a priori in the training data , but we can generate proxy labels by tting gmms to the examples in each class by maximum likelihood estimation , then for each example , computing the mixture component with the highest posterior probability .
in the setting where each class is represented by multiple ellipsoids , the goal of learning is to ensure that each example is closer to its target ellipsoid than the ellipsoids from all other classes .
speci - cally , for a labeled example ( xn , yn , mn ) , the constraint in eq .
( 123 ) is replaced by the m constraints :
c 123= yn , m , zt
n ( cm ynmn ) zn 123 ,
figure 123 : decision boundary in a large margin gmm : labeled examples lie at least one unit of distance away .
table 123 : test error rates on mnist digit recognition : maximum likeli - hood versus large margin gmms .
where m is the number of mixture components ( assumed , for simplicity , to be the same for each class ) .
we fold these multiple constraints into a single one by appealing to the softmax inequal -
ity : minm am logpm eam .
specically , using the inequality to derive a lower bound on
cm zn , we replace the m constraints in eq .
( 123 ) by the stricter constraint :
c 123= yn , logxm
ynmn zn 123
we will use a similar technique in section 123 to handle the exponentially many constraints that arise in sequential classication .
note that the inequality in eq .
( 123 ) implies the inequality of eq .
( 123 ) but not vice versa .
also , though nonlinear in the matrices ( cm ) , the constraint in eq .
( 123 ) is still convex .
the objective function in eq .
( 123 ) extends straightforwardly to this setting .
it balances a regularizing term that sums over ellipsoids versus a penalty term that sums over slack variables , one for each constraint in eq .
the optimization is given by :
min pnc nc + pcm trace ( cm )
ynmn zn + logpm ezt
c 123= yn , n = 123 , 123 ,
123 + zt
c = 123 , 123 , .
, c , m = 123 , 123 ,
this optimization is not an instance of semidenite programming , but it is convex .
we discuss how to perform the optimization efciently for large data sets in appendix a .
123 handwritten digit recognition
we trained large margin gmms for multiway classication of mnist handwritten digits ( 123 ) .
the mnist data set has 123 training examples and 123 test examples .
table 123 shows that the large margin gmms yielded signicantly lower test error rates than gmms trained by maximum likeli - hood estimation .
our best results are comparable to the best svm results ( 123 - 123% ) on deskewed images ( 123 ) that do not make use of prior knowledge .
for our best model , with four mixture compo - nents per digit class , the core training optimization over all training examples took ve minutes on a pc .
( multiple runs of this optimization on smaller validation sets , however , were also required to set two hyperparameters : the regularizer for model complexity , and the termination criterion for early
123 large margin hmms for sequential classication
in this section , we extend the framework in the previous section from multiway classication to sequential classication .
particularly , we have in mind the application to asr , where gmms are used to parameterize the emission densities of cd - hmms .
strictly speaking , the gmms in our framework cannot be interpreted as emission densities because their parameters are not constrained to represent normalized distributions .
such an interpretation , however , is not necessary for their use as discriminative models .
in sequential classication by cd - hmms , the goal is to infer the correct hidden state sequence y = ( y123 , y123 , .
, yt ) given the observation sequence x = ( x123 , x123 , .
, xt ) .
in the application to asr , the hidden states correspond to phoneme labels , and the observations are
acoustic feature vectors .
note that if an observation sequence has length t and each label can belong to c classes , then the number of incorrect state sequences grows as o ( c t ) .
this combinatorial explosion presents the main challenge for large margin methods in sequential classication : how to separate the correct hidden state sequence from the exponentially large number of incorrect ones .
the section is organized as follows .
section 123 explains the way that margins are computed for se - quential classication .
section 123 describes our algorithm for large margin training of cd - hmms .
details are given only for the simple case where the observations in each hidden state are modeled by a single ellipsoid .
the extension to multiple mixture components closely follows the approach in section 123 and can be found in ( 123 , 123 ) .
margin - based learning of transition probabilities is likewise straightforward but omitted for brevity .
both these extensions were implemented , however , for the experiments on phonetic recognition in section 123 .
123 margin constraints for sequential classication
we start by dening a discriminant function over state ( label ) sequences of the cd - hmm .
let a ( i , j ) denote the transition probabilities of the cd - hmm , and let s denote the ellipsoid pa - rameters of state s .
the discriminant function d ( x , s ) computes the score of the state sequence s = ( s123 , s123 , .
, st ) on an observation sequence x = ( x123 , x123 , .
, xt ) as :
d ( x , s ) = xt
log a ( st123 , st )
this score has the same form as the log - probability log p ( x , s ) in a cd - hmm with gaussian emis - sion densities .
the rst term accumulates the log - transition probabilities along the state sequence , while the second term accumulates acoustic scores computed as the mahalanobis distances to each states centroid .
in the setting where each state is modeled by multiple mixture components , the acoustic scores from individual mahalanobis distances are replaced with softmax distances of
stmzt , as described in section 123 and ( 123 , 123 ) .
the form logpm
we introduce margin constraints in terms of the above discriminant function .
let h ( s , y ) denote the hamming distance ( i . e . , the number of mismatched labels ) between an arbitrary state sequence s and the target state sequence y .
earlier , in section 123 on multiway classication , we constrained each labeled example to lie at least one unit distance from the decision boundary to each competing class; see eq .
here , by extension , we constrain the score of each target sequence to exceed that of each competing sequence by an amount equal to or greater than the hamming distance :
s 123= y , d ( x , y ) d ( x , s ) h ( s , y )
intuitively , eq .
( 123 ) requires that the ( log - likelihood ) gap between the score of an incorrect se - quence s and the target sequence y should grow in proportion to the number of individual label errors .
the appropriateness of such proportional constraints for sequential classication was rst noted by ( 123 ) .
123 softmax margin maximization for sequential classication
the challenge of large margin sequence classication lies in the exponentially large number of constraints , one for each incorrect sequence s , embodied by eq .
we will use the same softmax inequality , previously introduced in section 123 , to fold these multiple constraints into one , thus considerably simplifying the optimization required for parameter estimation .
we rst rewrite the constraint in eq .
( 123 ) as :
d ( x , y ) + max
( h ( s , y ) + d ( x , s ) ) 123
we obtain a more manageable constraint by substituting a softmax upper bound for the max term and requiring that the inequality still hold :
d ( x , y ) + logxs123=y
note that eq .
( 123 ) implies eq .
( 123 ) but not vice versa .
as in the setting for multiway classication , the objective function for sequential classication balances two terms : one regularizing the scale of
the gmm parameters , the other penalizing margin violations .
denoting the training sequences by ( x n , yn ) n n=123 and the slack variables ( one for each training sequence ) by n 123 , we obtain the following convex optimization :
min pn n + pcm trace ( cm ) s . t .
d ( x n , yn ) + logps123=yn
n 123 , n = 123 , 123 ,
c = 123 , 123 , .
, c , m = 123 , 123 ,
eh ( s , yn ) +d ( x n , s ) n ,
it is worth emphasizing several crucial differences between this optimization and previous ones ( 123 , 123 , 123 ) for discriminative training of cd - hmms for asr .
first , the softmax large margin constraint in eq .
( 123 ) is a differentiable function of the model parameters , as opposed to the hard maximum in eq .
( 123 ) and the number of classication errors in the mce training criteria ( 123 ) .
the constraint and its gradients with respect to gmm parameters cm and transition parameters a ( , ) can be computed efciently using dynamic programming , by a variant of the standard forward - backward procedure in hmms ( 123 ) .
second , due to the reparameterization in eq .
( 123 ) , the discriminant function d ( x n , yn ) and the softmax function are convex in the model parameters .
therefore , the optimization eq .
( 123 ) can be cast as convex optimization , avoiding spurious local minima ( 123 ) .
third , the optimization not only increases the log - likelihood gap between correct and incorrect state sequences , but also drives the gap to grow in proportion to the number of individually incorrect labels ( which we believe leads to more robust generalization ) .
finally , compared to the large margin framework in ( 123 ) , the softmax handling of exponentially large number of margin constraints makes it possible to train on larger data sets .
we discuss how to perform the optimization efciently in appendix a .
123 phoneme recognition
we used the timit speech corpus ( 123 , 123 , 123 ) to perform experiments in phonetic recognition .
we followed standard practices in preparing the training , development , and test data .
our signal pro - cessing front - end computed 123 - dimensional acoustic feature vectors from 123 mel - frequency cepstral coefcients and their rst and second temporal derivatives .
in total , the training utterances gave rise to roughly 123 million frames , all of which were used in training .
we trained baseline maximum likelihood recognizers and two different types of large margin recog - nizers .
the large margin recognizers in the rst group were low - cost discriminative cd - hmms whose gmms were merely trained for frame - based classication .
in particular , these gmms were estimated by solving the optimization in eq .
( 123 ) , then substituted into rst - order cd - hmms for se - quence decoding .
the large margin recognizers in the second group were fully trained for sequential classication .
in particular , their cd - hmms were estimated by solving the optimization in eq .
( 123 ) , generalized to multiple mixture components and adaptive transition parameters ( 123 , 123 ) .
in all the recognizers , the acoustic feature vectors were labeled by 123 phonetic classes , each represented by one state in a rst - order cd - hmm .
for each recognizer , we compared the phonetic state sequences obtained by viterbi decoding to the ground - truth phonetic transcriptions provided by the timit corpus .
for the purpose of comput - ing error rates , we followed standard conventions in mapping the 123 phonetic state labels down to 123 broader phone categories .
we computed two different types of phone error rates , one based on hamming distance , the other based on edit distance .
the former was computed simply from the percentage of mismatches at the level of individual frames .
the latter was computed by aligning the viterbi and ground truth transcriptions using dynamic programming ( 123 ) and summing the substitu - tion , deletion , and insertion error rates from the alignment process .
the frame - based phone error rate computed from hamming distances is more closely tracked by our objective function for large margin training , while the string - based phone error rate computed from edit distances provides a more relevant metric for asr .
tables 123 and 123 show the results of our experiments .
for both types of error rates , and across all model sizes , the best performance was consistently obtained by large margin cd - hmms trained for sequential classication .
moreover , among the two different types of large margin recognizers , utterance - based training generally yielded signicant improvement over frame - based training .
discriminative learning of cd - hmms is an active research area in asr .
two types of algorithms have been widely used : maximum mutual information ( mmi ) ( 123 ) and minimum classication er -
table 123 : frame - based phone error rates , from hamming distance , of different recognizers .
see text for details .
table 123 : string - based phone error rates , from edit distance , of different recognizers .
see text
ror ( 123 ) .
in ( 123 ) , we compare the large margin training proposed in this paper to both mmi and mce systems for phoneme recognition trained on the exact same acoustic features .
there we nd that the large margin approach leads to lower error rates , owing perhaps to the absence of local minima in the objective function and / or the use of margin constraints based on hamming distances .
discriminative learning of sequential models is an active area of research in both asr ( 123 , 123 , 123 ) and machine learning ( 123 , 123 , 123 ) .
this paper makes contributions to lines of work in both commu - nities .
first , in distinction to previous work in asr , we have proposed a convex , margin - based cost function that penalizes incorrect decodings in proportion to their hamming distance from the desired transcription .
the use of the hamming distance in this context is a crucial insight from the work of ( 123 ) in the machine learning community , and it differs profoundly from merely penalizing the log - likelihood gap between incorrect and correct transcriptions , as commonly done in asr .
second , in distinction to previous work in machine learning , we have proposed a framework for se - quential classication that naturally integrates with the infrastructure of modern speech recognizers .
using the softmax function , we have also proposed a novel way to monitor the exponentially many margin constraints that arise in sequential classication .
for real - valued observation sequences , we have shown how to train large margin hmms via convex optimizations over their parameter space of positive semidenite matrices .
finally , we have demonstrated that these learning algorithms lead to improved sequential classication on data sets with over one million training examples ( i . e . , phonet - ically labeled frames of speech ) .
in ongoing work , we are applying our approach to large vocabulary asr and other tasks such as speaker identication and visual object recognition .
the optimizations in eqs .
( 123 ) , ( 123 ) , ( 123 ) and ( 123 ) are convex : specically , in terms of the matrices that parameterize large margin gmms and hmms , the objective functions are linear , while the con - straints dene convex sets .
despite being convex , however , these optimizations cannot be managed by off - the - shelf numerical optimization solvers or generic interior point methods for problems as large as the ones in this paper .
we devised our own special - purpose solver for these purposes .
for simplicity , we describe our solver for the optimization of eq .
( 123 ) , noting that it is easily extended to eqs .
( 123 ) and ( 123 ) .
to begin , we eliminate the slack variables and rewrite the objective function in terms of the hinge loss function : hinge ( z ) = max ( 123 , z ) .
this yields the objective function :
l = xn , c123=yn
hinge ( cid : 123 ) 123 + zt
n ( yn c ) zn ( cid : 123 ) + xc
which is convex in terms of the positive semidenite matrices c .
we minimize l using a projected subgradient method ( 123 ) , taking steps along the subgradient of l , then projecting the matrices ( c ) back onto the set of positive semidenite matrices after each update .
this method is guaranteed to converge to the global minimum , though it typically converges very slowly .
for faster convergence , we precede this method with an unconstrained conjugate gradient optimization in the square - root matrices ( c ) , where c = ct c .
the latter optimization is not convex , but in practice it rapidly converges to an excellent starting point for the projected subgradient method .
this work was supported by the national science foundation under grant number 123
we thank f .
pereira , k .
crammer , and s .
roweis for useful discussions and correspondence .
part of this work was conducted while both authors were afliated with the university of pennsylvania .
