can we detect low dimensional structure in high dimensional data sets of images and video ? the problem of dimensionality reduction arises often in computer vision and pattern recognition .
in this paper , we propose a new solution to this problem based on semidefinite programming .
our algorithm can be used to analyze high dimensional data that lies on or near a low dimensional manifold .
it overcomes certain limitations of previous work in manifold learning , such as isomap and locally linear embedding .
we illustrate the algorithm on easily visualized examples of curves and surfaces , as well as on actual images of faces , handwritten digits , and solid
computer vision , pattern recognition
artificial intelligence and robotics
copyright 123 ieee .
reprinted from proceedings of the 123 ieee computer society conference on computer vision and pattern recognition , cvpr 123 , held 123 june - 123 july 123
volume 123 , pages 123 - 123
publisher url : http : / / dx . doi . org / 123 / cvpr . 123
this material is posted here with permission of the ieee .
such permission of the ieee does not in any way imply ieee endorsement of any of the university of pennsylvania ' s products or services .
internal or personal use of this material is permitted .
however , permission to reprint / republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the ieee by writing to pubs - permissions@ieee . org .
by choosing to view this document , you agree to all provisions of the copyright laws protecting it .
this conference paper is available at scholarlycommons : http : / / repository . upenn . edu / cis_papers / 123
unsupervised learning of image manifolds by semidenite programming
kilian q .
weinberger and lawrence k
department of computer and information science
university of pennsylvania , levine hall
123 walnut street , philadelphia , pa 123 - 123
can we detect low dimensional structure in high dimen - sional data sets of images and video ? the problem of di - mensionality reduction arises often in computer vision and pattern recognition .
in this paper , we propose a new solu - tion to this problem based on semidenite programming .
our algorithm can be used to analyze high dimensional data that lies on or near a low dimensional manifold .
it overcomes certain limitations of previous work in mani - fold learning , such as isomap and locally linear embedding .
we illustrate the algorithm on easily visualized examples of curves and surfaces , as well as on actual images of faces , handwritten digits , and solid objects .
many data sets of images and video are characterized by far fewer degrees of freedom than the actual number of pix - els per image .
the problem of dimensionality reduction is to understand and analyze these images in terms of their basic modes of variabilityfor example , the pose and ex - pression of a human face , or the rotation and scaling of a solid object .
mathematically , we can view an image as a point in a high dimensional vector space whose dimension - ality is equal to the number of pixels in the image ( 123 , 123 ) .
if the images in a data set are effectively parameterized by a small number of continuous variables , then they will lie on or near a low dimensional manifold in this high dimen - sional space ( 123 ) .
this paper is concerned with the unsuper - vised learning of such image manifolds .
beyond its applications in computer vision , manifold learning is best described as a problem at the intersection of statistics , geometry , and computation .
the problem is il - lustrated in fig .
given high dimensional data sampled from a low dimensional manifold , how can we efciently compute a faithful ( nonlinear ) embedding ? in the last few
years , researchers have uncovered a large family of algo - rithms for computing such embeddings from the top or bottom eigenvectors of an appropriately constructed ma - trix .
these algorithmsincluding isomap ( 123 ) , locally lin - ear embedding ( lle ) ( 123 , 123 ) , hessian lle ( 123 ) , laplacian eigenmaps ( 123 ) , and others ( 123 ) can reveal low dimensional manifolds that are not detected by classical linear methods , such as principal component analysis ( pca ) ( 123 ) .
our main contribution in this paper is a new algorithm for manifold learning based on semidenite programming .
like isomap and lle , it relies on efcient and tractable
figure 123
the problem of manifold learning , illustrated for n = 123 data points sampled from a swiss roll .
a discretized man - ifold is revealed by connecting each data point and its k = 123 nearest neighbors ( 123 ) .
an unsupervised learning algorithm unfolds the swiss roll while preserving the local ge - ometry of nearby data points ( 123 ) .
finally , the data points are projected onto the two dimen - sional subspace that maximizes their vari - ance , yielding a faithful embedding of the original manifold ( 123 ) .
proceedings of the 123 ieee computer society conference on computer vision and pattern recognition ( cvpr123 ) 123 - 123 / 123 $123 123 ieee
optimizations that are not plagued by spurious local min - ima .
interestingly , though , our algorithm is based on a com - pletely different geometric intuition ( and optimization ) , and it overcomes certain limitations of previous work .
mainly conned to a low dimensional subspace; in this case , their eigenvalues also reveal the correct underlying dimen - sionality .
they do not generally succeed , however , in the case that the inputs lie on a low dimensional manifold .
dimensionality reduction
from subspaces to manifolds
we study dimensionality reduction as a problem in un - supervised learning .
given n high dimensional ( cid : 123 ) xird ( where i = 123 , 123 , .
, n ) , the problem is to com - pute outputs ( cid : 123 ) yi rd in one - to - one correspondence with the inputs that provide a faithful embedding in d < d di - mensions .
by faithful , we mean that nearby points re - main nearby and that distant points remain distant; we shall make this intuition more precise in what follows .
ide - ally , an unsupervised learning algorithm should also estimate the intrinsic dimensionality d of the manifold sam - pled by the inputs ( cid : 123 ) xi .
our algorithm for manifold learning builds on classical linear methods for dimensionality reduction .
we therefore begin by briey reviewing principal component analysis ( pca ) ( 123 ) and metric multidimensional scaling ( mds ) ( 123 ) .
the generalization from subspaces to manifolds is then made by introducing the idea of local isometry .
linear methods
pca and mds are based on simple geometric intuitions .
in pca , the inputs are projected into the lower dimensional subspace that maximizes the projected variance; the basis vectors of this subspace are given by the top eigenvectors of the dd covariance matrix , c = 123 i .
( here and in what follows , we assume without loss of generality that the inputs are centered on the origin :
in mds with classical scaling , the inputs are projected into the subspace that best preserves their pairwise squared distances | ( cid : 123 ) xi ( cid : 123 ) xj|123 or , as done in practice , their dot prod - ucts ( cid : 123 ) xi ( cid : 123 ) xj .
the outputs of mds are computed from the top eigenvectors of the n n gram matrix with elements gij = ( cid : 123 ) xi ( cid : 123 ) xj .
note that a set of vectors is determined up to rotation by its gram matrix of dot products .
though based on different geometric intuitions , pca and mds yield the same resultsessentially a rotation of the inputs followed by a projection into the subspace with the highest variance .
the correlation matrix of pca and the gram matrix of mds have the same rank and nonzero eigenvalues up to a constant factor .
both matrices are semi - positive denite , and gaps in their eigenvalue spectra indi - cate that the high dimensional inputs xird lie to a good approximation in a lower dimensional subspace of dimen - sionality d , where d is the number of appreciably positive eigenvalues .
these linear methods for dimensionality re - duction generate faithful embeddings when the inputs are
we will refer to any method that computes a low dimen - sional embedding from the eigenvectors of an appropriately constructed matrix as a method in spectral embedding .
if pca and mds are linear methods in spectral embedding , what are their nonlinear counterparts ? in fact , there are sev - eral , most of them differing in the geometric intuition they take as starting points and in the generalizations of linear transformations that they attempt to discover .
the nonlinear method we propose in this paper is based fundamentally on the notion of isometry .
( for the sake of exposition , we defer a discussion of competing nonlinear methods based on isometries ( 123 , 123 ) to section 123 ) for - mally , two riemannian manifolds are said to be isometric if there is a diffeomorphism such that the metric on one pulls back to the metric on the other .
informally , an isom - etry is a smooth invertible mapping that looks locally like a rotation plus translation , thus preserving distances along the manifold .
intuitively , for two dimensional surfaces , the class of isometries includes whatever physical transforma - tions one can perform on a sheet of paper without intro - ducing holes , tears , or self - intersections .
many interesting image manifolds are isometric to connected subsets of eu - clidean space ( 123 ) .
i=123 and y = ( ( cid : 123 ) yi ) n
isometry is a relation between manifolds , but we can ex - tend the notion in a natural way to data sets .
consider two data sets x = ( ( cid : 123 ) xi ) n i=123 that are in one - to - one correspondence .
let the n n binary matrix in - dicate a neighborhood relation on x and y , such that we regard ( cid : 123 ) xj as a neighbor of ( cid : 123 ) xi if and only if ij = 123 ( and similarly , for ( cid : 123 ) yj and ( cid : 123 ) yi ) .
we will say that the data sets x and y are locally isometric under the neighborhood rela - tion if for every point ( cid : 123 ) xi , there exists a rotation , reec - tion and / or translation that maps ( cid : 123 ) xi and its neighbors pre - cisely onto ( cid : 123 ) yi and its neighbors .
we can translate the above denition into various sets of equality constraints on x and y .
to begin , note that the lo - cal mapping between neighborhoods will exist if and only if the distances and angles between points and their neighbors are preserved .
thus , whenever both ( cid : 123 ) xj and ( cid : 123 ) xk are neigh - bors of ( cid : 123 ) xi ( that is , ijik = 123 ) , for local isometry we must
( 123 ) is sufcient for local isometry because the triangle formed by any point and its neighbors is determined up to rotation , reection and translation by specifying the lengths
proceedings of the 123 ieee computer society conference on computer vision and pattern recognition ( cvpr123 ) 123 - 123 / 123 $123 123 ieee
of two sides and the angle between them .
in fact , such a tri - angle is similarly determined by specifying the lengths of all its sides .
thus , we can also say that x and y are lo - cally isometric under if whenever ( cid : 123 ) xi and ( cid : 123 ) xj are them - selves neighbors ( that is , ij = 123 ) or are common neigh - bors of another point in the data set ( that is , ( t ) ij > 123 ) ,
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) xj
this is an equivalent characterization of local isometry as eq .
( 123 ) , but expressed only in terms of pairwise distances .
finally , we can express these constraints purely in terms of dot products .
let gij = ( cid : 123 ) xi ( cid : 123 ) xj and kij = ( cid : 123 ) yi ( cid : 123 ) yj denote the gram matrices of the inputs and outputs , respectively .
we can rewrite eq .
( 123 ) as :
kii+kjjkijkji = gii+gjjgijgji .
( 123 ) expresses the conditions for local isometry purely in terms of gram matrices; it is in fact this formulation that will form the basis of our algorithm for manifold learning .
semidenite embedding
we can now formulate the problem of manifold learn - ing more precisely , taking as a starting point the notion of local isometry .
in particular , given n inputs ( cid : 123 ) xi rd and a prescription for identifying neighboring inputs , can we nd n outputs ( cid : 123 ) yi rd , where d < d , such that the in - puts and outputs are locally isometric , or at least approxi - mately so ? alternatively , we can state the problem in terms of gram matrices : can we nd a gram matrix kij that sat - ises the constraints in eq .
( 123 ) , and for which the vectors ( cid : 123 ) yi ( which are determined up to a rotation by the elements of the gram matrix ) lie in a subspace of dimensionality d < d , or at least approximately lie in such a subspace ? in this sec - tion , we show how this can be done by a constrained opti - mization over the cone of semidenite matrices .
like pca and mds , the algorithm we propose for man - ifold learning is based on a simple geometric intuition .
imagine each input ( cid : 123 ) xi as a steel ball that is connected to its k nearest neighbors by rigid rods .
the effect of the rigid rods is to x the distances and angles between near - est neighbors , no matter what other forces are applied to the inputs .
now imagine that the inputs are pulled apart , maximizing their total variance subject to the constraints imposed by the rigid rods .
123 shows the unraveling ef - fect of this transformation on inputs sampled from the swiss roll .
the goal of this section is to formalize the steps of this transformationin particular , the constraints that must be satised by the nal solution , and the nature of the opti - mization that must be performed .
the constraints that we need to impose for local isome - try are naturally represented by a graph with n nodes , one for each input .
consider the graph formed by connecting each input to its k nearest neighbors , where k is a free pa - rameter of the algorithm .
for simplicity , we assume that the graph formed in this way is connected; if not , then each con - nected component should be analyzed separately .
the con - straints for local isometry under this neighborhood relation are simply to preserve the lengths of the edges in this graph , as well as the angles between edges at the same node .
in practice , it is easier to deal only with constraints on dis - tances , as opposed to angles .
to this end , let us further con - nect the graph by adding edges between the neighbors of each node ( if they do not already exist ) .
now by preserv - ing the distances of all edges in this new graph , we preserve both the distances of edges and the angles between edges in the original graphbecause if all sides of a triangle are pre - served , so are its angles .
in addition to imposing the constraints represented by the neighborhood graph , we also constrain the outputs ( cid : 123 ) yi to be centered on the origin : ( cid : 123 )
( cid : 123 ) yi = ( cid : 123 ) 123
( 123 ) simply removes a translational degree of freedom from the nal solution .
the centering constraint can be ex - pressed in terms of the gram matrix kij as follows :
( cid : 123 ) yi ( cid : 123 ) yj =
note that eq .
( 123 ) is a linear equality constraint on the ele - ments of the output gram matrix , just like eq
because the geometric constraints on the outputs ( cid : 123 ) yi are so naturally expressed in terms of the gram matrix kij ( and because the outputs are determined up to rotation by their gram matrix ) , we may view manifold learning as an op - timization over gram matrices kij rather than vectors ( cid : 123 ) yi .
not all matrices , however , can be interpreted as gram ma - trices : only symmetric matrices with nonnegative eigenval - ues can be interpreted in this way .
thus , we must further constrain the optimization to the cone of semidenite ma -
in sum , there are three types of constraints on the gram matrix kij , arising from local isometry , centering , and semideniteness .
the rst two involve linear equality con - straints; the last one is not linear , but importantly it is con - vex .
we will exploit this property in what follows .
note that there are o ( n k123 ) constraints on o ( n 123 ) matrix elements , and that the constraints are not incompatible , since at the very least they are satised by the input gram matrix gij ( assuming , as before , that the inputs ( cid : 123 ) xi are centered on the
proceedings of the 123 ieee computer society conference on computer vision and pattern recognition ( cvpr123 ) 123 - 123 / 123 $123 123 ieee
what function of the gram matrix can we optimize to unfold a manifold , as in fig .
123 ? as motivation , consider the ends of a piece of string , or the corners of a ag .
any slack in the string serves to decrease the ( euclidean ) dis - tance between its two ends; likewise , any furling of the ag serves to bring its corners closer together .
more generally , we observe that any fold between two points on a mani - fold serves to decrease the euclidean distance between the points .
this suggests an optimization that we can perform to compute the outputs ( cid : 123 ) yi that unfold a manifold sampled by inputs ( cid : 123 ) xi .
in particular , we propose to maximize the sum of pairwise squared distances between outputs :
t ( y ) =
by maximizing eq .
( 123 ) , we pull the outputs as far apart as possible , subject to the constraints in the previous section .
before expressing this objective function in terms of the gram matrix kij , let us verify that it is indeed bounded , meaning that we cannot pull the outputs innitely far apart .
intuitively , the constraints to preserve local distances ( and the assumption that the graph is connected ) prevent such a divergence .
more formally , let ij =123 if ( cid : 123 ) xj is one of the k nearest neighbors of ( cid : 123 ) xi , and zero otherwise , and let be the maximal distance between any two such neighbors :
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) xj
assuming the graph is connected , then the longest path through the graph has a distance of at most n .
we ob - serve furthermore that given two nodes , the distance of the path through the graph provides an upper bound on their eu - clidean distance .
thus , for all outputs ( cid : 123 ) yi and ( cid : 123 ) yj , we must have | ( cid : 123 ) yi ( cid : 123 ) yj| < n .
using this to provide an upper bound on the objective function in eq .
( 123 ) , we obtain : ( n ) 123 = n 123 123
t ( y ) 123
thus , the objective function cannot increase without bound if we enforce the constraints to preserve local distances .
we can express the objective function in eq .
( 123 ) directly in terms of the gram matrix kij of the outputs ( cid : 123 ) yi .
expand - ing the terms on the right hand side , and enforcing the con - straint that the outputs are centered on the origin , we ob -
t ( y ) =
kii = tr ( k ) .
thus , we can interpret the objective function for the outputs in several ways : as a sum over pairwise distances in eq
as a measure of variance in eq .
( 123 ) , or as the trace of their gram matrix in eq .
the second interpretation is remi - niscent of pca , but whereas in pca we compute the lin - ear projection that maximizes variance , here we compute the locally isometric embedding .
put another way , the ob - jective function for maximizing variance remains the same; we have merely changed the allowed form of the dimen - sionality reduction .
we also emphasize that in eq .
( 123 ) , we are maximizing the trace , not minimizing it .
while a stan - dard relaxation to minimizing the rank ( 123 ) of a semidenite matrix is to minimize its trace , the intuition here is just the opposite : we will obtain a low dimensional embedding by maximizing the trace of the gram matrix .
let us now collect the costs and constraints of this opti - mization .
the problem is to maximize the variance of the i=123 subject to the constraints that they are cen - tered on the origin and locally isometric to the inputs i=123
in terms of the input gram matrix gij = ( cid : 123 ) xi ( cid : 123 ) xj , the output gram matrix kij = ( cid : 123 ) yi ( cid : 123 ) yj and the adja - cency matrix ij indicating nearest neighbors , the opti - mization can be written as :
maximize tr ( k ) subject to k ( cid : 123 ) 123 ,
ij kij = 123 ,
and ij such that ij =123 or ( t ) ij =123 ,
kii+kjjkijkji = gii+gjjgijgji .
this problem is an instance of semidenite programming ( sdp ) ( 123 ) : the domain is the cone of semidenite matrices intersected with hyperplanes ( represented by equality con - straints ) , and the objective function is linear in the matrix elements .
the optimization is bounded above by eq .
( 123 ) ; it is also convex , thus eliminating the possibility of spurious local maxima .
there exists a large literature on efciently solving sdps , as well as a number of general - purpose tool - boxes .
the results in this paper were obtained using the se - dumi and csdp 123 toolboxes ( 123 , 123 ) in matlab .
spectral embedding
from the gram matrix learned by semidenite program - ming , we can recover the outputs ( cid : 123 ) yi by matrix diagonaliza - tion .
let vi denote the ith element of the th eigenvec - tor , with eigenvalue .
then the gram matrix can be writ -
an n - dimensional embedding that is locally isometric to the inputs ( cid : 123 ) xi is obtained by identifying the th element of the output ( cid : 123 ) yi as :
proceedings of the 123 ieee computer society conference on computer vision and pattern recognition ( cvpr123 ) 123 - 123 / 123 $123 123 ieee
the eigenvalues of k are guaranteed to be nonnegative .
thus , from eq .
( 123 ) , a large gap in the eigenvalue spec - trum between the dth and ( d + 123 ) th eigenvalues indicates that the inputs lie on or near a manifold of dimensional - ity d .
in this case , a low dimensional embedding that is ap - proximately locally isometric is given by truncating the el - ements of ( cid : 123 ) yi .
this amounts to projecting the outputs into the subspace of maximal variance , assuming the eigenval - ues are sorted from largest to smallest .
the quality of the approximation is determined by the size of the truncated eigenvalues; there is no approximation error for zero eigen - values .
the situation is analogous to pca and mds , but here the eigenvalue spectrum reects the underlying dimen - sionality of a manifold , as opposed to merely a subspace .
the three steps of the algorithm , which we call semidef - inite embedding ( sde ) , are summarized in table 123
in its simplest formulation , the only free parameter of the algo - rithm is the number of nearest neighbors in the rst step .
compute the k nearest neighbors of each input .
form the graph that connects each input to its neighbors and each neighbor to other neighbors of the same input .
compute the gram matrix of the maxi - mum variance embedding , centered on the origin , that preserves the distances of all edges in the neighborhood graph .
extract a low dimensional embedding from the dominant eigenvectors of the gram matrix learned by semidenite pro -
table 123
steps of semidenite embedding .
by semidenite programming .
the top three eigenvectors are plotted , but the variance in the third dimension ( shown to scale ) is negligible .
the eigenvalue spectrum in fig .
123 re - veals two dominant eigenvaluesa major eigenvalue , rep - resenting the unwrapped length of the swiss roll , and a minor eigenvalue , representing its width .
( the unwrapped swiss roll is much longer than it is wide . ) the other eigen - values are nearly zero , indicating that sde has discovered the true underlying dimensionality ( d=123 ) of these inputs .
123 shows another easily visualized example .
the left plot shows n = 123 inputs sampled from a trefoil knot in d = 123 dimensions; the right plot shows the d = 123 embed - ding discovered by sde using k =123 nearest neighbors .
the color coding reveals that local neighborhoods have been preserved .
in this case , the underlying manifold is a one - dimensional curve , but due to the cycle , it can only be repre - sented in euclidean space by a circle .
the eigenvalue spec - trum in fig .
123 reveals two dominant eigenvalues; the rest are essentially zero , indicating the underlying ( global ) di - mensionality ( d=123 ) of the circle .
figure 123
left : n = 123 inputs sampled along a trefoil knot in d =123 dimensions .
right : d=123 embedding computed by sde using k = 123 nearest neighbors .
the color coding shows that local neighborhoods are preserved .
we used several data sets of curves , surfaces , and images to evaluate the algorithm in table 123 for low dimensional em - bedding of high dimensional inputs .
123 shows n = 123 inputs sampled off a swiss roll ( 123 ) .
the inputs to the algorithm had d =123 dimen - sions , consisting of the three dimensions shown in the g - ure , plus ve extra dimensions123 lled with low variance gaussian noise .
the bottom plot of the gure shows the un - folded swiss roll extracted from the gram matrix learned
for k = 123 nearest neighbors , the noise in extra dimensions helps to prevent the manifold from locking up when it is unfolded subject to the equality constraints in eqs .
alternatively , the constraints in the sdp can be slightly relaxed by introducing slack variables .
123 shows the results of sde applied to color im - ages of a three dimensional solid object .
the images were created by viewing a teapot from different angles in the plane .
the images have 123 123 pixels , with three byte color depth , giving rise to inputs of d =123 dimensions .
though very high dimensional , the images in this data set are effectively parameterized by one degree of freedom the angle of rotation .
sde was applied to n = 123 images spanning 123 degrees of rotation , with k = 123 nearest neigh - bors used to generate a connected graph .
the two dimen - sional embedding discovered by sde represents the rotat - ing object as a circlean intuitive result analogous to the embedding discovered for the trefoil knot .
the eigenvalue spectrum of the gram matrix learned by semidenite pro - gramming is shown in fig .
123; all but the rst two eigenval - ues are practically zero , indicating the underlying ( global ) dimensionality ( d=123 ) of the circle .
proceedings of the 123 ieee computer society conference on computer vision and pattern recognition ( cvpr123 ) 123 - 123 / 123 $123 123 ieee
figure 123
two dimensional embedding of n =123 images of a rotating teapot , obtained by sde using k = 123 nearest neighbors .
for this experiment , the teapot was rotated 123 degrees; the low dimensional embedding is a full circle .
a representative sample of im - ages are superimposed on top of the embed -
123 was generated from the same data set of images; however , for this experiment , only n = 123 images were used , sampled over 123 degrees of rotation .
in this case , the eigenvalue spectrum from sde detects that the images lie on a one dimensional curve ( see fig .
123 ) , and the d = 123 em - bedding in fig .
123 orders the images by their angle of rota -
123 shows the results of sde on a data set of n =123 images of faces .
the images contain different views and expressions of the same face .
the images have 123 123 grayscale pixels , giving rise to inputs with d = 123 di - mensions .
the plot in fig .
123 shows the rst two dimensions of the embedding discovered by sde , using k = 123 nearest neighbors .
interestingly , the eigenvalue spectrum in fig .
123 indicates that most of the variance of the spectral embed - ding is contained in the rst three dimensions .
123 shows the results of sde applied to another data set of images .
in this experiment , the images were a subset of n = 123 handwritten twos from the usps data set of handwritten digits ( 123 ) .
the images have 123 grayscale pixels , giving rise to inputs with d = 123 dimensions .
in - tuitively , one would expect these images to lie on a low di - mensional manifold parameterized by such features as size , slant , and line thickness .
123 shows the rst two dimen - sions of the embedding obtained from sde , with k =123 near - est neighbors .
the eigenvalue spectrum in fig .
123 indicates a latent dimensionality signicantly larger than two , but still much smaller than the actual number of pixels .
figure 123
top : two dimensional embedding of n = 123 images of faces , obtained by sde using k = 123 nearest neighbors .
representa - tive faces are shown next to circled points .
bottom : eigenvalues of sde and pca on this data set , indicating their estimates of the un - derlying dimensionality .
the eigenvalues are shown as a percentage of the trace of the out - put gram matrix for sde and the trace of the input gram matrix for pca .
the eigenvalue spectra show that most of the variance of the nonlinear embedding is conned to many fewer dimensions than the variance of the lin -
the last few years have witnessed a number of de - velopments in manifold learning .
recently proposed al - gorithms include isomap ( 123 ) , locally linear embedding ( lle ) ( 123 , 123 ) , hessian lle ( hlle ) ( 123 ) , and laplacian eigenmaps ( 123 ) ; there are also related algorithms for clus - tering ( 123 ) .
all these algorithms share the same basic struc - ture as sde , consisting of three steps : ( i ) computing neigh - borhoods in the input space , ( ii ) constructing a square ma - trix with as many rows as inputs , and ( iii ) spectral embed - ding via the top or bottom eigenvectors of this matrix .
sde is based on a rather different geometric intuition , however , and as a result , it has different properties .
comparing the algorithms , we nd that each one at - tempts to estimate and preserve a different geometric signa -
proceedings of the 123 ieee computer society conference on computer vision and pattern recognition ( cvpr123 ) 123 - 123 / 123 $123 123 ieee
figure 123
one dimensional embedding of n = 123 images of a rotating teapot , obtained by sde us - ing k =123 nearest neighbors .
for this experiment , the teapot was only rotated 123 degrees .
represen - tative images are shown ordered by their location in the embedding .
maps estimate the hessian and laplacian on the manifold , respectively; sde estimates local angles and distances .
of these algorithms , only isomap , hlle , and sde attempt to learn isometric embeddings; they are therefore the easiest to compare ( since they seek the same solution , up to rota - tion and scaling ) .
the results on the data set in fig .
123 reveal some salient differences between these algorithms .
while sde and hlle reproduce the original inputs up to isome - try , isomap fails in this example because the sampled man - ifold is not isometric to a convex subset of euclidean space .
( this is a key assumption of isomap , one that is not sat - ised by many image manifolds ( 123 ) . ) moreover , compar - ing the eigenvalue spectra of the algorithms , only sde de - tects the correct underlying dimensionality of the inputs; isomap is foiled by non - convexity , while the eigenvalue spectra of lle and hlle do not reveal this type of infor - mation ( 123 , 123 ) .
overall , the different algorithms for manifold learning should be viewed as complementary; each has its own ad - vantages and disadvantages .
lle , hlle , and laplacian eigenmaps construct sparse matrices , and as a result , they are easier to scale to large data sets .
on the other hand , their eigenvalue spectra do not reliably reveal the under - lying dimensionality of sampled manifolds , as do isomap and sde .
there exist rigorous proofs of asymptotic con - vergence for isomap ( 123 , 123 ) and hlle ( 123 ) , but not for the other algorithms .
on the other hand , sde by its very nature provides nite - size guarantees that its constraints will lead to locally isometric embeddings .
we are not aware of any nite - size guarantees provided by the other algorithms , and indeed , the hessian estimation in hlle relies on numeri - cal differencing , which can be problematic for small sample sizes .
finally , while the different algorithms have different computational bottlenecks , the second step in sde ( involv - ing semidenite programming ) is more computationally de - manding than the analogous steps in lle and isomap .
our initial results for sde appear promising .
there are many important directions for future work .
the most obvi - ous is the investigation of faster methods for solving the semidenite program in sde .
this study used a generic solver that did not exploit the special structure of the con - straints .
a data set with n = 123 points ( and k = 123 ) required about 123 minutes of computation time on a ma - chine with a 123ghz pentium 123 processor .
data sets with up to n = 123 points took several hours .
a specialized
figure 123
results of sde using k = 123 nearest neighbors on n = 123 images of handwritten twos .
representative images are shown next to circled points .
figure 123
eigenvalue spectra from sde on the data sets in this paper .
the eigenvalues are shown as a percentage of the trace of the gram matrix learned by semidenite pro - gramming .
sde identies the correct under - lying dimensionality of the swiss roll , tre - foil knot , and teapot data sets .
the images of faces and handwritten digits give rise to many fewer non - zero eigenvalues than the actual number of pixels .
ture of the underlying manifold .
isomap estimates geodesic distances between inputs; lle estimates the coefcients of local linear reconstructions; hlle and laplacian eigen -
proceedings of the 123 ieee computer society conference on computer vision and pattern recognition ( cvpr123 ) 123 - 123 / 123 $123 123 ieee
charting a manifold .
becker , s .
thrun , and k .
obermayer , editors , advances in neural information pro - cessing systems 123 , cambridge , ma , 123
mit press .
cox and m .
multidimensional scaling .
chapman &
hall , london , 123
donoho and c .
grimes .
when does isomap re - cover the natural parameterization of families of articulated images ? technical report 123 - 123 , department of statis - tics , stanford university , august 123
donoho and c .
grimes .
hessian eigenmaps : locally linear embedding techniques for high - dimensional data .
pro - ceedings of the national academy of arts and sciences ,
fazel , h .
hindi , and s .
a rank minimization heuristic with application to minimum order system approx - in proceedings of the american control confer - ence , volume 123 , pages 123 , june 123
( 123 ) j .
a database for handwritten text recognition re - search .
ieee transaction on pattern analysis and machine intelligence , 123 ( 123 ) : 123 , may 123
( 123 ) i .
jolliffe .
principal component analysis .
springer -
verlag , new york , 123
( 123 ) h .
fainman , and r .
hecht - nielsen .
image manifolds .
nasrabadi and a .
katsaggelos , editors , applica - tions of articial neural networks in image processing iii , proceedings of spie , volume 123 , pages 123 , belling - ham , wa , 123
( 123 ) r .
pless and i .
simon .
embedding images in non - at spaces .
technical report wu - cs - 123 - 123 , washington uni - versity , december 123
( 123 ) s .
roweis and l .
nonlinear dimensionality reduc - tion by locally linear embedding .
science , 123 : 123 ,
( 123 ) l .
saul and s .
roweis .
think globally , t locally : un - supervised learning of low dimensional manifolds .
journal of machine learning research , 123 : 123 , 123
( 123 ) b .
scholkopf , a .
smola , and k . - r .
muller .
nonlinear com - ponent analysis as a kernel eigenvalue problem .
neural com - putation , 123 : 123 , 123
( 123 ) j .
shi and j .
normalized cuts and image segmenta - tion .
ieee transactions on pattern analysis and machine intelligence ( pami ) , pages 123 , august 123
( 123 ) j .
using sedumi 123 , a matlab toolbox for optimization overy symmetric cones .
optimization methods and software , 123 - 123 : 123 , 123
( 123 ) j .
tenenbaum , v .
de silva , and j .
langford .
a global geometric framework for nonlinear dimensionality reduc - tion .
science , 123 : 123 , 123
( 123 ) m .
turk and a .
pentland .
eigenfaces for recognition
nal of cognitive neuroscience , 123 ( 123 ) : 123 , 123
( 123 ) l .
vandenberghe and s .
semidenite programming .
siam review , 123 ( 123 ) : 123 , march 123
( 123 ) h .
zha and z .
isometric embedding and continuum isomap .
in proceedings of the twentieth international con - ference on machine learning ( icml 123 ) , pages 123 ,
figure 123
top : embedding of a non - convex two dimensional data set ( n = 123 ) by differ - ent algorithms for manifold learning .
isomap , lle , and hlle were run with k = 123 nearest neighbors; sde , with k =123 nearest neighbors .
only hlle and sde reproduce the original in - puts up to isometry .
bottom : only sde has an eigenvalue spectrum that indicates the cor - rect dimensionality ( d=123 ) .
solver should allow us to scale sde up to larger data sets and larger neighborhood sizes .
another direction is relax the constraints in eqs .
( 123 ) by introducing slack variables .
while slack variables do not change the basic structure of the semidenite program , they may improve the robustness of the algorithm on small or noisy data sets .
other directions for future work include the investigation of image mani - folds with different topologies ( 123 ) ( such as those isomet - ric to low dimensional spheres or torii ) , the extrapolation of results to out - of - sample inputs ( 123 ) , and the relation of sde to kernel methods for nonlinear dimensionality reduc - tion ( 123 ) .
finally , as has been done for isomap ( 123 , 123 , 123 ) and hlle ( 123 ) , it would be desirable to formulate sde in the continuum limit and to construct rigorous proofs of asymp - totic convergence .
such theoretical results would likely pro - vide additional insight into the behavior of the algorithm .
