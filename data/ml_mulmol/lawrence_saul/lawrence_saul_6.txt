an introduction to locally linear embedding
lawrence k
at&t labs research
123 park ave , florham park , nj 123 usa
roweis
gatsby computational neuroscience unit , ucl
123 queen square , london wc123n 123ar , uk
many problems in information processing involve some form of dimension - ality reduction .
here we describe locally linear embedding ( lle ) , an unsu - pervised learning algorithm that computes low dimensional , neighborhood preserving embeddings of high dimensional data .
lle attempts to discover nonlinear structure in high dimensional data by exploiting the local symme - tries of linear reconstructions .
notably , lle maps its inputs into a single global coordinate system of lower dimensionality , and its optimizations though capable of generating highly nonlinear embeddingsdo not involve local minima .
we illustrate the method on images of lips used in audiovisual
many problems in statistical pattern recognition begin with the preprocessing of multidimensional signals , such as images of faces or spectrograms of speech .
often , the goal of preprocessing is some form of dimensionality reduction : to com - press the signals in size and to discover compact representations of their variability .
two popular forms of dimensionality reduction are the methods of principal com - ponent analysis ( pca ) ( 123 ) and multidimensional scaling ( mds ) ( 123 ) .
both pca and mds are eigenvector methods designed to model linear variabilities in high dimen - sional data .
in pca , one computes the linear projections of greatest variance from
the top eigenvectors of the data covariance matrix .
in classical ( or metric ) mds , one computes the low dimensional embedding that best preserves pairwise dis - tances between data points .
if these distances correspond to euclidean distances , the results of metric mds are equivalent to pca .
both methods are simple to implement , and their optimizations do not involve local minima .
these virtues ac - count for the widespread use of pca and mds , despite their inherent limitations as linear methods .
recently , we introduced an eigenvector methodcalled locally linear embedding ( lle ) for the problem of nonlinear dimensionality reduction ( 123 ) .
this problem is illustrated by the nonlinear manifold in figure 123
in this example , the dimen - sionality reduction by lle succeeds in identifying the underlying structure of the manifold , while projections of the data by pca or metric mds map faraway data points to nearby points in the plane .
like pca and mds , our algorithm is sim - ple to implement , and its optimizations do not involve local minima .
at the same time , however , it is capable of generating highly nonlinear embeddings .
note that mixture models for local dimensionality reduction ( 123 , 123 ) , which cluster the data and perform pca within each cluster , do not address the problem considered here namely , how to map high dimensional data into a single global coordinate system of lower dimensionality .
in this paper , we review the lle algorithm in its most basic form and illustrate a potential application to audiovisual speech synthesis ( 123 ) .
figure 123 : the problem of nonlinear dimensionality reduction , as illustrated for three dimensional data ( b ) sampled from a two dimensional manifold ( a ) .
an un - supervised learning algorithm must discover the global internal coordinates of the manifold without signals that explicitly indicate how the data should be embed - ded in two dimensions .
the shading in ( c ) illustrates the neighborhood - preserving mapping discovered by lle .
the lle algorithm , summarized in fig .
123 , is based on simple geometric intuitions .
sampled from some smooth underlying manifold .
provided there is sufcient data ( such that the manifold is well - sampled ) , we expect each data point and its neigh - bors to lie on or close to a locally linear patch of the manifold .
, each of dimensionality
we can characterize the local geometry of these patches by linear coefcients that reconstruct each data point from its neighbors .
in the simplest formulation of lle , nearest neighbors per data point , as measured by euclidean dis - tance .
( alternatively , one can identify neighbors by choosing all points within a ball of xed radius , or by using more sophisticated rules based on local metrics . ) reconstruction errors are then measured by the cost function :
suppose the data consist of
summarize the contribution of the th data point to structions .
the weights
, we minimize the cost func - the th reconstruction .
to compute the weights
does not belong to this set; second ,
from its neighbors , enforcing
the reason for the that the rows of the weight matrix sum to one : sum - to - one constraint will become clear shortly .
the optimal weights
to these constraints are found by solving a least squares problem , as discussed in
which adds up the squared distances between all the data points and their recon -
note that the constrained weights that minimize these reconstruction errors obey an important symmetry : for any particular data point , they are invariant to rotations , rescalings , and translations of that data point and its neighbors .
the invariance to rotations and rescalings follows immediately from the form of eq .
( 123 ) ; the invari - ance to translations is enforced by the sum - to - one constraint on the rows of the weight matrix .
a consequence of this symmetry is that the reconstruction weights characterize intrinsic geometric properties of each neighborhood , as opposed to properties that depend on a particular frame of reference .
tion subject to two constraints : rst , that each data point
is reconstructed only
design , the reconstruction weights
suppose the data lie on or near a smooth nonlinear manifold of dimensionality .
to a good approximation , then , there exists a linear mappingconsisting of a translation , rotation , and rescalingthat maps the high dimensional coordi - nates of each neighborhood to global internal coordinates on the manifold
reect intrinsic geometric properties of the
data that are invariant to exactly such transformations .
we therefore expect their characterization of local geometry in the original data space to be equally valid for dimensions should also reconstruct its embedded manifold
local patches on the manifold .
in particular , the same weights
the th data point in coordinates in ! dimensions .
( informally , imagine taking a pair of scissors , cutting out locally linear patches of the underlying manifold , and placing them in the low dimensional embedding space .
assume further that this operation is done in a way that preserves the angles formed by each data point to its nearest neighbors .
in this case , the transplantation of each patch involves no more than a translation , rotation , and rescaling of its data , exactly the operations to which the weights are invariant .
thus , when the patch arrives at its low dimensional destination , we expect the same weights to reconstruct each data point from its neighbors . )
constraints that make the problem well - posed , it can be minimized by solving a
an ordered set of orthogonal coordinates centered on the origin .
details of this eigenvector problem are discussed in appendix b .
this cost functionlike the previous oneis based on locally linear reconstruction
lle constructs a neighborhood preserving mapping based on the above idea .
in the is mapped to a
nal step of the algorithm , each high dimensional observation
low dimensional vector representing global internal coordinates on the manifold .
this is done by choosing ! - dimensional coordinates to minimize the embedding while optimizing the coordinates errors , but here we x the weights
embedding cost in eq .
( 123 ) denes a quadratic form in the vectors .
subject to eigenvector problem , whose bottom ! non - zero eigenvectors provide bedding coordinates are computed by an - * .
one free parameter : the number of neighbors per data point ,
note that while the reconstruction weights for each data point are computed from its local neighborhoodindependent of the weights for other data pointsthe em - eigensolver , a global operation that couples all data points in connected components of the graph dened by the weight matrix .
the different dimensions in the embedding space can be computed succes - sively; this is done simply by computing the bottom eigenvectors from eq .
( 123 ) one at a time .
but the computation is always coupled across data points .
this is how the algorithm leverages overlapping local information to discover global structure .
implementation of the algorithm is fairly straightforward , as the algorithm has only .
once neighbors
compute the neighbors of each data point ,
its neighbors , minimizing the cost in eq .
( 123 ) by constrained linear ts .
that best reconstruct each data point
the quadratic form in eq .
( 123 ) by its bottom nonzero eigenvectors .
figure 123 : summary of the lle algorithm , mapping high dimensional data points ,
methods in linear algebra .
the algorithm involves a single pass through the three steps in fig .
123 and nds global minima of the reconstruction and embedding costs in eqs .
( 123 ) and ( 123 ) .
as discussed in appendix a , in the unusual case where the
compute the weights
compute the vectors $ / best reconstructed by the weights
, to low dimensional embedding vectors , are chosen , the optimal weights
are computed by standard neighbors outnumber the input dimensionality , the least squares problem the algorithm , as described in fig .
123 , takes as input the .
in many settings , however , the user may not have access to data of this
form , but only to measurements of dissimilarity or pairwise distance between dif - ferent data points .
a simple variation of lle , described in appendix c , can be applied to input of this form .
in this way , matrices of pairwise distances can be analyzed by lle just as easily as mds ( 123 ) ; in fact only a small fraction of all pos - sible pairwise distances ( representing distances between neighboring points and their respective neighbors ) are required for running lle .
for nding the weights does not have a unique solution , and a regularization term for example , one that penalizes the squared magnitudes of the weightsmust be added to the reconstruction cost .
high dimensional vec -
the embeddings discovered by lle are easiest to visualize for intrinsically two
data points sampled off the s - shaped manifold .
the resulting embedding shows
dimensional manifolds .
in fig .
123 , for example , the input to lle consisted 123 : 123 neighbors per data point , successfully unraveled how the algorithm , using
the underlying two dimensional structure .
alize , the manifold of translated faces is highly nonlinear in the high dimensional
by contrast , the top portion shows the rst two components discovered by pca .
it is clear that the manifold structure in this example is much better modeled by lle .
123 shows another two dimensional manifold , this one living in a much higher dimensional space .
here , we generated examplesshown in the middle panel of the gureby translating the image of a single face across a larger background of random noise .
the noise was uncorrelated from one example to the next .
the only consistent structure in the resulting images thus described a two - dimensional manifold parameterized by the faces center of mass .
the input to lle consisted
<;123= grayscale images , with each image containing a123> background of noise .
note that while simple to visu - perimposed on a ? @123a; ) vector space of pixel coordinates .
the bottom portion of fig .
123 shows cb neighbors per data point .
the rst two components discovered by lle , with >123> color ( rgb ) images of lips at ea> heads ( 123 ) .
our database contained 123ag123= : 123 ) is useful for resolution .
dimensionality reduction of these images ( h : 123 ) .
rst two components discovered , respectively , by pca and lle ( with
if the lip images described a nearly linear manifold , these two methods would yield similar results; thus , the signicant differences in these embeddings reveal the presence of nonlinear structure .
note that while the linear projection by pca has a somewhat uniform distribution about its mean , the locally linear embedding has a distinctly spiny structure , with the tips of the spines corresponding to extremal congurations of the lips .
finally , in addition to these examples , for which the true manifold structure was known , we also applied lle to images of lips used in the animation of talking
faster and more efcient animation .
the top and bottom panels of fig .
123 show the
it is worth noting that many popular learning algorithms for nonlinear dimension - ality reduction do not share the favorable properties of lle .
iterative hill - climbing methods for autoencoder neural networks ( 123 , 123 ) , self - organizing maps ( 123 ) , and latent variable models ( 123 ) do not have the same guarantees of global optimality or con - vergence; they also tend to involve many more free parameters , such as learning rates , convergence criteria , and architectural specications .
the different steps of lle have the following complexities .
in step 123 , computing
nearest neighbors scales ( in the worst case ) asi
, and quadratically in the number of data points ,
, or linearly in the input
for many
figure 123 : the results of pca ( top ) and lle ( bottom ) , applied to images of a single face translated across a two - dimensional background of noise .
note how lle maps the images with corner faces to the corners of its two dimensional embedding , while pca fails to preserve the neighborhood structure of nearby images .
figure 123 : images of lips mapped into the embedding space described by the rst two coordinates of pca ( top ) and lle ( bottom ) .
representative lips are shown next to circled points in different parts of each space .
the differences between the two embeddings indicate the presence of nonlinear structure in the data .
set of linear equations for each data point .
in step 123 , computing
data distributions , however and especially for data distributed on a thin subman - ifold of the observation space constructions such as k - d trees can be used to time ( 123 ) .
in step 123 , computing the recon -
compute the neighbors ini struction weights scales asi to solve a the bottom eigenvectors scales asi dimensions , ! , and quadratically in the number of data points ,
methods for sparse eigenproblems ( 123 ) , however , can be used to reduce the complexity to sub - .
note that as more dimensions are added to the embedding space , the existing ones do not change , so that lle does not have to be rerun to compute higher dimensional embeddings .
the storage requirements of lle are limited by the weight matrix which is size n by k .
; this is the number of operations required , linearly in the number of embedding
lle illustrates a general principle of manifold learning , elucidated by tenenbaum et al ( 123 ) , that overlapping local neighborhoodscollectively analyzedcan pro - vide information about global geometry .
many virtues of lle are shared by the isomap algorithm ( 123 ) , which has been successfully applied to similar problems in nonlinear dimensionality reduction .
isomap is an extension of mds in which embeddings are optimized to preserve geodesic distances between pairs of data points; these distances are estimated by computing shortest paths through large sublattices of data .
a virtue of lle is that it avoids the need to solve large dy - namic programming problems .
lle also tends to accumulate very sparse matrices , whose structure can be exploited for savings in time and space .
lle is likely to be even more useful in combination with other methods in data analysis and statistical learning .
an interesting and important question is how to learn a parametric mapping between the observation and embedding spaces , given
a constrained least squares problem
amples for statistical models of supervised learning .
the ability to learn such map - pings should make lle broadly useful in many areas of information processing .
rq pairs as labeled ex - the results of lle .
one possible approach is to usep can be computed in closed form .
consider a particular data point
the constrained weights that best reconstruct each data point from its neighbors that sum to one .
we can write the
and reconstruction weightsu
reconstruction error as :
in terms of the inverse local covariance matrix , the
, and then to rescale
the solution , as written in eq .
( 123 ) , appears to require an explicit inversion of the local covariance matrix .
in practice , a more efcient way to minimize the error is
the weights so that they sum to one ( which yields the same result ) .
by construction , the local covariance matrix in eq .
( 123 ) is symmetric and semipositive denite .
if the covariance matrix is singular or nearly singularas arises , for example , when ) , or when the data points are not in general positionit can be conditioned ( before solving the system ) by adding a small multiple of the identity matrix ,
optimal weights are given by :
the constraint that simply to solve the linear system of equations , there are more neighbors than input dimensions ( 123g is small compared to the trace ofz the embedding vectors
b eigenvector problem
note that the cost denes a quadratic form ,
this amounts to penalizing large weights that exploit correlations beyond some level of precision in the data sam -
where in the rst identity , we have exploited the fact that the weights sum to one , and in the second identity , we have introduced the local covariance matrix ,
this error can be minimized in closed form , using a lagrange multiplier to enforce
$123 are found by minimizing the cost function , eq .
( 123 ) , for
is 123 if
requiring the coordinates to be centered on the origin :
this optimization is performed subject to constraints that make the problem well
involving inner products of the embedding vectors and the*| matrixz and 123 otherwise .
it is clear that the coordinates can be translated by a constant displace - ment without affecting the cost , % .
we remove this degree of freedom by
also , to avoid degenerate solutions , we constrain the embedding vectors to have unit covariance , with outer products that satisfy
identity matrix .
note that there is no loss in generality in to be diagonal and of order unity , since the cost function in eq .
( 123 ) is invariant to rotations and homogeneous rescalings .
the further constraint that the covariance is equal to the identity matrix expresses an assump - tion that reconstruction errors for different coordinates in the embedding space should be measured on the same scale .
the optimal embeddingup to a global rotation of the embedding spaceis found ; this is a version of the rayleitz - ritz theorem ( 123 ) .
the bottom eigenvector of this matrix , which we discard , is the unit vector with all equal components; it represents a free translation mode of eigenvalue zero .
discarding this eigenvector enforces the constraint that the embeddings have zero mean , since the components of other eigenvectors must
constraining the covariance of eigenvectors of the matrix , z by computing the bottom ! sum to zero , by virtue of orthogonality .
the remaining ! eigenvectors form the ! embedding coordinates found by lle .
eigenvectors of the matrixz note that the bottom ! eigenvalues ) can be found without performing a full matrix ing to its smallest ! diagonalization ( 123 ) .
moreover , the matrixz
can be stored and manipulated as the
( that is , those correspond -
sparse symmetric matrix
, both of never needs to be explicitly cre -
c lle from pairwise distances
in particular , left ( the subroutine required by most sparse eigensolvers ) can be
giving substantial computational savings for large values of
requiring just one multiplication by
and one multiplication by
which are extremely sparse .
thus , the matrixz ated or stored; it is sufcient to store and multiply the matrix
need to compute the local covariance matrixz ) ax between its nearest neighbors , ax denotes the squared distance between the th and th neighbors , ax .
in terms of this local covariance matrix , the
as dened by eq .
( 123 ) in appendix a .
this can be done by exploiting the usual relation between pairwise distances and dot products that forms the basis of metric mds ( 123 ) .
thus , for a particular data point , we set :
lle can be applied to user input in the form of pairwise distances .
in this case , nearest neighbors are identied by the smallest non - zero elements of each row in the distance matrix .
to derive the reconstruction weights for each data point , we
reconstruction weights for each data point are given by eq .
the rest of the algorithm proceeds as usual .
note that this variant of lle requires signicantly less user input than the com - plete matrix of pairwise distances .
instead , for each data point , the user needs only to specify its nearest neighbors and the submatrix of pairwise distances be - tween those neighbors .
is it possible to recover manifold structure from even less user inputsay , just the pairwise distances between each data point and its near - est neighbors ? a simple counterexample shows that this is not possible .
consider the square lattice of three dimensional data points whose integer coordinates sum to
imagine that points with evens - coordinates are colored black , and that points with odds - coordinates are colored red .
the two point embedding that maps all
black points to the origin and all red points to one unit away preserves the distance between each point and its four nearest neighbors .
nevertheless , this embedding completely fails to preserve the underlying structure of the original manifold .
the authors thank e .
cosatto , h . p .
graf , and y .
lecun ( at&t labs ) and b .
frey ( u .
toronto ) for providing data for these experiments .
roweis acknowledges the support of the gatsby charitable foundation , the national science foundation , and the national sciences and engineering research council of canada .
