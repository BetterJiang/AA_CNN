we derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines ( svms ) .
the updates have a simple closed form , and we prove that they converge monotoni - cally to the solution of the maximum margin hyperplane .
the updates optimize the traditionally proposed objective function for svms .
they do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration .
they can be used to adjust all the quadratic programming variables in parallel with a guarantee of im - provement at each iteration .
we analyze the asymptotic convergence of the updates and show that the coefcients of non - support vectors decay geometrically to zero at a rate that depends on their margins .
in practice , the updates converge very rapidly to good classiers .
support vector machines ( svms ) currently provide state - of - the - art solutions to many prob - lems in machine learning and statistical pattern recognition ( 123 ) .
their superior perfor - mance is owed to the particular way they manage the tradeoff between bias ( undertting ) and variance ( overtting ) .
in svms , kernel methods are used to map inputs into a higher , potentially innite , dimensional feature space; the decision boundary between classes is then identied as the maximum margin hyperplane in the feature space .
while svms pro - vide the exibility to implement highly nonlinear classiers , the maximum margin criterion helps to control the capacity for overtting .
in practice , svms generalize very well even better than their theory suggests .
computing the maximum margin hyperplane in svms gives rise to a problem in nonnega - tive quadratic programming .
the resulting optimization is convex , but due to the nonneg - ativity constraints , it cannot be solved in closed form , and iterative solutions are required .
there is a large literature on iterative algorithms for nonnegative quadratic programming in general and for svms as a special case ( 123 , 123 ) .
gradient - based methods are the simplest possible approach , but their convergence depends on careful selection of the learning rate , as well as constant attention to the nonnegativity constraints which may not be naturally enforced .
multiplicative updates based on exponentiated gradients ( eg ) ( 123 , 123 ) have been
investigated as an alternative to traditional gradient - based methods .
multiplicative updates are naturally suited to sparse nonnegative optimizations , but eg updateslike their addi - tive counterpartssuffer the drawback of having to choose a learning rate .
subset selection methods constitute another approach to the problem of nonnegative quadratic programming in svms .
generally speaking , these methods split the variables at each iteration into two sets : a xed set in which the variables are held constant , and a working set in which the variables are optimized by an internal subroutine .
at the end of each iteration , a heuristic is used to transfer variables between the two sets and improve the objective function .
an extreme version of this approach is the method of sequential minimal optimization ( smo ) ( 123 ) , which updates only two variables per iteration .
in this case , there exists an analytical solution for the updates , so that one avoids the expense of a potentially iterative optimization within each iteration of the main loop .
in general , despite the many proposed approaches for training svms , solving the quadratic programming problem remains a bottleneck in their implementation .
( some researchers have even advocated changing the objective function in svms to simplify the required optimization ( 123 , 123 ) . ) in this paper , we propose a new iterative algorithm , called multi - plicative margin maximization ( m123 ) , for training svms .
the m123 updates have a simple closed form and converge monotonically to the solution of the maximum margin hyper - plane .
they do not involve heuristics such as the setting of a learning rate or the switching between xed and working subsets; all the variables are updated in parallel .
they pro - vide an extremely straightforward way to implement traditional svms .
experimental and theoretical results conrm the promise of our approach .
123 nonnegative quadratic programming
we begin by studying the general problem of nonnegative quadratic programming .
con - sider the minimization of the quadratic objective function
f ( v ) =
vt av + bt v;
subject to the constraints that vi ( cid : 123 ) 123 123i .
we assume that the matrix a is symmetric and semipositive denite , so that the objective function f ( v ) is bounded below , and its opti - mization is convex .
due to the nonnegativity constraints , however , there does not exist an analytical solution for the global minimum ( or minima ) , and an iterative solution is needed .
123 multiplicative updates
our iterative solution is expressed in terms of the positive and negative components of the matrix a in eq .
in particular , let a+ and a ( cid : 123 ) denote the nonnegative matrices :
ij = ( cid : 123 ) aij
if aij > 123 ,
ij = ( cid : 123 ) jaij j
if aij < 123 ,
it follows trivially that a = a+ ( cid : 123 ) a ( cid : 123 ) .
in terms of these nonnegative matrices , our proposed updates ( to be applied in parallel to all the elements of v ) take the form :
vi ( cid : 123 ) vi " ( cid : 123 ) bi +pb123
i + 123 ( a+v ) i ( a ( cid : 123 ) v ) i
the iterative updates in eq .
( 123 ) are remarkably simple to implement .
their somewhat mys - terious form will be claried as we proceed .
let us begin with two simple observations .
first , eq .
( 123 ) prescribes a multiplicative update for the ith element of v in terms of the ith elements of the vectors b , a+v , and a+v .
second , since the elements of v , a+ , and a ( cid : 123 ) are nonnegative , the overall factor multiplying vi on the right hand side of eq .
( 123 ) is always nonnegative .
hence , these updates never violate the constraints of nonnegativity .
123 fixed points
we can show further that these updates have xed points wherever the objective func - tion , f ( v ) achieves its minimum value .
let v ( cid : 123 ) denote a global minimum of f ( v ) .
at such a point , one of two conditions must hold for each element v ( cid : 123 ) i > 123 and i = 123 and ( @f=@vi ) jv ( cid : 123 ) ( cid : 123 ) 123
the rst condition applies to the ( @f=@vi ) jv ( cid : 123 ) = 123 , or ( ii ) , v ( cid : 123 ) positive elements of v ( cid : 123 ) , whose corresponding terms in the gradient must vanish .
these derivatives are given by :
i : either ( i ) v ( cid : 123 )
= ( a+v ( cid : 123 ) ) i ( cid : 123 ) ( a ( cid : 123 ) v ( cid : 123 ) ) i + bi :
the second condition applies to the zero elements of v ( cid : 123 ) .
here , the corresponding terms of the gradient must be nonnegative , thus pinning v ( cid : 123 ) i to the boundary of the feasibility region .
the multiplicative updates in eq .
( 123 ) have xed points wherever the conditions for global minima are satised .
to see this , let
i + 123 ( a+v ( cid : 123 ) ) i ( a ( cid : 123 ) v ( cid : 123 ) ) i
denote the factor multiplying the ith element of v in eq .
( 123 ) , evaluated at v ( cid : 123 ) .
fixed points of the multiplicative updates occur when one of two conditions holds for each element vi : either ( i ) v ( cid : 123 ) i = 123
it is straightforward to show from eqs .
( 123 ) that ( @f=@vi ) jv ( cid : 123 ) = 123 implies ( cid : 123 ) i = 123
thus the conditions for global minima establish the conditions for xed points of the multiplicative updates .
i > 123 and ( cid : 123 ) i = 123 , or ( ii ) v ( cid : 123 )
123 monotonic convergence
the updates not only have the correct xed points; they also lead to monotonic improve - ment in the objective function , f ( v ) .
this is established by the following theorem :
theorem 123 the function f ( v ) in eq .
( 123 ) decreases monotonically to the value of its global minimum under the multiplicative updates in eq
the proof of this theorem ( sketched in appendix a ) relies on the construction of an auxil - iary function which provides an upper bound on f ( v ) .
similar methods have been used to prove the convergence of many algorithms in machine learning ( 123 , 123 , 123 , 123 , 123 , 123 ) .
123 support vector machines
we now consider the problem of computing the maximum margin hyperplane in svms ( 123 , 123 , 123 ) .
let f ( xi; yi ) gn i=123 denote labeled examples with binary class labels yi = ( cid : 123 ) 123 , and let k ( xi; xj ) denote the kernel dot product between inputs .
in this paper , we focus on the simple case where in the high dimensional feature space , the classes are linearly separable and the hyperplane is required to pass through the origin123
in this case , the maximum margin hyperplane is obtained by minimizing the loss function :
l ( ( cid : 123 ) ) = ( cid : 123 ) xi
subject to the nonnegativity constraints ( cid : 123 ) i ( cid : 123 ) 123
let ( cid : 123 ) ( cid : 123 ) denote the location of the minimum
of this loss function .
the maximal margin hyperplane has normal vector w = pi ( cid : 123 ) ( cid : 123 )
and satises the margin constraints yik ( w; xi ) ( cid : 123 ) 123 for all examples in the training set .
123the extensions to non - realizable data sets and to hyperplanes that do not pass through the origin
are straightforward .
they will be treated in a longer paper .
k = 123 ( cid : 123 ) = 123 : 123 k = 123 123% 123% 123% 123% 123% 123%
( cid : 123 ) = 123 : 123
( cid : 123 ) = 123 : 123
table 123 : misclassication error rates on the sonar and breast cancer data sets after 123 iterations of the multiplicative updates .
123 multiplicative updates
the loss function in eq .
( 123 ) is a special case of eq .
( 123 ) with aij = yiyjk ( xi; xj ) and bi = ( cid : 123 ) 123
thus , the multiplicative updates for computing the maximal margin hyperplane in hard margin svms are given by :
( cid : 123 ) i ( cid : 123 ) ( cid : 123 ) i " 123 +p123 + 123 ( a+ ( cid : 123 ) ) i ( a ( cid : 123 ) ( cid : 123 ) ) i
where a ( cid : 123 ) are dened as in eq .
we will refer to the learning algorithm for hard margin svms based on these updates as multiplicative margin maximization ( m123 ) .
it is worth comparing the properties of these updates to those of other approaches .
like multiplicative updates based on exponentiated gradients ( eg ) ( 123 , 123 ) , the m123 updates are well suited to sparse nonnegative optimizations123; unlike eg updates , however , they do not involve a learning rate , and they come with a guarantee of monotonic improvement .
like the updates for sequential minimal optimization ( smo ) ( 123 ) , the m123 updates have a simple closed form; unlike smo updates , however , they can be used to adjust all the quadratic programming variables in parallel ( or any subset thereof ) , not just two at a time .
finally , we emphasize that the m123 updates optimize the traditional objective function for svms; they do not compromise the goal of computing the maximal margin hyperplane .
123 experimental results
we tested the effectiveness of the multiplicative updates in eq .
( 123 ) on two real world prob - lems : binary classication of aspect - angle dependent sonar signals ( 123 ) and breast cancer data ( 123 ) .
both data sets , available from the uci machine learning repository ( 123 ) , have been widely used to benchmark many learning algorithms , including svms ( 123 ) .
the sonar and breast cancer data sets consist of 123 and 123 labeled examples , respectively .
train - ing and test sets for the breast cancer experiments were created by 123% / 123% splits of the
we experimented with both polynomial and radial basis function kernels .
the polynomial kernels had degrees k = 123 and k = 123 , while the radial basis function kernels had variances of ( cid : 123 ) = 123 : 123; 123 : 123 and 123 : 123
the coefcients ( cid : 123 ) i were uniformly initialized to a value of one in
misclassication rates on the test data sets after 123 iterations of the multiplicative updates are shown in table 123
as expected , the results match previously published error rates on these data sets ( 123 ) , showing that the m123 updates do in practice converge to the maximum margin hyperplane .
figure 123 shows the rapid convergence of the updates to good classiers in just one or two iterations .
123in fact , the multiplicative updates by nature cannot directly set a variable to zero .
however , a variable can be clamped to zero whenever its value falls below some threshold ( e . g . , machine precision ) and when a zero value would satisfy the karush - kuhn - tucker conditions .
figure 123 : rapid convergence of the multiplicative updates in eq .
the plots show results after different numbers of iterations on the breast cancer data set with the radial basis function kernel ( ( cid : 123 ) = 123 ) .
the horizontal axes index the coefcients ( cid : 123 ) i of the 123 training examples; the vertical axes show their values .
for ease of visualization , the training examples were ordered so that support vectors appear to the left and non - support vectors , to the right .
the coefcients ( cid : 123 ) i were uniformly initialized to a value of one .
note the rapid attenuation of non - support vector coefcients after one or two iterations .
intermediate error rates on the training set ( ( cid : 123 ) t ) and test set ( ( cid : 123 ) g ) are also shown .
123 asymptotic convergence
the rapid decay of non - support vector coefcients in fig .
123 motivated us to analyze their rates of asymptotic convergence .
suppose we perturb just one of the non - support vector coefcients in eq .
( 123 ) say ( cid : 123 ) iaway from the xed point to some small nonzero value ( cid : 123 ) ( cid : 123 ) i .
if we hold all the variables but ( cid : 123 ) i xed and apply its multiplicative update , then the new
i after the update is given asymptotically by ( ( cid : 123 ) ( cid : 123 ) 123
i ) ( cid : 123 ) ( ( cid : 123 ) ( cid : 123 ) i ) ( cid : 123 ) i , where
123 +p123 + 123 ( a+ ( cid : 123 ) ( cid : 123 ) ) i ( a ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ) i
and aij = yiyjk ( xi; xj ) .
( 123 ) is merely the specialization of eq .
( 123 ) to svms . ) we can thus bound the asymptotic rate of convergencein this idealized but instructive setting by computing an upper bound on ( cid : 123 ) i , which determines how fast the perturbed coefcient decays to zero .
( smaller ( cid : 123 ) i implies faster decay . ) in general , the asymptotic rate of con - vergence is determined by the overall positioning of the data points and classication hy - perplane in the feature space .
the following theorem , however , provides a simple bound in terms of easily understood geometric quantities .
feature space from xi to the maximum margin hyperplane , and let d = minj dj =
theorem 123 let di = jk ( xi; w ) j=pk ( w; w ) denote the perpendicular distance in the 123=pk ( w; w ) denote the one - sided margin of the classier .
also , let i =pk ( xi; xi )
denote the distance of xi to the origin in the feature space , and let = maxj j denote the largest such distance .
then a bound on the asymptotic rate of convergence ( cid : 123 ) i is given by :
( cid : 123 ) i ( cid : 123 ) ( cid : 123 ) 123 +
( di ( cid : 123 ) d ) d
figure 123 : quantities used to bound the asymptotic rate of convergence in eq .
( 123 ) ; see text .
solid circles denote support vectors; empty circles denote non - support vectors .
the proof of this theorem is sketched in appendix b .
figure 123 gives a schematic repre - sentation of the quantities that appear in the bound .
the bound has a simple geometric the more distant a non - support vector from the classication hyperplane , the faster its coefcient decays to zero .
this is a highly desirable property for large numeri - cal calculations , suggesting that the multiplicative updates could be used to quickly prune away outliers and reduce the size of the quadratic programming problem .
note that while the bound is insensitive to the scale of the inputs , its tightness does depend on their relative locations in the feature space .
svms represent one of the most widely used architectures in machine learning .
in this paper , we have derived simple , closed form multiplicative updates for solving the non - negative quadratic programming problem in svms .
the m123 updates are straightforward to implement and have a rigorous guarantee of monotonic convergence .
it is intriguing that multiplicative updates derived from auxiliary functions appear in so many other areas of machine learning , especially those involving sparse , nonnegative optimizations .
exam - ples include the baum - welch algorithm ( 123 ) for discrete hidden markov models , general - ized iterative scaling ( 123 ) and adaboost ( 123 ) for logistic regression , and nonnegative matrix factorization ( 123 , 123 ) for dimensionality reduction and feature extraction .
in these areas , simple multiplicative updates with guarantees of monotonic convergence have emerged over time as preferred methods of optimization .
thus it seems worthwhile to explore their full potential for svms .
