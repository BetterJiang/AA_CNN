in spite of their superior performance , neural probabilistic language models ( nplms ) re - main far less widely used than n - gram mod - els due to their notoriously long training times , which are measured in weeks even for moderately - sized datasets .
training nplms is computationally expensive because they are explicitly normalized , which leads to hav - ing to consider all words in the vocabulary when computing the log - likelihood gradients .
we propose a fast and simple algorithm for training nplms based on noise - contrastive estimation , a newly introduced procedure for estimating unnormalized continuous distri - butions .
we investigate the behaviour of the algorithm on the penn treebank corpus and show that it reduces the training times by more than an order of magnitude without af - fecting the quality of the resulting models .
the algorithm is also more ecient and much more stable than importance sampling be - cause it requires far fewer noise samples to
we demonstrate the scalability of the pro - posed approach by training several neural language models on a 123m - word corpus with a 123k - word vocabulary , obtaining state - of - the - art results on the microsoft research sentence completion challenge dataset .
appearing in proceedings of the 123 th international confer - ence on machine learning , edinburgh , scotland , uk , 123
copyright 123 by the author ( s ) / owner ( s ) .
by assigning probabilities to sentences , language mod - els allow distinguishing between probable and improb - able sentences , which makes such models an impor - tant component of speech recognition , machine trans - lation , and information retrieval systems .
probabilis - tic language models are typically based on the markov assumption , which means that they model the condi - tional distribution of the next word in a sentence given some xed number of words that immediately precede it .
the group of words conditioned on is called the context , denoted h , while the word being predicted is called the target word , denoted w .
n - gram models , which are eectively smoothed tables of normalized word / context co - occurrence counts , have dominated language modelling for decades due to their simplicity and excellent performance .
in the last few years neural language models have be - come competitive with n - grams and now routinely out - perform them ( mikolov et al . , 123 ) .
nplms model the distribution for the next word as a smooth function of learned multi - dimensional real - valued representa - tions of the context words and the target word .
similar representations are learned for words that are used in similar ways , ensuring that the network outputs simi - lar probability values for them .
word representations learned by language models are also used for natu - ral language processing applications such as semantic role labelling ( collobert & weston , 123 ) , sentiment analysis ( maas & ng , 123 ) , named entity recognition ( turian et al . , 123 ) , and parsing ( socher et al . , 123 ) .
unfortunately , nplms are very slow to train , which makes them unappealing for large - scale applications .
this is a consequence of having to consider the entire vocabulary when computing the probability of a single word or the corresponding gradient .
in fact , the time complexity of this computation scales as the product
a fast and simple algorithm for training neural probabilistic language models
of the vocabulary size and the word feature dimen - sionality .
one way to accelerate this computation is to reduce the vocabulary size for the nplm by using it to predict only the most frequent words and handling the rest using an n - gram model ( schwenk & gauvain ,
alternatively , the vocabulary can be structured into a tree with words at the leaves , allowing exponentially faster computation of word probabilities and their gra - dients ( morin & bengio , 123 ) .
unfortunately , the predictive performance of the resulting model is heav - ily dependent on the tree used and nding a good tree is a dicult problem ( mnih & hinton , 123 ) .
perhaps a more elegant approach is to keep the model the same and to approximate the expensive gradient computations using importance sampling ( bengio & senecal , 123 ) .
unfortunately , the variance in the im - portance sampling estimates can make learning unsta - ble , unless it is carefully controlled ( bengio & senecal ,
in this paper we propose an ecient algorithm for training nplms based on noise - contrastive estima - tion ( gutmann & hyvarinen , 123 ) , which is much more stable than importance sampling .
though it also uses sampling to approximate the gradients needed for learning , neither the number of samples nor the proposal distribution require dynamic adaptation for achieving performance on par with maximum likeli -
neural probabilistic language models
they use : neural language models learn their features jointly with other parameters , while maximum entropy models use xed hand - engineered features and only learn the weights for those features .
a neural language model represents each word in the vocabulary using a real - valued feature vector and denes the scoring func - tion in terms of the feature vectors of the context words and the next word .
in some models , dierent feature vector tables are used for the context and the next word vocabularies ( bengio et al . , 123 ) , while in oth - ers the table is shared ( bengio et al . , 123; mnih &
the feature vectors account for the vast majority of pa - rameters in neural language models , which means that their memory requirements are linear in the vocabu - lary size .
this compares favourably to the memory re - quirements of the n - gram models , which are typically linear in the training set size .
log - bilinear model
the training method we propose is directly applicable to all neural probabilistic and maximum - entropy lan - guage models .
for simplicity , we will perform our ex - periments using the log - bilinear language ( lbl ) model ( mnih & hinton , 123 ) , which is the simplest neu - ral language model .
the lbl model performs lin - ear prediction in the semantic word feature space and does not have non - linearities .
in spite of its simplic - ity , the lbl model has been shown to outperform n - grams , though the more complex neural language models ( mikolov et al . , 123; mnih et al . , 123 ) can
a statistical language model is simply a collection of conditional distributions for the next word , indexed by its context . 123 in a neural language model the condi - tional distribution corresponding to context h , p h ( w ) , is dened as
w ( cid : 123 ) exp ( s ( w ( cid : 123 ) , h ) )
where s ( w , h ) is the scoring function with parameters which quanties the compatibility of word w with context h .
the negated scoring function is sometimes referred to as the energy function ( bengio et al . , 123 ) .
depending on the form of s ( w , h ) , eq .
123 can de - scribe both neural and maximum entropy language models ( berger et al . , 123 ) .
the main dierence between these two model classes lies in the features
123though almost all statistical language models predict the next word , it is also possible to model the distribution of the word preceding the context or surrounded by the
in this paper we will use a slightly extended version of the lbl model that uses separate feature vector tables for the context words and the target words .
thus a context word w will be represented with feature vector rw , while a target word w will be represented with fea - ture vector qw .
given a context h , the model computes the predicted representation for the target word by linearly combining the feature vectors for the context words using position - dependent context weight matri -
the score for the match between the context and the next word is computed by taking the dot product be - tween the predicted representation and the represen - tation of the candidate target word w : s ( w , h ) = q ( cid : 123 ) qw + bw .
a fast and simple algorithm for training neural probabilistic language models
here bw is the base rate parameter used to model the popularity of w .
the probability of w in context h is then obtained by plugging the above score function into eq
maximum likelihood learning
maximum likelihood training of neural language mod - els is tractable but expensive because computing the log - likelihood takes time linear in the vocabulary size .
the contribution of a single con - text / word observation to the gradient of the log - likelihood is given by
log p h
s ( w , h ) ( cid : 123 )
s ( w , h ) ep h
( w ( cid : 123 ) ) is expensive to evaluate the expectation w . r . t .
p h because it requires computing s ( w , h ) for all words in the vocabulary .
since vocabularies typically con - tain tens of thousands of words , maximum likelihood learning tends to be very slow .
importance sampling
bengio and senecal ( 123 ) have proposed a method for speeding up training of neural language models based on approximating the expectation in eq .
123 us - ing importance sampling .
the idea is to generate k samples x123 , . . . , xk from an easy - to - sample - from distri - bution qh ( w ) and estimate the gradient with
log p h
s ( w , h ) 123
and v = ( cid : 123 ) k
where v ( x ) = exp ( s ( x , h ) ) j=123 v ( xj ) .
the normalization by v is necessary here because the im - portance weights v are computed using the unnormal - ized model distribution exp ( s ( x , h ) ) .
typically the proposal distribution is an n - gram model t to the training set , possibly with a context size dierent from the neural models .
though this approach is conceptually simple , it is non - trivial to use in practice because the high variance of the importance sampling estimates can make learn - ing unstable .
the variance tends to grow as learn - ing progresses , because the model distribution moves away from the proposal distribution . 123 one way to
123bengio and senecal ( 123 ) argue that this happens be -
control the variance is to keep increasing the number of samples during training so that the eective sam - ple size stays above some predetermined value ( ben - gio & senecal , 123 ) .
alternatively , the n - gram pro - posal distribution can be adapted to track the model distribution throughout training ( bengio & senecal , 123 ) .
the rst approach is simpler but less e - cient because the increasing number of samples makes learning slower .
the second approach leads to greater speedups but is considerably more dicult to imple - ment and requires additional memory for storing the adaptive proposal distribution .
noise - contrastive estimation
we propose using noise - contrastive estimation ( nce ) as a more stable alternative to importance sampling for ecient training of neural language models and other models dened by eq .
nce has recently been in - troduced by gutmann and hyvarinen ( 123 ) for train - ing unnormalized probabilistic models .
though it has been developed for estimating probability densities , we are interested in applying it to discrete distributions and so will assume discrete distributions and use prob - ability mass functions instead of density functions .
the basic idea of nce is to reduce the problem of density estimation to that of binary classication , dis - criminating between samples from the data distribu - tion and samples from a known noise distribution .
in the language modelling setting , the data distribution d ( w ) will be the distribution of words that occur af - ter a particular context h .
though it is possible to use context - dependent noise distributions , for simplicity we will use a context - independent ( unigram ) pn ( w ) .
we are interested in tting the context - dependent model p h
( w ) to p h
following gutmann and hyvarinen ( 123 ) , we assume that noise samples are k times more frequent than data samples , so that datapoints come from the mixture k+123 p h k+123 pn ( w ) .
then the posterior probability that sample w came from the data distribution is
d ( w ) + k
p h ( d = 123|w ) =
d ( w ) + kpn ( w )
since we would like to t p h place of p h a function of the model parameters :
d in eq .
123 , making the posterior probability
d , we use p h
to p h
p h ( d = 123|w , ) =
( w ) + kpn ( w )
cause neural language models and n - gram models learn very dierent distributions .
a fast and simple algorithm for training neural probabilistic language models
this quantity can be too expensive to compute , how - ever , because of the normalization required for eval - uating p h ( w ) ( eq .
nce sidesteps this issue by avoiding explicit normalization and treating normal - ization constants as parameters .
thus the model is parameterized in terms of an unnormalized distribu - tion p h123 123 and a learned parameter ch corresponding to the logarithm of the normalizing constant :
( w ) = p h123
123 ( w ) exp ( ch ) .
here 123 are the parameters of the unnormalized dis - tribution and = ( 123 , ch ) .
to t the context - dependent model to the data ( for the moment ignoring the fact that it shares parameters with models for other contexts ) , we simply maximize the expectation of log p h ( d|w , ) under the mixture of the data and noise samples .
this leads to the objective
( xi ) +kpn ( xi ) are always be - note that the weights tween 123 and 123 , which makes nce - based learning very stable ( gutmann & hyvarinen , 123 ) .
in contrast , the weights produced by importance sampling can be ar -
since the distributions for dierent contexts share pa - rameters , we cannot learn these distributions indepen - dently of each other by optimizing one j h ( ) at a time .
instead , we dene a global nce objective by combin - ing the per - context nce objectives using the empirical context probabilities p ( h ) as weights :
p ( h ) j h ( ) .
note that this is the traditional approach for combin - ing the per - context ml objectives for training neural
dealing with normalizing constants
our initial implementation of nce training learned a ( log - ) normalizing constant ( c in eq .
123 ) for each con - text in the training set , storing them in a hash table indexed by the context . 123 though this approach works well for small datasets , it requires estimating one pa - rameter per context , making it dicult to scale to huge numbers of observed contexts encountered by models with large context sizes .
surprisingly , we discovered that xing the normalizing constants to 123 , 123 instead of learning them , did not aect the performance of the resulting models .
we believe this is because the model has so many free parameters that meeting the approxi - mate per - context normalization constraint encouraged by the objective function is easy .
log p h
potential speedup
j h ( ) =ep h
( w ) + kpn ( w )
( w ) + kpn ( w )
with the gradient
j h ( ) =ep h
( w ) + kpn ( w )
log p h
( w ) + kpn ( w )
log p h
note that the gradient can also be expressed as
j h ( ) =
( w ) + kpn ( w ) d ( w ) p h
and that as k ,
j h ( ) ( cid : 123 )
d ( w ) p h
log p h
which is the maximum likelihood gradient .
thus as the ratio of noise samples to observations increases , the nce gradient approaches the maximum likelihood
in practice , given a word w observed in context h , we compute its contribution to the gradient by generating k noise samples x123 , . . . , xk and using the formula
j h , w ( ) =
( w ) + kpn ( w )
( xi ) + kpn ( xi )
log p h
log p h
we will now compare the gradient computation costs for nce and ml learning .
suppose c is the context size , d is the word feature vector dimensionality , and v is the vocabulary size of the model .
then com - puting the predicted representation using eq .
123 takes about cd123 operations for both nce and ml .
for ml , computing the distribution of the next word from the predicted representation takes about v d operations .
for nce , evaluating the probability of k noise samples under the model takes about kd operations .
since the gradient computation in each model has the same com - plexity as computing the probabilities , the speedup for
123we did not use the learned normalizing constants when computing the validation and test set perplexities .
rather we normalized the probabilities explicitly .
123this amounts to setting the normalizing parameters c
a fast and simple algorithm for training neural probabilistic language models
table 123
results for the lbl model with 123d feature vec - tors and a 123 - word context on the penn treebank corpus .
number of test training
table 123
the eect of the noise distribution and the num - ber of noise samples on the test set perplexity .
each parameter update due to using nce is about
cd123 + v d cd123 + kd
cd + v cd + k
for a model with a 123 - word context , 123d feature vec - tors , and a vocabulary size of 123k , an nce update using 123 noise samples should be about 123 times faster than an ml update .
since the time complexity of computing the predicted representation is quadratic in the feature vector di - mensionality , it can dominate the cost of the parameter update , making learning slow even for a small number of noise samples .
we can avoid this by making context weight matrices ci diagonal , reducing the complexity of computing the predicted representation to cd , and making the speedup factor c+v c+k .
for the model above this would amount to a factor of 123
the use of di - agonal context matrices was introduced by mnih & hinton ( 123 ) to speed up their hierarchical lbl - like
since the cost of a parameter update for importance - sampling - based learning is the same as for nce with the same number of noise samples , the algorithm that needs fewer samples to perform well will be faster .
penn treebank results
we investigated the properties of the proposed algo - rithm empirically on the penn treebank corpus .
as is common practice , we trained on sections 123 - 123 ( 123k
words ) and used sections 123 - 123 ( 123k words ) as the val - idation set and sections 123 - 123 ( 123k words ) as the test set .
the standard vocabulary of 123k most frequent words was used with the remaining words replaced by a special token .
we chose to use this dataset to keep the training time for exact maximum likelihood learn -
the learning rates were adapted at the end of each epoch based on the change in the validation set per - plexity since the end of the previous epoch .
the rates were halved when the perplexity increased and were left unchanged otherwise .
parameters were updated based on mini - batches of 123 context / word pairs each .
except when stated otherwise , nce training generated 123 noise samples from the empirical unigram distribution per context / word observation .
noise sam - ples were generated anew for each update .
we did not use a weight penalty as the validation - score - based learning rate reduction appeared to be sucient to avoid overtting .
all models used a two - word context and dierent 123d feature vector tables for context and target words .
our rst experiment compared ml learning to nce learning for various numbers of noise samples .
the re - sulting test perplexities and training times are shown in table 123
it is clear that increasing the number of noise samples produces better - performing models , with 123 samples being sucient to match the ml - in terms of training time , nce was 123 times faster than ml .
the number of noise sam - ples did not have a signicant eect on the running time because computing the predicted representation was considerably more expensive than computing the probability of ( at most ) 123 samples .
the main rea - son the speedup factor was less than 123 ( the value pre - dicted in section 123 ) is because nce took about twice as many epochs as ml to converge .
our nce imple - mentation is also less optimized than the ml imple - mentation which takes greater advantage of the blas
to explore the eect of the noise distribution on the performance of the algorithm , we tried generating noise samples from the unigram as well as the uniform distribution .
for each noise distribution we trained models using 123 , 123 , 123 , and 123 noise samples per dat - apoint .
as shown in table 123 , the unigram noise dis - tribution leads to much better test set perplexity in all cases .
however , the perplexity gap shrinks as the number of noise samples increases , from almost 123 for a single noise sample down to under 123 for 123 noise samples .
in spite of poor test set performance , a uniform noise distribution did not lead to learning
a fast and simple algorithm for training neural probabilistic language models
instability even when a single noise sample was used .
in addition to the ml and nce algorithms , we also implemented the importance sampling training algo - rithm from ( bengio & senecal , 123 ) to use as a base - line , but found it very unstable .
it diverged in virtually all of our experiments , even with adaptive sample size and the target eective sample size set to hundreds .
the only run that did not diverge involved learning a unigram model using the target unigram as the pro - posal distribution , which is the ideal situation for im - portance sampling .
the cause of failure in all cases was the appearance of extremely large importance weights once the model distribution became suciently dier - ent from the unigram proposal distribution123 , which is a known problem with importance sampling .
since is - based methods seem to require well over a hundred samples per gradient computation ( bengio & senecal , 123 ) , even when an adaptive proposal distribution is used , we expect is - based training to be consider - ably slower than nce , which , as we have shown , can achieve ml - level performance with only 123 noise sam -
sentence completion challenge
to demonstrate the scalability and eectiveness of our approach we used it to train several large neu - ral language models for the microsoft research sen - tence completion challenge ( zweig & burges , 123 ) .
the challenge was designed as a benchmark for seman - tic models and consists of sat - style sentence comple - tion problems .
given 123 , 123 sentences , each of which is missing a word , the task is to select the correct word out of the ve candidates provided for each sentence .
candidate words have been chosen from relatively in - frequent words using a maximum entropy model to ensure that the resulting complete sentences were not too improbable .
human judges then picked the best four candidates for each sentence so that all comple - tions were grammatically correct but the correct an - swer was unambiguous .
though humans can achieve over 123% accuracy on the task , statistical models fare much worse with the best result of 123% produced by a whole - sentence lsa model , and n - gram models achieving only about 123% accuracy ( zweig & burges ,
neural language models are a natural choice for this task because they can take advantage of larger con - texts than traditional n - gram models , which we expect
123though using a unigram proposal distribution might appear naive , bengio and senecal ( 123 ) reported that higher - order n - gram proposal distributions worked much worse than the unigram .
to be important for sentence completion .
we used a slightly modied lbl architecture for our models for this task .
in the interests of scalability , we used diag - onal context weight matrices which reduced the time complexity of gradient computations from quadratic to linear in the dimensionality of word feature vectors and allowed us to use more feature dimensions .
since the task was sentence completion , we made the mod - els aware of sentence boundaries by using a special out - of - sentence token for words in context positions outside of the sentence containing the word being pre - dicted .
for example , this token would be used as the context word when predicting the rst word in a sen - tence using a model with a single - word context .
we score a candidate sentence with a language model by using it to compute the probability of each word in the sentence and taking the product of those probabil - ities as the sentence score .
we then pick the candidate word that produces the highest - scoring sentence as our answer .
note that this way of using a model with a c - word context takes into account c words on both sides of the candidate word because the probabilities of the c words following the candidate word depend on it .
the models were trained on the standard training set for the challenge containing 123 works from project gutenberg .
after removing the project gutenberg headers and footers from the les , we split them into sentences and then tokenized the sentences into words .
we used the punkt sentence tokenizer and the penn treebank word tokenizer from nltk ( bird et al . , 123 ) .
we then converted all words to lowercase and replaced the ones that occurred fewer than 123 times with an unknown word token , resulting in a vocab - ulary size of just under 123 , 123
the sentences to be completed were preprocessed in the same manner .
the resulting dataset was then randomly split at the sen - tence level into a test and validation sets of 123k words ( 123 sentences ) each and a 123m - word training set .
we used the training procedure described in section 123 , with the exception of using a small weight penalty to avoid overtting .
each model took between one and two days to train on a single core of a modern cpu .
as a baseline for comparison , we also trained several n - gram models ( with modied kneser - ney smoothing ) using the srilm toolkit ( stolcke , 123 ) , obtaining results similar to those reported by zweig & burges
since we selected hyperparameters based on the ( gutenberg ) validation set perplexity , we report the scores on the entire collection of 123 , 123 sentences , which means that our results are directly comparable to those of zweig & burges ( 123 ) .
as can be seen from ta -
a fast and simple algorithm for training neural probabilistic language models
table 123
accuracy on the complete msr sentence com - pletion challenge dataset .
n 123 indicates a bidirectional context .
the lsa result is from ( zweig & burges , 123 ) .
context latent percent test
ble 123 , more word features and larger context leads to better performance in lbl models in terms of both ac - curacy and perplexity .
the lbl models perform con - siderably better on sentence completion than n - gram models , in spite of having higher test perplexity .
even the lbl model with a two - word context performs bet - ter than any n - gram model .
the lbl model with a ve - word context , matches the best published result on the dataset .
note that the lsa model that pro - duced that result considered all words in a sentence , while an lbl model with a c - word contexts considers only the 123c words that surround the candidate word .
the model with a 123 - word context and 123d feature vectors outperforms the lsa model by a large mar - gin and sets a new accuracy record for the dataset at
language models typically use the words preceding the word of interest as the context .
however , since we are interested in lling in a word in the middle of the sentence , it makes sense to use both the preceding and the following words as the context for the lan - guage model , making the context bidirectional .
we trained several lbl models with bidirectional con - text to see whether such models are superior to their unidirectional - context counterparts for sentence com - pletion .
scoring a sentence with a bidirectional model is both simpler and faster : we simply compute the probability of the candidate word under the model us - ing the context surrounding the word .
thus a model is applied only once per sentence , instead of c+123 times required by the unidirectional models .
as table 123 shows , the lbl models with bidirectional
contexts achieve much lower test perplexity than their unidirectional counterparts , which is not surprising be - cause they also condition on words that follow the word being predicted .
what is surprising , however , is that bidirectional contexts appear to be consider - ably less eective for sentence completion than unidi - rectional contexts .
though the c - word context model and c 123 - word context model look at the same words when using the scoring procedures we described above , the unidirectional model seems to make better use of the available information .
we have introduced a simple and ecient method for training statistical language models based on noise - contrastive estimation .
our results show that the learning algorithm is very stable and can produce mod - els that perform as well as the ones trained using maxi - mum likelihood in less than one - tenth of the time .
in a large - scale test of the approach , we trained several neu - ral language models on a collection of project guten - berg texts , achieving state - of - the - art performance on the microsoft research sentence completion chal -
though we have shown that the unigram noise distri - bution is sucient for training neural language models eciently , context - dependent noise distributions are worth investigating because they might lead to even faster training by reducing the number of noise sam -
recently , pihlaja et al .
( 123 ) introduced a family of estimation methods for unnormalized models that in - cludes nce and importance sampling as special cases .
other members of this family might be of interest for training language models , though our preliminary re - sults suggest that none of them outperform nce .
finally , we believe that nce can be applied to many models other than neural or maximum - entropy lan - guage models .
probabilistic classiers with many classes are a prime candidate .
we thank vinayak rao and lloyd elliot for their help - ful comments .
we thank the gatsby charitable foun - dation for generous funding .
