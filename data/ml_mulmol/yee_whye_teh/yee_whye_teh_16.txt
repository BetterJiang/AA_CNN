we show quite good face clustering is possible for a dataset of inaccurately and ambiguously labelled face images .
our dataset is 123 , 123 face images , obtained by applying a face nder to approximately half a million captioned news im - ages .
this dataset is more realistic than usual face recog - nition datasets , because it contains faces captured in the wild in a variety of congurations with respect to the cam - era , taking a variety of expressions , and under illumina - tion of widely varying color .
each face image is associated with a set of names , automatically extracted from the as - sociated caption .
many , but not all such sets contain the
we cluster face images in appropriate discriminant co - ordinates .
we use a clustering procedure to break ambigu - ities in labelling and identify incorrectly labelled faces .
a merging procedure then identies variants of names that re - fer to the same individual .
the resulting representation can be used to label faces in news images or to organize news pictures by individuals present .
an alternative view of our procedure is as a process that cleans up noisy supervised data .
we demonstrate how to use entropy measures to evaluate such procedures .
it is straightforward to obtain enormous datasets of images , with attached annotations .
examples include : collections of museum material ( 123 ) ; the corel collection of images; any video with sound or closed captioning; images collected from the web with their enclosing web pages; or captioned
exploiting partially supervised data is a widely stud - ied theme in vision research .
image regions may usefully and fairly accurately be linked with words , even though the words are not linked to the regions in the dataset originally ( 123 , 123 ) .
for example , models based around templates and re - lations may be learned from a dataset of motorcycle images
where one never species where in the image the motorcy - cle lies ( for faces , see ( 123 , 123 ) ; for animals in static images and in video , see ( 123 , 123 ) ; for a range of objects , see ( 123 ) ) .
in this paper , we show that faces and names can be linked in an enormous dataset , despite errors and ambiguities in proper name detection , in face detection and in correspondence .
face recognition is well studied , and cannot be surveyed reasonably in the space available .
early face recognition is done in ( 123 , 123 ) and is reviewed in ( 123 , 123 , 123 ) .
our problem is slightly different from face recognition , in that it is more important to identify discriminant coordinates which can be used to distinguish between faces , even for individuals not represented in the dataset than to classify the faces .
as a result , we focus on adopting the kpca / lda method - ology , rather than on building a multi - class classier .
our current work is a necessary precursor to real world face recognition machinery : building large and realistic sets of labelled data for recognition .
we can leverage past work by using it to determine what features might be useful for identifying similar faces .
the general approach involves using unambiguously labelled data items to estimate discriminant coordinates
( section 123 ) .
we then use a version of - means to allocate
ambiguously labelled faces to one of their labels ( section 123 ) .
once this is done , we clean up the clusters by removing data items far from the mean , and re - estimate discriminant coor - dinates ( section 123 ) .
finally , we merge clusters based on facial similarities ( section 123 ) .
we show qualitative and quantitative results in section 123
we have collected a dataset consisting of approximately half a million news pictures and captions from yahoo news over a period of roughly two years .
faces : using the face detector of ( 123 ) we extract 123 , 123 face images ( size 123x123 or larger with sufcient face de - tection scores and resized to 123x123 pixels ) .
since these pictures were taken in the wild rather than under xed
name frequencies have the long tails that occur in natu - ral language problems .
we expect that face images roughly follow the same distribution .
we have hundreds to thou - sands of images of a few individuals ( e . g .
president bush ) , and a large number of individuals who appear only a few times or in only one picture .
one expects real applications to have this property .
for example , in airport security cam - eras a few people , security guards , or airline staff might be seen often , but the majority of people would appear infre - quently .
studying how recognition systems perform under such a distribution is important .
the sheer volume of available data is extraordinary .
we have sharply reduced the number of face images we deal with by using a face detector that is biased to frontal faces and by requiring that faces be large and rectify properly .
even so , we have a dataset that is comparable to , or larger than , the biggest available lab sets and is much richer in content .
computing kernel pca and linear discriminants for a set this size requires special techniques ( section 123 ) .
before comparing images , we automatically rectify all faces to a canonical pose .
five support vector machines are trained as feature detectors ( corners of the left and right eyes , corners of the mouth , and the tip of the nose ) using 123 hand clicked faces .
we use the geometric blur of ( 123 ) applied to grayscale patches as the features for our svm .
a new face is tested by running each svm over the image with a weak prior on location for each feature .
we compute an afne transformation dened by the least squares solu - tion between maximal outputs of each svm and canonical feature locations .
we then perform gradient descent to nd the afne transformation which best maps detected points to canonical feature locations .
each image is then rectied to a common pose and assigned a score based on the sum of its feature detector responses .
larger rectication scores indicate better feature detec - tion and therefore better rectication .
we lter our dataset by removing images with poor rectication scores and are left with 123 , 123 face images .
each face is automatically cropped to a region surrounding the eyes , nose and mouth to eliminate effects of background on recognition .
the rgb pixel values from each cropped face are concatenated into a vector and used from here on .
discriminant analysis we perform kernel principal components analysis ( kpca ) to reduce the dimensionality of our data and linear discrimi - nant analysis ( lda ) to project data into a space that is suited for the discrimination task .
kernel principal components analysis : kernel pca ( 123 ) uses a kernel function to efciently compute a princi - pal component basis in a high - dimensional feature space ,
figure 123 : top row shows face detector results , bottom row shows images returned by the face rectier with rectication score shown center ( larger scores indicate better rectica - tion performance ) .
the face rectier uses an svm to detect feature points on each face .
gradient descent is then used to nd the best afne transformation to map feature points to canonical locations .
laboratory conditions , they represent a broad range of indi - viduals , pose , expression and illumination conditions .
in - dividuals also change over time , which has been shown to hamper recognition in ( 123 ) .
our face recognition dataset is more varied than any other to date .
names : we extract a lexicon of proper names from all the captions by identifying two or more capitalized words followed by a present tense verb ( ( 123 ) ) .
words are classied as verbs by rst applying a list of morphological rules to present tense singular forms , and then comparing these to a database of known verbs ( wordnet ( 123 ) ) .
this lexicon is matched to each caption .
each face detected in an image is associated with every name extracted from the associated caption ( e . g .
our job is to label each face detector response with the correct name ( if one exists ) .
scale : we obtain 123 , 123 large and reliable face detector responses .
we reject face images that cannot be rectied satisfactorily , leaving 123 , 123
finally , we concentrate on images associated with 123 or fewer names , leaving 123 , 123
123 distinctive properties performance gures reported in the vision literature are inconsistent with experience of deployed face recognition systems ( e . g . , see ( 123 ) ) .
this suggests that lab datasets lack important phenomena .
our dataset differs from typical face recognition datasets in a number of important ways .
pose , expression and illumination vary widely .
the face detector tends not to detect lateral views of faces , but ( gure 123 ) we often encounter the same face illuminated with markedly different colored light and in a broad range of ex - pressions .
spectacles and mustaches are common .
there are wigs , images of faces on posters , differences in reso - lution and identikit pictures .
quite often there are multi - ple copies of the same picture ( this is due to the way news pictures are prepared , rather than a collecting problem ) or multiple pictures of the same individual in similar congu - rations .
finally , some individuals are tracked across time .
president george w .
bush makes a state - ment in the rose garden while secretary of defense donald rumsfeld looks on , july 123 , 123
rumsfeld said the united states would release graphic photographs of the dead sons of saddam hussein to prove they were killed by american troops .
photo by larry down -
british director sam mendes and his part - ner actress kate winslet arrive at the london premiere of the road to perdition , septem - ber 123 , 123
the lms stars tom hanks as a chicago hit man who has a separate fam - ily life and co - stars paul newman and jude law .
reuters / dan chung
incumbent california gov .
gray davis ( news - web sites ) leads republican chal - lenger bill simon by 123 percentage points although 123 percent of voters are still unde - cided , according to a poll released october 123 , 123 by the public policy institute of cal - ifornia .
davis is shown speaking to reporters after his debate with simon in los angeles , on oct .
( jim ruymen / reuters )
world number one lleyton hewitt of aus - tralia hits a return to nicolas massu of chile at the japan open tennis championships in tokyo october 123 , 123
reuters / eriko
german supermodel claudia schiffer gave birth to a baby boy by caesarian section january 123 , 123 , her spokeswoman said .
the baby is the rst child for both schif - fer , 123 , and her husband , british lm pro - ducer matthew vaughn , who was at her side for the birth .
schiffer is seen on the ger - man television show bet it . . . ? ! dass . . . ? ! ) in braunschweig , on january 123 , 123
( alexandra winkler / reuters ) us president george w .
bush ( l ) makes re - marks while secretary of state colin pow - ell ( r ) listens before signing the us leader - ship against hiv / aids , tuberculosis and malaria act of 123 at the department of state in washington , dc .
the ve - year plan is designed to help prevent and treat aids , especially in more than a dozen african and caribbean nations ( afp / luke frazza )
figure 123 : given an input image and an associated caption ( images above and captions to the right of each image ) , our system automatically detects faces ( white boxes ) in the image and possible name strings ( bold ) .
we use a clustering procedure to build models of appearance for each name and then automatically label each of the detected faces with a name if one exists .
these automatic labels are shown in boxes below the faces .
multiple faces may be detected and multiple names may be extracted , meaning we must determine who is who ( e . g . , the picture of cluadia schiffer ) .
related to the input space by some nonlinear map .
ker - nel pca has been shown to perform better than pca at face recognition ( 123 ) .
kernel pca is performed as follows : compute a kernel matrix , k , where is the value of the kernel function comparing ( we use a gaussian kernel ) .
center the kernel matrix in feature space ( by subtracting off average row , average column and adding on average element values ) .
compute an eigendecomposi - tion of k , and project onto the normalized eigenvectors of
linear discriminant analysis : lda has been shown to work well for face discrimination ( 123 , 123 , 123 ) because it uses class information to nd a set of discriminants that push means of different classes away from each other .
123 nystrom approximation our dataset is too large to do kpca directly as the kernel matrix , k will be of size nxn , where n is the the num - ber of images in the dataset , and involve approximately image comparisons .
therefore , we instead use an ap - proximation to calculate the eigenvectors of k .
incomplete cholesky decomposition ( icd ( 123 ) ) can be used to calcu - late an approximation to k with a bound on the approxima - tion error , but involves accessing all n images for each col - umn computation ( where n is the number of images in the
dataset ) .
the nystrom approximation method ( cf ( 123 , 123 ) ) gives a similar result , but allows the images to be accessed in a single batch rather than once for each column com - putation ( thereby being much faster to compute for large matrices ) .
nystrom does not give the same error bound on its approximation to k .
however , because of the smoothing properties of kernel matrices we expect the number of large eigenvalues of our matrix to be small , where the number of large eigenvalues should go down relative to the amount of smoothing .
in our matrix we observed that the eigenvalues do tend to drop off quickly .
because of this , a subset of the columns of our matrix , should encode much of the data in the matrix .
this implies that the nystrom method may provide a good approximation to k .
the nystrom method computes two exact subsets of k , a and b , and uses these to approximate the rest of k .
using this approximation of k , the eigenvectors can be approxi -
first the n
n kernel matrix , k , is partitioned as
, ! ' / ) 123+123 - . + and$ : ' / ) 123;123<123+123 - 123=+123 .
here , a is a subset of the images , ( in our case 123 ran -
z _ udolph
aczg om tr@g123g=msoqp
acc123p dam hussein
figure 123 : the gure shows a representative set of clusters from our largest threshold series , illustrating a series of important properties of both the dataset and the method .
note that this picture greatly exaggerates our error rate in order to show interesting phenomena and all the types of error we encounter .
123 : some faces are very frequent and appear in many different expressions and poses , with a rich range of illuminations ( e . g .
clusters labelled state colin powell , or donald rumsfeld ) .
these clusters also demonstrate that our clusterer can cope with these phenomena .
123 : some faces are rare , or appear in either repeated copies of one or two pictures or only slightly different pictures ( e . g .
cluster labelled abu sayyaf or actress jennifer aniston ) .
123 : some faces are not , in fact , photographs ( ali imron ) .
123 : the association between proper names and faces is still somewhat noisy , because it remains difcult to tell which strings are names of persons ( united nations , which shows three separate face phenomena that co - occur frequently with this string; justice department , which is consistent as to face but not the name of a person; and president daniel arap moi or john paul , which show a names associated with the wrong face ) .
123 : some names are genuinely ambiguous ( james bond , which shows two different faces naturally associated with the name ( the rst is an actor who played james bond , the second an actor who was a villain in a james bond lm ) .
123 : some faces appear at both low and reasonable resolution ( saddam hussein ) .
123 : our cluster merging process is able to merge clusters depicting the same face but labelled with distinct strings ( the clusters in the light gray and dark gray polygons , respectively ) .
123 : our cluster merging process is not perfect and could benet from deeper syntactic knowledge of names ( eric rudolph and eric robert rudolph ) , but in cases such as defense secretary rumsfeld , donald rumsfeld and defense donald rumsfeld the mergings produced are correct .
123 : our clustering is quite resilient in the presence of spectacles ( hans blix ) , perhaps wigs ( john bolton ) and mustaches ( john bolton ) .
state colin powell
defense donald rumsfeld donald rumsfeld
table 123 : multiple names can often refer to the same person .
we link names to people based on images .
if two names have the same associated face , then they must refer to the same person .
the above pairs are the top name merges pro - posed by our system .
merges are proposed between two names if the clusters referring to each name contain similar
domly selected images ) compared to themselves , b is the comparison of each of the images of a , to the rest of the images in our dataset , and c is approximated by the nystrom method .
nystrom gives an approximation for c .
this gives an approximation to k ,
then we form
, the centered version of our approxima - , by calculating approximate average row , average column sums ( these are equal since k is symmetric ) , and average element values .
we can approximate the average row ( or column ) sum as :
is diagonalized by :
we center as usual ,
we then solve for the orthogonalized approximate eigen - vectors as follows .
first , we replace a and b by their
centered versions .
let* be the square root of a , and .
diagonalize s as
we proceed as usual for kpca , this decomposition of by normalizing the eigenvectors the normalized eigenvectors .
this gives a dimensionality reduction of our images that makes the discrimination task
then we have
we view our collection as a semi - supervised dataset with errors that we wish to clean up .
first we form discrim - inants from faces with only one common extracted name .
while this is a fairly small set of faces and the labels are not perfect , they let us form an initial discriminant space .
we project all of our images into this space and perform a mod - ied k - means procedure for clustering .
this gives a larger and less noisy dataset from which to recompute discrimi - nants .
these new discriminants give a better representation of identity and we use them to re - cluster .
this gives a reli - able set of clusters .
123 modied k - means clustering each image has an associated vector , given by the kpca and lda processes , and a set of extracted names ( those words extracted using our proper name detector from the
the clustering process works as follows : 123
randomly assign each image to one of its extracted names 123
for each distinct name ( cluster ) , calculate the mean of image vec - tors assigned to that name .
reassign each image to the closest mean of its extracted names .
repeat 123 - 123 until convergence ( i . e .
no image changes names during an itera - 123 pruning clusters we use a nearest neighbor model to describe our dataset and throw out points that have a low probability under this model .
we remove clusters with fewer than three images so that nearest neighbor has some meaning .
this leaves 123 , 123 images .
we then remove points with low likelihood for a variety of thresholds ( table 123 ) to get error rates as low as 123% , ( error rates are calculated by hand labelling pictures from our dataset ) .
we dene likelihood as the ratio of the probability that it came from its assigned cluster over the probability that it did not come from its assigned cluster :
where for a point x in cluster
is the number of points in cluster
, k is the number of near - is the number of those , n is the total number of points in the .
we are us - as the estimated probability of a cluster
est neighbors we are considering ,
neighbors that are in gives more weight to smaller clusters .
123 merging clusters we would like to merge clusters with different names that actually correspond to a single person such as defense don - ald rumsfeld and donald rumsfeld or venezuelan president hugo chavez and hugo chavez .
this can be extremely hard to do directly from text , in situations such as colin powell
table 123 : dataset sizes , number of clusters and error rates for different thresholds .
the number of people in each set is approximately the number of clusters and error rate is de - ned as : given an individual and a face from their associ - ated cluster , the error rate is the probability of that face be - ing incorrect .
we see that for mid - level pruning we get quite small error rates .
we have turned a large semi - supervised set of faces into a well supervised set and produced clean clusters of many people .
and secretary of state ( the names do not share any words ) .
we propose to merge names that correspond to faces that look the same .
our system automatically propose merges between clusters that have similar compositions i . e .
if the clusters have similar faces in them they possibly describe the same person .
we can judge the similarity of clusters by the distance between their means in discriminant coordi - nates .
table 123 shows that almost all the top proposed merges correspond to correct merges ( two names that refer to the same person ) , except that of richard myers and president 123
quantitative evaluation we view this method as taking a supervised dataset that con - tains errors and ambiguities in the supervisory signal and producing a dataset with an improved ( or , ideally , correct ) supervisory signal , possibly omitting some data items .
we must now determine how much better the new dataset is .
two things can have happened : first , the dataset may be smaller .
second , the supervisory signal should be more ac -
but how much more accurate ? if we are willing to as - sume that all errors are equivalent , we can see this issue as a coding problem .
in particular , one must determine how many bits need to be supplied to make the dataset correct .
we compute this score on a per - item basis , so that the size of the dataset does not affect the score .
the number computed is necessarily an estimate we may not have an optimal coding scheme but if one uses a reasonably competent coding scheme should allow us to rank methods against one 123 the cost of correcting unclustered data we have a set of image - text pairs , each of which consists of an image of one of the individuals , and a list of between 123 and 123 ( typically ) text labels .
for each data item , we must now determine whether this set of labels contains the correct
original data set cost given image labels cost given clustering
log123 ( number of images )
figure 123 : the gure shows the approximate cost per item of correcting each dataset plotted against the size of the dataset .
note that the original dataset ( the cross ) is large and noisy; if one clusters , merges and cleans , then ignores the clustering structure , the resulting dataset is , in fact , somewhat noisier ( dashed line ) .
finally , if one looks at the cluster structure , too , the dataset is much cleaner in par - ticular one is contributing information to a dataset by clus - tering it correctly .
finally , increasing settings of our re - ject threshold leads to datasets that tend to be smaller and
represent the random variable that takes on , we can do this with ) .
if the list labels contains the correct name , we can tell which it bits .
if it does not , we must supply the correct bits ( the entropy of the label
name , which will cost
is the conditional entropy of$
that have a correct label in that list , and portion of those with item of correcting the original dataset
123 the cost of correcting clustered data our clustering procedure introduces two types of structure .
first , it assigns each image from an image - text pair to one clusters .
second , it associates a text to each cluster , so implicitly labelling each image pair within that cluster .
if the labelling represented in this way is perfect , no further bits need to be provided .
we compute the additional cost for perfect labelling by examining how much work ( how many bits ) are required to x all the errors in an imperfect clustering and cluster labelling .
we assume the entropy of the label set remains xed .
we split the problem of xing up imperfect clusters into two steps .
in the rst step , we change the name of each cluster , if necessary , so that it corresponds to the person who appears most frequently in this cluster .
in the second step , we change the label of each image in a cluster if it does not correspond to the ( now correct ) label of the cluster .
for the proportion of images with
for the pro - labels that do not .
the total cost per in this fashion is
fixing the cluster labels : let be the proportion of the proportion of clusters with the correct label and clusters with an incorrect label .
let be the random vari - able representing whether a cluster is labelled correctly or not and recall our practice of writing the entropy of a ran - the cost per cluster of correcting the labels for all the clusters is then
obtain the correct name , and the cost is
for the total number of clusters per item ( we
fixing incorrect elements within the cluster : we as - sume for simplicity that , once the labels have been corrected as above , the proportion of correctly labelled elements in a is independent of the cluster .
this means that the cost per item of xing incorrectly labelled elements is independent of the specic cluster structure .
then to nish xing the labelling we must pay bits per item to iden - bits per incorrect item to
labelling of the data , after clustering , as
tify the incorrect items and$ ) .
we have a total per item cost for the correct
