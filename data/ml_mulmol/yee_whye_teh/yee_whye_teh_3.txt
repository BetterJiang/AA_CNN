latent dirichlet analysis , or topic modeling , is a exible latent variable framework for model - ing high - dimensional sparse count data .
various learning algorithms have been developed in re - cent years , including collapsed gibbs sampling , variational inference , and maximum a posteriori estimation , and this variety motivates the need for careful empirical comparisons .
in this paper , we highlight the close connections between these approaches .
we nd that the main differences are attributable to the amount of smoothing applied to the counts .
when the hyperparameters are op - timized , the differences in performance among the algorithms diminish signicantly .
the ability of these algorithms to achieve solutions of com - parable accuracy gives us the freedom to select computationally efcient approaches .
using the insights gained from this comparative study , we show how accurate topic models can be learned in several seconds on text corpora with thousands
latent dirichlet allocation ( lda ) ( blei et al . , 123 ) and probabilistic latent semantic analysis ( plsa ) ( hofmann , 123 ) are well - known latent variable models for high di - mensional count data , such as text data in the bag - of - words representation or images represented through fea - ture counts .
various inference techniques have been pro - posed , including collapsed gibbs sampling ( cgs ) ( grif - ths and steyvers , 123 ) , variational bayesian inference ( vb ) ( blei et al . , 123 ) , collapsed variational bayesian in - ference ( cvb ) ( teh et al . , 123 ) , maximum likelihood esti - mation ( ml ) ( hofmann , 123 ) , and maximum a posteriori estimation ( map ) ( chien and wu , 123 ) .
among these algorithms , substantial performance differ - ences have been observed in practice .
for instance , blei
( 123 ) have shown that the vb algorithm for lda outperforms ml estimation for plsa .
furthermore , teh et al .
( 123 ) have found that cvb is signicantly more ac - curate than vb .
but can these differences in performance really be attributed to the type of inference algorithm ?
in this paper , we provide convincing empirical evidence that points in a different direction , namely that the claimed differences can be explained away by the different settings of two smoothing parameters ( or hyperparameters ) .
fact , our empirical results suggest that these inference algo - rithms have relatively similar predictive performance when the hyperparameters for each method are selected in an op - timal fashion .
with hindsight , this phenomenon should not surprise us .
topic models operate in extremely high di - mensional spaces ( with typically more than 123 , 123 dimen - sions ) and , as a consequence , the curse of dimensionality is lurking around the corner; thus , hyperparameter settings have the potential to signicantly affect the results .
we show that the potential perplexity gains by careful treatment of hyperparameters are on the order of ( if not greater than ) the differences between different inference al - gorithms .
these results caution against using generic hy - perparameter settings when comparing results across algo - rithms .
this in turn raises the question as to whether newly introduced models and approximate inference algorithms have real merit , or whether the observed difference in pre - dictive performance is attributable to suboptimal settings of hyperparameters for the algorithms being compared .
in performing this study , we discovered that an algorithm which suggests itself in thinking about inference algo - rithms in a unied way but was never proposed by itself before performs best , albeit marginally so .
more impor - tantly , it happens to be the most computationally efcient algorithm as well .
in the following section , we highlight the similarities be - tween each of the algorithms .
we then discuss the impor - tance of hyperparameter settings .
we show accuracy re - sults , using perplexity and precision / recall metrics , for each algorithm over various text data sets .
we then focus on
asuncion et al .
figure 123 : graphical model for latent dirichlet allocation .
boxes denote parameters , and shaded / unshaded circles de - note observed / hidden variables .
computational efciency and provide timing results across algorithms .
finally , we discuss related work and conclude with future directions .
inference techniques for lda
lda has roots in earlier statistical decomposition tech - niques , such as latent semantic analysis ( lsa ) ( deer - wester et al . , 123 ) and probabilistic latent semantic anal - ysis ( plsa ) ( hofmann , 123 ) .
proposed as a generaliza - tion of plsa , lda was cast within the generative bayesian framework to avoid some of the overtting issues that were observed with plsa ( blei et al . , 123 ) .
a review of the similarities between lsa , plsa , lda , and other models can be found in buntine and jakulin ( 123 ) .
we describe the lda model and begin with general nota - tion .
lda assumes the standard bag - of - words representa - tion , where d documents are each represented as a vector of counts with w components , where w is the number of words in the vocabulary .
each document j in the corpus is modeled as a mixture over k topics , and each topic k is a distribution over the vocabulary of w words .
each topic , k , is drawn from a dirichlet with parameter , while each documents mixture , j , is sampled from a dirichlet with parameter 123
for each token i in the corpus , a topic as - signment zi is sampled from di , and the specic word xi is drawn from zi .
the generative process is below :
in figure 123 , the graphical model for lda is presented in a slightly unconventional fashion , as a bayesian network where kj and wk are conditional probability tables and i runs over all tokens in the corpus .
each tokens document index di is explicitly shown as an observed variable , in or - der to show ldas correspondence to the plsa model .
exact inference ( i . e .
computing the posterior over the hid - den variables ) for this model is intractable ( blei et al . , 123 ) , and so a variety of approximate algorithms have been developed .
if we ignore and and treat kj and wk as parameters , we obtain the plsa model , and max - imum likelihood ( ml ) estimation over kj and wk di - rectly corresponds to plsas em algorithm .
adding the hyperparameters and back in leads to map estimation .
123we use symmetric dirichlet priors for simplicity in this paper .
treating kj and wk as hidden variables and factorizing the posterior distribution leads to the vb algorithm , while collapsing kj and wk ( i . e .
marginalizing over these vari - ables ) leads to the cvb and cgs algorithms .
in the fol - lowing subsections , we provide details for each approach .
123 ml estimation
the plsa algorithm described in hofmann ( 123 ) can be understood as an expectation maximization algorithm for the model depicted in figure 123
we start the derivation by writing the log - likelihood as ,
p ( xi|zi , ) p ( zi|di , )
from which we derive via a standard em derivation the up - dates ( where we have left out explicit normalizations ) :
p ( zi|xi , di ) p ( xi|zi , ) p ( zi|di , )
i ( xi = w , zi = k ) p ( zi|xi , di )
i ( zi = k , di = j ) p ( zi|xi , di ) .
these updates can be rewritten by dening wjk = p ( z = k|x = w , d = j ) , nwj the number of observations for
word type w in document j , nwk = pj nwjwjk , nkj = pw nwjwjk , nk = pw nwk and nj = pk nkj ,
k , j nkj / nj .
plugging these expressions back into the expression for the posterior ( 123 ) we arrive at the update ,
where the constant nj is absorbed into the normalization .
hofmann ( 123 ) regularizes the plsa updates by raising the right hand side of ( 123 ) to a power > 123 and searching for the best value of on a validation set .
123 map estimation
we treat , as random variables from now on .
we add dirichlet priors with strengths for and for respec - tively .
this extension was introduced as latent dirichlet allocation in blei et al .
( 123 ) .
it is possible to optimize for the map estimate of , .
the derivation is very similar to the ml derivation in the previous section , except that we now have terms corre - sponding to the log of the dirichlet prior which are equal
to pwk ( 123 ) log wk and pkj ( 123 ) log kj
working through the math , we derive the following update ( de freitas and barnard ( 123 ) , chien and wu ( 123 ) ) ,
( nwk + 123 ) ( nkj + 123 )
( nk + w w )
asuncion et al .
where , > 123
upon convergence , map estimates are
nwk + 123 nk + w w
nkj + 123 nj + k k
123 variational bayes
the variational bayesian approximation ( vb ) to lda fol - lows the standard variational em framework ( attias , 123 , ghahramani and beal , 123 ) .
we introduce a factorized ( and hence approximate ) variational posterior distribution :
123 collapsed variational bayes
it is possible to marginalize out the random variables kj and wk from the joint probability distribution .
following a variational treatment , we can introduce variational pos - teriors over z variables which is once again assumed to be
factorized : q ( z ) = qi q ( zi ) .
this collapsed variational
free energy represents a strictly better bound on the ( nega - tive ) evidence than the original vb ( teh et al . , 123 ) .
the derivation of the update equation for the q ( zi ) is slightly more complicated and involves approximations to compute intractable summations .
the update is given below 123 :
q ( , , z ) = yk
using this assumption in the variational formulation of the em algorithm ( neal and hinton , 123 ) we readily derive the vb updates analogous to the ml updates of eqns
q ( , k ) = d ( + n , k ) , nwk = xi
q ( zi = k ) ( xi , w )
k + w ( cid : 123 ) n ij
kj + ( cid : 123 ) exp ( cid : 123 )
kj + ) 123
wk + ) 123
k + w ) 123 ( cid : 123 ) .
kj denotes the expected number of tokens in document j assigned to topic k ( excluding the current token ) , and can be calculated as follows : n ij kj = pi123=i ijk .
for cvb , there is also a variance associated with each count : v ij pi123=i ijk ( 123 ijk ) .
for further details we refer to teh
( 123 ) .
the update in ( 123 ) makes use of a second - order taylor ex - pansion as an approximation .
a further approximation can be made by using only the zeroth - order information123 :
k + w ( cid : 123 ) n ij
kj + ( cid : 123 ) .
we refer to this approximate algorithm as cvb123
123 collapsed gibbs sampling
mcmc techniques are available to lda as well .
collapsed gibbs sampling ( cgs ) ( grifths and steyvers , 123 ) , kj and wk are integrated out ( as in cvb ) and sam - pling of the topic assignments is performed sequentially in the following manner :
p ( zij = k|zij , xij = w )
k + w ( cid : 123 ) n ij
kj + ( cid : 123 ) .
nwk denotes the number of word tokens of type w as - signed to topic k , nkj is the number of tokens in docu -
ment j assigned to topic k , and nk = pw nwk
denotes the count with token ij removed .
note that stan - dard non - collapsed gibbs sampling over , , and z can also be performed , but we have observed that cgs mixes more quickly in practice .
123for convenience , we switch back to the conventional index -
ing scheme for lda where i runs over tokens in document j .
123the rst - order information becomes zero in this case .
q ( , j ) = d ( + n , j ) , nkj = xi
q ( zi = k ) ( di , j )
q ( zi ) exp ( cid : 123 ) e ( log xi , zi ) q ( ) e ( log zi , di ) q ( ) ( cid : 123 ) .
we can insert the expression for q ( ) at ( 123 ) and q ( ) at ( 123 ) into the update for q ( z ) in ( 123 ) and use the fact that
e ( logxi ) d ( x ) = ( xi ) ( pj xj ) with ( ) being the
digamma function .
as a nal observation , note that there is nothing in the free energy that would render any differ - ently the distributions q ( zi ) for tokens that correspond to the same word - type w in the same document j .
hence , we can simplify and update a single prototype of that equiva - lence class , denoted as wjk , q ( zi = k ) ( xi , w ) ( di , j )
exp ( ( nwk + ) ) exp ( ( nk + w ) )
exp ( cid : 123 ) ( nkj + ) ( cid : 123 ) .
we note that exp ( ( n ) ) n 123 for n > 123
since nwk , nkj , and nk are aggregations of expected counts , we ex - pect many of these counts to be greater than 123
thus , the vb update can be approximated as follows ,
( nwk + 123 )
( nk + w 123 ) ( cid : 123 ) nkj + 123 ( cid : 123 )
which exposes the relation to the map update in ( 123 ) .
in closing this section , we mention that the original vb algorithm derived in blei et al .
( 123 ) was a hybrid version between what we call vb and ml here .
although they did estimate variational posteriors q ( ) , the were treated as parameters and were estimated through ml .
asuncion et al .
123 comparison of algorithms a comparison of update equations ( 123 ) , ( 123 ) , ( 123 ) , ( 123 ) , ( 123 ) reveals the similarities between these algorithms .
all of these updates consist of a product of terms featuring nwk and nkj as well as a denominator featuring nk .
these updates resemble the callen equations ( teh et al . , 123 ) , which the true posterior distribution must satisfy ( with z as the normalization constant ) :
p ( zij = k|x ) = ep ( zij |x ) ( cid : 123 ) 123
wk + )
k + w ) ( cid : 123 ) n ij
we highlight the striking connections between the algo - interestingly , the probabilities for cgs ( 123 ) and cvb123 ( 123 ) are exactly the same .
the only difference is that cgs samples each topic assignment while cvb123 de - terministically updates a discrete distribution over topics for each token .
another way to view this connection is to imagine that cgs can sample each topic assignment zij r times using ( 123 ) and maintain a distribution over these samples with which it can update the counts .
as r , this distribution will be exactly ( 123 ) and this algorithm will be cvb123
the fact that algorithms like cvb123 are able to propagate the entire uncertainty in the topic distribution during each update suggests that deterministic algorithms should converge more quickly than cgs .
cvb123 and cvb are almost identical as well , the distinction being the inclusion of second - order information for cvb .
the conditional distributions used in vb ( 123 ) and map ( 123 ) are also very similar to those used for cgs , with the main difference being the presence of offsets of up to 123 and 123 in the numerator terms for vb and map , respectively .
through the setting of hyperparameters and , these extra offsets in the numerator can be eliminated , which suggests that these algorithms can be made to perform similarly with appropriate hyperparameter settings .
another intuition which sheds light on this phenomenon is as follows .
variational methods like vb are known to underestimate posterior variance ( wang and titterington , in the case of lda this is reected in the offset of - 123 : typical values of and in the variational poste - rior tend to concentrate more mass on the high probability words and topics respectively .
we can counteract this by in - crementing the hyperparameters by 123 , which encourages more probability mass to be smoothed to all words and top - ics .
similarly , map offsets by - 123 , concentrating even more mass on high probability words and topics , and requiring even more smoothing by incrementing and by 123
other subtle differences between the algorithms exist .
for instance , vb subtracts only 123 from the denominator while map removes w , which suggests that vb applies more smoothing to the denominator .
since nk is usually large , we do not expect this difference to play a large role in learn - ing .
for the collapsed algorithms ( cgs , cvb ) , the counts
nwk , nkj , nk are updated after each token update .
mean - while , the standard formulations of vb and map update these counts only after sweeping through all the tokens .
this update schedule may affect the rate of convergence ( neal and hinton , 123 ) .
another difference is that the collapsed algorithms remove the count for the current to -
as we will see in the experimental results section , the per - formance differences among these algorithms that were ob - served in previous work can be substantially reduced when the hyperparameters are optimized for each algorithm .
123 the role of hyperparameters the similarities between the update equations for these al - gorithms shed light on the important role that hyperparame - ters play .
since the amount of smoothing in the updates dif - ferentiates the algorithms , it is important to have good hy - perparameter settings .
in previous results ( teh et al . , 123 , welling et al . , 123b , mukherjee and blei , 123 ) , hyper - parameters for vb were set to small values like = 123 , = 123 , and consequently , the performance of vb was observed to be signicantly suboptimal in comparison to cvb and cgs .
since vb effectively adds a discount of up to 123 in the updates , greater values for and are nec - essary for vb to perform well .
we discuss hyperparameter learning and the role of hyperparameters in prediction .
123 hyperparameter learning it is possible to learn the hyperparameters during training .
one approach is to place gamma priors on the hyperpa - rameters ( g ( a , b ) , g ( c , d ) ) and use minkas xed - point iterations ( minka , 123 ) , e . g . :
c 123 + pj pk ( ( nkj + ) ( ) ) d + kpj ( ( nj + k ) ( k ) )
other ways for learning hyperparameters include newton - raphson and other xed - point techniques ( wallach , 123 ) , as well as sampling techniques ( teh et al . , 123 ) .
another approach is to use a validation set and to explore various settings of , through grid search .
we explore several of these approaches later in the paper .
hyperparameters play a role in prediction as well .
consider the update for map in ( 123 ) and the estimates for wk and kj ( 123 ) and note that the terms used in learning are the same as those used in prediction .
essentially the same can be said for the collapsed algorithms , since the following rao - blackwellized estimates are used , which bear resemblance to terms in the updates ( 123 ) , ( 123 ) :
nk + w
nj + k
in the case of vb , the expected values of the posterior dirichlets in ( 123 ) and ( 123 ) are used in prediction , leading to
asuncion et al .
table 123 : data sets used in experiments
estimates for and of the same form as ( 123 ) .
however , for vb , an offset of 123 is found in update equation ( 123 ) while it is not found in the estimates used for prediction .
the knowledge that vbs update equation contains an ef - fective offset of up to 123 suggests the use of an alterna - tive estimate for prediction :
exp ( ( nwk + ) ) exp ( ( nk + w ) )
exp ( ( nkj + ) ) exp ( ( nj + k ) )
note the similarity that these estimates bear to the vb up - date ( 123 ) .
essentially , the 123 offset is introduced into these estimates just as they are found in the update .
an - other way to mimic this behavior is to use + 123 and + 123 during learning and then use and for predic - tion , using the estimates in ( 123 ) .
we nd that correcting this mismatch between the update and the estimate re - duces the performance gap between vb and the other al - gorithms .
perhaps this phenomenon bears relationships to the observation of wainwright ( 123 ) , who shows that it certain cases , it is benecial to use the same algorithm for both learning and prediction , even if that algorithm is ap - proximate rather than exact .
seven different text data sets are used to evaluate the perfor - mance of these algorithms : craneld - subset ( cran ) , kos ( kos ) , medline - subset ( med ) , nips ( nips ) , 123 news - groups ( news ) , nyt - subset ( nyt ) , and patent ( pat ) .
several of these data sets are available online at the uci ml repository ( asuncion and newman , 123 ) .
the char - acteristics of these data sets are summarized in table 123
each data set is separated into a training set and a test set .
we learn the model on the training set , and then we mea - sure the performance of the algorithms on the test set .
we also have a separate validation set of the same size as the test set that can be used to tune the hyperparameters .
to evaluate accuracy , we use perplexity , a widely - used met - ric in the topic modeling community .
while perplexity is a somewhat indirect measure of predictive performance , it is nonetheless a useful characterization of the predictive qual - ity of a language model and has been shown to be well - correlated with other measures of performance such word - error rate in speech recognition ( klakow and peters , 123 ) .
we also report precision / recall statistics .
figure 123 : convergence plot showing perplexities on med , k=123; hyperparameters learned through minkas update .
we describe how perplexity is computed .
for each of our algorithms , we perform runs lasting 123 iterations and we obtain the estimate wk at the end of each of those runs .
to obtain kj , one must learn the topic assignments on the rst half of each document in the test set while holding wk xed .
for this fold - in procedure , we use the same learning algorithm that we used for training .
perplexity is evaluated on the second half of each document in the test set , given wk and jk .
for cgs , one can average over multiple sam - ples ( where s is the number of samples to average over ) :
test ) = xjw
s xs xk
in our experiments we dont perform averaging over sam - ples for cgs ( other than in figure 123 where we explicitly investigate averaging ) , both for computational reasons and to provide a fair comparison to the other algorithms .
us - ing a single sample from cgs is consistent with its use as an efcient stochastic mode - nder to nd a set of inter - pretable topics for a document set .
for each experiment , we perform three different runs us - ing different initializations , and report the average of these perplexities .
usually these perplexities are similar to each other across different initializations ( e . g .
123 or less ) .
123 perplexity results in our rst set of experiments , we investigate the effects of learning the hyperparameters during training using minkas xed point updates .
we compare cgs , vb , cvb , and cvb123 in this set of experiments and leave out map since minkas update does not apply to map .
for each run , we initialize the hyperparameters to = 123 , = 123 and turn on minkas updates after 123 iterations to prevent numerical instabilities .
every other iteration , we compute perplexity on the validation set to allow us to perform early - stopping if necessary .
figure 123 shows the test perplexity as a func - tion of iteration for each algorithm on the med data set .
these perplexity results suggest that cvb and cvb123 out - perform vb when minkas updates are used .
the reason is because the learned hyperparameters for vb are too small and do not correct for the effective 123 offset found in the vb update equations .
also , cgs converges more slowly than the deterministic algorithms .
asuncion et al .
figure 123 : perplexities achieved with hyperparameter learn - ing through minkas update , on various data sets , k=123
figure 123 : perplexities achieved through grid search , k=123
number of topics
figure 123 : perplexity as a function of number of topics , on nips , with minkas update enabled .
in figure 123 , we show the nal perplexities achieved with hyperparameter learning ( through minkas update ) , on each data set .
vb performs worse on several of the data sets compared to the other algorithms .
we also found that cvb123 usually learns the highest level of smoothing , fol - lowed by cvb , while minkas updates for vb learn small values for , .
in our experiments thus far , we xed the number at topics at k = 123
in figure 123 , we vary the number of topics from 123 to 123
in this experiment , cgs / cvb / cvb123 perform simi - larly , while vb learns less accurate solutions .
interestingly , the cvb123 perplexity at k = 123 is higher than the per - plexity at k = 123
this is due to the fact that a high value for was learned for cvb123
when we set = 123 ( to the k = 123 level ) , the cvb123 perplexity is 123 , matching cgs .
these results suggest that learning hyperparameters during training ( using minkas updates ) does not necessar - ily lead to the optimal solution in terms of test perplexity .
in the next set of experiments , we use a grid of hyperpa - rameters for each of and , ( 123 , 123 , 123 , 123 , 123 , 123 ) , and we run the algorithms for each combination of hy - perparameters .
we include map in this set of experiments , and we shift the grid to the right by 123 for map ( since hyper - parameters less than 123 cause map to have negative proba - bilities ) .
we perform grid search on the validation set , and
we nd the best hyperparameter settings ( according to val - idation set perplexity ) and use the corresponding estimates for prediction .
for vb , we report both the standard per - plexity calculation and the alternative calculation that was detailed previously in ( 123 ) .
in figure 123 , we report the results achieved through perform - ing grid search .
the differences between vb ( with the al - ternative calculation ) and cvb have largely vanished .
this is due to the fact that we are using larger values for the hyperparameters , which allows vb to reach parity with the other algorithms .
the alternative prediction scheme for vb also helps to reduce the perplexity gap , especially for the news data set .
interestingly , cvb123 appears to perform slightly better than the other algorithms .
figure 123 shows the test perplexity of each method as a function of .
it is visually apparent that vb and map perform better when their hyperparameter values are off - set by 123 and 123 , respectively , relative to the other meth - ods .
while this picture is not as clear - cut for every data set ( since the approximate vb update holds only when n > 123 ) , we have consistently observed that the minimum perplexi - ties achieved by vb are at hyperparameter values that are higher than the ones used by the other algorithms .
in the previous experiments , we used one sample for cgs to compute perplexity .
with enough samples , cgs should be the most accurate algorithm .
in figure 123 , we show the effects of averaging over 123 different samples for cgs , taken over 123 different runs , and nd that cgs gains sub - stantially from averaging samples .
it is also possible for other methods like cvb123 to average over their local poste - rior modes but we found the resulting gain is not as great .
we also tested whether the algorithms would perform sim - ilarly in cases where the training set size is very small or the number of topics is very high .
we ran vb and cvb with grid search on half of cran and achieved virtually the same perplexities .
we also ran vb and cvb on cran with k = 123 , and only found a 123 - point perplexity gap .
asuncion et al .
figure 123 : top : kos , k=123
bottom : med , k=123
per - plexity as a function of .
we xed to 123 ( 123 for map ) .
relative to the other curves , vb and map curves are in effect shifted right by approximately 123 and 123
figure 123 : the effect of averaging over 123 samples / modes on kos , k=123
to summarize our perplexity results , we juxtapose three different ways of setting hyperparameters in figure 123 , for nips , k = 123
the rst way is to have the same arbitrary values used across all algorithms ( e . g .
= 123 , = 123 ) .
the second way is to learn the hyperparameters through minkas update .
the third way way is to nd the hyperpa - rameters by grid search .
for the third way , we also show the vb perplexity achieved by the alternative estimates .
123 precision / recall results we also calculated precision / recall statistics on the news data set .
since each document in news is associated with one of twenty newsgroups , one can label each doc - ument by its corresponding newsgroup .
it is possible to use the topic model for classication and to compute pre - cision / recall statistics .
in figure 123 , we show the mean area under the roc curve ( auc ) achieved by cgs , vb , cvb , and cvb123 with hyperparameter learning through minkas update .
we also performed grid search over , and found that each method was able to achieve similar statistics .
for instance , on news , k = 123 , each algorithm achieved the
point minka grid grid ( alt )
figure 123 : point : when = 123 , = 123 , vb performs substantially worse than other methods .
minka : when using minkas updates , the differences are less prominent .
grid : when grid search is performed , differences dimin - ish even more , especially with the alternative estimates .
figure 123 : mean auc achieved on news , k=123 , with
same area under the roc curve ( 123 ) and mean average precision ( 123 ) .
these results are consistent with the per - plexity results in the previous section .
123 computational efficiency
while the algorithms can give similarly accurate solutions , some of these algorithms are more efcient than others .
vb contains digamma functions which are computationally ex - pensive , while cvb requires the maintenance of variance counts .
meanwhile , the stochastic nature of cgs causes it to converge more slowly than the deterministic algorithms .
in practice , we advocate using cvb123 since 123 ) it is faster than vb / cvb given that there are no calls to digamma or variance counts to maintain; 123 ) it converges more quickly than cgs since it is deterministic; 123 ) it does not have maps 123 offset issue .
furthermore , our empirical results suggest that cvb123 learns models that are as good or better ( predictively ) than those learned by the other algorithms .
these algorithms can be parallelized over multiple proces - sors as well .
the updates in map estimation can be per - formed in parallel without affecting the xed point since map is an em algorithm ( neal and hinton , 123 ) .
since the other algorithms are very closely related to map there is condence that performing parallel updates over tokens for the other algorithms would lead to good results as well .
asuncion et al .
table 123 : timing results ( in seconds )
med kos nips
while non - collapsed algorithms such as map and vb can be readily parallelized , the collapsed algorithms are se - quential , and thus there has not been a theoretical basis for parallelizing cvb or cgs ( although good empirical results have been achieved for approximate parallel cgs ( new - man et al . , 123 ) ) .
we expect that a version of cvb123 that parallelizes over tokens would converge to the same quality of solution as sequential cvb123 , since cvb123 is essentially map but without the 123 offset123
in table 123 , we show timing results for vb , cvb , cgs , and cvb123 on med , kos , and nips , with k = 123
we record the amount of time it takes for each algorithm to pass a xed perplexity threshold ( the same for each algorithm ) .
since vb contains many calls to the digamma function , it is slower than the other algorithms .
meanwhile , cgs needs more iterations before it can reach the same perplex - ity , since it is stochastic .
we see that cvb123 is computation - ally the fastest approach among these algorithms .
we also parallelized cvb123 on a machine with 123 cores and nd that a topic model with coherent topics can be learned in 123 seconds for kos .
these results suggest that it is feasible to learn topic models in near real - time for small corpora .
123 related work & conclusions some of these algorithms have been compared to each other in previous work .
teh et al .
( 123 ) formulate the cvb algorithm and empirically compare it to vb , while mukherjee and blei ( 123 ) theoretically analyze the differ - ences between vb and cvb and give cases for when cvb should perform better than vb .
welling et al .
( 123b ) also compare the algorithms and introduce a hybrid cgs / vb algorithm .
in all these studies , low values of and were used for each algorithm , including vb .
our insights sug - gest that vb requires more smoothing in order to match the performance of the other algorithms .
the similarities between plsa and lda have been noted in the past ( girolami and kaban , 123 ) .
others have uni - ed similar deterministic latent variable models ( welling et al . , 123a ) and matrix factorization techniques ( singh and gordon , 123 ) .
in this work , we highlight the similar - ities between various learning algorithms for lda .
while we focused on lda and plsa in this paper , we be - lieve that the insights gained are relevant to learning in gen -
123if one wants convergence guarantees , one should also not re -
move the current token ij .
eral directed graphical models with dirichlet priors , and generalizing these results to other models is an interesting avenue to pursue in the future .
in conclusion , we have found that the update equations for these algorithms are closely connected , and that using the appropriate hyperparameters causes the performance differences between these algorithms to largely disappear .
these insights suggest that hyperparameters play a large role in learning accurate topic models .
our comparative study also showed that there exist accurate and efcient learning algorithms for lda and that these algorithms can be parallelized , allowing us to learn accurate models over thousands of documents in a matter of seconds .
