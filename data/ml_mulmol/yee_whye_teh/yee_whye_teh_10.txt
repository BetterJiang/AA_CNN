we consider problems involving groups of data , where each observation within a group is a draw from a mixture model , and where it is desirable to share mixture components between groups .
we assume that the number of mixture components is unknown a priori and is to be inferred from the data .
in this setting it is natural to consider sets of dirichlet processes , one for each group , where the well - known clustering property of the dirichlet process provides a nonparametric prior for the number of mixture components within each group .
given our desire to tie the mixture models in the various groups , we consider a hierarchical model , specically one in which the base measure for the child dirichlet processes is itself distributed according to a dirichlet process .
such a base measure being discrete , the child dirichlet processes necessar - ily share atoms .
thus , as desired , the mixture models in the different groups necessarily share mixture components .
we discuss representations of hierarchical dirichlet processes in terms of a stick - breaking process , and a generalization of the chinese restaurant process that we refer to as the chinese restaurant franchise .
we present markov chain monte carlo algorithms for posterior inference in hierarchical dirichlet process mixtures , and describe applications to problems in information retrieval and text modelling .
keywords : clustering , mixture models , nonparametric bayesian statistics , hierarchical models , markov chain monte carlo
a recurring theme in statistics is the need to separate observations into groups , and yet allow the groups to remain linkedto share statistical strength .
in the bayesian formalism such sharing is achieved naturally via hierarchical modeling; parameters are shared among groups , and the random - ness of the parameters induces dependencies among the groups .
estimates based on the posterior distribution exhibit shrinkage .
in the current paper we explore a hierarchical approach to the problem of model - based clustering of grouped data .
we assume that the data are subdivided into a set of groups , and that within each group we wish to nd clusters that capture latent structure in the data assigned to that group .
the number of clusters within each group is unknown and is to be inferred .
moreover , in a sense that we make precise , we wish to allow clusters to be shared among the groups .
an example of the kind of problem that motivates us can be found in genetics .
consider a set of k binary markers ( e . g . , single nucleotide polymorphisms or snps ) in a localized region of the human genome .
while an individual human could exhibit any of 123k different patterns of markers on a single chromosome , in real populations only a small subset of such patternshaplotypesare actually observed ( gabriel et al .
given a meiotic model for the combination of a pair of haplotypes into a genotype during mating , and given a set of observed genotypes in a sample from a human population , it is of great interest to identify the underlying haplotypes ( stephens et al .
now consider an extension of this problem in which the population is divided into a set of groups; e . g . , african , asian and european subpopulations .
we may not only want to discover the sets of haplotypes within each subpopulation , but we may also wish to discover which haplotypes are shared between subpopulations .
the identication of such haplotypes would have signicant implications for the understanding of the migration patterns of ancestral populations of humans .
as a second example , consider the problem from the eld of information retrieval ( ir ) of mod - eling of relationships among sets of documents .
in ir , documents are generally modeled under an exchangeability assumption , the bag of words assumption , in which the order of words in a document is ignored ( salton and mcgill 123 ) .
it is also common to view the words in a document as arising from a number of latent clusters or topics , where a topic is generally modeled as a multinomial probability distribution on words from some basic vocabulary ( blei et al .
thus , in a document concerned with university funding the words in the document might be drawn from the topics education and nance .
considering a collection of such documents , we may wish to allow topics to be shared among the documents in the corpus .
for example , if the corpus also contains a document concerned with university football , the topics may be education and sports , and we would want the former topic to be related to that discovered in the analysis of the document on university funding .
moreover , we may want to extend the model to allow for multiple corpora .
for example , doc - uments in scientic journals are often grouped into themes ( e . g . , empirical process theory , mul - tivariate statistics , survival analysis ) , and it would be of interest to discover to what extent the latent topics that are shared among documents are also shared across these groupings .
thus in general we wish to consider the sharing of clusters across multiple , nested groupings of data .
our approach to the problem of sharing clusters among multiple , related groups is a nonpara - metric bayesian approach , reposing on the dirichlet process ( ferguson 123 ) .
the dirichlet process dp ( ( cid : 123 ) 123; g123 ) is a measure on measures .
it has two parameters , a scaling parameter ( cid : 123 ) 123 > 123 and a base probability measure g123
an explicit representation of a draw from a dirichlet process ( dp )
was given by sethuraman ( 123 ) , who showed that if g ( cid : 123 ) dp ( ( cid : 123 ) 123; g123 ) , then with probability one :
where the ( cid : 123 ) k are independent random variables distributed according to g123 , where ( cid : 123 ) ( cid : 123 ) k is an atom at ( cid : 123 ) k , and where the stick - breaking weights ( cid : 123 ) k are also random and depend on the parameter ( cid : 123 ) 123 ( the denition of the ( cid : 123 ) k is provided in section 123 ) .
the representation in ( 123 ) shows that draws from a dp are discrete ( with probability one ) .
the discrete nature of the dp makes it unsuitable for general applications in bayesian nonparametrics , but it is well suited for the problem of placing priors on mixture components in mixture modeling .
the idea is basically to associate a mixture component with each atom in g .
introducing indica - tor variables to associate data points with mixture components , the posterior distribution yields a probability distribution on partitions of the data .
a number of authors have studied such dirichlet process mixture models ( antoniak 123; escobar and west 123; maceachern and m uller 123 ) .
these models provide an alternative to methods that attempt to select a particular number of mixture components , or methods that place an explicit parametric prior on the number of components .
let us now consider the setting in which the data are subdivided into a number of groups .
given our goal of solving a clustering problem within each group , we consider a set of random measures gj , one for each group j , where gj is distributed according to a group - specic dirichlet process dp ( ( cid : 123 ) 123j; g123j ) .
to link these clustering problems , we link the group - specic dps .
many authors have considered ways to induce dependencies among multiple dps via links among the parameters g123j and / or ( cid : 123 ) 123j ( cifarelli and regazzini 123; maceachern 123; tomlinson 123; m uller et al .
123; de iorio et al .
123; kleinman and ibrahim 123; mallick and walker 123; ishwaran and james 123 ) .
focusing on the g123j , one natural proposal is a hierarchy in which the measures gj are conditionally independent draws from a single underlying dirichlet process dp ( ( cid : 123 ) 123; g123 ( ( cid : 123 ) ) ) , where g123 ( ( cid : 123 ) ) is a parametric distribution with random parameter ( cid : 123 ) ( carota and parmigiani 123; fong et al .
123; muliere and petrone 123 ) .
integrating over ( cid : 123 ) induces dependencies among the dps .
that this simple hierarchical approach will not solve our problem can be observed by consider - ing the case in which g123 ( ( cid : 123 ) ) is absolutely continuous with respect to lebesgue measure for almost all ( cid : 123 ) ( e . g . , g123 is gaussian with mean ( cid : 123 ) ) .
in this case , given that the draws gj arise as conditionally independent draws from g123 ( ( cid : 123 ) ) , they necessarily have no atoms in common ( with probability one ) .
thus , although clusters arise within each group via the discreteness of draws from a dp , the atoms associated with the different groups are different and there is no sharing of clusters between groups .
this problem can be skirted by assuming that g123 lies in a discrete parametric family , but such an assumption would be overly restrictive .
our proposed solution to the problem is straightforward : to force g123 to be discrete and yet have broad support , we consider a nonparametric hierarchical model in which g123 is itself a draw from a dirichlet process dp ( ( cid : 123 ) ; h ) .
this restores exibility in that the modeler can choose h to be continuous or discrete .
in either case , with probability one , g123 is discrete and has a stick - breaking representation as in ( 123 ) .
the atoms ( cid : 123 ) k are shared among the multiple dps , yielding the desired sharing of atoms among groups .
in summary , we consider the hierarchical specication :
g123 j ( cid : 123 ) ; h ( cid : 123 ) dp ( ( cid : 123 ) ; h )
gj j ( cid : 123 ) 123; g123 ( cid : 123 ) dp ( ( cid : 123 ) 123; g123 )
for each j ,
which we refer to as a hierarchical dirichlet process .
the immediate extension to hierarchical dirichlet process mixture models yields our proposed formalism for sharing clusters among related
related nonparametric approaches to linking multiple dps have been discussed by a number of authors .
our approach is a special case of a general framework for dependent dirichlet processes due to maceachern ( 123 ) and maceachern et al .
( 123 ) .
in this framework the random variables ( cid : 123 ) k and ( cid : 123 ) k in ( 123 ) are general stochastic processes ( i . e . , indexed collections of random variables ) ; this allows very general forms of dependency among dps .
our hierarchical approach ts into this framework; we endow the stick - breaking weights ( cid : 123 ) k in ( 123 ) with a second subscript indexing the groups j , and view the weights ( cid : 123 ) jk as dependent for each xed value of k .
indeed , as we show in section 123 , the denition in ( 123 ) yields a specic , canonical form of dependence among the weights
our approach is also a special case of a framework referred to as analysis of densities ( ande ) by tomlinson ( 123 ) and tomlinson and escobar ( 123 ) .
the ande model is a hierarchical model for multiple dps in which the common base measure g123 is random , but rather than treating g123 as a draw from a dp , as in our case , it is treated as a draw from a mixture of dps .
the resulting g123 is continuous in general ( antoniak 123 ) , which , as we have discussed , is ruinous for our problem of sharing clusters .
it is an appropriate choice , however , for the problem addressed by tomlin - son ( 123 ) , which is that of sharing statistical strength among multiple sets of density estimation problems .
thus , while the ande framework and our hierarchical dp framework are closely related formally , the inferential goal is rather different .
moreover , as we will see , our restriction to discrete g123 has important implications for the design of efcient mcmc inference algorithms .
the terminology of hierarchical dirichlet process has also been used by m uller et al .
( 123 ) to describe a different notion of hierarchy than the one discussed here .
these authors consider a model in which a coupled set of random measures gj are dened as gj = ( cid : 123 ) f123 + ( 123 ( cid : 123 ) ( cid : 123 ) ) fj , where f123 and the fj are draws from dps .
this model provides an alternative approach to sharing clusters , one in which the shared clusters are given the same stick - breaking weights ( those associated with f123 ) in each of the groups .
by contrast , in our hierarchical model , the draws gj are based on the same underlying base measure g123 , but each draw assigns different stick - breaking weights to the shared atoms associated with g123
thus , atoms can be partially shared .
finally , the terminology of hierarchical dirichlet process has been used in yet a third way by beal et al .
( 123 ) in the context of a model known as the innite hidden markov model , a hidden markov model with a countably innite state space .
the hierarchical dirichlet process of beal et al .
( 123 ) is , however , not a hierarchy in the bayesian sense; rather , it is an algorithmic description of a coupled set of urn models .
we discuss this model in more detail in section 123 , where we show that the notion of hierarchical dp presented here yields an elegant treatment of the innite hidden
in summary , the notion of hierarchical dirichlet process that we explore is a specic example of a dependency model for multiple dirichlet processes , one specically aimed at the problem of sharing clusters among related groups of data .
it involves a simple bayesian hierarchy where the base measure for a set of dirichlet processes is itself distributed according to a dirichlet process .
while there are many ways to couple dirichlet processes , we view this simple , canonical bayesian hierarchy as particularly worthy of study .
note in particular the appealing recursiveness of the denition; a hierarchical dirichlet process can be readily extended to multiple hierarchical levels .
this is natural in applications .
for example , in our application to document modeling , one level of hierarchy is needed to share clusters among multiple documents within a corpus , and second level of hierarchy is needed to share clusters among multiple corpora .
similarly , in the genetics example , it is of interest to consider nested subdivisions of populations according to various criteria ( geographic , cultural , economic ) , and to consider the ow of haplotypes on the resulting tree .
as is the case with other nonparametric bayesian methods , a signicant component of the chal -
lenge in working with the hierarchical dirichlet process is computational .
to provide a general framework for designing procedures for posterior inference in the hierarchical dirichlet process that parallel those available for the dirichlet process , it is necessary to develop analogs for the hi - erarchical dirichlet process of some of the representations that have proved useful in the dirichlet process setting .
we provide these analogs in section 123 where we discuss a stick - breaking repre - sentation of the hierarchical dirichlet process , an analog of the p olya urn model that we refer to as the chinese restaurant franchise , and a representation of the hierarchical dirichlet process in terms of an innite limit of nite mixture models .
with these representations as background , we present mcmc algorithms for posterior inference under hierarchical dirichlet process mixtures in section 123
we present experimental results in section 123 and present our conclusions in section 123
we are interested in problems where the observations are organized into groups , and assumed ex - changeable both within each group and across groups .
to be precise , letting j index the groups and i index the observations within each group , we assume that xj123; xj123; : : : are exchangeable within each group j .
we also assume that the observations are exchangeable at the group level , that is , if xj = ( xj123; xj123; : : : ) denote all observations in group j , then x123; x123; : : : are exchangeable .
assuming each observation is drawn independently from a mixture model , there is a mixture component associated with each observation .
let ( cid : 123 ) ji denote a parameter specifying the mixture component associated with the observation xji .
we will refer to the variables ( cid : 123 ) ji as factors .
note that these variables are not generally distinct; we will develop a different notation for the distinct values of factors .
let f ( ( cid : 123 ) ji ) denote the distribution of xji given the factor ( cid : 123 ) ji .
let gj denote a prior distribution for the factors ( cid : 123 ) j = ( ( cid : 123 ) j123; ( cid : 123 ) j123; : : : ) associated with group j .
we assume that the factors are conditionally independent given gj .
thus we have the following probability model :
( cid : 123 ) ji j gj ( cid : 123 ) gj xji j ( cid : 123 ) ji ( cid : 123 ) f ( ( cid : 123 ) ji )
for each j and i , for each j and i ,
to augment the specication given in ( 123 ) .
123 dirichlet processes
in this section , we provide a brief overview of dirichlet processes .
after a discussion of basic denitions , we present three different perspectives on the dirichlet process : one based on the stick - breaking construction , one based on a polya urn model , and one based on a limit of nite mixture models .
each of these perspectives has an analog in the hierarchical dirichlet process , which is described in section 123
let ( ( cid : 123 ) ; b ) be a measurable space , with g123 a probability measure on the space .
let ( cid : 123 ) 123 be a positive real number .
a dirichlet process dp ( ( cid : 123 ) 123; g123 ) is dened to be the distribution of a random probability measure g over ( ( cid : 123 ) ; b ) such that , for any nite measurable partition ( a123; a123; : : : ; ar ) of ( cid : 123 ) , the random vector ( g ( a123 ) ; : : : ; g ( ar ) ) is distributed as a nite - dimensional dirichlet distri - bution with parameters ( ( cid : 123 ) 123g123 ( a123 ) ; : : : ; ( cid : 123 ) 123g123 ( ar ) ) :
( g ( a123 ) ; : : : ; g ( ar ) ) ( cid : 123 ) dir ( ( cid : 123 ) 123g123 ( a123 ) ; : : : ; ( cid : 123 ) 123g123 ( ar ) ) :
we write g ( cid : 123 ) dp ( ( cid : 123 ) 123; g123 ) if g is a random probability measure with distribution given by the dirichlet process .
the existence of the dirichlet process was established by ferguson ( 123 ) .
123 the stick - breaking construction
measures drawn from a dirichlet process are discrete with probability one ( ferguson 123 ) .
this property is made explicit in the stick - breaking construction due to sethuraman ( 123 ) .
the stick - breaking construction is based on independent sequences of i . i . d .
random variables ( ( cid : 123 ) 123
k j ( cid : 123 ) 123; g123 ( cid : 123 ) beta ( 123; ( cid : 123 ) 123 )
( cid : 123 ) k j ( cid : 123 ) 123; g123 ( cid : 123 ) g123 :
now dene a random measure g as
( cid : 123 ) k = ( cid : 123 ) 123
( 123 ( cid : 123 ) ( cid : 123 ) 123
where ( cid : 123 ) ( cid : 123 ) is a probability measure concentrated at ( cid : 123 ) .
sethuraman ( 123 ) showed that g as dened in this way is a random probability measure distributed according to dp ( ( cid : 123 ) 123; g123 ) .
k=123 constructed by ( 123 ) and ( 123 ) satises it is important to note that the sequence ( cid : 123 ) = ( ( cid : 123 ) k ) 123 k=123 ( cid : 123 ) k = 123 with probability one .
thus we may interpret ( cid : 123 ) as a random probability measure on the positive integers .
for convenience , we shall write ( cid : 123 ) ( cid : 123 ) gem ( ( cid : 123 ) 123 ) if ( cid : 123 ) is a random probability measure dened by ( 123 ) and ( 123 ) ( gem stands for grifths , engen and mccloskey; e . g .
see pitman
123 the chinese restaurant process
a second perspective on the dirichlet process is provided by the p olya urn scheme ( blackwell and macqueen 123 ) .
the polya urn scheme shows that draws from the dirichlet process are both discrete and exhibit a clustering property .
the polya urn scheme does not refer to g directly; it refers to draws from g .
thus , let ( cid : 123 ) 123; ( cid : 123 ) 123; : : : be a sequence of i . i . d .
random variables distributed according to g .
that is , the variables ( cid : 123 ) 123; ( cid : 123 ) 123; : : : are conditionally independent given g , and hence exchangeable .
let us consider the successive conditional distributions of ( cid : 123 ) i given ( cid : 123 ) 123; : : : ; ( cid : 123 ) i ( cid : 123 ) 123 , where g has been integrated out .
blackwell and macqueen ( 123 ) showed that these conditional distributions have the following form :
( cid : 123 ) i j ( cid : 123 ) 123; : : : ; ( cid : 123 ) i ( cid : 123 ) 123; ( cid : 123 ) 123; g123 ( cid : 123 )
i ( cid : 123 ) 123 + ( cid : 123 ) 123
i ( cid : 123 ) 123 + ( cid : 123 ) 123
we can interpret the conditional distributions in terms of a simple urn model in which a ball of a distinct color is associated with each atom .
the balls are drawn equiprobably; when a ball is drawn it is placed back in the urn together with another ball of the same color .
in addition , with probability proportional to ( cid : 123 ) 123 a new atom is created by drawing from g123 and a ball of a new color is added to
expression ( 123 ) shows that ( cid : 123 ) i has positive probability of being equal to one of the previous draws .
moreover , there is a positive reinforcement effect; the more often a point is drawn , the more likely it is to be drawn in the future .
to make the clustering property explicit , it is helpful to introduce a new set of variables that represent distinct values of the atoms .
dene ( cid : 123 ) 123; : : : ; ( cid : 123 ) k to be the distinct values taken on by ( cid : 123 ) 123; : : : ; ( cid : 123 ) i ( cid : 123 ) 123 , and let mk be the number of values ( cid : 123 ) i123 that are equal to ( cid : 123 ) k for 123 ( cid : 123 ) i123 < i .
we can re - express ( 123 ) as
( cid : 123 ) i j ( cid : 123 ) 123; : : : ; ( cid : 123 ) i ( cid : 123 ) 123; ( cid : 123 ) 123; g123 ( cid : 123 )
i ( cid : 123 ) 123 + ( cid : 123 ) 123
i ( cid : 123 ) 123 + ( cid : 123 ) 123
figure 123 : ( left ) a representation of a dirichlet process mixture model as a graphical model .
( right ) a hierarchical dirichlet process mixture model .
in the graphical model formalism , each node in the graph is associated with a random variable , where shading denotes an observed variable .
rectangles denote replication of the model within the rectangle .
sometimes the number of replicates is given in the bottom right corner of the rectangle .
using a somewhat different metaphor , the polya urn scheme is closely related to a distribution on partitions known as the chinese restaurant process ( aldous 123 ) .
this metaphor has turned out to be useful in considering various generalizations of the dirichlet process ( pitman 123a ) , and it will be useful in this paper .
the metaphor is as follows .
consider a chinese restaurant with an unbounded number of tables .
each ( cid : 123 ) i corresponds to a customer who enters the restaurant , while the distinct values ( cid : 123 ) k correspond to the tables at which the customers sit .
the ith customer sits at the table indexed by ( cid : 123 ) k , with probability proportional to the number of customers mk already seated there ( in which case we set ( cid : 123 ) i = ( cid : 123 ) k ) , and sits at a new table with probability proportional to ( cid : 123 ) 123 ( increment k , draw ( cid : 123 ) k ( cid : 123 ) g123 and set ( cid : 123 ) i = ( cid : 123 ) k ) .
123 dirichlet process mixture models
one of the most important applications of the dirichlet process is as a nonparametric prior on the parameters of a mixture model .
in particular , suppose that observations xi arise as follows :
( cid : 123 ) i j g ( cid : 123 ) g xi j ( cid : 123 ) i ( cid : 123 ) f ( ( cid : 123 ) i ) ;
where f ( ( cid : 123 ) i ) denotes the distribution of the observation xi given ( cid : 123 ) i .
the factors ( cid : 123 ) i are conditionally independent given g , and the observation xi is conditionally independent of the other observations given the factor ( cid : 123 ) i .
when g is distributed according to a dirichlet process , this model is referred to as a dirichlet process mixture model .
a graphical model representation of a dirichlet process mixture model is shown in figure 123 ( left ) .
since g can be represented using a stick - breaking construction ( 123 ) , the factors ( cid : 123 ) i take on values ( cid : 123 ) k with probability ( cid : 123 ) k .
we may denote this using an indicator variable zi which takes on positive integral values and is distributed according to ( cid : 123 ) ( interpreting ( cid : 123 ) as a random probability measure on
the positive integers ) .
hence an equivalent representation of a dirichlet process mixture is given by the following conditional distributions :
( cid : 123 ) j ( cid : 123 ) 123 ( cid : 123 ) gem ( ( cid : 123 ) 123 ) ( cid : 123 ) k j g123 ( cid : 123 ) g123
zi j ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
xi j zi; ( ( cid : 123 ) k ) 123
k=123 ( cid : 123 ) f ( ( cid : 123 ) zi ) :
moreover , g =p123
k=123 ( cid : 123 ) k ( cid : 123 ) ( cid : 123 ) k and ( cid : 123 ) i = ( cid : 123 ) zi .
123 the innite limit of nite mixture models
a dirichlet process mixture model can be derived as the limit of a sequence of nite mixture mod - els , where the number of mixture components is taken to innity ( neal 123; rasmussen 123; green and richardson 123; ishwaran and zarepour 123 ) .
this limiting process provides a third perspective on the dirichlet process .
suppose we have l mixture components .
let ( cid : 123 ) = ( ( cid : 123 ) 123; : : : ( cid : 123 ) l ) denote the mixing proportions .
note that we previously used the symbol ( cid : 123 ) to denote the weights associated with the atoms in g .
we have deliberately overloaded the denition of ( cid : 123 ) here; as we shall see later , they are closely related .
in fact , in the limit l ! 123 these vectors are equivalent up to a random size - biased permutation of their entries ( pitman 123 ) .
we place a dirichlet prior on ( cid : 123 ) with symmetric parameters ( ( cid : 123 ) 123=l; : : : ; ( cid : 123 ) 123=l ) .
let ( cid : 123 ) k denote the parameter vector associated with mixture component k , and let ( cid : 123 ) k have prior distribution g123
drawing an observation xi from the mixture model involves picking a specic mixture component with probability given by the mixing proportions; let zi denote that component .
we thus have the
( cid : 123 ) j ( cid : 123 ) 123 ( cid : 123 ) dir ( ( cid : 123 ) 123=l; : : : ; ( cid : 123 ) 123=l )
zi j ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
( cid : 123 ) k j g123 ( cid : 123 ) g123
xi j zi; ( ( cid : 123 ) k ) l
k=123 ( cid : 123 ) f ( ( cid : 123 ) zi ) :
let gl = pl
integrable with respect to g123 , we have , as l ! 123 :
k=123 ( cid : 123 ) k ( cid : 123 ) ( cid : 123 ) k .
ishwaran and zarepour ( 123 ) show that for every measurable function f
z f ( ( cid : 123 ) ) dgl ( ( cid : 123 ) ) d ( cid : 123 ) ! z f ( ( cid : 123 ) ) dg ( ( cid : 123 ) ) :
a consequence of this is that the marginal distribution induced on the observations x123; : : : ; xn ap - proaches that of a dirichlet process mixture model .
123 hierarchical dirichlet processes
we propose a nonparametric bayesian approach to the modeling of grouped data , where each group is associated with a mixture model , and where we wish to link these mixture models .
by analogy with dirichlet process mixture models , we rst dene the appropriate nonparametric prior , which we refer to as the hierarchical dirichlet process .
we then show how this prior can be used in the grouped mixture model setting .
we present analogs of the three perspectives presented earlier for the dirichlet processa stick - breaking construction , a chinese restaurant process representation , and a representation in terms of a limit of nite mixture models .
a hierarchical dirichlet process is a distribution over a set of random probability measures over ( ( cid : 123 ) ; b ) .
the process denes a set of random probability measures gj , one for each group , and a
global random probability measure g123
the global measure g123 is distributed as a dirichlet process with concentration parameter ( cid : 123 ) and base probability measure h :
g123 j ( cid : 123 ) ; h ( cid : 123 ) dp ( ( cid : 123 ) ; h ) ;
and the random measures gj are conditionally independent given g123 , with distributions given by a dirichlet process with base probability measure g123 :
gj j ( cid : 123 ) 123; g123 ( cid : 123 ) dp ( ( cid : 123 ) 123; g123 ) :
the hyperparameters of the hierarchical dirichlet process consist of the baseline probability measure h , and the concentration parameters ( cid : 123 ) and ( cid : 123 ) 123
the baseline h provides the prior distribu - tion for the factors ( cid : 123 ) ji .
the distribution g123 varies around the prior h , with the amount of variability governed by ( cid : 123 ) .
the actual distribution gj over the factors in the jth group deviates from g123 , with the amount of variability governed by ( cid : 123 ) 123
if we expect the variability in different groups to be dif - ferent , we can use a separate concentration parameter ( cid : 123 ) j for each group j .
in this paper , following escobar and west ( 123 ) , we put vague gamma priors on ( cid : 123 ) and ( cid : 123 ) 123
a hierarchical dirichlet process can be used as the prior distribution over the factors for grouped data .
for each j let ( cid : 123 ) j123; ( cid : 123 ) j123; : : : be i . i . d .
random variables distributed as gj .
each ( cid : 123 ) ji is a factor corresponding to a single observation xji .
the likelihood is given by :
( cid : 123 ) ji j gj ( cid : 123 ) gj xji j ( cid : 123 ) ji ( cid : 123 ) f ( ( cid : 123 ) ji ) :
this completes the denition of a hierarchical dirichlet process mixture model .
the corresponding graphical model is shown in figure 123 ( right ) .
the hierarchical dirichlet process can readily be extended to more than two levels .
that is , the base measure h can itself be a draw from a dp , and the hierarchy can be extended for as many levels as are deemed useful .
in general , we obtain a tree in which a dp is associated with each node , in which the children of a given node are conditionally independent given their parent , and in which the draw from the dp at a given node serves as a base measure for its children .
the atoms in the stick - breaking representation at a given node are thus shared among all descendant nodes , providing a notion of shared clusters at multiple levels of resolution .
123 the stick - breaking construction
given that the global measure g123 is distributed as a dirichlet process , it can be expressed using a
where ( cid : 123 ) k ( cid : 123 ) h independently and ( cid : 123 ) = ( ( cid : 123 ) k ) 123 has support at the points ( cid : 123 ) = ( ( cid : 123 ) k ) 123 can thus be written as :
k=123 ( cid : 123 ) gem ( ( cid : 123 ) ) are mutually independent .
since g123 k=123 , each gj necessarily has support at these points as well , and
let ( cid : 123 ) j = ( ( cid : 123 ) jk ) 123 given g123 ) .
we now describe how the weights ( cid : 123 ) j are related to the global weights ( cid : 123 ) .
note that the weights ( cid : 123 ) j are independent given ( cid : 123 ) ( since the gj are independent
let ( a123; : : : ; ar ) be a measurable partition of ( cid : 123 ) and let kl = fk : ( cid : 123 ) k 123 alg for l = 123; : : : ; r .
note that ( k123; : : : ; kr ) is a nite partition of the positive integers .
further , assuming that h is non - atomic , the ( cid : 123 ) ks are distinct with probability one , so any partition of the positive integers cor - responds to some partition of ( cid : 123 ) .
thus , for each j we have :
( gj ( a123 ) ; : : : ; gj ( ar ) ) ( cid : 123 ) dir ( ( cid : 123 ) 123g123 ( a123 ) ; : : : ; ( cid : 123 ) 123g123 ( ar ) )
( cid : 123 ) jk; : : : ; xk123kr
a ( cid : 123 ) dir123
( cid : 123 ) k; : : : ; ( cid : 123 ) 123 xk123kr
for every nite partition of the positive integers .
hence each ( cid : 123 ) j is independently distributed accord - ing to dp ( ( cid : 123 ) 123; ( cid : 123 ) ) , where we interpret ( cid : 123 ) and ( cid : 123 ) j as probability measures on the positive integers .
if h is non - atomic then a weaker result still holds : if ( cid : 123 ) j ( cid : 123 ) dp ( ( cid : 123 ) 123; ( cid : 123 ) ) then gj as given in ( 123 ) is still dp ( ( cid : 123 ) 123; g123 ) distributed .
as in the dirichlet process mixture model , since each factor ( cid : 123 ) ji is distributed according to gj , it takes on the value ( cid : 123 ) k with probability ( cid : 123 ) jk .
again let zji be an indicator variable such that ( cid : 123 ) ji = ( cid : 123 ) zji .
given zji we have xji ( cid : 123 ) f ( ( cid : 123 ) zji ) .
thus we obtain an equivalent representation of the hierarchical dirichlet process mixture via the following conditional distributions :
( cid : 123 ) j ( cid : 123 ) ( cid : 123 ) gem ( ( cid : 123 ) )
( cid : 123 ) j j ( cid : 123 ) 123; ( cid : 123 ) ( cid : 123 ) dp ( ( cid : 123 ) 123; ( cid : 123 ) )
( cid : 123 ) k j h ( cid : 123 ) h
zji j ( cid : 123 ) j ( cid : 123 ) ( cid : 123 ) j
xji j zji; ( ( cid : 123 ) k ) 123
k=123 ( cid : 123 ) f ( ( cid : 123 ) zji ) :
we now derive an explicit relationship between the elements of ( cid : 123 ) and ( cid : 123 ) j .
recall that the stick -
breaking construction for dirichlet processes denes the variables ( cid : 123 ) k in ( 123 ) as follows :
k ( cid : 123 ) beta ( 123; ( cid : 123 ) )
( cid : 123 ) k = ( cid : 123 ) 123
( 123 ( cid : 123 ) ( cid : 123 ) 123
using ( 123 ) , we show that the following stick - breaking construction produces a random probability measure ( cid : 123 ) j ( cid : 123 ) dp ( ( cid : 123 ) 123; ( cid : 123 ) ) :
jk ( cid : 123 ) beta ( cid : 123 ) 123 ( cid : 123 ) k; ( cid : 123 ) 123 123 ( cid : 123 )
( cid : 123 ) jk = ( cid : 123 ) 123
( 123 ( cid : 123 ) ( cid : 123 ) 123
to derive ( 123 ) , rst notice that for a partition ( f123; : : : ; k ( cid : 123 ) 123g; fkg; fk + 123; k + 123; : : : g ) , ( 123 ) gives :
( cid : 123 ) jl ! ( cid : 123 ) dir ( cid : 123 ) 123
( cid : 123 ) l; ( cid : 123 ) 123 ( cid : 123 ) k; ( cid : 123 ) 123
removing the rst element , and using standard properties of the dirichlet distribution , we have :
l=123 ( cid : 123 ) jl ( cid : 123 ) jk;
( cid : 123 ) jl ! ( cid : 123 ) dir ( cid : 123 ) 123 ( cid : 123 ) k; ( cid : 123 ) 123
and observe that 123 ( cid : 123 ) pk
l=123 ( cid : 123 ) l = p123
l=k+123 ( cid : 123 ) l to obtain ( 123 ) .
finally , dene ( cid : 123 ) 123 together with ( 123 ) , ( 123 ) and ( 123 ) , this completes the description of the stick - breaking construction for hierarchical dirichlet processes .
figure 123 : a depiction of a chinese restaurant franchise .
each restaurant is represented by a rectan - gle .
customers ( ( cid : 123 ) jis ) are seated at tables ( circles ) in the restaurants .
at each table a dish is served .
the dish is served from a global menu ( ( cid : 123 ) k ) , whereas the parameter jt is a table - specic indicator that serves to index items on the global menu .
the customer ( cid : 123 ) ji sits at the table to which it has been assigned in ( 123 ) .
123 the chinese restaurant franchise
in this section we describe an analog of the chinese restaurant process for hierarchical dirichlet processes that we refer to as the chinese restaurant franchise .
in the chinese restaurant franchise , the metaphor of the chinese restaurant process is extended to allow multiple restaurants which share a set of dishes .
the metaphor is as follows ( see figure 123 ) .
we have a restaurant franchise with a shared menu across the restaurants .
at each table of each restaurant one dish is ordered from the menu by the rst customer who sits there , and it is shared among all customers who sit at that table .
multiple tables in multiple restaurants can serve the same dish .
in this setup , the restaurants correspond to groups and the customers correspond to the factors ( cid : 123 ) ji .
we also let ( cid : 123 ) 123; : : : ; ( cid : 123 ) k denote k i . i . d .
random variables distributed according to h; this is the global menu of dishes .
we also introduce variables jt which represent the table - specic choice of dishes; in particular , jt is the dish served at table t in restaurant j .
note that each ( cid : 123 ) ji is associated with one jt , while each jt is associated with one ( cid : 123 ) k .
we introduce indicators to denote these associations .
in particular , let tji be the index of the jt associ - ated with ( cid : 123 ) ji , and let kjt be the index of ( cid : 123 ) k associated with jt .
in the chinese restaurant franchise metaphor , customer i in restaurant j sat at table tji while table t in restaurant j serves dish kjt .
we also need a notation for counts .
in particular , we need to maintain counts of customers and counts of tables .
we use the notation njtk to denote the number of customers in restaurant j at table t eating dish k .
marginal counts are represented with dots .
thus , njt ( cid : 123 ) represents the number of customers in restaurant j at table t and nj ( cid : 123 ) k represents the number of customers in restaurant j eating dish k .
the notation mjk denotes the number of tables in restaurant j serving dish k .
thus , mj ( cid : 123 ) represents the number of tables in restaurant j , m ( cid : 123 ) k represents the number of tables serving dish k , and m ( cid : 123 ) ( cid : 123 ) the total number of tables occupied .
let us now compute marginals under a hierarchical dirichlet process when g123 and gj are
integrated out .
first consider the conditional distribution for ( cid : 123 ) ji given ( cid : 123 ) j123; : : : ; ( cid : 123 ) j;i ( cid : 123 ) 123 and g123 , where gj is integrated out .
from ( 123 ) :
( cid : 123 ) ji j ( cid : 123 ) j123; : : : ; ( cid : 123 ) j;i ( cid : 123 ) 123; ( cid : 123 ) 123; g123 ( cid : 123 )
i ( cid : 123 ) 123 + ( cid : 123 ) 123
( cid : 123 ) jt +
i ( cid : 123 ) 123 + ( cid : 123 ) 123
this is a mixture , and a draw from this mixture can be obtained by drawing from the terms on the right - hand side with probabilities given by the corresponding mixing proportions .
if a term in the rst summation is chosen then we set ( cid : 123 ) ji = jt and let tji = t for the chosen t .
if the second term is chosen then we increment mj ( cid : 123 ) by one , draw jmj ( cid : 123 ) ( cid : 123 ) g123 and set ( cid : 123 ) ji = jmj ( cid : 123 ) and tji = mj ( cid : 123 ) .
now we proceed to integrate out g123
notice that g123 appears only in its role as the distribution of the variables jt .
since g123 is distributed according to a dirichlet process , we can integrate it out by using ( 123 ) again and write the conditional distribution of jt as :
jt j 123; 123; : : : ; 123; : : : ; j t ( cid : 123 ) 123; ( cid : 123 ) ; h ( cid : 123 )
m ( cid : 123 ) ( cid : 123 ) + ( cid : 123 )
m ( cid : 123 ) ( cid : 123 ) + ( cid : 123 )
if we draw jt via choosing a term in the summation on the right - hand side of this equation , we set jt = ( cid : 123 ) k and let kjt = k for the chosen k .
if the second term is chosen then we increment k by one , draw ( cid : 123 ) k ( cid : 123 ) h and set jt = ( cid : 123 ) k and kjt = k .
this completes the description of the conditional distributions of the ( cid : 123 ) ji variables .
to use these equations to obtain samples of ( cid : 123 ) ji , we proceed as follows .
for each j and i , rst sample ( cid : 123 ) ji using ( 123 ) .
if a new sample from g123 is needed , we use ( 123 ) to obtain a new sample jt and set ( cid : 123 ) ji = jt .
note that in the hierarchical dirichlet process the values of the factors are shared between the
groups , as well as within the groups .
this is a key property of hierarchical dirichlet processes .
123 the innite limit of nite mixture models
as in the case of a dirichlet process mixture model , the hierarchical dirichlet process mixture model can be derived as the innite limit of nite mixtures .
in this section , we present two apparently different nite models that both yield the hierarchical dirichlet process mixture in the innite limit , each emphasizing a different aspect of the model .
consider the following collection of nite mixture models , where ( cid : 123 ) is a global vector of mixing
proportions and ( cid : 123 ) j is a group - specic vector of mixing proportions :
( cid : 123 ) j ( cid : 123 ) ( cid : 123 ) dir ( ( cid : 123 ) =l; : : : ; ( cid : 123 ) =l )
( cid : 123 ) j j ( cid : 123 ) 123; ( cid : 123 ) ( cid : 123 ) dir ( ( cid : 123 ) 123 ( cid : 123 ) )
( cid : 123 ) k j h ( cid : 123 ) h
zji j ( cid : 123 ) j ( cid : 123 ) ( cid : 123 ) j
xji j zji; ( ( cid : 123 ) k ) l
k=123 ( cid : 123 ) f ( ( cid : 123 ) zji ) :
the parametric hierarchical prior for ( cid : 123 ) and ( cid : 123 ) in ( 123 ) has been discussed by mackay and peto ( 123 ) as a model for natural languages .
we will show that the limit of this model as l ! 123 is the hierarchical dirichlet process .
let us consider the random probability measures gl k=123 ( cid : 123 ) jk ( cid : 123 ) ( cid : 123 ) k .
as in section 123 , for every measurable function f integrable with respect to h we have
z f ( ( cid : 123 ) ) dgl
123 ( ( cid : 123 ) ) d ( cid : 123 ) ! z f ( ( cid : 123 ) ) dg123 ( ( cid : 123 ) ) ;
as l ! 123
further , using standard properties of the dirichlet distribution , we see that ( 123 ) still holds for the nite case for partitions of f123; : : : ; lg; hence we have :
j ( cid : 123 ) dp ( ( cid : 123 ) 123; gl
it is now clear that as l ! 123 the marginal distribution this nite model induces on x approaches the hierarchical dirichlet process mixture model .
there is an alternative nite model whose limit is also the hierarchical dirichlet process mixture model .
instead of introducing dependencies between the groups by placing a prior on ( cid : 123 ) ( as in the rst nite model ) , each group can instead choose a subset of t mixture components from a model - wide set of l mixture components .
in particular consider the following model :
( cid : 123 ) j ( cid : 123 ) ( cid : 123 ) dir ( ( cid : 123 ) =l; : : : ; ( cid : 123 ) =l )
( cid : 123 ) j j ( cid : 123 ) 123 ( cid : 123 ) dir ( ( cid : 123 ) 123=t; : : : ; ( cid : 123 ) 123=t )
( cid : 123 ) k j h ( cid : 123 ) h
kjt j ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) tji j ( cid : 123 ) j ( cid : 123 ) ( cid : 123 ) j
xji j tji; ( kjt ) t
k=123 ( cid : 123 ) f ( ( cid : 123 ) kjtji
as t ! 123 and l ! 123 , the limit of this model is the chinese restaurant franchise process; hence the innite limit of this model is also the hierarchical dirichlet process mixture model .
in this section we describe three related markov chain monte carlo sampling schemes for the hi - erarchical dirichlet process mixture model .
the rst is a straightforward gibbs sampler based on the chinese restaurant franchise , the second is based upon an augmented representation involving both the chinese restaurant franchise and the posterior for g123 , while the third is a variation on the second sampling scheme with streamlined bookkeeping .
to simplify the discussion we assume that the base distribution h is conjugate to the data distribution f ; this allows us to focus on the issues specic to the hierarchical dirichlet process .
the nonconjugate case can be approached by adapt - ing to the hierarchical dirichlet process techniques developed for nonconjugate dp mixtures ( neal 123 ) .
moreover , in this section we assume xed values for the concentration parameters ( cid : 123 ) 123 and ( cid : 123 ) ; we present a sampler for these parameters in the appendix .
we recall the random variables of interest .
the variables xji are the observed data .
each xji is assumed to arise as a draw from a distribution f ( ( cid : 123 ) ji ) .
let the factor ( cid : 123 ) ji be associated with the table tji in the restaurant representation; i . e . , let ( cid : 123 ) ji = jtji .
the random variable jt is an instance of mixture component kjt; i . e . , jt = ( cid : 123 ) kjt .
the prior over the parameters ( cid : 123 ) k is h .
let zji = kjtji denote the mixture component associated with the observation xji .
we use the notation njtk to denote the number of customers in restaurant j at table t eating dish k , while mjk denotes the number of tables in restaurant j serving dish k .
marginal counts are represented with dots .
let x = ( xji : all j; i ) , xjt = ( xji : all i with tji = t ) , t = ( tji : all j; i ) , k = ( kjt : all j; t ) , z = ( zji : all j; i ) , m = ( mjk : all j; k ) and ( cid : 123 ) = ( ( cid : 123 ) 123; : : : ; ( cid : 123 ) k ) .
when a superscript is attached to a set of variables or a count , e . g . , x ( cid : 123 ) ji , k ( cid : 123 ) jt or n ( cid : 123 ) ji jt ( cid : 123 ) , this means that the variable corresponding to the superscripted index is removed from the set or from the calculation of the count .
examples , x ( cid : 123 ) ji = xnxji , k ( cid : 123 ) jt = knkjt and n ( cid : 123 ) ji is the number of observations in group j whose factor is associated with jt , leaving out item xji .
let f ( ( cid : 123 ) ) have density f ( ( cid : 123 ) j ( cid : 123 ) ) and h have density h ( ( cid : 123 ) ) .
since h is conjugate to f we integrate out the mixture component parameters ( cid : 123 ) in the sampling schemes .
denote the conditional density
of xji under mixture component k given all data items except xji as
( xji ) = r f ( xjij ( cid : 123 ) k ) qj 123i123=ji;zj 123i123 =k f ( xj 123i123j ( cid : 123 ) k ) h ( ( cid : 123 ) k ) d ( cid : 123 ) k
r qj 123i123=ji;zj 123i123 =k f ( xj 123i123j ( cid : 123 ) k ) h ( ( cid : 123 ) k ) d ( cid : 123 ) k
similarly denote f ( cid : 123 ) xjt mixture component k leaving out xjt .
( xjt ) as the conditional density of xjt given all data items associated with
