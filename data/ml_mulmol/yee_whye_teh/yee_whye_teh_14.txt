the indian buffet process ( ibp ) is a bayesian nonparametric distribution whereby objects are modelled using an unbounded number of latent features .
in this paper we derive a stick - breaking representation for the ibp .
based on this new rep - resentation , we develop slice samplers for the ibp that are efcient , easy to implement and are more generally applicable than the currently available gibbs sampler .
this representation , along with the work of thibaux and jordan ( 123 ) , also illuminates interesting theoretical connec - tions between the ibp , chinese restaurant pro - cesses , beta processes and dirichlet processes .
the indian buffet process ( ibp ) is a distribution over bi - nary matrices consisting of n > 123 rows and an unbounded number of columns ( 123 ) .
these binary matrices can be inter - preted as follows : each row corresponds to an object , each column to a feature , and a 123 in entry ( i , k ) indicates object i has feature k .
for example , objects can be movies like terminator 123 , shrek and shanghai knights , while features can be action , comedy , stars jackie chan , and the matrix can be ( 123; 123; 123 ) in matlab notation .
like the chinese restaurant process ( crp ) ( 123 ) , the ibp provides a tool for dening nonparametric bayesian mod - els with latent variables .
however , unlike the crp , in which each object belongs to one and only one of innitely many latent classes , the ibp allows each object to possess potentially any combination of innitely many latent fea - tures .
this added exibility has resulted in a great deal of interest in the ibp , and the development of a range of inter - esting applications .
these applications include models for choice behaviour ( 123 ) , protein - protein interactions ( 123 ) , the structure of causal graphs ( 123 ) , dyadic data for collabora - tive ltering applications ( 123 ) , and human similarity judg -
in this paper , we derive a new , stick - breaking represen - tation for the ibp , a development which is analogous to sethuramans seminal stick - breaking representation for crps ( 123 ) .
in this representation , as we will see in sec - tion 123 , the probability of each feature is represented explic - itly by a stick of length between 123 and 123
sethuramans representation paved the way for both novel samplers for and generalizations of crps ( 123 ) .
similarly , we show how our novel stick - breaking representation of the ibp can be used to develop new slice samplers for ibps that are ef - cient , easy to implement and have better applicability to non - conjugate models ( sections 123 , 123 , 123 ) .
this new repre - sentation also suggests generalizations of the ibp ( such as a pitman - yor variant , in section 123 ) .
moreover , although our stick - breaking representation of the ibp was derived from a very different model than the crp , we demonstrate a surprising duality between the sticks in these two repre - sentations which suggests deeper connections between the two models ( section 123 ) .
the theoretical developments we describe here , which show a stick - breaking representation which is to the ibp what sethuramans construction is to the crp , along with the recent work of thibaux and jordan ( 123 ) , showing that a particular subclass of beta processes is to the ibp as the dirichlet process is to the crp , rmly establish the ibp in relation to the well - known classes of bayesian nonparametric models .
123 indian buffet processes
the ibp is dened as the limit of a corresponding distri - bution over matrices with k columns , as the number of columns k .
let z be a random binary n k ma - trix , and denote entry ( i , k ) in z by zik .
for each feature k let k be the prior probability that feature k is present in an object .
we place a beta ( k , 123 ) prior on k , with being the strength parameter of the ibp .
the full model is :
k , 123 )
indepedently i , k
let us now consider integrating out the ks and taking the
limit of k to obtain the ibp .
for the rst object , the chance of it having each particular feature k is inde - k once k is integrated out , thus the distribu - tion over the number of features it has is binomial ( k , k ) .
as k , this approaches poisson ( ) .
for subse - quent objects i = 123 , .
, n , the probability of it also having a feature k already belonging to a previous object is the number of objects prior to i with feature k .
re - peating the argument for the rst object , object i will also have poisson ( i ) new features not belonging to pre - vious objects .
note that even though the total number of available features is unbounded , the actual number k + of used features is always nite ( and in fact is distributed as
i where m<i k = pj<i zjk > 123
k +m<i k k +123+i123 m<i k
the above generative process can be understood using the metaphor of an indian buffet restaurant .
customers ( ob - jects ) come into the restaurant one at a time , and can sample an innite number of dishes ( features ) at the buffet counter .
each customer will try each dish that previous customers have tried with probabilities proportional to how popular each dish is; in addition the customer will try a number of new dishes that others have not tried before .
to complete the model , let k be parameters associated with feature k and xi be an observation associated with ob - ject i
xi f ( zi , : , : )
where h is the prior over parameters , f ( zi , : , : ) is the data distribution given the features zi , : = ( zik ) ing to object i and feature parameters : = ( k ) assume that f ( zi , : , : ) depends only on the parameters of the present features .
123 gibbs sampler
the above generative process for the ibp can be used di - rectly in a gibbs sampler for posterior inference of z and given data x = ( xi ) ( 123 ) .
the representation consists of the number k + of used ( active ) features , the matrix z123 : n , 123 : k+ of occurrences among the k + active features , and their pa - rameters 123 : k+ .
the superscript + denotes active features .
the sampler iterates through i = 123 , .
, n , for each object i it updates the feature occurrences for the currently used features , then considers adding new features to model the
for the already used features k = 123 , .
, k + , the condi - tional probability of zik = 123 given other variables is just
p ( zik = 123|rest ) mi k
n f ( xi|zi , k , zik = 123 , 123 : k+ )
where mi k = pj123=i zjk .
the fraction is the conditional
prior of zik = 123 , obtained by using exchangeability among
the customers and taking customer i to be the last customer to enter the restaurant; the second term f ( | ) is the data likelihood of xi if zik = 123
it is possible to integrate 123 : k+ out from the likelihood term if h is conjugate to f .
in fact it is important for h to be conjugate to f when we consider the probabilities of new features being introduced , because all possible parameters for these new features have to be taken into account .
if li is the number of new features introduced , we have
n ) li e z f ( xi|zi , 123 : k+ , z
i , 123 : li = 123 , 123 : k+ ,
are occurrences for the new features and are their parameters , the superscript denoting cur -
rently unused ( inactive ) features .
the fraction comes from the probability of introducing li new features under n ) while the second term is the data likelihood , with the parameters integrated out with respect to the prior density h ( ) .
the need to integrate out the parameters for new features is similar to the need to integrate out parameters for new clusters in the dirichlet process ( dp ) mixture model case ( see ( 123 ) ) .
to perform this integration efciently , conju - gacy is important , but the requirement for conjugacy limits the applicability of the ibp in more elaborate settings .
it is possible to devise samplers in the non - conjugate case anal - ogous to those developed for dp mixture models ( 123 , 123 ) .
in the next section we develop a different representation of the ibp in terms of a stick - breaking construction , which leads us to an easy to implement slice sampler for the non -
123 stick breaking construction
in this section , we describe an alternative representation of the ibp where the feature probabilities are not integrated out , and a specic ordering is imposed on the features .
we call this the stick - breaking construction for the ibp .
we will see that the new construction bears strong rela - tionships with the standard stick - breaking construction for crps , paving the way to novel generalizations of and in - ference techniques for the ibp .
let ( 123 ) > ( 123 ) > .
> ( k ) be a decreasing ordering of 123 : k = ( 123 , .
, k ) , where each l is beta ( k , 123 ) .
we will show that in the limit k the ( k ) s obey the following law , which we shall refer to as the stick - breaking construction for the ibp ,
beta ( , 123 ) ( k ) = ( k ) ( k123 ) =
we start by considering ( 123 ) .
for nite k it is
( 123 ) = max
where each l is beta (
k , 123 ) and has density :
i ( 123 l 123 )
where i ( a ) is the indicator function for a condition ( mea - surable set ) a : i ( a ) = 123 if a is true , and 123 otherwise .
the cumulative distribution function ( cdf ) for l is then :
f ( l ) = z l
k 123i ( 123 t 123 ) dt
i ( 123 l 123 ) + i ( 123 < l )
since the ls are independent , the cdf of ( 123 ) is just the product of the cdfs of each l , so
f ( ( 123 ) ) = ( cid : 123 )
i ( 123 ( 123 ) 123 ) + i ( 123 < ( 123 ) < ) ( cid : 123 ) k
i ( 123 ( 123 ) 123 ) + i ( 123 < ( 123 ) )
differentiating , we see that the density of ( 123 ) is
p ( ( 123 ) ) = 123
i ( 123 ( 123 ) 123 )
and therefore ( 123 ) beta ( , 123 ) .
we now derive the densities for subsequent ( k ) s .
for each k 123 let lk be such that lk = ( k ) and let lk = ( 123 , .
, k ) \ ( l123 , .
since ( 123 : k ) = ( ( 123 ) , .
, ( k ) ) are the k largest values among 123 : k , we have
( k123 ) = ( k )
for each l lk .
restricting the range of l to ( 123 , ( k ) ) , the
f ( l| ( 123 : k ) ) = r l
k 123 dt k 123 dt
i ( 123 l ( k ) ) + i ( ( k ) < l )
now ( k+123 ) = maxllk l with each l independent given ( 123 : k ) .
the cdf of ( k+123 ) is again the product of the cdfs of l over l lk ,
i ( 123 ( k+123 ) ( k ) ) + i ( ( k ) < ( k+123 ) )
i ( 123 ( k+123 ) ( k ) ) + i ( ( k ) < ( k+123 ) )
as k .
differentiating , the density of ( k+123 ) is ,
i ( 123 ( k+123 ) ( k ) )
notice that the ( k ) s have a markov structure , with ( k+123 ) conditionally independent of ( 123 : k123 ) given ( k ) .
finally , instead of working with the variables ( k ) directly , we introduce a new set of variables ( k ) = ( k ) range ( 123 , 123 ) .
using a change of variables , the density of ( k ) is derived to be ,
p ( ( k ) | ( 123 : k123 ) ) = 123
i ( 123 ( k ) 123 )
thus ( k ) are independent from ( 123 : k123 ) and are simply beta ( , 123 ) distributed .
expanding ( k ) = ( k ) ( k123 ) =
l=123 ( l ) , we obtain the stick - breaking construction ( 123 ) .
the construction ( 123 ) can be understood metaphorically as follows .
we start with a stick of length 123
at iteration k = 123 , 123 , .
. , we break off a piece at a point ( k ) relative to the current length of the stick .
we record the length ( k ) of the stick we just broke off , and recurse on this piece , discarding the other piece of stick .
123 relation to dp
in iteration k of the construction ( 123 ) , after breaking the stick in two we always recurse on the stick whose length we de - note by ( k ) .
let ( k ) be the length of the other discarded stick .
we have ,
( k ) = ( 123 ( k ) ) ( k123 ) = ( 123 ( k ) )
making a change of variables v ( k ) = 123 ( k ) ,
( k ) = v ( k )
thus ( 123 : ) are the resulting stick lengths in a standard stick - breaking construction for dps ( 123 , 123 ) .
in both constructions the nal weights of interest are the lengths of the sticks .
in dps , the weights ( k ) are the lengths of sticks discarded , while in ibps , the weights ( k ) are the lengths of sticks we have left .
this difference leads to the different properties of the weights : for dps , the stick lengths sum to a length of 123 and are not decreasing , while in ibps the stick lengths need not sum to 123 but are decreasing .
both stick - breaking constructions are shown in figure 123
in both the weights decrease exponentially quickly in ex -
the direct correspondence to stick - breaking in dps implies that a range of techniques for and extensions to the dp can be adapted for the ibp .
for example , we can generalize the ibp by replacing the beta ( , 123 ) distribution on ( k ) s with other distributions .
one possibility is a pitman - yor ( 123 ) extension of the ibp , dened as
( k ) beta ( + kd , 123 d )
figure 123 : stick - breaking construction for the dp and ibp .
the black stick at top has length 123
at each iteration the vertical black line represents the break point .
the brown dotted stick on the right is the weight obtained for the dp , while the blue stick on the left is the weight obtained for
where d ( 123 , 123 ) and > d .
the pitman - yor ibp weights decrease in expectation as a o ( k 123 d ) power - law , and this may be a better t for some naturally occurring data which have a larger number of features with signi - cant but small weights ( 123 ) .
an example technique for the dp which we could adapt to the ibp is to truncate the stick - breaking construction after a certain number of break points and to perform inference in the reduced space .
( 123 ) gave a bound for the error introduced by the truncation in the dp case which can be used here as well .
let k be the truncation level .
we set ( k ) = 123 for each k > k , while the joint density of ( 123 : k ) is ,
p ( ( 123 : k ) ) =
i ( 123 ( k ) ( 123 ) 123 )
the conditional distribution of z given ( 123 : k ) is simply123
p ( z| ( 123 : k ) ) =
with zik = 123 for k > k .
gibbs sampling in this represen - tation is straightforward , the only point to note being that adaptive rejection sampling ( ars ) ( 123 ) should be used to sample each ( k ) given other variables ( see next section ) .
123 slice sampler
gibbs sampling in the truncated stick - breaking construc - tion is simple to implement , however the predetermined truncation level seems to be an arbitrary and unneces - in this section , we propose a non - approximate scheme based on slice sampling , which can be
123note that we are making a slight abuse of notation by using z both to denote the original ibp matrix with arbitrarily ordered columns , and the equivalent matrix with the columns reordered to decreasing s .
similarly for the feature parameters s .
seen as adaptively choosing the truncation level at each it - eration .
slice sampling is an auxiliary variable method that samples from a distribution by sampling uniformly from the region under its density function ( 123 ) .
this turns the problem of sampling from an arbitrary distribution to sam - pling from uniform distributions .
slice sampling has been successfully applied to dp mixture models ( 123 ) , and our ap - plication to the ibp follows a similar thread .
in detail , we introduce an auxiliary slice variable ,
s|z , ( 123 : ) uniform ( 123 , )
where is a function of ( 123 : ) and z , and is chosen to be the length of the stick for the last active feature ,
= min ( cid : 123 ) 123 , min
the joint distribution of z and the auxiliary variable s is
p ( s , ( 123 : ) , z ) = p ( z , ( 123 : ) ) p ( s|z , ( 123 : ) )
where p ( s|z , ( 123 : ) ) = 123 i ( 123 s ) .
clearly , integrat -
ing out s preserves the original distribution over ( 123 : ) and z , while conditioned on z and ( 123 : ) , s is simply drawn from ( 123 ) .
given s , the distribution of z becomes :
p ( z|x , s , ( 123 : ) ) p ( z|x , ( 123 : ) ) 123
i ( 123 s ) ( 123 )
which forces all columns k of z for which ( k ) < s to be zero .
let k be the maximal feature index with ( k ) > s .
thus zik = 123 for all k > k , and we need only consider updating those features k k .
notice that k serves as a truncation level insofar as it limits the computational costs to a nite amount without approximation .
let k be an index such that all active features have in - dex k < k ( note that k itself would be an inactive fea - ture ) .
the computational representation for the slice sam - pler consists of the slice variables and the rst k features : hs , k , k , z123 : n , 123 : k , ( 123 : k ) , 123 : k i .
the slice sampler proceeds by updating all variables in turn .
update s .
the slice variable is drawn from ( 123 ) .
if the new value of s makes k k ( equivalently , s < ( k ) ) , then we need to pad our representation with inactive features until k < k .
in the appendix we show that the stick lengths ( k ) for new features k can be drawn iteratively from the following distribution :
p ( ( k ) | ( k123 ) , z : , >k = 123 ) exp ( pn
( k ) ( 123 ( k ) ) n i ( 123 ( k ) ( k123 ) )
i ( 123 ( k ) ) i )
we used ars to draw samples from ( 123 ) since it is log - concave in log ( k ) .
the columns for these new features are initialized to z : , k = 123 and their parameters drawn from their prior k h .
update z .
given s , we only need to update zik for each i and k k .
the conditional probabilities are :
p ( zik = 123|rest )
f ( xi|zi , k , zik = 123 , 123 : k )
the denominator is needed when different values of zik induces different values of by changing the index of the last active feature .
update k .
for each k = 123 , .
, k , the conditional prob - ability of k is :
f ( xi|zi , 123 : k , k , k )
update ( k ) .
for k = 123 , .
, k 123 , combining ( 123 ) and ( 123 ) , the conditional probability of ( k ) is
( 123 ( k ) ) n mk i ( ( k+123 ) ( k ) ( k123 ) )
where mk = pn
i=123 zik .
for k = k , in addition to tak - ing into account the probability of features k is inactive , we also have to take into account the probability that all columns of z beyond k are inactive as well .
the ap - pendix shows that the resulting conditional probability of ( k ) is given by ( 123 ) with k = k .
we draw from both ( 123 ) and ( 123 ) using ars .
123 change of representations
both the stick - breaking construction and the standard ibp representation are different representations of the same nonparametric object .
in this section we consider updates which change from one representation to the other .
more precisely , given a posterior sample in the stick - breaking representation we wish to construct a posterior sample in the ibp representation and vice versa .
such changes of representation allow us to make use of efcient mcmc moves in both representations , e . g .
interlacing split - merge moves in ibp representation ( 123 ) with the slice sampler in stick - breaking representation .
furthermore , since both stick lengths and the ordering of features are integrated out in the ibp representation , we can efciently update both in the stick - breaking representation by changing to the ibp representation and back .
we appeal to the innite limit formulation of both repre - sentations to derive the appropriate procedures .
in particu - lar , we note that the ibp is obtained by ignoring the order - ing on features and integrating out the weights ( 123 : k ) in an arbitrarily large nite model , while the stick - breaking con - struction is obtained by enforcing an ordering with decreas - ing weights .
thus , given a sample in either representations , our approach is to construct a corresponding sample in an
arbitrarily large nite model , then to either ignore the or - dering and weights ( to get ibp ) or to enforce the decreasing weight ordering ( to get stick - breaking ) .
changing from stick - breaking to the standard ibp repre - sentation is easy .
we simply drop the stick lengths as well as the inactive features , leaving us with the k + active feature columns along with the corresponding parameters .
to change from ibp back to the stick - breaking represen - tation , we have to draw both the stick lengths and order the features in decreasing stick lengths , introducing inac - tive features into the representation if required .
we may index the k + active features in the ibp representation as k = 123 , .
, k + in the nite model .
let z123 : n , 123 : k+ be the feature occurrence matrix .
suppose that we have k ( cid : 123 ) k+ features in the nite model .
for the active features , the pos - terior for the lengths are simply
k |z : , k beta (
+ mk , 123 + n mk )
beta ( mk , 123 + n mk )
as k .
for the rest of the k k + inactive fea - tures , it is sufcient to consider only those inactive features with stick lengths larger than mink + k .
thus we consider a decreasing ordering ( 123 ) > on these lengths .
( 123 ) gives their densities in the k limit and ars k .
fi - can be used to draw nally , the stick - breaking representation is obtained by re - ( 123 : k ) in decreasing order , with the fea - ture columns and parameters taking on the same ordering ( columns and parameters corresponding to inactive features are set to 123 and drawn from their prior respectively ) , giving us k + + k features in the stick - breaking representation .
( k ) < mink +
( 123 : k ) until
123 semi - ordered stick - breaking
in deriving the change of representations from the ibp to the stick - breaking representation , we made use of an in - termediate representation whereby the active features are unordered , while the inactive ones have an ordering of de - creasing stick lengths .
it is in fact possible to directly work with this representation , which we shall call semi - ordered
the representation consists of k + active and unordered features , as well as an ordered sequence of inactive fea - tures .
the stick lengths for the active features have condi -
k |z : , k beta ( m , k , 123 + n m , k )
while for the inactive features we have a markov property :
( k123 ) , z : , >k = 123 ) exp ( pn
123 slice sampler
to use the semi - ordered stick - breaking construction as a representation for inference , we can again use the slice sampler to adaptively truncate the representation for inac - tive features .
this gives an inference scheme which works in the non - conjugate case , is not approximate , has an adap - tive truncation level , but without the restrictive ordering constraint of the stick - breaking construnction .
the repre - sentation hs , k + , z123 : n , 123 : k+ , + 123 : k+ , 123 : k+i consists only of the k + active features and the slice variable s ,
s uniform ( 123 , ) = min ( cid : 123 ) 123 , min
once a slice value is drawn , we generate k inactive features , with their stick lengths drawn from ( 123 ) until ( k +123 ) < s .
the associated feature columns z
are initialized to 123 and the parameters 123 : k drawn from their prior .
sampling for the feature entries and parameters for both the active and just generated inactive features pro - ceed as before .
afterwards , we drop from the list of active features any that became inactive , while we add to the list any inactive feature that became active .
finally , the stick lengths for the new list of active features are drawn from their conditionals ( 123 ) .
in this section we compare the mixing performance of the two proposed slice samplers against gibbs sampling .
we chose a simple synthetic dataset so that we can be assured of convergence to the true posterior and that mixing times can be estimated reliably in a reasonable amount of compu - tation time .
we also chose to apply the three samplers on a conjugate model since gibbs sampling requires conjugacy , although our implementation of the two slice samplers did not make use of this .
in the next section we demonstrate the modelling performance of a non - conjugate model us - ing the semi - ordered slice sampler on a dataset of mnist
we used the conjugate linear - gaussian binary latent fea - ture model for comparing the performances of the different samplers ( 123 ) .
each data point xi is modelled using a spher - ical gaussian with mean zi , : a and variance 123 x , where zi , : is the row vector of feature occurrences corresponding to xi , and a is a matrix whose kth row forms the parameters for the kth feature .
entries of a are drawn i . i . d .
from a zero a .
we generated 123 , 123 and mean gaussian with variance 123 123 dimensional datasets from the model with data variance xed at 123 x = 123 , varying values of the strength parameter = 123 , 123 and the latent feature variance 123 a = 123 , 123 , 123 , 123
for each combination of parameters we produced ve datasets with 123 data points , giving a total of 123 datasets .
for all datasets , we xed 123 a to the generating values and learned the feature matrix z and .
x and 123
stickbreaking semiordered gibbs sampling
figure 123 : autocorrelation times for k + for the slice sam - pler in decreasing stick lengths ordering , in semi - ordered stick - breaking representation , and for the gibbs sampler .
for each dataset and each sampler , we repeated 123 runs of 123 , 123 iterations .
we used the autocorrelation coefcients of the number of represented features k + and ( with a maximum lag of 123 ) as measures of mixing time .
we found that mixing in k + is slower than in for all datasets and report results only for k + here .
we also found that in this regime the autocorrelation times do not vary with a .
in figure 123 we report the auto - dimensionality or with 123 correlation times of k + over all runs , all datasets , and all three samplers .
as expected , the slice sampler using the de - creasing stick lengths ordering was always slower than the semi - ordered one .
surprisingly , we found that the semi - ordered slice sampler was just as fast as the gibbs sampler which fully exploits conjugacy .
this is about as well as we would expect a more generally applicable non - conjugate sampler to perform .
in this section we apply the semi - ordered slice sampler to 123 examples of handwritten images of 123s in the mnist dataset .
the model we worked with is a generalization of that in section 123 , where in addition to modelling feature occurrences , we also model per object features values ( 123 ) .
in particular , let y be a matrix of the same size as z , with i . i . d .
zero mean unit variance gaussian entries .
we model each xi as
xi|z , y , a , 123
x n ( ( zi , : ( cid : 123 ) yi , : ) a , 123
where ( cid : 123 ) is elementwise multiplication .
specication for the rest of the model is as in section 123
we can integrate y or a out while maintaining tractability , but not both .
the handwritten digit images are rst preprocessed by pro - jecting on to the rst 123 pca components , and the sampler ran for 123 iterations .
the trace plot of the log likeli - hood and the distribution of the number of active features are shown in figure 123
the model succesfully nds latent features to reconstruct the images as shown in figure 123
some of the latent features found are shown in figure 123
most appear to model local shifts of small edge segments
k ( feature label )
# active feats
# active feats
figure 123 : top - left : the log likelihood trace plot .
the sam - pler quickly nds a high likelihood region .
top - right : his - togram of the number of active features over the 123 iter - ations .
bottom - left : number of images sharing each feature during the last mcmc iteration .
bottom - right : histogram of the number of active features used by each input image .
note that about half of the features are used by only a few data points , and each data point is represented by a small subset of the active features .
figure 123 : features that are shared between many digits .
form a = p
a direct consequence of our stick - breaking con - struction is that a draw from such a beta process has the k=123 ( k ) k with ( k ) drawn from ( 123 ) and k drawn i . i . d .
from the base measure h .
this is a par - ticularly simply case of a more general construction called the inverse levy measure ( 123 , 123 ) .
generalizations to us - ing other stick - breaking constructions automatically lead to generalizations of the beta process , and we are currently exploring a number of possibilities , including the pitman - yor extension .
finally , the duality observed in section 123 seems to be a hitherto unknown connection between the beta process and the dp which we are currently trying to
as an aside , it is interesting to note the importance of fea - ture ordering in the development of the ibp .
to make the derivation rigorous , ( 123 ) had to carefully ignore the feature ordering by considering permutation - invariant equivalence classes before taking the innite limit .
in this paper , we de - rived the stick - breaking construction by imposing a feature ordering with decreasing feature weights .
to conclude , our development of a stick - breaking construc - tion for the ibp has lead to interesting insights and connec - tions , as well as practical algorithms such as the new slice
figure 123 : last column : original digits .
second last column : reconstructed digits .
other columns : features used for re -
we thank the reviewers for insightful comments .
ywt thanks the lee kuan yew endowment fund for funding .
of the digits , and are reminiscent of the result of learning models with sparse priors ( e . g .
ica ) on such images ( 123 ) .
123 discussion and future work
we have derived novel stick - breaking representations of the indian buffet process .
based on these representations new mcmc samplers are proposed that are easy to implement and work on more general models than gibbs sampling .
in experiments we showed that these samplers are just as efcient as gibbs without using conjugacy .
( 123 ) have recently showed that the ibp is a distribution on matrices induced by the beta process with a constant strength parameter of 123
this relation to the beta process is proving to be a fertile ground for interesting develop -
