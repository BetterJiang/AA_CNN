in this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini - batches .
by adding the right amount of noise to a standard stochastic gradient optimization al - gorithm we show that the iterates will con - verge to samples from the true posterior dis - tribution as we anneal the stepsize .
this seamless transition between optimization and bayesian posterior sampling provides an in - built protection against overtting .
we also propose a practical method for monte carlo estimates of posterior statistics which moni - tors a sampling threshold and collects sam - ples after it has been surpassed .
we apply the method to three models : a mixture of gaussians , logistic regression and ica with
in recent years there has been an increasing amount of very large scale machine learning datasets , ranging from internet trac and network data , computer vi - sion , natural language processing , to bioinformatics .
more and more advances in machine learning are now driven by these large scale data , which oers the op - portunity to learn large and complex models for solv - ing many useful applied problems .
recent successes in large scale machine learning have mostly been opti - mization based approaches .
while there are sophisti - cated algorithms designed specically for certain types of models , one of the most successful class of algo - rithms are stochastic optimization , or robbins - monro , algorithms .
these algorithms process small ( mini -
appearing in proceedings of the 123 th international con - ference on machine learning , bellevue , wa , usa , 123
copyright 123 by the author ( s ) / owner ( s ) .
) batches of data at each iteration , updating model parameters by taking small gradient steps in a cost function .
often these algorithms are run in an on - line setting , where the data batches are discarded af - ter processing and only one pass through the data is performed , reducing memory requirements drastically .
one class of methods left - behind by the recent ad - vances in large scale machine learning are the bayesian methods .
this has partially to do with the negative results in bayesian online parameter estimation ( an - drieu et al . , 123 ) , but also the fact that each iteration of typical markov chain monte carlo ( mcmc ) algo - rithms requires computations over the whole dataset .
nevertheless , bayesian methods are appealing in their ability to capture uncertainty in learned parameters and avoid overtting .
arguably with large datasets there will be little overtting .
alternatively , as we have access to larger datasets and more computational resources , we become interested in building more com - plex models , so that there will always be a need to quantify the amount of parameter uncertainty .
in this paper , we propose a method for bayesian learn - ing from large scale datasets .
our method combines robbins - monro type algorithms which stochastically optimize a likelihood , with langevin dynamics which injects noise into the parameter updates in such a way that the trajectory of the parameters will converge to the full posterior distribution rather than just the maximum a posteriori mode .
the resulting algorithm starts o being similar to stochastic optimization , then automatically transitions to one that simulates sam - ples from the posterior using langevin dynamics .
in section 123 we introduce the two ingredients of our stochastic optimization and langevin dy - namics .
section 123 describes our algorithm and how it converges to the posterior distribution .
section 123 describes a practical method of estimating when our algorithm will transition from stochastic optimization to langevin dynamics .
section 123 demonstrates our al -
stochastic gradient langevin dynamics
gorithm on a few models and section 123 concludes .
let denote a parameter vector , with p ( ) a prior distribution , and p ( x| ) the probability of data item x given our model parameterized by .
the posterior distribution of a set of n data items x = ( xi ) n is : p ( |x ) p ( ) in the optimization literature the prior regularizes the parameters while the likelihood terms constitute the cost function to be optimized , and the task is to nd the maximum .
a popular class a posteriori ( map ) parameters of methods called stochastic optimization ( robbins & monro , 123 ) operates as follows .
at each iteration t , a subset of n data items xt = ( xt123 , .
, xtn ) is given , and the parameters are updated as follows :
log p ( t ) + n
where t is a sequence of step sizes .
the general idea is that the gradient computed on the subset is used to approximate the true gradient over the whole dataset .
over multiple iterations the whole dataset is used and the noise in the gradient caused by using subsets rather than the whole dataset averages out .
for large datasets where the subset gradient approx - imation is accurate enough , this can give signicant computational savings over using the whole dataset to compute gradients at each iteration .
to ensure convergence to a local maximum , in addition to other technical assumptions , a major requirement is for the step sizes to satisfy the property
intuitively , the rst constraint ensures that parameters will reach the high probability regions no matter how far away it was initialized to , while the second ensures that the parameters will converge to the mode instead of just bouncing around it .
typically , step sizes t = a ( b + t ) are decayed polynomially with ( 123 , 123 ) .
the issue with ml or map estimation , as stochas - tic optimization aims to do , is that they do not cap - ture parameter uncertainty and can potentially overt data .
the typical way in which bayesian approaches capture parameter uncertainty is via markov chain monte carlo ( mcmc ) techniques ( robert & casella , 123 ) .
in this paper we will consider a class of mcmc techniques called langevin dynamics ( neal , 123 ) .
as before , these take gradient steps , but also injects gaus - sian noise into the parameter updates so that they do
not collapse to just the map solution :
t n ( 123 , )
log p ( t ) +
the gradient step sizes and the variances of the in - jected noise are balanced so that the variance of the samples matches that of the posterior .
langevin dy - namics is motivated and originally derived as a dis - cretization of a stochastic dierential equation whose equilibrium distribution is the posterior distribution .
to correct for discretization error , one can take ( 123 ) to just be a proposal distribution and correct using interestingly , as we decrease the discretization error decreases as well so that the re - jection rate approaches zero .
however typical mcmc practice is to allow an initial adaptation phase where the step sizes are adjusted , followed by xing the step sizes to ensure a stationary markov chain thereafter .
more sophisticated techniques use hamiltonian dy - namics with momentum variables to allow parameters to move over larger distances without the inecient random walk behaviour of langevin dynamics ( neal , 123 ) .
however , to the extent of our knowledge all mcmc methods proposed thus far require computa - tions over the whole dataset at every iteration , result - ing in very high computational costs for large datasets .
stochastic gradient langevin
given the similarities between stochastic gradient al - gorithms ( 123 ) and langevin dynamics ( 123 ) , it is nat - ural to consider combining ideas from the two ap - proaches .
this allows ecient use of large datasets while allowing for parameter uncertainty to be cap - tured in a bayesian manner .
the approach is straight - forward : use robbins - monro stochastic gradients , add an amount of gaussian noise balanced with the step size used , and allow step sizes to go to zero .
the pro - posed update is simply :
log p ( t ) + n
t = t t n ( 123 , t )
where the step sizes decrease towards zero at rates sat - isfying ( 123 ) .
this allows averaging out of the stochastic - ity in the gradients , as well as mh rejection rates that go to zero asymptotically , so that we can simply ignore the mh acceptance steps , which require evaluation of probabilities over the whole dataset , all together .
stochastic gradient langevin dynamics
in the rest of this section we will give an intuitive argu - ment for why t will approach samples from the pos - terior distribution as t .
in particular , we will show that for large t , the updates ( 123 ) will approach langevin dynamics ( 123 ) , which converges to the poste - rior distribution
g ( ) = log p ( ) +
be the true gradient of the log probability at and
ht ( ) = log p ( ) + n
log p ( xti| ) g ( ) ( 123 )
the stochastic gradient is then g ( ) +ht ( ) , with ht ( ) a zero mean random variable ( due to the stochasticity of the data items chosen at step t ) with nite variance v ( ) , and ( 123 ) is ,
( g ( t ) + ht ( t ) ) + t ,
t n ( 123 , t )
there are two sources of stochasticity in ( 123 ) : the in - jected gaussian noise with variance t , and the noise in the stochastic gradient , which has variance ( t 123 ) 123v ( t ) .
the rst observation is that for large t , t 123 , and the injected noise will dominate the stochastic gradient noise , so that ( 123 ) will be eectively langevin dynam - ics ( 123 ) .
the second observation is that as t 123 , the discretization error of langevin dynamics will be negligible so that the mh rejection probability will ap - proach 123 and we may simply ignore this step .
in other words , ( 123 ) , ( 123 ) eectively dene a non - stationary markov chain such that the tth step tran - sition operator , for all large t , will have as its equilib - rium distribution the posterior over .
the next ques - tion we address is whether the sequence of parameters 123 , 123 , .
will converge to the posterior distribution .
because the markov chain is not stationary and the step sizes reduce to 123 , it is not immediately clear that this is the case .
to see that this is indeed true , we will show that a subsequence t123 , t123 , .
will converge to the posterior as intended so the whole sequence will
first x an 123 such that 123 < 123 123
since ( t ) satisfy the step size property ( 123 ) , we can nd a subsequence t=ts+123 t 123 as s .
t123 < t123 < such that since the injected noise at each step is independent , for
we now between steps ts and ts+123 will be o ( show that the total noise due to the stochasticity of the gradients among these steps will be dominated by the total injected noise .
since 123 123 , we may take
large enough s the total injected noise ,
123 123 for t between ts and ts+123
making the assumption that the gradient g ( ) vary smoothly ( e . g .
they are lipschitz continuous in the models in section 123 ) , the total stochastic gradient is :
( g ( t ) + ht ( t ) )
123 g ( ts ) + o ( 123 ) +
since the parameters did not vary much between ts and ts+123 , the stochasticity in ht ( t ) will be dominated by the randomness in the choice of the mini - batches .
assuming that these are chosen randomly and inde - pendently , ht ( t ) for each t will be basically iid ( if mini - batches were chosen by random partitioning of the whole dataset , ht ( t ) will be negatively correlated instead and will not change the results here ) .
thus the variance of
123 ht ( t ) is o (
123 ) and
123 g ( ts ) + o ( 123 ) + o 123 g ( ts ) + o ( 123 )
the last equation says that the total stochastic gra - dient step is approximately the exact gradient step at ts with a step size of 123 , with a deviation dominated by o ( 123 ) .
since this is in turn dominated by the total injected noise which is o ( 123 ) , this means that the se - quence t123 , t123 , .
will approach a sequence generated by langevin dynamics with a xed step size 123 , so it will converge to the posterior distribution .
note also that it will have innite eective sample size .
the implication of this argument is that we can use stochastic gradient langevin dynamics as an any - time and general - purpose algorithm .
in the initial phase the stochastic gradient noise will dominate and the algorithm will imitate an ecient stochastic gra - dient ascent algorithm .
in the later phase the injected noise will dominate , so the algorithm will imitate a langevin dynamics mh algorithm , and the algorithm will transition smoothly between the two .
however a disadvantage is that to guarantee the algorithm to work it is important for the step sizes to decrease to zero , so that the mixing rate of the algorithm will slow down with increasing number of iterations .
to address this , we can keep the step size constant once it has decreased below a critical level where the mh rejection rate is considered negligible , or use this al - gorithm for burn - in , but switch to a dierent mcmc algorithm that makes more ecient use of the whole dataset later .
these alternatives can perform better
stochastic gradient langevin dynamics
but will require further hand - tuning and are beyond the scope of this paper .
the point of this paper is to demonstrate a practical algorithm that can achieve proper bayesian learning using only mini - batch data .
posterior sampling
in this section we consider the use of our stochastic gradient langevin dynamics algorithm as one which produces samples from the posterior distribution .
we rst derive an estimate of when the algorithm will transition from stochastic optimization to langevin dynamics .
the idea is that we should only start col - lecting samples after it has entered its posterior sam - pling phase , which will not happen until after it be - comes langevin dynamics .
then we discuss how the algorithm scales with the dataset size n and give a rough estimate of the number of iterations required for the algorithm to traverse the whole posterior .
finally we discuss how the obtained samples can be used to form monte carlo estimates of posterior expectations .
transition into langevin dynamics phase
we rst generalize our method to allow for precon - ditioning , which can lead to signicant speed ups by better adapting the step sizes to the local structure of the posterior ( roberts & stramer , 123; girolami & calderhead , 123 ) .
for instance , certain dimensions may have a vastly larger curvature leading to much bigger gradients .
in this case a symmetric precondi - tioning matrix m can transform all dimensions to the same scale .
the preconditioned stochastic gradient langevin dynamics is simply ,
g ( t ) + ht ( t )
t n ( 123 , tm )
as noted previously , whether the algorithm is in the stochastic optimization phase or langevin dynamics phase depends on the variance of the injected noise , which is simply tm , versus that of the stochastic gra - dient .
since the stochastic gradient is a sum over the current mini - batch , if its size n is large enough the central limit theorem will kick in and the variations ht ( t ) around the true gradient g ( t ) will become nor - mally distributed .
its covariance matrix can then be estimated from the empirical covariance : v ( t ) v ( ht ( t ) ) n 123
( sti st ) ( sti st ) ( 123 )
where sti = log p ( xti|t ) + 123 of data item i at iteration t and st = 123 the empirical mean .
note that v ( t ) = n 123
log p ( t ) is the score i=123 sti is n vs , where
vs is the empirical covariance of the scores ( sti ) , so scales as n 123 n .
from this we see that the variance of the stochastic gradient step is 123 t n 123 123n m vsm , so that to get the injected noise to dominate in all directions , we need the condition
123 ) = 123
where max ( a ) is the largest eigenvalue of a .
in other words , if we choose a stepsize such that the sample threshold 123 , the algorithm will be in its langevin dynamics phase and will be sampling approximately from the posterior .
we can now relate the step size at the sampling thresh - old to the posterior variance via the fisher informa - tion , which is related to vs as if n vs , and to the posterior variance i f .
using these relationships as well as ( 123 ) , we see that the step size at the sam - pling threshold is t 123n n min ( ) .
since langevin dynamics explores the posterior via a random walk , using this step size implies that we need on the order of n / n steps to traverse the posterior , i . e .
we process the whole dataset .
so we see this method is not a silver bullet .
however , the advantage of the method is its convenience : stochastic optimization smoothly and automatically transitions into posterior sampling without changing the update equation .
even without measuring the sampling threshold one will enjoy the benet of protection against overtting and the ability to perform bayesian learning .
measuring the sampling threshold will only be important if one needs to faith - fully represent the posterior distribution with a nite collection of samples .
estimating posterior expectations since 123 , 123 , .
converges to the posterior distribution , we can estimate the posterior expectation e ( f ( ) ) of some function f ( ) by simply taking the sample av - t=123 f ( t ) ( as typically in mcmc , we may remove the initial burn - in phase , say estimated using the sampling threshold ) .
since f ( t ) is an asymptoti - cally unbiased estimator for e ( f ( ) ) , this sample aver - age will be consistent .
observe however that because the step size decreases , the mixing rate of the markov chain decreases as well , and the simple sample aver - age will over - emphasize the tail end of the sequence where there is higher correlation among the samples , resulting in higher variance in the estimator .
instead we propose to use the step sizes to weight the samples :
stochastic gradient langevin dynamics
figure 123
true and estimated posterior distribution .
figure 123
left : variances of stochastic gradient noise and injected noise .
right : rejection probability versus step size .
we report the average rejection probability per iteration in each sweep through the dataset .
t=123 t = , this estimator will be consistent as well .
the intuition is that the rate at which the markov chain mixes is proportional to the step size , so that we expect the eective sample size of ( 123 , .
, t ) to be proportional to t=123 t , and that each t will contribute an eective sample size proportional to t .
simple demonstration
we rst demonstrate the workings of our stochastic gradient langevin algorithm on a simple example in - volving only two parameters .
to make the posterior multimodal and a little more interesting , we use a mix - ture of gaussians with tied means :
x ) + 123
123 n ( 123 , 123
123 n ( 123 , 123 123 = 123 , 123
123 n ( 123 , 123 123 n ( 123 + 123 , 123 x = 123
123 data points are drawn from the model with 123 = 123 and 123 = 123
there is a mode at this parameter setting , but also a secondary mode at 123 = 123 , 123 = 123 , with strong neg - ative correlation between the parameters .
we ran the stochastic gradient langevin algorithm with a batch -
123 = 123 and 123
figure 123
average log joint probability per data item ( left ) and accuracy on test set ( right ) as functions of the num - ber of sweeps through the whole dataset .
red dashed line represents accuracy after 123 iterations .
results are aver - aged over 123 runs; blue dotted lines indicate 123 standard
size of 123 and using 123 sweeps through the whole dataset .
the step sizes are t = a ( b + t ) where = . 123 and a and b are set such that t decreases from . 123 to . 123 over the duration of the run .
we see from figure 123 that the estimated posterior distribu - tion is very accurate .
in figure 123 we see that there are indeed two phases to the stochastic gradient langevin algorithm : a rst phase where the stochastic gradient noise dominates the injected noise , and a second phase where the converse occurs .
to explore the scaling of the rejection rate as a function of step sizes , we reran the experiment with step sizes exponentially decreas - ing from 123 to 123
in the original experiment the dynamic range of the step sizes is not wide enough for visual inspection .
figure 123 ( right ) shows the rejection probability decreasing to zero as step size decreases .
logistic regression
we applied our stochastic gradient langevin algorithm to a bayesian logistic regression model .
the probabil - ity of the ith output yi ( 123 , +123 ) given the corre - sponding input vector xi is modelled as :
p ( yi|xi ) = ( yi
where are the parameters , and ( z ) = the bias parameter is absorbed into by including 123 as an entry in xi .
we use a laplace prior for with a scale of 123
the gradient of the log likelihood is :
log p ( yi|xi ) = ( yi
while the gradient of the prior is simply sign ( ) , which is applied elementwise .
we applied our inference algorithm to the a123a dataset derived by ( lin et al . , 123 ) from the uci adult dataset .
it consists of 123 observations and 123 fea - tures , and we used batch sizes of 123
results from 123
123iterationnoise variance 123 noise123 noiseinjected noise123step sizeaverage rejection rate123number of iterations through whole datasetlog joint probability per datum123 - 123 - 123 - 123 - 123 - 123 - 123 - 123 . 123 . 123 . 123number of iterations through whole datasetaccuracy on test data accuracy after 123 iterationsaccuracy123 . 123 . 123 . 123 stochastic gradient langevin dynamics
runs are shown in figure 123 , with the model trained on a random 123% of the dataset and tested on the other 123% in each run .
we see that both the joint probability and the accuracy increase rapidly , with the joint probability converging after at most 123 iterations , while the accuracy converging after less than 123 itera - tion through the dataset , demonstrating the eciency of the stochastic gradient langevin dynamics .
independent components analysis
in the following we will briey review a popular ica algorithm based on stochastic ( natural ) gradient opti - mization ( amari et al . , 123 ) .
we start from a proba - bilistic model that assumes independent , heavy tailed
p ( x , w ) = | det ( w ) |
n ( wij; 123 , ) where we have used a gaussian prior over the weights .
it has been found that the eciency of gradient descent can be signicantly improved if we use a natural gradi - ent .
this is implemented by post - multiplication of the gradient with the term w t w ( amari et al . , 123 ) .
if we choose pi ( yi ) = i x , we get
123 yi ) with yi = wt
123 cosh123 ( 123
. = w log ( p ( x , w ) ) w t w =
w w w t w ( 123 )
the term w t w acts like a preconditioning matrix ( see section 123 ) , mij , kl = ik ( w t w ) jl which is symmetric under the exchange ( i k , j l ) .
it can be shown 123 = ( cid : 123 ) ( w t w ) 123 , that the inverse of m is given by m
and the matrix square root as w t w with
m = ( cid : 123 ) 123 u t if w t w = uu t .
w t w = u 123
the update equation for langevin dynamics thus be -
wt+123 = wt +
123 tdw + ( cid : 123 ) t
where every element of ( cid : 123 ) t is normally distributed with variance t : ij , t n ( 123 , t ) .
our stochastic version simply approximates the part of the gradient that sums over data - cases with a sum over a small mini - batch of size n and multiplies the result with n / n to bring it back to the correct scale .
we also anneal the stepsizes according to t a ( b + t ) .
to assess how well our stochastic langevin approach compares against a standard bayesian method we im - plemented the corrected langevin mcmc sampler .
, as in eqn . 123
this sampler , proposes a new state w note however that we sum over all data - cases and that we do not anneal the stepsize .
secondly , we need to accept or reject the proposed step based on all the data - cases in order to guarantee detailed balance .
the proposal distribution is given by ( suppressing depen - dence on t ) ,
q ( w w ) = n
123 dw ; m
where the quadratic function in the exponent is con - veniently computed as ,
123 dw ) t ) 123 dw ) ( w t w ) 123 ( w 123 w and the normalization constant with w = w requires the quantity det m = det ( w t w ) d .
the ac - cept / reject step is then given by the usual metropolis
p ( accept ) = min
p ( w ) q ( w w )
finally , to compute the sampling threshold of eqn . 123 , we can use
log p ( w ) + log p ( xi|w )
( w t w ) 123
with covn the sample covariance over the mini - batch of n data - cases .
to show the utility of our bayesian variant of ica we dene the following instability metric for indepen -
where var ( wij ) is computed over the posterior sam - ples and var ( xj ) is computed over the data - cases .
the reason that we scale the variance of the weight entry wij with the variance of xj is that the vari - ance of the sources yi = j wijxj is approximately equal for all i because they are t to the distribution
123 cosh123 ( 123
artificial data
in the rst experiment we generated 123 data - cases iid in six channels .
three channels had high kurtosis distributions while three others where normally dis - tributed .
we ran stochastic langevin dynamics with
stochastic gradient langevin dynamics
figure 123
left two ( cid : 123 ) gures : amari distance over time for stochastic langevin dynamics and corrected langevin dynamics .
thick line represents the online average .
first few hundred iterations were removed to show the scale of the ( cid : 123 ) uctuations .
right two ( cid : 123 ) gures : instability index for the 123 independent components computed in section 123 . 123 for stochastic langevin dynamics and corrected langevin dynamics .
figure 123
posterior density estimates for arti ( cid : 123 ) cial dataset for stochastic langevin and corrected langevin dynamics mea - sured across the w123 ( cid : 123 ) w123 and w123;123 ( cid : 123 ) w123;123 axes .
a batch - size of 123 for a total of 123 , 123 iterations and a polynomial annealing schedule t = 123 after around 123 , 123 iterations the sampling threshold at = 123 was met .
at that point we recorded the mixing distance as d123 = t and collected samples t t from the last sample time only when the sum exceeded d123 ( in other words , as t decreases we col - lect fewer samples per unit time ) .
we note that simply collecting all samples had no noticeable impact on the nal results .
the last estimate of w was used to ini - tialize corrected langevin dynamics ( this was done to force the samplers into the same local maximum ) after which we also collected 123 , 123 samples .
for corrected langevin we used a constant stepsize of = 123 the two left gures of figure 123 show the amari dis - tance ( amari et al . , 123 ) over time for stochastic and corrected langevin dynamics respectively .
the right two gures show the sorted values of our pro - posed instability index .
figures 123 show two dimen - sional marginal density estimates of the posterior dis - tribution of w .
ica cannot determine the gaussian components and this fact is veried by looking at the posterior distribution .
in fact , the stochastic langevin algorithm has mixed over a number of modes that pre - sumably correspond to dierent linear combinations of the gaussian components .
to a lesser degree the cor - rected langevin has also explored two modes .
due to the complicated structure of the posterior distri -
bution the stability index varies strongly between the two sampling algorithms for the gaussian components ( and in fact also varies across dierent runs ) .
we veri - ed that the last three components correspond to sta - ble , high kurtosis components .
meg data
there are 123 channels and 123 time - points , from which we extracted the rst 123 channels for our experiment .
to initialize the sampling algorithms , we rst ran fastica ( hyvarinen , 123 ) to nd an initial estimate of the de - mixing matrix w .
we then ran stochastic langevin and corrected langevin dynamics to sample from the posterior .
the settings were very similar to the previous experiment with a 123 for stochastic langevin and schedule of t = 123 a constant stepsize of 123 / n for corrected langevin .
we obtained 123 , 123 samples for stochastic langevin in 123 seconds and 123 , 123 samples for corrected langevin in 123 seconds .
we visually veried that the two dimensional marginal distributions of stochastic langevin and corrected langevin dynamics were very similar .
the instability values are shown in gure 123
due to the absence of gaussian components we see that the stability indices are very similar across the two sampling algorithms .
it was veried that
123x 123iterationamari distanceamari distance stoc .
lan . 123 . 123x 123iterationamari distanceamari distance corr .
lan . 123sorted component idinstability metricinstability metric stoc .
lan . 123sorted component idinstability metricinstability metric corr .
lan . w ( 123 , 123 ) w ( 123 , 123 ) pdf w ( 123 , 123 ) vs w ( 123 , 123 ) stoc .
lan . 123w ( 123 , 123 ) w ( 123 , 123 ) pdf w ( 123 , 123 ) vs w ( 123 , 123 ) corr .
lan . 123w ( 123 , 123 ) w ( 123 , 123 ) pdf w ( 123 , 123 ) vs w ( 123 , 123 ) stoc .
lan . 123w ( 123 , 123 ) w ( 123 , 123 ) pdf w ( 123 , 123 ) vs w ( 123 , 123 ) corr .
lan . 123 stochastic gradient langevin dynamics
we believe that this work represents only a tentative rst step to further work on ecient mcmc sampling based on stochastic gradients .
interesting directions of research include stronger theory providing a solid proof of convergence , deriving a mh rejection step based on mini - batch data , extending the algorithm to the online estimation of dynamical systems , and deriving algorithms based on more sophisticated hamiltonian monte carlo approaches which do not suer from ran - dom walk behaviour .
this material is based upon work supported by the na - tional science foundation under grant no .
123 ( mw ) and the gatsby charitable foundation
