we describe a neurally - inspired , unsupervised learning algorithm that builds a non - linear generative model for pairs of face images from the same individual .
individuals are then recognized by nding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known .
our method compares favorably with other methods in the literature .
the generative model consists of a single layer of rate - coded , non - linear feature detectors and it has the property that , given a data vector , the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation .
the weights of the feature detectors are learned by com - paring the correlations of pixel intensities and feature activations in two phases : when the network is observing real data and when it is observing reconstructions of real data generated from the feature activations .
face recognition is difcult when the number of individuals is large and the test and train - ing images of an individual differ in expression , lighting or the date on which they were taken .
in addition to being an important application , face recognition allows us to evaluate different kinds of algorithm for learning to recognize or compare objects , since it requires accurate representation of ne discriminative features in the presence of relatively large within - individual variations .
this is made even more difcult when there are very few exemplars of each individual .
we start by describing a new unsupervised learning algorithm for a restricted form of boltz - mann machine ( 123 ) .
we then show how to generalize the generative model and the learning algorithm to deal with real - valued pixel intensities and rate - coded feature detectors .
we then consider alternative ways of applying the rate - coded model to face recognition .
123 inference and learning in restricted boltzmann machines
a restricted boltzmann machine ( 123 ) has a layer of visible units and a single layer of hid - den units with no hidden - to - hidden connections .
inference in an rbm is much easier than
in a general boltzmann machine and it is also much easier than in a causal belief net be - cause there is no explaining away ( 123 ) .
there is therefore no need to perform any iteration to determine the activities of the hidden units .
the hidden states , j , are conditionally in - dependent given the visible states , i , and the distribution of j is given by the standard
j = 123 =
123 ex i wij i
conversely , the hidden states of an rbm are marginally dependent so it is easy for an rbm to learn population codes in which units may be highly correlated .
it is hard to do this in causal belief nets with one hidden layer because the generative model of a causal belief net assumes marginal independence .
an rbm can be trained using the standard boltzmann machine learning algorithm which follows a noisy but unbiased estimate of the gradient of the log likelihood of the data .
one way to implement this algorithm is to start the network with a data vector on the visible units and then to alternate between updating all of the hidden units in parallel and updating all of the visible units in parallel .
each update picks a binary state for a unit from its posterior distribution given the current states of all the units in the other set .
if this alternating gibbs sampling is run to equilibrium , there is a very simple way to update the weights so as to minimize the kullback - leibler divergence , 123 , between the data produced by the rbm ( 123 ) :
123 , and the equilibrium distribution of fantasies over the visible units ,
wij / <ij> 123 <ij> 123
where < ij > 123 is the expected value of ij when data is clamped on the visible units and the hidden states are sampled from their conditional distribution given the data , and <ij> 123 is the expected value of ij after prolonged gibbs sampling .
this learning rule does not work well because it can take a long time to approach thermal equilibrium and the sampling noise in the estimate of < ij> 123 can swamp the gradient .
hinton ( 123 ) shows that it is far more effective to minimize the difference between 123 is the distribution of the one - step reconstructions of the data that are produced by rst picking binary hidden states from their conditional distribution given the data and then picking binary visible states from their conditional distribution given the hidden states .
the exact gradient of this contrastive divergence is complicated because 123 depends on the weights , but this dependence can safely be ignored to yield a simple and effective learning rule for following the approximate gradient of the
wij / <ij> 123 <ij> 123
123 applying rbms to face recognition
for images of faces , binary pixels are far from ideal .
a simple way to increase the rep - resentational power without changing the inference and learning procedures is to imagine that each visible unit , i , has 123 replicas which all have identical weights to the hidden units .
so far as the hidden units are concerned , it makes no difference which particular replicas are turned on : it is only the number of active replicas that counts .
so a pixel can now have 123 different intensities .
during reconstruction of the image from the hidden activities , all the replicas can share the computation of the probability , i , of turning on , and then we can select replicas to be on with probability 123 i123 123i .
we actually approximat - ed this binomial distribution by just adding a little gaussian noise to 123 i and rounding .
the same trick can be used for the hidden units .
123 is unaffected except that i and j are now the number of active replicas .
the replica trick can be interpreted as a cheap way of simulating an ensemble of neurons by assuming they have identical weights .
alternatively , it can be seen as a way of simulating a single neuron over a time interval in which it may produce multiple spikes that constitute a rate - code .
for this reason we call the model rbmrate .
we assumed that the visible units can produce up to 123 spikes and the hidden units can produce up to 123 spikes .
we also made two further approximations : we replaced i and j in eq .
123 by their expected values and we used the expected value of i when computing the probability of activiation of the hidden units .
however , we continued to use the stochastically chosen integer ring rates of the hidden units when computing the one - step reconstructions of the data , so the hidden activities cannot transmit an unbounded amount of information from the data to the
a simple way to use rbmrate for face recognition is to train a single model on the training set , and identify a face by nding the gallery image that produces a hidden activity vector that is most similar to the one produced by the face .
this is how eigenfaces are used for recognition , but it does not work well because it does not take into account the fact that some variations across faces are important for recognition , while some variations are not .
to correct this , we instead trained an rbmrate model on pairs of different images of the same individual , and then we used this model of pairs to decide which gallery image is best paired with the test image .
to account for the fact that the model likes some individual face images more than others , we dene the t between two faces f 123 and f123 as gf123; f123 gf123; f123 gf123; f123 gf123; f123 where gv123; v123 is the goodness score of the image pair v123; v123 under the model .
the goodness score is the negative free energy which is an additive function of the total input received by each hidden unit .
each hidden unit has a set of weights going to each image in the pair , and weight - sharing is not used , hence gv123; v123 123= gv123; v123
however , to preserve symmetry , each pair of images of the same individual v123; v123 in the training set has a reversed pair v 123; v123 in the set .
we trained the model with 123 hidden units on 123 image pairs ( 123 distinct pairs ) for 123 iterations in batches of 123 , with a learning rate of 123 : 123 123 123 for the weights , a learning rate of 123 123 for the biases , and a momentum of 123 : 123
one advantage of eigenfaces over correlation is that once the test image has been converted into a vector of eigenface activations , comparisons of test and gallery images can be made in the low - dimensional space of eigenface activations rather than the high - dimensional space of pixel intensities .
the same applies to our face - pair network .
the total input to each hidden unit from each gallery image can be precomputed and stored , while the total input from a test image only needs to be computed once for comparisons with all gallery images .
123 the feret database
our version of the feret database contained 123 frontal face images of 123 individuals taken over a period of a few years under varying lighting conditions .
of these images , 123 are used as both the gallery and the training set and the remaining 123 are divided into four , disjoint test sets : the expression test set contains 123 images of different individuals .
these individuals all have another image in the training set that was taken with the same lighting conditions at the same time but with a different expression .
the training set also includes a further 123 pairs of images that differ only in expression .
the days test set contains 123 images that come from 123 individuals .
each of these individuals has two images from the same session in the training set and two images taken in a session 123 days later or earlier in the test set .
a further 123 individuals were photographed 123 days apart and all 123 of these images are in the training set .
figure 123 : images are normalized in ve stages : a ) original image; b ) locate centers of eyes by hand; c ) rotate image; d ) crop image and subsample at 123 123 pixels; e ) mask out all of the background and some of the face , leaving 123 pixels in an oval shape; f ) equalize the intensity histogram .
figure 123 : examples of preprocessed faces .
the months test set is just like the days test set except that the time between sessions was at least three months and different lighting conditions were present in the two sessions .
this set contains 123 images of 123 individuals .
a further 123 images of 123 more individuals were included in the training set .
the glasses test set contains 123 images of 123 different individuals .
each of these individ - uals has two images in the training set that were taken in another session on the same day .
the training and test pairs for an individual differ in that one pair has glasses and the other does not .
the training set includes a further 123 images , half with glasses and half without , from 123 more individuals .
the frontal face images include the whole head , parts of the shoulder and neck , and back - ground .
instead of training on the whole images , which contain much irrelevant informa - tion , we trained on face images that were normalized as shown in gure 123
masking out all of the background inevitably looses the contour of the face which contains much discrim - inative information .
the histogram equalization step removes most lighting effects , but it also removes some relevant information like the skin tone .
for the best performance , the contour shape and skin tone would have to be used as an additional source of discriminative information .
some examples of the processed images are shown in gure 123
123 comparative results
we compared rbmrate with four popular face recognition methods .
the rst and simplest is correlation ( 123 ) , which returns the similarity score as the angle between two images represented as vectors of pixel intensities .
this performed better than using the euclidean distance as a score .
the second method is eigenfaces ( 123 ) , which rst projects the images onto the principal
corr eigen fisher ppca rbmrate
corr eigen fisher ppca rbmrate
corr eigen fisher ppca rbmrate
corr eigen fisher ppca rbmrate
figure 123 : error rates of all methods on all test sets .
component subspaces , then returns the similarity score as the angle between the projected images .
we used 123 principal components , since we get better results as we increase the number of components .
we also omitted the rst principal component , as we determined manually that it encodes simply for lighting conditions .
this improved the recognition performances on all the probe sets except for expression .
the third method is sherfaces ( 123 ) .
instead of projecting the images onto the subspace of the principal components , which maximizes the variance among the projected images , sherfaces projects the images onto a subspace which , at the same time , maximizes the between individual variances and minimizes the within individual variances in the training set .
we used a subspace of dimension 123
the nal method , which we shall call ppca , is proposed by pentland et al ( 123 ) .
this method models differences between images of the same individual as a ppca ( 123 ) , and differences between images of different individuals as another ppca .
then given a difference image , it returns as the similarity score the likelihood ratio of the difference image under the two p - pca models .
it was the best performing algorithm in the september 123 feret test ( 123 ) and is consistently worse than rbmrate on our test sets .
we used 123 and 123 dimensional ppcas for the rst and second model respectively .
these are the same numbers used by pentland et al and gives the best results .
figure 123 shows the error rates of all ve methods on all four test sets .
correlation and eigenfaces perform poorly on expression , probably because they do not attempt to ignore the within - individual variations , whereas the other methods do .
all the models did very poorly on the months test set which is unfortunate as this is the test set that is most like real applications .
when the error rate of the best match is high , it is interesting to compare methods by considering the rate at which correct matches appear in the top few images .
on months , for example , rbmrate appears to be worse than correlation but it is far more likely to have the right answer in its top 123 matches .
however , the months test set is tiny so the differences are unreliable .
figure 123 shows that after our preprocessing ,
figure 123 : on the left is a probe image from months and on the right are the top 123 matches to the probe returned by rbmrate .
most human observers cannot nd the correct match within these 123
human observers also have great difculty with the months test set , probably because the task is intrinsically difcult and is made even harder by the loss of contour and skin tone information combined with the misleading oval contour produced by masking out all of the background .
123 receptive elds learned by rbmrate
the top half of gure 123 shows the weights of a few of the hidden units after training .
all the units encode global features , probably because the image normalization ensures that there are strong long range correlations in pixel intensities .
the maximum size of the weights is 123 : 123 , with most weights having magnitudes smaller than 123 : 123
note , however , that the hidden unit activations range from 123 to 123
on the left are 123 units exhibiting interesting features and on the right are 123 units chosen at random .
the top unit of the rst column seems to be encoding the presence of moustache in both faces .
the bottom unit seems to be coding for prominent right eyebrows in both faces .
note that these are facial features which often remain constant across images of the same individual .
in the second column are two features which seem to encode for different facial expressions in the two faces .
the right side of the top unit encodes a smile while the left side is expresionless .
this is reversed in the bottom unit .
so the network has discovered some features which are fairly constant across images in the same class , and some features which can differ substantially within a class .
inspired by ( 123 ) , we tried to enforce local features by restricting the weights to be non - negative .
the bottom half of gure 123 shows some of the hidden receptive elds learned by rbmrate when trained with non - negative weights .
except for the 123 features on the left , all other features are local and code for features like mouth shape changes ( third column ) and eyes and cheeks ( fourth column ) .
the 123 features on the left are much more global and clearly capture the fact that the direction of the lighting can differ for two images of the same person .
unfortunately , constraining the weights to be non - negative strongly limits the representational power of rbmrate and makes it worse than all the other methods on all the test sets ( except for ppca on months ) .
we have introduced a new method for face recognition based on a non - linear generative model .
the non - linear generative model can be very complex , yet retains the efciency required for applications .
good performance is obtained on the feret database .
there is plenty of room for further development using prior knowledge to constrain the weights or additional layers of hidden units to model the correlations of feature detector activities .
figure 123 : example features learned by rbmrate .
each pair of rfs constitutes a feature .
top half : with unconstrained weights; bottom half : with non - negative weight constraints .
