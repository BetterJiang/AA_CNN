we present an automatic alignment procedure which maps the disparate internal representations learned by several local dimensionality reduction experts into a single , coherent global coordinate system for the original data space .
our algorithm can be applied to any set of experts , each of which produces a low - dimensional local representation of a high - dimensional input .
unlike recent efforts to coordinate such models by modifying their objective functions ( 123 , 123 ) , our algorithm is invoked after training and applies an efcient eigensolver to post - process the trained models .
the post - processing has no local optima and the size of the sys - tem it must solve scales with the number of local models rather than the number of original data points , making it more efcient than model - free algorithms such as isomap ( 123 ) or lle ( 123 ) .
123 introduction : local vs .
global dimensionality reduction beyond density modelling , an important goal of unsupervised learning is to discover com - pact , informative representations of high - dimensional data .
if the data lie on a smooth low dimensional manifold , then an excellent encoding is the coordinates internal to that man - ifold .
the process of determining such coordinates is dimensionality reduction .
linear dimensionality reduction methods such as principal component analysis and factor analy - sis are easy to train but cannot capture the structure of curved manifolds .
mixtures of these simple unsupervised models ( 123 , 123 , 123 , 123 ) have been used to perform local dimensionality reduction , and can provide good density models for curved manifolds , but unfortunately such mixtures cannot do dimensionality reduction .
they do not describe a single , coher - ent low - dimensional coordinate system for the data since there is no pressure for the local coordinates of each component to agree .
roweis et al ( 123 ) recently proposed a model which performs global coordination of local coordinate systems in a mixture of factor analyzers ( mfa ) .
their model is trained by max - imizing the likelihood of the data , with an additional variational penalty term to encourage the internal coordinates of the factor analyzers to agree .
while their model can trade off modelling the data and having consistent local coordinate systems , it requires a user given trade - off parameter , training is quite inefcient ( although ( 123 ) describes an improved train - ing algorithm for a more constrained model ) , and it has quite serious local minima problems ( methods like lle ( 123 ) or isomap ( 123 ) have to be used for initialization ) .
in this paper we describe a novel , automatic way to align the hidden representations used by each component of a mixture of dimensionality reducers into a single global representation of the data throughout space .
given an already trained mixture , the alignment is achieved by applying an eigensolver to a matrix constructed from the internal representations of the mixture components .
our method is efcient , simple to implement , and has no local optima in its optimization nor any learning rates or annealing schedules .
123 the locally linear coordination algorithm
suppose we have a set of data points given by the rows of
- dimensional space , which we assume are sampled from a
we approximate the manifold coordinates using images dimensional embedding space .
suppose also that we have already trained , or have been th reducer produces a%$ dimen - given , a mixture of " sional internal representation& ( ' for data point ) ' as well as a responsibility* ' describing how reliable the# is .
these satisfy /
local dimensionality reducers .
the# th reducers representation of
and can be obtained , for example , using a gating network in a mixture of experts , or the posterior probabilities in a probabilistic network .
notice that the manifold coordinates and internal representations need not have the same number of dimensions .
given the data , internal representations , and responsibilities , our algorithm automatically aligns the various hidden representations into a single global coordinate system .
two key ideas motivate the method .
first , to use a convex cost function whose unique minimum is
attained at the desired global coordinates .
second , to restrict the global coordinates $ and responsibilities* depend on the data
thereby leveraging the structure of the mixture model to regularize and reduce the effective size of the optimization problem .
in effect , rather than working with individual data points , we work with large groups of points belonging to particular submodels .
' only through the local representations&
to these to obtain its guess at the global coordinates .
the nal global is obtained by averaging the guesses using the responsibilities as weights :
given an input we rst parameterize the global coordinates in terms of* $ and then applies a linear projection , each local model infers its internal coordinates& $ and offset 123 is the k th entry of & = , wherej .
for compactness , we will writej the domain of123lk $ and thej now dene the matricesi ( 123 ) becomes a system of linear equations ( 123 ) with xedi
is the k th column of 123 $ , e into a single new indexj
$ , and e th row of123 and unknown parameters123
is a bias .
this process is described in gure 123
to simplify our calculations , we have vectorized the is an invertible mapping from
figure 123 : obtaining global coordinates from data via responsibility - weighted local coordinates .
is linear in each
$ and the unknown parameters123
is highly non - linear since it depends on the multipli - cation of responsibilities and internal coordinates which are in turn non - linearly related to
the key assumption , which we have emphasized by re - expressing mapping between the local representations and the global coordinates ' and the images the data ' we now consider determining123
' above , is that the $ .
crucially , however , the mapping between the = .
for this we is convex in123
advocate using a convex as well , and there is a unique optimum that can be computed efciently using a variety of methods .
this is still true if we also have feasible convex constraints case where the cost and constraints are both quadratic is particularly appealing since we
notice that since
through the inference procedure of the mixture model .
according to some given cost function
is linear in123
in particular suppose
this gives :
dening the cost and constraints , and let
can use an eigensolver to nd the optimal123
is the trace operator .
the matrices
are typically obtained from the and summarize the essential geometries among them .
the solution to the
constrained minimization above is given by the smallest generalized eigenvectors
below , we investigate a cost function based on the locally linear embedding ( lle ) algo - rithm of roweis and saul ( 123 ) .
we call the resulting algorithm locally linear coordina - tion ( llc ) .
the idea of lle is to preserve the same locally linear relationships between
in particular the columns of123
are given by these generalized eigenvectors .
we identify for each point
' and their counterparts the original data points ' and then minimize
subject to the constraints /
the weights are unique123
with respect to and can be solved for efciently using constrained least squares ( since solving for
the weights summarize the local geometries relating the data points
arrange to minimize the same cost
to their neighbours , hence to preserve these relationships among the coordinates but with respect to
is invariant to translations and rotations of
in order to break these degeneracies we enforce the following constraints :
is a vector of 123 s .
for this choice , the cost function and constraints above become :
, and scales as
with cost and constraint matrices
123in the unusual case where the number of neighbours is larger than the dimensionality of the data , simple regularization of the norm of the weights once again makes them unique .
as shown previously , the solution to this problem is given by the smallest generalized that are orthogonal to the vector is the smallest generalized eigenvector , corresponding to an eigenvalue of 123
hence the solution to the problem is
- , we need to nd eigenvectors
to satisfy
fortunately ,
llc alignment algorithm :
of the generalized eigenvalue system
given by thes
note that the edge size of the matrices
, compute local linear reconstruction weights
smallest generalized eigenvectors instead .
, obtaining a local representation& for each submodel# and each data point
train or receive a pre - trained mixture of local dimensionality reducers .
apply this mixture to form the matrixi withh $ and calculate find the eigenvectors corresponding to the smallest let123 be a matrix with columns formed by thes nd to th row of123 as alignment weight123 return the global manifold coordinates as $ which scales with the number of components and dimensions of the local .
as a result , solving for the representations but not with the number of data points + alignment weights is much more efcient than the original lle computation ( or those .
in effect , we of isomap ) which requires solving an eigenvalue system of edge size + have leveraged the mixture of local models to collapse large groups of points together and worked only with those groups rather than the original data points .
notice however that still requires determining the neighbours of the original the computation of the weights in the worse case .
data points , which scales as coordination with llc also yields a mixture of noiseless factor analyzers over the global
123 st eigenvectors .
$ and factor loading 123 th factor analyzer having mean 123 , with the # , we can infer the responsibilities*p$ and the posterior means given any global coordinates $ over the latent space of each factor analyzer .
if our original local dimensionality reduc - $ , we can now infer the high dimensional mean ers also supports computing data point which corresponds to the global coordinates
this allows us to perform op - erations like visualization and interpolation using the global coordinate system .
this is the method we used to infer the images in gures 123 and 123 in the next section .
whose generalized eigenvectors we seek
123 experimental results using mixtures of factor analyzers the alignment computation we have described is applicable to any mixture of local dimen - sionality reducers .
in our experiments , we have used the most basic such model : a mixture th factor analyzer in the mixture describes a proba -
of factor analyzers ( mfa ) ( 123 ) .
the # bilistic linear mapping from a latent variable&
the model assumes that the data manifold is locally linear and it is this local structure that is captured by each factor analyzer .
the non - linearity in the data manifold is handled by patching multiple factor analyzers together , each handling a locally linear region .
to the data with additive gaussian noise .
mfas are trained in an unsupervised way by maximizing the marginal log likelihood of the observed data , and parameter estimation is typically done using the em algorithm123
123in our experiments , we initialized the parameters by drawing the means from the global covari - ance of the data and setting the factors to small random values .
we also simplied the factor analyzers to share the same spherical noise covariance although this is not essential to the process .
figure 123 : llc on the s curve ( a ) .
there are 123 factor analyzers in the mixture ( b ) , each with 123 latent dimensions .
each disk represents one of them with the two black lines being the factor loadings .
after alignment by llc ( c ) , the curve is successfully unrolled; it is also possible to retroactively align the original data space models ( d ) .
figure 123 : unknotting the trefoil curve .
we generated 123 noisy points from the curve .
then we t an mfa with 123 components with 123 latent dimension each ( a ) , but aligned them in a 123d space ( b ) .
we used 123 neighbours to recon - struct each data point .
$ conditioned on the data
th local representation of
as the responsibility .
th factor analyzer generated
$ , a mfa trained only
th factor analyzer generated ) as the#
to maximize likelihood cannot learn a global coordinate system for the manifold that is consistent across every factor analyzer .
hence this is a perfect model on which to apply , while we use the
since there is no constraint relating the various hidden variables& automatic alignment .
naturally , we use the mean of& posterior probability that the#
we illustrate llc on two synthetic toy problems to give some intuition about how it works .
the rst problem is the s curve given in gure 123 ( a ) .
an mfa trained on 123 points sampled uniformly from the manifold with added noise ( b ) is able to model the linear structure of the curve locally , however the internal coordinates of the factor analyzers are not aligned properly .
we applied llc to the local representations and aligned them in a 123d space ( c ) .
when solving for local weights , we used 123 neighbours to reconstruct each data point .
we see that llc has successfully unrolled the s curve onto the 123d space .
further , given the coordinate transforms produced by llc , we can retroactively align the latent spaces of the mfas ( d ) .
this is done by determining directions in the various latent spaces which get transformed to the same direction in the global space .
to emphasize the topological advantages of aligning representations into a space of higher dimensionality than the local coordinates used by each submodel , we also trained a mfa on data sampled from a trefoil curve , as shown in gure 123 ( a ) .
the trefoil is a circle with a knot in 123d .
as gure 123 ( b ) shows , llc connects these models into a ring of local topology faithful to the original data .
we applied llc to mfas trained on sets of real images believed to come from a complex manifold with few degrees of freedom .
we studied face images of a single person under varying pose and expression changes and handwritten digits from the mnist database .
after training the mfas , we applied llc to align the models .
the face models were aligned into a 123d space as shown in gure 123
the rst dimension appears to describe
figure 123 : a map of reconstructions of the faces when the global coordinates are specied .
contours describe the likelihood of the coordinates .
note that some reconstructions around the edge of the map are not good because the model is extrapolating from the training images to regions of low likelihood .
a mfa with 123 components and 123 latent dimensions each is used .
it is trained on 123 images
are calculated using 123 neighbours .
changes in pose , while the second describes changes in expression .
the digit models were aligned into a 123d space .
figure 123 ( top ) shows maps of reconstructions of the digits .
the rst dimension appears to describe the slant of each digit , the second the fatness of each digit , and the third the relative sizes of the upper to lower loops .
figure 123 ( bottom ) shows how llc can smoothly interpolate between any two digits .
in particular , the rst row interpolates between left and right slanting digits , the second between fat and thin digits , the third between thick and thin line strokes , and the fourth between having a larger bottom loop and larger top loop .
123 discussion and conclusions previous work on nonlinear dimensionality reduction has usually emphasized either a para - metric approach , which explicitly constructs a ( sometimes probabilistic ) mapping between the high - dimensional and low - dimensional spaces , or a nonparametric approach which merely nds low - dimensional images corresponding to high - dimensional data points but without probabilistic models or hidden variables .
compared to the global coordination model ( 123 ) , the closest parametric approach to ours , our algorithm can be understood as post coordination , in which the latent spaces are coordinated after they have been t to data .
by decoupling the data tting and coordination problems we gain efciency and avoid local optima in the coordination phase .
further , since we are just maximizing likelihood when tting the original mixture to data , we can use a whole range of known techniques to escape local minima , and improve efciency in the rst phase as well .
on the nonparametric side , our approach can be compared to two recent algorithms , lle
to its latent space , the nal coordinated model will also describe a ( probabilistic ) mapping from the whole data space to the coordinated embedding space .
our alignment algorithm improves upon local embedding or density models by elevating their status to full global dimensionality reduction algorithms without requiring any modi - cations to their training procedures or cost functions .
for example , using mixtures of factor analyzers ( mfas ) as a test case , we show how our alignment method can allow a model previously suited only for density estimation to do complex operations on high dimensional data such as visualization and interpolation .
brand ( 123 ) has recently proposed an approach , similar to ours , that coordinates local para - metric models to obtain a globally valid nonlinear embedding function .
like llc , his charting method denes a quadratic cost function and nds the optimal coordination di - rectly and efciently .
however , charting is based on a cost function much closer in spirit to the original global coordination model and it instantiates one local model centred on each training point , so its scaling is the same as that of lle and isomap .
in principle , brands method can be extended to work with fewer local models and our alignment procedure can be applied using the charting cost rather than the lle cost we have pursued here .
automatic alignment procedures emphasizes a powerful but often overlooked interpreta - tion of local mixture models .
rather than considering the output of such systems to be a single quantity , such as a density estimate or a expert - weighted regression , it is possible to view them as networks which convert high - dimensional inputs into a vector of internal coordinates from each submodel , accompanied by responsibilities .
as we have shown , this view can lead to efcient and powerful algorithms which allow separate local models to learn consistent global representations .
we thank geoffrey hinton for inspiration and interesting discussions , brendan frey and yann lecun for sharing their data sets , and the reviewers for helpful comments .
