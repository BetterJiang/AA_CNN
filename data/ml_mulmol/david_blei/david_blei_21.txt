we develop an online variational bayes ( vb ) algorithm for latent dirichlet al - location ( lda ) .
online lda is based on online stochastic optimization with a natural gradient step , which we show converges to a local optimum of the vb objective function .
it can handily analyze massive document collections , includ - ing those arriving in a stream .
we study the performance of online lda in several ways , including by tting a 123 - topic topic model to 123m articles from wikipedia in a single pass .
we demonstrate that online lda nds topic models as good or better than those found with batch vb , and in a fraction of the time .
hierarchical bayesian modeling has become a mainstay in machine learning and applied statistics .
bayesian models provide a natural way to encode assumptions about observed data , and analysis proceeds by examining the posterior distribution of model parameters and latent variables condi - tioned on a set of observations .
for example , research in probabilistic topic modelingthe applica - tion we will focus on in this paperrevolves around tting complex hierarchical bayesian models to large collections of documents .
in a topic model , the posterior distribution reveals latent semantic structure that can be used for many applications .
for topic models and many other bayesian models of interest , however , the posterior is intractable to compute and researchers must appeal to approximate posterior inference .
modern approximate posterior inference algorithms fall in two categoriessampling approaches and optimization ap - proaches .
sampling approaches are usually based on markov chain monte carlo ( mcmc ) sam - pling , where a markov chain is dened whose stationary distribution is the posterior of interest .
op - timization approaches are usually based on variational inference , which is called variational bayes ( vb ) when used in a bayesian hierarchical model .
whereas mcmc methods seek to generate inde - pendent samples from the posterior , vb optimizes a simplied parametric distribution to be close in kullback - leibler divergence to the posterior .
although the choice of approximate posterior intro - duces bias , vb is empirically shown to be faster than and as accurate as mcmc , which makes it an attractive option when applying bayesian models to large datasets ( 123 , 123 , 123 ) .
nonetheless , large scale data analysis with vb can be computationally difcult .
standard batch vb algorithms iterate between analyzing each observation and updating dataset - wide variational parameters .
the per - iteration cost of batch algorithms can quickly become impractical for very large datasets .
in topic modeling applications , this issue is particularly relevanttopic modeling promises
figure 123 : top : perplexity on held - out wikipedia documents as a function of number of documents analyzed , i . e . , the number of e steps .
online vb run on 123 million unique wikipedia articles is compared with online vb run on 123 , 123 wikipedia articles and with the batch algorithm run on the same 123 , 123 articles .
the online algorithms converge much faster than the batch algorithm does .
bottom : evolution of a topic about business as online lda sees more and more documents .
to summarize the latent structure of massive document collections that cannot be annotated by hand .
a central research problem for topic modeling is to efciently t models to larger corpora ( 123 , 123 ) .
to this end , we develop an online variational bayes algorithm for latent dirichlet allocation ( lda ) , one of the simplest topic models and one on which many others are based .
our algorithm is based on online stochastic optimization , which has been shown to produce good parameter estimates dramat - ically faster than batch algorithms on large datasets ( 123 ) .
online lda handily analyzes massive col - lections of documents and , moreover , online lda need not locally store or collect the documents each can arrive in a stream and be discarded after one look .
in the subsequent sections , we derive online lda and show that it converges to a stationary point of the variational objective function .
we study the performance of online lda in several ways , including by tting a topic model to 123m articles from wikipedia without looking at the same article twice .
we show that online lda nds topic models as good as or better than those found with batch vb , and in a fraction of the time ( see gure 123 ) .
online variational bayes is a practical new method for estimating the posterior of complex hierarchical bayesian models .
123 online variational bayes for latent dirichlet allocation
latent dirichlet allocation ( lda ) ( 123 ) is a bayesian probabilistic model of text documents .
it as - sumes a collection of k topics .
each topic denes a multinomial distribution over the vocabulary and is assumed to have been drawn from a dirichlet , k dirichlet ( ) .
given the topics , lda assumes the following generative process for each document d .
first , draw a distribution over topics d dirichlet ( ) .
then , for each word i in the document , draw a topic index zdi ( 123 , .
, k ) from the topic weights zdi d and draw the observed word wdi from the selected topic , wdi zdi .
for simplicity , we assume symmetric priors on and , but this assumption is easy to relax ( 123 ) .
note that if we sum over the topic assignments z , then we get p ( wdi|d , ) = ( cid : 123 )
k dkkw .
this leads to the multinomial pca interpretation of lda; we can think of lda as a probabilistic factorization of the matrix of word counts n ( where ndw is the number of times word w appears in document d ) into a matrix of topic weights and a dictionary of topics ( 123 ) .
our work can thus
123systemshealthcommunicationservicebillionlanguagecareroad123servicesystemshealthcompaniesmarketcommunicationcompanybillion123servicesystemscompaniesbusinesscompanybillionhealthindustry123servicecompaniessystemsbusinesscompanyindustrymarketbillion123businessservicecompaniesindustrycompanymanagementsystemsservices123businessservicecompaniesindustryservicescompanymanagementpublic123systemsroadmadeserviceannouncednationalwestlanguage123businessindustryservicecompaniesservicescompanymanagementpublicdocumentsanalyzedtop eightwordsdocuments seen ( log scale ) perplexity123 . 123 . 123batch 123konline 123konline 123m be seen as an extension of online matrix factorization techniques that optimize squared error ( 123 ) to more general probabilistic formulations .
we can analyze a corpus of documents with lda by examining the posterior distribution of the topics , topic proportions , and topic assignments z conditioned on the documents .
this reveals latent structure in the collection that can be used for prediction or data exploration .
this posterior cannot be computed directly ( 123 ) , and is usually approximated using markov chain monte carlo ( mcmc ) methods or variational inference .
both classes of methods are effective , but both present signicant computational challenges in the face of massive data sets . developing scalable approxi - mate inference methods for topic models is an active area of research ( 123 , 123 , 123 , 123 ) .
to this end , we develop online variational inference for lda , an approximate posterior inference algorithm that can analyze massive collections of documents .
we rst review the traditional vari - ational bayes algorithm for lda and its objective function , then present our online method , and show that it converges to a stationary point of the same objective function .
123 batch variational bayes for lda
in variational bayesian inference ( vb ) the true posterior is approximated by a simpler distribution q ( z , , ) , which is indexed by a set of free parameters ( 123 , 123 ) .
these parameters are optimized to maximize the evidence lower bound ( elbo ) :
q ( zdi = k ) = dwdik;
log p ( w| , ) l ( w , , , ) ( cid : 123 ) eq ( log p ( w , z , , | , ) ) eq ( log q ( z , , ) ) .
maximizing the elbo is equivalent to minimizing the kl divergence between q ( z , , ) and the posterior p ( z , , |w , , ) .
following ( 123 ) , we choose a fully factorized distribution q of the form the posterior over the per - word topic assignments z is parameterized by , the posterior over the per - document topic weights is parameterized by , and the posterior over the topics is parameterized by .
as a shorthand , we refer to as the topics .
equation 123 factorizes to
( cid : 123 ) eq ( log p ( wd|d , zd , ) ) + eq ( log p ( zd|d ) ) eq ( log q ( zd ) ) + eq ( log p ( d| ) ) eq ( log q ( d ) ) + ( eq ( log p ( | ) ) eq ( log q ( ) ) ) / d ( cid : 123 ) .
l ( w , , , ) = ( cid : 123 )
q ( k ) = dirichlet ( k; k ) ,
q ( d ) = dirichlet ( d; d ) ;
notice we have brought the per - corpus terms into the summation over documents , and divided them by the number of documents d .
this step will help us to derive an online inference algorithm .
we now expand the expectations above to be functions of the variational parameters .
this reveals that the variational objective relies only on ndw , the number of times word w appears in document d .
when using vbas opposed to mcmcdocuments can be summarized by their word counts ,
k dk ) + ( cid : 123 ) k dwk ( eq ( log dk ) + eq ( log kw ) log dwk ) k log ( ( cid : 123 ) k ( dk ) eq ( log dk ) + log ( dk )
+ log ( k ) k log ( ) + ( log ( w ) w log ( ) ) / d d ( cid : 123 ) ( nd , d , d , ) ,
w kw ) + ( cid : 123 )
w ( kw ) eq ( log kw ) + log ( kw ) ) / d
the expectations under q of log and log are
eq ( log dk ) = ( dk ) ( ( cid : 123 ) k
where w is the size of the vocabulary and d is the number of documents .
( cid : 123 ) ( nd , d , d , ) denotes the contribution of document d to the elbo .
l can be optimized using coordinate ascent over the variational parameters , , ( 123 ) : dwk exp ( eq ( log dk ) + eq ( log kw ) ) ;
dk = + ( cid : 123 )
kw = + ( cid : 123 ) i=123 di ) ; eq ( log kw ) = ( kw ) ( ( cid : 123 ) w
where denotes the digamma function ( the rst derivative of the logarithm of the gamma function ) .
the updates in equation 123 are guaranteed to converge to a stationary point of the elbo .
by analogy to the expectation - maximization ( em ) algorithm ( 123 ) , we can partition these updates into an e stepiteratively updating and until convergence , holding xedand an m stepupdating given .
in practice , this algorithm converges to a better solution if we reinitialize and before each e step .
algorithm 123 outlines batch vb for lda .
algorithm 123 batch variational bayes for lda
while relative improvement in l ( w , , , ) > 123 do
for d = 123 to d do
initialize dk = 123
( the constant 123 is arbitrary . ) set dwk exp ( eq ( log dk ) + eq ( log kw ) )
set dk = + ( cid : 123 ) k |change indk| < 123 set kw = + ( cid : 123 )
123 online variational inference for lda
algorithm 123 has constant memory requirements and empirically converges faster than batch col - lapsed gibbs sampling ( 123 ) .
however , it still requires a full pass through the entire corpus each iteration .
it can therefore be slow to apply to very large datasets , and is not naturally suited to set - tings where new data is constantly arriving .
we propose an online variational inference algorithm for tting , the parameters to the variational posterior over the topic distributions .
our algorithm is nearly as simple as the batch vb algorithm , but converges much faster for large datasets .
a good setting of the topics is one for which the elbo l is as high as possible after tting the per - document variational parameters and with the e step dened in algorithm 123
let ( nd , ) and ( nd , ) be the values of d and d produced by the e step .
our goal is to set to maximize
d ( cid : 123 ) ( nd , ( nd , ) , ( nd , ) , ) ,
l ( n , ) ( cid : 123 ) ( cid : 123 )
where ( cid : 123 ) ( nd , d , d , ) is the dth documents contribution to the variational bound in equation 123
this is analogous to the goal of least - squares matrix factorization , although the elbo for lda is less convenient to work with than a simple squared loss function such as the one in ( 123 ) .
online vb for lda ( online lda ) is described in algorithm 123
as the tth vector of word counts nt is observed , we perform an e step to nd locally optimal values of t and t , holding xed .
we then compute , the setting of that would be optimal ( given t ) if our entire corpus consisted of the single document nt repeated d times .
d is the number of unique documents available to the algorithm , e . g .
the size of a corpus .
( in the true online case d , corresponding to empirical bayes estimation of . ) we then update using a weighted average of its previous value and .
the weight given to is given by t ( cid : 123 ) ( 123 + t ) , where ( 123 , 123 ) controls the rate at which old values of are forgotten and 123 123 slows down the early iterations of the algorithm .
the condition that ( 123 , 123 ) is needed to guarantee convergence .
we show in section 123 that online lda corresponds to a stochastic natural gradient algorithm on the variational objective l ( 123 , 123 ) .
this algorithm closely resembles one proposed in ( 123 ) for online vb on models with hidden data the most important difference is that we use an approximate e step to optimize t and t , since we cannot compute the conditional distribution p ( zt , t| , nt , ) exactly .
mini - batches .
a common technique in stochastic learning is to consider multiple observations per update to reduce noise ( 123 , 123 ) .
in online lda , this means computing using s > 123 observations :
kw = + d
where nts is the sth document in mini - batch t .
the variational parameters ts and ts for this document are t with a normal e step .
note that we recover batch vb when s = d and = 123
in batch variational lda , point estimates of the hyperparameters and can be t given and using a linear - time newton - raphson method ( 123 ) .
we can likewise
algorithm 123 online variational bayes for lda
dene t ( cid : 123 ) ( 123 + t ) for t = 123 to do
initialize tk = 123
( the constant 123 is arbitrary . ) set twk exp ( eq ( log tk ) + eq ( log kw ) )
set tk = + ( cid : 123 )
k |change intk| < 123
compute kw = + dntwtwk set = ( 123 t ) + t
incorporate updates for and into online lda :
where ( t ) is the inverse of the hessian times the gradient ( cid : 123 ) ( nt , t , t , ) , ( ) is the inverse of the hessian times the gradient l , and t ( cid : 123 ) ( 123 + t ) as elsewhere .
123 analysis of convergence
in this section we show that algorithm 123 converges to a stationary point of the objective dened in equation 123
since variational inference replaces sampling with optimization , we can use results from stochastic optimization to analyze online lda .
stochastic optimization algorithms optimize an ob - jective using noisy estimates of its gradient ( 123 ) .
although there is no explicit gradient computation , algorithm 123 can be interpreted as a stochastic natural gradient algorithm ( 123 , 123 ) .
we begin by deriving a related rst - order stochastic gradient algorithm for lda .
let g ( n ) denote the population distribution over documents n from which we will repeatedly sample documents :
i ( n = nd ) is 123 if n = nd and 123 otherwise .
if this population consists of the d documents in the corpus , then we can rewrite equation 123 as
g ( n ) ( cid : 123 ) 123
i ( n = nd ) .
l ( g , ) ( cid : 123 ) deg ( ( cid : 123 ) ( n , ( n , ) , ( n , ) , ) | ) .
where ( cid : 123 ) is dened as in equation 123
we can optimize equation 123 over by repeatedly drawing an observation nt g , computing t ( cid : 123 ) ( nt , ) and t ( cid : 123 ) ( nt , ) , and applying the update
+ td ( cid : 123 ) ( nt , t , t , )
d ( cid : 123 ) ( nd , d , d , ) .
thus , since ( cid : 123 )
where t ( cid : 123 ) ( 123 + t ) as in algorithm 123
if we condition on the current value of and treat t and t as random variables drawn at the same time as each observed document nt , then eg ( d ( cid : 123 ) ( nt , t , t , ) | ) = , the analysis in ( 123 ) shows both that converges and that the gradient d ( cid : 123 ) ( nd , d , d , ) converges to 123 , and thus that converges to a stationary point . 123 the update in equation 123 only makes use of rst - order gradient information .
stochastic gradient algorithms can be sped up by multiplying the gradient by the inverse of an appropriate positive denite matrix h ( 123 ) .
one choice for h is the hessian of the objective function .
in variational inference , an alternative is to use the fisher information matrix of the variational distribution q ( i . e . , the hessian of the log of the variational probability density function ) , which corresponds to using
t=123 t = and ( cid : 123 )
123although we use a deterministic procedure to compute and given n and , this analysis can also be
applied if and are optimized using a randomized algorithm .
we address this case in the supplement .
a natural gradient method instead of a ( quasi - ) newton method ( 123 , 123 ) .
following the analysis in ( 123 ) , the gradient of the per - document elbo ( cid : 123 ) can be written as
( kv / d + / d + ntvtvk ) ( kv / d + / d + ntvtvk ) ,
v=123 123 log q ( k ) ( cid : 123 ) ( cid : 123 ) 123 log q ( log k )
where we have used the fact that eq ( log kv ) is the derivative of the log - normalizer of q ( log k ) .
by denition , multiplying equation 123 by the inverse of the fisher information matrix yields
= kw / d + / d + ntwtwk .
multiplying equation 123 by td and adding it to kw yields the update for in algorithm 123
thus we can interpret our algorithm as a stochastic natural gradient algorithm , as in ( 123 ) .
123 related work
comparison with other stochastic learning algorithms .
in the standard stochastic gradient op - timization setup , the number of parameters to be t does not depend on the number of observations ( 123 ) .
however , some learning algorithms must also t a set of per - observation parameters ( such as the per - document variational parameters d and d in lda ) .
the problem is addressed by on - line coordinate ascent algorithms such as those described in ( 123 , 123 , 123 , 123 , 123 ) .
the goal of these algorithms is to set the global parameters so that the objective is as good as possible once the per - observation parameters are optimized .
most of these approaches assume the computability of a unique optimum for the per - observation parameters , which is not available for lda .
efcient sampling methods .
markov chain monte carlo ( mcmc ) methods form one class of approximate inference algorithms for lda .
collapsed gibbs sampling ( cgs ) is a popular mcmc approach that samples from the posterior over topic assignments z by repeatedly sampling the topic assignment zdi conditioned on the data and all other topic assignments ( 123 ) .
one online mcmc approach adapts cgs by sampling topic assignments zdi based on the topic assignments and data for all previously analyzed words , instead of all other words in the corpus ( 123 ) .
this algorithm is fast and has constant memory requirements , but is not guaranteed to converge to the posterior .
two alternative online mcmc approaches were considered in ( 123 ) .
the rst , called incremental lda , periodically resamples the topic assignments for previously analyzed words .
the second approach uses particle ltering instead of cgs .
in a study in ( 123 ) , none of these three online mcmc algorithms performed as well as batch cgs .
instead of online methods , the authors of ( 123 ) used parallel computing to apply lda to large corpora .
they developed two approximate parallel cgs schemes for lda that gave similar predictive per - formance on held - out documents to batch cgs .
however , they require parallel hardware , and their complexity and memory costs still scale linearly with the number of documents .
except for the algorithm in ( 123 ) ( which is not guaranteed to converge ) , all of the mcmc algorithms described above have memory costs that scale linearly with the number of documents analyzed .
by contrast , batch vb can be implemented using constant memory , and parallelizes easily .
as we will show in the next section , its online counterpart is even faster .
we ran several experiments to evaluate online ldas efciency and effectiveness .
the rst set of experiments compares algorithms 123 and 123 on static datasets .
the second set of experiments evaluates online vb in the setting where new documents are constantly being observed .
both algorithms were implemented in python using numpy .
the implementations are as similar as possible . 123
123open - source python implementations of batch and online lda can be found at http : / / www . cs .
table 123 : best settings of and 123 for various mini - batch sizes s , with resulting perplexities on nature and wikipedia corpora .
best parameter settings for nature corpus
best parameter settings for wikipedia corpus
figure 123 : held - out perplexity obtained on the nature ( left ) and wikipedia ( right ) corpora as a func - tion of cpu time .
for moderately large mini - batch sizes , online lda nds solutions as good as those that the batch lda nds , but with much less computation .
when t to a 123 , 123 - document subset of the training corpus batch ldas speed improves , but its performance suffers .
we use perplexity on held - out data as a measure of model t .
perplexity is dened as the geometric mean of the inverse marginal probability of each word in the held - out set of documents :
perplexity ( ntest , , ) ( cid : 123 ) exp
test denotes the vector of word counts for the ith document .
since we cannot directly
i log p ( ntest
| , ) , we use a lower bound on perplexity as a proxy :
, i , zi| , ) ) eq ( log q ( i , zi ) ) ) ( ( cid : 123 )
compute log p ( ntest perplexity ( ntest , , ) exp
the per - document parameters i and i for the variational distributions q ( i ) and q ( zi ) are t using the e step in algorithm 123
the topics are t to a training set of documents and then held xed .
in all experiments and are xed at 123 and the number of topics k = 123
there is some question as to the meaningfulness of perplexity as a metric for comparing different topic models ( 123 ) .
held - out likelihood metrics are nonetheless well suited to measuring how well an inference algorithm accomplishes the specic optimization task dened by a model .
evaluating learning parameters .
online lda introduces several learning parameters : ( 123 , 123 ) , which controls how quickly old information is forgotten; 123 123 , which downweights early iterations; and the mini - batch size s , which controls how many documents are used each iteration .
although online lda converges to a stationary point for any valid , 123 , and s , the quality of this stationary point and the speed of convergence may depend on how the learning parameters are set .
we evaluated a range of settings of the learning parameters , 123 , and s on two corpora : 123 , 123 documents from the journal nature 123 and 123 , 123 documents downloaded from the english ver -
123for the nature articles , we removed all words not in a pruned vocabulary of 123 , 123 words .
time in seconds ( log scale ) perplexity123batch size123batch123kbatch123ktime in seconds ( log scale ) perplexity123batch size123batch123kbatch123k sion of wikipedia 123
for each corpus , we set aside a 123 , 123 - document test set and a separate 123 , 123 - document validation set .
we then ran online lda for ve hours on the remaining docu - ments from each corpus for ( 123 , 123 , 123 , 123 , 123 , 123 ) , 123 ( 123 , 123 , 123 , 123 , 123 , 123 ) , and s ( 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 ) , for a total of 123 runs per corpus .
after ve hours of cpu time , we computed perplexity on the test sets for the topics obtained at the end of each t .
table 123 summarizes the best settings for each corpus of and 123 for a range of settings of s .
the supplement includes a more exhaustive summary .
the best learning parameter settings for both corpora were = 123 , 123 = 123 , and s = 123
the best settings of and 123 are consistent across the two corpora .
for mini - batch sizes from 123 to 123 there is little difference in perplexity scores .
several trends emerge from these results .
higher values of the learning rate and the downweighting parameter 123 lead to better performance for small mini - batch sizes s , but worse performance for larger values of s .
mini - batch sizes of at least 123 documents outperform smaller mini - batch sizes .
comparing batch and online on xed corpora .
to compare batch lda to online lda , we evalu - ated held - out perplexity as a function of time on the nature and wikipedia corpora above .
we tried various mini - batch sizes from 123 to 123 , 123 , using the best learning parameters for each mini - batch size found in the previous study of the nature corpus .
we also evaluated batch lda t to a 123 , 123 - document subset of the training corpus .
we computed perplexity on a separate validation set from the test set used in the previous experiment .
each algorithm ran for 123 hours of cpu time .
figure 123 summarizes the results .
on the larger nature corpus , online lda nds a solution as good as the batch algorithms with much less computation .
on the smaller wikipedia corpus , the online al - gorithm nds a better solution than the batch algorithm does .
the batch algorithm converges quickly on the 123 , 123 - document corpora , but makes less accurate predictions on held - out documents .
true online .
to demonstrate the ability of online vb to perform in a true online setting , we wrote a python script to continually download and analyze mini - batches of articles chosen at random from a list of approximately 123 million wikipedia articles .
this script can download and analyze about 123 , 123 articles an hour .
it completed a pass through all 123 million articles in under three days .
the amount of time needed to download an article and convert it to a vector of word counts is comparable to the amount of time that the online lda algorithm takes to analyze it .
we ran online lda with = 123 , 123 = 123 , and s = 123
figure 123 shows the evolution of the perplexity obtained on the held - out validation set of 123 , 123 wikipedia articles by the online algorithm as a function of number of articles seen .
shown for comparison is the perplexity obtained by the online algorithm ( with the same parameters ) t to only 123 , 123 wikipedia articles , and that obtained by the batch algorithm t to the same 123 , 123 articles .
the online algorithm outperforms the batch algorithm regardless of which training dataset is used , but it does best with access to a constant stream of novel documents .
the batch algorithms failure to outperform the online algorithm on limited data may be due to stochastic gradients robustness to local optima ( 123 ) .
the online algorithm converged after analyzing about half of the 123 million articles .
even one iteration of the batch algorithm over that many articles would have taken days .
we have developed online variational bayes ( vb ) for lda .
this algorithm requires only a few more lines of code than the traditional batch vb of ( 123 ) , and is handily applied to massive and streaming document collections .
online vb for lda approximates the posterior as well as previous approaches in a fraction of the time .
the approach we used to derive an online version of batch vb for lda is general ( and simple ) enough to apply to a wide variety of hierarchical bayesian models .
blei is supported by onr 123 - 123 , nsf career 123 , afosr 123nl123 , the alfred p .
sloan foundation , and a grant from google .
bach is supported by anr ( mga project ) .
123for the wikipedia articles , we removed all words not from a xed vocabulary of 123 , 123 common words .
this vocabulary was obtained by removing words less than 123 characters long from a list of the 123 , 123 most com - mon words in project gutenberg texts obtained from http : / / en . wiktionary . org / wiki / wiktionary : frequency lists .
