we develop stochastic variational inference , a scalable algorithm for approximating posterior dis - tributions .
we develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models , latent dirichlet allocation and the hierarchical dirichlet pro - cess topic model .
using stochastic variational inference , we analyze several large collections of documents : 123k articles from nature , 123m articles from the new york times , and 123m arti - cles from wikipedia .
stochastic inference can easily handle data sets of this size and outperforms traditional variational inference , which can only handle a smaller subset .
( we also show that the bayesian nonparametric topic model outperforms its parametric counterpart . ) stochastic variational inference lets us apply complex bayesian models to massive data sets .
keywords : bayesian inference , variational inference , stochastic optimization , topic models , bayesian
modern data analysis requires computation with massive data .
as examples , consider the following .
( 123 ) we have an archive of the raw text of two million books , scanned and stored online .
we want to discover the themes in the texts , organize the books by subject , and build a navigator for users
c ( cid : 123 ) 123 matthew d .
hoffman , david m .
blei , chong wang and john paisley .
hoffman , blei , wang and paisley
to explore our collection .
( 123 ) we have data from an online shopping website containing millions of users purchase histories as well as descriptions of each item in the catalog .
we want to recommend items to users based on this information .
( 123 ) we are continuously collecting data from an online feed of photographs .
we want to build a classier from these data .
( 123 ) we have measured the gene sequences of millions of people .
we want to make hypotheses about connections between observed genes and other traits .
these problems illustrate some of the challenges to modern data analysis .
our data are com - plex and high - dimensional; we have assumptions to makefrom science , intuition , or other data analysesthat involve structures we believe exist in the data but that we cannot directly observe; and nally our data sets are large , possibly even arriving in a never - ending stream .
statistical machine learning research has addressed some of these challenges by developing the eld of probabilistic modeling , a eld that provides an elegant approach to developing new methods for analyzing data ( pearl , 123; jordan , 123; bishop , 123; koller and friedman , 123; murphy , 123 ) .
in particular , probabilistic graphical models give us a visual language for expressing as - sumptions about data and its hidden structure .
the corresponding posterior inference algorithms let us analyze data under those assumptions , inferring the hidden structure that best explains our
in descriptive tasks , like problems #123 and #123 above , graphical models help us explore the data the organization of books or the connections between genes and traitswith the hidden structure probabilistically lled in .
in predictive tasks , like problems #123 and #123 , we use models to form predictions about new observations .
for example , we can make recommendations to users or pre - dict the class labels of new images .
with graphical models , we enjoy a powerful suite of probability models to connect and combine; and we have general - purpose computational strategies for connect - ing models to data and estimating the quantities needed to use them .
the problem we face is scale .
inference algorithms of the 123s and 123s used to be considered scalable , but they cannot easily handle the amount of data that we described in the four examples above .
this is the problem we address here .
we present an approach to computing with graphical models that is appropriate for massive data sets , data that might not t in memory or even be stored locally .
our method does not require clusters of computers or specialized hardware , though it can be further sped up with these amenities .
as an example of this approach to data analysis , consider topic models .
topic models are prob - abilistic models of text used to uncover the hidden thematic structure in a collection of documents ( blei , 123 ) .
the main idea in a topic model is that there are a set of topics that describe the collec - tion and each document exhibits those topics with different degrees .
as a probabilistic model , the topics and how they relate to the documents are hidden structure and the main computational prob - lem is to infer this hidden structure from an observed collection .
figure 123 illustrates the results of our algorithm on a probabilistic topic model .
these are two sets of topics , weighted distributions over the vocabulary , found in 123m articles from the new york times and 123 , 123 articles from nature .
topic models are motivated by applications that require analyzing massive collections of documents like this , but traditional algorithms for topic model inference do not easily scale collections of this
our algorithm builds on variational inference , a method that transforms complex inference prob - lems into high - dimensional optimization problems ( jordan et al . , 123; wainwright and jordan , 123 ) .
traditionally , the optimization is solved with a coordinate ascent algorithm , iterating be - tween re - analyzing every data point in the data set and re - estimating its hidden structure
stochastic variational inference
figure 123 : posterior topics from the hierarchical dirichlet process topic model on two large data sets .
these posteriors were approximated using stochastic variational inference with 123m ar - ticles from the new york times ( top ) and 123k articles from nature ( bottom ) .
( see sec - tion 123 for the modeling details behind the hierarchical dirichlet process and section 123 for details about the empirical study . ) each topic is a weighted distribution over the vo - cabulary and each topics plot illustrates its most frequent words .
hoffman , blei , wang and paisley
is inefcient for large data sets , however , because it requires a full pass through the data at each
in this paper we derive a more efcient algorithm by using stochastic optimization ( robbins and monro , 123 ) , a technique that follows noisy estimates of the gradient of the objective .
when used in variational inference , we show that this gives an algorithm which iterates between subsampling the data and adjusting the hidden structure based only on the subsample .
this is much more efcient than traditional variational inference .
we call our method stochastic variational inference .
we will derive stochastic variational inference for a large class of graphical models .
we will study its performance on two kinds of probabilistic topic models .
in particular , we demonstrate stochastic variational inference on latent dirichlet allocation ( blei et al . , 123 ) , a simple topic model , and the hierarchical dirichlet process topic model ( teh et al . , 123a ) , a more exible model where the number of discovered topics grows with the data .
( this latter application demonstrates how to use stochastic variational inference in a variety of bayesian nonparametric settings . ) stochastic variational inference can efciently analyze massive data sets with complex probabilistic models .
technical summary .
we now turn to the technical context of our method .
in probabilistic modeling , we use hidden variables to encode hidden structure in observed data; we articulate the relationship between the hidden and observed variables with a factorized probability distribution ( i . e . , a graphical model ) ; and we use inference algorithms to estimate the posterior distribution , the conditional distribution of the hidden structure given the observations .
consider a graphical model of hidden and observed random variables for which we want to compute the posterior .
for many models of interest , this posterior is not tractable to compute and we must appeal to approximate methods .
the two most prominent strategies in statistics and machine learning are markov chain monte carlo ( mcmc ) sampling and variational inference .
in mcmc sampling , we construct a markov chain over the hidden variables whose stationary distribution is the posterior of interest ( metropolis et al . , 123; hastings , 123; geman and geman , 123; gelfand and smith , 123; robert and casella , 123 ) .
we run the chain until it has ( hopefully ) reached equilibrium and collect samples to approximate the posterior .
in variational inference , we dene a exible family of distributions over the hidden variables , indexed by free parameters ( jordan et al . , 123; wainwright and jordan , 123 ) .
we then nd the setting of the parameters ( i . e . , the member of the family ) that is closest to the posterior .
thus we solve the inference problem by solving an
neither mcmc nor variational inference scales easily to the kinds of settings described in the rst paragraph .
researchers have proposed speed - ups of both approaches , but these usually are tailored to specic models or compromise the correctness of the algorithm ( or both ) .
here , we develop a general variational method that scales .
as we mentioned above , the main idea in this work is to use stochastic optimization ( robbins and monro , 123; spall , 123 ) .
in stochastic optimization , we nd the maximum of an objective function by following noisy ( but unbiased ) estimates of its gradient .
under the right conditions , stochastic optimization algorithms provably converge to an optimum of the objective .
stochastic optimization is particularly attractive when the objective ( and therefore its gradient ) is a sum of many terms that can be computed independently .
in that setting , we can cheaply compute noisy gradients by subsampling only a few of these terms .
variational inference is amenable to stochastic optimization because the variational objective decomposes into a sum of terms , one for each data point in the analysis .
we can cheaply obtain noisy estimates of the gradient by subsampling the data and computing a scaled gradient on the
stochastic variational inference
if we sample independently then the expectation of this noisy gradient is equal to the true gradient .
with one more detailthe idea of a natural gradient ( amari , 123 ) stochastic variational inference has an attractive form :
subsample one or more data points from the data .
analyze the subsample using the current variational parameters .
implement a closed - form update of the variational parameters .
while traditional algorithms require repeatedly analyzing the whole data set before updating the variational parameters , this algorithm only requires that we analyze randomly sampled subsets .
we will show how to use this algorithm for a large class of graphical models .
related work .
variational inference for probabilistic models was pioneered in the mid - 123s .
in michael jordans lab , the seminal papers of saul et al .
( 123 ) ; saul and jordan ( 123 ) and jaakkola ( 123 ) grew out of reading the statistical physics literature ( peterson and anderson , 123; parisi , 123 ) .
in parallel , the mean - eld methods explained in neal and hinton ( 123 ) ( originally published in 123 ) and hinton and van camp ( 123 ) led to variational algorithms for mixtures of experts ( waterhouse et al . , 123 ) .
in subsequent years , researchers began to understand the potential for variational inference in more general settings and developed generic algorithms for conjugate exponential - family models ( attias , 123 , 123; wiegerinck , 123; ghahramani and beal , 123; xing et al . , 123 ) .
these innovations led to automated variational inference , allowing a practitioner to write down a model and immediately use variational inference to estimate its posterior ( bishop et al . , 123 ) .
for good reviews of variational inference see jordan et al .
( 123 ) and wainwright and jordan ( 123 ) .
in this paper , we develop scalable methods for generic bayesian inference by solving the vari - ational inference problem with stochastic optimization ( robbins and monro , 123 ) .
our algorithm builds on the earlier approach of sato ( 123 ) , whose algorithm only applies to the limited set of models that can be t with the em algorithm ( dempster et al . , 123 ) .
specically , we generalize his approach to the much wider set of probabilistic models that are amenable to closed - form coordi - nate ascent inference .
further , in the sense that em itself is a mean - eld method ( neal and hinton , 123 ) , our algorithm builds on the stochastic optimization approach to em ( capp and moulines , 123 ) .
finally , we note that stochastic optimization was also used with variational inference in platt et al .
( 123 ) for fast approximate inference in a specic model of web service activity .
for approximate inference , the main alternative to variational methods is markov chain monte carlo ( mcmc ) ( robert and casella , 123 ) .
despite its popularity in bayesian inference , relatively little work has focused on developing mcmc algorithms that can scale to very large data sets .
one exception is sequential monte carlo , although these typically lack strong convergence guarantees ( doucet et al . , 123 ) .
another is the stochastic gradient langevin method of welling and teh ( 123 ) , which enjoys asymptotic convergence guarantees and also takes advantage of stochastic optimization .
finally , in topic modeling , researchers have developed several approaches to parallel mcmc ( newman et al . , 123; smola and narayanamurthy , 123; ahmed et al . , 123 ) .
the organization of this paper .
in section 123 , we review variational inference for graphical models and then derive stochastic variational inference .
in section 123 , we review probabilistic topic models and bayesian nonparametric models and then derive the stochastic variational inference algorithms in these settings .
in section 123 , we study stochastic variational inference on several large text data sets .
hoffman , blei , wang and paisley
figure 123 : a graphical model with observations x123 : n , local hidden variables z123 : n and global hidden variables .
the distribution of each observation xn only depends on its corresponding local variable zn and the global variables .
( though not pictured , each hidden variable zn , observation xn , and global variable may be a collection of multiple random variables . )
stochastic variational inference
we derive stochastic variational inference , a stochastic optimization algorithm for mean - eld vari - ational inference .
our algorithm approximates the posterior distribution of a probabilistic model with hidden variables , and can handle massive data sets of observations .
we divide this section into four parts .
we dene the class of models to which our algorithm applies .
we dene local and global
hidden variables , and requirements on the conditional distributions within the model .
we review mean - eld variational inference , an approximate inference strategy that seeks a tractable distribution over the hidden variables which is close to the posterior distribution .
we derive the traditional variational inference algorithm for our class of models , which is a coordinate ascent algorithm .
we review the natural gradient and derive the natural gradient of the variational objective
function .
the natural gradient closely relates to coordinate ascent variational inference .
we review stochastic optimization , a technique that uses noisy estimates of a gradient to optimize an objective function , and apply it to variational inference .
specically , we use stochastic optimization with noisy estimates of the natural gradient of the variational objective .
these estimates arise from repeatedly subsampling the data set .
we show how the resulting algorithm , stochastic variational inference , easily builds on traditional variational inference algorithms but can handle much larger data sets .
123 models with local and global hidden variables
our class of models involves observations , global hidden variables , local hidden variables , and xed parameters .
the n observations are x = x123 : n; the vector of global hidden variables is ; the n local hidden variables are z = z123 : n , each of which is a collection of j variables zn = zn , 123 : j; the vector of xed parameters is .
( note we can easily allow to partly govern any of the random variables ,
stochastic variational inference
such as xed parts of the conditional distribution of observations .
to keep notation simple , we assume that they only govern the global hidden variables . )
the joint distribution factorizes into a global term and a product of local terms ,
p ( x , z , | ) = p ( | )
p ( xn , zn | ) .
figure 123 illustrates the graphical model .
our goal is to approximate the posterior distribution of the hidden variables given the observations , p ( , z | x ) .
the distinction between local and global hidden variables is determined by the conditional de - in particular , the nth observation xn and the nth local variable zn are conditionally
independent , given global variables , of all other observations and local hidden variables ,
p ( xn , zn | xn , zn , , ) = p ( xn , zn | , ) .
the notation xn and zn refers to the set of variables except the nth .
this kind of model frequently arises in bayesian statistics .
the global variables are parameters endowed with a prior p ( ) and each local variable zn contains the hidden structure that governs the nth observation .
for example , consider a bayesian mixture of gaussians .
the global variables are the mixture proportions and the means and variances of the mixture components; the local variable zn is the hidden cluster label for the nth observation xn .
we have described the independence assumptions of the hidden variables .
we make further assumptions about the complete conditionals in the model .
a complete conditional is the conditional distribution of a hidden variable given the other hidden variables and the observations .
we assume that these distributions are in the exponential family ,
p ( | x , z , ) = h ( ) exp ( g ( x , z , ) t ( ) ag ( g ( x , z , ) ) ) ,
p ( zn j | xn , zn , j , ) = h ( zn j ) exp ( ( xn , zn , j , ) t ( zn j ) a ( ( xn , zn , j , ) ) ) .
the scalar functions h ( ) and a ( ) are respectively the base measure and log - normalizer; the vector functions ( ) and t ( ) are respectively the natural parameter and sufcient statistics . 123 these are conditional distributions , so the natural parameter is a function of the variables that are being con - ditioned on .
( the subscripts on the natural parameter indicate complete conditionals for local or global variables . ) for the local variables zn j , the complete conditional distribution is determined by the global variables and the other local variables in the nth context , that is , the nth data point xn and the local variables zn , j .
this follows from the factorization in equation 123
these assumptions on the complete conditionals imply a conjugacy relationship between the global variables and the local contexts ( zn , xn ) , and this relationship implies a specic form of the complete conditional for .
specically , the distribution of the local context given the global variables must be in an exponential family ,
p ( xn , zn| ) = h ( xn , zn ) exp ( t ( xn , zn ) a ( ) ) .
we use overloaded notation for the functions h ( ) and t ( ) so that they depend on the names of their arguments; for example , h ( zn j ) can be thought of as a shorthand for the more formal ( but more cluttered ) notation hzn j ( zn j ) .
this is analogous to the standard convention of overloading the probability function p ( ) .
hoffman , blei , wang and paisley
the prior distribution p ( ) must also be in an exponential family ,
p ( ) = h ( ) exp ( t ( ) ag ( ) ) .
the sufcient statistics are t ( ) = ( , a ( ) ) and thus the hyperparameter has two components = ( 123 , 123 ) .
the rst component 123 is a vector of the same dimension as ; the second component 123 is a scalar .
equations 123 and 123 imply that the complete conditional for the global variable in equation 123 is in
the same exponential family as the prior with natural parameter
g ( x , z , ) = ( 123 + n
n=123 t ( zn , xn ) , 123 + n ) .
this form will be important when we derive stochastic variational inference in section 123 .
see bernardo and smith ( 123 ) for a general discussion of conjugacy and the exponential family .
this family of distributionsthose with local and global variables , and where the complete conditionals are in the exponential familycontains many useful statistical models from the ma - chine learning and statistics literature .
examples include bayesian mixture models ( ghahramani and beal , 123; attias , 123 ) , latent dirichlet allocation ( blei et al . , 123 ) , hidden markov models ( and many variants ) ( rabiner , 123; fine et al . , 123; fox et al . , 123b; paisley and carin , 123 ) , kalman lters ( and many variants ) ( kalman , 123; fox et al . , 123a ) , factorial models ( ghahramani and jordan , 123 ) , hierarchical linear regression models ( gelman and hill , 123 ) , hierarchical pro - bit classication models ( mccullagh and nelder , 123; girolami and rogers , 123 ) , probabilistic factor analysis / matrix factorization models ( spearman , 123; tipping and bishop , 123; collins et al . , 123; wang , 123; salakhutdinov and mnih , 123; paisley and carin , 123; hoffman et al . , 123b ) , certain bayesian nonparametric mixture models ( antoniak , 123; escobar and west , 123; teh et al . , 123a ) , and others . 123
analyzing data with one of these models amounts to computing the posterior distribution of the
hidden variables given the observations ,
p ( z , | x ) =
p ( x , z , )
r p ( x , z , ) dzd
we then use this posterior to explore the hidden structure of our data or to make predictions about future data .
for many models however , such as the examples listed above , the denominator in equation 123 is intractable to compute .
thus we resort to approximate posterior inference , a problem that has been a focus of modern bayesian statistics .
we now turn to mean - eld variational inference , the approximation inference technique which roots our strategy for scalable inference .
123 mean - field variational inference
variational inference casts the inference problem as an optimization .
we introduce a family of dis - tributions over the hidden variables that is indexed by a set of free parameters , and then optimize those parameters to nd the member of the family that is closest to the posterior of interest .
( close - ness is measured with kullback - leibler divergence . ) we use the resulting distribution , called the variational distribution , to approximate the posterior .
we note that our assumptions can be relaxed to the case where the full conditional p ( |x , z ) is not tractable , but each partial conditional p ( k|x , z , k ) associated with the global variable k is in a tractable exponential family .
the topic models of the next section do not require this complexity , so we chose to keep the derivation a little simpler .
stochastic variational inference
in this section we review mean - eld variational inference , the form of variational inference that uses a family where each hidden variable is independent .
we describe the variational objective func - tion , discuss the mean - eld variational family , and derive the traditional coordinate ascent algorithm for tting the variational parameters .
this algorithm is a stepping stone to stochastic variational in -
the evidence lower bound .
variational inference minimizes the kullback - leibler ( kl ) di - vergence from the variational distribution to the posterior distribution .
it maximizes the evidence lower bound ( elbo ) , a lower bound on the logarithm of the marginal probability of the observa - tions log p ( x ) .
the elbo is equal to the negative kl divergence up to an additive constant .
we derive the elbo by introducing a distribution over the hidden variables q ( z , ) and using ( jensens inequality and the concavity of the logarithm function imply that log e ( f ( y ) ) e ( log f ( y ) ) for any random variable y . ) this gives the following bound on the log
log p ( x ) = logz p ( x , z , ) dzd
= logz p ( x , z , ) q ( z , ) ( cid : 123 ) ( cid : 123 ) = log ( cid : 123 ) eq ( cid : 123 ) p ( x , z , )
eq ( log p ( x , z , ) ) eq ( log q ( z , ) )
the elbo contains two terms .
the rst term is the expected log joint , eq ( log p ( x , z , ) ) .
the second term is the entropy of the variational distribution , eq ( log q ( z , ) ) .
both of these terms depend on q ( z , ) , the variational distribution of the hidden variables .
we restrict q ( z , ) to be in a family that is tractable , one for which the expectations in the elbo can be efciently computed .
we then try to nd the member of the family that maximizes the elbo .
finally , we use the optimized distribution as a proxy for the posterior .
solving this maximization problem is equivalent to nding the member of the family that is
closest in kl divergence to the posterior ( jordan et al . , 123; wainwright and jordan , 123 ) ,
kl ( q ( z , ) ||p ( z , |x ) ) = eq ( log q ( z , ) ) eq ( log p ( z , | x ) )
= eq ( log q ( z , ) ) eq ( log p ( x , z , ) ) + log p ( x ) = l ( q ) + const .
log p ( x ) is replaced by a constant because it does not depend on q .
the mean - eld variational family .
the simplest variational family of distributions is the mean - eld family .
in this family , each hidden variable is independent and governed by its own parameter ,
q ( z , ) = q ( | )
q ( zn j | n j ) .
the global parameters govern the global variables; the local parameters n govern the local vari - ables in the nth context .
the elbo is a function of these parameters .
hoffman , blei , wang and paisley
equation 123 gives the factorization of the variational family , but does not specify its form .
we set q ( | ) and q ( zn j|n j ) to be in the same exponential family as the complete conditional distributions p ( |x , z ) and p ( zn j|xn , zn , j , ) , from equations 123 and 123
the variational parameters and n j are the natural parameters to those families ,
q ( | ) = h ( ) exp ( t ( ) ag ( ) ) ,
q ( zn j | n j ) = h ( zn j ) exp (
n jt ( zn j ) a ( n j ) ) .
these forms of the variational distributions lead to an easy coordinate ascent algorithm .
further , the optimal mean - eld distribution , without regard to its particular functional form , has factors in these families ( bishop , 123 ) .
note that assuming that these exponential families are the same as their corresponding condi - tionals means that t ( ) and h ( ) in equation 123 are the same functions as t ( ) and h ( ) in equation 123
likewise , t ( ) and h ( ) in equation 123 are the same as in equation 123
we will sometimes suppress the explicit dependence on and , substituting q ( zn j ) for q ( zn j|n j ) and q ( ) for q ( | ) .
the mean - eld family has several computational advantages .
for one , the entropy term decom -
eq ( log q ( z , ) ) = e ( log q ( ) )
en j ( log q ( zn j ) ) ,
where en j ( ) denotes an expectation with respect to q ( zn j | n j ) and e ( ) denotes an expectation with respect to q ( | ) .
its other computational advantages will emerge as we derive the gradients of the variational objective and the coordinate ascent algorithm .
the gradient of the elbo and coordinate ascent inference .
we have dened the objective function in equation 123 and the variational family in equations 123 , 123 and 123
our goal is to optimize the objective with respect to the variational parameters .
in traditional mean - eld variational inference , we optimize equation 123 with coordinate ascent .
we iteratively optimize each variational parameter , holding the other parameters xed .
with the assumptions that we have made about the model and variational distributionthat each conditional is in an exponential family and that the corresponding variational distribution is in the same expo - nential familywe can optimize each coordinate in closed form .
we rst derive the coordinate update for the parameter to the variational distribution of the
global variables q ( | ) .
as a function of , we can rewrite the objective as
l ( ) = eq ( log p ( | x , z ) ) eq ( log q ( ) ) + const .
the rst two terms are expectations that involve ; the third term is constant with respect to .
the constant absorbs quantities that depend only on the other hidden variables .
those quantities do not depend on q ( | ) because all variables are independent in the mean - eld family .
equation 123 reproduces the full elbo in equation 123
the second term of equation 123 is the entropy of the global variational distribution .
the rst term derives from the expected log joint likelihood , where we use the chain rule to separate terms that depend on the variable from terms that do not ,
eq ( log p ( x , z , ) ) = eq ( log p ( x , z ) ) + eq ( log p ( | x , z ) ) .
the constant absorbs eq ( log p ( x , z ) ) , leaving the expected log conditional eq ( log p ( | x , z ) ) .
stochastic variational inference
finally , we substitute the form of q ( | ) in equation 123 to obtain the nal expression for the
elbo as a function of ,
l ( ) = eq ( g ( x , z , ) ) ag ( ) ag ( ) + ag ( ) + const .
in the rst and second terms on the right side , we used the exponential family identity that the expec - tation of the sufcient statistics is the gradient of the log normalizer , eq ( t ( ) ) = ag ( ) .
the con - stant has further absorbed the expected log normalizer of the conditional distribution eq ( ag ( g ( x , z , ) ) ) , which does not depend on q ( ) .
equation 123 simplies the elbo as a function of the global variational parameter .
to derive
the coordinate ascent update , we take the gradient ,
ag ( ) ( eq ( g ( x , z , ) ) ) .
we can set this gradient to zero by setting
= eq ( g ( x , z , ) ) .
this sets the global variational parameter equal to the expected natural parameter of its complete conditional distribution .
implementing this update , holding all other variational parameters xed , optimizes the elbo over .
notice that the mean - eld assumption plays an important role .
the update is the expected conditional parameter eq ( g ( x , z , ) ) , which is an expectation of a function of the other random variables and observations .
thanks to the mean - eld assumption , this expectation is only a function of the local variational parameters and does not depend on .
we now turn to the local parameters n j .
the gradient is nearly identical to the global case ,
n j l = 123
a ( n j ) ( eq ( ( xn , zn , j , ) ) n j ) .
it equals zero when
n j = eq ( ( xn , zn , j , ) ) .
mirroring the global update , this expectation does not depend on n j .
however , while the global update in equation 123 depends on all the local variational parametersand note there is a set of local parameters for each of the n observationsthe local update in equation 123 only depends on the global parameters and the other parameters associated with the nth context .
the computa - tional difference between local and global updates will be important in the scalable algorithm of
the updates in equations 123 and 123 form the algorithm for coordinate ascent variational infer - ence , iterating between updating each local parameter and the global parameters .
the full algorithm is in figure 123 , which is guaranteed to nd a local optimum of the elbo .
computing the expecta - tions at each step is easy for directed graphical models with tractable complete conditionals , and in section 123 we show that these updates are tractable for many topic models .
figure 123 is the classical variational inference algorithm , used in many settings .
as an aside , these updates reveal a connection between mean - eld variational inference and gibbs sampling ( gelfand and smith , 123 ) .
in gibbs sampling , we iteratively sample from each complete conditional .
in variational inference , we take variational expectations of the natural param - eters of the same distributions .
the updates also show a connection to the expectation - maximization
hoffman , blei , wang and paisley
123 : initialize ( 123 ) randomly .
for each local variational parameter n j do
update n j , ( t )
n j = e
q ( t123 ) ( , j ( xn , zn , j , ) ) .
update the global variational parameters , ( t ) = e
123 : until the elbo converges
figure 123 : coordinate ascent mean - eld variational inference .
( em ) algorithm ( dempster et al . , 123 ) equation 123 corresponds to the e step , and equation 123 corresponds to the m step ( neal and hinton , 123 ) .
we mentioned that the local steps ( steps 123 and 123 in figure 123 ) only require computation with the global parameters and the nth local context .
thus , the data can be distributed across many machines and the local variational updates can be implemented in parallel .
these results can then be aggregated in step 123 to nd the new global variational parameters .
however , the local steps also reveal an inefciency in the algorithm .
the algorithm begins by initializing the global parameters randomlythe initial value of does not reect any regularity in the data .
but before completing even one iteration , the algorithm must analyze every data point using these initial ( random ) values .
this is wasteful , especially if we expect that we can learn something about the global variational parameters from only a subset of the data .
we solve this problem with stochastic optimization .
this leads to stochastic variational infer - ence , an efcient algorithm that continually improves its estimate of the global parameters as it analyzes more observations .
though the derivation requires some details , we have now described all of the computational components of the algorithm .
( see figure 123 ) at each iteration , we sam - ple a data point from the data set and compute its optimal local variational parameters; we form intermediate global parameters using classical coordinate ascent updates where the sampled data point is repeated n times; nally , we set the new global parameters to a weighted average of the old estimate and the intermediate parameters .
the algorithm is efcient because it need not analyze the whole data set before improving the global variational parameters , and the per - iteration steps only require computation about a single lo - cal context .
furthermore , it only uses calculations from classical coordinate inference .
any existing implementation of variational inference can be easily congured to this scalable alternative .
we now show how stochastic inference arises by applying stochastic optimization to the natural gradients of the variational objective .
we rst discuss natural gradients and their relationship to the coordinate updates in mean - eld variational inference .
123 the natural gradient of the elbo
the natural gradient of a function accounts for the information geometry of its parameter space , using a riemannian metric to adjust the direction of the traditional gradient .
amari ( 123 ) discusses natural gradients for maximum - likelihood estimation , which give faster convergence than standard
stochastic variational inference
natural gradient of the elbo .
in this section we describe riemannian metrics for probability distributions and the
gradients and probability distributions .
the classical gradient method for maximization tries
to nd a maximum of a function f ( ) by taking steps of size in the direction of the gradient ,
( t+123 ) = ( t ) + f ( ( t ) ) .
the gradient ( when it exists ) points in the direction of steepest ascent .
that is , the gradient f ( ) points in the same direction as the solution to
f ( + d )
subject to ||d||123 < 123
for sufciently small .
equation 123 implies that if we could only move a tiny distance away from then we should move in the direction of the gradient .
initially this seems reasonable , but there is a complication .
the gradient direction implicitly depends on the euclidean distance metric associated with the space in which lives .
however , the euclidean metric might not capture a meaningful notion of distance between settings of .
the problem with euclidean distance is especially clear in our setting , where we are trying to optimize an objective with respect to a parameterized probability distribution q ( | ) .
when optimizing over a probability distribution , the euclidean distance between two parameter vectors and is often a poor measure of the dissimilarity of the distributions q ( | ) and q ( | ) .
for example , suppose q ( ) is a univariate normal and is the mean and scale .
the distributions n ( 123 , 123 ) and n ( 123 , 123 ) are almost indistinguishable , and the euclidean distance between in contrast , the distributions n ( 123 , 123 ) and n ( 123 , 123 ) barely their parameter vectors is 123
overlap , but this is not reected in the euclidean distance between their parameter vectors , which is only 123 .
the natural gradient corrects for this issue by redening the basic denition of the gradient ( amari , 123 ) .
natural gradients and probability distributions .
a natural measure of dissimilarity between
probability distributions is the symmetrized kl divergence
kl ( , ) = ehlog q ( | )
q ( | ) i + ehlog q ( | ) q ( | ) i .
symmetrized kl depends on the distributions themselves , rather than on how they are parameter - ized; it is invariant to parameter transformations .
with distances dened using symmetrized kl , we nd the direction of steepest ascent in the
same way as for gradient methods ,
f ( + d )
subject to dsym
kl ( , + d ) < .
as 123 , the solution to this problem points in the same direction as the natural gradient .
while the euclidean gradient points in the direction of steepest ascent in euclidean space , the natural gradient points in the direction of steepest ascent in the riemannian space , that is , the space where local distance is dened by kl divergence rather than the l123 norm .
we manage the more complicated constraint in equation 123 with a riemannian metric g ( ) ( do carmo , 123 ) .
this metric denes linear transformations of under which the squared eu - clidean distance between and a nearby vector + d is the kl between q ( | ) and q ( | + d ) ,
dt g ( ) d = dsym
kl ( , + d ) ,
hoffman , blei , wang and paisley
and note that the transformation can be a function of .
amari ( 123 ) showed that we can compute the natural gradient by premultiplying the gradient by the inverse of the riemannian metric g ( ) 123 ,
where g is the fisher information matrix of q ( ) ( amari , 123; kullback and leibler , 123 ) ,
f ( ) , g ( ) 123 f ( ) ,
g ( ) = eh ( log q ( | ) ) ( log q ( | ) ) i .
we can show that equation 123 satises equation 123 by approximating log q ( | + d ) using the rst - order taylor approximations about
log q ( | + d ) = o ( d123 ) + log q ( | ) + d log q ( | ) ,
q ( | + d ) = o ( d123 ) + q ( | ) + q ( | ) d log q ( | ) ,
and plugging the result into equation 123 :
kl ( , + d ) = z
( q ( | + d ) q ( | ) ) ( log q ( | + d ) log q ( | ) ) d
= o ( d123 ) +z = o ( d123 ) + eq ( ( d log q ( | ) ) 123 ) = o ( d123 ) + dg ( ) d .
q ( | ) ( d log q ( | ) ) 123d
for small enough d we can ignore the o ( d123 ) term .
when q ( | ) is in the exponential family ( equation 123 ) the metric is the second derivative of
the log normalizer ,
g ( ) = eh ( log p ( | ) ) ( log p ( | ) ) i = eh ( t ( ) e ( t ( ) ) ) ( t ( ) e ( t ( ) ) ) i
this follows from the exponential family identity that the hessian of the log normalizer function a with respect to the natural parameter is the covariance matrix of the sufcient statistic vector t ( ) .
natural gradients and mean eld variational inference .
we now return to variational in - ference and compute the natural gradient of the elbo with respect to the variational parameters .
researchers have used the natural gradient in variational inference for nonlinear state space models ( honkela et al . , 123 ) and bayesian mixtures ( sato , 123 ) . 123
consider the global variational parameter .
the gradient of the elbo with respect to is in equation 123
since is a natural parameter to an exponential family distribution , the fisher metric dened by q ( ) is 123 ag ( ) .
note that the fisher metric is the rst term in equation 123
we premultiply the gradient by the inverse fisher information to nd the natural gradient .
this reveals that the natural gradient has the following simple form ,
l = e ( g ( x , z , ) ) .
our work hereusing the natural gradient in a stochastic optimization algorithmis closest to that of sato ( 123 ) , though we develop the algorithm via a different path and sato does not address models for which the joint conditional p ( zn| , xn ) is not tractable .
stochastic variational inference
an analogous computation goes through for the local variational parameters ,
n j l = e , n , j ( ( xn , zn , j , ) ) n j .
the natural gradients are closely related to the coordinate ascent updates of equation 123 or equa - tion 123
consider a full set of variational parameters and .
we can compute the natural gradient by computing the coordinate updates in parallel and subtracting the current setting of the parameters .
the classical coordinate ascent algorithm can thus be interpreted as a projected natural gradient al - gorithm ( sato , 123 ) .
updating a parameter by taking a natural gradient step of length one is equivalent to performing a coordinate update .
we motivated natural gradients by mathematical reasoning around the geometry of the parame - ter space .
more importantly , however , natural gradients are easier to compute than classical gradi - ents .
they are easier to compute because premultiplying by the fisher information matrixwhich we must do to compute the classical gradient in equation 123 but which disappears from the natural gradient in equation 123is prohibitively expensive for variational parameters with many compo - nents .
in the next section we will see that efciently computing the natural gradient lets us develop scalable variational inference algorithms .
123 stochastic variational inference
the coordinate ascent algorithm in figure 123 is inefcient for large data sets because we must opti - mize the local variational parameters for each data point before re - estimating the global variational parameters .
stochastic variational inference uses stochastic optimization to t the global variational parameters .
we repeatedly subsample the data to form noisy estimates of the natural gradient of the elbo , and we follow these estimates with a decreasing step - size .
we have reviewed mean - eld variational inference in models with exponential family condition - als and showed that the natural gradient of the variational objective function is easy to compute .
we now discuss stochastic optimization , which uses a series of noisy estimates of the gradient , and use it with noisy natural gradients to derive stochastic variational inference .
stochastic optimization algorithms follow noisy estimates of the gradient with a decreasing step size .
noisy estimates of a gradient are often cheaper to compute than the true gradient , and following such estimates can allow algorithms to escape shallow local optima of complex objective functions .
in statistical estimation problems , including variational inference of the global parameters , the gradient can be written as a sum of terms ( one for each data point ) and we can compute a fast noisy approximation by subsampling the data .
with certain conditions on the step - size schedule , these algorithms provably converge to an optimum ( robbins and monro , 123 ) .
spall ( 123 ) gives an overview of stochastic optimization; bottou ( 123 ) gives an overview of its role in machine learning .
consider an objective function f ( ) and a random function b ( ) that has expectation equal to the gradient so that eq ( b ( ) ) = f ( ) .
the stochastic gradient algorithm , which is a type of stochastic optimization , optimizes f ( ) by iteratively following realizations of b ( ) .
at iteration t , the update for is
( t ) = ( t123 ) + t bt ( ( t123 ) ) ,
where bt is an independent draw from the noisy gradient b .
if the sequence of step sizes t satises
t = ; 123
hoffman , blei , wang and paisley
then ( t ) will converge to the optimal ( if f is convex ) or a local optimum of f ( if not convex ) . 123 the same results apply if we premultiply the noisy gradients bt by a sequence of positive - denite
( whose eigenvalues are bounded ) ( bottou , 123 ) .
the resulting algorithm is
( t ) = ( t123 ) + t g123
as our notation suggests , we will use the fisher metric for gt , replacing stochastic euclidean gradi - ents with stochastic natural gradients .
stochastic variational inference .
we use stochastic optimization with noisy natural gradients to optimize the variational objective function .
the resulting algorithm is in figure 123
at each iteration we have a current setting of the global variational parameters .
we repeat the following steps :
sample a data point from the set; optimize its local variational parameters .
form intermediate global variational parameters , as though we were running classical coordi -
nate ascent and the sampled data point were repeated n times to form the collection .
update the global variational parameters to be a weighted average of the intermediate param -
eters and their current setting .
we show that this algorithm is stochastic natural gradient ascent on the global variational parame -
our goal is to nd a setting of the global variational parameters that maximizes the elbo .
writing l as a function of the global and local variational parameters , let the function ( ) return a local optimum of the local variational parameters so that
l ( , ( ) ) = 123
dene the locally maximized elbo l ( ) to be the elbo when is held xed and the local varia - tional parameters are set to a local optimum ( ) ,
l ( ) , l ( , ( ) ) .
we can compute the ( natural ) gradient of l ( ) by rst nding the corresponding optimal local pa - rameters ( ) and then computing the ( natural ) gradient of l ( , ( ) ) , holding ( ) xed .
the rea - son is that the gradient of l ( ) is the same as the gradient of the two - parameter elbo l ( , ( ) ) ,
l ( ) = l ( , ( ) ) + ( ( ) ) l ( , ( ) )
= l ( , ( ) ) ,
where ( ) is the jacobian of ( ) and we use the fact that the gradient of l ( , ) with respect to is zero at ( ) .
stochastic variational inference optimizes the maximized elbo l ( ) by subsampling the data to form noisy estimates of the natural gradient .
first , we decompose l ( ) into a global term and a sum of local terms ,
l ( ) = eq ( log p ( ) ) eq ( log q ( ) ) +
( eq ( log p ( xn , zn | ) ) eq ( log q ( zn ) ) ) .
to nd a local optimum , f must be three - times differentiable and meet a few mild technical requirements ( bottou ,
the variational objective satises these criteria .
stochastic variational inference
now consider a variable that chooses an index of the data uniformly at random , i unif ( 123 ,
dene li ( ) to be the following random function of the variational parameters ,
li ( ) , eq ( log p ( ) ) eq ( log q ( ) ) + n max
( eq ( log p ( xi , zi | ) eq ( log q ( zi ) ) ) .
the expectation of li is equal to the objective in equation 123
therefore , the natural gradient of li with respect to each global variational parameter is a noisy but unbiased estimate of the natural gradient of the variational objective .
this processsampling a data point and then computing the natural gradient of liwill provide cheaply computed noisy gradients for stochastic optimization .
we now compute the noisy gradient .
suppose we have sampled the ith data point .
notice that equation 123 is equivalent to the full objective of equation 123 where the ith data point is observed n times .
thus the natural gradient of equation 123which is a noisy natural gradient of the elbo can be found using equation 123 ,
li = eqhg ( cid : 123 ) x
, ( cid : 123 ) i ,
i o are a data set formed by n replicates of observation xn and hidden variables zn .
we compute this expression in more detail .
recall the complete conditional g ( x , z , ) from equation 123
from this equation , we can compute the conditional natural parameter for the global parameter given n replicates of xn ,
using this in the natural gradient of equation 123 gives a noisy natural gradient ,
, ( cid : 123 ) = + n ( t ( xn , zn ) , 123 ) .
li = + n ( cid : 123 ) ei ( ) ( t ( xi , zi ) ) , 123 ( cid : 123 ) ,
where i ( ) gives the elements of ( ) associated with the ith local context .
while the full natural gradient would use the local variational parameters for the whole data set , the noisy natural gradient only considers the local parameters for one randomly sampled data point .
these noisy gradients are cheaper to compute .
finally , we use the noisy natural gradients in a robbins - monro algorithm to optimize the elbo .
we sample a data point xi at each iteration .
dene the intermediate global parameter t to be the estimate of that we would obtain if the sampled data point was replicated n times ,
t , + nei ( ) ( ( t ( xi , zi ) , 123 ) ) .
this comprises the rst two terms of the noisy natural gradient .
at each iteration we use the noisy gradient ( with step size t ) to update the global variational parameter .
the update is
( t ) = ( t123 ) + t ( cid : 123 ) t ( t123 ) ( cid : 123 )
= ( 123 t ) ( t123 ) + t
this is a weighted average of the previous estimate of and the estimate of that we would obtain if the sampled data point was replicated n times .
hoffman , blei , wang and paisley
123 : initialize ( 123 ) randomly .
123 : set the step - size schedule t appropriately .
sample a data point xi uniformly from the data set .
compute its local variational parameter ,
compute intermediate global parameters as though xi is replicated n times ,
update the current estimate of the global variational parameters ,
( t ) = ( 123 t ) ( t123 ) + t
123 : until forever
figure 123 : stochastic variational inference .
figure 123 presents the full algorithm .
at each iteration , the algorithm has an estimate of the global variational parameter ( t123 ) .
it samples a single data point from the data and cheaply computes the intermediate global parameter t , that is , the next value of if the data set contained n replicates of the sampled point .
it then sets the new estimate of the global parameter to be a weighted average of the previous estimate and the intermediate parameter .
we set the step - size at iteration t as follows ,
t = ( t + ) .
this satises the conditions in equation 123
the forgetting rate ( 123 , 123 ) controls how quickly old information is forgotten; the delay 123 down - weights early iterations .
in section 123 we x the delay to be one and explore a variety of forgetting rates .
note that this is just one way to parameterize the learning rate .
as long as the step size conditions in equation 123 are satised , this iterative algorithm converges to a local optimum of the elbo .
we now describe two extensions of the basic stochastic inference algorithm in figure 123 : the use of multiple samples ( minibatches ) to improve the algorithms stability , and empirical bayes methods for hyperparameter estimation .
so far , we have considered stochastic variational inference algorithms where only one observation xt is sampled at a time .
many stochastic optimization algorithms benet from minibatches , that is , several examples at a time ( bottou and bousquet , 123; liang et al . , 123; mairal et al . , 123 ) .
in stochastic variational inference , we can sample a set of s examples at each iteration xt , 123 : s ( with or without replacement ) , compute the local variational parameters s ( ( t123 ) ) for
stochastic variational inference
each data point , compute the intermediate global parameters s for each data point xts , and nally average the s variables in the update
( t ) = ( 123 t ) ( t123 ) +
the stochastic natural gradients associated with each point xs have expected value equal to the gradient .
therefore , the average of these stochastic natural gradients has the same expectation and the algorithm remains valid .
there are two reasons to use minibatches .
the rst reason is to amortize any computational expenses associated with updating the global parameters across more data points; for example , if the expected sufcient statistics of are expensive to compute , using minibatches allows us to incur that expense less frequently .
the second reason is that it may help the algorithm to nd better local optima .
stochastic variational inference is guaranteed to converge to a local optimum but taking large steps on the basis of very few data points may lead to a poor one .
as we will see in section 123 , using more of the data per update can help the algorithm .
empirical bayes estimation of hyperparameters .
in some cases we may want to both estimate the posterior of the hidden random variables and z and obtain a point estimate of the values of the hyperparameters .
one approach to tting is to try to maximize the marginal likelihood of the data p ( x | ) , which is also known as empirical bayes ( maritz and lwin , 123 ) estimation .
since we cannot compute p ( x | ) exactly , an approximate approach is to maximize the tted variational lower bound l over .
in the non - stochastic setting , can be optimized by interleaving the coordinate ascent updates in figure 123 with an update for that increases the elbo .
this is called variational
in the stochastic setting , we update simultaneously with .
we can take a step in the direction
of the gradient of the noisy elbo lt ( equation 123 ) with respect to , scaled by the step - size t ,
( t ) = ( t123 ) + tlt ( ( t123 ) , , ( t123 ) ) .
here ( t123 ) are the global parameters from the previous iteration and are the optimized local parameters for the currently sampled data point .
we can also replace the standard euclidean gradient with a natural gradient or newton step .
stochastic variational inference in topic models
we derived stochastic variational inference , a scalable inference algorithm that can be applied to a large class of hierarchical bayesian models .
in this section we show how to use the general algorithm of section 123 to derive stochastic variational inference for two probabilistic topic models : latent dirichlet allocation ( lda ) ( blei et al . , 123 ) and its bayesian nonparametric counterpart , the hierarchical dirichlet process ( hdp ) topic model ( teh et al . , 123a ) .
topic models are probabilistic models of document collections that use latent variables to en - code recurring patterns of word use ( blei , 123 ) .
topic modeling algorithms are inference algo - rithms; they uncover a set of patterns that pervade a collection and represent each document ac - cording to how it exhibits them .
these patterns tend to be thematically coherent , which is why the models are called topic models .
topic models are used for both descriptive tasks , such as to build thematic navigators of large collections of documents , and for predictive tasks , such as to aid document classication .
topic models have been extended and applied in many domains .
hoffman , blei , wang and paisley
topic models assume that the words of each document arise from a mixture of multinomials .
across a collection , the documents share the same mixture components ( called topics ) .
each doc - ument , however , is associated with its own mixture proportions ( called topic proportions ) .
in this way , topic models represent documents heterogeneouslythe documents share the same set of top - ics , but each exhibits them to a different degree .
for example , a document about sports and health will be associated with the sports and health topics; a document about sports and business will be associated with the sports and business topics .
they both share the sports topic , but each combines sports with a different topic .
more generally , this is called mixed membership ( erosheva , 123 ) .
the central computational problem in topic modeling is posterior inference : given a collection of documents , what are the topics that it exhibits and how does each document exhibit them ? in practical applications of topic models , scale is importantthese models promise an unsupervised approach to organizing large collections of text ( and , with simple adaptations , images , sound , and other data ) .
thus they are a good testbed for stochastic variational inference .
more broadly , this section illustrates how to use the results from section 123 to develop algorithms for specic models .
we will derive the algorithms in several steps : ( 123 ) we specify the model assump - tions; ( 123 ) we derive the complete conditional distributions of the latent variables; ( 123 ) we form the mean - eld variational family; ( 123 ) we derive the corresponding stochastic inference algorithm .
in section 123 , we will report our empirical study of stochastic variational inference with these models .
we follow the notation of blei et al .
( 123 ) .
observations are words , organized into documents .
the nth word in the dth document is wdn .
each word is an element in a xed vocabulary of v terms .
a topic k is a distribution over the vocabulary .
each topic is a point on the v 123 - simplex , a positive vector of length v that sums to one .
we denote the wth entry in the kth topic as kw .
in lda there are k topics; in the hdp topic model there are an innite number of topics .
each document in the collection is associated with a vector of topic proportions d , which is a distribution over topics .
in lda d is a point on the k 123 - simplex .
in the hdp topic model , d is a point on the innite simplex .
( we give details about this below in section 123 . ) we denote the kth entry of the topic proportion vector d as dk .
each word in each document is assumed to have been drawn from a single topic .
the topic
assignment zdn indexes the topic from which wdn is drawn .
the only observed variables are the words of the documents .
the topics , topic proportions , and
topic assignments are latent variables .
123 latent dirichlet allocation
lda is the simplest topic model .
it assumes that each document exhibits k topics with different proportions .
the generative process is
draw topics k dirichlet ( , .
, ) for k ( 123 ,
for each document d ( 123 ,
stochastic variational inference
hoffman , blei , wang and paisley
( a ) draw topic proportions dirichlet ( ,
( b ) for each word w ( 123 ,
draw topic assignment zdn multinomial ( d ) .
draw word wdn multinomial ( zdn ) .
figure 123 illustrates lda as a graphical model .
in lda , each document exhibits the same shared topics but with different proportions .
lda assumes dirichlet priors for k and d .
dirichlet distributions over the d - simplex take d + 123 pa - rameters , but for simplicity we assume exchangeable dirichlet priors; that is , we require that all of these parameters are set to the same value .
( the prior on k has parameter ; the prior on d has parameter . ) .
we note that blei et al .
( 123 ) and wallach et al .
( 123 ) found improved empirical performance with non - exchangeable priors .
lda models an observed collection of documents w = w123 : d , where each wd is a collection of words wd , 123 : n .
analyzing the documents amounts to posterior inference of p ( , , z | w ) .
conditioned on the documents , the posterior distribution captures the topics that describe them ( = 123 : k ) , the degree to which each document exhibits those topics ( = 123 : d ) , and which topics each word was as - signed to ( z = z123 : d , 123 : n ) .
we can use the posterior to explore large collections of documents .
figure 123 illustrates posterior topics found with stochastic variational inference .
the posterior is intractable to compute ( blei et al . , 123 ) .
approximating the posterior in lda is a central computational problem for topic modeling .
researchers have developed many methods , including markov chain monte carlo methods ( grifths and steyvers , 123 ) , expectation propa - gation ( minka and lafferty , 123 ) , and variational inference ( blei et al . , 123; teh et al . , 123b; asuncion et al . , 123 ) .
here we use the results of section 123 to develop stochastic inference for lda .
this scales the original variational algorithm for lda to massive collections of documents . 123
figure 123 illustrates the performance of 123 - topic lda on three large collectionsnature con - tains 123k documents , new york times contains 123m documents , and wikipedia contains 123m documents .
( section 123 describes the complete study , including the details of the performance mea - sure and corpora . ) we compare two inference algorithms for lda : stochastic inference on the full collection and batch inference on a subset of 123 , 123 documents .
( this is the size of collection that batch inference can handle . ) we see that stochastic variational inference converges faster and to a better model .
it is both more efcient and lets us handle the full data set .
indicator vectors and dirichlet distributions .
before deriving the algorithm , we discuss two
mathematical details .
these will be useful both here and in the next section .
first , we represent categorical variables like the topic assignments zdn and observed words wdn with indicator vectors .
an indicator vector is a binary vector with a single one .
for example , the topic assignment zdn can take on one of k values ( one for each topic ) .
thus , it is represented as a k - vector with a one in the component corresponding to the value of the variable : if zk dn = 123 then the nth word in document d is assigned to the kth topic .
likewise , wv dn = 123 implies that the nth word in document d is v .
in a slight abuse of notation , we will sometimes use wdn and zdn as indicesfor example , if zk
dn = 123 , then zdn refers to the kth topic k .
second , we review the dirichlet distribution .
as we described above , a k - dimensional dirichlet is a distribution on the k 123 - simplex , that is , positive vectors over k elements that sum to one
the algorithm we present was originally developed in hoffman et al .
( 123a ) , which is a special case of the stochastic
variational inference algorithm we developed in section 123
stochastic variational inference
parameterized by a positive k - vector ,
dirichlet ( ; ) =
where ( ) is the gamma function , which is a real - valued generalization of the factorial function .
the expectation of the dirichlet is its normalized parameter ,
the expectation of its log uses ( ) , which is the rst derivative of the log gamma function ,
e ( k | ) =
e ( log k | ) = ( k ) ( cid : 123 ) k
i=123 i ( cid : 123 ) .
this can be derived by putting the dirichlet in exponential family form , noticing that log is the vec - tor of sufcient statistics , and computing its expectation by taking the gradient of the log - normalizer with respect to the natural parameter vector .
complete conditionals and variational distributions .
we specify the global and local variables of lda to place it in the stochastic variational inference setting of section 123
in topic modeling , the local context is a document d .
the local observations are its observed words wd , 123 : n .
the local hidden variables are the topic proportions d and the topic assignments zd , 123 : n .
the global hidden variables are the topics 123 : k .
recall from section 123 that the complete conditional is the conditional distribution of a vari - able given all of the other variables , hidden and observed .
in mean - eld variational inference , the variational distributions of each variable are in the same family as the complete conditional .
we begin with the topic assignment zdn .
the complete conditional of the topic assignment is a
p ( zdn = k | d , 123 : k , wdn ) exp ( log dk + log k , wdn ) .
thus its variational distribution is a multinomial q ( zdn ) = multinomial ( dn ) , where the variational parameter dn is a point on the k 123 - simplex .
per the mean - eld approximation , each observed word is endowed with a different variational distribution for its topic assignment , allowing different words to be associated with different topics .
the complete conditional of the topic proportions is a dirichlet ,
p ( d | zd ) = dirichlet ( cid : 123 ) + n
n=123 zdn ( cid : 123 ) .
since zdn is an indicator vector , the kth element of the parameter to this dirichlet is the sum of the hyperparameter and the number of words assigned to topic k in document d .
note that , although we have assumed an exchangeable dirichlet prior , when we condition on z the conditional p ( d|zd ) is a non - exchangeable dirichlet .
with this conditional , the variational distribution of the topic proportions is also dirichlet q ( d ) = dirichlet ( d ) , where d is a k - vector dirichlet parameter .
there is a different variational dirichlet parameter for each document , allowing different documents to be associated with different topics in
these are local hidden variables .
the complete conditionals only depend on other variables in the local context ( i . e . , the document ) and the global variables; they do not depend on variables from
hoffman , blei , wang and paisley
finally , the complete conditional for the topic k is also a dirichlet ,
p ( k | z , w ) = dirichlet ( cid : 123 ) + d
the vth element of the parameter to the dirichlet conditional for topic k is the sum of the hyper - parameter and the number of times that the term v was assigned to topic k .
this is a global variableits complete conditional depends on the words and topic assignments of the entire collec -
the variational distribution for each topic is a v - dimensional dirichlet ,
q ( k ) = dirichlet ( k ) .
as we will see in the next section , the traditional variational inference algorithm for lda is inef - cient with large collections of documents .
the root of this inefciency is the update for the topic parameter k , which ( from equation 123 ) requires summing over variational parameters for every word in the collection .
batch variational inference .
with the complete conditionals in hand , we now derive the coordinate ascent variational infer - ence algorithm , that is , the batch inference algorithm of figure 123
we form each coordinate update by taking the expectation of the natural parameter of the complete conditional .
this is the stepping stone to stochastic variational inference .
the variational parameters are the global per - topic dirichlet parameters 123 : k , local per - document dirichlet parameters 123 : d , and local per - word multinomial parameters 123 : d , 123 : n .
coordinate ascent variational inference iterates between updating all of the local variational parameters ( equation 123 ) and updating the global variational parameters ( equation 123 ) .
we update each document ds local variational in a local coordinate ascent routine , iterating
between updating each words topic assignment and the per - document topic proportions ,
dn exp ( ( dk ) + ( k , wdn ) ( v kv ) ) d = + n
for n ( 123 ,
these updates derive from taking the expectations of the natural parameters of the complete con - ditionals in equation 123 and equation 123
( we then map back to the usual parameterization of the multinomial . ) for the update on the topic assignment , we have used the dirichlet expectations in equation 123
for the update on the topic proportions , we have used that the expectation of an indicator is its probability , eq ( zk
dn ) = k
after nding variational parameters for each document , we update the variational dirichlet for
k = + d
this update depends on the variational parameters from every document .
batch inference is inefcient for large collections of documents .
before updating the topics 123 : k , we compute the local variational parameters for every document .
this is particularly wasteful in the beginning of the algorithm when , before completing the rst iteration , we must analyze every document with randomly initialized topics .
stochastic variational inference
stochastic variational inference
123 : initialize ( 123 ) randomly .
123 : set the step - size schedule t appropriately .
sample a document wd uniformly from the data set .
initialize dk = 123 , for k ( 123 ,
for n ( 123 , .
, n ) set
dn exp ( e ( log dk ) + e ( log k , wdn ) ) , k ( 123 ,
set d = + n dn .
until local parameters dn and d converge .
for k ( 123 , .
, k ) set intermediate topics
k = + d
set ( t ) = ( 123 t ) ( t123 ) + t
123 : until forever
figure 123 : stochastic variational inference for lda .
the relevant expectations for each update are
found in figure 123
stochastic variational inference provides a scalable method for approximate posterior inference in lda .
the global variational parameters are the topic dirichlet parameters k; the local variational parameters are the per - document topic proportion dirichlet parameters d and the per - word topic assignment multinomial parameters dn .
we follow the general algorithm of figure 123
let ( t ) be the topics at iteration t .
at each iteration we sample a document d from the collection .
in the local phase , we compute optimal variational parameters by iterating between updating the per - document parameters d ( equation 123 ) and d , 123 : n ( equation 123 ) .
this is the same subroutine as in batch inference , though here we only analyze one randomly chosen document .
in the global phase we use these tted local variational parameters to form intermediate topics ,
k = + d n
this comes from applying equation 123 to a hypothetical corpus containing d replicates of document d .
we then set the topics at the next iteration to be a weighted combination of the intermediate topics and current topics ,
= ( 123 t ) ( t )
hoffman , blei , wang and paisley
123 123 123 123 123
123 123 123 123 123
123 123 123 123 123
figure 123 : the per - word predictive log likelihood for a 123 - topic lda model on three large corpora .
( time is on the square root scale . ) stochastic variational inference on the full data con - verges faster and to a better place than batch variational inference on a reasonably sized subset .
section 123 gives the details of our empirical study .
figure 123 gives the algorithm for stochastic variational inference for lda . 123
123 bayesian nonparametric topic models with the hdp
stochastic inference for lda lets us analyze large collections of documents .
one limitation of lda , however , is that the number of topics is xed in advance .
typically , researchers nd the best number of topics with cross - validation ( blei et al . , 123 ) .
however , for very large data this approach is not practical .
we can address this issue with a bayesian nonparametric topic model , a model where the documents themselves determine the number of topics .
we derive stochastic variational inference for the bayesian nonparametric variant of lda , the hierarchical dirichlet process ( hdp ) topic model .
like lda , the hdp topic model is a mixed - membership model of text collections .
however , the hdp assumes an innite number of topics .
given a collection of documents , the posterior distribution of the hidden structure determines how many topics are needed to describe them .
further , the hdp is exible in that it allows future data to exhibit new and previously unseen topics .
more broadly , stochastic variational inference for the hdp topic model demonstrates the pos - sibilities of stochastic inference in the context of bayesian nonparametric statistics .
bayesian non - parametrics gives us a collection of exible modelsmixture models , mixed - membership models , factor models , and models with more complex structurewhich grow and expand with data ( hjort et al . , 123 ) .
flexible and expanding models are particularly important when analyzing large data sets , where it is prohibitive to search for a specic latent structure ( such as a number of topics or a tree structure of components ) with cross - validation .
here we demonstrate how to use stochastic
this algorithm , as well as the algorithm for the hdp , species that we initialize the topics k randomly .
there are
many ways to initialize the topics .
we use an exponential distribution ,
kv exponential ( d 123 / ( kv ) ) .
this gives a setting of similar to the one we would get by applying equation 123 after randomly assigning words to topics in a corpus of size d with 123 words per document .
stochastic variational inference
inference in the context of a simple bayesian nonparametric topic model .
in other work , we built on this algorithm to give scalable inference methods for bayesian nonparametric models of topic correlation ( paisley et al . , 123b ) and tree structures of topics ( paisley et al . , 123c ) .
this section is organized as follows .
we rst give some background on the dirichlet process and its denition via sethuramans stick breaking construction , which is a distribution on the innite simplex .
we then show how to use this construction to form the hdp topic model and how to use stochastic variational inference to approximate the posterior . 123
the stick - breaking construction of the dirichlet process .
bayesian nonparametric ( bnp ) meth - ods use distributions of distributions , placing exible priors on the shape of the data - generating density function .
bnp models draw a distribution from that prior and then independently draw data from that random distribution .
data analysis proceeds by evaluating the posterior distribution of the ( random ) distribution from which the data were drawn .
because of the exible prior , that posterior can potentially have mass on a wide variety of distribution shapes .
for a reviews of bnp methods , see the edited volume of hjort et al .
( 123 ) and the tutorial of gershman and blei ( 123 ) .
the most common bnp prior is the dirichlet process ( dp ) .
the dirichlet process is parame - terized by a base distribution g123 ( which may be either continuous or discrete ) and a non - negative scaling factor .
these are used to form a distribution over discrete distributions , that is , over dis - tributions that place their mass on a countably innite set of atoms .
the locations of the atoms are independently drawn from the base distribution g123 and the closeness of the probabilities to g123 is determined by the scaling factor .
when is small , more mass is placed on fewer atoms , and the draw will likely look very different from g123; when is large , the mass is spread around many atoms , and the draw will more closely resemble the base distribution .
there are several representations of the dirichlet process .
for example , it is a normalized gamma process ( ferguson , 123 ) , and its marginalization gives the chinese restaurant process ( pit - man , 123 ) .
we will focus on its denition via sethuramans stick breaking construction ( sethura - man , 123 ) .
the stick - breaking construction explicitly denes the distribution of the probabilities that make up a random discrete distribution .
it is the gateway to variational inference in bayesian nonparametric models ( blei and jordan , 123 ) .
let g dp ( , g123 ) be drawn from a dirichlet process prior .
it is a discrete distribution with mass on an innite set of atoms .
let k be the atoms in this distribution and k be their corresponding probabilities .
we can write g as
the atoms are drawn independently from g123
the stick - breaking construction species the distribu - tion of their probabilities .
the stick - breaking construction uses an innite collection of beta - distributed random variables .
recall that the beta is a distribution on ( 123 , 123 ) and dene the following collection ,
vi beta ( 123 , )
i ( 123 , 123 , 123 ,
these variables combine to form a point on the innite simplex .
imagine a stick of unit length .
break off the proportion of the stick given by v123 , call it 123 , and set it aside .
from the remainder ( of length 123 123 ) break off the proportion given by v123 , call it 123 , and set it aside .
the remainder of
this algorithm rst appeared in wang et al .
( 123 ) .
here we place it in the more general context of section 123 and
relate it to stochastic inference for lda .
hoffman , blei , wang and paisley
the stick is now 123 123 123 = ( 123 v123 ) ( 123 v123 ) .
repeat this process for the innite set of vi .
the resulting stick lengths i will sum to one .
more formally , we dene the function i to take the collection of realized vi variables and to
return the stick length of the ith component ,
i ( v ) = vi i123
j=123 ( 123 v j ) ,
and note that
i=123 i ( v ) = 123
we call vi the ith breaking proportion .
combining these steps , we form the distribution g according to the following process ,
vi beta ( 123 , )
i ( 123 , 123 , 123 ,
i ( 123 , 123 , 123 ,
in the random distribution g the ith atom i is an independent draw from g123 and it has proba - bility given by the ith stick length i ( v ) .
sethuraman ( 123 ) showed that the distribution of g is
the most important property of g is the clustering property .
even though g places mass on a countably innite set of atoms , n draws from g will tend to exhibit only a small number of them .
( how many depends on the scalar , as we described above . ) formally , this is most easily seen via other perspectives on the dp ( ferguson , 123; blackwell and macqueen , 123; pitman , 123 ) , though it can be seen intuitively with the stick - breaking construction .
the intuition is that as gets smaller more of the stick is absorbed in the rst break locations because the breaking proportions are drawn from beta ( 123 , ) .
thus , those atoms associated with the rst breaks of the stick will have larger mass in the distribution g , and that in turn encourages draws from the distribution to realize fewer individual atoms .
in general , the rst break locations tend to be larger than the later break locations .
this property is called size biasedness .
the hdp topic model .
we now construct a bayesian nonparametric topic model that has an innite number of topics .
the hierarchical dirichlet process topic model couples a set of document - level dps via a single top - level dp ( teh et al . , 123a ) .
the base distribution h of the top - level dp is a symmetric dirichlet over the vocabulary simplexits atoms are topics .
we draw once from this dp , g123 dp ( , h ) .
in the second level , we use g123 as a base measure to a document - level dp , gd dp ( , g123 ) .
we draw the words of each document d from topics from gd .
the consequence of this two - level construction is that all documents share the same collection of topics but exhibit them with different proportions .
we construct the hdp topic model using a stick - breaking construction at each levelone at the document level and one at the corpus level . 123 the generative process of the hdp topic model is as
draw an innite number of topics , k dirichlet ( ) for k ( 123 , 123 , 123 ,
draw corpus breaking proportions , vk beta ( 123 , ) for k ( 123 , 123 , 123 ,
for each document d :
see the original hdp paper of teh et al .
( 123a ) for other constructions of the hdpthe random measure con - struction , the construction by the chinese restaurant franchise , and an alternative stick - breaking construction .
this construction was mentioned by fox et al .
( 123 ) .
we used it for the hdp in wang et al .
( 123 ) .
stochastic variational inference
( a ) draw document - level topic indices , cdi multinomial ( ( v ) ) for i ( 123 , 123 , 123 ,
( b ) draw document breaking proportions , di beta ( 123 , ) for i ( 123 , 123 , 123 ,
( c ) for each word n :
draw topic assignment zdn multinomial ( ( d ) ) .
draw word wn multinomial ( cd , zdn
figure 123 illustrates this process as a graphical model .
in this construction , topics k are drawn as in lda ( step 123 ) .
corpus - level breaking proportions v ( step 123 ) dene a probability distribution on these topics , which indicates their relative prevalence in the corpus .
at the document level , breaking proportions d create a set of probabilities ( step 123b ) and topic indices cd , drawn from ( v ) , attach each document - level stick length to a topic ( step 123a ) .
this creates a document - level distribution over topics , and words are then drawn as for lda ( step
the posterior distribution of the hdp topic model gives a mixed - membership decomposition of a corpus where the number of topics is unknown in advance and unbounded .
however , it is not possible to compute the posterior .
approximate posterior inference for bnp models in general is an active eld of research ( escobar and west , 123; neal , 123; blei and jordan , 123; teh et al . ,
the advantage of our construction over others is that it meets the conditions of section 123
all the complete conditionals are in exponential families in closed form , and it neatly separates global variables from local variables .
the global variables are topics and corpus - level breaking proportions; the local variables are document - level topic indices and breaking proportions .
following the same procedure as for lda , we now derive stochastic variational inference for the hdp topic model .
complete conditionals and variational distributions .
we form the complete conditional distri -
butions of all variables in the hdp topic model .
we begin with the latent indicator variables ,
dn = 123|d , 123 : k , wdn , cd ) exp ( log i ( d ) + di = 123|v , 123 : k , wd , zd ) exp ( log k ( v ) + n
di log k , wdn ) , dn log k , wdn ) .
note the interaction between the two levels of latent indicators .
in lda the ith component of the topic proportions points to the ith topic .
here we must account for the topic index cdi , which is a random variable that points to one of the topics .
this interaction between indicators is also seen in the conditionals for the topics ,
p ( k|z , c , w ) = dirichlet ( cid : 123 ) + d
the innermost sum collects the sufcient statistics for words in the dth document that are allocated to the ith local component index .
however , these statistics are only kept when the ith topic index cdi points to the kth global topic .
the full conditionals for the breaking proportions follow those of a standard stick - breaking
construction ( blei and jordan , 123 ) ,
p ( vk|c ) = beta ( cid : 123 ) 123 + d p ( di|zd ) = beta ( cid : 123 ) 123 + n
dn , + n
di , + d
i=123 j>k c j
n=123 j>i z j
hoffman , blei , wang and paisley
the complete conditionals for all the latent variables are all in the same family as their corre - sponding distributions in the generative process .
accordingly , we will dene the variational distri - butions to be in the same family .
however , the main difference between bnp models and parametric models is that bnp models contain an innite number of hidden variables .
these cannot be com - pletely represented in the variational distribution as this would require optimizing an innite number of variational parameters .
we solve this problem by truncating the variational distribution ( blei and jordan , 123 ) .
at the corpus level , we truncate at k , tting posteriors to k breaking points , k topics , and allowing the topic pointer variables to take on one of k values .
at the document level we truncate at t , tting t breaking proportions , t topic pointers , and letting the topic assignment variable take on one of t values .
thus the variational family is ,
q ( , v , z , ) = k
q ( k | k ) q ( vk | ak ) ! d
q ( cdi | di ) q ( di | di )
q ( zdn | dn ) !
we emphasize that this is not a nite model .
with truncation levels set high enough , the vari - ational posterior will use as many topics as the posterior needs , but will not necessarily use all k topics to explain the observations .
( if k is set too small then the truncated variational distribution will use all of the topics , but this problem can be easily diagnosed and corrected . ) further , a partic - ular advantage of this two - level stick - breaking distribution is that the document truncation t can be much smaller than k .
though there may be hundreds of topics in a large corpus , we expect each document will only exhibit a small subset of them .
stochastic variational inference for hdp topic models .
from the complete conditionals , batch variational inference proceeds by updating each variational parameter using the expectation of its conditional distributions natural parameter .
in stochastic inference , we sample a data point , update its local parameters as for batch inference , and then update the global variables .
to update the global topic parameters , we again form intermediate topics with the sampled
documents optimized local parameters ,
k = + d t
we then update the global variational parameters by taking a step in the direction of the stochastic
( t+123 ) = ( 123 t ) ( t ) + t
this mirrors the update for lda .
the other global variables in the hdp are the corpus - level breaking proportions vk , each of which is associated with a set of beta parameters ak = ( a k ) for its variational distribution .
using the same randomly selected document and optimized variational parameters as above , rst construct the two - dimensional vector
ak = ( cid : 123 ) 123 + d t
then , update the parameters
di ) , + d t
= ( 123 t ) a
k + t ak .
note that we use the truncations k and t .
figure 123 summarizes the complete conditionals , vari - ational parameters , and relevant expectations for the full algorithm .
figure 123 gives the stochastic variational inference algorithm for the hdp topic model .
stochastic variational inference
hoffman , blei , wang and paisley
123 : initialize ( 123 ) randomly .
set a ( 123 ) = 123 and b ( 123 ) = .
123 : set the step - size schedule t appropriately .
sample a document wd uniformly from the data set .
for i ( 123 , .
, t ) initialize
e ( log k , wdn ) ) , k ( 123 ,
for n ( 123 , .
, n ) initialize
e ( log k , wdn ) ( cid : 123 ) , i ( 123 , .
, t ) .
for i ( 123 , .
, t ) set
di = 123 + n di = + n
di exp ( cid : 123 ) e ( log k ( v ) ) + n dn exp ( cid : 123 ) e ( log i ( d ) ) + k
for n ( 123 , .
, n ) set
until local parameters converge .
for k ( 123 , .
, k ) set intermediate topics
e ( log k , wdn ) ( cid : 123 ) , k ( 123 , .
e ( log k , wdn ) ( cid : 123 ) , i ( 123 , .
, t ) .
kv = + d t ak = 123 + d t bk = + d t
123 : until forever
( t ) = ( 123 t ) ( t123 ) + t a ( t ) = ( 123 t ) a ( t123 ) + t a , b ( t ) = ( 123 t ) b ( t123 ) + t b .
figure 123 : stochastic variational inference for the hdp topic model .
the corpus - level truncation is
k; the document - level truncation as t .
relevant expectations are found in figure 123
stochastic variational inference
figure 123 : the per - word predictive log likelihood for an hdp model on three large corpora .
( time is on the square root scale . ) as for lda , stochastic variational inference on the full data converges faster and to a better place than batch variational inference on a reasonably sized subset .
section 123 gives the details of our empirical study .
stochastic inference versus batch inference for the hdp .
figure 123 illustrates the performance of the hdp topic model on the same three large collections as in figure 123
as with lda , stochastic variational inference for the hdp converges faster and to a better model .
empirical study
in this section we study the empirical performance and effectiveness of stochastic variational infer - ence for latent dirichlet allocation ( lda ) and the hierarchical dirichlet process ( hdp ) topic model .
with these algorithms , we can apply and compare these models with very large collections of docu - ments .
we also investigate how the forgetting rate and mini - batch size s inuence the algorithms .
finally , we compare stochastic variational inference to the traditional batch variational inference
we evaluated our algorithms on three collections of documents .
for each collection , we computed a vocabulary by removing stop words , rare words , and very frequent words .
the data are
nature : this collection contains 123 , 123 documents from the journal nature ( spanning the years 123 ) .
after processing , it contains 123m observed words from a vocabulary of
new york times : this collection contains 123m documents from the new york times ( span - ning the years 123 ) .
after processing , this data contains 123m observed words from a vocabulary of 123 , 123 terms .
we implemented all algorithms in python using the numpy and scipy packages , making the implementations as similar as possible .
links to these implementations are available on the web at http : / / www . cs . princeton . edu /
hoffman , blei , wang and paisley
wikipedia : this collections contains 123m documents from wikipedia .
after processing , it
contains 123m observed words from a vocabulary of 123 , 123 terms .
for each collection , we set aside a test set of 123 , 123 documents for evaluating model tness; these test sets were not given to the algorithms for training .
evaluating model tness .
we evaluate how well a model ts the data with the predictive distribution ( geisser , 123 ) .
we are given a corpus and estimate its topics .
we then are given part of a test document , which we use to estimate that documents topic proportions .
combining those topic proportions with the topics , we form a predictive distribution over the vocabulary .
under this predictive distribution , a better model will assign higher probability to the held - out words .
in more detail , we divide each test documents words w into a set of observed words wobs and held - out words who , keeping the sets of unique words in wobs and who disjoint .
we approximate the posterior distribution of topics implied by the training data d , and then use that approximate posterior to estimate the predictive distribution p ( wnew | wobs , d ) of a new held - out word wnew from the test document .
finally , we evaluate the log probability of the words in who under this distribution .
this metric was used in teh et al .
( 123 ) and asuncion et al .
( 123 ) .
unlike previous methods , like held - out perplexity ( blei et al . , 123 ) , evaluating the predictive distribution avoids comparing bounds or forming approximations of the evaluation metric .
it rewards a good predictive distribution , however it is computed .
operationally , we use the training data to compute variational dirichlet parameters for the topics .
we then use these parameters with the observed test words wobs to compute the variational distribu - tion of the topic proportions .
taking the inner product of the expected topics and the expected topic proportions gives the predictive distribution .
to see this is a valid approximation , note the following for a k - topic lda model ,
p ( wnew | d , wobs ) = z z ( cid : 123 ) k z z ( cid : 123 ) k
k=123 kk , wnew ( cid : 123 ) p ( | wobs , ) p ( | d ) dd k=123 kkwnew ( cid : 123 ) q ( ) q ( ) dd
where q ( ) depends on the training data d and q ( ) depends on q ( ) and wobs .
the metric in - dependently evaluates each held out word under this distribution .
in the hdp , the reasoning is identical .
the differences are that the topic proportions are computed via the two - level variational stick - breaking distribution and k is the truncation level of the approximate posterior .
setting the learning rate .
stochastic variational inference introduces several parameters in setting the learning rate schedule ( see equation 123 ) .
the forgetting rate ( 123 , 123 ) controls how quickly old information is forgotten; the delay 123 down - weights early iterations; and the mini - batch size s is how many documents are subsampled and analyzed in each iteration .
although stochastic variational inference algorithm converges to a stationary point for any valid , , and s , the quality of this stationary point and the speed of convergence may depend on how these parameters
we set = 123 and explored the following forgetting rates and minibatch sizes : 123
forgetting rate ( 123 , 123 , 123 , 123 , 123 , 123 )
we also explored various values of the delay , but found that the algorithms were not sensitive .
to make this
presentaton simpler , we xed = 123 in our report of the empirical study .
stochastic variational inference
nature new york times wikipedia
figure 123 : stochastic inference lets us compare performance on several large data sets .
we xed the forgetting rate = 123 and the batch size to 123 documents .
we nd that lda is sen - sitive to the number of topics; the hdp gives consistently better predictive performance .
traditional variational inference ( on subsets of each corpus ) did not perform as well as
minibatch size s ( 123 , 123 , 123 , 123 , 123 )
we periodically paused each run to compute predictive likelihoods from the test data .
results on lda and hdp topic models .
we studied lda and the hdp .
in lda , we varied the number of topics k to be 123 , 123 , 123 , 123 and 123; we set the dirichlet hyperparameters = 123 / k .
in the hdp , we set both concentration parameters and equal to 123; we set the top - level truncation k = 123 and the second level truncation t = 123
( here t k because we do not expect documents to exhibit very many unique topics . ) in both models , we set the topic dirichlet parameter = 123 .
figure 123 shows example topics from the hdp ( on new york times and nature ) .
figure 123 gives the average predictive log likelihood for both models .
we report the value for a forgetting rate = 123 and a batch size of 123
stochastic inference lets us perform a large - scale comparison of these models .
the hdp gives consistently better performance .
for larger numbers of topics , lda overts the data .
as the modeling assumptions promise , the hdp stays robust to overtting . 123 that the hdp outperforms lda regardless of how many topics lda uses may be due in part to the additional modeling exibility given by the corpus breaking proportions v; these variables give the hdp the ability to say that certain topics are a priori more likely to appear than others , whereas the exchangeable dirichlet prior used in lda assumes that all topics are equally
we now turn to the sensitivity of stochastic inference to its learning parameters .
first , we consider the hdp ( the algorithm presented in figure 123 ) .
we xed the batch size to 123 and explored the forgetting rate . 123 figure 123 shows the results on all three corpora .
all three ts were sensitive to the forgetting rate; we see that a higher value ( i . e . , close to one ) leads to convergence to a better
fixing the forgetting rate to 123 , we explored various mini - batch sizes .
figure 123 shows the results on all three corpora .
batch sizes that are too small ( e . g . , ten documents ) can affect perfor -
though not illustrated , we note that using the traditional measure of t , held - out perplexity , does not reveal this overtting ( though the hdp still outperforms lda with that metric as well ) .
we feel that the predictive distribution is a better metric for model tness .
we t distributions using the entire grid of parameters described above .
however , to simplify presenting results we
will hold one of the parameters xed and vary the other .
hoffman , blei , wang and paisley
figure 123 : hdp inference : holding the batch size xed at 123 , we varied the forgetting rate .
slower forgetting rates are preferred .
figure 123 : hdp inference : holding the forgetting rate xed at 123 , we varied the batch size .
batch sizes may be set too small ( e . g . , ten documents ) but the difference in performance is small once set high enough .
mance; larger batch sizes are preferred .
that said , there was not a big difference between batch sizes of 123 and 123 , 123
the new york times corpus was most sensitive to batch size; the wikipedia corpus was least sensitive .
figure 123 and figure 123 illustrate ldas sensitivity to the forgetting rate and batch size , respec -
tively .
again , we nd that large learning rates and batch sizes perform well .
we developed stochastic variational inference , a scalable variational inference algorithm that lets us analyze massive data sets with complex probabilistic models .
the main idea is to use stochastic optimization to optimize the variational objective , following noisy estimates of the natural gradient where the noise arises by repeatedly subsampling the data .
we illustrated this approach with two probabilistic topic models , latent dirichlet allocation and the hierarchical dirichlet process topic
stochastic variational inference
123 123 123 123 123
123 123 123 123 123
123 123 123 123 123
figure 123 : 123 - topic lda inference : holding the batch size xed at 123 , we varied the forgetting
slower forgetting rates are preferred .
123 123 123 123 123
123 123 123 123 123
123 123 123 123 123
figure 123 : 123 - topic lda inference : holding the learning rate xed at 123 , we varied the batch
bigger batch sizes are preferred .
with stochastic variational inference , we can easily apply topic modeling to collections of millions of documents .
more importantly , this algorithm generalizes to many settings .
since developing this algorithm , we have improved on stochastic inference in a number of in gopalan et al .
( 123 ) , we applied it to the mixed - membership stochastic blockmodel for uncovering overlapping communities in large social networks .
this required sampling non - uniformly from the data and adjusting the noisy gradient accordingly .
in mimno et al .
( 123 ) , we developed a variant of stochastic inference that combines mcmc for the local updates with stochastic optimization for the global updates .
in topic modeling this allows for efcient and sparse updates .
finally , in ranganath et al .
( 123 ) , we developed adaptive learning rates for stochastic inference .
these outperform preset learning - rate schedules and require less hand - tuning by the
stochastic variational inference opens the door to several promising research directions .
we developed our algorithm with conjugate exponential family models .
this class of models is expressive , but nonconjugate modelsmodels where a richer prior is used at the expense of
hoffman , blei , wang and paisley
mathematical conveniencehave expanded the suite of probabilistic tools at our disposal .
for ex - ample , nonconjugate models can capture correlations between topics ( blei and lafferty , 123 ) or topics changing over time ( blei and lafferty , 123; wang et al . , 123 ) , and the general algorithm presented here cannot be used in these settings .
( in other work , paisley et al . , 123b developed a stochastic variational inference algorithm for a specic nonconjugate bayesian nonparametric model . ) recent research has developed general methods for non - conjugate models ( knowles and minka , 123; gershman et al . , 123; paisley et al . , 123a; wang and blei , 123 ) .
can these be scaled up with stochastic optimization ?
we developed our algorithm with mean - eld variational inference and closed form coordinate updates .
another promising direction is to use stochastic optimization to scale up recent advances in variational inference , moving beyond closed form updates and fully factorized approximate pos - teriors .
as one example , collapsed variational inference ( teh et al . , 123b , 123 ) marginalizes out some of the hidden variables , trading simple closed - form updates for a lower - dimensional posterior .
as another example , structured variational distributions relax the mean - eld approximation , letting us better approximate complex posteriors such as those arising in time - series models ( ghahramani and jordan , 123; blei and lafferty , 123 ) .
finally , our algorithm lets us potentially connect innovations in stochastic optimization to better methods for approximate posterior inference .
wahabzada and kersting ( 123 ) and gopalan et al .
( 123 ) sample from data non - uniformly to better focus on more informative data points .
we might also consider data whose distribution changes over time , such as when we want to model an innite stream of data but to forget data from the far past in a current estimate of the model .
or we can study and try to improve our estimates of the gradient .
are there ways to reduce its variance , but maintain its unbiasedness ?
david m .
blei is supported by nsf career iis - 123 , nsf bigdata iis - 123 , nsf neuro iis - 123 , onr n123 - 123 - 123 - 123 , and the alfred p .
sloan foundation .
the authors are grateful to john duchi , sean gerrish , lauren hannah , neil lawrence , jon mcauliffe , and rajesh ranganath for comments and discussions .
in section 123 , we assumed that we can calculate p ( |x , z ) , the conditional distribution of the global hidden variables given the local hidden variables z and observed variables x .
in this appendix , we show how to do stochastic variational inference under the weaker assumption that we can break the global parameter vector into a set of k subvectors 123 : k such that each conditional distribution p ( k|x , z , k ) is in a tractable exponential family :
p ( k|x , z , k ) = h ( k ) exp ( g ( x , z , k , ) t ( k ) ag ( g ( x , z , k , ) ) ) .
we will assign each k an independent variational distribution so that
q ( z , ) = ( n , j q ( zn , j ) ) k q ( k ) .
stochastic variational inference
we choose each q ( k ) to be in the same exponential family as the complete conditional p ( k|x , z , k ) ,
q ( k ) = h ( k ) exp (
k t ( k ) ag ( k ) ) .
we overload h ( ) , t ( ) , and a ( ) so that , for example , q ( k ) = p ( k|x , z , k ) when k = g ( x , z , k , ) .
the natural parameter g ( x , z , k , ) decomposes into two terms ,
g ( x , z , k , ) = g ( k , ) +
g ( xn , zn , k , ) .
the rst depends only on the global parameters k and the hyperparameters ; the second is a sum of n terms that depend on k , , and a single local context ( xn , zn ) .
proceeding as in section 123 , we will derive the natural gradient of the elbo implied by this model and choice of variational distribution .
focusing on a particular k , we can write the elbo as
l = eq ( log p ( k|x , z , k ) ) eq ( log q ( k ) ) + const .
= ( eq ( g ( x , z , k , ) ) k ) k ag ( k ) + ag ( k ) + const .
the gradient of l with respect to k is then
ag ( k ) ( eq ( g ( x , z , k , ) ) k ) ,
and the natural gradient of l with respect to k is
l = eq ( g ( x , z , k , ) ) k
= k + eq ( g ( k , ) ) +
eq ( g ( xn , zn , k , ) ) .
randomly sampling a local context ( xi , zi ) yields a noisy ( but unbiased ) estimate of the natural
li = k + eq ( g ( k , ) ) + neq ( g ( xi , zi , k , ) ) k + k .
we can use this noisy natural gradient exactly as in section 123
for each update t , we sample a context ( xt , zt ) , optimize the local variational parameters t by repeatedly applying equation equation 123 , and take a step of size t = ( t + ) in the direction of the noisy natural gradient :
k = ( 123 t ) ( t123 )
note that the update in equation 123 depends only on ( t123 ) ; we compute all elements of ( t ) simul - taneously , whereas in a batch coordinate ascent algorithm ( t )
k could depend on ( t )
