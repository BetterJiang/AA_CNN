managing large collections of documents is an impor - tant problem for many areas of science , industry , and culture .
probabilistic topic modeling offers a promising solution .
topic modeling is an unsupervised machine learning method that learns the underlying themes in a large collection of otherwise unorganized documents .
this discovered structure summarizes and organizes the documents .
however , topic models are high - level sta - tistical toolsa user must scrutinize numerical distri - butions to understand and explore their results .
in this paper , we present a method for visualizing topic mod - els .
our method creates a navigator of the documents , allowing users to explore the hidden structure that a topic model discovers .
these browsing interfaces reveal meaningful patterns in a collection , helping end - users explore and understand its contents in new ways .
we provide open source software of our method .
understanding and navigating large collections of docu - ments has become an important activity in many spheres .
however , many document collections are not coherently or - ganized and organizing them by hand is impractical .
we need automated ways to discover and visualize the structure of a collection in order to more easily explore its contents .
probabilistic topic modeling is a set of machine learning tools that may provide a solution ( blei and lafferty 123 ) .
topic modeling algorithms discover a hidden thematic struc - ture in a collection of documents; they nd salient themes and represent each document as a combination of themes .
however , topic models are high - level statistical tools .
a user must scrutinize numerical distributions to understand and explore their results; the raw output of the model is not enough to create an easily explored corpus .
we propose a method for using a tted topic model to organize , summarize , visualize , and interact with a corpus .
with our method , users can explore the corpus , moving be - tween high level discovered summaries ( the topics ) and the documents themselves , as figure 123 illustrates .
our design is centered around the idea that the model both summarizes and organizes the collection .
our method trans - lates these representations into a visual system for exploring a collection , but visualizing this structure is not enough .
the discovered structure induces relationshipsbetween topics and articles , and between articles and articleswhich lead to interactions in the visualization .
copyright c ( cid : 123 ) 123 , association for the advancement of articial intelligence ( www . aaai . org ) .
all rights reserved .
thus , we have three main goals in designing the visual - ization : summarize the corpus for the user; reveal the rela - tionships between the content and summaries; and , reveal the relationships across content .
we aim to present these in a ways that are accessible and useful to a spectrum of users , not just machine learning experts .
several solutions to the problem of understanding large doc - ument corpora include exemplar - based visualization ( chen et al .
123 ) , themeriver ( havre , hetzler , and nowell 123 ) , and facetatlas ( cao et al .
these visualizations help users understand the corpus as a whole , but do not enable exploration of individual documents .
our visualization pro - vides both a high - level summary of the corpus and links be - tween the summary and individual documents .
previous topic modeling research has focused on building new topic models and improving algorithms for tting them .
researchers have typically used browsers to evaluate model algorithms ( newman et al .
123; gretarsson et al .
123; gardener et al .
these browsers emphasize topics , giv - ing little attention to documents .
further , they include little visual representation , relying mostly on numbers to convey
we present a way of using topic models to help learn about and discover items in a corpus .
our navigator presents the output of a topic model in an interface that illuminates a given corpus to non - technical users .
probabilistic topic models
we review topic modeling , focusing on latent dirichlet al - location ( lda ) ( blei , ng , and jordan 123 ) , which is one of the simplest probabilistic topic models .
lda decomposes a collection of documents into topicsbiased probability dis - tributions over termsand represents each document with a ( weighted ) subset of the topics .
when t to a set of doc - uments , the topics are interpretable as themes in the col - lection , and the document representations indicate which themes each document is about .
thus , the learned topics summarize the collection , and the document representations organize the corpus into overlapping groups .
lda is a statistical model of a collection of texts .
hid - den random variables encode its thematic structure .
the as - sumptions lda makes can be articulated by its probabilistic generative process , the imaginary process by which the col -
figure 123 : navigating wikipedia with a topic model .
beginning in the upper left , we see a set of topics , each of which is a theme discovered by a topic modeling algorithm .
we click on a topic about lm and television .
we choose a document associated with this topic , which is the article about lm director stanley kubrick .
the page about this article includes its content and the topics that it is about .
we explore a related topic about philosophy and psychology , and nally view a related article about existentialism .
this browsing structurethe themes and how the documents are organized according to themis created by running a topic modeling algorithm on the raw text of wikipedia and visualizing its output .
this navigator can be found at http : / / bit . ly / wiki123
for k topics , choose each topic distribution k .
( each k is a distribution over the vocabulary . )
for each document in the collection : ( a ) choose a distribution over topics d .
( the variable d is a distribution over k elements . )
( b ) for each word in the document
choose a topic assignment zn from d .
( each zn is a number from 123 to k . )
choose a word wn from the topic distribution zn .
( notation zn selects the znth topic from step 123 )
notice that the same set of topics ( 123 , .
, k ) is used for every document , but that each document exhibits those topics with different proportions d .
this captures hetero - geneity in documents : lda can model that some articles are about sports and business , others are about sports and health , and that the topic of sports is similar in each .
in statistics , this is known as a mixed membership model , where each data point exhibits multiple components .
given a set of documents , the central problem for lda is posterior inference of the variables , or determining the hidden thematic structure that best explains an observed collection of documents .
this is akin to reversing the generative process to nd the topics , topic proportions , and topic assignments that best explain the observed doc - uments .
researchers have developed several algorithms for lda inference , including markov chain monte carlo sam - pling ( steyvers and grifths 123 ) and optimization - based variational inference ( blei , ng , and jordan 123 ) .
these al - gorithms take a collection of documents as input and return how the hidden topic variables decompose it .
in a set of
, k ) and a representation of each document of the collection according to those topics ( 123 , .
, d ) . 123 one of the main applications of topic models is for exploratory data analysis , that is , to help browse , understand , and summa - rize otherwise unstructured collections this is the applica - tion that motivates our work .
visualizing a topic model
our goals are to use the topic model to summarize the cor - pus , reveal the relationships between documents and the dis - covered summary , and reveal the relationships between the documents themselves .
we applied our method to 123 , 123 wikipedia articles , which we will use as a running example .
our visualization uses both the observed data from a cor - pus and the inferred topic model variables . 123 the topic model variables are the topics k , each of which is a distribution over a vocabulary , and the topic proportions d , one for each document and each of which is a distribution over the topics .
we use multiple views to illuminate this complex struc - ture and created a basic navigator that fully represents a cor - pus through the lens of an lda analysis .
in this section , we explain our design choices .
123that the topics are interpretable is an empirical nding; the name topic model is retrospective .
topic models correlate to cu - rated thesauri ( steyvers and grifths 123 ) and to human judge - ments of interpretability ( chang et al .
123 ) .
123note that the we use variables to indicate their posterior ex -
pectations .
this is to make the notation simple .
figure 123 : a topic page and document page from the navigator of wikipedia .
we have labeled how we compute each component of these pages from the output of the topic modeling algorithm .
visualizing the elements of a topic model the navigator has two main types of pages : one for display - ing discovered topics and another for the documents .
there are also overview pages , which illustrate the overall struc - ture of the corpus; they are a launching point for browsing .
these pages display the corpus and the discovered struc - ture .
but this is not sufcientwe also use the topic model inference to nd connections between these visualizations .
with these connections , a user can move between summary and document - level presentations .
hence , in our visualization every element on a page links a user to a new view .
with these links , a user can easily traverse the network of relationships in a given corpus .
for example , from a topic page a user can link to view a specic document .
this document might link to several topics , each of which the user can explore :
( son , year , death )
( god , call , give ) moses ( group , member , jewish )
( war , force , army ) we illustrated another navigation example in figure 123
an advantage of this design is that every type of relation - ship has a representation and an interaction .
this illuminates the structure of corpus to a user and helps her navigate that structure .
further , any variable may occur in multiple views; all relationships are many - to - many .
topic pages topics summarize the corpus .
in the output of an inference algorithm , they are probability distributions over the vocabulary .
but topics tend to be sparse , so a good representation is as a set of words that have high probability .
given such a set , users can often conceive meaning in a topic model ( chang et al .
for example , one can intuitively glean from the three words ( lm , series , show ) ( figure 123 ) that this topic is about lm and television .
we illustrate ex - ample topic pages in figure 123
in these pages , the terms are represented as a list of words in the left column , ordered by their topic - term probability kv .
the center column of the view lists documents that exhibit the topic , ordered by inferred topic proportion dk .
docu - ment titles links to the corresponding document pages .
we can see that the list of documents related to ( school , student , university ) ( figure 123 ) are tied to education and academics : ( school , student , university ) columbia university
finally , related topics are also listed with corresponding links , allowing a user to explore the high - level topic space .
topic similarity is not inferred directly with lda , but can be computed from the topic distributions that it discovers .
related topics are shown in the right column of the topic page by pairwise topic dissimilarity score
123r ( cid : 123 ) =123 ( iv ) 123r ( cid : 123 ) =123 ( jv ) |log ( iv ) log ( jv ) |
where the indicator function 123a ( x ) is dened as 123 if x a and 123 otherwise .
this is related to the average log odds ratio of the probability of each term in the two topics .
this metric nds topics that have similar distributions .
continuing with the topic from figure 123 , this metric scores the following topics highly . ( son , year , death ) ( lm , series , show ) ( work , book , publish ) ( album , band , music )
the original topic relates to lm and television .
the re - lated topics cover a spectrum of concepts from other forms of media to human relationships and life events .
document pages document pages render the original cor - pus .
we show the document and the topics that it exhibits , ordered by their topic proportions dk .
we also display these topics in a pie chart , showing their respective proportions within the document .
for example , figure 123 illustrates that
afunctionofd123 : d ( eq . 123 ) associatedtopics , orderedbydassociateddocuments , orderedby123 : drelateddocuments , orderedbyassociatedterms , orderedbykfunctionofk123 : k ( eq . 123 ) relatedtopics , orderedbyatermswdpresentinthedocument figure 123 : topic overviews from a visualization of wikipedia ( left ) , the new york times ( center ) , and us federal cases ( right ) .
all of these navigators are online ( see the implementation and study section ) .
the juris doctor article is a third about academia , a third about law , and a third about other topics .
each rendering of a topic links to its respective page .
finally , documents are associated with similar docu - ments .
document similarity is not inferred directly with lda , but is dened by the topic proportions :
123r ( cid : 123 ) =123 ( ik ) 123r ( cid : 123 ) =123 ( jk ) |log ( ik ) log ( jk ) | .
( 123 ) this metric says that a document is similar to other docu - ments that exhibit a similar combination of topics .
overview pages overview pages are the entry points to exploring the corpus .
in the simplest of these pages , we rank the topics by their relative presence in the corpus and display each in a bar with width proportional to the topics presence score pk : the sum of the topic proportions for a given topic over all documents ,
examples of this view can be found in figure 123
from this gure , we see that many documents are related to the topic ( household , population , female ) ; this is consistent with our observations of the corpus , which includes many wikipedia articles on individual cities , towns , and townships .
implementation and study
we provide an open source implementation of the topic modeling visualization .
there are three steps in applying our method to visualizing a corpus : ( 123 ) run lda inference on the corpus to obtain posterior expectations of the latent vari - ables ( 123 ) generate a database and ( 123 ) create the web pages to navigate the corpus .
any open - source lda package can be used; we used lda - c . 123 we implemented the remainder of the pipeline in python .
it can be found at http : / / code . google . com / p / tmve .
we created three examples of navigators using our vi - sualization .
we analyzed 123 , 123 wikipedia articles with a 123 - topic lda model ( http : / / bit . ly / wiki123 ) .
we ana - lyzed 123 , 123 us federal cases123 with a 123 - topic model ( http : / / bit . ly / case - demo ) .
we analyzed 123 , 123 new york times articles with a 123 - topic model ( http : / / bit . ly / nyt - demo ) .
a page from each of these three demos can be seen in figure 123
one week after we released the source code , we received links to a navigator of arxiv ( a large archive of sci - entic preprints ) that was generated using our code; it is at
preliminary user study we conducted a preliminary user study on seven individuals , asking for qualitative feed - back on the wikipedia navigator .
the reviews were positive , all noting the value of presenting the high - level structure of a corpus with its low - level content .
one reviewer felt it or - ganized similar to how he thinks .
six individuals responded that they discovered connec - tions that would have remained obscure by using wikipedia traditionally .
for example , one user explored articles about economics and discovered countries with ination or dea - tion problems of which he had previously been unaware .
