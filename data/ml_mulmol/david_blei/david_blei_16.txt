image classication and annotation are important prob - lems in computer vision , but rarely considered together .
in - tuitively , annotations provide evidence for the class label , and the class label provides evidence for annotations .
for example , an image of class highway is more likely anno - tated with words road , car , and trafc than words sh , boat , and scuba .
in this paper , we develop a new probabilistic model for jointly modeling the image , its class label , and its annotations .
our model treats the class label as a global description of the image , and treats an - notation terms as local descriptions of parts of the image .
its underlying probabilistic assumptions naturally integrate these two sources of information .
we derive an approximate inference and estimation algorithms based on variational methods , as well as efcient approximations for classifying and annotating new images .
we examine the performance of our model on two real - world image data sets , illustrating that a single model provides competitive annotation perfor - mance , and superior classication performance .
developing automatic methods for managing large vol - umes of digital information is increasingly important as on - line resources continue to be a vital resource in everyday life .
among these methods , automatically organizing and indexing multimedia data remains an important challenge .
we consider this problem for image data that are both la - beled with a category and annotated with free text .
in such data , the class label tends to globally describe each image , while the annotation terms tend to describe its individual components .
for example , an image in the outdoor cate - gory might be annotated with tree , ower , and sky .
image classication and image annotation are typically treated as two independent problems .
our motivating intu - ition , however , is that these two tasks should be connected .
an image annotated with car and pedestrian is unlikely to be labeled as a living room scene .
an image labeled as
an ofce scene is unlikely to be annotated with swimming pool or sunbather .
in this paper , we develop a proba - bilistic model that simultaneously learns the salient patterns among images that are predictive of their class labels and annotation terms .
for new unknown images , our model pro - vides predictive distributions of both class and annotation .
we build on recent machine learning and computer vi - sion research in probabilistic topic models , such as latent dirichlet allocation ( lda ) ( 123 ) and probabilistic latent se - mantic indexing ( 123 ) ( plsi ) .
probabilistic topic models nd a low dimensional representation of data under the assump - tion that each data point can exhibit multiple components or topics .
while topic models were originally developed for text , they have been successfully adapted and extended to many computer vision problems ( 123 , 123 , 123 , 123 , 123 , 123 ) .
our model nds a set of image topics that are predictive of both class label and annotations .
the two main contribu - tions of this work are :
we extended supervised topic modeling ( 123 ) ( slda ) to classication problems .
slda was originally devel - oped for predicting continuous response values , via a linear regression .
we note that the multi - class exten - sion presented here is not simply a plug - and - play extension of ( 123 ) .
as we show in section 123 , it re - quires substantial development of the underlying in - ference and estimation algorithms .
we embed a probabilistic model of image annotation into the resulting supervised topic model .
this yields a single coherent model of images , class labels and an - notation terms , allowing classication and annotation to be performed using the same latent topic space .
we nd that a single model , t to images with class la - bels and annotation terms , provides state - of - the - art annota - tion performance and exceeds the state - of - the - art in classi - cation performance .
this shows that image classication and annotation can be performed simultaneously .
this paper is organized as follows .
in section 123 , we de - scribe our model and derive variational algorithms for infer - ence , estimation , and prediction .
in section 123 , we describe
related work .
in section 123 , we study the performance of our models on classication and annotation for two real - world image datasets .
we summarize our ndings in section 123
models and algorithms
in this section , we develop two models : multi - class slda and multi - class slda with annotations .
we derive a variational inference algorithm for approximating the poste - rior distribution , and an approximate parameter estimation algorithm for nding maximum likelihood estimates of the model parameters .
finally , we derive prediction algorithms for using these models to label and annotate new images .
modeling images , labels and annotations
the idea behind our model is that class and annotation are related , and we can leverage that relationship by nding a latent space predictive of both .
our training data are im - ages that are categorized and annotated .
in testing , our goal is to predict the category and annotations of a new image .
each image is represented as a bag of codewords r123 : n , which are obtained by running the k - means algorithm on patches of the images ( 123 , 123 ) .
( see section 123 for more de - tails about our image features . ) the category c is a discrete class label .
the annotation w123 : m is a collection of words from a xed vocabulary .
we x the number of topics k and let c denote the num - ber of class labels .
the parameters of our model are a set of k image topics 123 : k , a set of k annotation topics 123 : k , and a set of c class coefcients 123 : c .
each coefcient c is a k - vector of real values .
each topic is a distribution over a vocabulary , either image codewords or annotation terms .
our model assumes the following generative process of an image , its class label , and its annotation .
draw topic proportions dir ( ) .
for each image region rn , n ( 123 , 123 , .
, n ) :
( a ) draw topic assignment zn | mult ( ) .
( b ) draw region codeword rn | zn mult ( zn ) .
draw class label c | z123 : n softmax ( z , ) , where z = n=123 zn is the empirical topic frequencies and the softmax function provides the following distribution ,
p ( c | z , ) = exp ( cid : 123 ) t
c z ( cid : 123 ) / pc
l z ( cid : 123 ) .
for each annotation term wm , m ( 123 , 123 , .
, m ) :
( a ) draw region identier ym unif ( 123 , 123 , .
, n ) ( b ) draw annotation term wm mult ( zn ) .
figure 123 ( a ) illustrates our model as a graphical model .
we refer to this model as multi - class slda with annota - tions .
it models both the image class and image annotation with the same latent space .
consider step 123 of the generative process .
in modeling the class label , we use a similar set - up as supervised lda ( slda ) ( 123 ) .
in slda , a response variable for each doc - ument ( here , an image ) is assumed drawn from a gener - alized linear model with input given by the empirical dis - tribution of topics that generated the image patches .
in ( 123 ) , that response variable is real valued and drawn from a linear regression , which simplied inference and estimation .
however , a continuous response is not appropriate for our goal of building a classier .
rather , we consider a class label response variable , drawn from a softmax regression for classication .
this complicates the approximate infer - ence and parameter estimation algorithms ( see section 123 and 123 ) , but provides an important extension to the slda framework .
we refer to this multi - class extension of slda ( without the annotation portion ) as multi - class slda .
we note that multi - class slda can be used in classication problems outside of computer vision .
we now turn to step 123 of the generative process .
to model annotations , we use the same generative process as correspondence lda ( corr - lda ) ( 123 ) , where each annota - tion word is assumed to be drawn from one of the topics that is associated with an image patch .
for example , this will encourage words like blue and white to be associ - ated with the image topics that describe patches of sky .
we emphasize that corr - lda and slda were developed for different purposes .
corr - lda nds topics predictive of annotation words; slda nds topics predictive of a global response variable .
however , both approaches employ sim - ilar statistical assumptions .
first , generate the image from a topic model .
then , generate its annotation or class label from a model conditioned on the topics which generated the image .
our model uses the same latent topic space to gen - erate both the annotation and class label .
approximate inference
in posterior inference , we compute the conditional dis - tribution of the latent structure given a model and a labeled annotated image .
as for lda , computing this posterior ex - actly is not possible ( 123 ) .
we employ mean - eld variational methods for a scalable approximation algorithm .
variational methods consider a simple family of distri - butions over the latent variables , indexed by free variational parameters , and try to nd the setting of those parameters that minimizes the kullback - leibler ( kl ) divergence to the true posterior ( 123 ) .
in our model , the latent variables are the per - image topic proportions , the per - codeword topic as - signment zn , and the per - annotation word region identier ym .
note that there are no latent variables explicitly asso - ciated with the class; its distribution is wholly governed by the per - codeword topic assignments .
annotations : skier , ski , tree , water , boat , building , sky , residential area
predicted class : snowboarding
predicted annotations : athlete , sky , tree , water , plant , ski , skier
figure 123
a graphical model representation of our model .
nodes represent random variables; edges denote possible dependence between random variables; plates denote replicated structure .
note that in this model , the image class c and image annotation wm are dependent on the topics that generated the image codewords rn .
an example image with the class label and annotations from the uiuc - sport dataset ( 123 ) .
the italic words are the predicted class label and annotations , using our model .
the mean - eld variational distribution is ,
address this , we lower bound this term with jensens inequality .
this gives :
l z ) ( cid : 123 ) i takes o ( k n ) time .
q ( , z , y ) = q ( | ) qn
where n is a variational multinomial over the k topics , is a variational dirichlet , and m is a variational multino - mial over the image regions .
we t these parameters with coordinate ascent to minimize the kl divergence between q and the true posterior .
( this will nd a local minimum . )
let = ( , 123 : k , 123 : c , 123 : k ) .
following jordan et al .
( 123 ) , we bound the log - likelihood of a image - class - annotation triple , ( r , c , w ) .
we have :
log p ( r , c , w| )
= logz p ( , z , y , r , c , w| ) q ( , z , y )
q ( , z , y )
eq ( log p ( , z , y , r , c , w| ) ) eq ( q ( , z , y ) ) =l ( , , ; ) .
the coordinate ascent updates for and are the same as those in ( 123 ) , which uses the same notation :
i=123 ni log i , wm ( cid : 123 ) .
we next turn to the update for the variational multino - mial .
here , the variational method derived in ( 123 ) cannot be used because the expectation of the log partition func - tion for softmax regression ( i . e . , multi - class classication ) cannot be exactly computed .
the terms in l containing n
mn log i , wm ! +
j ) + log i , rn+
ni log ni .
nj exp ( cid : 123 ) 123
plugging equation 123 into equation 123 , we obtain a lower bound of l ( n ) , which we will denote l
we present a xed - point iteration for maximizing this the idea is that given an old estimation of is constructed so that
n , a lower bound of l this lower bound is tight on old mizing this lower bound of l form and old
is solved in closed -
is updated correspondingly .
we note that
j=123 nj exp ( cid : 123 ) 123
n lj ( cid : 123 ) ( cid : 123 ) is only a linear func -
thus can be written as ht n , where h = tion of n , ( h123 , , hi , , hk ) t and does not contain n .
for con - venience , dene bi as follows ,
bi = ( i ) (
j ) + log i , rn +
mn log i , wm .
now , the lower bound l
( n ) can be written as
c nlog ( ht n )
ni log ni .
finally , suppose we have a previous value old
for log ( x ) , we know log ( x ) 123x + log ( ) 123 , x > 123 , > 123 , where the equality holds if and only if x =
x = ht n and = ht old
immediately , we have :
c n ( ht old
n ) 123ht n
n ) + 123
ni log ni .
this lower bound of l
( n ) is tight when n = old
mizing equation 123 under the constraintpk
to the xed point update ,
maxi - i=123 ni = 123 leads
ni i , rn exp ( cid : 123 ) ( i ) +pm n ) 123hi ( cid : 123 ) .
ci ( ht old
m=123 mn log i , wm
observe how the per - feature variational distribution over topics depends on both class label c and annotation infor - mation wm .
the combination of these two sources of data has naturally led to an inference algorithm that uses both .
the full variational inference procedure repeats the updates of equations 123 , 123 and 123 until equation 123 , the lower bound on the log marginal probability log p ( r , c , w| ) , converges .
parameter estimation
given a corpus of image data with class labels and anno - tations , d = ( ( rd , wd , cd ) ) d d=123 , we nd the maximum like - lihood estimation for image topics 123 : k , text topics 123 : k and class coefcients 123 : c .
we use variational em , which replaces the e - step of expectation - maximization with vari - ational inference to nd an approximate posterior for each data point .
in the m - step , as in exact em , we nd approxi - mate maximum likelihood estimates of the parameters using expected sufcient statistics computed from the e - step .
recall = ( , 123 : k , 123 : c , 123 : k ) .
the corpus log -
log p ( rd , cd , wd| ) .
( we do not optimize in this paper . ) again , we maximize the lower bound of l ( d ) by plugging equations 123 and 123 into equation 123
let vr denote the number of codewords , the terms con -
taining 123 : k ( with lagrangian multipliers ) are :
l ( 123 : k ) ( d ) =
dni log i , rn +
setting l ( 123 : k ) ( d ) / if = 123 leads to
123 ( rn = f ) dni .
next , let vw denote the number of total annotations , and the terms containing 123 : k ( with lagrangian multipli -
l ( 123 : k ) ( d ) =
mnni log i , wm +
iw 123 ! .
setting l ( 123 : k ) ( d ) / iw = 123 leads to
123 ( wm = w ) xn
finally , terms containing 123 : c are :
l ( 123 : c ) ( d ) =
d log c
dni exp ( cid : 123 ) 123
setting l ( 123 : c ) ( d ) / ci = 123 does not lead to a closed - form solution .
we optimize with conjugate gradient ( 123 ) .
let d =pc
i=123 dni exp ( cid : 123 ) 123
gate gradient only requires the derivatives :
xd=123 ( cid : 123 ) 123 ( cd = c ) di ( cid : 123 )
dnj exp ( cid : 123 ) 123
dni exp ( cid : 123 ) 123
j=123 dnj exp ( cid : 123 ) 123
classication and annotation
with inference and parameter estimation algorithms in place , it remains to describe how to perform prediction , i . e .
predicting both a class label and annotations from an un - known image .
the rst step is to perform variational in - ference given the unknown image .
we can use a variant of the algorithm in section 123 to determine q ( , z ) .
since the class label and annotations are not observed , we remove the mn terms from the variational distribution ( equation 123 ) and the terms involving c from the updates on the topic multinomials ( equation 123 ) .
in classication , we estimate the probability of the label c by replacing the true posterior p ( z|w , r ) with the varia -
z exp t
c z log c l z ) ! ! q ( z ) dz c z ( cid : 123 ) eq " log l l z ) ! # ! ,
where the last equation comes from jensens inequality , and q is the variational posterior computed in the rst step .
the second term in the exponent is constant with respect to class label .
thus , the prediction rule is
c = arg max
c z ( cid : 123 ) = arg max
there are two approximations at play .
first , we approxi - mate the posterior with q .
second , we approximate the ex - pectation of an exponential using jensens inequality .
while there are no theoretical guarantees here , we evaluate this classication procedure empirically in section 123
the procedure for predicting annotations is the same as in ( 123 ) .
to obtain a distribution over annotation terms , we average the contributions from each region ,
related work
image classication and annotation are both important problems in computer vision and machine learning .
much previous work has explored the use of global image features for scene ( or event ) classication ( 123 , 123 , 123 , 123 , 123 ) , and both discriminative and generative techniques have been applied to this problem .
discriminative methods include the work in ( 123 , 123 , 123 , 123 ) .
generative methods include the work in ( 123 , 123 , 123 , 123 ) .
in the work of ( 123 ) , the au - thors combine generative models for latent topic discov - ery ( 123 ) and discriminative methods for classication ( k - nearest neighbors ) .
lda - based image classication was in - troduced in ( 123 ) , where each category is identied with its own dirichlet prior , and that prior is optimized to distin - guish between them .
the multi - class slda model combines the generative and discriminative approaches , which may be better for modeling categorized images ( see section 123 ) .
for image annotation , several studies have explored the use of probabilistic models to learn the relationships be - tween images and annotation terms ( 123 , 123 , 123 ) .
our model is most related to the family of models based on lda , which were introduced to image annotation in ( 123 ) .
but the idea that image annotation and classication might share the same latent space has not been studied .
we will compare the
performance of our model to corr - lda ( 123 ) .
was shown to provide better performance than the previous lda - based annotation models in ( 123 ) and ( 123 ) . )
empirical results
we test our models with two real - world data sets that contain class labels and annotations : a subset from la - belme ( 123 ) and the uiuc - sport data from ( 123 ) .
labelme data , we used the on - line tool to obtain images from the following 123 classes : highway , inside city , tall building , street , forest , coast , mountain , and open country .
we rst only kept the images that were 123 123 pixels , and then randomly selected 123 images for each class .
( in doing this , we attempted to obtain the same im - age data as described in ( 123 ) . ) the total number of images is 123
the uiuc - sport dataset ( 123 ) contains 123 types of sports : badminton , bocce , croquet , polo , rock - climbing , rowing , sailing and snowboarding .
the number of images in each class varies from 123 ( bocce ) to 123 ( rowing ) .
the total number of images is 123
following the setting in ( 123 ) , we use the 123 - dimensional sift ( 123 ) region descriptors selected by a sliding grid ( 123 123 ) .
we ran the k - means algorithm ( 123 ) to obtain the codewords and codebook .
we report on a codebook of ( other codebook sizes gave similar per - formance . ) in both data sets , we removed annotation terms that occurred less than 123 times .
on average , there are 123 terms per annotation in the labelme data , and 123 terms per annotation in the uiuc - sport data .
finally , we evenly split each class to create the training and testing sets .
our procedure is to train the multi - class slda with an - notations on labeled and annotated images , and train the multi - class slda model on labeled images .
all testing is on unlabeled and unannotated images .
see figure 123 for ex - ample annotations and classications from the multi - class slda with annotations .
image classication .
to assess our models on image classication , we compared the following methods ,
fei - fei and perona , 123 : this is the model from ( 123 ) .
it is trained on labeled images without annotation .
bosch et al . , 123 : this is the model described in ( 123 ) .
it rst employs plsa ( 123 ) to learn latent topics , and then uses the k - nearest neighbor ( knn ) classier for classication .
we use unsupervised lda123 to learn the latent topics and , following ( 123 ) , set the number of neighbors to be 123
as for the other models considered here , we use sift features .
we note that ( 123 ) use other types of features as well .
123according to ( 123 ) , plsa performs similarly to unsupervised lda in
image classification on the labelme dataset
image classification on the uiucsport dataset
multiclass slda with annotations
feifei and perona , 123
bosch et al . , 123
figure 123
comparisons of average accuracy over all classes based on 123 random train / test subsets .
multi - class slda with annotations and multi - class slda ( red curves in color ) are both our models .
accuracy as a function of the number of topics on the labelme dataset .
accuracy as a function of the number of topics on the uiuc - sport dataset .
multi - class slda : this is the multi - class slda model ,
described in this paper .
multi - class slda with annotations : this is multi - class
slda with annotations , described in this paper .
note all testing is performed on unlabeled and unannotated
the results are illustrated in the graphs of figure 123 and in the confusion matrices of figure 123 our modelsmulti - class slda and multi - class slda with annotations per - form better than the other approaches .
they reduce the error of fei - fei and perona , 123 by at least 123% on both data sets , and even more for bosch et al . , 123
this demon - strates that multi - class slda is a better classier , and that joint modeling does not negatively affect classication ac - curacy when annotation information is available .
in fact , it usually increases the accuracy .
observe that the model of ( 123 ) , unsupervised lda com - bined with knn , gives the worst performance of these methods .
this highlights the difference between nding topics that are predictive , as our models do , and nding topics in an unsupervised way .
the accuracy of unsuper - vised lda might be increased by using some of the other visual features suggested by ( 123 ) .
here , we restrict ourselves to sift features in order to compare models , rather than
as the number of topics increases , the multi - class slda models ( with and without annotation ) do not overt until around 123 topics , while fei - fei and perona , 123 begins to overt at 123 topics .
this suggests that multi - class slda , which combines aspects of both generative and discrimina - tive classication , can handle more latent features than a purely generative approach .
on one hand , a large number
123other than the topic models listed , we also tested an svm - based ap - proach using sift image features .
the svm yielded much worse perfor - mance than the topic models ( 123% for the labelme data , and 123% for the uiuc - sport data ) .
these are not marked on the plots .
of topics increases the possibility of overtting; on the other hand , it provides more latent features for building the clas -
in the case of multi - class slda with annotations , we can use the same trained model for image annotation .
we emphasize that our models are designed for simultaneous classication and annotation .
for image an - notation , we compare following two methods ,
blei and jordan , 123 : this is the corr - lda model
from ( 123 ) , trained on annotated images .
multi - class slda with annotations : this is exactly the same model trained for image classication in the pre - vious section .
in testing annotation , we observe only
to measure image annotation performance , we use an evaluation measure from information retrieval .
speci - cally , we examine the top - n f - measure123 , denoted as f - measure@n , where we set n = 123
we nd that multi - class slda with annotations performs slightly better than corr - lda over all the numbers of topics tested ( about 123% relative improvement ) .
for example , considering models with 123 topics , the labelme f - measures are 123% ( corr - lda ) and 123% ( multi - class slda with annotations ) ; on uiuc - sport , they are 123% ( corr - lda ) and 123% ( multi - class slda with annotations ) .
these results demonstrate that our models can perform classication and annotation with the same latent space .
with a single trained model , we nd the annotation per - formance that is competitive with the state - of - the - art , and classication performance that is superior .
123f - measure is dened as 123 precision recall / ( precision + recall ) .
multiclass slda with annotations
multiclass slda with annotations
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
. 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123
( a ) labelme : avg .
accuracy : 123%
( b ) labelme : avg .
accuracy : 123%
( c ) uiuc - sport : avg .
accuracy : 123%
( d ) uiuc - sport : avg .
accuracy : 123%
figure 123
comparisons using confusion matrices , all from the 123 - topic models using multi - class slda with annotations and multi - class slda .
( a ) multi - class slda with annotations on the labelme dataset .
( b ) multi - class lda on the labelme dataset .
( c ) multi - class slda with annotations on the uiuc - sport dataset .
( d ) multi - class slda model on the uiuc - sport dataset .
we have developed a new graphical model for learning the salient patterns in images that are simultaneously pre - dictive of class and annotations .
in the process , we have derived the multi - class setting of supervised topic models and studied its performance for computer vision problems .
on real - world image data , we have demonstrated that the proposed model is on par with state - of - the - art image an - notation methods and outperforms current state - of - the - art image classication methods .
guided by the intuition that classication and annotation are related , we have illustrated that the same latent space can be used to predict both .
acknowledgments .
david m .
blei is supported by onr 123 - 123 , nsf career 123 , and grants from google and microsoft .
li fei - fei is supported by a mi - crosoft research new faculty fellowship and a grant from
