we develop the relational topic model ( rtm ) , a model of documents and the links between them .
for each pair of documents , the rtm models their link as a binary random variable that is conditioned on their contents .
the model can be used to summarize a network of documents , predict links between them , and predict words within them .
we derive efcient inference and learning algorithms based on variational meth - ods and evaluate the predictive performance of the rtm for large networks of scientic abstracts and web documents .
network data , such as citation networks of documents , hy - perlinked networks of web pages , and social networks of friends , are becoming pervasive in modern machine learn - ing applications .
analyzing network data provides useful predictive models , pointing social network members to - wards new friends , scientic papers towards relevant cita - tions , and web pages towards other related pages .
recent research in this eld has focused on latent variable models of link structure , models which decompose a net - work according to hidden patterns of connections between its nodes ( kemp et al .
123; hoff et al .
123; hofman and wiggins 123; airoldi et al .
though powerful , these models account only for the structure of the network , ig - noring observed attributes of the nodes .
for example , a network model can nd patterns which account for the ci - tation connections between scientic articles , but it cannot also account for the texts .
this type of information about the nodes , along with the
appearing in proceedings of the 123th international confe - rence on articial intelligence and statistics ( aistats ) 123 , clearwa - ter beach , florida , usa .
volume 123 of jmlr : w&cp 123
copyright 123 by the authors .
links between them , should be used for uncovering , under - standing and exploiting the latent structure in the data .
to this end , we develop a new model of network data that ac - counts for both links such as citations and attributes such accounting for patterns in both sources of data leads to a more powerful model than those that only consider links .
given a new node and some of its links , traditional models of network structure can provide a predictive distribution of other nodes with which it it might be connected .
our model need not observe any links of a new node; it can pre - dict links using only its attributes .
thus , we can suggest citations of newly written papers , predict the likely hyper - links of a web page in development , or suggest friendships in a social network based only on a new users prole of interests .
moreover , given a new node and its links , our model provides a predictive distribution of node attributes .
this complementary predictive mechanism can be used to predict keywords from citations or a users interests from his or her social connections .
these types of predictions are out of reach for traditional network models .
our model is the relational topic model ( rtm ) , a hierar - chical model of links and node attributes .
focusing on net - works of text data , the rtm explicitly ties the content of the documents with the connections between them .
first , we describe the statistical assumptions behind the rtm .
then , we derive efcient algorithms for approximate posterior in - ference , parameter estimation , and prediction .
finally , we study its performance on scientic citation networks and hyperlinked web pages .
the rtm provides signicantly better word prediction and link prediction than natural al - ternatives and the current state of the art .
123 relational topic models
the relational topic model ( rtm ) is a model of data com - posed of documents , which are collections of words , and links between them ( see figure 123 ) .
it embeds this data in a latent space that explains both the words of the documents and how they are connected .
123 relational topic models for document networks
figure 123 : example data appropriate for the relational topic model .
each document is represented as a bag of words and linked to other documents via citation .
the rtm denes a joint distribution over the words in each document and the citation links between them .
the rtm is based on latent dirichlet allocation ( lda ) ( blei et al .
lda is a generative probabilistic model that uses a set of topics , distributions over a xed vocab - ulary , to describe a corpus of documents .
in its genera - tive process , each document is endowed with a dirichlet - distributed vector of topic proportions , and each word of the document is assumed drawn by rst drawing a topic assignment from those proportions and then drawing the word from the corresponding topic distribution .
in the rtm , each document is rst generated from topics as in lda .
the links between documents are then modeled as binary variables , one for each pair of documents .
these are distributed according to a distribution that depends on the topics used to generate each of the constituent documents .
in this way , the content of the documents are statistically connected to the link structure between them .
the parameters to the rtm are k distributions over terms 123 : k , a k - dimensional dirichlet parameter , and a func - tion that provides binary probabilities .
( this function is explained in detail below . ) the rtm assumes that a set of observed documents w123 : d , 123 : n and binary links between them y123 : d , 123 : d are generated by the following process .
for each document d :
( a ) draw topic proportions d| dir ( ) .
( b ) for each word wd , n :
draw assignment zd , n|d mult ( d ) .
draw word wd , n|zd , n , 123 : k mult ( zd , n ) .
for each pair of documents d , d123 : ( a ) draw binary link indicator
y|zd , zd123 ( |zd , zd123 ) .
figure 123 illustrates the graphical model for this process for a single pair of documents .
the full model , which is dif - cult to illustrate , contains the observed words from all d documents , and d123 link variables for each possible con - nection between them .
the function is the link probability function that denes a distribution over the link between two documents .
this function is dependent on the topic assignments that gener - ated their words , zd and zd123
we explore two possibilities .
first , we consider
( y = 123 ) = ( t ( zd zd123 ) + ) ,
the notation denotes the where zd = 123 hadamard ( element - wise ) product , and the function is the sigmoid .
this link function models each per - pair bi - nary variable as a logistic regression with hidden covari - ates .
it is parameterized by coefcients and intercept .
the covariates are constructed by the hadamard product of zd and zd123 , which captures similarity between the hidden topic representations of the two documents .
second , we consider
e ( y = 123 ) = exp ( t ( zd zd123 ) + ) .
here , e uses the same covariates as , but has an ex - ponential mean function instead .
rather than tapering off when zd zd123 are close , the probabilities returned by this function continue to increases exponentially .
with some algebraic manipulation , the function e can be viewed as an approximate variant of the modeling methodology pre - sented in blei and jordan ( 123 ) .
in both of the functions we consider , the response is a function of the latent feature expectations , zd and zd123
123we address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high - accuracy concepts . . . irrelevant features and the subset selection problemin many domains , an appropriate inductive bias is the min - features bias , which prefers consistent hypotheses definable over as few features as possible . . . learning with many irrelevant featuresin this introduction , we define the term bias as it is used in machine learning systems .
we motivate the importance of automated methods for evaluating . . . evaluation and selection of biases in machine learningthe inductive learning problem consists of learning a concept given examples and nonexamples of the concept .
to perform this learning task , inductive learning algorithms bias their learning method . . . utilizing prior concepts for learningthe problem of learning decision rules for sequential tasks is addressed , focusing on the problem of learning tactical plans from a simple flight simulator where a plane must avoid a missile . . . improving tactical plans with genetic algorithmsevolutionary learning methods have been found to be useful in several areas in the development of intelligent robots .
in the approach described here , evolutionary . . . an evolutionary approach to learning in robotsnavigation through obstacles such as mine fields is an important capability for autonomous underwater vehicles .
one way to produce robust behavior . . . using a genetic algorithm to learn strategies for collision avoidance and local navigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . chang , blei
figure 123 : a two - document segment of the rtm .
the variable y indicates whether the two documents are linked .
the complete model contains this variable for each pair of documents .
the plates indicate replication .
this model captures both the words and the link structure of the data shown in figure 123
formulation , inspired by the supervised lda model ( blei and mcauliffe 123 ) , ensures that the same latent topic as - signments used to generate the content of the documents also generates their link structure .
models which do not enforce this coupling , such as nallapati et al .
( 123 ) , might divide the topics into two independent subsetsone for links and the other for words .
such a decomposition pre - vents these models from making meaningful predictions about links given words and words given links .
tion 123 we demonstrate empirically that the rtm outper - forms such models on these tasks .
inference , estimation , and
with the model dened , we turn to approximate poste - rior inference , parameter estimation , and prediction .
we develop a variational inference procedure for approximat - ing the posterior .
we use this procedure in a variational expectation - maximization ( em ) algorithm for parameter estimation .
finally , we show how a model whose parame - ters have been estimated can be used as a predictive model of words and links .
in posterior inference , we seek to compute the posterior distribution of the latent variables condi - tioned on the observations .
exact posterior inference is in - tractable ( blei et al .
123; blei and mcauliffe 123 ) .
we appeal to variational methods .
in variational methods , we posit a family of distributions over the latent variables indexed by free variational pa - rameters .
those parameters are t to be close to the true posterior , where closeness is measured by relative entropy .
see jordan et al .
( 123 ) for a review .
we use the fully -
q ( , z| , ) =q
n qz ( zd , n|d , n ) ) ,
123sums over document pairs ( d123 , d123 ) are understood to range
over pairs for which a link has been observed .
where is a set of dirichlet parameters , one for each doc - ument , and is a set of multinomial parameters , one for each word in each document .
note that eq ( zd , n ) = d , n .
minimizing the relative entropy is equivalent to maximiz - ing the jensens lower bound on the marginal probability of the observations , i . e . , the evidence lower bound ( elbo ) ,
eq ( log p ( yd123 , d123|zd123 , zd123 , , ) ) + eq ( log p ( wd , n|123 : k , zd , n ) ) + eq ( log p ( zd , n|d ) ) + eq ( log p ( d| ) ) + h ( q ) ,
where ( d123 , d123 ) denotes all document pairs .
the rst term of the elbo differentiates the rtm from lda ( blei et al .
the connections between documents affect the ob - jective in approximate posterior inference ( and , below , in we develop the inference procedure under the assumption that only observed links will be modeled ( i . e . , yd123 , d123 is ei - ther 123 or unobserved ) . 123 we do this for two reasons .
first , while one can x yd123 , d123 = 123 whenever a link is ob - served between d123 and d123 and set yd123 , d123 = 123 otherwise , this approach is inappropriate in corpora where the absence of a link cannot be construed as evidence for yd123 , d123 = 123
in these cases , treating these links as unobserved variables is more faithful to the underlying semantics of the data .
for example , in large social networks such as facebook the ab - sence of a link between two people does not necessarily mean that they are not friends; they may be real friends who are unaware of each others existence in the network .
treating this link as unobserved better respects our lack of knowledge about the status of their relationship .
second , treating non - links links as hidden decreases the
123nddwd , nzd , nkkyd , d ' nd ' d ' wd ' , nzd ' , n relational topic models for document networks
computational cost of inference; since the link variables are leaves in the graphical model they can be removed when - ever they are unobserved .
thus the complexity of compu - tation scales with the number of observed links rather than the number of document pairs .
this provides a signicant our aim now is to compute each term of the objective function given in equation 123
the rst term depends on our choice of link probability function .
this term is not tractable to compute when the logistic function of equa - tion 123 is chosen .
we use a rst - order approximation ( braun and mcauliffe 123 ) ,
t d123 , d123 + + log ( cid : 123 ) t d123 , d123 ( cid : 123 ) ,
ld123 , d123 eq ( log p ( yd123 , d123 = 123|zd123 , zd123 , , ) ) where d123 , d123 = d123 d123 and d = eq ( zd ) = n d , n .
when e is the response function , this term can be computed explicitly as eq ( log p ( yd123 , d123 = 123|zd123 , zd123 , , ) ) = t d123 , d123 + .
we use coordinate ascent to optimize the elbo with re - spect to the variational parameters , ,
d123=d ( d , d123 ld , d123 ) d123 eq ( log d|d ) + log , wd , j ) ,
where ld , d123 is computed according to either equation 123 or log , wd , j can be com - 123 depending on the choice of .
puted by taking the element - wise logarithm of the wd , jth
column of .
eq ( log d|d ) is ( d ) ( p d , i ) , where for lda ( blei et al .
123 ) , d +p
is the digamma function .
( a digamma of a vector is the vector of digammas . ) the update for is identical to that in variational inference
parameter estimation we t the model by nding max - imum likelihood estimates for each of the parameters : multinomial topic vectors 123 : k and link function param - eters , .
once again , this is intractable so we turn to an approximation .
we employ variational expectation - maximization , where we iterate between optimizing the elbo of equation 123 with respect to the variational distri - bution and with respect to the model parameters .
optimizing with respect to the variational distribution is de - scribed in section 123
optimizing with respect to the model parameters is equivalent to maximum likelihood estimation with expected sufcient statistics , where the expectation is taken with respect to the variational distribution .
since the terms in equation 123 that involve are identical to those in lda , estimating the topic vectors can be done via the same update :
123 ( wd , n = w ) k
in practice , we smooth our estimates of k , w using a sym - metric dirichlet prior on the topics .
it is not possible to directly optimize the parameters of the link probability function without negative observations ( i . e . , yd123 , d123 = 123 ) .
we address this by applying a regular - ization penalty parameterized by a scalar , .
the effect of this regularization is to posit some number of latent neg - ative observations in the network and to incorporate them into the parameter estimates .
the frequency of the nega - tive observations is controlled by .
( for space we omit the derivation of this regularization term . ) when using the logistic function of equation 123 , we use gradient - based optimization to estimate the parameters and .
using the approximation used in equation 123 , the relevant gradients of the elbo are
( cid : 123 ) 123 ( cid : 123 ) td123 , d123 + ( cid : 123 ) ( cid : 123 ) d123 , d123 ( cid : 123 ) 123 ( cid : 123 ) td123 , d123 + ( cid : 123 ) ( cid : 123 )
( cid : 123 ) / k 123 + ( cid : 123 ) / k 123 , ( cid : 123 ) 123t / k 123 + ( cid : 123 ) .
log ( cid : 123 ) 123 123t ( cid : 123 ) log log ( cid : 123 ) ( cid : 123 ) log
+ 123 123t
when using the exponential function of equation 123 , we can estimate the parameters and analytically ,
prediction with a tted model , our ultimate goal is to make predictions about new data .
we describe two kinds of prediction : link prediction from words and word prediction in link prediction , we are given a new document ( i . e .
a document which is not in the training set ) and its words .
we are asked to predict its links to the other documents .
this requires computing p ( yd , d123|wd , wd123 ) =
zd , zd123 p ( yd , d123|zd , zd123 ) p ( zd , zd123|wd , wd123 ) ,
an expectation with respect to a posterior that we cannot compute .
using the inference algorithm from section 123 , we nd variational parameters which optimize the elbo for the given evidence , i . e . , the words and links for the training documents and the words in the test document .
replacing the posterior with this approximation q ( , z ) , the predic - tive probability is approximated with
p ( yd , d123|wd , wd123 ) eq ( p ( yd , d123|zd , zd123 ) ) .
in a variant of link prediction , we are given a new set of documents ( documents not in the training set ) along with
123 chang , blei
their words and asked to select the links most likely to exist .
the predictive probability for this task is proportional to the second predictive task is word prediction , where we predict the words of a new document based only on its links .
as with link prediction , p ( wd , i|yd ) cannot be com - puted .
using the same technique , a variational distribution can approximate this posterior .
this yields the predictive
p ( wd , i|yd ) eq ( p ( wd , i|zd , i ) ) .
note that models which treat the endpoints of links as lex - ical tokens cannot participate in the two tasks presented here because they cannot make meaningful predictions for documents that do not appear in the training set ( nallap - ati and cohen 123; cohn and hofmann 123; sinkkonen et al .
by modeling both documents and links gen - eratively , our model is able to give predictive distributions for words given links , links given words , or any mixture
123 empirical results
we examined the rtm on three data sets .
words were stemmed; stop words and infrequently occurring words were removed .
directed links were converted to undirected links123 and documents with no links were removed .
the cora data ( mccallum et al .
123 ) contains abstracts from the cora research paper search engine , with links between documents that cite each other .
the webkb data ( craven et al .
123 ) contains web pages from the computer science departments of different universities , with links determined from the hyperlinks on each page .
the pnas data con - tains recent abstracts from the proceedings of the national academy of sciences .
the links between documents are
evaluating the predictive distribution as with any probabilistic model , the rtm denes a probability distri - bution over unseen data .
after inferring the latent variables from data ( as described in section 123 ) , we ask how well the model predicts the links and words of unseen nodes .
mod - els that give higher probability to the unseen documents better capture the joint structure of words and links .
we study the two variants of the rtm discussed above : lo - gistic rtm uses the logistic link of equation 123; exponential
123the rtm can be extended to accommodate directed connec -
here we modeled undirected links .
123after processing , the cora data contained 123 documents , 123 words , 123 links , and a lexicon of 123 terms .
the we - bkb data contained 123 documents , 123 words , 123 links , and a lexicon of 123 terms .
the pnas data contained 123 doc - uments , 123 words , 123 links , and had a lexicon of 123
rtm uses the exponential link of equation 123
we compare these models against three alternative approaches .
the rst ( baseline ) models words and links independently .
the words are modeled with a multinomial; the links are mod - eled with a bernoulli .
the second ( mixed - membership ) is the model proposed by nallapati et al .
( 123 ) , which is an extension of the mixed membership stochastic block model ( airoldi et al .
123 ) to model network structure and node attributes .
the third ( lda + regression ) rst ts an lda model to the documents and then ts a logistic regres - sion model to the observed links , with input given by the hadamard product of the latent class distributions of each pair of documents .
rather than performing dimensional - ity reduction and regression simultaneously , this method performs unsupervised dimensionality reduction rst , and then regresses to understand the relationship between the latent space and underlying link structure .
all models were trained such that the total mass of the dirichlet hyperpa - rameter was 123 .
( while we omit a full sensitivity study here , we observed that the performance of the models was similar for within a factor of 123 above and below the value we measured the performance of these models on link pre - diction and word prediction ( see section 123 ) .
we divided each data set into ve folds .
for each fold and for each model , we ask two predictive queries : given the words of a new document , what is the likelihood of its links; and given the links of a new document , what is the likelihood of its words ? again , the predictive queries are for com - pletely new test documents that are not observed in train - ing .
during training the test documents are removed along with their attendant links .
we show the results for both tasks in figure 123
in predicting links , the two variants of the rtm perform better than all of the alternative models for all of the data sets ( see figure 123 , top row ) .
cora is paradigmatic , showing a nearly 123% improvement in log likelihood for exponential rtm over baseline and 123% improvement over lda + re - gression .
logistic rtm performs nearly as well on cora with an approximately 123% improvement over baseline and 123% improvement over lda + regression .
we emphasize that the links are predicted to documents seen in the train - ing set from documents which were held out .
by incor - porating link and node information in a joint fashion , the model is able to generalize to new documents for which no link information was previously known .
the performance of the mixed - membership model rarely deviates from the baseline .
despite its increased dimen - sionality ( and commensurate increase in computational dif - culty ) , only on pnas and only when the number of top - ics is large is the mixed - membership model competitive with any of the proposed models .
we hypothesize that the mixed - membership model exhibits this behavior because it uses some topics to explain the words observed in the train -
123 relational topic models for document networks
figure 123 : average held - out predictive link log likelihood ( top ) and word log likelihood ( bottom ) as a function of the number of topics .
for all three corpora , rtms outperform baseline unigram , lda , and mixed - membership , which is the model of nallapati et al .
( 123 ) .
ing set , and other topics to explain the links observed in the training set .
therefore , it cannot use word observations to in predicting words , the two variants of the rtm again out - perform all of the alternative models ( see figure 123 , bottom row ) .
this is because the rtm uses link information to in - uence the predictive distribution of words .
in contrast , the predictions of lda + regression are similar to the base - line .
the predictions of the mixed - membership model are rarely higher than baseline , and often lower .
automatic link suggestion a natural real - world applica - tion of link prediction is to suggest links to a user based on the text of a document .
one might suggest citations for an abstract or friends for a user in a social network .
table 123 illustrates suggested citations using rtm ( e ) and lda + regression as predictive models .
these suggestions were computed from a model trained on one of the folds of the cora data .
the top results illustrate suggested links for markov chain monte carlo convergence diagnostics :
a comparative review , which occurs in this folds training set .
the bottom results illustrate suggested links for com - petitive environments evolve better solutions for complex tasks , which is in the test set .
rtm outperforms lda + regression in being able to iden - tify more true connections .
for the rst document , rtm nds 123 of the connected documents versus 123 for lda + regression .
for the second document , rtm nds 123 while lda + regression does not nd any .
this qualitative be - havior is borne out quantitatively over the entire corpus .
considering the precision of the rst 123 documents re - trieved by the models , rtm improves precision over lda + regression by 123% .
( twenty is a reasonable number of documents for a user to examine . ) while both models found several connections which were not observed in the data , those found by the rtm are qual - itatively different .
in the rst document , both sets of sug - gested links are about markov chain monte carlo .
how - ever , the rtm nds more documents relating specically to convergence and stationary behavior of monte carlo meth -
123 . 123 . 123coralink log likelihoodlllll123number of topicsword log likelihoodlllll123 . 123 . 123webkblllll123number of topicsllllllrtm , , yyssrtm , , yyelda + regression mixedmembershipunigram / bernoulli123 . 123 . 123pnaslllll123number of topicslllll chang , blei
markov chain monte carlo convergence diagnostics : a comparative review
minorization conditions and convergence rates for markov chain monte carlo
rates of convergence of the hastings and metropolis algorithms possible biases induced by mcmc convergence diagnostics
bounding convergence time of the gibbs sampler in bayesian image restoration
self regenerative markov chain monte carlo
auxiliary variable methods for markov chain monte carlo with applications rate of convergence of the gibbs sampler by gaussian approximation
diagnosing convergence of markov chain monte carlo algorithms
exact bound for the convergence of metropolis chains
self regenerative markov chain monte carlo
minorization conditions and convergence rates for markov chain monte carlo
auxiliary variable methods for markov chain monte carlo with applications
markov chain monte carlo model determination for hierarchical and graphical models
mediating instrumental variables
a qualitative framework for probabilistic inference
adaptation for self regenerative mcmc
competitive environments evolve better solutions for complex tasks
coevolving high level representations
a survey of evolutionary strategies
genetic algorithms in search , optimization and machine learning
strongly typed genetic programming in evolving cooperation strategies
solving combinatorial problems using evolutionary algorithms
a promising genetic algorithm approach to job - shop scheduling , rescheduling , and open - shop scheduling problems
an empirical investigation of multi - parent recombination operators in evolution strategies
evolutionary module acquisition
a promising genetic algorithm approach to job - shop scheduling , rescheduling , and open - shop scheduling problems
a new algorithm for dna sequence assembly
identication of protein coding regions in genomic dna
solving combinatorial problems using evolutionary algorithms
a genetic algorithm for passive management
the performance of a genetic algorithm on a chaotic objective function
adaptive global optimization with local search
mutation rates as adaptations
table 123 : top eight link predictions made by rtm ( e ) and lda + regression for two documents ( italicized ) from cora .
the models were trained with 123 topics .
boldfaced titles indicate actual documents cited by or citing each document .
over the whole corpus , rtm improves precision over lda + regression by 123% when evaluated on the rst 123 documents
lda + regression nds connections to documents in the milieu of mcmc , but many are only indirectly related to the input document .
the rtm is able to capture that the notion of convergence is an important predictor for ci - tations , and has adjusted the topic distribution and predic - tors correspondingly .
for the second document , the docu - ments found by the rtm are also of a different nature than those found by lda + regression .
all of the documents suggested by rtm relate to genetic algorithms .
lda + regression , however , suggests some documents which are about genomics .
by relying only on words , lda + re - gression conates two genetic topics which are similar in vocabulary but different in citation structure .
in contrast , the rtm partitions the latent space differently , recognizing that papers about dna sequencing are unlikely to cite pa - pers about genetic algorithms , and vice versa .
it is better able to capture the joint distribution of words and links .
123 related work and discussion
the rtm builds on previous research in statistics and ma - chine learning .
many models have been developed to explain network link structure ( wasserman and pattison 123; newman 123 ) and extensions which incorporate node attributes have been proposed ( getoor et al .
123; taskar et al .
however , these models are not la - tent space approaches and therefore cannot provide the benets of dimensionality reduction and produce the inter - pretable clusters of nodes useful for understanding commu - the rtm , in contrast , is a latent space approach which can provide meaningful clusterings of both nodes and at - tributes .
several latent space models for modeling net - work structure have been proposed ( kemp et al .
123; hoff et al .
123; hofman and wiggins 123; airoldi et al .
123 ) ; though powerful , these models only account for links in the data and cannot model node attributes as well .
123 relational topic models for document networks
because the rtm jointly models node attributes and link structure , it can make predictions about one given the other .
previous work tends to explore one or the other of these two prediction problems .
some previous work uses link struc - ture to make attribute predictions ( chakrabarti et al .
123; kleinberg 123 ) , including several topic models ( dietz et al .
123; mccallum et al .
123; wang et al .
how - ever , none of these methods can make predictions about links given words .
in addition to being able to make predictions about links given words and words given links , the rtm is able to do so for new documentsdocuments outside of training data .
approaches which generate document links through topic models ( nallapati and cohen 123; cohn and hof - mann 123; sinkkonen et al .
123; gruber et al .
123 ) treat links as discrete terms from a separate vocabulary .
this encodes the observed training data into the model , which cannot be generalized to observations outside of it .
link and word predictions for new documents , of the kind we evaluate in section 123 , are ill - dened in these models .
closest to the rtm is recent work by nallapati et al .
( 123 ) and mei et al .
( 123 ) , which attempts to address these issues by extending the mixed - membership stochastic block model ( airoldi et al .
123 ) to include word attributes .
because of their underlying exchangeability assumptions , these models allow for the links to be explained by some topics and the words to be explained by others .
this hin - ders their predictions when using information about words to predict link structure and vice versa .
in contrast , the rtm enforces the constraint that topics be used to explain both words and links .
we showed in section 123 that the rtm outperforms such models on these tasks .
the rtm is a new probabilistic generative model of doc - uments and links between them .
the rtm is used to ana - lyze linked corpora such as citation networks , linked web pages , and social networks with user proles .
we have demonstrated qualitatively and quantitatively that the rtm provides an effective and useful mechanism for analyzing and using such data .
it signicantly improves on previous models , integrating both node - specic information and link structure to give better predictions .
david m .
blei is supported by onr 123 - 123 , nsf ca - reer 123 , and grants from google and microsoft .
