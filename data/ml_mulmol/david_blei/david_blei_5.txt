topic models , such as latent dirichlet allocation ( lda ) , can be useful tools for the statistical analysis of document collections and other dis - crete data .
the lda model assumes that the words of each document arise from a mixture of topics , each of which is a distribution over the vo - cabulary .
a limitation of lda is the inability to model topic correlation even though , for example , a document about genetics is more likely to also be about disease than x - ray astronomy .
this limitation stems from the use of the dirichlet distribution to model the variability among the topic proportions .
in this paper we develop the correlated topic model ( ctm ) , where the topic proportions exhibit correlation via the logistic normal distribution ( 123 ) .
we derive a mean - eld variational inference al - gorithm for approximate posterior inference in this model , which is com - plicated by the fact that the logistic normal is not conjugate to the multi - nomial .
the ctm gives a better t than lda on a collection of ocred articles from the journal science .
furthermore , the ctm provides a nat - ural way of visualizing and exploring this and other unstructured data
the availability and use of unstructured historical collections of documents is rapidly grow - ing .
as one example , jstor ( www . jstor . org ) is a not - for - prot organization that main - tains a large online scholarly journal archive obtained by running an optical character recog - nition engine over the original printed journals .
jstor indexes the resulting text and pro - vides online access to the scanned images of the original content through keyword search .
this provides an extremely useful service to the scholarly community , with the collection comprising nearly three million published articles in a variety of elds .
the sheer size of this unstructured and noisy archive naturally suggests opportunities for the use of statistical modeling .
for instance , a scholar in a narrow subdiscipline , searching for a particular research article , would certainly be interested to learn that the topic of that article is highly correlated with another topic that the researcher may not have known about , and that is not explicitly contained in the article .
alerted to the existence of this new related topic , the researcher could browse the collection in a topic - guided manner to begin to investigate connections to a previously unrecognized body of work .
since the archive comprises millions of articles spanning centuries of scholarly work , automated analysis is
several statistical models have recently been developed for automatically extracting the topical structure of large document collections .
in technical terms , a topic model is a generative probabilistic model that uses a small number of distributions over a vocabulary to describe a document collection .
when t from data , these distributions often correspond to intuitive notions of topicality .
in this work , we build upon the latent dirichlet allocation ( lda ) ( 123 ) model .
lda assumes that the words of each document arise from a mixture of topics .
the topics are shared by all documents in the collection; the topic proportions are document - specic and randomly drawn from a dirichlet distribution .
lda allows each document to exhibit multiple topics with different proportions , and it can thus capture the heterogeneity in grouped data that exhibit multiple latent patterns .
recent work has used lda in more complicated document models ( 123 , 123 , 123 ) , and in a variety of settings such as image processing ( 123 ) , collaborative ltering ( 123 ) , and the modeling of sequential data and user proles ( 123 ) .
similar models were independently developed for disability survey data ( 123 ) and population genetics ( 123 ) .
our goal in this paper is to address a limitation of the topic models proposed to date : they fail to directly model correlation between topics .
in manyindeed mosttext corpora , it is natural to expect that subsets of the underlying latent topics will be highly correlated .
in a corpus of scientic articles , for instance , an article about genetics may be likely to also be about health and disease , but unlikely to also be about x - ray astronomy .
for the lda model , this limitation stems from the independence assumptions implicit in the dirichlet distribution on the topic proportions .
under a dirichlet , the components of the proportions vector are nearly independent; this leads to the strong and unrealistic modeling assumption that the presence of one topic is not correlated with the presence of another .
in this paper we present the correlated topic model ( ctm ) .
the ctm uses an alterna - tive , more exible distribution for the topic proportions that allows for covariance structure among the components .
this gives a more realistic model of latent topic structure where the presence of one latent topic may be correlated with the presence of another .
in the following sections we develop the technical aspects of this model , and then demonstrate its potential for the applications envisioned above .
we t the model to a portion of the jstor archive of the journal science .
we demonstrate that the model gives a better t than lda , as measured by the accuracy of the predictive distributions over held out documents .
fur - thermore , we demonstrate qualitatively that the correlated topic model provides a natural way of visualizing and exploring such an unstructured collection of textual data .
123 the correlated topic model
the key to the correlated topic model we propose is the logistic normal distribution ( 123 ) .
the logistic normal is a distribution on the simplex that allows for a general pattern of variability between the components by transforming a multivariate normal random variable .
consider the natural parameterization of a k - dimensional multinomial distribution :
p ( z | ) = exp ( t z a ( ) ) .
the random variable z can take on k values; it can be represented by a k - vector with exactly one component equal to one , denoting a value in ( 123 , .
the cumulant gener - ating function of the distribution is
a ( ) = log
the mapping between the mean parameterization ( i . e . , the simplex ) and the natural param - eterization is given by
notice that this is not the minimal exponential family representation of the multinomial because multiple values of can yield the same mean parameter .
i = log i / k .
figure 123 : top : graphical model representation of the correlated topic model .
the logistic normal distribution , used to model the latent topic proportions of a document , can represent correlations between topics that are impossible to capture using a single dirichlet .
bottom : example densities of the logistic normal on the 123 - simplex .
from left : diagonal covariance and nonzero - mean , negative correlation between components 123 and 123 , positive correlation between components 123 and 123
the logistic normal distribution assumes that is normally distributed and then mapped to the simplex with the inverse of the mapping given in equation ( 123 ) ; that is , f ( i ) = j exp j .
the logistic normal models correlations between components of the simplicial random variable through the covariance matrix of the normal distribution .
the logistic normal was originally studied in the context of analyzing observed compositional data such as the proportions of minerals in geological samples .
in this work , we extend its use to a hierarchical model where it describes the latent composition of topics associated with each document .
let ( , ) be a k - dimensional mean and covariance matrix , and let topics 123 : k be k multinomials over a xed word vocabulary .
the correlated topic model assumes that an n - word document arises from the following generative process :
draw | ( , ) n ( , ) .
for n ( 123 ,
( a ) draw topic assignment zn | from mult ( f ( ) ) .
( b ) draw word wn | ( zn , 123 : k ) from mult ( zn ) .
this process is identical to the generative process of lda except that the topic proportions are drawn from a logistic normal rather than a dirichlet .
the model is shown as a directed graphical model in figure 123
the ctm is more expressive than lda .
the strong independence assumption imposed by the dirichlet in lda is not realistic when analyzing document collections , where one may nd strong correlations between topics .
the covariance matrix of the logistic normal in the ctm is introduced to model such correlations .
in section 123 , we illustrate how the higher order structure given by the covariance can be used as an exploratory tool for better understanding and navigating a large corpus of documents .
moreover , modeling correlation can lead to better predictive distributions .
in some settings , such as collaborative ltering ,
zd , nwd , nndkdk the goal is to predict unseen items conditional on a set of observations .
an lda model will predict words based on the latent topics that the observations suggest , but the ctm has the ability to predict items associated with additional topics that are correlated with the conditionally probable topics .
123 posterior inference and parameter estimation
posterior inference is the central challenge to using the ctm .
the posterior distribution of the latent variables conditional on a document , p ( , z123 : n | w123 : n ) , is intractable to compute; once conditioned on some observations , the topic assignments z123 : n and log proportions are dependent .
we make use of mean - eld variational methods to efciently obtain an approximation of this posterior distribution .
in brief , the strategy employed by mean - eld variational methods is to form a factorized distribution of the latent variables , parameterized by free variables which are called the vari - ational parameters .
these parameters are t so that the kullback - leibler ( kl ) divergence between the approximate and true posterior is small .
for many problems this optimization problem is computationally manageable , while standard methods , such as markov chain monte carlo , are impractical .
the tradeoff is that variational methods do not come with the same theoretical guarantees as simulation methods .
see ( 123 ) for a modern review of variational methods for statistical inference .
in graphical models composed of conjugate - exponential family pairs and mixtures , the variational inference algorithm can be automatically derived from general principles ( 123 , 123 ) .
in the ctm , however , the logistic normal is not conjugate to the multinomial .
we will therefore derive a variational inference algorithm by taking into account the special structure and distributions used by our model .
we begin by using jensens inequality to bound the log probability of a document :
log p ( w123 : n | , , )
eq ( log p ( | , ) ) +pn
n=123 ( eq ( log p ( zn | ) ) + eq ( log p ( wn | zn , ) ) ) + h ( q ) , where the expectation is taken with respect to a variational distribution of the latent vari - ables , and h ( q ) denotes the entropy of that distribution .
we use a factorized distribution :
q ( 123 : k , z123 : n | 123 : k , 123
i=123 q ( i | i , 123
n=123 q ( zn | n ) .
123 : k , 123 : n ) =qk
the variational distributions of the discrete variables z123 : n are specied by the k - dimensional multinomial parameters 123 : n .
the variational distribution of the continuous variables 123 : k are k independent univariate gaussians ( i , i ) .
since the variational pa - rameters are t using a single observed document w123 : n , there is no advantage in introduc - ing a non - diagonal variational covariance matrix .
the nonconjugacy of the logistic normal leads to difculty in computing the expected log probability of a topic assignment :
eq ( log p ( zn | ) ) = eq
i=123 exp ( i ) ( cid : 123 ) i 123 ( pk
to preserve the lower bound on the log probability , we upper bound the log normalizer with a taylor expansion ,
where we have introduced a new variational parameter .
the expectation eq ( exp ( i ) ) is the mean of a log normal distribution with mean and variance obtained from the variational parameters ( i , 123
i ) ; thus , eq ( exp ( i ) ) = exp ( i + 123
i / 123 ) for i ( 123 ,
i=123 eq ( exp ( i ) ) ) 123 + log ( ) ,
figure 123 : a portion of the topic graph learned from 123 , 123 ocr articles from science .
each node represents a topic , and is labeled with the ve most probable phrases from its distribution ( phrases are found by the turbo topics method ( 123 ) ) .
the interested reader can browse the full model at http : / / www . cs . cmu . edu / lemur / science / .
given a model ( 123 : k , , ) and a document w123 : n , the variational inference algorithm op - timizes equation ( 123 ) with respect to the variational parameters ( 123 : k , 123 : k , 123 : n , ) .
we use coordinate ascent , repeatedly optimizing with respect to each parameter while holding the others xed .
in variational inference for lda , each coordinate can be optimized ana - lytically .
however , iterative methods are required for the ctm when optimizing for i and i .
the details are given in appendix a .
given a collection of documents , we carry out parameter estimation in the correlated topic model by attempting to maximize the likelihood of a corpus of documents as a function of the topics 123 : k and the multivariate gaussian parameters ( , ) .
we use variational expectation - maximization ( em ) , where we maximize the bound on the log probability of a collection given by summing equation ( 123 ) over the documents .
in the e - step , we maximize the bound with respect to the variational parameters by per - forming variational inference for each document .
in the m - step , we maximize the bound with respect to the model parameters .
this is maximum likelihood estimation of the top - ics and multivariate gaussian using expected sufcient statistics , where the expectation is taken with respect to the variational distributions computed in the e - step .
the e - step and m - step are repeated until the bound on the likelihood converges .
in the experiments reported below , we run variational inference until the relative change in the probability bound of equation ( 123 ) is less than 123 , and run variational em until the relative change in the likelihood bound is less than 123
123 examples and empirical results : modeling science
in order to test and illustrate the correlated topic model , we estimated a 123 - topic ctm on 123 , 123 science articles spanning 123 to 123
we constructed a graph of the la - tent topics and the connections among them by examining the most probable words from each topic and the between - topic correlations .
part of this graph is illustrated in fig - ure 123
in this subgraph , there are three densely connected collections of topics : material science , geology , and cell biology .
furthermore , an estimated ctm can be used to ex - plore otherwise unstructured observed documents .
in figure 123 , we list articles that are assigned to the cognitive science topic and articles that are assigned to both the cog -
wild typemutantmutationsmutantsmutationgeneyeastrecombinationphenotypegenesp123cell cycleactivitycyclinregulationproteinphosphorylationkinaseregulatedcell cycle progressionamino acidscdnasequenceisolatedproteinamino acidmrnaamino acid sequenceactinclonegenediseasemutationsfamiliesmutationalzheimers diseasepatientshumanbreast cancernormaldevelopmentembryosdrosophilagenesexpressionembryodevelopmentalembryonicdevelopmental biologyvertebratemantlecrustupper mantlemeteoritesratiosrocksgrainsisotopicisotopic compositiondepthco123carboncarbon dioxidemethanewaterenergygasfuelproductionorganic matterearthquakeearthquakesfaultimagesdataobservationsfeaturesvenussurfacefaultsancientfoundimpactmillion years agoafricasitebonesyears agodaterockclimateoceanicechangesclimate changenorth atlanticrecordwarmingtemperaturepastgeneticpopulationpopulationsdifferencesvariationevolutionlocimtdnadataevolutionarymalesmalefemalesfemalespermsexoffspringeggsspecieseggfossil recordbirdsfossilsdinosaursfossilevolutiontaxaspeciesspecimensevolutionarysynapsesltpglutamatesynapticneuronslong term potentiation ltpsynaptic transmissionpostsynapticnmda receptorshippocampusca123calciumreleaseca123 releaseconcentrationip123intracellular calciumintracellularintracellular ca123ca123 irasatpcampgtpadenylyl cyclasecftradenosine triphosphate atpguanosine triphosphate gtpgapgdpneuronsstimulusmotorvisualcorticalaxonsstimulimovementcortexeyeozoneatmosphericmeasurementsstratosphereconcentrationsatmosphereairaerosolstropospheremeasuredbrainmemorysubjectslefttaskbrainscognitivelanguagehuman brainlearning figure 123 : ( l ) the average held - out probability; ctm supports more topics than lda .
see gure at right for the standard error of the difference .
( r ) the log odds ratio of the held - out probability .
positive numbers indicate a better t by the correlated topic model .
nitive science and visual neuroscience topics .
the interested reader is invited to visit http : / / www . cs . cmu . edu / lemur / science / to interactively explore this model , in - cluding the topics , their connections , and the articles that exhibit them .
we compared the ctm to lda by tting a smaller collection of articles to models of vary - ing numbers of topics .
this collection contains the 123 , 123 documents from 123; we used a vocabulary of 123 , 123 words after pruning common function words and terms that occur once in the collection .
using ten - fold cross validation , we computed the log probability of the held - out data given a model estimated from the remaining data .
a better model of the document collection will assign higher probability to the held out data .
to avoid comparing bounds , we used importance sampling to compute the log probability of a document where the tted variational distribution is the proposal .
figure 123 illustrates the average held out log probability for each model and the average difference between them .
the ctm provides a better t than lda and supports more topics; the likelihood for lda peaks near 123 topics while the likelihood for the ctm peaks close to 123 topics .
the means and standard errors of the difference in log - likelihood of the models is shown at right; this indicates that the ctm always gives a better t .
another quantitative evaluation of the relative strengths of lda and the ctm is how well the models predict the remaining words after observing a portion of the document .
sup - pose we observe words w123 : p from a document and are interested in which model provides a better predictive distribution p ( w | w123 : p ) of the remaining words .
to compare these dis - tributions , we use perplexity , which can be thought of as the effective number of equally likely words according to the model .
mathematically , the perplexity of a word distribu - tion is dened as the inverse of the per - word geometric average of the probability of the
i=p +123 p ( wi | , w123 : p )
d=123 ( ndp ) ,
where denotes the model parameters of an lda or ctm model .
note that lower numbers denote more predictive power .
the plot in figure 123 compares the predictive perplexity under lda and the ctm .
when a
number of topicsheldout log likelihood123llllllllllllllllllllllllllctmldalllllllllllll123number of topicsl ( ctm ) l ( lda ) lllllllllllll figure 123 : ( left ) exploring a collection through its topics .
( right ) predictive perplexity for partially observed held - out documents from the 123 science corpus .
small number of words have been observed , there is less uncertainty about the remaining words under the ctm than under ldathe perplexity is reduced by nearly 123 words , or roughly 123% .
the reason is that after seeing a few words in one topic , the ctm uses topic correlation to infer that words in a related topic may also be probable .
in contrast , lda cannot predict the remaining words as well until a large portion of the document as been observed so that all of its topics are represented .
acknowledgments research supported in part by nsf grants iis - 123 and iis - 123 and by the darpa calo project .
