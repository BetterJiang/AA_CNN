a family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections .
the approach is to use state space models on the natural param - eters of the multinomial distributions that repre - sent the topics .
variational approximations based on kalman lters and nonparametric wavelet re - gression are developed to carry out approximate posterior inference over the latent topics .
in addi - tion to giving quantitative , predictive models of a sequential corpus , dynamic topic models provide a qualitative window into the contents of a large document collection .
the models are demon - strated by analyzing the ocred archives of the journal science from 123 through 123
managing the explosion of electronic document archives requires new tools for automatically organizing , searching , indexing , and browsing large collections .
recent research in machine learning and statistics has developed new tech - niques for nding patterns of words in document collec - tions using hierarchical probabilistic models ( blei et al . , 123; mccallum et al . , 123; rosen - zvi et al . , 123; grif - ths and steyvers , 123; buntine and jakulin , 123; blei and lafferty , 123 ) .
these models are called topic mod - els because the discovered patterns often reect the under - lying topics which combined to form the documents .
such hierarchical probabilistic models are easily generalized to other kinds of data; for example , topic models have been used to analyze images ( fei - fei and perona , 123; sivic et al . , 123 ) , biological data ( pritchard et al . , 123 ) , and survey data ( erosheva , 123 ) .
in an exchangeable topic model , the words of each docu -
appearing in proceedings of the 123 rd international conference on machine learning , pittsburgh , pa , 123
copyright 123 by
ment are assumed to be independently drawn from a mix - ture of multinomials .
the mixing proportions are randomly drawn for each document; the mixture components , or top - ics , are shared by all documents .
thus , each document reects the components with different proportions .
these models are a powerful method of dimensionality reduction for large collections of unstructured documents .
moreover , posterior inference at the document level is useful for infor - mation retrieval , classication , and topic - directed brows -
treating words exchangeably is a simplication that it is consistent with the goal of identifying the semantic themes within each document .
for many collections of interest , however , the implicit assumption of exchangeable doc - uments is inappropriate .
document collections such as scholarly journals , email , news articles , and search query logs all reect evolving content .
for example , the science article the brain of professor laborde may be on the same scientic path as the article reshaping the corti - cal motor map by unmasking latent intracortical connec - tions , but the study of neuroscience looked much different in 123 than it did in 123
the themes in a document col - lection evolve over time , and it is of interest to explicitly model the dynamics of the underlying topics .
in this paper , we develop a dynamic topic model which captures the evolution of topics in a sequentially organized corpus of documents .
we demonstrate its applicability by analyzing over 123 years of ocred articles from the jour - nal science , which was founded in 123 by thomas edi - son and has been published through the present .
under this model , articles are grouped by year , and each years arti - cles arise from a set of topics that have evolved from the last years topics .
in the subsequent sections , we extend classical state space models to specify a statistical model of topic evolution .
we then develop efcient approximate posterior inference techniques for determining the evolving topics from a se - quential collection of documents .
finally , we present qual - itative results that demonstrate how dynamic topic models allow the exploration of a large document collection in new
dynamic topic models
ways , and quantitative results that demonstrate greater pre - dictive accuracy when compared with static topic models .
dynamic topic models
while traditional time series modeling has focused on con - tinuous data , topic models are designed for categorical data .
our approach is to use state space models on the nat - ural parameter space of the underlying topic multinomials , as well as on the natural parameters for the logistic nor - mal distributions used for modeling the document - specic
first , we review the underlying statistical assumptions of a static topic model , such as latent dirichlet allocation ( lda ) ( blei et al . , 123 ) .
let 123 : k be k topics , each of which is a distribution over a xed vocabulary .
in a static topic model , each document is assumed drawn from the following generative process :
choose topic proportions from a distribution over
the ( k 123 ) - simplex , such as a dirichlet .
for each word :
( a ) choose a topic assignment z mult ( ) .
( b ) choose a word w mult ( z ) .
this process implicitly assumes that the documents are drawn exchangeably from the same set of topics .
for many collections , however , the order of the documents reects an evolving set of topics .
in a dynamic topic model , we suppose that the data is divided by time slice , for example by year .
we model the documents of each slice with a k - component topic model , where the topics associated with slice t evolve from the topics associated with slice t 123
for a k - component model with v terms , let t , k denote the v - vector of natural parameters for topic k in slice t .
the usual representation of a multinomial distribution is by its mean parameterization .
if we denote the mean param - eter of a v - dimensional multinomial by , the ith com - ponent of the natural parameter is given by the mapping i = log ( i / v ) .
in typical language modeling applica - tions , dirichlet distributions are used to model uncertainty about the distributions over words .
however , the dirichlet is not amenable to sequential modeling .
instead , we chain the natural parameters of each topic t , k in a state space model that evolves with gaussian noise; the simplest ver - sion of such a model is
t , k | t123 , k n ( t123 , k , 123i ) .
our approach is thus to model sequences of compositional random variables by chaining gaussian distributions in a dynamic model and mapping the emitted values to the sim - plex .
this is an extension of the logistic normal distribu -
figure123graphical representation of a dynamic topic model ( for three time slices ) .
each topics natural parameters t , k evolve over time , together with the mean parameters t of the logistic normal distribution for the topic proportions .
tion ( aitchison , 123 ) to time - series simplex data ( west and harrison , 123 ) .
in lda , the document - specic topic proportions are drawn from a dirichlet distribution .
in the dynamic topic model , we use a logistic normal with mean to express uncertainty over proportions .
the sequential structure be - tween models is again captured with a simple dynamic
t | t123 n ( t123 , 123i ) .
for simplicity , we do not model the dynamics of topic cor - relation , as was done for static models by blei and lafferty
by chaining together topics and topic proportion distribu - tions , we have sequentially tied a collection of topic mod - els .
the generative process for slice t of a sequential corpus is thus as follows :
draw topics t | t123 n ( t123 , 123i ) .
draw t | t123 n ( t123 , 123i ) .
for each document :
( a ) draw n ( t , a123i ) ( b ) for each word :
draw z mult ( ( ) ) .
draw wt , d , n mult ( ( t , z ) ) .
note that maps the multinomial natural parameters to the mean parameters , ( k , t ) w = exp ( k , t , w )
pw exp ( k , t , w ) .
the graphical model for this generative process is shown in figure 123
when the horizontal arrows are removed , break - ing the time dynamics , the graphical model reduces to a set of independent topic models .
with time dynamics , the kth
dynamic topic models
topic at slice t has smoothly evolved from the kth topic at slice t 123
for clarity of presentation , we now focus on a model with k dynamic topics evolving as in ( 123 ) , and where the topic proportion model is xed at a dirichlet .
the technical is - sues associated with modeling the topic proportions in a time series as in ( 123 ) are essentially the same as those for chaining the topics together .
approximate inference
working with time series over the natural parameters en - ables the use of gaussian models for the time dynamics; however , due to the nonconjugacy of the gaussian and multinomial models , posterior inference is intractable .
in this section , we present a variational method for approx - imate posterior inference .
we use variational methods as deterministic alternatives to stochastic simulation , in or - der to handle the large data sets typical of text analysis .
while gibbs sampling has been effectively used for static topic models ( grifths and steyvers , 123 ) , nonconjugacy makes sampling methods more difcult for this dynamic
the idea behind variational methods is to optimize the free parameters of a distribution over the latent variables so that the distribution is close in kullback - liebler ( kl ) diver - gence to the true posterior; this distribution can then be used as a substitute for the true posterior .
in the dynamic topic model , the latent variables are the topics t , k , mixture proportions t , d , and topic indicators zt , d , n .
the variational distribution reects the group structure of the latent vari - ables .
there are variational parameters for each topics se - quence of multinomial parameters , and variational param - eters for each of the document - level latent variables .
the approximate variational posterior is
q ( k , 123 , .
, k , t | k , 123 , .
, k , t )
q ( t , d | t , d ) qnt , d
n=123 q ( zt , d , n | t , d , n ) ! .
in the commonly used mean - eld approximation , each la - tent variable is considered independently of the others .
in the variational distribution of ( k , 123 , .
, k , t ) , however , we retain the sequential structure of the topic by positing a dynamic model with gaussian variational observations ( k , 123 , .
, k , t ) .
these parameters are t to minimize the kl divergence between the resulting posterior , which is gaussian , and the true posterior , which is not gaussian .
( a similar technique for gaussian processes is described in snelson and ghahramani , 123 )
the variational distribution of the document - level latent
figure123a graphical representation of the variational approxima - tion for the time series topic model of figure 123
the variational parameters and are thought of as the outputs of a kalman lter , or as observed data in a nonparametric regression setting .
variables follows the same form as in blei et al .
( 123 ) .
each proportion vector t , d is endowed with a free dirichlet parameter t , d , each topic indicator zt , d , n is endowed with a free multinomial parameter t , d , n , and optimization pro - ceeds by coordinate ascent .
the updates for the document - level variational parameters have a closed form; we use the conjugate gradient method to optimize the topic - level variational observations .
the resulting variational approx - imation for the natural topic parameters ( k , 123 , .
, k , t ) incorporates the time dynamics; we describe one approx - imation based on a kalman lter , and a second based on
variational kalman filtering
the view of the variational parameters as outputs is based on the symmetry properties of the gaussian density , f , ( x ) = fx , ( ) , which enables the use of the standard forward - backward calculations for linear state space mod - els .
the graphical model and its variational approximation are shown in figure 123
here the triangles denote varia - tional parameters; they can be thought of as hypothetical outputs of the kalman lter , to facilitate calculation .
to explain the main idea behind this technique in a sim - pler setting , consider the model where unigram models t ( in the natural parameterization ) evolve over time .
in this model there are no topics and thus no mixing parameters .
the calculations are simpler versions of those we need for the more general latent variable models , but exhibit the es -
dynamic topic models
sential features .
our state space model is
t | t123 n ( t123 , 123i ) wt , n | t mult ( ( t ) )
and we form the variational state space model where
t | t n ( t , 123
the variational parameters are t and t .
using standard kalman lter calculations ( kalman , 123 ) , the forward mean and variance of the variational posterior are given by
mt e ( t | 123 : t ) =
vt123 + 123 + 123
t ( cid : 123 ) mt123 + ( cid : 123 ) 123
vt123 + 123 + 123
vt e ( ( t mt ) 123 | 123 : t )
vt123 + 123 + 123
t ( cid : 123 ) ( vt123 + 123 )
take n = 123j and j = 123
to be consistent with our earlier notation , we assume that
t = emt + t
where t n ( 123 , 123 ) .
our variational wavelet regression algorithm estimates ( t ) , which we view as observed data , just as in the kalman lter method , as well as the noise
for concreteness , we illustrate the technique using the haar wavelet basis; daubechies wavelets are used in our actual examples .
the model is then
t = ( xt ) +
where xt = t / n , ( x ) = 123 for 123 x 123 ,
( x ) = ( cid : 123 ) 123 if 123 x 123
123 < x 123
123 if 123
with initial conditions specied by xed m123 and v123
the backward recursion then calculates the marginal mean and variance of t given 123 : t as
and jk ( x ) = 123j / 123 ( 123jx k ) .
our variational estimate for the posterior mean becomes
emt123 e ( t123 | 123 : t ) = vt123 + 123 ( cid : 123 ) mt123 + ( cid : 123 ) 123 vt123 + 123 ( cid : 123 ) emt evt123 e ( ( t123 emt123 ) 123 | 123 : t ) = vt123 + ( cid : 123 ) vt123 vt123 + 123 ( cid : 123 ) 123 ( cid : 123 ) evt ( vt123 + 123 ) ( cid : 123 ) with initial conditions emt = mt and evt = vt .
we ap -
proximate the posterior p ( 123 : t | w123 : t ) using the state space posterior q ( 123 : t | 123 : t ) .
from jensens inequality , the log - likelihood is bounded from below as
log p ( d123 : t )
z q ( 123 : t | 123 : t ) log p ( 123 : t ) p ( d123 : t | 123 : t )
q ( 123 : t | 123 : t )
= e q log p ( 123 : t ) +
e q log p ( dt | t ) + h ( q )
details of optimizing this bound are given in an appendix .
variational wavelet regression
the variational kalman lter can be replaced with varia - tional wavelet regression; for a readable introduction stan - dard wavelet methods , see wasserman ( 123 ) .
we rescale time so it is between 123 and 123
for 123 years of science we
emt = ( xt ) +
where = n123pn
olding the coefcients
t , and djk are obtained by thresh -
to estimate t we use gradient ascent , as for the kalman
soft thresholding is used , then we have that
lter approximation , requiring the derivatives emt / t
with / s = n123 and
djk / s = ( 123
if |zjk| >
note also that |zjk| > if and only if | djk| > 123
these derivatives can be computed using off - the - shelf software for the wavelet transform in any of the standard wavelet
sample results of running this and the kalman variational algorithm to approximate a unigram model are given in figure 123
both variational approximations smooth out the
dynamic topic models
figure123comparison of the kalman lter ( top ) and wavelet regression ( bottom ) variational approximations to a unigram model .
the variational approximations ( red and blue curves ) smooth out the local uctuations in the unigram counts ( gray curves ) of the words shown , while preserving the sharp peaks that may indicate a signicant change of content in the journal .
the wavelet regression is able to superresolve the double spikes in the occurrence of einstein in the 123s .
( the spike in the occurrence of darwin near 123 may be associated with the centennial of darwins birth in 123 )
local uctuations in the unigram counts , while preserving the sharp peaks that may indicate a signicant change of content in the journal .
while the t is similar to that ob - tained using standard wavelet regression to the ( normal - ized ) counts , the estimates are obtained by minimizing the kl divergence as in standard variational approximations .
in the dynamic topic model of section 123 , the algorithms are essentially the same as those described above .
how - ever , rather than tting the observations from true ob - served counts , we t them from expected counts under the document - level variational distributions in ( 123 ) .
analysis of science
we analyzed a subset of 123 , 123 articles from science , 123 from each of the 123 years between 123 and 123
our data were collected by jstor ( www . jstor . org ) , a not - for - prot organization that maintains an online scholarly archive obtained by running an optical character recogni - tion ( ocr ) engine over the original printed journals .
js - tor indexes the resulting text and provides online access to the scanned images of the original content through key -
our corpus is made up of approximately 123 million words .
we pruned the vocabulary by stemming each term to its root , removing function terms , and removing terms that oc - curred fewer than 123 times .
the total vocabulary size is
123 , 123
to explore the corpus and its themes , we estimated a 123 - component dynamic topic model .
posterior inference took approximately 123 hours on a 123ghz powerpc mac - intosh laptop .
two of the resulting topics are illustrated in figure 123 , showing the top several words from those topics in each decade , according to the posterior mean number of occurrences as estimated using the kalman lter variational approximation .
also shown are example articles which ex - hibit those topics through the decades .
as illustrated , the model captures different scientic themes , and can be used to inspect trends of word usage within them .
to validate the dynamic topic model quantitatively , we con - sider the task of predicting the next year of science given all the articles from the previous years .
we compare the pre - dictive power of three 123 - topic models : the dynamic topic model estimated from all of the previous years , a static topic model estimated from all of the previous years , and a static topic model estimated from the single previous year .
all the models are estimated to the same convergence crite - rion .
the topic model estimated from all the previous data and dynamic topic model are initialized at the same point .
the dynamic topic model performs well; it always assigns higher likelihood to the next years articles than the other two models ( figure 123 ) .
it is interesting that the predictive power of each of the models declines over the years .
we can tentatively attribute this to an increase in the rate of specialization in scientic language .
dynamic topic models
figure123examples from the posterior analysis of a 123 - topic dynamic model estimated from the science corpus .
for two topics , we illustrate : ( a ) the top ten words from the inferred posterior distribution at ten year lags ( b ) the posterior estimate of the frequency as a function of year of several words from the same two topics ( c ) example articles throughout the collection which exhibit these topics .
note that the plots are scaled to give an idea of the shape of the trajectory of the words posterior probability ( i . e . , comparisons across words are not meaningful ) .
we have developed sequential topic models for discrete data by using gaussian time series on the natural param - eters of the multinomial topics and logistic normal topic proportion models .
we derived variational inference algo - rithms that exploit existing techniques for sequential data; we demonstrated a novel use of kalman lters and wavelet regression as variational approximations .
dynamic topic models can give a more accurate predictive model , and also offer new ways of browsing large , unstructured document
there are many ways that the work described here can be extended .
one direction is to use more sophisticated state space models .
we have demonstrated the use of a simple
gaussian model , but it would be natural to include a drift term in a more sophisticated autoregressive model to ex - plicitly capture the rise and fall in popularity of a topic , or in the use of specic terms .
another variant would allow for heteroscedastic time series .
perhaps the most promising extension to the methods pre - sented here is to incorporate a model of how new topics in the collection appear or disappear over time , rather than as - suming a xed number of topics .
one possibility is to use a simple galton - watson or birth - death process for the topic population .
while the analysis of birth - death or branching processes often centers on extinction probabilities , here a goal would be to nd documents that may be responsible for spawning new themes in a collection .
dynamic topic models
figure123
this gure illustrates the performance of using dy - namic topic models and static topic models for prediction .
for each year between 123 and 123 ( at 123 year increments ) , we es - timated three models on the articles through that year .
we then computed the variational bound on the negative log likelihood of next years articles under the resulting model ( lower numbers are better ) .
dtm is the dynamic topic model; lda - prev is a static topic model estimated on just the previous years articles; lda - all is a static topic model estimated on all the previous articles .
this research was supported in part by nsf grants iis - 123 and iis - 123 , the darpa calo project , and a grant from google .
