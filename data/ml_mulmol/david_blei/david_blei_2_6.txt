a key problem in statistical modeling is model selection , how to choose a model at an appropriate level of complexity .
this problem appears in many settings , most prominently in choosing the number of clusters in mixture models or the number of factors in factor analysis .
in this tutorial we describe bayesian nonparametric methods , a class of methods that side - steps this issue by allowing the data to determine the complexity of the model .
this tutorial is a high - level introduction to bayesian nonparametric methods and contains several examples of
how many classes should i use in my mixture model ? how many factors should i use in factor analysis ? these questions regularly exercise scientists as they explore their data .
most scientists address them by rst tting several models , with dierent numbers of clusters or factors , and then selecting one using model comparison metrics ( claeskens and hjort , 123 ) .
model selection metrics usually include two terms .
the rst term measures how well the model ts the data .
the second term , a complexity penalty , favors simpler models ( i . e . , ones with fewer components or factors ) .
bayesian nonparametric ( bnp ) models provide a dierent approach to this problem ( hjort et al . , 123 ) .
rather than comparing models that vary in complexity , the bnp approach is to t a single model that can adapt its complexity to the data .
furthermore , bnp models allow the complexity to grow as more data are observed , such as when using a model to perform prediction .
for example , consider the problem of clustering data .
the traditional mixture modeling approach to clustering requires the number of clusters to be specied in advance of analyzing the data .
the bayesian nonparametric approach estimates how many clusters are needed to model the observed data and allows future data to exhibit previously unseen clusters . 123
using bnp models to analyze data follows the blueprint for bayesian data analysis in gen - eral ( gelman et al . , 123 ) .
each model expresses a generative process of the data that includes
123the origins of these methods are in the distribution of random measures called the dirichlet process ( ferguson , 123; antoniak , 123 ) , which was developed mainly for mathematical interest .
these models were dubbed bayesian nonparametric because they place a prior on the innite - dimensional space of random measures .
with the maturity of markov chain monte carlo sampling methods , nearly twenty years later , dirichlet processes became a practical statistical tool ( escobar and west , 123 ) .
bayesian nonparametric modeling is enjoying a renaissance in statistics and machine learning; we focus here on their application to latent component models , which is one of their central applications .
we describe their formal mathematical foundations in appendix a .
hidden variables .
this process articulates the statistical assumptions that the model makes , and also species the joint probability distribution of the hidden and observed random variables .
given an observed data set , data analysis is performed by posterior inference , computing the conditional distribution of the hidden variables given the observed data .
loosely , posterior inference is akin to reversing the generative process to nd the distribution of the hidden structure that likely gen - erated the observed data .
what distinguishes bayesian nonparametric models from other bayesian models is that the hidden structure is assumed to grow with the data .
its complexity , e . g . , the num - ber of mixture components or the number of factors , is part of the posterior distribution .
rather than needing to be specied in advance , it is determined as part of analyzing the data .
in this tutorial , we survey bayesian nonparametric methods .
we focus on bayesian nonparamet - ric extensions of two common models , mixture models and latent factor models .
as we mentioned above , traditional mixture models group data into a prespecied number of latent clusters .
the bayesian nonparametric mixture model , which is called a chinese restaurant process mixture ( or a dirichlet process mixture ) , infers the number of clusters from the data and allows the number of clusters to grow as new data points are observed .
latent factor models decompose observed data into a linear combination of latent factors .
dierent assumptions about the distribution of factors lead to variants such as factor analysis , principal components analysis , independent components analysis , and others .
as for mixtures , a limitation of latent factor models is that the number of factors must be specied in advance .
the indian buet process latent factor model ( or beta process latent factor model ) infers the number of factors from the data and allows the number of factors to grow as new data points are observed .
we focus on these two types of models because they have served as the basis for a exible suite of bnp models .
models that are built on bnp mixtures or latent factor models include those tailored for sequential data ( beal et al . , 123; paisley and carin , 123; fox et al . , 123 , 123 ) , grouped data ( teh et al . , 123; navarro et al . , 123 ) , data in a tree ( johnson et al . , 123; liang et al . , 123 ) , relational data ( kemp et al . , 123; navarro and griths , 123; miller et al . , 123 ) , and spatial data ( gelfand et al . , 123; duan et al . , 123; sudderth and jordan , 123 ) .
this tutorial is organized as follows .
in sections 123 and 123 we describe mixture and latent factor models in more detail , starting from nite - capacity versions and then extending these to their innite - capacity counterparts .
in section 123 we summarize the standard algorithms for inference in mixture and latent factor models .
finally , in section 123 we describe several limitations and extensions of these models .
in appendix a , we detail some of the mathematical and statistical foundations of bnp models .
we hope to demonstrate how bayesian nonparametric data analysis provides a exible alterna - tive to traditional bayesian ( and non - bayesian ) modeling .
we give examples of bnp analysis of published psychological studies , and we point the reader to available software for performing her
123 mixture models and clustering
in a mixture model , each observed data point is assumed to belong to a cluster .
inference , we infer a grouping or clustering of the data under these assumptionsthis amounts to inferring both the identities of the clusters and the assignments of the data to them .
mixture models are used for understanding the group structure of a data set and for exibly estimating the distribution of a population .
for concreteness , consider the problem of modeling response time ( rt ) distributions .
psychol - ogists believe that several cognitive processes contribute to producing behavioral responses ( luce , 123 ) , and therefore it is a scientically relevant question how to decompose observed rts into their underlying components .
the generative model we describe below expresses one possible process by which latent causes ( e . g . , cognitive processes ) might give rise to observed data ( e . g . , rts ) . 123 using bayes rule , we can invert the generative model to recover a distribution over the possible set of latent causes of our observations .
the inferred latent causes are commonly known as clusters .
123 finite mixture modeling
one approach to this problem is nite mixture modeling .
a nite mixture model assumes that there are k clusters , each associated with a parameter k .
each observation yn is assumed to be generated by rst choosing a cluster cn according to p ( cn ) and then generating the observation from its corresponding observation distribution parameterized by cn .
in the rt modeling problem , each observation is a scalar rt and each cluster species a hypothetical distribution f ( yn|cn ) over the observed rt . 123
finite mixtures can accommodate many kinds of data by changing the data generating distri - bution .
for example , in a gaussian mixture model the dataconditioned on knowing their cluster assignmentsare assumed to be drawn from a gaussian distribution .
the cluster parameters k are the means of the components ( assuming known variances ) .
figure 123 illustrates data drawn from a gaussian mixture with four clusters .
bayesian mixture models further contain a prior over the mixing distribution p ( c ) , and a prior over the cluster parameters : g123
( we denote the prior over cluster parameters g123 to later make a connection to bnp mixture models . ) in a gaussian mixture , for example , it is computationally convenient to choose the cluster parameter prior to be gaussian .
a convenient choice for the distribution on the mixing distribution is a dirichlet .
we will build on fully bayesian mixture modeling when we discuss bayesian nonparametric mixture models .
this generative process denes a joint distribution over the observations , cluster assignments ,
and cluster parameters ,
p ( y , c , ) =
f ( yn|cn ) p ( cn ) ,
where the observations are y = ( y123 , .
, yn ) , the cluster assignments are c = ( c123 , .
, cn ) , and the cluster parameters are = ( 123 , .
the product over n follows from assuming that each observation is conditionally independent given its latent cluster assignment and the cluster
123a number of papers in the psychology literature have adopted a mixture model approach to modeling rts ( e . g . , ratcli and tuerlinckx , 123; wagenmakers et al . , 123 ) .
it is worth noting that the decomposition of rts into constituent cognitive processes performed by the mixture model is fundamentally dierent from the diusion model analysis ( ratcli and rouder , 123 ) , which has become the gold standard in psychology and neuroscience .
in the diusion model , behavioral eects are explained in terms of variations in the underlying parameters of the model , whereas the mixture model attempts to explain these eects in terms of dierent latent causes governing each
123the interpretation of a cluster as a psychological process must be made with caution .
in our example , the hypothesis is that some number of cognitive processes produces the rt data , and the mixture model provides a characterization of the cognitive process under that hypothesis .
further scientic experimentation is required to validate the existence of these processes and their causal relationship to behavior .
figure 123 : draws from a gaussian mixture model .
ellipses show the standard deviation contour for each mixture component .
parameters .
returning to the rt example , the rts are assumed to be independent of each other once we know which cluster generated each rt and the parameters of the latent clusters .
given a data set , we are usually interested in the cluster assignments , i . e . , a grouping of the data . 123 we can use bayes rule to calculate the posterior probability of assignments given the data :
p ( c|y ) =
p ( y|c ) p ( c ) c p ( y|c ) p ( c )
where the likelihood is obtained by marginalizing over settings of :
p ( y|c ) =
a g123 that is conjugate to f allows this integral to be calculated analytically .
for example , the gaussian is the conjugate prior to a gaussian with xed variance , and this is why it is computa - tionally convenient to select g123 to be gaussian in a mixture of gaussians model .
the posterior over assignments is intractable because computing the denominator ( marginal likelihood ) requires summing over every possible partition of the data into k groups .
( this problem becomes more salient in the next section , where we consider the limiting case k . ) we can use approximate methods , such as markov chain monte carlo ( mclachlan and peel , 123 ) or variational inference ( attias , 123 ) ; these methods are discussed further in section 123
123under the dirichlet prior , the assignment vector c = ( 123 , 123 , 123 ) has the same probability as c = ( 123 , 123 , 123 ) .
that is , these vectors are equivalent up to a label switch .
generally we do not care about what particular labels are associated with each class; rather , we care about partitionsequivalence classes of assignment vectors that preserve the same groupings but ignore labels .
123 . 123 . 123 . 123 . 123 figure 123 : the chinese restaurant process .
the generative process of the crp , where numbered diamonds represent customers , attached to their corresponding observations ( shaded circles ) .
the large circles represent tables ( clusters ) in the crp and their associated parameters ( ) .
note that technically the parameter values ( ) are not part of the crp per se , but rather belong to the full
123 the chinese restaurant process
when we analyze data with the nite mixture of equation 123 , we must specify the number of latent clusters ( e . g . , hypothetical cognitive processes ) in advance .
in many data analysis settings , however , we do not know this number and would like to learn it from the data .
bnp clustering addresses this problem by assuming that there is an innite number of latent clusters , but that a nite number of them is used to generate the observed data .
under these assumptions , the posterior provides a distribution over the number of clusters , the assignment of data to clusters , and the parameters associated with each cluster .
furthermore , the predictive distribution , i . e . , the distribution of the next data point , allows for new data to be assigned to a previously unseen cluster .
the bnp approach nesses the problem of choosing the number of clusters by assuming that it is innite , while specifying the prior over innite groupings p ( c ) in such a way that it favors assigning data to a small number of groups .
the prior over groupings is called the chinese restaurant process ( crp; aldous , 123; pitman , 123 ) , a distribution over innite partitions of the integers; this distribution was independently discovered by anderson ( 123 ) in the context of his rational model of categorization ( see section 123 for more discussion of psychological implications ) .
the crp derives its name from the following metaphor .
imagine a restaurant with an innite number of tables , 123 and imagine a sequence of customers entering the restaurant and sitting down .
the rst customer enters and sits at the rst table .
the second customer enters and sits at the rst table with probability 123 123+ , where is a positive real .
when the nth customer enters the restaurant , she sits at each of the occupied tables with probability proportional to the number of previous customers sitting there , and at the next unoccupied table with probability proportional to .
at any point in this process , the assignment of customers to tables denes a random partition .
a schematic of this process is shown in figure 123
123+ , and the second table with probability
more formally , let cn be the table assignment of the nth customer .
a draw from this distribution
123the chinese restaurant metaphor is due to pitman and dubins , who were inspired by the seemingly innite
seating capacity of chinese restaurants in san francisco .
123tablescustomersobservationsparameters123 can be generated by sequentially assigning observations to classes with probability
p ( cn = k|c123 : n123 )
if k k+ ( i . e . , k is a previously occupied table ) otherwise ( i . e . , k is the next unoccupied table )
where mk is the number of customers sitting at table k , and k+ is the number of tables for which mk > 123
the parameter is called the concentration parameter .
intuitively , a larger value of will produce more occupied tables ( and fewer customers per table ) .
the crp exhibits an important invariance property : the cluster assignments under this dis - tribution are exchangeable .
this means that p ( c ) is unchanged if the order of customers is shued ( up to label changes ) .
this may seem counter - intuitive at rst , since the process in equation 123 is
consider the joint distribution of a set of customer assignments c123 : n .
it decomposes according
to the chain rule ,
p ( c123 , c123 , .
, cn ) = p ( c123 ) p ( c123 | c123 ) p ( c123 | c123 , c123 ) p ( cn | c123 , c123 , .
, cn123 ) ,
where each terms comes from equation 123
to show that this distribution is exchangeable , we will introduce some new notation .
let k ( c123 : n ) be the number of groups in which these assignments place the customers , which is a number between 123 and n .
( below , well suppress its dependence on c123 : n . ) let ik be the set of indices of customers assigned to the kth group , and let nk be the number of customers assigned to that group ( i . e . , the cardinality of ik ) .
now , examine the product of terms in equation 123 that correspond to the customers in group
this product is
123 123 ( nk 123 )
( ik , 123 123 + ) ( ik , 123 123 + ) ( ik , n 123 + )
to see this , notice that the rst customer in group k contributes probability starting a new table; the second customer contributes probability
table with one customer at it; the third customer contributes probability numerator of equation 123 can be more succinctly written as ( nk 123 ) !
ik , 123+ because he is ik , 123+ because he is sitting a ik , 123+ , and so on
with this expression , we now rewrite the joint distribution in equation 123 as a product over
p ( c123 : n ) =
( ik , 123 123 + ) ( ik , 123 123 + ) ( ik , nk 123 + )
finally , notice that the union of ik across all groups k identies each index once , because each customer is assigned to exactly one group .
this simplies the denominator and lets us write the
p ( c123 : n ) =
i=123 ( i 123 + )
equation 123 reveals that equation 123 is exchangeable .
it only depends on the number of groups k and the size of each group nk .
the probability of a particular seating conguration c123 : n does not depend on the order in which the customers arrived .
123 chinese restaurant process mixture models
the bnp clustering model uses the crp in an innite - capacity mixture model ( antoniak , 123; anderson , 123; escobar and west , 123; rasmussen , 123 ) .
each table k is associated with a cluster and with a cluster parameter k , drawn from a prior g123
we emphasize that there are an innite number of clusters , though a nite data set only exhibits a nite number of active clusters .
each data point is a customer , who sits at a table cn and then draws its observed value from the distribution f ( yn|cn ) .
the concentration parameter controls the prior expected number of clusters ( i . e . , occupied tables ) k+ .
in particular , this number grows logarithmically with the number of customers n : e ( k+ ) = log n ( for < n / log n ) .
if is treated as unknown , one can put a hyperprior over it and use the same bayesian machinery discussed in section 123 to infer its
returning to the rt example , the crp allows us to place a prior over partitions of rts into the hypothetical cognitive processes that generated them , without committing in advance to a particular number of processes .
as in the nite setting , each process k is associated with a set of parameters k specifying the distribution over rts ( e . g . , the mean of a gaussian for log - transformed rts ) .
figure 123 shows the clustering of rts obtained by approximating the posterior of the crp mixture model using gibbs sampling ( see section 123 ) ; in this gure , the cluster assignments from a single sample are shown .
these data were collected in an experiment on two - alternative forced - choice decision making ( simen et al . , 123 ) .
notice that the model captures the two primary modes of the data , as well as a small number of left - skewed outliers .
by examining the posterior over partitions , we can infer both the assignment of rts to hypothet - ical cognitive processes and the number of hypothetical processes .
in addition , the ( approximate ) posterior provides a measure of condence in any particular clustering , without committing to a single cluster assignment .
notice that the number of clusters can grow as more data are observed .
this is both a natural regime for many scientic applications , and it makes the crp mixture robust to new data that is far away from the original observations .
when we analyze data with a crp , we form an approximation of the joint posterior over all latent variables and parameters .
in practice , there are two uses for this posterior .
one is to examine the likely partitioning of the data .
this gives us a sense of how are data are grouped , and how many groups the crp model chose to use .
the second use is to form predictions with the posterior predictive distribution .
with a crp mixture , the posterior predictive distribution is
p ( yn+123|y123 : n ) =
p ( yn+123|cn+123 , ) p ( cn+123|c123 : n ) p ( c123 : n , |y123 : n ) d .
since the crp prior , p ( cn+123|c123 : n ) , appears in the predictive distribution , the crp mixture allows new data to possibly exhibit a previously unseen cluster .
123 latent factor models and dimensionality reduction
mixture models assume that each observation is assigned to one of k components .
latent factor models weaken this assumption : each observation is inuenced by each of k components in a dierent way ( see comrey and lee , 123 , for an overview ) .
these models have a long history in psychology and psychometrics ( pearson , 123; thurstone , 123 ) , and one of their rst applications was to modeling human intelligence ( spearman , 123 ) .
we will return to this application shortly .
figure 123 : response time modeling with the crp mixture model .
an example distribution of response times from a two - alternative forced - choice decision making experiment ( simen et al . , 123 ) colors denote clusters inferred by 123 iterations of gibbs sampling .
response time ( log sec ) count123cluster123cluster 123cluster 123cluster 123 latent factor models provide dimensionality reduction in the ( usual ) case when the number of components is smaller than the dimension of the data .
each observation is associated with a vector of component activations ( latent factors ) that describes how much each component contributes to it , and this vector can be seen as a lower dimensional representation of the observation itself .
when t to data , the components parsimoniously capture the primary modes of variation in the
the most popular of these modelsfactor analysis ( fa ) , principal component analysis ( pca ) and independent components analysis ( ica ) all assume that the number of factors ( k ) is known .
the bayesian nonparametric variant of latent factor models we describe below allows the number of factors to grow as more data are observed .
as with the bnp mixture model , the posterior distribution provides both the properties of the latent factors and how many are exhibited in the in classical factor analysis , the observed data is a collection of n vectors , y = ( y123 , .
, yn ) , each of which are m - dimensional .
thus , y is a matrix where rows correspond to observations and columns correspond to observed dimensions .
the data ( e . g . , intelligence test scores ) are assumed to be generated by a noisy weighted combination of latent factors ( e . g . , underlying intelligence
yn = gxn + n ,
where g is a m k factor loading matrix expressing how latent factor k inuences observation dimension m , xn is a k - dimensional vector expressing the activity of each latent factor , and n is a vector of independent gaussian noise terms . 123 we can extend this to a sparse model by decomposing the factor loading into the product of two components : gmk = zmkwmk , where zmk is a binary mask variable that indicates whether factor k is on ( zmk = 123 ) or o ( zmk = 123 ) for dimension m , and wmk is a continuous weight variable .
this is sometimes called a spike and slab model ( mitchell and beauchamp , 123; ishwaran and rao , 123 ) because the marginal distribution over xmk is a mixture of a ( typically gaussian ) slab p ( wmk ) over the space of latent factors and a spike at zero , p ( zmk = 123 ) .
we take a bayesian approach to inferring the latent factors , mask variables , and weights .
we place priors over them and use bayes rule to compute the posterior p ( g , z , w|y ) .
in contrast , classical techniques like ica , fa and pca t point estimates of the parameters ( typically maximum
as mentioned above , a classic application of factor analysis in psychology is to the modeling of human intelligence ( spearman , 123 ) .
spearman ( 123 ) argued that there exists a general intelligence factor ( the so - called g - factor ) that can be extracted by applying classical factor analysis methods to intelligence test data .
spearmans hypothesis was motivated by the observation that scores on dierent tests tend to be correlated : participants who score highly on one test are likely to score highly on another .
however , several researchers have disputed the notion that this pattern arises from a unitary intelligence construct , arguing that intelligence consists of a multiplicity of components ( gould , 123 ) .
although we do not aspire to resolve this controversy , the question of
123historically , psychologists have explored a variety of rotation methods for enforcing sparsity and interpretability in fa solutions , starting with early work summarized by thurstone ( 123 ) .
many recent methods are reviewed by browne ( 123 ) .
the bayesian approach we adopt diers from these methods by specifying a preference for certain kinds of solutions in terms of the prior .
123the assumption of gaussian noise in eq .
123 is not fundamental to the latent factor model , but is the most
common choice of noise distribution .
figure 123 : the indian buet process .
the generative process of the ibp , where numbered diamonds represent customers , attached to their corresponding observations ( shaded circles ) .
large circles represent dishes ( factors ) in the ibp , along with their associated parameters ( ) .
each customer selects several dishes , and each customers observation ( in the latent factor model ) is a linear combination of the selected dishs parameters .
note that technically the parameter values ( ) are not part of the ibp per se , but rather belong to the full latent factor model .
how many factors underlie human intelligence is a convenient testbed for the bnp factor analysis model described below .
since in reality the number of latent intelligence factors is unknown , we would like to avoid specifying k and instead allow the data to determine the number of factors .
following the model proposed by knowles and ghahramani ( 123 ) , z is a binary matrix with a nite number of rows ( each corresponding to an intelligence measure ) and an innite number of columns ( each corre - sponding to a latent factor ) .
like the crp , the innite - capacity distribution over z has been furnished with a similarly colorful culinary metaphor , dubbed the indian buet process ( ibp; griths and ghahramani , 123 , 123 ) .
a customer ( dimension ) enters a buet with an innite number of dishes ( factors ) arranged in a line .
the probability that a customer m samples dish k ( i . e . , sets zmk = 123 ) is proportional to its popularity hk ( the number of prior customers who have sampled the dish ) .
when the customer has considered all the previously sampled dishes ( i . e . , those for which hk > 123 ) , she samples an additional poisson ( / n ) dishes that have never been sampled before .
when all m customers have navigated the buet , the resulting binary matrix z ( encoding which customers sampled which dishes ) is a draw from the ibp .
the ibp plays the same role for latent factor models that the crp plays for mixture models : it functions as an innite - capacity prior over the space of latent variables , allowing an unbounded number of latent factors ( knowles and ghahramani , 123 ) .
whereas in the crp , each observation is associated with only one latent component , in the ibp each observation ( or , in the factor analysis model described above , each dimension ) is associated with a theoretically innite number of latent components . 123 a schematic of the ibp is shown in figure 123
comparing to figure 123 , the key
123most of these latent factors will be o because the ibp preserves the sparsity of the nite beta - bernoulli prior ( griths and ghahramani , 123 ) .
the degree of sparsity is controlled by : for larger values , more latent factors
123customersobservationsparametersdishes figure 123 : draws from the crp and ibp .
( left ) random draw from the chinese restaurant in the crp , each customer is assigned to a single component .
in the ibp , a customer can be assigned to multiple components .
( right ) random draw from the indian buet process .
customerstablesdishesa draw from a chinese restaurant processa draw from an indian buffet process figure 123 : ibp factor analysis of human performance on reasoning tasks .
( left ) histogram of the number of latent factors inferred by gibbs sampling applied to reasoning task data from kane et al .
( 123 ) .
123 samples were generated , and the rst 123 were discarded as burn - in .
( right ) relationship between the loading of the rst factor inferred by ibp factor analysis and spearmans g ( i . e . , the loading of the rst factor inferred by classical factor analysis; spearman , 123 ) .
dierence between the crp and the ibp is that in the crp , each customer sits at a single table , whereas in the ibp , a customer can sample several dishes .
this dierence is illustrated in figure 123 , which shows random draws from both models side - by - side .
returning to the intelligence modeling example , posterior inference in the innite latent factor model yields a distribution over matrices of latent factors which describe hypothetical intelligence
p ( x , w , z|y ) p ( y|x , w , z ) p ( x ) p ( w ) p ( z ) .
exact inference is intractable because the normalizing constant requires a sum over all possible binary matrices .
however , we can approximate the posterior using one of the techniques described in the next section ( e . g . , with a set of samples ) .
given posterior samples of z , one typically examines the highest - probability sample ( the maximum a posteriori , or map , estimate ) to get a sense of the latent factor structure .
as with the crp , if one is interested in predicting some function of z , then it is best to average this function over the samples .
figure 123 shows the results of the ibp factor analysis applied to data collected by kane et al .
( 123 ) .
we consider the 123 reasoning tasks administered to 123 participants .
the left panel displays a histogram of the factor counts ( the number of times zmk = 123 across posterior samples ) .
this plot indicates that the dataset is best described by a combination of around 123 123 factors; although this is obviously not a conclusive argument against the existence of a general intelligence factor , it suggests that additional factors merit further investigation .
the right panel displays the rst factor loading from the ibp factor analysis plotted against the g - factor , demonstrating that the nonparametric method is able to extract structure consistent with classical methods . 123
will tend to be active .
123it is worth noting that the eld of intelligence research has developed its methods far beyond spearmans g - factor .
in particular , hierarchical factor analysis is now in common use .
see kane et al .
( 123 ) for an example .
123 . 123 . 123 . 123 . 123classical factor loadingsibp factor loadings123number of latent factorscount figure 123 : inference in a chinese restaurant process mixture model .
the approximate predictive distribution given by variational inference at dierent stages of the algorithm .
the data are 123 points generated by a gaussian dp mixture model with xed diagonal covariance .
figure reproduced with permission from blei and jordan ( 123 ) .
we have described two classes of bnp modelsmixture models based on the crp and latent factor models based on the ibp .
both types of models posit a generative probabilistic process of a collection of observed ( and future ) data that includes hidden structure .
we analyze data with these models by examining the posterior distribution of the hidden structure given the observations; this gives us a distribution over which latent structure likely generated our data .
thus , the basic computational problem in bnp modeling ( as in most of bayesian statistics ) is computing the posterior .
for many interesting modelsincluding those discussed herethe posterior is not available in closed form .
there are several ways to approximate it .
while a comprehensive treatment of inference methods in bnp models is beyond the scope of this tutorial , we will describe some of the most widely - used algorithms .
in appendix b , we provide links to software packages implementing these algorithms .
the most widely used posterior inference methods in bayesian nonparametric models are markov chain monte carlo ( mcmc ) methods .
the idea mcmc methods is to dene a markov chain on the hidden variables that has the posterior as its equilibrium distribution ( andrieu et al . , 123 ) .
by drawing samples from this markov chain , one eventually obtains samples from the poste - rior .
a simple form of mcmc sampling is gibbs sampling , where the markov chain is constructed by considering the conditional distribution of each hidden variable given the others and the ob - servations .
thanks to the exchangeability property described in section 123 , crp mixtures are particularly amenable to gibbs samplingin considering the conditional distributions , each obser - vation can be considered to be the last one and the distribution of equation 123 can be used as one term of the conditional distribution .
( the other term is the likelihood of the observations under each partition . ) neal ( 123 ) provides an excellent survey of gibbs sampling and other mcmc algorithms for inference in crp mixture models ( see also escobar and west , 123; rasmussen , 123; ishwaran and james , 123; jain and neal , 123; fearnhead , 123; wood and griths , 123 ) .
gibbs sampling for the ibp factor analysis model is described in knowles and ghahramani ( 123 ) .
mcmc methods , although guaranteed to converge to the posterior with enough samples , have
two drawbacks : ( 123 ) the samplers must be run for many iterations before convergence and ( 123 ) it is dicult to assess convergence .
an alternative approach to approximating the posterior is variational inference ( jordan et al . , 123 ) .
this approach is based on the idea of approximating the posterior with a simpler family of distributions and searching for the member of that family that is closest to it . 123 although variational methods are not guaranteed to recover the true posterior ( unless it belongs to the simple family of distributions ) , they are typically faster than mcmc and convergence assessment is straightforward .
these methods have been applied to crp mixture models ( blei and jordan , 123; kurihara et al . , 123 , see fig .
123 for an example ) and ibp latent factor models ( doshi - velez et al . , 123; paisley et al . , 123 ) .
we note that variational inference usually operates on a the random measure representation of crp mixtures and ibp factor models , which are described in appendix a .
gibbs samplers that operate on this representation are also available ( ishwaran and james , 123 ) .
as we mentioned in the introduction , bnp methods provide an alternative to model selection over a parameterized family of models . 123 in eect , both mcmc and variational strategies for posterior inference provide a data - directed mechanism for simultaneously searching the space of models and nding optimal parameters .
this is convenient in settings like mixture modeling or factor analysis because we avoid needing to t models for each candidate number of components .
it is essential in more complex settings , where the algorithm searches over a space that is dicult to eciently enumerate and explore .
123 limitations and extensions
we have described the most widely used bnp models , but this is only the tip of the iceberg .
in this section we highlight some key limitations of the models described above , and the extensions that have been developed to address these limitations .
it is worth mentioning here that we cannot do full justice to the variety of bnp models that have been developed over the past 123 years; we have omitted many exciting and widely - used ideas , such as pitman - yor processes , gamma processes , dirichlet diusion trees and kingmans coalescent .
to learn more about these ideas , see the recent volume edited by hjort et al .
( 123 ) .
123 hierarchical structure
the rst limitation concerns grouped data : how can we capture both commonalities and idiosyn - crasies across individuals within a group ? for example , members of an animal species will tend to be similar to each other , but also unique in certain ways .
the standard bayesian approach to this problem is based on hierarchical models ( gelman et al . , 123 ) , in which individuals are coupled by virtue of being drawn from the same group - level distribution . 123 the parameters of this distribution govern both the characteristics of the group and the degree of coupling .
in the nonparametric setting , hierarchical extensions of the dirichlet process ( teh et al . , 123 ) and beta
123distance between probability distributions in this setting is measured by the kullback - leibler divergence ( relative
123the journal of mathematical psychology has published two special issues ( myung et al . , 123; wagenmakers and waldorp , 123 ) on model selection which review a broad array of model selection techniques ( both bayesian and
123see also the recent issue of journal of mathematical psychology ( volume 123 , issue 123 ) devoted to hierarchical
bayesian models .
lee ( 123 ) provides an overview for cognitive psychologists .
process ( thibaux and jordan , 123 ) have been developed , allowing an innite number of latent components to be shared by multiple individuals .
for example , hierarchical dirichlet processes can be applied to modeling text documents , where each document is represented by an innite mixture of word distributions ( topics ) that are shared across documents .
returning to the rt example from section 123 , imagine measuring rts for several subjects .
the goal again is to infer which underlying cognitive process generated each response time .
suppose we assume that the same cognitive processes are shared across subjects , but they may occur in dierent proportions .
this is precisely the kind of structure the hdp can capture .
123 time series models
the second limitation concerns sequential data : how can we capture dependencies between obser - vations arriving in a sequence ? one of the most well - known models for capturing such dependencies is the hidden markov model ( see , e . g . , bishop , 123 ) , in which the latent class for observation n depends on the latent class for observation n 123
the innite hidden markov model ( hmm; beal et al . , 123; teh et al . , 123; paisley and carin , 123 ) posits the same sequential structure , but employs an innite number of latent classes .
teh et al .
( 123 ) showed that the innite hmm is a special case of the hierarchical dirichlet process .
as an alternative to the hmm ( which assumes a discrete latent state ) , a linear dynamical system ( also known as an autoregressive moving average model ) assumes that the latent state is continuous and evolves over time according to a linear - gaussian markov process .
in a switching linear dynamical system , the system can have a number of dynamical modes; this allows the marginal transition distribution to be non - linear .
fox et al .
( 123 ) have explored nonparametric variants of switching linear dynamical systems , where the number of dynamical modes is inferred from the data using an hdp prior .
123 spatial models
another type of dependency arising in many datasets is spatial .
for example , one expects that if a disease occurs in one location , it is also likely to occur in a nearby location .
one way to capture such dependencies in a bnp model is to make the base distribution g123 of the dp dependent on a location variable ( gelfand et al . , 123; duan et al . , 123 ) .
in the eld of computer vision , sudderth and jordan ( 123 ) have applied a spatially - coupled generalization of the dp to the task of image segmentation , allowing them to encode a prior bias that nearby pixels belong to the same segment .
we note in passing a burgeoning area of research attempting to devise more general specications of dependencies in bnp models , particularly for dps ( maceachern , 123; grin and steel , 123; blei and frazier , 123 ) .
these dependencies could be arbitrary functions dened over a set of covariates ( e . g . , age , income , weight ) .
for example , people with similar age and weight will tend to have similar risks for certain diseases .
more recently , several authors have attempted to apply these ideas to the ibp and latent factor
models ( miller et al . , 123; doshi - velez and ghahramani , 123; williamson et al . , 123 ) .
123 supervised learning
we have restricted ourselves to a discussion of unsupervised learning problems , where the goal is to discover hidden structure in data .
in supervised learning , the goal is to predict some output
variable given a set of input variables ( covariates ) .
when the output variable is continuous , this corresponds to regression; when the output variable is discrete , this corresponds to classication .
for many supervised learning problems , the outputs are non - linear functions of the inputs .
the bnp approach to this problem is to place a prior distribution ( known as a gaussian pro - cess ) directly over the space of non - linear functions , rather than specifying a parametric family of non - linear functions and placing priors over their parameters .
supervised learning proceeds by posterior inference over functions using the gaussian process prior .
the output of inference is itself a gaussian process , characterized by a mean function and a covariance function ( analogous to a mean vector and covariance matrix in parametric gaussian models ) .
given a new set of inputs , the posterior gaussian process induces a predictive distribution over outputs .
although we do not discuss this approach further , rasmussen and williams ( 123 ) is an excellent textbook on this
recently , another nonparametric approach to supervised learning has been developed , based on the crp mixture model ( shahbaba and neal , 123; hannah et al . , 123 ) .
the idea is to place a dp mixture prior over the inputs and then model the mean function of the outputs as conditionally linear within each mixture component ( see also rasmussen and ghahramani , 123; meeds and osindero , 123 , for related approaches ) .
the result is a marginally non - linear model of the outputs with linear sub - structure .
intuitively , each mixture component isolates a region of the input space and models the mean output linearly within that region .
this is an example of a generative approach to supervised learning , where the joint distribution over both the inputs and outputs is modeled .
in contrast , the gaussian process approach described above is a discriminative approach , modeling only the conditional distribution of the outputs given the inputs .
bnp models are an emerging set of statistical tools for building exible models whose structure grows and adapts to data .
in this tutorial , we have reviewed the basics of bnp modeling and illustrated their potential in scientic problems .
it is worth noting here that while bnp models address the problem of choosing the number of mixture components or latent factors , they are not a general solution to the model selection problem which has received extensive attention within mathematical psychology and other disciplines ( see claeskens and hjort , 123 , for a comprehensive treatment ) .
in some cases , it may be preferable to place a prior over nite - capacity models and then compare bayes factors ( kass and raftery , 123; vanpaemel , 123 ) , or to use selection criteria motivated by other theoretical frameworks , such as information theory ( grunwald , 123 ) .
123 bayesian nonparametric models of cognition
we have treated bnp models purely as a data analysis tool .
however , there is a ourishing tradition of work in cognitive psychology on using bnp models as theories of cognition .
the earliest example dates back to anderson ( 123 ) , who argued that a version of the crp mixture model could explain human categorization behavior .
the idea in this model is that humans adaptively learn the number of categories from their observations .
a number of recent authors have extended this work ( griths et al . , 123; heller et al . , 123; sanborn et al . , 123 ) and applied it to other domains , such as classical conditioning ( gershman et al . , 123 ) .
the ibp has also been applied to human cognition .
in particular , austerweil and griths ( 123a ) argued that humans decompose visual stimuli into latent features in a manner consistent with the ibp .
when the parts that compose objects strongly covary across objects , humans treat whole objects as features , whereas individual parts are treated as features if the covariance is weak .
this nding is consistent with the idea that the number of inferred features changes exibly with
bnp models have been fruitfully applied in several other domains , including word segmentation ( goldwater et al . , 123 ) , relational theory acquisition ( kemp et al . , 123 ) and function learning ( austerweil and griths , 123b ) .
123 suggestions for further reading
a recent edited volume by hjort et al .
( 123 ) is a useful resource on applied bayesian nonparamet - rics .
for a more general introduction to statistical machine learning with probabilistic models , see bishop ( 123 ) .
for a review of applied bayesian statistics , see gelman et al .
( 123 ) .
we are grateful to andrew conway , katherine heller , irvin hwang , ed vul , james pooley and ken norman who oered comments on an earlier version of the manuscript .
the manuscript was greatly improved by suggestions from the reviewers .
we are also grateful to patrick simen , andrew conway and michael kane for sharing their data and oering helpful suggestions .
this work was supported by a graduate research fellowship from the nsf to sjg .
dmb is supported by onr 123 - 123 , nsf career 123 , afosr 123nl123 , the alfred p .
sloan foundation , and a grant
appendix a : foundations
we have developed bnp methods via the crp and ibp , both of which are priors over combinatorial structures ( innite partitions and innite binary matrices ) .
these are the easiest rst ways to understand this class of models , but their mathematical foundations are found in constructions of in this section , we review this perspective of the crp mixture and ibp
the dirichlet process
the dirichlet process ( dp ) is a distribution over distributions .
it is parameterized by a concentra - tion parameter > 123 and a base distribution g123 , which is a distribution over a space .
a random variable drawn from a dp is itself a distribution over .
a random distribution g drawn from a dp is denoted g dp ( , g123 ) .
its nite dimensional distributions .
consider a measurable partition of , ( t123 , .
, tk ) . 123 g dp ( , g123 ) then every measurable partition of is dirichlet - distributed ,
the dp was rst developed in ferguson ( 123 ) , who showed its existence by appealing to
( g ( t123 ) , .
, g ( tk ) ) dir ( g123 ( t123 ) , .
, g123 ( tk ) ) .
this means that if we draw a random distribution from the dp and add up the probability mass in a region t , then there will on average be g123 ( t ) mass in that region .
the concentration parameter plays the role of an inverse variance; for higher values of , the random probability mass g ( t ) will concentrate more tightly around g123 ( t ) .
ferguson ( 123 ) proved two properties of the dirichlet process .
the rst property is that random distributions drawn from the dirichlet process are discrete .
they place their probability mass on a countably innite collection of points , called atoms ,
( cid : 123 ) ( cid : 123 ) n ( cid : 123 )
in this equation , k is the probability assigned to the kth atom and that atom .
further , these atoms are drawn independently from the base distribution g123
k is the location or value of
the second property connects the dirichlet process to the chinese restaurant process .
consider a random distribution drawn from a dp followed by repeated draws from that random distribution ,
g dp ( , g123 ) i g i ( 123 ,
ferguson ( 123 ) examined the joint distribution of 123 : n , which is obtained by marginalizing out the random distribution g ,
, n | , g123 ) =
p ( i | g )
dp ( g| , g123 ) .
123a partition of denes a collection of subsets whose union is .
a partition is measurable if it is closed under
complementation and countable union .
he showed that , under this joint distribution , the i will exhibit a clustering propertythey will share repeated values with positive probability .
( note that , for example , repeated draws from a gaussian do not exhibit this property . ) the structure of shared values denes a partition of the integers from 123 to n , and the distribution of this partition is a chinese restaurant process with parameter .
finally , he showed that the unique values of i shared among the variables are independent draws from g123
note that this is another way to conrm that the dp assumes exchangeability of 123 : n .
in the foundations of bayesian statistics , de finettis representation theorem ( de finetti , 123 ) says that an exchangeable collection of random variables can be represented as a conditionally independent collection : rst , draw a data generating distribution from a prior over distributions; then draw random variables independently from that data generating distribution .
this reasoning in equa - tion 123 shows that 123 : n are exchangeable .
( for a detailed discussion of de finettis representation theorems , see bernardo and smith ( 123 ) . )
dirichlet process mixtures
a dp mixture adds a third step to the model above antoniak ( 123 ) ,
g dp ( , g123 ) xi p ( | i ) .
marginalizing out g reveals that the dp mixture is equivalent to a crp mixture .
good gibbs sampling algorithms for dp mixtures are based on this representation ( escobar and west , 123;
the stick - breaking construction
ferguson ( 123 ) proved that the dp exists via its nite dimensional distributions .
sethuraman ( 123 ) provided a more constructive denition based on the stick - breaking representation .
consider a stick with unit length .
we divide the stick into an innite number of segments k by the following process .
first , choose a beta random variable 123 beta ( 123 , ) and break of 123 of the stick .
for each remaining segment , choose another beta distributed random variable , and break o that proportion of the remainder of the stick .
this gives us an innite collection of weights k ,
k beta ( 123 , )
( 123 j ) k = 123 , 123 , 123 ,
finally , we construct a random distribution using equation 123 , where we take an innite number of draws from a base distribution g123 and draw the weights as in equation 123
sethuraman ( 123 ) showed that the distribution of this random distribution is a dp ( , g123 ) .
this representation of the dirichlet process , and its corresponding use in a dirichlet process mixture , allows us to compute a variety of functions of posterior dps ( gelfand and kottas , 123 ) and is the basis for the variational approach to approximate inference ( blei and jordan , 123 ) .
figure 123 : stick - breaking construction .
procedure for generating by breaking a stick of length 123 into segments .
inset shows the beta distribution from which k is drawn , for dierent values of
123 = 123 123 = 123 ( 123 - 123 ) 123 = 123 ( 123 - 123 ) ( 123 - 123 ) beta distribution the beta process and bernoulli process
latent factor models admit a similar analysis ( thibaux and jordan , 123 ) .
we dene the random measure b as a set of weighted atoms :
where wk ( 123 , 123 ) and the atoms ( k ) are drawn from a base measure b123 on .
note that in this case ( in contrast to the dp ) , the sum of the weights does not sum to 123 ( almost surely ) , which means that b is not a probability measure .
analogously to the dp , we can dene a distribution on distributions for random measures with weights between 123 and 123namely the beta process , which we denote by b bp ( , b123 ) .
unlike the dp ( which we could dene in terms of dirichlet - distributed marginals ) , the beta process cannot be dened in terms of beta - distributed marginals .
a formal denition requires an excursion into the theory of completely random measures , which would take us beyond the scope of this appendix ( see thibaux and jordan , 123 ) .
to build a latent factor model from the beta process , we dene a new random measure
where znk bernoulli ( wk ) .
the random measure xn is then said to be distributed according to a bernoulli process with base measure b , written as xn bep ( b ) .
a draw from a bernoulli process places unit mass on atoms for which znk = 123; this denes which latent factors are on for the nth observation .
n draws from the bernoulli process yield an ibp - distributed binary matrix z , as shown by thibaux and jordan ( 123 ) .
in the context of factor analysis , the factor loading matrix g is generated from this process by rst drawing the atoms and their weights from the beta process , and then constructing each g by turning on a subset of these atoms according to a draw from the bernoulli process .
finally , observation yn is generated according to eq
stick breaking construction of the beta process
a double - use of the same breakpoints leads to a stick - breaking construction of the beta process ( teh et al . , 123 ) ; see also paisley et al .
( 123 ) .
in this case , the weights correspond to the length of the remaining stick , rather than the length of the segment that was just broken o :
the innite limit of nite models
in this section , we show bnp models can be derived by taking the innite limit of a corresponding nite - capacity model .
for mixture models , we assume that the class assignments z were drawn from a multinomial distribution with parameters = ( 123 , .
, k ) , and place a symmetric dirichlet distribution with concentration parameter on .
the nite mixture model can be summarized
k|g123 g123 , yn|zn , f ( zn ) .
when k , this mixture converges to a dirichlet process mixture model ( neal , 123; rasmussen , 123; ishwaran and zarepour , 123 ) .
to construct a nite latent factor model , we assume that each mask variable is drawn from the
following two - stage generative process :
wk| beta ( / k , 123 )
intuitively , this generative process corresponds to creating a bent coin with bias wk , and then ip - ping it n times to determine whether to activate factors ( z123k , .
, zn k ) .
griths and ghahramani ( 123 ) showed that taking the limit of this model as k yields the ibp latent factor model .
appendix b : software packages
below we present a table of several available software packages implementing the models presented in the main text .
algorithm language author
