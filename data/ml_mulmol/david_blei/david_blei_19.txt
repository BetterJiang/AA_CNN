introduction .
large collections of documents are readily available on - line and widely accessed by diverse communities .
as a notable example , schol - arly articles are increasingly published in electronic form , and historical archives are being scanned and made accessible .
the not - for - prot organization jstor ( www . jstor . org ) is currently one of the leading providers of journals to the schol - arly community .
these archives are created by scanning old journals and running an optical character recognizer over the pages .
jstor provides the original scans on - line , and uses their noisy version of the text to support keyword search .
since the data are largely unstructured and comprise millions of articles spanning cen - turies of scholarly work , automated analysis is essential .
the development of new tools for browsing , searching and allowing the productive use of such archives is thus an important technological challenge , and provides new opportunities for
received march 123; revised april 123
123supported in part by nsf grants iis - 123 and iis - 123 , the darpa calo project and
a grant from google .
supplementary material and code are available at http : / / imstat . org / aoas / supplements key words and phrases .
hierarchical models , approximate posterior inference , variational meth -
ods , text analysis .
blei and j .
lafferty
the statistical analysis of documents has a tradition that goes back at least to the analysis of the federalist papers by mosteller and wallace ( 123 ) .
but document modeling takes on new dimensions with massive multi - author collections such as the large archives that now are being made accessible by jstor , google and other enterprises .
in this paper we consider topic models of such collections , by which we mean latent variable models of documents that exploit the correlations among the words and latent semantic themes .
topic models can extract surprisingly inter - pretable and useful structure without any explicit understanding of the language by computer .
in this paper we present the correlated topic model ( ctm ) , which explicitly models the correlation between the latent topics in the collection , and enables the construction of topic graphs and document browsers that allow a user to navigate the collection in a topic - guided manner .
the main application of this model that we present is an analysis of the js - tor archive for the journal science .
this journal was founded in 123 by thomas edison and continues as one of the most inuential scientic journals today .
the variety of material in the journal , as well as the large number of articles ranging over more than 123 years , demonstrates the need for automated methods , and the potential for statistical topic models to provide an aid for navigating the collection .
the correlated topic model builds on the earlier latent dirichlet allocation ( lda ) model of blei , ng and jordan ( 123 ) , which is an instance of a general family of mixed membership models for decomposing data into multiple latent components .
lda assumes that the words of each document arise from a mixture of topics , where each topic is a multinomial over a xed word vocabulary .
the topics are shared by all documents in the collection , but the topic proportions vary stochasti - cally across documents , as they are randomly drawn from a dirichlet distribution .
recent work has used lda as a building block in more sophisticated topic models , such as author - document models ( 123 , 123 ) , abstract - reference models ( 123 ) syntax - semantics models ( 123 ) and image - caption models ( 123 ) .
the same kind of modeling tools have also been used in a variety of nontext settings , such as image process - ing ( 123 , 123 ) , recommendation systems ( 123 ) , the modeling of user proles ( 123 ) and the modeling of network data ( 123 ) .
similar models were independently developed for disability survey data ( 123 , 123 ) and population genetics ( 123 ) .
in the parlance of the information retrieval literature , lda is a bag of words model .
this means that the words of the documents are assumed to be exchange - able within them , and blei , ng and jordan ( 123 ) motivate lda from this assumption with de finettis exchangeability theorem .
as a consequence , models like lda represent documents as vectors of word counts in a very high dimensional space , ignoring the order in which the words appear .
while it is important to retain the exact sequence of words for reading comprehension , the linguistically simplistic exchangeability assumption is essential to efcient algorithms for automatically eliciting the broad semantic themes in a collection .
the starting point for our analysis here is a perceived limitation of topic models such as lda : they fail to directly model correlation between topics .
in most text
a correlated topic model of science
corpora , it is natural to expect that subsets of the underlying latent topics will be highly correlated .
in science , for instance , an article about genetics may be likely to also be about health and disease , but unlikely to also be about x - ray astronomy .
for the lda model , this limitation stems from the independence assumptions im - plicit in the dirichlet distribution on the topic proportions .
under a dirichlet , the components of the proportions vector are nearly independent , which leads to the strong and unrealistic modeling assumption that the presence of one topic is not correlated with the presence of another .
the ctm replaces the dirichlet by the more exible logistic normal distribution , which incorporates a covariance struc - ture among the components ( 123 ) .
this gives a more realistic model of the latent topic structure where the presence of one latent topic may be correlated with the presence of another .
however , the ability to model correlation between topics sacrices some of the computational conveniences that lda affords .
specically , the conjugacy between the multinomial and dirichlet facilitates straightforward approximate posterior in - ference in lda .
that conjugacy is lost when the dirichlet is replaced with a logis - tic normal .
standard simulation techniques such as gibbs sampling are no longer possible , and metropolishastings based mcmc sampling is prohibitive due to the scale and high dimension of the data .
thus , we develop a fast variational inference procedure for carrying out approx - imate inference with the ctm model .
variational inference ( 123 , 123 ) trades the un - biased estimates of mcmc procedures for potentially biased but computationally efcient algorithms whose numerical convergence is easy to assess .
variational inference algorithms have been effective in lda for analyzing large document collections ( 123 ) .
we extend their use to the ctm .
the rest of this paper is organized as follows .
we rst present the correlated topic model and discuss its underlying modeling assumptions .
then , we present an outline of the variational approach to inference ( the technical details are col - lected in the appendix ) and the variational expectationmaximization procedure for parameter estimation .
finally , we analyze the performance of the model on the jstor science data .
quantitatively , we show that it gives a better t than lda , as measured by the accuracy of the predictive distributions over held out documents .
qualitatively , we present an analysis of all of science from 123 , including examples of topically related articles found using the inferred latent structure , and topic graphs that are constructed from a sparse estimate of the covariance structure of the model .
the paper concludes with a brief discussion of the results and future work that it suggests .
the correlated topic model .
the correlated topic model ( ctm ) is a hi - erarchical model of document collections .
the ctm models the words of each document from a mixture model .
the mixture components are shared by all doc - uments in the collection; the mixture proportions are document - specic random
blei and j .
lafferty
variables .
the ctm allows each document to exhibit multiple topics with differ - ent proportions .
it can thus capture the heterogeneity in grouped data that exhibit multiple latent patterns .
we use the following terminology and notation to describe the data , latent vari - ables and parameters in the ctm .
words and documents .
the only observable random variables that we consider are words that are organized into documents .
let wd , n denote the nth word in the dth document , which is an element in a v - term vocabulary .
let wd denote the vector of nd words associated with document d .
topics .
a topic is a distribution over the vocabulary , a point on the v 123 simplex .
the model will contain k topics 123 : k .
topic assignments .
each word is each assumed drawn from one of the k topics .
the topic assignment zd , n is associated with the nth word and dth document .
topic proportions .
finally , each document is associated with a set of topic pro - portions d , which is a point on the k 123 simplex .
thus , d is a distribution over topic indices , and reects the probabilities with which words are drawn from each topic in the collection .
we will typically consider a natural parame - terization of this multinomial = log ( i / k ) .
specically , the correlated topic model assumes that an n - word document arises from the following generative process .
given topics 123 : k , a k - vector and a k k covariance matrix : | ( , ) n ( , ) .
draw d 123
for n ( 123 , .
, nd ) : ( a ) draw topic assignment zd , n|d from mult ( f ( d ) ) .
( b ) draw word wd , n| ( zd , n , 123 : k
) from mult ( zd , n
where f ( ) maps a natural parameterization of the topic proportions to the mean
= f ( ) = exp ( ) i exp ( i ) .
( note that does not index a minimal exponential family .
adding any constant to will result in an identical mean parameter . ) this process is illustrated as a probabilistic graphical model in figure 123
( a probabilistic graphical model is a graph representation of a family of joint distributions with a graph .
nodes denote random variables; edges denote possible dependencies between them . )
the ctm builds on the latent dirichlet allocation ( lda ) model ( 123 ) .
latent dirichlet allocation assumes a nearly identical generative process , but one where the topic proportions are drawn from a dirichlet .
in lda and its variants , the dirichlet is a computationally convenient distribution over topic proportions be - cause it is conjugate to the topic assignments .
but , the dirichlet assumes near
a correlated topic model of science
top : probabilistic graphical model representation of the correlated topic model .
the lo - gistic normal distribution , used to model the latent topic proportions of a document , can represent correlations between topics that are impossible to capture using a dirichlet .
bottom : example den - sities of the logistic normal on the 123 - simplex .
from left : diagonal covariance and nonzero - mean , negative correlation between topics 123 and 123 , positive correlation between topics 123 and 123
independence of the components of the proportions .
in fact , one can simulate a draw from a dirichlet by drawing from k independent gamma distributions and normalizing the resulting vector .
( note that there is slight negative correlation due to the constraint that the components sum to one . )
rather than use a dirichlet , the ctm draws a real valued random vector from a multivariate gaussian and then maps it to the simplex to obtain a multinomial para - meter .
this is the dening characteristic of the logistic normal distribution ( 123 ) .
the covariance of the gaussian induces dependencies between the components of the transformed random simplicial vector , allowing for a general pattern of vari - ability between its components .
the logistic normal was originally studied in the context of analyzing observed compositional data , such as the proportions of min - erals in geological samples .
in the ctm , we use it to model the latent composition of topics associated with each document .
the drawback of using the logistic normal is that it is not conjugate to the multinomial , which complicates the corresponding approximate posterior infer - ence procedure .
the advantage , however , is that it provides a more expressive document model .
the strong independence assumption imposed by the dirichlet is not realistic when analyzing real document collections , where one nds strong correlations between the latent topics .
for example , a document about geology is
blei and j .
lafferty
more likely to also be about archeology than genetics .
we aim to use the covari - ance matrix of the logistic normal to capture such relationships .
in section 123 we illustrate how the higher order structure given by the covari - ance can be used as an exploratory tool for better understanding and navigating a large corpus of documents .
moreover , modeling correlation can lead to better predictive distributions .
in some applications , such as automatic recommendation systems , the goal is to predict unseen items conditioned on a set of observations .
a dirichlet - based model will predict items based on the latent topics that the obser - vations suggest , but the ctm will predict items associated with additional topics that are correlated with the conditionally probable topics .
computation with the correlated topic model .
we address two com - putational problems that arise when using the correlated topic model to analyze data .
first , given a collection of topics and distribution over topic proportions
( 123 : k , , ) , we estimate the posterior distribution of the latent variables con - ditioned on the words of a document p ( , z|w , 123 : k , , ) .
this lets us embed
newly observed documents into the low dimensional latent thematic space that the model represents .
we use a fast variational inference algorithm to approximate this posterior , which lets us quickly analyze large document collections under these complicated modeling assumptions .
second , given a collection of documents ( w123 , .
, wd ) , we nd maximum like - lihood estimates of the topics and the underlying logistic normal distribution un - der the modeling assumptions of the ctm .
we use a variant of the expectation - maximization algorithm , where the e - step is the per - document posterior inference problem described above .
furthermore , we seek sparse solutions of the inverse covariance matrix between topics , and we adapt ( cid : 123 ) 123 - regularized covariance estima - tion ( 123 ) for this purpose .
posterior inference with variational methods .
given a document w and a
model ( 123 : k , , ) , the posterior distribution of the per - document latent variables
p ( , z|w , 123 : k , , )
n=123 p ( zn| ) p ( wn|zn , 123 : k ) zn=123 p ( zn| ) p ( wn|zn , 123 : k ) d
which is intractable to compute due to the integral in the denominator , that is , the marginal probability of the document that we are conditioning on .
there are two reasons for this intractability .
first , the sum over the k values of zn occurs inside the product over words , inducing a combinatorial number of terms .
second , even if k n stays within the realm of computational tractability , the distribution of topic proportions p ( | , ) is not conjugate to the distribution of topic assignments p ( zn| ) .
thus , we cannot analytically compute the integrals of each term .
a correlated topic model of science
the nonconjugacy further precludes using many of the monte carlo markov chain ( mcmc ) sampling techniques that have been developed for computing with dirichlet - based mixed membership models ( 123 , 123 ) .
these mcmc methods are all based on gibbs sampling , where the conjugacy between the latent variables lets us compute coordinate - wise posteriors analytically .
to employ mcmc in the logistic normal setting considered here , we have to appeal to a tailored metropolis hastings solution .
such a technique will not enjoy the same convergence properties and speed of the gibbs samplers , which is particularly hindering for the goal of analyzing collections that comprise millions of words .
thus , to approximate this posterior , we appeal to variational methods as a deter - ministic alternative to mcmc .
the idea behind variational methods is to optimize the free parameters of a distribution over the latent variables so that the distrib - ution is close in kullbackleibler divergence to the true posterior ( 123 , 123 ) .
the tted variational distribution is then used as a substitute for the posterior , just as the empirical distribution of samples is used in mcmc .
variational methods have had widespread application in machine learning; their potential in applied bayesian statistics is beginning to be realized .
in models composed of conjugate - exponential family pairs and mixtures , the variational inference algorithm can be automatically derived by computing expec - tations of natural parameters in the variational distribution ( 123 , 123 , 123 ) .
however , the nonconjugate pair of variables in the ctm requires that we derive the variational inference algorithm from rst principles .
we begin by using jensens inequality to bound the log probability of a docu -
log p ( w123 : n| , , )
eq ( log p ( | , ) ) + n ( cid : 123 )
eq ( log p ( wn|zn , ) ) + h ( q ) ,
where the expectation is taken with respect to q , a variational distribution of the latent variables , and h ( q ) denotes the entropy of that distribution .
as a variational distribution , we use a fully factorized model , where all the variables are indepen - dently governed by a different distribution ,
q ( 123 : k , z123 : n|123 : k , 123
q ( i|i , 123
123 : k , 123 : n ) = k ( cid : 123 )
the variational distributions of the discrete topic assignments z123 : n are specied by the k - dimensional multinomial parameters 123 : n ( these are mean parameters of the multinomial ) .
the variational distribution of the continuous variables 123 : k
blei and j .
lafferty
are k independent univariate gaussians ( i , i ) .
since the variational parameters are t using a single observed document w123 : n , there is no advantage in introducing a nondiagonal variational covariance matrix .
the variational inference algorithm optimizes equation ( 123 ) with respect to the variational parameters , thereby tightening the bound on the marginal probability of the observations as much as the structure of variational distribution allows .
this is equivalent to nding the variational distribution that minimizes kl ( q||p ) , where p is the true posterior ( 123 , 123 ) .
details of this optimization for the ctm are given in the appendix .
note that variational methods do not come with the same theoretical guarantees as mcmc , where the limiting distribution of the chain is exactly the posterior of interest .
however , variational methods provide fast algorithms and a clear conver - gence criterion , whereas mcmc methods can be computationally inefcient and determining when a markov chain has converged is difcult ( 123 ) .
parameter estimation .
given a collection of documents , we carry out pa - rameter estimation for the correlated topic model by attempting to maximize the likelihood of a corpus of documents as a function of the topics 123 : k and the mul - tivariate gaussian ( , ) .
as in many latent variable models , we cannot compute the marginal likelihood of the data because of the latent structure that needs to be marginalized out .
to ad - dress this issue , we use variational expectationmaximization ( em ) .
in the e - step of traditional em , one computes the posterior distribution of the latent variables given the data and current model parameters .
in variational em , we use the varia - tional approximation to the posterior described in the previous section .
note that this is akin to monte carlo em , where the e - step is approximated by a monte carlo approximation to the posterior ( 123 ) .
given by summing equation ( 123 ) over the document collection ( w123 , .
, wd ) , ( log p ( d , zd , wd| , , 123 : k ) ) + h ( qd ) .
specically , the objective function of variational em is the likelihood bound
; w123 : d ) d ( cid : 123 )
l ( , , 123 : k
the variational em algorithm is coordinate ascent in this objective function .
in the e - step , we maximize the bound with respect to the variational parameters by performing variational inference for each document .
in the m - step , we maximize the bound with respect to the model parameters .
this amounts to maximum likeli - hood estimation of the topics and multivariate gaussian using expected sufcient statistics , where the expectation is taken with respect to the variational distribu - tions computed in the e - step ,
a correlated topic model of science
( cid : 123 ) = 123 ( cid : 123 ) = 123
+ ( d ( cid : 123 ) ) ( d ( cid : 123 ) ) t ,
where nd is the vector of word counts for document d .
the e - step and m - step are repeated until the bound on the likelihood con - verges .
in the analysis reported below , we run variational inference until the rel - ative change in the probability bound of equation ( 123 ) is less than 123% , and run variational em until the relative change in the likelihood bound is less than
topic graphs .
as seen below , the ability of the ctm to model the corre - lation between topics yields a better t of a document collection than lda .
but the covariance of the logistic normal model for topic proportions can also be used to visualize the relationships among the topics .
in particular , the covariance matrix can be used to form a topic graph , where the nodes represent individual topics , and neighboring nodes represent highly related topics .
in such settings , it is useful to have a mechanism to control the sparsity of the graph .
recall that the graph encoding the independence relations in a gaussian graph - ical model is specied by the zero pattern in the inverse covariance matrix .
more precisely , if x n ( , ) is a k - dimensional multivariate gaussian , and s = denotes the inverse covariance matrix , then we form a graph g ( ) = ( v , e ) with vertices v corresponding to the random variables x123 , .
, xk and edges e satis - fying ( s , t ) e if and only if sst ( cid : 123 ) = 123
if n ( s ) = ( t : ( s , t ) e ) denotes the set of neighbors of s in the graph , then the independence relation xs xu|xn ( s ) holds for any node u / n ( s ) that is not a neighbor of s .
recent work of meinshausen and bhlmann ( 123 ) shows how the lasso ( 123 ) can be adapted to give an asymptotically consistent estimator of the graph g ( ) .
the strategy is to regress each variable xs onto all of the other variables , imposing an ( cid : 123 ) 123 penalty on the parameters to encourage sparsity .
the nonzero components then serve as an estimate of the neighbors of s in the graph .
in more detail , let s = ( s123 , .
, sk ) rk be the parameters of the lasso t obtained by regressing xs onto ( xt ) t ( cid : 123 ) =s , with the parameter ss serving as the unregularized intercept .
the optimization problem is
( cid : 123 ) xs x\s s ( cid : 123 ) 123
where x\s denotes the set of variables with xs replaced by the vector of all 123s , and \s denotes the vector s with component ss removed .
the estimated set of neighbors is then
( cid : 123 ) s = arg min
( cid : 123 ) n ( s ) = ( t : ( cid : 123 ) st ( cid : 123 ) = 123 ) .
blei and j .
lafferty
( cid : 123 ) n ( s ) = n ( s ) ) 123 as the sample meinshausen and bhlmann ( 123 ) show that p ( size n increases , for a suitable choice of the regularization parameter n satisfying log ( k ) .
moreover , the convergence is exponentially fast , and as a con - sequence , if k = o ( nd ) grows only polynomially with sample size , the estimated graph is the true graph with probability approaching one .
to adapt the meinshausenbhlmann technique to the ctm , recall that we es - timate the covariance matrix using variational em , where in the m - step we maximize the variational lower bound with respect to approximation computed in the e - step .
for a given document d , the variational approximation to the posterior of is a normal with mean d rk .
we treat the standardized mean vectors ( d ) as data , and regress each component onto the others with an ( cid : 123 ) 123 penalty .
two simple procedures can be used to then form the graph edge set , by taking the conjunction or disjunction of the local neighborhood estimates :
( s , t ) eand ( s , t ) eor
in case t ( cid : 123 ) n ( s ) and s ( cid : 123 ) n ( t ) , in case t ( cid : 123 ) n ( s ) or s ( cid : 123 ) n ( t ) .
figure 123 shows an example of a topic graph constructed using this method , with edges eand formed by intersecting the neighborhood estimates .
varying the regularization parameter n allows control over the sparsity of the graph; the graph becomes increasingly sparse as n increases .
analyzing science .
jstor is an on - line archive of scholarly journals that scans bound volumes dating back to the 123s and runs optical character recogni - tion algorithms on the scans .
thus , jstor stores and indexes hundreds of millions of pages of noisy text , all searchable through the internet .
this is an invaluable re - source to scholars .
the jstor collection provides an opportunity for developing exploratory analysis and useful descriptive statistics of large volumes of text data .
as they are , the articles are organized by journal , volume and number .
but the users of jstor would benet from a topical organization of articles from different journals and automatic recommendations of similar articles to those known to be of interest .
in some modern electronic scholarly archives , such as the arxiv ( http : / / www .
arxiv . org / ) , contributors provide meta - data with their manuscripts that describe and categorize their work to aid in such a topical exploration of the collection .
in many text data sets , however , meta - data is unavailable .
moreover , there may be under - lying topics and connections between articles that the authors or curators have not determined .
to these ends , we analyzed a large portion of jstors corpus of arti - cles from science with the ctm .
qualitative analysis of science .
in this section we illustrate the possible applications of the ctm to automatic corpus analysis and browsing .
we estimated a 123 - topic model on the science articles from 123 to 123 using the variational
a portion of the topic graph learned from 123 , 123 ocr articles from science ( 123 ) .
each topic node is labeled with its ve most probable phrases and has font proportional to its popularity in the corpus .
( phrases are found by permutation test . ) the full model can be found in http : / / www . cs . cmu . edu / ~ lemur / science / and on statlib .
blei and j .
lafferty
em algorithm of section 123 .
( c code that implements this algorithm can be found at the rst authors web - site and statlib . ) the total vocabulary size in this col - lection is 123 , 123 terms .
we trim the 123 , 123 terms that occurred fewer than 123 times as well as 123 stop words , that is , words like the , but or with , which do not convey meaning .
this yields a corpus of 123 , 123 documents , 123 , 123 unique terms and a total of 123m words .
using the technique described in section 123 , we constructed a sparse graph ( = 123 ) of the connections between the estimated latent topics .
part of this graph is illustrated in figure 123
( for space , we manually removed topics that occurred very rarely and those that captured nontopical content such as front matter . ) this graph provides a snapshot of ten years of science , and reveals different substruc - tures of themes in the collection .
a user interested in the brain can restrict attention to articles that use the neuroscience topics; a user interested in genetics can restrict attention to those articles in the cluster of genetics topics .
further structure is revealed at the document level , where each document is as - sociated with a latent vector of topic proportions .
the posterior distribution of the proportions can be used to associate documents with latent topics .
for example , the following are the top ve articles associated with the topic whose most probable vocabulary items are laser , optical , light , electron , quantum :
vacuum squeezing of solids : macroscopic quantum states driven by
light pulses ( 123 ) .
superradiant rayleigh scattering from a boseeinstein condensate
physics and device applications of optical microcavities ( 123 ) .
photon number squeezed states in semiconductor lasers ( 123 ) .
a well - collimated quasi - continuous atom laser ( 123 ) .
moreover , we can use the expected distance between per - document topic pro - portions to identify other documents that have similar topical content .
we use the expected hellinger distance , which is a symmetric distance between distributions .
consider two documents i and j , e ( d ( i , j ) ) = eq
= 123 123
where all expectations are taken with respect to the variational posterior distri - butions ( see section 123 ) .
one example of this application of the latent variable analysis is illustrated in figure 123
the interested reader is invited to visit http : / / www . cs . cmu . edu / ~ lemur / science / to interactively explore this model , including the topics , their connections , the ar - ticles that exhibit them and the expected hellinger similarity between articles .
a correlated topic model of science
using the hellinger distance to nd similar articles to the query article earths solid iron core may skew its magnetic field .
illustrated are the top three articles by hellinger distance to the query article and the expected posterior topic proportions for each article .
notice that each document somehow combines geology and physics .
quantitative comparison to latent dirichlet allocation .
we compared the logistic normal to the dirichlet by tting a smaller collection of articles to ctm and lda models of varying numbers of topics .
this collection contains the 123 , 123 documents from 123; we used a vocabulary of 123 , 123 words after pruning common function words and terms that occur once in the collection .
using ten - fold cross validation , we computed the log probability of the held - out data given a model estimated from the remaining data .
a better model of the document collection will assign higher probability to the held out data .
to avoid comparing bounds , we used
blei and j .
lafferty
( left ) the 123 - fold cross - validated held - out log probability of the 123 science corpus , computed by importance sampling .
the ctm supports more topics than lda .
see gure at right for the standard error of the difference .
( right ) the mean difference in held - out log probability .
numbers greater than zero indicate a better t by the ctm .
importance sampling to compute the log probability of a document where the tted variational distribution is the proposal .
figure 123 illustrates the average held out log probability for each model and the average difference between them .
the ctm provides a better t than lda and supports more topics; the likelihood for lda peaks near 123 topics , while the likelihood for the ctm peaks close to 123 topics .
the means and standard errors of the difference in log - likelihood of the models is shown at right; this indicates that the ctm always gives a better t .
another quantitative evaluation of the relative strengths of lda and the ctm is how well the models predict the remaining words of a document after observing a portion of it .
specically , we observe p words from a document and are in - terested in which model provides a better predictive distribution of the remaining words p ( w|w123 : p ) .
to compare these distributions , we use perplexity , which can be thought of as the effective number of equally likely words according to the model .
mathematically , the perplexity of a word distribution is dened as the inverse of the per - word geometric average of the probability of the observations ,
p ( wi| , w123 : p )
where denotes the model parameters of an lda or ctm model .
note that lower numbers denote more predictive power .
the plot in figure 123 compares the predictive perplexity under lda and the ctm for different numbers of words randomly observed from the documents .
a correlated topic model of science
( left ) the 123 - fold cross - validated predictive perplexity for partially observed held - out doc - uments from the 123 science corpus ( k = 123 ) .
lower numbers indicate more predictive power from the ctm .
( right ) the mean difference in predictive perplexity .
numbers less than zero indicate better prediction from the ctm .
when a small number of words have been observed , there is less uncertainty about the remaining words under the ctm than under ldathe perplexity is reduced by nearly 123 words , or roughly 123% .
the reason is that after seeing a few words in one topic , the ctm uses topic correlation to infer that words in a related topic may also be probable .
in contrast , lda cannot predict the remaining words as well until a large portion of the document has been observed so that all of its topics are
summary .
we have developed a hierarchical topic model of documents that replaces the dirichlet distribution of per - document topic proportions with a logistic normal .
this allows the model to capture correlations between the occur - rence of latent topics .
the resulting correlated topic model gives better predictive performance and uncovers interesting descriptive statistics for facilitating brows - ing and search .
use of the logistic normal , while more complex , may have benet in the many applications of dirichlet - based mixed membership models .
one issue that we did not thoroughly explore is model selection , that is , choos - ing the number of topics for a collection .
in other topic models , nonparamet - ric bayesian methods based on the dirichlet process are a natural suite of tools because they can accommodate new topics as more documents are observed .
( the nonparametric bayesian version of lda is exactly the hierarchical dirich - let process ( 123 ) . ) the logistic normal , however , does not immediately give way to such extensions .
tackling the model selection issue in this setting is an important area of future research .
blei and j .
lafferty
appendix : details of variational inference
variational objective .
before deriving the optimization procedure , we put the objective function equation ( 123 ) in terms of the variational parameters .
the rst
eq ( log p ( | , ) )
log 123 123
123 ) + ( ) t
the nonconjugacy of the logistic normal to multinomial leads to difculty in computing the second term of equation ( 123 ) , the expected log probability of a topic
eq ( log p ( zn| ) ) = eq ( t zn ) eq
to preserve the lower bound on the log probability , we upper bound the negative log normalizer with a taylor expansion :
123 + log ( ) ,
using this additional bound , the second term of equation ( 123 ) is
eq ( log p ( zn| ) ) = k ( cid : 123 )
where we have introduced a new variational parameter .
the expectation eq ( exp ( i ) ) is the mean of a log normal distribution with mean and variance ob - i / 123 ) for ) : eq ( exp ( i ) ) = exp ( i + 123 tained from the variational parameters ( i , 123 i ( 123 , .
this is a simpler approach than the more exible , but more com - putationally intensive , method taken in ( 123 ) .
eq ( log p ( wn|zn , ) ) = k ( cid : 123 ) + log 123 + 123 ) n ( cid : 123 )
the fourth term is the entropy of the variational distribution :
the third term of equation ( 123 ) is
exp ( i + 123
+ 123 log .
n , i log i , wn .
n , i log n , i .
123 ( log 123
a correlated topic model of science
note that the additional variational parameter is not needed to compute this
coordinate ascent optimization .
finally , we maximize the bound in equa - tion ( 123 ) with respect to the variational parameters 123 : k , 123 : k , 123 : n and .
we use a coordinate ascent algorithm , iteratively maximizing the bound with respect to each parameter .
first , we maximize equation ( 123 ) with respect to , using the second bound in
equation ( 123 ) .
the derivative with respect to is exp ( i + 123
( ) = n
which has a maximum at
exp ( i + 123
second , we maximize with respect to n .
this yields a maximum at
i ( 123 ,
n , i exp ( i ) i , wn ,
which is an application of variational inference updates within the exponential family ( 123 , 123 , 123 ) .
third , we maximize with respect to i .
equation ( 123 ) is not amenable to analytic
123 ( ) + n ( cid : 123 )
maximization .
we use the conjugate gradient algorithm with derivative n , 123 : k ( n / ) exp ( + 123 / 123 ) .
finally , we maximize with respect to 123 use newtons method for each coordinate with the constraint that i > 123 ,
ii / 123 n / 123 exp ( i + 123
i / 123 ) + 123 / ( 123
again , there is no analytic solution
iterating between the optimizations of , , and denes a coordinate ascent algorithm on equation ( 123 ) .
( in practice , we optimize with respect to in between optimizations for , and . ) though each coordinates optimization is convex , the variational objective is not convex with respect to the ensemble of variational parameters .
we are only guaranteed to nd a local maximum , but note that this is still a bound on the log probability of a document .
acknowledgments .
we thank two anonymous reviewers for their excellent suggestions for improving the paper .
we thank jstor for providing access to journal material in the jstor archive .
we thank jon mcauliffe and nathan srebro for useful discussions and comments .
a preliminary version of this work appears
blei and j .
lafferty
