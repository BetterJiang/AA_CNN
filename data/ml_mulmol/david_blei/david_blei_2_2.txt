recent research in machine learning has focused on breaking audio spectrograms into separate sources of sound using latent variable decom - positions .
these methods require that the num - ber of sources be specied in advance , which is not always possible .
to address this problem , we develop gamma process nonnegative matrix factorization ( gap - nmf ) , a bayesian nonpara - metric approach to decomposing spectrograms .
the assumptions behind gap - nmf are based on research in signal processing regarding the expected distributions of spectrogram data , and gap - nmf automatically discovers the number of latent sources .
we derive a mean - eld variational inference algorithm and evaluate gap - nmf on both synthetic data and recorded music .
recent research in machine learning has focused on break - ing audio spectrograms into separate sources of sound us - ing latent variable decompositions .
such decompositions have been applied to identifying individual instruments and notes , e . g . , for music transcription ( smaragdis & brown , 123 ) , to predicting hidden or distorted signals ( bansal et al . , 123 ) , and to source separation ( fevotte et al . , 123 ) .
a problem with these methods is that the number of sources must be specied in advance , or found with expensive tech - niques such as cross - validation .
this problem is particu - larly relevant when analyzing music .
we want the discov - ered latent components to correspond to real - world sound sources , and we cannot expect the same number of sources to be present in every recording .
in this article , we develop gamma process nonnegative matrix factorization ( gap - nmf ) , a bayesian nonparamet -
appearing in proceedings of the 123 th international conference on machine learning , haifa , israel , 123
copyright 123 by the
ric ( bnp ) approach to decomposing spectrograms .
we posit a generative probabilistic model of spectrogram data where , given an observed audio signal , posterior inference reveals both the latent sources and their number .
the central computational challenge posed by our model is posterior inference .
unlike other bnp factorization methods , our model is not composed of conjugate pairs of distributionswe chose our distributions to be appropriate for spectrogram data , not for computational convenience .
we use variational inference to approximate the posterior , and develop a novel variational approach to inference in nonconjugate models .
variational inference approximates the posterior with a simpler distribution , whose parameters are optimized to be close to the true posterior ( jordan et al . , 123 ) .
in mean - eld variational inference , each variable is given an independent distribution , usually of the same fam - ily as its prior .
where the model is conjugate , optimization proceeds by an elegant coordinate ascent algorithm .
re - searchers usually appeal to less efcient scalar optimization where conjugacy is absent .
we instead use a bigger varia - tional family than the model initially asserts .
we show that this gives an analytic coordinate ascent algorithm , of the kind usually limited to conjugate models .
we evaluated gap - nmf on several problemsextracting the sources from music audio , predicting the signal in miss - ing entries of the spectrogram , and classical measures of bayesian model t .
our model performs as well as or better than the current state - of - the - art .
it nds simpler representa - tions of the data with equal statistical power , without need - ing to explore many ts over many numbers of sources , and thus with much less computation .
gap - nmf model we model the fourier power spectrogram x of an audio signal .
the spectrogram x is an m by n matrix of non - negative reals; the cell xmn is the power of our input au - dio signal at time window n and frequency bin m .
each column of the power spectrogram is obtained as follows .
first , take the discrete fourier transform of a window of
bayesian nonparametric matrix factorization for recorded music
123 ( m 123 ) samples .
next , compute the squared magnitude of the complex value in each frequency bin .
finally , keep only the rst m bins , since the remaining bins contain only we assume the audio signal is composed of k static sound sources .
as a consequence , we can model the observed spectrogram x with the product of two non - negative ma - trices : an m by k matrix w describing these sources and a k by n matrix h controlling how the amplitude of each source changes over time ( smaragdis & brown , 123 ) .
each column of w is the average power spectrum of an audio source; cell wmk is the average amount of en - ergy source k exhibits at frequency m .
each row of h is the time - varying gain of a source; cell hkn is the gain of source k at time n .
these matrices are unobserved .
abdallah & plumbley ( 123 ) and fevotte et al .
( 123 ) show that ( under certain assumptions ) mixing k sound sources in the time domain , with average power spectra de - ned by the columns of w and gains specied by the rows of h , yields a mixture whose spectrogram x is distributed
previous spectrogram decompositions assume the number of components k is known .
in practice , this is rarely true .
our goal is to develop a method that infers both the char - acters and number of latent audio sources from data .
we develop a bayesian nonparametric model with an innite number of latent components , a nite number of which are active when conditioned on observed data .
we now describe the gamma process nonnegative matrix factorization model ( gap - nmf ) .
as in previous matrix de - composition models , the spectrogram x arises from hid - den matrices w and h .
in addition , the model includes a hidden vector of non - negative values , where each ele - ment l is the overall gain of the corresponding source l .
the key idea is that we allow for the possibility of a large number of sources l , but place a sparse prior on .
during posterior inference , this prior biases the model to use no more sources than it needs .
specically , gap - nmf assumes that x is drawn according to the following generative process : wml gamma ( a , a ) hln gamma ( b , b ) l gamma ( / l , c )
as the truncation level l increases towards innity , the vector approximates an innite sequence drawn from a gamma process with shape parameter and inverse - scale parameter c ( kingman , 123 ) .
a property of this se -
quence is that the number of elements k greater than some number > 123 is nite almost surely .
specically :
for truncation levels l that are sufciently large relative to the shape parameter , we likewise expect that only a few of the l elements of will be substantially greater than 123
during posterior inference , this property leads to a prefer - ence for explanations that use relatively few components .
note that the expected value of xmn under this model is constant with respect to l , , a , and b :
ep ( l ) ep ( wml ) ep ( hln ) = 123
this equation suggests the heuristic of setting the expected mean of the spectrogram x under the prior equal to its empirical mean x by setting c = 123 / x .
variational inference posterior inference is the central computational problem for analyzing data with the gap - nmf model .
given an ob - served spectrogram x , we want to compute the posterior distribution p ( , w , h|x , , a , b , c ) .
exact bayesian in - ference is intractable .
we appeal to mean - eld variational inference ( jordan et al . , 123 ) .
inference is a deterministic alternative to markov chain monte carlo ( mcmc ) methods that re - places sampling with optimization .
it has permitted ef - cient large - scale inference for several bayesian nonpara - metric models ( e . g .
blei & jordan , 123; doshi - velez et al . , 123; paisley & carin , 123 ) .
variational inference algo - rithms approximate the true posterior distribution with a simpler variational distribution controlled by free param - eters .
these parameters are optimized to make the vari - ational distribution close ( in kullback - leibler divergence ) to the true posterior of interest .
mean - eld variational in - ference uses a fully factorized variational distributioni . e . , under the variational distribution all variables are indepen - in conjugate models this permits easy coordinate ascent updates using variational distributions of the same families as the prior distributions .
less frequently , variational methods are applied to non - conjugate models , which allow increased model expres - sivity at the price of greater algorithmic challenges .
our model is such a model .
the usual strategy is to use a fac - torized variational distribution with the same families as the priors , bound or approximate the objective function , and use numerical techniques to optimize difcult parameters ( blei & lafferty , 123; braun & mcauliffe , 123 ) .
we use a different strategy .
we adopt an expanded fam - ily for our variational distributions , one that generalizes the
bayesian nonparametric matrix factorization for recorded music
priors family .
this allows us to derive analytic coordinate ascent updates for the variational parameters , eliminating the need for numerical optimization .
variational objective function
it is standard in mean - eld variational inference to give each variable a variational distribution from the same fam - ily as its prior distribution ( jordan et al . , 123 ) .
we instead use the more exible generalized inverse - gaussian ( gig ) family ( jrgenson , 123 ) :
q ( wml ) = gig ( ( w ) q ( hln ) = gig ( ( h ) q ( l ) = gig ( ( )
, ( w ) ln , ( h )
, ( w ) ln , ( h )
the gig distribution is an exponential family distribution with sufcient statistics x , 123 / x , and log x , and its pdf ( in canonical exponential family form ) is
gig ( y; , , ) =
exp ( ( 123 ) log y y / y ) / 123
for x 123 , 123 , and 123
( k ( x ) denotes a modied bessel function of the second kind . ) note that the gig familys sufcient statistics ( y , 123 / y , and log y ) are a superset of those of the gamma family ( y and log y ) , and so the gamma family is a special case of the gig family where > 123 , 123
to compute the bound in equation 123 , we will need the ex - pected values of each wml , hln , and l and of their recip - rocals under our variational gig distributions .
for a vari - able y gig ( , , ) these expectations are
having chosen a fully factorized variational family , we can lower bound the marginal likelihood of the input spectro - gram under the gap - nmf model ( jordan et al . , 123 ) :
log p ( x| , a , b , c ) eq ( log p ( x|w , h , ) ) + eq ( log p ( w|a ) ) eq ( log q ( w ) ) + eq ( log p ( h|b ) ) eq ( log q ( h ) ) + eq ( log p ( | , c ) ) eq ( log q ( ) ) .
the difference between the left and right sides of equation 123 is the kullback - leibler ( kl ) divergence between the true posterior and the variational distribution q .
thus , maximiz - ing this bound with respect to q minimizes the kl diver - gence between q and our posterior distribution of interest .
the second , third , and fourth lines of equation 123 can be computed using the expectations in equation 123
the likelihood term in equation 123 expands to
eq ( log p ( x|w , h , ) ) =
we cannot compute either of the expectations on the right .
however , we can compute lower bounds on both of them .
says that for any vector such that l 123 and ( cid : 123 ) first , the function x123 is concave .
jensens inequality l l = 123
we use this inequality to derive a bound on the rst expec - tation in equation 123 :
second , the function log x is convex .
we can therefore bound the second expectation in equation 123 using a rst - order taylor approximation about an arbitrary ( positive ) point mn as in ( blei & lafferty , 123 ) 123 :
log ( mn ) + 123 123
eq ( lwmlhln ) .
we use equations 123 and 123 to bound equation 123 : eq ( log p ( x|w , h , ) )
log ( mn ) + 123 123
eq ( lwmlhln ) .
note that this bound involves the expectations both of the model parameters and of their reciprocals under the vari - ational distribution q .
since both y and 123 / y are sufcient statistics of gig ( y; , , ) , this will not pose a problem during inference , as it would if we were to use variational distributions from the gamma family .
we denote as l the sum of the likelihood bound in equa - tion 123 and the second , third , and fourth lines of equation
123braun & mcauliffe ( 123 ) observe that this bound is max - imized when the taylor approximation is taken around the ex - pected value of the argument to the logarithm function , which corresponds to the 123th - order delta method .
however , retaining the redundant parameter mn permits faster and simpler up - dates for our other parameters .
bayesian nonparametric matrix factorization for recorded music
l lower bounds the likelihood p ( x| , a , b , c ) .
our vari - ational inference algorithm maximizes this bound over the free parameters , yielding an approximation q ( w , h , ) to the true posterior p ( w , h , |x , , a , b , c ) .
we iterate between updating bound parameters and vari - ational parameters according to equations 123 , 123 , 123 , 123 , and 123
each update tightens the variational bound on log p ( x| , a , b , c ) , ultimately reaching a local optimum .
coordinate ascent optimization we maximize the bound l using coordinate ascent , iter - atively optimizing each parameter while holding all other parameters xed .
there are two sets of parameters to opti - mize : those used to bound the likelihood term in equation 123 and those that control the variational distribution q .
tightening the likelihood bound
in equations 123 and 123 , we derived bounds on the in - tractable expectations in equation 123
after updating the variational distributions on each set of parameters w , h , and , we update and to re - tighten these bounds .
using lagrange multipliers , we nd that the optimal is
the bound in equation 123 is tightest when eq ( lwmlhln ) .
i . e . , this bound is tightest when we take the taylor approxi - mation about the expected value of the functions argument .
optimizing the variational distributions the derivative of l with respect to any of ( w )
equals 123 when
, ( w )
ml = a + eq ( l ) ( cid : 123 )
ml = a; ml = eq
simultaneously updating the parameters ( w ) , ( w ) , and ( w ) according to equation 123 will maximize l with re - spect to those parameters .
similarly , the derivative of l with respect to any of ( h ) ln , or ( h )
equals 123 and l is maximized when
ln = b + eq ( l ) ( cid : 123 )
ln = b; ln = eq
finally , the derivative of l with respect to any of ( )
equals 123 and l is maximized when
l = c + ( cid : 123 )
accelerating inference paisley & carin ( 123 ) observed that if eq ( l ) becomes small for some component l , then we can safely skip the updates for the variational parameters associated with that ( in our experiments we used 123 db below eq ( l ) as a threshold . ) this heuristic allows the use of large truncation levels l ( yielding a better approximation to an innite gamma process ) without incurring too severe a performance penalty .
the rst few iterations will be expen - sive , but the algorithm will require less time per iteration as it becomes clear that only a small number of components ( relative to l ) are needed to explain the data .
we conducted several experiments to assess the decompo - sitions provided by the gap - nmf model .
we tested gap - nmfs ability to recover the true parameters used to gener - ate a synthetic spectrogram , compared the marginal likeli - hoods of real songs under gap - nmf to the marginal likeli - hoods of those songs under a simpler version of the model , evaluated gap - nmfs ability to predict held - out data with a bandwidth expansion task , and evaluated gap - nmfs ability to separate individual notes from mixed recordings .
we compared gap - nmf to two variations on the same finite bayesian model .
this is a nite version of the gap - nmf model t using the same variational updates but with - out the top - level gain parameters .
this simpler models generative process is
wmk gamma ( a , ac ) ; hkn gamma ( b , b ) ;
where k ( 123 , .
, k ) and the model order k is chosen a priori .
the hyperparameters a , b , and c are set to the same values as in the gap - nmf model in all experiments .
we will refer to this model as gig - nmf , for generalized inverse - gaussian nonnegative matrix factorization .
finite non - bayesian model .
this model ts w and h to maximize the likelihood in equation 123
fevotte et al .
( 123 ) derive iterative multiplicative updates to maximize this likelihood , calling the resulting algorithm itakura - saito nonnegative matrix factorization ( is - nmf ) .
we also compared gap - nmf to the two nonnegative ma - trix factorization ( nmf ) algorithms described by lee & seung ( 123 ) .
both of these algorithms also attempt to ap -
bayesian nonparametric matrix factorization for recorded music
proximately decompose the spectrogram x into an m by k matrix w and a k by n matrix h so that x w h .
the rst algorithm , which we refer to as eu - nmf , mini - mizes the sum of the squared euclidean distances between the elements of x and w h .
the second algorithm , which we refer to as kl - nmf , minimizes the generalized kl - divergence between x and w h .
kl - nmf ( and its exten - sions ) in particular is widely used to analyze audio spectro - grams ( e . g .
smaragdis & brown , 123; bansal et al . , 123 ) .
we focus on approaches that explain power spectrograms in terms of components that can be interpreted as audio power spectra .
other approaches may be useful for some tasks , but they do not decompose mixed audio signals into their component sources .
this requirement excludes , for example , standard linear gaussian factor models , whose latent factors cannot be interpreted as audio spectra unless audio signals are allowed to have negative power .
we normalized all spectrograms to have a maximum value of 123 .
( the high probability densities in our experiments result from low - power bins in the spectrograms . ) to avoid numerical issues , we forced the values of the spectrograms to be at least 123 , 123 db below the peak value of 123 .
in all experiments , we initialized the variational parameters for each w , h , and with random draws from a gamma distribution with shape parameter 123 and inverse - scale pa - rameter 123 , the variational parameters to 123 , and each mk = a , ( h ) k = / k .
this yields a dif - fuse and smooth initial variational posterior , which helped avoid local optima .
we ran variational inference until the variational bound increased by less than 123% .
the gig - nmf and is - nmf algorithms were optimized to the same criterion .
kl - nmf and eu - nmf were iterated until their cost functions decreased by less than 123 and 123 , re - spectively .
( we found no gains in performance from letting eu - nmf or kl - nmf run longer . ) all algorithms were implemented in matlab123
we found gap - nmf to be insensitive to the choice of , and so we set = 123 in all reported experiments .
kn = b , and ( )
synthetic data
we evaluated the gap - nmf models ability to correctly discover the latent bases that generated a matrix x , and how many such bases exist .
to test this , we t gap - nmf to random matrices x drawn according to the process :
wmk gamma ( 123 , 123 ) ; hkn gamma ( 123 , 123 ) ;
figure 123
true synthetic bases ( left ) and expected values under the variational posterior of the nine bases found by the model ( right ) .
brighter denotes more active .
the 123 - dimensional basis vectors are presented in 123 123 blocks for visual clarity .
where m ( 123 , .
, m = 123 ) , n ( 123 , .
, n = 123 ) , k ( 123 , .
, k ) for k = 123
we ran variational inference with the truncation level l set to 123 , and hyperparameters = 123 , a = b = 123 , c = 123 / x ( where x is the mean of x ) .
after convergence , only nine of these components were associated with the ob - served data .
( the smallest element of associated with one of these nine components was 123 , while the next largest element was 123 123 ) .
figure 123 shows that the latent components discovered by the model correspond closely to those used to generate the data .
marginal likelihood
we want to evaluate the ability of gap - nmf to choose a good number of components to model recorded music .
to determine a good number of components , we use varia - tional inference to t gig - nmf with various orders k and examine the resulting variational bounds on the marginal log - likelihood log p ( x|a , b , c ) .
as above , we set the prior parameters for the gap - nmf model to = 123 , a = b = 123 , and c = 123 / x .
we set the prior parameters for the simplied model to a = b = 123 and c = 123 / x .
the value of 123 for a and b was chosen because it gave slightly better bounds than higher or lower values .
the results were not very sensitive to .
we computed power spectrograms from three songs : pink moon by nick drake , funky kingston by toots and the maytals , and a clip from the kreutzer sonata by ludwig van beethoven .
these analyses used 123 - sample ( 123 ms ) hann windows with no overlap , yielding spectrograms of 123 frequency bins by 123 , 123 , and 123 time win - dows , respectively .
we t variational posteriors for gap - nmf and gig - nmf , conditioning on these spectrograms .
we used a truncation level l of 123 for the nonparametric model , and values of k ranging from 123 to 123 for the nite the computational cost of tting the gap - nmf model was lower than the cost of tting gig - nmf with k = 123 ( thanks to the accelerated inference trick in section 123 ) ,
bayesian nonparametric matrix factorization for recorded music
figure 123
left : bounds on log p ( x|prior ) for the nonparametric gap - nmf model and its parametric counterpart gig - nmf with differ - ent numbers of latent components k .
ticks on the horizontal lines showing the bound for the gap - nmf model indicate the number of components k used to explain the data .
for all three songs the values of k chosen by gap - nmf are close to the optimal value of k for the parametric model .
right : geometric mean of the likelihood assigned to each censored observation by the nonparametric , nite , and unregularized models .
ticks again indicate the number of components k used to explain the data .
the unregularized models overt .
eu - nmf performs badly , with likelihoods orders of magnitude lower than the other models .
and much lower than the cost of repeatedly tting gig - nmf with different values of k .
for example , on a single core of a 123 ghz amd opteron 123 quad - core proces - sor , tting the 123 - component gig - nmf model to pink moon took 123 seconds , while tting the gap - nmf model to the same song took 123 seconds .
the results are summarized in gure 123 ( left ) .
the gap - nmf model used 123 , 123 , and 123 components to explain the spectrograms of funky kingston , the kreutzer sonata , and pink moon respectively .
in each case the value of k chosen by gap - nmf was close to the best value of k tested for the gig - nmf model .
this suggests that gap - nmf performs automatic order selection as well as the more expensive ap - proach of tting multiple nite - order models .
bandwidth expansion
one application of statistical spectral analysis is band - width expansion , the problem of inferring what the high - frequency content of a signal is likely to be given only the low - frequency content of the signal ( bansal et al . , 123 ) .
this task has applications to restoration of low - bandwidth audio and lossy audio compression .
this is a missing data problem .
we compared the ability of different models and inference algorithms to predict the held - out data .
we computed a power spectrogram from 123 123 - sample ( 123 ms ) hann windows taken from the middles of the same three songs used to evaluate marginal like - lihoods : funky kingston , the kreutzer sonata , and pink moon .
for each song , this yielded a 123 123 spec - trogram x describing 123 seconds of the song .
we ran ve - fold cross - validation to compare gap - nmfs predic - tions of the missing high - frequency content to those of gig - nmf , eu - nmf , and is - nmf .
( it is more difcult to
evaluate kl - nmfs ability to predict missing data , since it does not correspond to a probabilistic model of continuous data . ) we divided each spectrogram into ve contiguous 123 - frame sections .
for each fold , we censored the top two octaves ( i . e . , the top 123 out of 123 frequency bins ) of one of those sections .
we then predicted the values in the cen - sored bins based on the data in the uncensored bins .
the prior hyperparameters for the bayesian models were set to a = b = 123 , c = 123 / x , and = 123 ( for gap - nmf ) .
we chose a higher value for a and b for this experiment since stronger smoothing can improve the models ability to generalize to held - out data .
for each t model , we computed an estimate xpred mn of each missing value xmiss mn .
for the models t using variational inference , we used the expected value of the missing data under the variational posterior q , eq ( ep ( xmiss mn ) ) .
for the gap - nmf model , this expectation is
mn = eq ( ep ( xmiss
and for the gig - nmf model it is
mn ) ) = ( cid : 123 ) mn ) ) = ( cid : 123 )
mn = eq ( ep ( xmiss
is - nmf and eu - nmf we predicted xpred
to evaluate the quality of t for is - nmf , gap - nmf , and gig - nmf , we compute the likelihood of each unobserved mn under an exponential distribution with mean mn .
to evaluate eu - nmf , we rst compute the mean squared error of the estimate of the observed data 123 = mean ( ( xobs ( w h ) obs ) 123 ) .
we then compute the like - lihood of each unobserved element xmiss mn under a normal distribution with mean xpred
mn and variance 123
klog p ( x ) 123funky kingstonkreutzer sonatapink moonkgeometric mean probabilityof hidden data123e+123e+123e+123e+123e+123e+123e+123e+123e+123e+123e+123funky kingstonkreutzer sonatapink moonmodelgap - nmfeu - nmfgig - nmfis - nmf bayesian nonparametric matrix factorization for recorded music
figure 123 ( right ) plots the geometric mean of the likelihood of each unobserved element of x for the nonparametric model and for models t with different numbers of com - ponents k .
the bayesian models do very well compared with the unregularized models , which overt badly for any number of components k greater than 123
gap - nmf used fewer components to explain the songs than in the previous experiment , which we attribute to the stronger smoothing , smaller number of observations , and smaller window size .
blind monophonic source separation
gap - nmf can also be applied to blind source separation , where the goal is to recover a set of audio signals that combined to produce a mixed audio signal .
for exam - ple , we may want to separate a polyphonic piece of music into notes to facilitate transcription ( smaragdis & brown , 123 ) , denoising , or upmixing ( fevotte et al . , 123 ) .
the gap - nmf model assumes that the audio signal is a linear combination of l sources ( some of which have ex - tremely low gain ) .
given the complex magnitude spectro - gram x c of the original audio and an estimate of the model parameters w , h , and , we can compute maximum - likelihood estimates of the spectrograms of the l unmixed sources using wiener ltering ( fevotte et al . , 123 ) :
xlmn = x c
where xlmn is the estimate of the complex magnitude spectrum of the lth source at time n and frequency n .
we can invert these spectrograms to obtain estimates of the au - dio signals that are combined in the mixed audio signal .
we evaluated gap - nmfs ability to separate signals from two synthesized pieces of music .
we used synthetic music rather than live performances so that we could easily isolate each note .
the pieces we used were a randomly generated four - voice clarinet piece using 123 unique notes , and a two - part bach invention synthesized using a physical model of a vibraphone using 123 unique notes .
we compared the signal - to - noise ratios ( snrs ) of the separated tracks for the gap - nmf model with those ob - tained using gig - nmf , is - nmf , eu - nmf , and kl - nmf .
for the nite models we also used wiener ltering to sepa - rate the tracks , dropping from equation 123
the models do not provide any explicit information about the correspondence between sources and notes .
to decide which separated signal to associate with which note , we adopt the heuristic of assigning each note to the component k whose gain signal h k has the highest correlation with the power envelope of the true note signal .
we only consider the v components that make the largest contribution to the mixed signal , where v is the true number of notes .
figure 123
average signal - to - noise ratios ( snrs ) across notes in the source separation task .
approaches based on the exponential likelihood model do well , eu - nmf and kl - nmf do less well .
ticks on the horizontal lines showing gap - nmfs performance denote the nal number of components k used to explain the data .
figure 123 shows the average snrs of the tracks correspond - ing to individual notes for each piece .
the approaches based on the exponential likelihood model do comparably well .
the kl - nmf and eu - nmf models perform con - siderably worse , and are sensitive to the model order k .
gap - nmf decomposed the clarinet piece into 123 compo - nents , and the bach invention into 123 components .
in both cases , some of these components were used to model the temporal evolution of the instrument sounds .
related work gap - nmf is closely related to recent work in bayesian nonparametrics and probabilistic interpretations of nmf .
bayesian nonparametrics most of the literature on bayesian nonparametric latent factor models focuses on conjugate linear gaussian models in conjunction with the indian buffet process ( ibp ) ( grifths & ghahramani , 123 ) or the beta process ( bp ) ( thibaux & jordan , 123 ) , using either mcmc or variational methods for posterior inference ( e . g .
doshi - velez et al . , 123; paisley & carin , 123 ) ) .
( an exception that uses mcmc in non - conjugate innite latent factor models is ( teh et al . , 123 ) . ) a standard linear gaussian likelihood model is not appro - priate for audio spectrogram data , whereas the exponential likelihood model has theoretical justication and gives in - terpretable components .
the nonlinearity and lack of con - jugacy of the exponential likelihood model make inference using an ibp or bp difcult .
our use of a gamma process prior allows us to derive an innite latent factor model that is appropriate for audio spectrograms and permits a simple and efcient variational inference algorithm .
probabilistic nmf fevotte & cemgil
ksignal - to - noise ratio - 123 - 123 - 123 - 123 - 123 - 123bach inventionclarinet piecemodelgap - nmfeu - nmfgig - nmfis - nmfkl - nmf bayesian nonparametric matrix factorization for recorded music
gest the outline of a variational inference algorithm for itakura - saito nmf based on the space - alternating gener - alized expectation - maximization algorithm in fevotte et al .
( 123 ) .
this approach introduces kmn complex hid - den variables whose posteriors must be estimated .
in our informal experiments , this gave a much looser variational bound , much longer convergence times , and a less exible approximate posterior than the variational inference algo - rithm presented in this paper .
our approach of weighting the contribution of each com - ponent k by a parameter k resembles the strategy of au - tomatic relevance determination ( ard ) , which has been used in a maximum a posteriori ( map ) estimation algo - rithm for a different nmf cost function ( tan & fevotte , 123 ) .
though similar in spirit , this ard approach is less amenable to fully bayesian inference .
we developed the gap - nmf model , a bayesian nonpara - metric model capable of determining the number of la - tent sources needed to explain an audio spectrogram .
we demonstrated the effectiveness of the gap - nmf model on several problems in analyzing and processing recorded mu - sic .
although this paper has focused on analyzing music , gap - nmf is equally applicable to other types of audio , such as speech or environmental sounds .
we thank the reviewers for their helpful observations and suggestions .
david m .
blei is supported by onr 123 - 123 , nsf career 123
