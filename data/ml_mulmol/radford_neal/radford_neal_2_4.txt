abstract .
ratios of normalizing constants for two distributions are needed in both bayesian statistics , where they are used to compare models , and in statistical physics , where they correspond to dierences in free energy .
two approaches have long been used to estimate ratios of normalizing constants .
the simple importance sampling ( sis ) or free energy perturbation method uses a sample drawn from just one of the two distributions .
the bridge sampling or acceptance ratio estimate can be viewed as the ratio of two sis estimates involving a bridge distribution .
for both methods , dicult problems must be handled by introducing a sequence of intermediate distributions linking the two distributions of interest , with the nal ratio of normalizing constants being estimated by the product of estimates of ratios for adjacent distributions in this sequence .
recently , work by jarzynski , and independently by neal , has shown how one can view such a product of estimates , each based on simple importance sampling using a single point , as an sis estimate on an extended state space .
this annealed importance sampling ( ais ) method produces an exactly unbiased estimate for the ratio of normalizing constants even when the markov transitions used do not reach equilibrium .
in this paper , i show how a corresponding linked importance sampling ( lis ) method can be constructed in which the estimates for individual ratios are similar to bridge sampling estimates .
as a further elaboration , bridge sampling rather than simple importance sampling can be employed at the top level for both ais and lis , which sometimes produces further improvement .
i show empirically that for some problems , lis estimates are much more accurate than ais estimates found using the same computation time , although for other problems the two methods have similar performance .
like ais , lis can also produce estimates for expectations , even when the distribution contains multiple isolated modes .
ais is related to the tempered transition method for handling isolated modes , and to a method for dragging fast variables .
linked sampling methods similar to lis can be constructed that are analogous to tempered transitions and to this method for dragging fast variables , which may sometimes work better than those analogous to ais .
consider two distributions on the same space , with probability mass or density functions 123 ( x ) = p123 ( x ) / z123 and 123 ( x ) = p123 ( x ) / z123
suppose that we are not able to directly compute 123 and 123 , but only p123 and p123 , since we do not know the normalizing constants , z123 and z123
we wish to nd a monte carlo estimate for the ratio of these normalizing constants , z123 / z123 , which we sometimes denote by r , using samples of values drawn ( at least approximately ) from 123 and from 123
sometimes , we may know z123 , in which case we can arrange for it to be one , so that estimation of this ratio will give the numerical value of z123
other times , we will be able to obtain only the ratio of normalizing constants , but this may be sucient for our purposes .
in statistical physics , x represents the state of some physical system , and the distributions are
typically canonical distributions having the following form ( for j = 123 , 123 ) :
pj ( x ) = exp ( ju ( x , j ) )
where u ( x , j ) is an energy function , which may depend on the parameter j , and j is the inverse temperature of system j .
many interesting properties of the systems are related to the free energy , dened as log ( zj ) / j .
often , only the dierence in free energy between systems 123 and 123 is relevant , and this is determined by the ratio z123 / z123
in bayesian statistics , x comprises the parameters and latent variables for some statistical model , 123 is the prior distribution for these quantities ( for which the normalizing constant is usually known ) , and 123 is the posterior distribution given the observed data .
we can compute p123 ( x ) as the product of the prior density for x and the probability of the data given x , but the normalizing constant , z123 , is dicult to compute .
we can interpret z123 as the marginal likelihood the probability of the observed data under this model , integrating over possible values of the models parameters and latent variables .
the marginal likelihood for a model indicates how well it is supported by the data .
although i will use simple distributions as illustrations in this paper , in real applications , x is usually high dimensional , and at least one of 123 and 123 is usually quite complex .
accordingly , sam - pling from these distributions generally requires use of markov chain methods , such as the venerable metropolis algorithm ( metropolis , et al 123 ) .
see ( neal 123 ) for a review of markov chain sampling methods .
sometimes , however , 123 will be relatively simple , and independent points drawn from it can be generated eciently , as would often be the case with the prior distribution for a bayesian model , or for a physical system at innite temperature ( 123 = 123 ) .
many methods for estimating ratios of normalizing constants from monte carlo data have been investigated in the physics literature ( for a review , see ( neal 123 , section 123 ) ) , and later rediscov - ered in the statistics literature ( gelman and meng 123 ) .
a logical method to start with is simple importance sampling ( sis ) , also called free energy perturbation , based on the following identity , which can easily be proved on the assumption that no region having zero probability under 123 has
non - zero probability under 123 :
= e123 ( cid : 123 ) p123 ( x )
sis = rsis
in the above equation , e123 denotes an expectation with respect to the distribution 123 , which is estimated by a monte carlo average over points x ( i ) , .
, x ( n ) drawn from 123 ( either independently , or using a markov chain sampler ) .
here and later , rm will denote an estimate of r = z123 / z123 , found by method m .
if this estimate is an average of unbiased estimates based on a number of samples , these individual estimates will be denoted by r ( i )
the simple importance sampling estimate , rsis , will be poor if 123 and 123 are not close enough in particular , if any region with non - negligible probability under 123 has very small probability under 123
such a region would have an important eect on the value of r , but very little information about it would be contained in the sample from 123
in such a situation , it may be possible to obtain a good estimate by introducing intermediate distributions .
parameterizing these distributions in some way using , we can dene a sequence of distributions , 123 , .
, n , with 123 = 123 and n = 123 so that the rst and last distributions in the sequence are 123 and 123 , with the intermediate distributions interpolating between them .
we can then write
provided that j+123 and j are close enough , we can estimate each of the factors zj+123 / zj using simple importance sampling , and from these estimates obtain an estimate for z123 / z123
we can obtain good estimates in a wider range of situations , or using fewer intermediate distributions ( sometimes none ) , by applying a technique introduced by bennett ( 123 ) , who called it the acceptance ratio method .
this method was later rediscovered by meng and wong ( 123 ) , who called it bridge sampling .
lu , singh , and kofke ( 123 ) provide a recent review and assessment .
one way of viewing this method is that it replaces the simple importance sampling estimate for z123 / z123 by a ratio of estimates for z / z123 and z / z123 , where z is the normalizing constant for a bridge distribution , ( x ) = p ( x ) / z , which is chosen so that it is overlapped by both 123 and 123
using simple importance sampling estimates for z / z123 and z / z123 , we can obtain the estimate
= e123 ( cid : 123 ) p ( x )
p123 ( x ) ( cid : 123 ) .
e123 ( cid : 123 ) p ( x )
p123 ( x123 , k )
where x123 , 123 , .
, x123 , n123 are drawn from 123 and x123 , 123 , .
, x123 , n123 are drawn from 123
one simple choice for the bridge distribution is the geometric bridge :
( x ) = pp123 ( x ) p123 ( x )
which is in a sense half - way between 123 and 123
as discussed by bennett ( 123 ) and by meng and wong ( 123 ) , the asymptotically optimal choice of bridge distribution is
r ( n123 / n123 ) p123 ( x ) + p123 ( x )
where r = z123 / z123
of course , we cannot use this bridge distribution in practice , since we do not know r .
we can use a preliminary guess at r to dene an initial bridge distribution , however , which will give us a bridge sampling estimate for z123 / z123
using this estimate as the new value of r , we can rene our bridge distribution , iterating this process as many times as desired .
the result of this iteration can also be viewed as a maximum likelihood estimate for r , as discussed by shirts , et al ( 123 ) , who argues on this basis that it is asymptotically as good as any estimate for r .
i have found that estimates with r set iteratively are often better than those found with the true value of r ( which does not contradict optimality of the true value for a xed choice of bridge distribution ) .
if 123 and 123 do not overlap suciently , no bridge distribution will produce good estimates , and we will have to introduce intermediate distributions as in equation ( 123 ) .
note , however , that the bridge sampling estimate with either of the above bridge distributions converges to the correct ratio asymptotically as long there is some region that has non - zero probability under both 123 and 123 , a much weaker requirement than that for simple importance sampling .
this advantage of bridge sampling over sis can be seen in a simple example involving distributions that are uniform over an interval of the reals .
let p123 ( x ) = i ( 123 , 123 ) ( x ) and p123 ( x ) = i ( 123 , 123 ) ( x ) , so that z123 = 123 and z123 = 123
the simple importance sampling estimate of equation ( 123 ) does not work , as it converges to 123 / 123 rather than 123 / 123
however , using a bridge distribution with p ( x ) = i ( 123 , 123 ) , which is eectively what both popt will be in this example , the bridge sampling estimate of equation ( 123 ) converges to the correct value , since the numerator converges to 123 / 123 and the denominator to 123 / 123
although both simple importance sampling and bridge sampling have been successfully used in many applications , they have some deciencies .
one issue is that although the sis estimate of equation ( 123 ) is unbiased for z123 / z123 , the bridge sampling estimate of equation ( 123 ) is not , and the same would appear to be the case for an estimate using intermediate distributions ( via equation ( 123 ) ) .
this is of no direct importance , particularly since we are often more interested in log ( z123 / z123 ) than in z123 / z123 itself .
however , it does preclude averaging independent replications of the bridge sampling estimate to obtain a better estimate , since the bias would prevent convergence to the correct value as the number of replications increases .
a more vexing diculty is that , except sometimes for 123 , sampling from the distributions must usually be done by markov chain methods , which approach the desired distribution only asymptotically .
to speed convergence , the markov chain for sampling j is often started from the last state sampled for j123 , but it is unclear how many iterations should then be discarded before an adequate approximation to the correct distribution is reached .
surprisingly , these diculties can be completely overcome when using simple importance sampling with a single point .
as shown by jarzynski ( 123 , 123 ) , and later independently by myself ( neal 123 ) , an estimate for z123 / z123 using intermediate distributions as in equation ( 123 ) will be exactly unbiased if
each of the ratios zj+123 / zj is estimated using the simple importance sampling estimate of equation ( 123 ) with n = 123 , sampling each distribution with a markov chain update starting with the point for the previous distribution .
averaging the estimates obtained from m independent replications of this process ( called runs ) produces the following estimate :
ais = rais
, x ( m )
invariant to x ( i )
are drawn independently from 123 , and each x ( i )
for j > 123 is generated by applying a markov chain transition that leaves j j123
this single markov transition ( which could , however , consist of several metropolis or other updates if we so choose ) , will usually not be enough to reach equilibrium , but the estimate rais is nevertheless exactly unbiased , and will converge to the true value as m increases , provided that no region having zero probability under j has non - zero probability under j+123
this can be proved by showing how the estimate above can be seen as a simple importance sampling estimate on an extended state space that includes the values sampled for the intermediate distributions .
i call this method annealed importance sampling ( ais ) , since the sequence of distributions used often corresponds to an annealing procedure , in which the temperature is gradually decreased .
as i discuss in ( neal 123 ) , this allows the procedure to sample dierent isolated modes of the distribution on dierent runs , properly weighting the points obtained from each of these runs to produce the correct probability for each mode .
ais is related to an earlier method for moving between isolated modes that i call tempered transitions ( neal 123 ) .
in a recent paper ( neal 123 ) , i show how tempered transitions can be modied to produce a method for ecient markov chain sampling when some of the state variables are fast ie , when it is possible to more quickly recompute the probability of a state when only these fast variables change than when the other slow variables change as well .
in this method , the fast variables are dragged through intermediate distributions in order to produce more appropriate values to go with a proposed change to the slow variables .
deciding whether to accept the nal proposal involves what is in eect an estimate of the ratio of normalizing constants for the conditional distributions of the fast variables .
in this paper , i show how the ideas behind annealed importance sampling and bridge sampling can be combined .
i call the resulting method linked importance sampling ( lis ) , since the two samples needed for bridge sampling are linked by a single state that is used in both .
intermediate distributions can be used , with each distribution being linked by a single state to the next distribution .
in contrast to bridge sampling , lis estimates are unbiased , and as is the case for ais , they remain exactly unbiased even when intermediate distributions are used , and when sampling is done using markov chain transitions that have not converged to their equilibrium distributions .
crooks ( 123 ) mentions a dierent way of combining ais with bridge sampling since ais esti - mates are simple importance sampling estimates on an extended state space , we can combine forward and reverse estimates to produce a bridge sampling estimate that may be superior .
i will call this
method bridged ais .
similarly , such a top - level application of bridge sampling can be combined with the low - level application of bridge sampling in lis , giving what i call bridged lis .
using tests on sequences of one - dimensional distributions , i demonstrate that for some problems lis is much more ecient than ais a result that should be expected , since in extreme cases , such as for the uniform distributions discussed above , the simple importance sampling estimates underlying ais do not converge to the correct answer even asymptotically , whereas bridge sampling estimates do .
for some other problems , however , ais and lis perform about equally well .
the bridged version of ais sometimes performs much better than the unbridged version , but still performs less well than lis and its bridged version on some problems .
i also analyse the asymptotic properties of ais and lis for some types of distribution , providing additional insight into their behaviour .
variants of tempered transitions and of my method for dragging fast variables can be constructed that are analogous to lis rather than to ais .
i discuss the linked variant of tempered transitions briey , and include a more detailed description of a linked version of dragging , which may sometimes be better than the version related to ais .
i conclude by discussing some possibilities for future research .
123 the linked importance sampling procedure
assume that we can evaluate the unnormalized probability or density functions p ( x ) , for any value of the parameter , with the normalized form of such a distribution being denoted by .
the values = 123 and = 123 dene the two distributions we are interested in , for which the normalizing constants are z123 and z123
a sequence of n123 intermediate values for dene distributions that will assist in estimating the ratio of these normalizing constants , r = z123 / z123
we denote the values of for the distributions used by 123 , .
, n , with 123 = 123 and n = 123
typically , j < j+123 for all j .
for problems in statistical physics , might be proportional to the inverse temperature , , of equation ( 123 ) , or might map to a value for .
for a bayesian inference problem , might be a power that the likelihood is raised to , so that = 123 causes the data to be ignored , and = 123 gives full weight to the data; the ratio z123 / z123 will then be the marginal likelihood .
in both of these examples , progressing in small steps from = 123 to = 123 is not only useful in estimating z123 / z123 , but also often has an annealing eect , which helps avoid being trapped in a local mode of the distribution .
123 details of the lis procedure
for each distribution , , assume we have a pair of markov chain transition probability ( or density )
functions , denoted by t ( x , x ) and t ( x , x ) , satisfying r t ( x , x ) dx = 123 and r t ( x , x ) dx = 123 , for
which the following mutual reversibility relationship holds :
( x ) t ( x , x ) = ( x ) t ( x , x ) ,
for all x and x
from this relationship , one can easily show that both t and t leave invariant ie , that
r ( x ) t ( x , x ) dx = ( x ) , and the same for t .
if t is reversible ( ie , satises detailed balance ) ,
then t will be the same as t .
non - reversible transitions often arise when components of state are updated in some predetermined order , in which case the reverse transition simply updates components in the opposite order .
as a special case , t might draw the next state from independently of the current state .
such independent sampling may often be possible for t123
these markov chain transitions are used to obtain samples that are approximately drawn from each of the n+123 distributions , 123 , .
we assume that we can begin sampling from 123 by drawing a single point independently from 123
for j > 123 , we begin sampling from j by selecting a link state , xj123j , from the sample associated with j123
for all j , we produce a sample of kj +123 states from this starting point by applying a total of kj forward ( tj ) or reversed ( t j ) markov transitions .
link states are selected using bridge distributions , pj j+123 , which are dened in terms of pj and pj+123 , perhaps using the form of equation ( 123 ) or ( 123 ) , with p123 replaced by pj and p123 by pj+123
in detail , the linked importance sampling procedure produces m estimates , r ( 123 )
, r ( m )
lis , that
are averaged to produce the nal estimate , rlis .
each r ( i )
lis is obtained by performing the following :
the lis procedure
123 ) pick an integer 123 uniformly at random from ( 123 , .
, k123 ) , and then set x123 , 123 to a value drawn
123 ) for j = 123 , .
, n , sample kj +123 states drawn ( at least approximately ) from j as follows :
a ) if j > 123 : pick an integer j uniformly at random from ( 123 , .
, kj ) , and then set xj , j to
b ) for k = j + 123 , .
, kj , draw xj , k according to the forward markov chain transition prob -
abilities tj ( xj , k123 , xj , k ) .
( if j = kj , do nothing in this step . )
c ) for k = j 123 , .
, 123 , draw xj , k according to the reverse markov chain transition probabil -
ities t j ( xj , k+123 , xj , k ) .
( if j = 123 , do nothing in this step . )
d ) if j < n : pick a value for j from ( 123 , .
, kj ) according to the following probabilities :
123 ( j | xj ) =
pj j+123 ( xj , j )
pj ( xj , j ) .
and then set xj j+123 to xj , j .
123 ) set n to a value chosen uniformly at random from ( 123 , .
( this selection has no eect on
the estimate , but is used in the proof of correctness . )
123 ) compute the estimate from this run as follows :
kj + 123
pj ( xj , k ) .
kj+123 + 123
( note that most of the factors of 123 / ( kj + 123 ) and 123 / ( kj+123 + 123 ) cancel , giving a nal result of ( kn +123 ) / ( k123 +123 ) , but the redundant factors are retained above for clarity of meaning . )
figure 123 : an illustration of linked importance sampling .
one intermediate distribution is used , with 123 = 123 / 123
the distributions 123 , 123 / 123 , and 123 are represented by ovals enclosing the regions of high probability under each distribution .
nine markov chain transitions are performed at each stage .
the two link states are shown as black dots .
the initial and nal states ( indexed by 123 and n ) are shown as gray dots .
other states generated by the forward and reverse markov chain transitions are shown as empty dots .
for this run , 123 = 123 , 123 = 123 , 123 = 123 , 123 = 123 , 123 = 123 , and 123 = 123
the result of performing steps ( 123 ) through ( 123 ) is illustrated in figure 123
after m runs of this procedure , the nal estimate is computed as
the crucial aspect of linked importance sampling is that when moving from distribution j to j+123 , a link state , xj j+123 , is randomly selected from among the sample of points xj , 123 , .
, xj , kj+123 that are associated with j .
we can view the link state as part of the sample associated with j+123 as well as that associated with j .
accordingly , when using the optimal bridge of equation ( 123 ) , i will set n123 / n123 to ( kj +123 ) / ( kj+123 +123 ) , though the proof of optimality for bridge sampling does not guarantee that this is an optimal choice when using this bridge distribution for lis .
123 proof that lis estimates are unbiased
in order to prove that r ( i ) lis is an unbiased estimate of r = z123 / z123 , we can regard steps ( 123 ) through ( 123 ) above as dening a distribution , 123 , over all the quantities involved in the procedure namely , xj , j , and j , for j = 123 , .
, n , with xj representing xj , 123 , .
, xj , kj .
we then consider the procedure for generating these same quantities in reverse , which operates as follows :
the reverse lis procedure
123 ) pick an integer n uniformly at random from ( 123 , .
, kn ) , and then set xn , n to a value drawn
123 ) for j = n , .
, 123 , sample kj +123 states drawn ( at least approximately ) from j as follows :
a ) if j < n : pick an integer j uniformly at random from ( 123 , .
, kj ) , and then set xj , j to
b ) for k = j + 123 , .
, kj , draw xj , k according to the forward markov chain transition prob -
abilities tj ( xj , k123 , xj , k ) .
( if j = kj , do nothing in this step . )
c ) for k = j 123 , .
, 123 , draw xj , k according to the reverse markov chain transition proba -
bilities t j ( xj , k+123 , xj , k ) .
( if j = 123 , do nothing in this step . )
d ) if j > 123 : pick a value for j from ( 123 , .
, kj ) according to the following probabilities :
123 ( j | xj ) =
pj ( xj , j ) .
and then set xj123j to xj , j .
123 ) set 123 to a value chosen uniformly at random from ( 123 , .
, k123 ) .
this reverse procedure also denes a distribution over all the quantities generated ( xj , j , and j for j = 123 , .
, n ) , which will be denoted by 123
we now dene the unnormalized probability ( density ) functions p123 ( x , , ) = z123 ( x , , ) and p123 ( x , , ) = z123 ( x , , ) .
the ratio of normalizing constants for these distributions is obviously r = z123 / z123
we can estimate this ratio by simple importance sampling , using the ratios
p123 ( x , , ) p123 ( x , , )
z123 123 ( n ) n ( xn , n )
z123 123 ( 123 ) 123 ( x123 , 123 )
123 ( j | xj ) 123 ( 123 )
123 ( xj | j , xj , j )
123 ( xj | j , xj , j ) 123 ( j | xj ) 123 ( n )
from steps ( 123b ) and ( 123c ) of the forward and reverse procedures , along with the mutual reversibility
relationship of equation ( 123 ) , we see that
123 ( xj | j , xj , j ) =
tj ( xj , k123 , xj , k )
tj ( xj , k123 , xj , k )
t j ( xj , k+123 , xj , k )
tj ( xj , k , xj , k+123 )
j ( xj , j )
tj ( xj , k123 , xj , k )
123 ( xj | j , xj , j ) =
j ( xj , j )
tj ( xj , k123 , xj , k )
from this , we see that parts of the ratio in equation ( 123 ) can be written as
123 ( xj | j , xj , j )
123 ( xj | j , xj , j )
j ( xj , j ) j ( xj , j )
pj ( xj , j )
the last step uses the fact that for j = 123 , .
, n , xj , j = xj123j = xj123 , j123
from steps ( 123 ) and ( 123a ) , we see that 123 ( j ) = 123 / ( kj +123 ) and 123 ( j ) = 123 / ( kj +123 ) .
using this ,
and again using xj , j = xj123 , j123 , we get that
123 ( j | xj )
123 ( j | xj )
123 ( j+123 | xj+123 ) ( kj+123 +123 )
123 ( j | xj ) ( kj +123 )
pj j+123 ( xj , j )
pj ( xj , j )
pj ( xj , j )
pj ( xj , k ) .
from steps ( 123 ) and ( 123 ) , we see that 123 ( 123 ) = 123 ( 123 ) = 123 / ( k123 + 123 ) and 123 ( n ) = 123 ( n ) = 123 / ( kn+123 ) , so these factors cancel in equation ( 123 ) .
the factors in equation ( 123 ) cancel with the rst part of equation ( 123 ) .
the nal result is that the simple importance sampling estimate based on a single lis run is as shown in equation ( 123 ) , demonstrating that rlis is indeed an unbiased estimate of r = z123 / z123
123 bridged lis estimates
since the lis estimate can be viewed as a simple importance sampling estimate on an extended space , we can consider a bridged lis estimate in which this top - level sis estimate is replaced by a bridge sampling estimate .
this will require that we actually perform the reverse lis procedure described above , from which an lis estimate for the reverse ratio , r = z123 / z123 , can be computed :
kj + 123
pj ( xj , k ) .
kj123 + 123
the reversed procedure requires independent sampling from 123
this will usually not be possible directly , but well - separated states from a markov chain sampler with 123 as its invariant distribution will provide a good approximation , provided that this sampler moves around the whole distribution , without being trapped in an isolated mode .
indeed , the entire sample of kn+123 states from 123 that is needed at the start of the reverse procedure can be obtained by taking consecutive states from such a markov chain sampler .
for the bridged form of lis , we also need a suitable bridge distribution , p , for which we must be able to evaluate the ratios p / p123 and p / p123
( note that this choice of a top - level bridge distribution is separate from the choices of low - level bridge distributions , pj j+123 , though we might use the same form for both . ) with the optimal bridge of equation ( 123 ) , these ratios can be written as follows , if the forward procedure is performed m times and the reverse procedure m times :
( x , , ) p123 ( x , , )
( x , , ) p123 ( x , , )
= " r ( m / m ) ( cid : 123 ) p123 ( x , , ) p123 ( x , , ) ( cid : 123 ) 123 p123 ( x , , ) ( cid : 123 ) 123#123 = " r ( m / m ) + ( cid : 123 ) p123 ( x , , )
the geometric bridge of equation ( 123 ) results in
( x , , ) p123 ( x , , )
( x , , ) p123 ( x , , )
p123 ( x , , )
= s p123 ( x , , ) = s p123 ( x , , )
p123 ( x , , )
these expressions allow us to express bridged lis estimates in terms of the simple lis estimate of equation ( 123 ) , and its reverse version of equation ( 123 ) .
for the optimal bridge , we get
r ( m / m ) / r ( i )
lis + 123
r ( m / m ) + 123 / r ( i )
similarly , for the geometric bridge , we get
123 lis estimates with independent sampling with no intermediate distributions
it is interesting to look at the special case of linked importance sampling with n = 123 ie , in which the are no intermediate distributions between 123 and 123 in which the points from both 123 and 123 are sampled independently .
the lis procedure can then be simplied somewhat , and it is also possible to improve the lis estimate by averaging over the choice of link state .
such averaging is not
feasible when markov chain sampling is used , since choosing a dierent link state would require a new simulation of the markov transitions .
since we will sample points independently , there is no need to decide how many points will be sampled by the forward transitions and how many by the reverse transitions in steps ( 123a ) and ( 123b ) of the lis procedure .
we simply obtain a pair of samples consisting of points x123 , 123 , .
, x123 , k123 drawn independently from 123 , and points x123 , 123 , .
, x123 , k123 drawn independently from 123
we then randomly select a link state , indexed by , from among x123 , 123 , .
, x123 , k123 according to the following probabilities , which depend on the choice of a single bridge distribution , denoted by p ( x ) :
123 ( | x123 ) =
the lis estimate for r = z123 / z123 based on this pair of samples from 123 and 123 is
p123 ( x123 , k )
the superscript i is used here to indicate that this estimate is based on the ith pair of samples .
we can see that it is very similar to the bridge sampling estimate of equation ( 123 ) , except that the link state is included in both samples .
since these lis estimates are unbiased , we can average m of them to obtain a nal lis estimate .
we can also average the estimate of equation ( 123 ) over the random choice of link state , which is guaranteed to produce an estimate ( also unbiased ) with smaller mean - squared - error ( see schervish 123 , section 123 ) .
the result is
p123 ( x123 , ) .
" p ( x123 , )
p123 ( x123 , k )
k123 +123 " p ( x123 , )
averaging these estimates over m pairs of samples produces a nal estimate denoted by rlis - ave .
to use bridged lis in this context , we need to nd reverse estimates as well , but these reverse estimates neednt be independent of the forward estimates , since the asymptotic validity of the bridge sampling estimate of equation ( 123 ) does not depend on the samples x123 and x123 being independent .
accordingly , we can use the same samples from 123 and 123 for the forward and the reverse operations .
however , to perform reverse sampling , we need to have a sample of k123 +123 points drawn from 123 , the rst of which is ignored when performing forward sampling .
conversely , the rst of the k123 +123 points drawn from 123 is ignored when performing the reverse sampling .
we can improve the bridged lis estimates by averaging the numerator and the denominator of equation ( 123 ) or ( 123 ) with respect to the random choice of link state .
we can also average with
respect to the omission of one of the points from one of the samples ie , rather than omitting the rst of k123 + 123 points in the sample from 123 when computing a forward estimate , we average with respect to a random choice of point to omit , and similarly for reverse estimates .
note that the averaging should be done over the sums in the numerator and denominator , not with respect to the entire estimate , nor with respect to the values of r ( i ) lis appearing inside the summands .
the eective sample size after this additional averaging of dependent points is unclear , so it is not obvious what the ratio of sample sizes in equation ( 123 ) should be , but using ( k123 + 123 ) / ( k123 + 123 ) is probably
lis and r ( i )
123 analytical comparisons of ais and lis
in this section , i analyse ( somewhat informally ) the performance of ais and lis asymptotically , and in other situations where analytical results are possible .
123 asymptotic properties of ais and lis estimates
i begin by analysing the asymptotic performance of ais and lis when the sequence of distributions is dened by an unnormalized density function of the following form :
p ( x ) = p123 ( x ) exp ( u ( x ) )
this class includes sequences of canonical distributions dened by equation ( 123 ) in which the inverse temperature varies , as well as sequences that can be used for bayesian analysis , in which p123 denes the prior and is a power that the likelihood ( expressed as exp ( u ( x ) ) ) is raised to , with = 123 giving the posterior distribution .
for these distributions , we can express r using the well - known thermodynamic integration formula as follows :
r = log ( z123 / z123 ) = z 123
e ( u ) d
the analysis here is asymptotic , as the number of intermediate distributions used , given by n123 , goes to innity .
i will assume the j dening these distributions are chosen according to a scheme in which for any a ( 123 , 123 ) , the spacing j+123 j when j = a n is asymptotically proportional to 123 / n in other words , the relative density of intermediate distributions in the neighborhood of dierent values of stays the same as the overall density increases .
the simplest such scheme is to let j = j / n , though other schemes may sometimes be better .
with the above form for p , the ais estimate from a single run ( from equation ( 123 ) ) can be written
pj ( x ( i )
j ) ( cid : 123 ) =
( j+123 j ) u ( cid : 123 ) x ( i )
when j = j / n , this can be seen as a stochastic form of riemanns rule for numerically integrating equation ( 123 ) , though one dierence is that log rais converges to the correct value as m goes to innity even if n stays xed .
provided that there is some nite bound on the variance of u under all the distributions , and that the markov transitions used mix well , a central limit theorem will apply , allowing us to conclude that the distribution of n = log r ( i ) ais becomes gaussian as n goes to innity .
let the mean of n be n , and let the variance of n asymptotically be 123 / n , where is determined by details of the spacing of intermediate distributions and of the degree of autocorrelation in the markov transitions .
note that e ( y q ) = exp ( q + q123 123 / 123 ) when y = exp ( x ) and x is gaussian with mean and variance 123
using this , the mean of exp ( n ) is exp ( n + 123 / 123n ) .
this must equal r , since rais is unbiased , so n = log ( r ) 123 / 123n .
using this , we can see that the variance of r ( i ) ais = exp ( n ) is r ( exp ( 123 / 123n ) 123 ) , which for large n will be approximately r123 / 123n .
the variance of rais will therefore be r123 / 123nm .
asymptotically , the total computational eort , which will generally be proportional to nm , can be divided in any way between more intermediate distributions ( n ) or more runs ( m ) without aecting the accuracy of estimation of r , provided that n is kept large enough that these asymptotic results apply a fact noted by hendrix and jarzynski ( 123 ) .
we can therefore use a value of m greater than one without penalty , in order to obtain an error estimate from the degree of variation over the
for lis , we can write the log of the estimate from one run ( equation ( 123 ) ) as follows :
kj + 123
suppose that we let kj = mk 123 j for all j and some set of k 123 j , and that we then let m go to innity .
assuming that the variances of the ratios of probabilities are nite , and that the markov chain transitions used mix suciently well , a central limit theorem will again apply , and we can conclude that all of the n terms in the sum above , and therefore also the sum itself , will approach gaussian distributions , with variances proportional to 123 / m .
kj+123 + 123
to analyse the lis estimate in more detail , we need to assume a form of bridge distribution , as well as a form for p .
if p has the form of equation ( 123 ) and we use the geometric bridge of equation ( 123 ) , we can write
kj + 123
kj+123 + 123
exp ( ( j+123j ) u ( xj , k ) / 123 )
exp ( ( jj+123 ) u ( xj+123 , k ) / 123 )
since exp ( z ) 123 + z and log ( 123 + z ) z when z is small , we can rewrite this when n is large ( and
hence j+123j is small ) as
kj + 123
kj+123 + 123
u ( xj , k ) +
kj + 123
kj+123 + 123
kn + 123
k123 + 123
kj + 123
when j = j / n , this looks like a stochastic form of the trapezoidal rule for numerically integrating equation ( 123 ) .
since the trapezoidal rule converges faster than reimanns rule , one might expect lis to perform better than ais asymptotically , but this is not so in this stochastic situation .
suppose for simplicity that we set all kj = m .
the variance of log r ( i ) lis will be dominated by the variance of the last sum above , which will be proportional to 123 / nm , assuming that m is large , so that the dependence between terms ( from sharing link states ) is negligible .
using the same argument as for ais above , the variance of log rlis will be proportional to 123 / nmm .
considering that the computation time for an lis run will be proportional to nm , versus n for ais , we see that the variances of the ais and lis estimates go down the same way in proportion to computation time , asymptotically as n and m go to
furthermore , the proportionality constant should be the same for ais and lis , assuming that the overhead of the two procedures is negligible compared to the time spent performing markov transitions , so that the proportionality constants for computation time are the same for ais ( multiplying n ) and for lis ( multiplying nm ) .
the proportionality constants for variance for ais ( multiplying 123 / nm ) and for lis ( multiplying 123 / nmm ) depend in a complex way on the form of the density of j values and on the mixing properties of the markov transitions , but the result should be the same for ais and lis , provided the same scheme is used for choosing j values , and the same markov transitions are used , parameterized smoothly in terms of .
a dierence that might appear signicant is that for ais only one markov transition is done for each j , whereas for lis , m such transitions are done .
however , as n goes to innity , nearby distributions become more similar , so transitions for m consecutive distributions become similar to m transitions for one of these distributions .
the apparently pessimistic conclusion from this is that when both n and m ( and hence the kj ) are large , the performance of lis should be about the same as that of ais ( with n for ais chosen to
equalize the computation time ) , assuming that the distributions used have the form of equation ( 123 ) , that the variance of u is nite under all of the distributions , and that the markov transitions used mix well enough .
fortunately , however , there is no reason to make both m and n large with lis .
for good performance , n must be large enough that j and j+123 overlap signicantly , but there is no reason to make n much larger than this .
the accuracy of the estimates can be improved as desired by increasing m and / or m while keeping n xed .
the results below show that lis estimates with n xed are sometimes much better than ais estimates .
finally , let us consider the asymptotic performance of the bridged versions of ais and lis , assuming that the variance of u is nite , so that the distribution of the estimates from individual runs becomes gaussian as n ( for ais ) or m ( for lis ) goes to innity .
looking at equations ( 123 ) and ( 123 ) , which also are applicable to bridged ais estimates , we see that the log of r ( i ) lis - bridged can for both optimal and geometric bridges be expressed as the dierence of the log of the numerator , which is the mean of a function of the forward estimates , r ( i ) lis , and the log of the denominator , which is the mean of a function of the reverse estimates , r ( i ) lis .
if these forward and reverse estimates have gaussian distributions with small variances , 123 and 123 , then r ( i ) lis - bridged will also be gaussian , with a variance that can be computed in terms of the derivatives of the summands in the numerator and the denominator , with respect to lis and r ( i ) lis , evaluated at the true values of r and 123 / r .
i will assume that r = 123 below , as can be done without loss of generality .
lis = r = 123 and r ( i )
for the geometric bridge , these derivatives are both 123 / 123 , from which it follows that the variance of the numerator in equation ( 123 ) is 123 / 123m and that of the denominator is 123 / 123m .
since the numerator and denominator evaluate to one for r ( i ) lis = 123 / r = 123 , the sum of the variances of the logs of the numerator and denominator is 123 / 123m + 123 / 123m .
if 123 = 123 and m = m , this reduces to 123 / 123m .
the variance of an unbridged lis estimate will be 123 / m .
however , the bridged estimate requires time proportional to m + m , compared to just m for the unbridged estimate .
the value of m for the unbridged method can therefore be twice as large as for the bridged method , with the result that bridged and unbridged estimates perform equally well asymptotically ( assuming the variance of u is nite ) .
for the optimal bridge , the derivatives of the summands in the numerator and denominator are both 123 / 123 , when evaluated at r ( i ) lis = 123 / r = 123 , and assuming that m = m .
the numerator and denominator both evaluate to 123 / 123 , with the result that asymptotically the variance of the bridged estimate , assuming 123 = 123 , is 123 / 123m , the same as for the geometric bridge .
lis = r = 123 and r ( i )
in conclusion , bridged ais and lis estimates asymptotically have the same performance as the corresponding unbridged estimates ( with twice the value of m ) , for both the optimal and geometric bridges , assuming u has nite variance .
this conclusion applies more generally , as long as a central limit theorem holds for the individual estimates , r ( i ) lis .
however , the bridged methods may be much better when the variance of u is innite , or for classes of distributions other than that of equation ( 123 ) .
the bridged methods may also provide improvement when the values of n or m are not large enough for the asymptotic results to apply .
lis and r ( i )
123 properties of ais and lis when sampling from uniform distributions
in this section , i will demonstrate that when n is kept suitably small , lis can perform much better than ais when these methods are applied to sequences of uniform distributions .
as a rst example , consider the class of nested uniform distributions with unnormalized densities
p ( x ) = ( 123 if s < x < s
for which the normalizing constants are z = 123s , so that r = z123 / z123 = s .
the results concerning this class of distributions can easily be extended to any class of uniform distributions , in any number of dimensions , that have nested regions of support .
for both ais and lis , i will assume that the intermediate distributions are dened by j = j / n .
with this choice , the probability that a point , x , randomly sampled from j will have pj+123 ( x ) = 123 is s123 / n , for any j .
during an ais run , only a single point is sampled from each distribution .
an ais run will produce an estimate for r of zero if any of the ratios pj+123 ( x ( i ) j ) in equation ( 123 ) are zero , which happens with probability 123 ( s123 / n ) n = 123 s , and will otherwise produce an estimate of one .
note that the distribution of estimates is independent of n .
ais is therefore not a useful technique for nested uniform distributions simple importance sampling ( ie , ais with n = 123 ) would work just as well ( or just as poorly , if s is very small ) .
bridged ais produces no improvement in this context .
j ) / pj ( x ( i )
suppose instead we use lis with all kj = m , and suppose that the markov transitions , tj , produce points that are almost independent of the previous point .
for this problem , both the geometric and optimal forms of the bridge distribution result in pj j+123 ( x ) = pj+123 ( x ) .
if m + 123 points are sampled independently from j , the fraction of these points for which pj+123 ( x ) is one will have variance s123 / n ( 123 s123 / n ) / ( m+123 ) .
for suciently large m , the variance of the log of this fraction will be approximately ( s123 / n ( 123s123 / n ) / ( m+123 ) ) / s123 / n , which simplies to ( s123 / n123 ) / ( m+123 ) .
for this approximation to be useful , the probability that none of the m + 123 points sampled from j lie in the region where pj+123 is one , equal to ( 123 s123 / n ) m+123 , must be negligible .
this probability must be fairly small anyway , if lis is to perform well .
suppose that the computational cost of an lis run is proportional to the sum of the number of if we x this cost , the points sampled from 123 and the number of markov transitions performed .
number of intermediate distributions , n , and the number of transitions for each distribution , m , will be related by m ( n + 123 ) = c , for some constant c .
assume for the moment that both n and m are large .
the probability of a run producing a zero estimate will then be negligible , and we can assess the accuracy of the estimate for one run by the variance of log r ( i ) lis ( modied in some way to eliminate the innity resulting from the negligible , but non - zero , probability that r ( i ) lis is zero ) .
looking at equation ( 123 ) , we see that for these nested uniform distributions , the second log term vanishes pj j+123 ( xj+123 , k ) / pj+123 ( xj+123 , k ) is always one , since pj j+123 is the same as pj+123 .
when m is large , the dependence between terms with dierent values of j will be negligible , so we can add the variances of
the terms to get the variance of the estimate , obtaining the result that
var ( cid : 123 ) log r ( i )
lis ( cid : 123 ) n ( s123 / n123 ) / ( m+123 )
when n is large , s123 / n = exp ( log ( 123 / s ) / n ) is approximately 123 + log ( 123 / s ) / n , and hence the variance above is approximately log ( 123 / s ) / ( m+123 ) .
so it seems that the larger the value of m , the better until we reach a value of m for which the corresponding value of n , equal to c / m 123 , is small enough that this result no longer applies .
best performance will therefore come using a fairly small value of n , but a large value of m .
substituting m = c / ( n+123 ) into equation ( 123 ) , and assuming m / ( m+123 ) 123 , we get lis ( cid : 123 ) n ( s123 / n123 ) / ( c / ( n+123 ) ) = n ( n+123 ) ( s123 / n123 ) / c
var ( cid : 123 ) log r ( i )
the value of n that minimizes this depends only on s , not on c .
the optimal choice of n increases slowly as s gets smaller : s = 123 gives n = 123 , s = 123 gives n = 123 , s = 123 gives n = 123 , and s = 123 gives n = 123
as a second example , consider the class of non - nested uniform distributions with unnormalized
densities given by
p ( x ) = ( 123 if t 123 < x < t + 123
for this class , z = 123 for all , so r = z123 / z123 = 123
i will again assume that the intermediate distributions are dened by j = j / n , and that all kj = m .
assuming that n is greater than t / 123 , the probability that a point , x , randomly sampled from j will have pj+123 ( x ) = 123 is 123 t / 123n , for any j .
for this example , ais estimates do not converge to the true value of r as m increases , regardless of the value of n .
to see this , note that the ratios in equation ( 123 ) will all be either zero or one , and that the estimate from one run , r ( i ) ais , will be one if all of these ratios are one , and zero otherwise .
the probability of a particular ratio being one is 123 t / 123n , so the probability that all are one ( assuming the t produce points independent of the current point ) is ( 123 t / 123n ) n , which approaches exp ( t / 123 ) as n goes to innity .
the ais estimate , averaging over m runs , will have mean exp ( t / 123 ) , rather than the correct value of one .
in contrast , bridged ais estimates will converge to the true value as m increases , as long as n is at least t / 123 , so that there is overlap between successive distributions in the sequence .
however , when t is large , the overlap between the distributions over paths produced by forward and reverse ais runs , given by exp ( t / 123 ) , will be very small , and the procedure will be very inecient .
to see how well lis performs , recall the formula for log rlis from equation ( 123 ) :
kj + 123
kj+123 + 123
due to symmetry , the two log terms above have the same distribution , for all j .
the variance of one of these log terms ( for large m ) is ( ( t / 123n ) ( 123 t / 123n ) / ( m + 123 ) ) / ( 123 t / 123n ) 123 , which simplies to 123 / ( ( 123n / t123 ) ( m+123 ) ) .
the second log term in equation ( 123 ) for one j will involve the same points , xj+123 , k , as the rst log term for the next j .
the eect of this is that these terms will be negatively correlated , with correlation of 123 if n = t .
however , since the two terms occur with opposite signs , the eect on the nal sum is that n123 pairs of terms ( out of 123n terms total ) are positively correlated .
straightforward calculations show that this correlation is 123n / t 123 for t / 123 < n t and 123 / ( 123n / t 123 ) for n t .
using the fact that when x and y have the same distribution , var ( x + y ) = 123 var ( x ) ( 123 + cor ( x , y ) ) , we obtain the result that , for large m ,
var ( cid : 123 ) log r ( i )
( 123n / t123 ) ( m+123 ) ( n + ( n123 ) ( 123n / t 123 ) n + ( n123 ) / ( 123n / t 123 )
if t / 123 < n t if n t
setting m = c / ( n+123 ) , and assuming m / ( m+123 ) 123 , gives
var ( cid : 123 ) log r ( i )
c ( 123n / t123 ) ( n + ( n123 ) ( 123n / t 123 ) n + ( n123 ) / ( 123n / t 123 )
if t / 123 < n t if n t
numerical investigation shows that the global minimum of the variance occurs where n is near ( 123 / 123 ) t .
a second local minimum where n is near ( 123 / 123 ) t also exists .
the two minima are nearly equally good when t is large .
there is a local maximum where n is near t , with the variance there being about 123% greater than at the global minimum .
the variance is much larger for very large and very small values of n .
we therefore see that for this example too , the best results are obtained by xing n to a moderate value; any desired level of accuracy can then be obtained by increasing m and / or m .
123 empirical comparisons of ais and lis
the analytical results of the previous section indicate that lis can sometimes perform much better than ais , but that the benets of lis may only be seen when the number of intermediate distributions used is kept suitably small ( but not so small that they do not overlap ) .
in this section , i investigate the performance of ais and lis ( and their bridged versions ) empirically .
the programs used for these tests ( written in r ) are available from my web page .
these tests were done using sequences of one - dimensional distributions having unnormalized density
functions of the following form :
p ( x ) = exp ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
( xt ) / s ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
where s , t , and q are xed constants .
as moves from 123 to 123 , the centre of this distribution shifts by t , and changes width by the factor s .
the power q controls how thick the tails of the distributions are .
when q = 123 , the distributions are gaussian; a larger value produces lighter tails .
note that z is proportional to s , and hence r = z123 / z123 is equal to s .
q = 123 , t = 123 , s = 123
q = 123 , t = 123 , s = 123
q = 123 , t = 123 , s = 123
q = 123 , t = 123 , s = 123
q = 123 , t = 123 , s = 123
q = 123 , t = 123 , s = 123
figure 123 : the sequences of unnormalized density functions used for the tests .
the plots show the unnormalized density functions for = 123 , 123 / 123 , 123 / 123 , 123 / 123 , 123 , for six combinations of s , t , and q .
if t = 123 , the distributions can be written in the form of equation ( 123 ) , after reparameterizing in terms of = 123 / sq , so that p ( x ) = exp ( |x|q ) .
in this case , we expect the asymptotic behaviour to be as discussed in section 123 , but the behaviour with samples of practical size may be dierent .
as q goes to innity , the distributions converge to uniform distributions over ( ts , t+s ) , and the results of section 123 become relevant .
i did an initial set of tests using six sequences of distributions .
three of these sequences were of gaussian distributions , with q = 123
the rst of these used s = 123 and t = 123 , producing a shift with no change in scale as increases from 123 to 123
the second used s = 123 and t = 123 , producing a contraction with no shift .
the last used s = 123 and t = 123 , combining a shift with a contraction .
a second set of three sequences used the same values of s and t , but with q = 123 , which produces more rectangular distributions with lighter tails .
the six sequences are shown in figure 123
each sequence in these plots consists of ve distributions , corresponding to = 123 , 123 / 123 , 123 / 123 , 123 / 123 , 123
these were the sequences used for the lis runs ( hence n = 123 for these runs ) .
the ais runs used more distributions , spaced more nely with respect to , so as to produce the same number of markov transitions and sampling operations as in the lis runs .
these distributions ( for any ) can easily be sampled from using rejection sampling .
samples from 123 and 123 were used to initialize forward and reverse runs of ais and lis .
for this test , we pretend that sampling for other must be done using markov chain methods .
the transition used for , t , was a random - walk metropolis update , using a gaussian proposal distribution with mean equal to the
current point and standard deviation s .
since metropolis updates are reversible , t was the same .
two sets of forward and reverse lis runs were done with n = 123 , all kj = 123 , and m = 123 , one set using the geometric bridge , the other using the optimal bridge with the true value of r .
the forward estimates were computed from equation ( 123 ) ; the reverse estimates from equation ( 123 ) , which is equivalent to using the forward procedure with the reverse sequence of distributions .
bridged lis estimates were also found using equation ( 123 ) , with the value of r found by iteration .
to make the comparison with forward and reverse estimates fair , the bridged lis estimates used m = 123 ie , only half of the forward and half of the reverse runs were used , for a total of 123 runs .
a corresponding set of forward , reverse , and bridged ais runs were also done , with n = 123 and m = 123 ( m = 123 for the bridged estimates ) .
if sampling a point from 123 or 123 takes about the same computation time as a metropolis update , these ais runs will take about the same time as the lis runs .
( this assumes that sampling and markov transitions dominate the time , which is typically true for real problems but perhaps not for this simple test problem . )
sets of longer lis and ais runs were also done , which were the same as the sets above except that
for lis , kj = 123 for all j , and for ais , n = 123 , which again equalizes the computation time .
experience , together with the asymptotic results of section 123 , shows that estimates produced using a small value of m are better than , or at least as good as , those produced with larger m .
i chose m = 123 ( m = 123 for bridged estimates ) since this is about the smallest value that allows reliable estimation of standard errors , which would usually be needed in practice .
the standard errors for ais and lis estimates of r were estimated by the sample standard deviation of the r ( i ) divided by m .
when comparing the methods , i looked primarily at the mean squared error when estimating log ( r ) ( rather than when estimating r ) .
the estimate i used was log ( r ) , and the standard error for this estimate was estimated by the standard error for r divided by r .
for the reverse runs , log ( r ) was estimated by log ( r ) .
for bridged ais and lis , the standard errors for the log of the numerator and the log of the denominator of equation ( 123 ) were found , and the overall standard error was computed as the square root of the sum of the squares of these two standard errors .
this method of converting estimates and standard errors for r to those for log ( r ) is valid asymptotically .
it might be improved upon for nite samples , but such improvements would probably not aect the relative merits of the methods compared here .
figures 123 through 123 plot the mean squared errors of estimates for log ( r ) for the six sets of runs .
results are shown for ais , for lis using the geometric bridge , and for lis using the optimal bridge , with the true value of r .
results for both the forward and reverse versions of each method are shown , together with the bridged version , using the optimal bridge , with r obtained by iteration .
results for the short runs ( n = 123 , kj = 123 for lis , n = 123 for ais ) are on the left , and for the long runs ( n = 123 , kj = 123 for lis , n = 123 for ais ) on the right .
the mean squared error for each method was estimated by simulating each method 123 times , and comparing the estimates with the true value of log ( r ) .
the bars in the plots are dark up to the estimated mean squared error minus twice its standard
ais : n=123 , lis : n=123 m=123
ais : n=123 , lis : n=123 m=123
figure 123 : results of short and long runs on the distribution sequence with s = 123 , t = 123 , and q = 123
ais : n=123 , lis : n=123 m=123
ais : n=123 , lis : n=123 m=123
figure 123 : results of short and long runs on the distribution sequence with s = 123 , t = 123 , and q = 123
ais : n=123 , lis : n=123 m=123
ais : n=123 , lis : n=123 m=123
figure 123 : results of short and long runs on the distribution sequence with s = 123 , t = 123 , and q = 123
ais : n=123 , lis : n=123 m=123
ais : n=123 , lis : n=123 m=123
figure 123 : results of short and long runs on the distribution sequence with s = 123 , t = 123 , and q = 123
ais : n=123 , lis : n=123 m=123
ais : n=123 , lis : n=123 m=123
figure 123 : results of short and long runs on the distribution sequence with s = 123 , t = 123 , and q = 123
ais : n=123 , lis : n=123 m=123
ais : n=123 , lis : n=123 m=123
figure 123 : results of short and long runs on the distribution sequence with s = 123 , t = 123 , and q = 123
error , and are then light up to the estimated mean squared error plus twice its standard error .
for bars that extend above the plot the estimated mean squared error is shown at the top of the bar .
the results for translated sequences of distributions ( t = 123 and s = 123 ) are shown in figures 123 and 123
when the distributions are gaussian ( q = 123 ) , no advantage is seen for lis if anything , lis performs slightly worse than ais , particularly when the geometric bridge is used .
the forward and reverse forms of ais and lis should have identical performance for these distribution sequences , due to symmetry; any dierences seen result from random variation .
the bridged forms of both ais and lis perform better than the unbridged forward and reverse forms .
the advantage of bridging is less for the longer runs , however , as expected from the analysis at the end of section 123 .
when q = 123 , the distributions have much lighter tails than the gaussian , more closely resembling the uniform distributions analysed in section 123 .
for these sequences of distributions , lis performs substantially better than ais .
the unbridged version of ais does particularly badly .
the mean squared error for the bridged version of ais is about 123 times greater than for the bridged version of lis .
it makes little dierence whether the geometric or optimal bridge is used for lis .
figures 123 and 123 show the results for sequences of distributions with the same mean ( t = 123 ) but decreasing width ( s = 123 ) .
for these sequences , a modest advantage of lis over ais is apparent for the sequence of gaussian distributions ( q = 123 ) , with the variance for ais estimates being about a factor of 123 greater than for lis estimates with the geometric bridge , and about a factor of 123 greater than for lis estimates with the optimal bridge .
the reversed ais and lis estimates are somewhat worse than the forward estimates for this sequence of distributions .
no advantage is seen for bridged ais or lis estimates .
the results for the sequence of distributions with q = 123 is similar , except that the advantage of lis
over ais is much greater about a factor of 123
results for the last type of sequence , with s = 123 and t = 123 , are shown in figures 123 and 123
this problem is a hybrid of the previous two , with both translation and change in width , producing results intermediate between those for the previous two problems .
no dierence in performance between ais and lis is apparent for the gaussian distributions ( q = 123 ) , but the bridged forms of both perform slightly better .
for the sequence of distributions with q = 123 , a clear advantage of lis over ais can be seen , but this advantage is not as great as for the sequence with t = 123 and s = 123 .
the bridged forms of both ais and lis are again better , more so for the short runs than for the long runs .
in addition to looking at the mean squared error of estimates found with these methods , i also looked at the fraction of times that the estimate for log ( r ) diered from the true value by more than twice the standard error estimated using the m runs .
this should be approximately 123% if the distribution of estimates is gaussian , and the standard errors are accurate .
for the longer runs , this fraction was indeed near or only slightly above 123% for all methods , except for the unbridged ais runs when these performed very poorly .
for the shorter runs , however , the unbridged ais and lis methods produced estimates more than two standard errors from the mean around 123% of the time ( sometimes
much more often , when unbridged ais performed poorly ) .
both the bridged ais and the bridged lis methods gave more reliable standard errors .
however , it is possible that better standard errors for the unbridged methods might be obtained with a more sophisticated approach than i used .
i performed additional runs to verify and extend some of the analytic results from section 123
figures 123 and 123 show results obtained using lis with increasing numbers of intermediate distributions , starting with the value of n = 123 used for the tests above , and continuing to n = 123 , n = 123 , and n = 123 , while keeping the computation time constant by decreasing m in proportion to n+123
the two distribution sequences with s = 123 and t = 123 and with s = 123 and t = 123 were used , in both cases with q = 123
the sequence with t = 123 and s = 123 has the form of equation ( 123 ) , so in accordance with the analysis of section 123 , we expect that asymptotically , as n increases , lis and ais should have the same performance .
this is indeed what we see in figure 123
we also see the same behaviour for the sequence with t = 123 and s = 123 in figure 123
as q increases , the distributions become close to uniform , and the results of section 123 should apply .
to test this , i tried values of q = 123 , q = 123 , q = 123 , and q = 123 for the distribution sequence with s = 123 and t = 123 and the sequence with s = 123 and t = 123
results are shown in figures 123 and 123
( the results for q = 123 and q = 123 are the same as on the left in figures 123 to 123 , though the scale diers . )
for the sequences with s = 123 and t = 123 , the limiting uniform distributions have the form of the second example in section 123 .
as noted there , ais estimates do not converge to the correct value of r for this distribution sequence; bridged ais estimates do converge , but may be rather inecient .
we see analogous behaviour in figure 123 when q is large .
the mean squared error of the ais estimates increases approximately linearly with q over the range q = 123 to q = 123
the bridged ais estimates also get worse as q increases , but more slowly .
in contrast , the mean squared error of the lis estimates changes hardly at all as q increases .
the story is similar for sequences with s = 123 and t = 123 , for which the limiting uniform distributions correspond to those in the rst example of section 123 .
the lis estimates perform about equally well for all values of q , but the ais estimates are dramatically worse for large values of q .
for this sequence , reverse ais estimates are much worse than forward ais estimates , and bridging does not help .
according to the analysis of section 123 , the choice of choice of n = 123 for lis used above is not optimal for either of these distribution sequences when q is large .
for the sequence with s = 123 and t = 123 , using n = 123 should be better by a factor of 123 .
however , in lis runs with q = 123 , the mean squared error using n == 123 and m = 123 is indistinguishable from that using n = 123 and m = 123 , given the standard errors ( a factor of 123 or more should have been detectable ) .
of course , q = 123 does not give exactly uniform distributions , and these values of m may not be large enough for the asymptotic results to apply , especially since the markov transitions do not sample independently .
for the sequence with s = 123 and t = 123 , the results in section 123 indicate that using n = 123 should be better by a factor of 123 .
in this case , lis runs with q = 123 using n = 123 and m = 123 are better than runs using n = 123 and m = 123 by a factor of 123 , signicantly greater than one given the standard errors , but not signicantly dierent from the expected ratio of 123 .
ais : n=123 , lis : n=123 m=123
ais : n=123 , lis : n=123 m=123
ais : n=123 , lis : n=123 m=123
ais : n=123 , lis : n=123 m=123
figure 123 : results using increasing values of n for lis , while keeping computation time constant , for the distribution sequence with s = 123 , t = 123 , and q = 123
the same ais procedure was used for all plots , but results vary randomly .
ais : n=123 , lis : n=123 m=123
ais : n=123 , lis : n=123 m=123
ais : n=123 , lis : n=123 m=123
ais : n=123 , lis : n=123 m=123
figure 123 : results using increasing values of n for lis , while keeping computation time constant , for the distribution sequence with s = 123 , t = 123 , and q = 123
the same ais procedure was used for all plots , but results vary randomly .
figure 123 : results with increasing values of q , for sequences of distributions with s = 123 and t = 123
the ais runs used n = 123; the lis runs used n = 123 and m = 123 , requiring the same amount of computation .
figure 123 : results with increasing values of q , for sequences of distributions with s = 123 and t = 123
the ais runs used n = 123; the lis runs used n = 123 and m = 123 , requiring the same amount of
123 other applications of linked sampling
so far in this paper , i have focused on how linked importance sampling can be used to estimate ratios of normalizing constants .
lis can also be used to estimate expectations with respect to 123 , however , and in some applications , this may be its most important use .
linked sampling methods related to lis can also be applied in other ways .
i briey described these other applications here , outlining the use of linked sampling for dragging fast variables in some detail .
123 estimating expectations
the expectation of some function , a ( x ) , with respect to 123 can be estimated using simple importance sampling , with points drawn from 123 , as follows :
e123 ( a ( x ) ) = e123 ( cid : 123 ) a ( x )
p123 ( x ) ( cid : 123 )
p123 ( x ( i ) )
where x ( i ) , .
, x ( n ) are drawn from 123
like equation ( 123 ) , this estimate is valid only if no region having zero probability under 123 has non - zero probability under 123
the two factors of 123 / n of course cancel , but are included to emphasize the connection with the estimate for r = z123 / z123 , which is simply the denominator of the estimate above .
since lis can be viewed as simple importance sampling on an extended state space , with distribu - tions 123 and 123 dened by the forward and reverse procedures of section 123 , we can use equation ( 123 ) to estimate any quantity that can be expressed as an expectation with respect ot 123
step ( 123 ) of the reverse procedure dening 123 sets xn , n to a value randomly chosen from n = 123
step ( 123 ) then sets the other xn , k to values obtained from xn , n by applying markov chain transitions that leave 123 invariant .
it follows that under 123 , all the points xn , k have marginal distribution 123 ( though they may not be independent ) .
accordingly ,
e123 ( a ( x ) ) = e 123 "
estimating the right side as in equation ( 123 ) , and using the fact that the ratio of probabilities under 123 over those under 123 is given by r ( i )
lis in equation ( 123 ) , we get the estimate
if the m runs of lis are started by sampling independently from 123 ( as will often be possible ) , the standard error of this estimate can be assessed in the usual fashion for importance sampling , as i have discussed for the analogous ais estimates in ( neal 123 ) .
this error assessment can be dicult , since when some r ( i ) lis is hard to estimate .
note , however , that the degree to which the markov chain transitions used have converged need not be assessed , a
lis are much larger than others , the variance of r ( i )
possible advantage compared with simple mcmc estimates .
the estimate of equation ( 123 ) will be asymptotically correct ( as m ) regardless of how far these markov chain transitions are from
the primary reason one might wish to use lis to estimate expectations is that going through the sequence of distributions parameterized by 123 , .
, n may produce an annealing eect , which prevents the markov chain sampler from being trapped in a local mode of the distribution .
compared with the analogous ais procedure , lis may perform better for some forms of distributions , for the same reasons as were discussed in sections 123 and 123
one should also note that lis estimates for expectations with respect to j for all j can easily be obtained from a single set of runs , by simply considering the results of each lis run up to the point where the sample for j is obtained .
123 a linked form of tempered transitions
my tempered transition method ( neal 123 ) is another approach to sampling from distributions with isolated modes , between which movement is dicult for markov chain transitions such as simple in this approach , such simple markov chain transitions are supplemented by occasional complex tempered transitions , composed of many simple markov chain transitions .
a tempered transition consists of several stages , which proceed through a sequence of distributions , from the distribution being sampled , to a higher temperature distribution in which movement between modes is easier , and then back down to the distribution being sampled .
at each stage of a tempered transition , we generate a single new state by applying a markov chain transition to the current state , after which we switch to the next distribution in the sequence .
the second half of a tempered transition is similar to an annealed importance sampling run , while the rst half is similar to an ais run with the reversed sequence of distributions .
a similar linked procedure can be dened , in which at each stage we generate a chain of states by applying a markov chain transition .
we then select a link state from this sequence ( using a suitable bridge distribution ) which serves as the starting point for the chain of states generated in the next stage .
in the nal stage , a chain of states is produced using a markov chain transition that leaves the distribution being sampled invariant , and a candidate state is selected uniformly at random from this chain .
the appropriate probability for accepting this candidate state is computed using ratios similar to those going into the lis estimate of equation ( 123 ) .
as discussed in section 123 , for ais to work well , all distributions in the sequence must assign reasonably high probability to regions of the space that have non - negligible probability under the next distribution in the sequence .
one would expect tempered transitions to work well only when this holds for both the sequence and its reversal .
in contrast , one would expect the linked version of tempered transitions to work well as long as the sequence satises the weaker condition that there be some overlap between adjacent distributions ( assuming a suitable bridge distribution is used ) .
123 dragging fast variables using linked chains
a slight modication of the tempered transition method can be applied to problems in which the state is composed of both fast and slow variables .
we will write the distribution of interest for such a
( x , y ) = ( 123 / z ) exp ( u ( x , y ) )
where x denotes the fast variables and y the slow variables .
we assume that the computation is dominated by the time required to evaluate u ( x , y ) , but that once u ( x , y ) has been evaluated , with relevant intermediate quantities saved , evaluating u ( x , y ) for any new x is much faster than evaluating u ( x , y ) for some y not previously encountered .
one example of such a problem is inference for gaussian process classication models ( neal 123 ) , in which y consists of the hyperparameters dening the covariance function used , and x consists of the latent variables associated with the n observations .
after a change to y , we must recompute the cholesky decomposition of an n n covariance matrix , which takes time proportional to n123 , whereas after a change to x only , u ( x , y ) can be re - computed in time proportional to n123 , assuming the cholesky decomposition for this value of y has been saved .
in my method for dragging fast variables ( neal 123 ) , the ability to quickly re - evaluate u ( x , y ) when only x changes is exploited to allow larger changes to be made to y than would be possible if x were kept xed , or were given a new value from some simple proposal distribution .
from the state ( x123 , y123 ) , a dragging update proposes a new value y123 , drawn from some symmetrical proposal distribution , in conjunction with a new value x123 that is found by applying a succession of markov chain updates that leave invariant distributions in the series , j ( x ) , for j = 123 , .
, n 123 , with 123 < j < j+123 < 123
the proposed state , ( x123 , y123 ) , is then accepted or rejected in a fashion analogous to
the distributions in the sequence used are dened by the following unnormalized probability or
density function , which depends on the current and proposed values for y :
p ( x ) = exp ( ( ( 123 ) u ( x , y123 ) + u ( x , y123 ) ) )
the corresponding normalized probability or density function will be written as .
note that 123 ( x ) = ( x|y123 ) and 123 ( x ) = ( x|y123 ) .
crucially , after u ( x , y123 ) and u ( x , y123 ) have been evaluated once ( for any x ) , we can evaluate p ( x ) for any and any x without any further slow computations .
indeed , since u ( x123 , y123 ) will usually have already been evaluated as part of the previous markov chain transition , only one slow computation will be required to evaluate p ( x ) for any number of values of and x .
a linked dragging update can be dened as follows .
given the sequence of distributions dened by 123 , .
, n , with 123 = 123 and n = 123 , the numbers of transitions ( t or t ) to perform for each distribution over x , denoted by k123 , .
, kn , and a set of bridge distributions , denoted by pj j+123 , for j = 123 , .
, n123 , an update from the current state ( x123 , y123 ) is done as follows :
the linked dragging procedure
123 ) propose a new value , y123 , from some proposal distribution s ( y123|y123 ) , which satises the symmetry
condition that s ( y123|y123 ) = s ( y123|y123 ) .
123 ) pick an integer 123 uniformly at random from ( 123 , .
, k123 ) , and then set x123 , 123 to the current values
of the fast variables , x123
123 ) for j = 123 , .
, n , create a chain of values for x associated with j as follows :
a ) if j > 123 : pick an integer j uniformly at random from ( 123 , .
, kj ) , and then set xj , j to
b ) for k = j + 123 , .
, kj , draw xj , k according to the forward markov chain transition prob -
abilities tj ( xj , k123 , xj , k ) .
( if j = kj , do nothing in this step . )
c ) for k = j 123 , .
, 123 , draw xj , k according to the reverse markov chain transition probabil -
ities t j ( xj , k+123 , xj , k ) .
( if j = 123 , do nothing in this step . )
d ) if j < n : pick a value for j from ( 123 , .
, kj ) according to the following probabilities
123 ( j | xj ) =
pj j+123 ( xj , j )
pj ( xj , j ) .
and then set xj j+123 to xj , j .
123 ) set n to a value chosen uniformly at random from ( 123 , .
, kn ) , and let the proposed new values
for the fast variables , x123 , be equal to xn , n .
123 ) accept ( x123 , y123 ) as the new state with probability
kj + 123
pj ( xj , k ) .
kj+123 + 123
if ( x123 , y123 ) is not accepted , the new state is the same as the old state , ( x123 , y123 ) .
one can show that this update leaves ( x , y ) invariant by showing that it satises detailed balance , which in turns follows from the stronger property that the probability of starting at ( x123 , y123 ) , assuming this start state comes from ( x , y ) , then generating the various quantities produced by the above procedure , and nally accepting ( x123 , y123 ) as the new state , is the same as the probability of starting this procedure at ( x123 , y123 ) , generating the same quantities in reverse , and nally accepting ( x123 , y123 ) .
the proof of this is analogous to the derivation of lis in section 123
to use the linked dragging procedure , we need to select suitable bridge distributions .
since the characteristics of ( x ) will depend on y123 and y123 , and of course , we may not know enough to select good estimates for the values of r needed to use the optimal bridge of equation ( 123 ) , though we might
try just setting r to one .
this is not a problem for the geometric bridge of equation ( 123 ) , for which the acceptance probability above can be written as
kj + 123
xk=123 s pj+123 ( xj , k ) pj ( xj , k ) .
kj+123 + 123
xk=123 s pj ( xj+123 , k )
from equation ( 123 ) , we see that
= exp ( ( j+123j ) ( u ( xj , k , y123 ) u ( xj , k , y123 ) ) )
= exp ( ( j+123j ) ( u ( xj+123 , k , y123 ) u ( xj+123 , k , y123 ) ) )
for the simplest case with no intermediate distributions ( ie , with n = 123 ) , the acceptance probability
k123 + 123
k123 + 123
exp ( ( u ( xj , k , y123 ) u ( xj , k , y123 ) ) / 123 )
exp ( ( u ( xj , k , y123 ) u ( xj , k , y123 ) ) / 123 )
123 conclusions and future work
in this paper , i have demonstrated that in some situations linked importance sampling is substan - tially more ecient than annealed importance sampling , provided a suitable number of intermediate distributions are used .
however , in other situations , where the tails of the distributions involved are suciently heavy , the two methods are about equally ecient .
more research is therefore needed to determine for which problems of practical interest lis , and related linked sampling methods , will be
in tests on multivariate gaussian distributions , i have not seen an advantage for lis over ais .
both perform about equally well on a sequence of 123 - dimensional spherical gaussian distributions with variances changing by a factor of two , so that log ( r ) = 123
this is in accord with the results in section 123 , where lis had little or no advantage over ais when the distributions were gaussian .
lis is more likely to be useful for problems involving continuous distributions with lighter tails .
one problem that may benet from lis is that of computing the probability of a very rare event , which can be cast as computing the normalizing constant for a distribution with the constraint that the state be in the set corresponding to this event .
intermediate distributions might use looser forms of this constraint .
if , in all these distributions , states violating the constraints have zero probability , ais will tend to have the same bad behaviour seen with uniform distributions in section 123 , while lis may work much better .
another context where lis may outperform ais is when only a xed number of intermediate distributions are available ie , only a nite number of values are allowed for .
this is the situation for the sequential importance sampler of maceachern , clyde , and liu ( 123 ) , which can be seen as an instance of ais ( neal 123 ) .
here , the intermediate distributions use only a fraction of the n items in the data set; such a fraction can only have the form j / n with j an integer .
the distance between successive distributions for this problem may sometimes be too great for ais to work well , but their overlap might nevertheless be sucient for lis .
it may be possible to improve lis by reducing the variance in how well it samples at each stage .
instead of performing a predetermined number , kj , of markov transitions at stage j , we might instead perform as many transitions as are necessary to obtain a good sample .
dene a tour to be a sequence of transitions that moves from a high value of some key quantity ( eg , u ( x ) for the canonical distributions of equation ( 123 ) ) to a low value of this quantity , or vice versa .
good sampling might be ensured by performing some predetermined number of tours , with the number of these tours that occur before and after the link state being chosen at random .
suitable high and low values would probably need to be found using preliminary runs .
more speculatively , it seems as if there should be some method that has the advantages of lis over ais , but that like ais uses many intermediate distributions , performing only a single markov transition for each .
intuitively , it seems that such a smooth method that does not abruptly change should be more ecient .
one can use lis with all kj set to one , but this will produce good results only if n is large , which we saw in the analysis of section 123 does not lead to an advantage over ais .
perhaps some way could be found of using states associated with all values of when estimating each of the ratios zj+123 / zj , while still producing an estimate that is exactly unbiased even when the markov transitions do not reach equilibrium .
this research was supported by the natural sciences and engineering research council of canada .
i hold a canada research chair in statistics and machine learning .
