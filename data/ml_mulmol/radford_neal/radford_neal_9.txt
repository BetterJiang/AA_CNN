we introduce a new nonlinear model for classication , in which we model the joint distribution of response variable , y , and covariates , x , non - parametrically using dirichlet process mixtures .
we keep the relationship between y and x linear within each component of the mixture .
the overall relationship becomes nonlinear if the mixture contains more than one component , with different regression coefcients .
we use simulated data to compare the performance of this new approach to alternative methods such as multinomial logit ( mnl ) models , decision trees , and support vector machines .
we also evaluate our approach on two classication problems : identifying the folding class of protein sequences and detecting parkinsons disease .
our model can sometimes improve predictive accuracy .
moreover , by grouping observations into sub - populations ( i . e . , mixture com - ponents ) , our model can sometimes provide insight into hidden structure in the data .
keywords : mixture models , dirichlet process , classication
in regression and classication models , estimation of parameters and interpretation of results are easier if we assume that distributions have simple forms ( e . g . , normal ) and that the relationship between a response variable and covariates is linear .
however , the performance of such a model depends on the appropriateness of these assumptions .
poor performance may result from assum - ing wrong distributions , or regarding relationships as linear when they are not .
in this paper , we introduce a new model based on a dirichlet process mixture of simple distributions , which is more exible in capturing nonlinear relationships .
a dirichlet process , d ( g123 , g ) , with baseline distribution g123 and scale parameter g , is a dis - tribution over distributions .
ferguson ( 123 ) introduced the dirichlet process as a class of prior distributions for which the support is large , and the posterior distribution is manageable analyti - cally .
using the polya urn scheme , blackwell and macqueen ( 123 ) showed that the distributions sampled from a dirichlet process are discrete almost surely .
the idea of using a dirichlet process as the prior for the mixing proportions of a simple dis - tribution ( e . g . , gaussian ) was rst introduced by antoniak ( 123 ) .
in this paper , we will describe the dirichlet process mixture model as a limit of a nite mixture model ( see neal , 123 , for further description ) .
suppose exchangeable random values y123 , . . . , yn are drawn independently from some
c ( cid : 123 ) 123 babak shahbaba and radford neal .
shahbaba and neal
unknown distribution .
we can model the distribution of y as a mixture of simple distributions , with probability or density function
here , pc are the mixing proportions , and f ( y , f ) is the probability or density for y under a distribu - tion , f ( f ) , in some simple class with parameters f for example , a normal in which f = ( , s ) .
we rst assume that the number of mixing components , c , is nite .
in this case , a common prior for pc is a symmetric dirichlet distribution , with density function
p ( p123 , . . . , pc ) =
g ( g ) g ( g / c ) c
where pc 123 and ( cid : 123 ) pc = 123
the parameters f c are independent under the prior , with distribution g123
we can use mixture identiers , ci , and represent this model as follows :
yi|ci , f f ( f ci ) ,
ci|p123 , . . . , pc discrete ( p123 , . . . , pc ) ,
p123 , . . . , pc dirichlet ( g / c , . . . . , g / c ) ,
f c g123
by integrating over the dirichlet prior , we can eliminate the mixing proportions , pc , and obtain the following conditional distribution for ci :
p ( ci = c|c123 , . . . , ci123 ) =
nic + g / c i 123 + g
here , nic represents the number of data points previously ( i . e . , before the ith ) assigned to component c .
as we can see , the above probability becomes higher as nic increases .
when c goes to innity , the conditional probabilities ( 123 ) reach the following limits :
p ( ci = c|c123 , . . . , ci123 )
p ( ci 123= c j for all j < i|c123 , . . . , ci123 )
i 123 + g , i 123 + g .
as a result , the conditional probability for q
i , where q
i = f ci , becomes
i|q 123 , . . . , q
i 123 + g
i 123 + g g123 ,
is a point mass distribution at q
we can regard any observation , i , as the last observation and write the conditional probability of q given the other q
since the observations are assumed to be exchangeable ,
j for j 123= i ( written q i ) as follows :
n 123 + g
n 123 + g g123
nonlinear models using dirichlet process mixtures
the above conditional probabilities are equivalent to the conditional probabilities for q to the dirichlet process mixture model ( as presented by blackwell and macqueen , 123 , using the polya urn scheme ) , which has the following form :
g d ( g123 , g ) .
i = f ci , the limit of model ( 123 ) as c
that is , if we let q becomes equivalent to the dirichlet process mixture model ( ferguson , 123; neal , 123 ) .
in model ( 123 ) , g is the distribution over q s , and has a dirichlet process prior , d .
phrased this way , each data point , i , has its own parameters , i , drawn independently from a distribution that is drawn from a dirichlet process prior .
but since distributions drawn from a dirichlet process are discrete ( almost surely ) as shown by blackwell and macqueen ( 123 ) , the q
i for different data points may be the same .
the parameters of the dirichlet process prior are g123 , a distribution from which q s are sampled , and g , a positive scale parameter that controls the number of components of the mixture that will be represented in the sample , such that a larger g results in a larger number of components .
to illustrate the effect of g on the number of mixture components in a sample of size 123 , we generated samples from four different dirichlet process priors with g = 123 , 123 , 123 , 123 , and the same baseline distribution g123 = n123 ( 123 , 123i123 ) ( where i123 is a 123 123 identity matrix ) .
for a given value of g , we rst i , where i = 123 , . . . , 123 , according to the conditional probabilities ( 123 ) , and then we sample i , 123i123 ) .
the data generated according to these priors are shown in figure 123
as we can see , the ( prior ) expected number of components in a nite sample increases as g becomes larger .
with a dirichlet process prior , we can we nd conditional distributions of the posterior distribu - tion of model parameters by combining the conditional prior probability of ( 123 ) with the likelihood
i|q i , yi ( cid : 123 )
j + rihi ,
where hi is the posterior distribution of q based on the prior g123 and the single data point yi , and the values of the qi j and ri are dened as follows :
qi j = b f ( yi , q ri = bg z f ( yi , q ) dg123 ( q ) .
here , b is such that ri + ( cid : 123 ) when sampling the posterior using mcmc methods , as discussed further in section 123
j123=i qi j = 123
these conditional posterior distributions are what are needed
bush and maceachern ( 123 ) , escobar and west ( 123 ) , maceachern and muller ( 123 ) , and neal ( 123 ) have used dirichlet process mixture models for density estimation .
muller et al .
( 123 ) used this method for curve tting .
they model the joint distribution of data pairs ( xi , yi ) as a dirich - let process mixture of multivariate normals .
the conditional distribution , p ( y|x ) , and the expected value , e ( y|x ) , are estimated based on this distribution for a grid of xs ( with interpolation ) to obtain a nonparametric curve .
the application of this approach ( as presented by muller et al . , 123 ) is restricted to continuous variables .
moreover , this model is feasible only for problems with a small number of covariates , p .
for data with moderate to large dimensionality , estimation of the joint
shahbaba and neal
figure 123 : data sets of size n = 123 generated according to four different dirichlet process mix - ture priors , each with the same baseline distribution , g123 = n123 ( 123 , 123i123 ) , but different scale parameters , g .
as g increases , the expected number of components present in the sam - ple becomes larger .
( note that , as can be seen above , when g is large , many of these components have substantial overlap . )
distribution is very difcult both statistically and computationally .
this is mostly due to the dif - culties that arise when simulating from the posterior distribution of large full covariance matrices .
in this approach , if a mixture model has c components , the set of full covariance matrices have cp ( p + 123 ) / 123 parameters .
for large p , the computational burden of estimating these parameters might be overwhelming .
estimating full covariance matrices can also cause statistical difculties since we need to assure that covariance matrices are positive semidenite .
conjugate priors based the inverse wishart distribution satisfy this requirement , but they lack exibility ( daniels and kass , 123 ) .
flat priors may not be suitable either , since they can lead to improper posterior distributions , and they can be unintentionally informative ( daniels and kass , 123 ) .
a common approach to ad - dress these issues is to use decomposition methods in specifying priors for full covariance matrices ( see for example , daniels and kass , 123; cai and dunson , 123 ) .
although this approach has demonstrated some computational advantages over direct estimation of full covariance matrices , it is not yet feasible for high - dimensional variables .
for example , cai and dunson ( 123 ) recommend their approach only for problems with less than 123 covariates .
nonlinear models using dirichlet process mixtures
we introduce a new nonlinear bayesian model , which also nonparametrically estimates p ( x , y ) , the joint distribution of the response variable y and covariates x , using dirichlet process mixtures .
within each component , we assume the covariates are independent , and model the dependence between y and x using a linear model .
therefore , unlike the method of muller et al .
( 123 ) , our approach can be used for modeling data with a large number of covariates , since the covariance matrix for one mixture component is highly restricted .
using the dirichlet process as the prior , our method has a built - in mechanism to avoid overtting since the complexity of the nonlinear model is controlled .
moreover , this method can be used for categorical as well as continuous response variables by using a generalized linear model instead of the linear model .
the idea of building a nonlinear model based on an ensemble of simple linear models has been explored extensively in the eld of machine learning .
jacobs et al .
( 123 ) introduced a supervised learning procedure for models that are comprised of several local models ( experts ) each handling a subset of data .
a gating network decides which expert should be used for a given data point .
for inferring the parameters of such models , waterhouse et al .
( 123 ) provided a bayesian framework to avoid over - tting and noise level under - estimation problems associated with traditional maximum likelihood inference .
rasmussen and ghahramani ( 123 ) generalized mixture of experts models by using innitely many nonlinear experts .
in their approach , each expert is assumed to be a gaussian process regression model , and the gating network is based on an input - dependent adaptation of dirichlet process .
meeds and osindero ( 123 ) followed the same idea , but instead of assuming that the covariates are xed , they proposed a joint mixture of experts model over covariates and response
our focus here is on classication models with a multi - category response , in which we have observed data for n cases , ( x123 , y123 ) , . . . , ( xn , yn ) .
here , the class yi has j possible values , and the covari - ates xi can in general be a vector of p covariates .
we wish to classify future cases in which only the covariates are observed .
for binary ( j = 123 ) classication problems , a simple logistic model can be used , with class probabilities dened as follows ( with the case subscript dropped from x and y ) :
p ( y = 123|x , a
, b ) =
exp ( a + xtb )
123 + exp ( a + xtb )
when there are three or more classes , we can use a generalization known as the multinomial logit ( mnl ) model ( called softmax in the machine learning literature ) :
p ( y = j|x , a
, b ) =
j + xtb
j + xtb
mnl models are discriminative , since they model the conditional distribution p ( y|x ) , but not the distribution of covariates , p ( x ) .
in contrast , our dpmnl model is generative , since it estimates the joint distribution of response and covariates , p ( x , y ) .
the joint distribution can be decompose into the product of the marginal distribution p ( x ) and the conditional distribution p ( y|x ) ; that is , p ( x , y ) = p ( x ) p ( y|x ) .
generative models have several advantages over discriminative models ( see for example , ulusoy and bishop , 123 ) .
they provide a natural framework for handling missing data or partially labeled data .
they can also augment small quantities of expensive labeled data with large quantities of cheap unlabeled data .
this is especially useful in applications such as document labeling and image analysis , where it may provide better predictions for new feature patterns not present in the data at
shahbaba and neal
figure 123 : an illustration of our model for a binary ( black and white ) classication problem with two covariates .
here , the mixture has two components , which are shown with circles and squares .
in each component , an mnl model separates the two classes into black or white with a linear decision boundary .
the overall decision boundary , which is a smooth function , is not shown in this gure .
the time of training .
for example , the latent dirichlet allocation ( lda ) model proposed by blei et al .
( 123 ) is a well - dened generative model that performs well in classifying documents with previously unknown patterns .
while generative models are quite successful in many problems , they can be computationally in - tensive .
moreover , nding a good ( but not perfect ) estimate for the joint distribution of all variables ( i . e . , x and y ) does not in general guarantee a good estimate of decision boundaries .
by contrast , discriminative models are often computationally fast and are preferred when the covariates are in fact non - random ( e . g . , they are xed by an experimental design ) .
using a generative model in our proposed method provides an additional benet .
modeling the distribution of covariates jointly with y allows us to implicitly model the dependency of covariates on each other through clustering ( i . e . , assigning data points to different components ) , which could provide insight into hidden structure in the data .
to illustrate this concept , consider figure 123 where the objective is to classify cases into black or white .
to improve predictive accuracy , our model has divided the data into two components , shown as squares and circles .
these components are distinguished primarily by the value of the second covariate , x123 , which is usually positive for squares and negative for circles .
for cases in the squares group , the response variable strongly depends on both x123 and x123 ( the linear separator is almost diagonal ) , whereas , for cases in the circles group , the model mainly depends on x123 alone ( the linear model is almost vertical ) .
therefore , by grouping the data into sub - populations ( e . g . , circles and squares in this example ) , our model not only improves classication accuracy , but also discovers hidden structure in the data ( i . e . , by clustering covariate observations ) .
this concept is briey discussed in section 123 , where we use our model to predict parkinsons disease .
a more detailed discussion on using our method to detect hidden structure in
nonlinear models using dirichlet process mixtures
data is provided elsewhere ( shahbaba , 123 ) , where a dirichlet process mixture of autoregressive models us used to analyze time - series processes that are subject to regime changes , with no specic economic theory about the structure of the model .
the next section describes our methodology .
in section 123 , we illustrate our approach and evalu - ate its performance on simulated data .
in section 123 , we present the results of applying our model to an actual classication problem , which attempts to identify the folding class of a protein sequence based on the composition of its amino acids .
section 123 discusses another real classication prob - lem , where the objective is to detect parkinsons disease .
this example is provided to show how our method can be used not only for improving prediction accuracy , but also for identifying hidden structure in the data .
finally , section 123 is devoted to discussion and future directions .
we now describe our classication model , which we call dpmnl , in detail .
we assume that for each case we observe a vector of continuous covariates , x , of dimension p .
the response variable , y , is categorical , with j classes .
to model the relationship between y and x , we non - parametrically model the joint distribution of y and x , in the form p ( x , y ) = p ( x ) p ( y|x ) , using a dirichlet process mixture .
within each component of the mixture , the relationship between y and x ( i . e . , p ( y|x ) ) is expressed using a linear function .
the overall relationship becomes nonlinear if the mixture contains more than one component .
this way , while we relax the assumption of linearity , the exibility of the relationship is controlled .
each component in the mixture model has parameters q = ( , s 123 , a
the distribution of x within a component is multivariate normal , with mean vector and diagonal covariance , with the vector s 123 on the diagonal .
the distribution of y given x within a component is given by a multinomial logit ( mnl ) modelfor j = 123 ,
p ( y = j|x , a
, b ) =
j + xtb
j + xtb
j is scalar , and b
this representation of the mnl model is redundant , since one of the b
the parameter a j is a vector of length p .
note that given x , the distribution of y does not depend on and s ( where j = 123 , . . . , j ) can be set to zero without changing the set of relationships expressible with the model , but removing this redundancy would make it difcult to specify a prior that treats all classes symmetrically .
in this parameterization , what matters is the difference between the parameters of
in addition to the mixture view , which follows equation ( 123 ) with c
observation , i , as having its own parameter , q a dirichlet process , as in equation ( 123 ) :
, one can also view each i , drawn independently from a distribution drawn from
i|g g , for i = 123 , .
, n , g d ( g123 , g ) .
since g will be discrete , many groups of observations will have identical q components in the mixture view .
i , corresponding to
although the covariates in each component are assumed to be independent with normal priors , this independence of covariates exists only locally ( within a component ) .
their global ( over all
shahbaba and neal
components ) dependency is modeled by assigning data to different components ( i . e . , clustering ) .
the relationship between y and x within a component is captured using an mnl model .
therefore , the relationship is linear locally , but nonlinear globally .
we could assume that y and x are independent within components , and capture the dependence between the response and the covariates by clustering too .
however , this may lead to poor per - formance ( e . g . , when predicting the response for new observations ) if the dependence of y on x is difcult to capture using clustering alone .
alternatively , we could also assume that the covariates are dependent within a component .
for continuous response variables , this becomes equivalent to the model proposed by muller et al .
( 123 ) .
if the covariates are in fact dependent , using full co - variance matrices ( as suggested by muller et al . , 123 ) could result in a more parsimonious model since the number of mixture component would be smaller .
however , as we discussed above , this approach may be practically infeasible for problems with a moderate to large number of covariates .
we believe that our method is an appropriate compromise between these two alternatives .
we dene g123 , which is a distribution over q = ( , s 123 , a
, b ) , as follows :
l|123 , s 123 n ( 123 , s 123 l ) |ms , vs n ( ms , v 123s ) ,
j|t n ( 123 , t 123 ) , jl|n n ( 123 , n 123 ) .
the parameters of g123 may in turn depend on higher level hyperparameters .
for example , we can regard the variances of coefcients as hyperparameters with the following priors :
log ( t 123 ) |mt , vt n ( mt , v 123t ) , log ( n 123 ) |mn , vn n ( mn , v 123n ) .
we use mcmc algorithms for posterior sampling .
we could use gibbs sampling if g123 is the conjugate prior for the likelihood given by f .
that is , we would repeatedly draw samples from i|q i , yi ( where i = 123 , . . . , n ) using the conditional distribution ( 123 ) .
neal ( 123 ) presented several algorithms for sampling from the posterior distribution of dirichlet process mixtures when non - conjugate priors are used .
throughout this paper , we use gibbs sampling with auxiliary parameters ( neals algorithm 123 ) .
this algorithm uses a markov chain whose state consists of c123 , . . . , cn and f = ( f c : c ( c123 , . . . , cn ) ) , so that q i = f ci .
in order to allow creation of new clusters , the algorithm temporarily supplements the f c parameters of existing clusters with m ( or m 123 ) additional parameter values drawn from the prior , where m a postive integer that can be adjusted to give good performance .
each iteration of the markov chain simulation operates as follows :
for i = 123 , . . . , n : let k be the number of distinct c j for j 123= i and let h = k + m .
label these c j with values in ( 123 , . . . , k ) .
if ci = c j for some j 123= i , draw values independently from g123 for those f c for which k < c h .
if ci 123= c j for all j 123= i , let ci have the label k + 123 , and draw values independently from g123 for those f c where k + 123 < c h .
draw a new value for ci from ( 123 , . . . , h ) using the following probabilities :
p ( ci = c|ci , yi , f 123 , . . . , f h ) =
n 123 + g f ( yi , f c ) n 123 + g f ( yi , f c )
for 123 c k ,
for k < c h ,
nonlinear models using dirichlet process mixtures
where ni , c is the number of c j for j 123= i that are equal to c , and b is the appropriate normalizing constant .
change the state to contain only those f c that are now associated with at least to
for all c ( c123 , . . . , cn ) draw a new value from the distribution f c | ( yi such that ci = c ) , or
perform some update that leaves this distribution invariant .
throughout this paper , we set m = 123
this algorithm resembles one proposed by maceachern and muller ( 123 ) , with the difference that the auxiliary parameters exist only temporarily , which avoids an inefciency in maceachern and mullers algorithm .
samples simulated from the posterior distribution are used to estimate posterior predictive prob - abilities .
for a new case with covariates x , the posterior predictive probability of the response variable , y , is estimated as follows :
p ( y = j|x ) =
p ( y = j , x )
p ( y = j , x ) =
p ( y = j , x|g123 , q ( s ) ) ,
here , s is the number of post - convergence samples from mcmc , and q ( s ) represents the set of parameters obtained at iteration s .
alternatively , we could predict new cases using p ( y = j , x ) = s=123 p ( y = j|x , g123 , q ( s ) ) .
while this would be computationally faster , the above approach allows us to learn from the covariates of test cases when predicting their response values .
note also that the above predictive probabilities include the possibility that the test case is from a new cluster .
we use these posterior predictive probabilities to make predictions for test cases , by assigning each test case to the class with the highest posterior predictive probability .
this is the optimal strat - egy for a simple 123 / 123 loss function .
in general , we could use more problem - specic loss functions and modify our prediction strategy accordingly .
implementations for all our models were coded in matlab , and are available online at http :
results for simulated data
in this section , we illustrate our dpmnl model using synthetic data , and compare it with other models as follows :
a simple mnl model , tted by maximum likelihood or bayesian methods .
a bayesian mnl model with quadratic terms ( i . e . , xlxk , where l = 123 , . . . , p and k = 123 , . . . , p ) ,
referred to as qmnl .
a decision tree model ( breiman et al . , 123 ) that uses 123 - fold cross - validation for pruning , as implemented by the matlab treet , treetest ( for cross - validation ) and treeprune
shahbaba and neal
support vector machines ( svms ) ( vapnik , 123 ) , implemented with the matlab svm - train and svmclassify functions from the bioinformatics toolbox .
both a linear svm ( lsvm ) and a nonlinear svm with radial basis function kernel ( rbf - svm ) were tried .
when the number of classes in a classication problem was bigger than two , lsvm and rbf - svm used the all - vs - all scheme as suggested by allwein et al .
( 123 ) , furnkranz ( 123 ) , and hsu and lin ( 123 ) .
in this scheme , ( cid : 123 ) j 123 ( cid : 123 ) binary classiers are trained where each classier separates a pair of classes .
the predicted class for each test case is decided by using a majority voting scheme where the class with the highest number of votes among all binary classes wins .
for rbf - svm , the scaling parameter , l , in the rbf kernel , k ( x , x ) = exp ( ||x x|| / 123l ) , was optimized based on a validation set comprised of 123% of training samples .
the models are compared with respect to their accuracy rate and the f123 measure .
accuracy rate is dened as the percentage of the times the correct class is predicted .
f123 is a common measurement in machine learning dened as :
123a j + b j +c j
where a j is the number of cases which are correctly assigned to class j , b j is the number cases incorrectly assigned to class j , and c j is the number of cases which belong to the class j but are assigned to other classes .
we do two tests .
in the rst test , we generate data according to the dpmnl model .
our objective is to evaluate the performance of our model when the distribution of data is comprised of multiple components .
in the second test , we generate data using a smooth nonlinear function .
our goal is to evaluate the robustness of our model when data actually come from a different model .
123 simulation 123
the rst test was on a synthetic four - way classication problem with ve covariates .
data are generated according to our dpmnl model , except the number of components was xed at two .
two hyperparameters dening g123 were given the following priors : log ( t 123 ) n ( 123 , 123 ) , log ( n 123 ) n ( 123 , 123 ) .
the prior for component parameters q = ( , s 123 , a
, b ) dened by this g123 was
l ) n ( 123 , 123 ) , j|t n ( 123 , t 123 ) , jl|n n ( 123 , n 123 ) ,
where l = 123 , . . . , 123 and j = 123 , . . . , 123
we randomly draw parameters q 123 and q 123 for two components as described from this prior .
for each component , we then generate 123 data points by rst drawing
l ) and then sampling y using the following mnl model :
p ( y = j|x , a
, b ) =
j + xb
j + xb
nonlinear models using dirichlet process mixtures
mnl ( maximum likelihood ) tree ( cross validation )
table 123 : simulation 123 : the average performance of models based on 123 simulated data sets .
the baseline model assigns test cases to the class with the highest frequency in the training set .
standard errors of estimates ( based on 123 repetitions ) are provided in parentheses .
the overall sample size is 123
we randomly split the data into a training set , with 123 data points , and a test set , with 123 data points .
we use the training set to t the models , and use the independent test set to evaluate their performance .
the regression parameters of the bayesian mnl model with bayesian estimation and the qmnl model have the following priors :
j|t n ( 123 , t 123 ) , jl|n n ( 123 , n 123 ) , log ( t ) n ( 123 , 123 ) , log ( n ) n ( 123 , 123 ) .
the above procedure was repeated 123 times .
each time , new hyperparameters , t 123 and n 123 , and new component parameters , q 123 and q 123 , were sampled , and a new data set was created based on these
we used hamiltonian dynamics ( neal , 123 ) for updating the regression parameters ( the a s and b s ) .
for all other parameters , we used single - variable slice sampling ( neal , 123 ) with the stepping out procedure to nd an interval around the current point , and then the shrinkage procedure to sample from this interval .
we also used slice sampling for updating the concentration parameter g , we used log ( g ) n ( 123 , 123 ) as the prior , which , encourages smaller values of g , and hence a smaller number of components .
note that the likelihood for g depends only on c , the number of unique components ( neal , 123; escobar and west , 123 ) .
for all models we ran 123 mcmc iterations to sample from the posterior distributions .
we discarded the initial 123 samples and used the rest for prediction .
our dpmnl model has the highest computational cost compared to all other methods .
simulat - ing the markov chain took about 123 seconds per iteration using a matlab implementation on an ultrasparc iii machine ( approximately 123 minutes for each simulated data set ) .
each mcmc iteration for the bayesian mnl model took about 123 second ( approximately 123 minutes for each data set ) .
training the rbf - svm model ( with optimization of the scale parameter ) took approxi - mately 123 second for each data set .
therefore , svm models have a substantial advantage over our approach in terms of computational cost .
shahbaba and neal
figure 123 : a random sample generated according to simulation 123 , with a123 = 123
the dotted line is
the optimal boundary function .
the average results ( over 123 repetitions ) are presented in table 123
as we can see , our dpmnl model provides better results compared to all other models .
the improvements are statistically signicant ( p - values < 123 ) for comparisons of accuracy rates using a paired t - test with n = 123
123 simulation 123
in the above simulation , since the data were generated according to the dpmnl model , it is not surprising that this model had the best performance compared to other models .
in fact , as we increase the number of components , the amount of improvement using our model becomes more and more substantial ( results not shown ) .
to evaluate the robustness of the dpmnl model , we performed another test .
this time , we generated xi123 , xi123 , xi123 ( where i = 123 , . . . , 123 ) from the uni f orm ( 123 , 123 ) distribution , and generated a binary response variable , yi , according the following model :
p ( y = 123|x ) =
123 + exp ( a123 sin ( x123
123 + 123 ) + x123 cos ( a123x123 + 123 ) + a123x123 123 )
where a123 , a123 and a123 are randomly sampled from n ( 123 , 123 ) .
the function used to generate y is a smooth nonlinear function of covariates .
the covariates are not clustered , so the generated data do not conform with the assumptions of our model .
moreover , this function includes a completely arbitrary set of constants to ensure the results are generalizable .
figure 123 shows a random sample from this model except that a123 is xed at zero ( so x123 is ignored ) .
in this gure , the dotted line is the optimal decision boundary .
table 123 shows the results for this simulation , which are averages over 123 data sets .
for each data set , we generated 123 cases by sampling new values for a123 , a123 , and a123 , new covariates , x , for each case , and new values for the response variable , y , in each case .
as before , models were trained on 123 cases , and tested on the remaining 123
as before , the dpmnl model provides signicantly
nonlinear models using dirichlet process mixtures
mnl ( maximum likelihood ) tree ( cross validation )
table 123 : simulation 123 : the average performance of models based on 123 simulated data sets .
the baseline model assigns test cases to the class with the highest frequency in the training set .
standard errors of estimates ( based on 123 repetitions ) are provided in parentheses .
( all p - values are smaller than 123 ) better performance compared to all other models .
this time , however , the performances of qmnl and rbf - svm are closer to the performance of the dpmnl
results on real classication problems
in this section , we rst apply our model the problem of predicting a proteins 123d structure ( i . e . , folding class ) based on its sequence .
we then use our model to identify patients with parkinsons disease ( pd ) based on their speech signals .
123 protein fold classication
when predicting a proteins 123d structure , it is common to presume that the number of possible folds is xed , and use a classication model to assign a protein to one of these folding classes .
there are more than 123 folding patterns identied in the scop ( structural classication of pro - teins ) database ( lo conte et al . , 123 ) .
in this database , proteins are considered to have the same folding class if they have the same major secondary structure in the same arrangement with the same
we apply our model to a protein fold recognition data set provided by ding and dubchak ( 123 ) .
the proteins in this data set are obtained from the pdb select database ( hobohm et al . , 123; hobohm and sander , 123 ) such that two proteins have no more than 123% of the sequence identity for aligned subsequences larger than 123 residues .
originally , the resulting data set included 123 unique folds .
however , ding and dubchak ( 123 ) selected only the 123 most populated folds ( 123 proteins ) for their analysis .
they evaluated their models based on an independent sample ( i . e . , test set ) obtained from pdb - 123d ( lo conte et al . , 123 ) .
pdb - 123d contains the scop sequences with less than 123% identity with each other .
ding and dubchak ( 123 ) selected 123 representatives of the same 123 folds in the training set with no more than 123% identity to the training sequences .
the training and test data sets are available online at http : / / crd . lbl . gov / cding / protein / .
the covariates in these data sets are the length of the protein sequence , and the percentage composition of the 123 amino acids .
while there might exist more informative covariates to pre - dict protein folds , we use these so that we can compare the results of our model to that of ding
shahbaba and neal
and dubchak ( 123 ) , who trained several support vector machines ( svm ) with nonlinear kernel
we centered the covariates so they have mean zero , and used the following priors for the mnl
i as covariates ) :
model and qmnl model ( with no interactions , only xi and x123 j|h n ( 123 , h 123 ) , log ( h 123 ) n ( 123 , 123 ) , l n ( 123 , x 123s 123 log ( x 123 ) n ( 123 , 123 ) ,
l ) n ( 123 , 123 ) .
l , is used to control the variance of all coefcients , b
here , the hyperparameters for the variances of regression parameters are more elaborate than in the previous section .
one hyperparameter , s ( where j = 123 , . . . , j ) , for covariate xl .
if a covariate is irrelevant , its hyperparameter will tend to be small , forcing the coefcients for that covariate to be near zero .
this method , termed automatic relevance determination ( ard ) , has previously been applied to neural network models by neal ( 123 ) .
we also used another hyperparameter , x , to control the overall magnitude of all b s .
this l controls the relevance of covariate xl compared to other covariates , and x controls the overall usefulness of all covariates in separating all classes .
the standard deviation of b jl is therefore equal
the above scheme was also used for the dpmnl model .
note that in this model , one s jlc , where j = 123 , . . . , j indexes classes , and c = 123 , . . . , c indexes the unique components in the ln c .
here , n c is specic to each component mixture .
therefore , the standard deviation of b c , and controls the overall effect of coefcients in that component .
that is , while s and x are global hyperparameters common between all components , n c is a local hyperparameter within a component .
similarly , the standard deviation of intercepts , a c .
we used n ( 123 , 123 ) as the prior for n c and t c .
jc in component c is h
jlc is x
we also needed to specify priors for l and s
l , the mean and standard deviation of covariate xl ,
where l = 123 , . . . , p .
for these parameters , we used the following priors :
lc|123 , l , s 123 , l n ( 123 , l , s 123
these priors make use of higher level hyperparameters to provide exibility .
for example , if the components are not different with respect to covariate xl , the corresponding variance , s 123 small , forcing lc close to their overall mean , 123 , l .
the mcmc chains for mnl , qmnl , and dpmnl ran for 123 iterations .
simulating the markov chain took about 123 seconds per iteration ( 123 hours in total ) for dpmnl and 123 seconds per iteration ( 123 hours in total ) for mnl using a matlab implementation on an ultrasparc iii machine .
training the rbf - svm model took about 123 minutes .
nonlinear models using dirichlet process mixtures
svm ( ding and dubchak , 123 )
accuracy ( % ) f123 ( % )
table 123 : performance of models based on protein fold classication data .
the results for mnl , qmnl , lsvm , rbf - svm , and dpmnl are presented in table 123 , along with the results for the best svm model developed by ding and dubchak ( 123 ) on the exact same data set .
as we can see , the nonlinear rbf - svm model that we t has better accuracy than the linear models .
our dpmnl model provides an additional improvement over the rbf - svm model .
this shows that there is in fact a nonlinear relationship between folding classes and the composition of amino acids , and our nonlinear model could successfully identify this relationship .
123 detecting parkinsons disease
the above example shows that our method can potentially improve prediction accuracy , though of course other classiers , such as svm and neural networks , may do better on some problems .
however , we believe the application of our method is not limited to simply improving prediction accuracyit can also be used to discover hidden structure in data by identifying subgroups ( i . e . , mixture components ) in the population .
this section provides an example to illustrate this concept .
neurological disorders such as parkinsons disease ( pd ) have profound consequences for pa - tients , their families , and society .
although there is no cure for pd at this time , it is possible to alleviate its symptoms signicantly , especially at the early stages of the disease ( singh et al . , 123 ) .
since approximately 123% of patients exhibit some form of vocal impairment ( ho et al . , 123 ) , and research has shown that vocal impairment could be one of the earliest indicators of onset of the illness ( duffy , 123 ) , voice measurement has been proposed as a reliable tool to detect and monitor pd ( sapir et al . , 123; rahn et al . , 123; little et al . , 123 ) .
for example , patients with pd com - monly display a symptom known as dysphonia , which is an impairment in the normal production of
in a recent paper , little et al .
( 123 ) show that by detecting dysphonia , we could identify pa - tients with pd .
their study used data on sustained vowel phonations from 123 subjects , of whom 123 were diagnosed with pd .
the 123 covariates used include traditional variables , such as measures of vocal fundamental frequency and measures of variation in amplitude of signals , as well as a novel measurement referred to as pitch period entropy ( ppe ) .
see little et al .
( 123 ) for a detailed de - scription of these variables .
this data set is publicly available at uci machine learning repository
little et al .
( 123 ) use an svm classier with gaussian radial basis kernel functions to identify patients with pd , chosing the svm penalty value and kernel bandwidth by an exhaustive search over a range of values .
they also perform an exhaustive search to select the optimal subset of features ( 123 features were selected ) .
their best model provides a 123% ( 123 ) accuracy rate based on a bootstrap algorithm .
this of course does not reect the true prediction accuracy rate of the model for future observations since the model is trained and evaluated on the same sample .
here , we use
shahbaba and neal
table 123 : performance of models based on detecting parkinsons disease .
standard errors of esti -
mates ( based on 123 cross - validation folds ) are provided in parentheses .
frequency age average male proportion
table 123 : the age average and male proportion for each cluster ( i . e . , mixture component ) identied
by our model .
standard errors of estimates are provided in parentheses .
a 123 - fold cross validation scheme instead in order to obtain a more accurate estimate of prediction accuracy rate and avoid inating model performance due to overtting .
as a result , our models cannot be directly compared to that of little et al .
( 123 ) .
we apply our dpmnl model to the same data set , along with mnl and qmnl ( no interactions , only xi and x123 i as covariates ) .
although the observations from the same subject are not independent , we assume they are , as done by little et al .
( 123 ) .
instead of selecting an optimum subset of fea - tures , we used pca and chose the rst 123 principal components .
the mcmc algorithm for mnl , qmnl , and dpmnl ran for 123 iterations ( the rst 123 iterations were discarded ) .
simulating the markov chain took about 123 second per iteration ( 123 minutes per data set ) for dpmnl and 123 second per iteration ( 123 minutes per data set ) for mnl using a matlab implementation on an ultrasparc iii machine .
training the rbf - svm model took 123 seconds for each data set .
using the dpmnl model , the most probable number of components in the posterior is four ( note that might change from one iteration to another ) .
table 123 shows the average and standard errors ( based on 123 - fold cross validation ) of the accuracy rate and the f123 measure for mnl , lsvm , rbf - svm , and dpmnl .
( but note that the standard errors assume independence of cross - validation folds , which is not really correct . )
while dpmnl provides slightly better results , the improvement is not statistically signicant .
however , examining the clusters ( i . e . , mixture components ) identied by dpmnl reveals some information about the underlying structure in the data .
table 123 shows the average age of subjects and male proportion for the four clusters ( based on the most probable number of components in the posterior ) identied by our model .
note that age and gender are not available from the uci machine learning repository , and they are not included in our model .
they are , however , available from table 123 in little et al .
( 123 ) .
the rst two groups include substantially higher percentages of male subjects than female subjects .
the average age in the second of these groups is higher compared to the rst group .
most of the subjects in the third group are female ( only 123% are male ) .
nonlinear models using dirichlet process mixtures
the fourth group also includes more female subjects than male subjects , but the disproportionality is not as high as for the third group .
when identifying parkinsons disease by detecting dysphonia , it has been shown that gender has a confounding effect ( cnockaert et al . , 123 ) .
by grouping the data into clusters , our model has identied ( to some extent ) the heterogeneity of subjects due to age and gender , even though these covariates were not available to the model .
moreover , by tting a separate linear model to each component ( i . e . , conditioning on mixture identiers ) , our model approximates the confounding effect of age and gender .
for this example , we could have simply taken the age and gender of subjects from table 123 in little et al .
( 123 ) and included them in our model .
in many situations , however , not all the relevant factors are measured .
this could result in unobservable changes in the structure of data .
we discuss this concept in more detail elsewhere ( shahbaba , 123 ) .
discussion and future directions
we introduced a new nonlinear classication model , which uses dirichlet process mixtures to model the joint distribution of the response variable , y , and the covariates , x , non - parametrically .
we com - pared our model to several linear and nonlinear alternative methods using both simulated and real data .
we found that when the relationship between y and x is nonlinear , our approach provides sub - stantial improvement over alternative methods .
one advantage of this approach is that if the rela - tionship is in fact linear , the model can easily reduce to a linear model by using only one component in the mixture .
this way , it avoids overtting , which is a common challenge in many nonlinear
we believe our model can provide more interpretable results .
in many real problems , the iden - tied components may correspond to a meaningful segmentation of data .
since the relationship between y and x remains linear in each segment , the results of our model can be expressed as a set of linear patterns for different data segments .
in rbf - svm and g
hyperparameters such as l
in dpmnl can substantially inuence the per - formance of the models .
therefore , it is essential to choose these parameters appropriately .
for rbf - svm , we optimized l using a validation set that includes 123% of the training data .
figure 123 ( a ) shows the effect of l on prediction accuracy for one data set .
the value of l with the highest accuracy rate based on the validation set was used to train the rbf - svm model .
the hyperparam - eters in our dpmnl model are not xed at some optimum values .
instead , we use hyperpriors that reect our opinion regarding the possible values of these parameters before observing the data , with the posterior for these parameters reecting both this prior opinion and the data .
hyperpriors for regression parameters , b , facilitate their shrinkage towards zero if they are not relevant to the classication task .
the hyperprior for the scale parameter g affects how many mixture components are present in the data .
instead of setting g to some constant number , we allow the model to decide the appropriate value of g , using a hyperprior that encourages a small number of components , but which is not very restrictive , and hence allows g to become large in the posterior if required to t the data .
choosing unreasonably restrictive priors could have a negative effect on model perfor - mance and mcmc convergence .
figure 123 ( b ) illustrates the negative effect of unreasonable priors for g .
for this data set , the correct number of components is two .
we gradually increase g , where log ( g ) n ( g , 123 ) , in order to put higher probability on larger values of g and lower probability on smaller values .
as we can see , setting g 123 , which makes the hyperprior very restrictive , results in
shahbaba and neal
figure 123 : effects of scale parameters when tting a data set generated according to simulation 123
( b ) the effect of the prior on the scale
( a ) the effect of l parameter , g log - n ( g , 123 ) , as g changes in the dpmnl model .
in the rbf - svm model .
a substantial decline in accuracy rate ( solid line ) due to overtting with a large number of mixture components ( dashed line ) .
the computational cost for our model is substantially higher compared to other methods such as mnl and svm .
this could be a preventive factor in applying our model to some problems .
the computational cost of our model could be reduced by using more efcient methods , such as the split - merge approach introduced by jain and neal ( 123 ) .
this method uses a metropolis - hastings procedure that resamples clusters of observations simultaneously rather than incrementally assigning one observation at a time to mixture components .
alternatively , it might be possible to reduce the computational cost by using a variational inference algorithm similar to the one proposed by blei and jordan ( 123 ) .
in this approach , the posterior distribution p is approximated by a tractable variational distribution q , whose free variational parameters are adjusted until a reasonable approximation to p is achieved .
we expect our model to outperform other nonlinear models such as neural networks and svm ( with nonlinear kernel functions ) when the population is comprised of subgroups each with their own distinct pattern of relationship between covariates and response variable .
we also believe that our model could perform well if the true function relating covariates to response variable contains
the performance of our model could be negatively affected if the covariates are highly correlated with each other .
in such situations , the assumption of diagonal covariance matrix for x adopted by our model could be very restrictive .
to capture the interdependencies between covariates , our model would attempt to increase the number of mixture components ( i . e . , clusters ) .
this however is not very efcient .
to address this issue , we could use mixtures of factor analyzers , where the covariance structure of high dimensional data is model using a small number of latent variables ( see for example , rubin and thayer , 123; ghahramani and hinton , 123 ) .
in this paper , we considered only continuous covariates .
our approach can be easily extended to situations where the covariate are categorical .
for these problems , we need to replace the nor -
nonlinear models using dirichlet process mixtures
mal distribution in the baseline , g123 , with a more appropriate distribution .
for example , when the covariate x is binary , we can assume x bernoulli ( ) , and specify an appropriate prior distribution ( e . g . , beta distribution ) for .
alternatively , we can use a continuous latent variable , z , such that = exp ( z ) / ( 123 + exp ( z ) ) .
this way , we can still model the distribution of z as a mixture of nor - mals .
for categorical covariates , we can either use a dirichlet prior for the probabilities of the k categories , or use k continuous latent variables , z123 , . . . , zk , and let the probability of category j be exp ( z j ) / ( cid : 123 ) k
j exp ( z j ) .
throughout this paper , we assumed that the relationship between y and x is linear within each component of the mixture .
it is possible of course to relax this assumption in order to obtain more exibility .
for example , we can include some nonlinear transformation of the original variables ( e . g . , quadratic terms ) in the model .
our model can also be extended to problems where the response variable is not multinomial .
for example , we can use this approach for regression problems with continuous response , y , which could be assumed normal within a component .
we would model the mean of this normal distribution as a linear function of covariates for cases that belong to that component .
other types of response variables ( i . e . , with poisson distribution ) can be handled in a similar way .
, a / b
, and a + b
in the protein fold prediction problem discussed in this paper , classes were regarded as a set of unrelated entities .
however , these classes are not completely unrelated , and can be grouped into four major structural classes known as a .
ding and dubchak ( 123 ) show the corresponding hierarchical scheme ( table 123 in their paper ) .
we have previously introduced a new approach for modeling hierarchical classes ( shahbaba and neal , 123 , 123 ) .
in this approach , we use a bayesian form of the multinomial logit model , called cormnl , with a prior that introduces correlations between the parameters for classes that are nearby in the hierarchy .
our dpmnl model can be extend to classication problems where classes have a hierarchical structure ( shahbaba , 123 ) .
for this purposse , we use a cormnl model , instead of mnl , to capture the relationship between the covariates , x , and the response variable , y , within each component .
the results is a nonlinear model which takes the hierarchical structure of classes into account .
finally , our approach provides a convenient framework for semi - supervised learning , in which both labeled and unlabeled data are used in the learning process .
in our approach , unlabeled data can contribute to modeling the distribution of covariates , x , while only labeled data are used to identify the dependence between y and x .
this is a quite useful approach for problems where the response variable is known for a limited number of cases , but a large amount of unlabeled data can be generated .
one such problem is classication of web documents .
this research was supported by the natural sciences and engineering research council of canada .
radford neal holds a canada research chair in statistics and machine learning .
