we describe a markov chain method for sampling from the distribution of the hidden state sequence in a non - linear dynamical system , given a sequence of observations .
this method updates all states in the sequence simultaneously using an embedded hidden markov model ( hmm ) .
an update begins with the creation of pools of candidate states at each time .
we then dene an embedded hmm whose states are indexes within these pools .
using a forward - backward dynamic programming algo - rithm , we can efciently choose a state sequence with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools .
we illustrate the method in a simple one - dimensional example , and in an example showing how an embed - ded hmm can be used to in effect discretize the state space without any discretization error .
we also compare the embedded hmm to a particle smoother on a more substantial problem of inferring human motion from 123d traces of markers .
consider a dynamical model in which a sequence of hidden states , x = ( x123; : : : ; xn ( cid : 123 ) 123 ) , is generated according to some stochastic transition model .
we observe y = ( y123; : : : ; yn ( cid : 123 ) 123 ) , with each yt being generated from the corresponding xt according to some stochastic ob - servation process .
both the xt and the yt could be multidimensional .
we wish to randomly sample hidden state sequences from the conditional distribution for the state sequence given the observations , which we can then use to make monte carlo inferences about this poste - rior distribution for the state sequence .
we suppose in this paper that we know the dynamics of hidden states and the observation process , but if these aspects of the model are unknown , the method we describe will be useful as part of a maximum likelihood learning algorithm such as em , or a bayesian learning algorithm using markov chain monte carlo .
if the state space is nite , of size k , so that this is a hidden markov model ( hmm ) , a hidden state sequence can be sampled by a forward - backwards dynamic programming al - gorithm in time proportional to nk 123 ( see ( 123 ) for a review of this and related algorithms ) .
if the state space is <p and the dynamics and observation process are linear , with gaussian noise , an analogous adaptation of the kalman lter can be used .
for more general models ,
or for nite state space models in which k is large , one might use markov chain sampling ( see ( 123 ) for a review ) .
for instance , one could perform gibbs sampling or metropolis up - dates for each xt in turn .
such simple markov chain updates may be very slow to converge , however , if the states at nearby times are highly dependent .
a popular recent approach is to use a particle smoother , such as the one described by doucet , godsill , and west ( 123 ) , but this approach can fail when the set of particles doesnt adequately cover the space , or when particles are eliminated prematurely .
in this paper , we present a markov chain sampling method for a model with an arbitrary state space , x , in which efcient sampling is facilitated by using updates that are based on temporarily embedding an hmm whose nite state space is a subset of x , and then applying the efcient hmm sampling procedure .
we illustrate the method on a simple one - dimensional example .
we also show how it can be used to in effect discretize the state space without producing any discretization error .
finally , we demonstrate the embedded hmm on a problem of tracking human motion in 123d based on the 123d projections of marker positions , and compare it with a particle smoother .
123 the embedded hmm algorithm
in our description of the algorithm , model probabilities will be denoted by p , which will denote probabilities or probability densities without distinction , as appropriate for the state space , x , and observation space , y .
the models initial state distribution is given by p ( x123 ) , transition probabilities are given by p ( xt j xt ( cid : 123 ) 123 ) , and observation prob - abilities are given by p ( yt j xt ) .
our goal is to sample from the conditional distribution p ( x123; : : : ; xn ( cid : 123 ) 123 j y123; : : : ; yn ( cid : 123 ) 123 ) , which we will abbreviate to ( cid : 123 ) ( x123; : : : ; xn ( cid : 123 ) 123 ) , or ( cid : 123 ) ( x ) .
to accomplish this , we will simulate a markov chain whose state space is x n i . e . , a state of this chain is an entire sequence of hidden states .
we will arrange for the equilib - rium distribution of this markov chain to be ( cid : 123 ) ( x123; : : : ; xn ( cid : 123 ) 123 ) , so that simulating the chain for a suitably long time will produce a state sequence from the desired distribution .
the state at iteration i of this chain will be written as x n ( cid : 123 ) 123 ) .
the transition probabilities for this markov chain will be denoted using q .
in particular , we will use some initial distribution for the state of the chain , q ( x ( 123 ) ) , and will simulate the chain according to the transition probabilities q ( x ( i ( cid : 123 ) 123 ) ) .
for validity of the sampling method , we need these transitions to leave ( cid : 123 ) invariant :
123 ; : : : ; x ( i )
( i ) = ( x ( i )
( i ) j x
123 ) = x x 123 x n
123 j x ) ;
for all x
123 in x n
( if x is continuous , the sum is replaced by an integral . ) this is implied by the detailed
123 j x ) = ( cid : 123 ) ( x
123 ) q ( x j x
for all x and x
123 in x n
( i ) j x
the transition q ( x ( i ( cid : 123 ) 123 ) ) is dened in terms of pools of states for each time .
the current state at time t is always part of the pool for time t .
other states in the pool are produced using a pool distribution , ( cid : 123 ) t , which is designed so that points drawn from ( cid : 123 ) t are plausible alternatives to the current state at time t .
the simplest way to generate these additional pool states is to draw points independently from ( cid : 123 ) t .
this may not be feasible , however , or may not be desirable , in which case we can instead simulate an inner markov chain dened by transition probabilities written as rt ( ( cid : 123 ) j ( cid : 123 ) ) , which leave the pool distribu - tion , ( cid : 123 ) t , invariant .
the transitions for the reversal of this chain with respect to ( cid : 123 ) t will be denoted by ~ rt ( ( cid : 123 ) j ( cid : 123 ) ) , and are dened so as to satisfy the following condition :
t j xt ) = ( cid : 123 ) t ( x123
t ) ~ rt ( xt j x123
for all xt and x123
t in x
if the transitions rt satisfy detailed balance with respect to ( cid : 123 ) t , ~ rt will be the same as rt .
to generate pool states by drawing from ( cid : 123 ) t independently , we can let rt ( x123jx ) = ~ rt ( x123jx ) = ( cid : 123 ) t ( x123 ) .
for the proof of correctness below , we must not choose ( cid : 123 ) t or rt based on the current state , x
( i ) , but we may choose them based on the observations , y .
to perform a transition q to a new state sequence , we begin by at each time , t , producing a pool of k states , ct .
one of the states in ct is the current state , x ( i ( cid : 123 ) 123 ) ; the others are produced using rt and ~ rt .
the new state sequence , x ( i ) , is then randomly selected from among all sequences whose states at each time t are in ct , using a form of the forward -
in detail , the pool of candidate states for time t is found as follows :
123 ) pick an integer jt uniformly from f123; : : : ; k ( cid : 123 ) 123g .
123 ) let x ( 123 ) 123 ) for j from 123 to jt , randomly pick x ( j )
t = x ( i ( cid : 123 ) 123 )
( so the current state is always in the pool . )
according to the transition probabilities
123 ) for j from ( cid : 123 ) 123 down to ( cid : 123 ) k + jt + 123 , randomly pick x ( j )
t according to the reversed
transition probabilities , ~ rt ( x ( j )
123 ) let ct be the pool consisting of x ( j )
t , for j 123 f ( cid : 123 ) k+jt+123; : : : ; 123; : : : ; jtg .
if some
of the x ( j )
t are the same , they will be present in the pool more than once .
once the pools of candidate states have been found , a new state sequence , x ( i ) , is picked from among all sequences , x , for which every xt is in ct .
the probability of picking ( i ) = x is proportional to ( cid : 123 ) ( x ) =qn ( cid : 123 ) 123
t=123 ( cid : 123 ) t ( xt ) , which is proportional to
the division by qn ( cid : 123 ) 123 t=123 ( cid : 123 ) t ( xt ) is needed to compensate for the pool states having been drawn from the ( cid : 123 ) t distributions .
if duplicate states occur in some of the pools , they are treated as if they were distinct when picking a sequence in this way .
in effect , we pick indexes of states in these pools , with probabilities as above , rather than states themselves .
the distribution of these sequences of indexes can be regarded as the posterior distribu - tion for a hidden markov model , with the transition probability from state j at time t ( cid : 123 ) 123 to state k at time t being proportional to p ( x ( k ) t ( cid : 123 ) 123 ) , and the probabilities of the hypo - thetical observed symbols being proportional to p ( yt j x ( k ) t ) .
crucially , using the forward - backward technique , it is possible to randomly pick a new state sequence from this distribution in time growing linearly with n , even though the number of possible sequences grows as k n .
after the above procedure has been used to produce the pool states , x ( j ) t = 123 to n ( cid : 123 ) 123 and j = ( cid : 123 ) k +jt + 123 to jt , this algorithm operates as follows ( see ( 123 ) ) :
123 ) for t = 123 to n ( cid : 123 ) 123 and for j = ( cid : 123 ) k +jt +123 to jt , let ut;j = p ( yt j x ( j ) 123 ) for j = ( cid : 123 ) k +j123 +123 to j123 , let w123;j = u123;j p ( x123 = x ( j ) 123 ) for t = 123 to n ( cid : 123 ) 123 and for j = ( cid : 123 ) k +jt + 123 to jt , let wt ( cid : 123 ) 123;k p ( x ( j )
wt;j = ut;j pk
123 ) randomly pick sn ( cid : 123 ) 123 from f ( cid : 123 ) k +jn ( cid : 123 ) 123 +123; : : : ; jn ( cid : 123 ) 123g , picking the value j with
probability proportional to wn ( cid : 123 ) 123;j .
t=123 p ( xt j xt ( cid : 123 ) 123 ) qn ( cid : 123 ) 123
t=123 p ( yt j xt )
123 ) for t = n ( cid : 123 ) 123 down to 123 , randomly pick st ( cid : 123 ) 123 from f ( cid : 123 ) k +jt ( cid : 123 ) 123 +123; : : : ; jt ( cid : 123 ) 123g ,
picking the value j with probability proportional to wt ( cid : 123 ) 123;j p ( x ( st )
note that when implementing this algorithm , one must take some measure to avoid oating - point underow , such as representing the wt;j by their logarithms .
finally , the embedded hmm transition is completed by letting the new state sequence , x be equal to ( x ( s123 )
; : : : ; x ( sn ( cid : 123 ) 123 )
123 proof of correctness
to show that a markov chain with these transitions will converge to ( cid : 123 ) , we need to show that it leaves ( cid : 123 ) invariant , and that the chain is ergodic .
ergodicity need not always hold , and proving that it does hold may require considering the particulars of the model .
however , it is easy to see that the chain will be ergodic if all possible state sequences have non - zero probability density under ( cid : 123 ) , the pool distributions , ( cid : 123 ) t , have non - zero density everywhere , and the transitions rt are ergodic .
this probably covers most problems that arise in prac -
to show that the transitions q ( ( cid : 123 ) j ( cid : 123 ) ) leave ( cid : 123 ) invariant , it sufces to show that they satisfy detailed balance with respect to ( cid : 123 ) .
this will follow from the stronger condition that the probability of moving from x to x 123 ( starting from a state picked from ( cid : 123 ) ) with given values for the jt and given pools of candidate states , ct , is the same as the corresponding proba - bility of moving from x 123 to x with the same pools of candidate states and with values j 123 t in the dened by j 123
t = jt ( cid : 123 ) ht , where ht is the index ( from ( cid : 123 ) k + jt + 123 to jt ) of x123
the probability of such a move from x to x 123 is the product of several factors .
first , there is the probability of starting from x under ( cid : 123 ) , which is ( cid : 123 ) ( x ) .
then , for each time t , there is the probability of picking jt , which is 123=k , and of then producing the states in the candidate pool using the transitions rt and ~ rt , which is
t ) ( cid : 123 )
finally , there is the probability of picking x the pools , ct , which is proportional to ( cid : 123 ) ( x
k n ( cid : 123 )
123 from among all the sequences with states from t ) .
the product of all these factors is
we can now see that the corresponding expression for a move from x apart from a relabelling of candidate state x ( j )
t as x ( j ( cid : 123 ) ht )
123 to x is identical ,
123 a simple demonstration
the following simple example illustrates the operation of the embedded hmm .
the state space x and the observation space , y , are both < , and each observation is simply the state plus gaussian noise of standard deviation ( cid : 123 ) i . e . , p ( yt j xt ) = n ( yt j xt; ( cid : 123 ) 123 ) .
the state transitions are dened by p ( xt j xt ( cid : 123 ) 123 ) = n ( xt j tanh ( ( cid : 123 ) xt ( cid : 123 ) 123 ) ; ( cid : 123 ) 123 ) , for some constant expansion factor ( cid : 123 ) and transition noise standard deviation ( cid : 123 ) .
figure 123 shows a hidden state sequence , x123; : : : ; xn ( cid : 123 ) 123 , and observation sequence , y123; : : : ; yn ( cid : 123 ) 123 , generated by this model using ( cid : 123 ) = 123 : 123 , ( cid : 123 ) = 123 : 123 , and ( cid : 123 ) = 123 : 123 , with n = 123
the state sequence stays in the vicinity of +123 or ( cid : 123 ) 123 for long periods , with rare switches between these regions .
because of the large observation noise , there is con - siderable uncertainty regarding the state sequence given the observation sequence , with the posterior distribution assigning fairly high probability to sequences that contain short - term switches between the +123 and ( cid : 123 ) 123 regions that are not present in the actual state sequence , or that lack some of the short - term switches that are actually present .
we sampled from this distribution over state sequences using an embedded hmm in which the pool distributions , ( cid : 123 ) t , were normal with mean zero and standard deviation one , and the pool transitions simply sampled independently from this distribution ( ignoring the current pool state ) .
figure 123 shows that after only two updates using pools of ten states , embedded hmm sampling produces a state sequence with roughly the correct characteristics .
figure 123 demonstrates how a single embedded hmm update can make a large change to the state sequence .
it shows a portion of the state sequence after 123 updates , the pools of states produced for the next update , and the state sequence found by the embedded hmm using these pools .
a large change is made to the state sequence in the region from time 123 to 123 , with states in this region switching from the vicinity of ( cid : 123 ) 123 to the vicinity of +123
this example is explored in more detail in ( 123 ) , where it is shown that the embedded hmm is superior to simple metropolis methods that update one hidden state at a time .
123 discretization without discretization error
a simple way to handle a model with a continuous state space is to discretize the space by laying down a regular grid , after transforming to make the space bounded if necessary .
an hmm with grid points as states can then be built that approximates the original model .
inference using this hmm is only approximate , however , due to the discretization error involved in replacing the continuous space by a grid of points .
the embedded hmm can use a similar grid as a deterministic method of creating pools of states , aligning the grid so that the current state lies on a grid point .
this is a special case of the general procedure for creating pools , in which ( cid : 123 ) t is uniform , rt moves to the next grid point and ~ rt moves to the previous grid point , with both wrapping around when the rst or last grid point is reached .
if the number of pool states is set equal to the number of points in a grid , every pool will consist of a complete grid aligned to include the current state .
on their own , such embedded hmm updates will never change the alignments of the grids .
however , we can alternately apply such an embedded hmm update and some other mcmc update ( eg , metropolis ) which is capable of making small changes to the state .
these small changes will change the alignment of the new grids , since each grid is aligned to include the current state .
the combined chain will be ergodic , and sample ( asymptotically ) from the correct distribution .
this method uses a grid , but nevertheless has no discretization error .
we have tried this method on the example described above , laying the grid over the trans - formed state tanh ( xt ) , with suitably transformed transition densities .
with k = 123 , the grid method samples more efciently than when using n ( 123; 123 ) pool distributions , as above .
figure 123 : a state sequence ( black dots ) and observation sequence ( gray dots ) of length 123 produced by the model with ( cid : 123 ) = 123 : 123 , ( cid : 123 ) = 123 : 123 , and ( cid : 123 ) = 123 : 123
figure 123 : the state sequence ( black dots ) produced after two embedded hmm updates , starting with the states set equal to the data points ( gray dots ) , as in the gure above .
figure 123 : closeup of an embedded hmm update .
the true state sequence is shown by black dots and the observation sequence by gray dots .
the current state sequence is shown by the dark line .
the pools of ten states at each time used for the update are shown as small dots , and the new state sequence picked by the embedded hmm by the light line .
figure 123 : the four - second motion se - quence used for the experiment , shown in three snapshots with streamers show - ing earlier motion .
the left plot shows frames 123 - 123 , the middle plot frames 123 - 123 , and the right plot frames 123 - 123
there were 123 frames per second .
the orthographic projection in these plots is the one seen by the model .
( these plots were produced using hertzmann and brands mosey program . )
123 tracking human motion
we have applied the embedded hmm to the more challenging problem of tracking 123d human motion from 123d observations of markers attached to certain body points .
we con - structed this example using real motion - capture data , consisting of the 123d positions at each time frame of a set of identied markers .
we chose one subject , and selected six markers ( on left and right feet , left and right hands , lower back , and neck ) .
these markers were projected to a 123d viewing plane , with the viewing direction being known to the model .
figure 123 shows the four - second sequence used for the experiment . 123 our goal was to recover the 123d motion of the six markers , by using the embedded hmm to generate samples from the posterior distribution over 123d positions at each time ( the hidden states of the model ) , given the 123d observations .
to do this , we need some model of human dynamics .
as a crude approximation , we used langevin dynamics with respect to a simple hand - designed energy function that penalizes unrealistic body positions .
in langevin dynamics , a gradient descent step in the energy is followed by the addition of gaussian noise , with variance related to the step size .
the equilibrium distribution for this dynamics is the boltzmann distribution for the energy function .
the energy function we used contains terms pertaining to the pairwise distances between the six markers and to the heights of the markers above the plane of the oor , as well as a term that penalizes bending the torso far backwards while the legs are vertical .
we chose the step size for the langevin dynamics to roughly match the characteristics of the actual data .
the embedded hmm was initialized by setting the state at all times to a single frame of the subject in a typical stance , taken from a different trial .
as the pool distribution at time t , we used the posterior distribution when using the boltzmann distribution for the energy as the prior and the single observation at time t .
the pool transitions used were langevin updates with respect to this pool distribution .
for comparison , we also tried solving this problem with the particle smoother of ( 123 ) , in which a particle lter is applied to the data in time order , after which a state sequence is selected at random in a backwards pass .
we used a stratied resampling method to reduce variance .
the initial particle set was created by drawing frames randomly from sequences other than the sequence being tested , and translating the markers in each frame so that their centre of mass was at the same point as the centre of mass in the test sequence .
both programs were implemented in matlab .
the particle smoother was run with 123 particles , taking 123 hours of compute time .
the resulting sampled trajectories roughly t the 123d observations , but were rather unrealistic for instance , the subjects feet often oated above the oor .
we ran the embedded hmm using ve pool states for 123 iterations , taking 123 hours of compute time .
the resulting sampled trajectories were more realistic
123data from the graphics lab of jessica hodgins , at http : / / mocap . cs . cmu . edu .
we chose markers 123 , 123 , 123 , 123 , 123 , 123 , downsampled to 123 frames per second .
the experiments reported here use frames 123 - 123 of trial 123 for subject 123
the elevation of the view direction was 123 degrees , and the azimuth was 123 degrees away from a front view of the person in the rst frame .
than those produced by the particle smoother , and were quantitatively better with respect to likelihood and dynamical transition probabilities .
however , the distribution of trajectories found did not overlap the true trajectory .
the embedded hmm updates appeared to be sampling from the correct posterior distribution , but moving rather slowly among those trajectories that are plausible given the observations .
we have shown that the embedded hmm can work very well for a non - linear model with a low - dimensional state .
for the higher - dimensional motion tracking example , the embed - ded hmm has some difculties exploring the full posterior distribution , due , we think , to the difculty of creating pool distributions with a dense enough sampling of states to allow linking of new states at adjacent times .
however , the particle smoother was even more severely affected by the high dimensionality of this problem .
the embedded hmm therefore appears to be a promising alternative to particle smoothers in such contexts .
the idea behind the embedded hmm should also be applicable to more general tree - structured graphical models .
a pool of values would be created for each variable in the tree ( which would include the current value for the variable ) .
the fast sampling algorithm possible for such an embedded tree ( a generalization of the sampling algorithm used for the embedded hmm ) would then be used to sample a new set of values for all variables , choosing from all combinations of values from the pools .
finally , while much of the elaboration in this paper is designed to create a markov chain whose equilibrium distribution is exactly the correct posterior , ( cid : 123 ) ( x ) , the embedded hmm idea can be also used as a simple search technique , to nd a state sequence , x , which maximizes ( cid : 123 ) ( x ) .
for this application , any method is acceptable for proposing pool states ( though some proposals will be more useful than others ) , and the selection of a new state sequence from the resulting embedded hmm is done using a viterbi - style dynamic pro - gramming algorithm that selects the trajectory through pool states that maximizes ( cid : 123 ) ( x ) .
if the current state at each time is always included in the pool , this viterbi procedure will al - ways either nd a new x that increases ( cid : 123 ) ( x ) , or return the current x again .
this embedded hmm optimizer has been successfully used to infer segment boundaries in a segmental model for voicing detection and pitch tracking in speech signals ( 123 ) , as well as in other applications such as robot localization from sensor logs .
acknowledgments .
this research was supported by grants from the natural sciences and engineering research council of canada , and by an ontario premiers research excel - lence award .
computing resources were provided by a cfi grant to geoffrey hinton .
