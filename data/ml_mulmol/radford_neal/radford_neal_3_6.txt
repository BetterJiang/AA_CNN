we describe two slice sampling methods for taking multivariate steps using the crumb framework
methods use the gradients at rejected proposals to adapt to the local curvature of the log - density surface ,
a technique that can produce much better proposals when parameters are highly correlated .
we evaluate
our methods on four distributions and compare their performance to that of a non - adaptive slice sampling
method and a metropolis method .
the adaptive methods perform favorably on low - dimensional target
distributions with highly - correlated parameters .
markov chain monte carlo methods often perform poorly when parameters are highly correlated .
our goal
has been to develop mcmc methods that work well on such distributions without requiring prior knowledge
about the distribution or extensive tuning runs .
slice sampling ( neal , 123 ) is an auxiliary - variable mcmc method based on the idea of drawing points
uniformly from underneath the density surface of a target distribution .
if one discards the density coordinate
from the sample , the resulting marginal distribution is the target distribution .
this document presents two samplers in the crumb framework ( neal , 123 , 123 ) , a general framework for slice sampling methods that take multivariate steps .
unlike many mcmc samplers , they perform well
when the target distribution has highly correlated parameters .
these methods can improve on univariate
slice sampling in the same way metropolis with a properly chosen multivariate proposal distribution can
improve on metropolis with a spherical proposal distribution and on metropolis updates of one coordinate
at a time .
123 multivariate steps in the crumb framework
this section describes the crumb framework for taking multivariate steps in slice sampling .
the overall
goal is to sample from a target distribution , such as the one with log - density contours shown in gure 123 ( a ) .
the dimension of the target distributions parameter space is denoted by p .
( the example in the gure has
figure 123 : ( a ) contours of a target distribution .
( b ) a slice , sy123 , and the current target component , x123
p = 123 ) the state space of the markov chain for slice sampling has dimension p + 123 , of which p correspond
to the target distribution and one to the current slice level .
slice sampling alternates between sampling the target coordinates and the density coordinate .
let f ( x ) be proportional to the density function of the target distribution , and let ( x123 , y123 ) be the current state in the augmented sample space , where x123 rp and y123 r .
to update the density component , y123 , we sample uniformly from ( 123 , f ( x123 ) ) .
updating the target component , x123 , is more dicult .
let sy123 be the slice through the distribution at level y123 :
sy123 = ( x : f ( x ) y123 )
the set sy123 is outlined by a thick line in gure 123 ( b ) .
the diculty in updating the target component is that we rarely have an analytic expression for the boundary of sy123 , which may not even be a connected curve , so we cannot sample uniformly from sy123 as we would like to .
the methods of this document instead perform updates that leave the uniform distribution on sy123 invariant , leaving the resulting chain with the desired
these methods begin by sampling a crumb , c123 , from a distribution that depends on x123 , then drawing a proposal , x123 , from the distribution of points that could have generated c123 , treating the crumb as data and x123 as an unknown .
an example crumb and proposal are drawn in gure 123 ( a ) , with the distribution of c123 shown as a dashed line and the distribution of x123 shown as a dotted line .
if x123 were inside sy123 , we would accept it as the new value for the target component of the state .
in gure 123 ( a ) , it is not , so we draw a new crumb , c123 , and draw a new proposal , x123 , from the distribution of x123 given both c123 and c123 as data .
this is plotted in gure 123 ( b ) .
while the distribution of proposed moves is determined by the crumbs and their distributions , we can choose the distribution of the crumbs arbitrarily ,
using the densities and gradients at rejected proposals if we desire .
in the example of gure 123 ( b ) , x123 is in sy123 , so we accept x123 as the new target component .
if it were not , we would keep drawing crumbs , adapting their distributions so that the proposal distribution would be as close as possible to uniform sampling over sy123 , and metaphorically following these crumbs back to x123 by drawing proposals from the distribution of x123 conditional on having observed the crumbs ( grimm and
figure 123 : ( a ) the rst crumb , c123 , and a proposal , x123
the distribution of c123 is shown as dashed .
the distribution of x123 is shown as dotted .
in this gure and subsequent ones , probability distributions in a two - dimensional space are represented by ellipses such that a uniform distribution over the ellipse has the same mean and covariance as the represented distribution .
this method allows multiple distributions to be drawn in the same plot .
( b ) the second crumb , c123 , and a proposal , x123
the distribution of c123 is shown as dashed .
the distribution of x123 is shown as dotted .
the distribution for c123 has been updated using the method described in section 123
neal ( 123 , 123 ) has more information on the crumb framework , including a proof that methods in this
framework leave the target distribution invariant .
123 overview of adaptive gaussian crumbs
we now specialize the crumb framework to gaussian crumbs and proposals , using log densities at proposals
and their gradients to choose the crumb covariances .
without violating detailed balance , the crumb distri -
bution can depend on these log densities and gradients .
we assume that while computing the log density at
a proposal , we can compute its gradient with minimal additional cost .
ideally , the proposal distribution would be a uniform distribution over sy123 .
to approximate uniform sampling over sy123 , we attempt to draw a sequence of crumbs that results in a gaussian proposal distribution with the same covariance as a uniform distribution over the slice .
in both adaptive methods discussed in this document , the rst crumb has a multivariate gaussian
c123 n ( x123 , w 123
) where w123 = 123
the standard deviation of the initial crumb , c , is the only tuning parameter for either method that is modied in normal use .
the distribution for x123 given c123 is a gaussian with mean c123 and precision matrix w123 , so we draw a proposal from this distribution :
x123 n ( c123 , w 123
if f ( x123 ) is at least y123 , then x123 is inside the slice , so we accept x123 as the target component of the next state of the chain .
this update leaves the density component , y123 , unchanged , though it is usually forgotten after
x123lc123lx123lx123lc123lx123l a proposal is accepted , anyway .
when the proposal is not in the slice , we choose a dierent covariance matrix , wk+123 , for the next crumb , so that the covariance of the next proposal will look more like that of uniform sampling over sy123
the two methods proposed in this document dier in how they make that choice; sections 123 and 123 describe the details .
after sampling k crumbs from gaussians with mean x123 and precision matrices ( w123 , .
, wk ) , the distri -
bution for the kth proposal ( the posterior for x123 given the crumbs ) is :
xk n ( ck , 123
where k = w123 + + wk
and ck = 123
k ( w123c123 + + wkck )
if xk sy123 that is , if f ( xk ) y123we accept xk as the target component .
otherwise , we must choose a covariance for the distribution of ( k + 123 ) th crumb , draw a new proposal , and repeat until a proposal is
123 first method : matching the slice covariance
in the method described in this section , we attempt to nd wk+123 so that the ( k + 123 ) th proposal distribution has the same conditional variance as uniform sampling from sy123 in the direction of the gradient of log f ( ) at xk .
this gradient is a good guess at the direction in which the proposal distribution is least like sy123 .
figure 123 ( a ) shows an example of this .
we plotted the gradients of the log density at thirty points drawn
from the same distribution ( shown as a dotted line ) as a rejected proposal; most of these gradients point in
the direction where the proposal variance is least like the slice .
generally , in an ill - conditioned distribution ,
these gradients do not point towards the mode , they point towards the nearest point on the slice .
a well - conditioned distribution , the directions to the mode and to the nearest point on the slice will be
123 choosing crumb covariances
to compute wk+123 , we will estimate the second derivative of the target distribution in the direction of the gradient , assuming the target distribution is approximately locally gaussian .
consider the ( approximately )
parabolic cut through the log - density surface in gure 123 ( b ) , which has the equation :
( cid : 123 ) = 123
t123 + t +
t is a parameter that is zero at xk and increases in the direction of the gradient; , , and are unknown constants we wish to estimate .
the coecient 123 123 is arbitrary; this choice makes equal to the negative of the second derivative of the parabola .
we already had to compute f ( xk ) so that we could determine whether xk was in sy123; we assume that log f ( xk ) was computed simultaneously .
to x two degrees of freedom of
figure 123 : ( a ) shows gradients at thirty points drawn from the rst proposal distribution ( shown as a dotted line ) .
these gradients tend to point towards the slice ( shown as a thick line ) .
( b ) shows a parabolic cut through the log - density surface that goes through a rejected proposal , x123 , in the direction of the gradient at x123 , shown as an arrow .
u123 is a point on the cut dened by equation 123
( c ) adds a parabolic cut through the mode in the same direction .
these two parabolas have approximately the same second derivative .
the distance between the two arms of the second parabola when the vertical coordinate is equal to log y123 is shown as a double - ended arrow , equal to d in equation 123
the parabola , we plug these quantities into equation 123 and its derivative with respect to t :
log f ( xk ) = 123
123 + 123 + =
( cid : 123 ) log f ( xk ) ( cid : 123 ) = 123 + =
we still have one degree of freedom left , so we must evaluate log f ( ) at another point on the parabola .
we choose a point as far away from xk as ck is , hoping that this distance is within the range where the distribution is approximately gaussian .
let be this distance :
let g be the normalized gradient at xk :
= ( cid : 123 ) xk ck ( cid : 123 )
log f ( xk ) ( cid : 123 ) log f ( xk ) ( cid : 123 )
then , the point uk is dened to be away from xk in the direction g :
uk = xk + g
in equation 123 , uk corresponds to t = .
we evaluate the density at uk to x the parabolic cuts third degree of freedom , and plug this into equation 123 :
log f ( uk ) = 123
123 + +
x123lx123lx123lx123lu123lx123lx123lu123l solving equations 123 , 123 , and 123 for gives :
( cid : 123 ) log f ( uk ) log f ( xk ) ( cid : 123 ) log f ( xk ) ( cid : 123 ) ( cid : 123 )
we can now use to approximate the conditional variance in the direction g .
if the hessian is locally approximately constant , as it is for gaussians , a cut through the mode in the direction of log f ( xk ) would have the same second derivative as the cut through xk .
this second parabola , shown in gure 123 ( c ) , has the
t123 + m
m is the log density at the mode .
we set t = 123 at the mode , so there is no linear term .
for now , assume f ( ) is unimodal and that m was computed with the conjugate gradient method ( nocedal and wright , 123 , ch .
123 ) or some other similar procedure before starting the markov chain .
we solve equation 123 for the parabolas diameter d at the level of the current slice , log y123 , shown as a doubled - ended arrow in gure 123 ( c ) :
( cid : 123 ) = 123
123 ( m log y123 )
since the distribution of points drawn from an ellipsoidal slice , conditional on their lying on that particular one - dimensional cut , is uniform with length d , the conditional variance in the direction of the gradient at xk
m log y123
with this variance , we can construct a crumb precision matrix that will lead to the desired proposal precision matrix .
we want to draw a crumb ck+123 so that the posterior of x123 given the k crumbs has a variance equal to 123 k+123 in the direction g .
using equation 123 , the precision of the proposal given these crumbs
k+123 = k + wk+123
if we multiply both sides of equation 123 by gt on the left and g on the right , the left side is the conditional precision in the direction g .
we would like to choose wk+123 so that this conditional precision is 123 equation 123 with that :
k+123 , so we replace the left hand side of
gt k+123g = gt ( cid : 123 ) k + wk+123 k+123 = gt ( cid : 123 ) k + wk+123
as will be discussed in section 123 , computation will be particularly easy if we choose wk+123 to be a scaled copy of k with a rank - one update , so we choose wk+123 to be of the form :
wk+123 = k + ggt
and are unknown scalars .
controls how fast the precision as a whole increases .
if we would like the
variance in directions other than g to shrink by 123 / 123 , for example , we choose = 123 / 123
since is constant ,
there will be exponentially fast convergence of the proposal covariance in all directions , which allows quick
recovery from overly diuse initial crumb distributions .
for this method , is not a critical choice; = 123 is
reasonable .
substituting equation 123 into equation 123 gives :
k+123 = gt ( cid : 123 ) k + k + ggt ( cid : 123 ) g
noting that gt g = 123 , we solve for , and set :
k+123 ( 123 + ) gt kg , 123 )
is restricted to be positive to guarantee positive deniteness of the crumb covariance .
( by choosing
simultaneously , we could perhaps encounter this restriction less frequently , but we have not explored this . ) once we know , we then compute wk+123 using equation 123
the resulting crumb distribution is :
ck+123 n ( x123 , w 123
after drawing such a crumb , we draw a proposal according to equations 123 to 123 , accepting or rejecting depending on whether f ( xk+123 ) y123 , drawing more crumbs and adapting until a proposal is accepted .
123 estimating the density at the mode
we now modify the method to remove the restriction that the target distribution be unimodal and remove the requirement that we precompute the log density at the mode .
estimating m each time we update x123 instead of precomputing it allows m to take on values appropriate to local modes .
since the proposal distribution only approximates the slice even in the best of circumstances , it is not essential that the estimate
of m be particularly good .
to estimate m , we initialize m to the slice level , log y123 , before drawing the rst crumb .
then , every time we t the parabola described by equation 123 , we update m to be the maximum of the current value of
m and the estimated peak of the parabola .
as more crumbs are drawn , m becomes a better estimate of the local maximum .
we could also use the values of the log density at rejected proposals and at the ( uk ) to bound m , but if the log density is locally concave , the log densities at the peaks of the parabola will always
be larger than these values .
123 ecient computation of the crumb covariance
this section describes a method for using cholesky factors of precision matrices to make implementation of
the covariance - matching method of section 123 ecient .
if implemented naively , the method of section 123 would use o ( p123 ) operations when drawing a proposal with equations 123 and 123
we would like to avoid this .
one way is to represent wk and k by their upper - triangular cholesky factors fk and rk , where f t k fk = wk
k rk = k .
first , we must draw proposals eciently .
if z123 and z123 are p - vectors of standard normal variates , we can
replace the crumb and proposal draws of equations 123 and 123 with :
ck = x123 + f 123 xk = ck + r123 since cholesky factors are upper - triangular , evaluation of f 123 k z123 and r123
k z123 by backward substitution takes
we must also update the cholesky factors eciently .
we replace the updates of wk and k in equations 123
and 123 with :
fk+123 = chud (
rk+123 = chud (
123 + rk ,
here , chud ( r , v ) is the cholesky factor of rt r + vvt .
the function name is an abbreviation for cholesky update .
it can be computed with the linpack routine dchud , which uses givens rotations to compute the update in o ( p123 ) operations ( dongarra et al . , 123 , ch
finally , we would like to compute the proposal mean eciently .
we do this by keeping a running sum of the un - normalized crumb mean ( the parenthesized expression in equation 123 ) , which we will represent by c
k = w123c123 + + wkck
k123 + wkck k123 + f t
then , using forward and backward substitution , we can compute the normalized crumb mean , ck , as :
ck = r123
this way , we can compute ck in o ( p123 ) operations and do not need to save all the crumbs and crumb
with these changes , the resulting algorithm is numerically stable even with ill - conditioned target dis - tributions .
each crumb and proposal draw takes o ( p123 ) operations .
figure 123 shows pseudocode for this
123 second method : shrinking the rank of the crumb covariance
the method of section 123 attempts to adapt the crumb distribution so that the proposal distribution matches the shape of the slice .
however , it often cant due to positive - deniteness constraints , requiring the max ( , ) operation in equation 123
even when it can perform the adaptation , it may not be appropriate if the
underlying distribution is not approximately gaussian .
this section describes a dierent method , also based on the framework of section 123
instead of attempting
to match the conditional variance in the direction of the gradient , it just sets it to zero .
this is reasonable
slice sampling with covariance matching
initialize n , f , x123 rp , c , and .
x ( ) repeat n times :
m log f ( x123 ) e draw from exponential ( 123 ) y123 m e repeat until a proposal is accepted :
z draw from np ( 123 , i ) c x123 + f 123z c c + f t f c c r123rt c z draw from np ( 123 , i ) x c + r123z y log f ( x ) if y y123 :
proposal accepted , break
g log f ( x ) u x + g ( cid : 123 ) u log f ( u ) 123 ( ( cid : 123 ) u y ( cid : 123 ) g ( cid : 123 ) ) m max ( m , ( cid : 123 ) x , u )
max ( cid : 123 ) 123 , 123 ( 123 + ) gt rt rg ( cid : 123 )
123 + r ,
append x to x .
figure 123 : this gure shows the adaptive algorithm of section 123
the variables are mostly the same as in the text , with a few exceptions : to avoid underow , we set y = log y and y123 = log y123; see neal ( 123 , 123 ) for discussion .
the k subscript is dropped since there is no need to keep copies of the values from any but the most recent iteration .
the variable ( cid : 123 ) x , u indicates the peak of the parabolic cut through x and u; n indicates the number of samples to draw .
the generated samples are stored in the ordered set x .
in that the gradient at a proposal probably points in a direction where the variance is small , so it is more
ecient to move in a dierent direction .
with this method , the crumb covariance is zero in some directions and spherically symmetric in the rest ,
so its simplest representation is the pair ( w , j ) , where j is a matrix of orthonormal columns of directions in which the conditional variance is zero , and w123 is the variance in the other directions .
dene p ( j , v ) to be the function that returns the component of vector v orthogonal to the columns of j :
v jj t v
p ( j , v ) =
if j has at least one column
if j has no columns
for simplicity of computation , since the crumbs are located in a common subspace with x123 , this method will consider the origin to be at x123 except when calling f ( ) , which is provided by the user , and when returning samples to the user .
each crumb is drawn by :
ck = c p ( j , z )
where z np ( 123 , i )
when the rst crumb is drawn , j has no columns , so p ( j , z ) = z and the rst crumb has the distribution
specied in section 123
given k crumbs and the covariances of their distributions , we know that x123 must lie in the intersection of the subspaces of their covariances .
since the subspace cj is drawn from contains the subspace ck is drawn from when k > j , this is equivalent to saying that x123 must lie in the subspace of cks covariance , the orthogonal complement of j .
so , the precision of the posterior for x123 in the direction of columns of j is innite .
it is equal to k123 in all other directions , since there are k crumbs , each with spherical variance equal to 123 c in the subspace they were drawn in .
the mean of the proposal distribution ( with origin x123 ) is the projection of :
c c123 + + 123
= k123 ( c123 + + ck )
therefore , to draw a proposal , we draw a vector of standard normal variates , project it into the orthogonal complement of j , scale by c / original origin , the proposal is :
k , and add c , also projected into the orthogonal complement of j .
with the
xk = x123 + p ( j , c ) + ( c /
k ) p ( j , z )
where z np ( 123 , i )
if the proposal is in the slice ( that is , if f ( xk ) y123 ) , we accept it .
otherwise , we adapt the crumb distribution .
if j has p 123 columns , we cant add any more without the crumb covariance having a rank of zero , so we do not adapt in that case .
otherwise , we add a single new column in the direction of log f ( xk ) , projected
into the directions not already spanned by j , which we denote by g .
thus , the new value of j would be :
g = p ( jk , log f ( xk ) )
to prevent meaningless adaptations , we only perform this operation when the angle between the gradient and its projection into the nullspace of j is less than 123
equivalently , we only adapt when :
gt log f ( xk ) ( cid : 123 ) g ( cid : 123 ) ( cid : 123 ) log f ( xk ) ( cid : 123 ) >
after possibly updating j , we draw another crumb and proposal , repeating until a proposal is accepted
method of this section is summarized in gure 123
a variation on the method ( which we use in the implementation tested in section 123 ) shrinks the crumb standard deviation in the nonzero directions , c , by a constant factor each time a crumb is drawn; section 123 uses 123 .
this results in exponentially falling proposal variance , which , as described in section 123 , allows
large initial crumb variances to be used .
123 procedure for evaluation
figures 123 and 123 demonstrate the performance of the methods described in this document .
figure 123 demon -
strates the performance of four samplers on a gaussian distribution ( to be described in section 123 ) .
it contains
two graphs , one for each of two gures of merit .
the top graph plots log density function evaluations per
independent sample against a tuning parameter .
the bottom graph plots processor - seconds per independent
sample against a tuning parameter .
for both gures of merit , smaller is better .
each of the four panes in each graph contains data from a single sampler , with each point representing
a run with a specic tuning parameter .
the tuning parameter for each sampler has the same scale; the
sampler initially attempts to take steps roughly that size .
both gures of merit require us to determine the correlation length , the number of correlated samples
equivalent to an independent sample .
for the runs done here , an ar ( 123 ) model captures the necessary
structure .
for each parameter , we t the following model :
xt = e ( xt ) + xt123 + at
where at is a noise process .
then , the number of samples equivalent to a single independent sample is :
var ( xt ) ( 123 ) 123
this formula is based on the effectivesize function in coda ( plummer et al . , 123 ) , which uses the spectral approach of heidelberger and welch ( 123 ) .
wei ( 123 , pp .
123 ) has a more in - depth discussion
of the spectrum of ar processes .
to estimate a correlation length for a multivariate distribution , we take
the largest estimated correlation length of each of its parameters .
this is not valid in general , but is an
crumbs with shrinking - rank covariance
initialize n , f , x123 rp , and c .
x ( ) repeat n times :
e draw from exponential ( 123 ) y123 log f ( x123 ) e j ( ) repeat until a proposal is accepted :
k k + 123 z draw from np ( 123 , i )
c k123 ( cid : 123 ) ( k 123 ) c + c ( cid : 123 )
z draw from np ( 123 , i ) x x123 + p j , c + c if log f ( x ) y123 :
proposal accepted , break
if j has less than p 123 columns :
g p ( j , log f ( x ) ) if gt log f ( x ) > 123 j ( j g / ( cid : 123 ) g ( cid : 123 ) )
123 ( cid : 123 ) g ( cid : 123 ) ( cid : 123 ) log f ( x ) ( cid : 123 ) :
append x to x .
figure 123 : the adaptive algorithm of section 123
this diers from that section in that the projection into the nullspace of j is delayed until drawing a proposal , reducing the number of matrix products computed .
acceptable approximation for this experiment .
most chains are displayed with a circle indicating an estimate of the gure of merit , with a line indicating
a nominal 123% condence interval .
the intervals are based on normality of the increments of the ar ( 123 )
process , so they should be viewed as a lower bound on the uncertainty of the point estimates .
chains whose
gures are plotted as question marks were estimated as having an eective sample size of less than four .
figure 123 contains information on four dierent samplers .
the panes of gure 123 are similar to those of
gure 123 , and each is labeled with the distribution and the sampler the chains in that pane come from
columns of panes correspond to samplers; the rows of panes correspond to distributions .
by reading across ,
one can see how dierent samplers perform on a given distribution .
by reading down , one can see how the
performance of a given sampler varies from distribution to distribution .
every chain has 123 , 123 samples .
in general , the chain length does not aect the results; we will point
out exceptions to this .
123 results of evaluation
we simulated four samplers on four distributions for twelve dierent tuning parameters each .
the samplers
we considered are :
covariance - matching : this is the method described in section 123
the tuning parameter is c .
shrinking - rank : this is the method described in section 123
the tuning parameter is c .
non - adaptive crumbs : this is a non - adaptive variant of the general method of section 123
it is like the method of section 123 , but never shrinks rank .
however , like that method , it scales the crumb standard deviation down by 123 after each proposal .
the tuning parameter is c .
metropolis ( with trials ) : this method is a metropolis sampler that uses trial runs to automatically determine a suitable gaussian proposal distribution .
the tuning parameter species the standard
deviation of the proposal distribution for the rst trial run .
the distributions we considered are :
n123 ( = 123 ) : this is a four - dimensional gaussian with highly - correlated parameters .
the marginal variances for each parameter are one; each parameter has a correlation of 123 with each other pa -
rameter .
the covariance matrix has a condition number of about 123 and is not diagonal .
n123 ( = 123 ) : this is a four - dimensional gaussian with negatively - correlated parameters .
the marginal variances for each parameter are one , and each parameter has a correlation of 123 with each other parameter .
the covariance has a condition number of about 123 , like n123 ( = 123 ) , but instead of one large eigenvalue and three small ones , this distribution has one small eigenvalue and
three large ones .
eight schools : this is a multilevel model in ten dimensions , consisting of eight group means and
hyperparameters for their mean and variance .
it comes from gelman et al .
( 123 , pp .
123 ) .
figure 123 : the performance of four mcmc samplers on n123 ( = 123 ) .
see section 123 for a description of the graphs and section 123 for discussion .
ten - component mixture : this is a ten - component gaussian mixture in r123
each mode is a spherically symmetric gaussian with unit variance .
the modes are uniformly distributed on a hypercube with
by comparing the top and bottom graphs of gure 123 , we see that for n123 ( = 123 ) , processor time and number of density evaluations tell the same story .
the plots of function evaluations and the plots of
processor time are nearly identical except for their vertical scales .
this is true of the other distributions as
well , so we do not repeat the processor - time plots for the others .
figure 123 ( and the identical rst row of gure 123 ) shows the performance of the four methods on a highly - correlated four - dimensional gaussian , n123 ( = 123 ) .
both adaptive slice sampling methods perform well once the tuning parameter is at least the same order as the standard deviation of the target distribution .
this hockey - stick performance curve is characteristic of slice sampling methods .
the non - adaptive slice
sampling method always takes steps of the order of the smallest eigenvalue once its tuning parameter is at
least that large , so its performance is bad , but after this threshold , its performance does not depend much
on the tuning parameter .
the metropolis sampler also fails to capture the shape of the distribution , a result
that depends more on chain length .
had the chain been longer , the preliminary runs the sampler uses to
comparison of density evaluations on n ( 123 ) ( rho=123 ) evals per indep .
sample123^123^123^123^123^123^123 . 123llllllllllll ? covariancematching123 . 123lllllllllllllshrinkingrank123 . 123llllllllllll ? nonadaptive crumbs123 . 123lllllllllllllmetropolis ( with trials ) comparison of processor usage on n ( 123 ) ( rho=123 ) scale tuning parametercpu sec .
per indep .
sample123^123^123^123^123^123^123 . 123llllllllllll ? covariancematching123 . 123lllllllllllllshrinkingrank123 . 123llllllllllll ? nonadaptive crumbs123 . 123lllllllllllllmetropolis ( with trials ) figure 123 : the performance of four mcmc samplers on four distributions .
see section 123 for a description of the graph and section 123 for discussion .
four samplers on four distributionsscale tuning parameterevals per indep .
sample123^123^123^123^123^123^123 . 123llllllllll ? ? ? covariancematchingtencomponent mixture123 . 123llllllllll ? ? ? shrinkingranktencomponent mixture123 . 123llllllllll ? ? ? nonadaptive crumbstencomponent mixture123 . 123llllllll ? ? ? ? ? metropolis ( with trials ) tencomponent mixture123^123^123^123^123^123^123llllllllll ? ? ? covariancematchingeight schoolsllllllllll ? ? ? shrinkingrankeight schoolslllllllllll ? ? nonadaptive crumbseight schoolsllll ? ? ? ? ? ? ? ? ? metropolis ( with trials ) eight schools123^123^123^123^123^123^123llllllllllll ? covariancematchingn123 ( r= - 123 ) llllllllllll ? shrinkingrankn123 ( r= - 123 ) lllllllllllllnonadaptive crumbsn123 ( r= - 123 ) lllllllllllllmetropolis ( with trials ) n123 ( r= - 123 ) 123^123^123^123^123^123^123llllllllllll ? covariancematchingn123 ( r=123 ) lllllllllllllshrinkingrankn123 ( r=123 ) llllllllllll ? nonadaptive crumbsn123 ( r=123 ) lllllllllllllmetropolis ( with trials ) n123 ( r=123 ) estimate a proposal distribution might have worked better , leading to average performance comparable to
the adaptive slice samplers .
the second row of gure 123 shows sampler performance on a similar but negatively correlated four - dimensional gaussian , n123 ( = 123 ) .
the adaptive slice sampling methods continue to perform well on this distribution , and the non - adaptive sampler improves somewhat .
the metropolis sampler improves a
great deal; on this target distribution , it is able to choose a reasonable proposal distribution , so it performs comparably to the adaptive slice sampling methods .
the third row of gure 123 shows sampler performance on eight schools .
the minimum threshold appears
again for both adaptive slice samplers as well as the non - adaptive slice sampler .
since the condition number of
the covariance of this distribution is only about seven , adaptivity is not as important , though slice samplings
robustness to improper tuning parameters remains important .
the adaptive metropolis method again fails
to identify a reasonable proposal distribution for small tuning parameters , and fails to generate any proposal
distribution at all for large ones .
this is partially a reection on this particular implementation , which only
tries preliminary runs with proposal distribution standard deviations within four orders of magnitude of the
the bottom row of gure 123 , which shows performance on ten - component mixture , has a similar pattern .
the results are more erratic since the distribution has multiple , moderately - separated modes , and none of
the samplers are designed to perform well on multimodal distributions .
the adaptive slice sampling methods of sections 123 and 123 generally perform at least as well as non - adaptive
slice sampling methods and metropolis .
slice sampling in general tends to be more robust to imperfect
choice of tuning parameters than metropolis .
preliminary chains are usually unnecessary , avoiding the
hassle of manual management of these runs or the idiosyncratic performance of automatic evaluation of the
the main disadvantage of the adaptive slice samplers relative to metropolis and non - adaptive slice
sampling is that they require the log density to have an analytically computable gradient , though this is a
standard requirement in numerical optimization , and experience in that domain has shown that computing
the gradient is often straightforward .
the two adaptive slice sampling methods tend to perform similarly to each other .
the shrinking - rank
method usually performs slightly better , but this advantage can be mitigated by making the approximation log f ( uk ) log y123
there is no theoretical justication for this , but it cuts the number of log density evaluations by half with negligible performance cost , making the performance of the two adaptive methods indistinguishable .
the shrinking - rank method is simpler , though , and requires only o ( min ( k , p ) p ) operations to draw the kth crumb and proposal , slightly better than the o ( p123 ) operations needed by the covariance -
like most variations of multivariate slice sampling , both the non - adaptive and adaptive methods described
here do not work well for target distributions in spaces higher than a few dozen .
the variation in log density
of samples increases with dimension , but slice sampling takes steps in the log density of only order one each
iteration .
so , in high - dimensional spaces , samples tend to be highly correlated .
due to this poor scaling with dimensionality , the methods of this document have limited usefulness on
their own .
they may be useful in highly - correlated low - dimensional problems , though , and can be used to
take steps in highly - correlated low - dimensional subspaces as part of larger sampling schemes .
we hope to
address this limitation in future work , possibly with polar slice sampling ( roberts and rosenthal , 123 ) .
an r implementation of these methods is available at http : / / www . cs . utoronto . ca / ~ radford .
