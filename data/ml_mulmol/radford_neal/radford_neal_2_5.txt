abstract .
the inferential problem of associating data to mixture components is dif - ( cid : 123 ) cult when components are nearby or overlapping .
we introduce a new split - merge markov chain monte carlo technique that e ( cid : 123 ) ciently classi ( cid : 123 ) es observations by splitting and merging mixture components of a nonconjugate dirichlet process mixture model .
our method , which is a metropolis - hastings procedure with split - merge proposals , sam - ples clusters of observations simultaneously rather than incrementally assigning observa - tions to mixture components .
split - merge moves are produced by exploiting properties of a restricted gibbs sampling scan .
a simulation study compares the new split - merge technique to a nonconjugate version of gibbs sampling and an incremental metropolis - hastings technique .
the results demonstrate the improved performance of the new
keywords : bayesian model , markov chain monte carlo , split - merge moves , nonconjugate prior
bayesian mixture models have gained in popularity as an alternative to traditional density estimation and clustering techniques .
in particular , bayesian mixture models in which a dirichlet process prior de ( cid : 123 ) nes the mixing distribution are of interest due to their ( cid : 123 ) exibility in ( cid : 123 ) tting a countably in ( cid : 123 ) nite number of components ( ferguson ( 123 ) ) .
much of the recent research related to the dirichlet process mixture model has been devoted to developing computational techniques , usually markov chain monte carlo methods , to sample from its posterior distribution ( neal ( 123 ) , maceachern and m ( cid : 123 ) uller ( 123 ) ) .
other techniques to estimate the dirichlet process model include sequential importance sampling ( maceachern et al .
( 123 ) ) and variational methods ( blei and jordan ( 123 ) ) .
the practical utility of these methods is illustrated by their recent use for complex bi - ological and genetics problems , such as haplotype reconstruction ( xing et al .
( 123 ) ) , estimation of rates of non - synonymous and synonymous nucleotide substitutions as evi - dence for natural selection in evolutionary biology problems ( huelsenbeck et al .
( 123 ) ) , and determination of di ( cid : 123 ) erential gene expression ( do et al .
( 123 ) ) .
the focus of this article is on markov chain sampling for nonconjugate dirichlet pro - cess mixture models , building on our previous work for conjugate models ( jain and neal ( 123 ) ) .
conjugate models are appropriate for some problems , which is convenient due
( cid : 123 ) division of biostatistics and bioinformatics , department of family and preventive medicine , uni -
versity of california at san diego , la jolla , ca , mailto : sojain@ucsd . edu
ydepartment of statistics and department of computer science , university of toronto , toronto ,
ontario , canada , http : / / www . cs . toronto . edu / ~ radford /
c ( cid : 123 ) 123 international society for bayesian analysis
splitting and merging components of a nonconjugate dpmm
to the analytical tractability of these priors .
however , in many situations , conjugate priors can be too restrictive .
forcing conjugacy on the model can lead to undesirable or even nonsensical priors .
a classic example is a simple model for normally distributed data , where conjugacy requires an assumption that the mean and variance are a priori dependent , which is often unrealistic in actual problems .
computationally , markov chain sampling procedures can operate di ( cid : 123 ) erently depend - ing on whether conjugacy is assumed .
in the conjugate case , we can analytically in - tegrate away the mixing proportions for the components and the parameters for each component .
this leads to markov chain monte carlo procedures that update only the latent indicator variable associating mixture components with data observations ( maceachern ( 123 ) , neal ( 123 ) ) .
however , in the nonconjugate case , the parameters of the model cannot be integrated away and must be included in the markov chain update .
further , since we lose the advantage of analytic tractability , computational di ( cid : 123 ) culties arise , which makes it more di ( cid : 123 ) cult , but not impossible , to construct valid markov chain monte carlo procedures .
nonconjugate markov chain sampling methods based on the gibbs sampler have been proposed previously; see , for instance , maceachern and m ( cid : 123 ) uller ( 123 ) and neal ( 123 ) .
when the mixture components are nearby or overlapping , these incremental samplers ( as well as those for conjugate models ) su ( cid : 123 ) er from computational di ( cid : 123 ) culties , such as remaining stuck in isolated modes and poor mixing between components .
alternative nonincremental markov chain samplers for the dirichlet process mixture model based on split - merge moves have been proposed by green and richardson ( 123 ) and by ourselves ( jain and neal ( 123 ) ) .
in a single iteration , these methods can split a mixture component moving all observations to an appropriate new component , or merge two distinct components together .
the green and richardson ( 123 ) method is based on the reversible - jump procedure , in which numerous ways to propose a split move are possible .
since speci ( cid : 123 ) c moment conditions must be preserved , the split - merge proposals are model - dependent .
jain and neal ( 123 ) introduce a metropolis - hastings technique with split - merge proposals for conjugate dirichlet process mixture models .
the inno - vation in this work is exploiting properties of a gibbs sampling scan to construct split - merge moves , such that their metropolis - hastings proposals are model - independent .
in this article , we extend the conjugate split - merge technique to a class of nonconjugate dirichlet process mixture models by developing a novel scheme to incorporate the model parameters into the sampling procedure .
this article is organized as follows .
section 123 de ( cid : 123 ) nes the nonconjugate dirichlet process mixture model .
section 123 brie ( cid : 123 ) y describes the metropolis - hastings split - merge technique based on gibbs sampling proposals .
the new split - merge technique for a class of nonconjugate models is proposed in section 123
next , in section 123 , we illustrate the utility of our method in by comparing it to an auxiliary gibbs sampling method ( neal ( 123 ) , algorithm 123 ) .
section 123 is a general discussion and concluding remarks .
details of a simulation study are provided in the appendix in section 123
jain and r
123 the model
the dirichlet process mixture model takes the following hierarchical model form for observed data y = ( y123; : : : ; yn ) that is considered exchangeable :
yi j ( cid : 123 ) i ( cid : 123 ) f ( ( cid : 123 ) i ) ( cid : 123 ) i j g ( cid : 123 ) g
g ( cid : 123 ) dp ( g123; ( cid : 123 ) )
here , f ( ( cid : 123 ) i ) is a component parameterized by ( cid : 123 ) i from a parametric distribution whose density will be written as f ( y; ( cid : 123 ) ) .
g is the mixing distribution .
g123 de ( cid : 123 ) nes a base distribution for the dirichlet process ( dp ) prior , while ( cid : 123 ) is a concentration parameter that takes values greater than zero .
the usual conditional independence assumptions for a hierarchical model apply , so that the only dependencies are those that are explicitly
realizations of the dirichlet process are discrete with probability one .
a conse - quence of this is that the mixture model in equation ( 123 ) can be viewed as a countably in ( cid : 123 ) nite mixture model ( ferguson ( 123 ) ) .
this is evident when we simplify the model in equation ( 123 ) by integrating g over its prior distribution .
the ( cid : 123 ) i follow a generalized polya urn scheme ( blackwell and macqueen ( 123 ) ) and the prior distribution for the ( cid : 123 ) i may be represented by the following conditional distributions :
( cid : 123 ) 123 ( cid : 123 ) g123
( cid : 123 ) i j ( cid : 123 ) 123; : : : ; ( cid : 123 ) i ( cid : 123 ) 123 ( cid : 123 )
( cid : 123 ) ( ( cid : 123 ) j ) +
where ( cid : 123 ) ( ( cid : 123 ) j ) is the distribution which is a point mass at ( cid : 123 ) j .
we can represent the fact that ( 123 ) results in some of the ( cid : 123 ) i being identical by setting ( cid : 123 ) i = ( cid : 123 ) ci , where ci represents the latent class associated with observation i , and all ( cid : 123 ) c are independently drawn from g123
the polya urn scheme for sampling the ( cid : 123 ) i is equivalent to the following scheme for sampling the latent variables , ci , and associated ( cid : 123 ) c :
p ( ci = c j c123; : : : ; ci ( cid : 123 ) 123 ) =
p ( ci 123= cj for all j < i j c123; : : : ; ci ( cid : 123 ) 123 ) =
i ( cid : 123 ) 123 + ( cid : 123 )
i ( cid : 123 ) 123 + ( cid : 123 )
for c 123 fcjgj<i
where ni;c is the number of ck for k < i that are equal to c .
the probabilities shown in ( 123 ) de ( cid : 123 ) ne the dirichlet process model .
this notation will be employed in subsequent
123 jain and neals conjugate split - merge procedure
we have previously introduced a split - merge metropolis - hastings procedure for conju - gate dirichlet process mixture models ( jain and neal ( 123 ) ; jain ( 123 ) ) .
in the con - jugate version of the algorithm , we assume that f is conjugate to g123 in equation ( 123 ) , so
splitting and merging components of a nonconjugate dpmm
the model parameters , ( cid : 123 ) c , in addition to the mixing distribution , g , can be integrated away .
the state of the markov chain consists only of the mixture component indicators ,
this sampler proposes nonincremental moves that can produce major changes to the con ( cid : 123 ) guration of observations to mixture components in a single iteration .
the split - merge proposals are evaluated by a metropolis - hastings procedure , in which split proposals are constructed by exploiting properties of a restricted gibbs sampling scan on the component indicators , ci .
the gibbs sampling scan is restricted in that it is only performed on a subset of the data ( the observations associated with the merged component that is proposed to be split ) and will only allocate observations between two
to achieve more reasonable split proposals , several intermediate restricted gibbs sampling scans are conducted prior to the ( cid : 123 ) nal restricted gibbs sampling scan , which is used to calculate the metropolis - hastings acceptance probability .
the result of the last intermediate gibbs sampling scan is denoted as the random launch state , from which the restricted gibbs sampling transition probability is explicitly calculated .
the number of intermediate restricted gibbs sampling scans is considered a tuning parameter of this
note that for a merge proposal , there is only one way to combine items in two components to one component .
however , deciding whether to accept or reject a merge proposal requires hypothetical consideration of the reverse split , which requires compu - tations similar to those done for an actual split .
a description of the steps involved in this algorithm , details to compute the metropolis - hastings acceptance probability , and a discussion of the validity of the conjugate version of the split - merge metropolis - hastings algorithm are provided in jain and neal ( 123 ) .
123 the nonconjugate split - merge procedure
we adapt jain and neals conjugate split - merge markov chain procedure described in section 123 to accommodate models with nonconjugate priors .
as mentioned earlier , because conjugate priors are not appropriate for all modeling situations , much of the recent bayesian mixture modeling literature has been dedicated to nonconjugate al - gorithms ( for instance , maceachern and m ( cid : 123 ) uller ( 123 ) , green and richardson ( 123 ) , and neal ( 123 ) ) .
a major impediment in designing nonconjugate procedures is the computational di ( cid : 123 ) culty that arises when the model is no longer analytically tractable .
we say the model is nonconjugate when g123 is not conjugate to f in the mixture model ( equation 123 ) .
aside from being unable to simplify the state of the markov chain by integrating away the model parameters , ( cid : 123 ) , the main obstacle occurs when trying to sample for a new mixture component .
when a ci is updated , it can be set either to one of the other components currently associated with some observation or to a new mixture component .
the probability of setting ci to a new component involves the
integral , r f ( yi; ( cid : 123 ) ) dg123 ( ( cid : 123 ) ) , which is analytically intractable in most nonconjugate situ -
jain and r
ations .
allowances that some previous nonconjugate methods have made when dealing with this integral include approximating the true posterior distribution by another sta - tionary distribution ( which can be extremely detrimental ) or creating model - speci ( cid : 123 ) c ad hoc algorithms ( which fail to generalize well ) .
neal ( 123 ) proposed two incremental markov chain sampling procedures : gibbs sampling with auxiliary parameters ( algorithm 123 ) , and an incremental metropolis - hastings technique ( algorithm 123 ) .
these are exact markov chain monte carlo methods that sample the correct posterior distribution and are straightforward to implement .
however , in situations where the mixture components are nearby or similar in struc - ture , these incremental methods performance is analogous to the incremental methods for conjugate models ( see jain and neal ( 123 ) ) .
to overcome their problems , such as remaining stuck in isolated modes and poor mixing between mixture components , we have developed a nonincremental split - merge alternative .
in the next section , we compare empirically the performance of the new sampler to neals two incremental
in this article , we show how such a nonincremental split - merge procedure can be applied when the model uses a particular type of nonconjugate prior , the conditionally conjugate family of priors .
in conditionally conjugate models , it is still impossible
to e ( cid : 123 ) ciently compute the integral , r f ( yi; ( cid : 123 ) ) dg123 ( ( cid : 123 ) ) .
however , the pair f and g123
are conditionally conjugate in one model parameter if the remaining parameters are held ( cid : 123 ) xed .
a well - known instance of this is the following normal model .
suppose the observations , y123; : : : ; yn , are distributed as f ( yi; ( cid : 123 ) ; ( cid : 123 ) 123 ) = normal ( yi; ( cid : 123 ) ; ( cid : 123 ) 123 ) , and the prior is g123 ( ( cid : 123 ) ; ( cid : 123 ) ( cid : 123 ) 123 ) = normal ( ( cid : 123 ) ; w; b ( cid : 123 ) 123 ) ( cid : 123 ) gamma ( ( cid : 123 ) ( cid : 123 ) 123; r; r ) .
the distributions , f ( yi; ( cid : 123 ) ; ( cid : 123 ) 123 ) and g123 ( ( cid : 123 ) ; ( cid : 123 ) ( cid : 123 ) 123 ) , are conjugate in ( cid : 123 ) when ( cid : 123 ) 123 is ( cid : 123 ) xed , and conjugate in ( cid : 123 ) 123 if ( cid : 123 ) is ( cid : 123 ) xed .
but , the joint posterior distribution is not analytically tractable .
for the sake of brevity , when this nonconjugate normal - gamma prior is applied to a normal mixture model , we will refer to it as the normal - gamma mixture model .
note , however , that this model using a conjugate prior , in which the mean and variance are a priori dependent , is sometime referred to similarly .
123 restricted gibbs sampling split - merge proposals
the conjugate split - merge algorithm of section 123 cannot be applied directly to the con - ditionally conjugate case , but the basic mechanism of creating restricted gibbs sampling split - merge proposals can still be applied .
since the model parameters , ( cid : 123 ) c , cannot be integrated away , the state of the markov chain for the split - merge sampler consists of both the component indicators and model parameters , denoted by ( cid : 123 ) = ( c; ( cid : 123 ) ) , where c = ( c123; : : : ; cn ) and ( cid : 123 ) = ( ( cid : 123 ) c : c 123 fc123; : : : ; cng ) .
conditional conjugacy in the model is required so that restricted gibbs sampling scans can be performed to allocate observations reasonably between two mixture com -
ponents .
during these scans , we do not need to compute the integral , r f ( yi; ( cid : 123 ) ) dg123 ( ( cid : 123 ) ) ,
since we are only allocating observations between two known components that have at least one observation already assigned to them .
for a nonconjugate model , a restricted
splitting and merging components of a nonconjugate dpmm
gibbs sampling scan also updates the parameters for the a ( cid : 123 ) ected mixture components , while holding the parameters of the other components ( cid : 123 ) xed .
note that use of a re - stricted gibbs sampling scan ( and consequently , conditional conjugacy ) is only crucial for the ( cid : 123 ) nal gibbs sampling scan from the launch state , since it allows the metropolis - hastings proposal density can be calculated .
the intermediate scans could be replaced by some other type of markov chain update .
due to the inclusion of the model parameters , when two separate components are being merged to a single component , there is no longer only one possible component to merge into .
the merged component is now de ( cid : 123 ) ned by component parameters , which must be accounted for in the metropolis - hastings acceptance probability ( in sec - tion 123 ) .
the algorithm addresses this problem by conducting intermediate restricted gibbs sampling for the merged components parameters to arrive at a launch state ( in a similar fashion as the \split " intermediate gibbs sampling ) .
from this launch state , one ( cid : 123 ) nal restricted gibbs sampling scan is performed to obtain the model parameters of the proposed merged component .
the number of intermediate gibbs sampling scans for the merged components parameters is an additional tuning parameter in this al - gorithm .
in this generalized version of the split - merge algorithm , there are therefore two launch states , ( cid : 123 ) lsplit and ( cid : 123 ) lmerge , that are necessary in order to calculate gibbs sampling transition kernels for the split and merge proposal distributions .
123 restricted gibbs sampling split - merge procedure for the noncon -
let the state of the markov chain consist of ( cid : 123 ) = ( c; ( cid : 123 ) ) where c = ( c123; : : : ; cn ) and ( cid : 123 ) = ( ( cid : 123 ) c : c 123 fc123; : : : ; cng ) .
select two distinct observations , i and j , at random uniformly .
let s denote the set of observations , k 123 f123; : : : ; ng , for which k 123= i and k 123= j , and
ck = ci or ck = cj .
de ( cid : 123 ) ne launch states , ( cid : 123 ) lsplit and ( cid : 123 ) lmerge , that will be used to de ( cid : 123 ) ne gibbs sampling
distributions required for the split and merge proposals .
( cid : 123 ) obtain launch state ( cid : 123 ) lsplit = ( clsplit ; ( cid : 123 ) lsplit ) as follows :
( if ci = cj , then let c
be set to a new component such that
= ci and c
=123 fc123; : : : ; cng and let c
pendently with equal probability , to either of the distinct components , c
for every k 123 s , randomly set c
initialize model parameters , ( cid : 123 )
otherwise , when ci
, associated with
the two distinct components by drawing new values from their prior distribu -
( modify ( cid : 123 ) lsplit by performing t intermediate restricted gibbs sampling scans
to update clsplit , ( cid : 123 )
, and ( cid : 123 )
( cid : 123 ) obtain launch state ( cid : 123 ) lmerge = ( clmerge ; ( cid : 123 ) lmerge ) as follows :
jain and r
( if ci = cj , then let clmerge if ci 123= cj , then set clmerge initialize model parameter , ( cid : 123 ) lmerge
= cj ( which is the same as ci ) .
similarly , = cj .
for every k 123 s , set clmerge , associated with the merged component
= cj .
by drawing a new value from its prior distribution .
( modify ( cid : 123 ) lmerge by performing r intermediate restricted gibbs sampling scans
to update ( cid : 123 ) lmerge
if items i and j are in the same mixture component , i . e .
ci = cj , then :
( a ) propose a new assignment of data items to mixture components , denoted as csplit ,
in which component ci = cj is split into two separate components , csplit and propose new values for the corresponding components parameters , ( cid : 123 ) split
de ( cid : 123 ) ne each element of the candidate state , ( cid : 123 ) split = ( csplit; ( cid : 123 ) split ) , as
( note that c
( cid : 123 ) let csplit ( cid : 123 ) let csplit ( cid : 123 ) by conducting one ( cid : 123 ) nal gibbs sampling scan from the launch state , ( cid : 123 ) lsplit ,
be set to either component csplit
( which is the same as cj )
=123 fc123; : : : ; cng )
for every observation k 123 s , let csplit and draw values for the model parameters , ( cid : 123 ) split
( cid : 123 ) for observations k =123 s ( fi; jg , let csplit
= ck , and for c =123 fcsplit
csplit = ( cid : 123 ) c .
( b ) compute the proposal densities , q ( ( cid : 123 ) splitj ( cid : 123 ) ) and q ( ( cid : 123 ) j ( cid : 123 ) split ) , that will be used to
calculate the metropolis - hastings acceptance probability .
( cid : 123 ) calculate the split proposal density , q ( ( cid : 123 ) splitj ( cid : 123 ) ) , by computing the gibbs sam - pling transition kernel from the split launch state , ( cid : 123 ) lsplit , to the ( cid : 123 ) nal proposed state , ( cid : 123 ) split .
the gibbs sampling transition kernel is the product of the in - dividual probabilities of setting each element in the launch state to its ( cid : 123 ) nal proposed value during the ( cid : 123 ) nal gibbs sampling scan .
( cid : 123 ) calculate the corresponding proposal density , q ( ( cid : 123 ) j ( cid : 123 ) split ) , by computing the gibbs sampling transition kernel from the merge launch state , ( cid : 123 ) lmerge , to the original merged con ( cid : 123 ) guration , ( cid : 123 ) .
the gibbs sampling transition kernel is the product of the probability of setting each element in the original merge state ( in this case , elements of ( cid : 123 ) cj ) to its original value in a ( hypothetical ) gibbs sampling scan from the merge launch state .
( c ) evaluate the proposal by the metropolis - hastings acceptance probability a ( ( cid : 123 ) split; ( cid : 123 ) ) .
if the proposal is accepted , ( cid : 123 ) split becomes the next state in the markov chain .
if the proposal is rejected , the original con ( cid : 123 ) guration and model parameter , ( cid : 123 ) , remain as the next state .
otherwise , if i and j are in di ( cid : 123 ) erent mixture components , i . e .
ci 123= cj , then :
( a ) propose a new assignment of data items to mixture components , denoted as cmerge , in which distinct components , ci and cj , are combined into a single component , and propose a new value for the corresponding merged components model parameter , .
de ( cid : 123 ) ne each element of the candidate state , ( cid : 123 ) merge = ( cmerge; ( cid : 123 ) merge ) , as
splitting and merging components of a nonconjugate dpmm
( cid : 123 ) let cmerge ( cid : 123 ) let cmerge ( cid : 123 ) for every observation k 123 s , let cmerge ( cid : 123 ) for observations k =123 s ( fi; jg ,
( which is the same as cj )
( which is the same as cj )
cmerge = ( cid : 123 ) c .
( which is the same as cj )
= ck , and for c 123= cmerge , let
( cid : 123 ) conduct one ( cid : 123 ) nal restricted gibbs sampling scan from the launch state ,
( cid : 123 ) lmerge , in order to draw a new value for the model parameter , ( cid : 123 ) merge
( b ) compute the proposal densities , q ( ( cid : 123 ) mergej ( cid : 123 ) ) and q ( ( cid : 123 ) j ( cid : 123 ) merge ) , that will be used to
calculate the metropolis - hastings acceptance probability .
( cid : 123 ) calculate the merge proposal density , q ( ( cid : 123 ) mergej ( cid : 123 ) ) , by computing the gibbs sampling transition kernel from the merge launch state , ( cid : 123 ) lmerge , to the ( cid : 123 ) nal proposed state , ( cid : 123 ) merge .
the gibbs sampling transition kernel is the probability of setting ( cid : 123 ) lmerge , via one gibbs sampling
to its ( cid : 123 ) nal proposed value , ( cid : 123 ) merge
( cid : 123 ) calculate the corresponding proposal density , q ( ( cid : 123 ) j ( cid : 123 ) merge ) , by computing the gibbs sampling transition kernel from the split launch state , ( cid : 123 ) lsplit , to the original split con ( cid : 123 ) guration , ( cid : 123 ) .
the gibbs sampling transition kernel is the product of the probabilities of setting each element in the original split state to its original value in a ( hypothetical ) gibbs sampling scan from the split
( c ) evaluate the proposal by the metropolis - hastings acceptance probability a ( ( cid : 123 ) merge; ( cid : 123 ) ) .
if the proposal is accepted , ( cid : 123 ) merge becomes the next state .
if the merge proposal is rejected , the original con ( cid : 123 ) guration and model parameters , ( cid : 123 ) , remain as the next
123 the metropolis - hastings acceptance probability
the metropolis - hastings acceptance probability ( metropolis et al .
( 123 ) , hastings ( 123 ) ) takes the following form when updating ( cid : 123 ) = ( c; ( cid : 123 ) ) :
a ( ( cid : 123 ) ( cid : 123 ) ; ( cid : 123 ) ) = min ( cid : 123 ) 123;
p ( ( cid : 123 ) ( cid : 123 ) )
where ( cid : 123 ) ( cid : 123 ) is either ( cid : 123 ) split or ( cid : 123 ) merge depending on the type of proposal .
the prior distribution , p ( ( cid : 123 ) ) , will be a product of the individual prior distributions for c and ( cid : 123 ) , since they are a priori independent .
as before , the prior distribution for p ( c ) will be a product of factors in equation ( 123 ) .
the ( cid : 123 ) c for di ( cid : 123 ) erent mixture components are independent .
therefore , the prior distribution for p ( ( cid : 123 ) ) is :
p ( ( cid : 123 ) ) = p ( c ) yc 123 c k=123 ( ( cid : 123 ) +k ( cid : 123 ) 123 ) yc 123 c
= ( cid : 123 ) d qc 123 c ( nc ( cid : 123 ) 123 ) !
jain and r
where d is the number of distinct mixture components , nc is the count of items belonging to mixture component c 123 c , and g ( ( cid : 123 ) c ) is the prior probability density function for ( cid : 123 ) c for mixture component c 123 c .
for the split proposal , the appropriate ratio of prior distributions is :
p ( ( cid : 123 ) split )
( nci ( cid : 123 ) 123 ) ! g ( ( cid : 123 ) ci )
where ( cid : 123 ) is the original state in which i and j belong to the same mixture component , are the number of observations associated with each split component .
the ratio of the prior distributions simpli ( cid : 123 ) es because the denominator in equation ( 123 ) and factors not associated with components that are directly involved in the metropolis - hastings update cancel .
for the merge proposal , the prior ratio simpli ( cid : 123 ) es to :
( nci ( cid : 123 ) 123 ) ! ( ncj ( cid : 123 ) 123 ) ! g ( ( cid : 123 ) ci ) g ( ( cid : 123 ) cj )
denotes the number of observations associated with the single merged component .
( cid : 123 ) represents the original state in which items i and j belong to separate
the likelihood , l ( ( cid : 123 ) jy ) , will be a product over n observations :
f ( yk; ( cid : 123 ) ck )
l ( ( cid : 123 ) jy ) can be expressed as a double product over components , c , and items , k 123 f123; : : : ; ng , associated with each component :
yc=123 yk : ck=c
f ( yk; ( cid : 123 ) c )
where d is the number of distinct components .
this expression to calculate the likeli - hood is often easier to use in real examples .
likelihood factors involving items associated with components not directly involved in the split proposal cancel .
the ratio of likelihoods in equation ( 123 ) reduces to the
yk : csplit
f ( yk; ( cid : 123 ) split
) yk : csplit
f ( yk; ( cid : 123 ) ci )
yk : ck=ci
f ( yk; ( cid : 123 ) split
splitting and merging components of a nonconjugate dpmm
likewise , for the merge proposal , the ratio of likelihoods is :
yk : cmerge yk : ck=ci
f ( yk; ( cid : 123 ) ci ) yk : ck=cj
f ( yk; ( cid : 123 ) merge
f ( yk; ( cid : 123 ) cj )
the metropolis - hastings proposal density , q ( ( cid : 123 ) ( cid : 123 ) j ( cid : 123 ) ) , is the restricted gibbs sampling transition kernel from launch state ( cid : 123 ) l to ( cid : 123 ) nal state ( cid : 123 ) ( cid : 123 ) .
this is a product of the conditional probabilities of each individual update of the vector c ( cid : 123 ) from cl and the conditional densities of assigning successive components of ( cid : 123 ) l to their ( cid : 123 ) nal values , ( cid : 123 ) ( cid : 123 ) .
typically , for each mixture component , ( cid : 123 ) is composed of more than one model parameter , i . e .
each ( cid : 123 ) c can be a vector of parameters .
for example , in the normal model , there are two parameters per component , ( cid : 123 ) c = ( ( cid : 123 ) c; ( cid : 123 ) 123 c ) .
in a gibbs sampling scan , each element of parameter ( cid : 123 ) c is updated individually , while holding the other elements of ( cid : 123 ) c ( cid : 123 ) xed .
a single element of ( cid : 123 ) c is updated in a restricted gibbs sampling scan by drawing a new value from its full conditional distribution .
we will denote the product of conditional probabilities obtained from one full scan of restricted gibbs sampling as pgs .
since ( cid : 123 ) is comprised of both c and ( cid : 123 ) , for clarity , we can split the gibbs sampling transition kernel into its factors .
the order of updating the variables does not a ( cid : 123 ) ect the validity of the method , but for presentation purposes , we assume that gibbs sampling updates ( cid : 123 ) ( cid : 123 ) rst ( as is done in the later examples ) :
q ( ( cid : 123 ) ( cid : 123 ) j ( cid : 123 ) ) = pgs ( ( cid : 123 ) ( cid : 123 ) j ( cid : 123 ) l; cl; y ) ( cid : 123 ) pgs ( c ( cid : 123 ) j cl; ( cid : 123 ) ( cid : 123 ) ; y )
an individual update of a particular ck is as follows :
p ( ck j c ( cid : 123 ) k; ( cid : 123 ) ck ; yk ) =
n ( cid : 123 ) k;ck f ( yk; ( cid : 123 ) ck )
n ( cid : 123 ) k;ci f ( yk; ( cid : 123 ) ci ) + n ( cid : 123 ) k;cj f ( yk; ( cid : 123 ) cj )
where c ( cid : 123 ) k represents the cl for l 123= k in s ( fi; jg , n ( cid : 123 ) k;c is the number of cl for l 123= k in s ( fi; jg that are equal to c , and f ( yk; ( cid : 123 ) c ) is the likelihood .
here , ck is restricted to being either ci or cj .
each time a ck or ( cid : 123 ) ck is incrementally modi ( cid : 123 ) ed during a restricted gibbs sampling scan , it is immediately used in the subsequent gibbs sampling computation .
the required ratios for the split and merge proposals are shown below in equa - tions ( 123 ) and ( 123 ) , respectively .
for the merge proposal , there is still only one way to combine items in two components into one component , so pgs ( cjclmerge ; ( cid : 123 ) ; y ) = 123 in equation ( 123 ) .
the same is true for p ( cmergejclmerge ; ( cid : 123 ) merge; y ) in equation ( 123 ) .
however , since speci ( cid : 123 ) c parameters now de ( cid : 123 ) ne the mixture components , there are nu - merous possibilities for choosing a particular mixture component .
we address this , in a similar method as the split scenario , by conducting intermediate gibbs sampling scans to decide the value of the merged components parameters .
one ( cid : 123 ) nal gibbs sampling scan is conducted from the launch state to calculate the gibbs sampling transition
jain and r
the ratio of transition densities for the split proposal is :
pgs ( ( cid : 123 ) ci j ( cid : 123 ) lmerge ; clsplit ; y ) pgs ( ( cid : 123 ) split
; clmerge ; y ) pgs ( cjclmerge ; ( cid : 123 ) ; y )
; clsplit ; y ) pgs ( csplitjclsplit ; ( cid : 123 ) split; y )
pgs ( ( cid : 123 ) ci j ( cid : 123 ) lmerge
; clmerge ; y )
lsplit ; y ) pgs ( ( cid : 123 ) split
lsplit ; y ) pgs ( c
lsplit ; ( cid : 123 )
to calculate q ( ( cid : 123 ) j ( cid : 123 ) split ) , the same intermediate gibbs sampling operations that are performed when proposing a merge must be conducted here to arrive at a suitable merge launch state , even though no actual merge is performed .
the gibbs sampling transition probability is calculated from the launch state ( which is the last intermediate gibbs sampling state ) to the original merged state .
these operations are necessary to produce the correct proposal ratios .
for the merge proposal , the ratio of transition densities is :
pgs ( ( cid : 123 ) ci j ( cid : 123 )
; clsplit ; y ) pgs ( ( cid : 123 ) cj j ( cid : 123 )
; clsplit ; y ) pgs ( cjclsplit ; ( cid : 123 ) ; y )
; clmerge ; y ) pgs ( cmergejclmerge ; ( cid : 123 ) merge; y )
pgs ( ( cid : 123 ) ci j ( cid : 123 )
; clsplit ; y ) pgs ( ( cid : 123 ) cj j ( cid : 123 )
; clsplit ; y ) pgs ( cjclsplit ; ( cid : 123 ) ; y )
; clmerge ; y )
to obtain q ( ( cid : 123 ) j ( cid : 123 ) merge ) , we similarly perform the same intermediate gibbs sampling moves when proposing a split , even though no actual split is proposed ( since it is already known ) .
this time the gibbs sampling transition probability is calculated from the launch state to the original split state .
this ensures correct proposal ratios .
the number of intermediate gibbs sampling scans used to arrive at suitable launch states for both split and merge proposals are tuning parameters of this algorithm .
there is an additional tuning parameter for the nonconjugate split - merge procedure that is not present in the conjugate version , which did not require a merge launch state .
123 validity of the algorithm
the nonconjugate split - merge procedure described here is justi ( cid : 123 ) ed as a valid two - stage random metropolis - hastings procedure .
in the ( cid : 123 ) rst stage , we randomly select of obser - vations i and j to decide which subset of metropolis - hastings proposals will be consid - ered .
in the second stage , we randomly select a launch state from among all possible launch states ( given the selection of observations i and j ) , by means of intermediate gibbs sampling scans .
we then perform a standard metropolis - hastings update with a proposal distribution that depends on the selection of i and j and on the launch state .
splitting and merging components of a nonconjugate dpmm
as discussed by tierney ( 123 ) , a random selection among transitions ( in this case , via random selection of a proposal distribution ) is a valid way of constructing markov chain monte carlo algorithms , as long as all the transitions that might be selected are valid on their own .
a subtle clari ( cid : 123 ) cation should be pointed out regarding the construction of the metropolis -
hastings acceptance probability for the nonconjugate procedure .
when a split is pro - posed from a merged state , only one ( cid : 123 ) c is included in the equations , since the merged component has only one set of parameters associated with it now .
we happen to ini - tially pick ( cid : 123 ) cj to be associated with the observations in the merged component , but this is equivalent to initially selecting ( cid : 123 ) ci since the labels are irrelevant .
to avoid changing dimensions when we compute the metropolis - hastings acceptance probability , we could include the appropriate ( cid : 123 ) ci terms in the computations .
since ( cid : 123 ) ci is an extra parameter for the merged component that is no longer associated with the data , we choose to propose a new value for it during the restricted gibbs sampling scan by drawing from its prior distribution .
this choice conveniently allows the prior density for this term to implicitly cancel with the corresponding term in the proposal density of the acceptance probability , showing that the change in dimensionality is not a problem .
consider the following set - up for the prior and proposal ratios for a split proposal which include the ( cid : 123 ) ci terms .
we intentionally omit the likelihoods and indicator terms for simplicity and
) p ( ( cid : 123 ) split
p ( ( cid : 123 ) ci ) p ( ( cid : 123 ) cj )
; clmerge ) pgs ( ( cid : 123 ) cj j ( cid : 123 ) lmerge ; clsplit; y ) pgs ( ( cid : 123 ) split
; clmerge ; y )
; clsplit; y )
the proposal factor , pgs ( ( cid : 123 ) cij ( cid : 123 ) lmerge
; clmerge ) does not depend on the data , since the ( cid : 123 ) cj factor has been selected earlier to be the merged components parameter .
there - fore , a new draw from ( cid : 123 ) ci s conditional distribution will be equivalent to drawing a new value from its prior distribution , and this will cancel with the prior term , p ( ( cid : 123 ) ci ) .
as a result , the ratios described earlier do not need to include these terms .
the identical situation occurs in the case when a merge is proposed from an original split state and is handled similarly .
note that it is possible to propose any con ( cid : 123 ) guration of observations from any ini - tial state via a sequence of split and then merge proposals .
however , to ensure ( cid : 123 ) - irreducibility on a continuous state space , it must be possible to propose any set of parameter values for each component .
this will be true if each individual restricted gibbs sampling conditional distribution for parameters of components that are involved in a particular split or merge update has a positive probability density of proposing any value .
to ensure that the split - merge algorithm is well - de ( cid : 123 ) ned , the model should satisfy the condition that the distributions f ( yi; ( cid : 123 ) i ) be mutually absolutely continuous for all ( cid : 123 ) in the support of g123
jain and r
123 performance of the nonconjugate split - merge proce -
suppose we consider a normal mixture model , in which the data , y = ( y123; : : : ; yn ) , are independent and identically distributed , such that each observation , yi , given the class , ci , has m normally distributed attributes , ( yi123; : : : ; yim ) .
an observations attributes are independent given the class , ci .
the normal mixture model is commonly used in bayesian mixture analysis because of its simplicity in constructing conditional distribu - tions and ( cid : 123 ) exibility in modeling a number of heterogeneous populations simultaneously .
123 the normal mixture model with normal - gamma prior
we model data from a mixture of normal distributions using a dirichlet process mixture model with normal - gamma prior , as follows :
yi j ( cid : 123 ) i; ( cid : 123 ) i ( cid : 123 ) f ( yi; ( cid : 123 ) i; ( cid : 123 ) i ) = n ( yi; ( cid : 123 ) i; ( cid : 123 ) ( cid : 123 ) 123 ( ( cid : 123 ) i; ( cid : 123 ) i ) j g ( cid : 123 ) g
g ( cid : 123 ) dp ( g123; ( cid : 123 ) )
g123 ( ( cid : 123 ) ; ( cid : 123 ) ) = n ( ( cid : 123 ) ; w; b ( cid : 123 ) 123 ) ( cid : 123 ) gamma ( ( cid : 123 ) ; r; r )
where ( cid : 123 ) , the precision parameter , is ( cid : 123 ) ( cid : 123 ) 123
hyperpriors could be placed on w; b; r , and r to add another stage to this hierarchy if desired .
here , we consider these parameters to be known .
the probability density function for the prior distribution of ( cid : 123 ) given in ( 123 ) is :
g ( ( cid : 123 ) j w; b ) = ( cid : 123 ) b
( ( cid : 123 ) ( cid : 123 ) w ) 123 ( cid : 123 )
where b is a precision parameter .
the probability density function for the prior for ( cid : 123 ) is :
g ( ( cid : 123 ) j r; r ) =
( cid : 123 ) r ( cid : 123 ) 123exp ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
this parameterization of the gamma density is adopted throughout this section .
these priors , equations ( 123 ) and ( 123 ) , are necessary to compute the priors for the
parameters in the metropolis - hastings acceptance probability of equation ( 123 ) .
it is straightforward to set up the conditional distributions required for the restricted gibbs sampling in the split - merge procedure used in the metropolis - hastings proposal densities .
for the model parameters , this amounts to sampling from the marginal posterior distributions for a particular parameter of component c .
the conditional posterior distribution for ( cid : 123 ) ch ( when ( cid : 123 ) ch is known ) for a speci ( cid : 123 ) c attribute h is :
( cid : 123 ) ch j c; y; ( cid : 123 ) ch; w; b ( cid : 123 ) n ( cid : 123 ) w b + ( cid : 123 ) ych nc ( cid : 123 ) ch
b + nc ( cid : 123 ) ch
b + nc ( cid : 123 ) ch ( cid : 123 )
splitting and merging components of a nonconjugate dpmm
where nc is the number of observations belonging to component c and ( cid : 123 ) ych is the mean of these observations for attribute h .
similarly , if ( cid : 123 ) ch is ( cid : 123 ) xed , the conditional posterior distribution for ( cid : 123 ) ch for a particular
attribute h is :
( cid : 123 ) ch j c; y; ( cid : 123 ) ch; r; r ( cid : 123 ) gamma123
( ykh ( cid : 123 ) ( cid : 123 ) ch ) 123
the conditional posterior distribution for an indicator variable , ci , is obtained by combining the probability of the data ( given in equation 123 ) given a value for ci with the prior for indicators , p ( c ) .
this yields for c 123 fcjgj123=i :
p ( ci = c j c ( cid : 123 ) i; ( cid : 123 ) c; ( cid : 123 ) c; yi ) / p ( ci = c j c ( cid : 123 ) i ) ( cid : 123 ) p ( yi j ( cid : 123 ) c; ( cid : 123 ) c; c ( cid : 123 ) i )
ch exp ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ch
( yih ( cid : 123 ) ( cid : 123 ) ch )
these conditional distributions are also employed in computations required for gibbs sampling with auxiliary parameters and incremental metropolis - hastings updates that will be used as comparisons to the nonconjugate split - merge technique later in this
the likelihood used in computing acceptance probabilities for split - merge updates is much simpler to obtain than in the conjugate case , since the parameters are not inte - grated away .
for the mixture of normals , the likelihood ( given component indicators )
yc=123 yk : ck=c
yh=123 ( cid : 123 ) ( cid : 123 ) ch
( ykh ( cid : 123 ) ( cid : 123 ) ch ) 123 ( cid : 123 )
interchanging the products over k and h of equation ( 123 ) yields the following :
yh=123 ( cid : 123 ) ( cid : 123 ) ch
123 xk : ck =c
( ykh ( cid : 123 ) ( cid : 123 ) ch ) 123 !
illustration : beetle data
the dirichlet process mixture model is a useful tool in model - based , unsupervised cluster analysis .
we illustrate the practical utility of our split - merge algorithm with a six - dimensional data set from lubischew ( 123 ) that has been previously used by west et al .
( 123 ) .
the data consists of six measurements of physical characteristics of three species
jain and r
of male beetles for a total of n = 123 beetles .
the three species are chactocnema concina , chactocnema heikertinger , and chactocnema heptapotamica , in which nconc = 123 , nheik = 123 , and nhept = 123
the measurements for the ith beetle are denoted as : yij = ( yi123; : : : ; yi123 ) for i =
( 123; : : : ; 123 ) .
the six measurements are :
y : 123 = width of the ( cid : 123 ) rst joint y : 123 = width of the second joint y : 123 = maximal width of the aedeagus y : 123 = front angle of the aedeagus y : 123 = maximal width of the head y : 123 = aedeagus side - width
^ ( cid : 123 ) 123 = 123 : 123 ^ ( cid : 123 ) 123 = 123 : 123 ^ ( cid : 123 ) 123 = 123 : 123 ^ ( cid : 123 ) 123 = 123 : 123 ^ ( cid : 123 ) 123 = 123 : 123 ^ ( cid : 123 ) 123 = 123 : 123
123 = 123 : 123 123 = 123 : 123 123 = 123 : 123 123 = 123 : 123 123 = 123 : 123 123 = 123 : 123
the objective of our analysis is to recover the three latent classes corresponding to the three di ( cid : 123 ) erent species of beetles without using the species information in the analysis .
we apply the normal - gamma dirichlet process mixture model to this data , identical to equation 123
the dirichlet process parameter , ( cid : 123 ) , is set to one .
the values for the priors of the parameters have been set for each dimension as follows : wj = ( w123; : : : ; w123 ) = ( 123; 123; 123; 123; 123; 123 ) , b ( cid : 123 ) 123 123 ) = ( 123; 123; 123; 123; 123; 123 ) where b is a precision parameter , r = 123 across all six dimensions , and r = 123 across all six
123 ; : : : ; b ( cid : 123 ) 123
j = ( b ( cid : 123 ) 123
we applied the nonconjugate split - merge algorithm ( 123 , 123 , 123 , 123 ) , in which ( cid : 123 ) ve interme - diate gibbs sampling scans were each used to reach the launch states for the split and merge proposals .
one split - merge update was used in a single iteration and one ( cid : 123 ) nal incremental gibbs sampling scan was conducted after the ( cid : 123 ) nal split - merge update .
for comparison purposes , we considered the gibbs sampling technique of neal ( 123 ) with v = 123 auxiliary components to this data .
computation time per iteration is similar for both algorithms .
for each algorithm , results are provided for the case in which all observations are initially assigned to the same mixture component , and each algorithm is run for 123 iterations .
from the two top trace plots given in figure 123 , it is evident that gibbs sampling is unable to separate the data and leaves all observations in the same mixture component .
it is clear that gibbs sampling will take longer to reach equilibrium .
on the other hand , split - merge splits the data into three major clusters ( corresponding to the correct proportion of observations to species , i . e .
123% , 123% and 123% . ) within the ( cid : 123 ) rst twenty
to generate the two bottom trace plots in figure 123 , we set the prior values of wj and b ( cid : 123 ) 123 to be more re ( cid : 123 ) ective of the data .
the values used are : wj = ( w123; : : : ; w123 ) = ( 123; 123; 123; 123; 123; 123 ) and b ( cid : 123 ) 123 gibbs sampling does recover the three di ( cid : 123 ) erent species groups almost immediately , it is important to note that it becomes stuck in a low probability two - component con ( cid : 123 ) gura - tion and mixes poorly .
however , split - merge continues to mix well in a three - component
123 ) = ( 123; 123; 123; 123; 123; 123 )
123 ; : : : ; b ( cid : 123 ) 123
j = ( b ( cid : 123 ) 123
as a ( cid : 123 ) nal check , the simulations were repeated by starting the simulation from
splitting and merging components of a nonconjugate dpmm
a typical state of the competing methods apparent equilibrium distribution .
gibbs sampling stayed in the three - component state that it was started from , con ( cid : 123 ) rming that the three - component state has high posterior probability , and that the di ( cid : 123 ) erence seen is not the result of some bug in the split - merge procedure .
when the simulations were repeated using an initial state in which each observation is in a di ( cid : 123 ) erent component , the gibbs sampler is able to reach equilibrium sooner and performs better .
the results from the beetle data illustration show that gibbs sampling experiences a long burn - in time compared to the nonconjugate split - merge technique and is not always suitable for high - dimensional analysis .
while it is true that the values of the priors for the parameters may not be ideal and that more realistic values may yield better sampling , often in real data analysis , there is no a priori information to suggest reasonable priors .
a markov chain monte carlo technique that can overcome poor choices in priors is preferred , as illustrated here , since this leads to shorter burn - in times and full exploration of the posterior distribution .
the nonincremental split - merge procedure for nonconjugate models introduced in this article avoids the problem of being trapped in local modes , allowing the posterior dis - tribution to be fully explored .
in general , the nonconjugate split - merge procedure can become computationally expensive , but when gibbs sampling or some other incremen - tal procedure fails to reach equilibrium in a sensible amount of time , this procedure becomes necessary .
another related issue is burn - in time .
even if an incremental pro - cedure reaches stationarity within a desired time limit , one must often discard a large number of early iterations , which can lead to poor estimates .
in split - merge type sit - uations , the computational burden of using a nonincremental procedure is o ( cid : 123 ) set by its quick burn - in and dramatic improvement in performance .
to further improve sampling performance in which both large changes to the clustering con ( cid : 123 ) guration and small re ( cid : 123 ) ne - ments are required , we recommend combining split - merge and gibbs sampling updates as a way to reap the bene ( cid : 123 ) ts of both samplers .
in higher dimensions , split - merge procedures continue to work well as the compo - nents are moved closer together .
convergence to the equilibrium distribution is rela - tively quick .
it is possible that the split - merge procedure may break down for very high dimensional problems , because appropriate splits will be rejected , since it will become unlikely that a merge operation from the split state would produce the same merged parameter values as the current state .
however , we have not encountered an example of this .
perhaps this issue arises only in situations where the dimensionality is in the
a possible extension of the split - merge technique is to employ the dahl ( 123 ) se - quentially allocated split - merge sampler as a method to initialize the intermediate gibbs sampling step .
this method could potentially provide a better starting state than our method of performing a random split of items and selecting values for the parameters from the prior .
jain and r
the purpose of the following simulation study is to classify observations into appropriate latent classes using the normal - gamma dirichlet process mixture model .
we can make this problem computationally more di ( cid : 123 ) cult by increasing the dimensionality of the data and by moving the components closer together .
various combinations of these factors were tested on all procedures .
we found that the split - merge procedures outperformed the incremental procedures even in very low - dimensional problems , in which distinct components were visible by eye , showing the di ( cid : 123 ) culty that incremental samplers have in reaching equilibrium even in simple problems when the components are similar .
we will consider two simulated data sets with a ( cid : 123 ) nite number of components .
we expect that the dirichlet process mixture model will model the ( cid : 123 ) nite situation perfectly well without problems such as over ( cid : 123 ) tting , even though the model allows an in ( cid : 123 ) nite number of components .
for each of the two examples , the data are composed of ( cid : 123 ) ve equally - probable mixture components , in which each component is a distribution over m dimensions .
to maintain uniformity amongst the examples , we generated n = 123 observations , strati ( cid : 123 ) ed so that 123 observations came from each of the ( cid : 123 ) ve mixture
data for the two examples were randomly generated from the mixture distributions shown in tables 123 and 123
scatterplots of the data are shown in figures 123 and 123
a standard deviation of 123 was selected for all normal distributions , so that only the means would vary .
the ( cid : 123 ) rst example holds the dimensionality at two .
the second example di ( cid : 123 ) ers from the ( cid : 123 ) rst in that the dimensionality is increased to three , and the components are closer together .
intentional asymmetry is introduced so that three components are more similar than the other two .
this is intended to test whether the nonconjugate split - merge techniques can split in three ways .
the dirichlet process parameter , ( cid : 123 ) , is set to one for all demonstrations .
recall that a small value of ( cid : 123 ) places stronger belief that the number of mixture components in the data is likely to be small .
the parameters of the priors for the parameters on the component distributions have been set to the same values over all dimensions as follows : w = 123 , b = 123=123 , r = 123 , and r = 123
here , b is a precision parameter .
for consistency , these parameters are ( cid : 123 ) xed at these values for all simulations .
in actual problems , these parameters could be set either by prior knowledge or given higher - level priors .
for the two examples , two incremental procedures , gibbs sampling with v = 123 auxil - iary variables , and an incremental metropolis - hastings method , are compared to four versions of the nonconjugate split - merge procedure .
we use four parameters to describe the various split - merge procedures :
number of intermediate gibbs sampling scans to reach the launch state for a split
splitting and merging components of a nonconjugate dpmm
table 123 : true mixture distribution for example 123
p ( ci = c )
p ( yihjci = c ) ; h = 123; 123
n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 )
table 123 : true mixture distribution for example 123
p ( ci = c )
p ( yihjci = c ) ; h = 123; 123; 123
n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 ) n ( 123 , 123 )
jain and r
number of split - merge updates done in a single overall iteration
number of complete incremental gibbs sampling scans after the ( cid : 123 ) nal split - merge
number of intermediate gibbs sampling scans to reach the launch state for a
the four split - merge procedures we tested are described using these numbers as split - merge ( 123 , 123 , 123 , 123 ) , split - merge ( 123 , 123 , 123 , 123 ) , split - merge ( 123 , 123 , 123 , 123 ) , and split - merge ( 123 , 123 , 123 , 123 ) .
we compared the split - merge procedures with both the auxiliary variable and
metropolis - hastings incremental samplers because we did not know beforehand which incremental method would perform better in situations where splits and merges might be necessary .
performance of the auxiliary variable gibbs sampling is expected to improve as we increase the number of auxiliary components , except that it also takes longer per iteration ( neal ( 123 ) ) .
we did vary this parameter , but will report ( cid : 123 ) ndings for v = 123 for all examples , since this version is comparable to the best version of split - merge in terms of computation time per iteration .
as the incremental ( cid : 123 ) nal scan for the split - merge procedure , gibbs sampling with one auxiliary variable is used for all
performance measures that were considered include trace plots over time ( figures 123 and 123 ) and computation time per iteration ( table 123 ) .
the trace plots show ( cid : 123 ) ve values which represent the fractions of observations associated with the most common , two most common , three most common , four most common , and ( cid : 123 ) ve most common mixture components .
since each of the ( cid : 123 ) ve components appear equally in the samples , if the true situation were captured exactly , the ( cid : 123 ) ve traces would occur at values of 123 , 123 , 123 , 123 , and 123 .
for each algorithm , all observations were assigned to the same mixture component for the initial state , and each algorithm was run for 123 iterations .
all simulations were performed on matlab , version 123 , on a dell precision 123 workstation ( which has a 123 ghz pentium 123 processor ) .
note that the computation times reported include the extra time spent due to matlabs ine ( cid : 123 ) ciencies when copying and incrementally updating arrays , which are not inherent in the algorithm .
123 . 123 example 123
the three types of procedures , incremental metropolis - hastings , incremental gibbs sam - pling with auxiliary variables , and split - merge , correctly classify the data in figure 123 into ( cid : 123 ) ve distinct clusters .
the main di ( cid : 123 ) erence in performance is the number of burn - in iterations that must be discarded .
the trace plots in figure 123 show that gibbs sampling with three auxiliary param - eters has fewer burn - in iterations than the incremental metropolis - hastings method ( compare 123 to 123 burn - in iterations ) .
however , since the incremental metropolis - hastings method is approximately 123 times faster per iteration than the auxiliary gibbs
splitting and merging components of a nonconjugate dpmm
table 123 : time per iteration ( in seconds ) for the algorithms tested .
example 123 example 123
sampling method , it actually converges sooner with respect to computation time .
split - merge ( 123 , 123 , 123 , 123 ) almost immediately splits the data into ( cid : 123 ) ve components , but notice that the proportions do not occur at exactly 123 intervals until after the ( cid : 123 ) rst thousand iterations .
it takes this procedure longer to move a few singleton observations between components , since there is no ( cid : 123 ) nal incremental update to make these minor adjust - ments .
in ( cid : 123 ) ve thousand iterations , it is not clear if split - merge ( 123 , 123 , 123 , 123 ) has actually reached the equilibrium distribution .
split - merge ( 123 , 123 , 123 , 123 ) does not reach the equilib - rium distribution in the ( cid : 123 ) ve thousand iterations shown .
because the split and merge proposals have no intermediate gibbs sampling scans , the proposals are not expected to be realistic .
split - merge ( 123 , 123 , 123 , 123 ) is essentially a simple random split procedure , except that one restricted gibbs sampling scan is conducted to reach the ( cid : 123 ) nal state , which of course will not lead to reasonable split and merge proposals .
however , either by adding intermediate gibbs sampling scans ( as in the case of split - merge ( 123 , 123 , 123 , 123 ) ) or adding a ( cid : 123 ) nal full incremental scan ( as in split - merge ( 123 , 123 , 123 , 123 ) ) , the correct proportion of items in each cluster is established .
split - merge ( 123 , 123 , 123 , 123 ) eventually reaches the ( cid : 123 ) ve component con ( cid : 123 ) guration after 123 burn - in iterations .
the ( cid : 123 ) nal procedure of figure 123 , split - merge ( 123 , 123 , 123 , 123 ) , ( cid : 123 ) nds the ( cid : 123 ) ve components immediately , and it appears that there is negligible burn - in ( four iterations ) .
the computation time per iteration is higher for split - merge ( 123 , 123 , 123 , 123 ) versus split - merge ( 123 , 123 , 123 , 123 ) and ( 123 , 123 , 123 , 123 ) , but the computation time to equilibrium is much lower .
123 . 123 example 123
example 123 has three dimensions and the mixture components are close together .
a perspective scatterplot of the data is given in figure 123 , and it shows that the compo - nents are di ( cid : 123 ) cult to distinguish .
given the priors selected , there is signi ( cid : 123 ) cant posterior probability for both the four and ( cid : 123 ) ve mixture component con ( cid : 123 ) gurations .
only split - merge ( 123 , 123 , 123 , 123 ) and split - merge ( 123 , 123 , 123 , 123 ) mix between these con ( cid : 123 ) gurations , as observed in figure 123
the incremental samplers and the split - merge procedures with zero in - termediate restricted gibbs sampling scans do not ( cid : 123 ) nd the ( cid : 123 ) ve components over the 123 iterations , but are stuck in either two or four components .
if each item is initially assigned to a di ( cid : 123 ) erent mixture component ( plots not included ) , these samplers do split the data into ( cid : 123 ) ve components , but take a long time to move to four components , indi -
jain and r
cating poor mixing .
here , the problem is that the deletion of a component is rare under both incremental updates and poor split - merge proposals .
comparing further the two procedures that appear to converge , the autocorrelation time for trace 123 is much lower for split - merge ( 123 , 123 , 123 , 123 ) than split - merge ( 123 , 123 , 123 , 123 ) ( 123 vs .
for the autocorrelation time of an indicator variable , i123;123 , coding if observations 123 and 123 are in the same component , the time is much lower for split - merge ( 123 , 123 , 123 , 123 ) ( 123 vs .
even though both algorithms do mix between the two con ( cid : 123 ) gurations and split - merge ( 123 , 123 , 123 , 123 ) is faster per iteration , the improvement in autocorrelation time for split - merge ( 123 , 123 , 123 , 123 ) cannot be ignored .
the extra full scan of incremental sampling for minor adjustments is worth the computational e ( cid : 123 ) ort .
123 . 123 summary of ( cid : 123 ) ndings
it appears that split - merge moves are necessary in nonconjugate problems of this sort .
incremental samplers perform adequately when the components are distinct clusters in low dimensions , but as the components become more di ( cid : 123 ) cult to distinguish , these sam - plers take much longer to reach equilibrium .
it is important to note that the incremental samplers begin to break down even in low dimensions .
the split - merge procedures are able to handle three - way splits without any problems , although this is done by two
the split - merge procedure with several intermediate gibbs sampling scans followed by an incremental full scan is the best version of the split - merge procedure .
the split - merge method relies on proposing appropriate new clusters , which is accomplished by conducting several intermediate scans to reach the split and merge launch states .
the split - merge methods generally have a longer computation time per iteration .
however , in the case of the gibbs sampling procedure with v = 123 auxiliary parameters , the best version of the split - merge procedure , split - merge ( 123 , 123 , 123 , 123 ) , is slightly faster in our implementation ( see table 123 ) .
therefore , there does not appear to be any advantage in using only incremental procedures for these types of problems .
