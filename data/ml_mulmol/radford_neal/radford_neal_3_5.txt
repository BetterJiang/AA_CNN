distribution with density ( x ) is done using updates operating on an ensemble of states .
the current state x is rst stochastically mapped to an ensemble , ( x ( 123 ) , .
, x ( k ) ) .
this ensemble is then updated using mcmc updates that leave invariant a suitable ensemble density , ( x ( 123 ) , .
, x ( k ) ) , dened in terms of ( x ( i ) ) for i = 123 , .
finally a single state is stochastically selected from the ensemble after these updates .
such ensemble mcmc updates can be useful when characteristics of and the ensemble permit ( x ( i ) ) for all i ( 123 , .
, k ) , to be computed in less than k times the amount of computation time needed to compute ( x ) for a single x .
one common situation of this type is when changes to some fast variables allow for quick re - computation of the density , whereas changes to other slow variables do not .
gaussian process regression models are an example of this sort of problem , with an overall scaling factor for covariances and the noise variance being fast variables .
i show that ensemble mcmc for gaussian process regression models can indeed substantially improve sampling performance .
finally , i discuss other possible applications of ensemble mcmc , and its relationship to the multiple - try metropolis method of liu , liang , and wong and the multiset sampler of leman , chen , and lavine .
in this paper , i introduce a class of markov chain monte carlo methods that utilize a state space that is the k - fold cartesian product of the space of interest ie , although our interest is in sampling for x in the space x , we use mcmc updates that operate on ( x ( 123 ) , .
, x ( k ) ) in the space y = x k .
several such methods have previously been proposed for example , adaptive directive sampling
( gilks , roberts , and george , 123 ) and parallel tempering ( geyer , 123; earl and deem , 123 ) .
the ensemble mcmc methods i introduce here dier from these methods in two fundamental respects .
first , use of the space y = x k is temporary after some number of updates on y , one can switch back to the space x , perform updates on that space , then switch back to y , etc .
( of course , one might choose to always remain in the y space , but the option of switching back and forth exists . ) secondly , the invariant density for an ensemble state ( x ( 123 ) , .
, x ( k ) ) in the y space is proportional to the sum of the probabilities of the individual x ( k ) ( or more generally a weighted sum ) , whereas in the previous methods mentioned above , the density is a product of densities for each x ( k ) .
as a consequence , the properties of the ensemble methods described here are quite dierent from those of the methods mentioned above .
i expect that use of an ensemble mcmc method will be advantageous only when , for the particular distributions being sampled from , and the particular ensembles chosen , a computational short - cut exists that allows computation of the density for all of x ( 123 ) , .
, x ( k ) with less than k times the eort needed to compute the density for a single state in x .
without this computational advantage , it is hard to see how performing updates on x k could be benecial .
in this paper , i mostly discuss one particular context where such a computational short - cut exists when the distribution being sampled has the characteristic that after changes to only a subset of fast variables it is possible to re - compute the density in much less time than is needed when other slow variables change .
by using ensembles of states in which the slow variables have the same values in all ensemble members , the ensemble density can be quickly computed .
in the limit as the size of the ensemble grows , it is possible using such a method to approach the ideal of updating the slow variables based on their marginal distribution , integrating over the fast variables .
i apply these fast - slow ensemble mcmc methods to gaussian process regression models ( rasmussen and williams 123; neal 123 ) that have a covariance function with unknown parameters , which in a fully bayesian treatment need to be sampled using mcmc .
the computations for such models require operations on the covariance matrix whose time grows in proportion to n123 , where n is the number of observations .
if these computations are done using the cholesky decomposition of the covariance matrix , a change to only the overall scale of the covariance function does not require recomputation of the cholesky decomposition; hence this scale factor can be a fast variable .
if computations are done by nding the eigenvectors and eigenvalues of the covariance function , both an overall scale factor and the noise variance can be fast variables .
i show that ensemble mcmc with either of these approaches can improve over mcmc on the original state space .
i conclude by discussing other possible applications of ensemble mcmc , and its relationship to the multiple - try metropolis method of liu , liang , and wong ( 123 ) , and to the multiset sampler of leman , chen , and lavine ( 123 ) .
mcmc with an ensemble of states
here , i introduce the idea of using an ensemble of states in general , using the device of stochastically mapping from the space of interest to another space .
i also discuss several ways of dening an ensemble .
i will suppose that we wish to sample from a distribution on x with probability density or mass function ( x ) .
the mcmc approach is to dene a markov chain that has as an invariant distribution .
provided this chain is ergodic , simulating the chain from any initial state for a suitably large number of transitions will produce a state whose distribution is close to .
to implement this strategy , we need to dene a transition probability , t ( x|x ) , for the chain to move to state x if it is currently in state x .
updates made according to this transition probability must leave invariant :
z ( x ) t ( x|x ) dx = ( x )
stochastically mapping to an ensemble and back .
an mcmc update that uses an ensemble of k states can be viewed as probabilistically mapping from the original space , x , to a new space , y = x k , performing updates on this new space , and then mapping back to the original space .
we can formalize this temporary mapping strategy , which has many other applications ( neal 123; neal 123 , section 123 ) , as follows .
we dene a transition distribution , t ( x|x ) , on the space x as the composition of three other stochastic mappings , t , t , and t :
that is , starting from the current state , x , we obtain a value in the temporary space by sampling from t ( y|x ) .
the target distribution for y y has probability / density function ( y ) .
we require that
z ( x ) t ( y|x ) dx = ( y )
t ( y|y ) denes a transition on the temporary space that leaves invariant :
finally , t gets us back to the original space .
it must satisfy
z ( y ) t ( y|y ) dy = ( y )
z ( y ) t ( x|y ) dy = ( x )
the above conditions imply that the overall transition , t ( x|x ) , will leave invariant , and so can be used for markov sampling of .
in ensemble mcmc , where y = x k , with the ensemble state written as y = ( x ( 123 ) , .
, x ( k ) ) , the stochastic mapping t , from x to y , is dened in terms of an ensemble base measure , a distribution with probability density or mass function ( x ( 123 ) , .
, x ( k ) ) , as follows :
t ( x ( 123 ) , .
, x ( k ) | x ) =
k|k ( x ( k ) | x ) x ( x ( k ) )
here , x ( ) is the distribution with mass concentrated at x .
the notation x ( k ) refers to all compo - nents of the ensemble state other than the kth .
the probability density or mass function for the
conditional distribution of components other than the kth given a value for the kth component is k|k ( x ( k ) |x ( k ) ) = ( x ( 123 ) , .
, x ( k ) ) / k ( x ( k ) ) , where k is the marginal density or mass function for the kth component , derived from the joint distribution .
we assume that k ( x ) is non - zero if ( x )
algorithmically , t creates a state in y by choosing an index , k , from 123 to k , uniformly at random , setting x ( k ) to the current state , x , and nally randomly generating all x ( j ) for j 123= k from their conditional distribution under given that x ( k ) = x .
note that t depend only on , not on .
a simple example : as a simple ( but not useful ) illustration , let x = ( 123 , 123 ) and set k = 123
choose the ensemble base measure , , to be uniform on pairs of points with x ( 123 ) = x ( 123 ) + ( mod 123 ) , for some constant ( 123 , 123 ) .
( ie , the ensemble base measure is uniform over pairs of points on a unit circle with the second point at an angle counterclockwise from the rst . ) then 123|123 ( |x ( 123 ) ) = x ( 123 ) + ( mod 123 ) ( ) and 123|123 ( |x ( 123 ) ) = x ( 123 ) ( mod 123 ) ( ) .
the algorithm for the map t is
123 ) pick k uniformly from ( 123 , 123 ) 123 ) set x ( k ) = x 123 ) if k = 123 , set x ( 123 ) = x + ( mod 123 ) ;
if k = 123 , set x ( 123 ) = x ( mod 123 ) .
having dened t as in equation ( 123 ) , the density function , , for the ensemble distribution that will
make condition ( 123 ) hold can be derived as follows :
( x ( 123 ) , .
, x ( k ) ) = z ( x ) t ( x ( 123 ) , .
, x ( k ) | x ) dx
k|k ( x ( k ) | x ( k ) ) ( x ( k ) )
( x ( 123 ) , .
, x ( k ) )
= ( x ( 123 ) , .
, x ( k ) )
so we see that the density function of with respect to the base measure is sometimes simply proportional to the sum of for all the ensemble members ( when the k are all the same uniform distribution ) , and more generally is proportional to the sum of ratios of and k .
the simple example continued : 123 and 123 are both uniform over ( 123 , 123 ) .
for pairs in x 123 satisfying the constraint that x ( 123 ) = x ( 123 ) + ( mod123 ) , the ensemble density follows the proportionality :
( x ( 123 ) , x ( 123 ) ) ( x ( 123 ) ) + ( x ( 123 ) )
( the probability under of a pair that violates the constraint is of course zero . )
we can let t be any sequence of markov updates for y = ( x ( 123 ) , .
, x ( k ) ) that leave invariant
( condition ( 123 ) ) .
we denote the result of applying t to y by y .
the simple example continued : we might dene t to be a sequence of some pre - dened number of metropolis updates ( metropolis , et al 123 ) , using a random - walk proposal , which from state ( x ( 123 ) , x ( 123 ) ) is ( x ( 123 ) ) = ( x ( 123 ) + ( mod123 ) , x ( 123 ) + ( mod123 ) ) , with being drawn from a distribution symmetrical around zero , such as uniform on ( / 123 , + / 123 ) .
such a proposal is accepted with probability min ( 123 , ( ( x ( 123 )
) ) / ( ( x ( 123 ) ) + ( x ( 123 ) ) ) .
) + ( x ( 123 )
to return to a single state , x , from the ensemble state y = ( x ( 123 ) , .
, x ( k ) ) , we set x to one of the x ( k ) , which we select randomly with probabilities proportional to ( x ( k ) ) / k ( x ( k ) ) .
we can see that this t satises condition ( 123 ) as follows :
z ( y ) t ( x|y ) dy
= z " ( x ( 123 ) , .
, x ( k ) )
xj=123z x ( j ) ( x ) ( x ( j ) ) xj=123z ( x ) j|j ( x ( j ) |x ) dx ( j )
( x ) = ( x )
dx ( 123 ) dx ( k ) ( 123 )
( x ( 123 ) , .
, x ( k ) )
the simple example continued : since 123 and 123 are uniform over ( 123 , 123 ) , t simply picks either x ( 123 ) or x ( 123 ) with probabilities proportional to ( x ( 123 ) ) and ( x ( 123 ) ) .
some possible ensembles .
the ensemble base measure , , can be dened in many ways , some of which i will discuss here .
note that the usefulness of these ensembles depends on whether they produce any computational advantage for the distribution being sampled .
this will be discussed in the next section for problems with fast and slow variables , where variations on some of the ensembles discussed here will be used .
one possibility is that x ( 123 ) , .
, x ( k ) are independent and identically distributed under , so that ( x ( 123 ) , .
, x ( k ) ) = ( x ( 123 ) ) ( x ( k ) ) , where ( x ) is the marginal distribution of all the x ( k ) .
the conditional distribution k|k , the components other than x ( k ) will also be independent , with distribution ( x ( k ) ) the same as their marginal distribution .
with this ensemble base measure , the
ordering of states in the ensemble is irrelevant , and the mapping , t , from a single state to an ensemble consists of combining the current state with k 123 states sampled from the marginal .
the mapping , t , from the ensemble to a single state consists of randomly selecting a state from x ( 123 ) , .
, x ( k ) with probabilities proportional to ( x ( k ) ) / ( x ( k ) ) .
rather than the x ( k ) being independent in the ensemble base measure , they might just be exchange - able .
this can be expressed as the x ( i ) being independent given some paramater , which has some prior , ( ) ( which is unrelated to the real prior for a bayesian inference problem ) .
in other words ,
( x ( 123 ) , .
, x ( k ) ) = z ( )
the mapping t can be implemented by sampling from the posterior density ( | x ( k ) = x ) ( ) ( x ( k ) = x | ) , for k randomly chosen from 123 , .
, k ( though due to exchangeability , this random choice of k isnt really necessary ) and then sampling x ( j ) for j 123= k independently from ( x ( j ) | ) ( which is the same for all j ) .
the marginal distributions for the x ( k ) in the ensemble base measure , needed to compute and to map from an ensemble to a single state , are of course all the same when this measure is exchangeable .
another possibility is for the ensemble base measure on x ( 123 ) , .
, x ( k ) to be dened by a stationary markov chain , which again leads to the x ( k ) all having the same marginal distribution , ( x ) .
for example , if x is the reals , we might let ( x ) = n ( x; 123 , 123 ) , where n ( x; , 123 ) is the normal density
function , and k+123|k ( x ( k+123 ) |x ( k ) ) = n ( x ( k+123 ) ; x ( k ) p123 123 / 123 , 123 ) for k = 123 , .
going from a
single state to an ensemble is done by randomly selecting a position , k , for the current state , and then simulating the markov chain forward from x ( k ) to x ( k ) and backward from x ( k ) to x ( 123 ) ( which will be the same as forward simulation when the chain is reversible , as in the example here ) .
the ensemble density is given by equation ( 123 ) , with all the marginal densities k ( x ) equal to ( x ) .
we return from an ensemble to a single state by selecting from x ( 123 ) , .
, x ( k ) with probabilities proportional to
ensemble base measures can also be dened using constellations of points .
let x ( 123 )
some set of k points in x .
we let be the distribution obtained by shifting all these points by an amount , , chosen from some distribution on x , with density ( ) that is nowhere zero , so that
, x ( k )
( x ( 123 ) , .
, x ( k ) ) = z ( )
+ ( x ( k ) ) d
the conditional distributions k|k do not depend on ( ) they are degenerate distributions in which the other constellation points are determined by x ( k ) .
we therefore move from a single point , x , to a constellation of points by selecting k at random from 123 , .
, k , setting x ( k ) to x , and setting x ( j ) for j 123= k to x ( j ) .
the ensemble density , ( x ( 123 ) , .
, x ( k ) ) , will also not depend on ( ) , as can be seen from equation ( 123 ) ) it will be proportional to the sum of ( x ( k ) ) for ensembles that have the shape of the constellation , and be zero for those that do not .
the marginal densities , k ( x ( k ) ) , will be the same for all constellation points , since = x ( k ) x ( k ) will be the same for all k .
( note that this is not the same as the marginal density functions being the same for all k . ) hence moving to a single
+ x x ( k )
point from a constellation is done by choosing a point , x ( k ) , from the constellation with probabilities proportional to ( x ( k ) ) .
the simple example used in the previous section can be seen as a constellation of two points with = 123 , x ( 123 )
= , ( ) uniform over ( 123 , 123 ) , and addition done modulo 123
in the presentation above , the ensemble base measure is assumed to be a proper probability distribution , but can instead be an improper limit of proper distributions , as long as the conditional distributions and ratios of marginal distributions that are needed have suitable limits .
in particular , the conditional distributions k|k used to dene t in equation ( 123 ) must reach limits that are proper distributions; this will also ensure that is well dened ( see equation ( 123 ) ) .
the ratios j ( x ( j ) ) / k ( x ( k ) ) must also reach limits , so that t will be well dened .
we can obviously let ( ) become improper when dening a constellation ensemble , since we have seen that the choice of this density has no eect .
for exchangeable ensembles , we can let ( ) be improper as long as the posterior , ( | x ( k ) = x ) , is proper .
we can also let go to inn - ity in the markov ensemble base measure dened above .
we will then have k+123|k ( x ( k+123 ) |x ( k ) ) = n ( x ( k+123 ) ; x ( k ) , 123 ) , so the conditional distributions k|k are simple random walks in both directions from x ( k ) .
since the marginal distributions for the x ( k ) are all normal with mean zero and variance 123 , for any x ( j ) and x ( k ) , the ratio ( x ( j ) ) / ( x ( k ) ) approaches one as goes to innity .
( note also that since the limit of k|k is proper , the relevant values of x ( k ) will not diverge , so uniform convergence of these ratios is not necessary . )
ensemble mcmc for problems with fast and slow variables
suppose that we wish to sample from a distribution on a space that can be written as a cartesian product , x = x123 x123 , with elements written as x = ( x123 , x123 ) .
suppose also that the probability density or mass function , ( x123 , x123 ) , is such that re - computing the density after a change to x123 is much faster than recomputing the density after a change to x123
that is , if we have computed ( x123 , x123 ) , and saved suitable intermediate results from this computation , we can quickly compute ( x123 , x 123 ) for any 123 , but there is no short - cut for computing ( x 123 123= x123
in general , x123 and x123 might both be multidimensional .
i refer to x123 as the slow variables , and x123 as the fast variables .
123 ) for x
i rst encountered this class of problems in connection with models for the cosmic microwave background ( cmb ) radiation ( lewis and bridle , 123 ) .
this is an example of data modelled using a complex simulation , in this case , of the early universe .
in such problems , the slow variables , x123 , are the unknown parameters of the simulation , which are being t to observed data .
when new values for these parameters are considered , as for a metropolis proposal , computing with these new values will require re - running the simulation , which we assume takes a large amount of computation time .
some fairly simple model relates the output of the simulation to the observed data , with parameters x123 ( for example , the noise level of the observations ) .
when a new value for x123 is considered in conjunction with a value for x123 that was previously considered , the simulation does not need to be re - run , assuming the output from the previous run was saved .
instead , computing with this new x123 and the old x123 requires only some much faster calculation involving the observation model .
fast and slow variables arise in many other contexts as well .
for example , in many bayesian models , the posterior density for a set of low - level parameters involves a large number of data items , which cannot be summarized in a few sucient statistics , while a small number of hyperparameters have densities that depend ( directly ) only on the low - level parameters .
the low - level parameters in such a situation will be slow , since when they are changed , all the data items must be looked at again , but the hyperparameters may be fast , since when they change , but the low - level parameters stay the same , there is no need to look at the data .
later in this paper , i will consider applying methods for fast and slow variables to the more spe - cialized problem of sampling from the posterior distribution of the parameters of a gaussian process regression model , in which an overall scale factor for the covariance function and the noise variance can be regarded as fast variables .
previous methods for problems with fast and slow variables .
a simple way of exploiting fast variables , used by lewis and bridle ( 123 ) , is to perform extra metropolis updates that change only the fast updates , along with less frequent updates that change both fast and slow variables .
lewis and bridle found that this is an improvement over always performing updates for all variables .
similarly , when using a metropolis method that updates only one variable at a time , one could perform more updates for the fast variables than for the slow variables .
i will refer to these as extra update
another simple fast - slow method i devised ( that has been found useful for the cmb modeling problem ) is to randomly choose a displacement in the slow space , 123 x123 , and then perform some pre - determined number of metropolis updates in which the proposal from state ( x123 , x123 ) is ( x123 123 , x where the sign before 123 is chosen randomly , and x 123 is chosen from some suitable proposal distribution , perhaps depending on x123 , x123 , and the chosen sign for 123
during such a sequence of random grid metropolis updates , all proposed and accepted states will have slow variables of the form x123 + i123 , where x123 is the initial value , and i is an integer .
if intermediate results of computations with all these values for the slow variables are saved , the number of slow computations may be much less than the number of metropolis proposals , since the same slow variables may be proposed many times in conjunction with dierent values for the fast variables .
if recomputation after a change to the fast variables is very much faster than after a change to the slow variables , we might hope to nd an mcmc method that samples nearly as eciently as would be possible if we could eciently compute the marginal distribution for just the slow variables , devised a dragging metropolis method ( neal , 123 ) that can be seen as attempting to achieve this .
the ensemble mcmc methods i describe next can also be viewed in this way .
( x123 ) = r ( x123 , x123 ) dx123 , since we could approximate this integral using many values of x123
applying ensemble mcmc to problems with fast and slow variables .
ensemble mcmc can be applied to this problem using ensembles in which all states have the same value for the slow variables , x123
computation of the ensemble probability density ( equation ( 123 ) ) can then take advantage of quick computation for multiple values of the fast variables .
many sampling schemes of this sort are possible ,
distinguished by the ensemble base measure used , and by the updates that are performed on the ensemble ( ie , by the transition t ) .
for all the schemes i have looked at , the fast and slow variables are independent in the base ensemble measure used , and the slow variables have the same value in all members of the ensemble .
the ensemble base measure therefore has the following form :
( x ( 123 ) , .
, x ( k ) ) = h ( x ( 123 )
123 ) i ( x ( 123 )
, x ( k )
here , ( x123 ) is some density for the slow variables the choice of will turn out not to matter , as long as it is nowhere zero .
that the slow variables have the same values in all ensemble members is expressed by the factors of x ( 123 ) 123 ) .
the joint distribution of the fast variables in the k members of the ensemble is given by the density function .
if we let k be the marginal distribution for the kth member of the ensemble under , we can write the ensemble density from equation ( 123 ) as follows :
( x ( 123 ) , .
, x ( k ) ) = ( x ( 123 ) , .
, x ( k ) )
123 ) i ( x ( 123 ) 123 , .
, x ( k )
possible choices for include those analogous to some of the general choices for discussed in the
previous section , in particular the following :
, x ( k )
be independent under , with all having the same marginal distribution .
for example , if the fast variables are parameters in a bayesian model , we might use their prior
, x ( k )
123 | n ( , 123 ) and n ( 123 , 123 ) .
we could then let go to innity , so that is improper , and all k are the same .
be exchangeable under .
if x123 is scalar , we might let x ( k )
123 = x ( k )
+ for k = 123 , .
here , if x123 has dimension d , is a d - dimensional oset that is applied to the set of points x ( k ) .
this is an example of a constellation , as discussed in the previous section .
one possibility is a rectangular grid , with k = md for some integer m , and with grid points spaced a distance dj in dimension j , so that the extent of the grid in dimension j is ( m123 ) dj .
the ensemble update , t , could be done in many ways , as long as the update leaves invariant .
a simple and general option is to perform some number of metropolis - hastings updates ( metropolis , et al
123; hastings 123 ) , in which we propose to change the current ensemble state , y = ( x ( 123 ) , .
, x ( k ) ) , to a proposed state , y , drawn according to some proposal density g ( y|y ) .
we accept the proposed y as the new state with probability min ( 123 , ( y ) g ( y|y ) / ( y ) g ( y|y ) ) , where is dened by equation ( 123 ) .
a technical issue arises when x is continuous since x ( k ) 123 must be the same for all members of the ensemble , the probability densities will be innite ( eg , due to the delta functions in equation ( 123 ) ) .
we can avoid this by looking at densities for only x ( 123 ) are just equal 123 .
( this assumes that we never propose states violating this constraint . ) a similar issue arises when using an ensemble of grid points , as dened above , in which the x ( k ) are constrained to a grid here also , we can simply leave out the innite factors in the density that enforce the deterministic constraints , assuming that our proposals never violate them .
, x ( k )
, x ( k )
123 and x ( 123 )
, since x ( 123 )
i will consider two classes of metropolis updates for the ensemble state , in which symmetry of the
proposal makes g ( y|y ) / g ( y|y ) equal to one , so the acceptance probability is min ( 123 , ( y ) / ( y ) ) :
fast variables xed
propose a new value for x123 in all ensemble members , by adding a random oset drawn from some symmetrical distribution to the current value of x123
keep the values of x ( 123 ) in the proposed state .
, x ( k )
fast variables shifted
propose a new value for x123 in all ensemble members as above , together with new values for 123 , .
, x ( k ) that is randomly drawn from some symmetrical distribution .
found by adding an oset to all of x ( 123 )
, x ( k )
these two schemes are illustrated in figure 123
either scheme could be combined with any of the three
figure 123 : illustration of ensemble updates with with one slow variable ( horizontal ) and one fast variable ( vertical ) .
the distribution is uniform over the outlined region .
a grid ensemble with k = 123 is used .
on the left , a move is proposed by changing only the slow variable; it will be accepted with probability 123 / 123 , since four members of the current ensemble have non - zero probability , compared with only one in the proposed ensemble .
on the right , a move is proposed by changing the slow variable and also shifting the grid ensemble; it will be accepted with probability 123 / 123
( of course , a less favourable shift in the grid ensemble might have led to zero probability of acceptance . )
types of ensembles above , but shifting the fast variables when they are sampled independently from some distribution seems unlikely to work well , since after shifting , the values for the fast variables would not be typical of the distribution .
one could perform just one metropolis update on the ensemble state before returning to a single point in x , or many such updates could be performed .
moving back to a single point and then regenerating an ensemble before performing the next ensemble update will requires some computation time , but may improve sampling .
updates might also be performed on x ( perhaps changing only fast variables ) before moving back to an ensemble .
fast and slow variables in gaussian process regression models
gaussian process regression ( neal 123; rasmussen and williams , 123 ) is one context where ensemble mcmc can be applied to facilitate sampling when there are fast and slow variables .
a brief introduction to gaussian process regression .
suppose that we observe pairs ( z ( i ) , y ( i ) ) for i = 123 , .
, n , where z ( i ) is a vector of p covariates , whose distribution is not modeled , and y ( i ) the corresponding real - valued response , which we model as y ( i ) = f ( z ( i ) ) + e ( i ) , where f is an unknown function , and e ( i ) is independent gaussian noise with mean zero and variance 123
we can give f a gaussian process prior , with mean zero and some covariance function c ( z , z ) = e ( f ( z ) f ( z ) ) .
we then base prediction for a future observed response , y , associated with covariate vector z , on the conditional distribution of y given y = ( y ( 123 ) , .
, y ( n ) ) .
this conditional distribution is gaussian , with mean and variance that can be computed as follows :
e ( y|y ) = kt 123y , var ( y|y ) = v kt 123k
here , = k + 123i is the covariance matrix of y , with kij = c ( z ( i ) , z ( j ) ) .
covariances between y and y are given by the vector k , with ki = c ( z , z ( i ) ) .
the marginal variance of y is v = c ( z , z ) + 123
if the covariance function c and the noise variance 123 are known , the matrix operations above are all that are needed for inference and prediction .
however , in practice , the noise variance is not known , and the covariance function will also have unknown parameters .
for example , one commonly - used covariance function has the form
c ( z , z ) = 123 ( cid : 123 ) a123 + exp ( cid : 123 )
xh=123 ( cid : 123 ) h ( zh z
here , the a123 term allows for the overall level of the function to be shifted upwards or downwards from zero; it might be xed a priori .
however , we typically do not know what are suitable values for the parameter , which expresses the magnitude of variation in the function , or for the parameters 123 , .
, p , which express how relevant each covariate is to predicting the response ( with h = 123 indicating that zh has no eect on predictions for y ) .
in a fully bayesian treatment of this model , , , and are given prior distributions .
independent gaussian prior distributions for the log of each parameter may be suitable , with means and variances
that are xed a priori .
in the models i t below , the logs of the components of are correlated , which is appropriate when learning that one covariate is highly relevant to predicting y increases our belief that other covariates might also be relevant .
after sampling the posterior distribution of the parameters with some mcmc method , predictions for future observations can be made by averaging the gaussian distributions given by ( 123 ) over values of the parameters taken from the mcmc run .
sampling from the posterior distribution requires the likelihood for the parameters given the data , which is simply the gaussian probability density for y with these parameters .
the log likelihood , omitting constant terms , can therefore be written as
log l ( , , ) = ( 123 / 123 ) log det ( , , ) ( 123 / 123 ) yt ( , , ) 123y
this log likelihood is usually computed by nding the cholesky decomposition of , which is the lower - triangular matrix l for which llt = .
from l , both terms above are easily calculated .
in the rst term , the determinant of can be found as ( det l ) 123 , with det l being just the product of the diagonal entries of l .
in the second term , yt 123y = yt ( llt ) 123y = ut u , where u = l123y can be found by forward substitution .
computing the cholesky decomposition of requires time proportional to n123 , with a rather small constant of proportionality .
computation of requires time proportional to n123p , with a somewhat larger constant of proportionality ( which may be larger still when covariance functions more complex than equation ( 123 ) are used ) .
if p is xed , the time for the cholesky decomposition will dominate for large n , but for moderate n ( eg , n = 123 , p = 123 ) , the time to compute may be greater .
i will show here how the likelihood expressing computations in a form with fast variables .
for a gaussian process regression model can be rewritten so that some of the unknown parameters are fast .
specically , if computations are done using the cholesky decomposition , can be a fast parameter , and if computations are instead done by nding the eigenvectors and eigenvalues of the covariance matrix , both and can be fast parameters .
note that the mcmc methods used work better when applied to the logs of the parameters , so the actual fast parameters will be log ( ) and log ( ) , though i sometimes omit the logs when referring to them here .
the form of covariance function shown in equation ( 123 ) has already been designed so that can be a fast parameter .
previously , i ( and others ) have used covariance functions in which the constant term a is not multiplied by the scale factor 123 , but writing it as in equation ( 123 ) will be essential for to be a fast parameter .
as an expression of prior beliefs , it makes little dierence whether or not a123 is multiplied by 123 , since a is typically chosen to be large enough that any reasonable shift of the function is possible , even if a is multiplied by a value of at the low end of its prior range .
indeed , one could let a go to innity analogous to letting the prior for the intercept term in an ordinary linear regression model be improper without causing any signicant statistical problem , although in practice , to avoid numerical problems from near - singularity of , excessively large values of a should be avoided .
a large value for a will be usually be unnecessary if the responses , y , are centred to have sample mean zero .
to make a fast variable when using cholesky computations , we also need to rewrite the contri -
bution of the noise variance to as 123 times 123 / 123 = 123
we will then do mcmc with log ( ) as a fast variable and log ( ) and log ( h ) for h = 123 , .
, p as slow variables .
( note that the jacobian for the transformation from ( log ( ) , log ( ) ) to ( log ( ) , log ( ) ) has determinant one , so no adjustment of posterior densities is required with this transformation . )
fast recomputation of the likelihood after changes can be done if we write = 123 ( + 123i ) ,
ij = a123 + exp ( cid : 123 )
+ 123i is not a function of log ( ) , but only of log ( ) and the log ( h ) .
given values for these slow variables , we can compute the cholesky decomposition of + 123i , and from that det ( + 123i ) and yt ( + 123i ) y , as described earlier .
if these results are saved , for any value of the fast variable , log ( ) , we can quickly compute det = 123n det ( + 123 ) and yt 123y = yt ( + 123i ) 123y / 123 , which suce for computing the likelihood .
rather than use the cholesky decomposition , we could instead compute the likelihood for a gaus - sian process model by nding the eigenvectors and eigenvalues of .
let e be the matrix with the eigenvectors ( of unit length ) as columns , and let 123 , .
, n be the corresponding eigenvalues .
the projections of the data on the eigenvectors are given by u = e t y .
the log likelihood of ( 123 ) can then be computed as follows :
log l =
both nding the cholesky decomposition of an nn matrix and nding its eigenvectors and eigenvalues take time proportional to n123 , but the constant factor for nding the cholesky decomposition is smaller than for nding the eigenvectors ( by about a factor of fteen ) , hence the usual preference for using the cholesky decomposition .
however , if computations are done using eigenvectors and eigenvalues , both and can be made fast variables .
to do this , we write = 123 + 123i .
given values for the slow variables , log ( h ) for h = 123 , .
, p , we can compute , and then nd its eigenvalues , 123 , .
, n , and the projections of y on each of its eigenvectors , which will be denoted ui for i = 123 , .
if we save these i and ui , we can quickly compute l for any values of and .
the eigenvectors of are the same as those of , and the eigenvalues of are 123i + 123 for i = 123 , .
the log likelihood for the log ( h ) values used to compute along with values for the fast variables log ( ) and log ( ) can therefore be found as
log l =
log ( 123i + 123 )
123i + 123
this procedure must be modied to avoid numerical diculties when is nearly singular , as can easily happen in practice .
the x is to add a small amount to the diagonal of , giving = + r123i , where r123 is a small amount of jitter , and then using in the computations described above .
the statistical eect of this is that the noise variance is changed to 123r123 + 123
by xing r to a suitably small value , the dierence of this from 123 can be made negligible .
for consistency , i also use for
the cholesky method ( so the cholesky decomposition is of + 123i ) , even though adding jitter is necessary in that context only when 123 might be very small .
to summarize , if we use the cholesky decomposition , we can compute the likelihood for all members of an ensemble diering only in using only slightly more time than is needed to compute the likeli - hood for one point in the usual way .
only a small amount of additional time , independent of n and p , is needed for each member of the ensemble .
computation of eigenvectors is about fteen times slower than the cholesky decomposition , and computing the likelihood for each ensemble member using these eigenvectors is also slower , taking time proportional to n .
however , when using eigenvector compu - tations , both and can be fast variables .
which method will be better in practice is unclear , and likely depends on the values of n and p for moderate n and large p , time to compute the covariance matrix , which is the same for both methods , will dominate , favouring eigenvector computations that let be a fast variable , whereas for large n and small p , the smaller time to compute the cholesky decomposition may be the dominant consideration .
if we wish to use a small ensemble over both and , it may sometimes be desirable to do the computations using the cholesky decomposition , recomputing it for every value of the fast variables .
this would make sense only if the time to compute the covariances ( apart from a nal scaling by 123 and addition of 123i ) dominates the time for these multiple cholesky computations ( otherwise using the ensemble will not be benecial ) , and the ensemble has less than about fteen members ( otherwise a single eigenvector computation would be faster ) .
however , i do not consider this possibility in the demonstraton below , where larger ensembles are used .
here , i demonstrate and compare standard metropolis and ensemble mcmc on the problem of sam - pling from the posterior distribution for a gaussian process regression model , using the fast - slow methods described above .
the model and synthetic test data that i use were chosen to produce an interesting posterior distribution , having several modes that correspond to dierent degress of predictability of the response ( and hence dierent values for ) .
the mcmc programs were written in r , and run on two machines , with two versions of r version 123 . 123 on a solaris / sparc machine , and version 123 . 123 on a linux / intel machine .
the programs used are available at my web site . 123
in the generated data , the true relationship of the response , y , to the vector the synthetic data .
of covariates , z , was y = f ( z ) + e , with e being independent gaussian noise with standard deviation 123 , and the regression function being
f ( z ) = 123z123
123 + 123 sin ( 123 + ( 123 + 123z123 ) z123 ) + 123 cos ( 123 + 123z123 + 123z123
the number of covariates was p = 123 , though as seen above , f ( z ) depends only on z123 , z123 , and z123
the covariate values were randomly drawn from a multivariate gaussian distribution in which all the
zh had mean zero and variance one .
there were weak correlations among z123 , z123 , and z123
there were strong ( 123 ) correlations between z123 and z123 , z123 and z123 , and z123 and z123 , and moderate ( 123 ) correlatons between z123 and z123 , z123 and z123 , and z123 and z123
covariates z123 , z123 , and z123 were independent of each other and the other covariates . 123
i generated n = 123 independent pairs of covariate vectors and response values in this way .
the gaussian process model .
this synthetic data was t with a gaussian process regression model of the sort described in the previous section , in which the prior covariance between responses y ( i ) and y ( j ) was
cov ( y ( i ) , y ( j ) ) = 123 ( cid : 123 ) a123 + exp ( cid : 123 )
h ) ( cid : 123 ) 123 ( cid : 123 ) + r123ij ( cid : 123 ) + 123ij
where ij is zero if i 123= j and one if i = j .
i xed a = 123 and r = 123 .
the priors for , , and were independent .
the prior for log ( ) was gaussian with mean log ( 123 ) and standard deviation 123; that for log ( ) was gaussian with mean of log ( 123 ) and standard deviation 123 .
the prior for was multivariate gaussian with each log ( h ) having mean log ( 123 ) and standard deviation 123 , and with the correlation of h and h for h 123= h being 123 .
having a non - zero prior correlation between the h is equivalent to using a prior expressed in terms of a higher - level hyperparameter , 123 , conditional on which the h are independent with mean 123
i tried sampling from the posterior distribution performance of standard metropolis sampling .
for this model and data using standard random - walk metropolis methods , with the state variables being the logs of the parameters of the covariance function ( the twelve h parameters , , and ) .
i rst tried using a multivariate gaussian metropolis proposal distribution , centred at the current state , with independent changes for each variable , with the same standard deviation ie , the proposal distribution was n ( x , s123i ) , where x is the current state , and s is the proposal standard deviation .
figure 123 shows results with s = 123 ( top ) and s = 123 ( bottom ) .
the plots show three quantities , on a logarithmic scale , for every 123th iteration from a total of 123 , 123 metropolis updates ( so that 123 points are plotted ) .
the quantity shown in red is the average of the h parameters ( giving relevances of the covariates ) , that shown in green is the parameter ( the overall scale ) , and that shown in blue is the parameter ( noise standard deviation ) .
when s = 123 , the rejection rate is 123% .
from the top plot , it appears that the chain has converged , and is sampling from two modes , characterized by values of around 123 , or much smaller values .
movement between these modes occurs fairly infrequently , but an adequate sample appears to have been obtained in 123 , 123 iterations .
123 in detail , the covariates were generated by letting wh for h = 123 , .
, 123 be independent standard normals , and then letting z123 = w123 , z123 = 123z123 + w123 123 , z123 = 123z123 + w123 123 , z123 = 123z123 + w123 123 , z123 = 123z123 + w123 123 , z123 = 123z123 +w123 123 , z123 = 123z123 +w123 123 , z123 = 123z123 +w123 123 , z123 = 123z123 +w123 123 ,
z123 = w123 , z123 = w123 , and z123 = w123
s = 123 :
s = 123 :
figure 123 : runs using metropolis updates of all parameters at once , with two values for the proposal standard deviation , s .
however , the bottom plot shows that this is not the case .
the rejection rate here , with s = 123 , is high , at 123% , but this chain explores additional modes that were missed in the run with s = 123 .
the sample from 123 , 123 iterations is nevertheless far from adequate .
some modes were moved to and from only once in the run , so an accurate estimate of the probality of each mode cannot be obtained .
metropolis updates that change only one parameter at a time , systematically updating all pa - rameters once per mcmc iteration , perform better .
( this is presumably because in the posterior distribution there is fairly low dependence of some parameters on others , so that fairly large changes can sometimes be made to a single parameter . ) updating a h parameter is slow , requiring about the same computation time as an update changing all parameters .
however , updates that change only , or ( if computations are done using eigenvectors ) only , will be fast .
to allow a fair comparison , the number of iterations done was set so that the number of slow evaluations was 123 , 123 , the same as for the runs with metropolis updates of all variables at once .
to make the comparison as favourable
figure 123 : runs using metropolis updates of one parameter at a time , with extra updates of fast parameters for the lower plot .
as possible for this method , i assumed that computations are done using eigenvectors , so that both and are fast variables .
the top plot in figure 123 shows results using such single - variable metropolis updates ( selected as among the best of runs with various settings that were tried ) .
here , the proposal standard deviation was 123 for the h parameters , and 123 for the and parameters .
( recall that all parameters are represented by their logs . ) the rejection rates for the updates of the variables ranged from 123% to 123% .
one can see that sampling is signicantly improved , but that some modes are still visited only a few times during the run .
one way to try to improve sampling further is to perform additional updates of the fast variables .
the bottom plot in figure 123 shows the results when 123 additional metropolis updates of the fast variables ( and ) are done each iteration .
this seems to produce little or no improvement on this problem .
( however , on some other problems i have tried , extra updates of fast variables do improve sampling . )
the time required for a slow evaluation using computations based on the cholesky decomposition was 123 seconds on the intel machine , and 123 seconds on the sparc machine .
when using computations based on eigenvectors , a slow evaluation took 123 seconds on the intel machine , and 123 seconds on the sparc machine .
the ratio of computation time using eigenvectors to computation time using the cholesky decomposition was therefore 123 for the intel machine and 123 for the sparc machine .
note that this ratio is much smaller than the ratio of times to compute eigenvectors versus the cholesky decomposition , since times for other operations , such as computing the covariances , are the same for the two methods , and are not negligible in comparison when n = 123 and p = 123
i now present the results of using ensemble mcmc methods performance of ensemble mcmc .
on this problem .
the metropolis method was used to update the ensemble , so that a direct comparison to the results above can be made , but of course other types of mcmc updates for the ensemble are quite possible .
i tried metropolis updates where the proposals both changed the slow variables and shifted the ensemble of values for fast variables , but keeping the ensemble of fast variables xed seemed to work as well or better for this problem .
updating only one slow variable at a time worked better than updating all at once .
accordingly , the results below are for ensemble updates consisting of a sequence of single - variable metropolis updates for each slow variable in turn , using gaussian proposals centred at the current value for the variable being updated .
after each such sequence of updates , the ensemble was mapped to a single state , and a new ensemble was then generated .
( one could keep the same ensemble for many updates , but for this problem that seems undesirable , considering that regenerating the ensemble is
i will rst show the results when using computations based on eigenvectors , for which both and are fast variables .
three ensembles for fast variables were tried an independent ensemble , with the distribution being the gaussian prior , an exchangeable ensemble , with the ensemble distribution being gaussian with standard deviations 123 times the prior standard deviations , and a 123 123 grid ensemble , with grid extent chosen uniformly between the prior standard deviation and 123 times the prior standard deviation .
all ensembles had k = 123 members .
the number of iterations was set so that 123 , 123 slow evaluations were done , to match the runs above using standard metropolis updates .
figure 123 shows the results .
comparing with the results of standard metropolis in figure 123 , one can see that there is much better movement among the modes , for all three choices of ensemble .
the exchangeable and grid ensembles perform slightly better than the independent ensemble .
i tried increasing the size of the ensemble to 123 , and performing extra updates of the fast variables after mapping back to a single state , but this only slightly improves the sampling ( mostly for the independent
the time per iteration for these ensemble methods ( with k = 123 ) was 123 seconds on the intel machine and 123 seconds on the sparc machine ( very close for all three ensembles ) .
this is slower than standard metropolis using cholesky computations by a factor of 123 for the intel machine and 123 for the sparc machine .
the gain in sampling eciency is clearly much larger than this .
grid ( 123 123 ) :
figure 123 : runs using ensemble mcmc with computations based on eigenvalues ( both and fast ) , for three ensembles .
figure 123 shows the results when computations are based on the cholesky decomposition , so that only can be a fast variable .
sampling is much better than with standard metropolis , but not as good as when both and are fast variables .
however , the computation time is lower 123 seconds per iteration for on the intel machine , and 123 seconds per iteration on the sparc machine .
these times are only about 123% higher than for standard metropolis , so use of an ensemble for only is
i will conclude by discussing other possible applications of ensemble mcmc , and its relationship with two previous mcmc methods .
some other applications .
bayesian inference problems with fast and slow variables arise in many contexts other than gaussian process regression models .
as mentioned early , many bayesian models have hyperparameters whose conditional distributions depend only on a fairly small number of pa - rameters , and which are therefore fast compared to parameters that depend on a large set of data
for example , in preliminary experiments , i have found a modest benet from ensemble mcmc for logistic regression models in which the prior for regression coecients is a t distribution , whose width parameter and degrees of freedom are hyperparameters .
when there are n observations and p covariates , recomputing the posterior density after a change only to these hyperparameters takes time proportional to p , whereas recomputing the posterior density after the regression coecients change takes time proportional to np ( or to n , if only one regression coecient changes , and suitable intermediate results were retained ) .
the hyperparameters may therefore be seen as fast variables .
in gaussian process models with latent variables , such as logistic classication models ( neal 123 ) , the latent variables are all fast compared to the parameters of the covariance function , since recom - puting the posterior density for the n latent variables , given the parameters of the covariance function , takes time proportional n123 , with a small constant factor , once the cholesky decomposition of their covariance matrix has been found in n123 time .
looking at ensemble mcmc for this problem would be interesting , though the high dimensionality of the fast variables may raise additional issues .
mcmc for state - space time series models using embedded hidden markov models ( neal , beal , and roweis , 123 ) can be interpreted as an ensemble mcmc method that maps to the ensemble and then immediately maps back .
looked at this way , one might consider updates to the ensemble before mapping back .
perhaps more promising , though , is to use a huge ensemble of paths dened using an embedded hidden markov model when updating the parameters that dene the state dynamics .
relationship to the multiple - try metropolis method .
a metropolis update on an ensemble of points as dened in this paper bears some resemblence to a multiple - try metropolis update as dened by liu , liang , and wong ( 123 ) for both methods , the update is accepted or rejected based on the ratio of two sums of terms over k points , and in certain cases , these sums are of ( x ( i ) ) for the
grid ( 123 123 ) :
figure 123 : runs using ensemble mcmc with computations based on cholesky decomposition ( only fast ) , for three ensembles .
k points , x ( 123 ) , .
, x ( k ) .
in a multiple - try metropolis update , k points are sampled independently from some proposal distribution conditional on the current point , one of these k proposed points is then selected to be the new point if the proposal is accepted , and a set of k points is then produced by combining the current point with k 123 points sampled independently from the proposal distribution conditional on this selected point .
finally , whether to accept the selected point , or reject it ( retaining the current point for another iteration ) , is decided by a criterion using a ratio of sums of terms for these two sets of k points .
in one simple situation , multiple - try metropolis is equivalent to mapping from a single point to an ensemble of k points , doing a metropolis update on this ensemble , and then mapping back to a single point .
this equivalence arises when the proposal distribution for multiple - try metropolis does not actually depend on the current state , in which case it can also be used as an independent ensemble base distribution , and as a proposal distribution for an ensemble update in which new values for all ensemble members are proposed independently . 123 this method will generally not be useful , however , since there is no apparent short - cut for evaluating ( x ) at the k ensemble points ( or k proposed points ) in less than k times the computational cost of evaluating ( x ) at one point .
applying multiple - try metropolis usefully to problems with fast and slow variables , as done here for ensemble mcmc , would require that the k proposals conditional on the current state be dependent , since for fast computation they need to all have the same values for the slow variables .
liu , et al .
mention the possibility of dependent proposals , but provide details only for a special kind of dependence that is not useful in this context .
in this paper i have emphasized the need for a computational short - cut if ensemble mcmc is to provide a benet , since otherwise it is hard to see how an ensemble update taking a factor of k more computation time can outperform k ordinary updates .
the analogous point regarding multiple - try metropolis was apparently not appreciated by liu , et al . , as none of the examples in their paper make use of such a short - cut .
accordingly , in all their examples one would expect a multiply - try metropolis method with k trials to be inferior to simply performing k ordinary metropolis updates with the same proposal distribution , a comparison which is not presented in their paper . 123
relationship to the multiset sampler .
leman , chen , and lavine ( 123 ) proposed a multiset sampler which can be seen as a particular example of sampling from an ensemble distribution as in this paper .
in their notation , they sample from a distribution for a value x and a multiset of k values for y , written as s = ( y123 , .
the y values are conned to some bounded region , y , which allows a joint density for x and s to be dened as follow :
( x , yj ) = ( x )
123in detail , using the notation of liu , et al .
( 123 ) , we obtain this equivalence by letting t ( x , y ) = ( y ) and ( x , y ) =
123 / ( ( x ) ( y ) ) , so that w ( x , y ) = ( x ) t ( x , y ) ( x , y ) = ( x ) / ( x ) .
123for their cgmc method , the k ordinary metropolis updates should use the same reference point .
this is valid ,
since the reference point is determined by a part of the state that remains unchanged during these k updates .
where ( x , y ) is the distribution of interest .
in some examples , they focus on the marginal ( x ) .
integrating the density above over y123 , .
, yk shows that the marginal ( x ) is the same as ( x ) , so that sampling x and s from will produces a sample of x values from .
leman , et al .
sample from by alternating a metropolis update for just x with with a metropolis update for yj with j selected randomly from ( 123 ,
this distribution for the multiset sampler is the same as the ensemble distribution that would be obtained using the methods for fast and slow variables in this paper , if x is regarded as slow ( and hence written as x123 in the notation of this paper ) , y is regarded as fast ( and hence written as x123 ) , and in the density used in dening the ensemble base measure , the fast variables in the ensemble are independent , with uniform density over y .
the density dened by equation ( 123 ) then corresponds to the multiset density above .
leman , et al .
do not distinguish between fast and slow variables , however , and hence do not argue that the multiset sampler would be benecial in such a context .
nor do they assume that there is any other computational short - cut allowing to sometimes be evaluated quickly , thereby reducing the amount of computation needed to evaluate .
they instead justify their multiset sampler as allowing k 123 of the y values in the multiset to freely explore the region y , since the one remaining y value can provide a reasonably high ( x , y ) and hence also a reasonably high value for .
the development in this paper conrms this picture .
with the distribution corresponding to multiset sampling , the mapping t will ( in the notation of leman , et al . ) go from a single ( x , y ) pair drawn from ( x , y ) to an ensemble with k values for y , one of which is the original y , and the other k 123 of which are drawn independently and uniformly from y .
this is therefore the equilibrium distribuiton of the multiset samplier .
the development here also shows that ( x , y ) can be recovered by applying the t mapping , which will select a y from the multiset with probabilities proportional to ( x , y ) .
this makes the complex calculations in section 123 of ( leman , et al . , 123 ) unnecessary .
unfortunately , it also seems that any benet of the multiset sampler can be obtained much more simply and eciently with a markov chain sampler that randomly chooses between two metropolis - hastings updates on the original distribution one that proposes a new value for x ( from some suitable proposal distribution ) with y unchanged , and another that proposes a new value for x along with a new value for y that is drawn uniformly from y .
as is the case as well for ensemble mcmc and multiple - try metropolis , one should expect that looking at k points at once will produce a benet only when a short - cut allows for all these points to be computed in less than k times the cost of computing for one point .
this research was supported by natural sciences and engineering research council of canada .
the author holds a canada research chair in statistics and machine learning .
