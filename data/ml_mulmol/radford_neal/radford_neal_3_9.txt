abstract .
i show how one can modify the random - walk metropolis mcmc method in such a way that a sequence of modied metropolis updates takes little computation time when the rejection rate is outside a desired interval .
this allows one to eectively adapt the scale of the metropolis proposal distribution , by performing several such short - cut metropolis sequences with varying proposal stepsizes .
unlike other adaptive metropolis schemes , this method converges to the correct distribution in the same fashion as the standard metropolis method .
the metropolis algorithm of metropolis , rosenbluth , rosenbluth , teller , and teller ( 123 ) is the rst and perhaps still the most widely - used markov chain monte carlo ( mcmc ) method .
the metropolis procedure denes a probabilistic transition from a state x ( usually of high dimension ) to a state x that leaves some desired distribution , with density function ( x ) , invariant .
starting from any initial state , a markov chain using such transitions will produce states that asymptotically converge in distribution to , provided that the transitions are capable of moving from any region of the state space to any other .
states from the latter parts of one or more simulations of such a markov chain ( called runs ) are then used to make monte carlo estimates for expectations of functions of state with respect to .
for more information on metropolis and other mcmc methods , and their applications in statistics and statistical physics , see ( for example ) neal ( 123 ) , tierney ( 123 ) , and liu ( 123 ) .
the metropolis algorithm requires that we specify a suitable proposal distribution , which may depend on the current state , whose conditional density , g ( x|x ) , satises the symmetry condition g ( x|x ) = g ( x|x ) .
a transition ( or update ) from state x is performed by generating an x according to g ( |x ) , and then accepting x as the new state , x , with probability
a ( x , x ) = min ( 123 , ( x ) / ( x ) )
if x is not accepted ( ie , is rejected ) , we let the new state , x , be the same as the old state , x .
the most generally - useful class of proposal distributions take the form of adding a random oset to the current state ie , x = x + w .
here , w is a scalar stepsize parameter , and is a vector
drawn from some distribution not depending on x , with density function f ( ) , which must be symmetrical around zero ( ie , f ( ) = f ( ) ) .
in this paper , i will mostly consider distributions for in which all components are non - zero , so that the proposed x diers in all components from the current x .
metropolis procedures in which only one ( or a small group ) of components are changed in each update are also common , and will be discussed briey at the end .
such random walk metropolis updates can explore complex distributions whose shape is not known a priori .
however , the eciency of this exploration depends critically on a proper choice for the stepsize , w .
if w is too large , we may nd that almost all proposals are rejected , leading to very inecient exploration .
but if w is too small , each update will change the state by only a small amount , so many updates will be required to move a substantial distance ( an eect exacerbated by the fact that movement is via a random walk ) .
we often do not know enough about initially to choose an appropriate value for the stepsize w .
a common approach is to perform some preliminary runs using various values for w , and then use statistics from these runs to select a value for w that appears to give a rejection rate between 123% and 123% .
alternatively , we can manually change w during the early part of a run until we believe weve found a suitable value .
with either method , we then use the chosen value for w in the remainder of the run , or in one or more new runs , states from which are used for estimating expectations .
although many useful results have been obtained in this way , this procedure seems a bit wasteful and inelegant .
it also may not work well in situations where dierent stepsizes are appropriate in dierent regions of the state space .
accordingly , many people have sought methods in which the stepsize is automatically adapted in some suitable way throughout the course of a run .
one obvious approach is to continually change the stepsize based on the rejection rate in past updates .
unfortunately , many naive methods of this sort will give wrong answers .
for example , we might for each update use one of two values for the stepsize , w123 or w123 ( with w123 < w123 ) , choosing w123 if the rejection rate in the last 123 updates was greater than 123% , and w123 otherwise .
there is no reason to think that this will produce the right answer , however , since the dependence on past updates destroys the markov property of the process , which undermines the proof that the distribution of the state converges to .
indeed , this and similar methods will tend to spend too much time in regions of the state space where a small stepsize is needed to achieve a small - enough rejection rate , since when the markov chain is in such a region , it will eectively be operating at a slower pace than in regions where a larger stepsize is chosen .
estimates that are asymptotically correct can be obtained if changes to the stepsize become smaller as the run progresses .
methods of this sort have been investigated by haario , saksman , and tamminen ( 123 ) , atchade and rosenthal ( 123 ) , and andrieu and moulines ( 123 ) , who show that when using certain adaptation schemes , estimates of expectations for functions of state will converge to the correct values under certain conditions .
these conditions may not be easy to satisfy or to verify , however , and even if they can be shown to hold , properly assessing the error in the estimates found will be more complex than for ordinary mcmc estimation , for which assessing accuracy is already a non - trivial problem , generally requiring some human judgement .
more fundamentally , since the correctness of these methods depends on the stepsize becoming more and more stable as the run progresses , it is unclear what advantage these methods might have over the much simpler procedure of adapting the stepsize only during an initial portion of the run , xing it after some number of updates essentially an automated version of one of the commonly - used manual procedures described above .
choosing a suitable point at which to x the
stepsize ( either manually or automatically ) seems easier than assessing the degree to which the accuracy of estimates may have been aected by continual changes in the stepsize .
we would in any case prefer a method that can use dierent stepsizes in dierent regions of the state space .
something like this is the objective of the delayed rejection metropolis method ( tierney and mira 123; green and mira 123 ) .
in this method , an update may consider several proposals before one is accepted , and later proposals may be inuenced by the results of earlier proposals within the same update .
slice sampling updates ( neal 123 ) achieve a similar eect , in a manner that may be more useful in practice .
however , these methods adapt only temporarily , as part of a larger update .
adaptation must begin over again at the start of the next update .
in this paper , i describe a new approach to adapting the stepsize , which is best understood by rst considering the following wasteful strategy .
suppose we are uncertain whether to use w = 123 or w = 123
we might therefore use both of these stepsizes in turn .
for instance , we might alternate between performing 123 updates with w = 123 and performing 123 updates with w = 123
although this procedure may avoid disaster by ensuring that we use a good stepsize at least some of the time , it will be reasonably ecient only when w = 123 is the best stepsize , in which case we will be using the right stepsize for 123 / ( 123 + 123 ) = 123 / 123 of the time .
when w = 123 is the best step size , but w = 123 is too large , almost all the 123 updates done with w = 123 will be rejected , so only 123 / ( 123 + 123 ) = 123 / 123 of the updates will be useful , which seems less than satisfactory .
suppose , however , that we could somehow arrange that simulating a sequence of metropolis updates done using a stepsize that is too large , and which therefore are almost always rejected , takes little computation time .
in particular , suppose that simulating a sequence of k metropolis updates with a stepsize that almost always leads to rejection can be done in the same time as 123 ordinary metropolis updates , regardless of how large k is .
with this short - cut method , the strategy described above is attractive even when w = 123 is the best stepsize the 123 updates with w = 123 are done in the same time as 123 ordinary updates , so 123 / ( 123 + 123 ) = 123 / 123 of the time is devoted to updates with the best stepsize .
a variation on this approach is possible if a we can also take a short - cut when simulating a sequence of metropolis updates in which the rejection rate is much smaller than desired .
suppose that a sequence of k updates can be eectively reduced to only 123 updates when almost no updates are rejected , or when almost all updates are rejected , regardless of how large k is .
we could then alternately perform a sequence of 123 updates with w = 123 and a sequence of 123 updates with w = 123
if only one of these values for w produces a reasonable rejection rate ( not near 123 or 123 ) , then 123 / ( 123+123 ) =123 / 123 of our time will be spent simulating the useful updates , with only 123 / 123 of our time wasted on updates for which the stepsize is too large or too small .
from a computational point of view , these short - cut metropolis updates can be used to adaptively change the stepsize used during the bulk of the computation , eectively allowing dierent stepsizes to be used in dierent regions of the state space .
from a mathematical point of view , however , we are still simply performing sequences of updates using pre - determined stepsizes .
the markov property still holds , and all the usual mcmc theorems regarding convergence of the distribution to and of estimates for expectations with respect to to their correct values still apply .
the reader may wonder , of course , how one might manage to simulate k metropolis updates in time that does not depend on k .
to see how the metropolis algorithm can be modied to achieve this , we rst need to re - interpret a metropolis update as a deterministic transformation .
this is the topic of the next section .
123 a deterministic view of the metropolis algorithm
in this section i will describe how a standard metropolis update can be viewed as a deterministic transformation following a stochastic extension of the state space to include auxiliary variables .
this sets the stage for the modication in the next section , which allows a sequence of metropolis updates with a badly - chosen stepsize to be simulated very quickly .
note that the complexities of viewing metropolis updates in this way need not be reected in the implementation in this section , the actual implementation is just the standard metropolis algorithm .
viewing metropolis updates in terms of auxiliary variables and deterministic transformations is necessary only to show that the modied updates still leave the desired distribution invariant .
to view a metropolis update as a deterministic transformation , we need to temporarily introduce auxiliary variables , a technique that is familiar from other mcmc methods as well , such as slice sampling ( neal 123 ) .
suppose we wish to sample x from distribution ( x ) , using a chain which has this as its unique invariant distribution .
rather than directly dening an update of x that leaves ( x ) invariant , we can expand the state space to pairs ( x , y ) , dene a distribution ( x , y ) = ( x ) ( y|x ) , where ( x ) is the original distribution of interest , and nd a way of updating the pair ( x , y ) that leaves ( x , y ) invariant .
we can introduce the auxiliary variable y temporarily by sampling a value for it from ( y|x ) , performing an update of ( x , y ) , and then discarding y .
it is easy to see that this procedure leaves ( x ) invariant if the update for ( x , y ) leaves ( x , y ) invariant .
deterministic updates can form part of a valid mcmc scheme as long as they leave the desired distribution invariant .
suppose the desired distribution for the state , z , has density ( z ) .
let t ( z ) be a 123 - to - 123 mapping from the state space onto itself .
if z has density ( z ) , and z = t ( z ) , then according to the standard formula for transformation of probability densities , the density of z is ( t 123 ( z ) ) / | det t ( t 123 ( z ) ) | , where t ( z ) is the jacobian matrix for the transformation .
it follows that the transformation z = t ( z ) will leave invariant as long as ( t ( z ) ) = ( z ) and | det t ( z ) | = 123 for all z .
if z is partially discrete , the jacobian condition applies only to the
consider a standard random - walk metropolis update of x , with invariant density ( x ) , symmet - rical proposal density f ( ) , and stepsize w .
to view this update in terms of auxiliary variables and a deterministic transformation , we rst expand the state space by introducing two auxiliary variables : , a vector of the same dimension as x , and e , a positive real number .
we then dene the joint density ( x , , e ) = ( x ) f ( ) exp ( e ) , in which x , , and e are independent .
finally , we dene a transformation , tmet , from ( x , , e ) to ( x , , e ) as follows :
tmet ( x , , e ) = ( ( x+w , , e + log ( ( x+w ) / ( x ) )
( x , , e )
if e + log ( ( x+w ) / ( x ) ) > 123
one can easily see that this transformation is 123 - to - 123 and onto , since for any point ( x , , e ) , exactly one point will transform to this point , namely the point ( x +w , , e log ( ( x ) / ( x +w ) ) ) if e log ( ( x ) / ( x + w ) ) > 123 or the point ( x , , e ) otherwise .
the jacobian matrix for this transformation is the identity if e + log ( ( x+w ) / ( x ) ) 123 , and is otherwise
i wi 123 123 i 123
where i is the identity matrix with dimensions equal to the number of components in x and , and the values of a and b are irrelevant .
one can easily see that the absolute value of the determinant of this matrix is one , since the only non - zero term in the determinant is the product of the entries on the main diagonal , which are all 123 or 123
finally , the probability density of ( x , , e ) is the same as that of ( x , , e ) , since when these points dier
( x , , e ) = ( x+w , , e + log ( ( x+w ) / ( x ) )
= ( x+w ) f ( ) exp ( ( e + log ( ( x+w ) / ( x ) ) ) = ( x ) f ( ) exp ( e ) = ( x , , e )
we can therefore conclude that the following procedure for updating x to x leaves ( x ) invariant :
123 ) randomly pick values for the temporary auxiliary variables , as follows :
a ) pick a value for according to the density function f ( ) .
b ) pick a value for e from the exponential distribution with mean one , whose density
function is exp ( e ) .
123 ) set ( x , , e ) = tmet ( x , , e ) , where tmet is dened by equation ( 123 ) .
123 ) forget and e , leaving just x as the new state .
the distribution sampled in step ( 123 ) is ( , e|x ) , so the joint distribution at this point will be ( x , , e ) if the distribution of x was ( x ) before .
step ( 123 ) leaves invariant this joint distribution , and hence also the marginal distribution for x , which we return to in step ( 123 ) .
the eect of this procedure is equivalent to a standard random - walk metropolis update .
step ( 123 ) , x will be set to either x or x+w .
the probability of the latter is the probability that a value drawn from an exponential distribution with mean one is greater than log ( ( x+w ) / ( x ) ) , which is min ( 123 , ( x+w ) / ( x ) ) , the same as the metropolis acceptance probability of ( 123 ) .
we can also view an entire sequence of k random - walk metropolis updates in terms of an initial stochastic extension of the state space to include auxiliary variables followed by a single deterministic transformation .
we now need k copies of the auxiliary variables used above , denoted by 123 , .
, k 123 and e123 , .
, ek123 , plus an index variable , i , in ( 123 , .
the joint density for x and these auxiliary variables is dened to be
( x , i , 123 , .
, k 123 , e123 , .
, ek 123 ) = ( x )
in other words , the auxilary variables are all independent of x and each other , and their distributions are as previously dened , except for i , which is given a uniform distribution .
we will repeatedly apply a transformation in which ( x , i , 123 , .
, k 123 , e123 , .
, ek 123 ) is mapped to the point found by transforming ( x , i , ei ) to ( x , i ) according to the mapping tmet of equation ( 123 ) , the value of i is change to i = i+123 ( mod k ) , and the other auxiliary variables are left unchanged .
as shown above , the tmet part of this transformation is 123 - to - 123 and onto , the absolute value of the determinant of its jacobian matrix is one , and the probability density of the transformed point is the same as that of
the original point .
the i = i + 123 ( mod k ) part is also 123 - to - 123 and onto , and since i is discrete , there is no jacobian to worry about .
since i has a uniform distribution , the density is also not changed by this part of the transformation .
the entire transformation therefore leaves invariant .
we can write the procedure using this
transformation in detail as follows :
123 ) randomly pick values for the temporary auxiliary variables , as follows :
a ) pick values for 123 , .
, k 123 independently according to the density function f ( ) .
b ) pick values for e123 , .
, ek 123 independently from the exponential distribution with mean
one , whose density function is exp ( e ) .
c ) pick a value for i uniformly from ( 123 , .
, k123 ) .
123 ) transform x and the auxiliary variables by repeating the following sequence of transformations
a ) apply the transformation tmet to ( x , i , ei ) b ) add one to i , modulo k .
123 ) forget 123 , .
, k 123 , e123 , .
, ek 123 , and i , leaving just x as the new state .
it easy to see that this procedure is equivalent to simply performing k successive random - walk metropolis updates .
the index i takes on each of its k possible values exactly once , and for each value of this index , a metropolis update is performed utilizing the auxiliary variables i and ei .
123 short - cut simulation of sequences of metropolis updates
consider now a sequence of k metropolis updates , divided into m groups of l updates ( so that k = m l ) .
i show here how we can look at the number of rejections within each group as we simulate it , and modify subsequent actions so that no further computation is required once two groups are simulated in which the number of rejections is outside some desired range .
for example , if we set l = 123 , we can achieve the result mentioned in the introduction if the stepsize is so large that almost all metropolis updates are rejected , or so small that almost none are rejected , the k updates can usually be computed using the time normally required for only 123 updates , regardless of how large k is .
first , however , lets once again look at how we can perform standard metropolis updates using auxiliary variables and a deterministic transformation , this time expressed in terms of m successive transformations , each of which corresponds to l metropolis updates .
we introduce a new auxiliary it will be variable , s , in ( 123 , +123 ) , which will determine in which direction the index i moves .
uniformly distributed , independently of the other variables , so the joint distribution of x and all auxiliary variables is now
( x , i , s , 123 , .
, k 123 , e123 , .
, ek 123 ) = ( x )
we can use arguments paralleling those used in the previous section to justify procedure 123 to
show that the following procedure for updating x leaves invariant :
123 ) randomly pick values for the temporary auxiliary variables , as follows :
a ) pick values for 123 , .
, k 123 independently according to the density function f ( ) .
b ) pick values for e123 , .
, ek 123 independently from the exponential distribution with mean
one , whose density function is exp ( e ) .
c ) pick a value for i uniformly from ( 123 , .
d ) pick a value for s uniformly from ( 123 , +123 ) .
123 ) transform x and the auxiliary variables by repeating the following sequence of transformations
a ) first , apply the transformation tmet to ( x , i , ei ) .
second , do the following l 123 times :
add s to i , modulo k , and then apply tmet to ( x , i , ei ) .
third , negate s .
b ) negate s .
c ) add s to i , modulo k .
123 ) forget 123 , .
, k 123 , e123 , .
, ek 123 , i , and s , leaving just x as the new state .
note that each of steps ( 123a ) , ( 123b ) , and ( 123c ) leaves invariant .
the negation of s in step ( 123b ) simply undoes the negation at the end of step ( 123a ) , so the only dierence between this procedure and the one in the preceding section is that the initial value of s determines whether the i and ei are utilized in increasing order or decreasing order .
the order makes no dierence , so this procedure is also equivalent to performing k = m l standard random - walk metropolis updates .
we can now modify this procedure to avoid computation when the rejection rate is greater or less than desired .
the modication will have the eect of negating s , thereby reversing the direction in which i moves , whenever the number of rejections within a group of l updates is outside some desired range .
the updates following this will undo earlier updates , leading back to previously computed states , which need not be computed again .
once the original state is reached , new states will be computed , but if a second reversal occurs , all subsequent updates will result in states that have already been computed .
an extreme , but not necessarily uncommon , case occurs when the number of rejections in the rst group of l updates is outside the desired range , and the same is true of the group of l updates simulated next .
the nal state will then be the same as the original state .
a total of only 123l updates requiring computation will have been done , regardless of k .
to see how this modication can be done , rst note that the transformation in step ( 123a ) call it tseq is its own inverse .
tseq will change i to i = i+ ( l123 ) s ( mod k ) and s to s = s .
applying tseq a second time would change i to i = i + ( l123 ) s ( mod k ) = i and change s to s = s = s .
changes to x and the i and ei would also be undone .
the values of i used in these updates would be visited in reverse order .
if the original update was a rejection , for which tmet ( x , i , ei ) = ( x , i , ei ) , it will be a rejection in this reverse pass as well , leaving the original state unchanged .
if the original update instead had tmet ( x , i , ei ) = ( x+wi , i , ei + log ( ( x+wi ) / ( x ) ) , applying tmet a second time will again restore the original state of ( x , i , ei ) .
in general , any transformation , z = t ( z ) , that is its own inverse ( and hence is also 123 - to - 123 and onto ) , that has a jacobian matrix for which the absolute value of the determinant is one , and that satises ( t ( z ) ) = ( z ) for all z will , as we have seen , leave the density ( z ) invariant .
for any set
a , consider the modied transformation , t a , dened by
t a ( z ) = ( z
if z a or t ( z ) a
t ( z ) otherwise
one can easily show that t a is also its own inverse : if z a or t ( z ) a , then t a ( t a ( z ) ) = t a ( z ) = z , and if z / a and t ( z ) / a , then t ( t ( z ) ) = z / a , and t a ( t a ( z ) ) = t a ( t ( z ) ) = t ( t ( z ) ) = z .
one can also easily see that the determinant of the jacobian matrix of t a has absolute value one , and that ( t a ( z ) ) = ( z ) .
it follows that t a leaves invariant .
if we modify step ( 123a ) in procedure 123 to use t a
seq rather than tseq , the overall procedure will therefore still leave invariant .
a variety of choices for the set a may be useful , but for the moment i will consider only sets , rl , h , consisting of values for x and the auxiliary variables for which applying tseq will result in l updates in which the number of rejections is outside the interval ( l , h ) .
we might , for instance , use the set r123 , l123 , in order to pick out groups in which either all updates are rejected or no updates are rejected , or the set r123 , l123 , in order to pick out only groups in which all updates are rejected .
by considering the details of step ( 123a ) , one can see that a point is in rl , h if and only if the image of this point under tseq is also in rl , j , since if an update is rejected when simulating in the forward direction , it will also be rejected when simulating backwards .
modifying procedure 123 to use t rl , h
rather than tseq in step ( 123a ) , and then merging this step
with step ( 123b ) , yields the following procedure :
procedure 123 ( short - cut metropolis ) :
123 ) randomly pick values for the temporary auxiliary variables , as follows :
a ) pick values for 123 , .
, k 123 independently according to the density function f ( ) .
b ) pick values for e123 , .
, ek 123 independently from the exponential distribution with mean
one , whose density function is exp ( e ) .
c ) pick a value for i uniformly from ( 123 , .
d ) pick a value for s uniformly from ( 123 , +123 ) .
123 ) transform x and the auxiliary variables by repeating the following sequence of transformations
a ) first , apply the transformation tmet to ( x , i , ei ) .
second , do the following l 123 times : add s to i , modulo k , and then apply tmet to ( x , i , ei ) .
finally , if the number of these applications of tmet that were rejections is less than l or greater than h , change i back to its value at the start of this step ( ie , subtract ( l123 ) s from it , modulo k ) , change x back to its value at the start of this step , and negate s .
b ) add s to i , modulo k .
123 ) forget 123 , .
, k 123 , e123 , .
, ek 123 , i , and s , leaving just x as the new state .
the new step ( 123a ) above reverses the direction , s , only when a group is simulated in which the number of rejections is outside the desired range .
procedure 123 will therefore mimic a sequence of standard metropolis updates as long as the number of rejections in each group of l updates is in this range .
if a group of updates fails this test , the direction reverses , and previous states are revisited , without the need to recompute them .
once these previous states have all been visited ,
new states are again simulated .
if a second group of updates fails the test , the direction reverses again , and states are again revisited , with no further computation of states being required .
it is possible that the direction will subsequently reverse many times , with states being revisited again and again in a back - and - forth manner .
note that if the numbers of rejections in the rst two groups of updates are outside the desired range , we simply stay at the initial state .
figure 123 illustrates this short - cut metropolis procedure , for the case where l = 123 and m = 123 , and hence k = m l = 123 , and where l = 123 and h = l 123 ie , we reverse direction when all updates in a group are rejected , but not when none of the updates are rejected .
123 implementation issues
when implementing the short - cut metropolis method , the description in procedure 123 can be sim - plied .
there is no need to randomly pick a value for i this step is needed for the proof of validity , but one can see that the distribution of the nal result is the same regardless of which value for i was picked .
time can be saved by generating values for i and ei only if and when they are needed .
once used , the values for these auxiliary variables can be forgotten , provided the states that were generated using them are saved .
most importantly , as was discussed above , although procedure 123 as written appears to involve k = m l applications of tmet ( dened by equation ( 123 ) ) , each requiring evaluation of ( x ) at two states , when the direction of simulation reverses due to a group of l rejections , many of these applications of tmet can be avoided , since they produce states that have already been computed .
furthermore , if the value of ( x ) is saved for all states that have been computed , an application of tmet will require only the evaluation of ( x + w ) , since ( x ) will already be known .
( this is also true for the standard metropolis method . )
procedure 123 was shown in the previous section to leave invariant .
accordingly , if we apply this procedure repeatedly , we are justied in estimating expectations of functions of state using the nal states obtained at the end of each such application ( after discarding a suitable initial burn - in period ) .
this may be inecient , however .
each application of procedure 123 involves k metropolis updates .
if we did these in the standard manner , we would be able to use all k of these states when estimating expectations , not just the last of them .
this will produce more accurate estimates , although for dicult problems , in which these k states are highly dependent , the gain may be small ( and may not be worth the cost of storing these additional states ) .
fortunately , we can estimate expectations using all these states with the short - cut metropolis method as well .
in equilibrium , the state before applying procedure 123 will have distribution .
since every application of step ( 123 ) leaves invariant , the states at the end of step ( 123 ) will also have distribution , so we could use all m of these states when estimating expectations , not just the last .
similarly , each application of tmet in step ( 123a ) leaves invariant , so we can decide to use all k = m l of these states when estimating expectations .
note that if we decide to do this , we must use all these states regardless of whether or not tmet involved a rejection , and whether or not the direction of simulation was reversed in step ( 123a ) any scheme in which the states to use are chosen based on the results of the simulation might introduce bias .
if we decide to use all the k states produced using tmet ( or if we decide to use all m states at the end of step ( 123 ) ) , we have several options when one of these states turns out to be a duplicate of an earlier state .
the simplest option is to simply copy the earlier state to the area of memory
figure 123 : an illustration of the short - cut metropolis method , with l = 123 , m = 123 , l = 123 , and h = 123
the vertical axis represents the state ( here one dimensional ) .
the horizontal axis represents the index , i , of the auxiliary variables , ( i , ei ) , used for each update .
the value of for each update is shown below either the original i or its negation .
grey arrows show the k applications of tmet; dotted arrows indicate applications that produce already - computed states .
black arrows show the results of the full transformation dened by step ( 123a ) of procedure 123 , with dotted arrows again indicating the results have already been computed .
the top panel shows the rst two groups of updates , with the second group consisting only of rejections .
the middle panel shows the next group revisiting the rst group of updates in reverse order , and then three additional groups , simulated starting with the original state , with the last group consisting only of rejections .
the bottom panel shows the nal group of l updates , which again revisits already computed states .
reserved for the new state , or to write the earlier state a second time to the output le , if the output is not stored in main memory .
if states are stored in main memory , we could instead just store a pointer to the earlier state .
perhaps the most ecient ( though more complicated ) method would be to store only a single copy of each state , but to accompany this copy with a count of how many times it should be included in the averages used to estimate expectations .
( a similar issue arises with the standard metropolis method whenever a proposal is rejected , but since rejection rates are usually not extreme , the usual practice of simply copying the rejected state is generally adequate . )
if states are very large , and only the nal states of short - cut sequences are used for estimating expectations , it is possible to use a method that avoids storing any but the initial and current states .
at the beginning of a short - cut sequence , the initial state and the state of the pseudo - random number generator are saved .
metropolis updates are then simulated , with only the current state being retained .
if a reversal occurs , the initial state may have to be restored , at which point the state of the pseudo - random generator is again saved .
during this procedure , we dont actually copy any states when performing updates that produce previously - computed states ( which we havent saved ) we just keep track of where we are in the earlier part of the sequence .
if the nal state of the sequence is not a copy of an earlier state , it will be available as the current state .
otherwise , we record the position of the nal state that was reached , save the nal state of the pseudo - random number generator , restore the initial state and the appropriate saved state of the pseudo - random number generator , and simulate the number of metropolis updates needed to re - create the desired nal state .
the nal state of the pseudo - random number generator is then restored .
this procedure reduces the amount of memory required , but may increase the computation time needed by up to a factor of two , though typically the average increase will be much less than this .
standard errors for estimates of expectations are usually found from the sample variance and sample autocorrelation function ( see , for example , neal 123 , section 123 ) .
if all k states produced by a short - cut metropolis update are used , one might wonder whether this is valid , since the process appears to be non - stationary , in which case the autocorrelation function would not be well - dened .
in fact , however , one can view the process as being stationary if the state is extended to include auxiliary variables , including one that produces alternation between steps ( 123 ) and ( 123 ) of procedure 123
alternatively , one can look at the sample variance and sample autocorrelation function of block averages , over the k states in each short - cut metropolis update , or over longer blocks consisting of several short - cut metropolis updates with dierent stepsizes .
123 demonstrations on simple distributions
to illustrate the operation of the short - cut metropolis procedure , and provide some insight into its performance , i will show how it works when sampling from three simple distributions a one - dimensional distribution for which the updates can easily be visualized , a multivariate gaussian distribution in which the variance in some directions is much smaller than in others , and a funnel distribution that has features typical of bayesian hierarchical models .
the program for these examples is listed in the appendix , and is available from my web page .
123 a one - dimensional mixture distribution
i will start with a one - dimensional example , since this allows for easy visualization , though perfor - mance on one - dimensional examples is not always typical of what happens in higher dimensions .
the distribution used here is an equal mixture of two gaussian distributions , one with mean 123 and standard deviation 123 , the other with mean 123 and standard deviation 123
the density function for this mixture is
we will try to estimate the mean of this distribution from points generated using metropolis updates .
the true value of the mean is ( 123 / 123 ) 123 + ( 123 / 123 ) 123 = 123
i used a gaussian proposal distribution centred on the current state , with standard deviation w ( in other words , was gaussian with mean zero and standard deviation 123 , so that w had standard deviation w ) .
if the distribution to sample from were gaussian with standard deviation 123 , a stepsize of w = 123 might be appropriate , while if the distribution to sample from were gaussian with standard deviation 123 , we might choose w = 123
since the actual distribution is a mixture of these two , we might be uncertain whether the best stepsize is w = 123 or w = 123 , or we might think that we need to use both stepsizes at dierent times .
i tried ve sampling methods on this distribution :
standard metropolis with w = 123
standard metropolis with w = 123
naive adaptive metropolis :
use w = 123 if there were more than 123 rejections in the last 123 updates use w = 123 otherwise
short - cut metropolis with l = 123 and h = l 123 , alternating between two sequences :
using w = 123 with l = 123 and m = 123 ( so k = 123 ) using w = 123 with l = 123 and m = 123 ( so k = 123 )
short - cut metropolis with l = 123 and h = l 123 , alternating between two sequences :
using w = 123 with l = 123 and m = 123 ( so k = 123 ) using w = 123 with l = 123 and m = 123 ( so k = 123 )
the standard and naive adaptive metropolis methods were run for 123 million iterations , and therefore required 123 million evaluations of ( x ) .
the short - cut metropolis method with l = 123 ( reversing only when all updates in a group were rejected ) was run for 123 pairs of sequences with w = 123 and w = 123 , which also required about 123 million evaluations of ( x ) .
the short - cut metropolis method with l = 123 ( reversing when either all or none of the updates in a group were rejected ) was run for 123 pairs of sequences with w = 123 and w = 123 , again requiring about 123 million evaluations of ( x ) .
all states generated by each method were averaged to estimate the mean of x .
for the standard and naive metropolis methods , there were 123 million states .
for the two short - cut metropolis methods , the numbers of states averaged was 123 million and 123 million , but many of these states were copies of other states .
the results are shown in table 123
the rejection rate for metropolis updates includes , for the short - cut methods , those that are not actually performed .
the autocorrelation time for x is dened to be one plus twice the sum of the autocorrelations for x at lags one to innity; it is here estimated using the estimated autocorrelations at lags one to 123
the estimated mean for x is the sample average for all states .
the standard error for this estimate is found from the variance of the mixture distribution , which is known to be exactly 123 , and the eective sample size , which is the number of states used for the average divided by the autocorrelation time .
for further details on computation of mcmc standard errors , see ( neal 123 , section 123 ) .
standard , w = 123 standard , w = 123 short - cut , l = 123 short - cut , l = 123
table 123 : results of ve metropolis methods on the one - dimensional mixture distribution .
although standard metropolis with stepsize w = 123 would be good for exploring the mixture com - ponent with standard deviation one , we can see from its high autocorrelation time ( and consequent large standard error ) that it does not move around the whole mixture distribution eciently .
the low overall rejection rate of 123 is another indication that w = 123 is too small .
the results with w = 123 are much better .
for both methods , the estimated mean for x diers from the true value of 123 by an amount that is compatible with the estimated standard error .
we might hope to improve on both of these standard metropolis methods using an adaptive scheme that chooses between w = 123 and w = 123 according to whether the number of rejections in the last 123 updates was more than 123 or not .
this naive adaptive scheme does produce a reasonable rejection rate , and a fairly low autocorrelation time .
but the estimate it produces for the mean of x is completely wrong .
this estimate of 123 diers from the true value of 123 by more than 123 times the standard error , showing that this method is heavily biased .
the two short - cut metropolis methods do get the right answer , within plus or minus twice the standard error .
they both are more ecient than standard metropolis with w = 123 , but they are less ecient than standard metropolis with w = 123
this is not too surprising , since for very low - dimensional distributions , large stepsizes can be desirable , even when they produce large rejection rates ( often even when the rejection rate is much larger than the fairly moderate value of 123 seen here for w = 123 ) .
we will have to look at higher - dimensional problems to assess the advantages of
this one - dimensional example does allow for a good visualization of short - cut metropolis , as seen in figure 123
the top panel of this gure shows the rst four short - cut metropolis sequences from the run in which reversals were done only when a group of l = 123 updates were all rejections .
in the rst sequence , with k = 123 and w = 123 , all the states are outside the narrow mixture component around x = 123
in this region , w = 123 is a very small stepsize .
consequently , none of the groups consists only of rejections , there are no reversals , and the entire sequence looks exactly like a sequence of standard metropolis updates .
the next short - cut sequence , with k = 123 and w = 123 , enters the region around x = 123 , where there is a peak in the mixture density , which results in a high probability of rejecting a proposal generated with w = 123
the group of updates from index 123 to 123 are all rejected , resulting in a reversal .
the states copied after that are shown as light dots .
once the initial state from index 123 has been copied , simulation of new states ( shown as darker dots ) resumes , continuing until another group of all rejections occurs at indexes 123 to 123 , after which no further computation of states is need to the end of this sequence .
the third and fourth short - cut sequences repeat this pattern the rst ( with w = 123 ) has no reversals , and the second ( with w = 123 ) has two reversals , with about half the states copied from other states .
the bottom panel in figure 123 shows four short - cut sequences from the run in which reversals
w = 123
w = 123
two pairs of short - cut metropolis sequences with l = 123 and h = l 123
w = 123
w = 123
two pairs of short - cut metropolis sequences with l = 123 and h = l 123
figure 123 : short - cut metropolis for the one - dimensional mixture distribution .
the top plot shows a portion of a short - cut metropolis run with l = 123 and h = l123 , for which reversals occur only when all updates in a group are rejected; the bottom plot shows a portion of a run with l = 123 and h = l123 , for which reversals occur when either all or none of the updates in a group are rejected .
the vertical axis is the state ( x ) , with 123 123 shown by horizontal lines .
the horizontal axis indexes the updates , with the gray vertical lines marking the ends of groups , and the black vertical lines marking the ends of short - cut sequences .
dark gray dots are states that required computation to nd; light gray dots are states that were copied from earlier states .
black circles are states at the ends of groups .
the stepsizes used for the short - cut sequences are shown above the plots .
occur when a group of updates has either no rejections or all rejections .
in the rst sequence , with w = 123 and k = 123 , the rst group has no rejections .
the direction of simulation is therefore reversed , and the state is restored to the initial state .
one update in the second group is a rejection , but the third group has no rejections , causing a second reversal .
note that at this reversal the state is restored to what it was at the beginning of the third group .
consequently , the state after simulating this group is dierent from the state produced by the last update in the group a situation that cannot happen when reversals occur only when all updates in a group are rejected .
subsequent groups in this sequence are just copies of previously - computed states .
eectively , the short - cut procedure has decided that the stepsize is too small , so that little work should be done for this sequence .
in the second sequence , with w = 123 and k = 123 , a reversal occurs after the second group of updates , all of which were rejected , but no second reversal occurs , so only a few states were copied from other states .
here , the short - cut method has decided that the stepsize is about right .
in the third sequence , the stepsize is again w = 123 , but the chain enters the region around x = 123 , for which this stepsize works well , so most of the states are not copies .
the fourth sequence , with w = 123 , is also in the region around x = 123 , for which w = 123 is too large a stepsize .
the rst two groups consist only of rejections , so the entire sequence of states is the same as the initial state , with only the rst two groups requiring any computation .
123 a multivariate gaussian distribution
as a second demonstration , i applied standard and short - cut metropolis methods to the problem of sampling from a seven - dimensional multivariate gaussian distribution .
the mean of this dis - tribution was the zero vector , and its covariance matrix was diagonal , with the variances of the rst two components being 123 and the remaining ve being 123 .
the proposal distribution was also gaussian , with mean equal to the current point , and covariance matrix of wi .
note that since the proposal distribution is spherically symmetric , behaviour would be unchanged if the distribution were rotated .
accordingly , although the seven coordinates are independent in the distribution ac - tually used , the same behaviour would be seen for any multivariate gaussian distribution whose covariance matrix has eigenvalues equal to the variances used here .
i applied the standard metropolis method to this distribution using stepsizes of w = 123 , w = 123 , and w = 123 , in each case for 123 updates .
the rst of these stepsizes is too small , and the last is too big , but we imagine that we do not realize this initially .
accordingly , i also did a run in which these three stepsizes were applied in turn , each for 123 updates at a time , with this cycle being repeated 123 times , again for a total of 123 updates .
all 123 states produced were used to estimate expectations .
i tried three versions of short - cut metropolis .
all versions used groups of l = 123 updates , and cycled among sequences with w = 123 , w = 123 , and w = 123 .
the total number of metropolis updates for these three stepsizes ( including updates that were copied from previous states ) was always 123
the number of times these three sequences were repeated was adjusted so that the total number of evaluations of ( x ) was approximately 123 , as for the standard metropolis runs .
more than 123 states were produced ( some copied from earlier states ) , all of which were used to estimate expectations .
in the rst version of short - cut metropolis , reversals were done only when all updates in a group were rejections ( ie , l = 123 and h = l123 ) .
the short - cut sequences using w = 123 were of length k = 123 , those using w = 123 were of length k = 123 , and those using w = 123 were of length
standard , w = 123 standard , w = 123 standard , w = 123 standard , three ws short - cut , l = 123 short - cut , l = 123 short - cut , l = 123
table 123 : results of seven metropolis methods on the multivariate gaussian distribution .
k = 123
these increasing lengths were chosen so that the method would spend most of its time using an appropriate stepsize , regardless of which of the three was the appropriate one .
for sequences using the smallest stepsize of w = 123 , no reversals were done , even if a group consisted only of rejections ( ie , l = 123 and h = l ) , since there is no smaller stepsize to try in any case .
( in other words , the 123 updates with w = 123 were done in the standard way , with no short - cut . ) the three sequences were repeated 123 times , producing 123 million states .
the second version of short - cut metropolis used sequences of length k = 123 for all three stepsizes .
reversals were done when either all or none of the updates in a group were rejections ( ie , l = 123 and h = l123 ) , except that no reversal was done for a group of all rejections when using the smallest stepsize , and no reversal was done for a group of no rejections for the biggest stepsize .
for this version , the three sequences were repeated 123 times , producing 123 million states .
in the third version of short - cut metropolis , reversals were done when either all of the updates in a group were rejections , or when the number of rejections was less than two ( ie , l = 123 and h = l123 ) except that , as before , reversals were done for the biggest stepsize only for groups of all rejections , and for the smallest stepsize only for groups with less than two rejections .
the rationale for this version is that the optimal rejection rate is often somewhat greater than 123% ( see roberts and rosenthal 123 ) , so using an asymmetrical range for the desired number of rejections ( from l = 123 to h = l 123 ) may be benecial .
for this version , the three sequences were repeated 123 times , producing 123 million states .
results of estimating the expected value of the rst component of state ( whose true value is zero ) are shown in table 123
autocorrelation times were estimated using the estimated autocorrelations up to lag 123 for standard metropolis with w = 123 and w = 123 , and to lag 123 for the other methods .
the standard errors shown account for the varying autocorrelation times , and the fact that the short - cut metropolis runs have a larger number of states ( found using the same number of evaluations of ( x ) ) .
the actual dierences between the estimates and the true mean of zero are all consistent with the standard errors , exhibiting the usual amount of chance variation .
the smallest standard error is for the standard metropolis run with w = 123 .
standard metropolis runs with w = 123 and w = 123 produced much larger standard errors , showing that these stepsizes are not suitable .
by assumption , however , we dont know ahead of time that w = 123 is the best stepsize .
if we therefore use all three stepsizes in turn , we pay a price in terms of a larger standard error .
the cost can be estimated by the square of the ratio of the standard errors , which is ( 123 / 123 ) 123 = 123 .
this measures how much longer a run we would need using all three stepsizes to get the same accuracy as when using just w = 123 .
as expected , it is near 123 , since only
w = 123
w = 123
w = 123
short - cut , l = 123
short - cut , l = 123 short - cut , l = 123
table 123 : fractions of states copied from earlier states for three versions of short - cut metropolis .
the lengths ( k ) of the sequences for each stepsize ( w ) are shown as well .
123 / 123 of the time is spend using the good stepsize of w = 123 in the run using all three stepsizes .
we hope to do better than this using the short - cut method .
as seen in table 123 , all three versions of the short - cut method that were tried do indeed have smaller standard errors than standard metropolis using all three stepsizes , though their standard errors are greater than that of standard metropolis using w = 123 .
the dierences between the three short - methods are fairly small .
the estimated advantages over the standard method using all three stepsizes range from ( 123 / 123 ) 123 = 123 to ( 123 / 123 ) 123 = 123 .
we can gain some insight into how the short - cut methods perform by looking at the fraction of states that were copied from earlier states ( and hence required no evaluation of ( x ) ) .
these fractions are shown in table 123
in the version in which reversals occur only when a group consists of all rejections , almost all of the updates with w = 123 were copies , while almost none of the updates with w = 123 were copies .
( there were no copies for w = 123 , since these updates were done with standard metropolis . ) this is just what we hoped for .
however , the updates done with w = 123 still represent an ineciency , which is diminished but not eliminated by the fact that the sequences with w = 123 are shorter than those with w = 123 .
in the second version , reversals are done when either all or no rejections occur in a group .
again , most updates with w = 123 are copies .
over half the updates with w = 123 required actual computation , however .
since in this method the lengths of the sequences for dierent stepsizes were the same , the result is that the ineciency from doing updates with w = 123 was greater than for the rst version .
the third version reduces this problem by reversing when the number of rejections in a group is either zero or one ( except for the largest stepsize of w = 123 ) .
reversals then happen sooner with w = 123 , and consequently more states are copies , and less time is wasted with this unsuitable stepsize .
the standard error is therefore reduced .
123 a funnel distribution
as a nal illustration , i will show how short - cut metropolis can be advantageous when no single stepsize is optimal for all regions of the distribution being sampled .
i have previously used this example to illustrate the advantages of slice sampling ( neal 123 ) .
the state for this example consists of ten real - valued components , v and x123 to x123
the marginal distribution of v is gaussian with mean zero and standard deviation 123
conditional on a given value of v , the variables x123 to x123 are independent , with the conditional distribution for each being
gaussian with mean zero and variance ev .
the resulting distribution , , has a shape resembling a ten - dimensional funnel , with small values for v at its narrow end , and large values for v at its wide end .
such a distribution is typical of priors for components of bayesian hierarchical models x123 to x123 might , for example , be random eects for nine subjects , with v being the log of the variance of these random eects .
if the data happen to be largely uninformative , the problem of sampling from the posterior will be similar to that of sampling from the prior , so this test is relevant to actual bayesian inference problems .
i will focus here on estimating the mean of the v component .
the distribution is dened so that the true mean of v is zero , but we pretend here that we dont know that , and see how well various metropolis methods can estimate this mean , and more generally , the marginal distribution of v .
the metropolis methods i will consider update v and x123 to x123 simultaneously , using a multivariate gaussian proposal distribution with mean equal to the current state and covariance matrix wi .
all the methods employ sequences of 123 metropolis updates , with only the last state from each sequence being used to estimate expectations .
the initial state had v = 123 and all xi = 123
using standard metropolis updates with a xed value for w can be disastrous .
figure 123 shows values sampled for v using four stepsizes of w = 123 , w = 123 , w = 123 , and w = 123 , in runs consisting of 123 sequences of 123 updates .
when w = 123 , the run never goes much below zero .
this behaviour can be explained by imagining what would happen if the chain reached a point with a much smaller value of v , such as v = 123 , along with suitable values for x123 to x123
the standard deviation of the xi given this value of v is e123 / 123 = 123 .
with w = 123 , the probability of proposing a state with reasonable values of x123 to x123 is extremely small , so the probability of rejection is extremely high , and the markov chain will stay at the state with this small value of v for a very long time .
since the chain leaves invariant , it follows that movement from a state with a large value of v to a state with a small value of v must be very infrequent so infrequent that , as we see in the gure , it never happens in 123 million metropolis updates .
we do see signs of the problem in the long strings of rejections when the chain enters states with v slightly less than zero .
when w = 123 , the same problem exists .
none of the states sampled have v less than 123 , which ought to occur with probability 123 .
worse , the plot in figure 123 for w = 123 shows no sign of there being any problem with his run .
this is the dreaded nightmare scenario of mcmc , in which lack of convergence leads to drastically wrong results that appear to be correct .
metropolis updates using the smallest stepsize of w = 123 also do not work well .
with this stepsize , small values of v are well visited , but large values , greater than about 123 , are visited only in rare extended excursions , such as the one in figure 123 between sequences 123 and 123
if only the rst quarter of this run had been done , no states with v > 123 would have been sampled , with no clear indication of a problem .
the run with w = 123 appears much better , sampling both states where v is above 123 and states where it is below 123
a deciency of very large and very small values for v is apparent , however .
in any case , it is hard to see how we would know enough to choose w = 123 ahead of time .
accordingly , we might try using standard metropolis with a stepsize that cycles amongst the values w = 123 , w = 123 , w = 123 , and w = 123 .
the top plot of figure 123 shows a run of this sort , in which these stepsizes are applied in turn for sequences of 123 updates .
the results now appear to be acceptable , although there is still some stickiness for very large or small values of v .
we may hope to do better using the short - cut metropolis method , by avoiding the computation
stepsize of 123
stepsize of 123
stepsize of 123
stepsize of 123
figure 123 : the standard metropolis method applied to the funnel distribution , using various step - sizes .
the horizontal axis indexes groups of 123 standard metropolis updates , with every eighth group being shown .
the vertical axis is the value of v at the end of each group of updates .
standard metropolis with stepsizes of 123 , 123 , 123 , and 123
shortcut metropolis with stepsizes of 123 , 123 , 123 , and 123
figure 123 : standard and short - cut metropolis methods using multiple stepsizes , applied to the funnel distribution .
the plots are analogous to those in figure 123
the run for the short - cut method was about twice as long as the standard metropolis run ( with the number of evaluations of being equal ) .
accordingly , while the state after every eighth sequence is shown for the standard method , every sixteenth is shown for the short - cut method , so that the plots can be more easily compared .
time spent on stepsizes that are not appropriate .
a short - cut metropolis run was done in which sequences of 123 metropolis updates were done as m = 123 groups of l = 123 updates .
reversals were done when a group consisted of all rejections , and when a group had fewer than three rejections ( ie , l = 123 and h = 123 ) , except that no reversals were done on all rejections for the largest stepsize , or on fewer than three rejections for the smallest stepsize .
because some states in the short - cut run are copied without further computation , 123 sequences of 123 updates can be done with the same number of evaluations of as with the 123 sequences done using standard metropolis .
the bottom plot of figure 123 shows that the short - cut method produces good results , sampling both large and small values of v at least as well as the standard metropolis run using four stepsizes .
table 123 provides a quantitative comparison of the methods .
the estimates in this table are based only on the nal state from each sequence of 123 metropolis updates , and autocorrelations are for these nal states , not for the states after each metropolis update .
autocorrelation times were estimated using estimated autocorrelations up to lag 123 for standard metropolis with w = 123 , w = 123 , and w = 123 , up to lag 123 for standard metropolis with w = 123 , and up to lag 123 for the standard and short - cut methods using all four stepsizes .
the standard metropolis runs with w = 123 and w = 123 produce estimates for the mean of v that are far from the true value , much further than would be expected given the computed standard errors .
these unrealistic standard errors result from underestimation of the autocorrelation time ,
standard , w = 123 standard , w = 123 standard , w = 123 standard , w = 123 standard , four ws short - cut , four ws
table 123 : results of several metropolis methods applied to the funnel distribution .
which for these runs is actually extremely large .
the standard error for the standard metropolis run with w = 123 may be realistic , but is uncomfortably large .
standard metropolis with w = 123 , standard metropolis using all four stepsizes , and short - cut metropolis using these four stepsizes all produce reasonable estimates , consistent with their standard errors .
the short - cut metropolis method produced the best results .
its advantage over standard metropolis using all four stepsizes is a factor of ( 123 / 123 ) 123 = 123 .
figure 123 shows how this advantage was obtained .
for each of the four stepsizes , a substantial fraction of states were sometimes copied from earlier states , with the amount of copying varying with the value of v at the start of a sequence .
in eect , the short - cut method allows the stepsize that is predominantly used to vary depending on where in the state space the chain is currently
stepsize of 123
stepsize of 123
stepsize of 123
stepsize of 123
figure 123 : how the fraction of states copied from earlier states by short - cut metropolis applied to the funnel distribution varies with stepsize .
the horizontal axis is the value of v at the start of a short - cut sequence .
the vertical axis is the fraction of states in a short - cut sequence that were copied from earlier states .
a point is plotted for every fth short - cut sequence in the run .
in this paper , i have shown that by using the short - cut metropolis method the stepsize for the random - walk metropolis method can in eect be adaptively chosen from a small number of alter - natives , without the complications that arise in other adaptive schemes that abandon the markov property .
the short - cut method is also capable of using dierent stepsizes in dierent regions of the state space .
two general strategies have been demonstrated in the examples above .
in one strategy , short - cuts are taken only when the rejection rate is high ( due to the stepsize being too big ) .
we cyclicly simulate short - cut sequences using a range of stepsizes , with the sequences using bigger stepsizes being much longer than those using smaller stepsizes .
if the biggest stepsize turns out to be best , it will dominate the computation time simply because the sequence using it is much longer than those using smaller stepsizes .
if instead a smaller stepsize turns out to be best , it will dominate because the sequences with bigger stepsizes will take little time to simulate , once two groups of updates consisting of all rejections are encountered .
a limitation of this strategy is that the sequences using increasing stepsizes must have lengths that increase exponentially .
if we need to use a large range of stepsizes , we might nd that the length of the longest sequence is greater than the total number of updates we wish to perform .
the second strategy does not suer from this problem .
it also cyclicly simulates short - cut sequences using several stepsizes , but these sequences can all be of the same length .
short - cuts are taken when the rejection rate is either too high ( stepsize too big ) or the rejection rate is too low ( stepsize too small ) .
if only one of the stepsizes used is appropriate , only the sequence using this stepsize will take appreciable time to simulate .
simulations of sequences using the other stepsizes will soon encounter two groups of updates for which the number of rejections is either below the limit lower established , or above the upper limit , after which no further computation of states is required .
a fairly large number of stepsizes can feasibly be used with this strategy .
with either strategy , it would be possible to pick stepsizes randomly from some range , rather than cycling through a small number of possible stepsizes .
the length ( k ) of the sequence could be a function of the chosen stepsize , as could l , h , and l .
one possible improvement would be to look not at the actual number of rejections within a group of updates , but rather at the expected number of rejections , which is just the sum of one minus the acceptance probability of equation ( 123 ) over all updates in a group .
the set a in equation ( 123 ) would be dened in terms of the expected number of rejections for a group of updates starting in the given state .
we must be careful , however .
for the method to remain valid , we must either look at whether the state at the start of a group or the state at the end of the group is in a , or dene the set a so that if the start state is in a then the end state will be also .
the latter approach is probably preferable .
we could , for example , dene a to consist of states that produce a group of updates for which the average of the expected number of rejections when simulating the group forwards and when simulating it backwards is outside some desired interval .
this change would reduce the amount of random variation in whether or not a group of updates causes a reversal in the simulation .
this is likely to be benecial , since it will lead to unsuitable stepsizes being more reliably identied .
situations where bad luck leads to two early reversals even though the stepsize is suitable will be less common .
conversely , it will be less likely that a sequence using an unsuitable stepsize will avoid encountering two reversals early on .
the examples in this paper use metropolis updates that change all components of the state at once .
it is common to instead perform a sequence of metropolis updates , each of which proposes to change only one component ( or sometimes a few components ) .
the simplest approach to applying the short - cut technique in this context would use a single stepsize parameter , w , multiplying the scale of all the proposal distributions , and base reversals on the total number of rejections for updates of all components .
this might , however , lead to most computation time being devoted to a stepsize that is suitable for most of the components , but is much too large or small for a one or a few components .
one might instead base reversals on the maximum rejection rate over all components , to avoid the possibility that one component becomes stuck as a result of using too large a w .
much better , however , would be to somehow adaptively choose dierent stepsizes for dierent components .
similarly , if we update all components at once , using a gaussian proposal distribu - tion , we would ideally adaptively choose the entire covariance matrix , not just a single scale factor , w .
unfortunately , the short - cut technique does not easily handle a large number of tuning param - eters .
we could perform short - cut sequences using randomly - chosen values for these parameters , and do reversals based on some criterion that indicates whether the chosen values are unsuitable .
however , with many parameters , suitable values might be chosen very rarely .
this appears to be a fundamental limitation of the short - cut method .
situations in which we wish to tune just one or a few parameters of an mcmc method are not uncommon , however .
aside from the metropolis methods discussed in this paper , one might try to extend the short - cut technique to slice sampling ( neal 123 ) , combining slice samplings short - term adaptation within each update with the short - cut methods longer - term adaptation .
exactly how to apply the short - cut method to slice sampling remains to be worked out , however .
it is possible to use the short - cut technique for mcmc methods based on simulation of hamiltonian dynamics ( for a review , see neal 123 , section 123 ) , where the parameter to be tuned is the stepsize for a discretization of the dynamics .
this stepsize must be kept small enough that the discretization error in the hamiltonian is not too large .
this is a very natural setting for the short - cut method , since the dynamical updates are already deterministic .
indeed , it is in this context that the short - cut idea rst occurred to me .
an irony with adaptive schemes is that while they are designed to ease the burden of setting the parameters of an mcmc method , they themselves introduce even more parameters to be set .
thus with the short - cut method , we must select the set of w values , the group size , l , the number of groups , m , and the parameters , l and h , that dene the desired range of rejections .
of course , the hope is that setting these parameters will be relatively easy , whereas picking a suitable value for w when we are ignorant about the characteristics of the distribution may be impossible .
it will take experience with a wide range of problems to see how well this hope is fullled , and to develop guidelines for how best to use the short - cut method in practice .
appendix an r function implementing short - cut metropolis
implements a short - cut the function below , written in r ( see http : / / www . r - project . org ) , metropolis update .
this program ( with additional code for gathering statistics ) is available , along with scripts for the demonstrations in this paper , at http : / / www . cs . utoronto . ca / radford .
note that this program is intended for demonstration purposes only .
i have made no serious at - tempt to optimize the code .
due to the interpretive implementation of r , the short - cut method suers from signicant overheads , which are not due to any fundamental aspect of the method .
the code below begins with a function for performing a standard metropolis update , followed by the function for short - cut metropolis .
the version here returns all k states produced in the course of the short - cut procedure , all of which may be used when estimating expectations of state .
the initial state ( a vector ) function returning the log probability of a state , plus an function returning the random offset ( a vector ) for a proposal the stepsize for proposals , multiplies the offset
# do one metropolis update .
# the value returned is a list containing the following elements :
initial . lpr the value of lpr ( initial . x ) .
the new state the value of lpr ( next . x ) true if the proposal was rejected
metropolis . update < - function ( initial . x , lpr , pf , w , initial . lpr )
# propose a candidate state , and evalute its log probability .
proposed . x < - initial . x + w*pf ( ) proposed . lpr < - lpr ( proposed . x )
# decide whether to accept or reject the proposed state as the new state .
if ( runif ( 123 ) <exp ( proposed . lpr - initial . lpr ) ) # accept ( next . x < - proposed . x
next . lpr < - proposed . lpr rejected < - false
else # reject ( next . x < - initial . x
next . lpr < - initial . lpr rejected < - true
# return the new state , its log probability , and whether a rejection occurred .
list ( next . x=next . x , next . lpr=next . lpr , rejected=rejected )
the initial state ( a vector of length n ) function returning the log probability of a state , plus an function returning the random offset ( a vector ) for a proposal the stepsize for proposals , multiplies the offset number of updates in each group number of groups to simulate minimum number of rejections for a good group of l updates maximum number of rejections for a good group of l updates
# the short - cut metropolis method .
simulates a short - cut metropolis update # consisting of m*l metropolis updates .
# the value returned is a list containing the following elements :
a m*l+123 by n matrix whose rows contain the initial state and the states after each metropolis update , including updates in groups after which a reversal occurred a m+123 by n matrix whose rows contain the initial state and the states after each group of l metropolis updates; the last row contains the final state from the whole sequence
short . cut . metropolis < - function ( initial . x , lpr , pf , w , l , m ,
# the function below puts together the list of results that we return .
results < - function ( ) list ( states123=states123 , states123=states123 )
# the function below performs the l metropolis updates in a group , storing the # results in states123
the last . x and last . lpr variables should contain the # previous state and its log probability; they are updated by this function .
# the variable k indexes where in states123 the new states should be stored .
# it is incremented in this function .
the n . rejected variable is set to the # number of the l updates that were rejections .
do . group < - function ( ) ( n . rejected << - 123
for ( l in 123 : l ) ( update < - metropolis . update ( last . x , lpr , pf , w , last . lpr )
states123 ( k+123 , ) << - last . x << - update$next . x last . lpr << - update$next . lpr k << - k + 123 n . rejected << - n . rejected + update$rejected
# allocate space for the results .
k < - m*l states123 < - matrix ( na , k+123 , length ( initial . x ) ) states123 < - matrix ( na , m+123 , length ( initial . x ) )
# store the initial state in the first row of states123 and of states123 , and # evaluate its log probability .
states123 ( 123 , ) < - initial . x states123 ( 123 , ) < - initial . x initial . lpr < - lpr ( initial . x )
# do groups of metropolis updates starting from the initial state , until weve # done many as were asked for , or until the number of rejections in a group # is outside the limits .
states after each metropolis update are saved # in states123
the final state for each group is saved in states123
note # that for a group with the number of rejections outside the limits , the # state saved in states123 is not state the last state in states123 , but rather # state from the start of the group .
the indexes in states123 of the start # ( upper123 ) and end ( upper123 ) of these groups in are recorded for later use # in copying states .
last . x < - initial . x last . lpr < - initial . lpr k < - 123
upper123 < - k ( states123 ( 123+ ( k - 123 ) / l , ) < - last . x
if ( n . rejected<min . rej || n . rejected>max . rej ) ( upper123 < - k
( states123 ( 123+ ( k - 123 ) / l , ) < - states123 ( k , )
if ( k>k ) return ( results ( ) ) # return if weve done all m groups .
# copy already - computed states as " new " states as we move backwards through # the groups previously simulated .
dont copy the last group causing the # reversal , for which the number of rejections was outside the limits .
j < - upper123 - l - 123 while ( k<=k && j>=upper123 ) ( states123 ( ( k+123 ) : ( k+l ) , ) < - states123 ( j : ( j - l+123 ) , ) states123 ( 123+ ( k+l - 123 ) / l , ) < - states123 ( 123+ ( j - l ) / l , ) k < - k + l; j < - j - l
if ( k>k ) return ( results ( ) ) # return if weve done all m groups .
# restore the initial state , then do more groups of metropolis updates , # until weve done as many as were asked for , or the number of rejections in # a group is outside the limits .
record the indexes of the start ( lower123 ) # and end ( lower123 ) of these groups in states123
last . x < - initial . x last . lpr < - initial . lpr
lower123 < - k ( states123 ( 123+ ( k - 123 ) / l , ) < - last . x
if ( n . rejected<min . rej || n . rejected>max . rej ) ( lower123 < - k
( states123 ( 123+ ( k - 123 ) / l , ) < - states123 ( k , )
if ( k>k ) return ( results ( ) ) # return if weve done all m groups .
# copy already - computed states as " new " states , going back and forth over # the " lower " groups and the " upper " groups .
# copy the lower states backwards , excluding the group causing the reversal .
j < - lower123 - l - 123 while ( k<=k && j>=lower123 ) ( states123 ( ( k+123 ) : ( k+l ) , ) < - states123 ( j : ( j - l+123 ) , ) states123 ( 123+ ( k+l - 123 ) / l , ) < - states123 ( 123+ ( j - l ) / l , ) k < - k + l; j < - j - l
if ( k>k ) return ( results ( ) ) # return if weve done all m groups .
# copy the upper states forwards , including the group causing the reversal .
j < - upper123+123 while ( k<=k && j<=upper123 ) ( states123 ( ( k+123 ) : ( k+l ) , ) < - states123 ( j : ( j+l - 123 ) , )
states123 ( 123+ ( k+l - 123 ) / l , ) < - states123 ( 123+ ( j+l - 123 ) / l , ) k < - k + l; j < - j + l
if ( k>k ) return ( results ( ) ) # return if weve done all m groups .
# copy the upper states backwards , excluding the group causing the reversal .
j < - upper123 - l - 123 while ( k<=k && j>=upper123 ) ( states123 ( ( k+123 ) : ( k+l ) , ) < - states123 ( j : ( j - l+123 ) , ) states123 ( 123+ ( k+l - 123 ) / l , ) < - states123 ( 123+ ( j - l ) / l , ) k < - k + l; j < - j - l
if ( k>k ) return ( results ( ) ) # return if weve done all m groups .
# copy the lower states forwards , including the group causing the reversal .
j < - lower123 + 123 while ( k<=k && j<=lower123 ) ( states123 ( ( k+123 ) : ( k+l ) , ) < - states123 ( j : ( j+l - 123 ) , )
states123 ( 123+ ( k+l - 123 ) / l , ) < - states123 ( 123+ ( j+l - 123 ) / l , ) k < - k + l; j < - j + l
if ( k>k ) return ( results ( ) ) # return if weve done all m groups .
i thank david mackay for helpful comments on the manuscript .
this research was supported by the natural sciences and engineering research council of canada .
i hold a canada research chair in statistics and machine learning .
