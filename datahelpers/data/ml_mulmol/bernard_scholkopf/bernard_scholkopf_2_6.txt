abstract .
we present a kernel - based framework for pattern recognition , regression estimation , function approximation , and multiple operator inversion .
adopting a regularization - theoretic framework , the above are formulated as constrained optimization problems .
previous approaches such as ridge regression , support vector methods , and regularization networks are included as special cases .
we show connections between the cost function and some properties up to now believed to apply to support vector machines only .
for appropriately chosen cost functions , the optimal solution of all the problems described above can be found by solving a simple quadratic programming problem .
key words .
kernels , support vector machines , regularization , inverse problems , regression , pattern
introduction .
estimating dependences from empirical data can be viewed as risk minimization ( 123 ) : we are trying to estimate a function such that the risk , dened in terms of some a priori chosen cost function measuring the error of our estimate for ( unseen ) inputoutput test examples , becomes minimal .
the fact that this has to be done based on a limited amount of training examples comprises the central problem of statistical learning theory .
a number of approaches for estimating functions have been proposed in the past , ranging from simple methods like linear regression over ridge regression ( see , e . g . , ( 123 ) ) to advanced methods like generalized additive models ( 123 ) , neural networks , and support vectors ( 123 ) .
in combination with different types of cost functions , as for instance quadratic ones , robust ones in hubers sense ( 123 ) , or - insensitive ones ( 123 ) , these yield a wide variety of different training procedures which at rst sight seem incompatible with each other .
the purpose of this paper , which was inspired by the treatments of ( 123 ) and ( 123 ) , is to present a framework which contains the above models as special cases and provides a constructive algorithm for nding global solutions to these problems .
the latter is of considerable practical relevance insofar as many common models , in particular neural networks , suffer from the possibility of getting trapped in local optima
our treatment starts by giving a denition of the risk functional general enough to deal with the case of solving multiple operator equations ( section 123 ) .
these provide a versatile tool for dealing with measurements obtained in different ways , as in the case of sensor fusion , or for solving boundary constrained problems .
moreover , we show that
123 this work was supported by the studienstiftung des deutschen volkes and a grant of the dfg #ja 123 / 123
123 gmd first , rudower chaussee 123 , 123 berlin , germany .
smola@rst . gmd . de .
123 max planck institut fur biologische kybernetik , spemannstrasse 123 , 123 tubingen , germany .
bs@mpik -
received january 123 , 123; revised june 123 , 123 , and july 123 , 123
communicated by p .
auer and w .
maass .
smola and b .
scholkopf
they are useful for describing symmetries inherent to the data , be it by the incorporation of virtual examples or by enforcing tangent constraints .
to minimize risk , we adopt a regularization approach which consists in minimizing the sum of training error and a complexity term dened in terms of a regularization operator ( 123 ) .
minimization is carried out over classes of functions written as kernel expansions in terms of the training data ( section 123 ) .
moreover , we describe several common choices of the regularization operator .
following that , section 123 contains a derivation of an algorithm for practically obtaining a solution of the problem of minimizing the regularized risk .
for appropriate choices of cost functions , the algorithm reduces to quadratic programming .
section 123 generalizes a theorem by morozov from quadratic cost functions to the case of convex ones , which will give the general form of the solution to the problems stated above .
finally , section 123 contains practical applications of multiple operators to the case of problems with prior knowledge .
appendices a and b contain proofs of the formulae of sections 123 and 123 , and appendix c describes an algorithm for incorporating prior knowledge in the form of transformation invariances in pattern recognition problems .
risk minimization .
in regression estimation we try to estimate a functional depen - dency f between a set of sampling points x = ( x123 , .
, x ( cid : 123 ) ) taken from a space v , and target values y = ( y123 , .
, y ( cid : 123 ) ) .
we now consider a situation where we cannot observe x , but some other corresponding points xs = ( xs123 , .
, xs ( cid : 123 ) s ) , nor can we observe y , but ys = ( ys123 , .
, ys ( cid : 123 ) s ) .
we call the pairs ( xss ( cid : 123 ) , yss ( cid : 123 ) ) measurements of the dependency f .
suppose we know that the elements of xs are generated from those of x by a ( possibly nonlinear ) transformation t :
( cid : 123 ) = 123 , .
, ( cid : 123 ) ) .
xss ( cid : 123 ) = t xs ( cid : 123 )
the corresponding transformation a t acting on f ,
( a t f ) ( x ) : = f ( t x ) ,
is then generally linear : for functions f , g and coefcients , we have
( f + g ) ) ( x ) = ( f + g ) ( t x )
= f ( t x ) + g ( t x ) = ( a t f ) ( x ) + ( a t g ) ( x ) .
knowing a t , we can use the data to estimate the underlying functional dependency .
for several reasons , this can be preferable to estimating the dependencies in the transformed data directly .
for instance , there are cases where we specically want to estimate the original function , as in the case of magnetic resonance imaging ( 123 ) .
moreover , we may have multiple transformed data sets , but only estimate one underlying dependency .
these data sets might differ in size; in addition , we might want to associate different costs with estimation errors for different types of measurements , e . g . , if we believe them to differ in reliability .
finally , if we have knowledge of the transformations , we may as well utilize it to improve the estimation .
especially if the transformations are complicated , the original function might be easier to estimate .
a striking example is the problem of backing up a
pattern recognition , regression , approximation , and operator inversion
truck with a trailer to a given position ( 123 ) .
this problem is a complicated classication problem ( steering wheel left or right ) when expressed in cartesian coordinates; in polar coordinates , however , it becomes linearly separable .
without restricting ourselves to the case of operators acting on the arguments of f only , but for general linear operators , we formalize the above as follows .
we consider pairs of observations ( x , y ) , with sampling points x and corresponding target values y .
the rst entry i of the multi - index : = ( i , i ( cid : 123 ) ) denotes the procedure by which we have runs over the observations 123 , .
, ( cid : 123 ) i .
in obtained the target values; the second entry i the following , it is understood that variables without a bar correspond to the appropriate entries of the multi - indices .
this helps us to avoid multiple summation symbols .
we may group these pairs in q pairs of sets xi and yi by dening x vi ,
xi = ( xi123 , .
, xi ( cid : 123 ) i yi = ( yi123 , .
, yi ( cid : 123 ) i
with vi being vector spaces .
we assume that these samples have been drawn independently from q corresponding probability distributions with densities p123 ( x123 , y123 ) , .
, pq ( xq , yq ) for the sets xi and yi ,
we further assume that there exists a hilbert space of real - valued functions on v ,
denoted by h ( v ) , and a set of linear operators a123 , .
, aq on h ( v ) such that
ai : h ( v ) h ( vi )
for some hilbert space h ( vi ) of real - valued functions on vi .
( in the case of pattern recognition , we consider functions with values in ( 123 ) only . )
our aim is to estimate a function f h ( v ) such that the risk functional
ci ( ( ai f ) ( xi ) , xi , yi ) pi ( xi , yi ) dxi dyi
r ( f ) = q ( cid : 123 )
is minimized . 123 ( in some cases , we restrict ourselves to subsets ofh ( v ) in order to control the capacity of the admissible models . )
the functions ci are cost functions determining the loss for deviations between the estimate generated by ai f and the target value yi at the position xi .
we require these
functions to be bounded from below and therefore , by adding a constant , we may as well require them to be nonnegative .
the dependence of ci on xi can , for instance , accommodate the case of a measurement device whose precision depends on the location of the measurement .
123 a note on underlying functional dependences : for each vi together with pi one might dene a function
yi ( xi ) : =
yi pi ( yi|xi ) dyi
and try to nd a corresponding function f such that ai f = yi holds .
this intuition , however , is misleading , as yi need not even lie in the range of ai , and ai need not be invertible either .
we resort to nding a pseudosolution
of the operator equation .
for a detailed treatment see ( 123 ) .
smola and b .
scholkopf
example 123 ( vapniks risk functional ) .
by specializing
q = 123 ,
we arrive at the denition of the risk functional of ( 123 ) : 123
r ( f ) =
c ( f , x , y ) p ( x , y ) dx dy .
specializing to c ( f ( x ) , x , y ) = ( f ( x ) y ) 123 leads to the denition of the least mean
square error risk ( 123 ) .
as the probability density functions pi are unknown , we cannot evaluate ( and mini -
mize ) r ( f ) directly .
instead we only can try to approximate
fmin : = argminh ( v ) r ( f )
by some function f , using the given data sets xi and yi .
in practice , this requires
considering the empirical risk functional , which is obtained by replacing the integrals over the probability density functions pi ( see ( 123 ) ) with summations over the empirical
remp ( f ) =
ci ( ( ai f ) ( x ) , x , y ) ) .
i ( cid : 123 ) =123 with = ( i , i
is a shorthand for
( cid : 123 ) ) .
the problem that here the notation arises now is how to connect the values obtained from remp ( f ) with r ( f ) : we can only compute the former , but we want to minimize the latter .
a naive approach is to minimize remp , hoping to obtain a solution f that is close to minimizing r , too .
the ordinary
least mean squares method is an example for these approaches , exhibiting overtting in the case of a high model capacity , and thus poor generalization ( 123 ) .
therefore it is not advisable to minimize the empirical risk without any means of capacity control or
regularization operators and additive models .
we assume a regularization term in the spirit of ( 123 ) and ( 123 ) , namely , a positive semidenite operator
mapping into a dot product space d ( whose closure d is a hilbert space ) , dening a regularized risk functional
p : h ( v ) d
rreg ( f ) = remp ( f ) +
( cid : 123 ) p f ( cid : 123 ) 123d
123 note that ( 123 ) already includes multiple operator equations for the special case where vi = v and pi = p for all i , even though this is not explicitly mentioned in ( 123 ) : c is a functional of f and therefore it may also be a sum of functionals ai f for several ai .
pattern recognition , regression , approximation , and operator inversion
with a regularization parameter 123
this additional term effectively reduces our model space and thereby controls the complexity of the solution .
note that the topic of this paper is not nding the best regularization parameter , which would require model selection criteria as , for instance , vc - theory ( 123 ) , bayesian methods ( 123 ) , the minimum description length principle ( 123 ) , aic ( 123 ) , nic ( 123 ) a discussion of these methods , however , would go beyond the scope of this work .
instead , we focus on how and un - der which conditions , given a value of , the function minimizing rreg can be found we do not require positive deniteness of p , as we may not want to attenuate contri - butions of functions stemming from a given class of models m ( e . g . , linear and constant p such that m ker p .
making more specic as - ones ) : in this case , we construct sumptions about the type of functions used for minimizing ( 123 ) , we assume f to have ( cid : 123 ) v ) with the property a function expansion based on a symmetric kernel k ( x , x that , for all x v , the function on v obtained by xing one argument of k to x is an element of h ( v ) .
to formulate the expansion , we use the tensor product notation for operators on h ( v ) h ( v ) , ( ( a b ) k ) ( x , x xed ) , and b vice versa .
the class of here a acts on k as a function of x only ( with x models that we investigate as admissible solutions for minimizing ( 123 ) are expansions of the form
f ( x ) =
( ( ai 123 ) k ) ( x , x ) + b ,
this may seem to be a rather arbitrary assumption; however , kernel expansions of the k ( x , x ) are quite common in regression and pattern recognition models ( 123 ) , and in the case of support vectors even follow naturally from optimality conditions with respect to a chosen regularization ( 123 ) , ( 123 ) .
moreover , an expansion with as many basis functions as data points is rich enough to interpolate all measurements exactly , except for some pathological cases , e . g . , if the functions k ( x ) : = k ( x , x ) are linearly dependent , or if there are conicting measurements at one point ( different target values for the same x ) .
finally , using additive models is a useful approach insofar as the computations of the coefcients may be carried out more easily .
to obtain an expression for ( cid : 123 ) p f ( cid : 123 ) 123d in terms of the coefcients , we rst note
( ( ai p ) k ) ( x , x ) .
for simplicity we have assumed the constant function to lie in the null space of p , i . e . , pb = 123
exploiting the linearity of the dot product in d , we can express ( cid : 123 ) p f ( cid : 123 ) 123d as
( p f p f ) =
( ( ( ai p ) k ) ( x , x ) ( ( aj p ) k ) ( x , x ) ) .
( p f ) ( x ) =
for a suitable choice of k and p , the coefcients
d : = ( ( ( ai p ) k ) ( x , . ) ( ( aj p ) k ) ( x , . ) )
smola and b .
scholkopf
can be evaluated in closed form , allowing an efcient implementation ( here , the dot in k ( x , . ) means that k is considered as a function of its second argument , with x xed ) .
positivity of ( 123 ) implies positivity of the regularization matrix d ( arranging and in dictionary order ) .
conversely , any positive semidenite matrix will act as a regularization matrix .
as we minimize the regularized risk ( 123 ) , the functions corresponding to the largest eigenvalue of d will be attenuated most; functions with expansion coefcient vectors lying in the null space of d , however , will not be dampened at all .
example 123 ( sobolev regularization ) .
smoothness properties of functions f can be enforced effectively by minimizing the sobolev norm of a given order .
our exposition at this point follows ( 123 ) : the sobolev space h s , p ( v ) ( s n , 123 p ) is dened as the space of those l p functions on v whose derivatives up to the order s are l p functions .
it is a banach space with the norm
( cid : 123 ) f ( cid : 123 ) h s , p ( v ) =
( cid : 123 ) d f ( cid : 123 ) l p
where is a multi - index and d is the derivative of order .
a special case of the sobolev embedding theorem ( 123 ) yields
h s , p ( v ) c k
s > k + d
here d denotes the dimensionality of v and c k is the space of functions with continuous derivatives up to order k .
moreover , there exists a constant c such that
| d f ( x ) | c ( cid : 123 ) f ( cid : 123 ) h s , p ( v ) ,
i . e . , convergence in the sobolev norm enforces uniform convergence in the derivatives up to order k .
for our purposes , we use p = 123 , for which h s , p ( v ) becomes a hilbert space .
in this
case , the coefcients of d are
( ( ( ai d ) k ) ( x , x ) ( ( aj d ) k ) ( x , x ) ) .
example 123 ( support vector regularization ) .
we consider functions which can be writ - ten as linear functions in some hilbert space h ,
f ( x ) = ( ( x ) ) + b
with : v h and h .
the weight vector is expressed as a linear combination of the images of x
the regularization operator p is chosen such that p f = for all ( in view of the expansion ( 123 ) , this denes a linear operator ) .
hence using the term ( cid : 123 ) p f ( cid : 123 ) 123d = ( cid : 123 ) ( cid : 123 ) 123
pattern recognition , regression , approximation , and operator inversion
corresponds to looking for the attest linear function ( 123 ) on h .
moreover , is chosen such that we can express the terms ( ( x ) ( x ) ) in closed form as some symmetric function k ( x , x ) , thus the solution ( 123 ) reads
k ( x , x ) + b ,
k ( x , x ) .
f ( x ) =
and the regularization term becomes
this leads to the optimization problem of ( 123 ) .
the mapping need not be known explicitly : for any continuous symmetric kernel k satisfying mercers condition ( 123 )
f ( x ) k ( x , y ) f ( y ) dx dy > 123
one can expand k into a uniformly convergent series k ( x , y ) = ( cid : 123 ) i i ( x ) i ( y ) with positive coefcients i for i n .
using this , it is easy to see that ( x ) : =
i i ( x ) ei ( ( ei ) denoting an orthonormal basis of ( cid : 123 ) 123 ) is a map satisfying ( ( x ) ( cid : 123 ) ) .
in particular , this implies that the matrix d = k ( x , x ) is positive .
( cid : 123 ) ) ) = k ( x , x different choices of kernel functions allow the construction of polynomial classiers ( 123 ) and radial basis function classiers ( 123 ) .
although formulated originally for the case where f is a function of one variable , mercers theorem also holds if f is dened on a space of arbitrary dimensionality , provided that it is compact ( 123 ) . 123
in the next example , as well as in the remainder of the paper , we use vector notation;
e . g . , ( cid : 123 ) denotes the vector with entries , with arranged in dictionary order .
if we dene p such that all functions used in the example 123 ( ridge regression ) .
expansion of f are attenuated equally and decouple , d becomes the identity matrix , d = .
this leads to
d = ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123
( cid : 123 ) p f ( cid : 123 ) 123d =
rreg ( f ) = remp ( f ) +
which is exactly the denition of a ridge - regularized risk functional , known in the neural network community as weight decay principle ( 123 ) .
the concept of ridge regression appeared in ( 123 ) in the context of linear discriminant analysis .
poggio and girosi ( 123 ) give an overview over some more choices of regularization
operators and corresponding kernel expansions .
123 the expansion of in terms of the images of the data follows more naturally if viewed in the support vector context ( 123 ) ; however , the idea of selecting the attest function in a high - dimensional space is preserved in the present exposition .
smola and b .
scholkopf
risk minimization by quadratic programming .
the goal of this section is to transform the problem of minimizing the regularized risk ( 123 ) into a quadratic program - ming problem which can be solved efciently by existing techniques .
in the following we only require the cost functions to be convex in the rst argument and
ci ( yi , xi , yi ) = 123
more specically , we require ci ( . , x , y ) to be zero exactly on the interval (
y ) with 123 ,
c ( ) : = 123 ) : = 123
, and c 123 everywhere else .
for brevity we write
ci ( y + + , x , y )
, x , y )
+ y , +
with x and y xed and
: = max ( ( ai f ) ( x ) y , 123 ) , : = max ( ( ai f ) ( x ) + y
in pattern recognition problems , the intervals (
the asterisk is used for distinguishing positive and negative slack variables and corre -
sponding cost functions .
the functions c and c describe the parts of the cost functions ci at the location ( x , y ) which differ from zero , split up into a separate treatment of ( ai f ) y and ( ai f ) y .
this is done to avoid the ( possible ) discontinuity in the rst derivative of ci at the point where it starts differing from zero .
, ) are either ( , 123 ) or ( 123 , ) .
in this case , we can eliminate one of the two appearing slack variables , thereby getting a simpler form for the optimization problem .
in the following , however , we deal with the more general case of regression estimation .
we may rewrite the minimization of rreg as a constrained optimization problem , using
, to render the subsequent calculus more amenable :
( c ( ) + c
subject to ( ai f ) ( x ) y + + ,
( ai f ) ( x ) y
rreg = 123
) ) + 123
( cid : 123 ) p f ( cid : 123 ) 123d
the dual of this problem can be computed using standard lagrange multiplier techniques .
in the following , we make use of the results derived in appendix a , and discuss some special cases obtained by choosing specic loss functions .
example 123 ( quadratic cost function ) .
we use ( 123 ) ( appendix a , example 123 ) in the special case p = 123 , = 123 to get the following unconstrained optimization problem : 123 k ( ( cid : 123 ) ( cid : 123 ) )
( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) y ) = 123 ,
( ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 + ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 ) 123
( ai 123 ) (
( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 )
pattern recognition , regression , approximation , and operator inversion
transformation back to is done by
( cid : 123 ) = d
123 k ( ( cid : 123 ) ( cid : 123 ) ) .
here , the symmetric matrix k is dened as
k : = ( ( ai aj ) k ) ( x , x ) ,
( ai 123 ) is the operator ai acting on the constant function with value 123
of course there would have been a simpler solution to this problem ( by combining and
variable resulting in an unconstrained optimization problem ) but in combination with other cost functions we may exploit the full exibility of our approach .
example 123 ( - insensitive cost function ) .
here we use ( 123 ) ( appendix a , example 123 ) for = 123
this leads to ( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) y ( ( cid : 123 ) + ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) 123
123 k ( ( cid : 123 ) ( cid : 123 ) )
( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 )
( ai 123 ) (
) = 123 ,
with the same back substitution rules ( 123 ) as in example 123
for the special case of support vector regularization , this leads to exactly the same equations as in support vector pattern recognition or regression estimation ( 123 ) .
in that case , one can show that d = k , and 123 k cancel out , with only the support vector equations remaining .
therefore the terms d this follows directly from ( 123 ) and ( 123 ) with the denitions of d and k .
note that the laplacian cost function is included as a special case for = 123
example 123 ( hubers robust cost function ) .
setting
in example 123 leads to the following optimization problem :
p = 123 ,
( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) y ( ai 123 ) ( ) = 123 ,
( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 )
123 k ( ( cid : 123 ) ( cid : 123 ) )
with the same backsubstitution rules ( 123 ) as in example 123
the cost functions described in the examples 123 , 123 , 123 , 123 , and 123 may be linearly com - bined into more complicated ones .
in practice , this results in using additional lagrange multipliers , as each of the cost functions has to be dealt with using one multiplier .
still , by doing so computational complexity is not greatly increased as only the linear part of the optimization problem is increased , whereas the quadratic part remains unaltered ( except for a diagonal term for cost functions of the huber type ) .
muller et al .
( 123 ) report excellent performance of the support vector regression algorithm for both - insensitive and huber cost function matching the correct type of the noise in an application to time
smola and b .
scholkopf
a generalization of a theorem of morozov .
we follow and extend the proof of a theorem originally stated by morozov ( 123 ) as described in ( 123 ) and ( 123 ) .
as in section 123 , we require the cost functions ci to be convex and c 123 in the rst argument with the extra
ci ( yi , xi , yi ) = 123
we use the notation d for the closure of d , and p p : h ( v ) d ,
d h ( v ) .
to refer to the adjoint123 of p ,
theorem 123 ( optimality condition ) .
under the assumptions stated above , a necessary and sufcient condition for
f = fopt : = argminf h ( v ) rreg ( f )
is that the following equation holds true :
p f = 123
123ci ( ( ai f ) ( x ) , x , y ) a
here , 123 denotes the partial derivative of ci by its rst argument , and x is the dirac distribution , centered on x .
for a proof of the theorem see appendix b .
in order to illustrate the theorem , we rst consider the special case of q = 123 and a = 123 , i . e . , the well - known setting of regression and pattern recognition .
greens functions
g ( x , xj ) corresponding to the operator p
pg ) ( x , xj ) = xj
as previously described in ( 123 ) .
in this case we derive from ( 123 ) the following system of equations which has to be solved in a self - consistent manner :
f ( x ) = ( cid : 123 ) ( cid : 123 )
i g ( x , xi ) + b
i = 123
123c ( f ( xi ) , xi , yi ) .
here the expansion of f in terms of kernel functions follows naturally with i correspond - ing to lagrange multipliers .
it can be shown that g is symmetric in its arguments , and
123 the adjoint of an operator o : ho do mapping from a hilbert space ho to a dot product space do is the operator o
such that , for all f ho and g do , = ( o
( g o f ) ho
g f ) do
pattern recognition , regression , approximation , and operator inversion
translation invariant for suitable regularization operators p .
equation ( 123 ) determines the size of i according to how much f deviates from the original measurements yi .
for the general case , ( 123 ) becomes a little more complicated , namely we have q
functions gi ( x , x ) such that
pgi ) ( x , x ) = ( a
in ( 123 ) greens formalism is used for nding suitable kernel expansions corre - sponding to the chosen regularization operators for the case of regression and pattern recognition .
this may also be applied to the case of estimating functional dependen - cies from indirect measurements .
moreover , ( 123 ) may also be useful for approximately solving some classes of partial differential equations by rewriting them as optimization
applications of multiple operator equations .
in the following we discuss some examples of incorporating domain knowledge by using multiple operator equations as contained in ( 123 ) .
example 123 ( additional constraints on the estimated function ) .
suppose we have ad - ditional knowledge on the function values at some points , for instance saying that for some , > 123
this can be incorporated by adding the points as an extra set xs = ( xs123 , .
, xs ( cid : 123 ) s ) x with corresponding target values ys = ( ys123 , .
, ys ( cid : 123 ) s
) y , an operator as = 123 , and a cost function ( dened on xs ) cs ( f ( xs ) , xs , ys ) =
if s f ( xs ) ys
dened in terms of s123 , .
, s ( cid : 123 ) s and
these additional hard constraints result in optimization problems similar to those ob - tained in the - insensitive approach of support vector regression ( 123 ) .
see example 123
monotonicity and convexity of a function f , along with other constraints on deriva -
tives of f , can be enforced similarly .
in that case , we use
instead of the as = 123 used above .
this requires differentiability of the function expansion
if we want to use general expansions ( 123 ) , we have to resort to nite difference
example 123 ( virtual examples ) .
suppose we have additional knowledge telling us that the function to be estimated should be invariant with respect to certain transformations
ti of the input .
for instance , in optical character recognition these transformations might sponding linear operators ai acting on h ( v ) as in ( 123 ) .
be translations , small rotations , or changes in line thickness ( 123 ) .
we then dene corre -
smola and b .
scholkopf
as the empirical risk functional ( 123 ) then contains a sum over original and trans - formed ( virtual ) patterns , this corresponds to training on an articially enlarged data set .
unlike previous approaches such as the one of ( 123 ) , we may assign different weight to the enforcement of the different invariances by choosing different cost functions ci .
if the ti comprise translations of different amounts , we may , for instance , use smaller cost
functions for bigger translations .
thus , deviations of the estimated function on these ex - amples will be penalized less severely , which is reected by smaller lagrange multipliers ( see ( 123 ) ) .
still , there are more general types of symmetries , especially nondeterministic ones , which also could be taken care of by modied cost functions .
for an extended discussion of this topic see ( 123 ) .
in appendix c we give a more detailed description of how to implement a virtual examples algorithm .
much work on symmetries and invariances ( e . g . , ( 123 ) ) is mainly concerned with global symmetries ( independent of the training data ) that have a linear representation in the domain of the input patterns .
this concept , however , can be rather restrictive .
even for the case of handwritten digit recognition , the above requirements can be fullled for translation symmetries only .
rotations , for instance , cannot be faithfully represented in this context .
moreover would a full rotation invariance not be desirable ( thereby transforming a 123 into a 123 ) only local invariances should be admitted .
some symmetries only exist for a class of patterns ( mirror symmetries are a reasonable concept for the digits 123 and 123 only ) and some can only be dened on the patterns themselves , e . g . , stroke changes , and do not make any sense on a random collection of pixels at all .
this requires a model capable of dealing with nonlinear , local , pattern dependent , and possibly only approximate symmetries , all of which can be achieved by the concept of virtual examples .
example 123 ( hints ) .
we can also utilize prior knowledge where target values or ranges for the function are not explicitly available .
for instance , we might know that f takes the same value at two different points x123 and x123 ( 123 ) ; e . g . , we could use unlabeled data together with known invariance transformations to generate such pairs of points .
to incorporate this type of invariance of the target function , we use a linear operator acting on the direct sum of two copies of input space , computing the difference between f ( x123 ) and f ( x123 ) , the technique of example 123 then allows us to constrain ( as f ) to be small , on a set of
( as f ) ( x123 x123 ) : = f ( x123 ) f ( x123 ) .
sampling points generated as direct sums of the given pairs of points .
as before ( 123 ) , we can modify the above methods using derivatives of f .
this will
lead to tangent regularizers as the ones proposed by ( 123 ) , as we shall presently show .
example 123 ( tangent regularizers ) .
we assume that g is a lie group of invariance transformations .
similar to ( 123 ) , we can dene an action of g on a hilbert space h ( v ) of functions on v , by
the generators in this representation , call them si , i = 123 , .
, r , generate the group si ) .
as rst - order
in a neighborhood of the identity via the exponential map exp (
( g f ) ( x ) : = f ( gx )
f h ( v ) .
pattern recognition , regression , approximation , and operator inversion
( tangential ) invariance is a local property at the identity , we may enforce it by requiring
to motivate this , note that
( si f ) ( x ) = 123
i = 123 ,
x j sj
= ( si f ) ( x ) , using ( 123 ) , the chain rule , and the identity exp ( 123 ) = 123
examples of operators si that can be used are derivative operators , which are the
generators of translations .
operator equations of the type ( 123 ) allow us to use virtual examples which incorporate knowledge about derivatives of f .
in the sense of ( 123 ) , this corresponds to having a regularizer enforcing invariance .
interestingly , our analy - sis suggests that this case is not as different from a direct virtual examples approach ( example 123 ) as it might appear supercially .
as in example 123 , prior knowledge could also be given in terms of allowed ranges or cost functions ( 123 ) for approximate symmetries , rather than enforced equalities as ( 123 ) .
moreover , we can apply the approach of example 123 to higher - order derivatives as well , generalizing what we said above about additional constraints on the estimated function
we conclude this section with an example of a possible application where the latter could be useful .
in three - dimensional surface mesh construction ( e . g . , ( 123 ) ) , one tries to represent a surface by a mesh of few points , subject to the following constraints .
first , the surface points should be represented accuratelythis can be viewed as a standard regression problem .
second , the normal vectors should be represented correctly , to make sure that the surface will look realistic when rendered .
third , if there are specular reections , say , geometrical optics comes into play , and thus surface curvature ( i . e . , higher - order derivatives ) should be represented accurately .
discussion .
we have shown that we can employ fairly general types of regulariza - tion and cost functions , and still arrive at a support vector type quadratic optimization problem .
an important feature of support vector machines , however , sparsity of the de - compositions of f , is due to a special type of cost function used .
the decisive part is the , y + ) inside of which the cost for approximation , regres - nonvanishing interval ( y
sion , or pattern recognition is zero .
therefore there exists a range of values ( ai f ) ( x ) = 123 for some .
by virtue of the karushkuhntucker in which ( 123 ) holds with ,
conditions , stating that the product of constraints and lagrange multipliers have to vanish at the point of optimality , ( 123 ) implies
( y + + ( ai f ) ( x ) ) = 123 , ( ( ai f ) ( x ) y + + ) = 123
smola and b .
scholkopf
therefore , the and inequalities .
this causes sparsity in the solution of and
have to vanish for the constraints of ( 123 ) that become strict
as shown in examples 123 and 123 , the special choice of a support vector regularization combined with the - insensitive cost function brings us to the case of support vector pattern recognition and regression estimation .
the advantage of this setting is that , in the low noise case , it generates sparse decompositions of f ( x ) in terms of the training data , i . e . , in terms of support vectors .
this advantage , however , vanishes for noisy data as the number of support vectors increases with the noise ( see ( 123 ) for details ) .
unfortunately , independent of the noise level , the choice of a different regularization 123 k may not generally prevents such an efcient calculation scheme due to ( 123 ) , as d be assumed to be diagonal .
consequently , the expansion of f is only sparse in terms of but not in .
yet this is sufcient for some encoding purposes as f is dened uniquely by the matrix d
123 k and the set of .
hence storing is not required .
the computational cost of evaluating f ( x123 ) also can be reduced .
for the case of a ( cid : 123 ) ) satisfying mercers condition ( 123 ) , the reduced set method ( 123 ) can be kernel k ( x , x applied to the initial solution .
in that case , the nal computational cost is comparable with the one of support vector machines , with the advantage of regularization in input space ( which is the space we are really interested in ) instead of high - dimensional space .
the computational cost is approximately cubic in the number of nonzero lagrange multipliers , as we have to solve a quadratic programming problem whose quadratic part is as large as the number of basis functions of the functional expansion of f .
optimization methods like the bunchkaufman decomposition ( 123 ) , ( 123 ) have the property of incuring computational cost only in the number of nonzero coefcients , whereas for cases with a large percentage of nonvanishing lagrange multipliers , interior point methods ( e . g . , ( 123 ) ) might be computationally more efcient .
by n the number of functions of which f is a linear combination , f ( x ) = ( cid : 123 )
we deliberately omitted the case of having fewer basis functions than constraints , as ( depending on the cost function ) optimization problems of this kind may become infeasible , at least for the case of hard constraints .
however , it is not very difcult to see how a generalization to an arbitrary number of basis functions could be achieved : denote i=123 fi ( x ) , and by m the number of constraints or cost functions on f .
then d will be an n n matrix and k an n m matrix , i . e . , we have n variables i and m lagrange multipliers i .
the calculations will lead to a similar class of quadratic optimization problems as described in ( 123 ) and ( 123 ) , with the difference that the quadratic part of the problem will be at most of rank n , whereas the quadratic matrix will be of size m m .
a possible way of dealing with this degeneracy is to use a singular value decomposition ( 123 ) and solve the optimization equations in the reduced space .
to summarize , we have embedded the support vector method into a wider regulari - zation - theoretic framework , which allow us to view a variety of learning approaches , including but not limited to least mean squares , ridge regression , and support vector machines as special cases of risk minimization using suitable loss functions .
we have shown that general arsenintikhinov regularizers may be used while still preserving important advantages of support vector machines .
specically , for particular choices of loss function , the solution to the above problems ( which can often be obtained only through nonlinear optimization , e . g . , in regression estimation by neural networks ) was reduced to a simple quadratic programming problem .
unlike many nonlinear optimiza -
pattern recognition , regression , approximation , and operator inversion
tion problems , the latter can be solved efciently without the danger of getting trapped in local minima .
finally , we have shown that the formalism is powerful enough to deal with indirect measurements stemming from different sources .
acknowledgments .
we would like to thank volker blanz , leon bottou , chris burges , patrick haffner , jorg lemm , klaus - robert muller , noboru murata , sara solla , vladimir vapnik , and the referees for helpful comments and discussions .
the authors are indebted to at&t and bell laboratories for the possibility to prot from an excellent research environment during several research stays .
appendix a .
optimization problems for risk minimization .
from ( 123 ) and ( 123 ) we arrive at the following statement of the optimization problem :
( c ( ) + c
) ) + 123 subject to ( ai f ) ( x ) y + + ,
( ai f ) ( x ) y
for all .
to this end , we introduce a lagrangian : ) ) + 123
( c ( ) + c
( y + + ( ai f ) ( x ) )
( ( ai f ) ( x ) y +
in ( 123 ) , the regularization term is expressed in terms of the function expansion coefcients
we next do the same for the terms stemming from the constraints on ( ai f ) ( x ) , and compute ai f by substituting the expansion ( 123 ) to get
( ai f ) ( x ) =
( ( aj ai ) k ) ( x , x ) + ai b =
k + ai b .
see ( 123 ) for the denition of k .
now we can compute the derivatives with respect to the primary variables , b , .
these have to vanish for optimality .
) ) = 123
( d k (
solving ( 123 ) for ( cid : 123 ) yields
( cid : 123 ) = d
123 k ( ( cid : 123 ) ( cid : 123 ) ) ,
smola and b .
scholkopf
123 is the pseudoinverse in case d does not have full rank .
we proceed to the
next lagrange condition , reading
( ai 123 ) (
) = 123 ,
using ai b = b ai 123
summands for which ( ai 123 ) = 123 vanish , thereby removing the constraint imposed by ( 123 ) on the corresponding variables .
partial differentiation with respect to and
now we may substitute ( 123 ) , ( 123 ) , and ( 123 ) back into ( 123 ) , taking into account the substitution ( 123 ) , and eliminate and , obtaining c ( ) + c
+ ( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) y ( ( cid : 123 ) + ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) 123 ( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 )
123 k ( ( cid : 123 ) ( cid : 123 ) ) .
c ( ) = +
the next step is to ll in the explicit form of the cost functions c , which will enable us to eliminate , with programming problems in the remaining .
however ( as one can
see ) , each of the c and c may have its own special functional form .
therefore we carry out the further calculations with
t ( ) : = 123
where ( ) and possible asterisks have been omitted for clarity .
this leads to
c ( ) = + ,
t ( ) +t
) + ( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) y ( ( cid : 123 ) + ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) 123
( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 )
123 k ( ( cid : 123 ) b
example 123 ( polynomial loss functions ) .
we assume the general case of functions with - insensitive loss zone ( which may vanish , if = 123 ) and polynomial loss of degree p > 123
in ( 123 ) this type of cost function was used for pattern recognition .
this contains all l p loss functions as special cases ( = 123 ) , with p > 123 , which is treated in example 123
c ( ) = 123
from ( 123 ) , ( 123 ) , and ( 123 ) it follows that
p123 = + ,
pattern recognition , regression , approximation , and operator inversion
t ( ) = 123
as we want to nd the maximum of l in terms of the dual variables we get = 123 as t is the only term where appears and t becomes maximal for that value .
this yields
123 / ( p123 ) ( + ) p / ( p123 ) .
t ( ) =
123 / ( p123 ) p / ( p123 )
moreover , we have the following relation between and ;
= ( ) 123 / ( p123 ) .
example 123 ( piecewise polynomial and linear loss functions ) .
here we discuss cost functions with polynomial growth for ( 123 , ) with 123 and linear growth for ( , ) such that c ( ) is c 123 and convex .
a consequence of the linear growth for large is that the range of the lagrange multipliers becomes bounded , namely , by the derivative of c ( ) .
therefore we will have to solve box constrained optimization problems :
123 p p123
t ( ) = 123
for < ,
for < ,
+ = 123
by the same reasoning as above we nd that the optimal solution is obtained for = 123
furthermore , we can see through the convexity of c ( ) that < iff < 123 / .
hence we may easily substitute for 123 / in the case of > .
( 123 , 123 / ) is always true as 123
combining these ndings leads to a simplication of ( 123 ) :
for < ,
t ( ) = 123 / ( p123 )
analogously to example 123 we can determine the error for ( 123 , 123 / ) by
= ( ) 123 / ( p123 ) .
example 123 ( hard - constraints ) .
the simplest case to consider , however , are hard constraints , i . e . , the requirement that the approximation of the data is performed with at most deviation .
in this case dening a cost function does not make much sense in the lagrange framework and we may skip all terms containing ( ) i j .
this leads to a simplied optimization problem : 123 k ( ( cid : 123 ) ( cid : 123 ) )
( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) y ( ( cid : 123 ) + ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) 123
( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 )
( ai 123 ) (
) = 123 ,
smola and b .
scholkopf
another way to see this is to use the result of example 123 and take the limit 123
loosely speaking , the interval ( 123 , 123 / ) then converges to r+
appendix b .
proof of theorem 123
we modify the proof given in ( 123 ) to deal with the more general case stated in theorem 123
as rreg is convex for all 123 , minimization of rreg is equivalent to fullling the eulerlagrange equations .
thus a necessary and suf - cient condition for f h ( v ) to minimize rreg on h ( v ) is that the gateaux functional derivative ( 123 ) ( / f ) rreg ( f , ) vanish for all h ( v ) .
we get
rreg ( f , ) = lim
rreg ( f + k ) rreg ( f )
ci ( ( ai ( f + k ) ) ) ( x ) , x , y ) ci ( ( ai f ) ( x ) , x , y )
( ( cid : 123 ) p ( f + k ) ( cid : 123 ) 123d ( cid : 123 ) p f ( cid : 123 ) 123d )
expanding ( 123 ) in terms of k and taking the limit k 123 yields
rreg ( f , ) =
123ci ( ( ai f ) ( x ) , x , y ) ( ai ) ( x ) + ( p f p ) d .
equation ( 123 ) has to vanish for f = fopt .
as d is a hilbert space , we can dene the
( p f p ) d = ( p similarly , we rewrite the rst term of ( 123 ) to get
p f ) h ( v ) .
123ci ( ( ai f ) ( x ) , x , y ) ( x
ai ) h ( v ) + ( p
p f ) h ( v ) .
ai ) h ( v ) = ( a
dot product with .
as was arbitrary , this proves the theorem . 123
) h ( v ) , the whole expression ( 123 ) can be written as a
123 note that this can be generalized to the case of convex functions which need not be c 123
we next briey sketch the modications in the proof .
partial derivatives of ci now become subdifferentials , with the consequence that the equations only have to hold for some variables
i 123ci ( ( ai f ) ( x ) , x , y ) .
in this case , 123 denotes the subdifferential of a function , which consists of an interval rather than just a single number .
for the proof , we convolve the non - c 123 cost functions with a positive c 123 smoothing kernel which preserves convexity ( thereby rendering them c 123 ) , and take the limit to smoothing kernels with innitely small support .
convergence of the smoothed cost functions to the nonsmooth originals is exploited .
pattern recognition , regression , approximation , and operator inversion
we start with an initial set of training data x123 = ( x123 , .
, x123 ( cid : 123 ) 123
appendix c .
an algorithm for the virtual examples case .
we discuss an appli - cation of this algorithm to the problem of optical character recognition .
for the sake of simplicity we assume the case of a dichotomy problem , e . g . , having to distinguish between the digits 123 and 123 , combined with a regularization operator of the support vector type , i . e . , d = k .
) together with class labels y123 = ( y123 , .
, y123 ( cid : 123 ) 123 | y123i ( 123 , 123 ) ) .
additionally we know that the decision function should be invariant under small translations , rotations , changes of line thickness , radial scaling , and slanting or deslanting operations . 123 assume transformations ts associated with the aforementioned symmetries , together with condence levels cs 123 regarding whether tsx123i will still belong to class y123i .
as in example 123 , we use xs : = x123 , ( as f ) ( x ) : = f ( tsx ) , and t123 : = 123
as we are dealing with the case of pattern recognition , i . e . , we are only interested in sgn ( f ( x ) ) , not in f ( x ) itself , it is benecial to use a corresponding cost function , namely , the soft margin loss as described in ( 123 ) :
f ( x ) y 123 ,
c123 ( f ( x ) , x , y ) =
123 f ( x ) y
for the transformed data sets xs we dene cost functions cs : = csc123 ( i . e . , we are going to penalize errors on xs less than on x123 ) .
as the effective cost functions ( see ( 123 ) ) are 123 for an interval unbounded in one direction ( either ( , 123 ) or ( 123 , ) , depending on the class labels ) , half of the lagrange multipliers vanish .
therefore our setting can be simplied by using : = y instead of , i . e . ,
f ( x ) =
this allows us to eliminate the asterisks in the optimization problem , reading
y = 123 ,
( cid : 123 ) ( cid : 123 ) k ( cid : 123 )
y ( ai k ) ( x , x ) + b .
k : = k ( ti x , tj x ) y y .
the fact that less condence has been put on the transformed samples tsx123i leads to a decrease in the upper boundary cs / for the corresponding lagrange multipliers .
in this point our algorithm differs from the virtual support vector algorithm as proposed in ( 123 ) .
moreover , their algorithm proceeds in two stages by rst nding the support vectors and then training on a database generated only from the support vectors and their transforms .
if one was to tackle the quadratic programming problem with all variables at a time , the proposed algorithm would incur a substantial increase of computational complexity .
123 unfortunately no general rule can be given on the number or the extent of these transformations , as they depend heavily on the data at hand .
a database containing only a very few ( but very typical ) instances of a class may benet from a large number of additional virtual examples .
a large database instead possibly may already contain realizations of the invariances in an explicit manner .
smola and b .
scholkopf
however , only a small fraction of lagrange multipliers corresponding to data relevant for the classication problem will differ from zero ( e . g . , ( 123 ) ) .
therefore it is advantageous to minimize the target function only on subsets of the i , keeping the other variables xed ( see ( 123 ) ) , possibly starting with the original data set x123
