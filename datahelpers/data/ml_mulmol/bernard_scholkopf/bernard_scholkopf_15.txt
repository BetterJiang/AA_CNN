the support vector ( sv ) machine is a novel type of learning machine , based on statistical learning theory , which contains polynomial classi ( cid : 123 ) ers , neural networks , and radial basis function ( rbf ) networks as special cases .
in the rbf case , the sv algorithm automatically determines centers , weights and threshold such as to minimize an upper bound on the expected test error .
the present study is devoted to an experimental comparison of these machines with a classical approach , where the centers are determined by k ( means clustering and the weights are found using error backprop - agation .
we consider three machines , namely a classical rbf machine , an sv machine with gaussian kernel , and a hybrid system with the centers determined by the sv method and the weights trained by error backpropagation .
our results show that on the us postal service database of handwritten digits , the sv machine achieves the highest test accuracy , followed by the hybrid approach .
the sv approach is thus not only theoretically well ( founded , but also superior in a practical application .
copyright c ( cid : 123 ) massachusetts institute of technology ,
this report describes research done at the center for biological and computational learning , the arti ( cid : 123 ) cial intelligence laboratory of the massachusetts institute of technology , and at at&t bell laboratories ( now at&t research , and lucent technologies bell laboratories ) .
support for the center is provided in part by a grant from the national science foundation under contract asc ( .
bs thanks the m . i . t .
for hospitality during a three ( week visit in march , where this work was started .
at the time of the study , bs , cb , and vv were with at&t bell laboratories , nj; ks , fg , pn , and tp were with the massachusetts institute of technology .
ks is now with the department of information systems and computer science at the national university of singapore , lower kent ridge road , singapore ; cb and pn are with lucent technologies , bell laboratories , nj; vv is with at&t research , nj .
bs was supported by the studienstiftung des deutschen volkes; cb was supported by arpa under onr contract number n - - c - .
we thank a .
smola for useful discussions .
please direct correspondence to bernhard sch ( cid : 123 ) olkopf , bs@mpik - tueb . mpg . de , max ( planck ( institut f ( cid : 123 ) ur biologische kybernetik , spemannstr .
, t ( cid : 123 ) ubingen , germany .
figure : a simple ( dimensional classi ( cid : 123 ) cation prob - lem : ( cid : 123 ) nd a decision function separating balls from cir - cles .
the box , as in all following pictures , depicts the region ( ( cid : 123 ) ; ) .
consider fig .
suppose we want to construct a radial basis function classi ( cid : 123 ) er
d ( x ) = sgn
kx ( cid : 123 ) xik
( cid : 123 ) + b !
( b and ci being constants , the latter positive ) separating balls from circles , i . e .
taking di ( cid : 123 ) erent values on balls and circles .
how do we choose the centers xi ? two extreme cases are conceivable :
the ( cid : 123 ) rst approach consists in choosing the centers for the two classes separately , irrespective of the classi ( cid : 123 ) ca - tion task to be solved .
the classical technique of ( cid : 123 ) nding the centers by some clustering technique ( before tackling the classi ( cid : 123 ) cation problem ) is such an approach .
the weights wi are then usually found by either error back - propagation ( rumelhart , hinton , & williams , ) or the pseudo ( inverse method ( e . g .
poggio & girosi , ) .
an alternative approach ( fig .
) consists in choosing as centers points which are critical for the classi ( cid : 123 ) cation task at hand .
recently , the support vector algorithm was developed ( boser , guyon & vapnik , cortes & vapnik , vapnik ) which implements the lat - ter idea .
it is a general algorithm , based on guaranteed risk bounds of statistical learning theory , which in par - ticular allows the construction of radial basis function classi ( cid : 123 ) ers .
this is done by simply choosing a suitable kernel function for the sv machine ( see sec .
the sv training consists of a quadratic programming prob - lem which can be solved e ( cid : 123 ) ciently and for which we are guaranteed to ( cid : 123 ) nd a global extremum .
the algorithm automatically computes the number and location of the above centers , the weights wi , and the threshold b , in the following way : by the use of a suitable kernel func - tion ( in the present case , a gaussian one ) , the patterns are mapped nonlinearly into a high ( dimensional space .
there , an optimal separating hyperplane is constructed , expressed in terms of those examples which are closest
figure : rbf centers automatically found by the sup - port vector algorithm ( indicated by extra circles ) , using ci = for all i ( cf .
the number of sv centers ac - cidentally coincides with the number of identi ( cid : 123 ) able clus - ters ( indicated by crosses found by k ( means clustering with k = and k = for balls and circles , respectively ) but the naive correspondence between clusters and cen - ters is lost; indeed , of the sv centers are circles , and only of them are balls .
note that the sv centers are chosen with respect to the classi ( cid : 123 ) cation task to be solved .
to the decision boundary ( vapnik ) .
these are the support vectors which correspond to the centers in input
the goal of the present study is to compare real ( world results obtained with k ( means clustering and classical rbf training to those obtained with the centers , weights and threshold automatically chosen by the support vec - tor algorithm .
to this end , we decided to undertake a performance study combining expertise on the support vector algorithm ( at&t bell laboratories ) and classi - cal radial basis function networks ( massachusetts insti - tute of technology ) .
we report results obtained on a us postal service database of handwritten digits .
we have organized the material as follows .
next section , we describe the algorithms used to train the di ( cid : 123 ) erent types of rbf classi ( cid : 123 ) ers used in this paper .
following that , we present an experimental comparison of the approaches .
we conclude with a discussion of our
di ( cid : 123 ) erent ways of constructing a
radial basis function classi ( cid : 123 ) er
we describe three radial basis function systems , trained in di ( cid : 123 ) erent ways .
in sec .
. , we discuss the ( cid : 123 ) rst sys - tem trained along more classical lines .
in the follow - ing section ( . ) , we discuss the support vector algo - rithm , which constructs an rbf network whose param - eters ( centers , weights , threshold ) are automatically op - timized .
in sec .
. , ( cid : 123 ) nally , we use the support vector algorithm merely to choose the centers of the rbf net - work and then optimize the weights separately .
classical spherical gaussian rbfs :
we begin by ( cid : 123 ) rst describing the classical gaussian rbf system .
a d - dimensional spherical gaussian rbf net - work with k centers has the mathematical form
wigi ( ~ x ) + b
k ~ x ( cid : 123 ) ~ cik
where gi is the ith gaussian basis function with center ~ ci and variance ( cid : 123 ) i .
the weight coe ( cid : 123 ) cients wi combine the gaussian terms into a single output value and b is a bias term .
in general , building a gaussian rbf net - work for a given learning task involves ( ) determining the total number of gaussian basis functions to use for each output class and for the entire system , ( ) locating the gaussian basis function centers , ( ) computing the cluster variance for each gaussian basis function , and ( ) solving for the weight coe ( cid : 123 ) cients and bias in the summa - tion term .
one can implement a binary pattern classi ( cid : 123 ) er on input vectors ~ x as a gaussian rbf network by de ( cid : 123 ) n - ing an appropriate output threshold that separates the two pattern classes .
in this ( cid : 123 ) rst system , we implement each individual digit recognizer as a spherical gaussian rbf network , trained with a classical rbf algorithm .
given a spec - i ( cid : 123 ) ed number of gaussian basis functions for each digit class , the algorithm separately computes the gaussian centers and variances for each of the digit classes to form the systems rbf kernels .
the algorithm then solves for an optimal set of weight parameters between the rbf kernels and each output node to perform the desired digit recognition task .
the training process con - structs all digit recognizers in parallel so one can re - use the same gaussian basis functions among the digit recognizers .
to avoid over ( cid : 123 ) tting the available training data with an overly complex rbf classi ( cid : 123 ) er connected to every gaussian kernel , we use a \bootstrap " like oper - ation that selectively connects each recognizers output node to only a \relevant " subset of all basis functions .
the idea is similar to how we choose relevant \near - miss " clusters for each individual digit recognizer in the origi - nal system .
the training procedure proceeds as follows ( for further details , see sung , ) :
the ( cid : 123 ) rst training task is to determine an appro - priate number k of gaussian kernels for each digit class .
this information is needed to initialize our clustering procedure for computing gaussian rbf kernels .
we opted for using the same numbers of gaussian kernels as the ones automatically com - puted by the sv algorithm ( see table ) .
our next task is to actually compute the gaussian kernels for each digit class .
we do this by sepa - rately performing classical k ( means clustering ( see e . g .
lloyd , ) on each digit class in the us postal service ( usps ) training database .
each clustering operation returns a set of gaussian centroids and
their respective variances for the given digit class .
together , the gaussian clusters from all digit classes form the systems rbf kernels .
for each single - digit recognizer , we build an initial rbf network using only gaussian kernels from its target class , using error backpropagation to train the weights .
we then separately collect all the false positive mistakes each initial digit recognizer makes on the usps training database .
in the ( cid : 123 ) nal training step , we augment each initial digit recognizer with additional gaussian kernels from outside its target class to help reduce mis - classi ( cid : 123 ) cation errors .
we determine which gaus - sian kernels are \relevant " for each recognizer as follows : for each false positive mistake the initial recognizer makes during the previous step , we look up the misclassi ( cid : 123 ) ed patterns actual digit class and include the nearest gaussian kernel from its class in the \relevant " set .
the ( cid : 123 ) nal rbf network for each single - digit recognizer thus contains every gaussian kernel from its target class , and several \relevant " kernels from the other digit classes , trained by error backpropagation .
because our ( cid : 123 ) nal digit rec - ognizers have fewer weight parameters than a naive system that fully connects all recognizers to ev - ery gaussian kernel , we expect our system to gen - eralize better on new data .
the support vector machine
structural risk minimization .
for the case of two ( class pattern recognition , the task of learning from ex - amples can be formulated in the following way : given a set of functions
ff ( cid : 123 ) : ( cid : 123 ) ( cid : 123 ) g;
f ( cid : 123 ) : r
! f ( cid : 123 ) ; +g
and a set of examples
( x; y ) ; : : : ; ( x; y ) ; xi r
n ; yi f ( cid : 123 ) ; +g;
each one generated from an unknown probability distri - bution p ( x; y ) ; we want to ( cid : 123 ) nd a function f ( cid : 123 ) ( cid : 123 ) which provides the smallest possible value for the risk
r ( ( cid : 123 ) ) =z jf ( cid : 123 ) ( x ) ( cid : 123 ) yj dp ( x; y ) :
the problem is that r ( ( cid : 123 ) ) is unknown , since p ( x; y ) is unknown .
therefore an induction principle for risk min - imization is necessary .
the straightforward approach to minimize the empir -
jf ( cid : 123 ) ( xi ) ( cid : 123 ) yij
turns out not to guarantee a small actual risk ( i . e .
a small error on the training set does not imply a small error on a test set ) , if the number of training examples is limited .
to make the most out of a limited amount of data , novel statistical techniques have been developed during the last years .
the structural risk minimiza - tion principle ( vapnik , ) is based on the fact that
for the above learning problem , for any ( cid : 123 ) ( cid : 123 ) with a probability of at least ( cid : 123 ) ( cid : 123 ) , the bound
r ( ( cid : 123 ) ) ( cid : 123 ) remp ( ( cid : 123 ) ) + ( cid : 123 ) (
holds , ( cid : 123 ) being de ( cid : 123 ) ned as
) =s h ( cid : 123 ) log
h + ( cid : 123 ) ( cid : 123 ) log ( ( cid : 123 ) = )
the parameter h is called the vc ( dimension of a set of functions .
it describes the capacity of a set of functions implementable by the learning machine .
for binary clas - si ( cid : 123 ) cation , h is the maximal number of points k which can be separated into two classes in all possible k ways by using functions of the learning machine; i . e .
possible separation there exists a function which takes the value on one class and ( cid : 123 ) on the other class .
according to ( ) , given a ( cid : 123 ) xed number of train - ing examples one can control the risk by controlling two quantities : remp ( ( cid : 123 ) ) and h ( ff ( cid : 123 ) : ( cid : 123 ) ( cid : 123 ) g ) ; ( cid : 123 ) denoting some subset of the index set ( cid : 123 ) .
the empirical risk de - pends on the function chosen by the learning machine ( i . e .
on ( cid : 123 ) ) , and it can be controlled by picking the right ( cid : 123 ) .
the vc ( dimension h depends on the set of functions ff ( cid : 123 ) : ( cid : 123 ) ( cid : 123 ) g which the learning machine can imple - ment .
to control h , one introduces a structure of nested subsets sn : = ff ( cid : 123 ) : ( cid : 123 ) ( cid : 123 ) ng of ff ( cid : 123 ) : ( cid : 123 ) ( cid : 123 ) g ,
s ( cid : 123 ) s ( cid : 123 ) : : : ( cid : 123 ) sn ( cid : 123 ) : : : ;
with the corresponding vc ( dimensions satisfying
h ( cid : 123 ) h ( cid : 123 ) : : : ( cid : 123 ) hn ( cid : 123 ) : : :
for a given set of observations ( x; y ) ; : : : ; ( x; y ) the structural risk minimization principle chooses the func - in the subset ff ( cid : 123 ) : ( cid : 123 ) ( cid : 123 ) ng for which the guaranteed risk bound ( the right hand side of ( ) ) is
the remainder of this section follows sch ( cid : 123 ) olkopf , burges & vapnik ( ) in brie ( cid : 123 ) y reviewing the sup - port vector algorithm .
for details , the reader is referred to ( vapnik , ) .
a structure on the set of hyperplanes .
each par - ticular choice of a structure ( ) gives rise to a learning algorithm .
the support vector algorithm is based on a structure on the set of hyperplanes .
to describe it , ( cid : 123 ) rst note that given a dot product space z and a set of vectors x; : : : ; xr z; each hyperplane fx z : ( w ( cid : 123 ) x ) + b = g corresponds to a canonical pair ( w; b ) z ( cid : 123 ) r if we ad -
j ( w ( cid : 123 ) xi ) + bj = :
let bx ; : : : ;xr = fx z : kx ( cid : 123 ) ak < rg ( a z ) be the smallest ball containing the points x; : : : ; xr , and
fw;b = sgn ( ( w ( cid : 123 ) x ) + b )
w ( ( cid : 123 ) ) =
the decision function de ( cid : 123 ) ned on these points .
the pos - sibility of introducing a structure on the set of hyper - planes is based on the result ( vapnik , ) that the set ffw;b : kwk ( cid : 123 ) ag has a vc - dimension h satisfying
h ( cid : 123 ) ra :
dropping the condition kwk ( cid : 123 ) a leads to a set of functions whose vc ( dimension equals n + , where n is the dimensionality of z .
due to kwk ( cid : 123 ) a , we can get vc ( dimensions which are much smaller than n , en - abling us to work in very high dimensional spaces .
the support vector algorithm .
now suppose we want to ( cid : 123 ) nd a decision function fw;b with the property fw;b ( xi ) = yi; i = ; : : : ; : if this function exists , canon - icality ( ) implies
yi ( ( w ( cid : 123 ) xi ) + b ) ( cid : 123 ) ;
i = ; : : : ; :
in many practical applications , a separating hyperplane does not exist .
to allow for the possibility of examples violating ( ) , cortes & vapnik ( ) introduce slack
( cid : 123 ) i ( cid : 123 ) ;
i = ; : : : ; ;
yi ( ( w ( cid : 123 ) xi ) + b ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) i;
i = ; : : : ; :
the support vector approach to minimizing the guaran - teed risk bound ( ) consists in the following : minimize
( cid : 123 ) ( w; ( cid : 123 ) ) = ( w ( cid : 123 ) w ) + ( cid : 123 )
ing the second term of the bound ( ) .
the termp
subject to the constraints ( ) and ( ) .
according to ( ) , minimizing the ( cid : 123 ) rst term amounts to minimizing the vc ( dimension of the learning machine , thereby minimiz - on the other hand , is an upper bound on the number of misclassi ( cid : 123 ) cations on the training set | this controls the empirical risk term in ( ) .
for a suitable positive con - stant ( cid : 123 ) , this approach therefore constitutes a practical implementation of structural risk minimization on the given set of functions .
introducing lagrange multipliers ( cid : 123 ) i and using the kuhn ( tucker theorem of optimization theory , the solu - tion can be shown to have an expansion
with nonzero coe ( cid : 123 ) cients ( cid : 123 ) i only for the cases where the corresponding example ( xi; yi ) precisely meets the con - straint ( ) .
these xi are called support vectors .
all the remaining examples xi of the training set are irrele - vant : their constraint ( ) is satis ( cid : 123 ) ed automatically ( with ( cid : 123 ) i = ) , and they do not appear in the expansion ( ) .
the coe ( cid : 123 ) cients ( cid : 123 ) i are found by solving the following quadratic programming problem : maximize
yiyj ( cid : 123 ) i ( cid : 123 ) j ( xi ( cid : 123 ) xj )
( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) ( cid : 123 ) ;
i = ; : : : ; ; and
( cid : 123 ) iyi = :
figure : a simple two ( class classi ( cid : 123 ) cation problem as solved by the support vector algorithm ( ci = for all i; cf .
note that the rbf centers ( indicated by extra circles ) are closest to the decision boundary .
by linearity of the dot product , the decision function ( ) can thus be written as
f ( x ) = sgn
yi ( cid : 123 ) i ( cid : 123 ) ( x ( cid : 123 ) xi ) + b ! :
so far , we have described linear decision surfaces .
to allow for much more general decision surfaces , one can ( cid : 123 ) rst nonlinearly transform the input vectors into a high ( dimensional feature space by a map ( cid : 123 ) and then do a linear separation there .
maximizing ( ) then requires the computation of dot products ( ( cid : 123 ) ( x ) ( cid : 123 ) ( cid : 123 ) ( xi ) ) in a high ( dimensional space .
in some cases , these expensive calcu - lations can be reduced signi ( cid : 123 ) cantly by using a suitable function k such that
( ( cid : 123 ) ( x ) ( cid : 123 ) ( cid : 123 ) ( xi ) ) = k ( x; xi ) :
we thus get decision functions of the form
f ( x ) = sgn
yi ( cid : 123 ) i ( cid : 123 ) k ( x; xi ) + b ! :
in practise , we need not worry about conceiving the map ( cid : 123 ) .
we will choose a k which is the kernel of a posi - tive hilbert ( schmidt operator , and mercers theorem of functional analysis then tells us that k corresponds to a dot product in some other space ( see boser , guyon & vapnik , ) .
consequently , everything that has been said above about the linear case also applies to nonlinear cases obtained by using a suitable kernel k instead of the euclidean dot product .
we are now in a position to explain how the support vector algorithm can construct radial basis function classi ( cid : 123 ) ers : we simply use
k ( x; xi ) = exp ( cid : 123 ) ( cid : 123 ) kx ( cid : 123 ) xik
( see aizerman , braverman & rozonoer , ) .
other possible choices of k include
k ( x; xi ) = ( x ( cid : 123 ) xi ) d;
figure : two ( class classi ( cid : 123 ) cation problem solved by the support vector algorithm ( ci = for all i; cf
yielding polynomial classi ( cid : 123 ) ers ( d n ) , and
k ( x; xi ) = tanh ( ( cid : 123 ) ( cid : 123 ) ( x ( cid : 123 ) xi ) + ( cid : 123 ) )
for constructing neural networks .
interestingly , these di ( cid : 123 ) erent types of sv machines use largely the same support vectors; i . e .
most of the centers of an sv machine with gaussian kernel coincide with the weights of the polynomial and neural network sv classi ( cid : 123 ) ers ( sch ( cid : 123 ) olkopf , burges & vapnik ) .
to ( cid : 123 ) nd the decision function ( ) , we have to maxi -
w ( ( cid : 123 ) ) =
yiyj ( cid : 123 ) i ( cid : 123 ) jk ( xi; xj )
under the constraint ( ) .
to ( cid : 123 ) nd the threshold b , one takes into account that due to ( ) , for support vectors xj for which ( cid : 123 ) j = we have
yi ( cid : 123 ) i ( cid : 123 ) k ( xj ; xi ) + b = yj :
finally , we note that the support vector algorithm has been empirically shown to exhibit good generaliza - tion ability ( cortes & vapnik , ) .
this can be fur - ther improved by incorporating invariances of a problem at hand , as with the virtual support vector method of generating arti ( cid : 123 ) cial examples from the support vec - tors ( sch ( cid : 123 ) olkopf , burges , & vapnik , ) .
in addition , the decision rule ( ) , which requires the computation of dot products between the test example and all support vectors , can be sped up with the reduced set technique ( burges , ) .
these methods have led to substantial improvements for polynomial support vector machines ( burges & sch ( cid : 123 ) olkopf , ) , and they are directly appli - cable also to rbf support vector machines .
a hybrid system : sv centers only
the previous section discusses how one can train rbf like networks using the support vector algorithm
# of svs
# of pos
table : numbers of centers ( support vectors ) automatically extracted by the support vector machine .
the ( cid : 123 ) rst row gives the total number for each binary classi ( cid : 123 ) er , including both positive and negative examples; in the second row , we only counted the positive svs .
the latter number was used in the initialization of the k ( means algorithm , cf
sv centers only
table : two ( class - classi ( cid : 123 ) cation : numbers of test errors ( out of test patterns ) for the three systems described in sections
involves the choice of an appropriate kernel function k and solving the optimization problem in the form of eq .
the support vector algorithm thus automati - cally determines the centers ( which are the support vec - tors ) , the weights ( given by yi ( cid : 123 ) i ) , and the threshold b for the rbf machine .
to assess the relative in ( cid : 123 ) uence of the automatic sv center choice and the sv weight optimization , respec - tively , we built another rbf system , constructed with centers that are simply the support vectors arising from the sv optimization , and with the weights trained sep -
toy examples .
what are the support vectors ? they are elements of the data set that are \important " in sep - arating the two classes from each other .
in general , the support vectors with zero slack variables ( see eq .
) lie on the boundary of the decision surface , as they precisely satisfy the inequality ( ) in the high ( dimensional space .
figures and illustrate that for the used gaussian kernel this is also the case in input space .
this raises an interesting question from the point of view of interpreting the structure of trained rbf net - works .
the traditional view of rbf networks has been one where the centers were regarded as \templates " or stereotypical patterns .
it is this point of view that leads to the clustering heuristic for training rbf networks .
in contrast , the support vector machine posits an alter - nate point of view , with the centers being those examples which are critical for a given classi ( cid : 123 ) cation task .
us postal service database .
we used the usps database of handwritten digits ( for training , for testing ) , collected from mail envelopes in buf - falo ( cf .
lecun et al . , ) .
each digit is a ( cid : 123 ) vector with entries between ( cid : 123 ) and .
preprocessing consisted in smoothing with a gaussian kernel of width ( cid : 123 ) = : .
the support vector machine results reported in the following were obtained with ( cid : 123 ) = ( cf .
( ) ) and
c = : ( cid : 123 ) ( cid : 123 ) ( cf .
in all experiments , we used the support vector algorithm with standard quadratic programming techniques ( conjugate gradient descent ) .
two ( class classi ( cid : 123 ) cation .
table shows the numbers of support vectors , i . e .
rbf centers , extracted by the sv algorithm .
table gives the results of binary clas - si ( cid : 123 ) ers separating single digits from the rest , for the sys - tems described in sections . , . , and . .
ten ( class classi ( cid : 123 ) cation .
for each test pattern , the arbitration procedure in all three systems simply re - turns the digit class whose recognizer gives the strongest response .
table shows the - class digit recognition error rates for our original system and the two rbf -
the fully automatic support vector machine exhibits the highest test accuracy .
using the support vector algorithm to choose an appropriate number and corre - sponding centers for the rbf network is also better than the baseline procedure of choosing the centers by a clus - tering heuristic .
it can be seen that in contrast to the k ( means cluster centers , the centers chosen by the sup - port vector algorithm allow zero training error rates .
summary and discussion
the support vector algorithm provides a principled way of choosing the number and the locations of rbf cen - ters .
our experiments on a real ( world pattern recogni - tion problem have shown that compared to a correspond - ing number of centers chosen by k ( means , the centers chosen by the support vector algorithm allowed a train - ing error of zero , even if the weights were trained by classical rbf methods .
our interpretation of this ( cid : 123 ) nd - ing is that the support vector centers are speci ( cid : 123 ) cally
the sv machine is rather insensitive to di ( cid : 123 ) erent choices for all values in : ; : ; : : : ; : , the performance is
about the same ( in the area of % ( cid : 123 ) : % ) .
in the support vector case , we constructed ten two ( class classi ( cid : 123 ) ers , each trained to separate a given digit from the other nine , and combined them by doing the ten ( class clas - si ( cid : 123 ) cation according to the maximal output ( before applying the sgn function ) among the two ( class classi ( cid : 123 ) ers .
centers full s . v . m .
training ( patterns )
test ( patterns )
classi ( cid : 123 ) cation error rate
table : - class digit recognition error rates for three rbf classi ( cid : 123 ) ers constructed with di ( cid : 123 ) erent algo - rithms .
the ( cid : 123 ) rst system is a more classical one choosing its centers by a clustering heuristic .
the other two are the gaussian rbf - based systems we trained , one with the support vectors were chosen to be the centers and the second where the entire network was trained using the support vector algorithm .
( ) poggio , t . , & girosi , f .
networks for approx - imation and learning .
ieee , : ( .
( ) rumelhart , d .
e . , hinton , g .
e . , & williams , r .
learning representations by back ( propagating errors .
nature , : ( .
( ) sch ( cid : 123 ) olkopf , b . ; burges , c . ; and vapnik , v .
ex - tracting support data for a given task .
in : fayyad , u .
m . , and uthurusamy , r .
( eds . ) : proceedings , first international conference on knowledge dis - covery and data mining , aaai press , menlo park ,
( ) sch ( cid : 123 ) olkopf , b . , burges , c . j . c . , vapnik , v .
in - corporating invariances in support vector learning machines .
von der malsburg , w .
von seelen , j .
vorbr ( cid : 123 ) uggen , and b .
sendho ( cid : 123 ) , editors , arti ( cid : 123 ) - cial neural networks | icann , pages ( , berlin .
( springer lecture notes in computer sci - ence , vol
( ) sung , k .
learning and example selection for object and pattern detection .
thesis , mas - sachusetts institute of technology .
( ) vapnik , v .
estimation of dependences based on empirical data , ( in russian ) nauka , moscow; english translation : springer ( verlag , new york ,
( ) vapnik , v .
the nature of statistical learning
theory .
springer verlag , new york .
chosen for the classi ( cid : 123 ) cation task at hand , whereas k ( means does not care about picking those centers which will make a problem separable .
in addition , the sv centers yielded lower test error rates than k ( means .
it is interesting to note that using sv centers , while sticking to the classical procedure for training the weights , improved training and test error rates by approximately the same margin ( per cent ) .
in view of the guaranteed risk bound ( ) , this can be understood in the following way : the improvement in test error ( risk ) was solely due to the lower value of the training error ( empirical risk ) ; the con ( cid : 123 ) dence term ( the second term on the right hand side of ( ) ) , depend - ing on the vc ( dimension and thus on the norm of the weight vector ( eq .
) , did not change , as we stuck to the classical weight training procedure .
however , when we also trained the weights with the support vector algo - rithm , we minimized the norm of the weight vector ( see eq .
) and thus the con ( cid : 123 ) dence term , while still keeping the training error zero .
thus , consistent with ( ) , the support vector machine achieved the highest test accu - racy of the three systems .
