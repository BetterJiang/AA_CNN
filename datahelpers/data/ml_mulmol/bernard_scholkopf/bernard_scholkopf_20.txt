abstractthis paper collects some ideas targeted at advancing our understanding of the feature spaces associated with support vector ( sv ) kernel functions .
we rst discuss the geometry of feature space .
in particular , we review what is known about the shape of the image of input space under the feature space map , and how this inuences the capacity of sv methods .
following this , we describe how the metric governing the intrinsic geometry of the mapped surface can be computed in terms of the kernel , using the example of the class of inhomogeneous polynomial kernels , which are often used in sv pattern recognition .
we then discuss the connection between feature space and input space by dealing with the question of how one can , given some vector in feature space , nd a preimage ( exact or approximate ) in input space .
we describe algorithms to tackle this issue , and show their utility in two applications of kernel methods .
first , we use it to reduce the computational complexity of sv decision functions; second , we combine it with the kernel pca algorithm , thereby constructing a nonlinear statistical denoising technique which is shown to perform well on real - world data .
index terms denoising , kernel methods , pca , reduced set
method , sparse representation , support vector machines .
reproducing kernels are functions
forall pattern sets
give rise to positive matrices
is some compact set in which the data lives , typically in the support vector ( but not necessarily ) a subset of ( sv ) community , reproducing kernels are often referred to as mercer kernels ( section ii - b will show why ) .
they provide an elegant way of dealing with nonlinear algorithms by reducing them to linear ones in some feature space instead of a dot product in related to input space : using corresponds to mapping the data into a possibly high - by a ( usually nonlinear ) map
dimensional dot product space
manuscript received january 123 , 123; revised may 123 , 123
part of this work was done while p .
knirsch was with bell labs and b .
scholkopf and a .
smola were with the department of engineering , australian national university , canberra .
this work was supported by the arc and the dfg under grant ja 123 / 123 , 123 , 123
scholkopf was with gmd first , 123 berlin , germany .
he is now
with mocrosoft research ltd . , cambridge cb123 , u . k .
mika , k . - r .
muller , g .
ratsch , and a .
smola are with gmd first ,
123 berlin , germany .
burges is with bell laboratories , holmdel nj , usa .
knirsch is with the max - planck - institut fur biologische kybernetik ,
123 tubingen , germany .
publisher item identier s 123 - 123 ( 123 ) 123 - 123
, and taking the dot product there , i . e . , ( 123 )
a feature map asso - by virtue of this property , we shall call ciated with any linear algorithm which can be carried out in terms of dot products can be made nonlinear by substituting an a priori chosen kernel .
examples of such algorithms include the potential function method , sv machines , and kernel pca ( 123 ) , ( 123 ) ( 123 ) .
the price that one has to pay for this elegance , however , is that the solutions are only obtained as expansions in terms of input patterns mapped into feature space .
for instance , the normal vector of an sv hyperplane is expanded in terms of svs , just as the kernel pca feature extractors are expressed in terms of training examples
with some mapped test point
when evaluating an sv decision function or a kernel pca feature extractor , this is normally not a problem : due to ( 123 ) , ( 123 ) into a kernel expansion which can be evaluated even lives in an innite - dimensional space .
in some cases , however , there are reasons mandating a more comprehensive understanding of what exactly is the connection between patterns in input space and elements of feature space , given as expansions such as ( 123 ) .
this eld being far from understood , the current paper attempts to gather some ideas elucidating the problem , and simultaneously proposes some algorithms for situations where the above connection is important .
these are the problem of denoising by kernel pca , and the problem of speeding up sv decision functions .
the remainder of
is organized as follows .
section ii discusses different ways of understanding the mapping from input space into feature space .
following that , we briey review two feature space algorithms , sv machines and kernel pca ( section iii ) .
the focus of interest of the paper , the way back from feature space to input space , is described in section iv , and , in the more general form of constructing sparse approximations of feature space expansions , in section v .
the algorithms proposed in these two sections are experimentally evaluated in section vi and discussed in section vii .
from input space to feature space
in this section , we show how the feature spaces in question are dened by choice of a suitable kernel function .
insight into
sch olkopf et al . : input space versus feature space
the structure of feature space can then be gained by considering their relation to reproducing kernel hilbert spaces , by how they can be approximated by an empirical map , by their extrinsic geometry , which leads to useful new capacity results , and by their intrinsic geometry , which can be computed solely in terms of the kernel . 123
the mercer kernel map
we start by stating the version of mercers theorem given to be a nite measure space . 123 by
in ( 123 ) .
we assume almost all we mean except for sets of measure zero .
theorem 123 ( mercer ) : suppose
metric real - valued kernel123 such that
is a sym - the integral operator
representation theorem , for each
, call it
, such that
there exists a unique
to , and
is the function on
in contrast , we use product ) .
in view of this property ,
obtained by xing the second is the dot product of the rkhs .
to denote the canonical ( euclidean ) dot is called a reproducing
note that by ( 123 ) ,
identically zero .
hence the set of functions spans the whole rkhs .
the dot product on the rkhs thus and can then only needs to be dened on be extended to the whole rkhs by linearity and continuity .
from ( 123 ) , it follows that in particular
be the normalized eigenfunctions of
associated with the eigenvalues
, sorted in nonincreasing
holds for almost all in the latter case , the series converges absolutely and uniformly for almost
from statement 123 , it follows that
corresponds to a dot
for almost all
in fact , the uniform convergence of the series implies that such that even if the range can be approximated within
there exists an
, between images of
as a dot product in
the reproducing kernel map
we can also think of the feature space as a reproducing kernel hilbert space ( rkhs ) .
to see this ( 123 ) , ( 123 ) , ( 123 ) , ( 123 ) , ( 123 ) , recall that a rkhs is a hilbert space of functions such that all evaluation functionals , i . e . , the maps , are continuous .
in that case , by the riesz
123 those readers who are chiey interested in applications and algorithms
might want to consider skipping over the present section .
123 a nite measure space is a set x with a - algebra dened on it , and a measure dened on the latter , satisfying ( x ) < 123 ( i . e . , up to a scaling factor , is a probability measure ) .
a - algebra on x is a family of subsets of x ; which is closed under elementary set - theoretic operations ( countable unions , intersections , and complements ) , and which contains x as a member .
a measure is a function : ! + which is - additive , i . e . , the measure of a set which is the disjoint unions of some other sets equals the sum of the
123 i . e . , a function of two variables which gives rise to an integral operator .
( this implies that
this means that any reproducing kernel product in another space .
is symmetric ) .
note that corresponds to a dot
let us now consider a mercer kernel , i . e . , one which satises the condition of theorem 123 , and construct a dot product such containing the functions
becomes a reproducing kernel for the hilbert space
by linearity , we have
is a mercer kernel , the
chosen to be orthogonal with respect to the dot product in hence it is straightforward to construct a dot product
( using the kronecker symbol to the reproducing kernel property ( 123 ) ( using ( 123 ) ) .
, in which case ( 123 ) reduces
therefore , ( 123 ) provides us with a feature map
proposition 123 : for any mercer kernel
there exists a
such that for
the empirical kernel map
while giving an interesting alternative theoretical viewpoint , does not appear all that useful at rst sight .
in practice , it would seem pointless to rst map the inputs into functions , i . e . , into innite - dimensional objects .
however , for from ( 123 ) by a given dataset , it is possible to approximate only evaluating it on these points ( cf . , ( 123 ) , ( 123 ) , ( 123 ) , ( 123 ) ) .
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
denition 123 : for given patterns
, we call
the empirical kernel map with regard to .
the case where
example : consider rst
is a mercer i . e . , we evaluate on the training patterns .
if we carry out a linear algorithm in feature space , then everything will take place in the linear span of the mapped training patterns .
therefore , we can represent the information .
however , the dot product to use in that represen - , since the tation is not simply the canonical dot product in will usually not form an orthonormal system .
to turn into a feature map associated with , we need to endow with a dot product
of ( 123 ) as
to this end , we use the ansatz positive matrix . 123 enforcing ( 123 ) on the training patterns , this yields the self - consistency condition ( cf . , ( 123 ) , ( 123 ) )
here , we have used
to denote the
kernel gram matrix
the condition ( 123 ) can be satised for instance by the pseu -
equivalently , we could have incorporated this rescaling operation , which corresponds to a kernel pca whitening ( 123 ) , ( 123 ) , ( 123 ) , directly into the map , by modifying ( 123 ) to
this simply amounts to dividing the eigenvector basis vectors parallels the rescaling of the eigenfunctions of the integral operator belonging to the kernel , given by ( 123 ) .
are the eigenvalues of
for data sets where the number of examples is smaller than their dimensionality , it can actually be computationally explicitly rather than using kernels attractive to carry out in whatever subsequent algorithm ( svms , or kernel pca , say ) one wants to use .
as an aside , note that in the case of kernel pca ( to be described in section iii ) , one does not even need to worry about the whitening step : using the canonical dot product in which yields the same lead to diagonalizing eigenvectors with squared eigenvalues .
this was pointed out by ( 123 ) and ( 123 ) .
we end this section with two notes which illustrate why the use of ( 123 ) need not be restricted to the special case we just
more general kernels .
when using nonsymmetric kernels in ( 123 ) , together with the canonical dot product , one 123 note that every dot product can be written in this form .
moreover , we do not require deniteness of a , as the null space can be projected out , leading to a lower - dimensional feature space .
123 it is understood that if k is singular , we use the pseudoinverse of k 123=123 :
will effectively work with a matrix
, with general note that each positive semidenite matrix can be
if we wanted to carry out the whitening step , it would ( cf . , footnote 123 concerning
have to be using
different evaluation sets .
mika ( 123 ) has performed exper -
iments to speed up kernel pca by choosing as a proper subset of
now that we have described the kernel map in some detail , including variations on it , we shall next look at its properties .
specically , we study its effect on the capacity of kernel methods ( section ii - d ) and the induced geometry in feature space ( section ii - e ) .
the capacity of the kernel map
vapnik ( 123 ) , ( 123 ) gives a bound on the capacity , measured by the vc - dimension , of optimal margin classiers .
it takes
is an upper bound constraining the length of the weight vector of the hyperplane in canonical form , and radius of the smallest sphere containing the data in the space where the hyperplane is constructed .
the smaller this sphere is , the smaller is also the capacity , with benecial effects on the generalization error bounds .
if the data is distributed in a reasonably isotropic way , which is often the case in input space , then ( 123 ) can be fairly precise . 123 if , however , the distribution of the data is such that it does not ll the sphere , then ( 123 ) is wasteful .
the argument in the remainder of the section , which is summarized from ( 123 ) , shows that using a kernel typically entails that the data in fact lies in some box with rapidly decaying sidelengths , which can be much smaller than the above sphere .
from statement 123 of theorem 123 , there exists some constant
depending on the kernel
and almost all
parallelepiped in with side lengths
is essentially contained in an axis parallel
to see how this effectively restricts the class of functions we is done in terms are using , we rst note that everything in of dot products .
therefore , we can compensate any invertible by the corresponding linear transformation of the data in inverse adjoint transformation on the set of admissible weight , we have
, i . e . , for any invertible operator
hence , we may construct a diagonal scaling operator which inates the sides of the above parallelepiped as much as possible , while ensuring that it still lives in a sphere of the factor on the right hand side of ( 123 ) , but it buys us something
this will not change the
123 in terms of entropy numbers , this is due to the tightness of the rate given
by a famous theorem of maurey ( e . g . , ( 123 ) ) .
sch olkopf et al . : input space versus feature space
since everything is done in terms of dot products , scaling up the data by a can be compensated by scaling the weight vectors with a123 : by choosing a such that the data is still contained in a ball of the same radius r; we effectively reduce our function class ( parameterized by the weight vector ) , which leads to better generalization bounds which depend on the kernel inducing the map :
regarding the second factor : one can show that the function class essentially behaves as if it was nite - dimensional , with a cut - off determined by the rate of decay of the reasoning that leads to the improved bounds is some - what intricate and cannot presently be explained in full detail .
in a nutshell , the idea is to compute the capacity ( measured in terms of covering numbers of the sv function class , evaluated on an - sample ) via the entropy numbers of a suitable linear , consider rst the operator
for our purpose , the entropy numbers of are crucial .
these can be computed a sphere of radius as the entropy numbers of the operator factorization properties of entropy numbers , these can be upper bounded taking into account the above scaling operator in a rather precise way .
the faster the eigenvalues of the associated with a given kernel decay , the smaller the entropy numbers ( and hence the capacity ) of the corresponding feature space algorithm function class , and the stronger the generalization error bounds that one can prove .
as an example , we now consider how the entropy numbers depend asymptotically on the eigenvalues of
is a mercer kernel with
an example of such a kernel ( for
is the gaussian
this proposition allows the formulation of a priori gener - alization error bounds depending on the eigenvalues of the kernel .
using similar entropy number methods , possible to give rather precise data - dependent bounds in terms of the eigenvalues of the kernel gram matrix ( 123 ) .
123 consider two normed spaces e and f : for n 123 , the nth entropy number of a set m e is dened as n ( m ) : = inf f > 123 : there exists an - cover for m in e containing n or fewer pointsg : ( recall that the covering number n ( ; m ) , being essentially its functional inverse , measures how many balls of radius one needs to cover m : ) the entropy numbers n ( t ) of an operator t : e ! f are dened as the entropy numbers of the image of the unit ball under t : note that 123 ( t ) = jjt jj; intuitively , the higher entropy number allow a ner characterization of the complexity of the image of t ( e . g . , ( 123 ) , ( 123 ) ) .
note that entropy numbers have some nice properties that covering numbers are lacking .
for instance , scaling a subset of a normed vector space by some factor simply scales its entropy numbers by the same factor .
entropy numbers are a promising tool for studying the capacity of feature space methods .
this is due to the fact that in the linear case , which is what we are interested in for feature space algorithms , they can be studied using powerful methods of functional analysis ( e . g . , ( 123 ) ) .
the metric of the kernel map
another way to gain insight into the structure of feature space is to consider the intrinsic shape of the manifold to which ones data is mapped .
it is important here to distinguish and the surface in that space to between the feature space actually map , which we will which points in input space sufciently smooth that structures such as a riemannian metric can be dened on it .
here we will follow the analysis of ( 123 ) , to which the reader is referred for more details , although the application to the class of inhomogeneous polynomial kernels
for simplicity , we assume here that
will be an
we rst note that all intrinsic geometrical properties of
can be derived once we know the riemannian metric induced the riemannian metric can be by the embedding of dened by a symmetric metric tensor do not need to know the explicit mapping it can be written solely in terms of the kernel .
to see this consider the line element
to input space
correspond to the vector space , we have
represent a small but nite
thus we can read off the components of the metric tensor
for the class of kernels which are functions only of dot prod - ucts between points in both covariant and contravariant
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
components of the metric take a simple form
where the prime denotes derivative with respect to the argu - to illustrate , let us compute the intrinsic geomet - corresponding to the class of rical properties of the surface inhomogeneous polynomial mercer kernels
is a constant .
properties relating to the intrinsic curvature of a surface are completely captured by the riemann
where the christoffel symbols of the second kind are dened
thus we nd that for this class of kernels , the riemann cur -
vature , for arbitrary input space dimension
, is given by
one can expect that the density on the surface will become ill behaved for data whose norm is small , for homogeneous polynomial kernels , but not for the inhomogeneous case .
similar problems may arise in the pattern recognition case if ones data lies near the singularity .
these considerations can be extended to compute the intrinsic volume element on on , and to give some simple necessary tests that must be satised by any kernel if it is to be a mercer kernel : the reader is referred to ( 123 ) for details .
, which may be used to compute the density
feature space algorithms
we next describe the two algorithms used in this paper : sv machines , and kernel pca .
the sv algorithm shall not be described in detail , let us only briey x the notation .
sv classiers ( 123 ) construct a maximum margin hyperplane in input space , this corresponds to a nonlinear decision
boundary of the form
are the training examples .
those with
, which are called svs; in many applications , most of the found by solving a quadratic program , turn out to be zero .
excellent classication accuracies in both ocr and object recognition have been obtained using sv machines ( 123 ) , ( 123 ) .
a generalization to the case of regression estimation , leading to similar function expansion , exists ( 123 ) , ( 123 ) .
kernel principal component analysis ( 123 ) carries out a the extracted features take
linear pca in the feature space the nonlinear form
it is interesting to compare this result with that for the
where , up to a normalization , the th eigenvector of the matrix
are the components of
this can be understood as follows .
we wish to nd eigen -
of the covariance matrix
the feature space , 123 where
this analysis shows that adding the constant
kernel results in striking differences in the geometries .
both curvatures vanish for
, as expected .
however for
vanishes for all powers whereas
does not vanish for any furthermore , all surfaces for homogeneous kernels with nonvanishing curvature ( i . e . , those none of the corresponding surfaces for inhomogeneous kernels
have a singularity at
thus beyond providing insight into the geometrical structure generated by choice of kernel , the geomet - of the surfaces in rical analysis also gives more concrete results .
for example , 123 note that if c < 123 the kernel is not a mercer kernel and the above analysis
does not apply .
is very high dimensional this will be in the case when impossible to compute directly .
to be still able to solve this problem , one uses mercer kernels .
to this end , we need to in dot products .
we derive a formulation which only uses then replace any occurrence of way we avoid dealing with the mapped data explicitly , which may be intractable in terms of memory and computational cost .
to nd a formulation for pca which uses only dot prod - ucts , we rst substitute the covariance matrix ( 123 ) into the note that all solutions to this
123 here we assume that the mapped data is centered , too .
in general this will not be true , but all computations can easily be reformulated to perform an explicit centering in f ( 123 ) .
sch olkopf et al . : input space versus feature space
training data .
thus , we may consider the equivalent system
must lie in the span of
- images of the
and expand the solution
substituting ( 123 ) and ( 123 ) into ( 123 ) , and dening a
we arrive at a problem which is cast
in terms of dot products : solve
normalizing the solution
( for details on the last step see ( 123 ) ) .
to extract features , we compute the projection of the
onto the th eigenvector in the feature
image of a test point
usually , this will be much cheaper than taking the dot product in the feature space explicitly .
to conclude the brief summary of kernel pca , we state a characterization which involves the same regularizer ( the length of the weight vector ) as the one used in sv machines .
, the th kernel pca feature extrac - , is optimal among all feature extractors of
proposition 123 : for all
tor , scaled by
the preimage problem .
not each point in the span of the mapped input data is necessarily the image of some input pattern .
therefore , not each point that can be written as an expansion in terms of mapped input patterns ( e . g . , a kernel pca eigenvector , or a svm hyperplane normal vector ) , can necessarily be expressed as the image of a single input pattern .
from feature space to input space
unlike section ii , which described how to get from input space into feature space , we now study the way back .
there has been a fair amount of work on aspects of this problem in the context of developing so - called reduced set methods ( e . g . , ( 123 ) , ( 123 ) , ( 123 ) , ( 123 ) , ( 123 ) ) .
for pedagogical reasons , we shall postpone reduced set methods to section v , as they focus on a problem that is already more complex than the one we would like to start with .
the preimage problem
as stated in the introduction , feature space algorithms express their solutions as expansions in terms of mapped input into the feature space points ( 123 ) .
however , since the map is nonlinear , we cannot generally assert that each such
expansion will have a preimage under be easy to compute , as shown by the following result ( 123 ) .
if the preimage existed , it would
, i . e . , a point
proposition 123 : consider a feature space expansion
if there exists a
in the sense that it has the shortest weight vector
is an invertible function
, then we can
subject to the conditions that
123 ) it is orthogonal to the rst extractors ( in feature space )
123 ) applied to the training set
variance set of outputs .
kernel pca feature
it leads to a unit
both sv machines and kernel pca utilize mercer kernels to generalize a linear algorithm to a nonlinear setting; moreover , both use the same regularizer , albeit in different domains of learningsupervised versus unsupervised .
nevertheless , fea - ture extraction experiments on handwritten digit images using kernel pca have shown that a linear hyperplane classier trained on the extracted features can perform as well as a nonlinear sv machine trained directly on the inputs ( 123 ) .
is any orthonormal basis of input space .
proof : we expand
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
several remarks on this proposition should be given .
first ,
examples of kernels which are invertible functions of are polynomial kernels
preimage .
as we shall see in experiments , however , even better preimages can be found , which makes some interesting applications possible ( 123 ) , ( 123 ) :
and sigmoid kernels
a similar result holds for rbf kernels ( using the polarization identity ) all we need is a kernel which allows the reconstruc - from , evaluated on some input points which we are allowed to choose ( for details , cf . , ( 123 ) ) .
the crucial assumption , clearly ,
is the existence of the preimage .
unfortunately , there are many situations where there are no preimages .
to illustrate this , we consider the feature ( 123 ) .
clearly , only points in feature space which can do have a preimage under this map .
to be written as characterize this set of points in a specic example , consider
, map it into
denoising : given a noisy higher components to obtain data set is captured in the rst components mainly pick up the noisein this sense , thought of as a denoised version of
, and then compute a here , the hope is that the main structure in the directions , and the remaining
compression : given the eigenvectors
and a small num - , compute a this is useful
( cf . , ( 123 ) ) of
ber of features preimage as an approximate reconstruction of
is smaller than the dimensionality of the input data .
interpretation : visualize a nonlinear feature extractor
, but not
by computing a preimage .
in this paper , we focus on the rst point .
in the next section , we shall develop a method for minimizing ( 123 ) , which we will later , in the experimental section , apply to the case where
an algorithm for approximate preimages
maps each input into a gaussian sitting on in this case , that point .
however , it is known ( 123 ) that no gaussian can be written as a linear combination of gaussians centered at other points .
therefore , in the gaussian case , none of the expansions ( 123 ) , excluding trivial cases with only one term , has an exact
the problem that we had initially set out to solve has turned out to be insolvable in the general case .
let us try to ask for less .
rather than trying to nd exact preimages , we now consider approximate ones .
we call
are there vectors
for which good approximate preimages exist ? as described in section iii , kernel pca is nothing but pca in
the present section ( 123 ) gives an analysis for the case of the gaussian kernel , which has proven to perform very well in applications ( 123 ) , and proposes an iteration procedure for computing preimages of kernel expansions .
we start by considering a problem slightly more general than the preimage problem : we are seeking to approximate first observe that rather
we can minimize the distance between
and the orthogonal
to this end , we maximize
with the following optimal approximation property ( e . g . , ( 123 ) ) .
are sorted according to nonin - here , we assume that the being the smallest nonzero
is the - dimensional projection minimiz -
imate preimage : trivially , already
can be expected to have a good approx - is a good approximate
123 just how small it needs to be in order to form a satisfactory approximation will depend on the problem at hand .
therefore , we have refrained from giving a formal denition .
is preferable to the one of ( 123 ) over
which can be expressed in terms of the kernel .
the maximiza - tion of ( 123 ) over , since it comprises a lower - dimensional problem , and since have different scaling behavior .
once the maximum of ( 123 ) is found , it is extended to the minimum of ( 123 ) by setting ( cf .
( 123 ) ) ( 123 ) can either be minimized using standard techniques ( as in ( 123 ) ) , or , for particular choices of kernels , using xed - point iteration methods , as shown presently .
for kernels which satisfy
gaussian kernels ) , ( 123 ) reduces to
sch olkopf et al . : input space versus feature space
the difference between two densities ( modulo normalization
to see this , we dene the sets
and the shorthands
given a vector 123 f , we try to approximate it by a multiple of a vector ( zzz ) in the image of input space ( n ) under the nonlinear map by nding zzz such that the projection distance of onto span ( ( zzz ) ) is
i . e . , we are trying to nd a where the difference between the ( unnormalized ) probabilities for the two classes is maximal , and estimate the approximation to ( 123 ) by a gaussian centered at moreover , note that we can rewrite ( 123 ) as
for the extremum , we have
, we substitute ( 123 ) to get
the sufcient condition
to evaluate the gradient
in terms of
, we obtain
, leading to
for the gaussian kernel we thus arrive at
reduced set ( rs ) methods
the problem
we now move on to a slightly more general problem , rst studied by ( 123 ) , where we are no longer only looking for single preimages , but expansions .
it will turn out that one can also use the method developed in the last section to design an algorithm for the more general case .
assume we are given a vector
, expanded in images
of input patterns
and devise an iteration
rather than looking for a single preimage , we now try to approximate it by a reduced set
and thus is nonzero in a the denominator equals neighborhood of the extremum of ( 123 ) , unless the extremum itself is zero .
the latter only occurs if the projection of is zero , in which case it is pointless the linear span of numerical instabilities related to to try to approximate being small can thus be approached by restarting
the iteration with different starting values .
interestingly , ( 123 ) can be interpreted in the context of clustering ( e . g . , ( 123 ) ) .
it determines the center of a single gaussian cluster , trying to capture as many of the as possible , and simultaneously avoids those the sign of the equals the label of the pattern is this sign which distinguishes ( 123 ) from plain clustering or parametric density estimation .
the occurrence of negative signs is related to the fact that we are not trying to estimate a parametric density but
for sv classiers ,
to this end , one can
the crucial point is that even if can be computed ( and minimized ) in terms of the kernel .
is not given explicitly , ( 123 )
in the nist benchmark of 123 123 handwritten digits , sv machines are more accurate than any other single classier ( 123 ) ; however , they are inferior to neural nets in run - time classication speed ( 123 ) .
in applications where the latter is an
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
issue , it is thus desirable to come up with methods to speed up things by making the sv expansion more sparse , i . e . , replacing ( 123 ) by ( 123 ) .
finding the coefcients
evidently , the rs problem consists of two parts .
one has , and one has to compute the to determine the rs vectors expansion coefcients we start with the latter; partly , as it is easier , partly , as it is common to different rs methods .
proposition 123 : the optimal coefcients
in the 123 - norm are
the gram matrix can either be computed only from those examples which have a nonzero , or from a larger set which we want to use for the expansion . ) interestingly , it will turn out that this problem is closely related to kernel pca .
let us start with the simplest case .
assume there exists an
using ( 123 ) , this reads
with eigenvalue 123 , i . e . ,
note that if the
are linearly independent , as they should be if we want to use them for approximation , then has full rank .
otherwise , one can use the pseudoinverse , or select the solution which has the largest number of zero
proof : see ( 123 ) .
we evaluate the derivative of the dis -
and set it to zero .
substituting
, we obtain
no rs algorithm using the 123 - norm optimality criterion can circumvent this result .
for instance , suppose we are given an algorithm that computes the and comes up with a solution .
then we can always use the proposition to recompute the optimal coefcients to get a solution which is at least as good as the original one .
different algorithms can , however , differ in the way they determine in the rst place .
the one dealt with in the , while the one in next section simply selects subsets of the section v - d uses vectors different from the original
reduced set selection
123 ) selection via kernel pca : the idea for the rst algo - the null space of rithm arises from the observation that precisely tells us the gram matrix how many vectors can be removed from an expansion while committing zero approximation error ( assuming we correctly adjust the coefcients ) , i . e . , how sparse we can make an sv expansion , say , without changing it the least ( 123 ) , ( 123 ) .
( here ,
are linearly dependent , and there - this means that the fore any of the be expressed in terms of the others .
hence , we may use the eigenvectors with eigenvalue zero to eliminate certain terms from any expansion in the
which comes with a nonzero
what happens if we do not have nonzero eigenvalues , such as in the case of gaussian kernels ( 123 ) ? intuitively , we would still believe that even though the above is no longer precisely true , it should give a good approximation .
however , the crucial difference is that in order to get the best possible approximation , we now need to take into account the coefcients of the expansion , say , then this error will commit an error by removing
how do we then select the optimal
clearly , we would like to nd coefcients minimizing the
error we commit by replacing
to establish a connection to kernel pca , we make a change of variables .
first , dene hence ( 123 ) equals
problem of minimizing
, this leads to the
a straightforward calculation shows that we can recover the , i . e . , the values to approximation coefcients for add to the
is invariant when rescaling
for leaving out
rather than minimizing the nonlinear function ( 123 ) , we now devise a computationally attractive approximate solution .
it is alone is minimized motivated by the observation that for the eigenvector with minimal eigenvalue , consistent with the special case discussed above ( cf . , ( 123 ) ) .
in that case ,
123 the idea of approximating the support vector expansion by optimally removing individual support vectors , and then adjusting the coefcients of those that remain to minimize the resulting error , was arrived at independently by olivier chapelle , who also derived the expression to be minimized , ( 123 )
sch olkopf et al . : input space versus feature space
, with eigenvalue
more generally , if
is any normalized
to dispose of the modulus , we rewrite
the quadratic programming problem
in terms of the new variables , we end up with
this can be minimized in kernel pca and scanning through the matrix complexity can be reduced to we can eliminate
operations by performing by only considering the chosen a priori .
hence , chosen in a principled yet
setting all computational considerations aside , the optimal greedy solution to the above selection problem , equivalent to ( 123 ) , can also be obtained by using proposition 123 : compute the optimal solution for all possible patterns that one could leave out ( i . e . , use subsets of
and evaluate ( 123 ) in each case .
the same applies to subsets of any size .
if we have the
resources to exhaustively scan through all subsets of size
, then proposition 123 provides the optimal way of selecting the best expansion of a given size .
better expansions can only be obtained if we drop the restriction that the approximation is written in terms of the original patterns , as done in section v - d .
no matter how we end up choosing , we approximate
this problem can be solved with standard quadratic pro - gramming tools .
the solution ( 123 ) could be used directly as expansion coefcients .
for optimal precision , however , we merely use it to select which patterns to use for the expansion ( those with nonzero coefcients ) , and recompute the optimal coefcients using proposition 123
123 ) the multiclass case : in many applications , we face the problem of simultaneously approximating a set of space expansions .
for instance , in digit classication , a com - binary recognizers , one for mon approach is to train each digit .
to this end , the quadratic programming formulation of section v - c . 123 can be modied to
the whole scheme can be iterated until the expansion of
is sufciently sparse .
if one wants to avoid having to nd the at each step anew , then approximate smallest eigenvalue of schemes using heuristics can be conceived .
in our experiments to be reported below , we did compute all eigenvectors at each step , using the gram matrix computed from the svs and then selected
according to ( 123 ) .
123 ) selection via
method for enforcing sparseness which is inspired by shrinkage penalizers ( cf . , ( 123 ) ) .
given some expansion
, we approximate it by
is a constant determining the tradeoff between sparseness and quality of approximation .
the con - can be set to one or is the mean of say .
in the latter case , we are hoping for a sparser decomposition , since more emphasis is put on shrinking terms which are already small .
this reects the intuition that it is less promising to try to shrink very large terms .
ideally , we would like to count the number of nonzero coefcients , rather than sum their moduli; however , the former does not lead to an efciently solvable optimization problem .
the indexes and are understood to range over all svs ( i . e . , expansion vectors which have a nonzero coefcient for at least one of the recognizers ) , and over the , we use either one or , with the same rationale as above ,
is the mean of all
together with the constraint ( 123 ) ensures that we only penalize the largest of the coefcients pertaining to each individual sv .
therefore , as soon as one coefcient is being used by one the expansions , it does not cost anything to also use it for another one .
this is precisely what we want for speed - up purposes : if we have to compute a certain dot product anyway , we might just as well reuse it for the other
, we arrive at the
ranges over all svs .
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
123 ) a note on the utility of reduced set selection : why should we expect these procedures to be useful ? first , for sv for some positive value of the , there is reason to believe that the sv expansion can be made sparser by removing the constraint ( note that ( 123 ) does not care about the constraint ) . 123 second , the number of eigenvalues which are zero ( and thus the number of patterns that can be removed without loss ) , or at least small , depends on the problem at hand and the kernel used .
for instance , gaussian kernel gram matrices do not have zero eigenvalues unless some of the patterns are duplicates ( 123 ) .
nevertheless , good approximations are possible , since the eigenvalues of gaussian kernels decay rapidly ( e . g . , ( 123 ) ) .
reduced set construction
so far , we have dealt with the problem of how to select a reduced set of expansion vectors from the original one .
we now return to the originally posed problem , which includes the construction of new vectors to reach high reduction rates .
to this end , suppose we want to approximate a vector
by an expansion of the type ( 123 ) with we iterate the procedure of section iv - b by , obtained by needs to be utilized in its
iterating ( 123 ) .
to apply ( 123 ) , representation in terms of mapped input images
to this end ,
i . e . , we need to set
could be computed as
if the vectors
, then the best approximation of
span is not obtained by computing orthogonal projections onto each direction .
instead , we need to compute the optimal anew in each step , using has not yet reached proposition 123 ( if the discrepancy will be invertible ) .
the iteration is stopped after
steps , either specied
in advance , or by monitoring when
falls below a specied threshold .
the solution
vector takes the form ( 123 ) .
we conclude this section by noting that in many cases , such as multiclass sv machines , or multiple kernel pca feature extractors , we may actually want to approximate several vectors simultaneously .
this leads to more complex equations , given in ( 123 ) .
123 imagine a case where a certain pattern appears twice in the training set , and the sv expansion has to utilize both of the copies only because the upper bound constraint limits the coefcient of each of them to c :
kernel pca toy example ( see text ) : lines of constant feature value for the rst eight nonlinear principal components extracted with k ( xxx; yyy ) = exp ( jjxxx yyyjj123=123 : 123 ) : the rst two principal components ( top middle / right ) separate the three clusters .
components 123 split the clusters .
components 123 split them again , orthogonal to the above splits .
to see how the proposed methods work in practice we ran several toy and real - world experiments .
in section vi - a , we give denoising results for the approach of nding approxi - mate preimages presented in section iv - a .
in section vi - b , we present some experiments for the reduced set methods described in sections v - c and v - d .
kernel pca denoising
123 ) toy examples : all experiments reported were carried out with gaussian kernels ( 123 ) , minimizing ( 123 ) with the iteration scheme given by ( 123 ) .
however , similar results were obtained with polynomial kernels .
matlab code for computing kernel pca is available on the web from
we generated an articial data set from three point sources at ( 123 , 123 ) , ( 123 , 123 ) , ( 123 , 123 ) ( 123 points each ) with ) , and performed kernel pca on gaussian noise ( it ( fig .
using the resulting eigenvectors , we extracted nonlinear principal components from a set of test points generated from the same model , and reconstructed the points from varying numbers of principal components .
123 shows that discarding higher order components leads to removal of the noisethe points move toward their respective sources .
in a second experiment ( table i ) , we generated a data set
with zero mean and variance
from 123 gaussians in in each component , by selecting from each source 123 points as a training set and 123 points for a test set ( centers of the gaussians randomly chosen in ( 123 , 123 ) then we applied kernel pca to the training set and computed the projections of the points in the test set .
with these , we carried out denoising , yielding an approximate preimage in test point .
this procedure was repeated for different numbers of components in reconstruction , and for different values of ( also used in the kernel ) .
we compared the results provided by our algorithm to those of linear pca via the mean squared distance of all denoised test points to their corresponding center .
table i shows the ratio of these values; here and below , ratios larger than one indicate that kernel pca performed better than linear pca .
for almost every choice of
sch olkopf et al . : input space versus feature space
denoising gaussians in 123 ( see text ) .
performance ratios larger than one indicate how much better kernel pca did , compared to linear pca , for different choices of the gaussians standard deviation ;
and different numbers of components used in reconstruction
kernel pca denoising by reconstruction from projections onto the eigenvectors of fig .
we generated 123 new points from each gaussian , represented them in feature space by their rst n = 123; 123; ; 123 nonlinear principal components , and computed approximate preimages , shown in the upper nine pictures ( top left : original data , top middle : n = 123 , top right : n = 123 , etc . ) .
note that by discarding higher order principal components ( i . e . , using a small n ) , we remove the noise inherent in the nonzero variance the gaussians .
the lower nine pictures show how the original points move in the denoising .
unlike the corresponding case in linear pca , where we obtain lines ( see fig .
123 ) , in kernel pca clusters shrink to points .
reconstructions and point movements for linear pca , based on the rst principal component .
, kernel pca did better .
note that using all ten components , linear pca is just a basis transformation and hence cannot denoise .
the extreme superiority of kernel pca for small is due to the fact that all test points are in this case located close to the 123 spots in input space , and linear pca has to cover them with less than 123 directions .
kernel pca moves each point to the correct source even when using only a small number of components .
to get some intuitive understanding in a low - dimensional case , fig .
123 depicts the results of denoising a half circle and a square in the plane , using kernel pca , a nonlinear autoencoder , principal curves , and linear pca .
the principal curves algorithm ( 123 ) iteratively estimates a curve captur - ing the structure of the data .
the data are projected to the closest point on a curve which the algorithm tries to construct such that each point is the average of all data points projecting onto it .
it can be shown that
denoising in 123 - d ( see text ) .
depicted are the data set ( small points ) and its denoised version ( big points , joining up to solid lines ) .
for linear pca , we used one component for reconstruction , as using two components , reconstruction is perfect and thus does not denoise .
note that all algorithms except for our approach have problems in capturing the circular structure in the bottom example ( taken from ( 123 ) ) .
straight lines satisfying the latter are principal components , so principal curves are a generalization of the latter .
the algorithm uses a smoothing parameter which is annealed during the iteration .
in the nonlinear autoencoder algorithm , a bottleneck ve - layer network is trained to reproduce the input values as outputs ( i . e . , it is used in autoassociative mode ) .
the hidden unit activations in the third layer form a lower - dimensional representation of the data , closely related to pca ( see , for instance , ( 123 ) ) .
training is done by conjugate gradient descent .
in all algorithms , parameter values were selected such that the best possible denoising result was obtained .
the gure shows that on the closed square problem , kernel pca does ( subjectively ) best , followed by principal curves and the nonlinear autoencoder; linear pca fails completely .
however , note that all algorithms except for kernel pca actually provide an explicit one - dimensional parameterization of the data , whereas kernel pca only provides us with a means of mapping points to their denoised versions ( in this case , we used four kernel pca features , and hence obtain a
123 ) handwritten digit denoising : to test our approach on real - world data , we also applied the algorithm to the usps database of handwritten digits ( e . g . , ( 123 ) , ( 123 ) ) of 123 training 123 ) .
for each of the patterns and 123 test patterns ( size 123 ten digits , we randomly chose 123 examples from the training set and 123 examples from the test set .
we used the method of section iv - b , with the width 123 equals twice the average of the datas variance
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
visualization of eigenvectors ( see text ) .
depicted are the 123 row : different visualizations for kernel pca .
123th eigenvector ( from left to right ) .
first row : linear pca , second and third
in each dimension .
in fig .
123 , we give two possible depictions of the eigenvectors found by kernel pca , compared to those found by linear pca for the usps set .
the second row shows the approximate preimages of the eigenvectors
) , lower right
, found by our iterative algorithm .
in the third row is the projection each image is computed as follows : pixel - image of the th canonical basis vector in input space onto the corresponding eigenvector in feature space ( upper left ) .
in the linear case , both methods would simply yield the eigenvectors of linear pca depicted in the rst row; in this sense , they may be considered as generalized eigenvectors in input space .
we see that the rst eigenvectors are almost identical ( except for arbitrary signs ) .
however , we also see that eigenvectors in linear pca start to focus on high - frequency structures already at smaller eigenvalue size .
to understand this , note that in linear pca we only have a maximum number of 123 eigenvectors , contrary to kernel pca which gives us the number of training examples ( here 123 ) possible eigenvectors .
this also explains some of the results we found when work - ing with the usps set ( figs .
123 and 123 ) .
in these experiments , linear and kernel pca were trained with the original data .
to the test set , we added
123 ) additive gaussian noise with zero mean and standard
123 ) speckle noise , where each pixel is ipped to black or
white with probability
for the noisy test sets , we computed the projections onto linear and nonlinear components , and carried out reconstruction for each case .
the results were compared by taking the mean squared distance of each reconstructed digit of the noisy test set to its original counterpart .
for the optimal number of components in linear and kernel pca , our approach did better by a factor of 123 for the gaussian noise , and 123 for the speckle noise ( the optimal number of components were 123 in linear pca , and 123 and 123 in kernel pca , respectively ) .
taking identical numbers of components in both algorithms , kernel pca becomes up to eight times better than linear pca .
however , note that kernel pca comes with a higher computational complexity .
speeding up support vector decision rules
as in section vi - a , we used the usps handwritten digit database .
we approximated the sv expansions ( 123 ) of ten binary classiers , each trained to separate one digit from the
denoising of usps data ( see text ) .
the left half shows : top : the rst occurrence of each digit in the test set , second row : the upper digit with additive gaussian noise ( = 123 : 123 ) , following ve rows : the reconstruction for linear pca using n = 123 , 123 , 123 , 123 , 123 components , and , last ve rows : the results of our approach using the same number of components .
in the right half we show the same but for speckle noise with probability p = 123 : 123 :
we used the gaussian kernel
, and the approximation techniques described
in sections v - c and v - d .
the original sv system had on average 123 svs per clas - sier .
tables ii and iii show the classication error results for approximation using the reduced set selection techniques de - scribed in section v - c , while table iv gives results for using the reduced set construction method described in section v - d , for varying numbers of rs vectors .
shown in each line is the number of misclassied digits for each single classier and the error of the combined 123 - class machine .
in all rs systems , the optimal sv threshold was recomputed on the training set .
for the method of section v - c , rss - means , that for each binary classier , we removed support vectors until on average were left; for the method of section section v - d , that we approximated each decision function using steps using the described iteration procedure ) .
for large , i . e . , small reductions of the decision functions complexity , the accuracy of the original system can be ap -
sch olkopf et al . : input space versus feature space
mean squared error of denoised images versus number of features used , for kernel pca and linear pca .
kernel pca exploits nonlinearities and has the potential to utilize more features to code structure rather than noise .
therefore , it outperforms linear pca denoising if a sufciently large number of features is used .
numbers of test errors for each binary recognizer , and test error
rates for 123 - class classification using the rs method of section
section v - c . 123
top : numbers of svs for the original sv rbf - system .
bottom , first row : original sv system , with 123 svs on average;
following rows : systems with varying average numbers of rs
vectors .
in the system rss - n , equal fractions of svs were removed from each recognizer such that on average , n rs vectors were left
numbers of test errors for each binary recognizer , and test error rates for ten - class classification using the rs method of section v - c . 123 ( ci = =j ij ) : first row : original sv system , with 123 svs on average; following rows : systems with varying average numbers of
rs vectors .
in the system rss123 - n , was adjusted such that the average number of rs vectors left was n ( the constant , given
in parentheses , was chosen such that the numbers n were
comparable to table ii ) .
the results can be further improved using the method of section v - c123 ( ci = = maxj j j i j ) : for
instance , using about 123 expansion vectors ( which is the same number that we get when taking the union of all svs in the rss123 123 system ) , this led to an improved error rate of 123%
proached closely with both techniques .
in tables ii and iii , we see that removal of about 123% of the support vectors leaves the error practically unchanged .
reducing the numbers of support vectors further can lead to large performance losses .
the reduced set construction method , which is computa - tionally and conceptually more complex , performs better in this situation as it is able to utilize vectors different from the original support patterns in the expansion .
to get a speedup by a factor of ten , we have to use a system with 123 rs vectors ( rsc - 123 ) .
for the method in table iv , accuracy only drops moderately from 123% to 123% , which
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
complete display of reduced set vectors constructed by the iterative approach of section v - d for n = 123 , with coefcients ( top : recognizer of digit 123 , , bottom : digit 123 ) .
note that positive coefcients ( roughly ) correspond to positive examples in the classication problem .
number of test errors for each binary recognizer , and test error
rates for ten - class classification using the rs construction method of section v - d .
first row : original sv system , with 123 svs on average ( see also tables ii and iii ) ; following rows : systems with varying numbers of rs vectors ( rsc - n stands for n vectors constructed ) per binary recognizer ,
computed by iterating one - term approximations , separately for
each recognizer .
last two rows : with a subsequent global
gradient descent , the results can be further improved ( see test )
the rst phase by about two orders of magnitude ) : this led to an error rate of 123% .
for the considered kernel , this is almost identical to the traditional rs method , which yielded 123% ( for polynomial kernels , the latter method led to 123% at the same speedup ( 123 ) ) .
note that the traditional rs method restarts the second phase many times to make sure that the global minimum of the cost function is actually found , which makes it plausible that the nal results are similar .
finally , fig .
123 shows the rs - 123 vectors of the ten binary classiers .
as an aside , note that unlike the approach of ( 123 ) , our algorithm produces images which do look meaningful ( i . e . ,
algorithms utilizing mercer kernels construct their solutions in terms of mapped input is often unknown , or too complex to provide any intuition about the solution has motivated our efforts to reduce the complexity of the expansion , summarized in this paper .
as an extreme case , we have rst studied how to approx - ( i . e . , how to nd an approximate ) , and proposed a xed - point iteration algorithm
by a single
to perform the task .
is still competitive with convolutional neural networks on that data base ( 123 ) .
moreover , we can further improve this result by adding the second phase of the traditional rs algorithm , where a global gradient descent is performed in the space ( 123 ) , ( 123 ) ( computationally more expensive than
in situations where no good approximate preimage exists , one can still reduce the complexity of expressing it as a sparser expansion have proposed methods for computing the optimal coefcients or by iterating the above preimage
and for coming up with suitable patterns
selecting among the
sch olkopf et al . : input space versus feature space
both types of approximations are of theoretical interest for feature space methods; however , they also lead to practical applications .
in this paper , we have considered two such applications , namely , the problem of statistical denoising via kernel pca reconstruction , and the problem of speeding up sv decision rules .
we address them in turn .
kernel pca denoising : in denoising experiments on real - world data , we obtained results signicantly better than using linear pca .
our interpretation of this nding is as follows .
linear pca can extract at most the dimensionality of the data .
being a basis transform , all components together fully describe the data .
if the data are noisy , this implies that a certain fraction of the components will be devoted to the extraction of noise .
kernel pca , on the other hand , allows the extraction of up to features , where the number of training examples .
accordingly , kernel pca can provide a larger number of features carrying information about the structure in the data ( in our experiments , we had in addition , if the structure to be extracted is nonlinear , then linear pca must necessarily fail , as we have illustrated with
open questions and problems include the choice of a suitable kernel for a given noise reduction problem , possibly in conjunction with the regularization properties of the kernel ( e . g . , ( 123 ) ) , the application of the approach to compression , and the comparison ( and connection ) to alternative nonlinear denoising methods ( cf . , ( 123 ) ) .
speeding up sv machines : we have shown experimentally that our approximation algorithms can be used to speed up sv machines signicantly .
note that in the gaussian rbf case , the approximation can never be as good as the original , since the kernel matrix
has full rank ( 123 ) .
as in ( 123 ) , good rs construction results were obtained even though the objective function did not decrease to zero ( in our rs construction experiments , it was reduced by a factor of two123 in the rst phase , depending on how many rs vectors were computed; the global gradient descent yielded another factor twothree ) .
we conjecture that this is due to the following : in classication , we are not interested in
is the underlying probability distribution of the patterns ( cf . , ( 123 ) ) .
this is consistent with the fact that the performance of a rs sv classier can be improved by recomputing an optimal threshold
the previous rs construction method ( 123 ) , ( 123 ) can be used
for any sv kernel; the new one is limited to
however , it is fast , and it led to interpretable rs images and an interesting connection between clustering and approx - imation in feature spaces .
it appears intriguing to pursue the question whether this connection could be exploited to form more general types of approximations of sv and kernel pca expansions by making use of gaussians of variable widths .
the rs selection methods , on the other hand , are appli - cable for any sv kernel .
in our experiments , they led to worse reduction rates than rs construction; however , they are simpler and computationally faster .
among the rst two rs selection methods , the one described in section v - c . 123 was slightly superior at higher reductions; however , the one
given in section v - c . 123 is computationally cheaper since unlike the former , it does not remove the svs one at a time and therefore it need not be iterated .
moreover , we found that it can be improved by simultaneously approximating several vectors , corresponding to the ten binary recognizers in a digit
the proposed methods are applicable to any feature space algorithm based on mercer kernels .
for instance , we could also speed up sv regression machines or kernel pca feature extractors .
moreover , we expect further possibilities to open up in the future , as mercer kernel methods are being applied in an increasing number of learning and signal processing problems .
the authors would like to thank a .
elisseeff for helpful
