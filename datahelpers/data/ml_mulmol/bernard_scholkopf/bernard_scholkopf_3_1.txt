this paper describes an algorithm for nding faces within an image .
the basis of the algorithm is to run an observa - tion window at all possible positions , scales and orientation within the image .
a non - linear support vector machine is used to determine whether or not a face is contained within the observation window .
the non - linear support vector ma - chine operates by comparing the input patch to a set of sup - port vectors ( which can be thought of as face and anti - face templates ) .
each support vector is scored by some non - linear function against the observation window and if the resulting sum is over some threshold a face is indicated .
be - cause of the huge search space that is considered , it is im - perative to investigate ways to speed up the support vector machine .
within this paper we suggest a method of speeding up the non - linear support vector machine .
a set of reduced set vectors ( rvs ) are calculated from the support vectors .
by considering the rvs sequentially , and if at any point a face is deemed too unlikely to cease the sequential evalua - tion , obviating the need to evaluate the remaining rvs .
the idea being that we only need to apply a subset of the rvs to eliminate things that are obviously not a face ( thus reducing the computation ) .
the key then is to explore the rvs in the right order and a method for this is proposed .
in this paper we consider the problem of face detection within a large collection of images , such as a large pho - tographic database , images bandied about in emails or dis - played on the internet .
we consider the most general prob - lem with no constraint on the position of the face , further - more we allow the images to be monochrome or colour so that colour information alone cannot be used to reduce the search ( leaving the exploration of colour cues to others ) .
this is a well researched problem and there have been a large number of different approaches to it .
the most suc - cessful have included those of osuna and girosi ( 123 ) who ap - plied support vectors ( svs ) to the problem , that of rowley et al ( 123 ) who used a neural network , and that of schneider - man and kanade ( 123 ) who pursued a maximum likelihood approach based on histograms of feature outputs .
the one common thing to all these methods is that they are all based on running a 123 ( cid : 123 ) 123 pixel observation window across the image at all possible locations , scales and orientations .
this involves a high degree of computation as ( a ) the observation window is a 123 dimensional vector that has to be classied in a very non - linear space ( b ) there are hundreds of thou - sands of positions to search .
within this paper we follow the support vector machine approach of osuna and girosi ( 123 ) , our new contribution be - ing the sequential application of the support vectors to speed up the algorithm , and an algorithm to determine the or - der of this evaluation .
nonlinear support vector machines are known to lead to excellent classication accuracies in a wide range of tasks ( 123 , 123 ) , including face detection ( 123 ) .
they utilize a set of support vectors to dene a boundary between two classes , this boundary depending on a ker - nel function that denes a distance between two vectors .
they are , however , usually slower classiers than neural networks .
the reason for this is that their run - time complex - ity is proportional to the number of svs , i . e .
to the number of training examples that the svm algorithm utilizes in the expansion of the decision function .
whilst it is possible to construct classication problems , even in high - dimensional spaces , where the decision surface can be described by two svs only , it is normally the case that the set of svs forms a substantial subset of the whole training set .
this is the case for face detection where several hundred support vec - tors can be needed .
there has been a fair amount of research on methods for reducing the run - time complexity of svms ( 123 , 123 ) .
in the
present article , we employ one of these methods and adapt it to the case where the reduced expansion is not evaluated at once , but rather in a sequential way , such that in most cases a very small number of svs are applied .
the paper is organised as follows : in section 123 the gen - eral theory of support vector machines is reviewed with em - phasis on non - linear support vector machines .
in section 123 it is explained how to compute a set of reduced support vec - tors and how to deduce a suitable order for their evaluation .
the training is explained in section 123 and the face nding algorithm in section 123
results are given in section 123 and conclusion plus avenues for future work suggested in sec -
spanned by all order d products of input features , and the gaussian rbf kernel
123 ) = exp ( cid : 123 ) ( cid : 123 ) kx ( cid : 123 ) x123k123
performance - wise , they have been found to do similarly well; in the present paper , we focus on the latter of the two .
this means that support vectors act as templates for faces and anti - faces , thus relating non - linear svs to vector quan -
123 reduced set vectors
123 non - linear support vector ma -
assume we are given a vector ( cid : 123 ) 123 f , expanded in images of input patterns xi 123 x ,
with ( cid : 123 ) i 123 r; xi 123 x .
to reduce the complexity of evalu - ating it , one can approximate it by a reduced set expansion
with nz ( cid : 123 ) nx , ( cid : 123 ) i 123 r , and reduced set vectors zi 123 x .
to this end , one can minimize ( 123 )
k ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123k123 =
( cid : 123 ) i ( cid : 123 ) jk ( xi; xj ) +
the key point of that method is that although ( cid : 123 ) is not given explicitly , ( 123 ) can be computed ( and minimized ) in terms of
the sequential approach used here requires an extension of the reduced set method , to compute a whole sequence of reduced set approximations
implicitly map the data ( x123; y123 ) ; : : : ; ( x; y ) 123 x ( cid : 123 ) f ( cid : 123 ) 123g ( in our case , x is the 123 ( cid : 123 ) 123 observation window being a 123 dimensional integer valued vector ) into a dot product space f via a ( usually nonlinear ) map ( cid : 123 ) : x ! f; x 123 ! ( cid : 123 ) ( x ) : f is often referred to as the feature space .
although f can be high - dimensional , it is usually not necessary to explicitly work in that space ( 123 ) .
there exists a class of kernels k ( x; x123 ) which can be shown to compute the dot products in associated feature spaces , i . e .
k ( x; x123 ) = ( ( cid : 123 ) ( x ) ( cid : 123 ) ( cid : 123 ) ( x123 ) ) : the sv algorithm computes a hyperplane which separates the data in f by a large margin .
once this geometrical problem is cast in terms of dot products , the kernel trick is used and thus all computations in f are reduced to the evaluation of the kernel .
it can be shown that the resulting training problem consists of computing ( for some positive value of the parameter c determining the trade - off between margin maximization and training error minimization )
subject to 123 ( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) c; i = 123; : : : ; ;
and that the solution has an expansion
f ( x ) = sgn
( cid : 123 ) iyik ( x; xi ) + b ! :
those training examples xi with ( cid : 123 ) i > 123 are called support
kernels commonly used include polynomials k ( x; x123 ) = ( x ( cid : 123 ) x123 ) d , which can be shown to map into a feature space
( cid : 123 ) iyi = 123;
for m = 123; : : : ; nz .
the reduced set vectors zi and the coefcients ( cid : 123 ) m;i are computed by iterative optimiza - tion ( 123 ) .
for the rst vector , we need to approximate i=123 ( cid : 123 ) i ( cid : 123 ) ( xi ) by ( cid : 123 ) 123 = ( cid : 123 ) ( cid : 123 ) ( z ) .
minimizing the dis - tance k ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123k123 between ( cid : 123 ) and ( cid : 123 ) 123 , with respect to z; ( cid : 123 ) , to give the rst reduced set vector z123 and its coefcient ( cid : 123 ) 123;123 , using the method set out in the appendix .
( cid : 123 ) = pnx
recall that the aim of the reduced set algorithm is to ap - proximate a vector ( cid : 123 ) as in equation ( 123 ) by an expansion of
the type ( 123 ) with nz > 123
the required higher order re - duced set vectors zi; i > 123 and their coefcients ( cid : 123 ) i , are obtained in recursive fashion by dening a residual vector
( cid : 123 ) m = ( cid : 123 ) ( cid : 123 )
where ( cid : 123 ) is the original feature - space vector dened in ( 123 ) .
then the procedure for obtaining the rst reduced set vec - tor z123 is repeated , now with ( cid : 123 ) m in place of ( cid : 123 ) to obtain zm .
however , the optimal ( cid : 123 ) from this step is not used , instead optimal ( cid : 123 ) m;i; i = 123; : : : ; m are jointly computed ( 123 ) .
fig - ure 123 demonstrates the effects on the classication boundary of sequential reduced set vector evaluation .
note that there is a law of diminishing returns , the rst few rvs yielding the greatest increase in discrimination .
figure 123 : the result of the sequential application of rvs ( stars ) to a classication problem , showing the result of using 123 , 123 , 123 , 123 , 123 and 123 rvs darker regions indicate strong support for the classi -
thresholds .
for any nz , the obtained expansion can be plugged into the svm decision function ( 123 ) to yield f ( x ) =
j=123 ( cid : 123 ) jk ( x; zj ) + b ( cid : 123 ) : it is , however , not optimal to
simply re - use the offset b stemming from the original sv machine .
reduced set approximations of decision functions can be improved by recomputing the thresholds bj based on the training set or some validation set ( 123 ) , to get
fnz ( x ) = sgn123
( cid : 123 ) jk ( x; zj ) + bnz123
this is especially true in the present setting , as will become clear in the following .
initially the svm was trained on 123 frontal face and 123 non - face examples using platts sequential minimal
optimisation ( 123 ) .
the kernel used was gaussian ( equa - tion 123 ) with a standard deviation ( cid : 123 ) of 123 .
the trade - off between margin maximization and training error minimiza - tion , was set to 123
the non - face patches were taken ran - domly on a set of 123 images containing no faces .
the svm selected 123 support vectors .
to improve the performance of the classier a second bout of training was initiated : to decrease the number of false positives the face detector was applied on a new set of 123 images which did not contain any faces .
this gener - ated 123 false positive patches which were then added to the training .
to decrease the number of false negatives , virtual faces were generated and added to the training set .
these virtual faces were computed by modifying the con - trast or by adding an illumination plane to the faces of the original training set .
this alleviates the need of computing a pre - processing at detection time and increase the run - time performance of our algorithm .
the svm was then retrained using this new training set which yielded 123 support vec - tors .
these were subsequently decreased to 123 reduced set vectors .
note that a retraining using the misclassications of a previous training has been shown in ( 123 ) to produce a greatly improved classier .
figure 123 : first 123 reduced set vectors .
note that all vectors can be interpreted as either faces ( e . g .
the rst one ) or anti - faces ( e . g .
the second one )
123 face detection by sequential eval -
at detection time , each pixel of an input image is a po - tential face ( a large number ) .
to detect faces at different scales an image pyramid is constructed .
if w and h are the width and the height of the input image and l and s the number of sub - sampling levels and the sub - sampling rate , respectively , the total number of patches to be evaluated is l=123 whs123 ( l ( cid : 123 ) 123 ) .
evaluating the full svm or even the whole set of reduced vectors on all patches would be slow .
a large portion of the patches can be easily classied using only a few reduced set vectors .
hence we propose the following sequential evaluation algorithm , to be applied to
np = pl
each overlapping patch x of an input image .
set the hierarchy level to m = 123
evaluate ym = sgn ( cid : 123 ) pm
j=123 ( cid : 123 ) m;jkj + bm ( cid : 123 ) where kj =
( cid : 123 ) if ym < 123 , x is classied as a non - face and the
( cid : 123 ) if ym ( cid : 123 ) 123 , m is incremented .
if m = nz the algorithm stops , otherwise evaluation continues at step 123
if yj ( cid : 123 ) 123 and j = nz , the full svm is applied on the patch x , using equation 123
if the evaluation is positive the patch is classied as a face .
the main feature of this approach is that on average , rela - tively few kernels kj have to be evaluated at any given im - age location i . e . , for most patches , the algorithm above stops at a level j ( cid : 123 ) nz .
this speeds up the algorithm rela - tive to the full reduced set ( by more than an order of magni - tude in the face classication experiments reported below ) .
note that in the case of gaussian kernels , the application of one reduced set vector amounts to a simple template match -
setting offsets .
the offsets bm are xed to obtain a de - sired point on the r . o . c .
for the overall sequential scheme .
suppose an overall false negative rate ( cid : 123 ) is required , then , given a decay rate ( cid : 123 ) , we express ( cid : 123 ) as a geometric series by setting false negative rates ( cid : 123 ) m for the mth level in the hi - erarchy to ( cid : 123 ) j = ( cid : 123 ) ( cid : 123 ) j ( cid : 123 ) 123 where ( cid : 123 ) 123 = ( cid : 123 ) ( 123 ( cid : 123 ) ( cid : 123 ) ) : now each bm is xed to achieve the desired ( cid : 123 ) m over a validation set .
the free parameter ( cid : 123 ) can now be set to maximize the overall true positive rate over the validation set .
within this section the new sequential evaluation algorithm is tested for speed and accuracy .
speed improvement .
at detection time , due to the se - quential evaluation of the patches , very few reduced set vec - tors are applied .
figure 123 shows the number of reduced set vectors evaluated per patches for different methods ( svm , rsm and srsm ( sequential reduced set machine ) ) , when the algorithm is applied to the photo in fig 123
the full svm and the rsm evaluate all their support or reduced set vectors on all the patches , while the srsm uses on average only 123 reduced set vectors per patch .
figure 123 shows the patches of an input image which remain after 123 , 123 , 123 and 123 sequential reduced set evaluations on an image with one
mean of hierarchical rs
number of patches ( np )
figure 123 : number of reduced set vectors used per patch for the full svm ( 123 support vectors ) , reduced set svm and sequential reduced set svm ( both at 123 reduced set vector )
figure 123 : from left to right : input image , followed by portions of the image which contain un - reject patches after the sequential evaluation of 123 ( 123% patches remaining ) , 123 ( 123% ) , 123 ( 123% ) and 123 ( 123% ) support vectors .
note that in these images , a pixel is displayed if it is part of any remaining un - rejected patch at any scale , orientation or position this explains the apparent discrepancy between the above percentages and the visual impres -
face , gure 123 shows the results on an image with multiple
figure 123 shows the number of reduced set vectors used to classify each patch of an image .
the intensities values of the pixels of the right image are proportional to the num - ber of reduced set vectors used to classify the corresponding spot in the left image ( note that the intensities are displayed at the center of the corresponding patches only ) .
the uni - form parts of the input image are easily rejected using a sin - gle reduced set vector , whereas the cluttered background re - quires more reduced set vectors .
note that very few patches needed all the reduced set vectors ( only the patches contain - ing the faces used all the reduced set vectors ) .
accuracy .
figure 123 shows a comparison of the accuracy of the different methods .
these r . o . c .
were computed on a test set containing 123 faces and 123 non - faces .
the ac - curacy of the srsm ( 123 reduced set vectors ) is very sim - ilar to the accuracy of the full svm ( 123 support vectors ) and the rs ( 123 reduced set vectors ) which perform equally
to compare our system with others , we used the row -
than rowleys , sungs and osunas results , although they are hard to compare due to the fact that they pre - process the patches before feeding them into their classier ( his - togram equalisation , background pixel removal and illumi - nation gradient compensation ) .
our main objective was speed , hence no pre - processing was made .
secondly , we used a different training set as their training set was partly proprietary .
speed gures are also hard to compare , but from the information given , we conjecture that the osuna et al .
rs system is comparable in speed to our rs system , which in turn is 123 times slower than our sequential evalu - ation system ( 123 ( cid : 123 ) s for the sequential evaluation , 123ms for the reduced set evaluation and 123ms for the full svm per patch on a 123mhz pentium ) .
figure 123 : top left : the intensity values of the pixels of the left image are proportional to the number of reduced set vectors used to classify their associated patches of the middle image .
light grey corresponds to the use of a single reduced set vector , black to the use of all the vectors .
top middle : 123 ( cid : 123 ) 123 middle im - age contains 123 patches and was detected in 123 : 123s .
top right : a 123 ( cid : 123 ) 123 image containing 123 patches detected in 123s .
bottom left : 123 ( cid : 123 ) 123 contains 123 patches and was detected in 123 : 123s .
bottom right : a 123 ( cid : 123 ) 123 image containing 123 patches detected in 123s ( note the false positives ) .
123 conclusion and future work
pattern detection systems usually have to scan large images .
therefore , the greatest challenge in engineering systems for real - world applications is that of reducing computational complexity .
within this paper we have demonstrated com - putational savings in classication by the use of a sequential
figure 123 : input image , followed by patches which remain after the evaluation of 123 ( 123% patches remaining ) , 123 ( 123% ) , 123 ( 123% ) and 123 ( 123% ) : : : 123 ( 123% ) support vectors .
note the comment in the caption of fig 123
figure 123 : left : r . o . c .
for the svm using 123 support vectors ( dotted line ) , the rs using 123 reduced set vectors ( dashed line ) and srsm using also 123 reduced set vectors ( solid line ) .
note that the svm and rs curves are so close that they are not distin - guishable .
right : r . o . c .
for an srsm using 123 ( dashed line ) , 123 ( dash - dot line ) , 123 ( dotted line ) and 123 ( solid line ) reduced set vec -
ley et al .
( 123 ) test set ( which also includes the sung et al .
( 123 ) and the osuna et al .
( 123 ) test images ) .
this set con - sists of 123 images containing 123 faces .
we used a sub - sampling ratio of s = 123 : 123 and the input images were sub - sampled as long as their width and height was larger than 123 ( i . e .
the number of levels in the sub - sampling pyramid is
log 123 : 123 ( cid : 123 ) ; ( cid : 123 ) oor ( cid : 123 ) log ( 123=h )
are , respectively , the width and the height of the input im - age ) .
we obtained a detection rate of 123% with a false detection rate of 123% .
these numbers are slightly worse
log 123 : 123 ( cid : 123 ) ( cid : 123 ) where w and h
reduced support vector evaluation .
there are several av - enues for future research .
( a ) we have explored the use of the gaussian kernel as a distance metric , however it may be possible to tailor the kernel to something much more suited to facial detection .
( b ) it may be that the criteria for choos - ing the reduced set of support vectors can be improved .
at present the reduced set of support vectors is chosen to min - imize ( 123 ) , which affects classication error only indirectly .
however , it might be advantageous to choose a reduced set that minimizes classication error directly .
( c ) it would be interesting to adapt the thresholds based on contextual in - formation : for instance , if a face is detected in the image , this places strong priors on the scale and orientation of any other faces we expect to see .
this could further speed up the detection .
finally , although the method has been im - plemented for the task of face detection , it could be readily applied to a wide class of other detection and classications
thanks to henry rowley for assisting us and for providing test images .
thanks to mike tipping , kentaro toyama and ben bradshaw for useful conversations .
