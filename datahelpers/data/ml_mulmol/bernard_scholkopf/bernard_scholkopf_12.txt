while classical kernel - based learning algorithms are based on a single kernel , in practice it is often desirable to use multiple kernels .
lanckriet et al .
( 123 ) considered conic combinations of kernel matrices for classication , leading to a convex quadratically constrained quadratic program .
we show that it can be rewritten as a semi - innite linear program that can be efciently solved by recy - cling the standard svm implementations .
moreover , we generalize the formulation and our method to a larger class of problems , including regression and one - class classication .
experimental re - sults show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined , and helps for automatic model selection , improving the interpretability of the learning result .
in a second part we discuss general speed up mechanism for svms , especially when used with sparse feature maps as appear for string kernels , allowing us to train a string kernel svm on a 123 million real - world splice data set from computational biology .
we integrated multi - ple kernel learning in our machine learning toolbox shogun for which the source code is publicly available at http : / / www . fml . tuebingen . mpg . de / raetsch / projects / shogun .
keywords : multiple kernel learning , string kernels , large scale optimization , support vector ma - chines , support vector regression , column generation , semi - innite linear programming
kernel based methods such as support vector machines ( svms ) have proven to be powerful for a wide range of different data analysis problems .
they employ a so - called kernel function k ( xi , x j ) which intuitively computes the similarity between two examples xi and x j .
the result of svm
123 sren sonnenburg , gunnar rtsch , christin schfer and bernhard schlkopf .
sonnenburg , rtsch , schfer and schlkopf
learning is an a - weighted linear combination of kernels with a bias b
f ( x ) = sign n ( cid : 123 )
iyik ( xi , x ) + b ! ,
where the xi , i = 123 , .
, n are labeled training examples ( yi ( 123 ) ) .
recent developments in the literature on svms and other kernel methods have shown the need to consider multiple kernels .
this provides exibility and reects the fact that typical learning problems often involve multiple , heterogeneous data sources .
furthermore , as we shall see below , it leads to an elegant method to interpret the results , which can lead to a deeper understanding of the
while this so - called multiple kernel learning ( mkl ) problem can in principle be solved via cross - validation , several recent papers have focused on more efcient methods for multiple kernel learning ( chapelle et al . , 123; bennett et al . , 123; grandvalet and canu , 123; ong et al . , 123; bach et al . , 123; lanckriet et al . , 123; bi et al . , 123 ) .
one of the problems with kernel methods compared to other techniques is that the resulting decision function ( 123 ) is hard to interpret and , hence , is difcult to use in order to extract relevant knowledge about the problem at hand .
one can approach this problem by considering convex combinations of k kernels , i . e .
k ( xi , x j ) =
b kkk ( xi , x j )
with b k 123 and ( cid : 123 ) k b k = 123 , where each kernel kk uses only a distinct set of features .
for ap - propriately designed sub - kernels kk , the optimized combination coefcients can then be used to understand which features of the examples are of importance for discrimination : if one is able to obtain an accurate classication by a sparse weighting b k , then one can quite easily interpret the re - sulting decision function .
this is an important property missing in current kernel based algorithms .
note that this is in contrast to the kernel mixture framework of bennett et al .
( 123 ) and bi et al .
( 123 ) where each kernel and each example are assigned an independent weight and therefore do not offer an easy way to interpret the decision function .
we will illustrate that the considered mkl formulation provides useful insights and at the same time is very efcient .
we consider the framework proposed by lanckriet et al .
( 123 ) , which results in a convex op - timization problem - a quadratically - constrained quadratic program ( qcqp ) .
this problem is more challenging than the standard svm qp , but it can in principle be solved by general - purpose opti - mization toolboxes .
since the use of such algorithms will only be feasible for small problems with few data points and kernels , bach et al .
( 123 ) suggested an algorithm based on sequential mini - mization optimization ( smo platt , 123 ) .
while the kernel learning problem is convex , it is also non - smooth , making the direct application of simple local descent algorithms such as smo infeasi - ble .
bach et al .
( 123 ) therefore considered a smoothed version of the problem to which smo can
in the rst part of the paper we follow a different direction : we reformulate the binary clas - sication mkl problem ( lanckriet et al . , 123 ) as a semi - innite linear program , which can be efciently solved using an off - the - shelf lp solver and a standard svm implementation ( cf .
sec - tion 123 for details ) .
in a second step , we show how easily the mkl formulation and the algorithm is generalized to a much larger class of convex loss functions ( cf .
section 123 ) .
our proposed wrap - per method works for any kernel and many loss functions : in order to obtain an efcient mkl
large scale mkl
algorithm for a new loss function , it now sufces to have an lp solver and the corresponding single kernel algorithm ( which is assumed to be efcient ) .
using this general algorithm we were able to solve mkl problems with up to 123 , 123 examples and 123 kernels within reasonable time . 123
we also consider a chunking algorithm that can be considerably more efcient , since it optimizes the svm a multipliers and the kernel coefcients b at the same time .
however , for large scale problems it needs to compute and cache the k kernels separately , instead of only one kernel as in the single kernel algorithm .
this becomes particularly important when the sample size n is large .
if , on the other hand , the number of kernels k is large , then the amount of memory available for caching is drastically reduced and , hence , kernel caching is not effective anymore .
( the same statements also apply to the smo - like mkl algorithm proposed in bach et al .
( 123 ) . )
since kernel caching cannot help to solve large scale mkl problems , we sought for ways to avoid kernel caching .
this is of course not always possible , but it certainly is for the class of kernels where the feature map f ( x ) can be explicitly computed and computations with f ( x ) can be implemented efciently .
in section 123 . 123 we describe several string kernels that are frequently used in biological sequence analysis and exhibit this property .
here , the feature space can be very high dimensional , but f ( x ) is typically very sparse .
in section 123 . 123 we discuss several methods for efciently dealing with high dimensional sparse vectors , which not only is of interest for mkl but also for speeding up ordinary svm classiers .
finally , we suggest a modication of the previously proposed chunking algorithm that exploits these properties ( section 123 . 123 ) .
in the experimental part we show that the resulting algorithm is more than 123 times faster than the plain chunking algorithm ( for 123 , 123 examples ) , even though large kernel caches were used .
also , we were able to solve mkl problems with up to one million examples and 123 kernels and a 123 million real - world splice site classication problem from computational biology .
we conclude the paper by illustrating the usefulness of our algorithms in several examples relating to the interpretation of results and to automatic model selection .
moreover , we provide an extensive benchmark study comparing the effect of different improvements on the running time of the algorithms .
we have implemented all algorithms discussed in this work in c++ with interfaces to matlab ,
octave , rand python .
the source code is freely available at
the examples used to generate the gures are implemented in matlab using the matlab inter - face of the shogun toolbox .
they can be found together with the data sets used in this paper at
a general and efcient multiple kernel learning algorithm
in this section we rst derive our mkl formulation for the binary classication case and then show how it can be extended to general cost functions .
in the last subsection we will propose algorithms for solving the resulting semi - innite linear programs ( silps ) .
123 multiple kernel learning for classication using silp in the multiple kernel learning problem for binary classication one is given n data points ( xi , yi ) ( yi ( 123 ) ) , where xi is translated via k mappings f k ( x ) 123 rdk , k = 123 , .
, k , from the input into k 123
the results are not shown .
sonnenburg , rtsch , schfer and schlkopf
feature spaces ( f 123 ( xi ) , .
, f k ( xi ) ) where dk denotes the dimensionality of the k - th feature space .
then one solves the following optimization problem ( bach et al . , 123 ) , which is equivalent to the linear svm for k = 123 : 123
mkl primal for classication
wk rdk , x rn , b r ,
i 123 and yi k ( cid : 123 )
k ( xi ) i + b ! 123 x
i , i = 123 ,
b k = 123 ( bach et al . , 123 ) .
note that therefore the 123 - norm of b
k with b k 123 , k = 123 , .
, k and note that the problems solution can be written as wk = b kw is constrained to one , while one is penalizing the 123 - norm of wk in each block k separately .
the idea is that 123 - norm constrained or penalized variables tend to have sparse optimal solutions , while 123 - norm penalized variables do not ( e . g .
rtsch , 123 , chapter 123 ) .
thus the above optimization problem offers the possibility to nd sparse solutions on the block level with non - sparse solutions within the blocks .
bach et al .
( 123 ) derived the dual for problem ( 123 ) .
taking their problem ( dk ) , squaring the leads to the
123 and nally substituting 123
constraints on gamma , multiplying the constraints by 123 to the following equivalent multiple kernel learning dual :
g 123 123 g
mkl dual for classication
g r , a rn 123 a 123c ,
iyi = 123
jyiy jkk ( xi , x j ) g , k = 123 ,
k ( x j ) i .
note that we have one quadratic constraint per kernel ( sk ( a ) where kk ( xi , x j ) = hf g ) .
in the case of k = 123 , the above problem reduces to the original svm dual .
we will now move the term ( cid : 123 ) n both sides of the constraints and substituting g ( cid : 123 ) n
i , into the constraints on g .
this can be equivalently done by adding ( cid : 123 ) n
i 123 g :
we assume tr ( kk ) = 123 , k = 123 , .
, k and set d j in bach et al .
( 123 ) to one .
large scale mkl
mkl dual for classication
g r , a rn 123 a 123c ,
iyi = 123
jyiy jkk ( xi , x j )
g , k = 123 ,
in order to solve ( 123 ) , one may solve the following saddle point problem : minimize
l : = g +
b k ( sk ( a ) g )
a rn , g r ( with 123 a c123 and ( cid : 123 ) setting the derivative w . r . t .
to g to : l = s ( a maximizes w . r . t .
the kernel weighting b
, b ) : = ( cid : 123 ) k
to zero , one obtains the constraint ( cid : 123 ) k
b ksk ( a ) .
while one minimizes the objective w . r . t
iyi = 123 ) , and maximize it w . r . t .
b rk , where 123 b b k = 123 and ( 123 ) simplies , at the same time one
this leads to a
b ksk ( a )
a rn , b rk 123 a c , 123 b ,
iyi = 123 and
b k = 123
this problem is very similar to equation ( 123 ) in bi et al .
( 123 ) when composite kernels , i . e .
linear combinations of kernels are considered .
there the rst term of sk ( a ) has been moved into the constraint , still b
including the ( cid : 123 ) k
b k = 123 is missing . 123
assume a were the optimal solution , then q : = s ( a , b ) would be minimal and , hence , s ( a
( subject to the above constraints ) .
hence , nding a saddle - point of ( 123 ) is equivalent to
q for all a solving the following semi - innite linear program :
semi - innite linear program ( silp )
q r , b rk 123 b , ( cid : 123 ) b ksk ( a ) q for all a rn with 123 a c123 and ( cid : 123 )
b k = 123 and
in bi et al .
( 123 ) it is argued that the approximation quality of composite kernels is inferior to mixtures of kernels where a weight is assigned per example and kernel as in bennett et al .
( 123 ) .
for that reason and as no efcient methods were available to solve the composite kernel problem , they only considered mixtures of kernels and in the experimental validation used a uniform weighting in the composite kernel experiment .
also they did not consider to use composite kernels as a method to interpret the resulting classier but looked at classication accuracy instead .
sonnenburg , rtsch , schfer and schlkopf
note that this is a linear program , as q innitely many constraints : one for each a rn satisfying 123 a c and ( cid : 123 ) n problems ( 123 ) and ( 123 ) have the same solution .
to illustrate that , consider b
in ( 123 ) .
let a be the solution that minimizes ( 123 ) .
then we can increase the value of q
are only linearly constrained .
however there are iyi = 123
both is xed and we minimize in ( 123 ) as b ksk ( a ) .
on the is found .
we will discuss in section 123
long as none of the innitely many a - constraints ( 123 ) is violated , i . e .
up to q = ( cid : 123 ) k other hand as we increase q how to solve such semi - innite linear programs .
the maximizing b
for a xed a
123 multiple kernel learning with general cost functions
in this section we consider a more general class of mkl problems , where one is given an arbitrary strictly convex and differentiable loss function , for which we derive its mkl silp formulation .
we will then investigate in this general mkl silp using different loss functions , in particular the soft - margin loss , the e - insensitive loss and the quadratic loss .
we dene the mkl primal formulation for a strictly convex and differentiable loss function
l ( f ( x ) , y ) as :
mkl primal for generic loss functions
l ( f ( xi ) , yi )
w = ( w123 , .
, wk ) rd123 rdk
f ( xi ) =
k ( xi ) , wki + b , i = 123 ,
in analogy to bach et al .
( 123 ) we treat problem ( 123 ) as a second order cone program ( socp )
leading to the following dual ( see appendix a for the derivation ) :
mkl dual for generic loss functions
g r , a rn
i , yi ) , yi ) +
i , yi ) g , k = 123 ,
here l123 denotes the inverse of the derivative of l ( f ( x ) , y ) w . r . t .
the prediction f ( x ) .
to derive the silp formulation we follow the same recipe as in section 123 : deriving the lagrangian leads to a max - min problem formulation to be eventually reformulated as a silp :
large scale mkl
silp for generic loss functions
q r , b rk 123 b ,
b k = 123 and
b ksk ( a ) q , a rn ,
i = 123 ,
sk ( a ) =
i , yi ) , yi ) +
i , yi ) +
we assumed that l ( x , y ) is strictly convex and differentiable in x .
unfortunately , the soft margin and e - insensitive loss do not have these properties .
we therefore consider them separately in the sequel .
soft margin loss we use the following loss in order to approximate the soft margin loss :
ls ( x , y ) =
log ( 123 + exp ( s ( 123 xy ) ) ) .
it is easy to verify that
lims ls ( x , y ) = c ( 123 xy ) + .
( 123 ) , we obtain ( cf .
appendix b . 123 ) :
is strictly convex and differentiable for s <
using this loss and assuming yi
sk ( a ) =
s ( cid : 123 ) log ( cid : 123 ) cyi
i +cyi ( cid : 123 ) ( cid : 123 ) + , then the rst two terms vanish provided that c a
i +cyi ( cid : 123 ) + log ( cid : 123 )
i 123 if yi = 123 and 123 a
i c if
yi = 123
substituting a
i = a
iyi , we obtain
sk ( a ) =
iyi = 123 ,
with 123 a
i c ( i = 123 , .
, n ) which is the same as ( 123 ) .
one - class soft margin loss the one - class svm soft margin ( e . g .
schlkopf and smola , 123 ) is very similar to the two - class case and leads to
subject to 123 a 123
n n 123 and ( cid : 123 ) n
sk ( a ) =
i = 123
sonnenburg , rtsch , schfer and schlkopf
e - insensitive loss using the same technique for the epsilon insensitive loss l ( x , y ) = c ( 123 |x y| ) + , we obtain
, a ) =
i ) yi = 123 , with 123 a
it is easy to derive the dual problem for other loss functions such as the quadratic loss or logistic loss ( see appendix b . 123 & b . 123 ) .
note that the dual silps only differ in the denition of sk and the domains of the a s .
123 algorithms to solve silps
all semi - innite linear programs considered in this work have the following structure :
q r , b rk 123 b ,
b k = 123
b ksk ( a ) q
for all a c .
they have to be optimized with respect to b .
the constraints depend on denition of sk and therefore on the choice of the cost function .
using theorem 123 in rtsch et al .
( 123 ) one can show that the above silp has a solution if the corresponding primal is feasible and bounded ( see also hettich and kortanek , 123 ) .
moreover , there is no duality gap , if m = co ( ( s123 ( a ) , .
, sk ( a ) ) | a c ) is a closed set .
for all loss functions considered in this paper this condition is satised .
we propose to use a technique called column generation to solve ( 123 ) .
the basic idea is to compute the optimal ( b , q ) in ( 123 ) for a restricted subset of constraints .
it is called the restricted master problem .
then a second algorithm generates a new , yet unsatised constraint determined by .
in the best case the other algorithm nds the constraint that maximizes the constraint violation for the given intermediate solution ( b , q ) , i . e .
b ksk ( a ) .
b ) q
b satises the constraint ( cid : 123 ) k
constraint is added to the set of constraints and the iterations continue .
, then the solution ( q , b ) is optimal .
otherwise , the
algorithm 123 is a special case of a set of silp algorithms known as exchange methods .
these methods are known to converge ( cf .
theorem 123 in hettich and kortanek , 123 ) .
however , no convergence rates for such algorithm are known . 123
since it is often sufcient to obtain an approximate solution , we have to dene a suitable con - vergence criterion .
note that the problem is solved when all constraints are satised .
hence , it is a
it has been shown that solving semi - innite problems like ( 123 ) , using a method related to boosting ( e . g .
meir and rtsch , 123 ) one requires at most t = o ( log ( m ) / e 123 ) iterations , where e is the remaining constraint viola - tion and the constants may depend on the kernels and the number of examples n ( rtsch , 123; rtsch and warmuth , 123; warmuth et al . , 123 ) .
at least for not too small values of e this technique produces reasonably fast good ap -
large scale mkl
natural choice to use the normalized maximal constraint violation as a convergence criterion , i . e .
the algorithm stops if e mkl e is the optimal solution at iteration t 123 and a t corresponds to the newly found maximally violating constraint of the next iteration .
, where e mkl is an accuracy parameter , ( b
in the following we will formulate algorithms that alternately optimize the parameters a and b .
123 . 123 a wrapper algorithm
the wrapper algorithm ( see algorithm 123 ) divides the problem into an inner and an outer subproblem .
the solution is obtained by alternatively solving the outer problem using the results of the inner problem as input and vice versa until convergence .
the outer loop constitutes the restricted master problem which determines the optimal b for a xed a using an of - the - shelf linear optimizer .
in the inner loop one has to identify unsatised constraints , which , fortunately , turns out to be particularly simple .
note that ( 123 ) is for all considered cases exactly the dual optimization problem of the single kernel case for xed b .
for instance for binary classication with soft - margin loss , ( 123 ) reduces to the standard svm dual using the kernel k ( xi , x j ) = ( cid : 123 )
k b kkk ( xi , x j ) :
jyiy jk ( xi , x j )
123 a c123 and
iyi = 123
hence , we can use a standard svm implementation with a single kernel in order to identify the most violated constraint v q .
since there exists a large number of efcient algorithms to solve the single kernel problems for all sorts of cost functions , we have therefore found an easy way to extend their applicability to the problem of multiple kernel learning .
also , if the kernels are computed on - the - y within the svm still only a single kernel cache is required .
the wrapper algorithm is very easy to implement , very generic and already reasonably fast for small to medium size problems .
however , determining a up to a xed high precision even for intermediate solutions , while b is still far away from the global optimal is unnecessarily costly .
thus there is room for improvement motivating the
123 . 123 a chunking algorithm for simultaneous optimization of a and b the goal is to simultaneously optimize a and b in svm training .
usually it is infeasible to use stan - dard optimization tools ( e . g .
minos , cplex , loqo ) for solving even the svm training problems on data sets containing more than a few thousand examples .
so - called decomposition techniques as chunking ( e . g .
used in joachims , 123 ) overcome this limitation by exploiting the special structure of the svm problem .
the key idea of decomposition is to freeze all but a small number of opti - mization variables ( working set ) and to solve a sequence of constant - size problems ( subproblems of the svm dual ) .
here we would like to propose an extension of the chunking algorithm to optimize the kernel weights b and the example weights a at the same time .
the algorithm is motivated from an insuf - ciency of the wrapper algorithm described in the previous section : if the b s are not optimal yet , then the optimization of the a s until optimality is not necessary and therefore inefcient .
it would
sonnenburg , rtsch , schfer and schlkopf
algorithm 123 the mkl - wrapper algorithm optimizes a convex combination of k kernels and em - ploys a linear programming solver to iteratively solve the semi - innite linear optimization problem ( 123 ) .
the accuracy parameter e mkl is a parameter of the algorithm .
sk ( a ) and c are determined by the cost function .
s123 = 123 , q 123 = for t = 123 , 123 ,
k for k = 123 ,
t = argmin
k , where st
k = sk ( a
ksk ( a ) by single kernel algorithm with k =
e mkl then break
t+123 ) = argmax q
b rk , q r 123 b ,
b k = 123 and
for r = 123 ,
in the chunking iterations , we could efciently
be considerably faster if for any newly obtained a recompute the optimal b and then continue optimizing the a s using the new kernel weighting .
intermediate recomputation of b involves solving a linear program and the problem grows with each additional a - induced constraint .
hence , after many iterations solving the lp may become infeasible .
fortunately , there are two facts making it still possible : ( a ) only a small number of the added constraints remain active and one may as well remove inactive ones this prevents the lp from growing arbitrarily and ( b ) for simplex - based lp optimizers such as cplex there exists the so - called hot - start feature which allows one to efciently recompute the new solution , if for instance only a few additional constraints are added .
the svmlight optimizer which we are going to modify , internally needs the output
jy jk ( xi , x j )
for all training examples i = 123 , .
, n in order to select the next variables for optimization ( joachims , 123 ) .
however , if one changes the kernel weights , then the stored gi values become invalid and need to be recomputed .
in order to avoid the full recomputation one has to additionally store a k n jy jkk ( xi , x j ) , i . e .
the outputs for each kernel separately .
if the b s change , then gi matrix gk , i = ( cid : 123 ) n k b kgk , i .
we implemented the nal chunking algorithm can be quite efciently recomputed by gi = ( cid : 123 ) for the mkl regression and classication case and display the latter in algorithm 123
the wrapper as well as the chunking algorithm have both their merits : the wrapper algorithm only relies on the repeated efcient computation of the single kernel solution , for which typically large scale algorithms exist .
the chunking algorithm is faster , since it exploits the intermediate a s however , it needs to compute and cache the k kernels separately ( particularly important when
large scale mkl
and the kernel weighting b
algorithm 123 outline of the mkl - chunking algorithm for the classication case ( extension to svmlight ) that optimizes a simultaneously .
the accuracy parameter e mkl and the subproblem size q are assumed to be given to the algorithm .
for simplicity we omit the removal of inactive constraints .
also note that from one iteration to the next the lp only differs by one additional constraint .
this can usually be exploited to save computing time for solving the
gk , i = 123 , gi = 123 , a for t = 123 , 123 ,
i = 123 , b 123
k for k = 123 , .
, k and i = 123 ,
check optimality conditions and stop if optimal select q suboptimal variables i123 , .
, iq based on g and a a old = a solve svm dual with respect to the selected variables and update a gk , i = gk , i + ( cid : 123 ) q for k = 123 ,
iq a old iq ) yiqkk ( xiq , xi ) for all k = 123 , .
, m and i = 123 ,
k = 123 st = ( cid : 123 ) k
t ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) e mkl
t+123 ) = argmax q w . r . t .
b rk , q r
123 b ,
k b k = 123 and ( cid : 123 ) m
for r = 123 ,
t+123 = q
gi = ( cid : 123 )
k gk , i for all i = 123 ,
n is large ) .
if , on the other hand , k is large , then the amount of memory available for caching is drastically reduced and , hence , kernel caching is not effective anymore .
the same statements also apply to the smo - like mkl algorithm proposed in bach et al .
( 123 ) .
in this case one is left with the wrapper algorithm , unless one is able to exploit properties of the particular problem or the sub - kernels ( see next section ) .
sparse feature maps and parallel computations
in this section we discuss two strategies to accelerate svm training .
first we consider the case where the explicit mapping f into the kernel feature space is known as well as sparse .
for this case we show that mkl training ( and also svm training in general ) can be made drastically faster , in particular , when n and k are large .
in the second part we discuss a simple , yet efcient way to parallelize mkl as well as svm training .
123 explicit computations with sparse feature maps
we assume that all k sub - kernels are given as
kk ( x , x ) = hf
sonnenburg , rtsch , schfer and schlkopf
and the mappings f k are given explicitly ( k = 123 , .
moreover , we suppose that the mapped k ( x ) are very sparse .
we start by giving examples of such kernels and discuss two in section 123 . 123 we kernels that are often used in biological sequence analysis ( section 123 . 123 ) .
discuss several strategies for efciently storing and computing with high dimensional sparse vectors ( in particular for these two kernels ) .
finally in section 123 . 123 we discuss how we can exploit these properties to accelerate chunking algorithms , such as svmlight , by a factor of up to q ( the chunking
123 . 123 string kernels
the spectrum kernel the spectrum kernel ( leslie et al . , 123 ) implements the n - gram or bag - of - words kernel ( joachims , 123 ) as originally dened for text classication in the context of bio - logical sequence analysis .
the idea is to count how often a d - mer ( a contiguous string of length d ) is contained in the sequences x and x .
summing up the product of these counts for every possible d - mer ( note that there are exponentially many ) gives rise to the kernel value which formally is de - ned as follows : let s be an alphabet and u s d a d - mer and #u ( x ) the number of occurrences of u in x .
then the spectrum kernel is dened as the inner product of k ( x , x ) = hf ( x ) , f ( x ) i , where f ( x ) = ( #u ( x ) ) us d .
note that spectrum - like kernels cannot extract any positional information from the sequence which goes beyond the d - mer length .
it is well suited for describing the content of a sequence but is less suitable for instance for analyzing signals where motifs may appear in a cer - tain order or at specic positions .
also note that spectrum - like kernels are capable of dealing with sequences with varying length .
the spectrum kernel can be efciently computed in o ( d ( |x| + |x| ) ) using tries ( leslie et al . , 123 ) , where |x| denotes the length of sequence x .
an easier way to compute the kernel for two sequences x and x is to separately extract and sort the n d - mers in each sequence , which can be done in a preprocessing step .
note that for instance dna d - mers of length d 123 can be efciently represented as a 123 - bit integer value .
then one iterates over all d - mers of sequences x and x simultaneously and counts which d - mers appear in both sequences and sums up the product of their counts .
the computational complexity of the kernel computation is o ( log ( |s the weighted degree kernel the so - called weighted degree ( wd ) kernel ( rtsch and sonnenburg , 123 ) efciently computes similarities between sequences while taking positional information of k - mers into account .
the main idea of the wd kernel is to count the ( exact ) co - occurrences of k - mers at corresponding positions in the two sequences to be compared .
the wd kernel of order d com - pares two sequences xi and x j of length l by summing all contributions of k - mer matches of lengths k ( 123 , .
, d ) , weighted by coefcients b k :
| ) d ( |x| + |x| ) ) .
k ( xi , x j ) =
i ( uk , l ( xi ) = uk , l ( x j ) ) .
here , uk , l ( x ) is the string of length k starting at position l of the sequence x and i ( ) is the indicator function which evaluates to 123 when its argument is true and to 123 otherwise .
for the weighting coefcients , rtsch and sonnenburg ( 123 ) proposed to use b k = 123 dk+123 d ( d+123 ) .
matching substrings are thus rewarded with a score depending on the length of the substring . 123 123
note that although in our case b k+123 < b k , longer matches nevertheless contribute more strongly than shorter ones : this is due to the fact that each long match also implies several short matches , adding to the value of ( 123 ) .
exploiting this
large scale mkl
note that the wd kernel can be understood as a spectrum kernel where the k - mers starting at different positions are treated independently of each other . 123 moreover , it does not only consider substrings of length exactly d , but also all shorter matches .
hence , the feature space for each position has ( cid : 123 ) d |123 123 dimensions and is additionally duplicated l times ( leading to |d ) dimensions ) .
however , the computational complexity of the wd kernel is in the worst case o ( dl ) as can be directly seen from ( 123 ) .
|k = |s
123 . 123 efficient storage of sparse weights
the considered string kernels correspond to a feature space that can be huge .
for instance in the case of the wd kernel on dna sequences of length 123 with k = 123 , the corresponding feature space is 123 dimensional .
however , most dimensions in the feature space are not used since only a few of the many different k - mers actually appear in the sequences .
in this section we briey discuss three methods to efciently deal with sparse vectors v .
we assume that the elements of the vector v are indexed by some index set u ( for sequences , e . g .
u = s d ) and that we only need three operations : clear , add and lookup .
the rst operation sets the vector v to zero , the add operation increases the weight of a dimension for an element u u by some amount a lookup requests the value vu .
the latter two operations need to be performed as quickly as possible ( whereas the performance of the lookup operation is of higher importance ) .
vu = vu + a
explicit map if the dimensionality of the feature space is small enough , then one might consider keeping the whole vector v in memory and to perform direct operations on its elements .
then each read or write operation is o ( 123 ) . 123 this approach has expensive memory requirements ( o ( |s is very fast and best suited for instance for the spectrum kernel on dna sequences with d 123 and on protein sequences with d 123
sorted arrays more memory efcient but computationally more expensive are sorted arrays of index - value pairs ( u , vu ) .
assuming the l indexes are given and sorted in advance , one can ef - ciently change or look up a single vu for a corresponding u by employing a binary search procedure ( o ( log ( l ) ) ) .
when given l look up indexes at once , one may sort them in advance and then si - multaneously traverse the two arrays in order to determine which elements appear in the rst array ( i . e .
o ( l + l ) operations omitting the sorting of the second array instead of o ( log ( l ) l ) ) .
this method is well suited for cases where l and l are of comparable size , as for instance for compu - tations of single spectrum kernel elements ( as proposed in leslie et al . , 123 ) .
if , l l , then the binary search procedure should be preferred .
tries another way of organizing the non - zero elements are tries ( fredkin , 123 ) : the idea is to use a tree with at most |s | siblings of depth d .
the leaves store a single value : the element vu , where u s d is a d - mer and the path to the leaf corresponds to u .
knowledge allows for a o ( l ) reformulation of the kernel using block - weights as has been done in sonnenburg et al .
it therefore is very position dependent and does not tolerate any positional shift .
for that reason we proposed in rtsch et al .
( 123 ) a wd kernel with shifts , which tolerates a small number of shifts , that lies in between the wd and the spectrum kernel .
more precisely , it is log d , but for small enough d ( which we have to assume anyway ) the computational effort is
exactly one memory access .
sonnenburg , rtsch , schfer and schlkopf
to add or lookup an element one only needs d operations to reach a leaf of the tree ( and to create necessary nodes on the way in an add operation ) .
note that the worst - case computational complexity of the operations is independent of the number of d - mers / elements stored in the tree .
while tries are not faster than sorted arrays in lookup and need considerably more storage ( e . g .
for pointers to its parent and siblings ) , they are useful for the previously discussed wd kernel .
here we not only have to lookup one substring u s d , but also all prexes of u .
for sorted arrays this amounts to d separate lookup operations , while for tries all prexes of u are already known when the bottom of the tree is reached .
in this case the trie has to store weights also on the internal nodes .
this is illustrated for the wd kernel in figure 123
figure 123 : three sequences aaa , aga , gaa with weights a 123 , a 123 & a 123 are added to the trie
gure displays the resulting weights at the nodes .
123 . 123 speeding up svm training
as it is not feasible to use standard optimization toolboxes for solving large scale svm train - ing problem , decomposition techniques are used in practice .
most chunking algorithms work by rst selecting q variables ( working set w ( 123 , .
, n ) , q : = |w | ) based on the current solution and then solve the reduced problem with respect to the working set variables .
these two steps are repeated until some optimality conditions are satised ( see e . g .
joachims ( 123 ) ) .
for se - lecting the working set and checking the termination criteria in each iteration , the vector g with gi = ( cid : 123 ) n i = 123 , .
, n is usually needed .
computing g from scratch in every iter - ation which would require o ( n123 ) kernel computations .
to avoid recomputation of g one typically starts with g = 123 and only computes updates of g on the working set w
jy jk ( xi , x j ) ,
i + ( cid : 123 )
j a old
j ) y jk ( xi , x j ) , i = 123 ,
large scale mkl
as a result the effort decreases to o ( qn ) kernel computations , which can be further speed up by using kernel caching ( e . g .
joachims , 123 ) .
however kernel caching is not efcient enough for large scale problems123 and thus most time is spend computing kernel rows for the updates of g on the working set w .
note however that this update as well as computing the q kernel rows can be easily parallelized; cf .
section 123 . 123
exploiting k ( xi , x j ) = hf ( xi ) , f ( x j ) i and w = ( cid : 123 ) n
iyif ( xi ) we can rewrite the update rule as
i + ( cid : 123 )
j a old
j ) y jhf ( xi ) , f ( x j ) i = gold
i + hww , f ( xi ) i ,
where ww = ( cid : 123 )
j a old
j ) y jf ( x j ) is the normal ( update ) vector on the working set .
if the kernel feature map can be computed explicitly and is sparse ( as discussed before ) , then computing the update in ( 123 ) can be accelerated .
one only needs to compute and store ww ( using j ( xq ) 123= 123 ) | add operations ) and performing the scalar product hww , f ( xi ) i the clear and ( cid : 123 ) qw | ( f
j ( xi ) 123= 123 ) | lookup operations ) .
depending on the kernel , the way the sparse vectors are stored section 123 . 123 and on the sparse - ness of the feature vectors , the speedup can be quite drastic .
for instance for the wd kernel one kernel computation requires o ( ld ) operations ( l is the length of the sequence ) .
hence , computing ( 123 ) n times requires o ( nqld ) operations .
when using tries , then one needs ql add operations ( each o ( d ) ) and nl lookup operations ( each o ( d ) ) .
therefore only o ( qld + nld ) basic opera - tions are needed in total .
when n is large enough it leads to a speedup by a factor of q .
finally note that kernel caching is no longer required and as q is small in practice ( e . g .
q = 123 ) the resulting trie has rather few leaves and thus only needs little storage .
the pseudo - code of our linadd svm chunking algorithm is given in algorithm 123
algorithm 123 outline of the chunking algorithm that exploits the fast computations of linear combi - nations of kernels ( e . g .
by tries ) .
gi = 123 , a ( loop until convergence ) for t = 123 , 123 ,
i = 123 for i = 123 ,
check optimality conditions and stop if optimal select working set w based on g and a solve reduced problem w and update a
, store a old = a
w w + ( a update gi = gi + hw , f ( xi ) i for all i = 123 , .
, n ( using lookup )
j ) y jf ( x j ) for all j w ( using add )
j a old
mkl case as elaborated in section 123 . 123 and algorithm 123 , for mkl one stores k vectors k = 123 , .
, k : one for each kernel in order to avoid full recomputation of g if a kernel weight b k is updated .
thus to use the idea above in algorithm 123 all one has to do is to store k normal vectors
for instance when using a million examples one can only t 123 rows into 123 gb .
moreover , caching 123 rows is
insufcient when for instance having many thousands of active variables .
sonnenburg , rtsch , schfer and schlkopf
k = ( cid : 123 )
j a old
j ) y jf
k = 123 ,
which are then used to update the k n matrix gk , i = gold i = 123 .
. n ) by which gi = ( cid : 123 )
k , i + hww k b kgk , i , ( for all i = 123 .
. n ) is computed .
k ( xi ) i ( for all k = 123 .
. k and
123 a simple parallel chunking algorithm
as still most time is spent in evaluating g ( x ) for all training examples further speedups are gained when parallelizing the evaluation of g ( x ) .
when using the linadd algorithm , one rst constructs the trie ( or any of the other possible more appropriate data structures ) and then performs parallel lookup operations using several cpus ( e . g .
using shared memory or several copies of the data structure on separate computing nodes ) .
we have implemented this algorithm based on multiple threads ( using shared memory ) and gain reasonable speedups ( see next section ) .
note that this part of the computations is almost ideal to distribute to many cpus , as only the ( or w depending on the communication costs and size ) have to be transfered before each
cpu computes a large chunk ik ( 123 , .
, n ) of
i = hw , f ( xi ) i , i ik , k = 123 , .
, n , where ( i123 in ) = ( 123 ,
which is transfered to a master node that nally computes g g + h , as illustrated in algorithm 123
results and discussion
in the following subsections we will rst apply multiple kernel learning to knowledge discovery tasks , demonstrating that it can be used for automated model selection and to interpret the learned model ( section 123 ) , followed by a benchmark comparing the running times of svms and mkl using any of the proposed algorithmic optimizations ( section 123 ) .
123 mkl for knowledge discovery
in this section we will discuss toy examples for binary classication and regression , showing that mkl can recover information about the problem at hand , followed by a brief review on problems for which mkl has been successfully used .
the rst example we deal with is a binary classication problem .
the task is to separate two concentric classes shaped like the outline of stars .
by varying the distance between the boundary of the stars we can control the separability of the problem .
starting with a non - separable scenario with zero distance , the data quickly becomes separable as the distance between the stars increases , and the boundary needed for separation will gradually tend towards a circle .
in figure 123 three scatter plots of data sets with varied separation distances are displayed .
we generate several training and test sets for a wide range of distances ( the radius of the inner star is xed at 123 , the outer stars radius is varied from 123 .
each data set contains 123 , 123 observations ( 123 , 123 positive and 123 , 123 negative ) using a moderate noise level ( gaussian noise with zero mean and standard deviation 123 ) .
the mkl - svm was trained for different values of the
large scale mkl
algorithm 123 outline of the parallel chunking algorithm that exploits the fast computations of linear combinations of kernels .
( master node ) gi = 123 , a ( loop until convergence ) for t = 123 , 123 ,
i = 123 for i = 123 ,
check optimality conditions and stop if optimal select working set w based on g and a solve reduced problem w and update a transfer to slave nodes : a for all j w fetch from n slave nodes : h = ( h ( 123 ) , .
, h ( n ) ) update gi = gi + hi for all i = 123 ,
j a old
, store a old = a
signal convergence to slave nodes
( slave nodes ) ( loop until convergence ) while not converged do
fetch from master node a w w + ( a j a old node k computes h ( k )
for all i = ( k 123 ) n transfer to master : h ( k )
j a old
for all j w
j ) y jf ( x j ) for all j w ( using add ) i = hw , f ( xi ) i
n 123 ( using lookup )
regularization parameter c , where we set e mkl = 123
for every value of c we averaged the test errors of all setups and choose the value of c that led to the smallest overall error ( c = 123 ) . 123
the choice of the kernel width of the gaussian rbf ( below , denoted by rbf ) kernel used for classication is expected to depend on the separation distance of the learning problem : an increased distance between the stars will correspond to a larger optimal kernel width .
this effect should be visible in the results of the mkl , where we used mkl - svms with ve rbf kernels with different widths ( 123s 123 ( 123 , 123 , 123 , 123 , 123 ) ) .
in figure 123 we show the obtained kernel weightings for the ve kernels and the test error ( circled line ) which quickly drops to zero as the problem becomes separable .
every column shows one mkl - svm weighting .
the courses of the kernel weightings reect the development of the learning problem : as long as the problem is difcult the best separation can be obtained when using the kernel with smallest width .
the low width kernel looses importance when the distance between the stars increases and larger kernel widths obtain a larger weight in mkl .
increasing the distance between the stars , kernels with greater widths are used .
note that the rbf kernel with largest width was not appropriate and thus never chosen .
this illustrates that mkl can indeed recover information about the structure of the learning problem .
note that we are aware of the fact that the test error might be slightly underestimated .
sonnenburg , rtsch , schfer and schlkopf
figure 123 : a 123 - class toy problem where the dark gray ( or green ) star - like shape is to be distinguished from the light gray ( or red ) star inside of the dark gray star .
the distance between the dark star - like shape and the light star increases from the left to the right .
we applied the newly derived mkl support vector regression formulation to the task of learning a sine function using three rbf - kernels with different widths ( 123s 123 ( 123 , 123 , 123 , 123 , 123 ) ) .
to this end , we generated several data sets with increasing frequency of the sine wave .
the sample size was chosen to be 123 , 123
analogous to the procedure described above we choose the value of c = 123 , minimizing the overall test error .
in figure 123 exemplarily three sine waves are depicted , where the frequency increases from left to right .
for every frequency the computed weights for each kernel width are shown .
one can see that mkl - sv regression switches to the width of the rbf - kernel tting the regression problem best .
in another regression experiment , we combined a linear function with two sine waves , one of lower frequency and one of high frequency , i . e .
f ( x ) = sin ( ax ) + sin ( bx ) + cx .
furthermore we increase the frequency of the higher frequency sine wave , i . e .
we varied a leaving b and c unchanged .
the mkl weighting should show a combination of different kernels .
using ten rbf - kernels of different width ( see figure 123 ) we trained a mkl - svr and display the learned weights ( a column in the gure ) .
again the sample size is 123 , 123 and one value for c = 123 is chosen via a previous experiment ( e mkl = 123 ) .
the largest selected width ( 123 ) models the linear component ( since rbf kernels with large widths are effectively linear ) and the medium width ( 123 ) corresponds to the lower frequency sine .
we varied the frequency of the high frequency sine wave from low to high ( left to right in the gure ) .
one observes that mkl determines an appropriate combination of kernels of low and high widths , while decreasing the rbf kernel width with increased frequency .
large scale mkl
figure 123 : mkl - support vector regression for the task of learning a sine wave ( please see text for
additionally one can observe that mkl leads to sparse solutions since most of the kernel weights in figure 123 are depicted in blue , that is they are zero . 123
123 . 123 real world applications in bioinformatics
mkl has been successfully used on real - world data sets in the eld of computational biology ( lanckriet et al . , 123; sonnenburg et al . , 123a ) .
it was shown to improve classication perfor - mance on the task of ribosomal and membrane protein prediction ( lanckriet et al . , 123 ) , where a weighting over different kernels each corresponding to a different feature set was learned .
in their result , the included random channels obtained low kernel weights .
however , as the data sets was rather small ( 123 , 123 examples ) the kernel matrices could be precomputed and simultaneously kept in memory , which was not possible in sonnenburg et al .
( 123a ) , where a splice site recognition task for the worm c .
elegans was considered .
here data is available in abundance ( up to one million ex - amples ) and larger amounts are indeed needed to obtain state of the art results ( sonnenburg et al . , 123b ) . 123 on that data set we were able to solve the classication mkl silp for n = 123 , 123 , 123 examples and k = 123 kernels , as well as for n = 123 , 123 examples and k = 123 kernels , using the linadd optimizations with the weighted degree kernel .
as a result we a ) were able to learn the instead of choosing a heuristic and b ) were able to use mkl as a tool for interpreting the svm classier as in sonnenburg et al .
( 123a ) ; rtsch et al .
( 123 ) .
as an example we learned the weighting of a wd kernel of degree 123 , which consist of a weighted sum of 123 sub - kernels each counting matching d - mers , for d = 123 , .
the learned
the training time for mkl - svr in this setup but with 123 , 123 examples was about 123 minutes , when kernel caches
of size 123mb are used .
in section 123 we will use a human splice data set containing 123 million examples , and train wd kernel based svm
classiers on up to 123 million examples using the parallelized linadd algorithm .
sonnenburg , rtsch , schfer and schlkopf
figure 123 : mkl support vector regression on a linear combination of three functions :
f ( x ) = sin ( ax ) + sin ( bx ) + cx .
mkl recovers that the original function is a combination of func - tions of low and high complexity .
for more details see text .
kernel index d ( length of substring )
figure 123 : the learned wd kernel weighting on a million of examples .
weighting is displayed in figure 123 and shows a peak for 123 - mers and 123&123 - mers .
it should be noted that the obtained weighting in this experiment is only partially useful for interpretation .
in the case of splice site detection , it is unlikely that k - mers of length 123 or 123 are playing the most important role .
more likely to be important are substrings of length up to six .
we believe that the large weights for the longest k - mers are an artifact which comes from the fact that we are combining kernels with
large scale mkl
quite different properties , i . e .
the 123th and 123th kernel leads to a combined kernel matrix that is most diagonally dominant ( since the sequences are only similar to themselves but not to other sequences ) , which we believe is the reason for having a large weight . 123
in the following example we consider one weight per position .
in this case the combined ker - nels are more similar to each other and we expect more interpretable results .
figure 123 shows an
123 123 123 123 123 exon
+123 +123 +123 +123 +123
position relative to the exon start
figure 123 : the gure shows an importance weighting for each position in a dna sequence ( around a so called splice site ) .
mkl was used to determine these weights , each corresponding to a sub - kernel which uses information at that position to discriminate splice sites from non - splice sites .
different peaks correspond to different biologically known signals ( see text for details ) .
we used 123 , 123 examples for training with 123 sub - kernels .
importance weighting for each position in a dna sequence ( around a so called acceptor splice site , the start of an exon ) .
we used mkl on 123 , 123 examples to compute these 123 weights , each cor - responding to a sub - kernel which uses information at that position to discriminate true splice sites from fake ones .
we repeated that experiment on ten bootstrap runs of the data set .
we can iden - tify several interesting regions that we can match to current biological knowledge about splice site recognition : a ) the region 123 nucleotides ( nt ) to 123nt , which corresponds to the donor splice site of the previous exon ( many introns in c .
elegans are very short , often only 123nt ) , b ) the region 123nt to 123nt that coincides with the location of the branch point , c ) the intronic region closest to the splice site with greatest weight ( 123nt to 123nt; the weights for the ag dimer are zero , since it appears in splice sites and decoys ) and d ) the exonic region ( 123nt to +123nt ) .
slightly surprising are the high weights in the exonic region , which we suspect only model triplet frequencies
this problem might be partially alleviated by including the identity matrix in the convex combination .
however as 123 - norm soft margin svms can be implemented by adding a constant to the diagonal of the kernel ( cortes and vapnik , 123 ) , this leads to an additional 123 - norm penalization .
sonnenburg , rtsch , schfer and schlkopf
decay of the weights seen from +123nt to +123nt might be explained by the fact that not all exons are actually long enough .
furthermore , since the sequence ends in our case at +123nt , the decay after +123nt is an edge effect as longer substrings cannot be matched .
123 benchmarking the algorithms
experimental setup to demonstrate the effect of the several proposed algorithmic optimiza - tions , namely the linadd svm training ( algorithm 123 ) and for mkl the silp formulation with and without the linadd extension for single , four and eight cpus , we applied each of the algo - rithms to a human splice site data set , 123 comparing it to the original wd formulation and the case where the weighting coefcients were learned using multiple kernel learning .
the splice data set contains 123 , 123 true acceptor splice site sequences and 123 , 123 , 123 decoys , leading to a total of 123 , 123 , 123 sequences each 123 base pairs in length .
it was generated following a procedure similar to the one in sonnenburg et al .
( 123a ) for c .
elegans which however contained only 123 , 123 , 123 examples .
note that the data set is very unbalanced as 123% of the examples are negatively la - beled .
we are using this data set in all benchmark experiments and trained ( mkl - ) svms using the shogun machine learning toolbox which contains a modied version of svmlight ( joachims , 123 ) on 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 and 123 , 123 , 123 randomly sub - sampled examples and measured the time needed in svm training .
for classication performance evaluation we always use the same re - maining 123 , 123 , 123 examples as a test data set .
we set the degree parameter to d = 123 for the wd kernel and to d = 123 for the spectrum kernel xing the svms regularization parameter to c = 123
thus in the mkl case also k = 123 sub - kernels were used .
svmlights subproblem size ( parameter qpsize ) , convergence criterion ( parameter epsilon ) and mkl convergence criterion were set to q = 123 , e sv m = 123 and e mkl = 123 , respectively .
a kernel cache of 123gb was used for all kernels except the precomputed kernel and algorithms using the linadd - smo extension for which the kernel - cache was disabled .
later on we measure whether changing the quadratic subproblem size q inuences svm training time .
experiments were performed on a pc powered by eight 123ghz amd opteron ( tm ) processors running linux .
we measured the training time for each of the algorithms ( single , quad or eight cpu version ) and data set sizes .
123 . 123 benchmarking svm
the obtained training times for the different svm algorithms are displayed in table 123 and in figure 123
first , svms were trained using standard svmlight with the weighted degree kernel precomputed ( wdpre ) , the standard wd kernel ( wd123 ) and the precomputed ( specpre ) and standard spectrum kernel ( spec ) .
then svms utilizing the linadd extension123 were trained using the wd ( linwd ) and spectrum ( linspec ) kernel .
finally svms were trained on four and eight cpus using the parallel version of the linadd algorithm ( linwd123 , linwd123 ) .
wd123 and wd123 demonstrate the effect of a simple parallelization strategy where the computation of kernel rows and updates on the working set are parallelized , which works with any kernel .
the training times obtained when precomputing the kernel matrix ( which includes the time needed to precompute the full kernel matrix ) is lower when no more than 123 , 123 examples are used .
the splice data set can be downloaded from http : / / www . fml . tuebingen . mpg . de / raetsch / projects / lsmkl .
more precisely the linadd and o ( l ) block formulation of the wd kernel as proposed in sonnenburg et al .
( 123b )
large scale mkl
note that this is a direct cause of the relatively large subproblem size q = 123
the picture is different for , say , q = 123 ( data not shown ) where the wdpre training time is in all cases larger than the times obtained using the original wd kernel demonstrating the effectiveness of svmlights kernel cache .
the overhead of constructing a trie on q = 123 examples becomes even more visible : only starting from 123 , 123 examples linadd optimization becomes more efcient than the original wd kernel algorithm as the kernel cache cannot hold all kernel elements anymore . 123 thus it would be appropriate to lower the chunking size q as can be seen in table 123
the linadd formulation outperforms the original wd kernel by a factor of 123 on a million examples .
the picture is similar for the spectrum kernel , here speedups of factor 123 on 123 , 123 examples are reached which stems from the fact that explicit maps ( and not tries as in the wd kernel case ) as discussed in section 123 . 123 could be used leading to a lookup cost of o ( 123 ) and a dramatically reduced map construction time .
for that reason the parallelization effort benets the wd kernel more than the spectrum kernel : on one million examples the parallelization using 123 cpus ( 123 cpus ) leads to a speedup of factor 123 ( 123 ) for the wd kernel , but only 123 ( 123 ) for the spectrum kernel .
thus parallelization will help more if the kernel computation is slow .
training with the original wd kernel with a sample size of 123 , 123 , 123 takes about 123 hours , the linadd version still requires 123 hours while with the 123 cpu parallel implementation only about 123 hours and in conjunction with the linadd optimization a single hour and 123 minutes are needed .
finally , training on 123 million examples takes about 123 days .
note that this data set is already 123gb in size .
classication performance figure 123 and table 123 show the classication performance in terms of classication accuracy , area under the receiver operator characteristic ( roc ) curve ( metz , 123; fawcett , 123 ) and the area under the precision recall curve ( prc ) ( see e . g .
davis and goadrich ( 123 ) ) of svms on the human splice data set for different data set sizes using the wd kernel .
recall the denition of the roc and prc curves : the sensitivity ( or recall ) is dened as the fraction of correctly classied positive examples among the total number of positive exam - ples , i . e .
it equals the true positive rate t pr = t p / ( t p + fn ) .
analogously , the fraction fpr = fp / ( t n + fp ) of negative examples wrongly classied positive is called the false positive rate .
plotting fpr against tpr results in the receiver operator characteristic curve ( roc ) metz ( 123 ) ; fawcett ( 123 ) .
plotting the true positive rate against the positive predictive value ( also precision ) ppv = t p / ( fp+t p ) , i . e .
the fraction of correct positive predictions among all positively predicted examples , one obtains the precision recall curve ( prc ) ( see e . g .
davis and goadrich ( 123 ) ) .
note that as this is a very unbalanced data set the accuracy and the area under the roc curve are almost meaningless , since both measures are independent of class ratios .
the more sensible auprc , how - ever , steadily increases as more training examples are used for learning .
thus one should train using all available data to obtain state - of - the - art results .
varying svmlights qpsize parameter as discussed in section 123 . 123 and algorithm 123 , using the linadd algorithm for computing the output for all training examples w . r . t .
to some working set can be speed up by a factor of q ( i . e .
the size of the quadratic subproblems , termed qpsize in svmlight ) .
however , there is a trade - off in choosing q as solving larger quadratic subproblems is expensive ( quadratic to cubic effort ) .
table 123 shows the dependence of the computing time from q and n .
for example the gain in speed between choosing q = 123 and q = 123 for 123 million of examples is 123% .
sticking with a mid - range q ( here q = 123 ) seems to be a good idea for this task .
however ,
when single precision 123 - byte oating point numbers are used , caching all kernel elements is possible when training
with up to 123 examples .
sonnenburg , rtsch , schfer and schlkopf
number of training examples ( logarithmic )
number of training examples ( logarithmic )
figure 123 : comparison of the running time of the different svm training algorithms using the weighted degree kernel .
note that as this is a log - log plot small appearing distances are large for larger n and that each slope corresponds to a different exponent .
in the upper gure the weighted degree kernel training times are measured , the lower gure displays spectrum kernel training times .
a large variance can be observed , as the svm training time depends to a large extend on which q variables are selected in each optimization step .
for example on the related c .
elegans splice data set q = 123 was optimal for large sample sizes while a midrange q = 123 lead to the overall best
large scale mkl
n wdpre wd123 wd123 wd123 linwd123 linwd123 linwd123
spec linspec123 linspec123 linspec123
table 123 : ( top ) speed comparison of the original single cpu weighted degree kernel algorithm ( wd123 ) in svmlight training , compared to the four ( wd123 ) and eight ( wd123 ) cpus par - allelized version , the precomputed version ( pre ) and the linadd extension used in con - junction with the original wd kernel for 123 , 123 and 123 cpus ( linwd123 , linwd123 , linwd123 ) .
( bottom ) speed comparison of the spectrum kernel without ( spec ) and with linadd ( lin - spec123 , linspec123 , linspec123using 123 , 123 and 123 processors ) .
specpredenotes the precomputed version .
the rst column shows the sample size n of the data set used in svm training while the following columns display the time ( measured in seconds ) needed in the training
performance .
nevertheless , one observes the trend that for larger training set sizes slightly larger subproblems sizes decrease the svm training time .
sonnenburg , rtsch , schfer and schlkopf
area under the roc area under the prc
number of training examples
figure 123 : comparison of the classication performance of the weighted degree kernel based svm classier for different training set sizes .
the area under the receiver operator charac - teristic ( roc ) curve , the area under the precision recall curve ( prc ) as well as the classication accuracy are displayed ( in percent ) .
note that as this is a very unbalanced data set , the accuracy and the area under the roc curve are less meaningful than the area under the prc .
123 . 123 benchmarking mkl
the wd kernel of degree 123 consist of a weighted sum of 123 sub - kernels each counting matching d - mers , for d = 123 , .
using mkl we learned the weighting on the splice site recognition task for one million examples as displayed in figure 123 and discussed in section 123 . 123
focusing on a speed comparison we now show the obtained training times for the different mkl algorithms applied to learning weightings of the wd kernel on the splice site classication task .
to do so , several mkl - svms were trained using precomputed kernel matrices ( premkl ) , kernel matrices which are computed on the y employing kernel caching ( mkl123 ) , mkl using the linadd extension ( linmkl123 ) and linadd with its parallel implementation123 ( linmkl123 and linmkl123 - on 123 and 123 cpus ) .
the results are displayed in table 123 and in figure 123
while precomputing kernel matrices seems benecial , it cannot be applied to large scale cases ( e . g .
> 123 , 123 examples ) due to the o ( kn123 ) memory constraints of storing the kernel matrices . 123 on - the - y - computation of the kernel matrices is computationally extremely demanding , but since kernel caching123 is used , it is still possible on 123 , 123 examples in about 123 hours .
note that no wd - kernel specic optimizations are involved here , so one expects a similar result for arbitrary kernels .
algorithm 123
algorithm 123 with the linadd extensions including parallelization of algorithm 123
using 123 kernels on 123 , 123 examples requires already 123gb , on 123 , 123 examples 123gb would be required ( both
using single precision oats ) .
each kernel has a cache of 123gb .
large scale mkl
n accuracy auroc auprc
table 123 : comparison of the classication performance of the weighted degree kernel based svm classier for different training set sizes .
the area under the roc curve ( auroc ) , the area under the precision recall curve ( auprc ) as well as the classication accuracy ( accuracy ) are displayed ( in percent ) .
larger values are better .
a optimal classier would achieve 123% note that as this is a very unbalanced data set the accuracy and the area under the roc curve are almost meaningless .
for comparison , the classication performance achieved using a 123th order markov chain on 123 million examples ( order 123 was chosen based on model selection , where order 123 to 123 using several pseudo - counts were tried ) is displayed in the last row ( marked ) .
the linadd variants outperform the other algorithms by far ( speedup factor 123 on 123 , 123 exam - ples ) and are still applicable to data sets of size up to one million .
note that without parallelization mkl on one million examples would take more than a week , compared with 123 ( 123 ) days in the quad - cpu ( eight - cpu ) version .
the parallel versions outperform the single processor version from the start achieving a speedup for 123 , 123 examples of 123 ( 123 ) , quickly reaching a plateau at a speedup factor of 123 ( 123 ) at a level of 123 , 123 examples and approaching a speedup factor of 123 ( 123 ) on 123 , 123 examples ( efciency : 123% ( 123% ) ) .
note that the performance gain using 123 cpus is relatively small as e . g .
solving the qp and constructing the tree is not parallelized .
in the rst part of the paper we have proposed a simple , yet efcient algorithm to solve the multiple kernel learning problem for a large class of loss functions .
the proposed method is able to exploit the existing single kernel algorithms , thereby extending their applicability .
in experiments we have illustrated that mkl for classication and regression can be useful for automatic model selection and for obtaining comprehensible information about the learning problem at hand .
it would be of interest to develop and evaluate mkl algorithms for unsupervised learning such as kernel pca
sonnenburg , rtsch , schfer and schlkopf
table 123 : inuence on training time when varying the size of the quadratic program q in svmlight , when using the linadd formulation of the wd kernel .
while training times do not vary dramatically one still observes the tendency that with larger sample size a larger q becomes optimal .
the q = 123 column displays the same result as column linwd123in table 123
mkl wd precompute mkl wd cache mkl wd linadd 123cpu mkl wd linadd 123cpu mkl wd linadd 123cpu
number of training examples ( logarithmic )
figure 123 : comparison of the running time of the different mkl algorithms when used with the weighted degree kernel .
note that as this is a log - log plot , small appearing distances are large for larger n and that each slope corresponds to a different exponent .
and one - class classication and to try different losses on the kernel weighting b ( such as l123 ) .
in the second part we proposed performance enhancements to make large scale mkl practical : the silp wrapper , silp chunking and ( for the special case of kernels that can be written as an inner product of sparse feature vectors , e . g . , string kernels ) the linadd algorithm , which also speeds up
large scale mkl
n premkl mkl linmkl123 linmkl123 linmkl123
table 123 : speed comparison when determining the wd kernel weight by multiple kernel learn - ing using the chunking algorithm ( mkl ) and mkl in conjunction with the ( parallelized ) linadd algorithm using 123 , 123 , and 123 processors ( linmkl123 , linmkl123 , linmkl123 ) .
the rst column shows the sample size n of the data set used in svm training while the fol - lowing columns display the time ( measured in seconds ) needed in the training phase .
standalone svm training .
for the standalone svm using the spectrum kernel we achieved speedups of factor 123 ( for the weighted degree kernel , about 123 ) .
for mkl we gained a speedup of factor 123
finally we proposed a parallel version of the linadd algorithm running on a 123 cpu multiprocessor system which lead to additional speedups of factor up to 123 for mkl , and 123 for vanilla svm
the authors gratefully acknowledge partial support from the pascal network of excellence ( eu #123 ) , dfg grants ja 123 / 123 - 123 and mu 123 / 123 - 123
we thank guido dornhege , olivier chapelle , cheng soon ong , joaquin quioero candela , sebastian mika , jason weston , manfred warmuth and k . - r .
mller for great discussions .
appendix a .
derivation of the mkl dual for generic loss functions
we start from the mkl primal problem equation ( 123 ) :
l ( f ( xi ) , yi )
w = ( w123 , .
, wk ) rd123 rdk
f ( xi ) =
k ( xi ) , wki + b , i = 123 ,
sonnenburg , rtsch , schfer and schlkopf
introducing u r allows us to move ( cid : 123 ) k
k=123 kwkk into the constraints and leads to the following
l ( f ( xi ) , yi )
u r , ( w123 , .
, wk ) rd123 rdk
k ( xi ) , wki + b , i = 123 ,
f ( xi ) =
using tk r , k = 123 , .
, k , it can be equivalently transformed into
l ( f ( xi ) , yi )
u r , tk r , wk rdk , k = 123 ,
f ( xi ) =
k ( xi ) , wki + b , i = 123 ,
recall that the second - order cone of dimensionality d is dened as
k d = ( ( x , c ) rd r , kxk123 c ) .
we can thus reformulate the original mkl primal problem ( equation ( 123 ) ) using the following equiv - alent second - order cone program , as the norm constraint on wk is implicitly taken care of :
l ( f ( xi ) , yi )
u r , tk r , ( wk , tk ) k dk , k = 123 ,
k ( xi ) , wki + b , i = 123 ,
f ( xi ) =
we are now going to derive the conic dual following the recipe of boyd and vandenberghe ( 123 ) ( see p .
first we derive the conic lagrangian and then using the inmum w . r . t .
the primal variables in order to obtain the conic dual .
we therefore introduce lagrange multipliers a rk , g r , g 123 and ( l k , k ) k d = k d .
then the conic
d living on the self dual cone k
large scale mkl
lagrangian is given as
l ( w , b , t , u , a
, g , l , ) =
l ( f ( xi ) , yi )
i f ( xi ) +
k ( xi ) , wki + b ) + g k ( cid : 123 )
( hl k , wki + ktk ) .
to obtain the dual , the derivatives of the lagrangian w . r . t .
the primal variables , w , b , t , u have to vanish which leads to the following constraints
wk l =
k ( xi ) l k l k =
tk l = g k g = k ul = u g g = u f ( xi ) l = l ( f ( xi ) , yi ) a
i f ( xi ) = l123 ( a
in the equation l is the derivative of the loss function w . r . t .
f ( x ) and l123 is the inverse of l ( w . r . t .
f ( x ) ) for which to exist l is required to be strictly convex and differentiable .
we now plug in what we have obtained above , which makes l k , k and all of the primal variables vanish .
thus the dual
, g ) =
i , yi ) , yi )
i , yi ) +
i , yi ) , yi )
as constraints remain g 123 , due to the bias ( cid : 123 ) n
i = 123 and the second - order cone constraints
this leads to :
g , k = 123 ,
kl kk = ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
g r , a rn
i , yi ) , yi )
g , k = 123 ,
sonnenburg , rtsch , schfer and schlkopf
squaring the latter constraint , multiplying by 123 as it is fullled implicitly , we obtain the mkl dual for arbitrary strictly convex loss functions .
g 123 123 g and dropping the g 123 constraint
123 , relabeling 123
i , yi ) , yi ) +
g r , a rn
g , k = 123 ,
finally adding the second term in the objective ( t ) to the constraint on g and relabeling g +t 123 g leads to the reformulated dual equation ( 123 ) , the starting point from which one can derive the silp formulation in analogy to the classication case .
appendix b .
loss functions
b . 123 quadratic loss for the quadratic loss case l ( x , y ) = c ( x y ) 123 we obtain as the derivative l ( x , y ) = 123c ( x y ) = : z and l123 ( z , y ) = 123
123c z + y for the inverse of the derivative .
recall the denition of
sk ( a ) =
i , yi ) , yi ) +
i , yi ) +
plugging in l , l123 leads to
sk ( a ) =
i + yi yi ) 123 +
i + yi ) +
l ( x , y ) =
123 + exy =
123 + e ( 123xy )
the inverse function for y 123= 123 and y + z 123= 123 is given by
l123 ( z , y ) =
y + z ( cid : 123 )
b . 123 logistic loss very similar to the hinge loss the derivation for the logistic loss l ( x , y ) = log ( 123 +exy ) will be given
large scale mkl
and nally we obtain
sk ( a ) =
yi + a
yi + a
b . 123 smooth hinge loss using the hinge loss l ( x , y ) = c
log ( 123 + es ( 123xy ) ) with s > 123 , y r xed , x r one obtains as
l ( x , y ) =
s cyes ( 123xy ) s ( 123 + es ( 123xy ) )
123 + es ( 123xy )
note that with y xed , z is bounded : 123 abs ( z ) abs ( cy ) and sign ( y ) = sign ( z ) and therefore
cy+z > 123 for cy + z 123= 123
the inverse function is derived as
z + ze ( cy + z ) e
s ( 123xy ) = cye s ( 123xy ) = z s ( 123xy ) =
s ( 123 xy ) = log (
cy + z
cy + z
123 xy =
l123 ( z , y ) =
cy + z
cy + z
cy + z
) ) , y 123= 123
dene c123 = 123
123 and c123 = ( cid : 123 ) n using these ingredients it follows for sk ( a )
log ( cid : 123 ) 123 + e
s ( cid : 123 ) 123 ( cid : 123 ) yi
cyi + a
cyi + a
) ( cid : 123 ) , yi ( cid : 123 ) +c123 +c123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) +c123 +c123
cyi + a
sk ( a ) =
