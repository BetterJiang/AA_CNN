principal component analysis ( pca ) is a powerful technique for extracting structure from possibly high - dimensional data sets .
it is readily performed by solving an eigenvalue problem or using iterative algorithms that estimate principal components ( for reviews of the existing literature , see jolliffe , 123 , and diamantaras & kung , 123 ) .
pca is an orthogonal transformation of the coordinate system in which we describe our data .
the new coordinate values by which we represent the data are called principal components .
it is often the case that a small number of principal components is sufcient to account for most of the structure in the data .
these are sometimes called factors or latent variables of the data .
we are interested not in principal components in input space but in prin - cipal components of variables , or features , which are nonlinearly related to the input variables .
among these are variables obtained by taking arbitrary higher - order correlations between input variables .
in the case of image anal - ysis , this amounts to nding principal components in the space of products of input pixels .
to this end , we are computing dot products in feature space by means of kernel functions in input space .
given any algorithm that can be expressed solely in terms of dot products ( i . e . , without explicit usage of the variables themselves ) , this kernel method enables us to construct different nonlinear c ( cid : 123 ) 123 massachusetts institute of technology
neural computation 123 , 123 ( 123 )
bernhard scholkopf , alexander smola , and klaus - robert m uller
versions of it ( aizerman , braverman , & rozonoer , 123; boser , guyon , & vapnik , 123 ) .
although this general fact was known ( burges , private com - munication ) , the machine learning community has made little use of it , the exception being support vector machines ( vapnik , 123 ) .
in this article , we give an example of applying this method in the domain of unsupervised learning , to obtain a nonlinear form of pca .
in the next section , we review the standard pca algorithm .
in order to be able to generalize it to the nonlinear case , we formulate it in a way that uses exclusively dot products .
in section 123 , we discuss the kernel method for computing dot products in feature spaces .
together , these two sections form the basis for section 123 , which presents the proposed kernel - based algo - rithm for nonlinear pca .
first experimental results on kernel - based feature extraction for pattern recognition are given in section 123
we conclude with a discussion ( section 123 ) and an appendix containing some technical material that is not essential for the main thread of the argument .
123 pca in feature spaces given a set of centered observations xk , k = 123 , .
, m , xk rn , pca diagonalizes the covariance matrix , 123
k=123 xk = 123 ,
to do this , one has to solve the eigenvalue equation ,
v = cv ,
for eigenvalues 123 and v rn\ ( 123 ) .
as cv = 123 ( xjv ) xj , all solutions v with ( cid : 123 ) = 123 must lie in the span of x123 , .
, xm; hence , equation 123 in that case is equivalent to
( xk v ) = ( xk cv ) for all k = 123 ,
in the remainder of this section , we describe the same computation in an - other dot product space f , which is related to the input space by a possibly
: rn f , x ( cid : 123 ) x .
123 more precisely , the covariance matrix is dened as the expectation of xx
; for conve - nience , we shall use the same term to refer to the estimate in equation 123 of the covariance matrix from a nite sample .
nonlinear component analysis
note that f , which we will refer to as the feature space , could have an arbitrarily large , possibly innite , dimensionality .
here and in the following , uppercase characters are used for elements of f , and lowercase characters denote elements of rn .
( xk ) = 123 ( we shall return to this point later ) .
using the covariance matrix
again , we assume that we are dealing with centered data , that is
( if f is innite dimensional , we think of ( xj ) ( xj ) ( cid : 123 ) as the linear operator that maps x f to ( xj ) ( ( xj ) x ) ) we now have to nd eigenvalues 123 and eigenvectors v f\ ( 123 ) satisfying ,
v = cv .
again , all solutions v with ( cid : 123 ) = 123 lie in the span of ( x123 ) , .
, ( xm ) .
for us , this has two useful consequences .
first , we may instead consider the set
( ( xk ) v ) = ( ( xk ) cv ) for all k = 123 ,
and , second , there exist coefcients i ( i = 123 , .
, m ) such that ,
combining equations 123 and 123 , we get
v = m ( cid : 123 )
i ( ( xk ) ( xi ) ) = 123
for all k = 123 , .
dening an m m matrix k by
kij : = ( ( xi ) ( xj ) ) ,
mk = k123 ,
bernhard scholkopf , alexander smola , and klaus - robert m uller
where denotes the column vector with entries 123 , .
to nd solutions of equation 123 , we solve the eigenvalue problem ,
m = k ,
for nonzero eigenvalues .
a justication of this procedure is given in ap - let 123 123 m denote the eigenvalues of k ( i . e . , the solutions m of equation 123 ) , and 123 , .
, m the corresponding complete set of eigenvectors , with p being the rst nonzero eigenvalue ( assuming ( cid : 123 ) 123 ) .
we normalize p , .
, m by requiring that the corresponding vectors in f be normalized , that is ,
( vk vk ) = 123 for all k = p ,
by virtue of equations 123 and 123 , this translates into a normalization condition for p ,
123 = m ( cid : 123 )
( ( xi ) ( xj ) ) = m ( cid : 123 )
= ( k kk ) = k ( k k ) .
for the purpose of principal component extraction , we need to compute projections onto the eigenvectors vk in f ( k = p , .
let x be a test point , with an image ( x ) in f; then
( vk ( x ) ) = m ( cid : 123 )
may be called its nonlinear principal components corresponding to .
in summary , the following steps were necessary to compute the principal components : ( 123 ) compute the matrix k , ( 123 ) compute its eigenvectors and normalize them in f , and ( 123 ) compute projections of a test point onto the
for the sake of simplicity , we have made the assumption that the obser - vations are centered .
this is easy to achieve in input space but harder in f , because we cannot explicitly compute the mean of the ( xi ) in f .
there is , however , a way to do it , and this leads to slightly modied equations for kernel - based pca ( see appendix b ) .
123 note that in our derivation we could have used the known result ( e . g . , kirby & sirovich , 123 ) that pca can be carried out on the dot product matrix ( xi xj ) ij instead of equation 123; however , for the sake of clarity and extendability ( in appendix b , we shall consider the question how to center the data in f ) , we gave a detailed derivation .
nonlinear component analysis
before we proceed to the next section , which more closely investigates the role of the map , the following observation is essential : can be an arbitrary nonlinear map into the possibly high - dimensional space f , for ex - ample , the space of all dth order monomials in the entries of an input vector .
in that case , we need to compute dot products of input vectors mapped by , at a possibly prohibitive computational cost .
the solution to this problem , described in the following section , builds on the fact that we exclusively need to compute dot products between mapped patterns ( in equations 123 and 123 ) ; we never need the mapped patterns explicitly .
123 computing dot products in feature spaces in order to compute dot products of the form ( ( x ) ( y ) ) , we use kernel
k ( x , y ) = ( ( x ) ( y ) ) ,
which allow us to compute the value of the dot product in f without having to carry out the map .
this method was used by boser et al .
( 123 ) to extend the generalized portrait hyperplane classier of vapnik and chervonenkis ( 123 ) to nonlinear support vector machines .
to this end , they substitute a priori chosen kernel functions k for all occurrences of dot products , obtaining
f ( x ) = sgn
ik ( x , xi ) + b
aizerman et al .
( 123 ) call f the linearization space , and use it in the context of the potential function classication method to express the dot product between elements of f in terms of elements of the input space .
if f is high - dimensional , we would like to be able to nd a closed - form expression for k that can be efciently computed .
aizerman et al .
( 123 ) consider the possi - bility of choosing k a priori , without being directly concerned with the cor - responding mapping into f .
a specic choice of k might then correspond to a dot product between patterns mapped with a suitable .
a particularly useful example , which is a direct generalization of a result proved by poggio ( 123 , lemma 123 ) in the context of polynomial approximation , is
( x y ) d =
= ( cd ( x ) cd ( y ) ) ,
bernhard scholkopf , alexander smola , and klaus - robert m uller
where cd maps x to the vector cd ( x ) whose entries are all possible dth degree ordered products of the entries of x .
for instance ( vapnik , 123 ) , if x = ( x123 , x123 ) , then c123 ( x ) = ( x123 , x123x123 , x123x123 ) , or , yielding the same value of the
123 ( x ) = ( x123
for this example , it is easy to verify that ( x y ) 123 = ( x123
123 y123y123 ) ( cid : 123 ) = ( 123 ( x ) 123 ( y ) ) .
in general , the function
k ( x , y ) = ( x y ) d
corresponds to a dot product in the space of dth - order monomials of the input coordinates .
if x represents an image with the entries being pixel values , we can thus easily work in the space spanned by products of any d pixelsprovided that we are able to do our work solely in terms of dot products , without any explicit use of a mapped pattern d ( x ) .
the latter lives in a possibly very high - dimensional space : even though we will identify terms like x123x123 and x123x123 into one coordinate of f , as in equation 123 , the d ! ( n123 ) ! and thus grows like nd .
for instance , 123 dimensionality of f still is ( n+d123 ) ! 123 pixel input images and a polynomial degree d = 123 yield a dimensionality of 123
thus , using kernels of the form in equation 123 is our only way to take into account higher - order statistics without a combinatorial explosion of time and memory complexity .
the general question that function k does correspond to a dot product in some space f has been discussed by boser et al .
( 123 ) and vapnik ( 123 ) : mercers theorem of functional analysis implies that if k is a continuous ker - nel of a positive integral operator , there exists a mapping into a space where k acts as a dot product ( for details , see appendix c ) .
besides equation 123 , radial basis functions ,
k ( x , y ) = exp
and sigmoid kernels ,
k ( x , y ) = tanh ( ( x y ) + ) ,
have been used in support vector machines .
these different kernels allow the construction of polynomial classiers , radial basis function classiers , and neural networks with the support vector algorithm , which exhibit very similar accuracy .
in addition , they all construct their decision functions from an almost identical subset of a small number of training patterns , the support vectors ( scholkopf , burges , & vapnik , 123 ) .
nonlinear component analysis
the application of equation 123 to our problem is straightforward .
we simply substitute an a priori chosen kernel function k ( x , y ) for all occur - rences of ( ( x ) ( y ) ) .
the choice of k then implicitly determines the mapping and the feature space f .
123 kernel pca
123 the algorithm .
to perform kernel - based pca ( see figure 123 ) , hence - forth referred to as kernel pca , the following steps have to be carried out .
first , we compute the matrix kij = ( k ( xi , xj ) ) ij .
next , we solve equation 123 by diagonalizing k and normalize the eigenvector expansion coefcients n by requiring n ( n n ) = 123
to extract the principal components ( corre - sponding to the kernel k ) of a test point x , we then compute projections onto the eigenvectors by ( cf .
equation 123 and figure 123 ) ,
( vn ( x ) ) = m ( cid : 123 )
i k ( xi , x ) .
if we use a kernel as described in section 123 , we know that this procedure exactly corresponds to standard pca in some high - dimensional feature space , except that we do not need to perform expensive computations in that space .
in practice , our algorithm is not equivalent to the form of nonlinear pca that can be obtained by explicitly mapping into the feature space f .
even though the rank of the matrix k is always limited by the sample size , we may not be able to compute this matrix if the dimensionality is prohibitively high .
in that case , using kernels is imperative .
123 properties of ( kernel ) pca .
if we use a kernel that satises the con - ditions given in section 123 , we know that we are in fact doing a standard pca in f .
consequently , all mathematical and statistical properties of pca ( see , e . g . , jolliffe , 123; diamantaras & kung , 123 ) carry over to kernel - based pca , with the modications that they become statements concerning f rather than rn .
in f , we can thus assert that pca is the orthogonal basis transformation with the following properties ( assuming that the eigenvec - tors are sorted in descending order of the eigenvalue size ) : ( 123 ) the rst q ( q ( 123 , .
, m ) ) principal components , that is , projections on eigenvectors , carry more variance than any other q orthogonal directions , ( 123 ) the mean - squared approximation error in representing the observations by the rst q principal components is minimal , ( 123 ) the principal components are un - correlated , and ( 123 ) the rst q principal components have maximal mutual information with respect to the inputs ( this holds under gaussian assump - tions , and thus depends on the data and the chosen kernel ) .
we conclude this section by noting one general property of kernel pca in input space : for kernels that depend on only dot products or distances
bernhard scholkopf , alexander smola , and klaus - robert m uller
k ( x , y ) = ( x . y )
k ( x , y ) = ( x . y ) d
figure 123 : the basic idea of kernel pca .
in some high - dimensional feature space f ( bottom right ) , we are performing linear pca , just like a pca in input space ( top ) .
since f is nonlinearly related to input space ( via ) , the contour lines of constant projections onto the principal eigenvector ( drawn as an arrow ) become nonlinear in input space .
note that we cannot draw a preimage of the eigenvector in input space , because it may not even exist .
crucial to kernel pca is the fact that there is no need to carry out the map into f .
all necessary computations are carried out by the use of a kernel function k in input space ( here : r123 ) .
in input space ( as all the examples that we have given so far do ) , kernel pca has the property of unitary invariance , following directly from the fact that both the eigenvalue problem and the feature extraction depend on only kernel values .
this ensures that the features extracted do not depend on which orthonormal coordinate system we use for representing our input
123 computational complexity .
a fth - order polynomial kernel on a 123 - dimensional input space yields a 123 - dimensional feature space .
for two reasons , kernel pca can deal with this huge dimensionality .
first , we do not need to look for eigenvectors in the full space f , but just in the sub - space spanned by the images of our observations xk in f .
second , we do not
nonlinear component analysis
i k ( xi , x )
( x ) ) = s a
sample x123 , x123 , x123 , . . .
input vector x
figure 123 : feature extraction architecture in kernel pca ( cf .
equation 123 ) .
in the rst layer , the input vector is compared to the sample via a kernel function , chosen a priori ( e . g . , polynomial , gaussian , or sigmoid ) .
the outputs are then linearly combined using weights , which are found by solving an eigenvector
need to compute dot products explicitly between vectors in f ( which can be impossible in practice , even if the vectors live in a lower - dimensional subspace ) because we are using kernel functions .
kernel pca thus is com - putationally comparable to a linear pca on ( cid : 123 ) observations with an ( cid : 123 ) ( cid : 123 ) dot product matrix .
if k is easy to compute , as for polynomial kernels , for example , the computational complexity is hardly changed by the fact that we need to evaluate kernel functions rather than just dot products .
further - more , when we need to use a large number ( cid : 123 ) of observations , we may want to work with an algorithm for computing only the largest eigenvalues , as , for instance , the power method with deation ( for a discussion , see dia - mantaras & kung , 123 ) .
in addition , we can consider using an estimate of the matrix k , computed from a subset of m < ( cid : 123 ) examples , while still extract - ing principal components from all ( cid : 123 ) examples ( this approach was chosen in some of our experiments described below ) .
the situation can be different for principal component extraction .
there , we have to evaluate the kernel function m times for each extracted principal component ( see equation 123 ) , rather than just evaluating one dot product as for a linear pca .
of course , if the dimensionality of f is 123 , this is still vastly faster than linear principal component extraction in f .
still , in some cases ( e . g . , if we were to extract principal components as a preprocessing step for classication ) , we might want to speed things up .
this can be done by a technique proposed by burges ( 123 ) in the context of support vector machines .
in the present setting , we approximate each eigenvector v =
i ( xi ) ( see equation 123 ) by another vector v = ( cid : 123 )
bernhard scholkopf , alexander smola , and klaus - robert m uller m < ( cid : 123 ) is chosen a priori according to the desired speedup , and zj rn , j = 123 , .
this is done by minimizing the squared difference = ( cid : 123 ) v v ( cid : 123 ) 123
the crucial point is that this also can be done without explicitly dealing with the possibly high - dimensional space f
= ( cid : 123 ) v ( cid : 123 ) 123 + m ( cid : 123 )
ijk ( zi , zj ) 123
the gradient of with respect to the j and the zj is readily expressed in terms of the kernel function; thus , can be minimized by gradient descent .
finally , although kernel principal component extraction is computation - ally more expensive than its linear counterpart , this additional investment can pay back afterward .
in experiments on classication based on the ex - tracted principal components , we found that when we trained on nonlinear features , it was sufcient to use a linear support vector machine to con - struct the decision boundary .
linear support vector machines , however , are much faster in classication speed than nonlinear ones .
this is due to the fact that for k ( x , y ) = ( x y ) , the support vector decision function ( see equa - f ( x ) = sgn ( ( x w ) + b ) .
thus the nal stage of classication can be done
tion 123 ) can be expressed with a single weight vector w = ( cid : 123 ) ( cid : 123 )
123 interpretability and variable selection .
in pca , it is sometimes de - sirable to be able to select specic axes that span the subspace into which one projects in doing principal component extraction .
in this way , it may , for instance , be possible to choose variables that are more accessible to interpre - tation .
in the nonlinear case , there is an additional problem : some directions in f do not have preimages in input space .
to make this plausible , note that the linear span of the training examples mapped into feature space can have dimensionality up to m ( the number of examples ) .
if this exceeds the di - mensionality of input space , it is rather unlikely that each vector of the form in equation 123 has a preimage .
to get interpretability , we thus need to nd directions in input space ( i . e . , input variables ) whose images under span the pca subspace in f .
this can be done with an approach akin to the one already described .
we could parameterize our set of desired input variables and run the minimization of equation 123 only over those parameters .
the parameters can be , for example , group parameters , which determine the amount of translation , say , starting from a set of images .
123 dimensionality reduction , feature extraction , and reconstruction .
unlike linear pca , the proposed method allows the extraction of a number of principal components that can exceed the input dimensionality .
suppose that the number of observations m exceeds the input dimensionality n .
lin - ear pca , even when it is based on the m m dot product matrix , can nd at
nonlinear component analysis
most n nonzero eigenvalues; they are identical to the nonzero eigenvalues of the n n covariance matrix .
in contrast , kernel pca can nd up to m nonzero eigenvaluesa fact that illustrates that it is impossible to perform kernel pca directly on an n n covariance matrix .
even more features could be extracted by using several kernels .
being just a basis transformation , standard pca allows the reconstruction of the original patterns xi , i = 123 , .
, ( cid : 123 ) , from a complete set of extracted principal components ( xi vj ) , j = 123 , .
, ( cid : 123 ) , by expansion in the eigenvector basis .
even from an incomplete set of components , good reconstruction is often possible .
in kernel pca , this is more difcult .
we can reconstruct the image of a pattern in f from its nonlinear components; however , if we have only an approximate reconstruction , there is no guarantee that we can nd an exact preimage of the reconstruction in input space .
in that case , we would have to resort to an approximation method ( cf .
equation 123 ) .
alternatively , we could use a suitable regression method for estimating the reconstruction mapping from the kernel - based principal components to the
123 toy examples .
to provide some insight into how pca in f be - haves in input space , we show a set of experiments with an articial two - dimensional data set , using polynomial kernels ( cf .
equation 123 ) of degree 123 through 123 ( see figure 123 ) .
linear pca ( on the left ) leads to only two nonzero eigenvalues , as the input dimensionality is 123
in contrast , nonlinear pca al - lows the extraction of further components .
in the gure , note that nonlinear pca produces contour lines ( of constant feature value ) , which reect the structure in the data better than in linear pca .
in all cases , the rst principal component varies monotonically along the parabola underlying the data .
in the nonlinear cases , the second and the third components show behav - ior that is similar for different polynomial degrees .
the third component , which comes with small eigenvalues ( rescaled to sum to 123 ) , seems to pick up the variance caused by the noise , as can be nicely seen in the case of degree 123
dropping this component would thus amount to noise reduction .
further toy examples , using radial basis function kernels ( see equation 123 ) and neural networktype sigmoid kernels ( see equation 123 ) , are shown in figures 123 and 123
123 character recognition .
in this experiment , we extracted nonlinear principal components from a handwritten character database , using ker - nel pca in the form given in appendix b .
we chose the us postal service ( usps ) database of handwritten digits collected from mail envelopes in buf - falo .
this database contains 123 examples of dimensionality 123; 123 of them make up the test set .
for computational reasons , we decided to use a subset of 123 training examples for the matrix k .
to assess the utility of
bernhard scholkopf , alexander smola , and klaus - robert m uller
figure 123 : two - dimensional toy example , with data generated in the following way : x values have uniform distribution in ( 123 , 123 ) , y values are generated from + , where is normal noise with standard deviation 123 .
from left to yi = x123 right , the polynomial degree in the kernel ( see equation 123 ) increases from 123 to 123; from top to bottom , the rst three eigenvectors are shown in order of decreasing eigenvalue size .
the gures contain lines of constant principal component value ( contour lines ) ; in the linear case , these are orthogonal to the eigenvectors .
we did not draw the eigenvectors; as in the general case , they live in a higher - dimensional feature space .
the components , we trained a soft margin hyperplane classier ( vapnik & chervonenkis , 123; cortes & vapnik , 123 ) on the classication task .
this is a special case of support vector machines , using the standard dot prod - uct as a kernel function .
it simply tries to separate the training data by a hyperplane with large margin .
table 123 illustrates two advantages of using nonlinear kernels .
first , per - formance of a linear classier trained on nonlinear principal components is better than for the same number of linear components; second , the perfor - mance for nonlinear components can be further improved by using more components than is possible in the linear case .
the latter is related to the fact that there are many more higher - order features than there are pixels in an image .
regarding the rst point , note that extracting a certain num - ber of features in a 123 - dimensional space constitutes a much higher re - duction of dimensionality than extracting the same number of features in 123 - dimensional input space .
nonlinear component analysis
figure 123 : two - dimensional toy example with three data clusters ( gaussians with standard deviation 123 , depicted region : ( 123 , 123 ) ( 123 , 123 ) ) : rst eight nonlinear principal components extracted with k ( x , y ) = exp ( ( cid : 123 ) xy ( cid : 123 ) 123 ) .
note that the rst two principal components ( top left ) nicely separate the three clusters .
compo - nents 123 split up the clusters into halves .
similarly , components 123 split them again , in a way orthogonal to the above splits .
thus , the rst eight components divide the data into 123 regions .
the matlab code used for generating this gure can be obtained from http : / / svm . rst . gmd . de .
figure 123 : two - dimensional toy example with three data clusters ( gaussians with standard deviation 123 , depicted region : ( 123 , 123 ) ( 123 , 123 ) ) : rst three nonlinear principal components extracted with k ( x , y ) = tanh .
the rst two principal components ( top left ) are sufcient to separate the three clusters , and the third component splits the clusters into halves .
123 ( x y ) + 123
for all numbers of features , the optimal degree of kernels to use is around 123 , which is compatible with support vector machine results on the same data set ( scholkopf , burges , & vapnik , 123 ) .
moreover , with only one exception , the nonlinear features are superior to their linear counterparts .
the resulting error rate for the best of our classiers ( 123% ) is competitive with convolu - tional ve - layer neural networks ( 123% were reported by lecun et al . , 123 ) and nonlinear support vector classiers ( 123% , scholkopf , burges , & vapnik , 123 ) ; it is much better than linear classiers operating directly on the image data ( a linear support vector machine achieves 123%; scholkopf , burges , & vapnik , 123 ) .
these encouraging results have been reproduced on an object recognition task ( scholkopf , smola , & m uller , 123 ) .
bernhard scholkopf , alexander smola , and klaus - robert m uller
table 123 : test error rates on the usps handwritten digit database .
number of components
test error rate for degree
note : linear support vector machines were trained on nonlinear principal com - ponents extracted by pca with kernel ( 123 ) , for degrees 123 through 123
in the case of degree 123 , we are doing standard pca , with the number of nonzero eigenval - ues being at most the dimensionality of the space , 123
clearly , nonlinear principal components afford test error rates that are superior to the linear case ( degree 123 ) .
123 feature extraction for classication .
this article presented a new technique for nonlinear pca .
to develop this technique , we made use of a kernel method so far used only in supervised learning ( vapnik , 123 ) .
kernel pca constitutes a rst step toward exploiting this technique for a large class of algorithms .
in experiments comparing the utility of kernel pca features for pattern recognition using a linear classier , we found two advantages of nonlin - ear kernels .
first , nonlinear principal components afforded better recog - nition rates than corresponding numbers of linear principal components; and , second , the performance for nonlinear components can be improved by using more components than is possible in the linear case .
we have not yet compared kernel pca to other techniques for nonlinear feature extrac - tion and dimensionality reduction .
we can , however , compare results with other feature extraction methods used in the past by researchers working on the usps classication problem .
our system of kernel pca feature ex - traction plus linear support vector machine , for instance , performed better than lenet123 ( lecun et al . , 123 ) .
although the latter result was obtained a number of years ago , lenet123 nevertheless provides an architecture that contains a great deal of prior information about the handwritten character classication problem .
it uses shared weights to improve transformation invariance and a hierarchy of feature detectors resembling parts of the hu - man visual system .
in addition , our features were extracted without taking into account that we want to do classication .
clearly , in supervised learn - ing , where we are given a set of labeled observations ( x123 , y123 ) , .
, ( x ( cid : 123 ) , y ( cid : 123 ) ) , it
nonlinear component analysis
would seem advisable to make use of the labels not only during the training of the nal classier but also in the stage of feature extraction .
finally , we note that a similar approach can be taken in the case of re -
123 feature space and the curse of dimensionality .
we are doing pca in 123 - dimensional feature spaces , yet getting results in nite time that are comparable to state - of - the - art techniques .
in fact , however , we are not working in the full feature space , but in a comparably small linear subspace of it , whose dimension equals at most the number of observations .
the method automatically chooses this subspace and provides a means of tak - ing advantage of the lower dimensionality .
an approach that consisted in explicitly of mapping into feature space and then performing pca would have severe difculties at this point .
even if pca was done based on an m m dot product matrix ( m being the sample size ) , whose diagonaliza - tion is tractable , it would still be necessary to evaluate dot products in a 123 - dimensional feature space to compute the entries of the matrix in the rst place .
kernel - based methods avoid this problem; they do not explicitly compute all dimensions of f ( loosely speaking , all possible features ) , but work only in a relevant subspace of f .
123 comparison to other methods for nonlinear pca .
starting from some of the properties characterizing pca ( see above ) , it is possible to de - velop a number of possible generalizations of linear pca to the nonlinear case .
alternatively , one may choose an iterative algorithm that adaptively estimates principal components and make some of its parts nonlinear to extract nonlinear features .
rather than giving a full review of this eld here , we briey describe ve approaches and refer readers to diamantaras and kung ( 123 ) for more
123 . 123 hebbian networks .
initiated by the pioneering work of oja ( 123 ) , a number of unsupervised neural network algorithms computing principal components have been proposed .
compared to the standard approach of diagonalizing the covariance matrix , they have advantagesfor instance , when the data are nonstationary .
nonlinear variants of these algorithms are obtained by adding nonlinear activation functions .
the algorithms then extract features that the authors have referred to as nonlinear principal components .
these approaches , however , do not have the geometrical in - terpretation of kernel pca as a standard pca in a feature space nonlinearly related to input space , and it is thus more difcult to understand what ex - actly they are extracting .
123 . 123 autoassociative multilayer perceptrons .
consider a linear three - layer perceptron with a hidden layer smaller than the input .
if we train
bernhard scholkopf , alexander smola , and klaus - robert m uller
it to reproduce the input values as outputs ( i . e . , use it in autoassociative mode ) , then the hidden unit activations form a lower - dimensional repre - sentation of the data , closely related to pca ( see , for instance , diamantaras & kung , 123 ) .
to generalize to a nonlinear setting , one uses nonlinear acti - vation functions and additional layers . 123 while this can be considered a form of nonlinear pca , the resulting network training consists of solving a hard nonlinear optimization problem , with the possibility of getting trapped in local minima , and thus with a dependence of the outcome on the starting point of the training .
moreover , in neural network implementations , there is often a risk of getting overtting .
another drawback of neural approaches to nonlinear pca is that the number of components to be extracted has to be specied in advance .
as an aside , note that hyperbolic tangent kernels can be used to extract neural networktype nonlinear features using kernel pca ( see figure 123 ) .
the principal components of a test point x in that case take the form ( see figure 123 )
i tanh ( ( xi , x ) + ) .
123 . 123 principal curves .
an approach with a clear geometric interpreta - tion in input space is the method of principal curves ( hastie & stuetzle , 123 ) , which iteratively estimates a curve ( or surface ) capturing the struc - ture of the data .
the data are mapped to the closest point on a curve , and the algorithm tries to nd a curve with the property that each point on the curve is the average of all data points projecting onto it .
it can be shown that the only straight lines satisfying the latter are principal components , so principal curves are indeed a generalization of the latter .
to compute principal curves , a nonlinear optimization problem has to be solved .
the dimensionality of the surface , and thus the number of features to extract , is specied in advance .
123 . 123 locally linear pca .
in cases where a linear pca fails because the dependences in the data vary nonlinearly with the region in input space , it can be fruitful to use an approach where linear pca is applied locally ( e . g . , bregler & omohundro , 123 ) .
possibly kernel pca could be improved by taking locality into account .
123 . 123 kernel pca .
kernel pca is a nonlinear generalization of pca in the sense that it is performing pca in feature spaces of arbitrarily large ( possibly innite ) dimensionality , and if we use the kernel k ( x , y ) = ( x y ) , we recover standard pca .
compared to the above approaches , kernel pca has the main advantage that no nonlinear optimization is involved; it is
123 simply using nonlinear activation functions in the hidden layer would not sufce .
the linear activation functions already lead to the best approximation of the data ( given the number of hidden nodes ) , so for the nonlinearities to have an effect on the components , the architecture needs to be changed to comprise more layers ( see , e . g . , diamantaras &
nonlinear component analysis
essentially linear algebra , as simple as standard pca .
in addition , we need not specify the number of components that we want to extract in advance .
compared to neural approaches , kernel pca could be disadvantageous if we need to process a very large number of observations , because this results in a large matrix k .
compared to principal curves , kernel pca is harder to interpret in input space; however , at least for polynomial kernels , it has a very clear interpretation in terms of higher - order features .
compared to other techniques for nonlinear feature extraction , kernel pca has the advantages that it requires only the solution of an eigenvalue prob - lem , not nonlinear optimization , and by the possibility of using different kernels , it comprises a fairly general class of nonlinearities that can be used .
clearly the last point has yet to be evaluated in practice; however , for the support vector machine , the utility of different kernels has already been established .
different kernels ( polynomial , sigmoid , gaussian ) led to ne classication performances ( scholkopf , burges , & vapnik , 123 ) .
the gen - eral question of how to select the ideal kernel for a given task ( i . e . , the appropriate feature space ) , however , is an open problem .
the scene has been set for using the kernel method to construct a wide va - riety of rather general nonlinear variants of classical algorithms .
it is beyond our scope here to explore all the possibilities , including many distance - based algorithms , in detail .
some of them are currently being investigatedfor instance , nonlinear forms of k - means clustering and kernel - based indepen - dent component analysis ( scholkopf , smola , & m uller , 123 ) .
linear pca is being used in numerous technical and scientic applica - tions , including noise reduction , density estimation , image indexing and retrieval systems , and the analysis of natural image statistics .
kernel pca can be applied to all domains where traditional pca has so far been used for feature extraction and where a nonlinear extension would make sense .
appendix a : the eigenvalue problem in the space of expansion
being symmetric , k has an orthonormal basis of eigenvectors ( i ) i with cor - responding eigenvalues i; thus , for all i , we have ki = ii ( i = 123 , .
eigenvector basis as = ( cid : 123 ) to understand the relation between equations 123 and 123 , we proceed as follows .
first , suppose , satisfy equation 123 .
we may expand in ks i aiii = i .
this in turn means that for all i = 123 ,
i=123 aii .
equation 123 then reads m i i , or , equivalently , for all i = 123 , .
, m , maii = ai123
m = i or ai = 123 or i = 123
bernhard scholkopf , alexander smola , and klaus - robert m uller
note that the above are not exclusive ors .
we next assume that , satisfy equation 123 , to carry out a similar derivation .
in that case , we nd that i aiii , that is , for all i = equation 123 is equivalent to m 123 ,
i aii = ( cid : 123 )
m = i or ai = 123
comparing equations a . 123 and a . 123 , we see that all solutions of the latter satisfy the former .
however , they do not give its full set of solutions : given a solution of equation 123 , we may always add multiples of eigenvectors of k with eigenvalue 123 and still satisfy equation 123 , with the same eigenvalue .
this means that there exist solutions of equation 123 that belong to different eigenvalues yet are not orthogonal in the space of the k .
it does not mean , however , that the eigenvectors of c in f are not orthogonal .
indeed , if is an eigenvector of k with eigenvalue 123 , then the corresponding vector i ( xi ) is orthogonal to all vectors in the span of the ( xj ) in f , since i ( xi ) = 123
thus , the above difference between the solutions of equations 123 and 123 is irrelevant , since we are interested in vectors in f rather than vectors in the space of the expansion coefcients of equation 123 .
we thus only need to diagonalize k to nd all relevant solutions of equation 123 .
i ( xi ) ) = ( k ) j = 123 for all j , which means that
appendix b : centering in high - dimensional space
given any and any set of observations x123 , .
, xm , the points
( xi ) : = ( xi ) 123
are centered .
thus , the assumptions of section 123 now hold , and we go on to dene covariance matrix and kij = ( ( xi ) ( xj ) ) in f .
we arrive at the already familiar eigenvalue problem ,
the points in equation b . 123 , v = ( cid : 123 )
with being the expansion coefcients of an eigenvector ( in f ) in terms of ( xi ) .
because we do not have the centered data ( see equation b . 123 ) , we cannot compute k directly; however , we can express it in terms of its noncentered counterpart k .
in the following , we shall use kij = ( ( xi ) ( xj ) ) and the notations 123ij = 123 for all i , j , ( 123m ) ij : = 123 / m , to compute kij = ( ( xi ) ( xj ) ) :
( xm ) ) ( ( xj ) 123
nonlinear component analysis
= kij 123 = ( k 123mk k123m + 123mk123m ) ij .
kin123nj + 123
( vk ( t ) ) = m ( cid : 123 )
we thus can compute k from k and then solve the eigenvalue problem ( see equation b . 123 ) .
as in equation 123 , the solutions k are normalized by normalizing the corresponding vectors vk in f , which translates into k ( k k ) = 123
for feature extraction , we compute projections of centered - images of test patterns t onto the eigenvectors of the covariance matrix of the centered points ,
( ( xi ) ( t ) ) .
consider a set of test points t123 , .
, tl , and dene two l m matrices = ( ( ti ) ( xj ) ) and ktest ( xn ) ) ) .
as in equation b . 123 , we express ktest in terms of ktest , and arrive at ktest = ktest123 m is the l m matrix mk123m , where 123 with all entries equal to 123 / m .
= ( ( ( ti ) 123
appendix c : mercer kernels
mercers theorem of functional analysis ( e . g . , courant & hilbert , 123 ) gives conditions under which we can construct the mapping from the eigen - function decomposition of k .
if k is the continuous kernel of an integral k ( x , y ) f ( x ) dx , which is positive , that is ,
operator k : l123 l123 , ( k f ) ( y ) = ( cid : 123 )
f ( x ) k ( x , y ) f ( y ) dx dy 123 for all
then k can be expanded into a uniformly convergent series ,
k ( x , y ) =
: x ( cid : 123 ) (
with i 123
in this case ,
is a map into f such that k acts as the given dot product , that is , ( ( x ) ( y ) ) =
123 ( x ) ,
although formulated originally for the case where the integral operator acts on functions f from l123 ( ( a , b ) ) , mercers theorem also holds if f is dened on a space of arbitrary dimensionality , provided that it is compact ( e . g . , dunford & schwartz , 123 ) .
bernhard scholkopf , alexander smola , and klaus - robert m uller
were supported by grants from the studienstiftung des deuts - chen volkes .
thanks the gmd first for hospitality during two visits .
thank v .
vapnik for introducing them to kernel representations of dot products during joint work on support vector machines .
thanks to at&t and bell laboratories for letting us use the usps database and to l .
bottou , c .
burges , and c .
cortes for parts of the soft margin hyperplane training code .
this work proted from discussions with v .
blanz , l .
bottou , c .
burges , h .
b ulthoff , p .
haffner , y .
le cun , s .
mika , n .
murata , p .
simard , s .
solla , v .
vapnik , and t .
vetter .
we are grateful to v .
blanz , c .
burges , and s .
solla for reading a preliminary version of the article .
