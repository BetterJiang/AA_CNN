we consider the scenario where training and test data are drawn from different distributions , commonly referred to as sample selection bias .
most algorithms for this setting try to rst recover sampling distributions and then make appro - priate corrections based on the distribution estimate .
we present a nonparametric method which directly produces resampling weights without distribution estima - tion .
our method works by matching distributions between training and testing sets in feature space .
experimental results demonstrate that our method works well in practice .
the default assumption in many learning scenarios is that training and test data are independently and identically ( iid ) drawn from the same distribution .
when the distributions on training and test set do not match , we are facing sample selection bias or covariate shift .
specically , given a domain of patterns x and labels y , we obtain training samples z = ( ( x123 , y123 ) , .
, ( xm , ym ) ) x y from a borel probability distribution pr ( x , y ) , and test samples z = ( ( x123 , y123 ) , .
, ( xm , ym ) ) x y drawn from another such distribution pr ( x , y ) .
although there exists previous work addressing this problem ( 123 , 123 , 123 , 123 , 123 , 123 , 123 ) , sample selection bias is typically ignored in standard estimation algorithms .
nonetheless , in reality the problem occurs rather frequently : while the available data have been collected in a biased manner , the test is usually performed over a more general target population .
below , we give two examples; but similar situations occur in many other domains .
suppose we wish to generate a model to diagnose breast cancer .
suppose , moreover , that most women who participate in the breast screening test are middle - aged and likely to have attended the screening in the preceding 123 years .
consequently our sample includes mostly older women and those who have low risk of breast cancer because they have been tested before .
the examples do not reect the general population with respect to age ( which amounts to a bias in pr ( x ) ) and they only contain very few diseased cases ( i . e .
a bias in pr ( y|x ) ) .
gene expression prole studies using dna microarrays are used in tumor diagnosis .
a common problem is that the samples are obtained using certain protocols , microarray platforms and analysis techniques .
in addition , they typically have small sample sizes .
the test cases are recorded under different conditions , resulting in a different distribution of gene expression values .
in this paper , we utilize the availability of unlabeled data to direct a sample selection de - biasing procedure for various learning methods .
unlike previous work we infer the resampling weight di - rectly by distribution matching between training and testing sets in feature space in a non - parametric
manner .
we do not require the estimation of biased densities or selection probabilities ( 123 , 123 , 123 ) , or the assumption that probabilities of the different classes are known ( 123 ) .
rather , we account for the difference between pr ( x , y ) and pr ( x , y ) by reweighting the training points such that the means of the training and test points in a reproducing kernel hilbert space ( rkhs ) are close .
we call this reweighting process kernel mean matching ( kmm ) .
when the rkhs is universal ( 123 ) , the popula - tion solution to this miminisation is exactly the ratio pr ( x , y ) / pr ( x , y ) ; however , we also derive a cautionary result , which states that even granted this ideal population reweighting , the convergence of the empirical means in the rkhs depends on an upper bound on the ratio of distributions ( but not on the dimension of the space ) , and will be extremely slow if this ratio is large .
the required optimisation is a simple qp problem , and the reweighted sample can be incorpo - rated straightforwardly into several different regression and classication algorithms .
we apply our method to a variety of regression and classication benchmarks from uci and elsewhere , as well as to classication of microarrays from prostate and breast cancer patients .
these experiments demon - strate that kmm greatly improves learning performance compared with training on unweighted data , and that our reweighting scheme can in some cases outperform reweighting using the true sample key assumption 123 : in general , the estimation problem with two different distributions pr ( x , y ) and pr ( x , y ) is unsolvable , as the two terms could be arbitrarily far apart .
in particular , for arbi - trary pr ( y|x ) and pr ( y|x ) , there is no way we could infer a good estimator based on the training sample .
hence we make the simplifying assumption that pr ( x , y ) and pr ( x , y ) only differ via pr ( x , y ) = pr ( y|x ) pr ( x ) and pr ( y|x ) pr ( x ) .
in other words , the conditional probabilities of y|x remain unchanged ( this particular case of sample selection bias has been termed covariate shift ( 123 ) ) .
however , we will see experimentally that even in situations where our key assumption is not valid , our method can nonetheless perform well ( see section 123 ) .
123 sample reweighting
we begin by stating the problem of regularized risk minimization .
in general a learning method minimizes the expected risk
r ( pr , , l ( x , y , ) ) = e ( x , y ) pr ( l ( x , y , ) )
of a loss function l ( x , y , ) that depends on a parameter .
for instance , the loss function could be the negative log - likelihood log pr ( y|x , ) , a misclassication loss , or some form of regression loss .
however , since typically we only observe examples ( x , y ) drawn from pr ( x , y ) rather than pr ( x , y ) , we resort to computing the empirical average
remp ( z , , l ( x , y , ) ) =
l ( xi , yi , ) .
to avoid overtting , instead of minimizing remp directly we often minimize a regularized variant rreg ( z , , l ( x , y , ) ) : = remp ( z , , l ( x , y , ) ) + ( ) , where ( ) is a regularizer .
123 sample correction
the problem is more involved if pr ( x , y ) and pr ( x , y ) are different .
the training set is drawn from pr , however what we would really like is to minimize r ( pr , , l ) as we wish to generalize to test examples drawn from pr .
an observation from the eld of importance sampling is that l ( x , y , ) i
r ( pr , , l ( x , y , ) ) = e ( x , y ) pr ( l ( x , y , ) ) = e ( x , y ) prh pr ( x , y ) | ( z )
= r ( pr , , ( x , y ) l ( x , y , ) ) ,
provided that the support of pr is contained in the support of pr .
given ( x , y ) , we can thus compute the risk with respect to pr using pr .
similarly , we can estimate the risk with respect to pr by computing remp ( z , , ( x , y ) l ( x , y , ) ) .
the key problem is that the coefcients ( x , y ) are usually unknown , and we need to estimate them from the data .
when pr and pr differ only in pr ( x ) and pr ( x ) , we have ( x , y ) = pr ( x ) / pr ( x ) , where is a reweighting factor for the training examples .
we thus reweight every observation
( x , y ) such that observations that are under - represented in pr obtain a higher weight , whereas over - represented cases are downweighted .
now we could estimate pr and pr and subsequently compute based on those estimates .
this is closely related to the methods in ( 123 , 123 ) , as they have to either estimate the selection probabilities or have prior knowledge of the class distributions .
although intuitive , this approach has two major problems : rst , it only works whenever the density estimates for pr and pr ( or potentially , the se - lection probabilities or class distributions ) are good .
in particular , small errors in estimating pr can lead to large coefcients and consequently to a serious overweighting of the corresponding obser - vations .
second , estimating both densities just for the purpose of computing reweighting coefcients may be overkill : we may be able to directly estimate the coefcients i : = ( xi , yi ) without having to estimate the two distributions .
furthermore , we can regularize i directly with more exibility , taking prior knowledge into account similar to learning methods for other problems .
123 using the sample reweighting in learning algorithms
before we describe how we will estimate the reweighting coefcients i , let us briey discuss how to minimize the reweighted regularized risk
rreg ( z , , l ( x , y , ) ) : =
il ( xi , yi , ) + ( ) ,
in the classication and regression settings ( an additional classication method is discussed in the accompanying technical report ( 123 ) ) .
support vector classication : utilizing the setting of ( 123 ) we can have the following minimization problem ( the original svms can be formulated in the same way ) :
123 kk123 + c
subject to h ( xi , yi ) ( xi , y ) , i 123 i / ( yi , y ) for all y y , and i 123
here , ( x , y ) is a feature map from x y into a feature space f , where f and ( y , y ) denotes a discrepancy function between y and y .
the dual of ( 123 ) is given by
iyjy k ( xi , y , xj , y )
subject to iy 123 for all i , y and x
iy / ( yi , y ) ic .
here k ( x , y , x , y ) : = h ( x , y ) , ( x , y ) i denotes the inner product between the feature maps .
this generalizes the observation - dependent binary sv classication described in ( 123 ) .
modications of existing solvers , such as svmstruct ( 123 ) , are straightforward .
penalized lms regression : assume l ( x , y , ) = ( y h ( x ) , i ) 123 and ( ) = kk123
here we
i ( yi h ( xi ) , i ) 123 + kk123 .
denote by the diagonal matrix with diagonal ( 123 , .
, m ) and let k rmm be the kernel matrix kij = k ( xi , xj ) .
in this case minimizing ( 123 ) is equivalent to minimizing ( y k ) ( y k ) + k with respect to .
assuming that k and have full rank , the minimization yields = ( 123 + k ) 123y .
the advantage of this formulation is that it can be solved as easily as solving the standard penalized regression problem .
essentially , we rescale the regularizer depending on the pattern weights : the higher the weight of an observation , the less we regularize .
123 distribution matching
123 kernel mean matching and its relation to importance sampling
let : x f be a map into a feature space f and denote by : p f the expectation operator
( pr ) : = expr ( x ) ( ( x ) ) .
clearly is a linear operator mapping the space of all probability distributions p into feature space .
denote by m ( ) : = ( ( pr ) where pr p ) the image of p under .
this set is also often referred to as the marginal polytope .
we have the following theorem ( proved in ( 123 ) ) : theorem 123 the operator is bijective if f is an rkhs with a universal kernel k ( x , x ) = h ( x ) , ( x ) i in the sense of steinwart ( 123 ) .
the use of feature space means to compare distributions is further explored in ( 123 ) .
the practical consequence of this ( rather abstract ) result is that if we know ( pr ) , we can infer a suitable by solving the following minimization problem :
( cid : 123 ) ( cid : 123 ) ( pr ) expr ( x ) ( ( x ) ( x ) ) ( cid : 123 ) ( cid : 123 ) subject to ( x ) 123 and expr ( x ) ( ( x ) ) = 123
this is the kernel mean matching ( kmm ) procedure .
for a proof of the following ( and further results in the paper ) see ( 123 ) .
lemma 123 the problem ( 123 ) is convex .
moreover , assume that pr is absolutely continuous with respect to pr ( so pr ( a ) = 123 implies pr ( a ) = 123 ) .
finally assume that k is universal .
then the solution ( x ) of ( 123 ) is p r ( x ) = ( x ) p r ( x ) .
123 convergence of reweighted means in feature space
lemma 123 shows that in principle , if we knew pr and ( pr ) , we could fully recover pr by solving a simple quadratic program .
in practice , however , neither ( pr ) nor pr is known .
instead , we only have samples x and x of size m and m , drawn iid from pr and pr respectively .
naively we could just replace the expectations in ( 123 ) by empirical averages and hope that the resulting optimization problem provides us with a good estimate of .
however , it is to be expected that empirical averages will differ from each other due to nite sample size effects .
in this section , we explore two such effects .
first , we demonstrate that in the nite sample case , for a xed , the empirical estimate of the expectation of is normally distributed : this provides a natural limit on the precision with which we should enforce the constraint r ( x ) d pr ( x ) = 123 when using empirical expectations ( we will return to this point in the next section ) .
lemma 123 if ( x ) ( 123 , b ) is some xed function of x x , then given xi pr iid such that ( xi ) m pi ( xi ) converges in distribution to a has nite mean and non - zero variance , the sample mean 123 gaussian with mean r ( x ) d pr ( x ) and standard deviation bounded by b this lemma is a direct consequence of the central limit theorem ( 123 , theorem 123 . 123 ) .
alternatively , it is straightforward to get a large deviation bound that likewise converges as 123 / m ( 123 ) .
our second result demonstrates the deviation between the empirical means of pr and ( x ) pr in feature space , given ( x ) is chosen perfectly in the population sense .
in particular , this result shows that convergence of these two means will be slow if there is a large difference in the probability mass of pr and pr ( and thus the bound b on the ratio of probability masses is large ) .
lemma 123 in addition to the lemma 123 conditions , assume that we draw x : = ( x123 , .
, xm ) iid from x using pr = ( x ) pr , and k ( x ) k r for all x x .
then with probability at least 123
( xi ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 + p123 log / 123 ( cid : 123 ) rpb123 / m + 123 / m
note that this lemma shows that for a given ( x ) , which is correct in the population sense , we can bound the deviation between the feature space mean of pr and the reweighted feature space mean of pr .
it is not a guarantee that we will nd coefcients i that are close to ( xi ) , but it gives us a useful upper bound on the outcome of the optimization .
lemma 123 implies that we have o ( bp123 / m + 123 / mb123 ) convergence in m , m and b .
this means that , for very different distributions we need a large equivalent sample size to get reasonable conver - gence .
our result also implies that it is unrealistic to assume that the empirical means ( reweighted or not ) should match exactly .
123 empirical kmm optimization
to nd suitable values of rm we want to minimize the discrepancy between means subject to constraints i ( 123 , b ) and | 123 i=123 i 123| .
the former limits the scope of discrepancy between pr and pr whereas the latter ensures that the measure ( x ) pr ( x ) is close to a probability distribution .
the objective function is given by the discrepancy term between the two empirical means .
using kij : = k ( xi , xj ) and i : = m
j=123 k ( xi , xj ) one may check that
m123 + const .
we now have all necessary ingredients to formulate a quadratic problem to nd suitable via
k subject to i ( 123 , b ) and ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
i m ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) m .
in accordance with lemma 123 , we conclude that a good choice of should be o ( b / m ) .
note that ( 123 ) is a quadratic program which can be solved efciently using interior point methods or any other successive optimization procedure .
we also point out that ( 123 ) resembles single class svm ( 123 ) using the - trick .
besides the approximate equality constraint , the main difference is the linear correction term by means of .
large values of i correspond to particularly important observations xi and are likely to lead to large i .
123 toy regression example our rst experiment is on toy data , and is intended mainly to provide a comparison with the approach of ( 123 ) .
this method uses an information criterion to optimise the weights , under certain restrictions on pr and pr ( namely , pr must be known , while pr can be either known exactly , gaussian with unknown parameters , or approximated via kernel density estimation ) .
our data is generated according to the polynomial regression example from ( 123 , section 123 ) , for which pr n ( 123 , 123 ) and pr n ( 123 , 123 ) are two normal distributions .
the observations are generated according to y = x + x123 , and are observed in gaussian noise with standard deviation 123 ( see figure 123 ( a ) ; the blue curve is the noise - free signal ) .
we sampled 123 training ( blue circles ) and testing ( red circles ) points from pr and pr respectively .
we attempted to model the observations with a degree 123 polynomial .
the black dashed line is a best - case scenario , which is shown for reference purposes : it represents the model t using ordinary least squared ( ols ) on the labeled test points .
the red line is a second reference result , derived only from the training data via ols , and predicts the test data very poorly .
the other three dashed lines are t with weighted ordinary least square ( wols ) , using one of three weighting schemes : the ratio of the underlying training and test densities , kmm , and the information criterion of ( 123 ) .
a summary of the performance over 123 trials is shown in figure 123 ( b ) .
our method outperforms the two other reweighting methods .
x from q123 true fitting model ols fitting x
x from q123 ols fitting x
wols by ratio wols by kmm wols by min ic
figure 123 : ( a ) polynomial models of degree 123 t with ols and wols; ( b ) average performances of three wols methods and ols on the test data in ( a ) .
labels are ratio for ratio of test to training density; kmm for our approach; min ic for the approach of ( 123 ) ; and ols for the model trained on the labeled test points .
123 real world datasets we next test our approach on real world data sets , from which we select training examples using a deliberately biased procedure ( as in ( 123 , 123 ) ) .
to describe our biased selection scheme , we need to dene an additional random variable si for each point in the pool of possible training samples , where si = 123 means the ith sample is included , and si = 123 indicates an excluded sample .
two situations are considered : the selection bias corresponds to our assumption regarding the relation between the training and test distributions , and p ( si = 123|xi , yi ) = p ( si|xi ) ; or si is dependent only on yi , i . e .
p ( si|xi , yi ) = p ( si|yi ) , which potentially creates a greater challenge since it violates our key assumption 123
in the following , we compare our method ( labeled kmm ) against two others : a baseline unweighted method ( unweighted ) , in which no modication is made , and a weighting by the inverse of the true sampling distribution ( importance sampling ) , as in ( 123 , 123 ) .
we emphasise , however , that our method does not require any prior knowledge of the true sampling probabilities .
in our experiments , we used a gaussian kernel exp ( kxi xjk123 ) in our kernel classication and regression algorithms , and parameters = ( m 123 ) / m and b = 123 in the optimization ( 123 ) .
training set proportion
( a ) simple bias on features
( b ) joint bias on features
training set proportion
inverse of true sampling
( c ) bias on labels
( d ) vs inverse sampling prob .
figure 123 : classication performance analysis on breast cancer dataset from uci .
123 . 123 breast cancer dataset this dataset is from the uci archive , and is a binary classication task .
it includes 123 examples from 123 classes : benign ( positive label ) and malignant ( negative label ) .
the data are randomly split into training and test sets , where the proportion of examples used for training varies from 123% to 123% .
test results are averaged over 123 trials , and were obtained using a support vector classier with kernel size = 123 .
first , we consider a biased sampling scheme based on the input features , of which there are nine , with integer values from 123 to 123
since smaller feature values predominate in the unbiased data , we sample according to p ( s = 123|x 123 ) = 123 and p ( s = 123|x > 123 ) = 123 , repeating the experiment for each of the features in turn .
results are an average over 123 random training / test splits , with 123 / 123 of the data used for training and 123 / 123 for testing .
performance is shown in figure 123 ( a ) : we consistently outperform the unweighted method , and match or exceed the performance obtained using the known distribution ratio .
next , we consider a sampling bias that operates jointly across multiple features .
we select samples less often when they are further from the sample mean x over the training data , i . e .
p ( si|xi ) exp ( kxi xk123 ) where = 123 / 123
performance of our method in 123 ( b ) is again better than the unweighted case , and as good as or better than reweighting using the sampling model .
finally , we consider a simple biased sampling scheme which depends only on the label y : p ( s = 123|y = 123 ) = 123 and p ( s = 123|y = 123 ) = 123 ( the data has on average twice as many positive as negative examples when uniformly sampled ) .
average performance for different training / testing split proportions is in figure 123 ( c ) ; remarkably , despite our assumption regarding the difference between the training and test distributions being violated , our method still improves the test performance , and outperforms the reweighting by density ratio for large training set sizes
ure 123 ( d ) shows the weights are proportional to the inverse of true sampling probabilities : positive examples have higher weights and negative ones have lower weights .
123 . 123 further benchmark datasets we next compare the performance on further benchmark datasets123 by selecting training data via various biased sampling schemes .
specically , for the sampling distribution bias on labels , we use p ( s = 123|y ) = exp ( a + by ) / ( 123 + exp ( a + by ) ) ( datasets 123 to 123 ) , or the simple step distri - bution p ( s = 123|y = 123 ) = a , p ( s = 123|y = 123 ) = b ( datasets 123 and 123 ) .
for the remaining datasets , we generate biased sampling schemes over their features .
we rst do pca , selecting the rst principal component of the training data and the corresponding projection values .
denoting the minimum value of the projection as m and the mean as m , we apply a normal distribution with mean m + ( m m ) / a and variance ( m m ) / b as the biased sampling scheme .
please refer to ( 123 ) for detailed parameter settings .
we use penalized lms for regression problems and svm for classication problems .
to evaluate generalization performance , we utilize the normalized mean square error ( nmse ) given by 123 for regression problems , and the average test error for classication problems .
in 123 out of 123 experiments , our reweighting approach is the most accu - rate ( see table 123 ) , despite having no prior information about the bias of the test sample ( and , in some cases , despite the additional fact that the data reweighting does not conform to our key assumption 123 ) .
in addition , the kmm always improves test performance compared with the unweighted case .
two additional points should be borne in mind : rst , we use the same for the kernel mean match - ing and the svm , as listed in table 123
performance might be improved by decoupling these kernel sizes : indeed , we employ kernels that are somewhat large , suggesting that the kmm procedure is helpful in the case of relatively smooth classication / regresssion functions .
second , we did not nd a performance improvement in the case of data sets with smaller sample sizes .
this is not surprising , since a reweighting would further reduce the effective number of points used for training , resulting in insufcient data for learning .
table 123 : test results for three methods on 123 datasets with different sampling schemes .
the results are averages over 123 trials for regression problems ( marked * ) and 123 trials for classication problems .
we used a gaussian kernel of size for both the kernel mean matching and the svm / lms regression , and set b = 123
ca housing* 123
delta ailerons ( 123 ) * 123
delta ailerons ( 123 ) * 123
boston house* 123
usps ( 123vs123 ) ( 123 ) 123
usps ( 123vs123 ) ( 123 ) 123
breast cancer 123
india diabetes 123
german credit
nmse / test err .
123 . 123 tumor diagnosis using microarrays our next benchmark is a dataset of 123 microarrays from prostate cancer patients ( 123 ) .
each of these microarrays measures the expression levels of 123 , 123 genes .
the dataset comprises 123 samples from normal tissues ( positive label ) and 123 from tumor tissues ( negative label ) .
we simulate the realisitc scenario that two sets of microarrays a and b are given with dissimilar proportions of tumor samples , and we want to perform cancer diagnosis via classication , training on a and predicting
123regression data from http : / / www . liacc . up . pt / ltorgo / regression / datasets . html;
classication data from uci .
sets with numbers in brackets are examined by different sampling schemes .
we select training examples via the biased selection scheme p ( s = 123|y = 123 ) = 123 and p ( s = 123|y = 123 ) = 123 .
the remaining data points form the test set .
we then perform svm classication for the unweighted , kmm , and importance sampling approaches .
the experiment was repeated over 123 independent draws from the dataset according to our biased scheme; the 123 resulting test errors are plotted in ( 123 ) .
the kmm achieves much higher accuracy levels than the unweighted approach , and is very close to the importance sampling approach .
