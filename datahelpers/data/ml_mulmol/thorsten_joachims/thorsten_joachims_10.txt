abstract discriminative training approaches like structural svms have shown much promise for building highly complex and accurate models in areas like natural language processing , protein structure prediction , and information retrieval .
how - ever , current training algorithms are computationally expensive or intractable on large datasets .
to overcome this bottleneck , this paper explores how cutting - plane methods can provide fast training not only for classication svms , but also for structural svms .
in particular , we show that in an equivalent 123 - slack reformula - tion of the linear svm training problem , our cutting - plane method has time com - plexity linear in the number of training examples , linear in the desired precision , and linear also in all other parameters .
furthermore , we present an extensive em - pirical evaluation of the method applied to binary classication , multi - class classi - cation , hmm sequence tagging , and cfg parsing .
the experiments show that the cutting - plane algorithm is broadly applicable and fast in practice .
on large datasets , it is typically several orders of magnitude faster than conventional training methods derived from decomposition methods like svm light , or conventional cutting - plane methods .
implementations of our methods are available online .
key words : structural svms , support vector machines , structured output predic - tion , training algorithms
of computer science , cornell university , ithaca , ny , usa , e - mail : tj@cs . cornell . edu
of computer science , cornell university , ithaca , ny , usa , e - mail : tomf@cs . cornell . edu
chun - nam john yu dept .
of computer science , cornell university , ithaca , ny , usa , e - mail : cnyu@cs . cornell . edu
thorsten joachims , thomas finley , and chun - nam john yu
consider the problem of learning a function with complex outputs , where the predic - tion is not a single univariate response ( e . g . , 123 / 123 for classication or a real number for regression ) , but a structured multivariate object .
examples of such structured prediction problems are the prediction of parse trees in natural language process - ing , the prediction of total orderings in web search , or the prediction of sequence alignments in protein threading .
recent years have provided intriguing advances in extending methods like logis - tic regression , perceptrons , and support vector machines ( svms ) to global train - ing of such structured prediction models ( e . g . , lafferty et al , 123; collins , 123; collins and duffy , 123; taskar et al , 123; tsochantaridis et al , 123 ) .
in contrast to conventional generative training , these methods are discriminative ( e . g . , condi - tional likelihood , empirical risk minimization ) .
akin to moving from naive bayes to an svm for classication , this provides greater modeling exibility through avoid - ance of independence assumptions , and it was shown to provide substantially im - proved prediction accuracy in many domains ( e . g . , lafferty et al , 123; taskar et al , 123; tsochantaridis et al , 123; taskar et al , 123; yu et al , 123 ) .
by eliminat - ing the need for modeling statistical dependencies between features , discriminative training enables us to freely use more complex and possibly interdependent features , which provides the potential to learn models with improved delity .
however , train - ing these rich models with a sufciently large training set is often beyond the reach of current discriminative training algorithms .
we focus on the problem of training structural svms in this paper .
formally , this can be thought of as solving a convex quadratic program ( qp ) with a large ( typi - cally exponential or innite ) number of constraints .
existing algorithm fall into two groups .
the rst group of algorithms relies on an elegant polynomial - size reformu - lation of the training problem ( taskar et al , 123; anguelov et al , 123 ) , which is possible for the special case of margin - rescaling ( tsochantaridis et al , 123 ) with linearly decomposable loss .
these smaller qps can then be solved , for example , with general - purpose optimization methods ( anguelov et al , 123 ) or decompo - sition methods similar to smo ( taskar et al , 123; platt , 123 ) .
unfortunately , decomposition methods are known to scale super - linearly with the number of ex - amples ( platt , 123; joachims , 123 ) , and so do general - purpose optimizers , since they do not exploit the special structure of this optimization problem .
but most sig - nicantly , the algorithms in the rst group are limited to applications where the polynomial - size reformulation exists .
similar restrictions also apply to the extra - gradient method ( taskar et al , 123 ) , which applies only to problems where sub - gradients of the qp can be computed via a convex real relaxation , as well as ex - ponentiated gradient methods ( bartlett et al , 123; globerson et al , 123 ) , which require the ability to compute marginals ( e . g .
via the sum - product algorithm ) .
the second group of algorithms works directly with the original , exponentially - sized qp .
this is feasible , since a polynomially - sized subset of the constraints from the original qp is already sufcient for a solution of arbitrary accuracy ( joachims , 123; tsochantaridis et al , 123 ) .
such algorithms either take stochastic subgradi -
cutting - plane training of structural svms
ent steps ( collins , 123; ratliff et al , 123; shalev - shwartz et al , 123 ) , or build a cutting - plane model which is easy to solve directly ( tsochantaridis et al , 123 ) .
the algorithm in ( tsochantaridis et al , 123 ) shows how such a cutting - plane can be constructed efciently .
compared to the subgradient methods , the cutting - plane approach does not take a single gradient step , but always takes an optimal step in the current cutting - plane model .
it requires only the existence of an efcient separation oracle , which makes it applicable to many problems for which no polynomially - sized reformulation is known .
in practice , however , the cutting - plane method of tsochantaridis et al ( 123 ) is known to scale super - linearly with the number of training examples .
in particular , since the size of the cutting - plane model typically grows linearly with the dataset size ( see tsochantaridis et al , 123 , and section 123 ) , qps of increasing size need to be solved to compute the optimal steps , which leads to the super - linear runtime .
in this paper , we explore an extension of the cutting - plane method presented in ( joachims , 123 ) for training linear structural svms , both in the margin - rescaling and in the slack - rescaling formulation ( tsochantaridis et al , 123 ) .
in contrast to the cutting - plane method presented in ( tsochantaridis et al , 123 ) , we show that the size of the cutting - plane models and the number of iterations are independent of the number of training examples n .
instead , their size and the number of iterations ) , where c is the regularization constant and is can be upper bounded by o ( c the desired precision of the solution ( see optimization problems op123 and op123 in section 123 ) .
since each iteration of the new algorithm takes o ( n ) time and memory , it also scales o ( n ) overall with the number of training examples both in terms of computation time and memory .
empirically , the size of the cutting - plane models and the qps that need to be solved in each iteration is typically very small ( less than a few hundred variables ) even for problems with millions of features and hundreds of thousands of examples .
a key conceptual difference of the new algorithm compared to the algorithm of tsochantaridis et al ( 123 ) and most other svm training methods is that not only individual data points are considered as potential support vectors ( svs ) , but also linear combinations of those .
this increased exibility allows for solutions with far fewer non - zero dual variables , and it leads to the small cutting - plane models
the new algorithm is applicable to all structural svm problems where the sep - aration oracle can be computed efciently , which makes it just as widely applica - ble as the most general training algorithms known to date .
even further , following the original publication in ( joachims , 123 ) , teo et al ( 123 ) have already shown that the algorithm can also be extended to conditional random field training .
we provide a theoretical analysis of the algorithms correctness , convergence rate , and scaling behavior for structured prediction .
furthermore , we present empirical results for several structured prediction problems ( i . e . , multi - class classication , part - of - speech tagging , and natural language parsing ) , and compare against conventional algorithms also for the special case of binary classication .
on all problems , the new algorithm is substantially faster than conventional decomposition methods and cutting - plane methods , often by several orders of magnitude for large datasets .
thorsten joachims , thomas finley , and chun - nam john yu
123 structural support vector machines
structured output prediction describes the problem of learning a function
h : x y
where x is the space of inputs , and y is the space of ( multivariate and structured ) outputs .
in the case of natural language parsing , for example , x is the space of sentences , and y is the space of trees over a given set of non - terminal grammar symbols .
to learn h , we assume that a training sample of input - output pairs
s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) ( x y ) n
is available and drawn i . i . d . 123 from a distribution p ( x , y ) .
for a given hypothesis space h , the goal is to nd a function h h that has low prediction error , or , more generally , low risk
( y , y ) is a loss function that quanties the loss associated with predicting y when y is the correct output value .
we assume that ( y , y ) = 123 and ( y , y ) 123 for y ( cid : 123 ) = y .
we follow the empirical risk minimization principle ( vapnik , 123 ) to infer a function h from the training sample s .
the learner evaluates the quality of a function h h using its empirical risk r
( h ) on the training sample s .
( h ) = 123
support vector machines select an h h that minimizes a regularized empirical risk on s .
for conventional binary classication where y = ( 123 , +123 ) , svm train - ing is typically formulated as the following convex quadratic optimization problem 123 ( cortes and vapnik , 123; vapnik , 123 ) .
optimization problem 123 ( classification svm ( primal ) )
wwwt www + c
i ( 123 , . . . , n ) : yi ( wwwt xi ) 123 i
it was shown that svm training can be generalized to structured outputs ( altun et al , 123; taskar et al , 123; tsochantaridis et al , 123 ) , leading to an optimiza -
123 note , however , that all formal results in this paper also hold for non i . i . d .
data , since our algo - rithms do not rely on the order or distribution of the examples .
123 for simplicity , we consider the case of hyperplanes passing through the origin .
by adding a constant feature , an offset can easily be simulated .
cutting - plane training of structural svms
tion problem that is similar to multi - class svms ( crammer and singer , 123 ) and extending the perceptron approach described in ( collins , 123 ) .
the idea is to learn a discriminant function f : x y over input / output pairs from which one derives a prediction by maximizing f over all y y for a given input x .
hwww ( x ) = argmax
we assume that fwww ( x , y ) takes the form of a linear function
fwww ( x , y ) = wwwt ( x , y )
where www n is a parameter vector and ( x , y ) is a feature vector relating input x and output y .
intuitively , one can think of f www ( x , y ) as a compatibility function that measures how well the output y matches the given input x .
the exibility in designing allows employing structural svms for problems as diverse as natu - ral language parsing ( taskar et al , 123 ) , protein sequence alignment ( yu et al , 123 ) , supervised clustering ( finley and joachims , 123 ) , learning ranking func - tions that optimize ir performance measures ( yue et al , 123 ) , and segmenting images ( anguelov et al , 123 ) .
for training the weights www of the linear discriminant function , the standard svm optimization problem can be generalized in several ways ( altun et al , 123; joachims , 123; taskar et al , 123; tsochantaridis et al , 123 , 123 ) .
this paper uses the formulations given in ( tsochantaridis et al , 123 ) , which subsume all other approaches .
we refer to these as the n - slack formulations , since they assign a dif - ferent slack variable to each of the n training examples .
tsochantaridis et al ( 123 ) identify two different ways of using a hinge loss to convex upper bound the loss , namely margin - rescaling and slack - rescaling .
in margin - rescaling , the position of the hinge is adapted while the slope is xed ,
mr ( y , hwww ( x ) ) = max
( ( y , y ) wwwt ( x , y ) + wwwt ( x , y ) ) ( y , hwww ( x ) )
while in slack - rescaling , the slope is adjusted while the position of the hinge is xed .
sr ( y , hwww ( x ) ) = max
( ( y , y ) ( 123 wwwt ( x , y ) + wwwt ( x , y ) ) ) ( y , hwww ( x ) )
this leads to the following two training problems , where each slack variable i is equal to the respective mr ( yi , hwww ( xi ) ) or sr ( yi , hwww ( xi ) ) for training example
thorsten joachims , thomas finley , and chun - nam john yu
optimization problem 123 ( n - slack structural svm with margin -
wwwt www + c
y123 y : wwwt ( ( x123 , y123 ) ( x123 , y123 ) ) ( y123 , y123 ) 123
yn y : wwwt ( ( xn , yn ) ( xn , yn ) ) ( yn , yn ) n
optimization problem 123 ( n - slack
wwwt www + c
y123 y : wwwt ( ( x123 , y123 ) ( x123 , y123 ) ) 123
yn y : wwwt ( ( xn , yn ) ( xn , yn ) ) 123
the objective is the conventional regularized risk used in svms .
the constraints state that for each training example ( xi , yi ) the score wwwt ( xi , yi ) of the correct struc - ture yi must be greater than the score wwwt ( xi , y ) of all incorrect structures y by a required margin .
this margin is 123 in slack - rescaling , and equal to the loss ( y i , y ) in margin rescaling .
if the margin is violated , the slack variable i of the example be - comes non - zero .
note that i is shared among constraints from the same example .
it is easy to verify that for both margin - rescaling and for slack - rescaling , i is an up - per bound on the empirical risk r ( h ) on the training sample s ( see tsochantaridis et al , 123 ) .
it is not immediately obvious that optimization problems op123 and op123 can be solved efciently , since they have o ( n|y | ) constraints .
|y | is typically extremely large ( e . g . , all possible alignments of two amino - acid sequence ) or even innite ( e . g . , real - valued outputs ) .
for the special case of margin - rescaling with linearly decomposable loss functions , taskar et al .
( taskar et al , 123 ) have shown that the problem can be reformulated as a quadratic program with only a polynomial number of constraints and variables .
a more general algorithm that applies to both margin - rescaling and slack - rescaling under a large variety of loss functions was given in ( tsochantaridis et al , 123 , 123 ) .
the algorithm relies on the theoretical result that for any desired pre - cision , a greedily constructed cutting - plane model of op123 and op123 requires only ) many constraints ( joachims , 123; tsochantaridis et al , 123 ) .
this greedy algorithm for the case of margin - rescaling is algorithm 123 , for slack - rescaling it leads to algorithm 123
the algorithms iteratively construct a working set w = w 123 . . wn
cutting - plane training of structural svms
algorithm 123 for training structural svms ( with margin - rescaling ) via the n - slack 123 : input : s = ( ( x123 , y123 ) , .
. , ( xn , yn ) ) , c , 123 : wi / 123 , i 123 for all i = 123 , . . . , n
y argmaxyy ( ( yi , y ) wwwt ( ( xi , yi ) ( xi , y ) ) ) if ( yi , y ) wwwt ( ( xi , yi ) ( xi , y ) ) > i + then
wi wi ( y )
for i=123 , . . . , n do
123 wwwt www + c
y123 w123 : wwwt ( ( x123 , y123 ) ( x123 , y123 ) ) ( y123 , y123 ) 123
yn wn : wwwt ( ( xn , yn ) ( xn , yn ) ) ( yn , yn ) n
123 : until no wi has changed during iteration
algorithm 123 for training structural svms ( with slack - rescaling ) via the n - slack 123 : input : s = ( ( x123 , y123 ) , .
. , ( xn , yn ) ) , c , 123 : wi / 123 , i 123 for all i = 123 , . . . , n
y argmaxyy ( ( yi , y ) ( 123 wwwt ( ( xi , yi ) ( xi , y ) ) ) ) if ( yi , y ) ( 123 wwwt ( ( xi , yi ) ( xi , y ) ) ) > i + then
wi wi ( y )
for i=123 , . . . , n do
y123 w123 : wwwt ( y123 , y123 ) ( ( x123 , y123 ) ( x123 , y123 ) ) ( y123 , y123 ) 123
123 wwwt www + c
yn wn : wwwt ( yn , yn ) ( ( xn , yn ) ( xn , yn ) ) ( yn , yn ) n
123 : until no wi has changed during iteration
of constraints , starting with an empty working set w = / 123
the algorithms iterate through the training examples and nd the constraint that is violated most by the current solution www , ( line 123 ) .
if this constraint is violated by more than the desired precision ( line 123 ) , the constraint is added to the working set ( line 123 ) and the qp is solved over the extended w ( line 123 ) .
the algorithms terminate when no constraint is added in the previous iteration , meaning that all constraints in op123 or op123 are ful - lled up to a precision of .
the algorithm is provably efcient whenever the most violated constraint can be found efciently .
the argmax in line 123 has an efcient solution for a wide variety of choices for , y , and ( see e . g . , tsochantaridis et al ,
thorsten joachims , thomas finley , and chun - nam john yu
123; joachims , 123; yu et al , 123; yue et al , 123 ) , and often the algorithm for making predictions ( see eq .
123 ) can be adapted to compute this solution .
related to algorithm 123 is the method proposed in ( anguelov et al , 123 ) , which applies to the special case where the argmax in line 123 can be computed as a linear program .
this allows them not to explicitly maintain a working set , but implicitly represent it by folding n linear programs into the quadratic program op123
to this special case also applies the method of taskar et al ( 123 ) , which casts the training of max - margin structured predictors as a convex - concave saddle - point problem .
it provides improved scalability compared to an explicit reduction to a polynomially - sized qp , but involves the use of a special min - cost quadratic ow solver in the projection steps of the extragradient method .
exponentiated gradient methods , originally proposed for online learning of lin - ear predictors ( kivinen and warmuth , 123 ) , have also been applied to the training of structured predictors ( globerson et al , 123; bartlett et al , 123 ) .
they solve the optimization problem in the dual , and treat conditional random eld and structural svm within the same framework using bregman divergences .
stochastic gradient methods vishwanathan et al ( 123 ) have been applied to the training of conditional random eld on large scale problems , and exhibit faster rate of convergence than bfgs methods .
recently , subgradient methods and their stochastic variants ( ratliff et al , 123 ) have also been proposed to solve the optimization problem in max - margin structured prediction .
while not yet explored for structured prediction , the pegasos algorithm ( shalev - shwartz et al , 123 ) has shown promising perfor - mance for binary classication svms .
related to such online methods is also the mira algorithm ( crammer and singer , 123 ) , which has been used for training structured predictors ( e . g .
mcdonald et al , 123 ) .
however , to deal with the expo - nential size of y , heuristics have to be used ( e . g .
only using a k - best subset of y ) , leading to only approximate solutions of optimization problem op123
123 training algorithm
while polynomial runtime was established for most algorithms discussed above , training general structural svms on large - scale problems is still a challenging prob - lem .
in the following , we present an equivalent reformulation of the training prob - lems for both margin - rescaling and slack - rescaling , leading to a cutting - plane train - ing algorithm that has not only provably linear runtime in the number of training examples , but is also several orders of magnitude faster than conventional cutting - plane methods ( tsochantaridis et al , 123 ) on large - scale problems .
nevertheless , the new algorithm is equally general as algorithms 123 and 123
cutting - plane training of structural svms
123 123 - slack formulation
the rst step towards the new algorithm is a reformulation of the optimization prob - lems for training .
the key idea is to replace the n cutting - plane models of the hinge loss one for each training example with a single cutting plane model for the sum of the hinge - losses .
since there is only a single slack variable in the new formula - tions , we refer to them as 123 - slack formulations .
optimization problem 123 ( 123 - slack structural svm with margin -
wwwt www +c
( y123 , . . . , yn ) y n :
( ( xi , yi ) ( xi , yi ) ) 123
optimization problem 123 ( 123 - slack
wwwt www +c
( y123 , . . . , yn ) y n :
( yi , yi ) ( ( xi , yi ) ( xi , yi ) ) 123
while op123 has |y |n constraints , one for each possible combination of labels ( y123 , . . . , yn ) y n , it has only one slack variable that is shared across all constraints .
( h ) respectively , and each constraint corresponds to a tangent to r the set of constraints forms an equivalent model of the risk function .
specically , the following theorems show that = r , ) of op123 , and , ) of op123 , since the n - slack and the 123 - slack formulations are equivalent in the following sense .
( hw ) at the solution ( w
( hw ) at the solution ( w
( h ) and r
of op123 is also a solution of op123 ( and vice versa ) , with =
theorem 123
( equivalence of op123 and op123 ) any solution www proof .
generalizing the proof in ( joachims , 123 ) , we will show that for every www the smallest feasible and n
for a given www , each i in op123 can be optimized individually , and the smallest
i are equal .
feasible i given www is achieved for
i = max
( ( yi , yi ) wwwt ( ( xi , yi ) ( xi , yi ) ) ) .
for op123 , the smallest feasible for a given www is
thorsten joachims , thomas finley , and chun - nam john yu
algorithm 123 for training structural svms ( with margin - rescaling ) via the 123 - slack 123 : input : s = ( ( x123 , y123 ) , .
. , ( xn , yn ) ) , c , 123 : w / 123
123 wwwt www +c s . t .
( y123 , . . . , yn ) w : 123
( ( xi , yi ) ( xi , yi ) ) 123
for i=123 , . . . , n do
yi argmax yy ( ( yi , y ) + wwwt ( xi , y ) )
123 : until 123
w w ( ( y123 , . . . , yn ) )
( yi , yi ) 123
( ( xi , yi ) ( xi , yi ) ) +
( yi , yi ) wwwt 123
( y123 , . . . , yn ) y n
since the function can be decomposed linearly in y i , for any given www , each yi can be
( yi , yi ) 123
wwwt ( ( xi , yi ) ( xi , yi ) )
therefore , the objective functions of both optimization problems are equal for any www given the corresponding smallest feasible and i .
consequently this is also true
and its corresponding smallest feasible slacks
of op123 is also a solution of op123 ( and vice versa ) , with =
theorem 123
( equivalence of op123 and op123 ) any solution www proof .
analogous to theorem 123
123 cutting - plane algorithm
what could we possibly have gained by moving from the n - slack to the 123 - slack formulation , exponentially increasing the number of constraints in the process ? we will show in the following that the dual of the 123 - slack formulation has a solution that is extremely sparse , with the number of non - zero dual variables independent of the number of training examples .
to nd this solution , we propose algorithms 123 and 123 , which are generalizations of the algorithm in ( joachims , 123 ) to structural svms .
similar to the cutting - plane algorithms for the n - slack formulations , algorithms 123 and 123 iteratively construct a working set w of constraints .
in each iteration , the al -
cutting - plane training of structural svms
algorithm 123 for training structural svms ( with slack - rescaling ) via the 123 - slack 123 : input : s = ( ( x123 , y123 ) , .
. , ( xn , yn ) ) , c , 123 : w / 123
( y123 , . . . , yn ) w : 123 yi argmax yy ( ( yi , y ) ( 123 wwwt ( ( xi , yi ) ( xi , y ) ) ) )
( yi , yi ) ( ( xi , yi ) ( xi , yi ) ) 123
for i=123 , . . . , n do
123 wwwt www +c
123 : until 123
w w ( ( y123 , . . . , yn ) )
( yi , yi ) 123
( yi , yi ) ( ( xi , yi ) ( xi , yi ) ) +
gorithms compute the solution over the current w ( line 123 ) , nd the most violated constraint ( lines 123 - 123 ) , and add it to the working set .
the algorithm stops once no constraint can be found that is violated by more than the desired precision ( line 123 ) .
unlike in the n - slack algorithms , only a single constraint is added in each iter - ation .
the following theorems characterize the quality of the solutions returned by algorithms 123 and 123
theorem 123
( correctness of algorithm 123 ) , ) is the for any training sample s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) and any > 123 , if ( www optimal solution of op123 , then algorithm 123 returns a point ( www , ) that has a better objective value than ( www proof .
we rst verify that lines 123 - 123 in algorithm 123 compute the vector ( y 123 , . . . , yn ) y n that maximizes
, ) , and for which ( www , + ) is feasible in op123
( yi , yi ) 123
( y123 , . . . , yn ) y n
is the minimum value needed to fulll all constraints in op123 for the current www .
the maximization problem is linear in the yi , so one can maximize over each yi
( yi , y ) wwwt ( ( xi , yi ) ( xi , y ) )
( cid : 123 ) = 123 wwwt ( xi , yi ) + 123
( yi , y ) + wwwt ( xi , y )
since the rst sum in equation ( 123 ) is constant , the second term directly corresponds to the assignment in line 123
as checked in line 123 , the algorithm terminates only if
does not exceed the from the solution over w by more than as desired .
123 wwwt www +c .
, ) , and for which ( www , + ) is feasible in op123
thorsten joachims , thomas finley , and chun - nam john yu since the ( www , ) returned by algorithm 123 is the solution on a subset of the con -
using a stopping criterion based on the accuracy of the empirical risk
straints from op123 , it holds that 123 theorem 123
( correctness of algorithm 123 ) , ) is the for any training sample s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) and any > 123 , if ( www optimal solution of op123 , then algorithm 123 returns a point ( www , ) that has a better objective value than ( www proof .
analogous to the proof of theorem 123
intuitive and practically meaningful , unlike the stopping criteria typically used in decomposition methods .
intuitively , can be used to indicate how close one wants to be to the empirical risk of the best parameter vector .
in most machine learning ap - plications , tolerating a training error that is suboptimal by 123% is very acceptable .
this intuition makes selecting the stopping criterion much easier than in other train - ing methods , where it is usually dened based on the accuracy of the kuhn - tucker conditions of the dual ( see e . g . , joachims , 123 ) .
nevertheless , it is easy to see that also bounds the duality gap of the solution by c .
solving the optimization problems to an arbitrary but xed precision of is essential in our analysis below , making sure that computation time is not wasted on computing a solution that is more accurate than necessary .
we next analyze the time complexity of algorithms 123 and 123
it is easy to see that each iteration of the algorithm takes n calls to the separation oracle , and that for the linear kernel the remaining work in each iteration scales linearly with n as well .
we show next that the number of iterations until convergence is bounded , and that this upper bound is independent of n .
the argument requires the wolfe - dual programs , which are straightforward to derive ( see appendix ) .
for a more compact notation , we denote vectors of labels as y = ( y123 , . . . , yn ) y n .
for such vectors of labels , we then dene ( y ) and the inner ( cid : 123 ) ) as follows .
note that yi and y j denote correct training labels , product hmr ( y , y while yi and y j denote arbitrary labels :
( y ) = 123 ( cid : 123 ) ) = 123
( xi , yi ) t ( x j , y j ) ( xi , yi ) t ( x j , y ( xi , yi ) t ( x j , y j ) + ( xi , yi ) t ( x j , y ( cid : 123 ) ) are computed either explicitly or via a kernel ( cid : 123 ) ) .
note that it is typically more efcient to compute
( ( x j , y j ) ( x j , y
the inner products ( x , y ) t ( x
( cid : 123 ) ) = ( x , y ) t ( x
if no kernel is used .
the dual of the 123 - slack formulation for margin - rescaling is :
cutting - plane training of structural svms
optimization problem 123 ( 123 - slack structural svm with margin -
for the case of slack - rescaling , the respective h ( y , y
( cid : 123 ) ) is dened as follows , and we again give an analogous factorization that is more efcient to compute if no kernel is used :
( xi , yi ) t ( x j , y j ) ( xi , yi ) t ( x j , y ( yi , yi ) ( y j , y ( xi , yi ) t ( x j , y j ) + ( xi , yi ) t ( x j , y
( yi , yi ) ( ( xi , yi ) ( xi , yi ) )
( y j , y
) ( ( x j , y j ) ( x j , y
the dual of the 123 - slack formulation is :
optimization problem 123 ( 123 - slack
using the respective dual solution solving the primal via
weight vector www
, one can compute inner products with the
( x , y ) t ( x j , y j ) ( x , y ) t ( x j , y j )
( ( x j , y j ) ( x j , y j ) )
( x , y ) t ( x j , y j ) ( x , y ) t ( x j , y j )
( y j , y j )
( y j , y j ) ( ( x j , y j ) ( x j , y j ) )
for margin - rescaling and via
thorsten joachims , thomas finley , and chun - nam john yu
for slack - rescaling .
we will show in the following that only a small ( i . e . , poly - nomial ) number of the y is non - zero at the solution .
in analogy to classication svms , we will refer to those y with non - zero y as support vectors .
however , note that support vectors in the 123 - slack formulation are linear combinations of multiple examples .
we can now state the theorem giving an upper bound on the number of iterations of the 123 - slack algorithms .
the proof extends the one in ( joachims , 123 ) to general structural svms , and is based on the technique introduced in ( joachims , 123 ) and generalized in ( tsochantaridis et al , 123 ) .
the nal step of the proof uses an improvement developed in ( teo et al , 123 ) .
theorem 123
( 123 - slack margin - rescaling svm iteration complexity ) for any 123 < c , 123 < 123r123c and any training sample s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) , algorithms 123 terminates after at most
iterations .
r123 = maxi , y|| ( xi , yi ) ( xi , y ) ||123 , = maxi , y ( yi , y ) , and ( cid : 123 ) . . ( cid : 123 ) is the integer ceiling function .
we will show that adding each new constraint to w increases the objective value at the solution of the quadratic program in line 123 by at least some constant positive value .
since the objective value of the solution of op123 is upper bounded by c ( since www = 123 and = is a feasible point in the primal ) , the algorithm can only perform a constant number of iterations before termination .
the amount by which the solution increases by adding one constraint that is violated by more then ( i . e . , the criteria in line 123 of algorithm 123 and algorithm 123 ) to w can be lower bounded
let y be the newly added constraint and let be the solution of the dual before the addition .
to lower bound the progress made by the algorithm in each iteration , consider the increase in the dual that can be achieved with a line search
( d ( + ) ) d ( ) .
the direction is constructed by setting y = 123 and y = 123 y for all other y .
note that the constraints on and the construction of ensure that + never leaves the feasible region of the dual .
to apply lemma 123 ( see appendix ) for computing the progress made by a line search , we need a lower bound for d ( ) t and an upper bound for t h .
starting with the lower bound for d ( ) t , note that
y ( cid : 123 ) hmr ( y , y
for all y with non - zero y at the solution over the previous working set w .
for the newly added constraint y and some > 123 ,
cutting - plane training of structural svms
( cid : 123 ) ) = + +
by construction due to line 123 of algorithms 123
it follows that
d ( ) t = +
the following gives an upper bound for t h , where hyy ( cid : 123 ) = hmr ( y , y
( cid : 123 ) ) for y , y
yhmr ( y , y ) + 123
t h = hmr ( y , y ) 123 cr123 + 123
r123 + 123
the bound uses that r123 hmr ( y , y ) r123
plugging everything into the bound of lemma 123 shows that the increase of the
objective is at least
( d ( + ) ) d ( ) min
note that the rst case applies whenever 123r123c , and that the second case applies
the nal step of the proof is to use this constant increase of the objective value in each iteration to bound the maximum number of iterations .
first , note that y = 123 for all incorrect vectors of labels y and y = c for the correct vector of labels = ( y123 , . . . , yn ) is a feasible starting point 123 with a dual objective of 123
this means the initial optimality gap ( 123 ) = d ( ) d ( 123 ) is at most c , where optimal dual solution .
an optimality gap of ( i ) = d ( ) d ( i ) ensures that there exists a constraint that is violated by at least ( i ) c .
this means that the rst case of ( 123 ) applies while ( i ) 123r123c123 , leading to a decrease in the optimality gap of at least
( i + 123 ) ( i ) 123
in each iteration .
starting from the worst possible optimality gap of ( 123 ) = c , the algorithm needs at most
thorsten joachims , thomas finley , and chun - nam john yu
iterations until it has reached an optimality gap of ( i 123 ) 123r123c123 , where the second case of ( 123 ) becomes valid .
as proposed in ( teo et al , 123 ) , the recurrence equation
( i + 123 ) ( i ) 123
for the second case of ( 123 ) can be upper bounded by solving the differential equation ( i ) 123 with boundary condition ( 123 ) = 123r123c123
the solution is ( i ) i+123 , showing that the algorithms does not need more than
iterations until it reaches an optimality gap of c when starting at a gap of 123r 123c123 , where is the desired target precision given to the algorithm .
once the optimal - ity gap reaches c , it is no longer guaranteed that an - violated constraint exists .
however , such constraints may still exist and so the algorithm does not yet termi - nate .
but since each such constraint leads to an increase in the dual objective of at
123r123 , only
can be added before the optimality gap becomes negative .
the overall bound results from adding i123 , i123 , and i123
note that the proof of the theorem requires only a line search in each step , while algorithm 123 actually computes the full qp solution .
this suggests the following .
on the one hand , the actual number of iterations in algorithm 123 might be substan - tially smaller in practice than what is predicted by the bound .
on the other hand , it suggests a variant of algorithm 123 , where the qp solver is replaced by a simple line search .
this may be benecial in structured prediction problems where the separa - tion oracle in line 123 is particularly cheap to compute .
theorem 123
( 123 - slack slack - rescaling svm iteration complexity ) for any 123 < c , 123 < 123r123c and any training sample s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) , algorithms 123 terminate after at most
iterations .
r123 = maxi , y|| ( xi , yi ) ( xi , y ) ||123 , = maxi , y ( yi , y ) , and ( cid : 123 ) . . ( cid : 123 ) is the integer ceiling function .
cutting - plane training of structural svms
the proof for the case of slack - rescaling is analogous .
the only difference is that 123r123 hsr ( y , y
the o ( 123
) convergence rate in the bound is tight , as the following example shows .
consider a multi - class classication problem with innitely many classes y = ( 123 , . . . , ) and a feature space x = that contains only one feature .
this problem can be encoded using a feature map ( x , y ) which takes value x in po - sition y and 123 everywhere else .
for a training set with a single training example ( x , y ) = ( ( 123 ) , 123 ) and using the zero / one - loss , the 123 - slack quadratic program for both margin - rescaling and slack - rescaling is
wwwt www +c
wwwt ( ( x , 123 ) ( x , 123 ) ) 123 wwwt ( ( x , 123 ) ( x , 123 ) ) 123 wwwt ( ( x , 123 ) ( x , 123 ) ) 123
lets assume without loss of generaltity that algorithm 123 ( or equivalently algo - rithm 123 ) introduces the rst constraint in the rst iteration .
for c 123 123 the solution , 123 , 123 , .
. ) and = 123
all other constraints are over this working set is wwwt = ( 123 now violated by 123 123 and one of them is selected at random to be added to the working set in the next iteration .
it is easy to verify that after adding k constraints , the solu - , 123 , 123 , .
. ) for c 123 tion over the working set is wwwt = ( k 123 , and all constraints outside the working set are violated by = 123 k+123 .
it therefore takes o ( 123 iterations to reach a desired precision of .
the o ( c ) scaling with c is tight as well , at least for small values of c .
consider - ing the same examples and c 123 123 , the solution over the working set after adding k constraints is wwwt = ( c , c , .
, c , 123 , 123 , .
this means that after k constraints , all constraints outside the working set are violated by = c k .
consequently , the bounds in ( 123 ) and ( 123 ) accurately reect the worst - case scaling with c up to the log - term for c 123
the following theorem summarizes our characterization of the time complex - ity of the 123 - slack algorithms .
in real applications , however , we will see that algo - rithms 123 scales much better than what is predicted by these worst - case bounds both w . r . t .
c and .
note that a support vector ( i . e .
point with non - zero dual variable ) no longer corresponds to a single data point in the 123 - slack dual , but is typically a linear combination of data points .
corollary 123
( time complexity of algorithms 123 and 123 for linear maxi , y|| ( xi , yi ) ( xi , y ) ||123 r123 < and maxi , y ( yi , y ) < for all
s = ( ( x123 , y123 ) , .
, ( xn , yn ) )
thorsten joachims , thomas finley , and chun - nam john yu n , the 123 - slack cutting plane algorithms 123 and 123 with constant and c using the require at most o ( n ) calls to the separation oracle , require at most o ( n ) computation time outside the separation oracle , nd a solution where the number of support vectors ( i . e .
the number of non - zero
dual variables in the cutting - plane model ) does not depend on n ,
for any xed value of c > 123 and > 123
theorems 123 and 123 shows that the algorithms terminate after a constant num - ber of iterations that does not depend on n .
since only one constraint is introduced in each iteration , the number of support vectors is bounded by the number of iter - ations .
in each iteration , the algorithm performs exactly n calls to the separation oracle , which proves the rst statement .
similarly , the qp that is solved in each iteration is of constant size and therefore requires only constant time .
it is easily veried that the remaining operations in each iterations can be done in time o ( n ) using eqs .
( 123 ) and ( 123 ) .
we further discuss the time complexity for the case of kernels in the following section .
note that the linear - time algorithm proposed in ( joachims , 123 ) for train - ing binary classication svms is a special case of the 123 - slack methods developed here .
in particular , for binary classication x = n and y = ( 123 , +123 ) , and plug -
( x , y ) = 123
yx and ( y , y
123 if y = y
into either n - slack formulation op123 or op123 produces the standard svm optimiza - tion problem op123
the 123 - slack formulations and algorithms are then equivalent to ) bound on the maximum number of those in ( joachims , 123 ) .
however , the o ( 123 iterations derived here is tighter than the o ( 123 ) bound in ( joachims , 123 ) .
us - ing a similar argument , it can also be shown that the ordinal regression method in ( joachims , 123 ) is a special case of the 123 - slack algorithm .
123 kernels and low - rank approximations
for problems where a ( non - linear ) kernel is used , the computation time in each iteration is o ( n123 ) instead of o ( n ) , since eqs .
( 123 ) and ( 123 ) not longer apply .
how - ever , the 123 - slack algorithm can easily exploit rank - k approximations , which we will show reduces the computation time outside of the separation oracle from o ( n 123 ) to o ( nk + k123 ) .
let ( x ) be a set of basis functions so that the subspace
spanned by ( x ) ( approximately ) contains the solution www and op123 respectively .
algorithms for nding such approximations have been sug - gested in ( keerthi et al , 123; fukumizu et al , 123; smola and scholkopf , 123 ) for
cutting - plane training of structural svms
classications svms , and at least some of them can be extended to structural svms as well .
in the simplest case , the set of k basis functions can be chosen randomly from the set of training examples .
for a kernel k ( . ) and the resulting gram matrix k with ki j = ( x of k in time o ( k123 ) .
assuming that www lently rewrite the 123 - slack optimization problems as
123 of the cholesky decomposition l actually lies in the subspace , we can equiva -
) , we can compute the inverse l
optimization problem 123 ( 123 - slack structural svm with margin - rescaling and k basis functions ( primal ) )
s . t . ( y123 , . . . , yn ) y n :
) k ( xi , yi , x ) k ( xi , yi , x
optimization problem 123 ( 123 - slack rescaling and k basis functions ( primal ) )
s . t . ( y123 , . . . , yn ) y n :
intuitively , the values of the kernel k ( . ) with each of the k basis functions form a new feature vector ( cid : 123 ) ( x , y ) t = ( k ( x , y , x ) ) t describing each 123 , op123 and op123 become identical to example ( x , y ) .
after multiplication with l a problem with linear kernel and k features , and it is straightforward to see that algorithms 123 and 123 apply to this new representation .
s = ( ( x123 , y123 ) , .
, ( xn , yn ) )
corollary 123
( time complexity of algorithms 123 and 123 for non - linear maxi , y|| ( xi , yi ) ( xi , y ) ||123 r123 < and maxi , y ( yi , y ) < for all n , the 123 - slack cutting plane algorithms 123 and 123 using a non - linear kernel require at most o ( n ) calls to the separation oracle , require at most o ( n123 ) computation time outside the separation oracle , require at most o ( nk + k123 ) computation time outside the separation oracle , if a nd a solution where the number of support vectors does not depend on n , for any xed value of c > 123 and > 123
set of k n basis functions is used ,
thorsten joachims , thomas finley , and chun - nam john yu
the proof is analogous to that of corollary 123
for the low - rank approxima - 123 before entering the tion , note that it is more efcient to once compute wwwt = t l 123 ( cid : 123 ) ( x , y ) for each example .
k123 is the cost of the loop in line 123 , than to compute l cholesky decomposition , but this needs to be computed only once .
we implemented both the n - slack algorithms and the 123 - slack algorithms in soft - ware package called svmstruct , which we make publicly available for download at http : / / svmlight . joachims . org .
svmstruct uses svmlight as the optimizer for solving the qp sub - problems .
users may adapt svm struct to their own struc - tural learning tasks by implementing api functions corresponding to task - specic , , separation oracle , and inference .
user api functions are in c .
a popular extension is svm python , which allows users to write api functions in python in - stead , and eliminates much of the drudge work of c including model serializa - tion / deserialization and memory management .
this extension is available for down - load at http : / / www . cs . cornell . edu / tomf / svmpython123 / .
an efcient implementation of the algorithms required a variety of design deci - sions , which are summarized in the following .
these design decisions have a sub - stantial inuence on the practical efciency of the algorithms .
restarting the qp sub - problem solver from the previous solution .
instead of solving each qp subproblem from scratch , we restart the optimizer from the dual solution of the previous working set as the starting point .
this applies to both the n - slack and the 123 - slack algorithms .
batch updates for the n - slack algorithm .
algorithm 123 recomputes the solution of the qp sub - problem after each update to the working set .
while this allows the algo - rithm to potentially nd better constraints to be added in each step , it requires a lot of time in the qp solver .
we found that it is more efcient to wait with recomputing the solution of the qp sub - problem until 123 constraints have been added .
managing the accuracy of the qp sub - problem solver .
in the initial iterations , a relatively low precision solution of the qp sub - problems is sufcient for identify - ing the next violated constraint to add to the working set .
we therefore adjust the precision of the qp sub - problem optimizer throughout the optimization process for removing inactive constraints from the working set .
for both the n - slack and the 123 - slack algorithm , constraints that were added to the working set in early itera - tions often become inactive later in the optimization process .
these constraints can be removed without affecting the theoretical convergence guarantees of the algo - rithm , leading to smaller qps being solved in each iteration .
at the end of each
cutting - plane training of structural svms
iteration , we therefore remove constraints from the working set that have not been active in the last 123 qp sub - problems .
caching ( xi , yi ) ( xi , yi ) in the 123 - slack algorithm .
if the separation oracle returns a label yi for an example xi , the constraint added in the n - slack algorithm en - sures that this label will never again produce an - violated constraint in a subsequent iteration .
this is different , however , in the 123 - slack algorithm , where the same label can be involved in an - violated constraint over and over again .
we therefore cache the f most recently used ( xi , yi ) ( xi , yi ) for each training example xi ( typically f = 123 in the following experiments ) .
lets denote the cache for example x i with ci .
instead of asking the separation oracle in every iteration , the algorithm rst tries to construct a sufciently violated constraint from the caches via
for i=123 , . . . , n do yi max yci
( ( yi , y ) + wwwt ( xi , y ) )
or the analogous variant for the case of slack - rescaling .
only if this fails will the algorithm ask the separation oracle and update the cache .
the goal of this caching strategy is to decrease the number of calls to the separation oracle .
note that in many applications , the separation oracle is very expensive ( e . g . , cfg parsing ) .
parallelization .
while currently not implemented , the loop in lines 123 - 123 of the 123 - slack algorithms can easily be parallelized .
in principle , one could make use of up to n parallel threads , each computing the separation oracle for a subset of the training sample .
for applications like cfg parsing , where more then 123% of the overall runtime is spent on the separation oracle ( see section 123 ) , parallizing this loop will lead to a substantial speed - up that should be almost linear in the number of threads .
solving the dual of the qp sub - problems in the 123 - slack algorithm .
as indi - cated by theorems 123 and 123 , the working sets in the 123 - slack algorithm stay small independent of the size of the training set .
in practice , typically less then 123 con - straints are active at the solutions and we never encountered a single instance where the working set grew beyond 123 constraints .
this makes it advantageous to store and solve the qp sub - problems in the dual instead of in the primal , since the dual is not affected by the dimensionality of ( x , y ) .
the algorithm explicitly stores the hessian h of the dual and adds or deletes a row / column whenever a constraint is added or removed from the working set .
note that this is not feasible for the n - slack algorithm , since the working set size is typically orders of magnitude larger ( often > 123 , 123 constraints ) .
for the experiments in this paper we will consider the following four applications , namely binary classication , multi - class classication , sequence tagging with lin - ear chain hmms , and cfg grammar learning .
they cover the whole spectrum of possible applications , from multi - class classication involving a simple y of low
thorsten joachims , thomas finley , and chun - nam john yu
cardinality and with a very inexpensive separation oracle , to cfg parsing with large and complex structural objects and an expensive separation oracle .
the particular setup for the different applications is as follows .
binary classication .
for binary classication x = n and y = ( 123 , +123 )
( x , y ) = 123
yx and ( y , y ) = 123 ( y ( cid : 123 ) = y ) =
123 if y = y
in the 123 - slack formulation , op123 results in the algorithm presented in ( joachims , 123 ) and implemented in the svm - perf software 123
in the n - slack formulation , one immediately recovers vapnik et al . s original classication svm formulation of op123 ( cortes and vapnik , 123; vapnik , 123 ) ( up to the more convenient percentage - scale rescaling of the loss function and the absence of the bias term ) , which we solve using svmlight .
multi - class classication .
this is another simple instance of a structual svm , where x = n and y = ( 123 , . . . , k ) .
using ( y , y ) = 123 ( y ( cid : 123 ) = y ) and
where the feature vector x is stacked into position y , the resulting n - slack prob - lem becomes identical to the multi - class svm of crammer and singer ( 123 ) .
our svm - multiclass ( v123 ) implementation123 is also built via the svmstruct api .
the argmax for the separation oracle and the prediction are computed by explicit enu -
we use the covertype dataset of blackard , jock & dean as our benchmark for the multi - class svm .
it is a 123 - class problem with n = 123 , 123 examples and 123 features .
this means that the dimensionality of ( x , y ) is n = 123
sequence tagging with linear chain hmms .
in sequence tagging ( e . g . , part - of - speech tagging ) each input x = ( x123 , . . . , xl ) is a sequence of feature vectors ( one for each word ) , and y = ( y123 , . . . , yl ) is a sequence of labels yi ( 123 , . . . , k ) of match - ing length .
isomorphic to a linear chain hmm , we model dependencies between each yi and xi , as well as dependencies between yi and yi123
using the denition of multi ( x , y ) from above , this leads to a joint feature vector of
123 available at svmlight . joachims . org
( yi = 123 ) ( yi123 = 123 ) ( yi = 123 ) ( yi123 = 123 )
( yi = k ) ( yi123 = k )
cutting - plane training of structural svms
hmm ( ( x123 , . . . , xl ) , ( y123 , . . . , yl ) ) =
) ) = l
( yi ( cid : 123 ) = y
we use the number of misclassied tags ( ( y123 , . . . , yl ) , ( y the loss function .
the argmax for prediction and the separation oracle are both com - puted via the viterbi algorithm .
note that the separation oracle is equivalent to the prediction argmax after adding 123 to the node potentials of all incorrect labels .
our svm - hmm ( v123 ) implementation based on svm struct is also available online123
we evaluate on the part - of - speech tagging dataset from the penn treebank cor - pus ( marcus et al , 123 ) .
after splitting the dataset into training and test set , it has n = 123 , 123 training examples ( i . e . , sentences ) , leading to a total of 123 , 123 tags over k = 123 labels .
the feature vectors x i describing each word consist of binary features , each indicating the presence of a particular prex or sufx in the current word , the previous word , and the following word .
all prexes and sufxes observed in the training data are used as features .
in addition , there are features encoding the length of the word .
the total number of features is approximately 123 , 123 , leading to a hmm ( x , y ) of dimensionality n = 123 , 123 , 123
parsing with context free grammars .
we use natural language parsing as an example application where the cost of computing the separation oracle is compar - atively high .
here , each input x = ( x123 , . . . , xl ) is a sequence of feature vectors ( one for each word ) , and y is a tree with x as its leaves .
admissible trees are those that can be constructed from a given set of grammar rules in our case , all grammar rules observed in the training data .
as the loss function , we use ( y , y ) = 123 ( y ( cid : 123 ) = y ) , and cfg ( x , y ) has one feature per grammar rule that counts how often this rule was applied in y .
the argmax for prediction can be computed efciently using a cky parser .
we use the cky parser implementation123 of johnson ( 123 ) .
for the separa - tion oracle the same cky parser is used after extending it to also return the second best solution .
again , our svm - cfg ( v123 ) implementation based on svm struct is
for the following experiments , we use all sentences with at most 123 words from the penn treebank corpus ( marcus et al , 123 ) .
restricting the dataset to short sen - tences is not due to a limitation of svmstruct , but due to the cky implementation we are using .
it becomes very slow for long sentences .
faster parsers that use pruning could easily handle longer sentences as well .
after splitting the data into training and test set , we have n = 123 , 123 training examples ( i . e . , sentences ) and cfg ( x , y ) has a dimensionality of n = 123 , 123
123 available at http : / / www . cog . brown . edu / mj / software . htm
thorsten joachims , thomas finley , and chun - nam john yu
oracle
123 123 123 , 123 , 123 123 , 123 , 123
# support vec .
table 123 training cpu - time ( in hours ) , number of calls to the separation oracle , and number of support vectors for both the 123 - slack ( with caching ) and the n - slack algorithm .
n is the number of training examples and n is the number of features in ( x , y ) .
123 experiment setup
unless noted otherwise , the following parameters are used in the experiments re - ported below .
both the 123 - slack algorithms ( svm struct options - w 123 and - w 123 with caching ) and the n - slack algorithms ( option - w 123 ) use = 123 as the stop - ping criterion ( option - e 123 ) .
given the scaling of the loss for multi - class clas - sication and cfg parsing , this corresponds to a precision of approximately 123% of the empirical risk for the 123 - slack algorithm , and it is slightly higher for the hmm problem .
for the n - slack problem it is harder to interpret the meaning of this , but we will see in section 123 that it gives solutions of comparable precision .
as the value of c , we use the setting that achieves the best prediction performance on the test set when using the full training set ( c = 123 , 123 , 123 for multi - class classi - cation , c = 123 , 123 for hmm sequence tagging , and c = 123 , 123 for cfg parsing ) ( option - c ) .
as the cache size we use f = 123 ( option - f 123 ) .
for multi - class classication , margin - rescaling and slack - rescaling are equivalent .
for the others two problems we use margin - rescaling ( option - o 123 ) .
whenever possible , run - time comparisons are done on the full training set .
all experiments are run on 123 mhz intel xeon processors with 123gb of main memory under linux .
123 how fast is the 123 - slack algorithm compared to the n - slack
we rst examine absolute runtimes of the 123 - slack algorithm , and then analyze and explain various aspects of its scaling behavior in the following .
table 123 shows the cpu - time that both the 123 - slack and the n - slack algorithm take on the multi - class , sequence tagging , and parsing benchmark problems .
for all problems , the 123 - slack algorithm is substantially faster , for multi - class and hmm by several orders of mag -
the speed - up is largest for the multi - class problem , which has the least expensive separation oracle .
not counting constraints constructed from the cache , less than 123% of the time is spend on the separation oracle for the multi - class problem , while it is 123% for the hmm and 123% for cfg parsing .
therefore , it is interesting to also compare the number of calls to the separation oracle .
in all cases , table 123 shows that
cutting - plane training of structural svms
# support vec .
s 123 - slack svmlight 123 - slack svmlight
table 123 training cpu time ( in seconds ) for ve binary classication problems comparing the 123 - slack algorithm ( without caching ) with svmlight .
n is the number of training examples , n is the number of features , and s is the fraction of non - zero elements of the feature vectors .
the svmlight results are quoted from ( joachims , 123 ) , the 123 - slack results are re - run with the latest version of svm - struct using the same experiment setup as in ( joachims , 123 ) .
the 123 - slack algorithm requires by a factor between 123 and 123 fewer calls , accounting for much of the time saved on the cfg problem .
the most striking difference between the two algorithms lies in the number of support vectors they produce ( i . e . , the number of dual variables that are non - zero ) .
for the n - slack algorithm , the number of support vectors lie in the tens or hundreds of thousands , while all solutions produced by the 123 - slack algorithm have only about 123 support vectors .
this means that the working sets that need to be solved in each iteration are orders of magnitude smaller in the 123 - slack algorithm , accounting for only 123% of the overall runtime in the multi - class experiment compared to more than 123% for the n - slack algorithm .
we will further analyze this in the following .
123 how fast is the 123 - slack algorithm compared to conventional
svm training algorithms ?
since most work on training algorithms for svms was done for binary classi - cation , we compare the 123 - slack algorithms against algorithms for the special case of binary classication .
while there are training algorithms for linear svms that scale linearly with n ( e . g . , lagrangian svm ( mangasarian and musicant , 123 ) ( using the 123 loss ) , proximal svm ( fung and mangasarian , 123 ) ( using an l 123 regression loss ) , and interior point methods ( ferris and munson , 123 ) ) , they use the sherman - morrison - woodbury formula ( or similar matrix factorizations ) for in - verting the hessian of the dual .
this requires operating on n n matrices , which makes them applicable only for problems with small n .
the l123 - svm - mfn method ( keerthi and decoste , 123 ) avoids explicitly representing n n matrices using conjugate gradient techniques .
while the worst - case cost is still o ( snmin ( n , n ) ) per iteration for feature vectors with sparsity s , they observe that their method empiri - cally scales much better .
the discussion in ( joachims , 123 ) concludes that runtime is comparable to the 123 - slack algorithm implemented in svm - perf .
the 123 - slack al - gorithm scales linearly in both n and the sparsity s of the feature vectors , even if the
thorsten joachims , thomas finley , and chun - nam john yu
total number n of features is large ( joachims , 123 ) .
note that it is unclear whether any of the conventional algorithms can be extended to structural svm training .
the most widely used algorithms for training binary svms are decomposition methods like svmlight ( joachims , 123 ) , smo ( platt , 123 ) , and others ( chang and lin , 123; collobert and bengio , 123 ) .
taskar et al ( 123 ) extended the smo al - gorithm to structured prediction problems based on their polynomial - size reformu - lation of the n - slack optimization problem op123 for the special case of decomposable models and decomposable loss functions .
in the case of binary classication , their smo algorithm reduces to a variant of the traditional smo algorithm , which can be seen as a special case of the svmlight algorithm .
we therefore use svmlight as a representative of the class of decomposition methods .
table 123 compares the run - time of the 123 - slack algorithm to svmlight on ve benchmark problems with varying numbers of features , sparsity , and numbers of training examples .
the benchmarks include two text classication problems from the reuters rcv123 collection 123 ( lewis et al , 123 ) , a problem of classifying arxiv abstracts , a binary classier for class 123 of the covertype dataset123 of blackard , jock & dean , and the kdd123 physics task from the kdd - cup 123 ( caruana et al , 123 ) .
in all cases , the 123 - slack algo - rithm is faster than svmlight , which is highly optimized to binary classication .
on large datasets , the difference spans several orders of magnitude .
after the 123 - slack algorithm was originally introduced , new stochastic subgradi - ent descent methods were proposed that are competitive in runtime for classication svms , especially the pegasos algorithm ( shalev - shwartz et al , 123 ) .
while currently only explored for classication , it should be possible to extend pega - sos also to structured prediction problems .
unlike exponentiated gradient methods ( bartlett et al , 123; globerson et al , 123 ) , pegasos does not require the compu - tation of marginals , which makes it equally easy to apply as cutting - plane methods .
however , unlike for our cutting - plane methods where the theory provides a practi - cally effective stopping criterion , it is less clear when to stop primal stochastic sub - gradient methods .
since they do not maintain a dual program , the duality gap cannot be used to characterize the quality of the solution at termination .
furthermore , there is a questions of how to incorporate caching into stochastic subgradient methods while still maintaining fast convergence .
as shown in the following , caching is es - sential for problems where the separation oracle ( or , equivalently , the computation of subgradients ) is expensive ( e . g .
cfg parsing ) .
123 how does training time scale with the number of training
a key question is the scalability of the algorithm for large datasets .
while corol - lary 123 shows that an upper bound on the training time scales linearly with the number
123 http : / / jmlr . csail . mit . edu / papers / volume123 / lewis123a / lyrl123 rcv123v123 readme . htm
cutting - plane training of structural svms
123 123 123 123e+123 number of training examples
123 123 123 number of training examples
number of training examples
123 training times for multi - class classication ( left ) hmm part - of - speech tagging ( middle ) and cfg parsing ( right ) as a function of n for the n - slack algorithm , the 123 - slack algorithm , and the 123 - slack algorithm with caching .
123 123 123e+123
number of training examples
123 - slack algorithm with caching
123 123 123e+123
number of training examples
123 training times as a function of n using the optimal value of c at each training set size for the the 123 - slack algorithm ( left ) and the 123 - slack algorithm with caching ( right ) .
of training examples , the actual behavior underneath this bound could potentially be different .
figure 123 shows how training time relates to the number of training exam - ples for the three structural prediction problems .
for the multi - class and the hmm problem , training time does indeed scale at most linearly as predicted by corol - lary 123 , both with and without using the cache .
however , the cache helps for larger datasets , and there is a large advantage from using the cache over the whole range for cfg parsing .
this is to be expected , given the high cost of the separation oracle in the case of parsing .
as shown in figure 123 , the scaling behavior of the 123 - slack algorithm remains largely unchanged even if the regularization parameter c is not held constant , but is set to the value that gives optimal prediction performance on the test set for each training set size .
the scaling with c is analyzed in more detail in section 123 .
123 123 123e+123
number of training examples
thorsten joachims , thomas finley , and chun - nam john yu
123 - slack algorithm with caching
123 123 123e+123
number of training examples
123 number of iterations as a function of n for the the 123 - slack algorithm ( left ) and the 123 - slack algorithm with caching ( right ) .
the n - slack algorithm scales super - linearly for all problems , but so does the 123 - slack algorithm for cfg parsing .
this can be explained as follows .
since the grammar is constructed from all rules observed in the training data , the number of grammar rules grows with the number of training examples .
even from the second - largest to the largest training set , the number of rules in the grammar still grows by almost 123% ( 123 rules vs .
123 rules ) .
this has two effects .
first , the separation oracle becomes slower , since its time scales with the number of rules in the grammar .
in particular , the time the cfg parser takes to compute a sin - gle argmax increases more then six - fold from the smallest to the largest training set .
second , additional rules ( in particular unary rules ) introduce additional fea - tures and allow the construction of larger and larger wrong trees y , which means that r123 = maxi , y|| ( xi , yi ) ( xi , y ) ||123 is not constant but grows .
indeed , figure 123 shows that consistent with theorem 123 the number of iterations of the 123 - slack algorithm is roughly constant for multi - class classication and the hmm 123 , while it grows slowly for cfg parsing .
finally , note that in figure 123 the difference in the number of iterations of the algo - rithm without caching ( left ) and with caching ( right ) is small .
despite the fact that the constraint from the cache is typically not the overall most violated constraint , but only a sufciently violated constraint , both versions of the algorithm appear to make similar progress in each iteration .
123 note that the hmm always considers all possible rules in the regular language , so that there is no growth in the number of rules once all symbols are added .
cutting - plane training of structural svms
123 123 123 123e+123 number of training examples
123 123 123 number of training examples
number of training examples
123 number of support vectors for multi - class classication ( left ) hmm part - of - speech tagging ( middle ) and cfg parsing ( right ) as a function of n for the n - slack algorithm , the 123 - slack algorithm , and the 123 - slack algorithm with caching .
123 what is the size of the working set ?
as already noted above , the size of the working set and its scaling has a substantial inuence on the overall efciency of the algorithm .
in particular , large ( and grow - ing ) working sets will make it expensive to solve the quadratic programs .
while the number of iterations is an upper bound on the working set size for the 123 - slack algorithm , the number of support vectors shown in figure 123 gives a much better idea of its size , since we are removing inactive constraints from the working set .
for the 123 - slack algorithm , figure 123 shows that the number of support vectors does not systematically grow with n for any of the problems , making it easy to solve the working set qps even for large datasets .
this is very much in contrast to the n - slack algorithm , where the growing number of support vectors makes each iteration increasingly costly , and is starting to push the limits of what can be kept in main
123 how often is the separation oracle called ?
next to solving the working set qps in each iteration , computing the separation ora - cle is the other major expense in each iteration .
we now investigate how the number of calls to the separation oracle scales with n , and how this is inuenced by caching .
figure 123 shows that for all algorithms the number of calls scales linearly with n for the multi - class problem and the hmm .
it is slightly super - linear for cfg parsing due to the increasing number of interations as discussed above .
for all problems and training set sizes , the 123 - slack algorithm with caching requires the fewest calls .
the size of the cache has a surprisingly little inuence on the reduction of calls to the separation oracle .
figure 123 shows that a cache of size f = 123 already provides all
thorsten joachims , thomas finley , and chun - nam john yu
123 123 123 123e+123 number of training examples
123 123 123 number of training examples
number of training examples
123 number of calls to the separation oracle for multi - class classication ( left ) hmm part - of - speech tagging ( middle ) and cfg parsing ( right ) as a function of n for the n - slack algorithm , the 123 - slack algorithm , and the 123 - slack algorithm with caching .
size of cache
123 number of calls to the separation oracle as a function of cache size for the the 123 - slack
of the benets , and that larger cache sizes do not further reduce the number of calls .
however , we conjecture that this might be an artifact of our simple least - recently - used caching strategy , and that improved caching methods that selectively call the separation oracle for only a well - chosen subset of the examples will provide further
123 are the solutions different ?
since the stopping criteria are different in the 123 - slack and the n - slack algorithm , it remains to verify that they do indeed compute a solution of comparable effective - ness .
the plot in figure 123 shows the dual objective value of the 123 - slack solution
cutting - plane training of structural svms
123 123 123e+123
123 relative difference in dual objective value of the solutions found by the 123 - slack algorithm and by the n - slack algorithm as a function of c at the maximum training set size ( left ) , and test - set prediction performance for the optimal value of c ( right ) .
relative to the n - slack solution .
a value below zero indicates that the n - slack solu - tion has a better dual objective value , while a positive value shows by which fraction the 123 - slack objective is higher than the n - slack objective .
for all values of c the so - lutions are very close for the multi - class problem and for cfg parsing , and so are their prediction performances on the test set ( see table in figure 123 ) .
this is not sur - prising , since for both the n - slack and the 123 - slack formulation the respective bound the duality gap by c .
for the hmm , however , this c is a substantial fraction of the objective value at the solution , especially for large values of c .
since the training data is almost linearly separable for the hmm , c becomes a substantial part of the slack contri - bution to the objective value .
furthermore , note the different scaling of the hmm loss ( i . e . , number of misclassied tags in the sentence ) , which is roughly 123 time smaller than the loss function on the other problems ( i . e . , 123 to 123 scale ) .
so , an = 123 on the hmm problem is comparable to an = 123 on the other problems .
nevertheless , the per - token test accuracy of 123% for the 123 - slack solution is even slightly better than the 123% accuracy of the n - slack solution .
123 how does the 123 - slack algorithm scale with ?
while the scaling with n is the most important criterion from a practical perspective , it is also interesting to look at the scaling with .
theorem 123 shows that the number of iterations ( and therefore the number of calls to the separation oracle ) scales o ( 123 in the worst cast .
figure 123 , however , shows that the scaling is much better in practice .
in particular , the number of calls to the separation oracle is largely independent of and remains constant when caching is used .
it seems like the additional iterations can be done almost entirely from the cache .
thorsten joachims , thomas finley , and chun - nam john yu
123 number of iterations for the 123 - slack algorithm ( left ) and number of calls to the separation oracle for the 123 - slack algorithm with caching ( right ) as a function of at the maximum training set
123 123 123 123e+123 123e+123 123e+123
123 123 123 123e+123 123e+123 123e+123
123 number of iterations for the 123 - slack algorithm ( left ) and number of calls to the separation oracle for the 123 - slack algorithm with caching ( right ) as a function of c at the maximum training
123 how does the 123 - slack algorithm scale with c ?
with increasing training set size n , the optimal value of c will typically change ( some theoretical results suggest an increase on the order of n ) .
in practice , nding the optimal value of c typically requires training for a large range of c values as part of a cross - validation experiment .
it is therefore interesting to know how the algorithm scales with c .
while theorem 123 bounds the number of iterations with o ( c ) , figure 123 shows that the actual scaling is again much better .
the number of iterations increases much more slowly on all problems .
furthermore , as already observed for above , the additional iterations are almost entirely based on the cache , so that c has hardly any inuence on the number of calls to the separation oracle .
cutting - plane training of structural svms
we presented a cutting - plane algorithm for training structural svms .
unlike ex - isting cutting - plane methods for this problems , the number of constraints that are generated does not depend on the number of training examples n , but only on c and the desired precision .
empirically , the new algorithm is substantially faster than existing methods , in particular decomposition methods like smo and svm light , and it includes the training algorithm of joachims ( 123 ) for linear binary classica - tion svms as a special case .
an implementation of the algorithm is available at svmlight . joachims . org with instances for multi - class classication , hmm sequence tagging , cfg parsing , and binary classication .
acknowledgements we thank evan herbst for implementing a prototype of the hmm instance of svmstruct , which was used in some of our preliminary experiments .
this work was supported in part through the grant nsf iis - 123 from the national science foundation and through a gift
