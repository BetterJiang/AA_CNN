this paper presents a support vector method for optimizing multivariate non - linear performance measures like the f123 - score .
taking a multivariate prediction ap - proach , we give an algorithm with which such multivariate svms can be trained in poly - nomial time for large classes of potentially non - linear performance measures , in partic - ular rocarea and all measures that can be computed from the contingency table .
the conventional classication svm arises as a special case of our method .
depending on the application , measuring the success of a learning algorithm requires application specic in text classication , for ex - ample , f123 - score and precision / recall breakeven point ( prbep ) are used to evaluate classier performance while error rate is not suitable due to a large imbal - ance between positive and negative examples .
how - ever , most learning methods optimize error rate , not the application specic performance measure , which is likely to produce suboptimal results .
how can we learn rules that optimize measures other than error rate ? current approaches that address this problem fall into three categories .
approaches of the rst type aim to produce accurate estimates of the probabilities of class membership of each example ( e . g .
( platt , 123; langford & zadrozny , 123 ) ) .
while based on these probabilities many performance measures can be ( ap - proximately ) optimized ( lewis , 123 ) , estimating the probabilities accurately is a dicult problem and ar - guably harder than the original problem of optimiz - ing the particular performance measure .
a second class of approaches circumvents this problem by op -
appearing in proceedings of the 123 nd international confer - ence on machine learning , bonn , germany , 123
copy - right 123 by the author ( s ) / owner ( s ) .
timizing many dierent variants of convenient and tractable performance measures , aiming to nd one that performs well for the application specic per - formance measure after post - processing the resulting model ( e . g .
( lewis , 123; yang , 123; abe et al . , 123; caruana & niculescu - mizil , 123 ) ) .
however , in par - ticular for non - linear performance measures like f123 - score or prbep , the relationship to tractable mea - sures is at best approximate and requires extensive search via cross - validation .
the nal category of ap - proaches aims to directly optimize the application spe - cic performance measure .
such methods exist for some linear measures .
in particular , most learning al - gorithms can be extended to incorporate unbalanced misclassication costs via linear loss functions ( e . g .
( morik et al . , 123; lin et al . , 123 ) in the context of svms ) .
also , methods for optimizing rocarea have been proposed in the area of decision trees ( ferri et al . , 123 ) , neural networks ( yan et al . , 123; herschtal & raskutti , 123 ) , boosting ( cortes & mohri , 123; fre - und et al . , 123 ) , and svms ( herbrich et al . , 123; rakotomamonjy , 123 ) .
however , for non - linear per - formance measures like f123 - score , the few previous at - tempts towards their direct optimization noted their computational diculty ( musicant et al . , 123 ) .
in this paper , we present a support vector method that can directly optimize a large class of performance measures like f123 - score , precision / recall breakeven point ( prbep ) , precision at k ( prec@k ) , and roc - area .
one diculty common to most application spe - cic performance measures is their non - linear and mul - tivariate nature .
this results in decision theoretic risks that no longer decompose into expectations over indi - vidual examples .
to accommodate this problem , we propose an approach that is fundamentally dierent from most conventional learning algorithms : of learning a univariate rule that predicts the label of a single example , we formulate the learning prob - lem as a multivariate prediction of all examples in the dataset .
based on the sparse approximation algorithm for structural svms ( tsochantaridis et al . , 123 ) , we propose a method with which the training problem can be solved in polynomial time .
we show that the
a support vector method for multivariate performance measures
method applies to any performance measure that can be computed from the contingency table , as well as to the optimization of rocarea .
the new method can be thought of as a direct generalization of classication svms , and we show that the conventional classica - tion svm arises as a special case when using error rate as the performance measure .
we present experiments that compare our algorithm to a conventional classi - cation svms with linear cost model and observe good performance without dicult to control heuristics .
multivariate performance measures
in this section we rst review the typical assumptions ( often implicitly ) made by most existing learning al - gorithms ( vapnik , 123 ) .
this gives insight into why they are not suitable for directly optimizing non - linear performance measures like the f123 - score .
most learning algorithms assume that the training data s = ( ( x123 , y123 ) , . . . , ( xn , yn ) ) as well as the test data s123 is independently identically distributed ( i . i . d . ) ac - cording to a learning task pr ( x , y ) .
the goal is to nd a rule h h from the hypothesis space h that optimizes the expected prediction performance on new samples s123 of size n123
123 ) , . . . , h ( x123
n123 ) ) d pr ( s123 )
if the loss function over samples decomposes linearly into a sum of a loss function over individual examples
123 ) , . . . , h ( x123
123 , . . . , y123
and since the examples are i . i . d . , this expression can be simplied to
r ( h ) = r ( h ) =
( h ( x123 ) , y123 ) d pr ( x123 , y123 )
discriminative learning algorithms approximate this expected risk r ( h ) using the empirical risk on the training data s .
s ( h ) is an estimate of r ( h ) for each h h .
select - ing a rule with low empirical risk r s ( h ) ( e . g .
training error ) in this decomposed form is the strategy followed by virtually all discriminative learning algorithms .
however , many performance measures ( e . g .
prbep ) do not decompose linearly like in eq .
they are a non - linear combination of the individ - an example is the f123 score p rec+rec , where p rec and rec are the precision and the recall of h
( ( h ( x123 ) , . . . , h ( xn ) ) , ( y123 , . . . , yn ) ) = 123 p rec rec
123 , . . . , y123
on the sample ( x123 , y123 ) , . . . , ( xn , yn ) .
there is no known example - based loss function which can be used to de - compose .
therefore , learning algorithms restricted to optimizing an empirical risk of the kind in eq .
( 123 ) are of questionable validity .
what we need instead are learning algorithms that directly optimize an empirical risk that is based on the sample loss .
s ( h ) = ( ( h ( x123 ) , . . . , h ( xn ) ) , ( y123 , . . . , yn ) )
clearly , at least if the size n of the training set and the size n123 of the test set are equal , r s ( h ) is again an estimate of r ( h ) for each h h .
note that r does not necessarily have higher variance than a de - composed empirical risk r s ( h ) just because it does not average over multiple examples .
the key factor is the variance of with respect to samples s drawn from pr ( x , y ) .
this variance can be low .
to design learning algorithms that do discriminative training with respect to r s ( h ) , we need algorithms that nd an h h that minimizes r s ( h ) over the training sample s .
since is some non - linear function of s , this can be a challenging computational problem .
we will now present a general approach to this prob - lem based on support vector machines .
svm approach to optimizing
non - linear performance measures
support vector machines ( svms ) were developed by vapnik et al .
( boser et al . , 123; cortes & vapnik , 123; vapnik , 123 ) as a method for learning linear and , through the use of kernels , non - linear rules .
for the case of binary classication with unbiased hyper - planes123 , svms learn a classier
h ( x ) = sign ( cid : 123 ) wt x ( cid : 123 )
by solving the following optimization problem .
optimization problem 123
( unbiased svmorg )
w w + c i=123 : yi ( w xi ) 123 i
sponding i is greater than 123
thereforepn
the i are called slack variables .
if a training example lies on the wrong side of the hyperplane , the corre - i=123 i is an upper bound on the number of training errors .
this means that the svm nds a hyperplane classier that
123unbiased hyperplanes assume a threshold of 123 in the classication rule .
this is not a substantial restriction , since a bias can be introduced by adding an articial fea - ture to each example .
a support vector method for multivariate performance measures
optimizes an approximation of the training error reg - ularized by the l123 norm of the weight vector .
the factor c in ( 123 ) controls the amount of regularization .
to dierentiate between dierent types of svms , we will denote this version as svmorg .
in the following , we will use the same principles used in svmorg to derive a class of svm algorithms that optimize a broad range of non - linear performance mea - sures .
the key idea is to treat the learning problem as a multivariate prediction problem .
instead of dening our hypotheses h as a function from a single feature vector x to a single label y ( 123 , +123 ) ,
h : x y
we will consider hypotheses h that map a tuple x x of n feature vectors x = ( x123 , . . . , xn ) to a tuple y y of n labels y = ( y123 , . . . , yn )
h : x y ,
where x = x . . .
x and y ( 123 , +123 ) n is the set of all admissible label vectors123
to implement this multivariate mapping , we will use linear discriminant functions of the following form .
( cid : 123 ) wt ( x , y123 ) ( cid : 123 )
hw ( x ) = argmax
123 , . . . , y123
intuitively , the prediction rule hw ( x ) returns the tu - ple of labels y123 = ( y123 n ) which scores highest according to a linear function .
w is a parameter vector and is a function that returns a feature vector describing the match between ( x123 , . . . , xn ) and 123 , . . . , y123 n ) .
whether this argmax can be computed ef - ciently hinges on the structure of .
for the purposes of this paper , we can restrict to be of the following
( x , y123 ) =
for this ( x , y ) and y = ( 123 , +123 ) n , the argmax is achieved when y123 i is assigned to h ( xi ) .
so , in terms of the resulting classication rule , this is equivalent to svmorg .
but did we gain anything from the reformu - lation of the prediction rule ? thinking about the prediction problem in term of a multivariate rule h instead of a univariate rule h al - lows us to formulate the svm optimization problem in a way that enables inclusion of a sample - based loss function instead of the example - based loss function in svmorg .
following ( tsochantaridis et al . , 123 ) , we formulate the following alternative optimization prob - lem for non - negative .
123note that y can be a strict subset for some measures , e . g .
for prec@k it is restricted to label vectors with k pos -
optimization problem 123
( multivar
kwk123 + c y123 y\ y : wt ( ( x , y ) ( x , y123 ) ) ( y123 , y )
like for the svmorg , this optimization problem is a convex quadratic program .
in contrast to the svmorg , however , there is one constraint for each possible y y .
due to the exponential size of y , this may seem like an intractably large problem .
however , by adapting the sparse approximation algorithm of ( tsochantaridis et al . , 123 ) implemented in svmstruct123 , we will show that this problem can be solved in polynomial time for many types of multivariate loss functions .
unlike in the svmorg optimization problem there is only one slack variable in this training problem .
similar to
p i in svmorg , the value of this slack variable is an
upper bound on the training loss .
multi optimization problem on the
theorem 123
at data x with labels y , the value of is an upper bound on the training loss ( hw ( x ) , y ) .
let y123 = hw ( x ) be the prediction of the learned multivariate hypothesis on the training data itself .
following from the denition of h , this is the la - beling y123 that minimizes wt ( ( x , y ) ( x , y123 ) ) , and this quantity will be less than zero unless y123 = y .
therefore ( y123 , y ) wt ( ( x , y ) ( x , y123 ) ) this shows that the multivariate svm multi is similar to the original svmorg in the sense that it optimizes a convex upper bound on the training loss regularized by the norm of the weight vector .
we will later show that , in fact , both formulations are identical if is the number of training errors .
straightforward to extend the multivariate multi to non - linear classication rules via the dual representation of h .
similar to the univariate svmorg , the wolfe dual of optimization problem 123 can be ex - pressed in terms of inner products between feature vec - tors , allowing the use of kernels .
we omit this exten - sion for brevity .
ecient algorithm
how can the optimization problem of the multivariate multi be solved despite the huge number of con - straints ? this problem is a special case of the mul - tivariate prediction formulations in ( tsochantaridis et al . , 123 ) as well as in ( taskar et al . , 123 )
a support vector method for multivariate performance measures
y123 argmaxy123 y ( cid : 123 ) ( y123 , y ) + wt ( x , y123 ) ( cid : 123 )
algorithm 123 algorithm for solving quadratic pro - gram of multivariate svm 123 : input : x = ( x123 , .
, xn ) y = ( y123 , .
, yn ) , c , , y 123 : until c has not changed during iteration
max ( 123 , ( y123 , y ) wt ( ( x , y ) ( x , y123 ) ) ) if ( y123 , y ) wt ( ( x , y ) ( x , y123 ) ) > + then
c c ( y123 ) w optimize svm
multi objective over c
algorithm proposed in ( taskar et al . , 123 ) for solving these types of large quadratic programs is not applica - ble to non - linear loss functions , since it assumes that the loss decomposes linearly .
the sparse approxima - tion algorithm of ( tsochantaridis et al . , 123 ) does not have this restriction , and we will show in the following how it can be used to solve optimization problem 123 in polynomial time for a large class of loss functions .
algorithm 123 is the sparse approximation algorithm adapted to the multivariate svm multi .
the algorithm iteratively constructs a sucient subset of the set of constraints in optimization problem 123
the algorithm starts with an empty set of constraints c and adds the currently most violated constraint in each iteration , i . e .
the constraint corresponding to the label that max -
imizes h ( y ) = ( cid : 123 ) ( y123 , y ) + wt ( x , y123 ) ( cid : 123 ) .
the next ap -
proximation to the solution of optimization problem 123 is then computed on the new set of constraints .
the algorithm stops when no constraint of optimization problem 123 is violated by more than .
it is easy to see that the solution w returned by algorithm 123 fullls all constraints up to precision , and that the norm of w is no bigger than the norm of the exact solution of op - timization problem 123
furthermore , tsochantaridis et al .
( 123 ) show that the algorithm terminates after a polynomial number of iterations .
we restate the the - orem adapted to the svm multi optimization problem .
theorem 123
for any > 123 and a training sample x = ( x123 , .
, xn ) and y = ( y123 , .
, yn ) with r = maxi ||xi|| and l = maxy123 y ( y123 , y ) , algorithm 123 terminates af - ter incrementally adding at most
constraints to the working set c .
the bound is rather lose .
in our experiments we ob - serve that the algorithm often converges after a few
hundred iterations even for large problems .
search for the most violated constraint
( cid : 123 ) ( y123 , y ) + wt ( x , y123 ) ( cid : 123 )
can be performed in polynomial time , the overall algo - rithm has polynomial time complexity .
we will show in the following that solving the argmax eciently is indeed possible for a large class of multivariate loss functions .
we will rst consider multivariate loss functions that can be computed from the contingency table , and then consider the case of roc area .
loss functions based on contingency
an exhaustive search over all y123 y is not feasible .
however , the computation of the argmax in eq .
( 123 ) can be stratied over all dierent contingency tables ,
so that each subproblem can be computed eciently .
algorithm 123 is based on the observation that there are only order o ( n123 ) dierent contingency tables for a binary classication problem with n examples .
there - fore , any loss function ( a , b , c , d ) that can be com - puted from the contingency table can take at most o ( n123 ) dierent values .
lemma 123
algorithm 123 computes the solution of
( cid : 123 ) ( a , b , c , d ) + wt ( x , y123 ) ( cid : 123 )
in polynomial time for any loss function ( a , b , c , d ) that can be computed from the contingency table in proof .
by iterating over all possible contingency ta - bles , the algorithm iterates over all possible values l of ( a , b , c , d ) .
for each contingency table ( a , b , c , d ) it computes the argmax over all yabcd , which is the set of y that correspond to this contingency table .
yabcd = argmax
( cid : 123 ) wt ( x , y123 ) ( cid : 123 )
since the objective function is linear in y123 , the solution can be computed by maximizing y123 element wise .
the maximum value for a particular contingency table is achieved when the a positive examples with the largest value of ( wt xi ) are classied as positive , and the d negative examples with the lowest value of ( wt xi ) are
a support vector method for multivariate performance measures
algorithm 123 algorithm for computing argmax with loss functions that can be computed from the contin - 123 : input : x = ( x123 , .
, xn ) , y = ( y123 , .
, yn ) , and y 123 : for a ( 123 , .
, #pos ) do
#pos ) sort ( i : yi = 123 ) by wt xi #neg ) sort ( i : yi = 123 ) by wt xi
, ip 123 , .
, in c #pos a , .
, y123 for d ( 123 , .
, #neg ) do
to 123 and set y123
123 : end for
b #neg d ,
v ( a , b , c , d ) + wtpn
to123 and set y123 if v is the largest so far then
123 , . . . , y123
classied as negative .
the overall argmax can be com - puted by maximizing over the stratied maxima plus their constant loss .
by slightly rewriting the algorithm , it can be imple - mented to run in time o ( n123 ) .
exploiting that many loss functions are upper bounded , pruning can further improve the runtime of the algorithm .
we will now give some examples of how this algorithm applies to the loss functions we will later use in experiments .
f - score : the f - score is a measure typically used to evaluate binary classiers in natural language appli - cations like text classication .
it is particularly prefer - able over error rate for highly unbalanced classes .
the f - score is a weighted harmonic average of precision and recall .
it can be computed from the contingency
( 123 + 123 ) a
( 123 + 123 ) a + b + 123c
the most common choice for is 123
for the corre - ( y123 , y ) = 123 ( 123 f ) , algorithm 123 sponding loss f123
precision / recall at k in web search engines , most users scan only the rst few links that are presented .
therefore , a common way to evaluate such systems is to measure precision only on these ( e . g .
ten ) positive predictions .
similarly , in an archival retrieval system not precision , but recall might be the most indicative measure .
for example , what fraction of the total num -
ber of relevant documents did a user nd after scan - ning the top 123 documents .
following this intuition , prec@k and rec@k measure the precision and recall of a classier that predicts exactly k documents to be
prec@k ( h ) = a a + b
rec@k ( h ) = a b + d
for these measures , the space of possible prediction vectors y is restricted to those that predict exactly k examples to be positive .
for this y , the multivariate discriminant rule hw ( x ) in eq .
( 123 ) can be computed by assigning label 123 to the k examples with highest wt xi .
similarly , a restriction to this y can easily be incorporated into algorithm 123 by excluding all y123 123= y from the search for which a + b 123= k .
precision / recall break - even point the preci - sion / recall break - even point ( prbep ) is a perfor - mance measure that is often used to evaluate text classiers .
it requires that the classier makes a pre - diction y so that precision and recall are equal , and the value of the prbep is dened to be equal to both .
as is obvious from the denition of precision and recall , this equality is achieved for contingency tables with a + b = a + c and we restrict y appro - priately .
again , we dene the corresponding loss as p rbep ( y123 , y ) = 123 ( 123 p rbep ) and it is straight - forward to compute hw ( x ) and modify algorithm 123 for
roc area
rocarea is a performance measure that cannot be computed from the contingency table , but requires predicting a ranking .
however , both svmorg and multi naturally predict a ranking by ordering all examples according to wt xi .
from such a rank - ing , rocarea can be computed from the number of swappedp airs = | ( ( i , j ) : ( yi> yj ) and ( wtxi<wtxj ) ) | ,
the number of pairs of examples that are ranked in the wrong order .
rocarea = 123 swappedp airs
we can adapt the svm multi to optimizing roc - area by ( implicitly ) considering a classication prob - lem of all #pos#neg pairs ( i , j ) of a positive example ( xi , 123 ) and a negative example ( xj , 123 ) , forming a new classication problem x and y = ( 123 , 123 ) #pos#neg as follows .
each pos / neg pair ( i , j ) receives the target
a support vector method for multivariate performance measures
algorithm 123 algorithm for computing argmax with 123 : input : x = ( x123 , .
, xn ) , y = ( y123 , .
, yn ) 123 : for i ( i : yi = 123 ) do si 123 + wt xi 123 : for i ( i : yi = 123 ) do si 123 + wt xi 123 : ( r123 , .
, rn ) sort ( 123 , .
, n ) by si 123 : sp = #pos , sn = 123 123 : for i ( 123 , .
, n ) do 123 : end for 123 : return ( c123 ,
if yri > 123 then cri ( #neg 123 sn ) sp sp 123 cri ( #pos + 123 sp ) sn sn + 123
( cid : 123 ) wt ( x , y
label yij = 123 and is described by the feature vector xij = xi xj .
in this representation , the discrim - inant rule hw ( x ) = argmaxy123 y sponds to labeling a pair ( i , j ) as sign ( wt xi wt xj ) , i . e .
according to the ordering w . r . t wt xi as desired .
note that the error between the prediction y123 and the true pairwise labels y = ( 123 , . . . , 123 ) t is proportional to 123 rocarea of the original data x and y .
we call this quantity the rocarea - loss .
ij ) = swappedp airs
, y ) =
actually representing all #pos #neg pairs would be rather inecient , but can be avoided using the fol - lowing representation which is linear in the number of
cixi with ci =
note that rocarea ( y computes the argmax in this representation .
lemma 123
for x and y of size n , algorithm 123 com - putes the solution c123 , .
, cn corresponding to
, y ) can now be computed as i=123 yi ( ci c123 i ) .
algorithm 123
, y ) + wt ( x , y
, y ) = 123
in time o ( n log n ) .
the argmax can be written as follows in the
ij ) + y123
ijwt ( xi xj )
since the loss function decomposes linearly over the pairwise representation , we can maximize each yij in -
ij ) + y123 yij ( ( wt xi 123
ijwt ( xi xj )
) ( wt xj +
this means that a pair ( i , j ) should be labeled as yij = 123 , if the score wt xi of the positive example 123 is larger than the score wt xj of the decremented by 123 negative example incremented by 123 123
this is precisely how the algorithm assigns the labels and collects them in the compressed representation .
the runtime of the algorithm is dominated by a single sort operation .
multi generalizes svmorg
the following theorem shows that the multivariate multi is a direct generalization of the conventional classication svm .
when using error rate as the loss function , the conventional svm arises as a special case theorem 123
using error as the loss function , in par - ticular err ( y123 , y ) = 123 ( b + c ) , svmerr multi with regu - larization parameter cmulti computes the same hyper - plane w as svmorg with corg = 123 cmulti .
we will show that both optimization problems have the same objective value and an equivalent set of constraints .
in particular , for every w the smallest
i i are related as = 123p
for a given w , the i in svmorg can be optimized individually , and the optimum is achieved for i = max ( 123 , 123 yi ( wt xi ) ) .
for the svmerr multi , the optimal for a given w is
iwt xi nx
err ( y123 , y ) +
since the function is linear in the y123 i , each y123 i can be optimized independently .
denote with err ( y123 i , yi ) the univariate loss function that returns 123 if both argu - ments dier , and 123 otherwise .
max ( cid : 123 ) 123 , 123 123yiwt xi
iwt xi yiwt xi i , yi ) + y123
( cid : 123 ) = 123
therefore , if corg = 123 cmulti , the objective functions of both optimization problems are equal for any w , and consequently so are their optima w . r . t
a support vector method for multivariate performance measures
table 123
comparing an svm optimized for the performance measure to one that is trained with linear cost model .
reuters ( 123 classes )
arxiv ( 123 classes )
optdigits ( 123 classes )
covertype ( 123 classes )
improvement +123 ( 123 / 123 ) ** +123 ( 123 / 123 ) ** +123 ( 123 / 123 ) +123 ( 123 / 123 ) *
to evaluate the proposed svm approach to optimizing non - linear performance measures , we conducted exper - iments on four dierent test collection .
we compare f123 - score , prbep , rec@k for k twice the number of positive examples ( rec@123p ) , and rocarea achieved multi with the performance of by the respective svm a classication svm that includes a cost model .
the cost model is implemented by allowing dierent reg - ularization constants for positive and negative exam - ples ( morik et al . , 123 ) .
using the parameter j of svmlight , the c parameter of positive examples is multiplied by j to increase their inuence .
this setup is a strong baseline to compare against .
for exam - ple , david lewis won the trec - 123 batch filtering evaluation ( lewis , 123 ) using svmlight with such cost models .
furthermore , musicant et al .
make a theoretical argument that such cost models approximately optimize f123 - score .
we compare performance on four test collections , namely the modapte reuters - 123 text classication benchmark123 , a dataset of abstracts from the physics e - print arxiv , and the optdigits and cover - type benchmarks 123
train / test split and the number of features are given in table 123
initial experiments indicated that biased hyperplane ( i . e .
adjustable threshold ) outperform unbiased hy - perplanes .
we therefore add a constant feature with value 123 to each training example for the svm use biased hyperplanes for the regular svm as imple - mented in svmlight .
to select the regularization para - meter c for the svm multi , and c and j for the classi - cation svm , we used holdout testing with a random 123 123 split of the training set for each class in a collection .
is available at
we search within c ( 123 , . . . , 123 ) and j ( 123 , . . . , 123 ) , but extended the search space if the most frequently selected parameter setting over all classes in the col - lection was on a boundary .
our implementation of svm table 123 shows the macro - average of the performance over all classes in a collection .
each improvement line shows the amount by which the svm performs ( or underperforms ) the regular svm .
both the dierence in performance , as well as the number of classes on which the svm multi won / lost are shown .
stars indicate the level of signicance according to a two - tailed wilcoxon test applied to pairs of results over classes .
one star indicates a signicance level of 123 , two stars a level of 123 .
overall , 123 macroaverages in table 123 show an improvement ( 123 signicant ) , and only 123 cases ( 123 signicant ) show a decline in perfor - mance .
comparing the results between datasets , the improvements are largest for the two text classication tasks , while especially for covertype there is no signicant dierence between the two methods .
with respect to dierent performance measures , the largest gains are observed for f123 - score on the text classica - tion tasks .
prbep and rocarea also show consis - tent , but smaller gains .
on rec@123p , the regular svm appears to perform better on average .
figure 123 further analyzes how the performance diers between the individual binary tasks in the reuters col - lection .
the 123 tasks were binned into 123 sets by their ratio of positive to negative examples .
figure 123 plots the average performance improvement in each bin from the most popular classes on the left to the least popu - lar classes on the right .
for most measures , especially f123 - score , improvements are larger for the less popular
a support vector method for multivariate performance measures
freund , y . , iyer , r . , schapire , r . , & singer , y .
( 123 ) .
an ecient boosting algorithm for combining pref - erences
herbrich , r . , graepel , t . , & obermayer , k .
( 123 ) .
large margin rank boundaries for ordinal regression .
in et a .
( ed . ) , advances in large margin clas - siers .
mit press .
herschtal , a . , & raskutti , b .
( 123 ) .
optimising area under the roc curve using gradient descent
langford , j . , & zadrozny , b .
( 123 ) .
estimating class membership probabilities using classier learners .
lewis , d .
( 123 ) .
evaluating and optimizing au - tonomous text classication systems .
lewis , d .
( 123 ) .
applying support vector machines to the trec - 123 batch ltering and routing tasks .
lin , y . , lee , y . , & wahba , g .
( 123 ) .
support vec - tor machines for classication in nonstandard situ - ations .
machine learning , 123 , 123 123
morik , k . , brockhausen , p . , & joachims , t .
( 123 ) .
combining statistical learning with a knowledge - based approach
musicant , d . , kumar , v . , & ozgur , a .
( 123 ) .
op - timizing f - measure with support vector machines .
platt , j .
( 123 ) .
probabilistic outputs for support vec - tor machines and comparisons to regularized likeli - in et a .
( ed . ) , advances in large margin classiers .
mit press .
rakotomamonjy , a .
( 123 ) .
svms and area under roc
curve ( technical report ) .
psi - insa de rouen .
taskar , b . , guestrin , c . , & koller , d .
( 123 ) .
maximum - margin markov networks
tsochantaridis , i . , hofmann , t . , joachims , t . , & al - tun , y .
( 123 ) .
support vector machine learning for interdependent and structured output spaces
vapnik , v .
( 123 ) .
statistical learning theory .
yan , l . , dodier , r . , mozer , m . , & wolniewicz , r .
( 123 ) .
optimizing classier performance via ap - proximation to the wilcoxon - mann - witney statistic .
yang , y .
( 123 ) .
a study of thresholding strategies for
text categorization .
sigir .
in prediction performance on figure 123
improvement reuters of svm multi over svmorg depending on the bal - ance between positive and negative examples .
results are averaged by binning the 123 categories according to their number of examples .
this paper generalized svms to optimizing large classes of multivariate non - linear performance mea - sures often encountered in practical applications .
we presented a training algorithm and showed that is it computationally tractable .
the new approach leads to improved performance particularly for text classi - cation problems with highly unbalanced classes .
fur - thermore , it provides a principled approach to opti - mizing such measures and avoids dicult to control this work was funded in part under nsf awards iis - 123 and iis - 123
