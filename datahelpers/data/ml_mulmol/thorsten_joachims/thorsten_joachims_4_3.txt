we now have incrementally - grown databases of text docu - ments ranging back for over a decade in areas ranging from personal email , to news - articles and conference proceedings .
while accessing individual documents is easy , methods for overviewing and understanding these collections as a whole are lacking in number and in scope .
in this paper , we ad - dress one such global analysis task , namely the problem of automatically uncovering how ideas spread through the col - lection over time .
we refer to this problem as information in contrast to bibliometric methods that are limited to collections with explicit citation structure , we in - vestigate content - based methods requiring only the text and timestamps of the documents .
in particular , we propose a language - modeling approach and a likelihood ratio test to detect inuence between documents in a statistically well - founded way .
furthermore , we show how this method can be used to infer citation graphs and to identify the most inuential documents in the collection .
experiments on the nips conference proceedings and the physics arxiv show that our method is more eective than methods based on categories and subject descriptors h . 123 ( information systems applications ) : miscellaneous algorithms , measurement , performance information genealogy , flow of ideas , language models , citation inference , text mining , temporal data
in many domains , complete electronic records of docu - ments now reach back for over a decade , including computer science research papers , us news articles , and most peo - ples personal email .
these databases incrementally grow
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page .
to copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specic permission and / or a fee .
kdd123 , august 123 , 123 , san jose , california , usa .
copyright 123 acm 123 - 123 - 123 - 123 - 123 / 123 / 123 . . . $123 .
through an evolutionary process , where new documents are inuenced by the content of already existing documents .
for example , scientic documents extend existing ideas , newssto - ries rene and comment on other articles , and emails aggre - gate or respond to other emails .
while keyword - based retrieval systems allow ecient ac - cess to individual documents in such corpora , we yet lack methods to understand the corpus as a whole .
to rem - edy this shortcoming , this paper investigates whether it is possible to uncover the temporal dependency structure of a corpus .
which documents inuenced each other ? how did ideas spread through the corpus over time ? which docu - ments ( or authors ) were most inuential ? while many of these question have been addressed for hyperlinked data with explicit citation structure , explicit citations are not available in most domains .
we therefore aim to address these questions based on the ( textual ) content of the docu -
the premise for this research is that ideas manifest them - selves in statistical properties of a document ( e . g .
the dis - tribution of words ) , and that these properties can act as a signature for an idea which can be traced through the database .
following this premise , we present a probabilistic model of inuence between documents and design a content - based signicance test to detect whether one document was inuenced by an idea rst presented in another document .
the test takes the form of a likelihood ratio test ( lrt ) and leads to a convex programming problem that can be solved eciently .
our goal is to use this test for inferring an inuence graph derived from the text of the documents alone .
analogous to detecting inheritance from genes , we refer to this text - mining problem as information genealogy .
using corpora of scientic literature , we show that it is in - deed possible to infer meaningful inuence graphs from the text of the documents .
evaluating against the explicit cita - tion graphs for these corpora , we nd that the automatically - computed inuence graphs are similar to the citation graphs .
the ablility to automatically generate an inuence graph for a collection enables a range of applications , from browsing , to visualizing and mining the structure of the network .
as a simple example , we demonstrate that the in - degree of the inuence graph provides an interesting measure of document impact , similar to the in - degree of the citation graph .
measuring influence
in this paper , we investigate and operationalize the no - tion of inuence between documents .
inuence is an inter - esting relationship between documents in historically grown
databases , since such corpora have grown through a self - referential process : documents are inuenced by the con - tent of prior documents , but also contribute new ideas which in turn inuence later documents .
our goal is to uncover and mine how ideas introduced in some document spread through the corpus over time .
at rst glance , one might think that similarity , as cap - tured by information retrieval metrics like tfidf cosine similarity ( see e . g .
( 123 ) ) , provides the full picture of inu - ence .
however , this is not the case .
on the one hand , similarity can occur without inuence .
first , if a document d ( 123 ) introduces an idea that is picked up in documents d ( 123 ) and d ( 123 ) , then d ( 123 ) and d ( 123 ) will likely be similar but do not necessarily inuence each other .
second , two documents might concurrently propose the same idea .
again , neither document inuences the other although the documents likely are similar .
on the other hand , inuence can occur with very little similarity .
in the scientic literature , for example , a large textbook might devote a section to an idea introduced in an earlier research paper .
clearly , the paper had inuence on the textbook .
however , the overall similarity between the book and the paper is small , since the book covers many other ideas as well .
as we will briey review in the following , most prior work on analyzing temporal corpora has focused on identifying relatedness between documents , not inuence .
we will then develop a probabilistic model and a statistical test for de - tecting inuence , and show that it captures inuence better than similarity and provides a more complete understanding and model of inuence .
123 topic detection and tracking
topic detection and tracking ( tdt ) ( 123 , 123 ) has the goal of grouping documents by topic .
unlike inuence , which is a directed relationship , tdt aims to group documents into equivalence classes .
while tdt approaches have relied heavily on nding similarity measures that capture closeness in topic , this approach is not necessarily detecting inuence , as we have argued above .
methods that model inuence not only can detect and track topics and ideas , but also can provide reference points for why a document collection developed as it did .
another minor dierence is that the tdt studies were performed in an online setting , while we assume access to the full corpus at any time .
similar work on detecting and visualizing topic develop - ment includes visualization methods such as temporal clus - ter histograms ( 123 ) and themeriver ( 123 ) , em - based cor - pus evolution detection ( 123 ) , temporal clustering methods ( 123 , 123 ) , continuous time clustering models ( 123 ) , thread decom - position ( 123 ) , independent component analysis ( 123 ) , topic - intensity tracking ( 123 ) , and topical precedence ( 123 ) .
123 real - world inuence on documents
research on burst detection ( 123 ) and timemines ( 123 ) aims to identify hidden causes based on changes in the word dis - tribution over time .
however , their notion of inuence is dierent from ours .
these approaches determine inuence from real - world events on topics ( e . g . , events inuencing us state of the union addresses ) .
instead , we model the inu - ence of documents on each other .
123 citation and hyperlink analysis
in bibliometrics , a documents inuence is measured through
properties of the citation graph ( 123 , 123 , 123 , 123 ) .
our work diers from citation analysis because our method is based on document content , not on citations .
we assume that in - uence is inherently reected in the statistical properties of documents .
in particular , we conjecture that when one doc - ument inuences another , the inuenced document shows traces of the word distribution of the original document123
besides bibliometrics consideration of citation analysis on research papers , other methods work on general hyperlink structure .
one of the most well - known such methods is pagerank ( 123 ) , which uses hyperlink structure to nd in - uential web pages .
123 automatic hypertext
there is related work on automatically adding hyperlinks in information retrieval and related elds .
most promi - nently , link detection was a key task in the tdt evalu - ations ( 123 ) .
several proposals and methods exist for intro - ducing hyperlinks between similar documents or passages of documents ( 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 ) .
good surveys are given in ( 123 ) and the 123 special issue of information processing and management ( 123 ) .
the work we propose is dierent in several respects .
first , our goal is to detect inuence between documents , not just their relatedness .
this will allow a causal interpretation of the resulting cita - tion graph .
second , we take a statistical testing approach to the problem of identifying inuence links , which can be seen as synonymous to citations .
this will give a formal semantic to the predictions of the methods , give theoretical guidance on how to apply the methods , and expose under - 123 language and topic models
we take a probabilistic language modeling approach in the development of our methods .
while we rely on a rather basic language model for the sake of simplicity , more de - tailed language models exist and can possibly be employed as well .
previous work by steyvers et al .
( 123 ) looks at how document text can be generated by a two - step model of gen - erating topics probabilistically from authors , and then words probabilistically from topics .
there has also been language modeling work done in the natural language processing and machine learning ( 123 , 123 , 123 ) , speech recognition ( 123 ) , and information retrieval communities ( 123 , 123 , 123 ) .
in constructing an inuence graph for a database of doc - uments , the core problem is to determine when and where ideas ow from one document to another document .
in the following , we propose a probabilistic model of inuence in a language - modeling framework , and develop a likelihood ratio test ( lrt ) ( 123 ) for detecting whether one document has signicantly inuenced another document .
123 probabilistic model and motivation
to make the method widely applicable , we have only two basic requirements for our corpus of documents rst , the documents contain text and , second , the documents have
123note that our goal is not plagiarism detection , where au - thors would try to disguise their choice of words .
timestamps .
formally , the corpus d is a collection of n doc - uments ( d ( 123 ) d ( n ) ) , where each document d ( i ) d has an associated timestamp time ( d ( i ) ) .
there are m dier - ent terms ( i . e .
words ) across the entire corpus , which are denoted by ( t123 tm ) .
we assume that the document is a vector - valued random variable d = ( w123 w|d| ) , which describes a document as a sequence of random variables wi , one for each word in the document .
a particular observed document is denoted as d = ( w123 w|d| ) .
in the following , we assume that each document d ( i ) d was generated by a unigram language model p ( d ( i ) = d ( i ) | ( i ) ) with parameters ( i ) specic to
model 123
( document language model ) a document d ( i ) d is assumed to be generated by independently drawing |d ( i ) | words from a document spe - cic distribution with individual word probabilities ( i ) = t123 , . . . , ( i ) p ( d ( i ) = d ( i ) | ( i ) ) = p ( d ( i ) = ( w ( i )
tm ) , i . e .
|d ( i ) | ) | ( i ) )
p ( w ( i ) = w ( i )
note that we do not explicitly model document length .
we chose this basic language model for mathematical and computational convenience .
however , our approach can be extended to more complex language models as well ( e . g
since we wish to detect the ow of ideas and inuence between documents , we also need a model of inter - document relationship .
we formalize this as a question of how the language model ( new ) of a new document d ( new ) depends on the language models ( ( 123 ) ) of the documents that precede ( new ) in time .
in particular , we assume that the language model of a new document can be ( approximately ) expressed as a mixture distribution over the language models of previous documents .
model 123
( inter - document influence model )
a new document d ( new ) is generated by a mixture distri - bution of the already existing documents d ( i ) with i p for p = ( i : time ( d ( i ) ) < t123 ) , in particular
p ( d ( new ) = d ( new ) | ) =
with mixing weights satisfying 123 i andp
pp ( w ( p ) = wj| ( p ) )
i = 123
in this dependency model , a new document is composed of parts generated by the word distributions of old docu - ments , where the mixing coecient p indicates the frac - tion of d ( new ) that is generated from d ( p ) .
clearly , there is direct inuence of a document d ( p ) on d ( new ) , if the respec - tive mixing coecient is non - zero .
note that the resulting language model for d ( new ) is again a unigram model , so that p ( d ( new ) = d ( new ) | ) = p ( d ( new ) = d ( new ) | ( new ) ) with
in actual document collections , documents typically con - tain some original part that does not come from previous documents .
to account for the original portion of a docu - ment in our model , we include a distribution ( o ) with weight o in the mixture .
it models the distribution of words that is original to the document and that cannot be explained by previous documents .
( in practice , we will assume that o is xed , but that we have no knowledge of ( o ) .
model 123
( inter - document influence model with
a new document d ( new ) is generated by a mixture distri - bution of the already existing documents d ( i ) with i p for p = ( i : time ( d ( i ) ) < t123 ) , and a document specic mixture component ( o ) with weight o , in particular
p ( d ( new ) = d ( new ) | ) =
with mixing weights s . t .
123 i , o and o +p
pp ( w ( p ) = wj| ( p ) )
i = 123
in the case when the documents have no original content , setting o = 123 in the inter - document inuence model with original content results in model 123
vice versa , model 123 also subsumes model 123 by simply introducing an articial single - word document for each term in the corpus and con - straining their mixture weights to sum to o .
we will there - fore focus our further derivations on model 123 for the sake of
we will now show how this probabilistic setup can be used in a signicance test for detecting whether a particular mixing weight p is non - zero in a given document collection .
123 a statistical test for detecting inuence how can one decide whether a candidate inuential doc - ument d ( can ) had a signicant inuence on d ( new ) given the other documents in the collection ? first , d ( can ) can only have had an inuence on d ( new ) if it had been pub - lished before d ( new ) ( i . e .
time ( d ( can ) ) < time ( d ( new ) ) ) .
note that this is already encoded in the inter - document inu - ence models dened above .
second , inuence should be attributed to the rst publication that introduced an idea through an original section or portion , not to other docu - ments that later copied an idea .
to illustrate this in the context of research papers , this means that inuence should be credited to the original article , not a tutorial that repro - duced the original idea .
under these conditions , the decision of whether document d ( new ) shows signicant inuence from d ( can ) can be phrased as a likelihood ratio test ( 123 ) .
in general , a likelihood ratio test decides between two families of densities described by sets of parameters and 123 that are nested , i . e .
applied to our case , will be all mixture models of d ( new ) as in eq .
( 123 ) with parameters i for all documents p published prior to t123 = time ( d ( can ) ) ( and therefore prior to d ( new ) ) , as well as a parameter can for d ( can ) .
i = 123 i 123
the subset 123 of the mixture models in will be the models where d ( can ) has zero mixture weight ( i . e .
can = 123 ) .
i = 123 i 123 can = 123
note that the set of prior documents p = ( i : time ( d ( i ) ) < time ( d ( can ) ) ) serves as a background model of what was already known when d ( can ) was published .
against this background , we can then measure how much the new ideas in document d ( can ) inuenced d ( new ) .
the null hypothesis of the likelihood ratio test is that the data comes from a model in 123 ( i . e .
document d ( new ) was not inuenced by d ( can ) given the documents published before d ( can ) ) .
to reject this null hypothesis , a likelihood ratio test considers the following test statistic
sup123 ( p ( d ( new ) = d ( new ) | ) ) sup123 ( p ( d ( new ) = d ( new ) |123 ) )
d ( can ) ( d ( new ) ) = note that p ( d ( new ) = d ( new ) | ) is convex over and 123 , so that the suprema can be computed eciently .
we will elaborate on the computational aspects below .
intuitively , the value of d ( can ) ( d ( new ) ) measures whether using d ( can ) in the mixture model better explains the content of d ( new ) than just using previously published documents .
more formally , d ( can ) ( d ( new ) ) compares the likelihood sup123 ( p ( d ( new ) = d ( new ) |123 ) ) of the best mixture model containing d ( can ) with the likelihood sup123 ( p ( d ( new ) = d ( new ) | ) ) of the best mixture model that does not use d ( can ) ( i . e .
can = 123 ) .
the test then decides whether there is signicant evidence that a non - empty part of d ( new ) was generated from d ( can ) , in comparison to using a mixture only over the other language if the null hypothesis is true , then the distribution of the lrt statistic 123 log ( d ( can ) ( d ( new ) ) ) is asymptotically ( in the document length under the unigram model ) 123 with one degree of freedom .
123 log ( d ( can ) ( d ( new ) ) ) 123 the null hypothesis h123 should be rejected , if 123 log ( d ( can ) ( d ( new ) ) ) > c
for some c selected dependent on the desired signicance level .
for a signicance level of 123% , c should be 123 .
this captures the intuition that we can reject the null hypothe - sis and conclude that d ( can ) had a signicant inuence on d ( new ) , if the best model that does not use d ( can ) has a much worse likelihood than the best model that considers d ( can ) .
specically , if 123 log ( d ( can ) ( d ( new ) ) ) is large , then d ( can ) signicantly inuenced d ( new ) given all other docu - ments published at that time .
to estimate the language models ( i ) of the documents en - tering into the mixture model of d ( new ) , we use the maximum - likelihood estimate .
we denote with tf ( i ) the term frequency ( tf ) vector of document d ( i ) , where each entry tf ( i ) number of times that term tj appears in the document d ( i ) .
the estimator is
which is simply the fraction of times the particular word oc - curs in the observed document d ( i ) .
using a more advanced
estimator instead is straightforward , but we will not discuss this for the sake of simplicity .
123 relating the lrt to detecting inuence
what does it mean for the lrt to signicantly reject the null hypothesis ? a good intuition is to think of this method in the context of trying to explain the ideas and content found in d ( new ) .
there are two choices .
first , ex - plain d ( new ) using only other documents preceding d ( can ) as well as some original component .
second , explain d ( new ) with these plus an additional d ( can ) .
if the rst case already provides a wonderful model for d ( new ) , then adding d ( can ) will not explain d ( new ) any more accurately .
thus , d ( can ) really does not contribute to d ( new ) .
on the other hand , if d ( can ) introduced some new ideas and terminology that then owed to d ( new ) , using d ( can ) will provide a better ex - planation than only using p .
consequently , the likelihood of d ( new ) using d ( can ) will be signicantly higher than with - out it , and we can reject the null hypothesis .
to summarize , rejecting the null hypothesis means that d ( can ) signicantly exerted inuence on d ( new ) .
123 computing the lrt
computing the value of d ( can ) ( d ( new ) ) requires solving
two optimization problems .
l123 = sup l = sup
( p ( d ( new ) = d ( new ) | ) ) and ( p ( d ( new ) = d ( new ) | ) ) .
given our model , these problems can be solved eciently .
note that we can write the log - likelihood l ( | d ( new ) , s ) of the document d ( new ) w . r . t .
a xed as
log l ( | d ( new ) ) = log p ( d ( new ) | )
with s we denote the set of documents considered in the model .
this gives s = p ( can ) for and s = p for 123
in this notation , each of the optimization problems in eq .
( 123 ) and ( 123 ) takes the form
log l ( | d ( new ) )
i s : i 123
for model 123 an additional linear constraint is introduced to limit the amount of original content o to not be more than a user - specied parameter .
this constraint is necessary , since otherwise the ( o ) mixture component could always perfectly explain d ( new ) .
it is easy to see that these optimization problems are con - vex , which means that they have no local optima and that there are ecient methods for computing the solution .
we currently use the separable convex implementation for the general - purpose solver mosek ( 123 ) to solve the optimization problems .
however , more specialized code is likely to be substantially more ecient .
while solving each optimization problem is ecient , an - alyzing a collection requires a quadratic number of lrts , each with on the order of n documents in the background
in particular , for each document d ( new ) , we need to test all prior documents
d ( i ) : time ( d ( i ) ) < time ( d ( new ) )
in the collection , since all of these are candidates for having inuenced d ( new ) .
for each document d ( can ) in the candidate candidate set c of d ( new ) , we then have a background model
d ( i ) : time ( d ( i ) ) < time ( d ( can ) )
computing all tests exhaustively for a large corpus can be expensive .
we therefore use the following approximations .
both approximations are based on the insight that some similarity is necessary for inuence .
the potentially inuen - tial document d ( can ) must have some similarity with d ( new ) .
therefore , we rst approximate the candidate set to con - tain the kc nearest neighbors of d ( new ) from c .
we use cosine distance between tf and tfidf vectors for docu - ment similarity .
second , an analogous argument applies to the background models pd ( can ) .
we therefore approximate the background model , using only the kp most similar docu - ments from p .
since selecting p combines document vectors by addition , we use cosine distance between document tf vectors to select p .
in the experiments we set kc = kp and refer to this parameter as k .
we will empirically evaluate the eect of these approximations depending on k .
we wish to measure how well these models assumptions match real data .
first , how does an inuence graph inferred by the lrt method compare against a citation graph ? sec - ond , can the inuence graph identify top inuential papers ? 123 experiment setup and corpora
the concept of inuence and idea ow between documents corresponds well with the notion of a citation .
consequently , we focus on research papers to provide a quantitative eval - uation of the lrt method by comparing with citations .
the rst corpus is the full - text proceedings of the neu - ral information processing systems ( nips ) conference ( 123 ) from 123 - 123 , with a timestamp of the publication year .
nips has 123 documents , with 123 terms ( features ) .
we manually constructed the graph of 123 intra - corpus cita - tions , but only compare to citations of previous documents in time .
we ignore citations of rst - year documents since the lrt requires a background model .
the second corpus is the theoretical high - energy physics ( hepth ) section of the physics arxiv ( 123 ) from aug .
123 to apr .
we aggregate the full - text papers by year .
hepth has 123 documents , 123 terms , and 123 citations .
slac - spires compiled these citations .
inferring inuence graphs
this set of experiments analyzes how well the lrt re - covers the inuence graph .
after an illustrative example , we explore the lrts sensitivity on synthetic data under controlled experiment conditions , and then evaluate on two 123 . 123 qualitative evaluation we rst discuss a simple example to illustrate the lrt methods behavior and how it compares to citations .
fig - ure 123 shows those documents that nips document 123
figure 123 : roc - area comparing the lrt method against a cosine similarity baseline .
the x - axis is can .
at a can level , the roc - area measures the quality of inuence prediction in documents with the specied can as compared against documents with can = 123
( schoelkopf et al .
on shrinking the tube : a new sup - port vector regression algorithm ) most signicantly in - uenced according to the lrt statistic .
three of the top ve papers actually cite document 123 ( or a document with equivalent content from another venue ) .
furthermore , the top document could arguably have cited 123 as well , since it relies on the - parameterization of svms that document 123 introduced to nips .
in fact , all papers ( except fast training of support vector classiers ) consider this new parameterization .
note that the paper - arc : ensemble learning in the presence of outliers is not about svms , but uses the - parametrization in the context of boosting .
the lrt appears to accurately focus on the papers origi - nal contribution , the - parameterization .
general svm pa - pers do not score highly , since they are already modeled by earlier papers , e . g .
paper 123 support vector method for function approximation , regression estimation , and sig - nal processing of v .
vapnik et al . , which was one of the rst svm papers in nips .
when considering inuencers of a support vector method for clustering by a .
ben - hur et al .
( using the conventional parameterization ) , the method correctly recognizes that paper 123s inuence is low ( 123 log ( d ( 123 ) ( d ( new ) ) ) = 123 ) even though the docu - ments are similar .
paper 123 already explains the svm content ( 123 log ( d ( 123 ) ( d ( new ) ) ) = 123 ) .
123 . 123 quantitative evaluation on synthetic data beyond this qualitative example , how accurately can the lrt discover inuence ? how much must d ( new ) copy from d ( can ) before the lrt can detect it ?
to explore these questions , we constructed articial docu - ments d ( new ) from the nips corpus as follows .
a candidate document d ( can ) and a set p of k = 123 previous documents are chosen at random form the nips corpus so that the documents in p preceed d ( can ) in time .
then , 123 articial new documents are generated according to eq .
123 , where each new document has been inuenced by d ( can ) at the fractional levels of can ( 123 , 123 , 123 , , 123 ) .
the remaining mixing weights i are selected by generating random num -
123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123roc - areacandidate document mixing weightlrt roc - areasim roc - area 123 log ( d ( 123 ) ( d ( new ) ) ) cite ? title and author ( s ) of d123
support vector method for novelty detection , b .
schoelkopf , robert c .
williamson , alex smola , john shawe - taylor , john c .
an improved decomposition algorithm for regression support vector machines , pavel - arc : ensemble learning in the presence of outliers , gunnar raetsch , b .
scholkopf , alex smola , kenneth d .
miller , takashi onoda , steve mims .
fast training of support vector classiers , fernando perez - cruz , pedro alarcon - diana , angel navia - vazquez , antonio artes - rodriguez .
uniqueness of the svm solution , christopher j .
burges , david j .
crisp .
figure 123 : papers that are inuenced by nips paper 123 , shrinking the tube : a new support vector regression algorithm written by b .
schoelkopf , p .
bartlett , a .
smola , and r .
williamson .
the leftmost column shows the lrt statistic value .
( larger lrt statistic values represent greater inuence . )
bers uniformly on the interval ( 123 , 123 ) , and then normalizing them so that they sum to 123 can .
the lrts are run on each new document .
additionally , tf document vector co - sine similarity is measured between d ( can ) and each d ( new ) .
the entire process is repeated for 123 random selections of p and d ( can ) .
we computed roc - area in the following manner .
first , we select a particular can ( 123 123 ) .
the generated documents at the can level are marked as positive exam - ples .
the negative examples are documents with can = 123
finally , a ranking , either lrt statistic scores or cosine dis - tance similarity , is used to compute roc - area .
figure 123 shows that even if only a small portion ( i . e .
a few percent ) of d ( new ) is drawn from d ( can ) , the lrt accu - rately detects the inuence .
the similarity baseline needs a much larger signal .
this example illustrates that similarity and inuence are in fact dierent , and that the well - founded statistical approach can be more accurate and sensitive than an ad - hoc heuristic .
123 . 123 quantitative evaluation on real data moving to real data , we use the lrts to discover the in - uence graph for nips and hepth .
for each document d ( new ) , we rst compute a set of candidate documents c based on similarity .
the elements of c are then ranked ac - cording to the lrt statistic ( i . e .
whether d ( can ) was signif - icant in explaining d ( new ) ) .
the higher d ( can ) is ranked , the more likely that it inuenced d ( new ) , and we can derive the inuence graph by thresholding ( discussed below ) .
we evaluate the inuence graph by a graph - based mean - average - precision ( g - map ) metric .
for a document d , aver - age the precision of the ranked predicted list of inuencers at the positions corresponding to documents that d actu - ally cites .
citations not in the list are averaged as 123 , i . e .
ranked at innity .
( as an information retrieval analogy , the inuence list is the search result page , with citations being relevant results . ) g - map is the mean of the per - document average precision scores .
we exclude documents from the rst two years due to edge eects ( the lrt cannot predict citations for the rst years since c or p are empty ) .
we compare g - map for the lrt method against g - map of a similarity - based heuristic , which serves as a baseline .
this baseline method ranks the elements of c not by lrt score , but by similarity .
we explored several similarity mea - sures .
the best similarity measures in our experiments are tf cosine and tfidf cosine .
we report their performance .
note that citations are not necessarily a perfect gold stan -
table 123 : g - map scores comparing the lrt against the similarity baseline .
the similarity measure to select p is the tf cosine and to select / rank c is either the tf cosine or the tfidf cosine .
results are reported for k = 123 and = 123 .
tfidf c for lrt with k = 123 and s = . 123
figure 123 : precision vs .
recall on nips .
the three lines are ( from top to bottom ) the lrt methods precision at a recall level with tfidf cosine used to select c , the tfidf distance c similarity baseline , and the tf distance c similarity baseline .
dard for inuence , since they reect idiosycracies of how sci - entic communities cite prior work .
for example , in figure 123 authors sometimes cited a journal paper or book instead of the nips paper .
therefore , a g - map of 123 is not achievable .
lrts are more accurate than similarities .
table 123 shows that the lrt achieves higher g - map scores than the similarity baselines on both nips and hepth .
among the two heuristic baselines , tfidf cosine performs better then tf cosine .
tfidf cosine also appears to select better sets c for the lrt .
the hepth results are reported for a random sample of 123 documents .
123 123 123 123 123 123 123 123 123 123 123 123precisionrecallnips k=123 prec vs rectf similarity baselinetfidf similarity baseline tf
table 123 : g - map scores comparing the lrt for a range of d ( can ) inuence mixing weights against the similarity baseline .
the similarity measure to select c is either tf or tfidf cosine .
results are reported on nips for k = 123
nips ( k = 123 ) nips ( k = 123 ) hepth ( k = 123 ) hepth ( k = 123 )
table 123 : g - map scores comparing the lrt against the similarity baseline for two k - nn approximation levels .
the similarity measure for selecting c is ei - ther tf or tfidf cosine .
results are reported on nips and hepth for = . 123
lrt scores are more comparable than similarities .
table 123 showed that the lrt can nd the most inuential papers for one particular document .
figure 123 measures how well it can nd the strongest edges in the whole inuence graph .
this precision - recall graph uses the ranking of all lrt statistic scores of all documents , with actual citations marked as positive examples .
figure 123 also shows the scores for using lists of tf and tfidf cosine similarities .
the lrt graph dominates the similarity baselines over the whole range and the dierence in performance is larger than in the per - document evaluation .
we conclude from this that lrt scores are more comparable between documents than similarity scores .
this is to be expected because the lrt values have a clear probabilistic semantic .
however , the similarity scores have no such guarantees .
effects of the parameter .
table 123 shows that the lrt is robust over a large range values .
the lrts g - map dominates the similarity base - lines .
however , = 123 seems to perform better than our initial guess of 123 used above .
effect of k parameter in lrt approximations .
table 123 shows g - map scores at diering levels of the k - nn approximation .
recall from table 123 that g - map scores for hepth are substantially lower than for nips .
we con - jecture that this is due to the size of the corpus in relation to k .
with a large corpus , k = 123 is likely to exclude too many relevant documents from consideration .
we further analyze the role of k , in its two roles in controlling the sizes of c and p .
first , k controls the size of c .
if k is too small , truly inuential documents will not be tested by the lrt .
e . g . , in hepth , each document has 123 citations on average .
with k = 123 , it would be simply impossible to recover the entire citation graph .
therefore we conclude that k must be large enough to include all documents that make contributions to d ( new ) .
on hepth , k = 123 is better than k = 123 for tf
gmap gmap ( perfect c )
table 123 : how close is the approximation to the op - timal ? g - map scores are reported for s = . 123
and tfidf cosine , and for lrt and similarity baseline .
we believe this is because k = 123 is too restrictive .
nips with tf cosine shows the same behavior .
to better understand how much loss in performance is due to the k - nn approximation of c , the following experiment explores the g - map scores of the lrt for a perfect c .
in particular , we construct c so that it includes all documents that d ( new ) actually cites , and then ll the remaining places in c with the most similar documents .
table 123 shows that for k = 123 the loss in performance due to an approximate c is fairly small on nips .
for hepth , on the other hand , k = 123 shows a much greater loss , with g - map scores only about 123 - 123% of the optimal .
we believe this loss oc - curs because c is too small to accomodate all the inuential
identifying inuential documents
what are the inuential documents that have the most eect on the document collections development ? which documents should one read to best grasp this development ? we have already shown that lrts can be used to infer an inuence graph that is similar to a citation graph .
we now investigate whether this inuence graph can be used to iden - tify the documents with the overall largest inuence on the collection .
in analogy to citation counts ( i . e .
the in - degree in the citation graph ) , we propose the in - degree in the inu - ence graph as a measure of impact .
if not noted otherwise , we form the inuence graph by connecting each document d ( new ) with the l other nodes that receive the highest lrt value .
we typically use l = 123 , although we also explore this parameters eect on performance .
123 . 123 qualitative evaluation for each year in nips , table 123 lists the paper with the highest in - degree in the inuence graph computed by the lrt method with k = 123 and l = 123
we expect these to have high citation counts , which we test by showing the papers citation counts both from within the nips corpus ( as of 123 ) and from google scholar ( as of 123 ) .
for most documents , the citation count is indeed high when compared to the average nips document citation count of 123 other nips papers .
an interesting example is sup - port vector method for function approximation , regres - sion estimation , and signal processing from 123
while this is one of the papers that introduced svms to nips , it has only 123 citations within nips and only 123 citations in google scholar .
nevertheless , svms had a huge impact on nips .
in this sense our lrt method is correct and is not inuenced by citation habits .
in this example , most au - thors cite vapniks later book ( with 123 citations ) instead of this paper .
the lrt method is unaected and correctly identies the svm idea as highly inuential on nips .
year document title and author ( s )
nips google scholar
an optimality principle for unsupervised learning by terence d .
sanger training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters by john s .
bridle learning theory and experiments with competitive networks by gni l .
bilbro , david e .
van den bout the eective number of parameters : an analysis of generalization and regularization in nonlinear learning systems by john moody reinforcement learning applied to linear quadratic regulation by steven j .
bradtke supervised learning from incomplete data via an em approach by zoubin ghahramani , michael i .
jordan reinforcement learning algorithm for partially observable markov decision problems by tommi jakkola , sizarad singhal , michael i .
jordan em optimization of latent - variable density models by chris m .
bishop , m .
svensen , chisto - pher k . i .
williams support vector method for function approximation , regression estimation , and signal pro - cessing by v .
vapnik , steven e .
golowich , alex smola em algorithms for pca and spca by sam roweis
table 123 : the most inuential paper per year in nips , as measured by inuence graph in - degree , with k = 123 , = . 123 , and tfidf cosine for c .
we exclude years with edge eects and the last 123 years , since they do not have statistically signicant counts .
comparison is against the within - nips citation counts , and google - scholar citation counts ( on feb .
123 , 123 ) .
table 123 : rank metrics comparing the lrt against similarity on nips ( k = 123 ) and hepth ( k = 123 ) , using = . 123 and tf or tfidf cosine for c .
we ignore the rst two and last two years because of edge eects .
123 . 123 quantitative evaluation we compare the ranking of documents by in - degree in the inuence graph to the ranking by citation count .
as simi - larity measures , we use kendalls and a ranking version of map , which we term r - map .
kendalls measures how many pairs two rankings rank in the same order .
it ranges between - 123 and 123 , with higher numbers indicating greater similarity .
formally , 123 number of concordant pairs
total number of pairs number of tied pairs
r - map@k measures the average precision of a ranking .
with the k top - ranked documents as positive examples , av - erage the rankings precision at the positions of these docu - ments .
we calculate r - map@123 and r - map@123
there is one caveat with rank - based metrics .
edge eects ( e . g . , older papers have more citations , papers from the last year have no citations ) make it dicult to present one uni - ed ranking of all documents .
therefore , we calculate each metric per - year and average the year - by - year values to get a single score for the entire corpus .
additionally , because of edge eects , the rst two and the last two years are not used , since they do not contain meaningful results .
the tf and tfidf baselines use the most similar docu -
ments instead of the lrt predictions .
lrts are better than similarity .
table 123 shows that the lrt gives substantially better rankings than the similarity baseline for all metrics on both nips and hepth with both tf and tfidf cosine c .
effect of the parameter l .
the left plot of figure 123 explores whether selecting in - uencers is sensitive to the parameter l .
for the inuence graph , we considered each documents l predicted inuencers with highest lrt scores .
figure 123 shows how varying l af - fects for both lrt and the similarity baseline .
since nips documents do not have many citations , we explore l = 123 to 123
the upper line is lrt performance with 123% con - dence interval error bars .
( the condence interval is com - puted using the multiple values per data point , because each graphed is the average of multiple ( here , 123 ) years of metric scores . ) the lower line depicts on the similar - ity baseline .
for the tfidf cosine c , when l is small , the method computes a count over only the few top inuential documents selected by the lrts for d ( new ) .
it turns out that small l seem to perform better than our initial guess of l = 123
as l increases , more non - inuential documents are counted and correspondingly falls .
when l approaches 123 ( not shown ) , the lrt and the baseline are identical as expected by construction .
thresholding on the lrt score .
the right plot of figure 123 depicts how varies if we do not select a xed number of l neighbors per document , but instead use a threshold on the lrt statistic .
the lrt is set up to reject the null hypothesis and declare that d ( can ) inuences d ( new ) if the lrt statistic is suciently large .
varying this threshold controls the level of condence in the lrt , so we use the threshold level as the x - axis and examine how it aects .
thresholding the lrt values actually gives better performance than using the l parameter , since we are not forcing a certain number of inuence links for each doc - ument .
there are four dierent regions in this graph .
first ,
figure 123 : using to compare the lrt against the similarity baseline , both with the l parameter ( left ) and by thresholding the lrt statistic values ( right ) .
results are for nips with tfidf cosine c and k = 123
the tf plot looks similar , except that the baseline is smoother .
if the threshold is too low , performance suers because the null hypothesis is being accepted erroneously .
second , per - formance increases as the threshold approaches reasonable condence levels .
third , a large range of threshold values ( approximately 123 - 123 ) give good and similar scores , showing that the lrt method is robust .
fourth , when the threshold is too high , many inuential documents are no longer detected , and performance subsequently falls .
note that a condence level of 123% per test ( i . e .
a thresh - old of 123 ) performs quite poorly .
this level means that 123% of the inuence links are erroneous .
nips , with 123 papers , would have an expected 123 , 123 false links ( and only 123 real citations ) .
therefore , we need a much higher condence level to account for the multiple - testing bias .
using bonfer - roni adjustment , each tests level is the overall level divided by the number of tests .
discussion and future work
one obvious limitation of the current model is the sim - plicity of the language model .
the assumption that each document is a sequence of independent words is , in reality , likely violated .
this observation motivates more expressive language models such as n - gram language models .
there is also the question of whether these methods can generalize to other domains .
lrts do not use citation data , so many domains should be applicable .
however , we have only conducted experiments on research publications .
finally , there is scalability and eciency .
much of the computing time is spent solving convex optimization prob - lems .
while c and p prune this space , there may be other criteria to provably eliminate certain lrts without aecting the results .
furthermore , the optimization problems have a special structure , which can probably be exploited by spe - cialized methods to solve the optimization problems .
we presented a probabilistic model of inuence between documents for corpora that have grown over time .
in this model , we derived a likelihood ratio test to detect inu - ence based on the content of documents and showed how the test can be computed eciently .
we found that the inuence
graphs derived from the content resemble the structure of ex - plicit citation graphs for corpora of scientic literature .
fur - thermore , we showed that in - degree in the inuence graph is an eective indicator of a documents impact .
the ability to create inuence graphs based on document content alone has the potential to open databases without explicit citation structure to the large repertoire of graph mining algorithms .
we thank johannes gehrke and rich caruana for the dis - cussions that lead to this work .
this work was funded in part by nsf career award iis - 123 , nsf award oise - 123 , and the kd - d grant .
