we present a large - margin formulation and algorithm for structured output prediction that allows the use of latent variables .
our proposal covers a large range of applica - tion problems , with an optimization problem that can be solved eciently using concave - convex programming .
the generality and performance of the approach is demonstrated through three applications including motif - nding , noun - phrase coreference resolution , and optimizing precision at k in information
in many structured prediction tasks , there is useful modeling information that is not available as part of the training data ( x123 , y123 ) , . . . , ( xn , yn ) .
in noun phrase coreference resolution , for example , one is typically given the clustering y of noun - phrases for a training document x , but not the set of informative links that connects the noun phrases together into clusters .
sim - ilarly , in machine translation , one may be given the translation y of sentence x , but not the linguistic struc - ture h ( e . g .
parse trees , word alignments ) that con - nects them .
this missing information h , even if not observable , is crucial for expressing high - delity mod - els for these tasks .
it is important to include these information in the model as latent variables .
latent variables have long been used to model observa - tions in generative probabilistic models such as hidden in discriminative models , however , the use of latent variables is much less explored .
re - cently , there has been some work on conditional ran - dom fields ( wang et al . , 123 ) with latent variables .
even less explored is the use of latent variables in
appearing in proceedings of the 123 th international confer - ence on machine learning , montreal , canada , 123
copy - right 123 by the author ( s ) / owner ( s ) .
large - margin structured output learning such as max - margin markov networks or structural svms ( taskar et al . , 123; tsochantaridis et al . , 123 ) .
while these non - probabilistic models oer excellent performance on many structured prediction tasks in the fully ob - served case , they currently do not support the use of latent variables , which excludes many interesting ap -
in this paper , we propose an extension of the struc - tural svm framework to include latent variables .
we identify a particular , yet rather general , formulation for which there exists an ecient algorithm to nd a local optimum using the concave - convex procedure .
the resulting algorithm is similarly modular as the structural svm algorithms for the fully observed case .
to illustrate the generality of our latent structural svm algorithm , we provide experimental results on three dierent applications in computational biology , natural language processing , and information retrieval .
related works
many of the early works in introducing latent vari - ables into discriminative models were motivated by computer vision applications , where it is natural to use latent variables to model human body parts or parts of objects in detection tasks .
the work in ( wang et al . , 123 ) introduces hidden conditional random fields , a discriminative probabilistic latent variable model for structured prediction , with applications to two com - puter vision tasks .
in natural language processing there is also work in applying discriminative proba - bilistic latent variable models , for example the training of pcfg with latent annotations in a discriminative manner ( petrov & klein , 123 ) .
the non - convex likeli - hood functions of these problems are usually optimized using gradient - based methods .
the concave - convex procedure ( yuille & rangara - jan , 123 ) employed in our work is a general frame - work for minimizing non - convex functions which falls into the class of dc ( dierence of convex ) program - ming .
in recent years there have been numerous appli -
learning structural svms with latent variables
cations of the algorithm in machine learning , includ - ing training non - convex svms and transductive svms ( collobert et al . , 123 ) .
the approach in ( smola et al . , 123 ) employs cccp to handle missing data in svms and gaussian processes and is closely related to our work .
however our approach is non - probabilistic and avoids the computation of partition functions , which is particularly attractive for structured prediction .
very recently the cccp algorithm has also been applied to obtain tighter non - convex loss bounds on structured learning ( chapelle et al . , 123 ) .
in the computer vision community there are recent works on training hidden crf using the max - margin criterion ( felzenszwalb et al . , 123; wang & mori , 123 ) .
in these works they focus on classication prob - lems only and their training problem formulations are a special case of our proposal below .
interestingly , the algorithm in ( felzenszwalb et al . , 123 ) coincides with our approach for binary classication but was derived in a dierent way .
structural svms
suppose we are given a training set of input - output structure pairs s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) ( x y ) n .
we want to learn a linear prediction rule of the form
fw ( x ) = argmaxyy ( w ( x , y ) ) ,
where is a joint feature vector that describes the relationship between input x and structured output y , with w being the parameter vector .
the optimization problem of computing this argmax is typically referred to as the inference or prediction problem .
when training structural svms , the parameter vec - tor w is determined by minimizing the ( regularized ) risk on the training set ( x123 , y123 ) , . . . , ( xn , yn ) .
risk is measured through a user - supplied loss function ( y , y ) that quanties how much the prediction y diers from the correct output y .
note that is typically non - convex and discontinuous and there are usually ex - ponentially many possible structures y in the output space y .
the structural svm formulation ( tsochan - taridis et al . , 123 ) overcomes these diculties by re - placing the loss function with a piecewise linear con - vex upper bound ( margin rescaling )
( yi , yi ( w ) ) max
( ( yi , y ) +w ( xi , y ) ) w ( xi , yi )
where yi ( w ) = argmaxyy w ( xi , y ) .
to train structural svms we then solve the following convex optimization problem :
( ( yi , y ) +w ( xi , y ) ) w ( xi , yi ) ( cid : 123 ) .
despite the typically exponential size of y , this op - timization problem can be solved eciently using cutting - plane or stochastic gradient methods .
struc - tural svms give excellent performance on many struc - tured prediction tasks , especially when the model is high - dimensional and it is necessary to optimize to non - standard loss functions .
structural svm with latent
as argued in the introduction , however , in many appli - cations the input - output relationship is not completely characterized by ( x , y ) x y pairs in the training set alone , but also depends on a set of unobserved la - tent variables h h .
to generalize the structural svm formulation , we extend our joint feature vector ( x , y ) with an extra argument h to ( x , y , h ) to de - scribe the relation among input x , output y , and latent variable h .
we want to learn a prediction rule of the
fw ( x ) = argmax ( y , h ) yh ( w ( x , y , h ) ) .
at rst glance , a natural way to extend the loss func - tion is to again include the latent variables h h similar to above , to give
i ( w ) ) , ( yi ( w ) , hi ( w ) ) ) ,
i ( w ) = argmaxhh w ( xi , yi , h ) and
( yi ( w ) , hi ( w ) ) = argmax ( y , h ) yh w ( xi , y , h ) .
essentially , this extended loss measures the dierence between the pair ( yi ( w ) , hi ( w ) ) given by the predic - tion rule and the best latent variable h i ( w ) that ex - plains the input - output pair ( xi , yi ) in the training set .
like in the fully observed case , we can derive a hinge - loss style upper bound
i ( w ) ) , ( yi ( w ) , hi ( w ) ) ) i ( w ) ) , ( yi ( w ) , hi ( w ) ) )
( w ( xi , yi , h
i ( w ) ) w ( xi , yi ( w ) , hi ( w ) ) )
w ( xi , y , h ) ! + ( ( yi , h w ( xi , yi , h ) ( cid : 123 ) .
i ( w ) ) , ( yi ( w ) , hi ( w ) ) )
in the case of structural svms without latent vari - ables , the complex dependence on w within the loss can be removed using the following inequality , com - monly referred to as loss - augmented inference in
learning structural svms with latent variables
structural svm training :
w ( x , y ) ( cid : 123 ) + ( yi , yi ( w ) )
( w ( xi , yi ) + ( yi , y ) ) .
when latent variables are included , however , the de - pendence of on the latent variables h i ( w ) of the correct label yi prevents us from applying this trick .
to circumvent this diculty , let us rethink the deni - tion of loss function from equation ( 123 ) .
as we will see below , many real world applications do not require the loss functions to depend on the oending h i ( w ) .
in ap - plications such as parsing and object recognition , the latent variables serve as indicator for mixture compo - nents or intermediate representations and are not part of the output .
as a result , the natural loss functions that we are interested in for these tasks usually do not depend on the latent variables .
we therefore focus on the case where the loss function does not depend on the latent variable h
i ( w ) ) , ( yi ( w ) , hi ( w ) ) ) = ( yi , yi ( w ) , hi ( w ) ) .
note that however the loss function may still depend on hi ( w ) .
in the case where the latent variable h is a part of the prediction which we care about we can still dene useful asymmetric loss functions ( y , y , h ) for learning .
the applications of noun phrase coreference resolution and optimizing for precision@k in document retrieval in the experiments section below are good examples of this .
with the redened loss ( yi , yi ( w ) , hi ( w ) ) , the bound in equation ( 123 ) becomes
i ( w ) ) , ( yi ( w ) , hi ( w ) ) )
( w ( xi , y , h ) + ( yi , y , h ) ) ! ( cid : 123 ) max
w ( xi , yi , h ) ( cid : 123 ) .
using the same reasoning as for fully observed struc - tural svms , this gives rise to the following optimiza - tion problem for structural svms with latent vari -
( w ( xi , y , h ) + ( yi , y , h ) ) !
w ( xi , yi , h ) ( cid : 123 ) .
it is easy to observe that the above formulation reduces to the usual structural svm formulation in the ab - sence of latent variables .
the formulation can also be
easily extended to include kernels , although the usual extra cost of computing inner products in nonlinear kernel feature space applies .
finally , note that the redened loss distinguishes our approach from transductive structured output learn - ing ( zien et al . , 123 ) .
when the loss depends only on the fully observed label yi , it rules out the possi - bility of transductive learning , but the restriction also results in simpler optimization problems compared to the transductive cases ( for example , the approach in ( zien et al . , 123 ) involves constraint removals to deal with dependence on h
i ( w ) within the loss ) .
solving the optimization problem
a key property of equation ( 123 ) that follows from the redened loss ( yi , yi ( w ) , hi ( w ) ) is that it can be writ - ten as the dierence of two convex functions :
( w ( xi , y , h ) + ( yi , y , h ) ) #
w ( xi , yi , h ) # .
this allows us to solve the optimization problem us - ing the concave - convex procedure ( cccp ) ( yuille & rangarajan , 123 ) .
the general template for a cccp algorithm for minimizing a function f ( w ) g ( w ) , where f and g are convex , works as follows :
algorithm 123 concave - convex procedure ( cccp ) 123 : set t = 123 and initialize w123
such that g ( w )
find hyperplane vt g ( wt ) + ( w wt ) vt for all w solve wt+123 = argminw f ( w ) + w vt set t = t + 123
123 : until ( f ( wt ) g ( wt ) ) ( f ( wt123 ) g ( wt123 ) ) <
the cccp algorithm is guaranteed to decrease the objective function at every iteration and to converge to a local minimum or saddle point ( yuille & rangara - jan , 123 ) .
line 123 constructs a hyperplane that upper bounds the concave part of the objective g , so that the optimization problem solved at line 123 is convex .
in terms of the optimization problem for latent struc - tural svm , the step of computing the upper bound for the concave part in line 123 involves computing
i = argmaxhh wt ( xi , yi , h )
for each i .
we call this the latent variable comple - tion problem .
the hyperplane constructed is vt =
i=123 ( xi , yi , h
learning structural svms with latent variables
computing the new iterate wt+123 in line 123 involves solv - ing the standard structural svm optimization prob - lem by completing yi with the latent variables h i as if they were completely observed :
( w ( xi , y , h ) + ( yi , y , h ) )
w ( xi , yi , h
thus the cccp algorithm applied to structural svm with latent variables gives rise to a very intuitive algorithm that alternates between imputing the la - tent variables h i that best explain the training pair ( xi , yi ) and solving the structural svm optimization problem while treating the latent variables as com - pletely observed .
this is similar to the iterative pro - cess of expectation maximization ( em ) .
but unlike em which maximizes the expected log likelihood un - der the marginal distribution of the latent variables , we are minimizing the regularized loss against a single latent variable h
i that best explains ( xi , yi ) .
in our implementation , we used an improved version of the cutting plane algorithm called the proximal bundle method ( kiwiel , 123 ) to solve the standard structural svm problem in equation ( 123 ) .
in our experience the proximal bundle method usually converges using fewer iterations than the cutting plane algorithm ( joachims et al . , to appear ) in the experiments below .
the al - gorithm also ts very nicely into the cccp algorith - mic framework when it is employed to solve the stan - dard structural svm optimization problem inside the cccp loop .
the solution wt123 from the last iteration can be used as a starting point in a new cccp iter - ation , without having to reconstruct all the cuts from scratch .
we will provide some computational experi - ence at the end of the experiments section .
below we demonstrate three applications of our la - tent structural svm algorithm .
some of them have been discussed in the machine learning literature be - fore , but we will show that our latent structural svm framework provides new and straightforward so - lution approaches with good predictive performance .
a software package implementing the latent struc - tural svm algorithm is available for download at
discriminative motif finding
our development of the latent structural svm was motivated by a motif nding problem in yeast dna through collaboration with computational biologists .
motifs are repeated patterns in dna sequences that are believed to have biological signicance .
our dataset consists of arss ( autonomously replicating se - quences ) screened in two yeast species s .
kluyveri and s .
cerevisiae .
our task is to predict whether a par - ticular sequence is functional ( i . e . , whether they start the replication process ) in s .
cerevisiae and to nd out the motif responsible .
all the native arss in s .
cerevisiae are labeled as positive , since by denition they are functional .
the ones that showed ars ac - tivity in s .
kluyveri were then further tested to see whether they contain functional ars in s .
cerevisiae , since they might have lost their function due to se - quence divergence of the two species during evolution .
they are labeled as positive if functional and negative otherwise .
in this problem the latent variable h is the position of the motif in the positive sequences , since current experimental procedures do not have enough resolution to pinpoint their locations .
altogether we have 123 positive examples and 123 negative examples .
in addition we have 123 sequences from the yeast in - tergenic regions for background model estimation .
popular methods for motif nding includes methods based on em ( bailey & elkan , 123 ) and gibbs - sampling .
for this particular yeast dataset we believe a discriminative approach , especially one incorporat - ing large - margin separation , is benecial because of the close relationship and dna sequence similarity among the dierent yeast species in the dataset .
let xi denote the ith base ( a , c , g , or t ) in our input sequence x of length n .
we use the common position - specic weight matrix plus background model approach in our denition of feature vector :
( x , y , h ) =
( if y = +123 )
( if y = 123 ) ,
p sm is the feature count for the jth position of the motif in the position - specic weight matrix , and bg is the feature count for the background model ( we use a markov background model of order 123 ) .
for the positive sequences , we randomly initialized the motif position h uniformly over the whole length of the sequence .
we optimized over the zero - one loss for classication and performed a 123 - fold cross vali - dation .
we make use of the set of 123 intergenic sequences in training by treating them as negative ex - amples ( but they are excluded in the test sets ) .
instead of penalizing their slack variables by c in the objec - tive we only penalize these examples by c / 123 to avoid
learning structural svms with latent variables
table 123
classication error on yeast dna ( 123 - fold cv )
gibbs sampler ( l = 123 ) gibbs sampler ( l = 123 ) latent structural svm ( l = 123 ) latent structural svm ( l = 123 )
overwhelming the training set with negative examples ( with the factor 123 / 123 picked by cross - validation ) .
we trained models using regularization constant c from ( 123 , 123 , 123 , 123 , 123 ) times the size of the training set ( 123 for each fold ) , and each model is re - trained 123 times using 123 dierent random seeds .
as control we ran a gibbs sampler ( ng & keich , 123 ) on the same dataset , with the same set of intergenic sequences for background model estimation .
it reports good signals on motif lengths l = 123 and l = 123 , which we compare our algorithm against .
to provide a stronger baseline we optimize the classication thresh - old of the gibbs sampler on the test set and report the best accuracy over all possible thresholds .
table 123 compares the accuracies of the gibbs sampler and our method averaged across 123 folds .
our algorithm shows a signicant improvement over the gibbs sam - pler ( with p - value < 123 in a paired t - test ) .
as for the issue of local minima , the standard deviations on the classication error over the 123 random seeds , av - eraged over 123 folds , are 123 for l = 123 and 123 for l = 123
there are variations in solution quality due to local minima in the objective , but they are rel - atively mild in this task and can be overcome with a few random restarts .
in this application the latent structural svm allows us to exploit discriminative information to better de - tect motif signals compared to traditional unsuper - vised probabilistic model for motif nding .
currently we are working with our collaborators on ways to inter - pret the position - specic weight matrix encoded in the weight vector trained by the latent structural svm .
noun phrase coreference via clustering
in noun phrase coreference resolution we would like to determine which noun phrases in a text refer to the same real - world entity .
in ( finley & joachims , 123 ) the task is formulated as a correlation clustering problem trained with structural svms .
in correlation clustering the objective function maximizes the sum of pairwise similarities .
however this might not be the most appropriate objective , because in a cluster of coreferent noun phrases of size k , many of the o ( k123 ) links contain only very weak signals .
for example , it is
figure 123
the circles are the clusters dened by the label y .
the set of solid edges is one spanning forest h that is consistent with y .
the dotted edges are examples of incorrect links that will be penalized by the loss function .
dicult to determine whether a mention of the name tom at the beginning of a text and a pronoun he at the end of the text are coreferent directly without scanning through the whole text .
following the intuition that humans might determine if two noun phrases are coreferent by reasoning tran - sitively over strong coreference links ( ng & cardie , 123 ) , we model the problem of noun phrase corefer - ence as a single - link agglomerative clustering problem .
each input x contains all n noun phrases in a docu - ment , and all the pairwise features xij between the ith and jth noun phrases .
the label y is a partition of the n noun phrases into coreferent clusters .
the latent variable h is a spanning forest of strong coref - erence links that is consistent with the clustering y .
a spanning forest h is consistent with a clustering y if every cluster in y is a connected component in h ( i . e . , a tree ) , and there are no edges in h that connects two distinct clusters in y ( figure 123 ) .
to score a clustering y with a latent spanning forest h , we use a linear scoring model that adds up all the edge scores for edges in h , parameterized by w :
w ( x , y , h ) = x ( i , j ) h
to predict a clustering y from an input x ( argmax in equation ( 123 ) ) , we can run any maximum span - ning tree algorithm such as kruskals algorithm on the complete graph of n noun phrases in x , with edge weights dened by w xij .
the output h is a spanning forest instead of a spanning tree because two trees will remain disconnected if all edges connecting the two trees have negative weights .
we then output the clus - tering dened by the forest h as our prediction y .
for the loss function , we would like to pick one that supports ecient computation in the loss - augmented inference , while at the same time penalizing incorrect spanning trees appropriately for our application
learning structural svms with latent variables
table 123
clustering accuracy on muc123 data
mitre loss pair loss
latent structural svm latent structural svm ( modied loss , r = 123 )
propose the loss function
( y , y , h ) = n ( y ) k ( y ) x ( i , j ) h
l ( y , ( i , j ) ) ,
where n ( y ) and k ( y ) are the number of vertices and the number of clusters in the correct clustering y .
the function l ( y , ( i , j ) ) returns 123 if i and j are within the same cluster in y , and - 123 otherwise .
it is easy to see that this loss function is non - negative and zero if and only if the spanning forest h denes the same clustering as y .
since this loss function is linearly decomposable into the edges in h , the loss - augmented inference can also be computed eciently using kruskals algorithm .
similarly the step of completing the latent variable h given a clustering y , which involves computing a highest scoring spanning forest that is consistent with y , can also be done with the same algorithm .
to evaluate our algorithm , we performed experiments on the muc123 noun phrase coreference dataset .
there are 123 documents in the dataset and we use the rst 123 for training and the remaining 123 for testing .
the pairwise features xij are the same as those in ( ng & cardie , 123 ) .
the regularization parameter c is picked from 123 to 123 using a 123 - fold cross valida - tion procedure .
the spanning forest h for each correct clustering y is initialized by connecting all coreferent noun phrases in chronological order ( the order in which they appear in the document ) , so that initially each tree in the spanning forest is a linear chain .
table 123 shows the result of our algorithm compared to the svm correlation clustering approach in ( finley & joachims , 123 ) .
we present the results using the same loss functions as in ( finley & joachims , 123 ) .
pair loss is the proportion of all o ( n123 ) edges incor - rectly classied .
mitre loss is a loss proposed for evaluating noun phrase coreference that is related to the f123 - score ( vilain et al . , 123 ) .
we can see from the rst two lines in the table that our method performs well on the pair loss but worse on the mitre loss when compared with the svm correlation clustering approach .
error analysis reveals that our method trained with the loss dened by equation ( 123 ) is very conservative when predicting links between noun phrases , having high precision but rather low recall .
therefore we adapt our loss function to make it more suitable for minimizing the mitre loss .
we modied the loss function in equation ( 123 ) to penalize less for adding edges that incorrectly link two distinct clusters , using a penalty r < 123 instead of 123 for each incorrect edge added .
with the modied loss ( with r = 123 picked via cross - validation ) our method performs much better than the svm correlation clustering approach on the mitre loss ( p - value < 123 in a z - test ) .
unlike the svm correlation clustering approach , where approximate inference is required , our inference proce - dure involves only simple and ecient maximum span - ning tree calculations .
for this noun phrase corefer - ence task , the new formulation with latent structural svm improves both the prediction performance and training eciency over conventional structural svms .
optimizing for precision@k in ranking
our last example application is related to optimizing for precision@k in document retrieval .
precision@k is dened to be the number of relevant documents in the top k positions given by a ranking , divided by k .
for each example in the training set , the pattern x is a collection of n documents ( x123 , .
, xn ) associated with a query q , and the label y ( 123 , 123 ) n classies whether each document in the collection is relevant to the query or not .
however for the purpose of evalu - ating and optimizing for information retrieval perfor - mance measures such as precision@k and ndcg@k , the partial order of the documents given by the label y is insucient .
the label y does not tell us which the top k documents are .
to deal with this problem , we can postulate the existence of a latent total order h on all documents related to the query , with h consistent with the partial order given by label y .
to be precise , let hj be the index of the jth most relevant document , such that xhj tot xhj+123 for j from 123 to n 123 , where tot is a total order of relevance on the documents xi , and let >tot be its strict version .
the label y is consistent with the latent variable h if yi > yj implies xi >tot xj , so that all relevant documents in y comes before the non - relevant documents in the total order h .
for optimizing for precision@k in this section , we can restrict h to be rst k documents h123 ,
we use the following construction for the feature vec - tor ( in a linear feature space ) :
( x , y , h ) =
the feature vector only consists of contributions from the top k documents selected by h , when all other doc - uments in the label y are ignored ( with the restriction that h has to be consistent with y ) .
learning structural svms with latent variables
figure 123
latent structural svm tries to optimize for ac - curacy near the region for the top k documents ( circled ) , when a good general ranking direction w is given
for the loss we use the following precision@k loss :
( y , y , h ) = min ( 123 ,
( yhj == 123 ) .
this loss function is essentially one minus precision@k , with slight modications when there are less than k relevant documents in a collection .
we replace 123 by n ( y ) / k so that the loss can be minimized to zero , where n ( y ) is the total number of relevant documents in y .
intuitively , with this particular design of the feature vector and the loss function , the algorithm is trying to optimize for the classication accuracy in the region near the top k documents , while ignoring most of the documents in the rest of the feature space ( figure 123 ) .
all the inference problems required for this application are ecient to solve .
prediction requires sorting based on the score w xj in decreasing order and picking the top k .
the loss - augmented inference requires sorting based on the score w xj ( yj == 123 ) and picking the top k for h .
latent variable completion for y requires a similar sorting procedure on w xj and picking the top k , but during sorting the partial order given by the label y has to be respected ( so that xi comes before xj when either yi > yj , or yi == yj and w xi > w xj ) .
to evaluate our algorithm , we ran experiments on the ohsumed tasks of the letor 123 dataset ( liu et al . , 123 ) .
we use the per - query - normalized version of the features in all our training and testing below , and employ exactly the same training , test , and vali - dation sets split as given .
for this application it is vital to have a good initial - ization of the latent varibles h .
simple initialization strategies such as randomly picking k relevant docu - ments indicated by the label y does not work for these datasets with noisy relevance judgements , which usu - ally give the trivial zero vector as solution .
we adopt the following initialization strategy .
using the same training and validation sets in each fold , we trained a model optimizing for weighted average clas -
table 123
precision@k on ohsumed dataset ( 123 - fold cv ) latent structural svm 123 initial weight vector
figure 123
number of cccp iterations against c
sication accuracy ( weighted by the reciprocal of the number of documents associated by each query ) .
then for each fold the trained model is used as the initial weight vector to optimize for precision@k .
we can see from table 123 that our latent struc - tural svm approach performs better than the ranking svm ( herbrich et al . , 123; joachims , 123 ) on pre - cision@123 , 123 , 123 , 123 , one of the stronger baselines in the letor 123 benchmark .
we also essentially tie with listnet ( cao et al . , 123 ) , one of the best overall rank - ing method in the letor 123 benchmark .
as a san - ity check , we also report the performance of the initial weight vectors used for initializing the cccp .
the la - tent structural svm consistently improves upon these , showing that the good performance is not simply a re - sult of good initialization .
eciency of the optimization algorithm
figure 123 shows the number of iterations required for convergence for the three tasks for dierent values of the parameter c , averaged across all folds in their re - spective cross validation procedures .
we x the pre - cision at 123 for the motif nding and optimizing for precision@k tasks , and use = 123 for the noun phrase coreference task due to a dierent scaling of the loss function .
we can see that in general the number of cccp iterations required only grows very mildly with c , and most runs nish within 123 iterations .
as the cost of each cccp iteration is no more than solving a standard structural svm optimization problem ( with
learning structural svms with latent variables
the completion of latent variables ) , the total number of cccp iterations gives us a rough estimate of the cost of training latent structural svms , which is not in practice the cost is even lower because we do not need to solve the optimiza - tion problem to high precision in the early iterations , and we can also reuse solution from the previous iter - ation for warm start in a new cccp iteration .
in advances in large margin classiers , chapter 123 , 123
mit press .
joachims , t .
( 123 ) .
optimizing search engines using clickthrough data .
acm sigkdd conf .
on knowl - edge discovery and data mining ( pp .
123 ) .
joachims , t . , finley , t . , & yu , c .
( to appear ) .
cutting - plane training of structural svms .
machine
we have presented a framework and formulation for learning structural svms with latent variables .
we identify a particular case that covers a wide range of application problems , yet aords an ecient training algorithms using convex - concave programming .
the algorithm is modular and easily adapted to new appli - cations .
we demonstrated the generality of the latent structural svm with three applications , and a future research direction will be to explore further applica - tions of this algorithm in dierent domains .
this work is supported by nsf award iis - 123
we would like to thank tom finley and professor uri keich for the datasets , and the anonymous reviewers for their helpful suggestions to improve this paper .
