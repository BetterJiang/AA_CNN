abstract .
we explore an algorithm for training svms with kernels that can represent the learned rule using arbitrary basis vectors , not just the support vectors ( svs ) from the training set .
this results in two benets .
first , the added exibility makes it possible to nd sparser so - lutions of good quality , substantially speeding - up prediction .
second , the improved sparsity can also make training of kernel svms more ecient , especially for high - dimensional and sparse data ( e . g .
text classication ) .
this has the potential to make training of kernel svms tractable for large training sets , where conventional methods scale quadratically due to the linear growth of the number of svs .
in addition to a theoretical analysis of the algorithm , we also present an empirical evaluation .
i=123 ik ( xi , x )
while support vector machines ( svms ) with kernels oer great exibility and prediction performance on many application problems , their practical use is often hindered by the following two problems .
both problems can be traced back to the number of support vectors ( svs ) , which is known to generally grow linearly with the data set size ( 123 ) .
first , training is slower than other methods and linear svms , where recent advances in training algorithms vastly improved training time .
second , since the prediction rule takes the form h ( x ) = sign it is too expensive to evaluate in many applications when the number of svs is
this paper tackles these two problems by generalizing the notion of support vector to arbitrary points in input space , not just training vectors .
unlike wu et al .
( 123 ) , who explore making the location of the points part of a large non - convex optimization problem , we propose an algorithm that iteratively constructs the set of basis vectors from a cutting - plane model .
this makes our algorithm , called cutting - plane subspace pursuit ( cpsp ) , ecient and modular .
we analyze the training eciency and the solution quality of the cpsp algorithm both theo - retically and empirically .
we nd that its classication rules can be orders of magnitude sparser than the conventional support - vector representation while providing comparable prediction accuracy .
the sparsity of the cpsp represen - tation not only makes predictions substantially more ecient , it also allows the user to control training time .
especially for large datasets with sparse feature vectors ( e . g .
text classication ) , the cpsp methods is substantially faster than methods that only consider basis vectors from the training set .
123 related work
w = ( cid : 123 ) n
most existing algorithms for training kernel svms follow the representer theo - rem and search for the optimal weight vector in the span of the training vectors i=123 i ( xi ) .
this includes decomposition methods ( 123 , 123 ) and all other dual approaches .
to overcome the problems resulting from the growing number of support vectors , burges and scholkopf ( 123 ) propose to post - process their so - lution and replace the support vector expansion with an approximation that is more sparse .
clearly , this can improve only the prediction eciency , while it is still necessary to compute a full solution during training .
for large datasets , this
an alternative to post - processing are methods for selecting a set of basis vectors a priori .
this includes sampling randomly from the training set in the nystrom method ( 123 ) , greedily minimizing reconstruction error ( 123 ) , and variants of the incomplete cholesky factorization ( 123 , 123 ) .
however , these selection methods are not part of the optimization process , which makes a goal - directed choice of basis vectors dicult .
in fact , all but ( 123 ) ignore label information , and all methods are limited to selecting basis vectors from the training set .
methods like the core vector machine ( cvm ) ( 123 ) , the ball vector ma - chine ( bvm ) ( 123 ) , and the active selection strategy of the lasvm method ( 123 ) greedily select which basis vectors to include in the classication rule .
while they allow the user to sacrice solution quality to gain sparsity and training eciency , they are also limited to selecting basis vectors from the training set .
another set of methods are basis pursuit approaches ( 123 , 123 ) .
they repeatedly solve the optimization problem for a given set of basis vector , and then greedily search for vectors to add or remove .
the cutting - plane subspace pursuit method we propose is similar in the respect that it iteratively constructs the basis set .
however , the construction of the basis set is part of the optimization algorithm itself , and the cutting - plane model makes it straightforward to add basis vectors that are not in the training set .
it is not clear how to eciently add such general basis vectors in other basis pursuit approaches .
the method most closely related to ours was proposed in ( 123 ) .
they treat the basis vectors as variables in the svm optimization problem , and solve the resulting non - convex program via gradient descent to a local optimum .
however , training eciency is a bottleneck in this approach and they focus only on small datasets in their evaluation .
we will consider datasets that are several orders of magnitude larger .
furthermore , we will provide theoretical results giving insight into the quality of the cpsp solution .
123 cutting - plane algorithm for svms
we rst introduce the cutting - plane algorithm for training svms ( 123 , 123 ) , since it is the basis for the cpsp algorithm proposed in this paper .
for a training sample , ( x123 , y123 ) , . . . , ( xn , yn ) , the following is a general formulation of the large - margin training problem for learning a rule h : x y mapping from some input
space x to some output space y ( 123 ) .
for dierent choices of the joint feature map ( x , y ) and the loss function ( y , y ) , it can be specialized to classication svms , to maximum - margin markov networks , or various structured prediction
( cid : 123 ) w , w ( cid : 123 ) + c
s . t . i , yy : ( cid : 123 ) w , ( xi , yi ) ( xi , y ) ( cid : 123 ) ( yi , y ) i ( 123 )
( cid : 123 ) . , . ( cid : 123 ) denotes an inner product .
for the sake of simplicity , this paper only deals with the special case of binary classication with x = ( cid : 123 ) n and y = ( 123 , +123 ) , 123 y ( x ) for the kernel where the joint feature map is ( x , y ) = 123 k ( x , x ( cid : 123 ) ) = ( cid : 123 ) ( x ) , ( x ( cid : 123 ) ) ( cid : 123 ) ) and where the loss function ( y , y ) is the zero / one - loss .
in this case , it is easy to verify that ( 123 ) is equivalent to the following program , which corresponds to a binary classication svm without explicit o -
123 yx ( or ( x , y ) = 123
i : yi ( cid : 123 ) w , xi ( cid : 123 ) 123 i
( cid : 123 ) w , w ( cid : 123 ) + c
123 linear svms
instead of solving ( 123 ) directly , ( 123 ) proposes to solve the following equivalent
( cid : 123 ) w , w ( cid : 123 ) + c
y123 . . yn y n :
( ( xi , yi ) ( xi , yi ) )
this program has only a single slack variable , and is therefore called the 123 - slack formulation .
it is shown in ( 123 ) that any w solving ( 123 ) is also a solution of ( 123 ) , and that = 123 i=123 i .
while ( 123 ) has a huge number of constraints , algorithm 123 is a cutting - plane procedure that always constructs a solution of precision with at most o ( c ) active constraints ( 123 , 123 , 123 ) .
in the experiments from section 123 , the number of active constraints was typically around 123 independent of the size of the training set .
algorithm 123 maintains a working set of m constraints ( cid : 123 ) w , i
( cid : 123 ) i over
which it solves the qp in line 123
in each iteration i , the algorithm nds the most violated constraint from ( 123 ) ( lines 123 - 123 ) and adds it to the working set , so m i .
typically , however , m << i , since constraints become inactive in later iterations and can be removed from the working set ( line 123 ) .
therefore , the size m of the working set is roughly equal to the number of active constraints ( i . e .
m 123 ) .
the algorithm is known to need at most i o ( c ) iterations to converge to an - accurate solution ( 123 , 123 , 123 ) .
this means that the number of iterations is independent of the number of training examples n and the number of features n .
algorithm 123 cutting - plane algorithm for structural svm ( primal ) 123 : input : s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) , c , 123 : 123 , 123 , m 123 ( w , ) argminw , 123
123 ( cid : 123 ) w , w ( cid : 123 ) + c
i : ( cid : 123 ) w , i
yi argmaxyy ( ( yi , y ) + wt ( xi , y ) )
( , , m ) = remove inactive ( , , w , )
for i=123 , . . . , n do
123 : m m + 123
123 : until ( cid : 123 ) w , m
( ( xi , yi ) ( xi , yi ) )
123 svms with kernels
and the solution vector in the rkhs is w ( cid : 123 )
while originally proposed for linear svms , the cutting - plane method can be ex - tended to the non - linear case with kernels .
since w now lies in the reproducing kernel hilbert space ( rkhs ) of the kernel k ( x , x ( cid : 123 ) ) = ( cid : 123 ) ( x ) , ( x ( cid : 123 ) ) ( cid : 123 ) , we need to move to the dual representation .
algorithm 123 is this dual variant of algo - rithm 123 and specialized to the case of binary classication as implemented in the svmperf software .
it replaces the primal qp with its wolfe dual in line 123 , i i i .
note that sums cannot be computed eciently in the rkhs .
therefore , the assignment operator is replaced with a rewrite operator where appropriate .
however , it is easy to verify that all inner products ( lines 123 , 123 , 123 ) can be computed as sums of kernel evaluations .
the o ( c ) bound on the number of iterations ( 123 , 123 ) holds inde - pendent of whether a kernel is used or not , but how does the time complexity per iteration change when moving from a linear to a kernelized svm ?
without kernels , any iteration in algorithm 123 takes at most o ( m123 ) for solving the qp , o ( m123 ) for , o ( mn ) for w , o ( nn ) for computing the most violated constraint , o ( n ) for , o ( nn ) for , and o ( mn ) for adding a row / column to h .
so the overall time complexity is o ( m123 + mn + nn ) .
when using a kernel , however , computing ( cid : 123 ) w , ( xi ) ( cid : 123 ) and h becomes more expensive than in the linear case .
denote with y the matrix with yij = ( yi yi ) for the j - the constraint in .
to nd the most violated constraint , for each example one now needs to evaluate
( cid : 123 ) w , ( xi ) ( cid : 123 ) =
algorithm 123 cutting - plane algorithm for classication svm ( dual ) 123 : input : s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) , c , , k ( x , x ( cid : 123 ) ) = ( cid : 123 ) ( x ) , ( x ( cid : 123 ) ) ( cid : 123 ) 123 : 123 , 123 , h 123 , m 123
123 : h ( hij ) 123i , jm , where hij = ( cid : 123 ) i , j 123 : w ( cid : 123 )
123 th s . t .
t 123 c
argmax123 t 123 c ( t t h )
( , , m ) = remove inactive ( , , )
for i=123 , . . . , n do
yi sign ( ( cid : 123 ) w , ( xi ) ( cid : 123 ) yi )
123 : m m + 123
123 : until ( cid : 123 ) w , m
over all n examples , this has a cost of o ( n123+mn ) .
similarly , adding a row / column for the new m to the gram matrix h now requires computing
i : hmi = him = ( cid : 123 ) i , m
this takes time o ( mn123 ) , counting a single kernel evaluation as o ( 123 ) .
so , the overall time complexity of an iteration when kernels are used is o ( m123 + mn123 ) .
this o ( n123 ) scaling is not practical for any reasonably - sized dataset , and the algorithm has worse constants than decomposition methods like svmlight that also typically scale o ( n123 ) .
however , algorithm 123 does provide a path to a sub - stantially more ecient algorithm that is explored in the next section .
123 cutting - plane subspace pursuit
constructs a low - dimensional subspace w = span ( 123 , . . . , m ) = ( ( cid : 123 ) m
is it possible to remove the o ( n123 ) scaling behavior ? here is the intuition for the approach we take .
a property of the cutting - plane algorithm is that it iteratively i=123 i i : ( cid : 123 ) m ) in which the nal solution
is guaranteed to lie .
instead of using the representer theorem and considering the larger subspace f = span ( ( x123 ) , . . . , ( xn ) ) to express the optimal weight
vector as w = ( cid : 123 ) n
i ( xi ) , the cutting - plane method tells us that we only need to consider the subspace w f in each iteration , where m << n and m does not grow with n .
our core idea is to nd a small set of basis vectors b123 , . . . , bk so that
which means that we can express the nal solution from ( 123 ) as
w ( cid : 123 ) = span ( ( b123 ) , . . . , ( bk ) ) w ,
this enables ecient prediction using the rule h ( x ) = sign ( ( cid : 123 ) k
i k ( bi , x ) ) , given that k is small .
furthermore , we will elaborate in the following how pro - jecting into the subspace w ( cid : 123 ) allows computing h and ( cid : 123 ) w , ( x ) ( cid : 123 ) in time inde - pendent of n .
to understand the intuition behind our approach , consider the ideal case where for every i there exists a vector bi in input space ( not necessarily from the training set ) so that i = ( bi ) ( as it does in the linear case , where bi = j=123 ( yj yj ) xj ) .
then we could replace each i with ( bi ) , and it is easy to verify that the time complexity of an iteration goes down to o ( m123+mn ) almost like in the linear case .
furthermore , the resulting classier would only have k = m 123 support vectors or , more generally named , basis vectors , making it much faster than conventional svm classiers that often have 123s
unfortunately , in most cases there will be no single pre - image b so that = ( b ) .
however , in any iteration it suces to nd a set of pre - image vectors so that 123 , . . . , m lie ( approximately ) in their span .
in particular , we are looking for a set of basis vectors b = ( b123 , . . . , bk ) , bi ( cid : 123 ) n , so that for every i in
then replace i with its projection i ( cid : 123 ) k
for some ( small ) 123
when computing h and ( cid : 123 ) w , ( xi ) ( cid : 123 ) in algorithm 123 , we j=123 j ( bj ) .
this is summarized in algorithm 123 , which we call the cutting - plane subspace pursuit ( cpsp ) algo - rithm .
it is easy to verify that h and all ( cid : 123 ) w , ( xi ) ( cid : 123 ) can now be computed in time o ( m123k123 ) ( or o ( k123 + m123k + mk123 ) ) and o ( mk + kn ) , respectively .
using instead of in algorithm 123 is straightforward .
however , we still have to dene how the function extend basis ( b , m ) ( line 123 ) computes the set of basis vectors b = ( b123 , . . . , bk ) and how the function project ( i , b ) ( line 123 ) computes the approximate cutting - planes i .
this is addressed in the following .
|| i k ( cid : 123 )
123 projecting cutting - planes onto b for a given subspace span ( ( b123 ) , . . . , ( bk ) ) , the function project ( i , b ) com - putes the projection i of a cutting - plane i via the following least - squares
algorithm 123 cutting - plane subspace pursuit ( cpsp ) algorithm 123 : input : s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) , c , , kmax , k ( x , x ( cid : 123 ) ) = ( cid : 123 ) ( x ) , ( x ( cid : 123 ) ) ( cid : 123 ) 123 : 123 , 123 , 123 , h 123 , b , m 123 123 : h ( hij ) 123i , jm , where hij =
123 th s . t .
t 123 c
argmax123 t 123 c ( t t h )
( cid : 123 ) i , j
123 : w ( cid : 123 )
for i=123 , . . . , n do
yi sign ( ( cid : 123 ) w , ( xi ) ( cid : 123 ) yi )
( , , m ) = remove inactive ( , , )
if |b| < kmax then b extend basis ( b , m ) for i=123 , . . . , k do
123 : m m + 123
i project ( i , b )
123 : until ( cid : 123 ) w , m
j ( bj ) where = min
|| i k ( cid : 123 )
to accomodate kernels , we maintain the k k - matrix g with gij = k ( bi , bj ) and the k n - matrix k with kij = k ( bi , xj ) .
the solution of the least - squares 123n g123kyi .
it is more ecient , however , problem can then be written as = 123 g ) .
with lg , the so - to use the cholesky decomposition lg of g ( i . e .
g = lglt lution can be computed via forward and back - substitution from lg = 123 g = in time o ( k123 + kn ) .
this excludes the time for computing k , g , and its cholesky decomposition lg , since these need to be computed only once and can then be used until b changes .
this is further discussed in the next
123 constructing the set of basis vectors b
the method for constructing the set of basis vectors b is the nal part of algorithm 123 that still needs to be specied .
the goal is to nd a set of basis vectors b = ( b123 , . . . , bk ) such that for some small 123 , all i that are active in the current iteration fulll ( 123 ) .
recomputing b in each iteration would be costly ,
but fortunately it is unnecessary .
only m is new and all other i are already well approximated by the set of basis vectors from the previous iteration .
the function extend basis ( b , m ) therefore only adds some new basis vectors to b that are required to t m .
note that this can only improve the t for the
to decide which basis vectors to add , we follow ( 123 ) and take a greedy ap - proach .
we search for the basis vector bk+123 that minimizes the residual error for m , where m is the projection for the current b .
( ( cid : 123 ) , b ( cid : 123 ) ) = argmin
|| m m k+123 ( bk+123 ) ||123
this optimization problem is commonly referred to as the preimage problem .
while exact solutions are dicult to obtain , approximate solutions can be found with gradient - based methods ( 123 , 123 ) or randomized search .
in this paper , we use the x - point iteration approach described in ( 123 , sec .
123 . 123 ) for the rbf kernel to solve ( 123 ) to a local optimum .
in this way we can eciently produce arbitrary vectors as basis vectors to add to b .
we refer to the cutting - plane subspace pursuit algorithm with this preimage method as cpsp in the following .
to evaluate in how far general basis vectors improve sparsity , we also ex - plore a second preimage method that is restricted to using basis vectors from the training set .
we refer to this method as cpsp ( tr ) .
as proposed by smola and scholkopf ( 123 ) ( and used by most of the methods we compare against ) , we randomly sample 123 feature vectors from the training set and pick the one with maximum objective value in ( 123 ) .
note that this alternative strategy is intro - duced only to evaluate the benet of selecting support vectors outside the
the number of new basis vectors to add for each m is a design choice .
one could either use a xed number , or keep adding until a certain is achieved .
in the following experiments , we use the simplest choice and add exactly one basis vector for each m until the maximum size kmax specied by the user has been reached .
at that point , no further vectors are added and extend basis ( b , m ) returns b unchanged .
after a new bk+123 is added to b , a column / row needs to be added to the kernel matrices g and k .
this takes o ( n + k ) kernel evaluations , and the cholesky factorization of g can be updated in time o ( k123 ) .
123 theoretical analysis
before evaluating the cpsp algorithm empirically , we rst give a theoretical characterization of the quality of its solutions and the number of iterations it takes until convergence .
the following theorem gives an upper bound on the number of iterations of algorithm 123
it extends the general results ( 123 , 123 , 123 ) for cutting - plane training of svms to the cpsp algorithm .
theorem 123
for parameter c , precision , training - set size n , and basis - set size kmax , algorithm 123 terminates after at most o ( kmax + c proof .
after the rst kmax iterations , the basis b becomes xed , and from then on we are essentially solving the optimization problem :
( cid : 123 ) w ( cid : 123 ) 123 + c
y ( 123 , 123 ) n :
( yi , yi ) and w ( cid : 123 )
let pb be the orthogonal projection operator onto the subspace spanned by b .
such an orthogonal projection operator always exists in a hilbert space .
after folding the subspace constraint into the objective by replacing w with pbw , the above optimization problem can be re - written as ( using the self - adjointness and linearity of pb ) :
( cid : 123 ) pbw ( cid : 123 ) 123 + c
y ( 123 , 123 ) n :
finally the operator pb in the objective can be dropped since if w contains any component in b , it will only increase the objective without changing value of the lhs of the constraints .
this is in the form of the general structural svm optimization problem solved by algorithm 123 , with the feature space changed from being spanned by ( xi ) to being spanned by pb ( xi ) .
the o ( c bound from ( 123 , 123 , 123 ) therefore applies .
the time complexity of each iteration was already discussed in section 123 , but can be summarized as follows .
in iterations where no new basis vector is added to b , the time complexity is o ( m123 + mk123 + kn ) , since only the new m needs to be projected and the respective column be added to h .
in iterations where b is extended , the time complexity is o ( m123 + k123m + km123 + kmn ) plus the time it takes to solve the preimage problem ( 123 ) .
note that typical values are m 123 , k ( 123 . 123 ) , and n > 123
the following theorem describes the quality of the solution at termination , accounting for the error incurred by projecting on an imperfect b .
most impor - tantly , the theorem justies our use of ( 123 ) for deciding which basis vectors to theorem 123
when algorithm 123 terminates with || i i|| for all i and i , then the primal objective value o of the solution found does not exceed the exact solution o by more than o o c ( proof .
let w be the optimal solution with value o .
we know that the optimal w satises ( cid : 123 ) w ( cid : 123 )
( cid : 123 ) | ( cid : 123 ) w ( cid : 123 ) ( cid : 123 ) i i ( cid : 123 )
hence for all i ,
123c + ) .
let pb be the orthogonal projection on the subspace spanned by ( bi ) in the nal basis b .
let v be the optimal solution to the optimization problem ( 123 ) restricted to the subspace b , we have :
( cid : 123 ) v ( cid : 123 ) 123 + c ( + ) ( cid : 123 ) v ( cid : 123 ) 123 + c max ( cid : 123 ) pbw ( cid : 123 ) 123 + c max ( since v is the optimal solution wrt the basis b ) ( cid : 123 ) pbw ( cid : 123 ) 123 + c max ( cid : 123 ) pbw ( cid : 123 ) 123 + c max ( cid : 123 ) w ( cid : 123 ) 123 + c max ( cid : 123 ) w ( cid : 123 ) 123 + c max o + c ( 123c + )
( i ( cid : 123 ) ( i ( cid : 123 ) ( i ( cid : 123 ) ( i ( cid : 123 ) ( i ( cid : 123 ) ( i ( cid : 123 ) w , i
w , pb i
123c ) + c
123 experimental analysis
the following experiments are designed to evaluate how the cpsp method compares to conventional training methods in terms of sparsity ( i . e .
prediction eciency ) and training eciency .
in particular , they explore whether the use of general basis vectors outside the training set improves prediction accuracy and training eciency , and how both quantities scale with basis set size kmax .
our implementation of the cpsp algorithm is available for download at
we compare the cpsp algorithm with the exact solution computed by svmlight , as well as approximate solutions of the nystrom method ( nystrom ) ( 123 ) , the incomplete cholesky factorization ( incchol ) ( 123 ) , the core vector ma - chine ( cvm ) ( 123 ) , the ball vector machine ( bvm ) ( 123 ) , and lasvm with margin - based active selection and nishing ( 123 ) .
both the nystrom method and the incomplete cholesky factorization are implemented in svmperf as described in ( 123 ) .
we use the rbf - kernel k ( x , x ( cid : 123 ) ) = exp ( ||xx ( cid : 123 ) ||123 ) in all experiments .
the cache sizes of svmlight , cvm , bvm , and lasvm were set to 123gb .
we compare on the following ve binary classication tasks , each split into training / validation / test set .
if not mentioned otherwise , parameters ( i . e .
c and ) are selected to maximize performance on the validation set for each method and kmax individually .
both c and are explored on a log - scale .
the rst dataset is adult as compiled by john platt with 123 features and using a train / validation / test split of 123 / 123 / 123
second is the reuters rcv123
table 123
prediction accuracy with kmax = 123 basis vectors ( except svmlight , where the number of svs is shown in the third line ) using the rbf kernel ( except linear ) .
adult ccat ocr123 ocr* ijcnn
ccat text - classication dataset with 123 features .
we use 123 examples from the original test set for training and split the original training set into validation and test sets of sizes 123 and 123 respectively .
third and fourth , we classify the digit 123 against the rest ( ocr123 ) , as well as classify the dig - its 123 against the digits 123 ( ocr* ) on the mnist dataset .
the mnist datasets have 123 features and we use a training / validation / test split of 123 / 123 / 123
finally , we use the ijcnn ( task 123 ) dataset as pre - processed by chih - jen lin .
it has 123 features and we use a training / validation / test split how accurate are the solutions for a given sparsity budget ? we rst explore a scenario where the application demands an upper bound on the number of support vectors to achieve a desired computational eciency at prediction time .
table 123 summarizes the results .
the rst two lines show the performance of svmlight for the linear kernel and svmlight for the rbf kernel as baselines to compare against .
all but the adult dataset show substantial non - linear structure , and the rbf kernel outperforms a linear svm .
the number of svs when using the rbf kernel is given in the third line .
the remaining lines in table 123 are for the sparse methods , all of which use kmax = 123 basis vectors .
note that this is well below the 123 to 123 support vectors required by the exact svm .
the cpsp algorithm with the general preimage method matches the accu - racy of svmlight up to 123 .
this means that the prediction accuracy is roughly the same as for the exact method , while speeding up prediction by a factor between 123 to 123
we will see in section 123 that far fewer than kmax = 123 basis vectors would have suced on some of the tasks for the cpsp algorithm , leading to an even larger speedup .
random sampling of the basis vectors in the nystrom method and the incomplete cholesky factorization ( incchol ) perform consistently worse than the cpsp method , except on the ocr123 dataset where all methods do well with kmax = 123 basis vectors .
the core vector machine ( cvm ) , the ball vector machine ( bvm ) , and the lasvm algorithm with active selection are not competitive on most datasets .
decrease in accuracy w . r . t .
exact svm for dierent basis - set sizes kmax .
how does accuracy scale with basis - set size ? as mentioned above , a lower number of basis vectors kmax << 123 could have suced to get reasonable accuracy on some datasets .
the plots in figure 123 investigate this question and show by how much the test accuracies for a given kmax are lower than the accuracy of the exact svm solution .
in each plot , 123 corresponds to the accuracy of the exact svm solution .
figure 123 shows that cpsp dominates all other methods not only for kmax = 123 , but over the whole range .
for all datasets , the cpsp method using general preimages outperforms the other methods especially for small numbers of basis vectors .
in particular , on three of the ve datasets , cpsp already performs within 123% of the exact solution with only 123 basis vectors .
similarly , on all ve datasets does cpsp perform equivalent or better than the linear svm when using 123 basis vectors or more .
especially on adult , ccat , and ocr123 far fewer than kmax = 123 basis vectors would have suced to reach an acceptable level of performance .
what is the benet of using general basis vectors ? a key premise of the paper is that using basis vectors outside the training set is benecial .
to test its validity , figure 123 and table 123 include the performance of the cpsp ( tr ) algorithm , which is identical to cpsp except for selecting basis vectors only
123 123 123 123 123 123 123 123 123decrease in accuracynumber of basis vectorsadultcpspcpsp ( tr ) nystrominccholcvmbvm 123 123 123 123 123 123 123 123 123number of basis vectorsreuters ccat 123 123 123 123 123 123 123 123 123number of basis vectorsmnist 123 - 123 123 123 123 123 123 123 123 123 123number of basis vectorsmnist 123 - 123 123 123 123 123 123 123 123 123 123number of basis vectorsijcnn123 123
primal objective value of the approximate solutions expressed as multiples of the exact svm solution .
from the training set .
consistently over all dataset , figure 123 shows that the general cpsp algorithm provides improved prediction accuracy over cpsp ( tr ) especially for small numbers of basis vectors .
the dierence is largest on the ccat dataset , where the general cpsp algorithm with 123 basis vectors already performs at an accuracy for which cpsp ( tr ) requires about 123 basis vectors .
this conrms our hypothesis that basis vectors outside the training set can lead to more accurate solutions at a given level of sparsity .
how accurate is the objective value ? the four methods cpsp , cpsp ( tr ) , nystrom , and incchol all optimize the same objective function as a regular svm .
how well do they manage to minimize this objective ? the plots in fig - ure 123 show by what factor their primal objective value is higher than the exact svm solution .
all methods use the same parameters ( i . e .
c and ) , which are picked to optimize validation set accuracy of the exact svm .
again , cpsp dominates the other methods , and the curves in figure 123 very much resemble the curves in figure 123
this veries that nding a subspace that contains a solution of low objective value is indeed crucial for good prediction accuracy , and that the subspaces found by cpsp are of superior delity ( also compared to cpsp ( tr ) ) .
123 123 123 123 123 123 123 123obj .
value ( in mult .
of exact solution ) number of basis vectorsadultcpspcpsp ( tr ) nystromincchol 123 123 123 123 123 123 123 123 123 123 123 123 123number of basis vectorsreuters ccat 123 123 123 123 123 123 123 123 123 123 123 123 123number of basis vectorsmnist 123 - 123 123 123 123 123 123 123 123 123 123 123 123 123 123number of basis vectorsmnist 123 - 123 123 123 123 123 123 123 123 123 123 123 123 123 123number of basis vectorsijcnn123 123
table 123
number of sv ( left ) and training time ( right ) to reach an accuracy that is not more than 123% below the accuracy of the exact solution of svm - light ( see table 123 ) .
the rbf kernel is used for all methods .
> indicates that the largest tractable solution did not achieve the target accuracy .
number of sv
training time ( cpu - seconds )
adult ccat ocr123 ocr* ijcnn adult ccat ocr123 ocr* ijcnn
svm - light 123 123 lasvm 123 123
what is the training and test eciency ? while eciency at test time may be the dominant criterion for many applications , training has to be tractable as well .
since cpsp does more work in each iteration ( e . g .
solve a pre - image problem ) , one supercial concern might be that the training process is slow .
however , the following shows that the increased sparsity observed in figure 123 not only improves prediction eciency , but also speeds up training .
this is a key dierence to the reduced set method ( 123 ) .
the reduced set method requires solving an exact svm , making it intractable for large training sets .
table 123 compares the training time and number of basis vectors that each method needs to reach a certain prediction accuracy .
the experiment simu - lates how a user may chose to trade prediction accuracy for improved train - ing and test eciency .
in particular , table 123 shows the number of basis vec - tors ( left ) and the training time ( right ) to reach a test accuracy that is not more than 123% below the test accuracy of the exact svm .
basis set sizes kmax ( 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 ) were tried for each method and the results for the smallest kmax that achieved the target accuracy are shown .
table 123 shows that the solutions found by cpsp are typically substantially more sparse than those of the other methods .
compared to the exact solution , they lead to an 123 to 123 fold speed - up at prediction time .
compared to the other approximate methods , the speed - up is still typically between 123 and 123
the increased sparsity also leads to very ecient training times for cpsp .
while it is dicult to rank methods by aggregated training times , cpsp is clearly among the fastest methods in the comparison , especially on those tasks where general basis vectors provide a substantial gain in sparsity .
on the ccat text - classication dataset , it is orders of magnitude faster than any of the other methods ( and cpsp ( tr ) ) .
for such large and sparse data , there simply does not appear to be a small subset of training vectors that can represent an accurate
training times of cpsp for varying basis - set sizes ( left ) and training - set sizes with kmax = 123 ( right ) .
classier , and the increased sparsity from allowing general basis vectors greatly improves training eciency .
more generally , all training methods for svms scale super - linearly with the number of svs , so that improving sparsity is the key to making large - scale training tractable .
the scaling properties of cpsp are explored in more detail in the following section .
comparing to the results published in ( 123 ) , our method is substantially faster .
they focus mostly on small training sets with less than 123 examples .
the usps ocr dataset with 123 and 123 features is their largest dataset , and they report and average training time of 123 seconds .
this dataset roughly compares to our ocr123 task .
however , the ocr123 dataset is an order of magnitude larger .
how does training time scale with basis - set size ? finally , let us in - vestigate the eciency of cpsp in more detail .
the left - hand plots in figure 123 show training time for dierent values of kmax .
parameters ( regularization c , rbf ) are individually picked via cv for each method and kmax .
while the theoretical time complexity is o ( kmax 123 ) , the actual scaling shown in figure 123 ( left ) is much more benign .
for kmax < 123 , the time contribution of the cubic parts of the algorithm ( e . g .
repeatedly updating the cholesky factorization lg ) is still rather small , and the scaling behavior is only modestly super - linear .
how does training time scale with training - sample size ? finally , the right - hand plot in figure 123 shows training time of cpsp for dierent training set sizes .
parameters ( regularization c , rbf ) are individually picked via cv for each method and training set size .
as expected from the theoretical analysis , the scaling behavior is roughly linear , making cpsp particularly attractive for
we presented a training algorithm for kernel svms that constructs a sparse set of basis vectors as part of the cutting - plane optimization process .
the algorithms
123 123 123 123 123 123 123 123training timenumber of basis vectors 123 123 123 123 123 123 123 123training timenumber of training examplesadultreuters ccatmnist 123 : 123 - 123mnist 123 - 123 : 123 - 123ijcnn123x123 123
eciency and eectiveness is characterized theoretically , and an experimental comparison shows that is produces solutions of a sparsity that is superior to nystrom , incchol , cvm , bvm , and lasvm .
we nd that the ability to use basis vectors outside the training set substantially contributes to this gain in sparsity and eciency , especially on large datasets with sparse feature vectors .
acknowledgments this work was funded in part under nsf awards iis -
