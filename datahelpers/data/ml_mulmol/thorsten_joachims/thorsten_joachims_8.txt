training a support vector machine ( svm ) leads to a quadratic optimization problem with bound constraints and one linear equality constraint .
despite the fact that this type of problem is well understood , there are many issues to be considered in designing an svm learner .
in particular , for large learning tasks with many training examples , o ( cid : 123 ) - the - shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements .
sv m light is an implementation of an svm learner which addresses the problem of large tasks .
this chapter presents algorithmic and computational results developed for sv m lightv . , which make large - scale svm training more practical .
the results give guidelines for the application of svms to large domains .
also published in :
advances in kernel methods - support vector learning , bernhard sch ( cid : 123 ) olkopf , christopher j .
burges , and alexander j .
smola ( eds . ) , mit press , cambridge , usa , .
sv m lightis available at http : / / www - ai . cs . uni - dortmund . de / svm light
vapnik ( ) shows how training a support vector machine for the pattern recognition problem leads to the following quadratic optimization problem ( qp ) op .
w ( ( cid : 123 ) ) = ( cid : 123 )
yiyj ( cid : 123 ) i ( cid : 123 ) j k ( xi; xj )
i : ( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) c
the number of training examples is denoted by .
( cid : 123 ) is a vector of variables , where each component ( cid : 123 ) i corresponds to a training example ( xi; yi ) .
the solution of op is the ( cid : 123 ) for which ( ) is minimized and the constraints ( ) and ( ) are ful ( cid : 123 ) lled .
de ( cid : 123 ) ning the matrix q as ( q ) ij = yiyj k ( xi; xj ) , this can equivalently be written as
w ( ( cid : 123 ) ) = ( cid : 123 ) ( cid : 123 )
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) c
the size of the optimization problem depends on the number of training examples .
since the size of the matrix q is , for learning tasks with training examples and more it becomes impossible to keep q in memory .
many standard implementations of qp solvers require explicit storage of q which prohibits their application .
an alternative would be to recompute q every time it is needed .
but this becomes prohibitively expensive , if q is needed often .
one approach to making the training of svms on problems with many training exam - ples tractable is to decompose the problem into a series of smaller tasks .
sv m lightuses the decomposition idea of osuna et al .
this decomposition splits op in an inactive and an active part - the so call \working set " .
the main advantage of this decomposition is that it suggests algorithms with memory requirements linear in the number of training examples and linear in the number of svs .
one potential disadvantage is that these algo - rithms may need a long training time .
to tackle this problem , this chapter proposes an algorithm which incorporates the following ideas :
( cid : 123 ) an e ( cid : 123 ) cient and e ( cid : 123 ) ective method for selecting the working set .
( cid : 123 ) successive \shrinking " of the optimization problem .
this exploits the property that
many svm learning problems have
( much less support vectors ( svs ) than training examples .
( many svs which have an ( cid : 123 ) i at the upper bound c .
( cid : 123 ) computational improvements like caching and incremental updates of the gradient
and the termination criteria .
general decomposition algorithm
this chapter is structured as follows .
first , a generalized version of the decompositon algorithm of osuna et al .
( a ) is introduced .
this identi ( cid : 123 ) es the problem of selecting the working set , which is addressed in the following section .
in section a method for \shrinking " op is presented and section describes the computational and implementa - tional approach of sv m light .
finally , experimental results on two benchmark tasks , a text classi ( cid : 123 ) cation task , and an image recognition task are discussed to evaluate the approach .
general decomposition algorithm
this section presents a generalized version of the decomposition strategy proposed by osuna et al .
this strategy uses a decomposition similar to those used in active set strategies ( see gill et al .
( ) ) for the case that all inequality constraints are simple bounds .
in each iteration the variables ( cid : 123 ) i of op are split into two categories .
( cid : 123 ) the set b of free variables
( cid : 123 ) the set n of ( cid : 123 ) xed variables
free variables are those which can be updated in the current iteration , whereas ( cid : 123 ) xed variables are temporarily ( cid : 123 ) xed at a particular value .
the set of free variables will also be referred to as the working set .
the working set has a constant size q much smaller than .
the algorithm works as follows :
( cid : 123 ) while the optimality conditions are violated
( select q variables for the working set b .
( cid : 123 ) q variables are fixed at their current value .
( decompose problem and solve qp - subproblem : optimize w ( ( cid : 123 ) ) on b .
( cid : 123 ) terminate and return ( cid : 123 ) .
how can the algorithm detect that it has found the optimal value for ( cid : 123 ) ? since op is guaranteed to have a positive - semide ( cid : 123 ) nite hessian q and all constraints are linear , op is a convex optimization problem .
for this class of problems the following kuhn - tucker conditions are necessary and su ( cid : 123 ) cient conditions for optimality .
denoting the lagrange multiplier for the equality constraint with ( cid : 123 ) eq and the lagrange multipliers for the lower and upper bounds with ( cid : 123 ) lo and ( cid : 123 ) up , ( cid : 123 ) is optimal for op , if there exist ( cid : 123 ) eq , ( cid : 123 ) lo , and ( cid : 123 ) up , so that ( kuhn - tucker conditions , see werner ( ) ) :
i ( : : n ) :
i ( : : n ) :
g ( ( cid : 123 ) ) + ( ( cid : 123 ) eqy ( cid : 123 ) ( cid : 123 ) lo + ( cid : 123 ) up ) =
i ( ( cid : 123 ) i ( cid : 123 ) c )
g ( ( cid : 123 ) ) is the vector of partial derivatives at ( cid : 123 ) .
for op this is
g ( ( cid : 123 ) ) = ( cid : 123 ) + q ( cid : 123 )
if the optimality conditions do not hold , the algorithm decomposes op and solves the smaller qp - problem arising from this .
the decomposition assures that this will lead to progress in the objective function w ( ( cid : 123 ) ) , if the working set b ful ( cid : 123 ) lls some minimum requirements ( see osuna et al .
( b ) ) .
in particular , op is decomposed by separating the variables in the working set b from those which are ( cid : 123 ) xed ( n ) .
lets assume ( cid : 123 ) , y , and q are properly arranged with respect to b and n , so that
( cid : 123 ) = ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
y = ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
qn b qn n
q = ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
since q is symmetric ( in particular qbn = qt
n b ) , we can write
w ( ( cid : 123 ) ) = ( cid : 123 ) ( cid : 123 ) b ( ( cid : 123 ) qbn ( cid : 123 ) n ) +
byb + ( cid : 123 )
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) c
n qn n ( cid : 123 ) n ( cid : 123 ) ( cid : 123 )
since the variables in n are ( cid : 123 ) xed , the terms
n are constant .
they can be omitted without changing the solution of op .
op is a positive semide ( cid : 123 ) nite quadratic programming problem which is small enough be solved by most o ( cid : 123 ) - the - shelf methods .
it is easy to see that changing the ( cid : 123 ) i in the working set to the solution of op is the optimal step on b .
so fast progress depends heavily on whether the algorithm can select good working sets .
n qn n ( cid : 123 ) n and ( cid : 123 ) ( cid : 123 )
selecting a good working set
when selecting the working set , it is desirable to select a set of variables such that the current iteration will make much progress towards the minimum of w ( ( cid : 123 ) ) .
the following proposes a strategy based on zoutendijks method ( see zoutendijk ( ) ) , which uses a ( cid : 123 ) rst - order approximation to the target function .
the idea is to ( cid : 123 ) nd a steepest feasible direction d of descent which has only q non - zero elements .
the variables corresponding to these elements will compose the current working set .
this approach leads to the following optimization problem :
v ( d ) = g ( ( cid : 123 ) ( t ) ) td
for i : ( cid : 123 ) i =
for i : ( cid : 123 ) i = c
( cid : 123 ) ( cid : 123 ) d ( cid : 123 )
jfdi : di = gj = q
shrinking : reducing the size of op
the objective ( ) states that a direction of descent is wanted .
a direction of descent has a negative dot - product with the vector of partial derivatives g ( ( cid : 123 ) ( t ) ) at the current point ( cid : 123 ) ( t ) .
constraints ( ) , ( ) , and ( ) ensure that the direction of descent is projected along the equality constraint ( ) and obeys the active bound constraints .
constraint ( ) normalizes the descent vector to make the optimization problem well - posed .
finally , the last constraint ( ) states that the direction of descent shall only involve q variables .
the variables with non - zero di are included into the working set b .
this way we select the working set with the steepest feasible direction of descent .
the selection strategy , the optimality conditions , and the decomposition together specify the optimization algorithm .
a minimum requirement this algorithm has to ful ( cid : 123 ) ll is that
( cid : 123 ) terminates only when the optimal solution is found
( cid : 123 ) if not at the solution , takes a step towards the optimum
the ( cid : 123 ) rst requirement can easily be ful ( cid : 123 ) lled by checking the ( necessary and su ( cid : 123 ) cient ) optimality conditions ( ) to ( ) in each iteration .
for the second one , lets assume the current ( cid : 123 ) ( t ) is not optimal .
then the selection strategy for the working set returns an optimization problem of type op .
since by construction for this optimization problem there exists a d which is a feasible direction for descent , we know using the results of zoutendijk ( ) that the current op is non - optimal .
so optimizing op will lead to a lower value of the objective function of op .
since the solution of op is also feasible for op and due to the decomposition ( ) , we also get a lower value for op .
this means we get a strict descent in the objective function of op in each iteration .
how to solve op
the solution to op is easy to compute using a simple strategy .
let ! i = yigi ( ( cid : 123 ) ( t ) ) and sort all ( cid : 123 ) i according to ! i in decreasing order .
lets futhermore require that q is an even number .
successively pick the q= elements from the top of the list for which < ( cid : 123 ) ( t ) i < c , or di = ( cid : 123 ) yi obeys ( ) and ( ) .
similarly , pick the q= elements from the bottom of the list for which < ( cid : 123 ) ( t ) i < c , or di = yi obeys ( ) and ( ) .
these q variables compose the
shrinking : reducing the size of op
for many tasks the number of svs is much smaller than the number of training examples .
if it was known a priori which of the training examples turn out as svs , it would be su ( cid : 123 ) cient to train just on those examples and still get to the same result .
this would make op smaller and faster to solve , since we could save time and space by not needing parts of the hessian q which do not correspond to svs .
similarly , for noisy problems there are often many svs with an ( cid : 123 ) i at the upper bound c .
lets call these support vectors \bounded support vectors " ( bsvs ) .
similar arguments
as for the non - support vectors apply to bsvs .
if it was known a priori which of the training examples turn out as bsvs , the corresponding ( cid : 123 ) i could be ( cid : 123 ) xed at c leading to a new optimization problem with fewer variables .
during the optimization process it often becomes clear fairly early that certain ex - amples are unlikely to end up as svs or that they will be bsvs .
by eliminating these variables from op , we get a smaller problem op of size .
from op we can construct the solution of op .
let x denote those indices corresponding to unbounded support vec - tors , y those indexes which correspond to bsvs , and z the indices of non - support vectors .
the transformation from op to op can be done using a decomposition similar to ( ) .
lets assume ( cid : 123 ) , y , and q are properly arranged with respect to x , y , and z , so that we
qx x qx y qx z qy x qy y qy z qzx qzy qzz
the decomposition of w ( ( cid : 123 ) ) is
w ( ( cid : 123 ) x ) = ( cid : 123 ) ( cid : 123 )
x ( ( cid : 123 ) ( qxy ) ( cid : 123 ) c ) +
ctqyyc ( cid : 123 ) jyjc
x yx + ctyy =
( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) c
ctqyyc ( cid : 123 ) jyjc is constant , it can be dropped without changing the solution .
so far it is not clear how the algorithm can identify which examples can be it is desirable to ( cid : 123 ) nd conditions which indicate early in the optimization process that certain variables will end up at a bound .
since su ( cid : 123 ) cient conditions are not known , a heuristic approach based on lagrange multiplier estimates is used .
at the solution , the lagrange multiplier of a bound constraint indicates , how much the variable \pushes " against that constraint .
a strictly positive value of a lagrange multiplier of a bound constraint indicates that the variable is optimal at that bound .
at non - optimal points , an estimate of the lagrange multiplier can be used .
let a be the current set of ( cid : 123 ) i ful ( cid : 123 ) lling < ( cid : 123 ) i < c .
by solving ( ) for ( cid : 123 ) eq and averaging over all ( cid : 123 ) i in a , we get the estimate ( ) for ( cid : 123 ) eq .
( cid : 123 ) jyj k ( xi; xj )
note the equivalence of ( cid : 123 ) eq and the threshold b in the decision function .
since variables ( cid : 123 ) i cannot be both at the upper and the lower bound simultanously , the multipliers of the bound constraints can now be estimated by
i = yi
( cid : 123 ) jyj k ( xi; xj )
for the lower bounds and by
i = ( cid : 123 ) yi
( cid : 123 ) j yjk ( xi; xj )
for the upper bounds .
lets consider the history of the lagrange multiplier estimates over the last h iterations .
if the estimate ( ) or ( ) was positive ( or above some threshold ) at each of the last h iterations , it is likely that this will be true at the optimal solution , too .
these variables are eliminated using the decomposition from above .
this means that these variables are ( cid : 123 ) xed and neither the gradient , nor the optimality conditions are computed .
this leads to a substantial reduction in the number of kernel evaluations .
since this heuristic can fail , the optimality conditions for the excluded variables are checked after convergence of op .
if necessary , the full problem is reoptimized starting from the solution of op .
while the previous sections dealt with algorithmic issues , there are still a lot of open questions to be answered before having an e ( cid : 123 ) cient implementation .
this section addresses these implementational issues .
termination criteria
there are two obvious ways to de ( cid : 123 ) ne termination criteria which ( cid : 123 ) t nicely into the algo - rithmic framework presented above .
first , the solution of op can be used to de ( cid : 123 ) ne a necessary and su ( cid : 123 ) cient condition for optimality .
if ( ) equals , op is solved with the current ( cid : 123 ) ( t ) as solution .
sv m lightgoes another way and uses a termination criterion derived from the optimality conditions ( ) - ( ) .
using the same reasoning as for ( ) - ( ) , the following conditions with ( cid : 123 ) = are equivalent to ( ) - ( ) .
i with < ( cid : 123 ) i < c :
i with ( cid : 123 ) i = :
i with ( cid : 123 ) i = c :
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) yi ( cid : 123 ) ( p
j= ( cid : 123 ) jyjk ( xi; xj ) ) ( cid : 123 ) ( cid : 123 ) eq + ( cid : 123 )
j= ( cid : 123 ) jyj k ( xi; xj ) ) + ( cid : 123 ) eq ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) j= ( cid : 123 ) jyj k ( xi; xj ) ) + ( cid : 123 ) eq ) ( cid : 123 ) + ( cid : 123 )
the optimality conditions ( ) , ( ) , and ( ) are very natural since they re ( cid : 123 ) ect the constraints of the primal optimization problem .
in practice these conditions need not be ful ( cid : 123 ) lled with high accuracy .
using a tolerance of ( cid : 123 ) = : is acceptable for most tasks .
using a higher accuracy did not show improved generalization performance on the tasks tried , but lead to considerably longer training time .
computing the gradient and the termination criteria e ( cid : 123 ) ciently
the e ( cid : 123 ) ciency of the optimization algorithm greatly depends on how e ( cid : 123 ) ciently the \house - keeping " in each iteration can be done .
the following quantities are needed in each itera -
what are the computational resources needed in each iteration ?
( cid : 123 ) the vector of partial derivatives g ( ( cid : 123 ) ( t ) ) for selecting the working set .
( cid : 123 ) the values of the expressions ( ) , ( ) , and ( ) for the termination criterion .
( cid : 123 ) the matrices qbb and qbn for the qp subproblem .
fortunately , due to the decompositon approach , all these quantities can be computed or updated knowing only q rows of the hessian q .
these q rows correspond to the variables in the current working set .
the values in these rows are computed directly after the working set is selected and they are stored throughout the iteration .
it is useful to introduce s ( t )
knowing s ( t ) , the gradient ( ) as well as in the termination criteria ( ) - ( ) can be computed very e ( cid : 123 ) ciently .
when ( cid : 123 ) ( t ( cid : 123 ) ) changes to ( cid : 123 ) ( t ) the vector s ( t ) needs to be updated .
this can be done e ( cid : 123 ) ciently and with su ( cid : 123 ) cient accuracy as follows
i = s ( t ( cid : 123 ) )
j ( cid : 123 ) ( cid : 123 ) ( t ( cid : 123 ) )
note that only those rows of q are needed which correspond to variables in the working set .
the same is true for qbb and qbn , which are merely subsets of columns from these
what are the computational resources needed in each iteration ?
most time in each iteration is spent on the kernel evaluations needed to compute the q rows of the hessian .
this step has a time complexity of o ( qlf ) , where f is the maximum number of non - zero features in any of the training examples .
using the stored rows of q , updating s ( t ) is done in time o ( ql ) .
setting up the qp subproblem requires o ( ql ) as well .
also the selection of the next working set , which includes computing the gradient , can be done in o ( ql ) .
the highest memory requirements are due to storing the q rows of q .
here o ( ql ) ( cid : 123 ) oating point numbers need to be stored .
besides this , o ( q ) is needed to store qbb and o ( l ) to store s ( t ) .
caching kernel evaluations
as pointed out in the last section , the most expensive step in each iteration is the evalua - tion of the kernel to compute the q rows of the hessian q .
throughout the optimization process , eventual support vectors enter the working set multiple times .
to avoid recom - putation of these rows , sv m lightuses caching .
this allows an elegant trade - o ( cid : 123 ) between memory consumption and training time .
sv m lightuses a least - recently - used caching strategy .
when the cache is full , the ele - ment which has not been used for the greatest number of iterations , is removed to make room for the current row .
only those columns are computed and cached which correspond to active variables .
after shrinking , the cache is reorganized accordingly .
how to solve op ( qp subproblems )
currently a primal - dual interior - point solver ( see vanderbei ( ) ) implemented by a .
smola is used to solve the qp subproblems op .
nevertheless , other optimizers can easily be incorporated into sv m lightas well .
the ( cid : 123 ) rst approach to splitting large svm learning problems into a series of smaller op - timization tasks was proposed by boser et al .
it is known as the \chunking " algorithm ( see also kaufman ( ) ) .
the algorithm starts with a random subset of the data , solves this problem , and iteratively adds examples which violate the optimality con - ditions .
osuna et al .
( b ) prove formally that this strategy converges to the optimal solution .
one disadvantage of this algorithm is that it is necessary to solve qp - problems scaling with the number of svs .
the decomposition of osuna et al .
( a ) , which is used in the algorithm presented here , avoids this .
currently , an approach called sequential minimal optimization ( smo ) is explored for svm training ( see platt ( a ) and platt ( b ) ) .
it can be seen a special case of the algorithm presented in this chapter , allowing only working sets of size .
the algorithms di ( cid : 123 ) er in their working set selection strategies .
instead of the steepest feasible descent approach presented here , smo uses a set of heuristics .
nevertheless , these heuristics are likely to produce similar decisions in practice .
another di ( cid : 123 ) erence is that smo treats linear svms in a special way , which produces a great speedup for training linear separators .
although possible , this is not implemented in sv m light .
on the other hand , sv m lightuses caching , which could be a valuable addition to smo .
the following experiments evaluate the approach on four datasets .
the experiments are conducted on a sparc ultra / mhz with mb of ram running solaris ii .
if not stated otherwise , in the following experiments the cache size is megabytes , the number of iterations h for the shrinking heuristic is , and op is solved up to a precision of ( cid : 123 ) = : in ( ) - ( ) .
how does training time scale with the number of training exam -
this task was compiled by john platt ( see platt ( a ) ) from the uci \adult " data set .
the goal is to predict whether a household has an income greater than $ , .
after discretization of the continuous attributes , there are binary features .
on average , there are ( cid : 123 ) non - zero attributes per example .
table and the left graph in ( cid : 123 ) gure show training times for an rbf - kernel
k ( x; y ) = exp ( cid : 123 ) ( cid : 123 ) kx ( cid : 123 ) yk
= ( ( cid : 123 ) ) ( cid : 123 ) ;
how does training time scale with the number of training examples ?
with ( cid : 123 ) = and c = .
the results for smo and chunking are taken from platt ( a ) .
when comparing absolute training times , one should keep in mind that smo and chunking were run on a faster computer ( mhz pentium ii ) .
examples sv m light
smo chunking minimum total sv
table : training times and number of svs for the income prediction data .
both sv m lightand smo are substantially faster than the conventional chunking algo - rithm , whereas sv m lightis about twice as fast as smo .
the best working set size is q = .
by ( cid : 123 ) tting lines to the log - log plot we get an empirical scaling of : for both sv m lightand smo .
the scaling of the chunking algorithm is : .
the column \minimum " gives a lower bound on the training time .
this bound makes the conjecture that in the general case any optimization algorithms needs to at least once look at the rows of the hessian q which correspond to the support vectors .
the column \minimum " shows the time to compute those rows once ( exploiting symmetry ) .
this time scales with : , showing the complexity inherent in the classi ( cid : 123 ) cation task .
for the training set sizes considered , sv m lightis both close to this minimum scaling as well as within a factor of approximately two in terms of absolute runtime .
classifying web pages
the second data set - again compiled by john platt ( see platt ( a ) ) - is a text clas - si ( cid : 123 ) cation problem with a binary representation based on keyword features .
this representation is extremely sparse .
on average there are only ( cid : 123 ) non - zero features per
table shows training times on this data set for an rbf - kernel ( ) with ( cid : 123 ) = and c = .
again , the times for smo and chunking are taken from platt ( a ) .
sv m lightis faster than smo and chunking on this data set as well , scaling with : .
the best working set size is q = .
the pentium ii takes only ( cid : 123 ) % of the time for running sv m light .
many thanks to john platt for
examples sv m light
smo chunking minimum total sv bsv
table : training times and number of svs for the web data .
number of examples
number of examples
figure : training times from tables ( left ) and ( right ) as graphs .
ohsumed data set
the task in this section is a text classi ( cid : 123 ) cation problem which uses a di ( cid : 123 ) erent represen - tation .
support vector machines have shown very good generalisation performance using this representation ( see joachims ( ) ) .
documents are represented as high dimensional vectors , where each dimension contains a ( tfidf - scaled ) count of how often a particular word occurs in the document .
more details can be found in joachims ( ) .
the par - ticular task is to learn \cardiovascular diseases " category of the ohsumed dataset .
it involves the ( cid : 123 ) rst documents from using features .
on average , there are ( cid : 123 ) non - zero features per example .
an rbf - kernel with ( cid : 123 ) = : and c = is used .
table shows that this tasks involves many svs which are not at the upper bound .
relative to this high number of svs the cache size is small .
to avoid frequent recomputa - tions of the same part of the hessian q , an additional heuristic is incorporated here .
the working set is selected with the constraint that at least for half of the selected variables the kernel values are already cached .
unlike for the previous tasks , optimum performance is achieved with a working set size of q = .
for the training set sizes considered here , runtime is within a factor of from the minimum .
what is the in ( cid : 123 ) uence of the working set selection strategy ?
examples sv m light minimum total sv bsv
table : training time ( in minutes ) and number of svs for the ohsumed data .
dectecting faces in images
in this last problem the task is to classify images according to whether they contain a human face or not .
the data set was collected by shumeet baluja .
the images consist of x pixels of continuous gray values .
so the average number of non - zero attributes per example is .
an rbf - kernel with ( cid : 123 ) = : and c = is used .
the working set size is q = .
examples sv m light minimum total sv bsv
table : training time and number of svs for the face detection data .
table shows the training time ( in seconds ) .
for this task , the training time is very close to the minimum .
this shows that the working set selection strategy is very well suited for avoiding unnecessary kernel evaluations .
the scaling is very close to the
lets now evaluate , how particular strategies of the algorithm in ( cid : 123 ) uence the perfor -
what is the in ( cid : 123 ) uence of the working set selection strategy ?
the left of ( cid : 123 ) gure shows training time dependent on the size of the working set q for the smallest ohsumed task .
the selection strategy from section ( lower curve ) is compared to a basic strategy similar to that proposed in osuna et al .
( ) ( upper curve ) .
in each iteration the basic strategy simply replaces half of the working set with variables that do not ful ( cid : 123 ) ll the optimality conditions .
the graph shows that the new selection strategy reduces time by a factor of more than .
cache - size in mb
size of working set
figure : training time dependent on working set size and cache size for the ohsumed
what is the in ( cid : 123 ) uence of caching ?
the curves in the graph on the right hand side of ( cid : 123 ) gure shows that caching has a strong impact on training time .
the lower curve shows training time ( for an rbf - kernel with ( cid : 123 ) = and c = on the examples of the ohsumed data ) dependent on the cache size when shrinking is used .
with the cache size ranging from megabytes to megabytes a speedup factor of .
is achieved .
the speedup generally increases with an increasing density of the feature vectors xi .
what is the in ( cid : 123 ) uence of shrinking ?
all experiments above use the shrinking strategy from section .
the upper curve in ( cid : 123 ) gure ( right ) shows training time without shrinking .
it can be seen that shrinking leads to a substantial improvement when the cache is small in relation to the size of the problem .
the gain generally increases the smaller the fraction of unbounded svs is compared to the number of training examples ( here unbounded svs , bsvs , and a total of
this chaper presents an improved algorithm for training svms on large - scale problems and describes its e ( cid : 123 ) cient implementation in sv m light .
the algorithm is based on a decomposition strategy and addresses the problem of selecting the variables for the working set in an e ( cid : 123 ) ective and e ( cid : 123 ) cient way .
furthermore , a technique for \shrinking " the problem during the optimization process is introduced .
this is found particularly e ( cid : 123 ) ective for large learning tasks where the fraction of svs is small compared to the sample size , or when many svs are at the upper bound .
the chapter also describes how this algorithm is e ( cid : 123 ) ciently implemented in sv m light .
it has a memory requirement linear in the number of training examples and in the number of svs .
nevertheless , the algorithms can bene ( cid : 123 ) t from additional storage space , since the caching strategy allows an elegant trade - o ( cid : 123 ) between training time and memory consumption .
this work was supported by the dfg collaborative research center on complexity re - duction in multivariate data ( sfb ) .
thanks to alex smola for letting me use his solver .
thanks also to shumeet baluja and to john platt for the data sets .
