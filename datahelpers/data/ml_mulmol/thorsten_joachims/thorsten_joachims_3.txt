machine learning is commonly used to improve ranked re - trieval systems .
due to computational diculties , few learn - ing techniques have been developed to directly optimize for mean average precision ( map ) , despite its widespread use in evaluating such systems .
existing approaches optimiz - ing map either do not nd a globally optimal solution , or are computationally expensive .
in contrast , we present a general svm learning algorithm that eciently nds a globally optimal solution to a straightforward relaxation of map .
we evaluate our approach using the trec 123 and trec 123 web track corpora ( wt123g ) , comparing against svms optimized for accuracy and rocarea .
in most cases we show our method to produce statistically signicant im - provements in map scores .
categories and subject descriptors h . 123 ( information search and retrieval ) : retrieval
algorithm , theory , experimentation
machine learning for information retrieval , support vector
state of the art information retrieval systems commonly use machine learning techniques to learn ranking functions .
however , most current approaches do not optimize for the evaluation measure most often used , namely mean average
instead , current algorithms tend to take one of two gen - eral approaches .
the rst approach is to learn a model that estimates the probability of a document being relevant given
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page .
to copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specic permission and / or a fee .
sigir123 , july 123 , 123 , amsterdam , the netherlands .
copyright 123 acm 123 - 123 - 123 - 123 - 123 / 123 / 123 . . . $123 .
a query ( e . g . , ( 123 , 123 ) ) .
if solved eectively , the ranking with best map performance can easily be derived from the prob - abilities of relevance .
however , achieving high map only requires nding a good ordering of the documents .
as a re - sult , nding good probabilities requires solving a more di - cult problem than necessary , likely requiring more training data to achieve the same map performance .
the second common approach is to learn a function that maximizes a surrogate measure .
performance measures op - timized include accuracy ( 123 , 123 ) , rocarea ( 123 , 123 , 123 , 123 , 123 , 123 ) or modications of rocarea ( 123 ) , and ndcg ( 123 , 123 ) .
learning a model to optimize for such measures might result in suboptimal map performance .
in fact , although some previous systems have obtained good map performance , it is known that neither achieving optimal accuracy nor roc - area can guarantee optimal map performance ( 123 ) .
in this paper , we present a general approach for learning ranking functions that maximize map performance .
specif - ically , we present an svm algorithm that globally optimizes a hinge - loss relaxation of map .
this approach simplies the process of obtaining ranking functions with high map performance by avoiding additional intermediate steps and heuristics .
the new algorithm also makes it conceptually just as easy to optimize svms for map as was previously possible only for accuracy and rocarea .
in contrast to recent work directly optimizing for map performance by metzler & croft ( 123 ) and caruana et al .
( 123 ) , our technique is computationally ecient while nding a globally optimal solution .
like ( 123 , 123 ) , our method learns a linear model , but is much more ecient in practice and , unlike ( 123 ) , can handle many thousands of features .
we now describe the algorithm in detail and provide proof of correctness .
following this , we provide an analysis of run - ning time .
we nish with empirical results from experiments on the trec 123 and trec 123 web track corpus .
we have also developed a software package implementing our algo - rithm that is available for public use123
the learning problem following the standard machine learning setup , our goal is to learn a function h : x y between an input space x ( all possible queries ) and output space y ( rankings over a corpus ) .
in order to quantify the quality of a prediction , y = h ( x ) , we will consider a loss function : y y < .
( y , y ) quanties the penalty for making prediction y if the correct output is y .
the loss function allows us to incorpo - rate specic performance measures , which we will exploit
for optimizing map .
we restrict ourselves to the supervised learning scenario , where input / output pairs ( x , y ) are avail - able for training and are assumed to come from some xed distribution p ( x , y ) .
the goal is to nd a function h such that the risk ( i . e . , expected loss ) ,
p ( h ) = zxy
( y , h ( x ) ) dp ( x , y ) ,
is minimized .
of course , p ( x , y ) is unknown .
but given a nite set of training pairs , s = ( ( xi , yi ) x y : i = 123 , .
, n ) , the performance of h on s can be measured by the empirical risk ,
s ( h ) =
in the case of learning a ranked retrieval function , x de - notes a space of queries , and y the space of ( possibly weak ) rankings over some corpus of documents c = ( d 123 , .
, d|c| ) .
we can dene average precision loss as
map ( y , y ) = 123 map ( rank ( y ) , rank ( y ) ) ,
where rank ( y ) is a vector of the rank values of each doc - ument in c .
for example , for a corpus of two documents , ( d123 , d123 ) , with d123 having higher rank than d123 , rank ( y ) = ( 123 , 123 ) .
we assume true rankings have two rank values , where relevant documents have rank value 123 and non - relevant doc - uments rank value 123
we further assume that all predicted rankings are complete rankings ( no ties ) .
let p = rank ( y ) and p = rank ( y ) .
the average precision
score is dened as
map ( p , p ) =
rel xj : pj =123
where rel = | ( i : pi = 123 ) | is the number of relevant docu - ments , and p rec@j is the percentage of relevant documents in the top j documents in predicted ranking y .
map is the mean of the average precision scores of a group of queries .
123 map vs rocarea
most learning algorithms optimize for accuracy or roc - area .
while optimizing for these measures might achieve good map performance , we use two simple examples to show it can also be suboptimal in terms of map .
rocarea assigns equal penalty to each misordering of a relevant / non - relevant pair .
in contrast , map assigns greater penalties to misorderings higher up in the predicted ranking .
using our notation , rocarea can be dened as
roc ( p , p ) =
rel ( |c| rel ) xi : pi=123 xj : pj =123
123 ( pi> pj ) ,
where p is the true ( weak ) ranking , p is the predicted rank - ing , and 123 ( b ) is the indicator function conditioned on b .
table 123 : toy example and models
suppose we have a hypothesis space with only two hy - pothesis functions , h123 and h123 , as shown in table 123
two hypotheses predict a ranking for query x over a corpus of eight documents .
hypothesis map rocarea
table 123 : performance of toy models
table 123 shows the map and rocarea scores of h123 and h123
here , a learning method which optimizes for roc - area would choose h123 since that results in a higher roc - area score , but this yields a suboptimal map score .
123 map vs accuracy
using a very similar example , we now demonstrate how optimizing for accuracy might result in suboptimal map .
models which optimize for accuracy are not directly con - cerned with the ranking .
instead , they learn a threshold such that documents scoring higher than the threshold can be classied as relevant and documents scoring lower as non -
table 123 : toy example and models
we consider again a hypothesis space with two hypothe - ses .
table 123 shows the predictions of the two hypotheses on a single query x .
hypothesis map best acc .
table 123 : performance of toy models
table 123 shows the map and best accuracy scores of h123 ( q ) and h123 ( q ) .
the best accuracy refers to the highest achiev - able accuracy on that ranking when considering all possi - ble thresholds .
for instance , with h123 ( q ) , a threshold be - tween documents 123 and 123 gives 123 errors ( documents 123 - 123 in - correctly classied as non - relevant ) , yielding an accuracy of 123 .
similarly , with h123 ( q ) , a threshold between documents 123 and 123 gives 123 errors ( documents 123 - 123 incorrectly classi - ed as relevant , and document 123 as non - relevant ) , yielding an accuracy of 123 .
a learning method which optimizes for accuracy would choose h123 since that results in a higher accuracy score , but this yields a suboptimal map score .
optimizing average precision
we build upon the approach used by ( 123 ) for optimiz - ing rocarea .
unlike rocarea , however , map does not decompose linearly in the examples and requires a substan - tially extended algorithm , which we describe in this section .
recall that the true ranking is a weak ranking with two rank values ( relevant and non - relevant ) .
let cx and c x de - note the set of relevant and non - relevant documents of c for query x , respectively .
s ( w ) r
we focus on functions which are parametrized by a weight vector w , and thus wish to nd w to minimize the empirical s ( h ( ; w ) ) .
our approach is to learn a discriminant function f : x y < over input - output pairs .
given query x , we can derive a prediction by nding the ranking y that maximizes the discriminant function :
h ( x; w ) = argmax
f ( x , y; w ) .
we assume f to be linear in some combined feature repre - sentation of inputs and outputs ( x , y ) rn , i . e . ,
f ( x , y; w ) = wt ( x , y ) .
the combined feature function we use is
( x , y ) =
|c x| |c x| xi : dicx xj : djc x
( yij ( ( x , di ) ( x , dj ) ) ) ,
where : x c <n is a feature mapping function from a query / document pair to a point in n dimensional space123
we represent rankings as a matrix of pairwise orderings , y ( 123 , 123 , +123 ) |c||c| .
for any y y , yij = +123 if di is ranked ahead of dj , and yij = 123 if dj is ranked ahead of di , and yij = 123 if di and dj have equal rank .
we consider only matrices which correspond to valid rankings ( i . e , obeying antisymmetry and transitivity ) .
intuitively , is a summa - tion over the vector dierences of all relevant / non - relevant document pairings .
since we assume predicted rankings to be complete rankings , yij is either +123 or 123 ( never 123 ) .
given a learned weight vector w , predicting a ranking ( i . e .
solving equation ( 123 ) ) given query x reduces to picking each yij to maximize wt ( x , y ) .
as is also discussed in ( 123 ) , this is attained by sorting the documents by wt ( x , d ) in descending order .
we will discuss later the choices of we used for our experiments .
123 structural svms
the above formulation is very similar to learning a straight - forward linear model while training on the pairwise dif - ference of relevant / non - relevant document pairings .
many svm - based approaches optimize over these pairwise dier - ences ( e . g . , ( 123 , 123 , 123 , 123 ) ) , although these methods do not optimize for map during training .
previously , it was not clear how to incorporate non - linear multivariate loss func - tions such as map loss directly into global optimization problems such as svm training .
we now present a method based on structural svms ( 123 ) to address this problem .
timization problem 123 , to learn a w rn .
we use the structural svm formulation , presented in op -
optimization problem 123
( structural svm )
i , y y \ yi :
wt ( xi , yi ) wt ( xi , y ) + ( yi , y ) i
the objective function to be minimized ( 123 ) is a tradeo between model complexity , kwk123 , and a hinge loss relaxation of map loss , p i .
as is usual in svm training , c is a
123for example , one dimension might be the number of times the query words appear in the document .
for i = 123 , .
, n do
algorithm 123 cutting plane algorithm for solving op 123 within tolerance .
123 : input : ( x123 , y123 ) , .
, ( xn , yn ) , c , 123 : wi for all i = 123 , .
, n wi wi ( y ) w optimize ( 123 ) over w = si wi 123 : until no wi has changed during iteration
h ( y; w ) ( yi , y ) + wt ( xi , y ) wt ( xi , yi ) compute y = argmaxyy h ( y; w ) compute i = max ( 123 , maxywi h ( y; w ) ) if h ( y; w ) > i + then
parameter that controls this tradeo and can be tuned to achieve good performance in dierent training tasks .
for each ( xi , yi ) in the training set , a set of constraints of the form in equation ( 123 ) is added to the optimization problem .
note that wt ( x , y ) is exactly our discriminant function f ( x , y; w ) ( see equation ( 123 ) ) .
during prediction , our model chooses the ranking which maximizes the discrim - inant ( 123 ) .
if the discriminant value for an incorrect ranking y is greater than for the true ranking yi ( e . g . , f ( xi , y; w ) > f ( xi , yi; w ) ) , then corresponding slack variable , i , must be at least ( yi , y ) for that constraint to be satised .
there -
this is stated formally in proposition 123
fore , the sum of slacks , p i , upper bounds the map loss .
proposition 123
let ( w ) be the optimal solution of the slack variables for op 123 for a given weight vector w
i=123 i is an upper bound on the empirical risk r
( see ( 123 ) for proof )
proposition 123 shows that op 123 learns a ranking function that optimizes an upper bound on map error on the train - ing set .
unfortunately there is a problem : a constraint is required for every possible wrong output y , and the num - ber of possible wrong outputs is exponential in the size of c .
fortunately , we may employ algorithm 123 to solve op 123
algorithm 123 is a cutting plane algorithm , iteratively intro - ducing constraints until we have solved the original problem within a desired tolerance ( 123 ) .
the algorithm starts with no constraints , and iteratively nds for each example ( xi , yi ) the output y associated with the most violated constraint .
if the corresponding constraint is violated by more than we introduce y into the working set wi of active constraints for example i , and re - solve ( 123 ) using the updated w .
it can be shown that algorithm 123s outer loop is guaranteed to halt within a polynomial number of iterations for any desired
theorem 123
let r = maxi maxy k ( xi , yi ) ( xi , y ) k , = maxi maxy ( yi , y ) , and for any > 123 , algorithm 123 terminates after adding at most
constraints to the working set w .
( see ( 123 ) for proof )
however , within the inner loop of this algorithm we have
to compute argmaxyy h ( y; w ) , where
h ( y; w ) = ( yi , y ) + wt ( xi , y ) wt ( xi , yi ) ,
( yi , y ) + wt ( xi , y ) ,
since wt ( xi , yi ) is constant with respect to y .
though closely related to the classication procedure , this has the substantial complication that we must contend with the ad - ditional ( yi , y ) term .
without the ability to eciently nd the most violated constraint ( i . e . , solve argmaxyy h ( y , w ) ) , the constraint generation procedure is not tractable .
123 finding the most violated constraint
using op 123 and optimizing to rocarea loss ( roc ) , the problem of nding the most violated constraint , or solving argmaxyy h ( y , w ) ( henceforth argmax h ) , is addressed in ( 123 ) .
solving argmax h for map is more dicult .
this is primarily because rocarea decomposes nicely into a sum of scores computed independently on each relative order - ing of a relevant / non - relevant document pair .
map , on the other hand , does not decompose in the same way as roc - area .
the main algorithmic contribution of this paper is an ecient method for solving argmax h for map .
one useful property of map is that it is invariant to swap - ping two documents with equal relevance .
for example , if documents da and db are both relevant , then swapping the positions of da and db in any ranking does not aect map .
by extension , map is invariant to any arbitrary permuta - tion of the relevant documents amongst themselves and of the non - relevant documents amongst themselves .
however , this reshuing will aect the discriminant score , wt ( x , y ) .
this leads us to observation 123
observation 123
consider rankings which are constrained by xing the relevance at each position in the ranking ( e . g . , the 123rd document in the ranking must be relevant ) .
every ranking which satises the same set of constraints will have the same map .
if the relevant documents are sorted by wt ( x , d ) in descending order , and the non - relevant docu - ments are likewise sorted by wt ( x , d ) , then the interleav - ing of the two sorted lists which satises the constraints will maximize h for that constrained set of rankings .
observation 123 implies that in the ranking which maxi - mizes h , the relevant documents will be sorted by wt ( x , d ) , and the non - relevant documents will also be sorted likewise .
by rst sorting the relevant and non - relevant documents , the problem is simplied to nding the optimal interleaving of two sorted lists .
for the rest of our discussion , we assume that the relevant documents and non - relevant documents are both sorted by descending wt ( x , d ) .
for convenience , we also refer to relevant documents as ( dx 123 , .
dx|cx| ) = cx , and non - relevant documents as ( dx
dx|c x| ) = c x .
we dene j ( i123 , i123 ) , with i123 < i123 , as the change in h from when the highest ranked relevant document ranked after dx
for i123 = i123 + 123 , we have
i123 to when it is dx
j ( i , i + 123 ) =
j + i 123 123 ( sx
|cx| |c x|
where si = wt ( x , di ) .
the rst term in ( 123 ) is the change in map when the ith relevant document has j non - relevant
documents ranked before it , as opposed to j123
the second term is the change in the discriminant score , wt ( x , y ) , when yij changes from +123 to 123
i , dx j , dx
j , dx i , dx
figure 123 : example for j ( i , i + 123 )
figure 123 gives a conceptual example for j ( i , i + 123 ) .
the bottom ranking diers from the top only where dx j slides up one rank .
the dierence in the value of h for these two rankings is exactly j ( i , i + 123 ) .
for any i123 < i123 , we can then dene j ( i123 , i123 ) as
j ( i123 , i123 ) =
j ( i123 , i123 ) =
j ( k , k + 123 ) ,
j + k 123 123 ( sx
|cx| |c x| .
let o123 , .
, o|c x| encode the positions of the non - relevant documents , where dx oj is the highest ranked relevant docu - ment ranked after the jth non - relevant document .
due to observation 123 , this encoding uniquely identies a complete ranking .
we can recover the ranking as
sign ( oj123 i123 123 ) sign ( j123 oi123 + 123 )
if i = j if di , dj equal relevance if di = dx if di = dx
i123 , dj = dx i123 , dj = dx
we can now reformulate h into a new objective function ,
, o|c x||w ) = h ( y|w ) +
k ( ok , |cx| + 123 ) ,
where y is the true ( weak ) ranking .
conceptually h123 starts with a perfect ranking y , and adds the change in h when each successive non - relevant document slides up the ranking .
we can then reformulate the argmax h problem as
k ( ok , |cx| + 123 )
o|c x| .
algorithm 123 describes the algorithm used to solve equa - tion ( 123 ) .
conceptually , algorithm 123 starts with a perfect ranking .
then for each successive non - relevant document , the algorithm modies the solution by sliding that docu - ment up the ranking to locally maximize h123 while keeping the positions of the other non - relevant documents constant .
123 . 123 proof of correctness algorithm 123 is greedy in the sense that it nds the best position of each non - relevant document independently from the other non - relevant documents .
in other words , the al - gorithm maximizes h123 for each non - relevant document , dx
algorithm 123 finding the most violated constraint ( argmax h ) for algorithm 123 with map 123 : input : w , cx , c x 123 : sort cx and c x in descending order of wt ( x , d ) i ) , i = 123 , .
, |cx| i wt ( x , dx i ) , i = 123 , .
, |c x| i wt ( x , dx 123 : for j = 123 , .
, |c x| do optj argmaxk j ( k , |cx| + 123 ) 123 : end for 123 : encode y according to ( 123 ) 123 : return y
without considering the positions of the other non - relevant documents , and thus ignores the constraints of ( 123 ) .
in order for the solution to be feasible , then jth non - relevant document must be ranked after the rst j 123 non - relevant documents , thus satisfying
opt123 opt123 .
opt|c x| .
if the solution is feasible , the it clearly solves ( 123 ) .
therefore , it suces to prove that algorithm 123 satises ( 123 ) .
we rst prove that j ( , ) is monotonically decreasing in j .
lemma 123
for any 123 i123 < i123 |cx| + 123 and 123 j <
|c x| , it must be the case that
j+123 ( i123 , i123 ) j ( i123 , i123 ) .
recall from ( 123 ) that both j ( i123 , i123 ) and j+123 ( i123 , i123 ) are summations of i123 i123 terms .
we will show that each term in the summation of j+123 ( i123 , i123 ) is no greater than the corresponding term in j ( i123 , i123 ) , or
j+123 ( k , k + 123 ) j ( k , k + 123 )
for k = i123 , .
, i123 123
each term in j ( k , k + 123 ) and j+123 ( k , k + 123 ) can be further decomposed into two parts ( see ( 123 ) ) .
we will show that each part of j+123 ( k , k + 123 ) is no greater than the corresponding part in j ( k , k + 123 ) .
in other words , we will show that both
j + k + 123
j + k 123
|cx| |c x| 123 ( sx
|cx| |c x|
it is easy to see that ( 123 ) is true by observing that for any
are true for the aforementioned values of j and k .
two positive integers 123 a < b ,
a + 123 b + 123
and choosing a = j and b = j + k .
the second inequality ( 123 ) holds because algorithm 123 rst
sorts dx in descending order of sx , implying sx
thus we see that each term in j+123 is no greater than the
corresponding term in j , which completes the proof .
the result of lemma 123 leads directly to our main correct -
theorem 123
in algorithm 123 , the computed values of optj satisfy ( 123 ) , implying that the solution returned by algorithm 123 is feasible and thus optimal .
we will prove that
holds for any 123 j < |c x| , thus implying ( 123 ) .
since algorithm 123 computes optj as
optj = argmax
j ( k , |cx| + 123 ) ,
then by denition of j ( 123 ) , for any 123 i < optj ,
j ( i , optj ) = j ( i , |cx| + 123 ) j ( optj , |cx| + 123 ) < 123
using lemma 123 , we know that
j+123 ( i , optj ) j ( i , optj ) < 123 ,
which implies that for any 123 i < optj ,
j+123 ( i , |cx| + 123 ) j+123 ( optj , |cx| + 123 ) < 123
suppose for contradiction that optj+123 < optj .
then j+123 ( optj+123 , |cx| + 123 ) < j+123 ( optj , |cx| + 123 ) ,
which contradicts ( 123 ) .
therefore , it must be the case that optj optj+123 , which completes the proof .
123 . 123 running time the running time of algorithm 123 can be split into two parts .
the rst part is the sort by wt ( x , d ) , which re - quires o ( n log n ) time , where n = |cx| + |c x| .
the second part computes each optj , which requires o ( |cx| |c x| ) time .
though in the worst case this is o ( n123 ) , the number of rel - evant documents , |cx| , is often very small ( e . g . , constant with respect to n ) , in which case the running time for the second part is simply o ( n ) .
for most real - world datasets , algorithm 123 is dominated by the sort and has complexity o ( n log n ) .
algorithm 123 is guaranteed to halt in a polynomial num - ber of iterations ( 123 ) , and each iteration runs algorithm 123
virtually all well - performing models were trained in a rea - sonable amount of time ( usually less than one hour ) .
once training is complete , making predictions on query x us - ing the resulting hypothesis h ( x|w ) requires only sorting by wt ( x , d ) .
we developed our software using a python interface123 to svmstruct , since the python language greatly simplied the coding process .
to improve performance , it is advisable to use the standard c implementation123 of svmstruct .
experiment setup
the main goal of our experiments is to evaluate whether directly optimizing map leads to improved map perfor - mance compared to conventional svm methods that opti - mize a substitute loss such as accuracy or rocarea .
we empirically evaluate our method using two sets of trec web track queries , one each from trec 123 and trec 123 ( topics 123 - 123 and 123 - 123 ) , both of which used the wt123g corpus .
for each query , trec provides the relevance judg - ments of the documents .
we generated our features using the scores of existing retrieval functions on these queries .
while our method is agnostic to the meaning of the fea - tures , we chose to use existing retrieval functions as a simple yet eective way of acquiring useful features .
as such , our
base funcs features
trec 123 indri trec 123 indri trec 123 submissions trec 123 submissions
table 123 : dataset statistics
experiments essentially test our methods ability to re - rank the highly ranked documents ( e . g . , re - combine the scores of the retrieval functions ) to improve map .
we compare our method against the best retrieval func - tions trained on ( henceforth base functions ) , as well as against previously proposed svm methods .
comparing with the best base functions tests our methods ability to learn a use - ful combination .
comparing with previous svm methods allows us to test whether optimizing directly for map ( as opposed to accuracy or rocarea ) achieves a higher map score in practice .
the rest of this section describes the base functions and the feature generation method in detail .
123 choosing retrieval functions
we chose two sets of base functions for our experiments .
for the rst set , we generated three indices over the wt123g corpus using indri123
the rst index was generated using default settings , the second used porter - stemming , and the last used porter - stemming and indris default stopwords .
for both trec 123 and trec 123 , we used the descrip - tion portion of each query and scored the documents using ve of indris built - in retrieval methods , which are cosine similarity , tfidf , okapi , language model with dirichlet prior , and language model with jelinek - mercer prior .
all parameters were kept as their defaults .
we computed the scores of these ve retrieval methods over the three indices , giving 123 base functions in total .
for each query , we considered the scores of documents found in the union of the top 123 documents of each base function .
for our second set of base functions , we used scores from the trec 123 ( 123 ) and trec 123 ( 123 ) web track submissions .
we used only the non - manual , non - short submissions from both years .
for trec 123 and trec 123 , there were 123 and 123 such submissions , respectively .
a typical submission con - tained scores of its top 123 documents .
figure 123 : example feature binning
123 generating features
in order to generate input examples for our method , a concrete instantiation of must be provided .
for each doc -
table 123 : comparison with indri functions
ument d scored by a set of retrieval functions f on query x , we generate the features as a vector
( x , d ) = h123 ( f ( d|x ) >k ) : f f , k kfi ,
where f ( d|x ) denotes the score that retrieval function f as - signs to document d for query x , and each kf is a set of real values .
from a high level , we are expressing the score of each retrieval function using |kf| + 123 bins .
since we are using linear kernels , one can think of the learning problem as nding a good piecewise - constant com - bination of the scores of the retrieval functions .
figure 123 shows an example of our feature mapping method .
in this example we have a single feature f = ( f ) .
here , kf = ( a , b , c ) , and the weight vector is w = hwa , wb , wci .
for any document d and query x , we have
wt ( x , d ) =
wa + wb wa + wb + wc
if f ( d|x ) < a if a f ( d|x ) < b if b f ( d|x ) < c if c f ( d|x )
this is expressed qualitatively in figure 123 , where wa and wb are positive , and wc is negative .
we ran our main experiments using four choices of f : the set of aforementioned indri retrieval functions for trec 123 and trec 123 , and the web track submissions for trec 123 and trec 123
for each f and each function f f , we chose 123 values for kf which are reasonably spaced and capture the sensitive region of f .
using the four choices of f , we generated four datasets for our main experiments .
table 123 contains statistics of the generated datasets .
there are many ways to generate features , and we are not advocating our method over others .
this was simply an ecient means to normalize the outputs of dierent functions and allow for a more expressive model .
for each dataset in table 123 , we performed 123 trials .
for each trial , we train on 123 randomly selected queries , and se - lect another 123 queries at random for a validation set .
mod - els were trained using a wide range of c values .
the model which performed best on the validation set was selected and tested on the remaining 123 queries .
map ) , an svm optimizing for rocarea ( svm
all queries were selected to be in the training , validation and test sets the same number of times .
using this setup , we performed the same experiments while using our method and a conventional classication svm ( svmacc ) ( 123 ) .
all svm methods used a linear kernel .
we reported the average performance of all models over the 123 trials .
123 comparison with base functions
in analyzing our results , the rst question to answer is , map learn a model which outperforms the best base
bca trec 123
table 123 : comparison with trec submissions
table 123 : comparison with trec subm .
( w / o best )
functions ? table 123 presents the comparison of svm the best indri base functions .
each column group contains the macro - averaged map performance of svm map or a base function .
the w / l columns show the number of queries map achieved a higher map score .
signicance tests were performed using the two - tailed wilcoxon signed rank test .
two stars indicate a signicance level of 123 .
all tables displaying our experimental results are structured identically .
here , we nd that svm map signicantly outper - forms the best base functions .
table 123 shows the comparison when trained on trec sub - missions .
while achieving a higher map score than the best base functions , the performance dierence between svm the base functions is not signicant .
given that many of these submissions use scoring functions which are carefully crafted to achieve high map , it is possible that the best performing submissions use techniques which subsume the techniques of the other submissions .
as a result , svm would not be able to learn a hypothesis which can signi - cantly out - perform the best submission .
hence , we ran the same experiments using a modied dataset where the features computed using the best submis - sion were removed .
table 123 shows the results ( note that we are still comparing against the best submission though we are not using it for training ) .
notice that while the perfor - mance of svm map degraded slightly , the performance was still comparable with that of the best submission .
123 comparison w / previous svm methods
the next question to answer is , does svm
higher map scores than previous svm methods ? tables 123 and 123 present the results of svm roc , and svmacc when trained on the indri retrieval functions and trec sub - missions , respectively .
table 123 contains the corresponding results when trained on the trec submissions without the
map and svm
to start with , our results indicate that svmacc was not competitive with svm roc , and at times un - derperformed dramatically .
as such , we tried several ap - proaches to improve the performance of svmacc .
123 . 123 alternate svmacc methods one issue which may cause svmacc to underperform is the severe imbalance between relevant and non - relevant doc -
table 123 : trained on indri functions
table 123 : trained on trec submissions
uments .
the vast majority of the documents are not rele - vant .
svmacc123 addresses this problem by assigning more penalty to false negative errors .
for each dataset , the ratio of the false negative to false positive penalties is equal to the ratio of the number non - relevant and relevant documents in that dataset .
tables 123 , 123 and 123 indicate that svmacc123 still performs signicantly worse than svm
another possible issue is that svmacc attempts to nd just one discriminating threshold b that is query - invariant .
it may be that dierent queries require dierent values of b .
having the learning method trying to nd a good b value ( when one does not exist ) may be detrimental .
we took two approaches to address this issue .
the rst method , svmacc123 , converts the retrieval function scores into percentiles .
for example , for document d , query q and re - trieval function f , if the score f ( d|q ) is in the top 123% of the scores f ( |q ) for query q , then the converted score is f123 ( d|q ) = 123 .
each kf contains 123 evenly spaced values between 123 and 123
tables 123 , 123 and 123 show that the perfor - mance of svmacc123 was also not competitive with svm the second method , svmacc123 , normalizes the scores given by f for each query .
for example , assume for query q that f outputs scores in the range 123 to 123 .
then for document d , if f ( d|q ) = 123 , the converted score would be f123 ( d|q ) = ( 123 123 ) / ( 123 123 ) = 123 .
each kf contains 123 evenly spaced values between 123 and 123
again , tables 123 , 123 and 123 show that svmacc123 was not competitive with svm
123 . 123 map vs rocarea
roc performed much better than svmacc in our ex - periments .
when trained on indri retrieval functions ( see table 123 ) , the performance of svm roc was slight , though not signicantly , worse than the performances of svm however , table 123 shows that svm map did signicantly out -
roc when trained on the trec submissions .
table 123 shows the performance of the models when trained on the trec submissions with the best submission removed .
the performance of most models degraded by a small amount ,
map still having the best performance .
table 123 : trained on trec subm .
( w / o best )
conclusions and future work we have presented an svm method that directly opti - mizes map .
it provides a principled approach and avoids dicult to control heuristics .
we formulated the optimiza - tion problem and presented an algorithm which provably nds the solution in polynomial time .
we have shown em - pirically that our method is generally superior to or com - petitive with conventional svms methods .
our new method makes it conceptually just as easy to optimize svms for map as was previously possible only for accuracy and rocarea .
the computational cost for training is very reasonable in practice .
since other methods typically require tuning multiple heuristics , we also expect to train fewer models before nding one which achieves good
the learning framework used by our method is fairly gen - eral .
a natural extension of this framework would be to develop methods to optimize for other important ir mea - sures , such as normalized discounted cumulative gain ( 123 , 123 , 123 , 123 ) and mean reciprocal rank .
this work was funded under nsf award iis - 123 , nsf career award 123 , and a gift from yahoo ! re - search .
the third author was also partly supported by a microsoft research fellowship .
