learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence .
while recent progress in machine learning has mainly focused on designing exible and powerful input representations , this paper addresses the comple - mentary issue of designing classication algorithms that can deal with more complex outputs , such as trees , sequences , or sets .
more generally , we consider problems involving multiple dependent output variables , structured output spaces , and classication problems with class attributes .
in order to accomplish this , we propose to appropriately generalize the well - known notion of a separation margin and derive a corresponding maximum - margin formulation .
while this leads to a quadratic program with a potentially prohibitive , i . e .
exponential , number of constraints , we present a cut - ting plane algorithm that solves the optimization problem in polynomial time for a large class of problems .
the proposed method has important applications in areas such as computational biology , natural language processing , information retrieval / extraction , and optical character recognition .
ex - periments from various domains involving different types of output spaces emphasize the breadth and generality of our approach .
this paper deals with the general problem of learning a mapping from input vectors or patterns x x to discrete response variables y y , based on a training sample of input - output pairs ( x123 , y123 ) , .
, ( xn , yn ) x y drawn from some xed but unknown probability distribution .
un - like multiclass classication , where the output space consists of an arbitrary nite set of labels or class identiers , y = ( 123 , . . . , k ) , or regression , where y = r and the response variable is a scalar , we consider the case where elements of y are structured objects such as sequences , strings , trees ,
c ( cid : 123 ) 123 ioannis tsochantaridis , thorsten joachims , thomas hofmann and yasemin altun .
tsochantaridis , joachims , hofmann and altun
lattices , or graphs .
such problems arise in a variety of applications , ranging from multilabel clas - sication and classication with class taxonomies , to label sequence learning , sequence alignment learning , and supervised grammar learning , to name just a few .
more generally , these problems fall into two generic cases : rst , problems where classes themselves can be characterized by certain class - specic attributes and learning should occur across classes as much as across patterns; second , problems where y represents a macro - label , i . e .
describes a conguration over components or state variables y = ( y123 , .
, yt ) , with possible dependencies between these state variables .
we approach these problems by generalizing large margin methods , more specically multiclass support vector machines ( svms ) ( weston and watkins , 123; crammer and singer , 123 ) , to the broader problem of learning structured responses .
the naive approach of treating each structure as a separate class is often intractable , since it leads to a multiclass problem with a very large number of classes .
we overcome this problem by specifying discriminant functions that exploit the structure and dependencies within y .
in that respect our approach follows the work of collins ( 123 ) on perceptron learning with a similar class of discriminant functions .
however , the maximum - margin algorithm we propose has advantages in terms of accuracy and tunability to specic loss functions .
a maximum - margin algorithm has also been proposed by collins and duffy ( 123a ) in the context of natural language processing .
however , it depends on the size of the output space , therefore it requires some external process to enumerate a small number of candidate outputs y for a given input x .
the same is true also for other ranking algorithms ( cohen et al . , 123; herbrich et al . , 123; schapire and singer , 123; crammer and singer , 123; joachims , 123 ) .
in contrast , we have proposed an efcient algorithm ( hofmann et al . , 123; altun et al . , 123; joachims , 123 ) even in the case of very large output spaces , that takes advantage of the sparseness of the maximum -
a different maximum - margin algorithm that can deal with very large output sets , maximum margin markov networks , has been independently proposed by taskar et al .
( 123a ) .
the structure of the output is modeled by a markov network , and by employing a probabilistic interpretation of the dual formulation of the problem , taskar et al .
( 123a ) propose a reparameterization of the problem , that leads to an efcient algorithm , as well as generalization bounds that do not depend on the size of the output space .
the proposed reparameterization , however , assumes that the loss function can be decomposed in the the same fashion as the feature map , thus does not support arbitrary loss functions that may be appropriate for specic applications .
on the surface our approach is related to the kernel dependency estimation approach described in weston et al .
( 123 ) .
there , however , separate kernel functions are dened for the input and output space , with the idea to encode a priori knowledge about the similarity or loss function in output space .
in particular , this assumes that the loss is input dependent and known beforehand .
more specically , in weston et al .
( 123 ) a kernel pca is used in the feature space dened over y to reduce the problem to a ( small ) number of independent regression problems .
the latter corresponds to an unsupervised embedding ( followed by dimension reduction ) performed in the output space and no information about the patterns x is utilized in dening this low - dimensional representation .
in contrast , the key idea in our approach is not primarily to dene more complex functions , but to deal with more complex output spaces by extracting combined features over inputs and outputs .
for a large class of structured models , we propose a novel svm algorithm that allows us to learn mappings involving complex structures in polynomial time despite an exponential ( or in - nite ) number of possible output values .
in addition to respective theoretical results , we empirically evaluate our approach for a number of specic problem instantiations : classication with class tax -
large margin methods for structured and interdependent output variables
figure 123 : illustration of natural language parsing model .
onomies , label sequence learning , sequence alignment , and natural language parsing .
this paper extends tsochantaridis et al .
( 123 ) with additional theoretical and empirical results .
the rest of the paper is organized as follows : section 123 presents the general framework of large margin learning over structured output spaces using representations of input - output pairs via joint feature maps .
section 123 describes and analyzes a generic algorithm for solving the resulting optimization problems .
sections 123 and 123 discuss numerous important special cases and experimental
large margin learning with joint feature maps we are interested in the general problem of learning functions f : x y between input spaces x and arbitrary discrete output spaces y based on a training sample of input - output pairs .
as an illustrating example , which we will continue to use as a prototypical application in the sequel , consider the case of natural language parsing , where the function f maps a given sentence x to a parse tree y .
this is depicted graphically in figure 123
the approach we pursue is to learn a discriminant function f : x y r over input - output pairs from which we can derive a prediction by maximizing f over the response variable for a specic given input x .
hence , the general form of our hypotheses f is
f ( x;w ) = argmax
where w denotes a parameter vector .
it might be useful to think of f as a compatibility function that measures how compatible pairs ( x , y ) are , or , alternatively , f can be thought of as a w - parameterized family of cost functions , which we try to design in such a way that the minimum of f ( x , ;w ) is at the desired output y for inputs x of interest .
inputs and outputs y
throughout this paper , we assume f to be linear in some combined feature representation of
the specic form of y depends on the nature of the problem and special cases will be discussed subsequently .
however , whenever possible we will develop learning algorithms and theoretical
f ( x , y;w ) = hw , y
tsochantaridis , joachims , hofmann and altun
results for the general case .
since we want to exploit the advantages of kernel - based method , we will pay special attention to cases where the inner product in the joint representation can be efciently
computed via a joint kernel function j ( ( x , y ) , ( x123 , y123 ) ) = hy
using again natural language parsing as an illustrative example , we can chose f such that we get a model that is isomorphic to a probabilistic context free grammar ( pcfg ) ( cf .
manning and schuetze , 123 ) .
each node in a parse tree y for a sentence x corresponds to grammar rule g j , which in turn has a score w j .
all valid parse trees y ( i . e .
trees with a designated start symbol s as the root and the words in the sentence x as the leaves ) for a sentence x are scored by the sum of the w j of their nodes .
this score can thus be written in the form of equation ( 123 ) , with y ( x , y ) denoting a histogram vector of counts ( how often each grammar rule g j occurs in the tree y ) .
f ( x;w ) can be efciently computed by nding the structure y y that maximizes f ( x , y;w ) via the cky algorithm ( younger , 123; manning and schuetze , 123 ) .
123 loss functions and risk minimization
the standard zero - one loss function typically used in classication is not appropriate for most kinds of structured responses .
for example , in natural language parsing , a parse tree that is almost correct and differs from the correct parse in only one or a few nodes should be treated differently from a parse tree that is completely different .
typically , the correctness of a predicted parse tree is measured by its f123 score ( see e . g .
johnson , 123 ) , the harmonic mean of precision and recall as calculated based on the overlap of nodes between the trees .
in order to quantify the accuracy of a prediction , we will consider learning with arbitrary loss functions 123 : y y r .
here 123 ( y , y ) quanties the loss associated with a prediction y , if the true output value is y .
it is usually sufcient to restrict attention to zero diagonal loss functions with 123 ( y , y ) = 123 and for which furthermore 123 ( y , y123 ) > 123 for y 123= y123 moreover , we assume the loss is bounded for every given target value y , i . e .
maxy ( 123 ( y , y ) ) exists .
we investigate a supervised learning scenario , where input - output pairs ( x , y ) are generated according to some xed distribution p ( x , y ) and the goal is to nd a function f in a given hypothesis class such that the risk ,
r 123p ( f ) = zxy 123 ( y , f ( x ) ) dp ( x , y ) ,
is minimized .
of course , p is unknown and following the supervised learning paradigm , we assume that a nite training set of pairs s = ( ( xi , yi ) x y : i = 123 , .
, n ) generated i . i . d .
according to p is given .
the performance of a function f on the training sample s is described by the empirical
r 123s ( f ) =
i=123 ( yi , f ( xi ) ) ,
which is simply the expected loss under the empirical distribution induced by s .
for w - parameterized hypothesis classes , we will also write r 123p ( w ) r 123p ( f ( ;w ) ) and similarly for the empirical risk .
cases where 123 ( y , y123 ) = 123 for y 123= y123 can be dealt with , but lead to additional technical overhead , which we chose to
avoid for the sake of clarity .
large margin methods for structured and interdependent output variables
123 margin maximization
we consider various scenarios for the generalization of support vector machine learning over struc - tured outputs .
we start with the simple case of hard - margin svms , followed by soft - margin svms , and nally we propose two approaches for the case of loss - sensitive svms , which is the most gen - eral case and subsumes the former ones .
123 . 123 separable case
first , we consider the case where there exists a function f parameterized by w such that the empirical risk is zero .
the condition of zero training error can then be compactly written as a set of nonlinear
i ( 123 , .
, n ) :
notice that this holds independently of the loss functions , since we have assumed that 123 ( y , y ) = 123 and 123 ( y , y123 ) > 123 for y 123= y123
every one of the nonlinear inequalities in equation ( 123 ) can be equivalently replaced by |y | 123 linear inequalities , resulting in a total of n|y | n linear constraints ,
i ( 123 , .
, n ) , y y \ yi :
( xi , y ) i 123 .
as we will often encounter terms involving feature vector differences of the type appearing in equation ( 123 ) , we dene d ( xi , y ) so that the constraints can be more compactly written as hw , d if the set of inequalities in equation ( 123 ) is feasible , there will typically be more than one solu - tion w .
to specify a unique solution , we propose to select the w for which the separation margin g , i . e .
the minimal differences between the score of the correct label yi and the closest runner - up y ( w ) = argmaxy123=yihw , y ( xi , y ) i , is maximal .
this generalizes the maximum - margin principle em - ployed in support vector machines ( vapnik , 123 ) to the more general case considered in this paper .
restricting the l123 norm of w to make the problem well - posed leads to the following optimization
i ( 123 , .
, n ) , y y \ yi :
i ( y ) i g .
this problem can be equivalently expressed as a convex quadratic program in standard form
i , y y \ yi : hw , d
i ( y ) i 123 .
123 . 123 soft - margin maximization
to allow errors in the training set , we introduce slack variables and propose to optimize a soft - margin criterion .
as in the case of multiclass svms , there are at least two ways of introducing slack variables .
one may introduce a single slack variable x i for violations of the nonlinear constraints
tsochantaridis , joachims , hofmann and altun
every instance xi ) ( crammer and singer , 123 ) or one may penalize margin violations for every linear constraint ( i . e .
every instance xi and output y 123= yi ) ( weston and watkins , 123; har - peled et al . , 123 ) .
since the former will result in a ( tighter ) upper bound on the empirical risk ( cf .
proposition 123 ) and offers some advantages in the proposed optimization scheme ( cf .
section 123 ) , we have focused on this formulation .
adding a penalty term that is linear in the slack variables to the objective , results in the quadratic program
i , y y \ yi : hw , d
i ( y ) i 123 x
i 123 .
alternatively , we can also penalize margin violations by a quadratic term leading to the following
i , y y \ yi : hw , d
i ( y ) i 123 x
in both cases , c > 123 is a constant that controls the trade - off between training error minimization and
123 . 123 general loss functions : slack re - scaling
the rst approach we propose for the case of arbitrary loss functions , is to re - scale the slack vari - ables according to the loss incurred in each of the linear constraints .
intuitively , violating a margin constraint involving a y 123= yi with high loss 123 ( yi , y ) should be penalized more severely than a vi - olation involving an output value with smaller loss .
this can be accomplished by multiplying the margin violation by the loss , or equivalently , by scaling the slack variable with the inverse loss ,
i , y y \ yi : hw , d
x i is an upper bound on the empirical risk r 123s ( w ) .
a justication for this formulation is given by the subsequent proposition .
proposition 123 denote by x ( w ) the optimal solution of the slack variables in svm123s weight vector w .
then 123 proof notice rst that x i = max ( 123 , maxy123=yi ( 123 ( yi , y ) ( 123hw , d case 123 : if f ( xi;w ) = yi , then x i 123 = 123 ( yi , f ( xi;w ) ) and the loss is trivially upper bounded .
case 123 : if y f ( xi;w ) 123= yi , then hw , d i ( y ) i 123 and thus x i 123 ( yi , y ) .
123 ( yi , y ) 123 which is equivalent to
since the bound holds for every training instance , it also holds for the average .
for a given
the optimization problem svm123s in order to obtain an upper bound on the empirical risk .
123 can be derived analogously , where 123 ( yi , y ) is replaced byp123 ( yi , y )
large margin methods for structured and interdependent output variables
123 . 123 general loss functions : margin re - scaling
in addition to this slack re - scaling approach , a second way to include loss functions is to re - scale the margin as proposed by taskar et al .
( 123a ) for the special case of the hamming loss .
it is straightforward to generalize this method to general loss functions .
the margin constraints in this setting take the following form :
i , y y :
i ( y ) i 123 ( yi , y ) x
the set of constraints in equation ( 123 ) combined with the objective in equation ( 123 ) yield an opti - mization problem svm123m
123 which also results in an upper bound on r 123s ( w ) .
proposition 123 denote by x ( w ) the optimal solution of the slack variables in svm123m weight vector w .
then 123 proof the essential observation is that x i = max ( 123 , maxy ( 123 ( yi , y ) hw , d anteed to upper bound 123 ( yi , y ) for y such that hw , d
x i is an upper bound on the empirical risk r 123s ( w ) .
i ( y ) i ) ) which is guar -
for a given
the optimization problem svm123m
can be derived analogously , where 123 ( yi , y ) is replaced byp123 ( yi , y ) .
123 . 123 general loss functions : discussion
let us discuss some of the advantages and disadvantages of the two formulations presented .
an appealing property of the slack re - scaling approach is its scaling invariance .
, the optimization problems svm123s
proposition 123 suppose 123 h 123 with h > 123 , i . e .
123 is a scaled version of the original loss 123
then by re - scaling c123 = c / h ( c123 ) are equivalent as far as w is concerned .
in particular the optimal weight vector w is the same in both cases .
proof first note that each w is feasible for svm123s in the sense that we can nd slack variables such that all the constraints are satised .
in fact we can chose them optimally and dene i x i 123 ( w ) , where x and x 123 refer to the optimal slacks in svm123s , respectively , for given w .
it is easy to see that they are given
i x i ( w ) and h123 ( w ) 123 123 and svm123s
123 and svm123s
123kwk123 + c123
( c ) and svm123s
123kwk123 + c
x i = max ( 123 , max
x i 123 = max ( 123 , max
y123=yi ( h 123 ( yi , y ) ( 123hw , d
respectively .
pulling h out of the max , one gets that x i 123 = h from that it follows immediately that h = h123
i and thus ( cid : 123 )
i x i = ch
i x i 123 = c123 ( cid : 123 )
i x i 123
in contrast , the margin re - scaling formulation is not invariant under scaling of the loss function .
one needs , for example , to re - scale the feature map y by a corresponding scale factor as well .
this seems to indicate that one has to calibrate the scaling of the loss and the scaling of the feature map
tsochantaridis , joachims , hofmann and altun
more carefully in the svm123m the loss scale explicitly in terms of the constant c .
formulation .
the svm123s
formulation on the other hand , represents
a second disadvantage of the margin scaling approach is that it potentially gives signicant weight to output values y y that are not even close to being confusable with the target values yi , because every increase in the loss increases the required margin .
if one interprets f ( xi , yi;w ) f ( xi , y;w ) as a log odds ratio of an exponential family model ( smola and hofmann , 123 ) , then the margin constraints may be dominated by incorrect values y that are exponentially less likely than the target value .
to be more precise , notice that in the svm123s formulation , the penalty part only depends on y for which hw , d i ( y ) i 123
these are output values y that all receive , x i a relatively high ( i . e .
123 - close to the optimum ) value of f ( x , y;w ) .
however , in svm123m i ( y ) i for all y .
this means x i can be dominated by a value has to majorize 123 ( yi , y ) hw , d y = argmaxy ( 123 ( yi , y ) hw , d i ( y ) i ) which has a large loss , but whose value of f ( x , y;w ) comes
nowhere near the optimal value of f .
support vector algorithm for structured output spaces
so far we have not discussed how to solve the optimization problems associated with the various formulations svm123 , svm123 , svm123 , svm123s .
the key challenge is that the size of each of these problems can be immense , since we have to deal with n|y | n margin inequalities .
in many cases , |y | may be extremely large , in particular , if y is a product space of some sort ( e . g .
in grammar learning , label sequence learning , etc . ) , its cardinality may grow exponentially in the description length of y .
this makes standard quadratic programming solvers unsuitable for this type of problem .
, and svm123m
in the following , we will propose an algorithm that exploits the special structure of the maximum - margin problem , so that only a much smaller subset of constraints needs to be explicitly examined .
we will show that the algorithm can compute arbitrary close approximations to all svm optimiza - tion problems posed in this paper in polynomial time for a large range of structures and loss func - tions .
since the algorithm operates on the dual program , we will rst derive the wolfe dual for the various soft margin formulations .
123 dual programs we will denote by a ( iy ) the lagrange multiplier enforcing the margin constraint for label y 123= yi and example ( xi , yi ) .
using standard lagragian duality techniques , one arrives at the following dual quadratic program ( qp ) .
proposition 123 the objective of the dual problem of svm123 from equation ( 123 ) is given by
q ( a )
( jy ) j ( iy ) ( jy ) + ( cid : 123 )
where j ( iy ) ( jy ) = ( cid : 123 ) d
j ( y ) ( cid : 123 ) .
the dual qp can be formulated as a = argmax
q ( a ) ,
a 123 .
large margin methods for structured and interdependent output variables
proof ( sketch ) forming the lagrangian function and eliminating the primal variables w by using the optimality condition
w ( a ) = ( cid : 123 )
directly leads to the above dual program .
notice that the j function that generates the quadratic from in the dual objective can be computed from inner products involving values of y , which is a simple consequence of the linearity of the inner product .
j can hence be alternatively computed from a joint kernel function over x y .
penalties modify the kernel function .
in the non - separable case , linear penalties introduce additional constraints , whereas the squared
proposition 123 the dual problem to svm123 is given by the program in proposition 123 with additional
, i = 123 ,
in the following , we denote with d ( a , b ) the function that returns 123 if a = b , and 123 otherwise .
proposition 123 the dual problem to svm123 is given by the program in proposition 123 with modied
j ( iy ) ( jy ) ( cid : 123 ) d
j ( y ) ( cid : 123 ) + d ( i , j )
in the non - separable case with slack re - scaling , the loss function is introduced in the constraints for linear penalties and in the kernel function for quadratic penalties .
proposition 123 the dual problem to svm123s
is given by the program in proposition 123 with additional
, i = 123 ,
proposition 123 the dual problem to svm123s
is given by the program in proposition 123 with modied
j ( iy ) ( jy ) = ( cid : 123 ) d
j ( y ) ( cid : 123 ) + d ( i , j )
cp123 ( yi , y ) p123 ( y j , y )
in the non - separable case with margin re - scaling , the loss function is introduced in the linear part of the objective function proposition 123 the dual problems to svm123m svm123 with the linear part of the objective replaced by
are given by the dual problems to svm123 and
tsochantaridis , joachims , hofmann and altun
the algorithm we propose aims at nding a small set of constraints from the full - sized optimization problem that ensures a sufciently accurate solution .
more precisely , we will construct a nested sequence of successively tighter relaxations of the original problem using a cutting plane method ( kelley , 123 ) , implemented as a variable selection approach in the dual formulation .
similar to its use with the ellipsoid method ( grotschel et al . , 123; karmarkar , 123 ) , we merely require a separation oracle that delivers a constraint that is violated by the current solution .
we will later show that this is a valid strategy , since there always exists a polynomially sized subset of constraints so that the solution of the relaxed problem dened by this subset fullls all constraints from the full optimization problem up to a precision of e .
this means , the remainingpotentially exponentially manyconstraints are guaranteed to be violated by no more than e , without the need for explicitly adding these constraints to the optimization problem .
we will base the optimization on the dual program formulation which has two important advan - tages over the primal qp .
first , it only depends on inner products in the joint feature space dened , hence allowing the use of kernel functions .
second , the constraint matrix of the dual program supports a natural problem decomposition .
more specically , notice that the constraint matrix de - rived for the svm123 and the svm123 variants is diagonal , since the non - negativity constraints involve only a single a - variable at a time , whereas in the svm123 case , dual variables are coupled , but the couplings only occur within a block of variables associated with the same training instance .
hence , the constraint matrix is ( at least ) block diagonal in all cases , where each block corresponds to a specic training instance .
pseudo - code of the algorithm is depicted in algorithm 123
the algorithm maintains working sets si for each training instance to keep track of the selected constraints which dene the current relaxation .
iterating through the training examples ( xi , yi ) , the algorithm proceeds by nding the ( potentially ) most violated constraint for xi , involving some output value y .
if the ( appropriately scaled ) margin violation of this constraint exceeds the current value of x i by more than e , the dual variable corresponding to y is added to the working set .
this variable selection process in the dual program corresponds to a successive strengthening of the primal problem by a cutting plane that cuts off the current primal solution from the feasible set .
the chosen cutting plane corresponds to the constraint that determines the lowest feasible value for x i .
once a constraint has been added , the solution is re - computed with respect to s .
alternatively , we have also devised a scheme where the optimization is restricted to si only , and where optimization over the full s is performed much less frequently .
this can be benecial due to the block diagonal structure of the constraint matrix , which implies that variables a ( jy ) with j 123= i , y s j can simply be frozen at their current values .
notice that all variables not included in their respective working set are implicitly treated as 123
the algorithm stops , if no constraint is violated by more than e .
with respect to the optimization in step 123 , we would like to point out that in some applications the constraint selection in step 123 may be more expensive than solving the relaxed qp .
hence it may be advantageous to solve the full relaxed qp in every iteration , instead of just optimizing over a subspace of the dual variables .
the presented algorithm is implemented in the software package svmstruct , available on the web at http : / / svmlight . joachims . org .
note that the svm optimization problems from iteration to iteration differ only by a single constraint .
we therefore restart the svm optimizer from the current solution , which greatly reduces the runtime .
large margin methods for structured and interdependent output variables
algorithm 123 algorithm for solving svm123 and the loss re - scaling formulations svm123 and svm123 .
123 : input : ( x123 , y123 ) , .
, ( xn , yn ) , c , e 123 : si / 123 for all i = 123 ,
for i = 123 , .
, n do
/ * prepare cost function for optimization * / set up cost function
j ( cid : 123 ) y123s j where w ( cid : 123 ) / * nd cutting plane * / compute y = argmaxyy h ( y ) / * determine value of current slack variable * / if h ( y ) > x
i = max ( 123 , maxysi h ( y ) ) i + e
/ * add constraint to the working set * / si si ( y ) / * variant ( a ) : perform full optimization * / a s optimize the dual of svm123 , svm123 or svm123 over s , s = isi .
a si optimize the dual of svm123 , svm123 or svm123 over si
/ * variant ( b ) : perform subspace ascent * /
123 : until no si has changed during iteration
a convenient property of both variants of the cutting plane algorithm is that they have a very general and well - dened interface independent of the choice of y and 123
to apply the algorithm , it is sufcient to implement the feature mapping y ( x , y ) ( either explicitly or via a joint kernel function ) , the loss function 123 ( yi , y ) , as well as the maximization in step 123
all of those , in particular the constraint / cut selection method , are treated as black boxes .
while the modeling of y and 123 ( yi , y ) is typically straightforward , solving the maximization problem for constraint selection typically requires exploiting the structure of y for output spaces that can not be dealt with by
tsochantaridis , joachims , hofmann and altun
in the slack re - scaling setting , it turns out that for a given example ( xi , yi ) we need to identify
the maximum over
we will discuss several cases for how to solve this problem in section 123
typically , it can be solved by an appropriate modication of the prediction problem in equation ( 123 ) , which recovers f from f .
for example , in the case of grammar learning with the f123 score as the loss function via 123 ( yi , y ) = ( 123 f123 ( yi , y ) ) , the maximum can be computed using a modied version of the cky algorithm .
more generally , in cases where 123 ( yi , ) only takes on a nite number of values , a generic strategy is a two stage approach , where one rst computes the maximum over those y for which the loss is constant , 123 ( yi , y ) = const , and then maximizes over the nite number of levels .
in the margin re - scaling setting , one needs to solve the maximization problem
in cases where the loss function has an additive decomposition that is compatible with the feature map , one can fold the loss function contribution into the weight vector hw123 , d 123 ( yi , y ) for some w123
this means the class of cost functions dened by f ( x , ;w ) and f ( x , ;w ) 123 ( y , ) may actually be identical .
the algorithm for the zero - one loss is a special case of either algorithm .
we need to identify the highest scoring y that is incorrect ,
i ( y ) i = hw , d
it is therefore sufcient to identify the best solution y = argmaxyy hw , y ( xi , y ) i as well as the second best solution y = argmaxyy \yhw , y ( xi , y ) i .
the second best solution is necessary to detect margin violations in cases where y = yi , but hw , d i ( y ) i < 123
this means that for all problems where we can solve the inference problem in equation ( 123 ) for the top two y , we can also apply our learning algorithms with the zero - one loss .
in the case of grammar learning , for example , we can use any existing parser that returns the two highest scoring parse trees .
we will now proceed by analyzing the presented family of algorithms .
in particular , we will show correctness and sparse approximation properties , as well as bounds on the runtime complexity .
123 correctness and complexity of the algorithm
what we would like to accomplish rst is to obtain a lower bound on the achievable improvement of the dual objective by selecting a single variable a ( iy ) and adding it to the dual problem ( cf .
step 123 in algorithm 123 ) .
while this is relatively straightforward when using quadratic penalties , the svm123 formulation introduces an additional complication in the form of upper bounds on non - overlapping subsets of variables , namely the set of variables a ( iy ) in the current working set that correspond to the same training instance .
hence , we may not be able to answer the above question by optimizing ( iy ) alone , but rather have to deal with a larger optimization problem over a whole subspace .
in order to derive useful bounds , it sufces to restrict attention to simple one - dimensional families of solutions that are dened by improving an existing solution along a specic direction h
large margin methods for structured and interdependent output variables
that one can make sufcient progress along a specic direction , clearly implies that one can make at least that much progress by optimizing over a larger subspace that includes the direction h .
a rst step towards executing this idea is the following lemma .
lemma 123 let j be a symmetric , positive semi - denite matrix , and dene a concave objective in a
q ( a ) =
123ja +hh , a i ,
which we assume to be bounded from above .
assume that a solution a o and an optimization direc - starting from a o along the chosen direction h will increase the objective by
( a o ) , h i > 123
then optimizing q
is given such that h ( cid : 123 )
b >123 ( q ( a o + b
) ) q ( a o ) =
( a o ) , h i123
is given by
proof the difference obtained by a particular b ( b ) b ( cid : 123 ) h ( cid : 123 )
123jh ( cid : 123 ) , as can be veried by elementary algebra .
solving for b one arrives at ( a o ) , h i
q = 123 b = h ( cid : 123 )
( a o ) , h i
notice that this requires h 123 for any h .
moreover h
123jh > 123
obviously , the positive semi - deniteness of j guarantees h ( a o ) , h i > 123 would imply that limb 123jh = 123 together with h ( cid : 123 )
q ( a o + is bounded .
plugging the value for
, which is in contradiction with the assumption that q
b back into the above expression for d
yields the claim .
corollary 123 under the same assumption as in lemma 123 and for the special case of an optimiza - tion direction h = er , the objective improves by
( b ) =
proof notice that h = er implies h ( cid : 123 )
, h i =
123jh = jrr .
corollary 123 under the same assumptions as in lemma 123 and enforcing the constraint b d for some d > 123 , the objective improves by
123<b d ( q ( a o + b
) ) q ( a o ) =
( a o ) , h i d123
( a o ) , h i dh
tsochantaridis , joachims , hofmann and altun
moreover , the improvement can be upper bounded by
123<b d ( q ( a o + b
) ) q ( a o )
( a o ) , h i
( a o ) , h i .
proof we distinguish two cases of either b d or b > d .
in the rst case , we can simple apply lemma 123 since the additional constraint is inactive and does not change the solution .
in the second case , the concavity of q over the constrained range .
plugging in this result for b into d
implies that b = d achieves the maximum of d
yields the second case in the claim .
finally , the bound is obtained by exploiting that in the second case
b > d d < h ( cid : 123 )
( a o ) , h i
replacing one of the d factors in the d123 term of the second case with this bound yields an upper bound .
the rst ( exact ) case and the bound in the second case can be compactly combined as shown in the formula of the claim .
corollary 123 under the same assumption as in corollary 123 and for the special case of a single - coordinate optimization direction h = er , the objective improves at least by
q ( a o + b er ) q ( a o )
123jh = jrr .
proof notice that h = er implies h ( cid : 123 )
, h i =
we now apply the above lemma and corollaries to the four different svm formulations , starting
with the somewhat simpler squared penalty case .
proposition 123 ( svm123s jective is lower bounded by
) for svm123s
step 123 in algorithm 123 the improvement d
of the dual ob -
, where 123i max
y ( 123 ( yi , y ) ) and ri max
proof using the notation in algorithm 123 one can apply corollary 123 with multi - index r = ( iy ) , h = 123 , and j such that
notice that the partial derivative of q with respect to a
j ( iy ) ( jy ) = hd
( a o ) = 123 ( cid : 123 )
( jy ) j ( iy ) ( jy ) = 123hw , d
d ( i , j ) n
( iy ) is given by
large margin methods for structured and interdependent output variables
since the optimality equations for the primal variables yield the identities
w = ( cid : 123 )
x i = ( cid : 123 )
now , applying the condition of step 123 , namelyp123 ( yi , y ) ( 123hw , d
i ( y ) i ) > x i + e , leads to the
finally , jrr = kd
expression from corollary 123 yields
i ( y ) k123 + n c123 ( yi , y ) and inserting this expression and the previous bound into the
i ( y ) k123 + n
the claim follows by observing that jointly optimizing over a set of variables that include a only further increase the value of the dual objective .
proposition 123 ( svm123m objective is lower bounded by
) for svm123m
step 123 in algorithm 123 the improvement d
of the dual
proof by re - dening d
, where ri = max
we are back to proposition 123 with
i ( y ) k123 ) = max
i ( y ) k123 ) = r123
i ( y ) i p123 ( yi , y ) x
proposition 123 ( svm123s jective is lower bounded by
) for svm123s
step 123 in algorithm 123 the improvement d
of the dual ob -
i ( cid : 123 ) where 123i = max
y ( 123 ( yi , y ) ) and ri = max
tsochantaridis , joachims , hofmann and altun
if the working set does not contain an element ( iy ) , then we can optimize over a constraint that a
notice that
( iy ) 123 ( yi , y ) c
( iy ) under the
( a o ) = 123hw , d
x i + e
where the rst inequality follows from the pre - condition for selecting ( iy ) and the last one from x i 123
moreover , notice that j ( iy ) ( iy ) r123 i .
evoking corollary 123 with the obvious identications
i 123 ( yi , y ) 123 ( cid : 123 )
the second term can be further bounded to yield the claim .
if there are already active constraints for instance xi in the current working set , i . e .
si 123= / 123 , then we may need to reduce dual variables a ( iy ) in order to get some slack for increasing the newly added ( iy ) .
we thus investigate search directions h c 123 for y si , ( jy123 ) = 123 in all other cases .
for such h in nding a suitable direction to derive a good bound , we have two ( possibly conicting ) goals .
first of all , we want the directional derivative to be positively bounded away from zero .
notice that
h 123 since b c
, we guarantee that a o + b
such that h
( iy ) = 123 , h
( a o ) , h i = ( cid : 123 ) furthermore , by the restrictions imposed on h active and hence 123 ( yi , y ) ( 123hw , d that 123 ( yi , y ) ( 123hw , d
( iy ) < 123 implies that the respective constraint is i ( y ) i ) = x i .
moreover the pre - condition of step 123 ensures
i ( y ) i ) = x i + d where d e > 123
hence 123 ( yi , y ) 123
( a o ) , h i =
the second goal is to make sure the curvature along the chosen direction is not too large .
123jh = j ( iy ) ( iy ) 123 ( cid : 123 )
j ( iy ) ( iy ) + ( cid : 123 ) c123 ( yi , y ) 123 ( cid : 123 ) 123 ( yi , y ) 123 .
large margin methods for structured and interdependent output variables
this follows from the fact that ( cid : 123 ) y123=y a o
( iy ) 123i ( cid : 123 ) y123=y ( a o ) , h i
evoking corollary 123 yields
( a o ) , h i
proposition 123 ( svm123m objective is lower bounded by
) for svm123m
step 123 in algorithm 123 the improvement d
of the dual
, where ri = max
proof by re - dening d
123 ( yi , y ) we are back to proposition 123 with i ( y ) k123 ) = r123
i ( y ) k123 ) = max
i ( y ) i 123 ( yi , y ) x
this leads to the following polynomial bound on the maximum size of s .
theorem 123 with r = maxi ri , 123 = maxi123i and for a given e > 123 , algorithm 123 terminates after
incrementally adding at most e 123 ( cid : 123 ) , max ( cid : 123 ) 123n 123e 123c 123 r123
123c 123 r123 e 123 ( cid : 123 ) ,
c 123 r123 + n 123
c 123 r123 + n 123
constraints to the working set s for the svm123s proof with s = / 123 the optimal value of the dual is 123
in each iteration a constraint is added that is violated by at least e , provided such a constraint exists .
after solving the s - relaxed qp in step 123 , the objective will increase by at least the amounts suggested by propositions 123 , 123 , 123 and 123 respectively .
hence after t constraints , the dual objective will be at least t times these increments .
the result follows from the fact that the dual objective is upper bounded by the minimum of the primal , which in turn can be bounded by c 123 and 123
123c 123 for svm123 and svm123 respectively .
123 and svm123m
tsochantaridis , joachims , hofmann and altun
note that the number of constraints in s does not depend on |y | .
this is crucial , since |y | is exponential or innite for many interesting problems .
for problems where step 123 can be computed in polynomial time , the overall algorithm has a runtime polynomial in n , r , 123 , 123 / e , since at least one constraint will be added while cycling through all n instances and since step 123 is polynomial .
this shows that the algorithm considers only a small number of constraints , if one allows an extra e slack , and that the solution is correct up to an approximation that depends on the precision parameter e .
the upper bound on the number of active constraints in such an approximate solution depends on the chosen representation , more specically , we need to upper bound the difference vectors ( xi , y ) k123 for arbitrary y , y y .
in the following , we will thus make sure that suitable
upper bounds are available .
specic problems and special cases
in the sequel , we will discuss a number of interesting special cases of the general scenario outlined in the previous section .
to model each particular problem and to be able to run the algorithm and bound its complexity , we need to examine the following three questions for each case :
modeling : how can we dene suitable feature maps y ( x , y ) for specic problems ? algorithms : how can we compute the required maximization over y for given x ? sparseness : how can we bound ky
123 multiclass classication
a special case of equation ( 123 ) is winner - takes - all ( wta ) multiclass classication , where y = ( y123 , .
, yk ) and w = ( v123 , .
, v123k ) 123 is a stack of vectors , vk being a weight vector associated with the k - th class yk .
the wta rule is given by
f ( x ) = arg max
f ( x , yk;w ) = hvk , f ( x ) i .
here f ( x ) rd denotes an arbitrary feature representation of the inputs , which in many cases may
be dened implicitly via a kernel function .
the above decision rule can be equivalently represented by making use of a joint feature map as follows .
first of all , we dene the canonical ( binary ) representation of labels y y by unit vectors
c ( y ) ( d ( y123 , y ) , d ( y123 , y ) , .
, d ( yk , y ) ) 123 ( 123 , 123 ) k ,
so that hl c ( y123 ) i = d ( y , y123 ) .
it will turn out to be convenient to use direct tensor products to combine feature maps over x and y .
in general , we thus dene the - operation in the following
: rd rk rdk ,
( a b ) i+ ( j123 ) d ai b j .
now we can dene a joint feature map for the multiclass problem by
( x , y ) f ( x ) l
large margin methods for structured and interdependent output variables
it is is easy to show that this results in an equivalent formulation of the multiclass wta as expressed in the following proposition .
proposition 123 f ( x , y;w ) = hw , y
( x , y ) i , where f is dened in equation ( 123 ) and y
proof for all yk y : hw , y
( x , yk ) i = ( cid : 123 ) dk
r ( x , yk ) = ( cid : 123 ) k
j=123 ( cid : 123 ) d
d=123 v jdf d ( x ) d ( j , k ) = ( cid : 123 ) d
d=123 vkdf d ( x ) =
it is usually assumed that the number of classes k in simple multiclass problems is small enough , so that an exhaustive search can be performed to maximize any objective over y .
similarly , we can nd the second best y y .
in order to bound the norm of the difference feature vectors , we prove the following simple result .
proposition 123 dene ri kf ( xi ) k .
then ky
( xi , y123 ) k123 = 123kf ( xi ) k123 ,
where the rst step follows from the cauchy - schwarz inequality and the second step exploits the sparseness of l
123 multiclass classication with output features the rst generalization we propose is to make use of more interesting output features l canonical representation in equation ( 123 ) .
apparently , we could use the same approach as in equa - tion ( 123 ) to dene a joint feature function , but use a more general form for l
we rst show that for any joint feature map y following relation holds :
constructed via the direct tensor product the
proposition 123 for y = f l
the inner product can be written as
( x123 , y123 ) ( cid : 123 ) = ( cid : 123 ) f ( x ) , f ( x123 ) ( cid : 123 ) ( cid : 123 ) l ( y ) , l ( y123 ) ( cid : 123 ) .
tsochantaridis , joachims , hofmann and altun
proof by simple algebra
f d ( x ) f d123 ( x123 )
f d ( x ) l k ( y ) f d123 ( x123 ) l k123 ( y123 ) l k ( y ) l k123 ( y123 ) = ( cid : 123 ) f ( x ) , f ( x123 ) ( cid : 123 ) ( cid : 123 ) l ( y ) , l ( y123 ) ( cid : 123 ) .
this implies that for feature maps f
hf ( x ) , f ( x123 ) i , one can dene a joint kernel function as follows :
that are implicitly dened via kernel functions k , k ( x , x123 )
j ( ( x , y ) , ( x123 , y123 ) ) = ( cid : 123 ) y
( x123 , y123 ) ( cid : 123 ) = ( cid : 123 ) l ( y ) , l ( y123 ) ( cid : 123 ) k ( x , x123 ) .
of course , nothing prevents us from expressing the inner product in output space via yet another
kernel function l ( y , y123 ) = hl ( y ) , l ( y123 ) i .
notice that the kernel l is simply the identity in the stan -
dard multiclass case .
how can this kernel be chosen in concrete cases ? it basically may encode any type of prior knowledge one might have about the similarity between classes .
it is illuminating to note the following proposition .
proposition 123 dene y f ( x , y;w ) can be written as
( x , y ) = f ( x ) l ( y ) with l ( y ) rr; then the discriminant function
l r ( y ) hvr , f ( x ) i ,
where w = ( v123 , .
, v123r ) 123 is the stack of vectors vr rd , one vector for each basis function of l
vrdf d ( x ) =
wd ( d123 ) +rl r ( y ) f d ( x ) = hw , f ( x ) l ( y ) i ( x , y ) i = f ( x , y;w ) .
we can give this a simple interpretation : for each output feature l r a corresponding weight vector vr is introduced .
the discriminant function can then be represented as a weighted sum of contributions coming from the different features .
in particular , in the case of binary features l : y ( 123 , 123 ) r , this will simply be a sum over all contributions hvr , f ( x ) i of features that are active for the class y , i . e .
for which l r ( y ) = 123
it is also important to note that the orthogonal representation provides a maximally large hypoth - esis class and that nothing can be gained in terms of representational power by including additional
large margin methods for structured and interdependent output variables
corollary 123 assume a mapping l ( y ) = ( l ( y ) 123 , l l ( y ) and y and vice versa .
( x , y ) = f ( x ) l
now , for every w there is w such that ( cid : 123 ) w , y
c ( y ) 123 ) 123 , l ( y ) rr and dene y
( x , y ) = f ( x )
( x , y ) ( cid : 123 ) = hw , y
proof applying proposition 123 twice it follows that
( cid : 123 ) w , y
l r ( y ) hvr , f ( x ) i =*r+k ( cid : 123 )
l r ( y ) vr , f ( x ) + = hvy , f ( x ) i = hw , y
where we have dened vy = ( cid : 123 ) r+k vr = 123 for r = 123 ,
l r ( y ) vr .
the reverse direction is trivial and requires setting
in the light of this corollary , we would like to emphasize that the rationale behind the use of class features is not to increase the representational power of the hypothesis space , but to re - parameterize ( or even constrain ) the hypothesis space such that a more suitable representation for y is produced .
we would like to generalize across classes as we want to generalize across input patterns in the stan - dard formulation of classication problems .
obviously , orthogonal representations ( corresponding to diagonal kernels ) will provide no generalization whatsoever across different classes y .
the choice of a good output feature map l is thus expected to provide an inductive bias , namely that learning can occur across a set of classes sharing a common property .
let us discuss some special cases of interest .
classication with taxonomies assume that class labels y are arranged in a taxonomy .
we will dene a taxonomy as a set of elements z y equipped with a partial order .
the partially ordered set ( z , ) might , for example , represent a tree or a lattice .
now we can dene binary features for classes as follows : associate one feature l z with every element in z according to
l z ( y ) = ( 123 if y z or y = z
this includes multiclass classication as a special case of an unordered set z = y .
in general , however , the features l z will be shared by all classes below z , e . g .
all nodes y in the subtree rooted at z in the case of a tree .
one may also introduce a relative weight b z for every feature and dene a b - weighted ( instead of binary ) output feature map l as l z = b zl z .
if we reect upon the implication of this denition in the light of proposition 123 , one observes that this effectively introduces a weight vector vz for every element of z , i . e .
for every node in the hierarchy .
learning with textual class descriptions as a second motivating example , we consider prob - lems where classes are characterized by short glosses , blurbs or other textual descriptions .
we would like to exploit the fact that classes sharing some descriptors are likely to be similar , in order to specify a suitable inductive bias .
this can be achieved , for example , by associating a feature l with every keyword used to describe classes , in addition to the class identity .
hence standard vector space models like term - frequency of idf representations can be applied to model classes and the
inner product hl ( y ) , l ( y123 ) i then denes a similarity measure between classes corresponding to the
standard cosine - measure used in information retrieval .
tsochantaridis , joachims , hofmann and altun
learning with class similarities the above example can obviously be generalized to any situa - tion , where we have access to a positive denite similarity function for pairs of classes .
to come up with suitable similarity functions is part of the domain modelvery much like determining a good representation of the inputsand we assume here that it is given .
as in the multiclass case , we assume that the number of classes is small enough to perform an
proposition 123 can be generalized in the following way :
proposition 123 dene ri kf ( xi ) k and s maxyy kl ( y ) k then ky for all y , y123 y .
( xi , y ) i = kf ( xi ) k123 kl ( y ) k123 r123
in the last step , we have used proposi -
123 label sequence learning
the next problem we would like to formulate in the joint feature map framework is the problem of label sequence learning , or sequence segmentation / annotation .
here , the goal is to predict a label sequence y = ( y123 , .
, yt ) for a given observation sequence x = ( x123 , .
in order to simplify the presentation , let us assume all sequences are of the same length t .
let us denote by s set of possible labels for each individual variable yt , i . e .
y = s t .
hence each sequence of labels is considered to be a class of its own , resulting in a multiclass classication problem with |s classes .
to model label sequence learning in this manner would of course not be very useful , if one were to apply standard multiclass classication methods .
however , this can be overcome by an appropriate denition of the discriminant function .
inspired by hidden markov model ( hmm ) type of interactions , we propose to dene y interactions between input features and labels via multiple copies of the input features as well as features that model interactions between nearby label variables .
it is perhaps most intuitive to start from the discriminant function
s s h ws , f ( xt ) id ( yt , s ) + h
c denotes the orthogonal representation of labels over s
c ( yt ) + + h * w ,
f ( xt ) l
d ( yt , s ) d ( yt+123 , s ) , and h 123 is a scaling
here w = ( w123 , w123 ) 123 , l factor which balances the two types of contributions .
it is straightforward to read off the joint feature
large margin methods for structured and interdependent output variables
map implicit in the denition of the hmm discriminant from equation ( 123 ) ,
( x , y ) = ( cid : 123 ) ( cid : 123 ) t
f ( xt ) l
notice that similar to the multiclass case , we can apply proposition 123 in the case of an implicit representation of f via a kernel function k and the inner product between labeled sequences can thus be written as
( x , y ) i =
d ( yt , ys ) k ( xt , xs ) + h 123
d ( yt , ys ) d ( yt+123 , ys+123 ) .
a larger family of discriminant functions can be obtained by using more powerful feature functions .
we would like to mention three ways of extending the previous hmm discriminant .
first of all , one can extract features not just from xt , but from a window around xt , e . g .
replacing f ( xt ) with f ( xtr , .
, xt+r ) .
since the same input pattern xt now occurs in multiple terms , this has been called the use of overlapping features ( lafferty et al . , 123 ) in the context of label sequence learning .
secondly , it is also straightforward to include higher order label - label interactions beyond pairwise interactions by including higher order tensor terms , for instance , label triplets ( cid : 123 ) c ( yt+123 ) , etc .
thirdly , one can also combine higher order y features with input features , for example , by including terms of the type ( cid : 123 )
t f ( xt ) l
the maximization of hw , y ( xi , y ) i over y can be carried out by dynamic programming , since the cost contributions are additive over sites and contain only linear and nearest neighbor quadratic contributions .
in particular , in order to nd the best label sequence y 123= yi , one can perform viterbi decoding ( forney jr . , 123; schwarz and chow , 123 ) , which can also determine the second best sequence for the zero - one loss ( 123 - best viterbi decoding ) .
viterbi decoding can also be used with other loss functions by computing the maximization for all possible values of the loss function .
( xi , y ) k123 = k ( cid : 123 ) squared norm can be upper bounded by
proposition 123 dene ri maxt kf ( xt proof notice that ky
c ( yt ) k123 = ( cid : 123 )
i ) k; then ky t f ( xt c ( yt ) k123 + h 123k ( cid : 123 ) t ( cid : 123 ) f ( xs
i ) , f ( xt
and the second one by h 123t 123 , which yields the claim .
( xi , y123 ) k123 123t 123 ( r123
i + h 123 ) .
c ( yt+123 ) k123
the rst
i ) ( cid : 123 ) d ( ys , yt ) t 123r123
123 sequence alignment
next we show how to apply the proposed algorithm to the problem of learning to align sequences .
for a given pair of sequences
x s , where s is the set of all strings over some nite alphabet s
tsochantaridis , joachims , hofmann and altun
x s and y s , alignment methods like the smith - waterman algorithm select the sequence of operations ( e . g .
insertion , substitution ) that transforms x into y and that maximizes a linear objective
a ( x , y ) = argmax
that is parameterized by the operation scores w .
y ( x , y , a ) is the histogram of alignment operations .
the value of hw , y ( x , y , a ( x , y ) ) i can be used as a measure of similarity between x and y .
it is the score of the highest scoring sequence of operations that transforms x into y .
such alignment models are used , for example , to measure the similarity of two protein sequences .
in order to learn the score vector w we use training data of the following type .
for each native sequence xi there is a most similar homologous sequence yi along with the optimal alignment ai .
in addition we are given a set of decoy sequences yt i , t = 123 , .
, k with unknown alignments .
note that this data is more restrictive than what ristad and yianilos ( 123 ) consider in their generative modeling approach .
the goal is to learn a discriminant function f that recognizes the homologous sequence among the decoys .
in our approach , this corresponds to nding a weight vector w so that homologous sequences align to their native sequence with high score , and that the alignment scores for the decoy sequences are lower .
with yi = ( yi , y123 i ) as the output space for the i - th example , we seek a w so that hw , y i , a ) i for all t and a .
this implies a zero - one loss and hypotheses of the form
( xi , yi , ai ) i exceeds hw , y
i , . . . , yk
f ( xi ) = argmax
depends on the set of operations used in the sequence alignment
the design of the feature map y
in order to nd the optimal alignment between a given native sequence x and a homologous / decoy sequence y as the solution of
we can use dynamic programming as e . g .
in the smith - waterman algorithm .
to solve the argmax in equation ( 123 ) , we assume that the number k of decoy sequences is small enough , so that we can select among the scores computed in equation ( 123 ) via exhaustive search .
if we select insertion , deletion , and substitution as our possible operations , each ( non - redundant ) operation reads at least one character in either x or y .
if the maximum sequence length is n , then ( x , y123 , a123 ) is at most 123n .
the l123 - norm of y
( x , y , a ) is at most 123n and the l123 - norm of y
large margin methods for structured and interdependent output variables
123 weighted context - free grammars
in natural language parsing , the task is to predict a labeled tree y based on a string x = ( x123 , . . . , xk ) of terminal symbols .
for this problem , our approach extends the approaches of collins ( 123 ) and collins and duffy ( 123b ) to an efcient maximum - margin algorithm with general loss functions .
we assume that each node in the tree corresponds to the application of a context - free grammar rule .
the leaves of the tree are the symbols in x , while interior nodes correspond to non - terminal symbols from a given alphabet n .
for simplicity , we assume that the trees are in chomsky normal form .
this means that each internal node has exactly two children .
an exception are pre - terminal nodes ( non - leaf nodes that have a terminal symbol as child ) which have exactly one child .
we consider weighted context - free grammars to model the dependency between x and y .
grammar rules are of the form nl ( ci c j , ck ) or nl ( ci xt ) , where ci , c j , ck n are non - terminal symbols , and xt t is a terminal symbol .
each such rule is parameterized by an individual weight wl .
a particular kind of weighted context - free grammar are probabilistic context - free grammars ( pcfgs ) , where this weight wl is the log - probability of expanding node hi with rule nl .
in pcfgs , the indi - vidual node probabilities are assumed to be independent , so that the probability p ( x , y ) of sequence x and tree y is the product of the node probabilities in the tree .
the most likely parse tree to yield x from a designated start symbol is the predicted label h ( x ) .
this leads to the following maximization problem , where we use rules ( y ) to denote the multi - set of nodes in y ,
h ( x ) = argmax
p ( y|x ) = argmax
yy ( ( cid : 123 )
more generally , weighted context - free grammars can be used in our framework as follows .
y contains one feature fi jk for each node of type ni jk ( ci c j , ck ) and one feature fit for each node of type nit ( ci xt ) .
as illustrated in figure 123 , the number of times a particular rule occurs in the tree is the value of the feature .
the weight vector w contains the corresponding weights so that
( x , y ) i = ( cid : 123 ) nlrules ( y ) wl .
note that our framework also allows more complex y
( x , y ) , making it more exible than in particular , each node weight can be a ( kernelized ) linear function of the full x and
the span of the subtree .
the solution of argmaxyy hw , y ( x , y ) i for a given x can be determined efciently using a cky - parser ( see manning and schuetze , 123 ) , which can also return the second best parse for learn - ing with the zero - one loss .
to implement other loss functions , like 123 ( yi , y ) = ( 123 f123 ( yi , y ) ) , the cky algorithm can be extended to compute both argmaxyy ( 123hw , d i ( y ) i ) 123 ( yi , y ) as well as i ( y ) i ) by stratifying the maximization over all values of 123 ( yi , y ) as
described in joachims ( 123 ) for the case of multivariate classication .
tsochantaridis , joachims , hofmann and altun
123 training instances per class 123 training instances per class
t 123 tax 123 123 +123 %
123 +123 %
table 123 : results on the wipo - alpha corpus , section d with 123 groups using 123 - fold and 123 - fold cross validation , respectively .
t is a standard ( at ) svm multiclass model , tax the hierarchical architecture .
123 / 123 denotes training based on the classication loss , 123 refers to training based on the tree loss .
since the trees branch for each internal node , a tree over a sequence x of length n has n 123 internal nodes .
furthermore , it has n pre - terminal nodes .
this means that the l123 - norm of y ( x , y ) is 123n 123 and that the l123 - norm of y
( x , y123 ) is at mostp123n123 + 123 ( n 123 ) 123 < 123n .
experimental results
to demonstrate the effectiveness and versatility of our approach , we applied it to the problems of taxonomic text classication ( see also cai and hofmann , 123 ) , named entity recognition , sequence alignment , and natural language parsing .
123 classication with taxonomies
we have performed experiments using a document collection released by the world intellectual property organization ( wipo ) , which uses the international patent classication ( ipc ) scheme .
we have restricted ourselves to one of the 123 sections , namely section d , consisting of 123 , 123 documents in the wipo - alpha collection .
for our experiments , we have indexed the title and claim tags .
we have furthermore sub - sampled the training data to investigate the effect of the training set size .
document parsing , tokenization and term normalization have been performed with the mindserver retrieval engine . 123 as a suitable loss function 123 , we have used a tree loss function which denes the loss between two classes y and y123 as the height of the rst common ancestor of y and y123 in the taxonomy .
the results are summarized in table 123 and show that the proposed hierarchical svm learning architecture improves performance over the standard multiclass svm in terms of classication accuracy as well as in terms of the tree loss .
123 label sequence learning
we study our algorithm for label sequence learning on a named entity recognition ( ner ) problem .
more specically , we consider a sub - corpus consisting of 123 sentences from the spanish news wire article corpus which was provided for the special session of conll123 devoted to ner .
this software is available at http : / / www . recommind . com .
large margin methods for structured and interdependent output variables
method hmm crf perceptron svm
table 123 : results of various algorithms on the named entity recognition task .
train err test err 123 . 123 123 . 123 123 123 . 123 123 . 123 123 . 123 123 123 . 123 123 . 123 123 . 123 123 123 . 123
table 123 : results for various svm formulations on the named entity recognition task ( e = 123 ,
c = 123 ) .
the label set in this corpus consists of non - name and the beginning and continuation of person names , organizations , locations and miscellaneous names , resulting in a total of |s | = 123 different labels .
in the setup followed in altun et al .
( 123 ) , the joint feature map y ( x , y ) is the histogram of state transition plus a set of features describing the emissions .
an adapted version of the viterbi algorithm is used to solve the argmax in line 123
for both perceptron and svm a second degree polynomial kernel was used .
the results given in table 123 for the zero - one loss , compare the generative hmm with condi - tional random elds ( crf ) ( lafferty et al . , 123 ) , collins perceptron and the svm algorithm .
all discriminative learning methods substantially outperform the standard hmm .
in addition , the svm performs slightly better than the perceptron and crfs , demonstrating the benet of a large margin approach .
table 123 shows that all svm formulations perform comparably , attributed to the fact the vast majority of the support label sequences end up having hamming distance 123 to the correct label sequence .
notice that for 123 - 123 loss functions all three svm formulations are equivalent .
123 sequence alignment
to analyze the behavior of the algorithm for sequence alignment , we constructed a synthetic dataset according to the following sequence and local alignment model .
the native sequence and the decoys
are generated by drawing randomly from a 123 letter alphabet s = ( 123 , . . , 123 ) so that letter c s has probability c / 123
each sequence has length 123 , and there are 123 decoys per native sequence .
to generate the homologous sequence , we generate an alignment string of length 123 consisting of 123 characters match , substitute , insert , delete .
for simplicity of illustration , substitutions are always c ( c mod 123 ) + 123
in the following experiments , matches occur with probability 123 , substitutions with 123 , insertion with 123 , deletion with 123 .
the homologous sequence is created by applying the alignment string to a randomly selected substring of the native .
the shortening of the sequences through insertions and deletions is padded by additional random characters .
we model this problem using local sequence alignment with the smith - waterman algorithm .
table 123 shows the test error rates ( i . e .
the percentage of times a decoy is selected instead of the homologous sequence ) depending on the number of training examples .
the results are averaged over 123 train / test samples .
the model contains 123 parameters in the substitution matrix p for insert / delete .
we train this model using the svm123 and compare against a generative
tsochantaridis , joachims , hofmann and altun
table 123 : error rates and number of constraints |s| depending on the number of training examples
( e = 123 , c = 123 ) .
number of training examples
figure 123 : number of constraints added to s depending on the number of training examples ( middle )
and the value of e
( right ) .
if not stated otherwise , e = 123 , c = 123 , and n = 123
sequence alignment model , where the substitution matrix is computed as p e . g .
durbin et al . , 123 ) using laplace estimates .
for the generative model , we report the results
p ( xi ) p ( z j ) ( cid : 123 ) ( see for d = 123 , which performs best on the test set .
despite this unfair advantage , the svm performs
better for low training set sizes .
for larger training sets , both methods perform similarly , with a small preference for the generative model .
however , an advantage of the svm approach is that it is straightforward to train gap penalties .
i j = log ( cid : 123 ) p ( xi , z j )
figure 123 shows the number of constraints that are added to s before convergence .
the graph on the left - hand side shows the scaling with the number of training examples .
as predicted by theorem 123 , the number of constraints is low .
it appears to grow sub - linearly with the number of examples .
the graph on the right - hand side shows how the number of constraints in the nal s changes with log ( e ) .
the observed scaling appears to be better than suggested by the upper bound in theorem 123
a good value for e is 123 .
we observed that larger values lead to worse prediction accuracy , while smaller values decrease efciency while not providing further benet .
large margin methods for structured and interdependent output variables
acc prec rec
acc prec rec 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123
cpu - h %svm iter const
table 123 : results for learning a weighted context - free grammar on the penn treebank .
123 weighted context - free grammars
we test the feasibility of our approach for learning a weighted context - free grammar ( see figure 123 ) on a subset of the penn treebank wall street journal corpus .
we consider the 123 sentences of length at most 123 from sections f123 - 123 as the training set , and the 123 sentences of length at most 123 from f123 as the test set .
following the setup in johnson ( 123 ) , we start based on the part - of - speech tags and learn a weighted grammar consisting of all rules that occur in the training data .
to solve the argmax in line 123 of the algorithm , we use a modied version of the cky parser of mark johnson . 123 the results are given in table 123
they show micro - averaged precision , recall , and f123 for the training and the test set .
the rst line shows the performance of the generative pcfg model using the maximum likelihood estimate ( mle ) as computed by johnsons implementation .
the second line show the svm123 with zero - one loss , while the following lines give the results for the f123 - loss .
all results are for c = 123 and e = 123 .
all val - 123 ( yi , y ) = ( 123f123 ( yi , y ) ) using svm123s ues of c between 123 to 123 gave comparable prediction performance .
while the zero - one loss which is also implicitly used in perceptrons ( collins and duffy , 123a; collins , 123 ) achieves better accuracy ( i . e .
predicting the complete tree correctly ) , the f123 - score is only marginally better compared to the pcfg model .
however , optimizing the svm for the f123 - loss gives substantially better f123 - scores , outperforming the pcfg substantially .
the difference is signicant according to a mcnemar test on the f123 - scores .
we conjecture that we can achieve further gains by incorporating more complex features into the grammar , which would be impossible or at best awkward to use in a generative pcfg model .
note that our approach can handle arbitrary models ( e . g .
with kernels and overlapping features ) for which the argmax in line 123 can be computed .
experiments with such complex features were independently conducted by taskar et al .
( 123b ) based on the algorithm in taskar et al .
( 123a ) .
while their algorithm cannot optimize f123 - score as the training loss , they report substantial gains from the use of complex features .
123 and svm123m
in terms of training time , table 123 shows that the total number of constraints added to the working set is small .
it is roughly twice the number of training examples in all cases .
while the training is faster for the zero - one loss , the time for solving the qps remains roughly comparable .
the re - scaling formulations lose time mostly on the argmax in line 123 of the algorithm .
this might be sped up , since we were using a rather naive algorithm in the experiments .
we presented a maximum - margin approach to learning functional dependencies for complex output in particular , we considered cases where the prediction is a structured object or where the prediction consists of multiple dependent variables .
the key idea is to model the problem as
this software is available at http : / / www . cog . brown . edu / mj / software . htm .
tsochantaridis , joachims , hofmann and altun
a ( kernelized ) linear discriminant function over a joint feature space of inputs and outputs .
we demonstrated that our approach is very general , covering problems from natural language parsing and label sequence learning to multilabel classication and classication with output features .
while the resulting learning problem can be exponential in size , we presented an algorithm for which we prove polynomial convergence for a large class of problems .
we also evaluated the al - gorithm empirically on a broad range of applications .
the experiments show that the algorithm is feasible in practice and that it produces promising results in comparison to conventional genera - tive models .
a key advantage of the algorithm is the exibility to include different loss functions , making it possible to directly optimize the desired performance criterion .
furthermore , the ability to include kernels opens the opportunity to learn more complex dependencies compared to conven - tional , mostly linear models .
