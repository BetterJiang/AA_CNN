algorithms for learning to rank web docu - ments usually assume a documents relevance is independent of other documents .
this leads to learned ranking functions that pro - duce rankings with redundant results .
contrast , user studies have shown that di - versity at high ranks is often preferred .
we present two online learning algorithms that directly learn a diverse ranking of documents based on users clicking behavior .
we show that these algorithms minimize abandon - ment , or alternatively , maximize the proba - bility that a relevant document is found in the top k positions of a ranking .
moreover , one of our algorithms asymptotically achieves optimal worst - case performance even if users
web search has become an essential component of the internet infrastructure , and has hence attracted sig - nicant interest from the machine learning community ( e . g .
herbrich et al . , 123; burges et al . , 123; radlin - ski & joachims , 123; chu & ghahramani , 123; met - zler & croft , 123; yue et al . , 123; taylor et al . , 123 ) .
the conventional approach to this learning - to - rank problem has been to assume the availability of manually labeled training data .
usually , this data consists of a set of documents judged as relevant or not to specic queries , or of pairwise judgments compar - ing the relative relevance of pairs of documents .
these judgments are used to optimize a ranking function o - line , to a standard information retrieval metric , then deploying the learned function in a live search engine .
we propose a new learning to rank problem formu - lation that diers in three fundamental ways .
first , unlike most previous methods , we learn from usage
appearing in proceedings of the 123 th international confer - ence on machine learning , helsinki , finland , 123
copy - right 123 by the author ( s ) / owner ( s ) .
data rather than manually labeled relevance judg - ments .
usage data is available in much larger quan - tities and at much lower cost .
moreover , unlike man - ual judgments , which need to be constantly updated to stay relevant , usage data naturally reects current users needs and the documents currently available .
although some researchers have transformed usage data into relevance judgments , or used it to generate features ( e . g .
joachims , 123; radlinski & joachims , 123; agichtein et al . , 123 ) , we go one step further by directly optimizing a usage - based metric .
second , we propose an online learning approach for learning from usage data .
as training data is being collected , it immediately impacts the rankings shown .
this means the learning problem we address is regret minimization , where the goal is to minimize the total number of poor rankings displayed over all time .
in particular , in this setting there is a natural tradeo be - tween exploration and exploitation : it may be valuable in the long run to present some rankings with unknown documents , to allow training data about these docu - ments to be collected .
in contrast , in the short run exploitation is typically optimal .
with only few ex - ceptions ( e . g .
radlinski & joachims , 123 ) , previous work does not consider such an online approach .
third and most importantly , except for ( chen & karger , 123 ) , previous algorithms for learning to rank have considered the relevance of each document in - dependently of other documents .
this is reected in the performance measures typically optimized , such as precision , recall , mean average precision ( map ) ( baeza - yates & ribeiro - neto , 123 ) and normalized discounted cumulative gain ( ndcg ) ( burges et al . , 123 ) .
in fact , recent work has shown that these mea - sures do not necessarily correlate with user satisfaction ( turpin & scholer , 123 ) .
additionally , it intuitively stands to reason that presenting many slight varia - tions of the same relevant document in web search re - sults may increase the map or ndcg score , yet would be suboptimal for users .
moreover , web queries often have dierent meanings for dierent users ( a canonical example is the query jaguar ) suggesting that a ranking with diverse documents may be preferable .
learning diverse rankings with multi - armed bandits
we will show how clickthrough data can be used to learn rankings maximizing the probability that any new user will nd at least one relevant document high in the ranking .
related work
the standard approach for learning to rank uses train - ing data , in the form of judgments assessing the rele - vance of individual documents to a query , to learn pa - rameters for a scoring function f ( q , di , ) .
given a new query q , this function computes f ( q , di , ) for each document di independently and ranks documents by decreasing score ( e . g .
herbrich et al . , 123; joachims , 123; burges et al . , 123; chu & ghahramani , 123 ) .
this also applies to recent algorithms that learn to maximize nonlinear performance measures such as map ( metzler & croft , 123; yue et al . , 123 ) and ndcg ( burges et al . , 123; taylor et al . , 123 ) .
the theoretical model that justies ranking docu - ments in this way is the probabilistic ranking principle ( robertson , 123 ) .
it suggests that documents should be ranked by their probablility of relevance to the query .
however , the optimality of such a ranking relies on the assumption that there are no statistical depen - dencies between the probabilities of relevance among documents an assumption that is clearly violated in practice .
for example , if one document about jaguar cars is not relevant to a user who issues the query jaguar , other car pages become less likely to be rele - vant .
furthermore , empirical studies have shown that given a xed query , the same document can have dif - ferent relevance to dierent users ( teevan et al . , 123 ) .
this undermines the assumption that each document has a single relevance score that can be provided as training data to the learning algorithm .
finally , as users are usually satised with nding a small number of , or even just one , relevant document , the usefulness and relevance of a document does depend on other documents ranked higher .
as a result , most search engines today attempt to elim - inate redundant results and produce diverse rankings that include documents that are potentially relevant to the query for dierent reasons .
however , learning op - timally diverse rankings using expert judgments would require document relevance to be measured for dier - ent possible meanings of a query .
while the trec interactive track123 provides some documents labeled in this way for a small number of queries , such document collections are even more dicult to create than stan - dard expert labeled collections .
several non - learning algorithms for obtaining a diverse ranking of documents from a non - diverse ranking have been proposed .
one common one is maximal marginal relevance ( mmr ) ( carbonell & goldstein , 123 ) .
given a similarity ( relevance ) measure between docu - ments and queries sim123 ( d , q ) and a similarity measure between pairs of documents sim123 ( di , dj ) , mmr iter - atively selects documents by repeatedly nding di = argmaxdd sim123 ( d , q ) ( 123 ) maxdjs sim123 ( d , dj ) where s is the set of documents already selected and is a tuning parameter .
in this way mmr selects the most relevant documents that are also dierent from any documents already selected .
critically , mmr requires that the relevance function sim123 ( d , q ) , and the similarity function sim123 ( di , dj ) is known .
it is usual to obtain sim123 and sim123 using al - gorithms such as those discussed above .
the goal of mmr is to rerank an already learned ranking ( that of ranking documents by decreasing sim123 score ) to im - prove diversity .
all previous approaches of which we are aware that optimize diversity similarly require a relevance function to be learned prior to performing a diversication step ( zhu et al . , 123; zhang et al . , 123; zhai et al . , 123 ) , with the exception of chen and karger ( 123 ) .
rather , they require that a model for estimating the probability a document is relevant , given a query and other non - relevant documents , is available .
in contrast , we directly learn a diverse rank - ing of documents using users clicking behavior .
problem formalization
we address the problem of learning an optimally diver - sied ranking of documents d = ( d123 , .
, dn ) for one xed query .
suppose we have a population of users , where each user ui considers some subset of documents ai d as relevant to the query , and the remainder of the documents as non - relevant .
intuitively , users with dierent interpretations for the query would have dif - ferent relevant sets , while users with similar interpre - tations would have similar relevant sets .
at time t , we interact with user ut with relevant set at .
we present an ordered set of k documents , bt = ( b123 ( t ) , .
, bk ( t ) ) .
the user considers the results in order , and clicks on up to one document .
the prob - ability of user ut clicking on document di ( conditional on the user not clicking on a document presented ear - lier in the ranking ) is assumed to be pti ( 123 , 123 ) .
we refer to the vector of probabilities ( pti ) id as the type of user ut .
in the simplest case , we could take pti = 123 if di at and 123 otherwise , in which case the user clicks on the rst relevant document or does not click if no documents in bt are relevant .
however , in reality clicks tend to be noisy although more relevant docu -
learning diverse rankings with multi - armed bandits
at every rank
pj 123 for counter=123
algorithm 123 ranked explore and commit 123 : input : documents ( d123 , . . , dn ) , parameters , , k .
123 : x ( cid : 123 ) 123k123 / 123 log ( 123k / ) ( cid : 123 ) 123 : ( b123 , .
, bk ) k arbitrary documents .
123 : for i=123 .
k do 123 : end for
display ( b123 , .
, bk ) to user; record clicks if user clicked on bi then pj pj + 123
j argmaxj pj
for j=123
over every document dj
loop x times
commit to best document at this rank
ments are more likely to be clicked on .
in our analysis , we will take pti ( 123 , 123 ) .
we get payo 123 if the user clicks , 123 if not .
the goal is to maximize the total payo , summing over all time .
this payo represents the number of users who clicked on any result , which can be interpreted as the user nding at least one potentially relevant document ( so long as pti is higher when di at than when di / at ) .
the event that a user does not click is called aban - donment since the user abandoned the search results .
abandonment is an important measure of user satis - faction because it indicates that users were presented with search results of no potential interest .
learning algorithms
we now present two algorithms that directly mini - mize the abandonment rate .
at a high level , both algorithms learn a marginal utility for each document at each rank , displaying documents to maximize the probability that a new user of the search system would nd at least one relevant document within the top k positions .
the algorithms dier in their assumptions .
ranked explore and commit
the rst algorithm we present is a simple greedy strat - egy that assumes that user interests and documents do not change over time .
as we will see , after t time steps this algorithm achieves a payo of at least ( 123 / e ) op t o ( k123n / 123 ln ( k / ) ) with probability at least 123 .
op t denotes the maximal payo that could be obtained if the click probabilities pti were known ahead of time for all users and documents , and ( 123 123 / e ) op t is the best obtainable polynomial time approximation , as will be explained in section 123 .
as described in algorithm 123 , ranked explore and
sequentially select documents
bi ( t ) select - arm ( mabi ) if bi ( t ) ( b123 ( t ) , . . , bi123 ( t ) ) then replace repeats
bi ( t ) arbitrary unselected document
for i = 123
algorithm 123 ranked bandits algorithm 123 : initialize mab123 ( n ) , .
, mabk ( n ) 123 : for t = 123 .
t do 123 : end for
display ( b123 ( t ) , .
, bk ( t ) ) to user; record clicks for i = 123 .
k do if user clicked bi ( t ) and bi ( t ) = bi ( t ) then
update ( mabi , arm = bi ( t ) , reward = fit )
determine feedback for mabi
fit = 123
fit = 123
commit ( rec ) iteratively selects documents for each rank .
at each rank position i , every document dj is presented a xed number x times , and the number of clicks it receives during these presentations is recorded .
after nx presentations , the algorithm permanently as - signs the document that received the most clicks to the current rank , and moves on to the next rank .
ranked bandits algorithm
ranked explore and commit is purely greedy , mean - ing that after each document is selected , this deci - sion is never revisited .
in particular , this means that if user interests or documents change , rec can per - form arbitrarily poorly .
in contrast , the ranked ban - dits algorithm ( rba ) achieves a combined payo of ( 123 / e ) op t o ( k t n log n ) after t time steps even if documents and user interests change over time .
this algorithm leverages standard theoretical results for multi - armed bandits .
multi - armed bandits ( mab ) are modeled on casino slot machines ( sometimes called one - armed bandits ) .
the goal of standard mab algo - rithms is to select the optimal sequence of slot ma - chines to play to maximize the expected total reward collected .
for further details , refer to ( auer et al . , 123a ) .
the ranked bandits algorithm runs an mab instance mabi for each rank i .
each of the k copies of the multi - armed bandit algorithm maintains a value ( or index ) for every document .
when selecting the ranking to display to users , the algorithm mab123 is responsible for choosing which document is shown at rank 123
next , the algorithm mab123 determines which
learning diverse rankings with multi - armed bandits
document is shown at rank 123 , unless the same docu - ment was selected at the highest rank .
in that case , the second document is picked arbitrarily .
this pro - cess is repeated to select all top k documents .
next , after a user considers up to the top k documents in order and clicks on one or none , we need to update the indices .
if the user clicks on a document actually selected by an mab instance , the reward for the arm corresponding to that document for the multi - armed bandit at that rank is 123
the reward for the arms corresponding to all other selected documents is 123
in particular , note that the rba treats the bandits corre - sponding to each rank independently .
precise pseudo - code for the algorithm is presented in algorithm 123
a generalization of this algorithm , in an abstract set - ting without the application to information retrieval , was discovered independently by streeter and golovin the actual mab algorithm used for each mabi in - stance is not critical , and in fact any algorithm for the non - stochastic multi - armed bandit problem will suf - ce .
our theoretical analysis only requires that : the algorithm has a set s of n strategies .
in each period t a payo function ft : s ( 123 , 123 ) is dened .
this function is not revealed to the algo - rithm , and may depend on the algorithms choices before time t .
in each period the algorithm chooses a ( random ) element yt s based on the feedback revealed in
the feedback revealed in period t is ft ( yt ) .
the expected payos of the chosen strategies sat -
e ( ft ( y ) ) r ( t )
where r ( t ) is an explicit function in o ( t ) which depends on the particular multi - armed bandit al - gorithm chosen , and the expectation is over any randomness in the algorithm .
we will use the exp123 algorithm in our analysis , where r ( t ) =
t n log n ( cid : 123 ) ( auer et al . , 123b ) .
we will also later see that although these conditions are needed to bound worst - case performance , better practical performance may be obtained at the expense of worst - case performance if they are relaxed .
theoretical analysis
we now present a theoretical analysis of the algorithms presented in section 123
first however , we discuss the oine version of this optimization problem .
the oine optimization problem
the problem of choosing the optimum set of k docu - ments for a given user population is np - hard , even if all the information about the user population ( i . e .
the set of relevant documents for each user ) is given oine and we restrict ourselves to pij ( 123 , 123 ) .
this is be - cause selecting the optimal set of documents is equiva - lent to the maximum coverage problem : given a posi - tive integer k and a collection of subsets s123 , s123 , .
, sn of an m - element set , nd k of the subsets whose union has the largest possible cardinality .
the standard greedy algorithm for the maximum cov - erage problem , translated to our setting , iteratively chooses the document that is relevant to the most users for whom a relevant document has not yet been se - lected .
this algorithm is a ( 123 123 / e ) - approximation algorithm for this maximization problem ( nemhauser et al . , 123 ) .
the ( 123 123 / e ) factor is optimal and no better worst - case approximation ratio is achievable
in polynomial time unless np dt im e ( cid : 123 ) nlog log n ( cid : 123 )
( khuller et al . , 123 ) .
analysis of ranked bandits algorithm
we start by analyzing the ranked bandits algorithm .
this algorithm works by simulating the oine greedy algorithm , using a separate instance of the multi - armed bandit algorithm for each step of the greedy algorithm .
except for the sublinear regret term , the combined payo is as high as possible without violat - ing the hardness - of - approximation result stated in the to analyze the rba , we rst restrict ourselves to users who click on any given document with probability ei - ther 123 or 123
we refer to this restricted type of user as a deterministic user; we will relax the requirement later .
additionally , this analysis applies to a worst case ( and hence xed ) sequence of users .
further , it is useful to introduce some notation .
for a set a and a sequence b = ( b123 , b123 , .
, bk ) , let
if a intersects ( b123 ,
gi ( a , b ) = gi ( a , b ) = gi ( a , b ) gi123 ( a , b )
recalling that at is the set of documents relevant to user ut , we see that gk ( at , b ) is the payo of present - ing b to the user ut .
let b = argmax
op t =
learning diverse rankings with multi - armed bandits
recall that ( b123 ( t ) , .
, bk ( t ) ) is the sequence of docu - ments chosen by the algorithms mab123 , .
, mabk at time t , and that ( b123 ( t ) , .
, bk ( t ) ) is the sequence of documents presented to the user .
dene the feedback function fit for algorithm mabi at time t , as follows :
( cid : 123 ) 123 if gi123 ( at , bt ) = 123 and b at
note that the value of fit dened in the pseudocode for the ranked bandits algorithms is equal to fit ( bi ( t ) ) .
lemma 123
for all i ,
we will prove , by induction on i , that
op t +ir ( t ) .
the theorem follows by taking i = k and using the
in the base case i = 123 , inequality ( 123 ) is trivial .
for the induction step , let
zi = op t e
op t 123
( gk ( at , b ) gi123 ( at , bt ) )
and lemma 123 says that
zi = zi123 e
zi123 r ( t ) .
first , note that
gi ( at , bt ) fit ( bi ( t ) ) .
this is trivially true when fit ( bi ( t ) ) = 123
when fit ( bi ( t ) ) = 123 , gi123 ( at , bt ) = 123 and bi ( t ) at .
this implies that bi ( t ) = bi ( t ) and that gi ( at , bt ) = 123
now using the regret bound for mabi we obtain
e ( fit ( b ) ) r ( t )
to complete the proof of the lemma , we will prove that
fit ( b ) gk ( at , b ) gi123 ( at , bt ) .
the lemma follows immediately by combining ( 123 ) - ( 123 ) .
observe that the left side of ( 123 ) is a non - negative integer , while the right side takes one of the values ( 123 , 123 , 123 ) .
thus , to prove ( 123 ) it suces to show that the left side is greater than or equal to 123 whenever the right side is equal to 123
the right side equals 123 only when gi123 ( at , bt ) = 123 and at intersects b .
in this case it is clear that there exists at least one b b such that fit ( b ) = 123 , hence the left side is greater than or equal to 123
theorem 123
the algorithms combined payo after t
op t kr ( t )
combining ( 123 ) with ( 123 ) , we obtain
zi123 + r ( t ) .
combining this with the induction hypothesis proves
the general case , in which user uis type vector ( pij ) jd is an arbitrary element of ( 123 , 123 ) d , can be re - duced via a simple transformation to the case of de - terministic users analyzed above .
we replace user ui with a random deterministic user ui whose type vector pi ( 123 , 123 ) d is sampled using the following rule : the random variable pij has distribution
( cid : 123 ) 123 with probability pij
123 with probability 123 pij ,
and these random variables are mutually independent .
note that the clicking behavior of user ui when pre - sented with a ranking b is identical to the clicking behavior observed when a random user type ui is sam - pled from the above distribution , and the ranking b is presented to this random user .
thus , if we apply the specied transformation to users u123 , u123 , .
, ut , obtaining a random sequence u123 , u123 , .
, ut of deter - ministic users , this transformation changes neither the algorithms expected payo nor that of the optimum ranking b .
thus , theorem 123 for general users can be deduced by applying the same theorem to the ran - dom sequence u123 , .
, ut and taking the expectation of the left and right sides of ( 123 ) over the random choices involved in sampling u123 , .
, ut .
learning diverse rankings with multi - armed bandits
note also that b is dened as the optimal subset of k documents , and op t is the payo of presenting b , without specifying the order in which documents are presented .
however , the ranked bandits algorithm learns an order for the documents in addition to iden - tifying a set of documents .
in particular , given k ( cid : 123 ) < k , rba ( k ( cid : 123 ) ) would receive exactly the same feedback as the rst k ( cid : 123 ) instances of mabi receive when running rba ( k ) .
hence any k ( cid : 123 ) sized prex of the learned rank - ing also has the same performance bound with respect the appropriate smaller set b ( cid : 123 ) .
finally , it is worth noting that this analysis cannot be trivially extended to non - binary payos , for exam - ple when learning a ranking of web advertisements .
in particular , the greedy algorithm on which rba is based in the non - binary payo case can obtain a payo that is a factor of k below optimal , for any > 123
analysis of ranked explore and commit
the analysis of the ranked explore and commit ( rec ) algorithm is analogous to that of the ranked bandits algorithm , except that the equivalents of lemma 123 and theorem 123 are only true with high prob - ability after t123 = nxk time steps of exploration have occurred .
let b denote the ranking selected by rec .
lemma 123
let x = 123k123 / 123 log ( 123k / ) .
assume at is drawn i . i . d .
from a xed distribution of user types .
for any i , with probability 123 / k ,
( gk ( at , b ) gi123 ( at , b ) )
proof outline .
first note that in this setting , b and op t are dened in expectation over the at drawn .
for any document , by hoedings inequality , with probability 123 / 123k the true payo of that document explored at rank i is within / 123k of the observed mean payo .
hence the document selected at rank i is within / k of the payo of the best document available at rank i .
now , the same proof as for lemma 123 applies , although with a dierent regret r ( t ) .
theorem 123
with probability ( 123 ) , the algorithms combined payo after t rounds satises :
op t t nkx ( 123 )
proof outline .
applying lemma 123 for all i ( 123 , . . , k ) , with probability ( 123 k / k ) = ( 123 ) the conclusion of the lemma holds for all i .
next , an analogous proof as for theorem 123 applies , except replacing r ( t ) with k t and noting that the regret during the nkx exploration steps is at most 123 for every time step .
it is interesting to note that , in contrast to the ranked bandits algorithm , this algorithm can be adapted to the case where clicked documents provide real valued payos .
the only modication necessary is that docu - ments should always be presented by decreasing payo value .
however , we do not address this extension fur - ther due to space constraints .
in this section , we evaluate the ranked bandits and ranked explore and commit algorithms , as well as two variants of rba , with simulations using a user and document model .
we chose a model that produces a user population and document distribution designed to be realistic yet al - low us to evaluate the performance of the presented algorithms under dierent levels of noise in user click - ing behavior .
our model rst assigns each of 123 users to topics of interest using a chinese restaurant pro - cess ( aldous , 123 ) with parameter = 123
this led to a mean of 123 unique topics , with topic popularity decaying according to a power law .
taking a collection of 123 documents , we then randomly assigned as many documents to each topic as there were users assigned to the topic , leading to topics with more users having more relevant documents .
we set each document as - signed to a topic as relevant to all users assigned to that topic , and all other documents as non relevant .
the probabilities of a user clicking on relevant and non - relevant documents were set to constants pr and we tested by drawing one user uniformly from the user population at each time step , and presented this user with the ranking selected by each algorithm , using k = 123
we report the average number of time steps where the user clicked on a result , and the average number of time steps where at least one of the pre - sented documents was relevant to the user .
all num - bers we report are averages over 123 , 123 algorithm runs .
performance without click noise
we start by evaluating how well the rec and rba algorithms maximize the clickthrough rate in the sim - plest case when pr = 123 and pnr = 123
we also compare their performance to the clickthrough rate that the same users would generate if presented with a static system that orders documents by decreasing true prob -
learning diverse rankings with multi - armed bandits
figure 123
clickthrough rate of the learned ranking as a function of the number of times the ranking was presented
ability of relevance to the users assuming document relevances are independent .
figure 123 shows that both rec and rba perform well above the static baseline and well above the performance guarantee provided by the theoretical results .
this is not surprising , as the ( 123 123 / e ) op t bound is a worst - case bound .
in fact , we see that rec with x = 123 nearly matches the performance of the best possible ranking after nish - ing its initial exploration phase .
we also see that the exploration parameter of rec plays a signicant role in the performance , with lower exploration leading to faster convergence but slightly lower nal performance .
note that despite rec performing best here , the rank - ing learned by rec is xed after the exploration steps have been performed .
if user interests and documents change over time , the performance of rec could fall arbitrarily .
in contrast , rba is guaranteed to remain near or above the ( 123 123 / e ) op t bound .
eect of click noise
in figure 123 , the clickthrough rate and fraction of users who found a relevant document in the top k positions is identical ( since users click if and only if they are presented with a relevant document ) .
figure 123 shows how the fraction of users who nd a relevant document decays as the probability of a user clicking becomes noisier .
the gure presents the performance lines for rec and rba across a range of click probabilities , from ( pr = 123 , pnr = 123 ) to ( pr = 123 , pnr = 123 ) .
we see that both algorithms decay gracefully : as the clicks become noisier noisy , the fraction of users presented with a relevant docu - ments decays slowly .
optimizing practical eectiveness
despite the theoretical results shown earlier , it would be surprising if an algorithm designed for the worst
figure 123
eect of noise in clicking behavior on the quality of the learned ranking .
case had best average case performance .
figure 123 shows the clickthrough rate ( which the algorithms op - timize ) , and fraction of users who nd relevant doc - uments ( which is of more interest to information re - trieval practitioners ) , for variants building on the in - sights of the ranked bandits idea .
specically , two variants of rba that have the best performance we could obtain in our simulation are shown .
we found that using a ucb123 - based multi - armed bandit algo - rithm ( auer et al . , 123a ) in place of exp123 improves the performance of rba substantially when user inter - ests are static .
note however , that ucb123 does not sat - isfy the constraints presented in section 123 because it assumes rewards are identically distributed over time , an assumption violated in our setting when changes in the documents presented above rank i alter the reward distribution at rank i .
nevertheless , we see that this modication substantially improves the performance of rba .
we expect such an algorithm to perform best when few documents are prone to radical shifts in pop -
conclusions and extensions
we have presented a new formulation of the learning to rank problem that explicitly takes into account the relevance of dierent documents being interdependent .
we presented , analyzed and evaluated two algorithms and two variants for this learning setting .
we have shown that the learning problem can be solved in a theoretically sound manner , and that our algorithms can be expected to perform reasonably in practice .
we plan to extend this work by addressing the non - binary document relevance settings , and perform em - pirical evaluations using real users and real documents .
furthermore , we plan to investigate how prior knowl - edge can be incorporated into the algorithms to im - prove speed of convergence .
finally , we plan to inves -
123 123 123 123 123 123 123 123 123 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 123clickthrough rate ( 123 - abandonment ) number of user presentationsbest possible performance ( opt ) lower bound performance ( 123 - 123 / e ) optranked bandits algorithmranked explore and commit ( x=123 ) ranked explore and commit ( x=123 ) popularity - based static ranking 123 123 123 123 123 123 123 123 123 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 123fraction of rankings with a relevant documentnumber of user presentationsbest possible relevance performanceranked bandits algorithmranked explore and commit ( x=123 ) 123 / 123; 123 / 123 . 123 / 123 . 123 / 123 . 123 / 123 . 123 / 123 . 123 / 123 learning diverse rankings with multi - armed bandits
baeza - yates , r . , & ribeiro - neto , b .
( 123 ) .
modern in -
formation retrieval .
new york , ny : addison wesley .
burges , c . , shaked , t . , renshaw , e . , lazier , a . , deeds , m . , hamilton , n . , & hullender , g .
( 123 ) .
learning to rank using gradient descent .
in icml ( pp .
123 ) .
burges , c .
c . , ragno , r . , & le , q .
( 123 ) .
learning to rank with nonsmooth cost functions .
in nips ( pp .
123 ) .
mit press .
carbonell , j . , & goldstein , j .
( 123 ) .
the use of mmr , diversity - based reranking for reordering documents and producing summaries .
in sigir ( pp .
123 ) .
chen , h . , & karger , d .
( 123 ) .
less is more : proba - bilistic models for retrieving fewer relevant documents .
in sigir ( pp .
123 ) .
chu , w . , & ghahramani , z .
( 123 ) .
gaussian processes for ordinal regression .
journal of machine learning re - search , 123 , 123
herbrich , r . , graepel , t . , & obermayer , k .
( 123 ) .
large margin rank boundaries for ordinal regression .
advances in large margin classiers ( pp .
123 ) .
joachims , t .
( 123 ) .
optimizing search engines using click -
through data .
in kdd ( pp .
123 ) .
khuller , s . , moss , a . , & naor , j .
( 123 ) .
the budgeted
maximum coverage problem .
letters , 123 , 123
metzler , d . , & croft , w .
( 123 ) .
a markov random eld model for term dependencies .
in sigir ( pp .
123 ) .
nemhauser , g .
l . , wolsey , l .
a . , & fisher , m .
( 123 ) .
an analysis of approximation for maximizing submodu - lar set functions .
mathematical programming , 123 , 123
radlinski , f . , & joachims , t .
( 123 ) .
query chains : learn - ing to rank from implicit feedback .
in kdd ( pp
radlinski , f . , & joachims , t .
( 123 ) .
active exploration for learning rankings from clickthrough data .
in kdd
robertson , s .
( 123 ) .
the probability ranking principle
journal of documentation , 123 , 123
streeter , m . , & golovin , d .
( 123 ) .
an online algorithm for maximizing submodular functions ( technical report cmu - cs - 123 - 123 ) .
carnegie mellon university .
taylor , m .
j . , guiver , j . , robertson , s .
e . , & minka , t .
( 123 ) .
softrank : optimizing non - smooth ranking met - rics .
in wsdm ( pp .
123 ) .
teevan , j . , dumais , s .
t . , & horvitz , e .
( 123 ) .
charac - terizing the value of personalizing search .
in sigir ( pp .
turpin , a . , & scholer , f .
( 123 ) .
user performance versus
precision measures for simple search tasks .
yue , y . , finley , t . , radlinski , f . , & joachims , t .
( 123 ) .
a support vector method for optimizing average precision .
in sigir ( pp .
123 ) .
zhai , c . , cohen , w .
w . , & laerty , j .
( 123 ) .
beyond independent relevance : methods and evaluation metrics for subtopic retrieval .
in sigir ( pp .
123 ) .
zhang , b . , li , h . , liu , y . , ji , l . , xi , w . , fan , w . , chen , z . , & ma , w . - y .
( 123 ) .
improving web search results using anity graph .
in cikm ( pp .
123 ) .
zhu , x . , goldberg , a .
b . , gael , j .
v . , & andrzejewski , d .
( 123 ) .
improving diversity in ranking using absorbing random walks .
proceedings of naacl hlt .
figure 123
in a practical setting , it may be benecial to use a variant of rba to obtain improved performance at the cost of weaker theoretical guarantees .
performance is shown in realistic settings pr = 123 , pnr = 123 .
tigate if the bandits at dierent ranks can be coupled to improve the rate at which rba converges .
we would like to thank the reviewers for helpful com - ments .
this work was supported by nsf career award ccf - 123 , nsf award ccf - 123 , nsf career award 123 and a gift from google .
the rst author was supported by a microsoft research
