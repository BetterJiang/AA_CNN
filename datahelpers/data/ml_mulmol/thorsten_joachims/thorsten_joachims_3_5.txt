we propose a discriminative method for learning the parameters of linear se - quence alignment models from training examples .
compared to conventional gen - erative approaches , the discriminative method is straightforward to use when op - erations ( e . g .
substitutions , deletions , insertions ) and sequence elements are de - scribed by vectors of attributes .
this admits learning exible and more complex alignment models .
while the resulting training problem leads to an optimization problem with an exponential number of constraints , we present a simple algorithm that nds an arbitrarily close approximation after considering only a subset of the constraints that is linear in the number of training examples and polynomial in the length of the sequences .
we also evaluate empirically that the method effectively learns good parameter values while being computationally feasible .
methods for sequence alignment are common tools for analyzing sequence data rang - ing from biological applications ( 123 ) to natural language processing ( 123 ) ( 123 ) .
they can be thought of as measures of similarity between sequences where the similarity score is the result of a discrete optimization problem that is typically solved via dynamic pro - gramming .
while the dynamic programming algorithm determines the general notion of similarity ( e . g .
local alignment vs .
global alignment ) , any such similarity measure requires specic parameter values before it is fully specied .
examples of such para - meter values are the costs for substituting one sequence elements for another , as well as costs for deletions and insertions .
these parameter values determine how well the measure works for a particular task .
in this paper we tackle the problem of inferring the parameter values from training data .
our goal is to nd parameter values so that the resulting similarity measure best
m m s m m i m m
figure 123 : example of a local sequence alignment .
reects the desired notion of similarity .
instead of assuming a generative model of se - quence alignment ( e . g .
( 123 ) ) , we take a discriminative approach to training following the general algorithm described in ( 123 ) .
a key advantage of discriminative training is that operations can easily be described by features without having to model their de - pendencies like in generative training .
in particular , we aim to nd the set of parameter values that corresponds to the best similarity measure a given alignment model can represent .
taking a large - margin approach , we show that we can solve the resulting training problem efciently for a large class of alignment algorithms that implement a linear scoring function .
while the resulting optimization problems have exponen - tially many constraints , our algorithm nds an arbitrarily good approximation after considering only a subset of constraints that scales polynomially with the length of the sequences and linearly with the number of training examples .
we empirically and theoretically analyze the scaling of the algorithm and show that the learned similarity score performs well on test data .
123 sequence alignment sequence alignment computes a similarity score for two ( or more ) sequences s 123 and s123 from an alphabet = ( 123 , . . , ) .
an alignment a is a sequence of operations that transforms one sequence into the other .
in global alignment , the whole sequence is transformed .
in local alignment , only an arbitrarily sized subsequence is aligned .
com - monly used alignment operations are match ( m ) , substitution ( s ) , deletion ( d ) and insertion ( i ) .
an example of a local alignment is given in figure 123
in the example , there are 123 matches , 123 substitution , and 123 insertion / deletion .
with each operation there is an associated cost / reward .
assuming a reward of 123 for match , a cost of 123 for substi - tution , and a cost of 123 for insertion / deletion , the total alignment score d ( cid : 123 ) w ( s123 , s123 , a ) in the example is 123
the optimal alignment a is the one that maximizes the score for a given cost model .
more generally , we consider alignment algorithms that optimize a linear scoring
d ( cid : 123 ) w ( s123 , s123 , a ) = ( cid : 123 ) wt ( s123 , s123 , a )
where ( s123 , s123 , a ) is a feature vector describing the alignment a applied to s 123 and s123
( cid : 123 ) w is a given cost vector .
instead of assuming a nite alphabet and a nite set of operations , we only require that the reward / cost of each operation a o on any two characters c123 , c123 can be expressed as a linear function over attributes ( c 123 , c123 , ao )
score ( c123 , c123 , ao ) = ( cid : 123 ) wt ( c123 , c123 , ao ) .
( c123 , c123 , ao ) can be thought of as a vector of attributes describing the match of c 123 and c123 under operation ao .
note that c123 and c123 can take dummy values for insertions , deletions , etc .
this representation allows different scores depending on various properties of the characters or the operation .
the feature vector for a complete alignment ( s 123 , s123 , a ) is the sum of the individual feature vectors
( s123 , s123 , a ) =
( c123 ( ai ) , c123 ( ai ) , ai )
where c123 ( ai ) and c123 ( ai ) indicate the characters that ai is applied to .
only those op - eration sequences are valid that transform s 123 into s123
note that a special case of this model is the conventional parameterization using a substitution matrix and xed scores for deletions and insertions .
finding the optimal alignment corresponds to the follow - ing optimization problem
d ( cid : 123 ) w ( s123 , s123 ) = maxa ( d ( cid : 123 ) w ( s123 , s123 , a ) ) ( cid : 123 ) wt ( s123 , s123 , a )
score ( c123 ( ai ) , c123 ( ai ) , ai )
this type of problem is typically solved via dynamic programming .
in the following we consider local alignment via the smith / waterman algorithm ( 123 ) .
however , the results can be extended to any alignment algorithm that optimizes a linear scoring function and that solves ( 123 ) globally optimally .
this also holds for other structures besides
123 inverse sequence alignment inverse sequence alignment is the problem of using training data to learn the parame - ters ( cid : 123 ) w of an alignment model and algorithm so that the resulting similarity measure d ( cid : 123 ) w ( s123 , s123 ) best represents the desired notion of similarity on new data .
while previ - ous approaches to this problem exist ( 123 , 123 ) , they are limited to special cases and small numbers of parameters .
we will present an algorithm that applies to any linear align - ment model with no restriction on the function or number of parameters , instantiating the general algorithm we described in ( 123 ) .
an interesting related approach is outlined in ( 123 , 123 ) , but it is not clear in how far it leads to practical algorithms .
we assume the following two scenarios , for which the notation is inspired by pro -
123 alignment prediction
in the rst scenario , the goal is to predict the optimal sequence of alignment operations a for a given pair of sequences s n and sh , which we call native and homolog sequence .
we assume that examples are generated i . i . d .
according to a distribution p ( s n , sh , a ) .
we approach this prediction problem using the following linear prediction rule which is parameterized by ( cid : 123 ) w .
a = argmaxa
d ( cid : 123 ) w ( sn , sh , a )
this rule predicts the alignment sequence a which scores highest according to the lin - ear model .
by changing the cost of alignment operations via ( cid : 123 ) w , the behavior of the prediction rule can be modied .
the error of a prediction a compared to the true align - ment a is measured using a loss function l ( a , a ) .
the goal of learning is to nd a ( cid : 123 ) w that minimizes the expected loss ( i . e .
risk ) .
p ( ( cid : 123 ) w ) =
d ( cid : 123 ) w ( sn , sh , a )
dp ( sn , sh , a )
one reasonable loss function l ( . , . ) to use is the number of alignment operations that are different in a and a .
for simplicity , however , we will only consider the 123 / 123 - loss l ( . , . ) in the following .
it return the value 123 if both arguments are equal , and value 123
123 homology prediction
in the second scenario the goal is to predict whether two proteins are homologous .
we assume that examples are generated i . i . d .
according to a distribution p ( s n , sh , sd ) .
sn is the native sequence , sh the homologous sequence , and s d is a set of decoy se - quences sd123 , . . . , sdd .
the goal is a similarity measure d ( cid : 123 ) w ( . , . ) so that native sequence sn and homolog sh are more similar than the native sequence s n and any decoy sdj ,
d ( cid : 123 ) w ( sn , sh ) > d ( cid : 123 ) w ( sn , sdj ) .
the goal of learning is to nd the cost parameters ( cid : 123 ) w that minimize the probability p ( ( cid : 123 ) w ) that the similarity with any decoy sequence d ( cid : 123 ) w ( sn , sdj ) is higher than the similarity with the homolog sequence d ( cid : 123 ) w ( sn , sh ) .
ssd ( sh ) d ( cid : 123 ) w ( sn , s )
p ( ( cid : 123 ) w ) =
sh , arg max
dp ( sn , sh , sd )
again , we assume a 123 / 123 - loss l ( . , . ) .
123 a maximum - margin approach to learning the cost
in both scenarios , the data generating distributions p ( s d , sh , a ) and p ( sd , sh , sd ) are unknown .
however , we have a training sample s drawn i . i . d from p ( . ) .
this training sample will be used to learn the parameters ( cid : 123 ) w .
we will rst consider the case of alignment prediction , and then extend the algorithm to the problem of homology
123 alignment predictions given is a training sample s = ( ( sd n , an ) ) of n sequence pairs with their desired alignment .
in the following , we will design a discriminative training algorithm that nds a parameter vector ( cid : 123 ) w for rules of type ( 123 ) by minimizing the loss on the training data s .
123 , a123 ) , . . . , ( sd
n , sh
123 , sh
s ( ( cid : 123 ) w ) =
i , sh
i , a )
first , consider the case where there exists a ( cid : 123 ) w so that the training loss r l since we assume a scoring function that is linear in the parameters
s ( ( cid : 123 ) w ) is zero .
d ( cid : 123 ) w ( s123 , s123 , a ) = ( cid : 123 ) wt ( s123 , s123 , a ) ,
the condition of zero training error can be written as a set of linear inequality con - straints .
for each native / homolog pair s n i , we need to introduce one linear con - straint for each possible alignment a of s n
a ( cid : 123 ) = a123 :
a ( cid : 123 ) = an :
123 , sh
123 , a ) < d ( cid : 123 ) w ( sn
123 , sh
123 , a123 )
n , sh
n , a ) < d ( cid : 123 ) w ( sn
n , sh
n , an )
that fullls this set of constraints has a training loss rl
s ( ( cid : 123 ) w any parameter vector ( cid : 123 ) w of zero .
this approach of writing the training problem as a linear system follows the method in ( 123 ) proposed for the special case of global alignment without free inser - tions / deletions .
however , for the general case in equation ( 123 ) the number of con - straints is exponential , since the number of alignments a between s n i can be exponential in the length of s n i .
unlike the restricted case in ( 123 ) , standard op - timization algorithms cannot handle this size of problem .
to overcome this limitation , in section 123 we will propose an algorithm that exploits the special structure of equa - tion ( 123 ) so that it needs to examine only a subset that is polynomial in the length of i and sh
i and sh
i and sh
to specify a unique solution , we select the ( cid : 123 ) w
if the set of inequalities in equation ( 123 ) is feasible , there will typically be more than one solution ( cid : 123 ) w for which each score d ( cid : 123 ) w ( sn i , a ) for all i .
this corresponds to the maximum - margin principle employed in support vector machines ( svms ) ( 123 ) .
denoting the margin by and restricting the l 123 norm of ( cid : 123 ) w to make the problem well - posed , this leads to the following optimization problem .
i , ai ) is uniformly most different from max a ( cid : 123 ) =ai d ( cid : 123 ) w ( sn
i , sh
i , sh
a ( cid : 123 ) = a123 :
a ( cid : 123 ) = an :
123 , sh
n , sh
|| ( cid : 123 ) w|| = 123
123 , a ) d ( cid : 123 ) w ( sn n , a ) d ( cid : 123 ) w ( sn
123 , a123 ) n , an )
123 , sh
n , sh
due to the linearity of the similarity function ( 123 ) , the length of ( cid : 123 ) w is a free variable and we can x it to 123 / .
substituting for and rearranging leads to the equivalent
a ( cid : 123 ) = a123 :
a ( cid : 123 ) = an :
123 ( cid : 123 ) wt ( cid : 123 ) w
123 , sh
123 , a123 ) ( sn n , an ) ( sn
n , sh
n , sh
n , a )
123 , sh
123 , a )
since this quadratic program ( qp ) has a positive - denite objective function and ( feasi - ble ) linear constraints , it is strictly convex .
this means it has a unique global minimum and no local minima ( 123 ) .
the constraints are similar to the ordinal regression approach in ( 123 ) and it has a structure similar to the ranking svm described in ( 123 ) for information retrieval .
however , the number of constraints is much larger .
to allow errors in the training set , we introduce slack variables i ( 123 ) .
correspond - p ( ( cid : 123 ) w ) we have one slack variable for each native sequence .
ing to the error measure r l this is different from a normal classication or regression svm , where there is a differ - ent slack variable for each constraint .
the slacks enter the objective function according to a trade - off parameter c .
for simplicity , we consider only the case where the slacks enter the objective function squared .
a ( cid : 123 ) = a123 :
a ( cid : 123 ) = an :
123 ( cid : 123 ) wt ( cid : 123 ) w + c 123 , sh
n , sh
123 , a123 ) ( sn n , an ) ( sn
123 , sh
123 , a )
n , sh
n , a )
( cid : 123 ) w 123 123
( cid : 123 ) w 123 n
analogous to classication and regression svms ( 123 ) , this formulation minimizes a regularized upper bound on the training loss r l
s ( ( cid : 123 ) w ) .
proposition 123 for any feasible point ( ( cid : 123 ) w , ( cid : 123 ) ) of ( 123 ) - ( 123 ) , 123 on the training loss rl
s ( ( cid : 123 ) w ) for the 123 / 123 - loss l ( . , . ) .
i is an upper bound
the proof is given in ( 123 ) .
the quadratic program and the proposition can be extended to any non - negative loss function .
123 homology prediction
123 , . . . , sh
for the problem of homology prediction , we can derive a similar training problem .
here , the training sample s consists of native sequences s n n , homolog se - 123 , . . . , sn as a simplifying assumption , we assume that between native and homolog sequences of maximum score is known123
the goal is to nd an op - the alignment an h p ( ( cid : 123 ) w ) is low .
again , nding a ( cid : 123 ) w such that the error timal ( cid : 123 ) w so that the error rate errl
n , and a set of decoy sequences s d
n for each native sn
123 , . . . , sd
123 , . . . , sn
, . . . , an h
123for protein alignment , for example , this could be generated via structural alignment .
on the training set
p ( ( cid : 123 ) w ) =
i , arg max
) d ( cid : 123 ) w ( sn
i , s )
is zero can be written as a set of linear inequality constraints .
there is one constraint for each combination of native sequence s n , and possible alignment a of sn
i , decoy sequence sdj
123 , sdj
123 , a ) < d ( cid : 123 ) w ( sn
123 , sh
123 , an h
n , sdj
n , a ) < d ( cid : 123 ) w ( sn
n , sh
n , an h
similar to the case of alignment prediction , one can add a margin criterion and slacks and arrives at the following convex quadratic program .
123 , an h
123 ( cid : 123 ) wt ( cid : 123 ) w + c
123 , sh
123 , sdj
123 , a )
( cid : 123 ) w 123 123
n , sh
n , an h
n ) ( sn
n , sdj
n , a )
( cid : 123 ) w 123 n
i is an upper bound on the training error err l
s ( ( cid : 123 ) w ) .
123 training algorithm
due to the exponential number of constraints in both the optimization problem for alignment prediction and homology prediction , naive use of off - the - shelf tools for their solution is computationally intractable for problems of interesting size .
however , by exploiting the special structure of the problem , we propose the algorithms shown in figures 123 and 123 that nd the solutions of ( 123 ) - ( 123 ) and ( 123 ) - ( 123 ) after examining only a small number of constraints .
the algorithms proceeds by greedily adding con - straints from ( 123 ) - ( 123 ) or ( 123 ) - ( 123 ) to a working set k .
the algorithms stop , when all constraints in ( 123 ) - ( 123 ) or ( 123 ) - ( 123 ) are fullled up to a precision of .
the following two theorems show that the algorithms return a solutions of ( 123 ) - ( 123 ) and ( 123 ) - ( 123 ) that are accurate with a precision of some predened , and that they stop after a polynomially bounded number of iterations through the repeat - loop .
theorem 123 ( correctness ) the algorithms return an approximation that has an objective value not higher than the solution of ( 123 ) - ( 123 ) and ( 123 ) - ( 123 ) , and that fullls all constraints up to a precision of .
for = 123 , the algorithm returns the exact solution ( ( cid : 123 ) w
input : native sequences sn
a123 , . . . , an , tolerated approximation error 123
n , homolog sequences sh
123 , . . . , sn
123 , . . . , sh
n , alignments
k = , ( cid : 123 ) w = 123 , ( cid : 123 ) = 123
korg = k for i from 123 to n
i , sh
nd a = argmaxa if ( cid : 123 ) wt ( ( sn
i , ai ) ( sn i , sh
k = k ( cid : 123 ) i , sh i , sh i , ai ) ( sn solve qp ( ( cid : 123 ) w , ( cid : 123 ) ) = argmin ( cid : 123 ) w , ( cid : 123 ) 123 ( cid : 123 ) wt ( cid : 123 ) w + c
i , a ) i , a ) ) < 123 i i , a ) ) 123 i
i , sh
via dynamic programming
i subject to
until ( k = korg )
figure 123 : sparse approximation algorithm for the alignment prediction task .
proof let ( cid : 123 ) w i be the solution of ( 123 ) - ( 123 ) or ( 123 ) - ( 123 ) respectively .
since the algorithm solves the qp on a subset of the constraints in each iteration , it returns a solution ( cid : 123 ) w with 123 .
this follows from the fact that restricting the feasible region cannot lead to a lower minimum .
123 ( cid : 123 ) wt ( cid : 123 ) w + c
i , sh
i , sdj
it is left to show that the algorithm does not terminate before all constraints ( 123 ) - ( 123 ) or ( 123 ) - ( 123 ) are fullled up to precision .
in the nal iteration , the algorithms nd the most violated constraint .
for alignment prediction , this is the constraint i , a ) ) < 123 i corresponding to the highest scoring i , ai ) ( sn i , sh alignment a .
for homology prediction , it is the constraint ( cid : 123 ) w t ( ( sn , a ) ) < 123 i corresponding to the highest scoring alignment a for each decoy .
three cases can occur : first , the constraint can be violated by more than and the algorithm will not terminate yet .
second , it is already in k and is fullled by construction .
third , the constraint is not in k but fullled anyway and it is not added .
if the constraint is fullled , the constraints for all other alignments into this decoy are fullled as well , since we checked the constraint for which the margin was smallest for the given ( cid : 123 ) w .
it follows that the algorithm terminates only if all constraints are fullled up to precision .
i , an h
i , sh
it is left to show that the algorithms terminates after a number of iterations that is smaller than the set of constraints .
the following theorem shows that the algorithm stops after a polynomial number of iterations .
input : native sequences sn
123 , . . . , sn
n , homolog sequences sh
n , sets of decoy sequences s d
123 , . . . , sd
123 , . . . , sh
n , alignments n , tolerated approximation
, . . . , an h
k = , ( cid : 123 ) w = 123 , ( cid : 123 ) = 123
korg = k for i from 123 to n
for j from 123 to |s d
i , sdj
nd a = argmaxa ( cid : 123 ) =ai if ( cid : 123 ) wt ( ( sn k = k i , an h i , sh solve qp ( ( cid : 123 ) w , ( cid : 123 ) ) = argmin ( cid : 123 ) w , ( cid : 123 ) 123 ( cid : 123 ) wt ( cid : 123 ) w + c
i , an h
i , sdj
i , sh
via dynamic pro -
, a ) ) < 123 i
i , sdj
, a ) ) 123i
until ( k = korg )
figure 123 : sparse approximation algorithm for the homology prediction task .
theorem 123 ( termination ) the algorithms stops after adding at most
constraints to the set k .
v is the minimum of ( 123 ) - ( 123 ) or ( 123 ) - ( 123 ) respectively .
r 123 is a constant bounded by the maximum of ( ( s n
i , ai ) ( sn
i , a ) ) 123 + 123
, a ) ) 123 + 123
i , sh
i , sh
i , sh
i , an h
i , sdj
in the following , we focuses on the homology prediction task .
the proof for the alignment prediction task is analogous .
the rst part of the proof is to show that the objective value increases by some constant with every constraint that is added to k .
denote with vk the solution vk = p ( ( cid : 123 ) w i subject to kk after adding k constraints .
this primal optimization problem can be transformed into an equivalent problem of the form v k = p ( ( cid : 123 ) w k , where each constraint has the form ( cid : 123 ) w t ( cid : 123 ) x 123 with ( cid : 123 ) x = ( ( sn i , sh i , an h
i , sdj its corresponding wolfe dual is d ( ( cid : 123 )
k ) = p ( ( cid : 123 ) w k ) = k ) = vk and for every feasible point d ( ( cid : 123 ) ) p ( ( cid : 123 ) w , ( cid : 123 ) ) .
primal and dual are
p ( ( cid : 123 ) w
k ) = min ( cid : 123 ) w , ( cid : 123 ) k ) = min ( cid : 123 ) w ( cid : 123 ) 123
123c; 123; . . . ; 123 ) .
j=123 ij ( cid : 123 ) xi ( cid : 123 ) xj .
at the solution d ( ( cid : 123 )
, a ) ; 123; . . . ; 123; 123 / i=123 i 123
123 ( cid : 123 ) wt ( cid : 123 ) w + c
connected via ( cid : 123 ) w
i ( cid : 123 ) xi ( cid : 123 ) xk+123 123 means extending the dual to
i ( cid : 123 ) xi .
adding a constraint to the dual with ( cid : 123 ) w
( cid : 123 ) t ( cid : 123 ) xk+123 =
k+123 ) = max
k ) + max
k ) + max
i ( cid : 123 ) xi ( cid : 123 ) xk+123 123
k+123 k+123 ( 123 ) 123
solving the remaining scalar optimization problem over k+123 shows that and that vk+123 vk +
since the algorithm only adds constraints that are violated by the current solution by more than , after adding kmax = 123v r constraints the solution vkmax over the subset kkmax is at least vkmax v123 + 123v r 123r123 = 123 + v .
any additional constraint
that is violated by more than would lead to a minimum that is larger than v .
since the minimum over a subset of constraints can only be smaller than the minimum over all constraints , there cannot be any more constraints violated by more than and the
since v can be upper bounded as v c n using the feasible point ( cid : 123 ) w = 123 and ( cid : 123 ) = 123 in ( 123 ) - ( 123 ) or ( 123 ) - ( 123 ) , the theorem directly leads to the conclusion that the maximum number of constraints in k scales linearly with the number of training examples n .
furthermore , it scales only polynomially with the length of the sequences , since r is polynomial in the length of the sequences .
while the number of constraints can potentially explode for small values of , ex - perience with support vector machines for classication showed that relatively large values of are sufcient without loss of generalization performance .
we will verify the efciency and the prediction performance of the algorithm empirically in the following .
to analyze the behavior of the algorithm under varying conditions , we constructed a synthetic dataset according to the following sequence and alignment model .
while this simple model does not exploit the exibility of the parameterized linear model d ( cid : 123 ) w ( s123 , s123 , a ) = ( cid : 123 ) wt ( s123 , s123 , a ) , it does serve as a feasibility check of the learning algorithm .
the native sequence and the decoys are generated by drawing randomly from a 123 letter alphabet = ( 123 , . . , 123 ) so that letter c has probability c / 123
each sequence has length 123 , and there are 123 decoys per native .
to generate the ho - molog , we generate an alignment string of length 123 consisting of 123 characters match , substitute , insert , delete .
for simplicity of illustration , substitutions are always c ( c mod 123 ) + 123
while we experiment with several alignment models , we only report typical results here where matches occur with probability 123 , substitutions with
number of training examples
figure 123 : left : train and test error rates for the 123 and the 123 parameter model depend - ing on the number of training examples .
right : typical learned substitution matrix after 123 training examples for the 123 - parameter model .
123 , insertion with 123 , deletion with 123 .
the homolog is created by applying the alignment string to a randomly selected substring of the native .
the shortening of the sequences through insertions and deletions is padded by additional random characters .
in the following experiments , we focus on the problem of homology prediction .
figure 123 shows training and test error rates for two models depending on the number of training examples averaged over 123 trials .
the rst model has only 123 parameters ( match , substitute , insert / delete ) and uses a uniform substitution matrix .
this makes the feature vectors ( c123 , c123 , ai ) three - dimensional with a 123 indicating the appro - priate operation .
the second model also learns the 123 parameters of the substitution matrix , resulting in a total of 123 parameters .
here , ( c 123 , c123 , ai ) indicates the element of the substitution matrix in addition to the operation type .
we chose c = 123 and = 123 .
the left - hand graph of figure 123 shows that for the 123 - parameter model , the generalization error is high for small numbers of training examples , but quickly drops as the number of examples increases .
the 123 - parameter model cannot t the data as well .
its training error starts out much higher and training and test error essentially converge after only a few examples .
the right - hand graph of figure 123 shows the learned matrix of substitution costs for the 123 - parameter model .
as desired , the elements of the ma - trix are close to zero except for the off - diagonal .
this captures the substitution model c ( c mod 123 ) + 123
figure 123 analyzes the efciency of the algorithm via the number of constraints that are added to k before convergence .
the left - hand graph shows the scaling with the number of training examples .
as predicted by theorem 123 , the number of constraints grows ( sub - ) linearly with the number of examples .
furthermore , the actual number of constraints encountered during any iteration of the algorithm is small enough to be handled by standard quadratic optimization software .
the right - hand graph shows how the number of constraints in the nal k changes with log ( ) .
the observed scaling appears to be better than suggested by the upper bound in theorem 123
a good value for is 123 .
we observed that larger values lead to worse prediction accuracy , while smaller values decrease efciency while not providing further benet .
number of training examples
figure 123 : number of constraints added to k depending on the number of training examples ( left ) and the value of ( right ) .
if not stated otherwise , = 123 , c = 123 , and n = 123
the paper presented a discriminative learning approach to inferring the cost parameters of a linear sequence alignment model from training data .
we proposed an algorithm for solving the resulting training problem and showed that it is computationally efcient .
experiments show that the algorithm can effectively learn the alignment parameters on a synthetic task .
we are currently applying the algorithm to learning alignment models for protein homology detection and protein alignment prediction .
