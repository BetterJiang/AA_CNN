linear support vector machines ( svms ) have become one of the most prominent machine learning techniques for high - dimensional sparse data commonly encountered in applica - tions like text classication , word - sense disambiguation , and drug design .
these applications involve a large number of examples n as well as a large number of features n , while each example has only s << n non - zero features .
this paper presents a cutting - plane algorithm for training lin - ear svms that provably has training time o ( sn ) for clas - sication problems and o ( sn log ( n ) ) for ordinal regression problems .
the algorithm is based on an alternative , but equivalent formulation of the svm optimization problem .
empirically , the cutting - plane algorithm is several orders of magnitude faster than decomposition methods like svm - light for large datasets .
categories and subject descriptors i . 123 ( articial intelligence ) : learning algorithms , performance , experimentation support vector machines ( svm ) , training algorithms , or - dinal regression , large - scale problems , roc - area
many applications of machine learning deal with problems where both the number of features n as well as the number of examples n is large ( in the millions ) .
examples of such problems can be found in text classication , word - sense dis - ambiguation , and drug design .
while problems of such size seem daunting at rst glance , the examples mentioned above have extremely sparse feature vectors , which gives hope that these problems can be handled eciently .
linear support vector machines ( svms ) are among the most prominent machine learning techniques for such high - dimensional and sparse data .
on text classication prob -
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page .
to copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specic permission and / or a fee .
kdd123 , august 123 , 123 , philadelphia , pennsylvania , usa .
copyright 123 acm 123 - 123 - 123 - 123 / 123 / 123 . . . $123 .
lems , for example , linear svms provide state - of - the - art pre - diction accuracy ( 123 , 123 , 123 ) .
while conventional training methods for linear svms , in particular decomposition meth - ods like svm - light ( 123 ) , smo ( 123 ) , libsvm ( 123 ) , and svm - torch ( 123 ) handle problems with a large number of features n quite eciently , their super - linear scaling behavior with n ( 123 , 123 , 123 ) makes their use inecient or even intractable on large datasets .
on the other hand , while there are train - ing methods that scale linear in n ( e . g .
( 123 , 123 , 123 , 123 ) ) , such methods empirically ( or at least in the worst case ) scale quadratically with the number of features n .
even more dicult is the current situation for training lin - ear ordinal regression svms ( or - svms ) ( 123 ) .
in herbrich et al . s formulation , an ordinal regression svm over n ex - amples is solved by translating it into a classication svm with o ( n123 ) examples , which obviously makes scaling with n even worse than for straightforward classication svms .
nevertheless , or - svm are very interesting even beyond ac - tual ordinal regression problems like those in information retrieval .
when applied to problems with only two ranks , or - svms are know to directly optimize the roc - area of the classication rule ( 123 , 123 ) .
this is a desirable criterion to optimize in many applications .
in this paper , we propose the rst general training algo - rithm for linear svms that provably scales o ( s n ) for clas - sication and o ( s n log ( n ) ) for ordinal regression , where s is the average number of non - zero features .
obviously , this scaling is very attractive for high - dimensional and sparse data .
the algorithm is based on an alternative , yet equiv - alent formulation of the svm training problem .
compared to existing methods , the algorithm has several advantages .
first , it is very simple and easy to implement .
second , it is several orders of magnitude faster than existing decompo - sition methods on large classication problems .
on a text classication problem with 123 , 123 examples and 123 , 123 fea - tures , the new algorithm is roughly 123 times faster than svm - light .
third , the algorithm has a meaningful stopping criterion that directly relates to training error .
this avoids wasting time on solving the optimization problem to a higher precision than necessary .
and , fourth , the algorithm can handle ordinal regression problems with hundred - thousands of examples with ease , while existing methods become in - tractable with only a few thousand examples .
structural svms
we rst introduce the formulation of the svm optimiza - tion problem that provides the basis of our algorithm , both for classication and for ordinal regression svms .
both for -
mulations are derived from structural svms ( 123 , 123 ) previ - ously used for predicting structured outputs and optimizing to multivariate performance measures .
for each alternative formulation , we will show that it is equivalent to the respec - tive conventional svm optimization problem .
for a given training set ( x123 , y123 ) , . . . , ( xn , yn ) with xi ( cid : 123 ) n and yi ( 123 , +123 ) , training a binary classication svm means solving the following optimization problem .
for sim - plicity of the following theoretical results , we focus on classi - cation rules hw ( x ) = sign ( wt x+b ) with b = 123
a non - zero b can easily be modeled by adding an additional feature of constant value to each x ( see e . g .
( 123 ) ) .
( classification svm ( primal ) )
i ( 123 , . . . , n ) : yi ( w
note that we adopted the formulation of ( 123 , 123 ) where
i is divided by n to better capture how c scales with the
training set size .
most training algorithms solve either op123 or its dual ( see ( 123 ) for the dual ) .
the algorithm we explore in the following considers a dif - ferent optimization problem , which was proposed for train - ing svms to predict structured outputs ( 123 ) and to optimize multivariate performance measures like f123 - score or the pre - cision / recall break - even point ( 123 ) .
the following is a spe - cialization of this formulation for the case of error rate , and we will refer to it as the structural formulation .
( structural classification svm ( primal ) )
while op123 has 123n constraints , one for each possible vec - tor c = ( c123 , . . . , cn ) ( 123 , 123 ) n , it has only one slack variable that is shared across all constraints .
each constraint in this structural formulation corresponds to the sum of a sub - set of constraints from op123 , and the ci select the subset .
i=123 ci can be seen as the maximum fraction of training errors possible over each subset , and is an upper bound on the fraction of training errors made by hw .
interestingly , op123 and op123 are equivalent in the following sense .
theorem 123
any solution w of op123 ( and vice versa ) , with
of op123 is also a solution
adapting the proof from ( 123 ) , we will show that both optimization problems have the same objective value and an equivalent set of constraints .
in particular , for every
w the smallest feasible andi i are related as = 123 n i i .
for a given w , the i in op123 can be optimized individually , and the optimum is achieved for i = max ( 123 , 123yi ( wt xi ) ) .
for op123 , the optimal for a given w is
since the function is linear in ci , each ci can be optimized
min c =
xi ) ( cid : 123 ) = min
therefore , the objective functions of both optimization prob - lems are equal for any w given the optimal and i , and consequently so are their optima .
the theorem shows that it is possible to solve op123 instead of op123 to nd the same soft - margin hyperplane .
while op123 does not appear particularly attractive at rst glance , we will show in section 123 that its wolfe dual has desirable sparseness properties .
before we show that a similar formu - lation also exists for ordinal regression svms ( 123 ) , we rst state the wolfe dual of op123 , since it will be referred to later .
denote with xc the sum 123 l123 - norm of c ( i . e .
the number ones in c for binary c ) .
i=123 ciyixi and with ||c||123 the
( structural classification svm ( dual ) )
123 ( cid : 123 ) c ( 123 , 123 ) n ( cid : 123 ) c ( cid : 123 ) ( 123 , 123 ) n
123 ordinal regression
in ordinal regression , the label yi of an example ( xi , yi ) indicates a rank instead of a nominal class .
without loss of generality , let yi ( 123 , . . . , r ) so that the values 123 , . . . , r are related on an ordinal scale .
in the formulation of herbrich et al .
( 123 ) , the goal is to learn a function h ( x ) so that for any pair of examples ( xi , yi ) and ( xj , yj ) it holds that
h ( xi ) > h ( xj ) yi > yj .
given a training set ( x123 , y123 ) , . . . , ( xn , yn ) with xi ( cid : 123 ) n and yi ( 123 , . . . , r ) , herbrich et al .
formulate the following or - dinal regression svm ( or - svm ) .
denote with p the set of pairs ( i , j ) for which example i has a higher rank than example j , i . e .
p = ( ( i , j ) : yi > yj ) , and let m = |p| .
( ordinal regression svm ( primal ) )
( i , j ) p : ( w
xj ) + 123ij
intuitively , this formulation nds a large - margin linear function h ( x ) that minimizes the number of pairs of training examples that are swapped w . r . t .
their desired order .
like for classication svms , op123 is a convex quadratic program .
ordinal regression problems have applications in learning re - trieval functions for search engines ( 123 ) .
furthermore , if the labels y take only two values , op123 optimizes the roc - area of the classication rule ( 123 , 123 ) .
in general , op123 has m o ( n123 ) constraints and slack vari - ables .
while this problem can be brought into the same form as op123 by rewriting the constraints as wt ( xixj ) 123ij , even relatively small training sets with only a few thousand
examples are already intractable for conventional training methods .
so far , researchers have tried to cut down on the number of constraints with various heuristics ( 123 ) which , however , cannot guarantee that the computed solution is
we will now derive a similar structural formulation of the ordinal regression svm as we have already done for the bi - nary classication svm .
( structural ord .
svm ( primal ) )
( i , j ) p cij ( 123 , 123 ) :
cij ( xixj ) 123
like for classication , the structural formulation has o ( 123n ) constraints , but only a single slack variable .
analogous to the classication svm , the following theorem establishes that both formulations of the or - svm have equivalent so -
theorem 123
any solution w of op123 ( and vice versa ) , with
of op123 is also a solution
as mentioned above , the constraints in op123 can be rewritten as yij ( wt ( xi xj ) ) 123 ij with all yij set to 123
theorem 123 applies immediately after substituting xij = xi xj , since op123 has the same form as op123 , and op123 has the same form as op123
cutting - plane algorithm
while the alternative formulations of the classication and the ordinal regression svm from above have an exponential number of constraints , they are very convenient in several
first , there is a single slack variable that measures train - ing loss , and there is a direct correspondence between and the ( in ) feasibility of the set of constraints .
in particular , if we have a point ( w , ) which fullls all constraints up to precision , then ( w , + ) is feasible .
so , the approxima - tion accuracy of an approximate solution to op123 or op123 is directly related to training loss , which provides an intuitive
second , op123 and op123 are special cases of structural svms ( 123 , 123 ) .
as we will detail below , for this type of formula - tion it is possible to prove bounds on the sparsity of an - approximate solution of the wolfe dual .
in particular , we will show that the sparsity is independent of the training set size n , and that simple cutting - plane algorithms ( 123 ) nd an - approximate solution in a constant number of iterations for both op123 or op123
algorithm 123 is our adaptation of the cutting - plane al - gorithm for the classication svm optimization problem it is an adaptation of the structural svm training algorithm ( 123 , 123 ) .
the algorithm iteratively constructs a sucient subset w of the set of constraints in op123
the al - gorithm starts with an empty set of constraints w .
in each iteration , it rst computes the optimum over the current working set w ( i . e .
w = 123 and = 123 in the rst iteration ) in line 123
in lines 123 - 123 it then nds the most violated con - straint in op123 and adds it to the working set w in line 123
algorithm 123 for training classication svms via op123
123 : input : s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) , c ,
( w , ) argminw , 123
123 wt w + c
for i=123 , . . . , n do
cw : 123 ci ( cid : 123 ) 123 yi ( wt xi ) < 123
123 : w w ( c ) 123 : until 123
ciyi ( wt xi ) ) +
note that this assignment to ( c123 , . . . , cn ) = c corresponds to the constraint in op123 that requires the largest to make it feasible given the current w , i . e .
c = argmax
the algorithm then continues in line 123 by optimizing over the new working set , unless the most violated constraint is not violated by more than the desired precision .
in the following , we will analyze the correctness and the time complexity of the algorithm .
we will show that the algorithm always terminates after a polynomial number of iterations that does not depend on the size n of the train - ing set .
regarding its correctness , the following theorem characterizes the accuracy of the solution computed by al -
theorem 123
( correctness of algorithm 123 )
for any training sample s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) and any > 123 , if ( w ) is the optimal solution of op123 , then algorithm 123 returns a point ( w , ) that has a better objective ) , and for which ( w , + ) is feasible in value than ( w
we rst verify that lines 123 - 123 compute the vector
c ( 123 , 123 ) n that maximizes
is the minimum value needed to fulll all constraints in op123 for the current w .
since the function is linear in ci , each ci can be maximized independently .
this directly corresponds to the assignment in line 123
as checked in line 123 , the algorithm terminates only if ( cid : 123 ) not exceed the from the solution over w by more than as
since the ( w , ) returned by algorithm 123 is the solu - tion on a subset of the constraints from op123 , it holds that
+ c 123
123 wt w + c .
using a stopping criterion based on the accuracy of the training loss is very intuitive and practically meaningful ,
unlike the stopping criteria typically used in decomposition intuitively , can be used to indicate how close one wants to be to the error rate of the best hyperplane .
in most machine learning applications , tolerating a training error that is suboptimal by 123% is very acceptable .
this intuition makes selecting the stopping criterion much easier than in decomposition methods , where it is usually dened based on the accuracy of the kuhn - tucker conditions of the dual ( see e . g .
solving op123 to an arbitrary but xed precision of is essential in our analysis below , making sure that computation time is not wasted on computing a solution that is more accurate than necessary .
we next analyze the time complexity of algorithm 123
it is easy to see that each iteration of the algorithm takes poly - nomial time , and that time scales linearly with n and s .
we then show that the number of iterations until convergence is bounded , and that this upper bound is independent of n .
lemma 123
each iteration of algorithm 123 takes time o ( sn )
for a constant working set size |w| .
each dot - product in lines 123 and 123 takes time o ( s ) when using sparse vector algebra , and n dot - products are computed in each line .
instead of solving the primal quadratic program , one can instead solve the dual op123 in line 123
setting up the dual over w in line 123 is dominated by computing the o ( |w|123 ) elements of the hessian , which can be done in o ( |w|123sn ) after rst computing 123 for each constraint in w .
note that n sn .
the time for solving the dual is then independent of n and s .
this leads to an overall time complexity of o ( sn ) per iteration .
lemma 123
for any > 123 , c > 123 , and any training sam - ple s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) , algorithms 123 and 123 termi - nate after at most
iterations .
r = maxi ||xi|| for algorithm 123 and for algo - rithm 123 it is r = 123 maxi ||xi|| .
following the proof scheme in ( 123 , 123 ) , we will show that adding each new constraint to w increases the objective value at the solution of the quadratic program in line 123 by at least some constant positive value .
since the objective value of the solution of op123 is upper bounded by c ( since w = 123 and = 123 is a feasible point in the primal ) , the algorithm can only perform a constant number of itera - tions before termination .
the amount by which the solution increases by adding one constraint that is violated by more then ( i . e .
the criteria in lines 123 and 123 respectively ) to w is characterized by proposition 123 in ( 123 ) .
a lower bound on the increase is
where q is an upper bound on the l123 - norm of the coecient vectors in the constraints .
for op123
in the case of algorithm 123 and for op123
q = max
cij ( xixj ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
q = max
cij 123 max
in the case of algorithm 123
due to this constant increase of the objective value in each iteration , either algorithm can
add at most max 123
value exceeds c , which is an upper bound on the objective value at the solution of op123 and op123
123 constraints before the objective
note that the formulation of op123 with the scaled c
stead of c in the objective is essential for this lemma .
we will empirically evaluate the adequacy of this scaling in sec - tion 123
putting everything together leads to the following bound on the time complexity of algorithm 123
theorem 123
( time complexity of algorithm 123 )
for any distribution p ( x , y ) that generates feature vectors of bounded l123 - norm ||x|| and any xed value of c > 123 and > 123 , algorithm 123 has time complexity o ( sn ) for any train - ing sample of size n and sparsity s .
lemma 123 bounds the number of iterations ( and therefore the maximum working set size |w| ) to a constant that is independent of n and s .
each iteration has time complexity o ( sn ) as established by lemma 123
to our knowledge , algorithm 123 has the best scaling be - havior of all known training algorithms for linear svms .
de - composition methods like svm - light ( 123 ) , smo ( 123 ) , lib - svm ( 123 ) , and svmtorch ( 123 ) handle sparse problems with a large number of features n quite eciently .
however , their super - linear scaling behavior with n ( 123 , 123 , 123 ) makes them inecient or even intractable on large datasets .
we will compare our algorithm against svm - light as a representa - tive decomposition methods .
other methods sacrice the statistical robustness ( 123 ) of
the i loss in the objective for the numerically more con - i loss .
with additional restrictions on how the data is normalized , core vector machines ( 123 ) are shown to scale linear in n .
however , the restrictions make the method inapplicable to many datasets .
generally applicable are la - grangian svm ( 123 ) ( using the 123 loss ) , proximal svm ( 123 ) ( using an l123 regression loss ) , and interior point meth - ods ( 123 ) .
while these method scale linearly with n , they use the sherman - morrison - woodbury formula for inverting the hessian of the dual .
this requires operating on n n ma - trices , which makes them applicable only for problems with small n .
as a representative of this group of methods , we will compare against the lagrangian svm in section 123
the recent l123 - svm - mfn method ( 123 ) avoids explicitly representing n n matrices using conjugent gradient tech - niques .
while the worst - case cost is still o ( sn min ( n , n ) ) per iteration , they observe that their method empirically scales better .
we will compare against this method as well .
123 ordinal regression
algorithm 123 solves the ordinal regression svm in the form of op123 and has a structure that is very similar to algo - rithm 123
it is a generalization of the algorithm for optimiz - ing roc - area in ( 123 ) and similar to the algorithm indepen - dently developed in ( 123 ) .
the key dierence to algorithm 123 lies in computing the most violated constraint of op123
without enumerating all m o ( n123 ) constraints from op123
to avoid o ( n123 ) cost , algorithm 123 makes use of a condensed
while ( j n ) ( wt xi wt xj < 123 ) do
j + ( nr a + 123 )
if yj < r then
b + +; c
123 : w w ( ( c+ , c
i ) 123
a + +; c+
algorithm 123 for training ord .
svms via op123
123 : input : s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) , c ,
( w , ) argminw , 123
123 wt w + c sort s by decreasing wt xi c+ 123; c nr number of examples with yi = r for r = 123 , . . . , r do i 123; j 123; a 123; b 123 while i n do if yi = r then
i ) xi 123
i ) ( wt xi ) +
representation of the constraints as follows .
while the left - hand side of the linear constraints in op123 contains a sum over m vectors of dierences ( xi xj ) , most individual vec - tors xi are added and subtracted multiple time .
with proper j , each constraint can be rewritten as a sum of n vectors
i and c
i ) xi 123
if c+ and c
is the number of times xi occurs with positive sign ( i . e .
cij = 123 ) and c is the number of times xi oc - curs with negative sign ( i . e .
cji = 123 ) .
known , each constraint can be evaluated in time o ( sn ) in - stead of o ( sm ) .
furthermore , the right hand side of each
constraint can be computed from c+ and c in time o ( n ) i + c instead of o ( m ) , since 123 i ) .
the following theorem shows that algorithm 123 computes the co -
ecient vectors c+ and c of the most violated constraint , and therefore converges to the optimal solution in the same sense as algorithm 123
i=123cij = 123
theorem 123
( correctness of algorithm 123 )
for any training sample s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) and any > 123 , if ( w ) is the optimal solution of op123 , then algorithm 123 returns ( w , ) that have a better objective value
) , and for which ( w , + ) is feasible in op123
analogous to the proof of theorem 123 ,
is reached for
cij ( cid : 123 ) 123 ( wt xi ) ( wt xj ) < 123
this means that the number of times xi enters with positive and negative sign is
i = | ( j : ( yi > yj ) ( ( w i = | ( j : ( yj > yi ) ( ( w
xj ) < 123 ) ) | , xi ) < 123 ) ) | .
to compute these quantities eciently , algorithm 123 rst sorts the training examples by decreasing value of wt xi .
then , for each rank r in turn , it updates the values of c+ and c+ for all constraints ( i , j ) p in op123 with yi = r .
by going through the examples in order of wt xi , the algorithm can keep track of
a = |l : ( yl = r ) ( w b = |l : ( yl < r ) ( w
xl > w xl > w
via incremental updates .
whenever it encounters an exam - ple with yi = r , there are exactly b constraints ( i , j ) p in op123 with yj < r and ( ( wt xi ) ( wt xj ) < 123 ) .
similarly , whenever it encounters an example with yj < r , there are ex - actly ( nr a ) constraints ( i , j ) p in op123 with yi = r and ( ( wt xi ) ( wt xj ) < 123 ) .
by exhaustively going through all r , yi = r , and yj < r and adding the respective quantities to i and c j , the algorithm implicitly considers all constraints
like algorithm 123 , the iteration terminate only if no con -
straint in op123 is violated by more than , and
+ c 123
since w is a subset of the constraints in op123
the following lemma characterizes the time algorithm 123
takes in each iteration as a function of n and s .
lemma 123
each iteration of algorithm 123 requires time o ( sn + n log ( n ) + rn ) for a constant working set size |w| .
the proof is analogous to that of lemma 123
the greatest expense per iteration in terms of n is the sort in line 123 and the computation of n inner products wt xi .
lines 123 - 123 take r 123 passes through the training set .
due to the condensed representation , setting up the quadratic program in line 123 can again be done in time o ( |w|123sn ) analogous to lemma 123
lemma 123 already established an upper bound on the num - ber of iterations of algorithm 123
analogous to theorem 123 , the following characterizes its the scaling behavior .
theorem 123
( time complexity of algorithm 123 )
for any distribution p ( x , y ) that generates feature vectors of bounded l123 - norm ||x|| and any xed value of c > 123 and > 123 , algorithm 123 has time complexity o ( sn log ( n ) ) for any training sample of size n and sparsity s .
note that conventional methods for training ordinal re - gression svms based on op123 have much worse scaling be - havior .
they scale roughly o ( sn123 ) even under the ( opti - mistic ) assumption that a problem with m constraints can be solved in o ( m ) time .
only small training sets with hun - dreds or at best a few thousand examples are tractable .
table 123 : training time in cpu - seconds .
heuristic approaches for pushing the limits by removing con - straints oer no performance guarantees ( 123 ) .
we will see in the following experiments that algorithm 123 can handle problems with hundred - thousands of examples with ease .
while theorems 123 and 123 characterize the asymptotic scal - ing of algorithms 123 and 123 , the behavior for small sample sizes may be dierent .
we will empirically analyze the scal - ing behavior in the following experiments , as well as its sensi - tivity to c and .
furthermore , we compare the algorithms against existing methods , in particular the decomposition
we implemented algorithms 123 and 123 using svm - light as the basic quadratic programming software that is called in line 123 of each algorithm .
however , other quadratic pro - gramming tools would work just as well , since |w| remained small in all our experiments .
we will refer to our implemen - tation of algorithms 123 and 123 as svm - perf in the following .
svm - perf is available at http : / / svmlight . joachims . org .
we use 123 datasets in our experiments , selected to cover a
wide range of properties .
first , we consider the binary text classication task ccat from the reuters rcv123 collection123 ( 123 ) .
there are 123 , 123 examples split into 123 , 123 training and 123 , 123 test examples , and there are 123 , 123 features with sparsity 123% .
this task has an almost balanced
second , we include the task c123 from the rcv123 collec - tion , since it has an unbalanced class ratio .
existing decomposition methods like svm - light are know to run faster for unbalanced tasks .
the third problem is classifying abstracts of scientic papers from the physics arxiv by whether they are in the astro - physics section .
we picked this task since it has a large number of features ( 123 , 123 ) with high sparsity ( 123% ) .
there are 123 , 123 examples split into 123 , 123 training examples and 123 , 123 test examples .
the fourth problem is class 123 in the covertype dataset123 of blackard , jock & dean , which is comparably low - dimensional with 123 features and a sparsity of 123% .
there are 123 , 123 examples which we split into 123 , 123 training examples and 123 test examples .
finally , we added the kdd123 physics task from the kdd - cup 123 ( 123 ) , with 123 features ( sparsity 123% )
lyrl123 rcv123v123 readme . htm
and 123 , 123 examples , which are split into 123 , 123 train - ing examples and 123 , 123 test examples .
we use the precision / recall break - even point ( prbep ) ( 123 ) ) as the measure of performance for the text - classication tasks , and accuracy for the other problems .
the following parameters are used in our experiments , unless noted otherwise .
both svm - light and svm - perf use = 123 ( note that their interpretation of is dif - ferent , though ) .
as the value of c , we use the setting that achieves the best performance on the test set when using the full training set ( c = 123 , 123 for reuters ccat , c = 123 , 123 for reuters c123 , c = 123 , 123 for arxiv astro - ph , c = 123 , 123 , 123 for covertype 123 , and c = 123 , 123 for kdd123 physics ) .
whenever possible , runtime comparisons are done on the full set of examples , joining training and test data together to get larger datasets .
experiments that com - pare prediction performance report results for the standard test / training split .
all experiments are run on 123 mhz intel xeon processors with 123gb main memory under linux .
123 how fast are the algorithms compared
to existing methods ?
table 123 compares the cpu - time of svm - perf and svm - light on the full data for the 123 tasks described above .
for the classication svm , svm - perf is substantially faster than svm - light on all problems , achieving a speedup of several orders of magnitude on most problems .
we will analyze these results in detail in the following sections .
we also applied the ordinal regression svm to these data - sets , treating the binary classication problems as ordinal problems with two classes .
an alternative view on this setup is that the or - svm learns a classication rule that optimizes roc area ( 123 , 123 ) .
the runtimes are somewhat slower than for classication , but still very tractable .
we tried to train svm - light in its ordinal regression mode on these problems as well .
however , training with svm - light is intractable with more than 123 , 123 examples .
a method that was recently proposed for training linear svms is the l123 - svm - mfn algorithm ( 123 ) .
while they do not provide an implementation of their method , they re - port training times for the two publicly available dataset adult and web in the version produced by john platt .
on the adult data with 123 , 123 examples and 123 features , they report a training time of 123 cpu - seconds for the value of c = 123 123 , 123 achieving optimal cross - validation error , which is comparable to 123 cpu - seconds needed by svm - perf for c = 123 123 , 123 as recommended by platt .
sim - ilarly , for the web data with 123 , 123 examples and 123 fea - tures , l123 - svm - mfn is reported to take 123 cpu - seconds ( c = 123 123 , 123 ) while svm - perf takes 123 cpu - seconds for the same value of c .
while both methods seem to perform comparably for these rather small training sets , it is unclear
svm - perf ( class
svm - perf ( ord .
regr . )
123 123 123 123e+123
123 123 123 123e+123
123 123 123 123e+123
123 123 123 123e+123
number of training examples
number of training examples
number of training examples
number of training examples
figure 123 : training time of svm - perf ( left ) and svm - light ( left - middle ) for classication as a function of n for the value of c that gives best test set performance for the maximum training set size .
the middle - right plot shows training time of svm - perf for the value of c with optimum test set performance for the respective training set size .
the right - most plot is the cpu - time of svm - perf for ordinal regression .
how l123 - svm - mfn scales .
in the worst case , the authors conclude that each iteration may scale o ( sn min ( n , n ) ) , al - though practical scaling is likely to be substantially better .
finally , note that l123 - svm - mfn uses squared slack vari - i to measure training loss instead of linear slacks
i like in svm - light and svm - perf .
mfn , the lsvm uses squared slack variables 123
the lagrangian svm ( lsvm ) ( 123 ) is another method par - ticularly suited for training linear svms .
like the l123 - svm - i to mea - sure training loss .
the lsvm can be very fast if the number of features n is small , scaling roughly as o ( nn 123 ) .
we ap - plied the implementation of mangasarian and musicant123 to the adult and the web data using the values of c from above .
with 123 cpu - seconds , the training time of the lsvm is still comparable on adult .
for the higher - dimensional web task , the lsvm runs into convergence problems .
apply - ing the lsvm to tasks with thousands of features is not tractable , since the algorithm requires storing and inverting an n n matrix .
123 how does training time scale with the
number of training examples ?
figure 123 shows log - log plots of how cpu - time increases with the size of the training set .
the left - most plot shows the scaling of svm - perf for classication , while the left - middle plot shows the scaling of svm - light .
lines in a log - log plot correspond to polynomial growth o ( nd ) , where d cor - responds to the slope of the line .
the middle plot shows that svm - light scales roughly o ( n123 ) , which is consistent with previous observations ( 123 ) .
svm - perf has much better scaling , which is ( to some surprise ) better than linear with roughly o ( n123 ) over much of the range .
figure 123 gives insight into the reason for this scaling be - havior .
the graph shows the number of iterations of svm - perf ( and therefore the maximum number of constraints in the working set ) in relation to the training set size n .
turns out that the number of iterations is not only upper bounded independent of n as shown in lemma 123 , but that
number of training examples
figure 123 : number of iterations of svm - perf for clas - sication as a function of sample size n .
it does not grow with n even in the non - asymptotic region .
in fact , for some of the problems the number of iterations decreases with n , which explains the sub - linear scaling in cpu - time .
another explanation lies in the high xed cost that is independent of n , which is mostly the cost for solving a quadratic program in each iteration .
since lemma 123 identies that the number of iterations depends on the value of c , scaling for the optimal value of c might be dierent if the optimal c increases with training set size .
to analyze this , the middle - right plot of figure 123 shows training time for the optimal value of c .
while the curves look more noisy , the scaling still seems to be roughly
finally , the right - most plot in figure 123 shows training time of svm - perf for ordinal regression .
the scaling is slightly steeper than for classication as expected .
the num - ber of iterations is virtually identical to the case of classi - cation shown in figure 123
note that training time of svm - light would scale roughly o ( n123 ) on this problem .
figure 123 : dierence in prediction performance be - tween svm - perf and svm - light for classication as a function of c .
123 is the prediction performance of svm -
perf different from svm - light ?
one potential worry is that the speedup of svm - perf over svm - light somehow comes at the expense of prediction ac - curacy , especially due to the choice of = 123 .
however , this is not the case .
figure 123 shows the dierence in test set accuracy / prbep between the classiers produced by svm - light and svm - perf .
for better readability , the dif - ference is shown in terms of percentage points .
a positive value indicates that svm - perf has higher prediction perfor - mance , a negative value indicates that svm - light performs better .
for almost all values of c both methods perform al - most identically .
in particular , there is not indication that the rules learned by svm - perf are less accurate .
one case where there is a large dierence is the covertype 123 task for small values of c , since svm - light stops before fully con - 123 how small does need to be ?
the previous section showed that = 123 is sucient to get prediction accuracy comparable to svm - light .
but maybe a lower precision would suce and reduce training time ? figure 123 shows the dierence ( in percentage points ) in prediction accuracy / prbep compared to the performance svm - perf reaches for = 123 .
values above ( below ) 123 indicate that the accuracy of svm - perf for that is better ( worse ) than the accuracy at = 123 .
the graph shows that for all 123 the prediction performance is within half a percentage point .
for larger values of the resulting rules are starting to have more variable and less reliable performance .
so , overall , = 123 seems accurate enough with some safety margin .
however , one might elect to use larger at the expense of prediction accuracy , if training time was substantially faster .
we will evaluate this next .
123 how does training time scale with ?
lemma 123 indicates that the number of iterations , and therefore the training time , should decrease as increases .
figure 123 shows number of iterations as a function of .
in - terestingly , the empirical scaling of roughly o ( 123 123 ) is much better than o ( 123 123 ) in the bound from lemma 123
for training time , as shown in figure 123 , the scaling is o ( 123
figure 123 : dierence in accuracy or prbep of svm - perf compared to its performance at = 123 as a function of .
figure 123 : number of iterations of svm - perf for clas - sication as a function of .
figure 123 : cpu - time of svm - perf for classication as a function of .
figure 123 : relative dierence between the objective value of the svm - perf solution for classication and the ( approximately ) true solution as a function of c ( left ) ( with = 123 ) and as a function of ( right ) ( with c is set to maximize test set prediction performance ) .
could the runtime of svm - light be improved by increas - ing the value of as well ? while svm - light converges faster for larger values of , the dierence is much smaller .
even when increasing to 123 , the speedup is less than a factor of 123 on all ve problems .
123 is the solution computed by svm - perf
close to optimal ?
while we have already established that training with = 123 gives rules of comparable prediction accuracy , it is also interesting to look at how close the objective value of the relaxed solution is to the true optimum for dierent values of .
as theorems 123 and 123 show , the objective value is lower than the true objective , but by how much ? figure 123 shows the relative dierence
between the solution of svm - perf and a high - precision solu - tion computed by svm - light .
the left - hand plot of figure 123 indicates that for = 123 the relative error is roughly be - tween 123% and 123% over all values of c .
the missing points correspond to values where svm - light failed to converge .
the right - hand plot shows how the relative error decreases 123 how does training time scale with c ?
lets examine how the number of iterations of svm - perf scales with the value of c .
the upper bound of lemma 123 suggest a linear scaling , however , figure 123 shows c ) for clas - that the actual scaling is much better with o ( sication ( and similarly for ordinal regression ) .
figure 123 shows the resulting training times ( left ) and compares them against those of svm - light ( right ) .
except for excessively large values of c , the training time of svm - perf scales sub - linearly with c .
note that the optimal values of c lie be - tween 123 , 123 and 123 , 123 for all tasks except covertype 123
for all values of c , svm - perf is faster than svm - light .
figure 123 : number of iterations of svm - perf as a function of c .
we presented a simple cutting - plane algorithm for train - ing linear svms that is shown to converge in time o ( sn ) for classication and o ( sn log ( n ) ) for ordinal regression .
it is based on an alternative formulation of the svm optimiza - tion problem that exhibits a dierent form of sparsity com - pared to the conventional formulation .
the algorithm is em - pirically very fast and has an intuitively meaningful stopping
the algorithm opens several areas for research .
since it takes only a small number of sequential iterations through the data , it is promising for parallel implementations using out - of - core memory .
also , the algorithm can in principle be applied to svms with kernels .
while a straightforward implementation is slower by a factor of n , matrix approxi - mation techniques and the use of sampling might overcome
