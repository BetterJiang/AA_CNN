we compare two recently proposed frameworks for combin - ing generative and discriminative probabilistic classiers and apply them to semi - supervised classication .
in both cases we explore the tradeo between maximizing a discriminative likelihood of labeled data and a generative likelihood of la - beled and unlabeled data .
while prominent semi - supervised learning methods assume low density regions between classes or are subject to generative modeling assumptions , we con - jecture that hybrid generative / discriminative methods allow semi - supervised learning in the presence of strongly overlap - ping classes and reduce the risk of modeling structure in the unlabeled data that is irrelevant for the specic classication task of interest .
we apply both hybrid approaches within naively structured markov random eld models and pro - vide a thorough empirical comparison with two well - known semi - supervised learning methods on six text classication tasks .
a semi - supervised hybrid generative / discriminative method provides the best accuracy in 123% of the experi - ments , and the multi - conditional learning hybrid approach achieves the highest overall mean accuracy across all tasks .
categories and subject descriptors
i . 123 ( articial intelligence ) : learning
algorithms , experimentation , performance
semi - supervised learning , hybrid generative / discriminative methods , text classication
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page .
to copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specic permission and / or a fee .
kdd123 , august 123 , 123 , san jose , california , usa .
copyright 123 acm 123 - 123 - 123 - 123 - 123 / 123 / 123 . . . $123 .
most machine learning methods rely on the availability of large labeled datasets .
however , human annotation is time - consuming , making labeled data costly to obtain in practice .
motivated by this problem , researchers have proposed semi - supervised learning methods that leverage large amounts of relatively inexpensive unlabeled data along with small amounts of labeled data .
the increasing interest in apply - ing machine learning to new domains and the vast availabil - ity of unlabled data from the web and elsewhere are driving interest in semi - supervised learning .
semi - supervised learning is especially relevant for applica - tions in data mining , as when initially analyzing data from a new domain , obtaining any labeled data requires labori - ous human annotation .
the lowest eort approach to data mining would use unsupervised learning .
however , super - vised learning methods typically provide better , more task -
for example , consider the problem of classifying messages as belonging to a mac hardware or pc hardware newsgroup .
although there are word features in the data relevant to this task ( such as powerbook indicating mac , or dell indicat - ing pc ) , because mac and pc postings have high word overlap , an unsupervised clustering algorithm could discover many dierent ways to partition this data .
for example , messages about hard drives or networking may appear as clusters , but these clusters may not be directly relevant to the classica - tion task of interest .
if posed as a supervised classication task , however , with labeled examples from each newsgroup , the classier will learn to focus on the features relevant to the mac pc task , and make the desired separation .
training methods for machine learning classiers are often characterized as being generative or discriminative .
gen - erative training estimates the joint distribution of all vari - ables , both the classes and the input data , while discrim - inative training is concerned only with the decision bound - ary .
classiers trained discriminatively seem to have lower asymptotic error than analogous generatively - trained classi - ers because , intuitively , they are able to focus limited rep - resentational capacity on predicting just the class variable .
however , discriminative approaches do not always provide the highest accuracy .
when the amount of training data is small , generative training can provide better accuracy even when the model is not a very good t to the data .
ng and jordan demonstrate this , comparing naive bayes and logistic
motivated by these observations , several researchers have proposed hybrid methods that combine generative and dis - criminative training .
these hybrid methods have delivered promising results in the domains of text classication ( 123 , 123 ) , pixel classication ( 123 ) , and object recognition ( 123 , 123 ) ,
a variety of semi - supervised techniques have been de - veloped for both generative and discriminative models .
a straightforward , generative semi - supervised method is the expectation maximization ( em ) algorithm .
the em ap - proach for naive bayes text classication models is discussed by nigam et al .
in ( 123 ) .
generative semi - supervised meth - ods rely on a model for the distribution of the input data , and can fail either when this model is wrong , or when the structure of the input data is not correlated with the clas - sication task ( as illustrated in the mac - pc example above ) .
discriminative semi - supervised methods , including proba - bilistic and non - probabilistic approaches , such as transduc - tive or semi - supervised support vector machines ( tsvms , s123vms ) ( 123 , 123 ) and a variety of other graph based meth - ods ( 123 , 123 ) assume high density within class and low density between classes , and can fail when the classes are strongly overlapping .
hence , these approaches for semi - supervised learning in discriminative classiers also use model assump - tions about the structure of the input data , but typically do not encode these assumptions as explicit models of input
in this paper , we apply hybrid generative / discriminative methods to semi - supervised learning .
we compare two re - cently proposed approaches to combining generative and dis - criminative methods in detail .
the rst is multi - conditional learning ( 123 ) , a class of training objective functions com - posed of the product of multiple likelihoods that share one set of parameters and are derived from an underlying joint model .
we formulate the semi - supervised training problem in terms of the optimization of a multi - conditional objective function that is a weighted combination of a discriminative likelihood of labeled data and a marginal likelihood of both labeled and unlabeled data .
we also consider a framework proposed by lasserre et al .
( 123 ) which we henceforth refer to as the parameter coupling prior 123 method .
in this approach , the discriminative and generative components derived from a common joint model have separate sets of parameters .
these parameters are coupled by a prior distribution that species how one set of parameters inuences the other .
both of these hybrid approaches can be interpreted as dis - criminative classiers trained using the marginal likelihood of the input data as parameter regularization .
we conjec - ture that for many problems this form of regularization is more helpful than typical discriminative regularization ap - proaches penalizing decision boundaries passing through re - gions of high marginal density .
in contrast , these gener - ative / discriminative hybrids are not constrained to avoid low - density regions between classes when placing decision boundaries .
additionally , they are able to balance between leveraging innate clusters in the input data ( which may or may not be useful ) and task - specic evidence from the la - beled data ( which may or may not be representative ) .
hy - brid methods can avoid relying on generative modeling as -
123we note that parameter coupling prior is a short name we devised to refer to the work of lasserre , bishop , and minka .
this term is not used in their paper .
sumptions by emphasizing the discriminative likelihood dur - ing maximization .
in summary , these methods allow us to avoid the assumptions of discriminative semi - supervised ap - proaches and mitigate the assumptions of generative semi - supervised methods .
by emphasizing each component of the objective function appropriately , they allow semi - supervised learning in cases that other methods fail .
in addition to the motivation provided above , the contri -
butions of this paper are :
we apply multi - conditional learning to semi - supervised
we compare the multi - conditional learning approach with a framework recently proposed by lasserre , et al .
in ( 123 ) .
we subject this method to much more thor - ough evaluation than was provided in ( 123 ) ( only one dataset , no comparisons to other methods ) .
we implement these model - independent approaches in a naively structured markov random eld model and derive the appropriate gradients .
we provide an empirical comparison of the two ap -
proaches along with two other prominent semi - supervised methods for classication .
a hybrid method outper - forms other methods in 123% of the experiments and the multi - conditional learning approach gives the highest overall mean accuracy .
two general generative -
first , we dene the learning problem .
suppose we have data d = dl du , where dl and du represent the labeled and unlabeled data , respectively .
each example in dl is a pair ( y , x ) , where the vector y has length equal to the number of classes and a 123 in the position corresponding to the index of the correct class ( other entries are 123 ) .
vector x has length equal to the number of features of the input and each position contains the value of a particular feature for this example .
in du , each example is only ( x ) , as the value of y is hidden .
in the case of document classication , for example , each example corresponds to a document , and each position in x might contain the number of times a particular
notice that x can be decomposed into pn n = p|x|
i wi , where i xi , so that each wi corresponds to the event of observing a feature .
vector wi has a single 123 in one position and 123 elsewhere .
for example , in document classication wi represents a word occurrence .
another occurrence of the same word in the document would correspond to separate event wk .
this decomposition of x into individual events is useful for understanding the graphical model introduced in section 123
first , we discuss two model - independent hybrid 123 multi - conditonal learning
multi - conditional learning ( 123 ) is a class of training objec - tive functions composed of the product of multiple weighted likelihoods , each with parameters derived from the same un - derlying joint model .
an advantage of the multi - conditional
framework is the exibility it allows to craft an objective function for a specic learning task .
for example , an objec - tive function composed of the product of weighted discrimi - native likelihoods for multiple tasks is a natural framework for transfer learning or multitask learning ( 123 ) .
mccallum et al .
( 123 ) combine discriminative and genera - tive likelihoods using the multi - conditional objective func -
p ( y |x; ) p ( x|y ; )
training text classication models with this objective func - tion was found to produce improvements in classication ac - curacy .
here , we express semi - supervised training in terms of a multi - conditional objective function by combining the weighted discriminative likelihood of the labeled data and the weighted marginal likelihood of labeled and unlabeled data .
this objective function is :
o ( ) = p ( yl|xl; ) p ( x; ) ,
where xl and yl denote the labeled data and the term p ( x; ) includes both labeled and unlabeled data .
it is convenient to maximize the natural log of o : lno ( ) = ln p ( yl|xl; ) + ln p ( x; )
we choose the model parameters that maximize o :
= arg max
in equation ( 123 ) , increasing gives more weight to the dis - criminative component during maximization , while increas - ing gives more weight to the generative component .
a practical concern is that each component and its gra - dient may be dierent in scale .
notice that p ( yl|xl; ) is a distribution is over the number of labels , and p ( x; ) is a distribution over the number of features .
this means that if the distributions were uniform the magnitude of the log - likelihood for the generative component would be much smaller than that of the discriminative component .
addi - tionally , in semi - supervised learning the number of labeled examples is typically much smaller than the number of un - labeled examples , so the sums inside each likelihood calcu - lation have a dierent number of terms .
this makes it di - cult to choose values of in an interpretable way .
choosing = 123 and = 123 does not correspond to maximizing with 123% of the weight on the discriminative component , as the discriminative gradient magnitudes tend to be larger than those of the generative component .
one potential solution to this problem is to normalize each of the components so that they have the same mag - nitude , and weight the normalized componenets .
in non - log space , normalizing each component corresponds to raising each component to a power x .
if x > 123 , then this makes the probability distribution more peaked , whereas if x < 123 , the probability distribution is attened .
since p ( yl|xl; ) is convex , stretching or attening it should make little dier - ence in terms of the ability of a gradient - based optimizer to nd the maximum .
however , p ( x; ) is not convex , and consequently attening it could actually change the maxi - mum found by the maximizer , if x is small enough to su - ciently smooth the distribution .
because the generative like - lihood is smaller in magnitude and attening it by raising it
to a power x < 123 may be detrimental , we avoid normaliza - tion and set = 123 and >> .
the dierence in the magnitude of the likelihoods could also cause maximization to appear to converge when one component conceals the changes in the other .
to deal with this issue , we adapt convergence criteria so that training converges only when both components , considered indepen - dently , have converged .
in addition to the terms above , we use a standard zero -
mean gaussian prior over parameters :
ln p ( ) ||||123
123 parameter coupling prior
in the approach of lasserre , et al .
( 123 ) , which again we refer to as the parameter coupling prior approach , the gen - erative and discriminative components have separate sets of parameters .
the two sets of parameters are jointly trained and are coupled using a prior distribution .
following ( 123 ) , we dene the joint distribution of features x , classes y , and parameters d and g ( for the discriminative and genera - tive models , respectively ) as :
p ( x , y , d , g ) = p ( d , g ) p ( y |x; d ) p ( x; g )
let us consider two special cases of priors .
if the prior p ( d , g ) constrains d = g , then we have a genera - tive model based on the joint distribution .
p ( x , y , g ) = p ( g ) p ( x , y ; g )
if the prior assumes that the two sets of parameters are independent p ( d , g ) = p ( d ) p ( g ) , then we have : p ( x , y , d , g ) = p ( d ) p ( y |x; d ) ( p ( g ) p ( x; g ) ) in other words , if the underlying joint model is such that the parameters of the marginal model and the conditional model are completely independent , then the terms inside the brackets are constant with respect to d , and hence play no role in classication .
therefore , this is a discriminative
a prior that imposes a soft constraint that the parameters must be similar allows blending of the generative and dis - criminative .
as in ( 123 ) , we couple the two sets of parameters by a prior of the form :
ln p ( d , g ) = ||d g||123
lasserre et al .
noted that the generative component p ( x; g ) can make use of unlabeled data , and can hence be used for semi - supervised learning .
experimental results on one dataset using the above prior demonstrated the potential for semi - supervised learning using this method .
it is important to note that the two approaches described above are model - independent because they only specify the form of the objective function .
we can derive concrete versions of the objective functions for a specic graphical model .
here , we apply them to a markov random eld
the gradient of ly|x with respect to parameters y is similar .
the log marginal likelihood for p ( x ) is
and has gradient
( exp ( yy + yt t
xy x ) ) ln z
( exp ( yy + yt t
xy x ) ) ln z
y ( exp ( yy + yt t
y ( exp ( yy + yt t
i ex , y ( xyt )
figure 123 : a factor graph for a naively - structured makrov random eld model .
( mrf ) model , an undirected graphical model .
the model is structured so that the input variables wi are conditionally independent given the class y .
this can be interpreted as an undirected analog to naive bayes models .
for this rea - son we refer to this specic structure as a naively structured mrf .
the factor graph for this model is shown in figure 123
we could also use these training objective functions in more complicated models with hidden topic variables or models
we estimate the parameters of the model by nding the parameters that maximize the objective functions dened in section 123
we use gradient methods to nd the max - imum .
below we dene these objective functions for the naively - structed mrf model , and compute the gradients .
the components of each objective are derived from the joint distribution of the model given by :
where z = p
p ( y , x ) =
y y + yt t
y y + yt t
xyx ) is a normalizing factor that ensures a true probability distribution .
first , we derive the gradient for the discriminative component of the objective function , the conditional log - likelihood ly|x = log p ( y|x ) , where y and x denote observations and
xy x ln z ( x ) ,
y y + yt t
exp ( yy + yt t
the gradient is then computed as
ex , y ( xyt ) ex
y exp ( yy + yt t
y exp ( yy + yt t
where ex ( ) denotes the expectation under p ( x ) and ex ( ) denotes an expectation under the empirical distribution of
for the multi - conditional objective function , the parameter matrices xy and y are the same for both the discrimina - tive and generative components .
for the parameter coupling prior method , xy and y are dierent for each component and are coupled by ( 123 ) .
the derivative of this prior with respect to each set of parameters is :
ln p ( d , g ) ||d g||123 ln p ( d , g ) = d g ln p ( d , g ) = g d
if = 123 in the multi - conditional approach or = in the parameter coupling priors approach , then the lx term drops out of the objective function and we only maximize ly|x which uses no unlabeled data .
maximizing only ly|x in the naive mrf model yields a maximum entropy classier ( 123 ) .
we use maximum entropy classiers as the supervised counterpart to our hybrid semi - supervised methods .
we use limited - memory - bfgs , a quasi - newton optimiza - tion method that has been shown to work well for maxi - mum entropy models ( 123 ) , in conjunction with the adapted converge criteria discussed in section 123 .
note that the marginal density p ( x ) is not convex , which means neither the multi - conditional nor the parameter coupling prior ob - jective functions are convex .
because l - bfgs is a convex optimizer , we converge to a local maximum .
empirically , we have found that l - bfgs requires fewer training iterations converges to better maxima than other convex optimizers .
using a method to specically address the lack of convex - ity may be benecial , but is an issue we leave for future
related work
123 semi - supervised learning
although many semi - supervised methods have been pro - posed , they each fall into one of a small number of classes of methods , each of which make certain assumptions .
co - training ( 123 ) and related multi - view learning methods ( 123 , 123 ) assume that multiple classiers are trained over multiple feature views ( splits ) of the same labeled examples .
as ca - pacity control , these classiers are encouraged to make the
yw123w123wn .
. same prediction on any unlabeled example .
however , multi - ple feature views often do not naturally exist in practice , and these methods resort to articially creating random feature
graph based semi - supervised learning ( 123 , 123 ) assumes that labeled and unlabeled examples are connected by a graph , where edges represent similarity between examples .
the dis - criminant function is encouraged to vary smoothly with re - spect to the graph .
as a result , connected nodes tend to have the same label .
one interpretation of this it is that labels propagate through unlabeled examples via graph edges .
however , the graph is usually constructed from the distances in feature space , and is susceptible to overlap - ping classes .
indeed if unlabeled data from dierent classes strongly overlap , the graph will be wrong , and the method can be expected to be inferior to supervised learning .
semi - supervised or transductive support vector machines ( s123vms , tsvms ) ( 123 , 123 ) also assume that there is a wide margin in kernel induced feature space between unlabeled data from dierent classes .
the margin may be dierent than the traditional margin of labeled examples .
s123vms attempt to place the decision boundary in the unlabeled margin .
however , there are two issues : first , such margin may not exist if the classes strongly overlap even in the kernel induced feature space .
second , s123vms involves a highly non - convex optimization problem which is dicult to solve ( 123 ) .
generative semi - supervised methods based on the expec - tation maximization ( em ) algorithm assume a model for the input distribution .
in ( 123 ) nigam , et al .
use the em al - gorithm in conjunction with a mixture of multinomials , or naive bayes , generative model .
in the e step , each unlabeled example is assigned a label distribution according to its ex - pected value under the current model .
in the m step , the multinomial parameters are re - estimated .
in practice this model can fail when the assumption of independent input features is violated or when the best generative structure does not correspond to the decision boundary .
123 generative discriminative hybrids
there have been several successful applications of hybrid generative / discriminative methods in addition to the two approaches that are the focus of this paper .
in many ap - proaches , parameters are separated into two subsets , one of which is trained discriminatively and the other generatively .
raina et al .
( 123 ) , present a model for document classi - cation in which documents are split into multiple regions .
for a newsgroup message , regions might include the header and the body .
in this model , each region has its own set parameters that are trained generatively , while the param - eters that weight the importance of each region in the nal classication are trained discriminatively .
experimental re - sults show that this hybrid algorithm gives better classica - tion accuracy than either naive bayes or logistic regression ( a generative / discriminative pair ( 123 ) ) alone .
additionally , raina , et al .
show that because the number of discrimina - tive parameters in the model is small , only a small amount of training data is required to estimate these parameters .
kang and tian ( 123 ) extend naive bayes by splitting fea - tures into two sets x123 and x123
the directed graphical model
for this approach has nodes x123 as the parents of the class variable y , and parameters x123 as the children of y .
param - eters of this model are estimated by maximizing p ( y |x123 ) ( the discriminative component ) , and p ( x123|y ) ( the genera - tive component ) .
an iterative algorithm based on classica - tion accuracy is used to decide which features go in x123
bouchard and triggs ( 123 ) propose a method to trade - o generative and discriminative modeling that is similar to multi - conditional learning because it maximizes a weighted combination of two likelihood terms using one set of param - eters .
dening lgen = ln p ( y , x ) and ldisc = ln p ( y |x ) , bouchard and triggs present a combined objective function l = ln p ( x , y ) + ( 123 ) ln p ( y |x ) .
a subtle dierence between this objective function and those presented here is that in their approach the generative component is the full joint distribution .
as in other related work , experimental results show that highest accuracy is obtained somewhere between fully generative ( in this case = 123 ) and fully dis - criminative ( = 123 ) .
semi - supervised learning methods are rarely applied in practice , in part because there are few empirical compar - isons of multiple semi - supervised methods .
additionally , many semi - supervised experiments use binary datasets that have few features and are easily separable .
here we pro - vide a substantial comparison of two semi - supervised hy - brid generative / discriminative methods and two prominent semi - supervised learning methods , as well as their super - vised counterparts , on multi - class datasets with large num - bers of features .
we rst discuss the supervised / semi - supervised pairs of
methods we use in the experiments .
naive bayes / em naive bayes ( nb , emnb ) we use the naive bayes implementation in the mallet toolkit ( 123 ) .
as in nigam et . al ( 123 ) , we use laplace ( plus - 123 ) smooth - ing , so that unseen events do not get zero probability .
svm / transductive svm ( svm , tsvm ) in our experiments we use universvm , an svm implementa - tion introduced in ( 123 ) that uses the concave - convex proce - dure ( cccp ) to optimize transductive svms .
collobert et al .
show that optimizing tsvms with cccp improves ac - curacy and decreases training time when compared to other heuristic methods .
the tsvm introduces several hyperpa - rameters that need to be tuned .
in our experiments , we tune c ( 123 , 123 , 123 , 123 , 123 , 123 ) and c ( 123 , 123 , 123 , 123 , 123 ) , the cost parameters for the labeled and unlabeled data , respectively .
collobert et al .
also tune the symmetric ramp loss for the unlabeled data , but doing a grid search over three parameters , each with several possible values , was not practical for large - scale com - parison .
we set the symmetric ramp loss parameter to 123 in all experiments and use a linear kernel .
although we do not perform normalization of feature counts for other meth - ods , we nd that this is very important to achieve reason -
able tsvm results and therefore normalize feature vectors to have euclidean length 123 for the svm and tsvm experi -
max entropy / multi - conditional method ( me , mcl ) we use the implementation of maximum entropy models in mallet ( 123 ) and also use this framework to implement the multi - conditional method .
for both we tune the gaussian prior variance 123 ( 123 , 123 , 123 , 123 , 123 ) .
for the multi - conditional method , we also tune the relative weighting of the discriminative component ( 123 , 123 ) at every order of magnitude .
we use = 123 for all experiments .
see section 123 for some discussion of settings for and .
parameter coupling prior ( pcp ) we implement this model in mallet ( 123 ) as well .
following lasserre et al .
( 123 ) , we tune the parameter , which is trans - lated into a value for , the strength of the coupling prior , 123 ) 123
we use = ( 123 , 123 ) in intervals of using ( ) = ( 123 .
we use 123 = 123 for the gaussian prior on parameters , because we nd that tuning this value provides little benet when compared to the extra time it requires ( we later show that pcp requires more time to train than the other meth - ods ) .
as above , the supervised counterpart for this method is the maximum entropy classier .
we run experiments on ve text classication datasets and one sliding - window sequence labeling classication dataset .
for the text classication datasets , features correspond to word occurrence counts .
for the ner task , features are bi - nary word occurrences and properties of those words ( such as capitalization ) within three time steps .
stopwords , html , message headers ( where appropriate ) , and features that only occurred once are removed from all datasets .
where noted , low frequency features are also removed .
movie ( 123 , 123 features , 123 classes ) classify the senti - ment of movie reviews from imdb as positive or neg -
webkb ( 123 , 123 features , 123 classes ) classify university
webpages as student , course , faculty , or project .
sraa ( 123 , 123 features , 123 labels ) classify messages by the newsgroup to which they were posted : simulated - aviation , real - aviation , simulated - autoracing , realauto .
sector ( 123 , 123 features 123 labels ) classify webpages
into specic industry sectors .
blogs ( 123 , 123 features , 123 labels ) classify the age of a blogger given blog posts .
due to the large number of features in this dataset , those that occur less than 123 times are removed .
this dataset was introduced in
ner ( 123 , 123 features , 123 labels ) sliding - window named -
entity recognition using the conll 123 dataset .
although all datasets are labeled , we simulate unlabeled data by ignoring the labels for some examples .
specically ,
we choose examples to remain labeled randomly , but en - sure that the number of labeled examples is the same for each class as in ( 123 ) .
we treat the remaining examples as unlabeled , up to a maximum of 123 , 123 unlabeled examples .
the success of semi - supervised learning is dependent on the quality of the seed set of labeled examples .
therefore , we average results over ve random labeled sets .
we report accuracy on a held - out test data , rather than reporting ac - curacy on the unlabeled data , as is done with tsvms ( 123 ) .
many semi - supervised methods introduce hyperparame - ters including graph methods ( 123 , 123 ) and tsvms ( 123 ) .
we discuss the issue of choosing hyperparameters and provide some heuristics that reduce the need for hyperparameter tuning for generative / discriminative methods in section 123 .
for these experiments we use a grid search to nd the best settings , and use parameters settings that give the best test set accuracy , as also in ( 123 ) .
therefore , these results can therefore be interpreted as an indication of the potential of these methods , though further research is needed for prac - tical parameter tuning .
123 results and discussion
classication accuracy results are presented in table 123
either mcl or pcp achieves the highest accuracy in 123% of the experiments , and mcl and pcp achieve the largest mean accuracy improvements over their supervised counter - part at 123% and 123% , respectively .
additionally , mcl is the only method to show semi - supervised improvements on every dataset .
figure 123 illustrates that for both mcl and pcp the best accuracies are attained in the space between purely generative and purely discriminative .
we argue in the introduction that hybrid approaches are able to avoid or mitigate the assumptions of other semi - supervised methods .
we would like to determine if the cases in which the hybrid methods perform well empirically match our intuitive justications .
it is dicult to quanti - ably compare the degree of overlapping classes or the degree to which model assumptions fail across text datasets , but we can gain some insight by considering the datasets on which hybrid semi - supervised methods do well and other methods
first notice that naive bayes performs worst on datasets sector and ner , in which em naive bayes gives lower ac - curacy than supervised naive bayes .
additionally , despite semi - supervised improvements , em naive bayes gives much lower accuracy on blogs than other methods .
these three datasets have the largest number of classes , most compli - cated and correlated features , and greatest number of fea - tures , respectively .
highly correlated features clearly violate the naive assumption of feature independence .
we also ex - pect that for these tasks the natural clusters in the data will not necessarily be correlated with the decision boundary .
for example , both ner and blog involve a very specic task on text that includes a wide variety of words and topics .
the hybrid methods are able to mitigate these generative assumptions using the discriminative component .
relative to the other methods , tsvms perform poorly on ner and sraa , cases in which we expect classes to be strongly overlapping .
the sraa dataset contains messages from news - groups on simulated aviation , real aviation , simulated auto
svm tsvm me mcl
table 123 : classication accuracy results .
parenthesized values indicate the number of labeled documents per
di me mcl
table 123 : parameters with largest positive and negative dierences in discriminative power for the blog dataset with 123 labeled documents per class for label age 123 - 123 between supervised and semi - supervised with the multi - conditional method .
the gaussian prior variance for the maximum entropy classier is 123 .
for the mcl method , = 123 and the gaussian prior variance is 123 , corresponding to a case when the testing accuracy was 123 .
table 123 : parameters with largest positive and negative dierences in discriminative power for the sraa dataset with 123 labeled documents per class for label real aviation between supervised and semi - supervised with the parameter coupling prior method .
the gaussian prior variance for the maximum entropy classier is 123 .
for the pcp method , = 123 , corresponding to a case when the testing accuracy was 123 .
figure 123 : mean accuracies vs .
for hybrid methods .
the dashed line represents the supervised accuracy .
the best accuracies are obtained in the region between purely generative and purely discriminative .
racing , and real automobiles .
these classes have high word overlap , as both simulated and real automobile newsgroups will include words like tire and engine , for example .
the ner dataset contains classes for the start of a token type , such as location , and the continuation of a token type .
we expect that the begin and continuation classes for a to - ken type will not have a low density region between them , and we see in the results that the tsvm chooses poor lo - cations for the decision boundaries as the accuracy is much worse than supervised .
we note that we originally conducted the svm / tsvm experiments without normalizing feature vectors , and the tsvm only improved accuracy over the supervised svm in two cases ( blog ( 123 , 123 ) ) .
interestingly , normalization seems to have a much larger impact on semi - supervised learning than on supervised learning .
excluding the ner dataset , the mean svm accuracy increases by 123% using normaliza - tion , whereas the mean tsvm accuracy increases by 123% .
even when using normalization , tsvms do not always give improvements over supervised svms .
one possible explana - tion is that most published evaluations of tsvms use sim - ple , two - class datasets .
additionally , tsvm experiments typically use the eventual test data as unlabeled data dur - ing training , whereas here we use a held - out test set that is separate from the unlabeled training data .
a more compli - cated kernel may improve accuracy , but note that the other methods only use linear combinations of features .
comparing the two hybrid generative / discriminative meth - ods , mcl achieves higher mean accuracy than pcp on seven of the twelve experiments , as well as higher overall mean ac - curacy across all tasks .
notice that the two methods rarely perform well on the same dataset .
the fundamental dier - ence between the two methods is that mcl has one shared set of parameters while pcp has two coupled sets of parame - ters .
an advantage of the pcp method is that the gradients of the two components do not directly compete to modify parameter values .
this allows each component of the pcp objective to have more freedom in modeling the data .
some generative parameter needs to be set in a way which does not correspond to a good discriminative setting , this
penalty can absorbed if it leads to an improved model .
ad - ditionally , the pcp method allows the inclusion of a prior that only eects the discriminative parameters .
with mcl , a gaussian prior on parameters helps prevent the discrimi - native component from overtting , but because the param - eters are shared , this prior has a negative impact on the generative component .
namely , we observe that the penalty on large parameter values tends to prevent large changes in parameter value due to the generative component , as em - pirically the generative parameters need to be more spread out than their discriminative counterparts .
however , it ap - pears that the extra modeling power aorded by the pcp method sometimes makes it more dicult to nd good pa - rameter settings during training .
we note that the cases in which mcl gives improved accuracy over pcp tend to be more complicated datasets in terms of the number of labels or features .
this suggests that pcp may be preferable for less complicated datasets and mcl for more complicated
in tables 123 and 123 we show how parameter values change after introducing the generative component that uses unla - beled data .
these tables show dierences in the discrimi - native power of features in a supervised maximum entropy model and a semi - supervised hybrid .
the values for each feature can be interpreted as the probability that a single word document containing that word feature belongs to the class listed in the table caption .
in table 123 , we see that for the blogs dataset and label age 123 - 123 , the discriminative power of im , lol , school and home increases , and the discriminative power of wife , couple , and bar decreases .
in table 123 , we see that for dataset sraa and label real avia - tion , the discriminative power of airspeed , altitude , and preight increases , while the power of autos , racing , and cars decreases .
intuitively , we see that the addition of the generative component helps to boost the discrimina - tive power of features that the supervised model could not discover with limited labeled data .
123 . 123 . 123 . 123 . 123 . 123log alphamean accuracymean accuracy vs .
alpha for multi ! criteria method : blogs ( 123 ) semi ! supervised hybridsupervised discriminative123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123alphamean accuracymean accuracy vs .
alpha for parameter coupling prior method : sraa ( 123 ) semi ! supervised hybridsupervised discriminative method movie ( 123 )
table 123 : mean training time in seconds .
123 practical considerations
hyperparameter values are often important to the suc - cess of semi - supervised learning methods , but tuning hy - perparameters can be dicult in practice .
learning , hyperparameter values are typically chosen using cross - validation .
if we apply this method for semi - supervised learning , each fold test set ends up with a very small number of labeled examples , since the total number of labeled ex - amples is small .
this means that the performance estimate obtained from this test set may be inaccurate .
addition - ally , optimal hyperparameter values may depend upon the specic set of labeled examples .
training with a subset of those labeled examples could result in drastically dierent ideal settings for the the hyper - parameters , as , for example , a poor seed set of labeled examples may require a hybrid classier to put more weight on the generative component and rely on unlabeled data .
another option is to choose the hyperparameters that give the highest likelihood on the training data .
however , we have found that these hyperpa - rameters produce the highest likelihood models because of overtting .
finally , choosing hyperparameters using a vali - dation set is not practical , as if more labeled data is avail - able , it would almost certainly be more benecial to use that data during training than to use it to tune hyperparameters .
another practical issue in semi - supervised learning is that the added complexity of semi - supervised training algorithms increases the overall model training time , as shown in ta - ble 123
on average , the pcp method takes the longest to train .
in addition to having twice as many parameters as the mcl method , it also seem to take more iterations to converge .
the em naive bayes semi - supervised learning is extremely fast because the maximum likelihood estimates can be computed in closed form .
we propose a heuristic for hybrid models to simultane - ously reduce training time and reliance on hyperparameter settings that involves ensuring reasonable initial parameter settings .
for the pcp method , we initialize both sets of pa - rameters to the results of supervised training on the labeled data .
we present results using supervised initialization of the pcp method in table 123
this also reduces training time , as shown by the entry pcp - init in table 123
for the mcl method , we propose to use the discriminative compo - nent exclusively in the rst few iterations of training .
since the discriminative component is convex , this guarantees that we move into a reasonable region parameter space .
at the same time , by avoiding going the whole way to the global maximum , we give the parameters room to move away from the discriminative maximum .
table 123 : classication results when using supervised initialization for parameter coupling prior model .
parenthesized values indicate the number of labeled documents per class .
these heuristics seem to make the algorithms more con - sistent , as the mean accuracy across all hyperparameter set - tings is higher .
however , the maximum accuracies attained are sometimes lower .
intuitively , one of the real benets of these methods is that the generative component can pull the discriminative component into regions of parameter space very dierent from those chosen by supervised discrimina - tive learning on the limited labeled data .
with these initial - izations , we ensure that we do not converge to a poor local maximum , but sacrice the potential to nd drastically dif - ferent parameter settings .
conclusion and future work
we have considered hybrid generative / discriminative ap - proaches to semi - supervised classication in which the gen - erative component includes unlabeled data .
we compare two methods for combining generative and discriminative likelihood in detail : a multi - conditional learning method and a method where each component has its own set of parame - ters that are coupled by a prior distribution .
in a substantial empirical comparison , a hybrid method provides the best accuracy in eight of the twelve experiments .
intuitively , we conjecture that they perform well by mitigating the model - ing assumptions of generative semi - supervised methods and avoiding the low - density - between - class assumptions of dis - criminative semi - supervised methods .
in future work , we would like to apply hybrid genera -
tive / discriminative approaches to transfer or multi - task learn - ing , consider heuristic and probabilistic methods to learn hyperparameters from data , and research alternative opti - mization techniques utilizing ideas from the multi - criteria
this work was supported in part by the center for intel - ligent information retrieval , in part by the central intel - ligence agency , the national security agency and national science foundation under nsf grant #iis - 123 , in part by nsf nano #dmi - 123 , and in part by the defense advanced research projects agency ( darpa ) , through the
department of the interior , nbc , acquisition services di - vision , under contract number nbchd123
cp appre - ciates support by microsoft research under the memex and escience funding programs and support from kodak re - search .
any opinions , ndings and conclusions or recom - mendations expressed in this material are the authors and do not necessarily reect those of the sponsor .
