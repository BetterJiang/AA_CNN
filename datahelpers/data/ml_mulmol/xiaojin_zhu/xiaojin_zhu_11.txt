kernel conditional random elds ( kcrfs ) are introduced as a framework for discriminative modeling of graph - structured data .
a repre - senter theorem for conditional graphical mod - els is given which shows how kernel condi - tional random elds arise from risk minimization procedures dened using mercer kernels on la - beled graphs .
a procedure for greedily select - ing cliques in the dual representation is then pro - posed , which allows sparse representations .
by incorporating kernels and implicit feature spaces into conditional graphical models , the framework enables semi - supervised learning algorithms for structured data through the use of graph kernels .
the framework and clique selection methods are demonstrated in synthetic data experiments , and are also applied to the problem of protein sec - ondary structure prediction .
many classication problems involve the annotation of data items having multiple components , with each component requiring a classication label .
such problems are chal - lenging because the interaction between the components can be rich and complex .
in text , speech , and image pro - cessing , for example , it is often useful to label individual words , sounds , or image patches with categories to enable higher level processing; but these labels can depend on one another in a highly complex manner .
for biological se - quence annotation , it is desirable to annotate each amino acid in a protein with a label , with the collection of labels representing the global geometric structure of the molecule .
here the labels in principle depend on the physical char - acteristics of the molecule and its ambient chemical envi -
appearing in proceedings of the 123 st international conference on machine learning , banff , canada , 123
copyright 123 by
ronment .
in each case , classication tasks naturally arise which clearly violate the assumption of independent and identically distributed instances that is made in the majority of classication procedures in statistics and machine learn - ing .
it is therefore of central importance to extend recent advances in classication theory and practice to structured , non - independent data classication problems .
conditional random elds ( lafferty et al . , 123 ) have been proposed as an approach to modeling the interactions be - tween labels in such problems using the tools of graphical models .
a conditional random eld ( crf ) is a model that assigns a joint probability distribution over labels condi - tional on the input , where the distribution respects the in - dependence relations encoded in a graph .
in general , the labels are not assumed to be independent , nor are the ob - servations conditionally independent given the labels , as is assumed in generative models such as hidden markov models .
the crf framework has already been used to ob - tain promising results in a number of domains where there is interaction between labels , including tagging , parsing and information extraction in natural language processing ( collins , 123; sha & pereira , 123; pinto et al . , 123 ) and the modeling of spatial dependencies in image processing ( kumar & hebert , 123 ) .
in related work , taskar et al .
( 123 ) have studied random elds ( also known as markov networks ) t using loss functions that incorporate a gener - alized notion of margin , and have observed how the kernel trick applies to this family of models .
we present an extension of conditional random elds that permits the use of implicit features spaces through mercer kernels , using the framework of regularization theory .
such an extension is motivated by the signicant body of recent work that has shown kernel methods to be extremely effec - tive in a wide variety of machine learning techniques; for example , they enable the integration of multiple sources of information in a principled manner .
our introduction of mercer kernels into conditional graphical models is also motivated by the problem of semi - supervised learning .
in many domains , the collection of annotated training data is difcult and costly , as it requires the efforts of expert hu - man annotators , while the collection of unlabeled data may
be relatively easy and inexpensive .
the emerging theme in recent research in semi - supervised learning is that kernel methods , in particular those based on graphical representa - tions of unlabeled data , form a theoretically attractive and empirically promising set of techniques for combining la - beled and unlabeled data ( belkin & niyogi , 123; chapelle et al . , 123; smola & kondor , 123; zhu et al . , 123 ) .
in section 123 we formalize the learning problem and present a version of the classical representer theorem of kimeldorf and wahba ( 123 ) .
unlike the classical result , for kernel conditional random elds the dual parameters depend on all potential assignments of labels to cliques in the graph , not only the observed labels .
this motivates the need for algorithms to derive sparse representations , since the full representation has parameters for each labeled clique in the graphs appearing in the training data .
in section 123 we present a greedy algorithm for selecting a small number of representative cliques .
this clique selection algorithm parallels the import vector selection algorithms of kernel logistic regression ( zhu & hastie , 123 ) , and the feature selection methods that have been previously proposed for random elds and conditional random elds using explicit features ( mccallum , 123 ) .
in section 123 the ideas and methods are demonstrated on two synthetic data sets , where the effects of the underly - ing graph kernels , clique selection , and sequential model - ing can be clearly seen .
in section 123 we report the results of experiments using kernel crfs for protein secondary structure prediction .
this is the task of mapping primary sequences of amino acids onto a string of secondary struc - ture assignments , such as helix , sheet , or coil .
it is widely believed that secondary structure can contribute valuable information to discerning how proteins fold in three dimen - sions .
we compare kernel conditional random elds , esti - mated using clique selection , against support vector ma - chine classiers , with both methods using kernels derived from position - specic scoring matrices ( psi - blast pro - les ) as input features .
in addition , we give results for the use of graph kernels derived from the psi - blast proles in a transductive , semi - supervised framework for estimat - ing the kernel crfs .
the paper concludes with a brief dis - cussion in section 123
before proceeding with formalism , we give some intuition for what our framework is intended to capture .
our goal is to annotate structured data , where the structure is repre - sented by a graph .
labels are to be assigned to the nodes in the graph in order to minimize some loss function , such as 123 - 123 error; the labels come from a small set y , for ex - ample , y = ( red , blue , green ) .
each vertex in the graph is associated with a feature vector xv x .
image processing , the feature vector at a node might in - clude a pixel intensity , as well as average pixel intensities smoothed over neighboring regions using wavelets .
in pro - tein secondary structure prediction , each node might corre - spond to an amino acid in the protein , and the feature vector at a node may include an amino acid histogram of all pro - tein fragments in a database which closely match the given protein at that node .
in the following section we present our notation and formal framework for such problems .
cliques and labeled graphs
let g denote a collection of nite graphs .
for example , g might be the set of nite chains , appropriate for sequence modeling , or the rectangular 123 - dimensional grids , appro - priate for some image processing tasks .
the set of ver - tices of a graph g g is denoted by v ( g ) , and size of the graph is the number of vertices , denoted |g| = |v ( g ) | .
a clique is a subset of the vertices which is fully connected , with any pair of vertices joined by an edge; we denote the set of cliques in the graph by c ( g ) .
the number of vertices in a clique is denoted by |c| .
similarly , we de - note by c ( g ) = ( ( g , c ) | g g , c c ( g ) ) the collection of cliques across varying graphs .
in other words , a mem - ber of c ( g ) consists of a graph and a distinguished clique of that graph .
we will work with kernels that compare components of different graphs .
for example , we could consider a kernel k : c ( g ) c ( g ) ( 123 , 123 ) given by k ( ( g , c ) , ( g123 , c123 ) ) = ( |c| , |c123| ) .
we next consider labelings of a graph .
let y be a - nite set of labels; innite y is also possible in a re - gression framework , but we restrict to nite y for sim - plicity .
the set of y - labelings of a graph g is denoted
labeled graphs is y ( g ) = ( ( g , x ) | g g , y y ( g ) ) .
similarly , let x be an input feature space; for example ,
y ( g ) = ( cid : 123 ) y | y y |g| ( cid : 123 ) , and the collection of all y - x = rn .
the set x ( g ) = ( cid : 123 ) x | x x |g| ( cid : 123 ) denotes the yc ( g ) = ( cid : 123 ) ( c , yc ) | c c ( g ) , yc y |c| ( cid : 123 ) be the set of y -
set of assignments of a feature vector to each vertex of the graph g; x ( g ) = ( ( g , x ) | g g , x x ( g ) ) is the collection of all such annotated graphs .
labeled cliques in a graph .
as above , we similarly dene x yc ( g ) = ( ( x , c , yc ) | x x ( g ) , ( c , yc ) yc ( g ) ) and x yc ( g ) = ( ( g , x , c , yc ) | ( x , c , yc ) x yc ( g ) ) .
representer theorem
the prediction task for conditional graphical models is to learn a function h : x ( g ) y ( g ) where h ( g , x ) y ( g ) is a labeling of g , with the goal of minimizing a suitably dened loss function .
the classier h = hn is chosen , with each ( i ) ) being a labeled graph , the graph possibly
based on a labeled sample ( cid : 123 ) ( g ( i ) , x
changing from example to example .
to limit the complexity of the hypothesis , we will as - sume that it is determined completely by a function f : x yc ( g ) r .
let f ( g , x ) denote the collection of values ( f ( g , x , c , yc ) ) , with c c ( g ) varying over the cliques of g and yc y |c| varying over all possible labelings of that clique .
we assume that a loss function ( y , f ( g , x ) ) is given .
as an important example , and the loss function used in this paper , consider the negative log loss
( y , f ( g , x ) ) =
fc ( x , yc ) + log xy123y ( g )
where fc ( x , yc ) is shorthand for f ( g , x , c , yc ) .
the neg - ative log marginal loss could also be considered for mini - mizing the per - node error .
the negative log loss function corresponds to a conditional random eld given by
p ( y | g , x ) = z 123 ( g , x , f ) exp xc
we now discuss how the representer theorem of kernel machines ( kimeldorf & wahba , 123 ) applies to condi - tional graphical models .
while this is a simple extension , were not aware of an analogous formulation in the statis - tics or machine learning literature .
let k be a mercer kernel on x yc ( g ) ; thus
k ( ( g , x , c , yc ) , ( g123 , x
123 , c123 , y
123 , c123 , y
for each ( x , c , yc ) x yc ( g ) and ( x c123 ) x yc ( g123 ) .
intuitively , this assigns a measure of similarity between a labeled clique in one graph and a labeled clique in a ( pos - sibly ) different graph .
we denote by hk the associated re - producing kernel hilbert space , and by kkk the associated norm on l123 ( x yc ( g ) ) .
consider a regularized loss function of the form
( i ) , f ( g ( i ) , x
( i ) ) ( cid : 123 ) + ( kf kk )
it is important to note that the loss depends on all possi - ble assignments yc of labels to each clique , not just those observed in the labeled data y ( i ) .
suppressing the depen - dence on the graph g in the notation , let kc ( x , yc; , ) = k ( ( g , x , c , yc ) , ) .
following the argument for the stan - dard representer theorem , it can easily be shown that the minimizer of a regularized loss function of the above form can be expressed in terms of the basis functions f ( ) = k ( , ( g ( i ) , x
( i ) , c , yc ) ) .
kkk , and let : r+ r+ be strictly increasing .
then theminimizerf ? of
( i ) , f ( g ( i ) , x
( i ) ) ( cid : 123 ) + ( kf kk )
f ? ( ) =
nxi=123 xcc ( g ( i ) ) xycy |c|
c ( yc ) kc ( x
( i ) , yc; )
the key property distinguishing this result from the stan - dard representer theorem is that the dual parameters c ( yc ) now depend on all assignments of labels .
two special cases
let k be a mercer kernel on z = x y the kernel is dened in terms of the ma - 123 ) where z = ( x , y123 , y123 ) .
using trix entries k ( z , z k we can dene a kernel on edges in x yc ( g ) by k ( ( g , x , ( v123 , v123 ) , ( y123 , y123 ) ) , ( g123 , x 123 ) ) .
for the regularized risk k ( ( xv123 , y123 , y123 ) , ( x
r ( x , f , ) = min
( i ) , f ( x
( i ) ) ) + kf kk
where f hk , the crf representer theorem implies that the solution f ? has the form
( v123 , v123 ) ( x , y123 , y123 ) =
( v , v123 ) ( y , y123 ) k ( ( xv123 , y123 , y123 ) , ( x
v , y , y123 ) )
in the special case of kernel k ( z , z it follows that
123 ) = k ( x , x123 ) ( y123 , y123
( v123 , v123 ) ( x , y123 , y123 ) =
nxi=123 xvv ( g ( i ) )
v ( y123 ) k ( xv123 , x
under the probabilistic model ( 123 ) , this is simply kernel k ( x , x123 ) ( y123 , y123
in the special case of k ( z , z
123 ) we get that
123 ) + ( y123 , y123
123 ) ( y123 , y123
( v123 , v123 ) ( x , y123 , y123 ) =xi , v
v ( y123 ) k ( x
v , xv123 ) + ( y123 , y123 )
and we recover a simple type of semiparametric crf .
clique selection
proposition ( representer theorem for crfs ) .
letk be a mercerkernelon x yc ( g ) withassociated rkhs norm
the representer theorem shows that the minimizing func - tion f is supported by labeled cliques over the training
examples; however , this may result in an extremely large number of parameters .
we therefore pursue a strategy of incrementally selecting cliques in order to greedily reduce the regularized risk .
the resulting procedure is parallel to forward stepwise logistic regression , and to related meth - ods for kernel logistic regression ( zhu & hastie , 123 ) , as well as to the greedy selection procedure presented in ( della pietra et al . , 123 ) .
our algorithm will maintain an active set a =
( cid : 123 ) ( g ( i ) , c , yc ) ( cid : 123 ) yc ( g ) of labeled cliques , where the la -
belings are not restricted to those appearing in the training data .
each such candidate clique can be represented by a ( i ) , c , yc ) , ) hk , and basis function h ( ) = k ( ( g ( i ) , x is assigned a parameter h = ( i ) c ( yc ) .
we work with the
r ( f ) =xi
( i ) , f ( g ( i ) , x
where is the log - loss of equation ( 123 ) .
to evaluate a can - didate h , one strategy is to compute the gain sup r ( f ) r ( f + h ) , and to choose the candidate h having the largest gain .
this presents an apparent difculty , since the optimal parameter cannot be computed in closed form , and must be evaluated numerically .
for sequence models this involves forward - backward calculations for each can - didate h , the cost of which is prohibitive .
as an alternative , we adopt the functional gradient descent approach , which evaluates a small change to the current function .
for a given candidate h , consider adding h to the current model with small weight ; thus f 123 f + h .
then r ( f + h ) = r ( f ) + dr ( f , h ) + o ( 123 ) , where the functional derivative of r at f in the direction h is
( i ) , f ) h ( x
dr ( f , h ) = ef ( h ) ee ( h ) + hf , hik
where ee ( h ) = pi h ( x tion and ef ( h ) = pipy p ( y | x
( i ) ) is the empirical expecta - ( i ) , y ) is the model expectation conditioned on x , combined with the empirical distribution on x .
the idea is that in directions h where the functional gradient dr ( f , h ) is large , the model is mismatched with the labeled data; this direction should be added to the model to make a correction .
this results in the greedy clique selection algorithm summarized in fig -
following our earlier notation ,
( i ) , y ) = xcc ( g ( i ) )
( i ) , c , yc )
is the sum over all cliques .
the candidate functions h might include functions of the form
h ( ) = k ( ( g ( i ) , x
( i ) , c , yc ) , )
initialize with f = 123 , and iterate :
for each candidate h hk , supported by a sin - gle labeled clique , calculate the functional derivative
select the candidate h = arg maxh |dr ( f , h ) | hav - ing the largest gradient direction .
set f 123 f + hh .
estimate parameters f for each active f by minimiz -
ing r ( f ) .
figure123greedy clique selection .
labeled cliques encode basis functions h which are greedily added to the model , using a form of functional gradient descent .
where i is a specic instance , c is a particular clique of g ( i ) , and yc is a labeling of that clique .
alternatively , in a slightly less greedy manner , at each step in the selection procedure a specic instance and clique may be selected , and functions for each clique labeling may be added .
in the experiments reported below for sequences , marginal probabilities p ( yt = y | x ) and expected counts for the state transitions are required; these are computed using the forward - backward algorithm , with log domain arithmetic to avoid underow .
a quasi - newton method ( bfgs , cubic - polynomial line search ) is used to estimate the parameters in step 123
prediction is carried out using the forward - backward algorithm to compute marginals rather than using the viterbi algorithm .
combining multiple kernels
the above use of kernels enables semi - supervised learn - ing for structured prediction problems .
one of the emerg - ing themes in semi - supervised learning is that graph ker - nels can provide a useful framework for combining labeled and unlabeled data .
here an undirected graph is dened over labeled and unlabeled data instances , and generally the assumption is that labels vary smoothly over the graph .
the graph is represented by the weight matrix w , and one can construct a kernel from the graph laplacian , substitut - ing eigenvalues by r ( ) , where r is a non - negative and ( typically ) decreasing function .
this regularizes high fre - quency components and encourages smooth functions on the graph; see ( smola & kondor , 123 ) for a description of this unifying view of graph kernels .
it is important to note that such a use of a graph kernel for semi - supervised learning introduces an additional graphi - cal structure , which should not be confused with the graph representing the explicit dependencies between labels in a crf .
for example , when modeling sequences , the natural crf graph structure is a chain .
by incorporating unla - beled data through the use of a graph kernel , an additional
graph that will generally have many cycles is implicitly in - troduced .
however , the graph kernel and a more standard kernel may be naturally combined as a linear combination; see , for example , ( lanckriet et al . , 123 ) .
synthetic data experiments
to demonstrate the properties and advantages of kcrfs , we prepared two synthetic datasets : a galaxy dataset to investigate the relation to semi - supervised and sequential learning , and an hmm with gaussian mixture emission probabilities to demonstrate the properties of clique selec - tion and the advantages of incorporating kernels .
galaxy .
the galaxy dataset is a variant of two spirals; see figure 123 ( left ) .
note the dense core of points from both classes .
the sequences are generated from a 123 - state hidden markov model ( hmm ) , where each state emits instances uniformly from one of the classes .
there is a 123% chance of staying in the same state .
the idea is that under a se - quence model , an example from the core will have a better than random chance to be labeled correctly based on the context .
this is not true under a non - sequence model , and the dataset as a whole will thus have about a 123% bayes error rate under the iid assumption .
we sample 123 se - quences of length 123
note the choice of semi - supervised vs .
standard kernels and sequence vs .
non - sequence mod - els are orthogonal; the four combinations are all tested on .
we construct a semi - supervised graph kernel by rst creating an unweighted 123 - nearest neighbor graph .
we then compute the graph laplacian l , and form the kernel
k = 123 ( cid : 123 ) l + 123i ( cid : 123 ) 123
this corresponds to a function
r ( ) = 123 / ( + 123 ) on ls eigenvalues .
the standard kernel is the radial basis function ( rbf ) kernel with band - width = 123 .
all parameters here and below are tuned by cross validation .
figure 123 ( center ) shows the results of using kernel logis - tic regression with the semi - supervised kernel and with the rbf kernel; here the sequence structure is ignored .
for each training set size , which ranges from 123 to 123 points , 123 random trials were performed .
the error inter - vals shown are one standard error .
when the labeled set size is small , the graph kernel is much better than the rbf kernel .
however both kernels saturate at the 123% bayes
next we apply both kernels to the semiparametric kcrf model in section 123; see figure 123 ( right ) .
note the x - axis is the number of training sequencessince each sequence has 123 instances , the range is the same as figure 123 ( center ) .
the kernel crf is capable of getting under the 123% bayes error oor of the non - sequence model , with both kernels and sufcient labeled data .
however , the graph kernel is able to learn the structure much faster than the rbf ker - nel .
evidently , the high error rate for low label data sizes
prevents the rbf model from effectively using the context .
hmm with gaussian mixtures .
this more difcult dataset is generated from a 123 - state hmm .
each state is a mixture of 123 gaussians with random mean and covariance .
the gaussians strongly overlap; see figure 123 ( left ) .
the transition probabilities favor remaining in the state , with a probability of 123 , and to transition to each of the other two states with equal probability 123; we generate 123 se - quences of length 123
we use an rbf kernel with = 123 .
( a graph kernel is slightly worse than the rbf kernel on this dataset , and is not shown . ) we perform 123 trials for each training set size , and in each trial we perform clique selection to select the top 123 vertices .
the center and right plots in figure 123 show that the semiparametric kcrf again outperforms kernel logistic regression with the same rbf
figure 123 shows clique selection , with a training size 123 se - quences , averaged over 123 random trials .
the regularized risk ( left ) , which is training set likelihood plus regularizor , always decreases as we select more vertices into the kcrf .
on the other hand , the test set likelihood ( center ) and ac - curacy ( right ) saturate and even worsen slightly , showing signs of overtting .
all curves change dramatically at rst , demonstrating the effectiveness of the clique selection al - gorithm .
in fact , fewer than 123 vertex cliques are sufcient for this problem .
protein secondary structure prediction
for the protein secondary structure prediction task , we used the rs123 dataset , on which many current methods have been developed and tested ( cuff & barton , 123 ) .
a non - homologous dataset , since among the 123 protein chains , no two proteins share more than 123% sequence identity over a length of more than 123 residues ( cuff & bar - ton , 123 ) .
the dataset can be downloaded from http :
we adopt the dssp denition of protein secondary struc - ture ( kabsch & sander , 123 ) , which is based on hydrogen bonding patterns and geometric constraints .
following the discussion in ( cuff & barton , 123 ) , the 123 dssp labels are reduced to a 123 state model as follows : h & g map to helix ( h ) , e & b to sheets ( e ) , and all other states to coil ( c ) .
the state - of - the - art performance for secondary structure prediction is achieved by window - base methods , using the position - specic scoring matrices ( pssm ) as input fea - tures , i . e . , psi - blast proles , together with support vec - tor machines ( svms ) as the underlying learning algorithm ( jones , 123; kim & park , 123 ) .
finally , the raw predic - tions are fed into a second layer svm to lter out physi - cally unrealistic predictions , such as one sheet residue sur - rounded by helix residues ( jones , 123 ) .
training set size
training set size
figure123left : the galaxy data .
center : kernel logistic regression , comparing two kernels : rbf and a graph kernel using the unlabeled data .
right : kernel conditional random elds , which take into account the sequential structure of the data .
figure123left : the gaussian mixture data ( only a few data points are shown ) .
center : kernel logistic regression with an rbf kernel .
right : kernel crf with the same kernel .
in our experiments , we apply a linear transformation l to the pssm matrix elements according to l ( x ) = 123 for x 123 , l ( x ) = 123 123 for 123 x 123 , and l ( x ) = 123 for x 123
this is the same transform used by kim and park ( 123 ) , which achieved one of the best results in the recent casp ( critical assessment of structure predictions ) competition .
the window size is set to 123 by cross - validation .
therefore the number of features per position is 123 123 ( the number of amino acids plus gap ) .
123 ) = k ( x , x123 ) ( y123 , y123
clique selection .
we use an rbf kernel with bandwidth = 123 chosen by cross - validation .
figure 123 ( left ) shows the kernel crf risk reduction as clique selection proceeds , when only vertex clique candidates are allowed ( note there are always position independent edge parameters in the kcrf models , to prevent the models from degrading into kernel logistic regression ) , and when both vertex and edge cliques are allowed .
( the kernel between vertex cliques is 123 ) , and between edge cliques 123 ) . ) the total it is k ( z , z number of clique candidates is about 123 ( vertex only ) and 123 ( vertex and edge ) .
the rapid reduction in risk indicates sparse training of kernel crfs is successful .
also when more exibility is allowed by including edge cliques , the risk reduction is much faster .
the more exible model also has higher test set log likelihood ( center ) although this does not improve the test set accuracy too much ( right ) .
these observations are generally true for other trials too .
123 ) = k ( x , x123 ) ( y123 , y123
123 ) ( y123 , y123
per - residue accuracy .
to evaluate prediction performance , we use the overall per - residue accuracy ( also known as q123 ) .
we experiment with training set size of 123 and 123 se - quences respectively .
for each size we perform 123 trials where the training sequences are randomly sampled , and the remaining proteins are used as the test set .
for ker - nel crf we select 123 cliques , again from either vertex candidates alone or vertex and edge candidates .
we com - pare them with the svm - light package ( joachims , 123 ) for svm classier .
all methods use the same rbf kernel .
see table 123
kcrfs and svms have comparable perfor -
transition accuracy .
further information can be obtained by studying transition boundaries , for example , the tran - sition from coil to sheet .
from the point of view of structural biology , these transition boundaries may provide important information about how proteins fold in three di - mension .
on the other hand , those are the positions where most secondary structure prediction systems will fail .
the transition boundary is dened as a pair of adjacent positions ( i , i + 123 ) whose true labels differ .
it is classied correctly only if both labels are correct .
this is a very hard problem , as can be seen in table 123 and kcrfs are able to achieve a considerable improvement over svm .
semi - supervised learning .
we start with an unweighted 123 nearest neighbor graph over positions in both training and
number of selected vertices
number of selected vertices
number of selected vertices
figure123clique selection on the gaussian mixture data .
left : regularized risk; center : test set log likelihood; right : test set accuracy .
vertex and edge
vertex and edge
number of cliques
number of cliques
vertex and edge
number of cliques
figure123clique selection for kcrfs on the protein data .
left : regularized risk; center : test set log likelihood; right : test set accuracy .
the two curves represent the cases where only vertex cliques are selected ( dashed ) vs .
both vertex and edge cliques are selected ( solid ) .
test sequences , with the metric being euclidean distance in the feature space .
then the eigensystem of the normalized laplacian is computed .
the semi - supervised graph kernel is obtained with the function r ( i ) = i+123 on the rst i 123 eigenvalues .
the rest eigenvalues are set to zero .
we use the graph kernel together with the rbf kernel in kcrf .
as a clique candidate is associated with a kernel , we now select two best candidates per iteration , one with the graph kernel and the other with the rbf kernel .
we still run for 123 iterations for all trials .
we also report the re - sults using transductive svms ( tsvms ) ( joachims , 123 ) with the rbf kernel .
from the results in table 123 , we can see that the semi - supervised graph kernel is signicantly better than tsvms on the 123 - protein dataset while achieves no improvement on the other one .
to diagnose the cause , we look at the graph together with all the test labels .
we nd that the labels are not smooth w . r . t .
the graph : on aver - age only 123% of a nodes neighbors have the same label as that node .
detecting faulty graphs without using large amount of labels , and constructing better graphs remain fu -
the approximate average running time of each trial , in - cluding both training and testing , is 123 minutes for kcrfs , 123 minutes for svms , and 123 hours for tsvms .
for kcrfs the majority of the time is spent on clique selection .
kernel conditional random elds have been introduced as a framework for approaching graph - structured classica - tion problems .
a representer theorem was derived which shows how kcrfs can be motivated by regularization the - ory .
the resulting techniques combine the strengths of hid - den markov models , or more general bayesian networks , kernel machines , and standard discriminative linear classi - ers including logistic regression and svms .
the formal - ism presented is quite general , and should apply naturally to a wide range of problems .
our experimental results on synthetic data , while care - fully controlled to be simple , clearly indicate how sequence modeling , graph kernels for semi - supervised learning , and clique selection for sparse representations work together within this framework .
the success of these methods in real problems will depend on the choice of suitable kernels that capture the structure of the data .
for protein secondary structure prediction , our results are only suggestive .
secondary structure prediction is a prob - lem that has been extensively studied for more than 123 years; yet the task remains difcult , with prediction accura - cies remaining low .
the major bottleneck lies in beta - sheet prediction , where there are long range interactions between regions of the protein chain that are not necessarily consec - utive in the primary sequence .
our experimental results indicate that kcrfs and semi - supervised kernels have the
123 protein set
123 protein set
table123per - residue accuracy of different methods for secondary structure prediction , with the rbf kernel .
kcrf ( v ) uses vertex cliques only; kcrf ( v+e ) uses vertex and edge cliques .
123 protein set
123 protein set
table123
transition accuracy with different methods .
123 protein set
123 protein set
table123per - residue accuracy with semi - supervised methods .
potential to lead to progress on this problem , where the state of the art has been based on heuristic sliding win - dow methods .
however , our results also suggest that the improvement due to semi - supervised learning is hindered by the lack of a good similarity measure with which to con - struct the graph .
the construction of an effective graph is a challenge that may best be tackled by biologists and ma - chine learning researchers working together .
this work was supported in part by nsf itr grants ccr - 123 , iis - 123 and iis - 123
