we present an algorithm based on convex optimization for constructing kernels for semi - supervised learning .
the kernel matrices are derived from the spectral decomposition of graph laplacians , and combine la - beled and unlabeled data in a systematic fashion .
unlike previous work using diffusion kernels and gaussian random eld kernels , a nonpara - metric kernel approach is presented that incorporates order constraints during optimization .
this results in exible kernels and avoids the need to choose among different parametric forms .
our approach relies on a quadratically constrained quadratic program ( qcqp ) , and is compu - tationally feasible for large datasets .
we evaluate the kernels on real datasets using support vector machines , with encouraging results .
semi - supervised learning has been the focus of considerable recent research .
in this learn - ing problem the data consist of a set of points , with some of the points labeled and the remaining points unlabeled .
the task is to use the unlabeled data to improve classication performance .
semi - supervised methods have the potential to improve many real - world problems , since unlabeled data are often far easier to obtain than labeled data .
kernel - based methods are increasingly being used for data modeling and prediction be - cause of their conceptual simplicity and good performance on many tasks .
a promising family of semi - supervised learning methods can be viewed as constructing kernels by trans - forming the spectrum of a local similarity graph over labeled and unlabeled data .
these kernels , or regularizers , penalize functions that are not smooth over the graph ( 123 ) .
infor - mally , a smooth eigenvector has the property that two elements of the vector have similar values if there are many large weight paths between them on the graph .
this results in the desirable behavior of the labels varying smoothly over the graph , as sought by , e . g . , spectral clustering approaches ( 123 ) , diffusion kernels ( 123 ) , and the gaussian random eld approach ( 123 ) .
however , the modication to the spectrum , called a spectral transformation , is often a function chosen from some parameterized family .
as examples , for the diffusion kernel the spectral transformation is an exponential function , and for the gaussian eld kernel the transformation is a smoothed inverse function .
in using a parametric approach one faces the difcult problem of choosing an appropriate family of spectral transformations .
for many familes the number of degrees of freedom in the parameterization may be insufcient to accurately model the data .
in this paper
we propose an effective nonparametric method to nd an optimal spectral transformation using kernel alignment .
the main advantage of using kernel alignment is that it gives us a convex optimization problem , and does not suffer from poor convergence to local minima .
a key assumption of a spectral transformation is monotonicity , so that unsmooth functions over the data graph are penalized more severly .
we realize this property by imposing order constraints .
the optimization problem in general is solved using semi - denite programming ( sdp ) ( 123 ) ; however , in our approach the problem can be formulated in terms of quadratically constrained quadratic programming ( qcqp ) , which can be solved more efciently than a general sdp .
this paper is structured as follows .
in section 123 we review some graph theoretic concepts and relate them to the construction of kernels for semi - supervised learning .
in section 123 we introduce convex optimization via qcqp and relate it to the more familiar linear and quadratic programming commonly used in machine learning .
section 123 poses the problem of kernel based semi - supervised learning as a qcqp problem with order constraints .
ex - perimental results using the proposed optimization framework are presented in section 123
the results indicate that the semi - supervised kernels constructed from the learned spectral transformations perform well in practice .
123 semi - supervised kernels from graph spectra we are given a labeled dataset consisting of input - output pairs ( ( x123 , y123 ) , .
, ( xl , yl ) ) and a ( typically much larger ) unlabeled dataset ( xl+123 , .
, xn ) where x is in some general input space and y is potentially from multiple classes .
our objective is to construct a kernel that is appropriate for the classication task .
since our methods use both the labeled and unlabeled data , we will refer to the resulting kernels as semi - supervised kernels .
more specically , we restrict ourselves to the transductive setting where the unlabeled data also serve as the test data .
as such , we only need to nd a good gram matrix on the points ( x123 , .
for this approach to be effective such kernel matrices must also take into account the distribution of unlabeled data , in order that the unlabeled data can aid in the classication task .
once these kernel matrices have been constructed , they can be deployed in standard kernel methods , for example support vector machines .
in this paper we motivate the construction of semi - supervised kernel matrices from a graph theoretic perspective .
a graph is constructed where the nodes are the data instances ( 123 , .
, n ) and an edge connects nodes i and j if a local similarity measure between xi and xj suggests they may have the same label .
for example , the local similarity measure can be the euclidean distance between feature vectors if x rm , and each node can con - nect to its k nearest neighbors with weight value equal to 123
the intuition underlying the graph is that even if two nodes are not directly connected , they should be considered similar as long as there are many paths between them .
several semi - supervised learning algorithms have been proposed under the general graph theoretic theme , based on techniques such as random walks ( 123 ) , diffusion kernels ( 123 ) , and gaussian elds ( 123 ) .
many of these methods can be unied into the regularization framework proposed by ( 123 ) , which forms the basis of the graph can be represented by an n n weight matrix w = ( wij ) where wij is the edge weight between nodes i and j , with wij = 123 if there is no edge .
we require the entries of w to be non - negative , and assume that it forms a symmetric matrix; it is not necessary for w itself to be positive semi - denite .
in semi - supervised learning w is an essential quantity; we assume it is provided by domain experts , and hence do not study its construction .
let d be a diagonal matrix where dii = pj wij is the degree of node i .
this allows us to dene the combinatorial graph laplacian as l = d w ( the normalized laplacian l = d123 / 123ld123 / 123 can be used as well ) .
we denote ls eigensystem by ( i , i ) , so that l = pn i where we assume the eigenvalues are sorted in non - decreasing order .
the matrix l has many interesting properties ( 123 ) ; for instance , it is always positive
semi - denite , even if w is not .
perhaps the most important property of the laplacian related to semi - supervised learning is the following : a smaller eigenvalue corresponds to a smoother eigenvector over the graph; that is , the value pij wij ( ( i ) ( j ) ) 123 is small .
in a physical system the smoother eigenvectors correspond to the major vibration modes .
assuming the graph structure is correct , from a regularization perspective we want to encourage smooth functions , to reect our belief that labels should vary slowly over the graph .
specically , ( 123 ) and ( 123 ) suggest a general principle for creating a semi - supervised kernel k from the graph laplacian l : transform the eigenvalues into r ( ) , where the spectral transformation r is a non - negative and decreasing function123
k ) = ( p c123
note that it may be that r reverses the order of the eigenvalues , so that smooth is have larger eigenvalues in k .
a soft labeling function f = p cii in a kernel machine has a penalty term in the rkhs norm given by ( ||f ||123 i / r ( i ) ) .
since r is de - creasing , a greater penality is incurred for those terms of f corresponding to eigenfunctions that are less smooth .
in previous work r has often been chosen from a parametric family .
for example , the diffusion kernel ( 123 ) corresponds to r ( ) = exp ( 123 123 ) and the gaussian eld kernel ( 123 ) corresponds to r ( ) = 123 + .
cross validation has been used to nd the hyperparameters or for these spectral transformations .
although the general principle of equation ( 123 ) is appealing , it does not address question of which parametric family to use for r .
moreover , the number of degrees of freedom ( or the number of hyperparameters ) may not suit the task at hand , resulting in overly constrained kernels .
the contribution of the current paper is to address these limitations using a convex optimization approach by imposing an ordering constraint on r but otherwise not assuming any parametric form for
123 convex optimization using qcqp i , i = 123 n be the outer product matrices of the eigenvectors .
the semi - let ki = i> supervised kernel k is a linear combination k = pn i=123 iki , where i 123
we formulate the problem of nding the spectral transformation as one that nds the interpolation coef - cients ( r ( i ) = i ) by optimizing some convex objective function on k .
to maintain the positive semi - deniteness constraint on k , one in general needs to invoke sdps ( 123 ) .
semi - denite optimization can be described as the problem of optimizing a linear function of a symmetric matrix subject to linear equality constraints and the condition that the matrix be positive semi - denite .
the well - known linear programming problem can be generalized to a semi - denite optimization by replacing the vector of variables with a symmetric ma - trix , and replacing the non - negativity constraints with a positive semi - denite constraints .
this generalization inherits several properties : it is convex , has a rich duality theory and allows theoretically efcient solution algorithms based on iterating interior point methods to either follow a central path or decrease a potential function .
however , a limitation of sdps is their computational complexity ( 123 ) , which has restricted their application to small scale problems ( 123 ) .
however , an important special case of sdps are quadratically con - strained quadratic programs ( qcqp ) which are computationally more efcient .
here both the objective function and the constraints are quadratic as illustrated below ,
x>p123x + q>
123 x + r123
x>pix + q>
i x + ri 123
i = 123 m
ax = b
123we use a slightly different notation where r is the inverse of that in ( 123 ) .
+ , i = 123 , .
, m , where s n
+ denes the set of square symmetric positive where pi s n semi - denite matrices .
in a qcqp , we minimize a convex quadratic function over a feasible region that is the intersection of ellipsoids .
the number of iterations required to reach the solution is comparable to the number required for linear programs , making the approach feasible for large datasets .
however , as observed in ( 123 ) , not all sdps can be relaxed to qcqps .
for the semi - supervised kernel learning task presented here solving an sdp would be computationally infeasible .
recent work ( 123 , 123 ) has proposed kernel target alignment that can be used not only to assess the relationship between the feature spaces generated by two different kernels , but also to assess the similarity between spaces induced by a kernel and that induced by the labels themselves .
desirable properties of the alignment measure can be found in ( 123 ) .
the cru - cial aspect of alignnement for our purposes is that its optimization can be formulated as a qcqp .
the objective function is the empirical kernel alignment score :
a ( ktr , t ) =
hktr , t if
phktr , ktrif ht , t if
where ktr is the kernel matrix restricted to the training points , hm , n if denotes the frobe - nius product between two square matrices hm , n if = pij mijnij = t r ( m n > ) , and t is the target matrix on training data , with entry tij set to +123 if yi = yj and 123 otherwise .
note for binary ( +123 , 123 ) training labels y this is simply the rank one matrix t = yy is guaranteed to be positive semi - denite by constraining i 123
previous work using ker - nel alignment did not take into account that the kis were derived from the graph laplacian with the goal of semi - supervised learning .
as such , the is can take arbitrary values and there is no preference to penalize components that do not vary smoothly over the graph .
this can be rectied by requiring smoother eigenvectors to receive larger coefcients , as shown in the next section .
123 semi - supervised kernels with order constraints as stated above , we would like to maintain a decreasing order on the spectral transforma - tion i = r ( i ) to encourage smooth functions over the graph .
this motivates the set of
i = 123 n 123
and we can specify the desired semi - supervised kernel as follows .
denition 123 anorder constrained semi - supervised kernel k isthesolutiontothefollow -
a ( ktr , t )
k = pn
trace ( k ) = 123
i = 123 n 123
wheret isthetrainingtargetmatrix , ki = i>
the formulation is an extension to ( 123 ) with order constraints , and with special components kis from the graph laplacian .
since i 123 and kis are outer products , k will auto - matically be positive semi - denite and hence a valid kernel matrix .
the trace constraint is needed to x the scale invariance of kernel alignment .
it is important to notice the order constraints are convex , and as such the whole problem is convex .
let vec ( a ) be the column
vectorization of a matrix a .
dening m = ( cid : 123 ) vec ( k123 , tr ) vec ( km , tr ) ( cid : 123 ) , it is not hard to show that the problem can then be expressed as
the objective function is linear in , and there is a simple cone constraint , making it a quadratically constrained quadratic program ( qcqp ) .
||m || 123
i = 123 n 123
an improvement of the above order constrained semi - supervised kernel can be obtained by studying the laplacian eigenvectors with zero eigenvalues .
for a graph laplacian there will be k zero eigenvalues if the graph has k connected subgraphs .
the k eigenvectors are piecewise constant over individual subgraphs , and zero elsewhere .
this is desirable when k > 123 , with the hope that subgraphs correspond to different classes .
however if k = 123 , the graph is connected .
the rst eigenvector 123 is a constant vector .
the corresponding k123 is a constant matrix , and acts as a bias term .
in this situation we do not want to impose the order constraint 123 123 on the constant bias term .
instead we let 123 vary freely during
denition 123 an improved order constrained semi - supervised kernel k is the solution to
i = 123 n 123 , and i notconstant
in practice we do not need all n eigenvectors of the graph laplacian , or equivalently all n kis .
the rst m < n eigenvectors with the smallest eigenvalues work well empirically .
also note we could have used the fact that kis are from orthogonal eigenvectors i to further simplify the expression .
however we neglect this observation , making it easier to incorporate other kernel components if necessary .
it is illustrative to compare and contrast the order constrained semi - supervised kernels to other semi - supervised kernels with different spectral transformation .
we call the original kernel alignment solution in ( 123 ) a maximal - alignment kernel .
it is the solution to deni - tion 123 without the order constraints ( 123 ) .
because it does not have the additional constraints , it maximizes kernel alignment among all spectral transformation .
the hyperparameters and of the diffusion kernel and gaussian elds kernel ( described earlier ) can be learned by maximizing the alignment score also , although the optimization problem is not neces - sarily convex .
these kernels use different information from the original laplacian eigen - values i .
the maximal - alignment kernels ignore i altogether .
the order constrained semi - supervised kernels only use the order of i and ignore their actual values .
the diffu - sion and gaussian eld kernels use the actual values .
in terms of the degree of freedom in choosing the spectral transformation is , the maximal - alignment kernels are completely free .
the diffusion and gaussian eld kernels are restrictive since they have an implicit parametric form and only one free parameter .
the order constrained semi - supervised ker - nels incorporates desirable features from both approaches .
123 experimental results we evaluate the order constrained kernels on seven datasets .
baseball - hockey ( 123 in - stances / 123 classes ) , pc - mac ( 123 / 123 ) and religion - atheism ( 123 / 123 ) are document catego - rization tasks taken from the 123 - newsgroups dataset .
the distance measure is the standard cosine similarity between tf . idf vectors .
one - two ( 123 / 123 ) , odd - even ( 123 / 123 ) and ten digits ( 123 / 123 ) are handwritten digits recognition tasks .
one - two is digits 123 vs .
123; odd - even is the articial task of classifying odd 123 , 123 , 123 , 123 , 123 vs .
even 123 , 123 , 123 , 123 , 123 digits ,
such that each class has several well dened internal clusters; ten digits is 123 - way clas - sication .
isolet ( 123 / 123 ) is isolated spoken english alphabet recognition from the uci repository .
for these datasets we use euclidean distance on raw features .
we use 123nn unweighted graphs on all datasets except isolet which is 123nn .
for all datasets , we use the smallest m = 123 eigenvalue and eigenvector pairs from the graph laplacian .
these values are set arbitrarily without optimizing and do not create a unfair advantage to the proposed kernels .
for each dataset we test on ve different labeled set sizes .
for a given labeled set size , we perform 123 random trials in which a labeled set is randomly sampled from the whole dataset .
all classes must be present in the labeled set .
the rest is used as unlabeled ( test ) set in that trial .
we compare 123 semi - supervised kernels ( improved order constrained kernel , order constrained kernel , gaussian eld kernel , diffusion kernel123 and maximal - alignment kernel ) , and 123 standard supervised kernels ( rbf ( bandwidth learned using 123 - fold cross validation ) , linear and quadratic ) .
we compute the spectral transforma - tion for order constrained kernels and maximal - alignment kernels by solving the qcqp using standard solvers ( sedumi / yalmip ) .
to compute accuracy we use a standard svm .
we choose the the bound on slack variables c with cross validation for all tasks and ker - nels .
for multiclass classication we perform one - against - all and pick the class with the the results123 are shown in table 123 , which has two rows for each cell : the upper row is the average test set accuracy with one standard deviation; the lower row is the average training set kernel alignment , and in parenthesis the average run time in seconds for se - dumi / yalmip on a 123ghz linux computer .
each number is averaged over 123 random trials .
to assess the statistical signicance of the results , we perform paired t - test on test accuracy .
we highlight the best accuracy in each row , and those that can not be determined as different from the best , with paired t - test at signicance level 123 .
the semi - supervised kernels tend to outperform standard supervised kernels .
the improved order constrained kernels are consistently among the best .
figure 123 shows the spectral transformation i of the semi - supervised kernels for different tasks .
these are for the 123 trials with the largest labeled set size in each task .
the x - axis is in increasing order of i ( the original eigenvalues of the laplacian ) .
the mean ( thick lines ) and 123 standard deviation ( dotted lines ) of only the top 123 is are plotted for clarity .
the i values are scaled vertically for easy compari - son among kernels .
as expected the maximal - alignment kernels spectral transformation is zigzagged , diffusion and gaussian elds are very smooth , while order constrained kernels are in between .
the order constrained kernels ( green ) have large 123 because of the order constraint .
this seems to be disadvantageous the spectral transformation tries to balance it out by increasing the value of other is so that the constant k123s relative inuence is smaller .
on the other hand the improved order constrained kernels ( black ) allow 123 to be small .
as a result the rest is decay fast , which is desirable .
we have proposed and evaluated a novel approach for semi - supervised kernel construction using convex optimization .
the method incorporates order constraints , and the resulting convex optimization problem can be solved efciently using a qcqp .
in this work the base kernels were derived from the graph laplacian , and no parametric form for the spectral transformation was imposed , making the approach more general than previous approaches .
experiments show that the method is both computationally feasible and results in improve - ments to classication performance when used with support vector machines .
123 and are learned with the fminbnd ( ) function in matlab to maximize
123results on baseball - hockey and odd - even are similar and omitted for space .
full results can be
found at http : / / www . cs . cmu . edu / zhuxj / pub / ocssk . pdf
religion vs .
atheism
ten digits ( 123 classes )
isolet ( 123 classes )
figure 123 : comparison of spectral transformation for the 123 semi - supervised kernels .
