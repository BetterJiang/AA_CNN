graph - based methods for semi - supervised learn - ing have recently been shown to be promising for combining labeled and unlabeled data in classi - cation problems .
however , inference for graph - based methods often does not scale well to very large data sets , since it requires inversion of a large matrix or solution of a large linear program .
moreover , such approaches are inherently trans - ductive , giving predictions for only those points in the unlabeled set , and not for an arbitrary test point .
in this paper a new approach is presented that preserves the strengths of graph - based semi - supervised learning while overcoming the lim - itations of scalability and non - inductive infer - ence , through a combination of generative mix - ture models and discriminative regularization us - ing the graph laplacian .
experimental results show that this approach preserves the accuracy of purely graph - based transductive methods when the data has manifold structure , and at the same time achieves inductive learning with sig - nicantly reduced computational cost .
the availability of large data collections , with only limited human annotation , has turned the attention of a growing community of machine learning researchers to the problem of semi - supervised learning .
the broad research agenda of semi - supervised learning is to develop methods that can leverage a large amount of unlabeled data to build more accurate classication algorithms than can be achieved us - ing purely supervised learning .
an attractive new family of semi - supervised methods is based on the use of a graphi - cal representation of the unlabeled dataexamples of this
appearing in proceedings of the 123 nd international conference on machine learning , bonn , germany , 123
copyright 123 by
paradigm include the work of blum and chawla ( 123 ) ; zhu et al .
( 123 ) ; zhou et al .
( 123 ) ; belkin et al .
( 123a ) .
many graph - based methods are inherently transductive in nature : a graph is formed with vertices representing the la - beled and unlabeled data , and a graph algorithm is used to somehow separate the nodes according to the predicted class labels .
however , when a new data point is presented , it is unclear how to make a predictionother than to re - build the graph with the new test point and rerun the graph algorithm from scratch .
since this may involve solving a large linear program or inverting a huge matrix , these procedures have limited generalization ability .
yet semi - supervised methods should be most attractive when the un - labeled data set is extremely large , and thus scalability be - comes a central issue .
in this paper we address the prob - lems of scalability and non - inductive inference by combin - ing parametric mixture models with graph - based methods .
our approach is related to , but different from , the recent work of delalleau et al .
( 123 ) and belkin et al .
( 123b ) .
the mixture model has long been recognized as a natural approach to modeling unannotated data; indeed , some of the earliest studies of the semi - supervised learning prob - lem investigated the statistical or learning - theoretic ef - ciency of estimating mixture models through a combina - tion of labeled and unlabeled data ( castelli & cover , 123; ratsaby & venkatesh , 123 ) .
as a generative model , a mixture is naturally inductive , and typically has a relatively small number of parameters .
various applied studies sug - gested that multinomial mixtures can be effective at us - ing unlabeled data for classifying text documents ( nigam et al . , 123 ) , where the learning is typically carried out using the em algorithm to estimate the map model over the unlabeled set .
however , the anecdotal evidence is that many more studies were not published because they ob - tained negative results , showing that learning a mixture model will often degrade the performance of a model t using only the labeled data; one published study with these conclusions is ( cozman et al . , 123 ) .
one of the reasons for this phenomenon is that the data may have a manifold structure that is incompatible with the generative mixture
harmonic mixtures : combining mixture models and graph - based methods for inductive and scalable semi - supervised learning
model; thus em may have difculty in making the labels follow the manifold .
an illustrative example is given in the left plot of figure 123
the desired behavior is shown in the right plot , which is achieved using the harmonic mixture model presented in this paper .
mixture models and graph - based semi - supervised learning methods make different assumptions about the relation be - tween the data and the labelsbut these assumptions are not mutually exclusive .
it is possible that the data ts the component model ( a gaussian , for example ) locally , while the manifold structure appears globally .
the present work attempts to combine the strengths of both approaches .
we show how the mixture model can be combined with the graph to yield a much smaller backbone graph with nodes induced from the mixture components .
the number of mix - ture components controls the size of the backbone graph , leading to computationally efcient algorithms .
the har - monic mixture is a special case of our general framework , where a harmonic function ( zhu et al . , 123 ) is induced over the backbone graph to specify the class membership of the mixture model .
since the mixture model is genera - tive it handles new points , while the graph allows the labels to follow the data manifold .
importantly , our procedure for combining the mixture model with the backbone graph in - volves a convex optimization problem .
after a brief overview of mixture models and previous work on graph - based semi - supervised learning in sec - tion 123 , we detail our combined approach in section 123
experimental results for synthetic data , handwritten dig - its recognition , image analysis and text categorization are given in section 123
the paper concludes with a discussion and summary of the results .
background and notation
let ( xl , yl ) = ( ( x123 , y123 ) .
( xl , yl ) ) be the labeled data .
for simplicity we consider binary classication , with y ( 123 , 123 ) .
let xu = ( xl+123 .
xn ) be the unlabeled data , and u = n l .
the letters l and u will be used to repre - sent the labeled and unlabeled data , respectively .
in semi - supervised learning the goal is to learn a classier from both ( xl , yl ) and xu .
mixture models
in the standard view of a mixture model for classication , the generative process is to sample m mult ( y ) from a multinomial depending on the class y , and to then sam - ple x g ( m ) for some generative model g .
we will work with a different , but equivalent view where a mixture component m mult ( ) is rst sampled from a multino - mial model over m outcomes , where m is the number of mixture components .
then , the label y mult ( m ) and
features x g ( m ) are generated ( conditionally indepen - dently ) given m .
note that p ( y | m ) can take soft values between 123 and 123 , enabling classes to share a mixture com - ponent .
in unlabeled data , both the mixing component m and class label y is latent for each example .
the param - eters of the mixture model are = ( ( m , m , m ) ) m the em algorithm is the standard procedure for estimat - ing the parameters to maximize the incomplete likelihood
l ( ) =qipm , y p ( xi | m ) p ( y | m ) m .
combining the
labeled and unlabeled data together , the log likelihood is
` ( ) = xil
m myi p ( xi | m ) +
m p ( xi | m )
label smoothness on the data graph
graph - based semi - supervised learning methods are based on the principle that the label probability should vary smoothly over the data graph .
the graph has n nodes , with two nodes connected by a ( weighted ) edge if they are deemed similar according to some similarity function , cho - sen by prior knowledge .
the graph is thus represented by the n n symmetric weight matrix w ; the combinatorial laplacian is = d w , where the diagonal degree ma -
trix satises dii =pj wij .
label smoothness can be expressed in different ways .
we adopt the energy used in semi - supervised learning by zhu et al .
( 123 ) , given by
e ( f ) =
wij ( fi fj ) 123 = f >f
where f is the label posterior vector of a mixture model ,
p ( yi = 123 | xi , )
that is , fi is the probability that point i has label y = 123 under the mixture model ; since this is a function of the parameters of the mixture , we will write it also as e ( ) .
the energy is small when f varies smoothly over the graph .
zhu et al .
( 123 ) proposed the use of the harmonic solution f = 123 subject to the constraints specied by the labeled data .
since is the combinatorial laplacian , this implies that for i u , fi is the average of the values of f at neigh - boring nodes ( the harmonic property ) , which is consistent with the smoothness assumption .
alternative measures of smoothness are based on the normalized laplacian ( zhou et al . , 123 ) or spectral transforms ( zhu et al . , 123 ) , and work similarly in the framework below .
note that by extending the notion of a weighted graph to allow self - transitions and possibly negative edge weights ,
harmonic mixtures : combining mixture models and graph - based methods for inductive and scalable semi - supervised learning
any symmetric matrix with zero row sums can be consid - ered to be a graph laplacian .
we will make use of this notion of a generalized laplacian in the following section .
em estimation .
the second advantage is that , as we will show below , o ( , , ) is a convex function of .
combining mixture models and graph
our goal is to combine the mixture model and graph - based learning algorithms .
intuitively , to enforce the smoothness assumption encoded in the weight matrix w , we might reg - ularize the mixture model by the energy of the posterior with respect to the graph .
this leads naturally to minimiza - tion of the objective function
o ( ) = ` ( ) + ( 123 ) e ( )
this objective function makes explicit the tension between maximizing the data likelihood and minimizing the graph energy .
the most direct way of proceeding is to estimate parameters = ( , , ) to minimize the objective o ( ) where ( 123 , 123 ) is a coefcient that controls the relative strength of the two terms .
note that while the energy e ( f ) may appear to be the log - arithm of a prior p ( f ) exp ( f >f ) , it in fact involves the observed labels yl , since f is xed on the labeled data .
thus , it is perhaps best thought of as a discriminative com - ponent of the objective function , while ` ( ) is the genera - tive component .
in other words , optimizing o will carry out a combination of discriminative and generative learn - ing .
this is closely related to , but different from , the graph regularization framework of belkin et al .
( 123b ) .
unfortunately , learning all of the parameters together is difcultsince the energy e ( ) is discriminative , em training is computationally demanding as the m - step does not have a closed - form solution; moreover , it has the usual drawback of local minima .
we propose instead the follow - ing two - step approach .
select the number of mixture components m , and initialize the parameters = ( , , ) .
train the mixture model using the objective function
o123 ( ) = ` ( ) with standard em .
fix and , and reestimate the multinomial to min -
figure123
training combining ` ( ) and e ( ) .
clearly this algorithm is suboptimal in terms of optimizing the objective function .
however it has two important ad - vantages .
one advantage is that the rst step is standard
convexity of o
o involves ` ( ) and e ( ) .
first let us consider ` ( ) .
in ( 123 ) the sum over i u is constant w . r . t .
the sum over i l can be written as
`l ( ) = xil , yi=123
m m p ( xi | m ) +
m ( 123 m ) p ( xi | m )
since we x and , the term within the rst sum has the
form logpm amm .
it can be directly veried that its hes -
sian is negative - denite :
( cid : 123 ) logpm amm
( pm amm ) 123 ( cid : 123 ) 123
a similar calculation shows that the hessian for the second term is negative - denite as well .
thus `l ( ) is concave in , and ` ( ) is convex .
next let us consider e ( ) .
dene a u m responsibility matrix r by rim = p ( m | xi ) , depending on and , with rm denoting the m - th column .
we can write f u = r .
we partition into labeled and unlabeled parts , with u u being the submatrix on unlabeled points , ll on labeled points and so on .
the graph energy is written as
e ( ) = f >f
= f > = f >
lllf l + 123f > lllf l + 123f >
llu f u + f > llu r + >r>u u r
u u u f u
since u u ( cid : 123 ) 123 , the hessian 123r>u u r ( cid : 123 ) 123 is positive semi - denite in .
thus ( 123 ) e ( ) is convex in .
putting it together , o is convex in .
special case : = 123
the graph - discriminative training in step 123 has a very spe - cial structure , which we now explain .
we rst consider the special case = 123 and then the general case ( 123 , 123 ) .
the case = 123 has a particularly simple closed form solution and interpretation .
notice that although = 123 , the solution depends on the incomplete log - likelihood ` ( ) through the choice of and learned in step 123
the parameters are constrained within ( 123 , 123 ) m .
however rst let us consider that minimize e in the unconstrained problem .
the solution to the linear system
e = r> ( 123u u r + 123u lf l ) = 123
harmonic mixtures : combining mixture models and graph - based methods for inductive and scalable semi - supervised learning
figure123predictions of gaussian mixture models learned with the standard em algorithm do not follow the manifold structure .
small dots are unlabeled data .
two labeled points , marked with a white + and a black box , are at roughly the ends of the spirals .
each plot shows a gaussian mixture model with m = 123 components , with the ellipses showing contours of the covariance matrices .
the central dots have sizes proportional to the component weight p ( m ) = m ( tiny components are not plotted ) , and its brightness indicates the strength of class membership , given by m p ( y = 123 | m ) : white denotes m = 123 , black denotes m = 123 , and intermediate gray denotes values in between .
although the density p ( x ) is estimated well by the standard mixture t using em ( left ) , does not follow the data manifold .
the right plot shows the harmonic mixture , where is ret to be harmonic on the backbone graph .
is given by
? = ( r>u u r ) 123 r>u lf l
note that , in general , the constraints 123 m 123 must also be explicitly enforced .
if the above solution lies in the interior of the hypercube ( 123 , 123 ) m then it must also be the solution of the constrained problem . 123 in this case , ( 123 ) de - termines the class membership probabilities for each mix - ture component the soft label for the unlabeled data is given by f u = r .
previously unseen test points can be
compare the solution ( 123 ) , which we will refer to as the har - monic mixture , with the completely graph - based harmonic function solution ( zhu et al . , 123 ) :
u u u lf l
f u = 123 f u = r ( r>u u r ) 123 r>u lf l
computationally , obtaining the harmonic mixture requires the inversion of an m m matrix , or if the solution lies on
123more generally , the karush - kuhn - tucker optimality condi - tions imply that the harmonic mixture can be expressed as ? = `r>u u r123 `r>u lf l + , where is a vector of la - grange multipliers .
geometrically , this can be viewed as the solu - tion of an inhomogeneous dirichlet boundary value problem for a generalized laplacian .
computationally if some m are out of bounds , we clip them as the starting point for constrained convex optimization , which converges quickly .
pseudo inverse is used if r is rank decient .
the boundary solving the associated constrained optimiza - tion problem .
solving the system ( 123 ) will be much less computationally expensive than the u u matrix inversion required by harmonic solution , when the number of mix - ture components m is much smaller than the number of unlabeled points u .
this reduction is possible because the f u are now obtained by marginalizing the mixture model .
graphical interpretation
the procedure just described can be interpreted graphically in terms of a much smaller backbone graph with supern - odes induced by the mixture components .
the backbone graph has the same l labeled nodes as in the original graph , but only m unlabeled supernodes .
by rearranging terms it is not hard to show that in the back - bone graph , the generalized laplacian is
123 r ( cid : 123 ) > ( cid : 123 ) ll lu
u l u u ( cid : 123 ) ( cid : 123 ) i
e = ( cid : 123 ) i
123 r ( cid : 123 ) ( 123 ) note that e has zero row sums .
the harmonic mixture
parameter is then a harmonic function on the generalized laplacian .
the harmonic mixture algorithm is summarized in figure 123
perhaps the best intuition for the backbone graph comes from considering hard clustering .
in this case rim = 123 if m is the cluster to which point i belongs , and rim = 123
harmonic mixtures : combining mixture models and graph - based methods for inductive and scalable semi - supervised learning
harmonicmixturetraining : = 123
select the number of mixture components m , initialize the parameters = ( , , ) , and form the graph laplacian .
run standard em to obtain , , and .
fixing and , compute
form the generalized laplacian e , from ( 123 ) .
? = arg min >eu u + 123>eu lfl
output : harmonic mixture model = ( , ? , )
figure123
the harmonic mixture algorithm , = 123
otherwise .
let c ( m ) = ( i | rim = 123 ) denote cluster m .
in this case the supernodes are the clusters themselves .
let wij be the weight between nodes i , j in the original graph .
the equivalent weight between supernodes ( s , t ) reduces
between a supernode s and a labeled node j l is wsj =
to wst = pic ( s ) , jc ( t ) wij; and the equivalent weight pic ( s ) wij .
in this case the solution ( 123 ) is also guaranteed
to satisfy the constraints .
one can create such a backbone graph using , for instance , k - means clustering .
general case : > 123
in the general case of > 123 step 123 does not have a closed - form solution .
as m must lie in the interval ( 123 , 123 ) , we perform constrained convex optimization in this case .
the gradient of the objective function is easily computed
= xil , yi=123
m p ( xi | m ) k=123 k p ( xi | k ) k m p ( xi | m )
k=123 k p ( xi | k ) ( 123 k )
and e / was given in ( 123 ) .
one can also use sigmoid function to transform it into an unconstrained optimization problem with m = ( m ) = 123 / ( exp ( m ) + 123 ) and optimize the parameters .
although the objective function is convex , a good starting point for is important for fast convergence .
we select an initial value for by solving a one dimensional problem rst .
we have two parameters at hand : em , the solution from the standard em algorithm in step 123 , and hm , the harmonic mixture solution from the special case = 123
we nd the optimal interpolated coefcient ( 123 , 123 ) 123 = em + ( 123 ) hm that minimizes the objective function .
then , we start from 123 and use a quasi - newton algorithm to nd the global optimum for .
we test harmonic mixtures on synthetic data , handwrit - ten digits , image analysis and text categorization tasks .
the emphases are on how the harmonic mixtures ( denoted hm below ) perform against several baseline methods on unlabeled data; how they handle unseen data; and whether they can reduce the problem size .
unless otherwise noted , the harmonic mixtures are computed with = 123
we use three baseline methods : sampling unlabeled data to create a smaller graph ( sample ) , mixture models con - structed with the standard em algorithm ( em ) , and har - monic functions on the original large graphs ( graph ) .
in sample , we randomly draw m unlabeled points from u .
we create a small ( size l + m ) graph with these and the labeled points , and compute the harmonic func - tion fi on the small graph rst .
the graph computa - tion cost is thus the same as hm .
then as in ( delalleau et al . , 123 ) , we compute the labels for other points j by
fj = ( pl+m
synthetic data
first let us look at the synthetic dataset in figure 123
it has a swiss roll structure , and we hope the labels can follow the spiral arms .
there is one positive and one negative labeled point , at roughly the opposite ends .
we use u = 123 un - labeled points and an additional 123 points as unseen test
the mixture model and standard em .
to illustrate the idea , consider a gaussian mixture model ( gmm ) with m = 123 components , each with full covariance .
the left panel shows the converged gmm after running em .
the gmm models the manifold density p ( x ) well .
however the component class membership m p ( y = 123 | m ) ( bright - ness of the central dots ) does not follow the manifold .
in fact takes the extreme values of 123 or 123 along a somewhat linear boundary instead of following the spiral arms , which is undesirable .
the classication of data points will not follow the manifold either .
the graph and harmonic mixtures .
next we combine the mixture model with a graph to compute the harmonic mix - tures , as in the special case = 123
we construct a fully connected graph on the l u data points with weighted
edges wij = exp ( cid : 123 ) ||xi xj||123 / 123 ( cid : 123 ) .
the weight pa -
rameters in all experiments are selected with 123 - fold cross validation .
we then reestimate , which are shown in the right panel of figure 123
note now follow the manifold as it changes from 123 ( black ) to approximately 123 ( gray ) and nally 123 ( white ) .
this is the desired behavior .
the particular graph - based method we use needs extra care .
the harmonic function solution f is known to sometimes
harmonic mixtures : combining mixture models and graph - based methods for inductive and scalable semi - supervised learning
skew toward 123 or 123
this problem is easily corrected if we know or have an estimate of the proportion of positive and negative points , with the class mass normalization heuris - tic ( zhu et al . , 123 ) .
in this paper we use a similar but sim - pler heuristic .
assuming the two classes are about equal in size , we simply set the decision boundary at median ( f ) .
sensitivity to m .
if the number of mixture components m is too small , the gmm is unable to model p ( x ) well , let alone .
in other words , the harmonic mixture is sensitive to m .
m has to be larger than a certain threshold so that the manifold structure can appear .
in fact m may need to be larger than the number of labeled points l , which is unusual in traditional mixture model methods for semi - supervised learning .
but once m is over the threshold , further increase should not dramatically change the solution .
in the end the harmonic mixture may approach the harmonic function solution when m = u .
figure 123 ( top left ) shows the classication accuracies on u as we change m .
graph is the ideal performance .
we nd that hm threshold is around m = 123 , at which point the accuracy jumps up and stabilizes thereafter .
this is the number of mixture components needed for hm to capture the manifold structure .
sample needs far more samples ( m > 123 , not shown ) to reach 123% accuracy .
em fails to make the labels to follow the manifold structure regard - less of the number of mixtures .
computational savings .
hm performs almost as good as graph but with a much smaller problem size .
as fig - ure 123 ( left ) shows we only need to invert a 123 123 matrix , instead of a 123 123 one as required by graph .
the dif - ference can be signicant if u is even larger .
there is of course the overhead of em training .
handling unseen data .
because hm is a mixture model , it naturally handles unseen points .
on 123 new test points hm performs well , with accuracy 123% after m 123 as shown in figure 123 ( bottom left ) .
note graph cannot handle unseen data and is therefore not shown in the plot .
handwritten digits recognition
we use the 123vs123 dataset which contains handwritten dig - its of 123s and 123s .
each gray scale image is 123 123 , which is represented by a 123 dimensional vector of pixel values .
we use l + u = 123 images as the labeled and unlabeled set , and 123 additional images as unseen new data to test induction .
the total numbers of 123s and 123s are the same .
the mixture model .
we use gaussian mixture models .
to avoid data sparseness problem , we model each gaussian component with a spherical covariance , i . e .
diagonal co - variance matrix with the same variance in all dimensions .
different components may have different variances .
we set the initial means and variances of the gmm with k - means
algorithm before running em .
the graph .
we use a symmetrized 123 - nearest - neighbor weighted graph on the 123 images .
that is , images i , j are connected if i is within js 123nn or vice versa , as measured by euclidean distance .
the weights are wij =
sensitivity to m .
as illustrated in the synthetic data , the number of mixture components m needs to be large enough for harmonic mixture to work .
we vary m and observe the classication accuracies on the unlabeled data with different methods .
for each m we perform 123 trials with random l / u split , and plot the mean of classication accuracies on u in figure 123 ( top center ) .
the experiments were performed with labeled set size xed at l = 123
we conclude that hm needs only m 123 components to match the performance of graph .
hm outperforms both sample and em .
computational savings .
in terms of graph method compu - tation , we invert a 123 123 matrix instead of the original 123 123 matrix for harmonic function .
this is good saving with little sacrice in accuracy .
handling unseen data .
on 123 unseen data points ( figure 123 bottom center ) , hm is better than sample and em
the general case > 123
we also vary the parameter between 123 and 123 , which balances the generative and dis - criminative objectives .
in our experiments = 123 always gives the best accuracies .
teapots image analysis
we perform binary classication on the teapots dataset , which was previously used for dimensionality reduction .
see ( weinberger et al . , 123 ) for details .
the dataset con - sists of a series of teapot photos , each rotated by a small angles .
our task is to identify whether the spout points to the left or the right .
excluding the few images from the original dataset in which the spout is roughly in the middle , we arrive at 123 images .
we process each image by con - verting it to gray scale and down - sizing it to 123 123
each image is thus represented by a 123 - dimensional vector of pixel values .
nonetheless we believe the dataset resides on a much lower dimensional manifold , since image pairs in which the teapot rotates by a small angle are close to each other .
therefore we expect graph - based semi - supervised learning methods to perform well on the dataset .
we use 123 images as l u , and the remaining 123 as unseen test
the mixture model .
we again use gaussian mixture mod - els with spherical covariances .
we initialize the models with k - means before running em .
harmonic mixtures : combining mixture models and graph - based methods for inductive and scalable semi - supervised learning
accuracies on unlabeled training data u
accuracies on unseen test data ( induction )
figure123sensitivity to m in the synthetic data ( left ) , 123 vs .
123 ( center ) and teapots ( right ) .
shown are the classication accuracies on u ( top row ) and unseen new data ( bottom row ) as m changes .
graph is the harmonic function on the complete l u graph; hm is the harmonic mixture , sample is the smaller graph with sampled unlabeled data , and em is the standard em algorithm .
note y - axes are not on the same scale .
the graph .
we use a symmetrized 123nn weighted graph on the 123 images , with weights wij =
sensitivity to m .
the classication accuracies on u with different number of components m is shown in figure 123 ( top right ) .
each curve is the average of 123 trials .
xed at ( merely ) 123
with m > 123 components , hm ap - proaches the graph performance .
sample and em are
computational savings .
the graph computation for hm inverts a 123 123 matrix , which is much cheaper than 123 123 for graph .
handling unseen data .
hm performs similarly on the 123 unseen images ( figure 123 bottom right ) , achieving high ac - curacy with small m , and outperforms sample and em .
text categorization : a discussion
we also perform text categorization on the pc vs .
mac groups from the 123 - newsgroups data .
of the 123 docu - ments , we use 123 as lu and the rest as unseen test data .
we use a symmetrized 123nn weighted graph on the 123 documents with weight wuv = exp ( ( 123 cuv ) / 123 ) , where cuv is the cosine between the tf . idf document vec -
tors u , v .
with l = 123 , graph accuracy is around 123% .
we use multinomial mixture models on documents .
however unlike other tasks , hm suffers from a loss of transductive accuracy , and only reaches 123% accuracy on u and unseen data .
it does so with an undesirably large m around 123
furthermore hm and sample perform about the same ( though both are better than em ) .
why does hm perform well on other tasks but not on text categorization ? we think the reasons are : 123 ) the mani - fold assumption needs to hold strongly .
for instance in the synthetic and the teapots data the manifolds are evident , and hm achieved close approximations to graph .
the text data seems to have a weaker manifold structure .
123 ) on top of that , the text data has a very high dimensionality ( d = 123 ) .
the curse of dimensionality may prevent a generative mixture model from tting the manifold well .
in addition the multinomial model may not be appropriate for creating localized supernodes .
interestingly we do not have to use generative models .
if we work with = 123 , all we need from the mixture model is the responsibility r .
one can instead rst use simple pro - totype methods like k - means to cluster the data , and then train discriminative models to obtain r .
this remains a future research direction .
harmonic mixtures : combining mixture models and graph - based methods for inductive and scalable semi - supervised learning
related work
recently delalleau et al .
( 123 ) used a small subset of the unlabeled data to create a small graph for semi - supervised learning .
this is related to the nystrom method in spec - tral clustering ( fowlkes et al . , 123 ) , and to the random landmarks in dimensionality reduction ( weinberger et al . , 123 ) .
our method is different in that it incorporates a gen - erative mixture model , which is a second knowledge source besides the original graph .
our method outperforms ran - dom subset selection , and can be viewed as a principled way to carry out the elaborate subset selection heuristics in ( delalleau et al . , 123 ) .
in terms of handling unseen data , our approach is closely related to the regularization framework of belkin et al .
( 123b ) ; krishnapuram et al .
( 123 ) as graph regulariza - tion on mixture models .
but instead of a regularization term we used a discriminative term , which allows for the closed form solution in the special case .
to summarize , our proposed harmonic mixture method reduces the graph problem size , and handles unseen test points .
it achieves comparable accuracy as the harmonic function on complete graph for semi - supervised learning .
there are some open questions .
one is when > 123 would be useful in practice .
another is whether we can use fast prototype methods instead of em .
finally , we want to au - tomatically select the appropriate number of mixture com -
we thank the reviewers for their useful comments , and guy lebanon and lillian lee for interesting discussions .
