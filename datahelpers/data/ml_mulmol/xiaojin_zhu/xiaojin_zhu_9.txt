tcp throughput prediction is an important capability in wide area overlay and multi - homed networks where multiple paths may exist between data sources and receivers .
in this paper we describe a new , lightweight method for tcp throughput prediction that can gener - ate accurate forecasts for a broad range of le sizes and path condi - tions .
our method is based on support vector regression modeling that uses a combination of prior le transfers and measurements of simple path properties .
we calibrate and evaluate the capabilities of our throughput predictor in an extensive set of lab - based exper - iments where ground truth can be established for path properties using highly accurate passive measurements .
we report the perfor - mance for our method in the ideal case of using our passive path property measurements over a range of test congurations .
our results show that for bulk transfers in heavy trafc , tcp through - put is predicted within 123% of the actual value 123% of the time , representing nearly a 123 - fold improvement in accuracy over prior in the same lab environment , we assess our method using less accurate active probe measurements of path properties , and show that predictions can be made within 123% of the actual value nearly 123% of the time over a range of le sizes and trafc conditions .
this result represents approximately a 123% improvement over history - based methods with a much lower impact on end - to - end paths .
finally , we implement our predictor in a tool called pathperf and test it in experiments conducted on wide area paths .
the results demonstrate that pathperf predicts tcp through - put accurately over a variety of paths .
categories and subject descriptors : c . 123 ( performance of sys - tems ) : measurement techniques general terms : measurement keywords : tcp throughput prediction , active measurements , ma - chine learning , support vector regression
the availability of multiple paths between sources and receivers enabled by content distribution , multi - homing , and overlay or vir -
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page .
to copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specic permission and / or a fee .
sigmetrics123 , june 123 , 123 , san diego , california , usa .
copyright 123 acm 123 - 123 - 123 - 123 - 123 / 123 / 123 . . . $123 .
tual networks suggests the need for the ability to select the best path for a particular data transfer .
a common starting point for this problem is to dene best in terms of the throughput that can be achieved over a particular path between two end hosts for a given sized tcp transfer .
in this case , the fundamental challenge is to develop a technique that provides an accurate tcp throughput fore - cast for arbitrary and possibly highly dynamic end - to - end paths .
there are several difculties in generating accurate tcp through - put predictions .
prior work on the problem has largely fallen into those that investigate formula - based approaches and those that investigate history - based approaches .
formula - based methods , as the name suggests , predict throughput using mathemat - ical expressions that relate a tcp senders behavior to path and end host properties such as rtt , packet loss rate , and receive window size .
in this case , different measurement tools can be used to gather the input data that is then plugged into the formula to generate a pre - diction .
however , well - known network dynamics and limited in - strumentation access complicate the basic task of gathering timely and accurate path information , and the ever evolving set of tcp implementations means that a corresponding set of formula - based models must be maintained .
history - based tcp throughput prediction methods are concep - tually straightforward .
they typically use some kind of standard time series forecasting based on throughput measurements derived from prior le transfers , gathered either passively ( e . g . , by tapping a link ) or actively ( e . g . , by periodically sending a le ) .
in recent work , he et al .
show convincingly that history - based methods are generally more accurate than formula - based methods .
however , the authors carefully outline the conditions under which history - based prediction can be effective ( 123 ) .
also , history - based approaches described to date remain relatively inaccurate and potentially heavy weight processes focused on bulk transfer throughput prediction .
our goal is to develop an accurate , lightweight tool for predicting end - to - end tcp throughput for arbitrary le sizes .
we investigate the hypothesis that the accuracy of history - based predictors can be improved and their impact on a path reduced by augmenting the pre - dictor with periodic measurements of simple path properties .
the questions addressed in this paper include : 123 ) which path proper - ties or combination of path properties increase the accuracy of tcp throughput prediction the most ? and 123 ) what is a minimum set of le sizes required to generate history - based throughput predictors for arbitrary le sizes ? additional goals for our tcp throughput prediction tool are : 123 ) to make it robust to level shifts ( i . e . , when path properties change signicantly ) which he et al .
show to be a challenge in history - based predictors , and 123 ) to include a con - dence value with predictionsa metric with little treatment in prior history - based throughput predictors .
the analytical framework for the study that we report in this pa - per is based on the use of support vector regression ( svr ) , a pow - erful machine learning technique that has shown good empirical performance in many domains .
svr has several attractive proper - ties that make it well suited for our study : 123 ) it can accept multiple inputs ( i . e . , multivariate features ) and will use all of these to gen - erate the throughput prediction , which is a requirement for our ap - proach .
123 ) svr does not commit to any particular parametric form , unlike formula - based approaches .
instead , svr models are exible based on their use of so - called non - linear kernels .
this expressive power is an important reason for the potential for more accurate pre - dictions than formula - based methods .
123 ) svr is computationally efcient , which makes it attractive for inclusion in a tool that can be deployed and used in the wide area .
for our application , we extend the basic svr predictor with a condence interval estimator based on an assumption that prediction errors are normally distributed , an assumption that we test in our laboratory experiments .
estima - tion of condence intervals is critical for on - line prediction , since retraining can be triggered if measured throughput falls outside a condence interval computed through previous measurements .
we begin by using laboratory - based experiments to investigate the relationship between tcp throughput and measurements of path properties including available bandwidth ( ab ) , queuing delays ( q ) , and packet loss ( l ) .
the lab environment enables us to gather highly accurate passive measurements of throughput and all path properties , and develop and test our svr - based predictor over a range of realistic trafc conditions .
our initial experiments fo - cus on bulk transfers and compare ground truth measurements of throughput of target tcp ows ( i . e . , actual throughput ) with pre - dicted tcp throughput values generated by multiple instances of our svr - based tool trained with different combinations of path properties .
we compare the actual and predicted throughput using the relative prediction error ( e ) metric described in ( 123 ) .
our re - sults show that throughput predictions can be improved by as much as a factor of 123 when including path properties in the svr - based tool versus a history - based predictor .
for example , our results show that the svr - based predictions are within 123% of actual 123% of the time for bulk transfers under heavy trafc conditions ( 123% av - erage utilization on the bottleneck link ) .
interestingly , we nd that the path properties that provide the most improvement to the svr - based predictor are q and l respectively , and that including ab provides almost no improvement to the predictor .
toward our goal of developing a robust tool that can be used in the wide area , we expand the core svr - based tool in three ways .
first , the initial tests were based entirely on passive trafc measure - ments , which are unlikely to be widely available in the internet .
to address this , we tested our svr - based approach using measure - ments of q and l provided by the badabing tool ( 123 ) .
the reduc - tion in accuracy of active versus passive measurements of q and l resulted in a corresponding reduction in accuracy of svr - based throughput predictions for bulk transfers under heavy trafc con - ditions on the order of about 123%still a signicant improvement on history - base estimates .
it is also important to note that through - put prediction based on training plus lightweight active measure - ments results in a dramatically lower network probe load than prior history - based methods using long - lived tcp transfers and heavy - weight probe - based estimates of available bandwidth such as de - scribed in ( 123 ) .
we quantify this difference in section 123
second , we experimented with training data in order to enable predictions over a range of le sizes instead of only bulk transfers which is the focus of prior work .
we found that a training set of only three
le sizes results in accurate throughput predictions for a wide range of le sizes , which highlights another strength of our svr - based approach .
third , he et al .
showed that level shifts in path con - ditions pose difculties for throughput prediction ( 123 ) , suggesting the need for adaptivity .
to accomplish this , we augmented the basic svr predictor with a condence interval estimator as a mechanism for triggering a retraining process .
we show in section 123 that our technique is able to adapt to level shifts quickly and to maintain high accuracy on paths where level shifts occur .
this combination of capabilities was sufcient for us to develop an active probe tool for tcp throughput prediction we call path - perf 123 that we deployed and tested in the wide area .
through a se - ries of experiments over six end - to - end paths in the ron testbed ( 123 ) paths , we found that in the best case pathperf provides tcp through - put estimates within 123% of actual value 123% of the time , and in the worst case provides estimates within 123% of actual 123% of the time .
we believe that improvements in tools for gathering path property information will lead to corresponding improvements in throughput prediction accuracy .
we plan to investigate these possi - bilities in future work .
related work
since the seminal work by jacobson and karels established the basic mechanisms for modern tcp implementations ( 123 ) , it has been well known that many factors affect tcp throughput .
in gen - eral , these include the tcp implementation , the underlying network structure , and the dynamics of the trafc sharing the links on the path between two hosts .
steps toward understanding tcp behavior have been taken in a number of studies including ( 123 , 123 ) which devel - oped stochastic models for tcp based on packet loss characteris - tics .
a series of studies develop increasingly detailed mathematical expressions for tcp throughput based on modeling the details of the tcp congestion control algorithm and measurements of path properties ( 123 , 123 , 123 , 123 , 123 ) .
while our predictor also relies on mea - surement of path properties , the svr - based approach is completely distinguished from prior formula - based models .
a large number of empirical studies of tcp le transfer and throughput behavior have provided valuable insight into tcp per - formance .
paxson conducted one of the most comprehensive stud - ies of tcp behavior ( 123 , 123 ) .
while that work exposed a plethora of issues , it provided some of the rst empirical data on the character - istics of packet delay , queuing , and loss within tcp le transfers .
barford and crovellas application of critical path analysis to tcp le transfers provides an even more detailed perspective on how delay , queuing , and loss relate to tcp performance ( 123 ) .
in ( 123 ) , bal - akrishnan et al .
studied throughput from the perspective of a large web server and showed how it varied depending on end - host and time of day characteristics .
finally , detailed studies of throughput variability over time and correlations between throughput and ow size can be found in ( 123 , 123 ) , respectively .
these studies inform our work in terms of the basic characteristics of throughput that must be considered when building our predictor .
past studies of history - based methods for tcp throughput pre - diction are based on the use of standard time series forecasting methods .
vazhkudia et al .
compare several different simple fore - casting methods to estimate tcp throughput for transfers of large les and nd similar performance across predictors ( 123 ) .
a well - known system for throughput prediction is the network weather service ( 123 ) .
that system makes bulk transfer forecasts by attempt -
ing to correlate measurements of prior large tcp le transfers with periodic small ( 123kb ) tcp le transfers ( referred to as bandwidth probes ) .
the dualpats system for tcp throughput prediction is described in ( 123 ) .
that system makes throughput estimates based on an exponentially weighted moving average of larger size band - width probes ( 123mb total ) .
similar to our work , lu et al .
found that prediction errors generally followed a normal distribution .
as mentioned earlier , he et al .
extensively studied history - based pre - dictors using three different time series forecasts ( 123 ) .
our svr - based method includes information from prior transfers for training , but otherwise only requires measurements from lightweight probes and is generalized for all les sizes , not just bulk transfers .
many techniques have been developed to measure path prop - erties ( see caidas excellent summary page for examples ( 123 ) ) .
prior work on path property measurement directs our selection of lightweight probe tools to collect data for our predictor .
recent studies have focused on measuring available bandwidth on a path .
ab is dened informally as the minimum unused capacity on an end - to - end path , which is a conceptually appealing property with respect to throughput prediction .
a number of studies have de - scribed techniques for measuring ab including ( 123 , 123 , 123 ) .
we investigate the ability of ab measurement as well as other path properties to enhance tcp throughput predictions .
finally , machine learning techniques have not been widely ap - plied to network measurement .
one notable exception is in network intrusion detection ( e . g . , ( 123 ) ) .
the only other application of sup - port vector regression that we know of is to the problem of using ip address structure to predict round trip time latency ( 123 ) .
a multivariate machine
the main hypothesis of this work is that history - based tcp through -
put prediction can be improved by incorporating measurements of end - to - end path properties .
the task of throughput prediction can be formulated as a regression problem , i . e . , predicting a real - valued number based on multiple real - valued input features .
each le transfer is represented by a feature vector x rd of dimension d .
each dimension is an observed feature , e . g . , the le size , proximal measurements of path properties such as queuing delay , loss , avail - able bandwidth , etc .
given x , we want to predict the throughput y r .
this is achieved by training a regression function f : rd 123 r , and applying f to x .
the function f is trained using training data , i . e . , historical le transfers with known features and the cor - responding measured throughput .
the analytical framework that we apply to this problem is sup - port vector regression ( svr ) , a state - of - the - art machine learning tool for multivariate regression .
svr is the regression version of the popular support vector machines ( 123 ) .
it has a solid theoretical foundation , and is favored in practice for its good empirical perfor - mance .
we briey describe svr below , and refer readers to ( 123 , 123 ) for details , and to ( 123 ) as an example of an svr software package .
to understand svr we start from a linear regression function f ( x ) = b x + b 123
assume we have a training set of n le transfers ( ( x123 , y123 ) , .
, ( xn , yn ) ) .
training involves estimating the d - dimensional weight vector b and offset b 123 so that f ( xi ) is close to the truth yi for all training examples i = 123 .
there are many ways to mea - sure closeness .
the traditional measure used in svr is the e - insensitive loss , dened as
l ( f ( x ) , y ) = ( cid : 123 )
| f ( x ) y| e
if | f ( x ) y| e
this loss function measures the absolute error between prediction and truth , but with a tolerance of e .
the value e dependent in general , and in our experiments we set it to zero .
other loss functions ( e . g . , the squared loss ) are possible too , and often give similar performance .
they are not explored in this paper .
i=123 l ( f ( xi ) , yi ) .
it might seem that the appropriate way to estimate the parameters b 123 is to minimize the overall loss on the training set ( cid : 123 ) n
however if d is large compared to the number of training examples n , one can often t the training data perfectly .
this is dangerous , because the truth y in training data actually contain random uctua - tions , and f is partly tting the noise .
such f will generalize poorly , i . e .
causing bad predictions on future test data .
this phenomenon is known as overtting .
to prevent overtting , one can reduce the degree of freedom in f by selecting a subset of features , thus re - ducing d .
an implicit but more convenient alternative is to require f to be smooth123 , dened as having a small parameter norm kb k123
combining loss and smoothness , we estimate the parameters b by solving the following optimization problem
l ( f ( xi ) , yi ) + kb k123
where c is a weight parameter to balance the two terms .
the value of c is usually selected by a procedure called cross - validation , where the training set is randomly split into two parts , then regression functions with different c are trained on one part and their per - formance measured on the other part , and nally one selects the c value with the best performance .
in our experiments we used c = 123 using cross - validation .
the optimization problem can be solved using a quadratic program .
123 , x123x123 , x123 , x123
nonetheless , a linear function f ( x ) is fairly restrictive and may not be able to describe the true function y .
a standard mathemati - cal trick is to augment the feature vector x with non - linear bases de - rived from x .
for example , if x = ( x123 , x123 ) , one can augment it with f ( x ) = ( x123 , x123 123 ) .
the linear regressor in the augmented feature space f ( x ) = b f ( x ) +b 123 then produces a non - linear t in the original feature space .
note b has more dimensions than before .
the more dimensions f ( x ) has , the more expressive f becomes .
in the extreme ( and often benecial ) case f ( x ) can even have in - nite dimensions .
it seems computationally impossible to estimate the corresponding innite - dimensional parameter b .
however , if we convert the primal optimization problem ( 123 ) into its dual form , one can show that the number of dual parameters is actually n in - stead of the dimension of f ( x ) .
furthermore , the dual problem never uses the augmented feature f ( x ) explicitly .
it only uses the inner product between pairs of augmented features f ( x ) f ( x ) k ( x , x ) .
the function k is known as the kernel , and can be com - puted from the original feature vectors x , x .
for instance , the ra -
dial basis function ( rbf ) kernel k ( x , x ) = exp ( cid : 123 ) g kx xk123 ( cid : 123 ) im - plicitly corresponds to an innite dimensional feature space .
in our experiments we used a rbf kernel with g = 123 , again selected by cross - validation .
the dual problem can still be efciently solved using a quadratic program .
svr therefore works as follows : for training , one collects a training set ( ( x123 , y123 ) , .
, ( xn , yn ) ) , and species a kernel k .
svr solves the dual optimization problem , which equivalently nds the potentially very high - dimensional parameter b and b 123 in the aug - mented feature space dened by k .
this produces a potentially 123if b are the coefcients of a polynomial function , the function will tend to be smooth ( changes slowly ) if kb k123 is small , or noisy if kb k123 is large .
highly non - linear prediction function f ( x ) .
the function f ( x ) can then be applied to arbitrary test cases x , and produces a prediction .
in our case , test cases are the le size for which a prediction is to be made and current path properties based on active measurements .
experimental environment and
this section describes the laboratory environment and experi - mental procedure that we used to evaluate our throughput predictor .
123 experimental environment
the laboratory testbed used in our experiments is shown in fig - ure 123
it consisted of commodity end hosts connected to a dumbbell - like topology of cisco gsr 123 routers .
both measurement and background trafc was generated and received by the end hosts .
trafc owed from the sending hosts on separate paths via giga - bit ethernet to separate cisco gsrs ( hop b in the gure ) where it was forwarded on oc123 ( 123 mb / s ) links .
this conguration was created in order to accommodate a precision passive measurement system , as we describe below .
trafc from the oc123 links was then multiplexed onto a single oc123 ( 123 mb / s ) link ( hop c in the gure ) which formed the bottleneck where congestion took place .
we used an adtech sx - 123 hardware - based propagation delay emulator on the oc123 link to add 123 milliseconds delay in each direction for all experiments , and congured the bottleneck queue to hold approx - imately 123 milliseconds of packets .
packets exited the oc123 link via another cisco gsr 123 ( hop d in the gure ) and passed to receiving hosts via gigabit ethernet .
the measurement hosts and trafc generation hosts were identi - cally congured workstations running freebsd 123 .
the worksta - tions had 123 ghz intel pentium 123 processors with 123 gb of ram and intel pro / 123 network cards .
they were also dual - homed , so that all management trafc was on a separate network than depicted in figure 123
we disabled the tcp throughput history caching feature in freebsd 123 , controlled by the variable net . inet . tcp . inight . enable , to allow tcp throughput to be determined by current path proper - ties rather than throughput history .
a key aspect of our testbed was the measurement system used to establish the true path properties for our evaluation .
optical split - ters were attached to both the ingress and egress links at hop c and endace dag 123 and 123 passive monitoring cards were used to capture traces of all packets entering and leaving the bottleneck node .
by comparing packet headers , we were able to identify which packets were lost at the congested output queue during experiments , and accurately measure available bandwidth on the congested link .
furthermore , the fact that the measurements of packets entering and leaving hop c were synchronized at a very ne granularity ( i . e . , a single microsecond ) enabled us to precisely measure queuing de - lays through the congested router .
123 experimental protocol
we generated background trafc by running the harpoon ip traf - c generator ( 123 ) between up to four pairs of trafc generation hosts as illustrated in figure 123
harpoon produced open - loop self - similar trafc using a heavy - tailed le size distribution , mimicking a mix of application trafc such as web and peer - to - peer applications com - mon in todays internet .
harpoon was congured to produce aver - age offered loads ranging from approximately 123% to 123% on the bottleneck link ( the oc123 between hops c and d ) .
measurement trafc in the testbed consisted of le transfers , and active measurements of queuing delay , packet loss , and available
bandwidth .
for the measurement trafc hosts , we set the tcp re - ceive window size to 123 kb .
in receive window limited transfers , le transfer throughput was approximately 123 mb / s .
that is , if the available bandwidth on the bottleneck link was 123 mb / s or more , the ow was receive window ( rwnd ) limited , otherwise it was con - gestion window ( cwnd ) limited .
we experimented with both rwnd - and cwnd - limited scenarios .
for active measurements of available bandwidth and queuing / loss , we used the yaz ( 123 ) and badabing ( 123 ) tools , respectively .
yaz estimates end - to - end available bandwidth using a relatively low - overhead , iterative method similar to pathload ( 123 ) .
since the probe process is iterative , the time taken to produce an estimate can vary from a few seconds to tens of seconds .
badabing reports two characteristics of loss episodes , namely the frequency of loss episodes , and mean duration of loss episodes , using a lightweight probe process .
we used a probe probability parameter p of 123 .
other parameters were set according to ( 123 ) .
in the rest of the paper , we refer to both loss characteristics combined as loss or l .
badabing requires the sender and receiver to be time - synchronized .
to accommodate our wide area experiments , the badabing receiver was modied to reect probes back to the sender , where they were timestamped and logged as on the original receiver .
thus , the sender clock was used for all probe timestamps .
we used badabing to measure loss characteristics because it is the most accurate loss characteristics measurement tool currently available; if accurate loss rate measurement tools become available in the future , loss rate may replace frequency and duration as the loss characteristic we use for our prediction mechanism .
the measurement collection protocol consisted of the following :
run badabing for 123 seconds .
run yaz to obtain an estimate of the available bandwidth .
transfer a le .
in the remainder of the paper , we refer to the above series of steps as a single experiment , and to a number of consecutive experiments as a series .
experiments in the wide area omit the available band - width measurement .
individual experiments in a series are sepa - rated by a 123 second period .
series of experiments differ from each other in that either the background trafc is different between series , or the distribution of le sizes transferred is different , or the exper - iments are conducted over different physical paths .
each series of experiments is divided into two mutually exclusive training and test sets for the svr .
the svr mechanism does not require that the sets of experiments that form the training and test set be consecutive or contiguous in time .
in contrast , history - based prediction methods generally require consecutive historical information since they rely on standard timeseries - based forecasting models .
in our evaluation we use contiguous portions for training and test sets , i . e .
ginning part of a series becomes the training set and the rest the test set .
the number of experiments in training and test sets may be the same or different .
notions of separate training and test data sets are not required for history - based methods; rather , predictions are made over the continuous notion of distant and recent history .
in our evaluation of history - based methods , we use the nal prediction of the training set as the starting point for the test set .
from each series of experiments , we gather three different sets of measurements .
the rst set , oracular passive measurements ( opm ) , are ab , q , and l measurements during a le transfer that we obtain from packet traces .
we refer to these measurements as oracular because they give us essentially perfect information about
traffic generator hosts
dag monitor host
cisco 123 cisco 123
traffic generator hosts
123 : laboratory testbed .
cross trafc owed across one of two routers at hop b , while probe trafc owed through the other .
optical splitters connected endace dag 123 and 123 passive packet capture cards to the testbed between hops b and c , and hops c and d .
measurement trafc ( le transfers , loss probes , and available bandwidth probes ) owed from left to right .
congestion in the testbed occurred at hop c .
in practice , this information would not be available when making a prediction for an arbitrary path .
we use this information to establish the best possible accuracy of our pre - diction mechanism .
the second set , active measurements ( am ) , are the measurements from our active measurement tools .
note that , unlike the opm , the am provide ab , q , and l values before the actual transfer .
the third set , practical passive measurements ( ppm ) , are trace - based measurements of ab , q and l taken at the same time as am are taken .
their purpose is to show the best possi - ble accuracy of our prediction mechanism with measurements that can be obtained in practice , or to show how much better the accu - racy would be if the active measurements had perfect accuracy .
all measurements are aggregates for conditions on the path : they are not specic to any single tcp ow on the path .
for experiments in the wide area , we created a tool , pathperf .
this tool , designed to run between a pair of end hosts , initiates tcp le transfers and path property measurements ( using our modied version of badabing ) , and produces throughput estimates using our svr - based method .
it can be congured to generate arbitrary le size transfers for both training and testing and initiates retrain - ing when level shifts are identied as described in section 123
123 evaluating prediction accuracy
we denote the actual throughput by r and the predicted through - put by r .
we use the metric relative prediction error e introduced in ( 123 ) to evaluate the accuracy of an individual throughput predic - tion .
relative prediction error is dened as
min ( r , r )
in what follows , we use the distribution of the absolute value of e to compare different prediction methods .
building a robust predictor
this section describes how we developed , calibrated , and eval - uated our prediction mechanism through an extensive set of tests conducted in our lab test - bed .
123 calibration and evaluation in the high
the rst step in developing our svr - based throughput predic - tor is to nd the combination of training features which lead to the most accurate predictions over a wide variety of path conditions .
we trained the predictor using a feature vector for each test that contained different combination of our set of target path measure - ments ( ab , q , l ) and the measured throughput .
although we re - fer to both loss frequency and loss duration together as l or loss
for expositional ease , these measures are two different features in the feature vector .
the trained svr model is then used to predict throughput for a feature vector containing the corresponding sets of network measurements , and we compare the prediction accuracy for the different combinations .
we also compare the accuracy of svr to the exponentially weighted
moving average ( ewma ) history - based predictor ( hb ) described in ( 123 ) , ri+123 = a ri + ( 123 a ) ri , with an a value of 123 .
we do not report detailed results from tests with low utiliza - tion on the bottleneck link , i . e . , receive window bound ows , be - cause in the low utilization scenarios there is very little variance in throughput of ows .
our svr - based predictor generated 123% ac - curate forecasts for these tests .
however , any reasonable prediction method can forecast throughput quite accurately in this scenario .
for example , the formula - based predictor used in ( 123 ) , which is based on ( 123 ) , performs poorly in high utilization scenarios , but is accurate in low utilization scenarios .
for most of the results reported , we generated an average of 123 mb / s of background trafc to create high utilization on our oc123 ( 123 mb / s ) bottleneck link .
we used one set of 123 experiments for training and another set of 123 experiments for testing .
an 123 mb le was transferred in each experiment .
in our initial experiments we use an 123 mb le because it is large enough to make slow - start an insignicant factor in throughput for our receive window size of 123 kb; in later sections we consider smaller le sizes where slow - start has a bigger impact on overall throughput .
figures 123a to 123l show scatter plots comparing the actual and pre - dicted throughput using different prediction methods as discussed below .
a point on the diagonal represents perfect prediction accu - racy; the farther a point is from the diagonal , the greater the predic -
123 . 123 using path measurements from an oracle
figure 123a shows the prediction accuracy scatter plot for the hb method .
figures 123b to 123h show the prediction error with svr using oracular passive measurements ( opm ) for different combinations of path measurements in the feature vector .
for example , svr - opm - queue means that only queuing delay measurements were used to train and test , while svr - opm - ab - queue means that both available bandwidth and queuing delay measurements were used to train and test .
table 123 shows relative prediction errors for hb forecasting and for svm - opm - based predictions .
values in the table indicate the fraction of predictions for a given method within a given accuracy level .
for example , the rst two columns of the rst row in table 123 mean that 123% of hb predictions have relative prediction errors of 123% or smaller while 123% of svr - opm - ab predictions have relative prediction errors of 123% or smaller .
we present scatter plots
actual throughput ( mb / s )
actual throughput ( mb / s )
actual throughput ( mb / s )
actual throughput ( mb / s )
actual throughput ( mb / s )
actual throughput ( mb / s )
actual throughput ( mb / s )
actual throughput ( mb / s )
actual throughput ( mb / s )
actual throughput ( mb / s )
actual throughput ( mb / s )
actual throughput ( mb / s )
123 : comparison of prediction accuracy of hb , svr - opm , svr - ppm , and svr - am in high background trafc conditions .
in addition to tabular data to provide insight into how different path properties contribute to throughput prediction in the svr method .
from figure 123a , we can see that the predictions for the hb pre - dictor are rather diffusely scattered around the diagonal , and that predictions in low - throughput conditions tend to have large rela - tive error .
figures 123b , 123c , and 123d show the behavior of the svr predictor using a single measure of path properties in the feature vectorab , l , and q respectively .
the ab and q graphs have a similar overall trend : predictions are accurate ( i . e . , points are close to the diagonal ) for high actual throughput values , but far from the diagonal , almost in a horizontal line , for lower values of actual throughput .
the l graph has the opposite trend : points are close to the diagonal for lower values of actual throughput , and form a horizontal line for higher values of actual throughput .
the explanation for these trends lies in the fact that le trans - fers with low actual throughput experience loss , while le transfers with high actual throughput do not experience any loss .
when loss occurs , the values of ab and q for the path are nearly constant .
ab is almost zero , and q is the maximum possible value ( which depends on the amount of buffering available at the bottleneck link in the path ) .
in this case , throughput depends on the value of l .
hence , l appears to be a good predictor when there is loss on the path , and ab and q , being constants in this case , have no predictive power , resulting in horizontal lines , i . e . , a single value of predicted throughput .
on the other hand , when there is no loss , l is a con - stant with value zero , so l has no predictive power , while ab and q are able to predict throughput quite accurately .
figures 123e to 123h show improvements on the prediction accuracy obtained by using more than one path property in the svr feature vector .
we can see that when l is combined with either ab or q , the horizontal lines on the graphs are replaced by points much closer to the diagonal , so combining l with ab or q allows the svr method to predict accurately in both lossy and lossless network conditions .
measurements of ab and q appear to serve the same function , namely , helping to predict throughput in lossless conditions .
this observation begs the question : do we really need both ab and q , or can we use just one of the two and still achieve the same predic - tion accuracy ? to answer this question , we compared ab - loss and loss - queue predictions with each other and with ab - loss - queue predictions ( i . e . , figures 123e , 123g , and 123h ) .
the general trend in all three cases , as seen from the scatter plots , is the same : the horizon - tal line of points is reduced or eliminated , suggesting that predic - tion from non - constant - value measurements is occurring for both lossy and lossless network conditions .
if we compare the ab - loss and loss - queue graphs more closely , we observe two things .
first , in the lossless prediction case , the points are closer to the diago - nal in the loss - queue case than in the ab - loss case .
second , in the loss - queue case , the transition in the prediction from the loss - less to the lossy case is smooth , i . e . , there is no horizontal line of points , while in the ab - loss case there is still a horizontal line of points in the actual throughput range of 123 mb / s .
this sug - gests that q is a more accurate predictor than ab in the lossless case .
the relative prediction error data of table 123 supports this : svr with a feature vector containing loss - queue information pre - dicts throughput within 123% of actual for 123% of transfers , while a feature vector containing ab - loss measurements predicts with the same accuracy level for 123% of transfers .
finally , there is no dif - ference in accuracy ( either qualitatively or quantitatively ) between loss - queue and ab - loss - queue .
the above discussion suggests that ab measurements are not re - quired for highly accurate throughput prediction , and that a combi -
123 : relative accuracy of history - based ( hb ) throughput prediction and svr - based predictors using different types of oracular passive path measurements ( svr - opm ) in the feature vector .
table values indicate the fraction of predictions within a given accuracy level .
123 : relative accuracy of history - based ( hb ) throughput prediction and svr - based predictors using trace - based passive path measure - ments ( ppm ) or active path measurements ( am ) .
table values in - dicate the fraction of predictions within a given accuracy level .
nation of l and q is sufcient ( given our framework ) .
this obser - vation is not only surprising , but rather good news .
prior work has shown that accurate measurements of ab require at least moder - ate amounts of probe trafc ( 123 , 123 ) , and some formula - based tcp throughput estimation schemes take as a given that ab measure - ments are necessary for accurate throughput prediction ( 123 ) .
contrast , measurements of l and q can be very lightweight probe processes ( 123 ) .
we discuss measurement overhead further in sec -
123 . 123 using practical passive and active path mea -
so far , we have considered the prediction accuracy of svr based on only oracular passive measurements ( opm ) .
this gives us the baseline for the best - case accuracy with svr , and also provides in - sight into how svr uses different path properties for prediction .
table 123 and the graphs in figure 123 shows that hb predicts 123% of transfers within 123% of actual while svr - opm - loss - queue pre - dicts 123% , an almost 123 - fold improvement .
in practice , however , perfect measurements of path properties are not available , so in what follows we assess svr using measurements that are more like those available in the wide area .
we compare hb with svr - ppm and svr - am .
due to space limitations , we present only loss - queue and ab - loss - queue re - sults for svr .
we choose these because we expect ab - loss - queue to have the highest accuracy as it has the most information about path properties , and loss - queue because it is very lightweight and has accuracy equal to ab - loss - queue for svr - opm .
figures 123i and 123j show predicted and actual throughput for svr - ppm and figures 123k and 123l show predicted and actual throughput
for svr - am .
table 123 presents relative prediction error data for hb , svr - ppm and svr - am .
we wish to examine three issues : rst , whether our nding that loss - queue has the same prediction accu - racy as ab - loss - queue from the svr - opm case holds for the svr - ppm and svr - am case; second , whether svr - ppm and svr - am have the same accuracy; and third , how svr - am accuracy com - pares with hb prediction accuracy .
all the scatter plots from figures 123i to 123l are qualitatively very similar; this is encouraging because it suggests two things .
first , loss - queue has similar prediction accuracy as ab - loss - queue , i . e . , the observation from svr - opm still holds , so we can achieve good prediction accuracy without having to measure ab .
the relative prediction error data in table 123 supports this graphical observation : ab - loss - queue has only slightly better accuracy than loss - queue for both svr - ppm and svr - am .
second , svr - am has accuracy similar to svr - ppm , i . e . , using active measurement tools to esti - mate path properties yields predictions almost as accurate as hav - ing ground - truth measurements .
the data in table 123 further sub - stantiates this observation .
similar accuracy between svr - ppm predictions and svr - am predictions is important because in real wide - area internet paths instrumentation is generally not available for providing accurate passive measurements .
finally , we compare hb with svr - am .
although figures 123a , 123k and 123l are qualitatively similar , svr - am has a tighter cluster of points around the diagonal for high actual throughput than hb .
thus , svr - am appears to have higher accuracy than hb .
as ta - ble 123 shows , svr - am - loss - queue predicts throughput within 123% of actual accuracy 123% of the time , while hb does so only 123% of the time .
hence , for high trafc scenarios , svr - am - loss - queue , the practically deployable lightweight version of the svr - based prediction mechanism , signicantly outperforms hb prediction .
123 . 123 the nature of prediction error
lu et al .
( 123 ) observed in their study that throughput prediction errors were approximately normal in distribution .
as the authors noted , normality would justify standard computations of condence intervals .
we examined the distribution of errors in our experiments and also found evidence suggestive of normality .
figure 123 shows two normal quantile - quantile ( q - q ) plots for svr - opm - loss - queue ( figure 123a ) and svr - am - loss - queue ( figure 123b ) .
samples that are consistent with a normal distribution form approx - imately a straight line in the graph , particularly toward the center .
in figures 123a and 123b , we see that the throughput prediction sam - ples in each case form approximately straight lines .
these observa - tions are consistent with normality in the distribution of prediction errors .
error distributions from other experiments were also con - sistent with the normal distribution , but are not shown due to space limitations .
we further discuss the issues of retraining and of de - tecting estimation problems in section 123
123 evaluation of prediction accuracy with
level - shift background trafc
in the previous section , we considered svr and hb prediction accuracy under variable but stationary background trafc .
in this section , we consider the case where there is a shift in average load of background trafc .
we congure our trafc generation process to cycle between 123 mb / s and 123 mb / s average offered loads .
with 123 mb / s trafc , there is virtually no loss along the path and the throughput is bound by the receive window size .
with 123 mb / s trafc , the oc123 bot - tleneck is saturated and endemic loss ensues .
the procedure is to run 123 mb / s background trafc for 123 minutes , followed by 123
( a ) q - q plot svr - opm - loss - queue .
( b ) q - q plot
123 : normal q - q plots for prediction errors with ( a ) oracular mea - surements and ( b ) active measurements of loss - queue .
mb / s of trafc for 123 hours 123 min , repeating this cycle several times .
we follow the measurement protocol in section 123 for collecting ab , l , q , and throughput measurements .
we rst train our svr predictor in the 123 mb / s environment , and test it in the 123 mb / s environment .
the column labeled opm - l - q trainlowtesthigh in table 123 shows the results .
clearly , the prediction accuracy is very poor : all predictions are off by a factor of two or more .
next , we train the predictor on one whole cycle of 123 mb / s and 123 mb / s , and test it on a series of cycles .
the last two columns of table 123 , opm - loss - queue and am - loss - queue , and figure 123a show the re - sults from these experiments .
the x - axis in figure 123a is time repre - sented by the number of le transfers that have completed , and the y - axis is throughput .
in this case , both svr - opm and svr - am pre - dict throughput with high accuracy .
svr - opm predicts throughput within 123% of actual 123% of the time , and svr - am 123% of the time .
for comparison , the rst column of table 123 and figure 123b present results for the history - based predictor .
hb accuracy is sig - nicantly lower than svr accuracy , only about half as much as svr - opm .
figure 123b explains why .
after every level - shift in the background trafc , the hb predictor takes time to re - adapt .
in con - trast , no adaptation time is required for the svr predictor if it has already been trained on the range of expected trafc conditions .
one issue regarding shifts in average trafc volume is how many samples from a new , previously unseen trafc level are needed to adequately train the predictor .
to examine this issue , we train our predictor on two new kinds of average trafc loads : a step con - stituting 123 minutes of 123 mb / s trafc followed by 123 minutes of 123 mb / s trafc resulting in 123 experiments at the higher trafc level , and a step constituting of 123 minutes of 123 mb / s trafc fol - lowed by 123 minutes of 123 mb / s trafc resulting in 123 experiments
file transfer timeline
( a ) level shift : svr - am - loss - queue
file transfer timeline
( b ) level shift : hb
123 : prediction accuracy in fluctuating background trafc
at the higher trafc level .
we call these conditions svr - am - l - q - 123 and svr - am - l - q - 123 respectively .
the prediction accuracy of these schemes is presented in table 123
as can be seen in the table , these training schemes yield most of the accuracy of svr - am - l - q , which trains at the 123 mb / s level for approximately 123 experi - ments .
these results demonstrate that fairly small training sets from new trafc levels are needed for accurate prediction .
a comparison of time series plots from these experiments ( not shown due to space constraints ) provides further insight into how prediction accuracy improves with larger training sets .
both svr - am - l - q - 123 and svr - am - l - q - 123 have periods where the predicted throughput is virtu - ally constant even though the actual throughput is not .
such periods are shorter for svr - am - l - q - 123 compared to svr - am - l - q - 123
svr - am - l - q ( figure 123a ) has no such periods .
the reason for this effect is that predictors incorporating fewer samples may not include a broad enough range of possible network conditions .
however , the svr prediction mechanism can still yield high accuracy using a small set of training samples if the samples are representative of the range of network conditions along a path .
123 evaluation of prediction accuracy for dif -
ferent file sizes
we have so far considered only 123 mb le transfers in step 123 of the experimental protocol of section 123 .
for a tcp receive win - dow of 123 kb , the majority of the lifetime of an 123 mb transfer is spent in tcps congestion avoidance phase .
however , our goal is an accurate predictor for a range of le sizes , not just bulk transfers .
predicting throughput for small les is complicated by tcps slow start phase in which throughput changes rapidly with increasing le size due to the doubling of the window every round - trip time , and because packet loss during the transfer of a small le can have a large relative impact on throughput .
we hypothesize that using dif -
123 : relative accuracy of history - based ( hb ) and svr - based throughput predictors using oracular path measurements ( opm ) and active measurements ( am ) when predictors are subjected to shifts in average background trafc volume .
123 : comparison of relative accuracy of svr - based throughput pre - diction using active measurements ( am ) and different numbers of training samples .
123 training samples are used in am - loss - queue , 123 samples for am - l - q - 123 , and 123 samples for am - l - q - 123
back - ground trafc consists of shifts in average trafc volume between 123 mb / s and 123 mb / s .
ferent le sizes to train the svr predictor will lead to accurate fore - casts for a broad range of le sizes - something not treated in prior hb prediction studies .
we conducted experiments using background trafc at an aver - age offered load of 123 mb / s and using a series of 123 training sets consisting of between 123 and 123 unique le sizes .
the le sizes for the training sets are between 123 kb and 123 mb .
the rst training set consists of a single le size of 123 mb , the second training set consists of two le sizes of 123 kb and 123 mb , and the third training set adds a le size of 123 kb .
subsequent training sets sample the range between 123 kb and 123 mb such that the fraction of a transfer lifetime spent in slow start covers a wider range .
test sets for our experiments consist of 123 le sizes between 123 kb and 123 mb a much more diverse set than the training set .
the test le sizes are drawn from a biased random number generator in such a way that the resulting le transfers exhibit a wide range of behavior , i . e . , les that are fully transferred during slow start , and transfers consisting of a varying proportion of time spent in slow start versus congestion avoidance .
we use a wider range of small les in the test set to allow us to see how our predictor performs for unseen and difcult to predict le sizes .
we do not use a wider range for large le sizes because we expect the throughput to be almost constant ( i . e . , window or congestion limited ) once slow start becomes an insignicant fraction of the transfer time .
tables 123 and 123 present prediction accuracy for training sets con - sisting of 123 , 123 , 123 , 123 , or 123 distinct le sizes for svr - opm and svr - am .
graphs in figure 123 present svr - am results for one , two , and three le sizes in the training set .
we do not include graphs for re - maining training sets due to space limitations : they are similar to
123 : relative accuracy of svr - based predictor using oracular passive measurements ( opm ) and training sets consisting of 123 , 123 , 123 , 123 , or 123 distinct le sizes .
of distinct le sizes in training
123 : relative accuracy of svr - based predictor using active measure - ments ( am ) and training sets consisting of 123 , 123 , 123 , 123 , or 123 distinct
of distinct le sizes in training
123 : details of ron paths used for wide - area experiments
new mexico - new york
path rtt ( ms )
123 : wide area results for 123kb transfers
123 : wide area results for 123mb transfers
those for two and three le sizes in the training set .
the rst obser - vation is that for the single le size in training , the prediction error is very high .
this inaccuracy is expected because the predictor has been given no information about the relationship between size and throughput for small les .
the second observation is that for more than one le size , prediction becomes dramatically more accurate , i . e . , the predictor is able to successfully extrapolate from a handful of sizes in training to a large number of sizes in testing .
the third observation is that relative error is low for large le sizes ( corre - sponding to high actual throughput ) while it is higher for small les ( low actual throughput ) .
this is consistent with our expectation that it would be more difcult to accurately predict throughput for small les .
the fourth observation is that for small le sizes ( i . e . , small actual throughput ) , the error is always that of over - prediction .
the smallest le in the training set is 123kb while the smallest le in the test set is 123kb .
this difference is the cause of over - prediction errors : the relationship between le size and throughput is compli - cated for small les , and without a broader training set , the svr mechanism is unable to provide accurate prediction .
an important nal observation is that prediction accuracy reaches a maximum at three le sizes in the training set , and there is no clear trend for four to nine le sizes in the training set .
a feature of our training set is that the number of transfers is always constant at one hundred , so for the single training size , there are one hundred 123 mb transfers , and for the two training sizes , there are fty 123 kb transfers and fty 123 mb transfers .
we believe that accuracy is max - imum at three training sizes in our experiments because there is a trade - off between capturing a diversity of le sizes and the number of samples for a single le sizethis was alluded to in our discus - sion of the number of samples needed for good prediction accuracy in section 123 .
in other words , we believe that we would not see maximum accuracy occurring at three le sizes and would instead see an increase in accuracy with increasing number of le sizes in the training set if we kept the number of samples of a single le size constant in the training set and allowed the size of the training set to increase from 123 to 123 , 123 , etc . , as we increase the number of le sizes in the training set .
a thorough characterization of the trade - off between diversity of le sizes and number of samples of each le size is future work .
wide area test results
to further evaluate our svr throughput prediction method we created a prototype tool called pathperf that can generate mea - surements and make forecasts on wide area paths .
we used path - perf to conduct experiments over a small set of paths in the ron
the ron nodes used in our experiments have a range of cpu and memory congurations , but all ran freebsd 123 and had lim - ited additional experimental load during the time of our tests .
we conducted our tests over the set of paths described in table 123
these include ve trans - continental paths and one trans - atlantic path .
while this path set is modest in size , it has relative diversity in path charac - teristics and round - trip times that range between 123ms and 123ms .
since we have shown that ab measurement does not improve throughput prediction accuracy , we eliminate it in the experimental protocol described in section 123 .
we conduct experiments for two target le sizes : 123 kb and 123mb .
the training and test sets consist of 123 transfers of a particular le size .
the experiments were conducted between october 123 , 123 and october 123 , 123
tables 123 and 123 show the results of the tests .
the prediction ac - curacy is high on all paths for both le sizes : in many cases svr
actual throughput ( mb / s )
actual throughput ( mb / s )
actual throughput ( mb / s )
( a ) svr - am with le size of 123 mb in
( b ) svr - am with le sizes of 123 kb and 123 mb in training set .
( c ) svr - am with le sizes of 123 kb , 123 kb , and 123 mb in training set .
123 : scatter plots for the svr - based predictor using 123 , 123 , or 123 distinct le sizes in the training set .
all results shown use active measurements to train the predictor .
testing is done using a range of 123 le sizes from 123 kb to 123 mb .
prediction accuracy is within 123% of actual greater than 123% of the time .
in most cases , the prediction accuracy for 123kb and 123mb transfers is similar .
the one exception is path 123 , where accuracy is within 123% of original only 123% of the time for 123kb trans - fers , but is much higher at 123% of the time for 123mb transfers .
we found that on this particular path the coefcient of variation for the actual throughput of 123kb les is twice that of 123mb les , making prediction for 123kb les more difcult and suggesting that path conditions were more dynamic during this test .
our measurement data showed that many wide area paths were lightly loaded during our tests and exhibited very little variation in throughput between different le transfers .
we are in the process of extending our wide area experiment set considerably in the hope that we will encounter more variability on these paths , which will enable us to further rene pathperfs capability .
finally , we also evaluated the hb predictors performance in these experiments and found its forecasts to be approximately the same as svr .
we ex - pect that , as we nd paths with more dynamic conditions , our svr predictor will distinguish itself as our lab experiments demonstrate .
this section addresses two key issues related to running pathperf
in operational settings .
network load introduced by pathperf .
trafc introduced by active measurement tools is of concern because excessive trafc can skew the network property being measured .
furthermore , net - work operators and engineers generally wish to minimize any im - pact measurement trafc may have on customer trafc .
for history - based tcp throughput estimation methods , the spe - cic amount of trafc introduced depends on the measurement pro - tocol .
for example , two approaches may be taken .
the rst method is to periodically transfer xed - size les; the second is to allow a tcp connection to transfer data for a xed time period .
the lat - ter approach was taken in the history - based evaluation of he et al .
to estimate the overhead of a history - based predictor us - ing xed - duration transfers , assume that ( 123 ) the tcp connection is not rwnd - limited , ( 123 ) that the xed duration of data transfer is 123 seconds ( as in ( 123 ) ) , ( 123 ) throughput measurements are initiated ev - ery 123 minutes , and ( 123 ) throughput closely matches available band - width .
for this example , assume that the average available band - width for the duration of the experiment is approximately 123 mb / s .
thus , over a 123 minute period , nearly 123 gb in measurement trafc is produced , resulting in an average bandwidth of about 123 mb / s .
in the case of pathperf , measurement overhead in the training pe - riod consists of le transfers and queuing / loss probe measurements .
in the testing phase , overhead consists solely of loss measurements .
assume that we have a 123 minute training period followed by a 123 minute testing period .
assume that le sizes of 123 kb , 123 kb , and 123 mb are transferred during the training period , using 123 sam - ples of each le , and that each le transfer is preceeded by a 123 second badabing measurement .
with a probe probability of 123 , badabing trafc for each measurement is about 123 mb .
for test - ing , only badabing measurements must be ongoing .
assume that a 123 second badabing measurement is initiated every three min - utes .
thus , over the 123 minute training period about 123 mb of measurement trafc is produced , resulting in an average bandwidth of about 123 mb / s for the rst 123 minutes .
for the testing period , a total of 123 mb of measurement trafc is produced , resulting in a rate of about 123 kb / s .
overall , pathperf produces 123 kb / s on average over the 123 minute measurement period , dramatically dif - ferent from a standard history - based measurement approach .
even if more conservative assumptions are made on the history - based approach , the differences in overhead are signicant .
again , the reason for the dramatic savings is that once the svr predictor has been trained , only lightweight measurements are required for accu - detecting problems in estimation .
an important capability for throughput estimation in live deployments is to detect when there are signicant estimation errors .
such errors could be indicative of a change in network routing , causing an abrupt change in delay , loss , and throughput .
it could also signal a pathological network condition , such as an ongoing denial - of - service attack leading to endemic network loss along a path .
on the other hand , it may sim - ply be a measurement outlier with no network - based cause .
as discussed in section 123 . 123 , normality allows us to use stan - dard statistical machinery to compute condence intervals ( i . e . , us - ing measured variance of prediction error ) .
we show that prediction errors are consistent with a normal distribution and further propose using condence intervals as a mechanism for triggering retraining of the svr in the following way .
assume that we have trained the svr predictor over n measurement periods ( i . e . , we have n through - put samples and n samples of l and q ) .
assume that we then col - lect k additional throughput samples , making predictions for each sample and recording the error .
we therefore have k error samples between what was predicted and what was subsequently measured .
given a condence level , e . g . , 123% , we can calculate condence intervals on the sample error distribution .
we can then , with low
frequency , collect additional throughput samples to test whether the prediction error exceeds the interval bounds .
( note that these addi - tional samples may be application trafc for which predictions are used . ) if so , we may decide that retraining the svr predictor is appropriate .
a danger in triggering an immediate retraining is that such a policy may be too sensitive to outliers regardless of the con - dence interval chosen .
more generally , we can consider a threshold m of consecutive prediction errors that exceed the computed con - dence interval bounds as a trigger for retraining .
summary and future work
in this paper we address the problem of how to generate accurate tcp throughput predictions for arbitrary paths in the internet .
our approach uses a powerful machine learning tool - support vector regression - which provides an efcient mechanism for generating a predictor using multiple inputs .
we investigate measurements of path properties including queuing delay , packet loss , and available bandwidth that can be used along with prior throughput measure - ments as the feature set for our predictor .
through an extensive se - ries of lab - based experiments we nd that our svr predictor makes highly accurate forecasts using measurements of queuing and loss , and that available bandwidth measurement does not improve pre - dictions .
in heavy trafc conditions , the svr forecasts are nearly 123 times more accurate than prior hb predictors .
we make a se - ries of extensions to the svr predictor to make it operationally viable .
further lab experiments show that these extensions enable the predictor to work well for a wide range of le sizes , to be robust to measurements from active probe tools , and to adapt to changing path conditions .
we created a tool called pathperf which enables us to test our svr predictor in the internet .
in initial tests in the ron testbed , we show that pathperf generates highly accurate through - put predictions .
pathperf also generates far less probe trafc com - pared to a hb predictor congured as suggested in prior work .
in future work , we intend to continue to rene pathperf by inves - tigating how to better tune the training set , and possibly experiment with other machine learning tools .
we are continuing to expand the tests run in the wide area so that we can evaluate pathperfs capability under a broader range of path conditions .
we would like to thank david andersen for providing us access to the ron testbed , and ana bizzaro for help with the wail lab testbed .
we also thank the anonymous reviewers for their help - ful suggestions .
this work was supported in part by nsf grants numbers cns - 123 , cns - 123 , cns - 123 , and ccr - 123
any opinions , ndings , conclusions , or recommenda - tions expressed in this material are those of the authors and do not necessarily reect the views of the nsf .
