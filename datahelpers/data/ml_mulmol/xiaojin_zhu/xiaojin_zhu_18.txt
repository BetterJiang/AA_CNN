we pose transductive classication as a matrix completion problem .
by assuming the underlying matrix has a low rank , our formulation is able to handle three prob - lems simultaneously : i ) multi - label learning , where each item has more than one label , ii ) transduction , where most of these labels are unspecied , and iii ) miss - ing data , where a large number of features are missing .
we obtained satisfactory results on several real - world tasks , suggesting that the low rank assumption may not be as restrictive as it seems .
our method allows for different loss functions to apply on the feature and label entries of the matrix .
the resulting nuclear norm minimization problem is solved with a modied xed - point continuation method that is guaranteed to nd the global optimum .
semi - supervised learning methods make assumptions about how unlabeled data can help in the learning process , such as the manifold assumption ( data lies on a low - dimensional manifold ) and the cluster assumption ( classes are separated by low density regions ) ( 123 , 123 ) .
in this work , we present two transductive learning methods under the novel assumption that the feature - by - item and label - by - item matrices are jointly low rank .
this assumption effectively couples different label pre - diction tasks , allowing us to implicitly use observed labels in one task to recover unobserved labels in others .
the same is true for imputing missing features .
in fact , our methods learn in the dif - cult regime of multi - label transductive learning with missing data that one sometimes encounters in practice .
that is , each item is associated with many class labels , many of the items labels may be unobserved ( some items may be completely unlabeled across all labels ) , and many features may also be unobserved .
our methods build upon recent advances in matrix completion , with efcient algo - rithms to handle matrices with mixed real - valued features and discrete labels .
we obtain promising experimental results on a range of synthetic and real - world data .
123 problem formulation let x123 .
xn rd be feature vectors associated with n items .
let x = ( x123 .
xn ) be the d n feature matrix whose columns are the items .
let there be t binary classication tasks , y123 .
yn ( 123 , 123 ) t be the label vectors , and y = ( y123 .
yn ) be the t n label matrix .
entries in x or y can be missing at random .
let x be the index set of observed features in x , such that ( i , j ) x if and only if xij is observed .
similarly , let y be the index set of observed labels in y .
our main goal is to predict the missing labels yij for ( i , j ) / y .
of course , this reduces to standard transductive learning when t = 123 , |x| = nd ( no missing features ) , and 123 < |y| < n ( some missing labels ) .
in our more general setting , as a side product we are also interested in imputing the missing features , and de - noising the observed features , in x .
123 model assumptions
the above problem is in general ill - posed .
we now describe our assumptions to make it a well - dened problem .
in a nutshell , we assume that x and y are jointly produced by an underlying low rank matrix .
we then take advantage of the sparsity to ll in the missing labels and features using a modied method of matrix completion .
specically , we assume the following generative story .
it starts from a d n low rank pre - feature matrix x123 , with rank ( x123 ) ( cid : 123 ) min ( d , n ) .
the actual feature matrix x is obtained by adding iid gaussian noise to the entries of x123 : x = x123 + , where ij n ( 123 , 123 j rt of item j are j + b , where w is a t d weight matrix , and b rt is a bias vector .
let produced by y123 123
meanwhile , the t soft labels ( cid : 123 ) y123 ( cid : 123 ) be the soft label matrix .
note the combined ( t + d ) n matrix ( cid : 123 ) y123; x123 ( cid : 123 ) is low rank too : rank ( ( cid : 123 ) y123; x123 ( cid : 123 ) ) rank ( x123 ) + 123
the actual label yij ( 123 , 123 ) is generated randomly ij ) = 123 / ( cid : 123 ) 123 + exp ( yijy123 ij ) ( cid : 123 ) .
finally , two random masks x , y
via a sigmoid function : p ( yij|y123 are applied to expose only some of the entries in x and y , and we use to denote the percentage of observed entries .
this generative story may seem restrictive , but our approaches based on it perform well on synthetic and real datasets , outperforming several baselines with linear classiers .
j = wx123
123 matrix completion for heterogeneous matrix entries
with the above data generation model , our task can be dened as follows .
given the partially observed features and labels as specied by x , y , x , y , we would like to recover the interme -
diate low rank matrix ( cid : 123 ) y123; x123 ( cid : 123 ) .
then , x123 will contain the denoised and completed features , and the key assumption is that the ( t + d ) n stacked matrix ( cid : 123 ) y123; x123 ( cid : 123 ) is of low rank .
we will start
sign ( y123 ) will contain the completed and correct labels .
from a hard formulation that is illustrative but impractical , then relax it .
sign ( zij ) = yij , ( i , j ) y;
here , z is meant to recover ( cid : 123 ) y123; x123 ( cid : 123 ) by directly minimizing the rank while obeying the observed
features and labels .
note the indices ( i , j ) x are with respect to x , such that i ( 123 , .
to index the corresponding element in the larger stacked matrix z , we need to shift the row index by t to skip the part for y123 , and hence the constraints z ( i+t ) j = xij .
the above formulation assumes that there is no noise in the generation processes x123 x and y123 y .
of course , there are several issues with formulation ( 123 ) , and we handle them as follows :
z ( i+t ) j = xij , ( i , j ) x
rank ( ) is a non - convex function and difcult to optimize .
following recent work in matrix completion ( 123 , 123 ) , we relax rank ( ) with the convex nuclear norm kzk = k ( z ) , where ks are the singular values of z .
the relationship between rank ( z ) and kzk is analogous to that of 123 - norm and 123 - norm for vectors .
there is feature noise from x123 to x .
instead of the equality constraints in ( 123 ) , we minimize 123 ( u v ) 123 in this similarly , there is label noise from y123 to y .
the observed labels are of a different type than the observed features .
we therefore introduce another loss function cy ( zij , yij ) to in this work , we use the logistic loss cy ( u , v ) = account for the heterogeneous data .
log ( 123 + exp ( uv ) ) .
a loss function cx ( z ( i+t ) j , xij ) .
we choose the squared loss cx ( u , v ) = 123 work , but other convex loss functions are possible too .
in addition to these changes , we will model the bias b either explicitly or implicitly , leading to two alternative matrix completion formulations below .
formulation 123 ( mc - b ) .
in this formulation , we explicitly optimize the bias b rt in addition to
z r ( t+d ) n , hence the name .
here , z corresponds to the stacked matrix ( cid : 123 ) wx123; x123 ( cid : 123 ) instead of ( cid : 123 ) y123; x123 ( cid : 123 ) , making it potentially lower rank .
the optimization problem is
cy ( zij + bi , yij ) +
where , are positive trade - off weights .
notice the bias b is not regularized .
this is a convex problem , whose optimization procedure will be discussed in section 123
once the optimal z , b are found , we recover the task - i label of item j by sign ( zij + bi ) , and feature k of item j by z ( k+t ) j .
formulation 123 ( mc - 123 ) .
in this formulation , the bias is modeled implicitly within z .
similar to how bias is commonly handled in linear classiers , we append an additional feature with constant value
one to each item .
the corresponding pre - feature matrix is augmented into ( cid : 123 ) x123; 123> ( cid : 123 ) , where 123 is the y123 are linear combinations of rows in ( cid : 123 ) x123; 123> ( cid : 123 ) , i . e . , rank ( ( cid : 123 ) y123; x123; 123> ( cid : 123 ) ) = rank ( ( cid : 123 ) x123; 123> ( cid : 123 ) ) .
we then let z correspond to the ( t + d + 123 ) n stacked matrix ( cid : 123 ) y123; x123; 123> ( cid : 123 ) , by forcing its last row to
all - 123 vector .
under the same label assumption y123
j + b , the rows of the soft label matrix
j = wx123
be 123> ( hence the name ) :
z ( t+d+123 ) = 123> .
cy ( zij , yij ) +
this is a constrained convex optimization problem .
once the optimal z is found , we recover the task - i label of item j by sign ( zij ) , and feature k of item j by z ( k+t ) j .
mc - b and mc - 123 differ mainly in what is in z , which leads to different behaviors of the nuclear norm .
despite the generative story , we do not explicitly recover the weight matrix w in these formulations .
other formulations are certainly possible .
one way is to let z correspond to ( cid : 123 ) y123; x123 ( cid : 123 ) directly ,
without introducing bias b or the all - 123 row , and hope nuclear norm minimization will prevail .
this is inferior in our preliminary experiments , and we do not explore it further in this paper .
123 optimization techniques
we solve mc - b and mc - 123 using modications of the fixed point continuation ( fpc ) method of ma , goldfarb , and chen ( 123 ) . 123 while nuclear norm minimization can be converted into a semidenite programming ( sdp ) problem ( 123 ) , current sdp solvers are severely limited in the size of problems they can solve .
instead , the basic xed point approach is a computationally efcient alternative , which provably converges to the globally optimal solution and has been shown to outperform sdp solvers in terms of matrix recoverability .
123 fixed point continuation for mc - b
we rst describe our modied fpc method for mc - b .
it differs from ( 123 ) in the extra bias variables and multiple loss functions .
our xed point iterative algorithm to solve the unconstrained problem of ( 123 ) consists of two alternating steps for each iteration k :
( gradient step ) bk+123 = bk bg ( bk ) , ak = zk zg ( zk ) 123
( shrinkage step ) zk+123 = sz ( ak ) .
in the gradient step , b and z are step sizes whose choice will be discussed next .
overloading notation a bit , g ( bk ) is the vector gradient , and g ( zk ) is the matrix gradient , respectively , of the two loss terms in ( 123 ) ( i . e . , excluding the nuclear norm term ) :
123 + exp ( yij ( zij + bi ) )
123+exp ( yij ( zij +bi ) ) ,
i t and ( i , j ) y i > t and ( i t , j ) x
note for g ( zij ) , i > t , we need to shift down ( un - stack ) the row index by t in order to map the element in z back to the item x ( it ) j .
123while the primary method of ( 123 ) is fixed point continuation with approximate singular value decom - position ( fpca ) , where the approximate svd is used to speed up the algorithm , we opt to use an exact svd for simplicity and will refer to the method simply as fpc .
input : initial matrix z123 , bias b123 , parameters , , step sizes b , z determine 123 > 123 > > l = > 123
set z = z123 , b = b123
foreach = 123 , 123 , .
, l do
while not converged do
compute b = b bg ( b ) , a = z zg ( z ) compute svd of a = uv> compute z = u max ( z , 123 ) v>
output : recovered matrix z , bias b
algorithm 123 : fpc algorithm for mc - b .
parameters , , step sizes z
input : initial matrix z123 , determine 123 > 123 > > l = > 123
set z = z123
foreach = 123 , 123 , .
, l do
while not converged do
compute a = z zg ( z ) compute svd of a = uv> compute z = u max ( z , 123 ) v> project z to feasible region z ( t+d+123 ) = 123>
output : recovered matrix z
algorithm 123 : fpc algorithm for mc - 123
in the shrinkage step , sz ( ) is a matrix shrinkage operator .
let ak = uv> be the svd of ak .
then sz ( ak ) = u max ( z , 123 ) v> , where max is elementwise .
that is , the shrinkage operator shifts the singular values down , and truncates any negative values to zero .
this step reduces the nuclear norm .
even though the problem is convex , convergence can be slow .
we follow ( 123 ) and use a con - tinuation or homotopy method to improve the speed .
this involves beginning with a large value 123 > and solving a sequence of subproblems , each with a decreasing value and using the pre - vious solution as its initial point .
the sequence of values is determined by a decay parameter : k = 123 , .
, l 123 , where is the nal value to use , and l is the number k+123 = max ( k , ) , of rounds of continuation .
the complete fpc algorithm for mc - b is listed in algorithm 123
a minor modication of the argument in ( 123 ) reveals that as long as we choose non - negative step sizes satisfying b < 123|y| / ( n ) and z < min ( 123|y| / , |x| ) , the algorithms mc - b will be guaranteed to converge to a global optimum .
indeed , to guarantee convergence , we only need that the gradient step is non - expansive in the sense that kb123bg ( b123 ) b123 +bg ( b123 ) k123 +kz123zg ( z123 ) z123 +zg ( z123 ) k123 f kb123b123k123 +kz123z123k123 for all b123 , b123 , z123 , and z123
our choice of b and z guarantee such non - expansiveness .
once this non - expansiveness is satised , the remainder of the convergence analysis is the same as in ( 123 ) .
123 fixed point continuation for mc - 123
our modied fpc method for mc - 123 is similar except for two differences .
first , there is no bias variable b .
second , the shrinkage step will in general not satisfy the all - 123 - row constraints in ( 123 ) .
thus , we add a third projection step at the end of each iteration to project zk+123 back to the feasible region , by simply setting its last row to all 123s .
the complete algorithm for mc - 123 is given in algo - rithm 123
we were unable to prove convergence for this gradient + shrinkage + projection algorithm .
nonetheless , in our empirical experiments , algorithm 123 always converges and tends to outperform mc - b .
the two algorithms have about the same convergence speed .
we now empirically study the ability of matrix completion to perform multi - class transductive clas - sication when there is missing data .
we rst present a family of 123 experiments on a synthetic task by systematically varying different aspects of the task , including the rank of the problem , noise level , number of items , and observed label and feature percentage .
we then present experiments on two real - world datasets : music emotions and yeast microarray .
in each experiments , we compare mc - b and mc - 123 against four other baseline algorithms .
our results show that mc - 123 consistently outperforms other methods , and mc - b follows closely .
parameter tuning and other settings for mc - b and mc - 123 : to tune the parameters and , we use 123 - fold cross validation ( cv ) separately for each experiment .
specically , we randomly
, |x| ) , b = 123|y|
123 of the observed entries , measure its performance on the remaining 123
divide x and y into ve disjoint subsets each .
we then run our matrix completion algorithms 123 , and average over the ve folds .
since our main goal is to predict unobserved labels , we use label error as the cv performance criterion to select parameters .
note that tuning is quite efcient since all values under consideration can be evaluated in one run of the continuation method .
we set = 123 and , as in ( 123 ) , consider values starting at 123 , where 123 is the largest singular value of the matrix of observed entries in ( y; x ) ( with the unobserved entries set to 123 ) , and decrease until 123
the range of values considered was ( 123 , 123 , 123 , 123 ) .
we initialized b123 to be all zero and z123 to be the rank - 123 approximation of the matrix of observed entries in ( y; x ) ( with unobserved entries set to 123 ) obtained by performing an svd and reconstructing the matrix using only the largest singular value and corresponding left and right singular vectors .
the step sizes were set as follows : z = min ( 123|y| n .
convergence was dened as relative change in objective functions ( 123 ) ( 123 ) smaller than 123
baselines : we compare to the following baselines , each consisting of some missing feature impu - tation step on x rst , then using a standard svm to predict the labels : ( fpc+svm ) matrix com - pletion on x alone using fpc ( 123 ) .
( em ( k ) +svm ) expectation maximization algorithm to impute missing x entries using a mixture of k gaussian components .
as in ( 123 ) , missing features , mixing component parameters , and the assignments of items to components are treated as hidden variables , which are estimated in an iterative manner to maximize the likelihood of the data .
( mean+svm ) impute each missing feature by the mean of the observed entries for that feature .
( zero+svm ) impute missing features by lling in zeros .
after imputation , an svm is trained using the available ( noisy ) labels in y for that task , and predictions are made for the rest of the labels .
all svms are linear , trained using svmlin123 , and the regularization parameter is tuned using 123 - fold cross validation separately for each task .
the range of parameter values considered was ( 123 , 123 , .
, 123 , 123 ) .
evaluation method : to evaluate performance , we consider two measures : transductive label error , i . e . , the percentage of unobserved labels predicted incorrectly; and relative feature imputation error ij , where x is the predicted feature value .
in the tables below , for each parameter setting , we report the mean performance ( and standard deviation in parenthesis ) of different algorithms over 123 random trials .
the best algorithm within each parameter setting , as well as any statistically indistinguishable algorithms via a two - tailed paired t - test at signicance level = 123 , are marked in bold .
123 synthetic data experiments
synthetic data generation : we generate a family of synthetic datasets to systematically explore the performance of the algorithms .
we rst create a rank - r matrix x123 = lr> , where l rdr and r rnr with entries drawn iid from n ( 123 , 123 ) .
we then normalize x123 such that its entries have variance 123
next , we create a weight matrix w rtd and bias vector b rt , with all entries drawn iid from n ( 123 , 123 ) .
we then produce x , y123 , y according to section 123 .
finally , we produce the random x , y masks with percent observed entries .
using the above procedure , we vary = 123% , 123% , 123% , n = 123 , 123 , r = 123 , 123 , and 123 123 , 123 , while xing t = 123 , d = 123 , to produce 123 different parameter settings .
for each setting , we generate 123 trials , where the randomness is in the data and mask .
synthetic experiment results : table 123 shows the transductive label errors , and table 123 shows the relative feature imputation errors , on the synthetic datasets .
we make several observations .
observation 123 : mc - b and mc - 123 are the best for feature imputation , as table 123 shows .
however , the imputations are not perfect , because in these particular parameter settings the ratio between the number of observed entries over the degrees of freedom needed to describe the feature matrix ( i . e . , r ( d + n r ) ) is below the necessary condition for perfect matrix completion ( 123 ) , and because there is some feature noise .
furthermore , our cv tuning procedure selects parameters , to optimize label error , which often leads to suboptimal imputation performance .
in a separate experiment ( not reported here ) when we made the ratio sufciently large and without noise , and specically tuned for
table 123 : transductive label error of six algorithms on the 123 synthetic datasets .
the varying pa - , rank ( x123 ) = r , number of items n , and observed label and feature rameters are feature noise 123 percentage .
each row is for a unique parameter combination .
each cell shows the mean ( standard deviation ) of transductive label error ( in percentage ) over 123 random trials .
the meta - average row is the simple average over all parameter settings and all trials .
fpc+svm em123+svm mean+svm
imputation error , both mc - b and mc - 123 did achieve perfect feature imputation .
also , fpc+svm is slightly worse in feature imputation .
this may seem curious as fpc focuses exclusively on imputing x .
we believe the fact that mc - b and mc - 123 can use information in y to enhance feature imputation in x made them better than fpc+svm .
observation 123 : mc - 123 is the best for multi - label transductive classication , as suggested by table 123
surprisingly , the feature imputation advantage of mc - b did not translate into classication , and fpc+svm took second place .
observation 123 : the same factors that affect standard matrix completion also affect classication performance of mc - b and mc - 123
as the tables show , everything else being equal , less feature noise ) , lower rank r , more items , or more observed features and labels , reduce label error .
benecial combination of these factors ( the 123th row ) produces the lowest label errors .
matrix completion benets from more tasks .
we performed one additional synthetic data exper - iment examining the effect of t ( the number of tasks ) on mc - b and mc - 123 , with the remaining data parameters xed at = 123% , n = 123 , r = 123 , d = 123 , and 123 = 123 .
table 123 reveals that both mc methods achieve statistically signicantly better label prediction and imputation performance with t = 123 than with only t = 123 ( as determined by two - sample t - tests at signicance level 123 ) .
123 music emotions data experiments
in this task introduced by trohidis et al .
( 123 ) , the goal is to predict which of several types of emotion are present in a piece of music .
the data123 consists of n = 123 songs of a variety of musical genres , each labeled with one or more of t = 123 emotions ( i . e . , amazed - surprised , happy - pleased , relaxing - calm , quiet - still , sad - lonely , and angry - fearful ) .
each song is represented by d = 123 features ( 123 rhythmic , 123 timbre - based ) automatically extracted from a 123 - second sound clip .
123available at http : / / mulan . sourceforge . net / datasets . html
table 123 : relative feature imputation error on the synthetic datasets .
the algorithm zero+svm is not shown because it by denition has relative feature imputation error 123
table 123 : more tasks help matrix completion ( = 123% , n = 123 , r = 123 , d = 123 , 123
transductive label error
relative feature imputation error
table 123 : performance on the music emotions data .
transductive label error
relative feature imputation error
we vary the percentage of observed entries = 123% , 123% , 123% .
for each , we run 123 random trials with different masks x , y .
for this dataset , we tuned only with cv , and set = 123
the results are in table 123
most importantly , these results show that mc - 123 is useful for this real - world multi - label classication problem , leading to the best ( or statistically indistinguishable from the best ) transductive error performance with 123% and 123% of the data available , and close to the best with only 123% .
we also compared these algorithms against an oracle baseline ( not shown in the table ) .
in this baseline , we give 123% features ( i . e . , no indices are missing from x ) and the training labels in y to a standard svm , and let it predict the unspecied labels .
on the same random tri - als , for observed percentage = 123% , 123% , 123% , the oracle baseline achieved label error rate 123 ( 123 ) , 123 ( 123 ) , 123 ( 123 ) respectively .
interestingly , mc - 123 with = 123% ( 123 ) is statisti - cally indistinguishable from the oracle baseline .
123 yeast microarray data experiments
this dataset comes from a biological domain and involves the problem of yeast gene functional classication .
we use the data studied by elisseeff and weston ( 123 ) , which contains n = 123 examples ( yeast genes ) with d = 123 input features ( results from microarray experiments ) . 123 we follow the approach of ( 123 ) and predict each genes membership in t = 123 functional classes .
for this larger dataset , we omitted the computationally expensive em123+svm methods , and tuned only for matrix completion while xing = 123
table 123 reveals that mc - b leads to statistically signicantly lower transductive label error for this bi - ological dataset .
although not highlighted in the table , mc - 123 is also statistically better than the svm methods in label error .
in terms of feature imputation performance , the mc methods are weaker than fpc+svm .
however , it seems simultaneously predicting the missing labels and features appears to provide a large advantage to the mc methods .
it should be pointed out that all algorithms except zero+svm in fact have small but non - zero standard deviation on imputation error , despite what the xed - point formatting in the table suggests .
for instance , with = 123% , the standard deviation is 123 for mc - 123 , 123 for fpc+svm , and 123 for mean+svm .
again , we compared these algorithms to an oracle svm baseline with 123% observed entries in x .
the oracle svm approach achieves label error of 123 ( 123 ) , 123 ( 123 ) , and 123 ( 123 ) for =123% , 123% , and 123% observed labels , respectively .
both mc - b and mc - 123 signicantly outperform this oracle under paired t - tests at signicance level 123 .
we attribute this advantage to a combination of multi - label learning and transduction that is intrinsic to our matrix completion methods .
table 123 : performance on the yeast data .
transductive label error
relative feature imputation error
123 discussions and future work
we have introduced two matrix completion methods for multi - label transductive learning with miss - ing features , which outperformed several baselines .
in terms of problem formulation , our methods differ considerably from sparse multi - task learning ( 123 , 123 , 123 ) in that we regularize the feature and label matrix directly , without ever learning explicit weight vectors .
our methods also differ from multi - label prediction via reduction to binary classication or ranking ( 123 ) , and via compressed sensing ( 123 ) , which assumes sparsity in that each item has a small number of positive labels , rather than the low - rank nature of feature matrices .
these methods do not naturally allow for missing fea - tures .
yet other multi - label methods identify a subspace of highly predictive features across tasks in a rst stage , and learn in this subspace in a second stage ( 123 , 123 ) .
our methods do not require separate stages .
learning in the presence of missing data typically involves imputation followed by learning with completed data ( 123 ) .
our methods perform imputation plus learning in one step , similar to em on missing labels and features ( 123 ) , but the underlying model assumption is quite different .
a drawback of our methods is their restriction to linear classiers only .
one future extension is to explicitly map the partial feature matrix to a partially observed polynomial ( or other ) kernel gram matrix , and apply our methods there .
though such mapping proliferates the missing entries , we hope that the low - rank structure in the kernel matrix will allow us to recover labels that are nonlinear functions of the original features .
acknowledgements : this work is supported in part by nsf iis - 123 , nsf iis - 123 , afosr fa123 - 123 - 123 - 123 , and afosr a123 - 123 - 123 - 123
we also wish to thank brian eriksson for useful discussions and source code implementing em - based imputation .
123available at http : / / mulan . sourceforge . net / datasets . html
