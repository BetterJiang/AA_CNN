we propose a novel method for using the world wide web to acquire trigram estimates for statistical lan - guage modeling .
we submit an n - gram as a phrase query to web search engines .
the search engines return the number of web pages containing the phrase , from which the n - gram count is estimated .
the n - gram counts are then used to form web - based trigram probability estimates .
we discuss the properties of such estimates , and methods to interpolate them with traditional corpus based trigram estimates .
we show that the interpolated models improve speech recognition word error rate significantly over a small test set .
distribution statement a
approved for public release a - aajaa^ a 123 123 p distribution unlimited 123 123
keywords : language models , speech recognition and synthesis , web - based services
a language model is a critical component for many applications , including speech recognition .
enormous effort has been spent on building and improving language models .
broadly speaking , this effort develops along two orthogonal directions : the first direction is to apply increasingly sophisticated estimation methods to a fixed training data set ( corpus ) to achieve better estimation .
examples include various interpolation and backoff schemes for smoothing , variable length n - grams , vocabulary clustering , decision trees , probabilistic context free grammar , maximum entropy models , etc ( 123 ) .
we can view these methods as trying to " squeeze out " more benefit from a fixed corpus .
the second direction is to acquire more training data .
however , automatically collecting and incoiporating new training data is non - trivial , and there has been relatively little research in this direction .
adaptive models are examples of the second direction .
for instance , a cache language model uses recent utterances as additional training data to create better n - gram estimates .
the recent rapid development of the world wide web ( www ) makes it an extremely large and valuable data source .
just - in - time language modeling ( 123 ) submits previous user utterances as queries to www search engines , and uses the retrieved web pages as unigram adaptation data .
in this paper , we propose a novel method for using the www and its search engines to derive additional training data for n - gram language modeling , and show significant improvements in terms of speech recognition word error rate .
the rest of the paper is organized as follows .
section 123 gives the outline of our method , and discusses the relevant properties of the www and search engines .
section 123 investigates the problem of combining a traditional corpus with data from the web .
section 123 presents our experimental results .
finally section 123 discusses both the potential and the limitations of our proposed method , and lists some possible extensions .
123 the www as trigram training data
the basic problem in trigram language modeling is to estimate 123> ( w123\wiiw123 ) > i - e - the probability of a word given the two words preceding it .
this is typically done by smoothing the maximum likelihood estimate
with various methods , where c ( wiio123ws ) and c ( wiw123 ) are the counts of , 123iviw123w^ and " wiu^ " in some training data respectively .
the main idea behind our method is to obtain the counts of nwiw123w123n and " wiw123 " as they appear on the www , to estimate
- / , x cweb ( wiw123w123 )
and combine pweb with the estimates from a traditional corpus ( here and elsewhere , when cweb ( wiw123 ) = 123 , we regard pweb ( w123 , \wi , w123 ) as unavailable ) .
essentially , we are using the searchable web as additional training data for trigram language modeling .
there are several questions to be addressed .
first , how to obtain the counts from the web ? what is the quality of these web estimates ? how could they be used to improve language modeling ? we will examine these questions in the following sections , in the context of n - best list rescoring for speech recognition .
123 obtaining n - gram counts from the www
v from the web , we use the ' exact phrase search ' function to obtain the count of an n - gram " w\ . . .
wn v as a single quoted phrase query to a search engine .
of web search engines .
that is , we send " w\ . . .
wn ideally , we would like the search engine to report the phrase count , i . e .
the total number of occurrences of
the phrase in all its indexed web pages .
however in practice , most search engines only report the web page count , i . e .
the number of web pages containing the phrase .
since one web page may contain one or more occurrence of the phrase , we need to estimate the phrase count from the web page count .
many web search engines claim they can perform exact phrase search .
however , most of them seem to use an internal stop word list to remove common words from a query phrase .
an interesting test phrase is " to be or not to be " : some search engines return totally irrelevant web pages for this query , since most , if not all , words are ignored .
in addition , a few search engines perform stemming so the query " she say " will return some web pages only containing " she says " or " she said " .
furthermore , some search engines report neither phrase counts nor web page counts .
we experimented with a dozen popular search engines , and found three that meet our criteria : altavista ( 123 ) advanced search mode , lycos ( 123 ) , and fast ( 123 ) ) .
they all report web page counts .
one brate force method to get the phrase counts is to actually download all the web pages the search engine finds .
however , queries of common words typically result in tens of thousands of web pages , and this method is clearly infeasible .
fortunately at the time of our experiment altavista had a simple search mode , which reported both the phrase count and the web page count for a query .
figure 123 shows the phrase count vs .
web page count for 123 queries .
trigram queries ( phrases consisting of three consecutive words ) , bigram queries and unigram queries are plotted separately .
there are horizontal branches in the bigram and trigram plots that don ' t make sense ( more web pages than total phrase counts ) .
we regard these as outliers due to idiosyncrasies of the search engine , and exclude them from further consideration .
the three plots are largely log - linear .
this prompted us to perform the following log - linear regression separately for trigrams , bigrams , and unigrams :
c = q123 * pg ' m
where c is the phrase count , and pg the web page count .
table 123 lists the coefficients .
the three regression functions are also plotted in figure 123
we assume these functions apply to other search engines as well .
in the rest of the paper , all web n - gram counts are estimated by applying the corresponding regression function to the web page counts reported by search engines .
123 123 " 123 "
webpage number returned by search engines
figure 123 : web phrase count vs .
web page count
' our selection is admittedly incomplete .
in addition , since search engines develop and change rapidly , all our comments are
only valid during the period of this experiment .
table 123 : coefficients of log - linear regression for estimating web n - gram counts from web page counts reported by search engines .
123 the quality of web estimates
to investigate the quality of web estimates , we needed a baseline corpus for comparison .
the baseline we used is a 123 million word broadcast news corpus .
123 . 123 web n - gram coverage
the first experiment we ran was n - gram coverage test on unseen text .
that is , we wanted to see how many n - grams in the test text are not on the web , and / or not in the baseline corpus .
we were hoping to show that the web covers many more n - grams than the baseline corpus .
note that by ' the web ' we mean the searchable portion of the web as indexed by the search engines we chose .
the unseen news test text consisted of 123 randomly chosen sentences from 123 web news sources ( cnn , abc , fox , bbc ) and 123 categories ( world , domestic , technology , health , entertainment , politics ) .
all the sentences were selected from the day ' s news stories , on the day the experiment was carried out .
this was to make sure that the search engines hadn ' t had the time to index the web pages containing these sentences .
after the experiment was completed , we checked each sentence , and indeed none of them were found by the search engines yet .
therefore the test text is truly unseen to both the web search engines and the baseline corpus .
( the test text is of written news style , which might be slightly different from the broadcast news style in the baseline corpus . )
there are 123 unigram types ( i . e .
unique words ) , 123 bigram types and 123 trigram types in the test text .
table 123 lists the number of n - gram types not covered by the different search engines and the baseline
not covered by
altavista lycos fast corpus
table 123 : novel n - gram types in 123 news sentences
clearly , the web ' s coverage , under any of the search engines , is much better than that of the baseline corpus .
it is also worth noting that for this test text , any n - gram not covered by the web was also not covered by the baseline corpus .
in the next experiment , we focused on the trigrams in the test text to answer the question " if one ran - domly picks a trigram from the test text , what ' s the chance the trigram has appeared c times in the training data ? " figure 123 shows the comparison , with the training data being the baseline corpus and the web through
i baseline corpus
123 123 123 123 123 123 123 123 123 123 123
figure 123 : empirical frequency - frequency plot
the different search engines , respectively .
this figure is also known as a " frequency - of - frequency " plot .
ac - cording to this figure , a trigram from the test text has more than 123% chance of being absent in the baseline corpus , and the chance goes down to about 123% on the web , regardless of the search engine .
this is consis - tent with table 123
moreover , the trigram has a much larger chance in having a small count in the baseline corpus than on the web .
since small counts usually mean unreliable estimates , resorting to the web could be
123 . 123 the effective size of the web
recently , fienberg et al .
( 123 ) estimated the size of the indexable web as of 123 to be close to 123 billion pages .
the web grows exponentially , and as of this writing some search engines claim they have indexed more than 123 billion pages .
we would like to estimate the effective size of the web as a language model training corpus .
let ' s assume that the web and the baseline corpus are homogeneous ( which is patently false , since the web has much more than news , but we will ignore this for the time being ) .
then the probability of a particular n - gram appearing in the baseline corpus is the same as the probability that it appears on the web :
since the probabilities can be approximated by their respective frequencies , we have
? wpus ( n - gram ) = pw ( b ( n - gram )
ccorp us ( n - gram ) _ cwcb ( n - gram
| corpus |
| web |
, from which we can estimate |web| , the size of the web in words .
note that it doesn ' t matter if the n - gram is a unigram , bigram or trigram , though n - grams with small counts are unreliable and should be excluded .
in our experiment , we considered all unigrams , bigrams and trigrams in the test text with ccorpus > 123
each such n - gram will gave us an estimate , and we took the median of all these estimates for robustness .
table 123 gives our estimates of the size with different search engines .
some points to notice :
the ' effective web size ' estimates we obtained are very rough at best .
moreover , they are defined relative to the specific baseline corpus and specific test set we happened to choose .
therefore , table 123 should not be used to rank the performance of individual search engines .
effective size of the web
123 billion words 123 billion words 123 billion words
table 123 : the effective size of the web for language model training
this method tends to underestimate the web size .
we assumed homogeneity , which in actuality does not hold .
the test text comes from a news domain , and so does the baseline corpus .
we used n - grams from the test text to estimate the web size , which gives rise to a selectional bias .
intuitively , only " news terms " are in the test text .
and since the corpus is in news domain , as a whole we have pcorpus ( news terms ) > p , e; , ( news terms ) .
this is what leads to underestimation .
123 . 123 normalization of the web counts
an interesting sanity check is to see whether
cweb ( lv123w123 ) = yl cu>eb ( w123u>123w123 )
holds for any bigram " wiw123 " - if this is true , the relative frequency estimation plueb ( ' m\ti , w123 ) would already be normalized , i . e .
x ) pweb ( w123\w123 , w123 ) = l , vwi , w123
of course there are too many " wiw123 ' 123 " combinations to verify this directly .
instead , we randomly chose six " io123w123 " pairs from the baseline corpus .
for each pair , we chose 123 w123 ' s according to the fol - lowing heuristic : first , we selected words from a list of all w123 ' s such that the trigram " wi w123 w123 " appeared in the baseline corpus , sorted by decreasing frequency; if fewer than 123 words were chosen that way , we added words from a list of all w123 ' s such that the bigram " w123 w123 " appeared in the baseline corpus , in decreas - ing frequency order; if this was still not enough , we added w123 ' s according to their unigram frequencies .
we expected this heuristic to give us a list of w123 ' s that covers the majority of the conditional probability mass given history " wi w123 " .
table 123 shows web bigram count estimates obtained with fast search , together with their respective cumulative web trigram count estimates as described above .
ideally , the ratio should be close to , but less than , 123% .
it is evident from the table that the web counts are not perfectly normalized .
the reasons are not entirely clear to us , but the fact that the n - gram counts are estimated from page counts is an obvious candidate .
the web n - gram count estimates should therefore be used with caution .
123 . 123 the variance and bias of web trigram estimates
as stated earlier , we are interested in estimating conditional trigram probabilities based on their relative frequency on the web :
it would be informative to compare pweb ( w123\wi , w123 ) to a traditional ( corpus derived ) trigram probability
table 123 : sanity check : are web counts normalized ?
123* i '
123 123 123 123ooo123 123oo123o123
count of www in baseline corpus
figure 123 : ratio of web trigram estimates to corpus trigram estimates
to this end , we created a baseline trigram language model lj\i123 from the 123 million word baseline cor - pus .
we used modified kneser - ney smoothing ( 123 ) ( 123 ) which , according to ( 123 ) , is one of the best smoothing methods available .
in building lm123 , we discarded all singleton trigrams in the baseline corpus , a common practice to reduce language model size .
we denote lmq ' s probability estimates by jjq .
with lm123 , we were able to compare p , lf123 ( ' 123| ' i^ ' 123 ) with 123^123 ( ' 123 ' ' ii " ' 123 ) - we computed the ratio r :
/ ' u ' j , - 123> ' 123 =
between these two estimates .
we expected r to be more spread out ( having larger variance ) when ccorpus ( wi w123 ' " 123 ) is small , since in this case poiu^wi , ' 123 ) tends to be unreliable .
we computed r ( wi , w123 , u ' 123 ) for every trigram in the test text , excluding those with cu , ci , ( wiw123 ) = 123
we plot r ( wi , w - 123 , ' 123 ) vs .
ccorpus ( wiic123w123 ) in figure 123
we found that :
for trigrams with large ccorpu . s ( ' iw ' 123w ' 123 ) .
? ' averages to about 123
thus the web estimates are consistent
with lmo in this case .
as we expected , the variance of r is largest when corpus ( ! < ' i ' 123 ' 123 ) =123 , and decreases when it gets
hence the ' funnel ' shape .
when ccor123 , us ( u>iu>123* " 123 ) is small , especially 123 and 123 , r is biased upward .
this is of course good news ,
as it suggests that this is where the web estimates tend to improve on the corpus estimates .
all the search engines give similar results .
123 combining web estimates with existing language model
in the previous section , we saw the potential of the web : it is huge , it has better trigram coverage , and its trigram estimates are largely consistent with the corpus - based estimates .
nevertheless , to query each and every n - gram on the web is infeasible .
this prevents us from building a full fledged language model from the web via search engines .
more over , table 123 indicates that web estimates are not well normalized .
in addition , the content of the web is heterogeneous and usually doesn ' t coincide with our domain of interest .
based on these considerations , we decided not to try to build an entire language model from the web .
rather , we will start from a traditional language model lm123 , and interpolate its least reliable trigram estimates with the appropriate estimates from the web .
unreliable trigram estimates , especially those involving backing off to lower order n - grams , have been shown to be correlated with increased speech recognition errors ( 123 ) ( 123 ) .
by going to the much larger web for reliable estimates , our hope was to alleviate this problem .
we used the trigram counts in the baseline corpus as a heuristic to decide the reliability of trigram estimates in lm123 .
a trigram estimate po ( w123 ui , w123 ) is deemed unreliable , if
cwpus ( u ' l ' 123 ' 123 ) < t
where r , the ' reliability threshold ' , is a predetermined small positive integer , e . g .
admittedly this defini - tion of unreliable estimates is biased .
even with this definition , there are still too many unreliable trigram estimates to query the web for .
since we were interested in n - best list rescoring , we further restricted the queries to those unreliable trigrams that appeared in the particular n - best list being processed .
this greatly reduces the number of web queries at the price of some further bias .
let uwl w123 be the set of words that have unreliable trigram estimates with history " j123w123 " in the current n - best list , i . e .
uwlw123 = ( w123\n wiw123w123
n g n - best a ccorpus ( w123w123i123 ) < r )
v " wiw123 " n - best a cweb ( w123w123 ) > 123 , ( 123 )
we obtain cweb ( wi , w123 , u ) , u g uwlw123 and cweb ( wiw123 ) via search engines , and compute pweb ( u\wi , w123 ) , the web relative frequency estimates , from these web counts .
letj ) * ( u\wi , w123 ) denote the final interpolated estimates , which combine p123 ( u\wi , w123 ) sxidpweb ( u\wu w123 ) we would like to have a tunable parameter so that on one extreme p* ( u\wi , w123 ) po ( u\wi , w123 ) , while on the other extreme p* ( u\wi , w123 ) > pweb ( u\wi , w123 ) .
we now present three different methods for doing this .
123 exponential models with gaussian priors
we define a set of binary functions , or ' features ' , as follows :
, , , _ j 123 if u = id123 uuw123 , u^123 ) - | 123 otherwise
for all u ? i , w123 , u g uwlw123 in the n - best list .
next , for any given wi , w123 , we define a conditional exponential
e with these features :
z^po ( w123\wllw123 ) exp ( e ( / u , iu , 123 au / u , li ( 123iu ( u>123 ) )
where p123 is the estimate provided by lm123 , a ' s are parameters to be optimized , and zwl , , is a normalization factor .
this model has exactly the same form as a conventional maximum entropy / minimum discrim - inative information ( me / mdi ) model ( 123 ) ( 123 ) .
let a denote the set of parameters .
if we maximize the likelihood of the web counts :
i ( a ) = n ^ ( u ' 123i ,
with the standard generalized iterative scaling algorithm ( gis ) ( 123 ) , we get the me / mdi solution that satisfies the following constraints :
123^ ( | ' i , ' 123 ) = p , rrb ( ii\iri , ir123 ) ( 123 )
fi ( . ( ' l , m ' 123 , w )
- ywi . w123it e f ' * . ) l ( , 123
this corresponds to one extreme of the interpolation .
but since we want to control the degree of interpola - tion , we introduce a gaussian prior with mean 123 and variance a123 over a :
and instead of seeking the maximum likelihood solution , we seek the maximum a posteriori ( map ) solution
this can be done by slightly modifying the gis algorithm , as described in ( 123 ) .
with this gaussian prior , we can control the degree of interpolation by choosing the value of a123 ( 123 , +oo ) .
a123 acts as a tuning parameter : if a123 - > +oo , the gaussian prior is flat and has virtually no restriction on the values of the a ' s .
thus the a ' s can reach their me / mdi solutions , and hence pe reaches one extreme as in ( 123 ) .
on the other hand if a123 - > 123 , the gaussian prior forces a ' s to be close to the mean , which is 123
from ( 123 ) we know in this case p*e > p123
this corresponds to the other extreme of the interpolation .
a a123 between 123 and +oo results in an intermediate p*e distribution .
for the purpose of comparison , we experimented with two other interpolation methods , which are easy to implement but may be theoretically less well motivated : linear interpolation and geometric interpolation .
123 linear interpolation
in linear interpolation , we have
pl ( ' 123|w ' i , ' 123 ) = ( 123 )
( 123 - a ) po ( u^\wuw123 ) + apwfb ( w123\wi , w123 ) , if ' 123 e umu , 123
- r - * ^ r - ; rvn ! 123 u ' i , w123
in this case , a ( 123 , 123 ) is the tuning parameter .
if a = 123 , pi = p123
if a = 123 , p*l satisfies ( 123 ) .
an o in between results in an intermediate p*l .
123 geometrie interpolation
in geometric interpolation , we have
cweb ( wiw123 ) + \v\c_
, ifw123 e uunw123
note that here we have to smooth the web estimates to avoid zeros ( which is not a problem in the previous two methods ) .
to do this , we simply add a small positive value e to the web counts .
this is known as additive smoothing ( 123 ) .
the value of e is determined to minimize the perplexity with = 123
once e is chosen it is fixed , and we tune .
e ( 123 , 123 ) is the interpolation parameter .
if = 123 , pg = po - if = 123 , pg satisfies the smoothed web estimates .
a in between results in an intermediate pg .
123 experimental result
we randomly selected 123 utterance segments from the trec - 123 spoken document retrieval track data ( 123 ) as our test set for this experiment .
for each utterance we have its correct transcript and an n - best list with n = 123 , i . e .
123 decoding hypotheses .
we performed n - best list rescoring to measure the word error rate ( wer ) improvement , and computed the perplexity of the transcript .
note that the test set is relatively small and n = 123 is not very deep , since we wanted to limit the number of web queries to within a
123 word error rate
if we rescore the n - best lists with lm123 and pick the top hypotheses , the wer is 123% .
this is our baseline wer .
the oracle wer , i . e .
if we were able to pick the least errorful hypothesis among the 123 for each n - best list , is 123% .
of course we cannot achieve the oracle wer , but it indicates there is room for improvement over lmq .
since each utterance has 123 hypotheses in the n - best list , the total number of trigrams is very large .
table 123 lists the number of trigram tokens ( occurrences ) and types ( unique ones ) in all the n - best lists com - bined , together with the percentage of unreliable trigram types and tokens as determined by the reliability threshold r .
note that trigrams containing start - of - sentence or end - of - sentence ( commonly designated by < s > and < / s > ) are excluded from the table , since they can ' t be queried from the web .
for each n - best list , we queried the unreliable trigrams ( and associated bigrams ) in the list , from which we computed p* with the three different interpolation methods .
we then used p* to rescore the n - best list , and calculated the wer of the top hypothesis after rescoring .
first , we set the reliability threshold r = 123 , i . e .
we regard only those trigrams that never occur in the baseline corpus as unreliable .
figure 123 ( a ) shows the wer with exponential models and gaussian priors .
the three curves stand for different search engines , which turn out to be very similar .
the horizontal dashed line is the baseline wer .
as predicted , when the variance of the gaussian prior a123 > 123 ( the left side of the figure ) , p*e converges to p123 and the wer converges to the baseline wer .
on the other hand when a123 > +oo , the estimates of the unreliable trigrams come solely from the web .
such estimates seem inferior
reliability threshold r
table 123 : number of unreliable trigrams in the n - best lists
( a ) exponential models
( b ) linear interpolation
123 ! *^ / ' j
* " **^\ / /
xr* - jf " ' / x ^* / '
x x / ' '
\ \ / .
'
\ \\ .
\ *; ^ ' ^^
\vn do ^ y*^
^sc , + - ' ~ f<
\ * 123 p f / / \ \v* ? / , > ' \ ' if .
/ v ' / "
\ fr , ' altavista
123 123 123 123 123 123 123 123 123 123 123
( c ) geometric interpolation
( d ) reliability threshold
123 123 123 123 123 123 123 123 123
123 123 123 123 123 123 123 123 123 123 123
figure 123 : word error rates of web - improved language models as function of the smoothing parameter for several different interpolation schemes , based on n - best rescoring
and the model has higher wer than the baseline .
between these two extremes , wer reaches minimum ( 123% with altavista ) around a123 = 123
figure 123 ( b ) is the wer with linear interpolation .
again , the minimum wer 123% is reached between
the two extremes at a = 123 by altavista .
to use geometric interpolation , we needed to choose a value for e first .
we chose e = 123 because this minimized the perplexity when = 123
next we vary while keeping e fixed , and plotted the wer of the interpolated model in figure 123 ( c ) .
as with the previous interpolation methods , the wer reaches minimum when the interpolation factor is near the middle .
the minimum is 123% when = 123 with fast .
next , we adjusted the reliability threshold r and observe its effect on wer .
the interpolation method used here is the exponential model with gaussian prior and a123 = 123
we varied r from 123 to 123
with larger threshold , more trigrams are regarded as unreliable , and hence more web queries had to be issued .
as shown in figure 123 ( d ) , there is a slight but definite improvement in wer when we increase r from 123 to 123
for example , the wer with r = 123 and altavista is 123% .
further increment results in about the same wer , averaged over search engines .
note that lm123 , the language model we are incorporating web estimate into , was built after excluding all singleton trigrams in the corpus .
this may explain why r = 123 is better since trigrams with counts 123 or 123 in the corpus are indeed unreliable : in lm123 they must backoff to bigram
to analyze the source of improvement , we broke down the wer according to the trigram backoff modes in lmq .
first , we marked each word w ( in the transcript with one of several labels , using the following rules : let wj - _123 and wj_i be the two words preceding w ( .
if the trigram " w^w ' i - iti ' ; " exists in lm123 , label u ? , - as ' 123 ' .
otherwise if the trigram doesn ' t exist in lm123 , but the bigram " u>j_i>; " does , label w; as ' 123 - 123 ' , meaning lm123 has to backoff to the bigram for ; , ; .
if the bigram doesn ' t exist in lm123 either , label w ( as ' 123 - 123 - 123 ' since lm123 has to backoff to the unigram .
in the second step , we compared the transcript with the top hypotheses after rescoring the n - best lists with p123
each word in the transcript obtains a second label of either " correct " or " wrong " depending on whether the word is correct in the corresponding top hypothesis .
we then collect the percentage of correct words within categories ' 123 ' , ' 123 - 123 ' and ' 123 - 123 - 123 ' respectively .
in the third step we repeated the second step , except that the top hypotheses are now obtained by rescoring the n - best lists with pe , where a123 = 123 , r = 123 , and the search engine is altavista .
we compare the percentage of errors in step 123 and step 123 in table 123
note that insertion errors are not counted in our error break down .
not surprisingly , the ' 123 - 123 - 123 ' category has the highest error rate for both p123 and pe , since the words in this category are the hardest from the language model ' s point of view .
the ' 123 - 123 ' category has lower error rate , and ' 123 ' has the lowest .
the interpolated language model pe improves error rate for all three categories , compared to p123
the largest improvement is in the ' 123 - 123 - 123 ' category , which suggests the web helps lm123 most with the hardest cases .
it is not clear though why the ' 123 - 123 ' category is not improved as much .
table 123 : error break down by lmq backoff mode
123 approximate perplexity
there are 123 words in the transcript .
the baseline perplexity of the transcript with lm123 is 123 .
we wanted to compute the perplexity of the transcript with different interpolated language models .
we define uwlwo in ( 123 ) based on the transcript .
however this introduces a subtle bias : the interpolated models now depend on the transcript .
in other words , we are dynamically choosing models according to the words we will be predicting .
the resulting scores are therefore not strictly interpretable as probabilities .
for this reason we consider the perplexities we get on the transcript to be approximate only .
we still report these values in this section because we believe that the distortion is not too severe , and the approximation still provides useful insight into the true perplexity of web - improved language models .
note that , although the same kind of bias exists in wer computation , it doesn ' t diminish the validity of the wer improvement we get there , since in classification it is not the particular probability value but the ranking that matters .
figure 123 ( a - c ) compares different interpolation methods when the reliability threshold r = 123
there are 123 unique unreliable trigrams in the transcript .
we submitted them ( and the corresponding bigrams ) as queries to the search engines , and computed p* with the three different interpolation methods described in the last sections respectively .
from p* we computed the approximate perplexities .
figure 123 ( a ) shows the approximate perplexity with the exponential model and a gaussian prior .
like the wer in figure 123 ( a ) , the approximate perplexity converges to the baseline when the gaussian prior a123 - > 123
the approximate perplexity worsens when a123 > +oc .
the best value 123 is achieved by fast also between these two extremes at a123 = 123
again , different search engines are similar .
figure 123 ( b ) is the approximate perplexity with linear interpolation .
it is also similar to the wer in
figure 123 ( b ) .
the minimum 123 is reached by fast at a = 123 .
figure 123 ( c ) shows the approximate perplexity with geometric interpolation and f = 123 .
as with the previous interpolation methods , the approximate perplexity converges to the baseline when > 123 and is worse when y 123
but unlike the other methods , approximate perplexity seems to be always worse than the baseline , and increases monotonically with .
figure 123 ( d ) compares the effect of the reliability threshold r on the approximate perplexity .
as in figure 123 ( d ) , the interpolation method used is exponential model with gaussian prior and a123 = 123
again we see improvement when we increase r from 123 to 123
for example , fast ' s approximate perplexity goes down to 123 .
we believed this can be explained similarly to figure 123 ( d ) .
in this paper , we demonstrated that trigram estimates obtained from the web can significantly improve wer relative to pure corpus - based estimates , even though the web estimates are noisy , and the web and the test set are not in the same domain .
we believe the improvement largely comes from better trigram coverage due to the sheer size of the web , which acts as a ' general english ' knowledge source .
interestingly , which search engine is used doesn ' t make much difference .
furthermore , which interpolation method is used doesn ' t make much difference either ( at least for wer ) , as long as an appropriate interpolation parameter is chosen .
our method has certain advantages .
besides having better n - gram coverage , the content of the web is constantly changing .
our method would enable automatic up - to - date language modeling .
however , there are also several disadvantages .
the most severe one is the large number of web queries .
in our experiment , we needed to submit an average of 123 queries to the web for each utterance .
this results in heavy web traffic and workload on the search engines , and very slow rescoring process .
another concern is privacy : one may be sending fragments of potentially sensitive utterances to the web .
both problems , however , can be partly solved by using a web - in - a - box setting , i . e .
if we have a snapshot of the text content of the whole www on local storage .
yet another problem is the lack of focus on domain specific language .
this might
( a ) exponential models
- t - fast
/ - ' s
( b ) linear interpolation
123 123 123 123 123 123 123 123 123 123 123
( c ) geometrie interpolation
( d ) reliability threshold
123 123 123 123 123
123 123 123 123
figure 123 : approximate perplexity of web - improved language models as function of the smoothing parameter for several different interpolation schemes .
be solved by querying specific domain hosts instead of the whole web , although by doing so the n - gram coverage may deteriorate .
the method proposed in this paper is only one crude way of exploiting the web as a knowledge source for language modeling .
instead of focusing on trigrams , one could look for more complex phenomena , e . g .
semantic coherence ( 123 ) among the content words in a hypothesis .
intuitively , if a hypothesis has content words that ' go with each other ' , it is more likely than one whose content words seldom appear together in a large training text set .
the web + search engine approach seems well suited for this purpose .
we are currently pursing this direction .
the authors are grateful to stanley chen , matthew siegler , chris paciorek and kevin lenzo for their help .
the first author has been supported in part by nsf lis under grant rec - 123
