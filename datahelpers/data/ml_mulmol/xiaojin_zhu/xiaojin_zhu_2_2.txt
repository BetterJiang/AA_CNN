we present a novel text - to - picture system that synthe - sizes a picture from general , unrestricted natural lan - guage text .
the process is analogous to text - to - speech synthesis , but with pictorial output that conveys the gist of the text .
our system integrates multiple ai compo - nents , including natural language processing , computer vision , computer graphics , and machine learning .
we present an integration framework that combines these components by rst identifying informative and pic - turable text units , then searching for the most likely image parts conditioned on the text , and nally optimiz - ing the picture layout conditioned on both the text and image parts .
the effectiveness of our system is assessed in two user studies using childrens books and news ar - ticles .
experiments show that the synthesized pictures convey as much information about childrens stories as the original artists illustrations , and much more infor - mation about news articles than their original photos alone .
these results suggest that text - to - picture synthe - sis has great potential in augmenting human - computer and human - human communication modalities , with ap - plications in education and health care , among others .
a picture is worth a thousand words .
however , very few systems convert general text to pictorial representations that can be used in many circumstances to replace or augment the text .
we present a novel text - to - picture ( ttp ) synthe - sis system which automatically generates pictures , that aims to convey the primary content of general natural language text .
figure 123 shows a picture automatically generated by our ttp system .
our system employs ai techniques ranging from natural language processing , computer vision , com - puter graphics , to machine learning .
we integrate these com - ponents into a concatenative synthesizer , where the synergy of text unit selection , image parts generation , and layout op - timization produces coherent nal pictures .
for example , we use picturability to inuence word selection , and use word importance to inuence the layout of the picture
we thank the anonymous reviewers for their constructive com - ments .
research supported in part by the wisconsin alumni re - copyright c ( cid : 123 ) 123 , association for the advancement of articial intelligence ( www . aaai . org ) .
all rights reserved .
hay to the
figure 123 : a picture generated by our ttp system .
details , as well as an appropriate evaluation metric , are pre - sented .
user study experiments show that participants de - scriptions of ttp collages contain words that are a closer ( or equivalent ) match to the original text than their descriptions of original illustrations or photos that accompany the text .
ttp has many applications when a text interface is not appropriate .
one important application is literacy develop - ment .
for children who are learning to read and for sec - ond language learners , seeing pictures together with text may enhance learning ( mayer 123 ) .
another application is as a reading aid for people with learning disabilities or brain damage .
ttp can convert textual menus , signs , and safety and operating instructions into graphical representa - tions .
importantly , ttp output can be created on demand by a user and does not depend on a vendor to produce it .
even - tually , a person might carry a pda equipped with ttp and optical character recognition so that the person could gen - erate visual translations as needed during their daily activi - ties .
ttp naturally acts as a universal language when com - munication is needed simultaneously to many people who speak different languages , for example for airport public an - nouncements ( mihalcea & leong 123 ) .
ttp can produce visual summaries for rapidly browsing long text documents .
the current work differs from previous text - to - scene type systems in its focus on conveying the gist of general , unre - stricted text .
previous systems were often meant to be used by graphics designers as an alternative way to specify the layout of a scene .
such text - to - scene systems tend to empha - size spatial reasoning .
examples include nalig ( adorni , manzo , & giunchiglia 123 ) , sprint ( yamada et al .
123 ) , put ( clay & wilhelms 123 ) and , notably , words - eye ( coyne & sproat 123 ) .
wordseye is able to produce highly realistic 123d scenes by utilizing thousands of pre - dened 123d polyhedral object models with detailed manual tags , and deep semantic representations of the text .
conse - quently , wordseye works best with certain descriptive sen - tences , e . g . , the lawn mower is 123 feet tall .
john pushes the lawn mower .
the cat is 123 feet behind john .
the cat is 123 feet tall .
other systems include ( brown & chandrasekaran 123; lu & zhang 123 ) .
carsim ( johansson et al .
123 ) converts special - domain narratives on road accidents into an animated scene using icons .
blissymbols ( hehner 123 ) and other graphic symbol systems create symbol - for - word strings rather than a coherent picture that conveys a global
the text - to - picture system
let the input text be a word sequence w123 : n of length n .
in our concatenative ttp synthesizer , we rst use natural lan - guage processing techniques to select k keyphrases ( impor - tant words or phrases found within w123 : n ) to draw .
then for each selected keyphrase , we use computer vision tech - niques to nd a corresponding image ii .
( we use the word picture to denote the overall composed output , while im - age to denote the individual constituents . ) finally we use computer graphics techniques to spatially arrange all k im - ages to create the output picture .
to integrate these com - ponents together , we formulate the ttp problem as nding the most likely keyphrases k 123 : k , and placement 123 : k given the input text w123 : n :
in our implementation , the placement ci of image i is speci - ed by the center coordinates , but other factors such as scale , rotation and depth can be incorporated too .
to make the optimization problem tractable , we factorize the probability
123 : k ) = argmaxk , i , c p ( k , i , c|w123 : n ) .
123 : k , images i
p ( k , i , c|w ) = p ( k|w ) p ( i|k , w ) p ( c|i , k , w ) ,
and approximate the joint maximizer of eq .
( 123 ) by the max - imizers of each factor in eq .
( 123 ) , as described below .
selecting keyphrases given a piece of text , e . g . , a sentence or a whole book , the rst question is , which keyphrases should be selected to form the picture ? formally , we solve the subproblem
123 : k = argmaxk p ( k|w ) .
our approach is based on extractive picturable keyword summarization .
that is , it builds on standard keyword - based text summarization ( turney 123; mihalcea & tarau 123 ) , where keywords and keyphrases are extracted from the text based on lexicosyntactic cues .
the central issue in key - word summarization is to estimate the importance of lexical units .
we do so using an unsupervised learning approach based on the textrank algorithm ( mihalcea & tarau 123 ) .
textrank denes a graph over candidate words based on co - occurrence in the current text , and uses the stationary distri - bution of a teleporting random walk on the graph as the im - portance measure .
our novelty is that we include a special
teleporting distribution over the words in the graph .
our teleporting distribution is based on picturability , which measures the probability of nding a good image for a word .
our approach thus selects keyphrases that are important to the meaning of the text and are also easy to represent by an the textrank graph following mihalcea and ta - rau ( 123 ) , we dene the textrank graph over individual words .
the ranking of these words will be used later to con - struct the nal set of longer keyphrases .
all nouns , proper nouns , and adjectives ( except those in a stop list ) are selected as candidate words using a part - of - speech tagger .
we then build a co - occurrence graph with each word as a vertex .
we represent this unweighted graph as a co - occurrence matrix , where entry ij is 123 if term i and term j co - occur within a window of size 123
teleporting distribution based on picturability we base each graph vertexs teleporting probability on whether we are likely to nd an image for the corresponding word .
we call this measure picturability and compute it using a logistic regression model .
the picturability logistic re - gression model was trained on a manually - labeled set of 123 words , randomly selected from a large vocabulary .
five an - notators independently labeled the words .
a word is labeled as picturable ( y = 123 ) if an annotator is able to draw or nd a good image of the word .
when shown the image , other peo - ple should be able to guess the word itself or a similar word .
words labeled as non - picturable ( y = 123 ) lack a clearly rec - ognizable associated image ( e . g . , dignity ) .
we represent a word using 123 candidate features , derived from the log - ratios between 123 raw counts .
we obtain the raw counts from various web statistics , such as the num - ber of hits from image and web page search engines ( e . g . , google , yahoo ! , flickr ) in response to a query of the word .
we perform forward feature selection with l123 - regularized logistic regression .
the log - ratio between google image search hit count and google web search hit count domi - nated all other features in terms of cross - validation log like - lihood .
with the practical consideration that a light system should request as few raw web counts as possible , we de - cided to create a model with only this one feature .
itively , number of images vs .
web pages is a good pictura - bility feature that measures image frequency with respect to word frequency .
the resulting picturability logistic regres - sion model is
p ( y = 123|x ) =
123 + exp ( ( 123x + 123 ) )
( c123 + 123 ) / ( c123 + 123 )
where x = log is the log ra - tio between smoothed counts c123 ( google image hits ) and c123 ( google web hits ) , and 123 is a smoothing constant to prevent zero counts .
for example , the word banana has 123 , 123 google image hits and 123 , 123 , 123 web hits .
we nd that p ( y = 123|banana ) = 123 , meaning banana is probably a picturable word .
on the other hand , the word bayesian has 123 , 123 google image hits and 123 , 123 , 123 web hits , so p ( y = 123|bayesian ) = 123 , indicating it is not so picturable .
we use eq .
( 123 ) to compute a picturability value for each candidate word in the textrank graph .
these values are normalized to form the teleporting distribution vector r .
determining the final keyphrases to obtain the rank - ing of words , we compute the stationary distribution of the teleporting random walk
p + ( 123 ) 123r
where p is the graph - based transition matrix ( i . e . , row - normalized co - occurrence matrix ) and r is the teleporting distribution dened above .
this is the same computation used by pagerank .
is an interpolation weight , which we set to 123 , and 123 is an all - ones vector .
the stationary distri - bution indicates the centrality or relative importance of each word in the graph , taking into account picturability .
we se - lect the 123 words with the highest stationary probabilities , and form keyphrases by merging adjacent instances of the selected words ( as long as the resulting phrase has a pic - turability probability greater than 123 ) .
next , we discard phrases lacking nouns , multiple copies of the same phrase , and phrases that are subsumed by other longer phrases .
the end result is a list of keyphrases that appear important and are likely to be representable by an image .
finally , each ex - tracted keyphrase ki is assigned an importance score s ( ki ) , which is equal to the average stationary probability of the words comprising it .
i = argmaxii
selecting images the goal of this stage is to nd one image to represent each extracted keyphrase .
our algorithm handles each keyphrase i ) , i = 123 .
our image selection module combines two sources to nd such an image .
first , we use a manually labeled clipart li - brary .
second , if the keyphrase cannot be found in the li - brary , we use an image search engine and computer vision techniques .
combining the two sources ensures accurate re - sults for common keyphrases , which are likely to exist in the library , and good results for other arbitrary keyphrases .
we focus on the second source below .
image search engines are not perfect , which means many images returned do not visually represent the keyphrase the rst image returned by an im - age search engine is often not a good image to depict the keyphrase .
our approach to selecting the best image from search results , which is similar to the method by ben - haim et al .
( 123 ) , consists of the following steps .
first , the top 123 images for this keyphrase are retrieved using google image search .
next , each image is segmented into a set of disjoint regions using an image segmentation algorithm ( felzen - szwalb & huttenlocher 123 ) .
parameters for the algorithm were set so that , on average , each image is segmented into a small number of segments so that over - segmentation of the object of interest is less likely .
for each region extracted in each image , we next com - pute a feature vector to describe the appearance of that re - gion .
color histograms have been shown to perform well for databases of arbitrary color photographs ( deselaers , key - sers , & ney 123 ) .
we compute a vector of color features
figure 123 : the image selection process on three retrieved im - ages for the word pyramids .
segmentation boundaries are overlaid on the images .
the region closest to the centroid of the largest cluster is indicated by the arrow , and that image is selected as the best for the word .
to describe each region .
specically , the color histogram in luv color space of all pixels in a region is computed .
the l component is then quantized into 123 bins , and the uv pairs of values are quantized into 123 bins , resulting in a feature vector of size 123
the feature vectors in all images are now clustered in fea - ture space .
assuming there are several regions that corre - spond to the keyphrase and their appearances are similar , we expect to nd a compact cluster in feature space .
we use the mean shift clustering algorithm ( comaniciu & meer 123 ) .
assuming that regions corresponding to background parts of an image are not as similar to one another as the regions that correspond to the keyphrase , we treat the largest cluster as the one that is most likely to correspond to the keyphrase .
once the largest cluster is found , we nd the region whose feature vector is closest to the centroid of this cluster .
the image which contains this region is then selected as the best image for this keyphrase .
figure 123 shows an example of the result of this algorithm .
picture layout the third and nal stage takes the text , the keyphrases , and their associated images , and determines a 123d spatial layout of the images , c 123 : k = argmaxc p ( c|w , k , i ) , to create the output picture .
our problem of composing a set of images is similar to the problem of creating picture collages , e . g . , ( wang et al .
however , our goal is to create a layout that helps to convey the meaning of the text by revealing the impor - tant objects and their relations .
since we are interested in handling unrestricted text , we do not assume the availability of semantic knowledge or object recognition components , relying instead on the structure of the text and general lay - out rules that make the picture intuitively readable .
to this end , we rst scale all the images to make them roughly the same size .
to determine the best locations for the images , we dene a good layout to have the following three proper - 123
minimum overlap : overlap between images should be
centrality : important images should be near the center ,
closeness : images corresponding to keyphrases that are
close in the input text should be close in the picture .
finding the best positions for all the images is formulated as an optimization problem to minimize the objective :
where s are weights , o ( ii , ij ) is the area of overlap be - tween pictures ii and ij , atotal is the sum of the areas of all images , s ( ki ) is the importance of keyphrase ki , d ( ii ) is the distance of image ii from the center of the picture , and q ( i , j ) is an indicator function dened as
q ( i , j ) =
if the closeness constraint is violated
the closeness constraint is violated if two keyphrases , ki and kj , are close in the text but their corresponding images , ii and ij , are not touching in the picture .
two keyphrases are said to be close if they are less than 123 words apart and no other keyphrase separates them in the input text .
to solve this highly non - convex optimization problem , we use a monte carlo randomized algorithm to construct multi - ple candidate pictures and then pick the one that minimizes the objective function .
at each step of the algorithm for constructing a candidate picture , one image is selected and its position in the picture is determined .
when all images have been selected , the candidate picture is complete .
the most important image is always placed rst at the center of the picture .
to select the next image to add to the picture , we make a random decision between selecting an image based on importance or based on obeying closeness constraints .
to select an image based on importance , a ran - dom image is selected from the remaining images , where the probability of selecting image ii is s ( kj ) and the sum - mation is over all remaining images ij .
recall that s ( ki ) is image iis associated keyphrase importance .
to choose an image based on closeness constraints , an image is selected , uniformly at random , from the remaining images that are close to one of the images already placed .
a local gradient descent move is used to remove any overlap between im -
the process of creating a candidate picture is repeated a large number of times ( currently 123 ) , and the best picture ( with the lowest objective function ) is selected as the nal result .
branch - and - bound was also implemented so that a partial picture is immediately rejected if the objective func - tion exceeds that of the best picture found so far .
figure 123 shows an example of the picture layout optimization proce -
to assess the systems performance , an evaluation measure was used to gauge the amount of information conveyed by the picture produced .
the user is shown the gener - ated picture alone without the original text , and is asked to write down the meaning of the picture in text .
such user - generated text , u , is automatically compared to the original
figure 123 : the minimum value of the objective function as a function of the number of candidate pictures generated .
at selected points , the best layout found is shown .
closeness constraints were ( a , b ) , ( b , c ) and ( c , d ) .
darker images repre - sent more important keyphrases .
( reference ) the large chocolatecolored horse trotted in the pasture .
( user ) the brown horse runs in the grass .
figure 123 : an example ttp alignment for evaluation .
reference text , r , used to generate the picture .
the assump - tion is that the closer u is to r , the better the ttp system , because the user gets more correct information out of the picture .
this procedure is similar to the game of pictionary .
the key to this measure is an appropriate similarity func - tion to compare u and r .
for example , as shown in figure 123 , assume the ttp system generates a picture from the refer - ence sentence r =the large chocolate - colored horse trot - ted in the pasture , and , during evaluation , the user produces the sentence u =the brown horse runs in the grass .
note several words are different but similar ( i . e . , substitutions ) .
insertions and deletions can occur too .
intuitively , we want two things simultaneously .
on one hand , the user sentence u should have all the words in the reference sentence r ( so important concepts are covered ) .
this can be captured by recall ( r ) .
the standard rouge measure for text summarization ( lin & hovy 123 ) is such a recall - based measure .
on the other hand , the user sentence u should not have too many irrelevant words ( otherwise u can always be the entire vocabulary , which would perfectly cover r ) .
this can be captured by precision ( p ) .
the stan - dard bleu measure for machine translation ( papineni et al .
123 ) is such a precision - based measure .
since both recall and precision are important for evaluat - ing ttp systems , we combine them and compute the stan - dard f - score , f = 123p r / ( p + r ) .
in order to compute precision and recall , we need to ( i ) handle ( near ) synonyms , and ( ii ) dene an alignment between the reference and user
we address the synonym issue by dening a substitution function that takes a pair of words and returns a similarity measure between them .
for example , the substitution func - tion returns 123 if the two words are identical or share the same
stem ( e . g . , run vs .
the function returns a score less than 123 if the two words are synonymous ( e . g . , pasture and grass , mare and horse ) .
several wordnet - based similarity measures exist ( e . g . , ( pedersen , patwardhan , & michelizzi 123 ) ) .
the results reported here use a similarity that decays exponentially ( by a factor of 123 ) as the number of levels be - tween the two words in the wordnet lattice increases .
words more than three levels apart receive a substitution score of 123
using the substitution function , a greedy alignment al - gorithm was dened .
that is , among all reference - user word pairs , the pair with the highest substitution score is picked .
all pairs containing either one of the two words are removed , and the procedure is then repeated until word pairs are exhausted .
in the example in figure 123 , the result of greedy alignment is shown with the assumed substitu - tion score of 123 for identical words and 123 for synonyms .
let a ( w ) be the substitution score attached to word w af - ter alignment , and |u| and |r| be the lengths of u and r , re - spectively .
the soft precision , recall , and f - score , which use substitution and alignment , are p = i=123 a ( ui ) / |r| , f = 123p r / ( p + r ) .
for the example in figure 123 , p = 123 / 123 , r = 123 / 123 , and the nal f - score is 123 .
note that the actual evaluation measure ignores stop words in both sentences .
user studies were conducted to assess the ttp systems per - formance in two scenarios : childrens book illustration and news article visual summarization .
in the rst scenario , ttp was used to produce pictures to represent short texts that originate from single pages of illustrated childrens books .
our hope is that ttp - generated pictures convey as much in - formation content as the original illustrations presented in the childrens book .
in the second scenario , we examine ttps ability to present a visual summary of a news article , which is more descriptive than the original news photograph .
we hope to show that , while the news photograph often lacks enough details for a viewer to determine the main idea in the article , combining the photograph with a ttp - generated composite picture allows the user to understand the gist of childrens book illustration for the rst user study , the ttp system was used to illustrate 123 randomly selected texts from a large pool of childrens books .
these texts range from 123 to 123 words and span one or more sentences .
fig - ure 123 shows the ttp output produced for one example text .
note each text also has an original illustration , so there are 123 pictures in all ( 123 ttp , 123 illustrations ) .
users were asked to write a short text description ( i . e . , the user text ) of each of the 123 pictures , so we can compare whether the ttp picture or the illustration is better at presenting the mean - ing of the original story ( i . e . , reference text ) .
astute users may be able to gure out which illustration and ttp picture present the same story , and thus may have more informa - tion when describing the latter of the pair .
to counteract this phenomenon , we displayed all the ttp - generated pic - tures ( in random order ) before all the illustrations ( in differ -
nose and big she had a
figure 123 : a ttp picture for the above text .
note the monkey image obtained from image search represents , incorrectly , the keyphrase soft eyes .
ent random order ) .
this actually gives the book illustrations an advantage , since users might have remembered ttp pic - tures of the same stories shown before ( and thus are able to mention details not explicitly illustrated ) .
six participants provided 123 short text descriptions each , ranging from a few words to a few sentences .
for example , the responses for the ttp picture in figure 123 were : a girls pet puts its paw on her nose .
the dog walked up to the girl and sniffed her .
the dog bit the girl in her nose and ran away .
the girls nose smelled the dog and monkey as they walked away .
the girl walked her dog and saw a hairy man with a big nose .
the girl monkey nose smells dog paw prints .
note that the actual book illustration shows only a girl sit - ting on a sofa hugging a large dog .
while the responses for that picture ( e . g . , the girl and her giant dog hugged on the couch . ) tend to be accurate descriptions , they also differ greatly from the true text of the story .
post - study , we compared each of the user texts to the cor - responding reference text using the f - score introduced ear - lier .
the scatter plot in figure 123 ( a ) shows the relationship between f - score based on ttp pictures ( x - axis ) and f - score based on original illustrations ( y - axis ) .
each point repre - sents one users score for one of the 123 stories .
just over half ( 123% ) of the points fall below the diagonal .
if we aver - age out individual user differences by combining the points for the same stories , 123% of the aggregate points fall be - low the diagonal ( i . e . , ttp helps recreate the reference text better in 123 of the 123 cases ) .
averaged over all stories and all users , the f - score based on ttp pictures is 123 times the average f - score based on the hand - drawn illustrations , suggesting that the ttp provides users with a picture that conveys almost ( but not quite ) as much information as the texts original illustration .
news article visual summarization in the second study , 123 associated press news articles ( 123 words , plus one photograph ) were randomly selected from different do - mains .
here , the goal is to investigate ttps ability to aug - ment a simple news photo with more information
( a ) childrens books
( b ) news articles
figure 123 : scatter plots comparing ttp pictures ( x - axis ) vs .
childrens book illustrations or news photographs ( y - axis ) .
we rst show each real photograph , followed by the photo - graph next to the ttp - generated picture .
note that in such a long article , there will be many potentially picturable items , but the keyphrase extraction algorithm selects the ones most central to the texts meaning .
to evaluate the difference in information provided by the original and combined pictures , the f - score was computed using the users text and the cor - responding full article text .
given the length of the full text compared to a typical user response , we expect these scores to be low , but we care only about the difference between the picture sources .
eight participants provided 123 user texts each .
figure 123 ( b ) plots f - score using photograph+ttp pic - tures ( x - axis ) versus f - score based on original photographs alone ( y - axis ) , where each point represents a single user on a single article .
123% of the points lie below the diagonal , and if we average over users , 123% of the aggregate points lie below the diagonal .
the overall average f - score based on ttp - augmented pictures is 123 times the average f - score based on the original news photographs alone .
this indi - cates that ttp renders a visual representation that is far su - perior in conveying the news article than its original photo - graph .
this is to be expected , as the photos typically show only a single person or scene , whereas the articles discuss many entities that the ttp pictures capture .
overall , these experiments show that our ttp system conveys as much or more of the content of the text through the generated pictures , than the original illustrations or pho - tos that accompany the text .
we presented a general - purpose text - to - picture synthesis system , built upon a synergy of ai components using natural language processing , computer vision and graphics , and ma - chine learning .
two user studies quantitatively demonstrate the ttp systems ability to generate pictures that convey the gist of input text .
the current work is a rst step towards automatically producing pictures that realistically depict ar - bitrary text .
future work includes incorporating context to produce scenes , performing deeper semantic analysis , and depicting actions with animation .
we plan to investigate several ttp applications , including literacy development for children and rehabilitation for brain - injured patients .
