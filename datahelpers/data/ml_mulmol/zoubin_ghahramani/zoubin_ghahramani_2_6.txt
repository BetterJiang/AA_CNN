variational approximations are becoming a widespread tool for bayesian learning of graphical models .
we provide some theoret - ical results for the variational updates in a very general family of conjugate - exponential graphical models .
we show how the belief propagation and the junction tree algorithms can be used in the inference step of variational bayesian learning .
applying these re - sults to the bayesian analysis of linear - gaussian state - space models we obtain a learning procedure that exploits the kalman smooth - ing propagation , while integrating over all model parameters .
we demonstrate how this can be used to infer the hidden state dimen - sionality of the state - space model in a variety of synthetic problems and one real high - dimensional data set .
bayesian approaches to machine learning have several desirable properties .
bayesian integration does not suer overtting ( since nothing is t to the data ) .
prior knowl - edge can be incorporated naturally and all uncertainty is manipulated in a consis - tent manner .
moreover it is possible to learn model structures and readily compare between model classes .
unfortunately , for most models of interest a full bayesian analysis is computationally intractable .
until recently , approximate approaches to the intractable bayesian learning prob - lem had relied either on markov chain monte carlo ( mcmc ) sampling , the laplace approximation ( gaussian integration ) , or asymptotic penalties like bic .
the recent introduction of variational methods for bayesian learning has resulted in the series of papers showing that these methods can be used to rapidly learn the model struc - ture and approximate the evidence in a wide variety of models .
in this paper we will not motivate advantages of the variational bayesian approach as this is done in previous papers ( 123 , 123 ) .
rather we focus on deriving variational bayesian ( vb ) learn - ing in a very general form , relating it to em , motivating parameter - hidden variable factorisations , and the use of conjugate priors ( section 123 ) .
we then present several theoretical results relating vb learning to the belief propagation and junction tree algorithms for inference in belief networks and markov networks ( section 123 ) .
fi - nally , we show how these results can be applied to learning the dimensionality of the hidden state space of linear dynamical systems ( section 123 ) .
123 variational bayesian learning the basic idea of variational bayesian learning is to simultaneously approximate the intractable joint distribution over both hidden states and parameters with a simpler distribution , usually by assuming the hidden states and parameters are independent; the log evidence is lower bounded by applying jensens inequality twice :
ln p ( y|m ) ( cid : 123 ) d q ( ) ( cid : 123 ) ( cid : 123 ) dx qx ( x ) ln p ( x , y| , m )
+ ln p ( |m )
q ( ) ( cid : 123 ) ( 123 )
= f ( q ( ) , qx ( x ) , y )
where y , x , and m , are observed data , hidden variables , parameters and model class , respectively; p ( |m ) is a parameter prior under model class m .
the lower bound f is iteratively maximised as a functional of the two free distributions , qx ( x ) and q ( ) .
from ( 123 ) we can see that this maximisation is equivalent to minimising the kl divergence between qx ( x ) q ( ) and the joint posterior over hidden states and parameters p ( x , |y , m ) .
this approach was rst proposed for one - hidden layer neural networks ( 123 ) under the restriction that q ( ) is gaussian .
it has since been extended to models with hidden variables and the restrictions on q ( ) and qx ( x ) have been removed in certain models to allow arbitrary distributions ( 123 , 123 , 123 , 123 , 123 ) .
free - form optimisation with respect to the distributions q ( ) and qx ( x ) is done using calculus of variations , often resulting in algorithms that appear closely related to the corresponding em algorithm .
we formalise this relationship and others in the following sections .
123 conjugate - exponential models we consider variational bayesian learning in models that satisfy two conditions : condition ( 123 ) .
the complete data likelihood is in the exponential family :
p ( x , y| ) = f ( x , y ) g ( ) exp ( cid : 123 ) ( ) ( cid : 123 )
where ( ) is the vector of natural parameters , and u and f and g are the functions that dene the exponential family .
the list of latent - variable models of practical interest with complete - data likeli - hoods in the exponential family is very long .
we mention a few : gaussian mixtures , factor analysis , hidden markov models and extensions , switching state - space mod - els , boltzmann machines , and discrete - variable belief networks . 123 of course , there are also many as yet undreamed - of models combining gaussian , gamma , poisson , dirichlet , wishart , multinomial , and other distributions .
condition ( 123 ) .
the parameter prior is conjugate to the complete data likelihood :
p ( | , ) = h ( , ) g ( ) exp ( cid : 123 ) ( ) ( cid : 123 )
where and are hyperparameters of the prior .
condition ( 123 ) in fact usually implies condition ( 123 ) .
apart from some irregular cases , it has been shown that the exponential families are the only classes of distributions with a xed number of sucient statistics , hence allowing them to have natural conjugate priors .
from the denition of conjugacy it is easy to see that the hyper - parameters of a conjugate prior can be interpreted as the number ( ) and values ( ) of pseudo - observations under the corresponding likelihood .
we call models that satisfy conditions ( 123 ) and ( 123 ) conjugate - exponential .
123models whose complete - data likelihood is not in the exponential family ( such as ica with the logistic nonlinearity , or sigmoid belief networks ) can often be approximated by models in the exponential family with additional hidden variables .
in bayesian inference we want to determine the posterior over parameters and hidden variables p ( x , |y , , ) .
in general this posterior is neither conjugate nor in the exponential family .
we therefore approximate the true posterior by the following factorised distribution : p ( x , |y , , ) q ( x , ) = qx ( x ) q ( ) , and minimise
kl ( q ( cid : 123 ) p ) = ( cid : 123 ) dx d q ( x , ) ln
p ( x , |y , , )
which is equivalent to maximising f ( qx ( x ) , q ( ) , y ) .
we provide several general results with no proof ( the proofs follow from the denitions and gibbs inequality ) .
theorem 123 given an iid data set y = ( y123 , .
yn ) , if the model satises conditions ( 123 ) and ( 123 ) , then at the maxima of f ( q , y ) ( minima of kl ( q ( cid : 123 ) p ) ) :
q ( ) = h ( , ) g ( ) exp ( cid : 123 ) ( ) ( cid : 123 ) ( cid : 123 )
i=123 u ( yi ) , and u ( yi ) = ( cid : 123 ) u ( xi , yi ) ( cid : 123 ) q , using ( cid : 123 ) ( cid : 123 ) q
( a ) q ( ) is conjugate and of the form :
to denote expectation under q .
where = + n , = + ( cid : 123 ) n ( b ) qx ( x ) = ( cid : 123 ) n where ( ) = ( cid : 123 ) ( ) ( cid : 123 ) q .
qxi ( xi ) f ( xi , yi ) exp ( cid : 123 ) ( ) ( cid : 123 )
i=123 qxi ( xi ) and qxi ( xi ) is of the same form as the known pa -
u ( xi , yi ) ( cid : 123 ) = p ( xi|yi , ( ) )
since q ( ) and qxi ( xi ) are coupled , ( a ) and ( b ) do not provide an analytic so - lution to the minimisation problem .
we therefore solve the optimisation problem numerically by iterating between the xed point equations given by ( a ) and ( b ) , and we obtain the following variational bayesian generalisation of the em algorithm :
ve step : compute the expected sucient statistics t ( y ) = ( cid : 123 ) i u ( yi )
under the hidden variable distributions qxi ( xi ) .
vm step : compute the expected natural parameters ( ) under the parameter distribution given by and .
this reduces to the em algorithm if we restrict the parameter density to a point estimate ( i . e .
dirac delta function ) , q ( ) = ( ) , in which case the m step note that unless we make the assumption that the parameters and hidden variables factorise , we will not generally obtain the further hidden variable factorisation over n in ( b ) .
in that case , the distributions of xi and xj will be coupled for all cases i , j in the data set , greatly increasing the overall computational complexity of inference .
123 belief networks and markov networks the above result can be used to derive variational bayesian learning algorithms for exponential family distributions that fall into two important special classes . 123 let m be a corollary 123 : conjugate - exponential belief networks .
conjugate - exponential model with hidden and visible variables z = ( x , y ) that sat - p ( z| ) = ( cid : 123 ) j p ( zj|zpj , ) .
then the approximating joint distribution for m satis - isfy a belief network factorisation .
that is , each variable zj has parents zpj and
es the same belief network factorisation :
q ( zj|zpj , )
123a tutorial on belief networks and markov networks can be found in ( 123 ) .
where the conditional distributions have exactly the same form as those in the original model but with natural parameters ( ) = ( ) .
furthermore , with the modied parameters , the expectations under the approximating posterior qx ( x ) qz ( z ) required for the ve step can be obtained by applying the belief propagation algorithm if the network is singly connected and the junction tree algorithm if the network is multiply - connected .
this result is somewhat surprising as it shows that it is possible to infer the hidden states tractably while integrating over an ensemble of model parameters .
this result generalises the derivation of variational learning for hmms in ( 123 ) , which uses the forward - backward algorithm as a subroutine .
theorem 123 : markov networks .
let m be a model with hidden and visible vari - sity can be written as a product of clique - potentials j , p ( z| ) = g ( ) ( cid : 123 ) j j ( cj , ) , ables z = ( x , y ) that satisfy a markov network factorisation .
that is , the joint den - where each clique cj is a subset of the variables in z .
then the approximating joint distribution for m satises the same markov network factorisation :
qz ( z ) = g ( cid : 123 )
where j ( cj ) = exp ( ( cid : 123 ) ln j ( cj , ) ( cid : 123 ) q ) are new clique potentials obtained by averag - ing over q ( ) , and g is a normalisation constant .
furthermore , the expectations under the approximating posterior qx ( x ) required for the ve step can be obtained by applying the junction tree algorithm .
corollary 123 : conjugate - exponential markov networks .
let m be a imating joint distribution for m is given by qz ( z ) = g ( cid : 123 ) j j ( cj , ) , where the conjugate - exponential markov network over the variables in z .
then the approx -
clique potentials have exactly the same form as those in the original model but with natural parameters ( ) = ( ) .
for conjugate - exponential models in which belief propagation and the junction tree algorithm over hidden variables is intractable further applications of jensens in - equality can yield tractable factorisations in the usual way ( 123 ) .
in the following section we derive a variational bayesian treatment of linear - gaussian state - space models .
this serves two purposes .
first , it will illustrate an application of theorem 123
second , linear - gaussian state - space models are the cornerstone of stochastic ltering , prediction and control .
a variational bayesian treatment of these models provides a novel way to learn their structure , i . e .
identify the optimal dimensionality of their state - space .
123 state - space models in state - space models ( ssms ) , a sequence of d - dimensional real - valued observation vectors ( y123 , .
, yt ) , denoted y123 : t , is modeled by assuming that at each time step t , yt was generated from a k - dimensional real - valued hidden state variable xt , and that the sequence of xs dene a rst - order markov process .
the joint probability of a sequence of states and observations is therefore given by ( figure 123 ) :
p ( x123 : t , y123 : t ) = p ( x123 ) p ( y123|x123 )
p ( xt|xt123 ) p ( yt|xt ) .
we focus on the case where both the transition and output functions are linear and time - invariant and the distribution of the state and observation noise variables is gaussian .
this model is the linear - gaussian state - space model :
xt = axt123 + wt ,
yt = cxt + vt
figure 123 : belief network representation of a state - space model .
where a and c are the state transition and emission matrices and wt and vt are state and output noise .
it is straightforward to generalise this to a linear system driven by some observed inputs , ut .
a bayesian analysis of state - space models using mcmc methods can be found in ( 123 ) .
the complete data likelihood for state - space models is gaussian , which falls within the class of exponential family distributions .
in order to derive a variational bayesian algorithm by applying the results in the previous section we now turn to dening conjugate priors over the parameters .
priors .
without loss of generality we can assume that wt has covariance equal to the unit matrix .
the remaining parameters of a linear - gaussian state - space model are the matrices a and c and the covariance matrix of the output noise , vt , which we will call r and assume to be diagonal , r = diag ( ) 123 , where i are the precisions ( inverse variances ) associated with each output .
each row vector of the a matrix , denoted a ( cid : 123 ) i , is given a zero mean gaussian prior with inverse covariance matrix equal to diag ( ) .
each row vector of c , c ( cid : 123 ) i , is given a zero - mean gaussian prior with precision matrix equal to diag ( i ) .
the dependence of the precision of c ( cid : 123 ) i on the noise output precision i is motivated by conjugacy .
intuitively , this prior links the scale of the signal and noise .
the prior over the output noise covariance matrix , r , is dened through the pre - cision vector , , which for conjugacy is assumed to be gamma distributed123 with exp ( bi ) .
here , , are hyperparameters that we can optimise to do automatic relevance determination ( ard ) of hidden states , thus inferring the structure of the ssm .
hyperparameters a and b : p ( |a , b ) = ( cid : 123 ) d
variational bayesian learning for ssms since a , c , and x123 : t are all unknown , given a sequence of observations y123 : t , an exact bayesian treatment of ssms would require computing marginals of the poste - rior p ( a , c , , x123 : t|y123 : t ) .
this posterior contains interaction terms up to fth order ( for example , between elements of c , x and ) , and is not analytically manageable .
however , since the model is conjugate - exponential we can apply theorem 123 to de - rive a variational em algorithm for state - space models analogous to the maximum - likelihood em algorithm ( 123 ) .
moreover , since ssms are singly connected belief networks corollary 123 tells us that we can make use of belief propagation , which in the case of ssms is known as the kalman smoother .
writing out the expression for log p ( a , c , , x123 : t , y123 : t ) , one sees that it contains interaction terms between and c , but none between a and either or c .
this observation implies a further factorisation , q ( a , c , ) = q ( a ) q ( c , ) , which falls out of the initial factorisation and the conditional independencies of the model .
starting from some arbitrary distribution over the hidden variables , the vm step obtained by applying theorem 123 computes the expected natural parameters of q ( ) , where = ( a , c , ) .
123more generally , if we let r be a full covariance matrix for conjugacy we would give
123 a wishart distribution : p ( v | , s ) |v | ( d123 ) / 123 exp ( cid : 123 ) 123
its inverse v = r where tr is the matrix trace operator .
123 tr v s
( cid : 123 ) and w
123 , bi = b + 123
i , ui = ( cid : 123 ) t
we proceed to solve for q ( a ) .
we know from theorem 123 that q ( a ) is multivariate gaussian , like the prior , so we only need to compute its mean and covariance .
a ( cid : 123 ) ( diag ( ) + w ) 123 and each row of a has covariance ( diag ( ) + w ) 123 , has mean s
( cid : 123 ) = w + ( cid : 123 ) xt x ( cid : 123 )
t ( cid : 123 ) , w = ( cid : 123 ) t123 t ( cid : 123 ) , and ( cid : 123 ) . ( cid : 123 ) denotes averaging w . r . t .
where s = ( cid : 123 ) t the q ( x123 : t ) distribution .
123 gi , gi = ( cid : 123 ) t q ( c , ) is also of the same form as the prior .
q ( ) is a product of gamma densities q ( i ) = g ( i; a , bi ) where a = a + t t ( cid : 123 ) .
given , each row of ( cid : 123 ) ) 123 / i and mean ci = c is gaussian with covariance cov ( ci ) = ( diag ( ) + w i ui cov ( ci ) .
note that s , w and ui are the expected complete data sucient statistics u mentioned in theorem 123 ( a ) .
using the parameter distributions the hyperparameters can also be optimised . 123 we now turn to the ve step : computing q ( x123 : t ) .
since the model is a conjugate - exponential singly - connected belief network , we can use belief propagation ( corol - lary 123 ) .
for ssms this corresponds to the kalman smoothing algorithm , where every appearance of the natural parameters of the model is replaced with the fol - lowing corresponding expectations under the q distribution : ( cid : 123 ) ici ( cid : 123 ) , ( cid : 123 ) icic ( cid : 123 ) like for pca ( 123 ) , independent components analysis ( 123 ) , and mixtures of factor analysers ( 123 ) , the variational bayesian algorithm for state - space models can be used to learn the structure of the model as well as average over parameters .
specically , using f it is possible to compare models with dierent state - space sizes and optimise the dimensionality of the state - space , as we demonstrate in the following section .
a ( cid : 123 ) .
details can be found in ( 123 ) .
experiment 123 : the goal of this experiment was to see if the variational method could infer the structure of a variety of state space models by optimising over and .
we generated a 123 - step time series of 123 - dimensional data from three models : 123 ( a ) a factor analyser ( i . e .
an ssm with a = 123 ) with 123 factors ( static state variables ) ; ( b ) an ssm with 123 dynamical interacting state variables , i . e .
a ( cid : 123 ) = 123; ( c ) an ssm with 123 interacting dynamical and 123 static state variables .
the variational bayesian method correctly inferred the structure of each model in 123 - 123 minutes of cpu time on a 123 mhz pentium iii ( fig .
123 ( a ) ( c ) ) .
experiment 123 : we explored the eect of data set size on complexity of the recov - ered structure .
123 - dim time series were generated from a 123 state - variable ssm .
on reducing the length of the time series from 123 to 123 steps the recovered structure became progressively less complex ( fig .
123 ( d ) ( j ) ) , to a 123 - variable static model ( j ) .
this result agrees with the bayesian perspective that the complexity of the model should reect the data support .
experiment 123 ( steel plant ) : 123 sensors ( temperatures , pressures , etc ) were sampled at 123 hz from a continuous casting process for 123 seconds .
these sensors covaried and were temporally correlated , suggesting a state - space model could cap - ture some of its structure .
the variational algorithm inferred that 123 state variables were required , of which 123 emitted outputs .
while we do not know whether this is reasonable structure we plan to explore this as well as other real data sets .
123 the ard hyperparameters become k =
, and k =
hyperparameters a and b solve the xed point equations ( a ) = ln b + 123 b = 123 unif ( 123 , 123 ) , and a chosen with eigen - values in ( 123 , 123 ) .
w ln ( w ) is the digamma function .
( cid : 123 ) i ( cid : 123 ) , where ( w ) =
123parameters were chosen as follows : r = i , and elements of c sampled from
( cid : 123 ) ln i ( cid : 123 ) , and
figure 123 : the elements of the a and c matrices after learning are displayed graphically .
a link is drawn from node k in xt123 to node l in xt i 123 > , for a small threshold .
similarly links are drawn from node k of xt to yt if 123 therefore the graph shows the links that take part in the dynamics and the output .
> , and either 123
we have derived a general variational bayesian learning algorithm for models in the conjugate - exponential family .
there are a large number of interesting models that fall in this family , and the results in this paper should allow an almost automated protocol for implementing a variational bayesian treatment of these models .
we have given one example of such an implementation , state - space models , and shown that the vb algorithm can be used to rapidly infer the hidden state dimen - sionality .
using the theory laid out in this paper it is straightforward to generalise the algorithm to mixtures of ssms , switching ssms , etc .
for conjugate - exponential models , integrating both belief propagation and the junc - tion tree algorithm into the variational bayesian framework simply amounts to com - puting expectations of the natural parameters .
moreover , the variational bayesian algorithm contains em as a special case .
we believe this paper provides the founda - tions for a general algorithm for variational bayesian learning in graphical models .
