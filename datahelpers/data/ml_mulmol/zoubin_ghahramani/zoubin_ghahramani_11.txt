we investigate the use of unlabeled data to help labeled data in classi - cation .
we propose a simple iterative algorithm , label propagation , to propagate labels through the dataset along high density areas dened by unlabeled data .
we analyze the algorithm , show its solution , and its con - nection to several other algorithms .
we also show how to learn parame - ters by minimum spanning tree heuristic and entropy minimization , and the algorithms ability to perform feature selection .
experiment results
labeled data are essential for supervised learning .
however they are often available only in small quantities , while unlabeled data may be abundant .
using unlabeled data together with labeled data is of both theoretical and practical interest .
recently many approaches have been proposed for combining unlabeled and labeled data ( 123 ) ( 123 ) .
among them there is a promising family of methods which assume that closer data points tend to have similar class labels in a manner analogous to k - nearest - neighbor ( knn ) in traditional supervised learning .
as a result , these methods propagate labels through dense unlabeled data regions .
we propose a new algorithm to propagate labels .
we formulate the problem as a particular form of label propagation , where a nodes labels propagate to all nodes according to their proximity .
meanwhile we x the labels on the labeled data .
thus labeled data act like sources that push out labels through unlabeled data .
we prove the convergence of the algorithm , nd a closed form solution for the xed point , and analyze its behavior on several datasets .
we also propose a minimum spanning tree heuristic and an entropy minimization criterion to learn the parameters , and show our algorithm learns to detect irrelevant features .
letx123;y123 : : : x;y be labeled data , wherey=fy123 : : : yg123f123 : : : cg are the class labels .
we assume the number of classesc is known , and all classes are present in the labeled data .
letx123;y123 : : : x ;y be unlabeled data whereyu=
123 label propagation
123 problem setup
for example , positive or discrete .
we have chosen to focus on euclidean distance in this
all nodes have soft labels which can be interpreted as distributions over labels .
we let the labels of a node propagate to all nodes through the edges .
larger edge weights allow labels
intuitively , we want data points that are close to have similar labels .
we create a fully connected graph where the nodes are all data points , both labeled and unlabeled
fy123 : : : y g are unobserved; usually ( cid : 123 ) .
letx=fx123 : : : x g123rd problem is to estimateyu fromx andy .
edge between any nodesi;j is weighted so that the closer the nodes are in local euclidean distance , dij , the larger the weightwij .
the weights are controlled by a parameter ( cid : 123 ) : wij=ex d123ij ( cid : 123 ) 123 ! =ex dd=123xdixdj123 other choices of distance metric are possible , and may be more appropriate if thex are , paper , later allowing different ( cid : 123 ) s for each dimension , corresponding to length scales for to travel through more easily .
dene a probabilistic transition matrixt wheretij is the probability of jumping from nodej toi .
also dene a c label matrixy , whoseith row representing the label probabilities of nodeyi .
the initialization of the rows ofy corresponding to unlabeled data points is not important .
we are now ready 123
all nodes propagate labels for one step : y ty 123
row - normalizey to maintain the class probability interpretation .
clamp the labeled data .
repeat from step 123 untily converges .
, so the probability mass is concentrated on the given class .
y ( cid : 123 ) ty with ( cid : 123 ) t being the row - normalized matrix oft , i . e . ( cid : 123 ) tij=tij=ktik .
lety be the top rows ofy ( the labeled data ) andyu the remaining rows .
notice thaty never really changes since it is clamped in step 123 , and we are solely interested inyu .
we split ( cid : 123 ) t after the - th row and the - th column into 123 sub - matrices it can be shown that our algorithm isyu ( cid : 123 ) t yu ( cid : 123 ) t y , which leads toyu= ( cid : 123 ) t y , wherey123 is the initialy .
we need to show
( cid : 123 ) t y123 ! 123
by construction , all elements in ( cid : 123 ) t are greater than zero .
since ( cid : 123 ) t is row normalized , and ( cid : 123 ) t is a sub - matrix of ( cid : 123 ) t , it follows that123 ( cid : 123 ) <123; such that ( cid : 123 ) ;123i=123 : : : .
therefore
the intuition is that with this constant push from labeled nodes , the class boundaries will be pushed through high density data regions and settle in low density gaps .
if this structure of data ts the classication goal , our algorithm can use unlabeled data to help learning .
the algorithm converges to a simple solution .
first , step 123 and 123 can be combined into
to present the algorithm .
123 the algorithm
the label propagation algorithm is as follows :
step 123 is critical : instead of letting the labeled data points fade away , we clamp their class
123 parameter setting
123 rebalancing class proportions
is inconsequential .
obviously
is a xed point .
therefore it is the unique xed point and the solution to our iterative
no node is connected .
during tree growth , the edges are examined one by one from short to long .
an edge is added to the tree if it connects two separate components .
the process repeats until the whole graph is connected .
we nd the rst tree edge that connects two
of this ( and longer ) edge is close to 123 , with the hope that local propagation is then mostly
so the row sums of ( cid : 123 ) t converge to zero , which means ( cid : 123 ) t y123 ! 123
thus the initial pointy123 yu= ( cid : 123 ) t 123 ( cid : 123 ) t y we set the parameter ( cid : 123 ) with a heuristic .
we nd a minimum spanning tree ( mst ) over all data points under euclidean distancesdij , with kruskals algorithm ( 123 ) .
in the beginning components with different labeled points in them .
we regard the length of this edged123 a heuristic of the minimum distance between classes .
we set ( cid : 123 ) =d123=123 so that the weight within classes .
later we will propose an entropy - based criterion to learn the ( cid : 123 ) parameters .
for classication purposes , onceyu is computed , we can take the most likely ( ml ) class of
=123 ) are either estimated from labeled data or known a priori ( i . e .
from ( cid : 123 ) class mass normalization : find coefcients ( cid : 123 ) to scale columns ofyu s . t .
( cid : 123 ) 123yu : 123 : : : : : ( cid : 123 ) cyu : c=123 : : : : : c .
once decisions are made for ( cid : 123 ) label bidding : we have labels for sale .
each pointi bids $yui .
bids are processed from high to low .
assumeyui is currently the highest bid .
if class labels remain , a label is sold to pointi , who then quits the bidding .
bandsc=123;=123; =123; ( cid : 123 ) =123 : 123; for springsc=123;=123; =123; ( cid : 123 ) =123 : 123
both ( cid : 123 ) s are from the mst heuristic .
simple ml classication is used .
here , obviously reduce the size of each image down to123 by down - sampling and gaussian smoothing ,
each unlabeled point as its label .
however , this procedure does not provide any control over the nal proportions of the classes , which are implicitly determined by the distribution of data .
if classes are not well separated and labeled data is scarce , incorporating constraints on class proportions can improve nal classication .
we assume the class proportions
to demonstrate properties of this algorithm we investigated both synthetic datasets and a real - world classication problem .
figure 123 shows label propagation on two synthetic datasets .
large symbols are labeled data , other points are originally unlabeled .
for 123 -
knn would fail to follow the structure of data .
for a real world example we test label propagation on a handwritten digits dataset , origi - nally from the cedar buffalo binary digits database ( 123 ) .
the digits were preprocessed to
with pixel values ranging from 123 to 123 ( 123 ) .
we use digits 123 , 123 and 123 in our experi - ment as three classes , with 123 images in each class .
each image is represented by a 123
otherwise the bid is ignored and the second highest bid is processed , and so on .
label bidding guarantees that strict class proportions will be met .
123 experimental results
an oracle ) .
we propose two post - processing alternatives to ml class assignment :
each point , this procedure does not guarantee that class proportion will be exactly
that it can work very well; in this section we propose a criterion for learning the model parameters that can be applied in more general settings .
data label likelihood does not
figure 123 : label propagation on two synthetic datasets .
dimensional vector .
figure 123 ( a ) shows a random sample of 123 images from the dataset .
( 123 ) lbo : label bidding post processing , with oracle class proportions .
we use two algo -
trial we randomly sample labeled data from the whole dataset , and use the rest of images as unlabeled data .
if any class is absent from the sampled labeled set , we redo the sampling .
( see section 123 ) : ( 123 ) ml : the most likely labels; ( 123 ) cne : class mass normalization , with maximum likelihood estimate of class proportions from labeled data; ( 123 ) lbe : label bid - ding , with maximum likelihood estimate of class proportions from labeled data; ( 123 ) cno :
we vary labeled data size from 123 up to 123
for each size , we perform 123 trials .
in each thus labeled and unlabeled data are approximatelyiid .
we nd ( cid : 123 ) by the mst heuristic ( all trials have ( cid : 123 ) close to 123 ) .
to speed up computation , only the top 123 neighbors of each image are considered to maket sparse .
we measure 123 error rates on unlabeled data class mass normalization , with knowledge of the oracle ( true ) class proportions ( i . e . 123=123 ) ; rithms as baselines .
the rst one is standardknn .
we report 123nn results since it is the best amongk=123 : : : 123
the second baseline algorithm is propagating 123nn ( p123nn ) : among all unlabeled data , nd the pointx closest to a labeled point ( call itx ) .
label x withxs label , addx to the labeled set , and repeat .
p123nn is a crude version of label figure 123 ( b ) ( f ) shows the results .
ml labeling is better than 123nn when ( cid : 123 ) 123 ( b ) .
but performance when is small ( c ) .
if we know the true class proportion , the performance is lbo .
each entry is averaged over 123 trials .
all differences are statistically signicant at ( cid : 123 ) level 123 ( test ) , except for the pairs in thin face .
when ( cid : 123 ) ! 123 , the label propagation result approaches p123nn , because under the exponential weights ( 123 ) the inuence of the nearest point dominates .
when ( cid : 123 ) ! 123 , the whole dataset all labeled points , resulting in equal class probabilities .
the appropriate ( cid : 123 ) is in between .
we used mst as a heuristic to set the parameter ( cid : 123 ) and have shown in the previous section
even better ( e , f ) , with label bidding being slightly superior to class mass normalization .
on the other hand since label bidding requires exact proportions , its performance is bad when the class proportions are estimated ( d ) .
to summarize , label bidding is the best when exact proportions are known , otherwise class mass normalization is the best .
p123nn consistently performs no better than 123nn .
table 123 lists the error rates for p123nn , 123nn , ml , cne and
if we rebalance class proportions , we can do much better .
if we use class frequency of labeled data as class proportions and perform class mass normalization , we improve the
propagation .
it performs well on the two synthetic datasets , with the same results as in
effectively shrinks to a single point .
all unlabeled points receive the same inuence from
123 parameter learning by entropy minimization
( a ) a sample
figure 123 : the digits dataset .
each point is an average of 123 random trials .
the error bars
make sense as a criterion in our setting , especially with very few labeled points , since intuitively the quality of a solution depends on how unlabeled data are assigned labels .
points condently .
there are many arbitrary labelings of the unlabeled data that have low entropy , which might suggest that this criterion would not work .
however , it is important to point out that most of these arbitrary low entropy labelings cannot be achieved by prop - agating labels using our algorithm .
in fact , we nd that the space of low entropy labelings
are123 standard deviation .
we propose to minimize the entropy=ijyijgyij , which is the sum of the en - tropy on individual data points .
this captures the intuition that good ( cid : 123 ) should label all achievable by label propagation is small and lends itself well to tuning the ( cid : 123 ) parameters .
one complication remains , which is that has a minimum 123 at ( cid : 123 ) ! 123 ( notice p123nn gives
table 123 : digits : error rate of different post processing methods .
each point a hard label ) , but this ( p123nn ) is not always desirable .
figure 123 ( a , b ) shows the problem on the bridge dataset , where the upper grid is slightly tighter than the lower grid .
this can be xed by smoothingt .
inspired by the analysis on the pagerank algorithm ( 123 ) , we smootht with a uniform transition matrixu , whereuij=123= ;123i;j : ~ t is then used in place oft in the algorithm .
figure 123 ( c ) shows vs . ( cid : 123 ) before and after smoothing on the bridge dataset , with different ( cid : 123 ) values .
smoothing helps to get rid of the nuisance minimum at ( cid : 123 ) ! 123
in the following we use the value ( cid : 123 ) =123 : 123
although we have to add one more parameter ( cid : 123 ) to learn ( cid : 123 ) , the advantage is apparent when ( a ) p123nn or ( cid : 123 ) ! 123 figure 123 : the bridge dataset .
p123nn ( or ( cid : 123 ) ! 123 ) result is undesirable .
smoothingt helps to remove the minimum of at ( cid : 123 ) ! 123
we introduce multiple parameters ( cid : 123 ) 123 : : : ( cid : 123 ) d , one for each dimension .
now the weights are wij=exdd=123xdixdj123= ( cid : 123 ) 123d .
the ( cid : 123 ) ds are analogous to the relevance or length scales in gaussian process .
we use gradient descent to nd the parameters ( cid : 123 ) 123 : : : ( cid : 123 ) d that minimizes .
readers are referred to ( 123 ) for a derivation of= ( cid : 123 ) d .
the learned single ( cid : 123 ) is 123 and 123 for the 123 - bands and springs datasets respectively , very close to the mst heuristic .
classication remains the same .
with multiple ( cid : 123 ) ds , our algorithm can detect irrelevant dimensions .
for the bridge dataset ( cid : 123 ) 123 keeps increasing while ( cid : 123 ) 123 and asymptote during learning , meaning the algorithm thinks the horizontal dimension ( corresponding to ( cid : 123 ) 123 ) is irrelevant to classication ( large ( cid : 123 ) d allows labels to 123 - dimensional unit hypersphere with gaps ( figure 123 ) .
there are two123 are ( cid : 123 ) 123=123 : 123 , ( cid : 123 ) 123=123 : 123 , ( cid : 123 ) 123=123 : 123 , and ( cid : 123 ) 123=123 : 123 , i . e .
to compute the - step ancestorhood of any nodei , that is , given that the random walk is at nodei , what is the probability that it was at some nodej at steps before .
to understand
freely propagate along that dimension ) .
classication is the same as figure 123 ( b ) .
let us look at another synthetic dataset , ball , with 123 data points uniformly sampled within a gaps when the dataset is projected onto dimensions 123 - 123 and 123 - 123 respectively , but no gap in dimensions 123 - 123 , 123 - 123 or 123 - 123
the gap in dimensions 123 - 123 is related to classication while the one in 123 - 123 is not .
but this information is only hinted by the 123 labeled points .
the learned parameters the algorithm learns that dimensions 123 , 123 are irrelevant to classication , even though the data are clustered along those dimensions .
classication follows the gap in dimensions 123 , 123
the proposed label propagation algorithm is closely related to the markov random walks algorithm ( 123 ) .
both utilize the manifold structure dened by large amount of unlabeled data , and assume the structure is correlated to the goal of classication .
both dene a probabilistic process for labels to transit between nodes .
but the markov random walks al - gorithm approaches the problem from a different perspective .
it uses the transition process
123 related work
the algorithm , it is helpful to imagine that each node has two separate sets of labels , one
figure 123 : the ball dataset .
the algorithm learns that dimensions 123 , 123 are irrelevant .
labels , weighted by their ancestorhood .
this is in fact kernel regression , with the kernel
will be the same .
in our algorithm , labeled data are constant sources that push out labels ,
margin of the observed labels are optimized .
the algorithm is sensitive to the time scale
there seems to be a resemblance between label propagation and mean eld approximation ( 123 ) ( 123 ) .
in label propagation , upon convergence we have the equations ( for unlabeled data )
hidden and one observable .
a nodeis observable label is the average of all nodes hidden being the - step ancestorhood .
the hidden labels are learned such that the likelihood or , since when ! 123 every node looks equally like an ancestor , and all observable labels and the system achieves equilibrium when ! 123
consider the labeled / unlabeled data graph as a conditional markov random eldf with pairwise interactionwij between nodesi;j , and with labeled nodes clamped .
each un - clamped ( unlabeled ) nodei inf can be in one ofc states , denoted by a vector also called yi=yi;123; : : : ;yi;c .
the probability of a particular congurationy inf is fy=123zex ( ijwijyiyj> .
we now show label propagation ( 123 ) is approximately a mean eld solution to a markov random eldf123 asf123y=123zex ( gijwijyiyj> , which is the same asf up to the rst order : f123y ( cid : 123 ) 123zex ( ijwijyiyj>123=fy .
the mean eld solution tof123
wherehi denotes the mean .
equation ( 123 ) is an approximation to ( 123 ) in the sense that if we kwik are the same for alli , we can replacetij withwij in ( 123 ) .
therefore we nd that label propagation is approximately the mean eld approximation tof .
markov random eldf , since the minimum cut corresponds to minimum energy .
there is random eldf to learn from labeled and unlabeled data , optimizing the length scale pa -
a subtle difference : assume the middle band in figure 123 ( a ) has no labeled point .
mincut will classify the middle band as either all o or all + , since these are the two most likely state congurations ( 123 ) .
but label propagation , being more in the spirit of a mean eld approximation , splits the middle band , classifying points in the upper half as o and lower half as + ( with low condence though ) .
in addition , label propagation is not limited to
in related work , we have also attempted to use boltzmann machine learning on the markov
the graph mincut algorithm ( 123 ) nds the most likely state conguration of the same
rameters using the likelihood criterion on the labeled points ( 123 ) .
we proposed a label propagation algorithm to learn from both labeled and unlabeled data .
labels were propagated with a combination of random walk and clamping .
we showed the solution to the process , and its connection to other methods .
we also showed how to learn the parameters .
as with various semi - supervised learning algorithms of its kind , label prop - agation works only if the structure of the data distribution , revealed by abundant unlabeled data , ts the classication goal .
in the future we will investigate better ways to rebalance class proportions , applications of the entropy minimization criterion to learn propagation parameters from real datasets , and possible connections to the diffusion kernel ( 123 ) .
we thank sam roweis , roni rosenfeld , teddy seidenfeld , guy lebanon , jin rong and jing liu for helpful discussions .
sam roweis provided the handwritten digits dataset .
the rst author is supported in part by a microsoft research graduate fellowship .
