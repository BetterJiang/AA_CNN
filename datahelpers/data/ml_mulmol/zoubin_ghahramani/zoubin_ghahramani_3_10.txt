real - world learning tasks often involve high - dimensional data sets with complex patterns of missing in this paper we review the problem of learning from incomplete data from two statistical perspectives|the likelihood - based and the bayesian .
the goal is two - fold : to place current neural net - work approaches to missing data within a statistical framework , and to describe a set of algorithms , derived from the likelihood - based framework , that handle clustering , classi ( cid : 123 ) cation , and function approximation from incomplete data in a principled and e ( cid : 123 ) cient manner .
these algorithms are based on mixture mod - eling and make two distinct appeals to the expectation - maximization ( em ) principle ( dempster et al . , ) |both for the estimation of mixture components and for coping with the missing data .
copyright c ( cid : 123 ) massachusetts institute of technology ,
this report describes research done at the center for biological and computational learning and the arti ( cid : 123 ) cial intelligence laboratory of the massachusetts institute of technology .
support for the center is provided in part by a grant from the national science foundation under contract asc ( .
support for the laboratorys arti ( cid : 123 ) cial intelligence research is provided in part by the advanced research projects agency of the department of defense .
the authors were supported in part by a grant from atr auditory and visual perception research laboratories , by a grant from siemens corporation , by grant iri - from the national science foundation , and by grant n - - j - from the o ( cid : 123 ) ce of naval research .
zoubin ghahramani was supported by a grant from the mcdonnell - pew foundation .
michael i .
jordan is a nsf presidential
in computational and biological learning the environ - ment does not often provide complete information to the learner .
for example , a vision system may encounter many partially occluded examples of an object , yet have to recover a model for the unoccluded object .
similarly , an adaptive controller may be required to learn a map - ping from sensor readings to actions even if its sensors are unreliable and sometimes fail to give readings .
ex - amples of data sets with missing values abound in ma -
in this paper we review the problem of learning from incomplete data from a statistical perspective .
the goal is two - fold : to place current neural network treatments of missing data within a statistical framework , and to derive from this framework a set of algorithms that han - dle incomplete data in a principled manner .
to maintain the breadth of the review we discuss classi ( cid : 123 ) cation , func - tion approximation , and clustering problems .
because missing data can arise in both the input and the target variables , we treat both missing inputs and unlabeled
the statistical framework that we adopt ( cf .
little and rubin , ) makes a distinction between the envi - ronment , which we assume to generate complete data , and the missing data mechanism which renders some of the output of the environment unobservable to the learner .
the supervised learning problem consists of forming a map from inputs to targets .
the unsuper - vised learning process generally consists of extracting some compact statistical description of the inputs .
both these cases the learner may bene ( cid : 123 ) t from knowl - edge of constraints both on the data generation process ( e . g . , that it falls within a certain parametric family ) , and on the mechanism which caused the pattern of in - completeness ( e . g . , that it is independent of the data generation process ) .
the use of statistical theory allows us to formalize the consequences of these constraints and provides us with a framework for deriving learning algo - rithms that make use of these consequences .
before developing a framework for incomplete data , let us motivate the problem with perhaps the simplest statistical example that illustrates an interaction be - tween the missing data and the data generation mecha - nisms .
imagine we wish to estimate the mean ( ( cid : 123 ) x; ( cid : 123 ) y ) and covariance matrix ( cid : 123 ) of a bivariate normal distribu - tion , from a data set x = f ( xi; yi ) gn i= where some of the observations of yi are missing ( see fig .
if we estimate ( cid : 123 ) x by the mean of the xi and ( cid : 123 ) y by the mean of the ob - served values of yi , we will underestimate ( cid : 123 ) y as we have ignored the covariance structure in the observed data .
a more intelligent heuristic would use the covariance struc - ture to ( cid : 123 ) ll - in the values of the missing yi by regressing them on the xi .
however , even this heuristic will yield a biased estimate of the covariance matrix as the ( cid : 123 ) lled - in data points will all fall along the regression line .
both of the above \ ( cid : 123 ) lling - in " techniques , known as mean im - putation and regression imputation , yield unsatisfactory
results even on this simple parameter estimation prob -
the paper is organized as follows .
in section we outline the statistical framework that de ( cid : 123 ) nes the miss - ing data and data generation mechanisms .
in section we proceed to describe a likelihood - based approach to learning from incomplete data .
in section we use this approach to derive a set of learning algorithms for func - tion approximation , classi ( cid : 123 ) cation , and clustering .
sec - tion describes an alternative to the likelihood - based approach , the bayesian approach , and several algorithms that implement it .
section discusses boltzmann ma - chines and incomplete data .
finally , we conclude in sec -
figure : a simple example .
complete data were gen - erated from a gaussian with mean ( ; ) and covariance matrix ( = =; = = ) .
data points with missing y values are denoted by hollow circles on the y = line .
the solid square indicates the ( x; y ) mean calcu - lated over the observed data .
the hollow square and the ellipse indicate the mean and standard deviation calcu - lated from the incomplete data using a maximum likeli - hood ( ml ) algorithm .
note that the ml estimate of ( cid : 123 ) y is higher than any of the observed values of y !
the statistical framework we present is based on lit - tle and rubin ( ) .
we assume that the data set x = fxign i= can be divided into an observed component x o and a missing component x m .
each data vector xi may have di ( cid : 123 ) erent patterns of missing features .
we will not distinguish for now between the input and target components of the data vector .
we formalize the notion of a missing data mechanism by de ( cid : 123 ) ning a missing data indicator matrix r , such that
see , for example , the uci repository of machine learn -
ing databases ( murphy and aha , ) .
rij = ( cid : 123 ) ; xij observed ,
; xij missing .
both the data generation process and the missing data mechanism are considered to be random processes , with joint probability distribution given by
p ( x o
p ( x ; rj ( cid : 123 ) ; ( cid : 123 ) ) = p ( x j ( cid : 123 ) ) p ( rjx ; ( cid : 123 ) ) :
we use the parameters ( cid : 123 ) for the data generation process and a distinct set of parameters , ( cid : 123 ) , for the missing data
once the data probability model has been decom - posed as in ( ) , we can distinguish three nested types of missing data mechanism .
the data can be
random ( mcar ) : p ( rjx ; ( cid : 123 ) ) = p ( rj ( cid : 123 ) ) .
that is , the probability that xij is missing is independent of the values in the data vector .
missing at random ( mar ) : p ( rjx o
; ( cid : 123 ) ) = p ( rjx o ; ( cid : 123 ) ) .
that is , the probability that xij is missing is independent of values of missing com - ponents of the data , but may depend on the val - ues of observed components .
for example , xij may be missing for certain values of xik; ( k=j ) provided that xik is always observed .
figure illustrates
not missing at
random ( nmar )
p ( rjx o ; ( cid : 123 ) ) may depend on the value of xij .
if p ( rijjxij; ( cid : 123 ) ) is a function of xij the data is said to be censored .
for example , if a sensor fails when its input exceeds some range its output will be cen -
the type of the missing data mechanism is critical in evaluating learning algorithms for handling incomplete data .
full maximum likelihood and bayesian approaches can handle data that is missing at random or completely at random .
several simpler learning approaches can han - dle mcar data but fail on mar data in general .
no general approaches exist for nmar data .
for both bayesian and maximum likelihood tech - niques the estimates of the parameters ( cid : 123 ) and ( cid : 123 ) are linked to the observed data , x o and r , via p ( x o ; rj ( cid : 123 ) ; ( cid : 123 ) ) .
for maximum likelihood methods the likelihood is
l ( ( cid : 123 ) ; ( cid : 123 ) jx o
; r ) / p ( x o
; rj ( cid : 123 ) ; ( cid : 123 ) ) ;
and for bayesian methods the posterior probability is
p ( ( cid : 123 ) ; ( cid : 123 ) jx o
; r ) / p ( x o
; rj ( cid : 123 ) ; ( cid : 123 ) ) p ( ( cid : 123 ) ; ( cid : 123 ) ) :
we wish to ascertain under which conditions the pa - rameters of the data generation process can be estimated independently of the parameters of the missing data mechanism .
given that
p ( x o
; rj ( cid : 123 ) ; ( cid : 123 ) ) = z p ( x o
we note that if
; x mj ( cid : 123 ) ) p ( rjx o
; ( cid : 123 ) ) dx m
p ( rjx o
; ( cid : 123 ) ) = p ( rjx o
for succinctness will use the non - bayesian phrase \esti - mating parameters " in this section; this can be replaced by \calculating the posterior probabilities of the parameters " for the parallel bayesian argument .
; rj ( cid : 123 ) ; ( cid : 123 ) ) = p ( rjx o
; ( cid : 123 ) ) z p ( x o
; x mj ( cid : 123 ) ) dx m
p ( rjx o
; ( cid : 123 ) ) p ( x oj ( cid : 123 ) ) :
equation ( ) states that if the data is mar then the likelihood can be factored .
for maximum like - lihood methods this implies directly that maximizing l ( ( cid : 123 ) jx o ) / p ( x oj ( cid : 123 ) ) as a function of ( cid : 123 ) is equivalent to maximizing l ( ( cid : 123 ) ; ( cid : 123 ) jx o ; r ) .
therefore the parameters of the missing data mechanism can be ignored for the pur - poses of estimating ( cid : 123 ) ( little and rubin , ) .
for bayesian methods , the missing data mechanism cannot be ignored unless we make the additional require - ment that the prior is factorizable :
p ( ( cid : 123 ) ; ( cid : 123 ) ) = p ( ( cid : 123 ) ) p ( ( cid : 123 ) ) :
these results imply that data sets that are nmar , such as censored data , cannot be handled by bayesian or likelihood - based methods unless a model of the missing data mechanism is also learned .
on the positive side , they also imply that the mar condition , which is weaker than the mcar condition , is su ( cid : 123 ) cient for bayesian or
likelihood - based methods for
in the previous section we showed that maximum likeli - hood methods can be utilized for estimating the param - eters of the data generation model , ignoring the missing data mechanism , provided that the data is missing at random .
we now turn to the problem of estimating the parameters of a model from incomplete data .
we focus ( cid : 123 ) rst on feedforward neural network models before turning to a class of models where the missing data can be incorporated more naturally into the esti - mation algorithm .
for feedforward neural networks we know that descent in the error cost function can be inter - preted as ascent in the model parameter likelihood ( e . g .
in particular if the target vector is as -
sumed to be gaussian , p ( yijxi; ( cid : 123 ) ) ( cid : 123 ) n ( yi; f ( cid : 123 ) ( xi ) ; ( cid : 123 ) then the log likelihood is equivalent to the sum - squared error weighted by the output variances :
log p ( yijxi; ( cid : 123 ) ) ( ) min
( yi ( cid : 123 ) f ( cid : 123 ) ( xi ) )
if a target yi is missing or unknown the variance of that
output can be taken to be in ( cid : 123 ) nite , ( cid : 123 ) i ! .
similarly , if certain components of a target vector are missing we can assume that the variance of that component is in ( cid : 123 ) nite .
the missing targets drop out of the likelihood and the minimization can proceed as before , simply with certain targets replaced by \dont cares . "
if components of an input vector are missing , how - ever , then the likelihood is not properly de ( cid : 123 ) ned since p ( yijxi; ( cid : 123 ) ) depends on the full input vector .
the con - ditional over the observed inputs needed for the likeli - hood requires integrating out the missing inputs .
this , in turn , requires a model for the input density , p ( x ) ,
which is not explicitly available in a feedforward neural
tresp et al .
( ) proposed solving this problem by separately estimating the input density , p ( x ) , with a gaussian mixture model and the conditional density , p ( yjx ) , with a feedforward network .
this approach can be seen as maximizing the joint input - output log likeli -
l = xi
log p ( xi; yij ( cid : 123 ) ; ( cid : 123 ) )
log p ( yijxi; ( cid : 123 ) ) + xi
log p ( xij ( cid : 123 ) )
where the feedforward network is parametrized by ( cid : 123 ) and the mixture model is parametrized by ( cid : 123 ) .
if some com - ponents of an input vector are missing the observed data likelihood can be expressed as
i ; ( cid : 123 ) ) = z p ( yijxo
i ; xm
; ( cid : 123 ) ) p ( xmjxo
i ; ( cid : 123 ) ) dxm
where the input has been decomposed into its observed and missing components , x = ( xo ; xm ) .
the mixture model is used to integrate the likelihood over the missing inputs of the feedforward network .
the gradient of this likelihood with respect to the network parameters ,
i ; ( cid : 123 ) ) z p ( yijxo
i ; xm
; ( cid : 123 ) ) p ( xmjxo
i ; ( cid : 123 ) )
( yi ( cid : 123 ) f ( cid : 123 ) ( xo
i ; xm ) )
i ; xm )
exhibits error terms which weight each completion of the missing input vector by p ( xmjyi; xo i ; ( cid : 123 ) ; ( cid : 123 ) ) .
this term , by bayes rule , is proportional to the product of the prob - ability of the completion given the input , p ( xmjxo i ; ( cid : 123 ) ) , and the posterior probability of the output given that completion p ( yijxo ; ( cid : 123 ) ) .
the integral can be approx - imated by a monte carlo method , where , for each miss - ing input , several completions are generated according to the input distribution .
an intuitively appealing as - pect of this method is that more weight is placed on error gradients from input completions that better ap - proximate the target ( tresp et al . , ; buntine and
i ; xm
these arguments imply that computing maximum likelihood estimates from missing inputs requires a model of the joint input density .
in principle this could be achieved by multiple feedforward networks each learn - ing a particular conditional density of inputs .
for exam - ple , if the pattern of missing inputs is monotone , i . e .
the d input dimensions can be ordered such that if xij is observed then all xik for k < j are also observed , then the missing data can be completed by a cascade of d ( cid : 123 ) networks .
each network is trained to predict one in - put dimension from completed instances of all the lower index input dimensions and therefore models that par - ticular conditional density ( cf .
regression imputation for monotone multivariate normal data; little and rubin ,
however , to accommodate general patterns of miss - ing inputs and targets the approach of using multiple
feedforward networks becomes practically cumbersome as the number of such networks grows exponentially with the data dimensionality .
this problem can be avoided by modeling both the input and output densities using a mixture model .
mixture models and incomplete data
the mixture modeling framework allows learning from data sets with arbitrary patterns of incompleteness .
learning in this framework is a classical estimation prob - lem requiring an explicit probabilistic model and an al - gorithm for estimating the parameters of the model .
a possible disadvantage of parametric methods is their lack of ( cid : 123 ) exibility when compared with nonparametric meth - ods .
mixture models , however , largely circumvent this problem as they combine much of the ( cid : 123 ) exibility of non - parametric methods with certain of the analytic advan - tages of parametric methods ( mclachlan and basford ,
mixture models have been utilized recently for super - vised learning problems in the form of the \mixtures of experts " architecture ( jacobs et al . , ; jordan and jacobs , ) .
this architecture is a parametric re - gression model with a modular structure similar to the nonparametric decision tree and adaptive spline models ( breiman et al . , ; friedman , ) .
the approach presented here di ( cid : 123 ) ers from these regression - based ap - proaches in that the goal of learning is to estimate the density of the data .
no distinction is made between in - put and output variables; the joint density is estimated and this estimate is then used to form an input / output map .
similar density estimation approaches have been discussed by specht ( ) for nonparametric models , and nowlan ( ) and tresp et al .
( ) , among oth - ers , for gaussian mixture models .
to estimate the vec - tor function y = f ( x ) the joint density p ( x; y ) is esti - mated and , given a particular input x , the conditional density p ( yjx ) is formed .
to obtain a single estimate of y rather than the full conditional density one can evalu - ate ^y = e ( yjx ) , the expectation of y given x .
the most appealing feature of mixture models in the context of this paper is that they can deal naturally with incomplete data .
in fact , the problem of estimating mix - ture densities can itself be viewed as a missing data prob - lem ( the \labels " for the component densities are miss - ing ) and an expectation ( maximization ( em ) algorithm ( dempster et al . , ) can be developed to handle both kinds of missing data .
the em algorithm for mixture models
this section outlines the estimation algorithm for ( cid : 123 ) nd - ing the maximum likelihood parameters of a mixture model ( dempster et al . , ) .
we model the data x = fxign i= as being generated independently from a
p ( xi ) =
p ( xij ! j; ( cid : 123 ) j ) p ( ! j ) ;
where each component of the mixture is denoted ! j and parametrized by ( cid : 123 ) j .
we start by assuming complete
from equation ( ) and the independence assump - tion we see that the log likelihood of the parameters given the data set is
l ( ( cid : 123 ) jx ) =
p ( xij ! j; ( cid : 123 ) j ) p ( ! j ) :
by the maximum likelihood principle the best model of the data has parameters that maximize l ( ( cid : 123 ) jx ) .
this function , however , is not easily maximized numerically because it involves the log of a sum .
intuitively , there is a \credit - assignment " problem : it is not clear which component of the mixture gener - ated a given data point and thus which parameters to adjust to ( cid : 123 ) t that data point .
the em algorithm for mixture models is an iterative method for solving this credit - assignment problem .
the intuition is that if one had access to a \hidden " random variable z indicating which data point was generated by which component , then the overall maximization problem would decou - ple into a set of simple maximizations .
using the bi - nary indicator variables z = fzign i= , de ( cid : 123 ) ned such that zi = ( zi; : : : ; zim ) and zij = i ( cid : 123 ) xi is generated by gaussian j , a \complete - data " log likelihood function can be written
lc ( ( cid : 123 ) jx ; z ) =
zij log ( p ( xijzi; ( cid : 123 ) ) p ( zi; ( cid : 123 ) ) ) ;
which does not involve a log of a summation .
since z is unknown lc cannot be utilized directly , so we instead work with its expectation , denoted by q ( ( cid : 123 ) j ( cid : 123 ) k ) .
as shown by ( dempster et al . , ) , l ( ( cid : 123 ) jx ) can be max - imized by iterating the following two steps :
e - step : q ( ( cid : 123 ) j ( cid : 123 ) k ) = e ( lc ( ( cid : 123 ) jx ; z ) jx ; ( cid : 123 ) k )
= arg max
the expectation or e - step computes the expected com - plete data log likelihood , and the maximization or m - step ( cid : 123 ) nds the parameters that maximize this likelihood .
in practice , for densities from the exponential family the e - step reduces to computing the expectation over the missing data of the su ( cid : 123 ) cient statistics required for the m - step .
these two steps form the basis of the em algo - rithm for mixture modeling .
incorporating missing values into the em
in the previous section we presented one aspect of the em algorithm : learning mixture models .
another im - portant application of em is to learning from data sets with missing values ( little and rubin , ; dempster et al . , ) .
this application has been pursued in the statistics literature mostly for non - mixture density es - timation problems .
we now show how combining the
some exceptions are the use of mixture densities in the context of contaminated normal models for robust estima - tion ( little and rubin , ) , and in the context of mixed categorical and continuous data with missing values ( little and schluchter , ) .
missing data application of em with that of learning mixture parameters results in a set of clustering , classi - ( cid : 123 ) cation , and function approximation algorithms for in -
i ; xm
using the previously de ( cid : 123 ) ned notation , xi is divided i ) where each data vector can have di ( cid : 123 ) erent patterns of missing components .
( to denote the missing and observed components in each data vector we would ordinarily introduce superscripts mi and oi , however , we have simpli ( cid : 123 ) ed the notation for the sake of clarity . )
to handle missing data we rewrite the em algorithm incorporating both the indicator variables from algo - rithm ( ) and the missing inputs , x m .
e - step : q ( ( cid : 123 ) j ( cid : 123 ) k ) = e ( lc ( ( cid : 123 ) jx o = arg max
; z ) jx o
the expected value in the e - step is taken with respect to both sets of missing variables .
we proceed to illus - trate this algorithm for two classes of models , mixtures of gaussians and mixtures of bernoullis , which we later use as building blocks for classi ( cid : 123 ) cation and function ap -
real - valued data : mixture of gaussians
real - valued data can be modeled as a mixture of gaussians .
we start with the estimation algorithm for complete data ( duda and hart , ; dempster et al . , ; nowlan , ) .
for this model the e - step simpli - ( cid : 123 ) es to computing e ( zijjxi; ( cid : 123 ) k ) .
given the binary nature of zij , e ( zijjxi; ( cid : 123 ) k ) , which we denote by hij , is the prob - ability that gaussian j generated data point i .
j ^ ( cid : 123 ) jj ( cid : 123 ) = expf ( cid : 123 ) l= j ^ ( cid : 123 ) lj ( cid : 123 ) = expf ( cid : 123 )
( xi ( cid : 123 ) ^ ( cid : 123 ) j ) t ^ ( cid : 123 ) ( cid : 123 )
j ( xi ( cid : 123 ) ^ ( cid : 123 ) j ) g ( xi ( cid : 123 ) ^ ( cid : 123 )
( xi ( cid : 123 ) ^ ( cid : 123 )
the m - step re - estimates the means and covariances of the gaussians using the data set weighted by the hij :
j = pn i= hij ( xi ( cid : 123 ) ^ ( cid : 123 )
j = pn
) ( xi ( cid : 123 ) ^ ( cid : 123 )
to incorporate missing data we begin by rewriting the
log likelihood of the complete data ,
; z ) =
zij log p ( xijzi; ( cid : 123 ) ) +
zij log p ( zij ( cid : 123 ) ) :
we can ignore the second term since we will only be es - timating the parameters of the p ( xijzi; ( cid : 123 ) ) .
specializing equation ( ) to the mixture of gaussians we note that
though this derivation assumes equal priors for the gaussians , if the priors are viewed as mixing parameters they can also be learned in the maximization step .
if only the indicator variables zi are missing , the e step can be reduced to estimating e ( zijjxi; ( cid : 123 ) ) as before .
for the case we are interested in , with both zi and xm ing , we expand equation ( ) using m and o superscripts to denote subvectors and submatrices of the parameters matching the missing and observed components of the data , to obtain
; z ) =
log ( cid : 123 ) +
i ( cid : 123 ) ( cid : 123 )
i ( cid : 123 ) ( cid : 123 )
j ) t ( cid : 123 ) ( cid : 123 ) ;oo j ) t ( cid : 123 ) ( cid : 123 ) ;om
i ( cid : 123 ) ( cid : 123 )
i ( cid : 123 ) ( cid : 123 )
i ( cid : 123 ) ( cid : 123 )
j ) t ( cid : 123 ) ( cid : 123 ) ;mm
i ( cid : 123 ) ( cid : 123 )
note that after taking the expectation , cient statistics for the parameters include three un - known terms , zij , zijxm we must compute : e ( zij jxo i ; ( cid : 123 ) k ) , and
i ; ( cid : 123 ) k ) , e ( zijxm
i , and zijxm
i ; ( cid : 123 ) k ) :
one intuitive approach to dealing with missing data is to use the current estimate of the data density to com - pute the expectation of the missing data in an e - step , complete the data with these expectations , and then use this completed data to re - estimate parameters in an m - step .
however , as we have seen in section , this intuition fails even when dealing with a single two - dimensional gaussian; the expectation of the missing data always lies along a line , which biases the estimate of the covariance .
on the other hand , the approach arising from applica - tion of the em algorithm speci ( cid : 123 ) es that one should use the current density estimate to compute the expectation of whatever incomplete terms appear in the likelihood maximization .
for the mixture of gaussians these in - complete terms are the interactions between the indica - tor variable zij and the ( cid : 123 ) rst and second moments of xm thus , simply computing the expectation of the missing data zi and xm from the model and substituting those values into the m step is not su ( cid : 123 ) cient to guarantee an increase in the likelihood of the parameters .
to compute the above expectations we de ( cid : 123 ) ne
ij ( cid : 123 ) e ( xm
i jzij = ; xo
i ; ( cid : 123 ) k ) = ( cid : 123 )
j + ( cid : 123 ) mo
i ( cid : 123 ) ( cid : 123 )
which is the least - squares linear regression between xm i predicted by gaussian j .
then , the ( cid : 123 ) rst expec - tation is e ( zijjxo i ; ( cid : 123 ) k ) = hij , the probability as de ( cid : 123 ) ned in ( ) measured only on the observed dimensions of xi .
similarly , we get
i ; ( cid : 123 ) k ) = hij ^xm
for example , ( cid : 123 ) is divided into ( cid : 123 ) ( cid : 123 ) oo ( cid : 123 ) om
( cid : 123 ) mo ( cid : 123 ) mm ( cid : 123 ) corre - m ( cid : 123 ) : also note that the superscript
sponding to x = ( cid : 123 ) x
( ( cid : 123 ) ; oo ) denotes inverse followed by submatrix operations , whereas ( oo ( cid : 123 ) ) denotes the reverse order .
i ; ( cid : 123 ) k ) = hij ( ( cid : 123 ) mm
the m - step uses these expectations substituted into equations ( ) and ( ) to re - estimate the means and co - variances .
to re - estimate the mean vector , ( cid : 123 ) j , we sub - stitute the values of ^xm ij for the missing components of xi in equation ( ) .
to re - estimate the covariance matrix we substitute the values of the bracketed term in ( ) for the outer product matrices involving the missing compo - nents of xi in equation ( ) .
discrete - valued data : mixture of
binary data can be modeled as a mixture of bernoulli densities .
that is , each d - dimensional vector x = ( x; : : : ; xd; : : : xd ) , xd f; g , is modeled as generated from the mixture of m bernoulli densities :
p ( xj ( cid : 123 ) ) =
jd ( ( cid : 123 ) ( cid : 123 ) j d ) ( ( cid : 123 ) xd )
for this model the complete data e - step computes
jd ( ( cid : 123 ) ^ ( cid : 123 ) jd ) ( ( cid : 123 ) xid )
ld ( ( cid : 123 ) ^ ( cid : 123 ) ld ) ( ( cid : 123 ) xid ) and the m - step re - estimates the parameters by
hij = qd j = pn
to incorporate missing data we must compute the appropriate expectations of the su ( cid : 123 ) cient for the bernoulli mixture statistics in the e - step .
these include the incomplete terms e ( zijjxo i ; ( cid : 123 ) k ) and i ; ( cid : 123 ) k ) .
the ( cid : 123 ) rst is equal to hij calculated over the observed subvector of xi .
the second , since we as - sume that within a class the individual dimensions of the bernoulli variable are independent , is simply hij ( cid : 123 ) j .
the m - step uses these expectations substituted into equa -
more generally , discrete or categorical data can be modeled as generated by a mixture of multinomial den - sities and similar derivations for the learning algorithm can be applied .
finally , the extension to data with mixed real , binary , and categorical dimensions can be readily derived by assuming a joint density with mixed compo - nents of the three types .
such mixed models can serve to solve classi ( cid : 123 ) cation problems , as will be discussed in a
gaussian mixture model estimation is a form of soft clus - tering ( nowlan , ) .
furthermore , if a full covariance model is used the principal axes of the gaussians align with the principal components of the data within each soft cluster .
for binary or categorical data soft clus - tering algorithms can also be obtained using the above bernoulli and multinomial mixture models .
we illus - trate the extension of these clustering algorithms to miss - ing data problems with a simple example from character
figure : learning digit patterns .
first row : the ten ( cid : 123 ) templates used to generate the data set .
second row : templates with gaussian noise added .
third row : templates with noise added and % missing pixels .
the training set consisted of ten such noisy , incomplete samples of each digit .
fourth row : means of the twelve gaussians at asymptote ( ( cid : 123 ) passes through the data set of patterns ) using the mean imputation heuristic .
fifth row : means of the twelve gaussians at asymptote ( ( cid : 123 ) passes , same incomplete data set ) using the em algorithm .
gaussians constrained to diagonal covariance matrices .
in this example ( fig .
) , the gaussian mixture algo - rithm was used on a training set of - dimensional noisy greyscale digits with % of the pixels missing .
the em algorithm approximated the cluster means from this highly de ( cid : 123 ) cient data set quite well .
we compared em to mean imputation , a common heuristic where the missing values are replaced with their unconditional means .
the results showed that em outperformed mean imputation when measured both by the distance be - tween the gaussian means and the templates ( see fig .
) , and by the likelihoods ( log likelihoods ( cid : 123 ) s . e . : em ( cid : 123 ) ( cid : 123 ) ; mean imputation ( cid : 123 ) ( cid : 123 ) ; n = ) .
function approximation
so far , we have alluded to data vectors with no refer - ence to \inputs " and \targets . " in supervised learning , however , we generally wish to predict target variables from some set of input variables|that is , we wish to ap - proximate a function relating these two sets of variables .
if we decompose each data vector xi into an \input " i , and a \target " or output subvector , xt then the relation between input and target variables can be expressed through the conditional density p ( xt this conditional density can be readily obtained from the joint input / target density , which is the density which all the above mixture models seek to estimate .
thus , in this framework , the distinction between supervised learning , i . e .
function approximation , and unsupervised learning , i . e .
density estimation , is semantic , resulting from whether the data is considered to be composed of inputs and targets or not .
focusing on the gaussian mixture model we note that
the conditional density p ( xt i ) is also a gaussian mix - ture .
given a particular input the estimated output should summarize this density .
if we require a single estimate of the output , a natu - ral candidate is the least squares estimate ( lse ) , which takes the form ^xt ( xi i ) .
expanding the expec - tation we get
i ) = e ( xt
j + ( cid : 123 ) ti
i ( cid : 123 ) ( cid : 123 )
which is a convex sum of the least squares linear approx - imations given by each gaussian .
the weights in the sum , hij , vary nonlinearly over the input space and can be viewed as corresponding to the output of a classi ( cid : 123 ) er that assigns to each point in the input space a probability of belonging to each gaussian .
the least squares esti - mator has interesting relations to models such as cart ( breiman et al . , ) , mars ( friedman , ) , and mixtures of experts ( jacobs et al . , ; jordan and jacobs , ) , in that the mixture of gaussians com - petitively partitions the input space , and learns a linear regression surface on each partition .
this similarity has also been noted by tresp et al
if the gaussian covariance matrices are constrained to be diagonal , the least squares estimate further simpli ( cid : 123 ) es
the hij in equation ( ) are computed by substituting i into equation ( ) and evaluating the exponentials over the
dimensions of the input space .
the average of the output means , weighted by the prox - imity of xi i to the gaussian input means .
this expression has a form identical to normalized radial basis function ( rbf ) networks ( moody and darken , ; poggio and girosi , ) , although the two algorithms are derived from disparate frameworks .
in the limit , as the covari - ance matrices of the gaussians approach zero , the ap - proximation becomes a nearest - neighbor map .
not all learning problems lend themselves to least squares estimates|many problems involve learning a one - to - many mapping between the input and target vari - ables ( ghahramani , ) .
the resulting conditional densities are multimodal and no single value of the output given the input will appropriately re ( cid : 123 ) ect this fact ( shizawa , ; ghahramani , ; bishop , ) .
for such problems a stochastic estimator , where the out - put is sampled according to ^xt ( xi i ) , is to be preferred to the least squares estimator .
i ) ( cid : 123 ) p ( xt
for learning problems involving discrete variables the lse and stochastic estimators have a di ( cid : 123 ) erent interpre - tation .
if we wish to obtain the posterior probability of the output given the input we would use the lse esti - mator .
on the other hand , if we wish to obtain output estimates that fall in our discrete output space we would use the stochastic estimator .
classification with missing inputs
% missing features
figure : classi ( cid : 123 ) cation of the iris data set .
data points were used for training and for testing .
each data point consisted of real - valued attributes and one of three class labels .
the ( cid : 123 ) gure shows classi ( cid : 123 ) cation per - formance ( cid : 123 ) standard error ( n = ) as a function of proportion missing features for the em algorithm and for mean imputation ( mi ) , a common heuristic where the missing values are replaced with their unconditional
classi ( cid : 123 ) cation , though strictly speaking a special case of function approximation , merits attention of its own .
classi ( cid : 123 ) cation problems involve learning a mapping from an input space of attributes into a set of discrete class
labels .
the mixture modeling framework presented here lends itself readily to classi ( cid : 123 ) cation problems by modeling the class label as a multinomial variable .
for example , if the attributes are real - valued and there are d class labels , a mixture model with gaussian and multinomial components can be used;
p ( x; c = dj ( cid : 123 ) ) =
( x ( cid : 123 ) ( cid : 123 ) j ) t ( cid : 123 ) ( cid : 123 )
j ( x ( cid : 123 ) ( cid : 123 ) j ) g
denotes the joint probability that the data point has at - tributes x and belongs to class d , where the ( cid : 123 ) jd are the parameters for the multinomial class variable .
that is ,
d= ( cid : 123 ) jd = .
( cid : 123 ) jd = p ( c = dj ! j; ( cid : 123 ) ) , and pd
missing attributes and missing class labels ( i . e . , unla - beled data points ) are readily handled via the em algo - rithm .
in the e - step , missing attributes are completed using the same formulas as for the gaussian mixture ex -
hij = p ( xo
i ; ci = dj ! j; ( cid : 123 ) ) =
i j ! j ; ( cid : 123 ) )
l= ( cid : 123 ) ld p ( xo
i j ! l ; ( cid : 123 ) )
on the other hand , if a class label is missing hij becomes l= p ( xij ! l; ( cid : 123 ) ) , exactly as in the gaussian mixture .
the class label is then completed with a prob -
p ( xij ! j ; ( cid : 123 ) ) =pm ability vector whose dth component is pm
once the classi ( cid : 123 ) cation model has been estimated , the most likely label for a particular input x may be obtained by computing p ( c = djx; ( cid : 123 ) ) .
similarly , the class conditional densities can be computed by evaluat - ing p ( xjc = d; ( cid : 123 ) ) .
conditionalizing over classes in this way yields class conditional densities which are in turn mixtures of gaussians .
figure shows the performance of the em algorithm on a sample classi ( cid : 123 ) cation problem with varying proportions of missing features .
j= hij ( cid : 123 ) j d .
this mixture - based approach to classi ( cid : 123 ) cation is closely related to the mixture discriminant analysis ( mda ) approach recently proposed by hastie and tib - shirani ( ) .
in mda , classes are also ( cid : 123 ) t by mixture densities using the em algorithm and an optimal dis - criminant is obtained .
hastie and tibshirani extend this basic mda procedure by combining it with reduced rank discrimination .
like fisher - rao linear discriminant analysis this results in an interpretable , low dimensional projection of the data and often also leads to improved classi ( cid : 123 ) cation performance .
while the authors do not mention missing data , it seems likely that em methods can be used in the context of their algorithm .
previous approaches to classi ( cid : 123 ) cation from incomplete patterns have proceeded along di ( cid : 123 ) erent lines .
cheese - man et al .
( ) describe a bayesian classi ( cid : 123 ) cation method in which each class is modeled as having gaus - sian real - valued attributes and multinomial discrete at - tributes .
the learning procedure ( cid : 123 ) nds the maximum a posteriori parameters of the model by di ( cid : 123 ) erentiating the posterior probability of the class parameters and setting to zero .
this yields a coupled set of nonlinear equations ,
similar to the em steps , which can be iterated to ( cid : 123 ) nd the posterior mode of the parameters ( dempster et al . , ) .
to handle missing data the authors state that \for discrete attributes it can be shown that the correct procedure for treating an unknown value is equivalent to adding an unknown category to the value set " ( p .
for real - valued attributes they add a known / unknown category to each attribute and set its value appropri - ately .
three comments can be made about this ap - proach .
first , each unknown category added to the multinomial value set results in an extra parameter that has to be estimated .
furthermore , adding an unknown category does not re ( cid : 123 ) ect the fact that the unobserved data actually arises from the original multinomial value set ( an argument also made by quinlan , ; see be - low ) .
for example , for a data set in which one attribute is often unknown the algorithm may form a class based on that attribute taking on the value unknown|a situ - ation which is clearly undesirable in a classi ( cid : 123 ) er .
finally , as each class is modeled by a single gaussian or multino - mial and the data points are assumed to be unlabeled , the cheeseman et al .
( ) algorithm is in fact a form of soft clustering .
southcott and bogner ( ) have approached the problem of classi ( cid : 123 ) cation of incomplete data using an ap - proximation to em for clustering .
in the e - step , the observed data are classi ( cid : 123 ) ed using the current mixture model , and each data point is assigned to its most likely class .
the parameters of each class are then re - estimated in the m - step .
in our notation this approximation corre - sponds to setting the highest hij for each data point to and all the others to .
they compared this method with a neural network based algorithm in which each missing input is varied through the possible range of ( discrete ) attribute values to ( cid : 123 ) nd the completion result - ing in minimum classi ( cid : 123 ) cation error .
they reported that their approximation to em outperformed both the neu - ral network algorithm and an algorithm based on linear discriminant analysis .
they did not include the exact em algorithm in their comparison .
quinlan ( , ) discusses the problem of missing data in the context of decision tree classi ( cid : 123 ) ers .
quinlans decision tree framework uses a measure of information gain to build a classi ( cid : 123 ) er , resulting in a tree structure of queries on attribute values and a set of leaves rep - resenting class membership .
the author concludes that treating unknown as a separate value is not a good so - lution to the missing value problem , as querying on at - tributes with unknown values will have higher apparent information gain ( quinlan , ) .
the approach that he advocates instead is to compute the expected infor - mation gain , by assuming that the unknown attribute is distributed according to the observed values in the sub - set of the data at that node of the tree .
this approach is consistent with the information theoretic framework adopted in his work and parallels the em and bayesian treatments of missing data which suggest integrating over the possible missing values .
an alternative method of handling missing data in de - cision trees is presented by breiman et al .
( ) for the cart algorithm .
cart initially constructs a large de -
cision tree based on a splitting criterion closely related to the above measure of information gain .
the tree is then pruned recursively using a measure of model complexity proportional to the number of terminal nodes , resulting in a smaller , more interpretable tree with better gener - alization properties .
if a case is missing the value of an attribute then it is not considered when evaluating the goodness of splits on that attribute .
cases are assigned to branches of a split on an attribute where they have missing values using the best surrogate split|i . e .
the split on another attribute which partitions the data most similarly to the original split .
this method works well when there is a single , highly correlated attribute that predicts the e ( cid : 123 ) ects of a split along the missing attribute .
however , if no single attribute can predict the e ( cid : 123 ) ects of the split this method may not perform well .
an approach based on computing the expected split from all the ob - served variables , similar to quinlans , would be more suitable from a statistical perspective and may provide improved performance with missing data .
in bayesian learning the parameters are treated as un - known random variables characterized by a probability distribution .
bayesian learning utilizes a prior distribu - tion for the parameters , which may encode world knowl - edge , initial biases of the learner , or constraints on the probable parameter values .
learning proceeds by bayes rule|multiplying the prior probability of the parameters by the likelihood of the data given the parameters , and normalizing by the integral over the parameter space| resulting in a posterior distribution of the parameters .
the information learned about the unknown parameters is expressed in the form of this posterior probability dis -
in the context of learning from incomplete data , the bayesian use of priors can have impact in two arenas .
first , the prior may re ( cid : 123 ) ect assumptions about the initial distribution of parameter values as described above .
the learning procedure converts this prior into a posterior via the data likelihood .
we have seen that to perform this conversion independently of the missing data mechanism requires both that the mechanism be missing at random and that the prior be factorizable .
second , the prior may re ( cid : 123 ) ect assumptions about the initial distribution of the missing values .
thus , if we have a prior distribution for input values we can complete the missing data by sampling from this distribution .
for complete data problems and simple models the judicious choice of conjugate priors for the parameters often allows analytic computation of their posterior dis - tribution ( box and tiao , ) .
however , in incomplete data problems the usual choices of conjugate priors do not generally lead to recognizable posteriors , making it - erative simulation and sampling techniques for obtaining the posterior distribution indispensable ( schafer , ) .
data augmentation and gibbs sampling
one such technique , which is closely related in form to the em algorithm , is data augmentation ( tanner and wong , ) .
this iterative algorithm consists of two
in the imputation or i - step , instead of comput - ing the expectations of the missing su ( cid : 123 ) cient statistics , we simulate m random draws of the missing data from their conditional distribution p ( x mjx o ; ( cid : 123 ) ) .
in the pos - terior or p - step we sample m times from the posterior distribution of the parameters , which can now be more easily computed with the imputed data : p ( ( cid : 123 ) jx o ; x m ) .
thus , we obtain samples from the joint distribution of p ( x m ; ( cid : 123 ) jx o ) by alternately conditioning on one or the other of the unknown variables , a technique known as gibbs sampling ( geman and geman , ) .
under some mild regularity conditions this algorithm can be shown to converge in distribution to the posterior ( tanner and wong , ) .
note that the augmented data can be cho - sen so as to simplify the p - step in much the same way as indicator variables can be chosen to simplify the m - step
mentation techniques have been recently combined with the metropolis ( hastings algorithm ( schafer , ) .
in metropolis ( hastings ( metropolis et al . , ; hastings , ) , one creates a monte carlo markov chain by draw - ing from a probability distribution meant to approximate the distribution of interest and accepting or rejecting the drawn value based on an acceptance ratio .
the accep - tance ratio , e . g .
the ratio of probabilities of the drawn state and the previous state , can often be chosen to be easy to calculate as it does not involve computation of the normalization factor .
if the transition probabilities allow any state to be reached eventually from any other state ( i . e .
the chain is ergodic ) then the markov chain will approach its stationary distribution , chosen to be the distribution of interest , from any initial distribution .
the combination of data augmentation and metropolis ( hastings can be used , for example , in problems where the posterior itself is di ( cid : 123 ) cult to sample from in the p - step .
for such problems one may generate a markov chain whose stationary distribution is p ( ( cid : 123 ) jx o
; x m ) .
multiple imputation and bayesian
multiple imputation ( rubin , ) is a technique in which each missing value is replaced by m simulated val - ues which re ( cid : 123 ) ect uncertainty about the true value of the missing data .
after multiple imputation , m completed data sets exist , each of which can be analyzed using com - plete data methods .
the results can then be combined to form a single inference .
though multiple imputation requires sampling from p ( x mjx o ; ( cid : 123 ) ) , which may be dif - ( cid : 123 ) cult , iterative simulation methods can also be used in this context ( schafer , ) .
the bayesian backpropagation technique for missing data presented by buntine and weigend ( ) is a spe - cial case of multiple imputation .
in bayesian backpropa - gation , multiple values of the input are imputed accord - ing to a prior distribution so as to approximate the inte - gral in ( ) , which in turn is used to compute the gradient required for backpropagation .
this procedure is similar to that of tresp et al .
( ) , except that whereas the former completes the data by sampling from a prior dis - tribution of inputs , the latter estimates this distribution
directly from the data .
boltzmann machines and incomplete
boltzmann machines are networks of binary stochastic units with symmetric connections , in which learning cor - responds to minimizing the relative entropy between the probability distribution of the visible states and a target distribution ( hinton and sejnowski , ) .
the relative entropy cost function can be rewritten to reveal that , if the target distribution is taken to be the empirical distribution of the data , it is equivalent to the model likelihood .
therefore , the boltzmann learning rule im - plements maximum likelihood density estimation over
the boltzmann learning procedure ( cid : 123 ) rst estimates cor - relations between unit activities in a stage where both input and target units are clamped and in a stage where the target units are unclamped .
these correlations are then used to modify the parameters of the network in the direction of the relative entropy cost gradient .
this moves the output unit distribution in the unclamped phase closer to the target distribution in the clamped
reformulated in terms of maximum likelihood condi - tional density estimation , the boltzmann learning rule is an instance of the generalized em algorithm ( gem; dempster , laird , and rubin , ) : the estimation of the unit correlations given the current weights and the clamped values corresponds to the e - step , and the up - date of the weights corresponds to the m - step ( hinton and sejnowski , ) .
it is generalized em in the sense that the m - step does not actually maximize the likeli - hood but simply increases it by gradient ascent .
the incomplete variables in the boltzmann machine are the states of the hidden units|those that are not denoted as the visible input or output units .
this sug - gests that the principled way of handling missing inputs or targets in a boltzmann machine is to treat them as hidden units , that is , to leave them unclamped .
ex - actly as in the formulation for mixture models presented above , the em algorithm will then estimate the appro - priate su ( cid : 123 ) cient statistics|the ( cid : 123 ) rst order correlations| in the e - step .
these su ( cid : 123 ) cient statistics will then be used to increase the model likelihood in the m - step .
there are several ways of handling missing data dur - ing learning .
heuristics , such as ( cid : 123 ) lling in the missing data with unconditional or conditional means , are not al - ways e ( cid : 123 ) cient , discarding information latent in the data set .
more principled statistical approaches yield inter - pretable results , providing a guarantee to ( cid : 123 ) nd the max - imum likelihood parameters despite the missing data .
these statistical approaches argue convincingly that the missing data has to be integrated out using an esti - mate of the data density .
one class of models in which
from a strictly bayesian point of view both procedures are improper in that they dont take into account the vari - ability of the parameters in the integration .
this can be performed naturally and e ( cid : 123 ) ciently are mix - ture models .
for these models , we have described appli - cations to clustering , function approximation , and clas - si ( cid : 123 ) cation from real and discrete data .
in particular , we have shown how missing inputs and targets can be incor - porated into the mixture model framework|essentially by making a dual use of the ubiquitous em algorithm .
finally , our principal conclusion is that virtually all of the incomplete data techniques reviewed from the neural network and machine learning literatures can be placed within this basic statistical framework .
thanks to d . b .
rubin for helpful discussions on miss - ing data .
the iris data set was obtained from the uci repository of machine learning databases .
