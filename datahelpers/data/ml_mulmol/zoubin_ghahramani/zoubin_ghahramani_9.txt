we present a new gaussian process ( gp ) regression model whose co - variance is parameterized by the the locations of m pseudo - input points , which we learn by a gradient based optimization .
we take m ( cid : 123 ) n , where n is the number of real data points , and hence obtain a sparse regression method which has o ( m 123n ) training cost and o ( m 123 ) pre - diction cost per test case .
we also nd hyperparameters of the covari - ance function in the same joint optimization .
the method can be viewed as a bayesian regression model with particular input dependent noise .
the method turns out to be closely related to several other sparse gp ap - proaches , and we discuss the relation in detail .
we nally demonstrate its performance on some large data sets , and make a direct comparison to other sparse gp methods .
we show that our method can match full gp performance with small m , i . e .
very sparse solutions , and it signicantly outperforms other approaches in this regime .
the gaussian process ( gp ) is a popular and elegant method for bayesian non - linear non - parametric regression and classication .
unfortunately its non - parametric nature causes computational problems for large data sets , due to an unfavourable n 123 scaling for training , where n is the number of data points .
in recent years there have been many attempts to make sparse approximations to the full gp in order to bring this scaling down to m 123n where m ( cid : 123 ) n ( 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 ) .
most of these methods involve selecting a subset of the training points of size m ( active set ) on which to base computation .
a typical way of choosing such a subset is through some sort of information criterion .
for example , seeger et al .
( 123 ) employ a very fast approximate information gain criterion , which they use to greedily select points into the active set .
a major common problem to these methods is that they lack a reliable way of learning kernel hyperparameters , because the active set selection interferes with this learning proce - dure .
seeger et al .
( 123 ) construct an approximation to the full gp marginal likelihood , which they try to maximize to nd the hyperparameters .
however , as the authors state , they have persistent difculty in practically doing this through gradient ascent .
the reason for this is that reselecting the active set causes non - smooth uctuations in the marginal likelihood
and its gradients , meaning that they cannot get smooth convergence .
therefore the speed of active set selection is somewhat undermined by the difculty of selecting hyperparame - ters .
inappropriately learned hyperparameters will adversely affect the quality of solution , especially if one is trying to use them for automatic relevance determination ( ard ) ( 123 ) .
in this paper we circumvent this problem by constructing a gp regression model that en - ables us to nd active set point locations and hyperparameters in one smooth joint optimiza - tion .
the covariance function of our gp is parameterized by the locations of pseudo - inputs an active set not constrained to be a subset of the data , found by a continuous optimiza - tion .
this is a further major advantage , since we can improve the quality of our t by the ne tuning of their precise locations .
our model is closely related to several sparse gp approximations , in particular seegers method of projected latent variables ( plv ) ( 123 , 123 ) .
we discuss these relations in section 123
in principle we could also apply our technique of moving active set points off data points to approximations such as plv .
however we empirically demonstrate that a crucial difference between plv and our method ( spgp ) prevents this idea from working for plv .
123 gaussian processes for regression
we provide here a concise summary of gps for regression , but see ( 123 , 123 , 123 , 123 ) for more detailed reviews .
we have a data set d consisting of n input vectors x = ( xn ) n of dimension d and corresponding real valued targets y = ( yn ) n n=123
we place a zero mean gaussian process prior on the underlying latent function f ( x ) that we are trying to model .
we therefore have a multivariate gaussian distribution on any nite subset of latent variables; in particular , at x : p ( f|x ) = n ( f|123 , kn ) , where n ( f|m , v ) is a gaussian distribution with mean m and covariance v .
in a gaussian process the covariance matrix is constructed from a covariance function , or kernel , k which expresses some prior notion of smoothness of the underlying function : ( kn ) nn123 = k ( xn , xn123 ) .
usually the covariance function depends on a small number of hyperparameters , which control these smoothness properties .
for our experiments later on we will use the standard gaussian covariance with
k ( xn , xn123 ) = c exp
in standard gp regression we also assume a gaussian noise model or likelihood p ( y|f ) = n ( y|f , 123i ) .
integrating out the latent function values we obtain the marginal likelihood :
= ( c , b ) .
p ( y|x , ) = n ( y|123 , kn + 123i ) ,
which is typically used to train the gp by nding a ( local ) maximum with respect to the hyperparameters and 123
prediction is made by considering a new input point x and conditioning on the observed data and hyperparameters .
the distribution of the target value at the new point is then :
x ( kn + 123i ) 123kx + 123 ( cid : 123 ) ,
p ( y|x , d , ) = n ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) k>
x ( kn + 123i ) 123y , kxx k>
where ( kx ) n = k ( xn , x ) and kxx = k ( x , x ) .
the gp is a non - parametric model , because the training data are explicitly required at test time in order to construct the predictive distribution , as is clear from the above expression .
gps are prohibitive for large data sets because training requires o ( n 123 ) time due to the inversion of the covariance matrix .
once the inversion is done , prediction is o ( n ) for the predictive mean and o ( n 123 ) for the predictive variance per new test case .
123 sparse pseudo - input gaussian processes ( spgps )
in order to derive a sparse model that is computationally tractable for large data sets , which still preserves the desirable properties of the full gp , we examine in detail the gp predictive distribution ( 123 ) .
consider the mean and variance of this distribution as functions of x , the new input .
regarding the hyperparameters as known and xed for now , these functions are effectively parameterized by the locations of the n training input and target pairs , x and y .
in this paper we consider a model with likelihood given by the gp predictive distribution , and parameterized by a pseudo data set .
the sparsity in the model will arise because we will generally consider a pseudo data set d of size m < n : pseudo - inputs x = ( xm ) m m=123
we have denoted the pseudo targets f instead of y because as they are not real observations , it does not make much sense to include a noise variance for them .
they are therefore equivalent to the latent function values f .
the actual observed target value will of course be assumed noisy as before .
these assumptions therefore lead to the following single data point likelihood :
m=123 and pseudo targets f = ( fm ) m
p ( y|x , x , f ) = n ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) k>
f , kxx k>
m kx + 123 ( cid : 123 ) ,
where ( km ) mm123 = k ( xm , xm123 ) and ( kx ) m = k ( xm , x ) , for m = 123 , .
this can be viewed as a standard regression model with a particular form of parameterized mean function and input - dependent noise model .
the target data are generated i . i . d .
given the inputs , giving the complete data likelihood :
p ( y|x , x , f ) =yn
p ( yn|xn , x , f ) = n ( y|knmk123
f , + 123i ) ,
where = diag ( ) , n = knn k> learning in the model involves nding a suitable setting of the parameters an appropriate pseudo data set that explains the real data well .
however rather than simply maximize the likelihood with respect to x and f it turns out that we can integrate out the pseudo targets f .
we place a gaussian prior on the pseudo targets :
m kn , and ( knm ) nm = k ( xn , xm ) .
p ( f| x ) = n ( f|123 , km ) .
this is a very reasonable prior because we expect the pseudo data to be distributed in a very similar manner to the real data , if they are to model them well .
it is not easy to place a prior on the pseudo - inputs and still remain with a tractable model , so we will nd these by maximum likelihood ( ml ) .
for the moment though , consider the pseudo - inputs as known .
we nd the posterior distribution over pseudo targets f using bayes rule on ( 123 ) and ( 123 ) :
p ( f|d , x ) = n ( cid : 123 ) f|kmq123
m kmn ( + 123i ) 123y , kmq123
where qm = km + kmn ( + 123i ) 123knm .
given a new input x , the predictive distribution is then obtained by integrating the likeli - hood ( 123 ) with the posterior ( 123 ) : p ( y|x , d , x ) =
df p ( y|x , x , f ) p ( f|d , x ) = n ( y| , 123 ) ,
m kmn ( + 123i ) 123y
123 = k k>
m ) k + 123 .
note that inversion of the matrix + 123i is not a problem because it is diagonal .
the computational cost is dominated by the matrix multiplication kmn ( + 123i ) 123knm in the calculation of qm which is o ( m 123n ) .
after various precomputations , prediction can then be made in o ( m ) for the mean and o ( m 123 ) for the variance per test case .
figure 123 : predictive distributions ( mean and two standard deviation lines ) for : ( a ) full gp , ( b ) spgp trained using gradient ascent on ( 123 ) , ( c ) spgp trained using gradient ascent on ( 123 ) .
initial pseudo point positions are shown at the top as red crosses; nal pseudo point positions are shown at the bottom as blue crosses ( the y location on the plots of these crosses is not meaningful ) .
we are left with the problem of nding the pseudo - input locations x and hyperparameters = ( , 123 ) .
we can do this by computing the marginal likelihood from ( 123 ) and ( 123 ) :
p ( y|x , x , ) =
df p ( y|x , x , f ) p ( f| x )
= n ( y|123 , knmk123
m kmn + + 123i ) .
the marginal likelihood can then be maximized with respect to all these parameters ( x , ) by gradient ascent .
the details of the gradient calculations are long and tedious and therefore omitted here for brevity .
they closely follow the derivations of hyperparam - eter gradients of seeger et al .
( 123 ) ( see also section 123 ) , and as there , can be most efciently coded with cholesky factorisations .
note that km , kmn and are all functions of the m pseudo - inputs x and .
the exact form of the gradients will of course depend on the functional form of the covariance function chosen , but our method will apply to any co - variance that is differentiable with respect to the input points .
it is worth saying that the spgp can be viewed as a standard gp with a particular non - stationary covariance function parameterized by the pseudo - inputs .
since we now have m d +|| parameters to t , instead of just || for the full gp , one may be worried about overtting .
however , consider the case where we let m = n and x = x the pseudo - inputs coincide with the real inputs .
at this point the marginal likelihood is equal to that of a full gp ( 123 ) .
this is because at this point kmn = km = kn and = 123
moreover the predictive distribution ( 123 ) also collapses to the full gp predictive distribution ( 123 ) .
these are clearly desirable properties of the model , and they give condence that a good solution will be found when m < n .
however it is the case that hyperparameter learning complicates matters , and we discuss this further in section 123
123 relation to other methods
it turns out that seegers method of plv ( 123 , 123 ) uses a very similar marginal likelihood approximation and predictive distribution .
if you remove from all the spgp equations you get precisely their expressions .
in particular the marginal likelihood they use is :
p ( y|x , x , ) = n ( y|123 , knmk123
which has also been used elsewhere before ( 123 , 123 , 123 ) .
they have derived this expression from a somewhat different route , as a direct approximation to the full gp marginal likelihood .
m kmn + 123i ) ,
xy ( a ) xy ( c ) xy ( b ) figure 123 : sample data drawn from the marginal likelihood of : ( a ) a full gp , ( b ) spgp , ( c ) plv .
for ( b ) and ( c ) , the blue crosses show the location of the 123 pseudo - input points .
as discussed earlier , the major difference between our method and these other methods , is that they do not use this marginal likelihood to learn locations of active set input points only the hyperparameters are learnt from ( 123 ) .
this begged the question of what would happen if we tried to use their marginal likelihood approximation ( 123 ) instead of ( 123 ) to try to learn pseudo - input locations by gradient ascent .
we show that the that appears in the spgp marginal likelihood ( 123 ) is crucial for nding pseudo - input points by gradients .
figure 123 shows what happens when we try to optimize these two likelihoods using gradient ascent with respect to the pseudo inputs , on a simple 123d data set .
plotted are the predictive distributions , initial and nal locations of the pseudo inputs .
hyperparameters were xed to their true values for this example .
the initial pseudo - input locations were chosen adver - sarially : all towards the left of the input space ( red crosses ) .
using the spgp likelihood , the pseudo - inputs spread themselves along the extent of the training data , and the predictive distribution matches the full gp very closely ( figure 123 ( b ) ) .
using the plv likelihood , the points begin to spread , but very quickly become stuck as the gradient pushing the points towards the right becomes tiny ( figure 123 ( c ) ) .
figure 123 compares data sampled from the marginal likelihoods ( 123 ) and ( 123 ) , given a partic - ular setting of the hyperparameters and a small number of pseudo - input points .
the major difference between the two is that the spgp likelihood has a constant marginal variance of knn + 123 , whereas the plv decreases to 123 away from the pseudo - inputs .
alternatively , the noise component of the plv likelihood is a constant 123 , whereas the spgp noise grows to knn + 123 away from the pseudo - inputs .
if one is in the situation of figure 123 ( c ) , under the spgp likelihood , moving the rightmost pseudo - input slightly to the right will imme - diately start to reduce the noise in this region from knn + 123 towards 123
hence there will be a strong gradient pulling it to the right .
with the plv likelihood , the noise is xed at 123 everywhere , and moving the point to the right does not improve the quality of t of the mean function enough locally to provide a signicant gradient .
therefore the points become stuck , and we believe this effect accounts for the failure of the plv likelihood in
it should be emphasised that the global optimum of the plv likelihood ( 123 ) may well be a good solution , but it is going to be difcult to nd with gradients .
the spgp likelihood ( 123 ) also suffers from local optima of course , but not so catastrophically .
it may be interesting in the future to compare which performs better for hyperparameter optimization .
in the previous section we showed our gradient method successfully learning the pseudo - inputs on a 123d example .
there the initial pseudo input points were chosen adversarially , but on a real problem it is sensible to initialize by randomly placing them on real data points ,
xy ( a ) xy ( b ) xy ( c ) figure 123 : our results have been added to plots reproduced with kind permission from ( 123 ) .
the plots show mean square test error as a function of active / pseudo set size m .
top row data set kin - 123k , bottom row pumadyn - 123nm123
we have added circles which show spgp with both hyperparameter and pseudo - input learning from random initialisation .
for kin - 123k the squares show spgp with hyperparameters obtained from a full gp and xed .
for pumadyn - 123nm the squares show hyperparameters initialized from a full gp .
random , info - gain and smo - bart are explained in the text .
the horizontal lines are a full gp trained on a subset of the data .
and this is what we do for all of our experiments .
to compare our results to other methods we have run experiments on exactly the same data sets as in seeger et al .
( 123 ) , following precisely their preprocessing and testing methods .
in figure 123 , we have reproduced their learning curves for two large data sets123 , superimposing our test error ( mean squared ) .
seeger et al .
compare three methods : random , info - gain and smo - bart .
random involves picking an active set of size m randomly from among training data .
info - gain is their own greedy subset selection method , which is extremely cheap to train barely more expensive than random .
smo - bart is smola and bartletts ( 123 ) more expensive greedy subset selection method .
also shown with horizontal lines is the test error for a full gp trained on a subset of the data of size 123 for data set kin - 123k and 123 for pumadyn - 123nm .
for these learning curves , they do not actually learn hyperparameters by maximizing their approximation to the marginal likelihood ( 123 ) .
instead they x them to those obtained from the full gp123
for kin - 123k we follow seeger et al . s procedure of setting the hyperparameters from the full gp on a subset .
we then optimize the pseudo - input positions , and plot the results as red squares .
we see the spgp learning curve lying signicantly below all three other methods in figure 123
we rapidly approach the error of a full gp trained on 123 points , using a pseudo set of only a few hundred points .
we then try the harder task of also nding the hyperparameters at the same time as the pseudo - inputs .
the results are plotted as blue circles .
the method performs extremely well for small m , but we see some overtting
123kin - 123k : 123 training , 123 test , 123 attributes , see www . igi . tugraz . at / aschwaig / data . html .
pumadyn - 123nm : 123 training , 123 test , 123 attributes , see www . cs . toronto / delve .
123seeger et al .
have a separate section testing their likelihood approximation ( 123 ) to learn hyper - parameters , in conjunction with the active set selection methods .
they show that it can be used to reliably learn hyperparameters with info - gain for active set sizes of 123 and above .
they have more trouble reliably learning hyperparameters for very small active sets .
123n = 123random 123n = 123infogain 123n = 123smobart 123random 123infogain 123infogain 123smobart figure 123 : regression on a data set with input dependent noise .
left : standard gp .
right : spgp .
predictive mean and two stan - dard deviation lines are shown .
crosses show nal locations of pseudo - inputs for spgp .
hyper - parameters are also learnt .
behaviour for large m which seems to be caused by the noise hyperparameter being driven too small ( the blue circles have higher likelihood than the red squares below them ) .
for data set pumadyn - 123nm , we again try to jointly nd hyperparameters and pseudo - inputs .
again figure 123 shows spgp with extremely low error for small pseudo set size with just 123 pseudo - inputs we are already close to the error of a full gp trained on 123 points .
however , in this case increasing the pseudo set size does not decrease our error .
in this problem there is a large number of irrelevant attributes , and the relevant ones need to be singled out by ard .
although the hyperparameters learnt by our method are reasonable ( 123 out of the 123 relevant dimensions are found ) , they are not good enough to get down to the error of the full gp .
however if we initialize our gradient algorithm with the hyperparam - eters of the full gp , we get the points plotted as squares ( this time red likelihoods > blue likelihoods , so it is a problem of local optima not overtting ) .
now with only a pseudo set of size 123 we reach the performance of the full gp , and signicantly outperform the other methods ( which also had their hyperparameters set from the full gp ) .
another main difference between the methods lies in training time .
our method performs optimization over a potentially large parameter space , and hence is relatively expensive to train .
on the face of it methods such as info - gain and random are extremely cheap .
how - ever all these methods must be combined with obtaining hyperparameters in some way either by a full gp on a subset ( generally expensive ) , or by gradient ascent on an approx - imation to the likelihood .
when you consider this combined task , and that all methods involve some kind of gradient based procedure , then none of the methods are particularly cheap .
we believe that the gain in accuracy achieved by our method can often be worth the extra training time associated with optimizing in a larger parameter space .
123 conclusions , extensions and future work
although gps are very exible regression models , they are still limited by the form of the covariance function .
for example it is difcult to model non - stationary processes with a gp because it is hard to construct sensible non - stationary covariance functions .
although the spgp is not specically designed to model non - stationarity , the extra exibility associated with moving pseudo inputs around can actually achieve this to a certain extent .
figure 123 shows the spgp t to some data with an input dependent noise variance .
the spgp achieves a much better t to the data than the standard gp by moving almost all the pseudo - input points outside the region of data123
it will be interesting to test these capabilities further in the future .
the extension to classication is also a natural avenue to explore .
we have demonstrated a signicant decrease in test error over the other methods for a given small pseudo / active set size .
our method runs into problems when we consider much larger
123it should be said that there are local optima in this problem , and other solutions looked closer to the standard gp .
we ran the method 123 times with random initialisations .
all runs had higher likelihood than the gp; the one with the highest likelihood is plotted .
xystandard gpxyspgp pseudo set size and / or high dimensional input spaces , because the space in which we are optimizing becomes impractically big .
however we have currently only tried using an off the shelf conjugate gradient minimizer , or l - bfgs , and there are certainly improvements that can be made in this area .
for example we can try optimizing subsets of variables iteratively ( chunking ) , or stochastic gradient ascent , or we could make a hybrid by picking some points randomly and optimizing others .
in general though we consider our method most useful when one wants a very sparse ( hence fast prediction ) and accurate solution .
one further way in which to deal with large d is to learn a low dimensional projection of the input space .
this has been considered for gps before ( 123 ) , and could easily be applied to our model .
in conclusion , we have presented a new method for sparse gp regression , which shows a signicant performance gain over other methods especially when searching for an ex - tremely sparse solution .
we have shown that the added exibility of moving pseudo - input points which are not constrained to lie on the true data points leads to better solutions , and even some non - stationary effects can be modelled .
finally we have shown that hyperpa - rameters can be jointly learned with pseudo - input points with reasonable success .
thanks to the authors of ( 123 ) for agreeing to make their results and plots available for repro - duction .
thanks to all at the shefeld gp workshop for helping to clarify this work .
