in microarray studies , an important problem is to compare a predictor of disease outcome derived from gene expression levels to standard clinical predictors .
comparing them on the same dataset that was used to derive the microarray predictor can lead to results strongly biased in favor of the microarray predictor .
we propose a new technique called pre - validation for making a fairer comparison between the two sets of predictors .
we study the method analytically and explore its application in a recent study on breast cancer .
tibshirani and efron : pre - validation and inference in microarrays
a dna microarray dataset typically consists of expression measurements on a large number of genes ( say 123 , 123 ) over a small set of cases ( say 123 ) .
in addition , a true class label is often available for each case , and the objective is to nd a function of the gene expression values that accurately predicts the class membership .
a number of techniques have been proposed for this ( see e . g .
dudoit et al .
( 123 ) ) .
having formed a prediction function from the microarray values , the following problem often arises in a medical context : how do we compare the microarray predictor to an existing clinical predictor of the class membership .
does the new microarray predictor add anything to the clinical predictor ?
an example of this problem arose in the paper of vant veer et al .
( 123 ) .
their microarray data has 123 genes measured over 123 cases , taken from a study of breast cancer .
there are 123 cases in the good prognosis group and 123 in the poor prognosis group .
the microarray predictor was constructed
123 genes were selected , having largest absolute correlation with the 123
using these 123 genes , a nearest centroid classier ( described in detail
in section 123 ) was constructed .
applying the classier to the 123 microarrays gave a dichotomous pre -
dictor zj for each case j .
it was of interest to compare this predictor to a number of clinical pre - dictors including tumor grade , estrogen receptor ( er ) status , progesteron receptor ( pr ) status , tumor size , patient age , and angioinvasion .
the top part of table 123 , labelled re - use , shows the result when a multivariate logis - tic regression was t to the microarray predictor and clinical predictors .
the microarray predictor looks very strong , but this is not surprising as it was derived using the same set of cases that are used in the logistic regression .
in the bottom half of the table , we have derived a pre - validated version of the microarray predictor , and used it in the logistic regression .
now the microarray predictor is much less signicant , and the clinical predictors have
the same idea behind pre - validation was used in the supplementary material for vant veer et al .
( 123 ) , and is the topic of this paper .
it is also
published by the berkeley electronic press , 123
statistical applications in genetics and molecular biology , vol .
123 ( 123 ) , iss .
123 , art
table 123 : results of model tting to breast cancer data .
top panel : re - using the microarray scores zj in the logistic model; bottom panel : using pre - validated scores zj .
the last column is the change in misclassication rate , when the given predictor is deleted from the full model .
the full - model misclassication rates are 123 and 123 for the re - use and pre - validated models respectively .
coef stand .
z score p - value odds ratio mr
similar to the method of stacking due to wolpert ( 123 ) , in the area of
in order to avoid the overtting problem apparent in the top half of table 123 , we might try to use some sort of cross - validation :
divide the cases up into say k approximately equal - sized parts 123
set aside one of parts .
using the other k 123 parts , select the 123 genes
tibshirani and efron : pre - validation and inference in microarrays
having the largest absolute correlation with the class labels , and form a nearest centroid classier .
fit a logistic model to the kth part , using the microarray class predictor
and clinical predictors
do steps 123 and 123 for each of the k = 123 , 123 , .
k parts , and average the
results from the k resulting logistic models .
the main problem with this idea is step 123 , where there will typically be too few cases to t the model .
in the above example , with k = 123 , the 123th part would consist of only 123 or 123 cases .
using a smaller value of k ( say 123 ) would yield a larger number of cases , but then might make the training sets too small in step 123
use of multiple random splits can help cross - validation a little in this case .
pre - validation is a variation on cross - validation that avoids these prob - lems .
it derives a fairer version of the microarray predictor , and then this predictor is t along side the clinical predictors in the usual way .
here is how pre - validation was used in the bottom half of table 123 :
divide the cases up into k = 123 equal - sized parts of 123 cases each .
set aside one of parts .
using only the data from the other 123 parts , select the genes having absolute correlation at least . 123 with the class labels , and form a nearest centroid classication rule .
use the rule to the predict the class labels for the 123th part
do steps 123 and 123 for each of the 123 parts , yielding a pre - validated
microarray predictor zj for each of the 123 cases .
fit a logistic regression model to the pre - validated microarray predic - tor and the 123 clinical predictors .
figure 123 illustrates the logic of this
notice that the cross - validation in steps 123 deals only with the microarray predictor : it creates a fairer version of this predictor , in which the predictor for case j has not seen the true class label for case j .
this pre - validated predictor is then compared to the clinical predictor in the standard way at step 123
the issues behind the choice of k are similar to those in cross -
published by the berkeley electronic press , 123
statistical applications in genetics and molecular biology , vol .
123 ( 123 ) , iss .
123 , art
figure 123 : schematic of pre - validation process .
the cases are divided up into ( say ) 123 equal - sized groups .
the cases in one group are left out , and a mi - croarray predictor is derived from the expression data of the remaining cases .
evaluating this predictor on the left - out cases yields the pre - validated predic - tor z for those cases .
this is done for each of the 123 groups in turn , producing the pre - validated predictor z for all cases .
finally , z can be included along with clinical predictors in a logistic regression model , in order to assess its relative strength in predicting the outcome .
tibshirani and efron : pre - validation and inference in microarrays
validation .
the choice k = n ( leave - one - out ) doesnt perturb the data enough and results in higher variance estimates .
with k = 123 the training sets are too small relative to the full training set .
the values k = 123 or 123 are a good compromise .
123 pre - validation in detail we have microarray data x , a p n matrix of measurements on p genes over n cases .
123 the outcome is an n - vector y , and suppose we also have set of k clinical predictors , represented by a n k matrix c , for predicting y .
let xj
denote the jth column of x .
an expression predictor z = ( z123 , z123 , .
zn ) is adaptively chosen from the
zj = fx , y ( xj ) .
the notation indicates that zj is a function of the data x and y , and is evaluated at xj .
in our motivating example earlier , the function f ( ) consisted
of nding the 123 top correlated genes , and using them to form a nearest centroid classier .
our challenge is to compare c to z , in terms of their strength in predicting y .
the re - use method uses the predictor z as is , and then ts a model to examine the relative contributions of z and c in predicting y .
in our motivating example , we used a logistic model .
clearly this comparison is biased in favor of z , since the outcomes y were already used in the construction of z
it is helpful to consider what we would do given an independent test set ( x 123 , y123 ) with corresponding clinical predictors c123
we could use the test set to derive the predictor z123 = ( z123 j ) and then use this to t a model to predict y123 from z123 and c123
this would allow us to directly examine the relative contributions of z123 and c123 in predicting y123
n ) where z123
j = fx , y ( x123
k - fold pre - validation tries to mimic this , without use of a test set .
we divide the cases into k roughly equal - sized groups , and let g ( k ) be the cases
123it is conventional in the microarray area to arrange the data matrix x with genes ( predictors ) in the rows and cases in the columns .
although this is the transpose of the usual statistical convention , we adopt it here .
accordingly , we index the cases by the
published by the berkeley electronic press , 123
statistical applications in genetics and molecular biology , vol .
123 ( 123 ) , iss .
123 , art
composing each part k .
for k = 123 , 123 , .
k , we form the pre - validated pre -
zg ( k ) = fx
g ( k ) , yg ( k ) ( xg ( k ) ) ; for k = 123 , 123 ,
the notation indicating that cases g ( k ) have been removed from x and y .
finally , we t the model to predict y from z and c , and compare their contributions in this prediction .
123 does pre - validation work ?
the goal of pre - validation is to construct a fairer version of the microarray predictor that acts as if it hasnt seen the response y .
one way to quantify this goal is as follows .
when the pre - validated predictor z is included with clinical predictors in a linear or linear logistic model for predicting y , it should spend one degree of freedom .
for the usual ( non pre - validated ) predictor z , we expect more than one degree of freedom to be spent .
in this section we will make these notions precise and see if pre - validation works in this sense .
as before we dene a microarray predictor as
z ( x ) = fx , y ( x ) ,
let z be the vector of values ( z ( x123 ) , z ( x123 ) , .
z ( xn ) ) and let c be an n by k matrix of k clinical predictors .
using these we t a linear or linear logistic model to predict y , with predicted values
( x , c; z , c , y ) .
the rst two arguments indicate the predicted values are evaluated at x ( a p - vector of expression values ) and c ( a k - vector of clinical predictors ) .
let j = ( xj , cj; z , c , y ) , the predicted values for the training data , and let 123 be the variance of each yj .
following efron et al .
( 123 ) ( see also stein ( 123 ) , ye ( 123 ) ) , we dene the degrees of freedom in the predicted values j to be
df ( ) = e (
the leftmost relation denes degrees of freedom by the total change in the predicted values as each yj is perturbed .
on the right , it is dened as the
tibshirani and efron : pre - validation and inference in microarrays
total self - inuence of each observation on its predicted value .
these two notions are equivalent , as shown in efron ( 123 ) .
he proves that ( 123 ) holds exactly for the gaussian model , and approximately when is an expectation parameter in an exponential family model .
in the special case that f is a linear function of y and the model giving ( x , c; z , c , y ) is linear least squares , we can derive an explicit expression for let z = ay and let m be the n ( k + 123 ) matrix ( z , c ) .
let p project onto the column space of m and pc project onto the column space of c = ( i p ) a , zc = ( i pc ) z and = p y .
then we have alone .
dene a the following results :
= ( k + 123 ) +
|y= = ( k + 123 ) +
the proof is given in the appendix .
the term ( k + 123 ) is the degrees of freedom if z were the usual kind of predictor , constructed independently of the training data .
the second term is the additional degrees of freedom due to the possible dependence of z on
if a is a least squares projection matrix of rank p and there are no clinical predictors ( k = 123 ) , then one can show that the second term in expression ( 123 ) is p 123 , so that the total degrees of freedom is ( 123 + 123 ) + ( p 123 ) = p .
it turns out that leave - one - out pre - validation can also be expressed as a linear operator .
let h be the projection matrix onto the row space of x ( recall that x is p n , with genes in the rows ) .
let d be a diagonal matrix consisting of the diagonal of h , and let i be the n n identity matrix .
then the pre - validated predictors have the form z = ay; with a = ( i d )
( using the sherman - morrison woodbury identity; see e . g .
hastie & tibshi - rani ( 123 ) , chapter 123 ) .
the matrix a has some interesting properties , for example tr ( a ) = 123
our hope here is that with a dened as in ( 123 ) , the additional degrees of freedom ( second term in ( 123 ) ) will be zero .
this is dicult to study analyti - cally , so we turn to a numerical study in the next section .
published by the berkeley electronic press , 123
statistical applications in genetics and molecular biology , vol .
123 ( 123 ) , iss .
123 , art
123 numerical study of degrees of freedom
in this section we carry out some simulations to study the degrees of freedom in pre - validated predictors .
in our rst study , we generated standard independent normal expression measurements on n = 123 or 123 cases and p genes , p ranging from 123 to 123
the outcome y was generated y = xt + where n ( 123 , . 123 ) .
the coecient vector was set to zero in the null case , and n ( 123 , 123 ) in the non - null case .
finally , a samples of independent clinical predictors c n ( 123 , 123 ) was
the linear pre - validation t ( 123 ) was computed from the expression data , and included along with c in a linear least - squares model to predict y .
note that for simplicity , the outcome y was centered and no intercept was included in the linear least squares model .
the mean and standard error of the total degrees of freedom ( formula ( 123 ) ) over 123 simulations is shown in table 123
while the degrees of freedom tends to be less than p+123 ( the value without pre - validation ) , we see it exceeds the ideal value of 123 in the null case , and is less than 123 in the non - null case .
the null case is most bothersome , for in that case treating the microarray predictor as a one degree of freedom predictor will cause us to overestimate its eect ( we have no explanation for this ) .
in the null setting with p = 123 , it is remarkable that the degrees of freedom is actually greater than p + 123 , which is the value for the non pre - validated predictor .
we think of this increase in degrees of freedom as leakage .
while pre - validation makes each value zj independent of its outcome value yj , the outcome yj does inuence other pre - validated values zk , causing some degrees of freedom to leak into the nal t .
one might guess that leakage will tend to disappear as the sample size n gets large .
but that is not borne out in the results for n = 123
the rightmost column of table 123 gives a parametric bootstrap estimate of degrees of freedom .
fixing the expression data , new response values were n ( 123 , 123 ) , 123 being the usual unbiased generated as y estimate of variance .
using ve such sets of bootstrap values , the covariance expression in ( 123 ) was estimated .
we see that the bootstrap does a reasonable job of estimating the degrees of freedom , although sometimes it underesti - mates the actual value .
the bootstrap method can be applied to general situations where the actual value of degrees of freedom is not available .
this estimate would be used to modify the assessment of the likelihood ratio test
tibshirani and efron : pre - validation and inference in microarrays
table 123 : simulation results .
degrees of freedom of pre - validated predictor from formula ( 123 ) and parametric bootstrap estimate .
ideal value is 123
formula ( se ) parametric bootstrap ( se )
null case , n = 123
null case , n = 123
non - null case , n = 123
non - null case , n = 123
table 123 : simulation results - large example .
as in table 123 , with the ideal value for degrees of freedom equal to 123
formula ( se ) parametric bootstrap ( se )
published by the berkeley electronic press , 123
statistical applications in genetics and molecular biology , vol .
123 ( 123 ) , iss .
123 , art
to compare the full model to that containing just the clinical predictors .
in table 123 , we generated a scenario closer to actual microarray experi - ments .
here n = 123 and p = 123 , with the expression values again being standard independent gaussian .
since p > n we cannot use least squares regression to construct the microarray predictor z , so we used pre - validated ridge regression .
as before , the outcome was generated as y = xt + and linear least squares was used for the nal model to predict y from z and c , with n ( 123 , . 123 ) in the non - null case .
in this setup the mapping fx , y ( x ) is not linear , so it is not convenient to use formula ( 123 ) .
hence we computed the covariance expression on the right - hand side of ( 123 ) directly .
the results in table 123 show that leakage is again a problem , in the null case .
our conclusion from these studies is that pre - validation greatly reduces the degrees of freedom of the microarray predictor , but does not reliably reduce it to the ideal value of one .
hence we recommend that for each application of pre - validation , a parametric bootstrap be used to estimate the degrees of freedom .
this is illustrated in the breast cancer example in the
123 further analysis of the breast cancer data
we re - analyzed the breast cancer data from vant veer et al .
( 123 ) .
the authors use the following steps :
starting with 123 , 123 genes , they apply ltering based on fold - change and a p - value criterion , to yield 123 genes ( personal communication
they select the genes have absolute correlation . 123 with the class
labels , giving 123 genes
they nd the 123 - dimensional centroid vector for the 123 good - prognosis
they compute the correlation of each case with this centroid and choose a cuto value so that exactly 123 of the poor groups are misclassied .
this value turned out to be . 123
finally they classify to the good prognosis group if the correlation with the normal centroid is . 123 , otherwise they classify to the poor prognosis group .
tibshirani and efron : pre - validation and inference in microarrays
table 123 : odds ratios from pre - validated data
friend et .
our analysis
starting with the top 123 , 123 , 123 .
genes , they carried out this classica - tion procedure with leave - one - out cross - validation , to pick the optimal number of genes .
reporting an optimal number of 123
even with some help from the authors , we were unable to exactly reproduce this analysis .
at stages 123 and 123 , we obtained 123 and 123 genes , respectively .
the authors told us they could not release their list of 123 genes for legal reasons .
we xed the number of genes ( 123 ) in step 123
the authors carry out what we call a pre - validation analysis in the sup - plementary material to their paper .
table 123 shows the odds ratios from the pre - validated data , both from their analysis and our .
the odds ratios for the microarray predictor dier greatly , and we were unable to reproduce their
we estimated the degrees of freedom for the full model in table 123 nu - merically , using the bootstrap method of the previous section , obtaining an estimate of 123 , with a standard error of . 123
the nominal value would be 123 ( 123 predictors plus the intercept ) , so about one degree of freedom has leaked .
in the remainder of this section we apply full cross - validation to this dataset pre - validation is a partial form of cross - validation that is especially convenient when the microarray score z might be applied in a wide variety of possible models .
the model illustrated in figure 123 can be described as follows : for each
case j , a score zj is constructed from the 123 123 microarray data matrix
x and the 123 dimensional response vector y , say
published by the berkeley electronic press , 123
statistical applications in genetics and molecular biology , vol .
123 ( 123 ) , iss .
123 , art
zj = f ( xj|x , y )
according to algorithm ( 123 ) ( 123 ) at the beginning of this section .
the nota - tion in ( 123 ) indicates that ( x , y ) determines the form of the rule f , which is then evaluated at the 123 - vector xj for that cases microarray .
the nal prediction for case j is
( cid : 123 ) j = g ( cj , zj|c , z ) .
here g is the logistic regression rule based on the 123 123 matrix of covariates c and the vector z of 123 z scores , then evaluated at ( cj , zj ) , the vector of 123
covariates and zj ( the top panel of table 123 was based on the predictions ( cid : 123 ) j ) .
full cross - validation modies ( 123 ) ( 123 ) so that the data for case j is not likewise c ( j ) , y ( j ) etc .
the cross - validated predictor ( cid : 123 ) j is obtained as involved in constructing the form of the rules for its own prediction .
let x ( j ) indicate the 123 123 matrix obtained by deleting column xj from x , and
( cid : 123 ) zj = f ( xj|x ( j ) , y ( j ) ) ( cid : 123 ) j = g ( cj , ( cid : 123 ) zj|c ( j ) , ( cid : 123 ) z ( j ) ) .
( by contrast , the pre - validated predictors used in the bottom of table 123
if q ( yj , j ) is a measure of error for predicting that outcome yj by ( cid : 123 ) j ,
full cross - validation permits an almost unbiased estimate of the predic - tion error we would obtain if rule ( 123 ) were applied to an independent test
is nearly unbiased for the expected error rate of rule ( cid : 123 ) j applied to an inde -
pendent set of test cases , see efron & tibshirani ( 123 ) and efron ( 123 ) .
table 123 refers to the error function
tibshirani and efron : pre - validation and inference in microarrays
if yj = 123 and ( cid : 123 ) j 123 / 123 or yj = 123 and ( cid : 123 ) j > 123 / 123 then q ( yj , ( cid : 123 ) j ) = 123
and q ( yj , ( cid : 123 ) yj ) = 123 otherwise .
in other words , ( cid : 123 ) err is the proportion of pre -
diction errors ( with the prediction threshold set at 123 / 123 rather than 123 / 123 to reect the 123 / 123 division of cases in the training cases . )
table 123 : estimates of prediction error for two logistic regression models : c alone ( only the 123 covariates ) and c plus z ( 123 covariates plus the microarray predictor z . ) naive reuse method suggests that adding z cuts the error rate nearly in half , from 123% to 123% .
most of the apparent improvement disappears under full cross - validation , where now the comparison is 123% versus 123% .
bootstrap methods give similar results .
the standard errors were obtained from jackknife calculations , and show that this experiment was too small to detect genuine dierences of less than about 123% .
c plus z :
table 123 compares the prediction error rates ( 123 ) from two logistic re - gression models : one based on just the six covariates in c , the other using these plus the microarray predictor z .
the naive reuse error rates make z
look enormously helpful , reducing ( cid : 123 ) err from 123% to 123% .
most of zs advantage disappears under cross - validation , giving ( cid : 123 ) err ( c ) = 123% versus ( cid : 123 ) err ( c , z ) = 123% , for a dierence of only 123% .
another important point is clear from table 123 : the cross - validated dier - ence of 123% has an estimated standard error of 123% .
in other words there is not enough data in the vant veer et al .
study to establish a prediction advantage for z of less than say 123% even if it exists ( which does not appear to be the case . )
published by the berkeley electronic press , 123
statistical applications in genetics and molecular biology , vol .
123 ( 123 ) , iss .
123 , art
cases each , so that calculations ( 123 ) - ( 123 ) produced ( cid : 123 ) zj and ( cid : 123 ) j values six at a
the cross - validation in table 123 grouped the 123 cases into 123 groups of 123
the jackknife was used to calculate standard errors : a typical jackknife pseudo - value deleted one of the 123 groups and repeated the cross - validation calculations using just the data from the other 123 groups , nally obtaining the standard error as in ( 123 ) or ( 123 ) of efron & tibshirani ( 123 ) .
cross - validation can be overly variable for estimating prediction error ( 123 ) , see efron ( 123 ) .
table 123 also reports two bootstrap estimates de - scribed in efron & tibshirani ( 123 ) : zero - boot and the 123+ rule ,
equations 123 and 123 - 123 of that paper , with ( cid : 123 ) = . 123 ) , the latter having a par -
ticularly good track record .
the story here does not change much , though 123+ gives a slightly larger dierence estimate , 123% , with a smaller stan - dard error , 123% .
bootstrap estimates were based on b = 123 bootstrap replications , with standard errors estimated by the jackknife - after - bootstrap computations of efron ( 123 ) .
in this paper we have analyzed pre - validation , a technique for deriving a fairer version of an adaptively chosen predictor .
it seems especially well - suited to microarray problems .
the promise of pre - validation is that the resulting predictor will act similarly to one that has been derived from an independent dataset .
hence when included in a model with clinical predictors , it should have have one degree of freedom .
this is contrast to the usual ( non pre - validated ) predictor , which has degrees of freedom equal to the total number of parameters t in each of the two stages of the analysis .
we have found that pre - validation is only partially successful in achieving its goal .
generally it controls the degrees of freedom of the predictor , as compared to the non pre - validated version .
however in null situations where the microarray predictor is independent of the response , degrees of freedom can leak from one case to another , so that the total degrees of freedom of the pre - validated predictor is more than the ideal value of one .
conversely , in non - null settings , the total degrees of freedom of the pre - validated predictor can be less than expected .
overall we recommend use of the parametric bootstrap , to estimate the degrees of freedom of the pre - validated predictor .
with the estimated value of degrees of freedom in hand , one can use the pre - validated predictor along with clinical predictors , in a model to compare
tibshirani and efron : pre - validation and inference in microarrays
their predictive accuracy .
finally , while pre - validation is a promising method for building and as - sessing an adaptive predictor on the same set of data , it is no substitute for full cross - validation or test set validation , in situations where there is sucient data to use these techniques .
acknowledgments : tibshirani was partially supported by nih grant 123 r123 ca123 , and nsf grant dms - 123
efron was partially supported by nih grant 123r123 ca123 and nsf grant dms - 123
appendix : proof of formula ( 123 )
z = ay for a xed n n matrix a , and then ( cid : 123 ) = p y where p is the
formula ( 123 ) concerns pre - validation in linear model situations .
we compute n n projection matrix into the linear space spanned by the columns of the n ( k + 123 ) matrix m = ( c , z ) , c being the n k matrix of xed covariates :
p = m g
( g = m t m ) .
notice that p = p ( y ) is not xed , being a function of y though z .
= i p , a
an innitesimal change in the response vector , y y + dy , changes z by amount dz = ady , which we can write as
and z ( c ) = ( i c ( ct c ) dz d ( cid : 123 ) z + dz
dz = p dz + p
the resulting change in p is calculated as follows .
dp = ( dz
component d ( cid : 123 ) z has no eect on the projection matrix p since it preserves the
changes in y aect p only through changes in z .
moreover , the
( c ) + z ( c ) dz
linear space m , so we can consider dp to be a function of only dz change in g = m t m is zero to rst order ,
g + dg = ( c , z + dz
) t ( c , z + dz
) = g ,
published by the berkeley electronic press , 123
statistical applications in genetics and molecular biology , vol .
123 ( 123 ) , iss .
123 , art
since ct dz
= 123 = zt dz
p + dp
( ( c , z ) + ( 123 , dz
= p + ( 123 , dz
dp = dz
here we have partitioned g
+ ( c , z ) g
+ ( c , z )
( with g123 = g123t ) .
then , also partitioning g ,
v = ( c , z )
ct v = ( g123 , g123 )
( a123 ) following from ( cid : 123 )
= 123 and zt v = ( g123 , g123 )
since v is a linear combination of z and the columns of c , ct v = 123 shows that v must lie along the projection of z orthogonal to c , that is v = z ( c ) .
also zt v = 123 implies = 123 / ( cid : 123 ) z ( c ) ( cid : 123 ) 123 , or v = z ( c ) / ( cid : 123 ) z ( c ) ( cid : 123 ) 123
the lemma follows
lemma ( a123 ) combined with dz
ii z ( c ) j + a
tibshirani and efron : pre - validation and inference in microarrays
pijyj = pii +
= ( k + 123 ) +
ii z ( c ) j + a
which is the top version of ( 123 ) .
the bottom version follows since
z ( c ) = ( ( cid : 123 ) t p
) ( az ( c ) ) = 123t ( az ( c ) ) = 123
it is not necessary for the rst mapping z = f ( y ) to be linear .
result ( 123 ) holds in the nonlinear case if a is dened to be the matrix of
a = ( zi / yj ) .
