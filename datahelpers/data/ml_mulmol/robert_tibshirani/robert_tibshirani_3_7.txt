we consider the problems of estimating the parameters as well as the structure of binary - valued markov networks .
for maximizing the penalized log - likelihood , we implement an approximate procedure based on the pseudo - likelihood of besag ( 123 ) and generalize it to a fast exact algo - rithm .
the exact algorithm starts with the pseudo - likelihood solution and then adjusts the pseudo - likelihood criterion so that each additional iterations moves it closer to the exact solution .
our results show that this procedure is faster than the competing exact method proposed by lee , gana - pathi , and koller ( 123a ) .
however , we also nd that the approximate pseudo - likelihood as well as the approaches of wainwright et al .
( 123 ) , when implemented using the coordinate descent procedure of friedman , hastie , and tibshirani ( 123b ) , are much faster than the exact methods , and only slightly less accurate .
keywords : markov networks , logistic regression , l123 penalty , model selection , binary variables
||123 is maximized , where is the gaussian log - likelihood , ||q
in recent years a number of authors have proposed the estimation of sparse undirected graphical models for continuous as well as discrete data through the use of l123 ( lasso ) regularization .
for continuous data , one assumes that the observations have a multivariate gaussian distribution with .
then an l123 penalty is applied to q = s 123
that is , the penalized mean and covariance matrix s log - likelihood ( q ) r ||q ||123 is the sum of the absolute values of the elements of q and r 123 is a user - dened tuning parameter .
several papers proposing estimation procedures for this gaussian model have been published .
meinshausen and buhlmann ( 123 ) develop a lasso based method for estimating the graph structure and give theoretical consistency results .
yuan and lin ( 123 ) , banerjee et al .
( 123 ) and dahl et al .
( 123 ) as well as friedman , hastie , and tibshirani ( 123a ) propose algorithms for solving this penalized log - likelihood with the procedure in friedman , hastie , and tibshirani ( 123a ) , the graphical lasso , being especially fast and efcient .
here we focus on estimation of networks of discrete , more specically binary - valued , units with pairwise interactions .
these are a special class of markov networks .
the use of l123 penalties for this
c ( cid : 123 ) 123 holger hoing and robert tibshirani .
h ofling and tibshirani
figure 123 : a simple graph for illustration .
table 123 : sample data for graph of figure 123
special class as well as more general markov networks was proposed by lee , ganapathi , and koller ( 123a ) .
this problem is more difcult than the continuous gaussian version because of the need to compute the rst and second moments under the model , which are derivatives of the log - partition function .
figure 123 shows an example , and table 123 shows some sample data from this graph .
given this data and a model for binary graphical data ( detailed later ) , we would like to a ) infer a structure something like that of figure 123 , and b ) estimate the link parameters itself .
for the data in table 123 , figure 123 shows the path of solutions for varying penalty parameter .
most edges for l123 - norm 123 are correctly identied as in the graph in figure 123
however , edge ( 123 , 123 ) is absent in the true model but included in the l123 penalized model relatively early .
our main focus in this paper is to develop and implement fast approximate and exact proce - dures for solving this class of l123 - penalized binary pairwise markov networks and compare the accuracy and speed of these to other methods proposed by lee , ganapathi , and koller ( 123a ) and wainwright et al .
( 123 ) .
here , by exact procedures we refer to algorithms that nd the exact maximizer of the penalized log - likelihood of the model whereas approximate procedures only nd an approximate solution .
in section 123 we describe the ising model as well as the details of the competing methods of lee , ganapathi , and koller ( 123a ) and wainwright et al .
( 123 ) .
section 123 then introduces the basic pseudo - likelihood model and outlines the computational approach to increase the speed of the algorithm .
the pseudo - likelihood is a very interesting and fast approximate method which has
sparse markov networks
figure 123 : toy example : proles of estimated edge parameters as the penalty parameter is varied .
the added advantage that it can be used as a building block to a new algorithm for maximizing the penalized log - likelihood exactly .
the adjustments necessary to achieve this are described in section 123
finally , section 123 discusses the results of the simulations with respect to speed and accuracy of the competing algorithms .
the model and competing methods
in this section we briey outline the competing methods for maximizing the penalized log - likelihood .
apart from the method proposed in lee , ganapathi , and koller ( 123a ) , which we already men - tioned above , we also discuss a very simple solution that was presented in wainwright et al .
( 123 ) .
we rst describe the underlying model in more detail .
consider data generated under the ising
p ( x , q ) = exp " ( cid : 123 )
q sxs + ( cid : 123 )
q stxsxt y
for a single observation x = ( x123 , .
, xp ) t ( 123 , 123 ) p and model parameters q s and q st for s , t v = ( q ) is the normalization ( 123 , .
here , v denotes the vertices and e the edges of a graph .
constant , which is also known as the log - partition function .
by setting q st = 123 for ( s , t ) 123 e , and using the fact that x is a binary vector we can write the log - likelihood as
l ( x , q ) = log p ( x , q ) =
q stxsxt y
h ofling and tibshirani
is a symmetric p p matrix with q ss = q s for s = 123 , .
note that for notational conve - nience , we do not distinguish between q st and q the log - likelihood only uses the lower - triangular matrix of q .
for the l123 - penalty let r be a p p lower triangular matrix of penalty parameters .
the penalized log - likelihood for all n observations
ts and therefore we enforce symmetry of q
( xt x ) stq st ny
( q ) n||r q
where denotes component - wise multiplication .
the algorithms that we will discuss in the following sections could be generalized to more general categorical variables or higher order interaction terms than those included in the ising model .
however , as we will see , solving these problems exactly is already computationally demanding in the pairwise binary case so that we chose not to adapt the algorithms to these more general settings .
123 the lee , ganapathi , and koller ( 123a ) method
the penalized log - likelihood is a concave function , so standard convex programming techniques can be used to maximize it .
the main difculty is that the log - partition function y ( q ) is the sum over 123p elements , and therefore it is computationally expensive to calculate y or its derivatives in general .
however , for sparse matrices q , algorithms such as the junction tree algorithm exist that can calculate y and its derivatives efciently .
therefore , it is especially important to maintain sparsity of q for any optimization method .
lee , ganapathi , and koller ( 123a ) achieve this by optimizing the penalized log - likelihood only over a set f of active variables which they gradually enlarge until an optimality criterion is satised .
to be more precise , they start out with a set of active variables f = f123 ( e . g . , the diagonal of if it is unpenalized ) .
using either conjugate gradients or bfgs , the penalized log - likelihood is maximized over the set of variables f .
then one of the currently inactive variables is selected by the grafting procedure ( see perkins et al . , 123 ) and added to the set f .
these steps are repeated until grafting does not add any more features .
the algorithm can be used for more general markov networks , but for ease of implementation they choose to work with binary random variables and we do the same as well .
their procedure provides an exact solution to the problem when the junction tree algorithm is used to calculate y ( q ) and its derivatives .
in their implementation , however , they used loopy belief propagation , which is faster on denser matrices , but only provides approximate results .
in their method as well as ours , any procedure to evaluate y ( q ) can be plugged in without any further changes to the rest of the algorithm; we decided to evaluate the speed and performance of only the exact algorithms .
the relative performance of an approximate method using loopy belief propagation would likely be similar .
they also provide a proof that under certain assumptions and using the l123 regularized log - likelihood , it is possible to recover the true expected log - likelihood up to an error e .
123 the wainwright et al .
( 123 ) method
wainwright et al .
( 123 ) propose estimation of the markov network by applying a separate l123 - penalized logistic regression to each of the p variables on the remaining variables .
for every s v regress xs onto x\s = ( x123 , .
, xs123 , xs+123 , .
, xp ) t .
let the p p matrix q
denote the estimate of q
sparse markov networks
then set q ss = b 123 , the intercept of the logistic regression , and q st = b associated with xt in the regression .
t , where b
t is the coefcient
in the outline above , we assumed that q
is not necessarily symmetric .
we investigate two methods for symmetrizing q
is a symmetric matrix .
however , due to the way it was
rst way is to dene q
q st = q
ts = ( q st
if |q st| > |q if |q st| |q
which we call wainwright - max .
similarly , wainwright - min is dened by
q st = q
ts = ( q st
if |q st| < |q if |q st| |q
wainwright et al .
( 123 ) mainly intended their method to be used in order to estimate the presence or absence of an edge in the underlying graph of the model .
they show that under certain assumptions , their method correctly identies the non - zero edges in a markov graph , as n even for increasing number of parameters p or neighborhood sizes of the graph d , as long as n grows more quickly than d123 log p ( see wainwright et al . , 123 ) .
due to the simplicity of the method it is obvious that it could also be used for parameter estimation itself and here we will compare its performance in these cases to the pseudo - likelihood approach proposed below and the exact solution of the penalized log - likelihood .
furthermore , as an important part of this article is the comparison of the speeds of the underlying algorithms , we implement their method , using the fast coordinate descent algorithm for logistic regression with a lasso penalty ( see friedman , hastie , and tibshirani , 123b ) .
pseudo - likelihood model
in this section we rst introduce an approximate method to infer the structure of the graph that is based on pseudo - likelihoods ( besag , 123 ) .
as we will see in the simulations section , the results are very close to the exact solution of the penalized log - likelihood .
in the next section , we use the pseudo - likelihood model to design a very fast algorithm for nding an exact solution for the penalized ising model .
the main computational problem in the ising model is the complexity of the partition function .
one possibility in this case is to solve an approximate version of the likelihood instead .
approaches of this kind have been proposed in various papers in the statistical literature before , for example the pseudo - likelihood approach of besag ( besag , 123 ) and the treatments of composite likelihoods in lindsay ( 123 ) and cox and reid ( 123 ) among others .
here , we want to apply the approximation proposed in besag ( 123 ) to our problem .
this approach is also related to the method of wainwright et al .
( 123 ) , however instead of performing separate logistic regressions for every column of the parameter matrix q , the pseudo - likelihood approach here allows us to estimate all parameters at the same time .
this way , the resulting matrix q is symmetric and no additional step like the max or min - rule described above is necessary .
the log - pseudo - likelihood is then given by
|x\s ) = xi ( q ss + ( cid : 123 )
xtq st ) y
h ofling and tibshirani
s ( x , q ) is the log - normalization constant when conditioning xs on the other variables , which
is exactly the same as in logistic regression with a logit link - function , that is ,
s ( x , q ) = log ( 123 + exp ( q ss + ( cid : 123 )
where as above for notational convenience we set q st = q pseudo - likelihood for a single observation x is given by
ts for t 123= s .
putting all this together , the
in the usual way , the pseudo - likelihood for all n observations is given by the sum of the pseudo - likelihood of the individual observations
where xk is the kth row of matrix with observations x rnp .
as this is just a sum of logistic likelihoods , the pseudo - likelihood is a concave function and
therefore the l123 - penalized pseudo - likelihood
|xk ) n||s q
is concave as well .
here s = 123r diag ( r ) and is chosen to be roughly equivalent to the penalty terms in the penalized log - likelihood .
the penalty term is doubled on the off - diagonal , as the derivative of the pseudo - likelihood on the off - diagonal is roughly twice as large as the derivative of the log - likelihood ( see equation 123 ) .
123 basic optimization algorithm
due to its simple structure , a wide range of standard convex programming techniques can be used to solve this problem , although the non - differentiability of the l123 penalty poses a problem .
here , we want to use a local quadratic approximation to the pseudo - likelihood .
as the number of variables is p ( p + 123 ) / 123 , the hessian could get very large , we restrict our quadratic approximation to have a
in order to construct the approximation , we need the rst and second derivative of l w . r . t q st .
s x ) t ( pt
= 123 ( xt x ) st ( pt
= ( xt x ) ss
s 123= t ,
where 123 psk = 123 / ( 123 + exp ( q ss + ( cid : 123 )
s123=t xktq st ) ) .
the second derivatives are
st ) 123 = ( xt diag ( ps ) diag ( 123 ps ) x ) tt ( xt diag ( pt ) diag ( 123 pt ) x ) ss
s 123= t ,
sparse markov networks
algorithm 123 : estimation for l123 penalized pseudo - likelihood
( 123 ) = diag ( logit ( p ( 123 ) ) ) where p ( 123 )
( 123 ) not given then
while not converged do
with current estimate q find solution q perform backtracking line search on the line from q
( k ) , dene local approximation fq
( k ) ( q ) to l n||s q
( k ) to q
to nd q
then dene the local approximation
assume that at the k - th step the parameter estimate is q
|x ) n||s q
( k ) ( q ) = c + ( cid : 123 )
( q st q ( k )
st ) +
st ) 123 ( q st q ( k )
st ) 123 n||s q
where c is some constant .
as stated above , this is just a quadratic approximation with linear term equal to the gradient of l and a diagonal hessian with diagonal elements equal to the diagonal of the hessian of l .
the main reasons for using this simple structure are that it keeps the computation complexity per iteration low and it is very easy to solve this l123 penalized local approximation .
let q be the minimizer of the unpenalized fq
( k ) ( q ) , then
q st = q ( k )
as the hessian is diagonal , the l123 - penalized solution q st = sign ( q st ) ( cid : 123 ) q st sst /
( k ) ( q ) can be obtained by soft thresh -
, the next step q
( k+123 ) can now be obtained by , for example , a backtracking line search .
the whole algorithm can be seen in algorithm 123 and a proof of convergence that closely follows lee , lee , abbeel , and ng ( 123b ) is given in the appendix .
123 speed improvements
in practice , there are several things that can be done to speed up the algorithm given above .
first of all , as q will in general be sparse , all computations to calculate p should exploit this sparse structure .
however , the sparseness of q
can also be used in another way :
123 . 123 using active variables
as the solutions are usually sparse , calculating the gradient for all variables in every step is wasteful .
most variables are zero and will not change from one iteration to the next .
in order to be more
h ofling and tibshirani
algorithm 123 : pseudo - likelihood algorithm using active variables
( 123 ) not given then
( 123 ) = diag ( logit ( p ( 123 ) ) ) where p ( 123 )
set a = ( ( s , t ) : s t , q st 123= 123 ) as active variables;
while not converged over variables in a do
( k+123 ) using local approximation over variables in a;
set a =n ( s , t ) : q st 123= 123 or ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
until a did not change ;
efcient , it is possible to move variables that are zero only once in a while .
several different kinds of methods have been proposed to exploit this situation , for example grafting ( perkins et al . , 123 ) and the implementation of the penalized logistic regression in friedman , hastie , and tibshirani ( 123b ) among others .
in our case here , we use an outer and an inner loop .
the outer loop decides which variables are active .
the inner loop then optimizes over only the active variables until convergence occurs .
active variables are those that are either non - zero , or that have a gradient large enough so that they would become non - zero in the next step .
more details are given in algorithm 123
when using this method , convergence is still guaranteed .
in the outer loop , the criterion chooses variables to be active that are either non - zero already or will be non - zero after one step of the local approximation over all variables .
therefore , if the active set stays the same , no variables would be moved in the next step as all active variables are already optimal and therefore , we have a solution over all variables .
however , if the active set changes , the inner loop is guaranteed to improve the penalized pseudo - likelihood and nd the optimum for the given set of active variables .
as there are only a nite number of different active variables sets , the algorithm has to converge after a nite number of iterations of the outer loop .
123 . 123 substituting the line search
calculating the pseudo - likelihood is computationally expensive compared to the cost of the local approximation .
therefore , we save time by not performing a line search after every step .
instead , we x a step size g and skip the line search .
however , with this method , the algorithm may diverge .
in order to detect this , we calculate the penalized pseudo - likelihood every 123 iterations and check that we improved during the last 123 steps .
if yes , the step size remains the same .
if no , reset the variables to where they were 123 steps ago and divide the step size by 123
if the step size drops below a pre - specied threshold , we revert to the original line - search algorithm .
this way , in most cases , we do not have to perform the line - search .
however , the algorithm is still guaranteed to converge as it automatically detects convergence problems when the xed step size is used and reverts to the line - search algorithm , which as stated above is guaranteed to converge .
sparse markov networks
exact solution using pseudo - likelihood
we now turn to our new method for optimizing the penalized log - likelihood .
as the log - likelihood is a concave function , there are several standard methods that can be used for maximizing it , for example , gradient descent , bfgs and newtons method among others .
for each step of these methods , the gradient of the log - likelihood has to be calculated .
this requires the evaluation of the partition function and its derivatives , which is computationally much more expensive than any other part of the algorithms .
taking this into considerations , the standard methods mentioned above have the following drawbacks :
gradient descent : it can take many steps to converge and therefore require many evaluations of the partition function and its derivatives ( using the junction tree algorithm ) .
also , it does not control the number of non - zero variables in intermediate steps well to which the runtime of the junction tree algorithm is very sensitive .
therefore , intermediate steps can take very long .
bfgs : takes less steps than gradient descent , but similar to gradient descent , intermediate steps can have more non - zero variables than the solution .
thus , same as above , computations of intermediate steps can be slow .
newtons method : in order to locally t the quadratic function , the second derivatives of the log -
likelihood are needed .
computing these is computationally prohibitive .
lee , ganapathi , and koller ( 123a ) use the bfgs method only on a set of active variables onto which additional variables are grafted until convergence .
this mitigates the problem of slow intermediate steps and makes using bfgs feasible .
however , this comes at the expense of an increased total number of steps , as only one variable at a time is being grafted .
here , we want to use the pseudo - likelihood in a new algorithm for maximizing the penalized log - likelihood .
the functional form of the pseudo - likelihood is by its denition closely related to the real like - lihood and in section 123 we will see that its solutions are also very similar , indicating that it approxi - mates the real likelihood reasonably well .
furthermore , we can also maximize the pseudo - likelihood very quickly .
we want to leverage this by using a quasi - newton method in which we t a tilted pseudo - likelihood instead of a quadratic function .
specically , among all functions of the form
ast ( q st q ( k )
st ) g
( q st q ( k )
st ) 123 n||r q
we t the one that at q ( k ) is a rst order approximation to the penalized log - likelihood l .
essen - ( k ) is an l123 - penalized pseudo - likelihood with an added linear term as well as a very simple quadratic term for some g > 123
here , ast will be chosen so that the sub - gradient is equal to the penal - ized log - likelihood l at q is only being included to ensure the existence of a global maximum .
in practice , we can set g = 123 , unless a convergence problem occurs .
in our simulations , g = 123 always worked very well .
the additional quadratic term with coefcient g
h ofling and tibshirani
in particular , for the approximation at q
( k ) , choose ast such that
( q st q ( k )
ss ) ( cid : 123 ) ( q ss q ( k ) st ) 123 n||r q g ( q st q ( k )
s x ) t + ( p ( q
( k ) ) + ( xt x ) ss 123 n wss ( q
t x ) s 123 n wst ( q
where w is a matrix with elements wst ( q ) = function and ps is as dened in the pseudo - likelihood section .
for the algorithm , we need an initial parameter estimate q
( q ) = eq
( xsxt ) is the derivative of the partition
( 123 ) , which we pick as follows : let
z ( q ) = eq
123+eq be the logistic function and let z123 denote its inverse .
then choose
in this case we than have psk = 123
and together with g = 123 we then get that
wst = ( ( cid : 123 ) 123
if s 123= t if s = t
k=123 xks k and also
k=123 xks ( cid : 123 ) ( cid : 123 ) 123
|x ) n||r q
if s 123= t if s = t
and thus just a regular pseudo - likelihood step .
the only slight difference is that in this case the penalty term on the diagonal is twice as large as in the pseudo - likelihood case presented above .
however , in practice we recommend not to penalize the diagonal at all , so that this difference vanishes .
therefore , this algorithm is a natural extension of the pseudo - likelihood approach that starts out by performing a regular pseudo - likelihood calculation and then proceeds to converge to the solution by a series of adjusted pseudo - likelihood steps .
the algorithm for maximizing the penalized log - likelihood is now very similar to the one pre - sented for maximizing the penalized pseudo - likelihood .
assume our current estimate is q ( k ) .
this is essen - approximate the log - likelihood locally by fq tially the pseudo - likelihood and the algorithm presented above can easily be adjusted to accommo - date the additional terms .
now , using a line search on the line between q , nd the next ( k+123 ) .
this algorithm is guaranteed to converge by the same argument as the pseudo - ( k ) to rst order can be found in likelihood algorithm .
the proof that fq appendix b , which is a prerequisite for the convergence proof of the algorithm in appendix a .
( k ) and nd the maximizer q
( k ) approximates l at q
( k ) and q
123 speed improvement
as for the pseudo - likelihood algorithm , we can again save computations by using the active vari - ables technique presented above .
here , the savings in time are especially large due to a special fea - ture of the junction tree algorithm that we use to calculate the derivatives of the partition function .
sparse markov networks
algorithm 123 : likelihood algorithm using active variables
( 123 ) not given then
( 123 ) = diag ( logit123 ( p ( 123 ) ) ) where p ( 123 )
set a = ( ( s , t ) : s t , q st 123= 123 ) as active variables;
while not converged over variables in a do
calculate w only over variables in a;
( k+123 ) using local approximation over variables in a;
calculate the whole matrix w;
set a =n ( s , t ) : q st 123= 123 or ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
until a did not change ;
in order to calculate derivatives with respect to non - zero variables , only one pass of the junction tree algorithm is necessary .
however , p passes of the junction tree are needed in order to get the full matrix of derivatives w .
therefore , depending on the size of p , using only active variables can be considerable faster .
for details , see algorithm 123
123 graphical lasso for the discrete case
in the case of gaussian markov networks , the graphical lasso algorithm ( see friedman , hastie , and tibshirani , 123a ) is an very efcient method and implementation for solving the l123 - penalized log - likelihood .
in order to leverage this speed , we extended the methodology to the binary case treated in this article .
however , the resultant algorithm was not nearly as fast as expected .
in the gaussian case , the algorithm is very fast as the approach to update the parameter matrix q one row at a time allows for a closed form solution and efcient calculations .
in the binary case on the other hand , the computational bottleneck is not all of the calculation involved in the update of q , but specically by a large margin the evaluations of the partition function itself .
therefore , any fast algorithm for solving the penalized log - likelihood exactly has to use as few evaluations of the partition function as possible .
the graphical lasso approach is thus not suitable for the binary case as it takes a lot of small steps towards the solution .
this observation also explains the improvement in speed of the pseudo - likelihood based exact algorithm over the specic methods proposed in lee , lee , abbeel , and ng ( 123b ) .
standard convex optimization procedures often rely on either the rst or second derivatives of the functions they seek to optimize .
newton - like procedures that use the second derivative often converge in very few steps , however these cannot be used here as it is prohibitively expensive to evaluate the second derivative of the partition function , even in small examples .
approaches like conjugate gradients of bfgs as proposed in lee , lee , abbeel , and ng ( 123b ) are somewhat less efcient and take more steps .
this is where the advantage of using the pseudo - likelihood as a local approximation comes into play .
it usually only takes very few steps to converge and is therefore faster than standard
h ofling and tibshirani
simulation results
in order to compare the performance of the estimation methods for sparse graphs described in this article as well as lee , ganapathi , and koller ( 123a ) and wainwright et al .
( 123 ) , we use simulated data and compare the speed as well as the accuracy of these methods .
before simulating the data it is necessary to generate a sparse symmetric matrix q .
first , the di - agonal is drawn uniformly from the set ( 123 , 123 , 123 ) .
then , using the average number of edges per node , upper - triangular elements of q are drawn uniformly at random to be non - zero .
these non - zero elements are then set to either 123 or 123 , again each uniformly .
in order for q symmetric , the lower triangular matrix is set equal to the upper triangular matrix .
the actual data is generated by gibbs sampling using q
as described above .
with respect to the penalty parameters that we use for the different methods , we always leave
the diagonal unpenalized and all off - diagonal elements have the same parameter r
, that is we set
if s = t
and the penalty term matrix for the pseudo - likelihood s = 123r diag ( r ) as dened above .
for the wainwright - methods , the penalty parameter is r with no penalty on the intercept .
although the log - likelihood functions that are being penalized are somewhat different , this choice of parameters makes them perform roughly equivalent , as can be seen in figure 123
the number of edges is plotted against the penalty parameter used and all methods behave very similar .
however , in order not to confound some results by these slight differences of the effects of the penalty , all the following plots are with respect to the number of edges in the graph , not the penalty parameter itself .
123 speed comparison
first , we compare the speed of the four methods for the l123 - penalized model .
we used an an - nealing schedule for lee , ganapathi , and koller ( 123a ) to improve convergence as suggested in their article .
plots of the speeds of the exact methods can be seen in figure 123 and the approxi - mate methods are shown in figure 123
each plot shows the time the algorithm needed to converge versus the number of edges in the estimated graph .
as can be seen , the pseudo - likelihood based exact algorithm described above is considerable faster than the one proposed in lee , ganapathi , and koller ( 123a ) .
for the approximate algorithms , we can see that the p logistic regressions in the wainwright et al .
( 123 ) algorithm take roughly the same amount of time as the pseudo - likelihood algorithm presented above .
this is not surprising due to the similarity of the optimization methods and any difference that can be observed in figure 123 is mostly due to the specic implementations used .
furthermore , we would like to note that we decided to plot the speed against the number of edges in the graph instead of the penalty parameter , as the runtime of the algorithm is very closely related to the actual sparseness of the graph .
overall , when comparing the computation times for the exact algorithms to the approximate algorithms , we can see that the approximate methods are orders of magnitude faster and do not suffer from an exponentially increasing computation time for decreasing sparsity as the exact methods .
therefore , if an exact solution is required , our exact algorithm is preferable .
on the other hand , the
sparse markov networks
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
figure 123 : number of edges in the graph vs .
penalty parameter for different problem sizes , averaged
over 123 simulations .
superior speed of the pseudo - likelihood and wainwright et al .
( 123 ) algorithm warrants a closer look at the trade - off with respect to accuracy .
123 accuracy comparisons
in this subsection we compare the algorithms mentioned above with respect to the accuracy with which they recover the original model .
as both our l123 penalized exact algorithm and the one by lee , ganapathi , and koller ( 123a ) nd the exact maximizer of the l123 - penalized log - likelihood , we only use our algorithm in the comparison .
the other 123 methods we compare to are wainwright - min , wainwright - max and the pseudo - likelihood .
h ofling and tibshirani
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
123 123 123 123
number of edges
number of edges
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
123 123 123 123
123 123 123 123
number of edges
number of edges
figure 123 : computation time of the exact algorithms versus the number of non - zero elements in q
values are averages over 123 simulation runs , along with 123 standard error curves .
also , p is the number of variables in the model , n the number of observations and neigh is the average number of neighbors per node in the simulated data .
here , pseudo - exact refers to the the exact solution algorithm that uses adjusted pseudo - likelihoods as presented in
first we investigate how closely the edges in the estimated graph correspond to edges in the true graph .
in figure 123 , roc curves are shown , plotting the false positive ( fp ) rates against true positive ( tp ) rates for edge identication , for various problem sizes .
note that only partial roc curves are shown since our method cannot estimate non - sparse graphs due to very long computation times .
overall , we see that all approximate algorithms match the results of the exact solution very closely and in some of the plots , the curves even lie almost on top of each other .
sparse markov networks
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
123 123 123 123
number of edges
number of edges
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
number of edges
number of edges
figure 123 : computation time for the approximate algorithms versus the number of non - zero ele - ments in q .
values are averages over 123 simulation runs , along with 123 standard error curves .
also , p is the number of variables in the model , n the number of observations and neigh is the average number of neighbors per node in the simulated data .
apart from the accuracy of edge identication , we also consider other statistics .
the unpenal - ized log - likelihood is a measure of how well the estimated model ts the observed data ( higher values are better ) .
again , the approximate solutions are all very close to the exact solution ( see figure 123 ) and the differences are always smaller than 123 standard deviations .
in figure 123 , we plot the difference of the log - likelihood with respect to the exact solution .
also in this plot , no clear winner can be identied .
we also use the kullback - leibler divergence dkl ( p||q ) , which is a measure of difference be - tween a true probability distribution p and an arbitrary other distribution q .
here , for the true
h ofling and tibshirani
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
123 123 123 123 123 123
false positive rate
false positive rate
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
false positive rate
false positive rate
figure 123 : roc curves : false positive versus true positive rate for edge identication .
values are
averages over 123 simulation runs .
probability distribution we use the distribution of the binary markov network using the true q matrix that was used to generate the simulated data .
the distribution q in our case is the binary markov network using the estimated q
- matrix .
we can compute dkl ( p||q ) as
dkl ( p||q ) = ( cid : 123 )
( q ) + ( cid : 123 ) p ( x ) tr ( cid : 123 ) xxt ( q q ( q ) + tr ( cid : 123 ) ep ( xxt ) ( q q
if the distributions p and q are identical , then dkl ( p||q ) = 123
in our simulations , the exact solution has lower kl - divergence than the other methods , however the differences are very small .
for a plot of the kl - divergence against the number of edges in the model see figure 123
again , all approximate
sparse markov networks
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
123 123 123 123 123 123
number of edges
number of edges
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
number of edges
number of edges
figure 123 : log - likelihood of the estimated model vs number of edges in the graph for different
problem sizes , averaged over 123 simulations .
methods match the exact solution very closely and any differences are well within the 123 standard deviation error band .
in figure 123 the differences of the kl - divergence of the approximate to the exact method can be seen .
again , all methods are very close with the pseudo - likelihood approach performing the best in this case .
when we embarked on this work , our goal was to nd a fast method for maximizing the l123 penal - ized log - likelihood of binary - valued markov networks .
we succeeded in doing this , and found that the resulting procedure is faster than competing exact methods .
however , in the course of this work ,
h ofling and tibshirani
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
123 123 123 123 123 123
number of edges
number of edges
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
number of edges
number of edges
figure 123 : difference of log - likelihood of the estimated model to the exact model vs number of
edges in the graph for different problem sizes , averaged over 123 simulations .
we also learned something surprising : several approximate methods exist that are much faster and only slightly less accurate than the exact methods .
in addition , when a dense solution is required , the exact methods become infeasible while the approximate methods can still be used .
our imple - mentation of the methods of wainwright et al .
( 123 ) uses the fast coordinate descent procedure of friedman , hastie , and tibshirani ( 123b ) , a key to its speed .
the pseudo - likelihood algorithm also uses similar techniques , which make it very fast as well .
we conclude that the wainwright and pseudo - likelihood methods should be seriously considered for computation in markov networks .
in this article , we treated the case of pairwise markov networks with a binary response variable .
we think these methods can also be extended to more general cases .
with respect to the response variables , a multinomial instead of a binary response could be used .
in addition to this , it would
sparse markov networks
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
123 123 123 123 123 123
number of edges
number of edges
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
number of edges
number of edges
figure 123 : kullback - leibler divergence of the estimated model vs .
number of edges in the graph for
different problem sizes , averaged over 123 simulations .
also be possible to generalize the graph structure by introducing higher order interaction terms .
apart from these extensions , an interesting possibility for future work would also be to prove the theoretical results of wainwright et al .
( 123 ) for the pseudo - likelihood model .
furthermore , we believe that both the exact and fast approximate methods can also be applied to the learning of multilayer generative models , such as restricted boltzmann machines ( see hinton , 123 ) .
an r language package for tting sparse graphical models , both by exact and approximate
methods , will be made available on the authors websites .
h ofling and tibshirani
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
123 123 123 123 123 123
number of edges
number of edges
p=123 , n=123 , neigh=123
p=123 , n=123 , neigh=123
number of edges
number of edges
figure 123 : difference of kullback - leibler divergence of the approximate methods to the exact so - lution vs .
number of edges in the graph for different problem sizes , averaged over 123
sparse markov networks
hoeing was supported by a stanford graduate fellowship .
tibshirani was partially supported by national science foundation grant dms - 123 and national institutes of health contract n123 - hv - 123
we would like to thank trevor hastie and jerome friedman for code implementing the fast l123 logistic regression and daniela witten for helpful comments on a draft of this article .
we also want to thank the editor and two anonymous reviewers for their work and helpful reviews .
appendix a .
proof of convergence for penalized pseudo - likelihood and penalized
lee , lee , abbeel , and ng ( 123b ) gives a proof of convergence for an algorithm that solves an l123 constrained problem by quadratic approximation of the objective function .
here , we will follow this proof very closely and make few changes to accommodate that we are only using a rst order approximation and are working with the lagrangian form of the l123 constrained problem instead of the standard form .
assume that g ( q ) is a strictly convex function with a global minimum that we want to mini - 123 and assume that fq 123 , q ) .
here , by rst order g is twice continuously differentiable with derivative 123 at
( q ) be a rst order approximation of g at q mize .
furthermore , let fq is strictly convex , has a global optimum and is jointly continuous in ( q approximation at q 123
assume that our algorithm works as follows :
123 , we mean that fq
while not converged do
with current estimate q find solution q perform backtracking line search on the line from q
( k ) , dene local approximation fq
( k ) ( q ) to g;
( k ) to q
to nd q
lemma 123 let q and a constant kq will return a point f
proof first , let fq
( k ) converges to the global minimizer of g ( q ) .
in order to show this , we rst need the
123 be any point that is not the global optimum .
then there is an open subset sq every iteration of the algorithm starting at f
123 in sq
such that for every f 123 that improves the objective by at least kq be an approximation to g at q
, that is , g ( f
123 with global optimum q
then set
d = fq
and we know that d > 123 as q
: ||q q
123||123 < e ) the following holds :
123 is not the global optimum .
now , there exists an e > 123 such that for
h ofling and tibshirani
convexity of f and g
123 is the global minimum of ff
the existence of this e
follows from the continuity and
with step size 123 < t < 123 in the line search , and using the previous result , it holds that
123 + t ( f
123 ) ) ( 123 t ) ff
123 ) + t ff
for the next step observe that the minimizer f convexity of f in the second argument and the continuity in both arguments .
then , as sq compact set , there exists a compact set tq .
thus , as fq rst order approximation of g at q
123 , there exists a c such that for all q tq
is a continuous function of f
for all f
123 of ff
123 due to the
g ( q ) fq
( q ) +c||q q
123 + t ( f
where d is the diameter of tq
123cd123 ( cid : 123 ) and thus we know that it exists a f
now set t = min ( cid : 123 ) 123 ,
123 t123cd123 > 123 now nishes the proof .
now , using the lemma the rest of the proof is again very similar as in lee , lee , abbeel , and ng
( 123b ) and we only repeat it here for completeness .
theorem 123 the algorithm converges in a nite number of steps .
proof pick d > 123 arbitrary .
let q then there exists a compact set k such that g ( q ) > g ( q nite number of steps in pd .
for every q
123 the starting point of the algorithm .
|| d ) k .
we will show convergence by showing that the algorithm can only spend a
the global optimum and q
dene pd = ( q
123 ) for every q
there exists an open set sq
pd q pd sq
sparse markov networks
is compact , heine - borel guarantees that there is a nite set qd such that
furthermore , as qd
is nite , dene
pd q qd sq
cd = min
improves the objective by at least as the lemma guarantees that every step of the algorithm inside pd cd and a global optimum exists by assumption , the algorithm can at most spend a nite number of steps in pd .
therefore , the algorithm has to converge in a nite number of steps .
for the penalized pseudo - likelihood algorithm , by denition of the approximation it is evident that it is a rst order approximation .
the situation for the penalized log - likelihood algorithm is a little more complicated and it will be shown in the next section of the appendix that the proposed approximation is to rst order and therefore satises the assumptions of the proof .
appendix b .
first order approximation of log - likelihood
the convergence ||123
here , we want to show that this is in fact the case .
for this , we need to show that
( k ) to calculate the next estimate q ( k ) is a rst order approximation of the objective l ( q
in section 123 , we dened a function fq proof in appendix a requires that fq
|x ) + n||r q
||123 is twice continuously differentiable with derivative 123 at q
first , inserting fq
( k ) from section 123 yields
|x ) + n||r q
( k ) = fq
ss ) ( cid : 123 ) ( q ss q ( k ) st ) 123 l ( q g ( q st q ( k )
( q st q ( k )
s x ) t + ( p ( q
( k ) ) + ( xt x ) ss 123 n wss ( q
t x ) s 123 n wst ( q
which has derivative
= 123 ( xt x ) st ( p ( q ) t 123 n wst ( q
s x ) t ( p ( q ) t
t x ) s + ( p ( q
s x ) t + ( p ( q st ) 123 ( xt x ) st + 123 n wst ( q )
( k ) ) 123g ( q st q ( k )
for s 123= t and
= ( xt x ) ss ( cid : 123 ) 123g ( q ss q ( k )
psk ( q ) + ( cid : 123 )
( k ) ) + ( xt x ) ss 123 n wss ( q
ss ) 123 ( xt x ) ss + 123 n wss ( q )
for s = t .
these are clearly continuous and differentiable .
furthermore , inserting q = q ( k ) is a rst order approximation and our proof holds .
that the derivative is 123
therefore , fq
h ofling and tibshirani
