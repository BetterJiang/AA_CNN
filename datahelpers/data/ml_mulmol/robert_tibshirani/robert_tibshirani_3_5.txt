introduction .
suppose that we wish to estimate the probability den - sity g y that produced an observed random sample of vectors y , y ,
y ; g y
for i s 123 , 123 ,
the vectors y take values in a sample space yy .
the numerical examples in this paper have yy being portions of the real line or of the plane , but the methodology applies just as well to higher dimensionalities and to more
estimates of g y are traditionally constructed in two quite different ways : by maximum likelihood tting within some parametric family such as the normal or by nonparametric methods such as kernel density estimation .
these two methods can be combined by putting an exponential family through a nonparametric estimator .
the resulting hybrid estimators are the specially designed exponential families of the title .
figure 123 shows a simple example of this methodology .
the y are pain scores for n s 123 women , each obtained by averaging the results from a questionnaire administered after an operation .
the scale runs from y s 123 s no pain to y s 123 s worst pain , so the sample space yy is the interval 123 , 123 .
the 123 scores y , indicated by the histogram , run from 123 to 123 .
the dashed curve g y is a normal kernel density estimator with window width ls 123 , described more carefully in section 123
also shown are two special exponential family estimates , g y and g y , described below .
received january 123; revised october 123
123supported by nsf grant dms - 123 - 123 and public health service grant 123 roi ca123 - 123
ams 123 subject classications .
123f123 , 123g123
key words and phrases .
poisson regression , degrees of freedom , expected deviance , local and
global smoothing , moment - matching .
efron and r .
tibshirani
pain - score data .
left : histogram of pain scores for 123 women following an operation : 123 s no pain; 123 s worst pain .
right : g y is normal kernel density estimator , window width 123;
g y is the special exponential family through g y with sufcient statistic t y s y; g y uses sufcient statistics t y s y , y .
density g y matches the empirical mean and variance of the 123 data points .
an exponential family of densities on yy , g
is given by y exp b q t y b ,
which is called the exponential family through g y with sufcient statistics t y .
here g y is a carrier density , t y is a 123 = p vector of sufcient statistics , b is a p = 123 parameter vector and b is a normalizing parameter y integrate to 123 over yy .
for example , the one - dimensional that makes g normal family , with all possible choices of expectation and variance , can be obtained using the standard normal carrier g y s w y s exp y123y r
123p , with the sufcient statistics t y s y , y .
the densities in 123 are dened with respect to some background measure , which we will take to be lebesgue measure .
see section 123 of lehmann 123 .
y exp b q t y b ,
the estimates g y and g y in figure 123 are of the form
where g y is the kernel density estimate indicated by the dashed curve .
estimate g y uses the single sufcient statistic t y s y , so p s 123 , while
g y uses t y s y , y , p s 123
the parameter values bs b , b were chosen by maximum likelihood , that is , by maximizing ( cid : 123 ) g y , ignoring
the fact that the carrier g y is itself data - dependent .
this choice of b matches the t y moments of g
y to their empirical averages :
y dy s
exponential families for density estimation
thus g matches the rst two empirical moments of the 123 pain scores or , equivalently , it matches the empirical mean and variance .
property 123 implies that the special exponential family estimate of g y is unbiased for the moments of t y .
linear transformations to post - repair the mean and variance of a kernel estimate are familiar in the density - estimation litera - ture; see , for example , jones 123 .
the two - dimensional example of section 123 shows a more ambitious example of moment - matching .
we can think of a special exponential family estimator in two complemen - tary ways : 123 as being a standard exponential family estimator , except one that is preceded by an adaptive choice of the carrier , or 123 as being a standard nonparametric smoothing estimator , except one that is followed by a correction to match certain sample moments .
either way , we will argue that special exponential families can be a favorable compromise between parametric and nonparametric density esti - mation .
from the rst point of view , we will be able to use exponential family theory more exibly than when restricted to the usual small catalogue of normals , gammas , betas and so forth .
an approach more in the spirit of a regression analysis than a density estimate is possible , including exploratory choices of the sufcient statistics t y .
from the second point of view , the moment - matching correction will usually reduce the bias of a nonparametric smoother , since the moments t y are estimated unbiasedly .
this has an important practical consequence shown in the numerical examples : it allows the nonparametric smoother to use a substantially greater window width without badly degrading the overall t to the data .
the result is a substantially smoother estimate of the density g y .
this phenomenon is illustrated in the bivariate example of figures 123 and 123
to put things another way , the special exponential family estimate 123 works at two different scales .
the nonparametric smoother allows local adaptation to the data , while the exponential term matches some of the datas global properties .
sections 123 ) 123 describe how to compute and interpret special exponential family sef estimates such as those in figure 123
most of our computations are done using a poisson regression model for density estimation section 123 , originally introduced in lindsey 123a , b .
section 123 gives a delta - method formula for the covariance of b in 123 , which takes into account the data - based choice of the carrier g y .
we can use this formula in the usual way to select among possible choices of the sufcient statistic .
section 123 concerns formulas for choosing the window width of the smoother that produces g y .
this choice involves degrees of freedom calculations like those introduced by hastie and tibshirani 123 .
multisample sef esti - mates are discussed in section 123
remarks appear in section 123
special exponential families are an example of what green and silverman 123 call semiparametric methods .
many other semiparametric methods have been proposed for density estimation .
hjort and glad 123 propose reversing the sef order : rst t a parametric family to the data and then t a nonparametric smoother to the residuals from the parametric estimator .
efron and r .
tibshirani
hastie and tibshiranis backtting approach can also be applied to semipara - metric density estimation .
it amounts to an iteration to convergence between the smoother and the exponential family .
the principal advantage of the sef estimator is its theoretical tractability .
the relationship between these ideas is discussed in section 123
stone 123 and kooperberg and stone 123 use adaptive exponential families based on spline functions for density estimation .
the carrier g y is effectively the uniform density in this work , but adaptation is still possible because of the local character of the spline basis .
olkin and spiegelman 123 combine parametric and nonparametric density estimates by mixtures , rather than by exponential family methods as in this paper .
poisson regression for density estimation .
density estimation problems can be rephrased in terms of poisson regression models .
this technique was introduced by lindsey 123a , b as a way of using generalized linear model software to t difcult exponential family models .
see also aitken 123 , lindsey and mersch 123 and efron 123 .
our results here will be phrased in poisson regression terms , mainly for reasons of conceptual clarity , though there are also some computational advantages .
lindseys construction begins with a discretization of the problem : the
sample space yy is partitioned into k disjoint cells yy , k
and the data y s y , y ,
s s a y g yy
are reduced to the cell counts
for k s 123 , 123 ,
the vector of counts s s s , s , .
, s has sum s s n .
table 123 shows the counts for the pain - score data of figure 123 , where yy s 123 , 123 has been parti - tioned into k s 123 cells yy each of length 123 .
suppose that g y , ug q is a family of probability densities on yy
probability of observing y in the kth cell is
p u s g
discretized version of the pain - score data of figure 123; yy s 123 , 123 partitioned into k s 123 cells of
length 123; s s 123 of the 123 scores occurred in yy s 123 , 123 , 123 in yy s 123 , 123 and so forth
exponential families for density estimation
with sum p u s 123
then s has a multinomial distribution on k categories , with n draws and probability vector p u s p u , p u , .
, p u , we could nd the maximum likelihood estimate mle of u , based on s , by maximizing the multinomial probability of s .
n , p u .
s ; mult
instead we consider the s to be independent poisson observations
k s 123 , 123 ,
s ; po m g , u ,
m g , u s gp u .
s ; po g and s s ; mult
here g is a free parameter , restricted only to be positive .
standard poisson properties allow 123 and 123 to be expressed as this means that the maximum likelihood estimates from 123 and 123 are and u is equal to the mle for u in 123 .
lindseys method is to approxi - mately maximize the original likelihood ( cid : 123 ) g y by nding the poisson mle in 123 and 123 .
note : the parameters g and u are orthogonal in cox and reids 123 sense , so that the information for estimating u is the same in 123 and 123 .
this method of nding the mle is particularly convenient when the original densities are of the exponential family form 123 .
we will consider the density estimation problem 123 and 123 in the poisson regression form 123 and 123 :
s , p u .
gs s s n ,
s ; po m b
for k s 123 , 123 ,
m b s mo exp b q t b .
here m is proportional to p s h the sufcient vector t y evaluated at a convenient point y parameter b corresponds to gs e x s 123 , t
t s t y
in 123 .
dene x to be the k = p q 123 matrix whose kth row equals the maximum likelihood equations for bs b , b in the generalized linear model 123 are
x 123 s y m b s 123 ,
g y dy , a discretized version of the
in yy .
the free
efron and r .
tibshirani
where m b indicates the vector with kth component m exp x b .
standard generalized linear model software easily solves for b in 123 , even for difcult nonstandard forms of the exponential family 123 .
this was lindseys
here is how figure 123 was constructed .
the problem was discretized into k s 123 cells as in table 123
the carrier vector m s m , m , .
, m in 123b was estimated using a k = k smoothing matrix m l , matrix m l was taken to be a normal kernel smoother , having kjth element
mo s m l s .
m l s w
with y s k y 123 r123 , the midpoint of cell yy .
the constants c were chosen to make m s 123
the starred curve labelled g in figure 123 is actually m plotted as a function of y , with the window width l in 123 set equal
the curves labelled g y and g y are really the discrete analogs of the
special exponential family 123 , say m s m , m ,
m s m exp b q t b ,
, while g
is based on the quadratic .
the mle estimates b were obtained by iterative
plotted versus y : g uses t s y vector t s y solution of 123 , so where x is a 123 = 123 matrix for g and a 123 = 123 matrix for g .
the estimates were actually computed using a centered version of y , y s y y 123 r123
to t this model in the glim language or the glm function in splus , one simply includes log mo as an offset in a poisson generalized linear model .
x 123 s y m s 123 ,
equation 123 shows that
m s s s n
the discrete analog of the moment - matching property 123 .
notice that because of 123a and the fact that the cells are of length 123 , the curves in figure 123 integrate over yy to 123 rather than to 123
changing k to 123 or to 123 made very little difference in figure 123
the numerical calculations in this paper were insensitive to the form of dis - cretization .
in fact , discretization is not really necessary for any of our results , as discussed in remark e of section 123
exponential families for density estimation
nevertheless , it is conceptually easier to discuss special exponential family density estimation in terms of the discrete poisson model 123 .
the problem becomes one of tting a smooth regression curve to the independent observa - tions s , and this lets us make use of the arsenal of regression tools .
it also emphasizes the important point that density estimation is equivalent to poisson regression , and not to ordinary least squares regression .
the poisson nature of the problem will be evident in the formulas developed below .
estimating the covariance of b .
this section derives an approxi - mate covariance matrix for bs b , b , the estimated parameters in a special exponential family such as 123 or 123 .
the formula for the covari - ance takes into account the data - based choice of the carrier .
we will use the estimated standard errors of the components of b for model building , check - ing the signicance of the corresponding components of the sufcient statistic t y in the usual way .
we consider the poisson form 123 of the sef model ,
s ; po m b
with m b s moe xb ,
x k b , x s 123 , t
where po m indicates a vector of k independent poisson variates having expectations m , m , .
, m s m .
the notation m e indicates the vector with kth component m e , as in 123 .
generalizing 123 , we rst estimate m by some function of s , say m s , and then solve for b in the mle equations 123 :
the special exponential family estimate of m is
and b : x 123 s y m e s 123
m s m s
m s m e
lemma 123
let d be the k = k diagonal matrix with kth diagonal element
log m s m s m e
log m , log m , .
, log m with respect to s , with j indexing columns ,
and let h be the k = k derivative matrix of
d log mo
then the p q 123 = k derivative matrix of b with respect to s is
s x 123dx
z123 s x 123 i y dh .
in case 123 , m s ms , this becomes where d e proof appears below .
z123 s x 123 i y d e m ,
is the diagonal matrix with kth diagonal element e
efron and r .
tibshirani
by the usual delta - method argument , an approximate covariance matrix
estimate for b is given by
corollary 123
the sef vector b obtained from 123 and 123 has approx -
imate covariance matrix
cov b s x 123dx
z123dz x 123dx
where d is the diagonal matrix with kth diagonal element s .
an alternative
cov b s x 123dx
z123dz x 123dx
for any k - vector v , we let d v be the k = k diagonal matrix with kth diagonal element v .
the true covariance of s ; po m is d m .
approxima -
tion 123a estimates cov s by d s s d in 123 , while 123b uses d m s d .
the former may be preferred if model 123 is suspect .
in our numerical examples the two formulas gave nearly the same results .
table 123 applies the corollary to the pain - score data .
the quadratic model , described in 123 ) 123 has b s y123 , y123 , with standard errors 123 , 123 according to 123a .
b serves only to normalize m to m s n , 123a , so its value is of no statistical interest .
the coefcient of y s y y . 123 r123 , the centered version of y , is nearly 123 standard errors below zero .
the coefcient of y 123 is y123 standard errors below zero , so it is not so clear that the quadratic term is signicant .
the cubic model , in which t y s y , y , y , has the cubic coefcient only 123 standard errors above zero .
either the linear or the quadratic model seem reasonable here; the cubic model is denitely excessive .
section 123 discusses model selection in more detail .
parameter estimates b and estimated standard errors for the pain - score data of table 123
the quadratic model is described in 123 ) 123 ; y s y y 123 r123 , centered version of y; se square root of diagonal elements 123a ; se from 123b ; jack is jackknife standard error; naive is the usual exponential family sterr estimated ignoring data - based choice of carrier .
the cubic model uses the
same m , but t y s y , y , y ; the cubic term is not at all signicant
exponential families for density estimation
the jackknife standard errors for the components of b formula 123 of efron and tibshirani 123 were computed as a check on the corollary .
they came out a little larger than the delta - method estimates from 123a or 123b , as is often the case; see section 123 of efron 123 .
suppose that we ignore the fact that the carrier m in 123 is a function of the data s .
this amounts to taking h s 123 in 123 , so z123 s x 123 in 123b .
then 123b reduces to the usual covariance estimate for exponential families ,
cov b s x 123dx
while 123a becomes the sandwich estimate x 123dx the standard error estimates from 123 , labelled naive in table 123 , are considerably larger than the standard errors that take into account the adaptive choice of the carrier .
x 123dx x 123dx
the naive standard errors will usually exceed those from 123 .
the reason is that the adaptive choice of m absorbs some of the variability in b .
here is a simple normal - theory version of the same phenomenon : suppose we observe z ; n m q b , 123 and we wish to estimate b , the distance of the expectation ms m q b from some origin of measurement m .
then bs z y m has standard error 123
however , if the origin is chosen adaptively , say by mo s 123 ? z , then bs z y m has standard error 123 .
observing z s 123 , for exam - ple , gives bs 123 , which in the adaptive case is 123 standard errors above 123
see remark b in section 123
proof of lemma 123
using the d notation for diagonal matrices , 123 for
b can be expressed as produces change db in the mle vector and change
is the vector with components e
x 123 s y d m e s 123 ,
a small change ds in s
then 123 gives
dm 123 d m h ds
123 s x 123 s q ds y d m q dm exp x bq db
s x 123 s q d m exp xb q ds y d dm exp xb
using 123 , 123 and the fact that d dm e s d e
123 s x 123 ds y x 123d m e
yd m exp xb x db .
h ds y x 123d m e
s x 123 i y dh ds y x 123dx db .
dm , 123 becomes
this veries 123
efron and r .
tibshirani
a bivariate example .
density estimation becomes more interesting , and more difcult , when the sample space yy is of higher dimension .
this section introduces an example when yy is a portion of the plane .
we will use this example to illustrate some of the advantages of special exponential family density estimation .
figure 123 shows l s log redshift and m equal to the apparent magnitude
for 123 galaxies taken from loh and spillars 123 redshift survey , hubbles law , that larger redshift implies greater distance from earth , is apparent in the gure .
the galaxies with larger values of l tend to appear dimmer , that is , to have larger apparent magnitudes , leaving the lower right corner nearly empty .
for i s 123 , 123 , 123 , .
, n s 123
y s l , m
the data in figure 123 are a truncated subsample of the 123 galaxies in the loh ) spillar catalog .
it is all of the catalog entries falling into the rectangle we will take this rectangle to be the sample space yy .
some of the scientic reasons for truncation are discussed in efron and petrosian 123 .
and 123 f m f 123 .
log 123 f l f log 123
figure 123 shows yy partitioned into k s 123 rectangular cells yy , by dividing the l axis into 123 equal strips and dividing the m axis into 123 equal strips .
the corresponding counts s , 123 , are shown on the right side of the gure .
it is convenient to index the cells by the midpoint of rectangle yy is y l
i s 123 , 123 , .
, 123 , j s 123 , 123 ,
k s i , j ,
the galaxy data : 123 galaxies from loh and spillars 123 redshift survey log redshift l and apparent magnitude m discretized into 123 s 123 = 123 equal cells .
the counts s for the 123 cells are shown at right .
m l s exp y
i y i123 q j y j123
exponential families for density estimation
the right side of figure 123 shows the results of applying a linear smoother
m s m l s to the 123 - dimensional count vector s given in figure 123
the k = k matrix m l is a two - dimensional version of 123 , with kk123th
for k s i , j and k123 s i123 , j123 .
the c are chosen to make ( cid : 123 ) m l s 123
the choice ls 123 , suggested by the expected deviance calculations of section 123 , gave the smoothed estimate m s m 123 ? s plotted versus y on the right side of figure 123
mo s m 123 s and x s 123 , i , j , i123 , j123 , ij .
the left side of figure 123 shows an sef estimate m of form 123 , with
here i is the k vector with kth element i y 123 , and j is the k vector with kth element j y 123
this choice amounts to using the usual sufcient statistics for t y s l , m , l , m , lm .
the tted density a bivariate normal matches the empirical means , variances and correlation of the galaxy data .
in 123 ,
the variance calculations of section 123 and the expected deviance calcula - tions of section 123 suggest that these two estimates are roughly equal in their overall ability to predict the true density .
however , the sef estimate is much smoother than the smoothing - only choice .
this is obvious in figure 123 , which shows contour plots of the two density estimates .
suppose that the carrier m in 123 was taken to be the constant vector
m 123 instead of m 123 s .
then the sef m would be the discretized trun - cated bivariate normal mle for the galaxy data , the truncation being to the rectangle 123 .
in fact , the sef in figure 123 looks like the lower corner of a bivariate normal , though there are some discrepancies due to the adaptation of mo to the galaxy data .
left panel : sef estimate 123 for the galaxy data as discretized in figure 123; m s m 123 ? s , quadratic matrix x , 123 .
right panel : the smoothing - only estimate m s m 123 ? s .
the calculations of sections 123 and 123 suggest that the two estimates are roughly equal in accuracy .
however , the sef estimate is much smoother .
efron and r .
tibshirani
contour plots of the density estimates in figure 123
the sef contours left panel are much smoother than those from the smoothing - only estimate right panel .
the smoothing - only estimate has a weak second mode at the starred point .
parametric models such as the bivariate normal are erce data smoothers .
this can be a big advantage if the statistician is interested in global proper - ties of the density , like the general shape of its contours , and especially if the data - sampling process is suspect .
in figure 123 we can see that the galaxy data are quite patchy and clumpy , which is not surprising since the loh ) spillar catalog was a census of the brighter galaxies in a few degrees of sky , and not a random sample of the full sky .
if those few degrees of sky are of great individual interest , then a narrow - band smoother like that in the right side of figure 123 may be appropriate .
notice that it estimates a weak second mode near l , m s y123 , 123 .
how - ever if we really want to know the density for the whole sky , then it pays to oversmooth the estimate from a awed sample like that in figure 123
the sef methodology allows us to oversmooth without losing much estimating ef - ciency compared to smoothing - only estimates and without making the drastic assumptions of the usual parametric models .
various sef models besides the quadratic choice of x in 123 were tried .
is the component of i j
mo s m 123 s and x s 123 , i , j , i123 , j123 , ij , i123 j123 ( cid : 123 ) 123 123
one of these added a further cross - term to 123 , where i j orthogonal to 123 , i , j , i , j , ij .
orthogonalization makes the rst six components of b have roughly the same values and standard errors as in 123 .
the term i j allows the regression surface to turn more quickly near the corners of the
( cid : 123 ) 123 123
the mle vector b for model 123 appears in table 123 , along with the standard error estimates se from 123a and the t - values br se .
all of the coefcients except the one for m are signicantly different than zero the
exponential families for density estimation
parameter estimates and standard errors for the components of b in the sef model 123 .
all of the components of b are signicantly nonzero , except for the coefcient of m .
l indicates the coefcient corresponding to i , lm to ij and so forth .
standard errors are from 123a .
model 123
gave similar estimates , standard errors and t - values for l , m , l 123 , m123 , lm
l m orthog
same is true for model 123 .
this includes the coefcient for the term , which takes us beyond the normal - theory sef analogue 123 .
compared to the left side of figure 123 , the sef density estimate from 123 has its highest point at the upper right corner of the rectangle yy .
the next two sections discuss several criteria for choosing among possible sef models : observed deviance , expected deviance , degrees of freedom and so forth .
however , none of these criteria is sharp enough to entirely free the statistician from model - choice quandries .
a considerable amount of subjectiv - ity must still go into the model - building process , just as in ordinary regres -
total relative variance .
a variety of diagnostic tools is available to assist model selection in standard regression situations .
similar tools are available for model selection in special exponential families .
these ideas are developed in the next two sections , beginning here with the total relative variance , a simple measure of overall variability for an sef density estimate .
for example , looking ahead to table 123 the reader can compare the total relative variances for the two galaxy - data sef estimates in figure 123 : 123 for the quadratic model in the left panel versus 123 for the smoothing - only model in the right panel .
m s sef s; m , x
indicate m s m e , the special exponential family estimate 123 .
first we will compute the k = k derivative matrix of m with respect to s , which leads immediately to a delta - method estimate of cov m .
this derivative matrix involves the projection matrices where d s d m as before , p is the symmetric projection matrix into the linear space spanned by the columns of x , in the inner product d , the projection of vector v being pdv , and q is the projection orthogonal to x s column space .
because they represent orthogonal projections , we have
x 123 and q s d y p ,
p s x x 123dx
pdp s p ,
qdq s q and pdq s 123
efron and r .
tibshirani
lemma 123
the derivative matrix of m s sef s; m , x with respect to s is
where , in terms of h s d log m rds , 123 ,
o s p q qdh s p q h y pdh .
if m s ms , then h s d 123rm m and
o s p q qd e
the proof appears below .
the canonical parameter vector for the poisson family 123 is
h s log m s log m , log m , .
, log m .
likewise , dene h s log m and h s log m .
then we can write lemma 123
s o s p q qd
this decomposes dhrds into a part p coming from the exponential family and an orthogonal part qd dh rds coming from the adaptive choice of the carrier .
lemma 123 leads directly to delta - method estimates of the covariance matrix
of m , as in 123 ,
cov m s dodo123d or cov m s dodo123d
with d s d s .
the diagonal elements give variance estimates var m or var m for the individual components .
for poisson variables it is natural to measure variance relative to the etsimate m .
we dene the total relative variance trv estimate for m to be
or trv s
corollary 123
for m s sef s; m , x the total relative variance estimates
trv s tr dp q hdh123 dqd
trv s p q 123 q tr hdh123 dqd ,
where p q 123 is the number of columns of x , d s d s , d s d m and p , q are as in 123 .
exponential families for density estimation
the proof of corollary 123 follows directly from 123 by writing trv in the trace form tr d cov m and using the orthogonality relationship 123 , and similarly for trv .
computationally more efcient expressions for trv and trv appear in remark i , section 123
table 123 shows trv for various sef estimates for the galaxy data , as discretized in figure 123
the three estimates correspond to three choices of x in 123 : sef 123 is for x as in 123 , sef 123 for x as in 123 , and sef 123 for x s 123
this last choice is the smoothing - only estimate m rescaled to nrm m , so that it sums to n s s .
the carrier mo for the sef is where m l is the matrix 123 .
mo s m l s ,
all three trv estimates decrease as the smoothing parameter l increases because greater window width l decreases the variability of mo .
if the carrier
mo were prechosen instead of adaptive , then sef 123 would exceed sef 123 by about 123 , this being the increased number of free parameters , and likewise sef 123 would exceed sef 123 by about 123
this is seen in 123 for the case h s 123
in fact , this is nearly the case at the right side of table 123 , where l is so large that mo has nearly constant entries .
the difference between the three estimates decreases at smaller values of l because the adaptability of the common carrier m s m l ? s absorbs some of the difference in the exponential family
small variability is a good property of course , but we also want m to have small bias for estimating the true density vector m .
the next section puts trv into the context of bias ) variance tradeoffs for poisson regression esti -
proof of lemma 123
in the notation following 123 , h s h q xb .
differ -
entiating this with respect to s and using 123 gives
s h q x x 123dx
x 123 i y dh s p q h y pdh ,
which is 123 , the equivalent of lemma 123
total relative variance trv for three sef estimates , galaxy data , at increasing values of
smoothing parameter l
l s 123
sef 123 smoothing only sef 123 123 sef 123 123 aright panel of figure 123
bleft panel of figure 123
efron and r .
tibshirani
degrees of freedom and estimated deviance .
selecting a good sef estimate m for a particular application involves making the usual tradeoffs between variance and bias .
this section concerns estimating the total expected deviance of m from the expectation vector m .
this is a measure of accuracy for m that involves both variance and bias .
an important role is played by the degrees of freedom of the estimator m , an idea related to the total relative variance of section 123
the ideas here are an extension of those in section 123 of hastie and tibshirani 123 .
$ figure 123 relates to the expected deviance measure edev developed below , a diagnostic measure for comparing the goodness - of - t of different sef models .
it shows edev for both the pain - score and galaxy examples .
edev is plotted versus the smoothing parameter l for the normal kernel estimates 123 and 123 used to obtain m s m l ? s .
the different curves correspond to different choices of x in the sef formula 123 .
smoothing - only refers to x s 123 , which gives the estimate mo renormalized to sum to n .
for the pain - score data , linear and quadratic are the cases referred to in 123 .
for the galaxy data , 123 and 123 are as in table 123
notice that edev l has a sharp minimum as a function of l for both smoothing - only cases .
at ls 123 for the pain - score data and at ls 123 for the galaxy data .
this is not true for the genuine sef estimates .
they allow the smoothing parameter l to be chosen much larger without incurring too much edev penalty .
in other words , the sef methodology allows us to oversmooth the density estimate , with the advantages seen in figure 123
expected deviance estimates edev , 123 .
left panel : sef estimates for pain - score data 123 ) 123 .
right panel : sef estimates for galaxy data as in figure 123
in both cases the sef estimates permit the use of larger smoothing parameters l , compared to the smoothing - only
exponential families for density estimation
in order to motivate edev , it helps to begin the discussion with the normal case .
suppose that the statistician observes a k - dimensional normal and wishes to estimate m using some linear estimator s being a k = k matrix .
model selection amounts to making a good choice
z ; n m , i
m s sz ,
n s e m ,
err s z y m .
df s tr s and tv s tr ss123 ,
let err be the observed total residual squared error of m :
where df stands for degrees of freedom and tv stands for total variance .
total variance tv equals ( cid : 123 ) var m , and if s is a projection matrix , then tr s is the usual degrees of freedom .
it is easy to prove the following two
e err y k y 123df s e m y m
e err y k y 123df q tv s n y m .
both 123a and 123b play an important role in normal - theory model selection .
the rst of these is essentially the c or aic criterion .
its exten - sion to nonlinear estimators is steins unbiased risk estimate .
extensions of formula 123b are used in hypothesis testing .
the quantity k y 123df q tv equals the residual degrees of freedom : if s is a projection matrix , then 123b can be improved to where the notation indicates a noncentral chi - square variate with noncentral - ity parameter n y m .
we test the adequacy of the estimate m s sz by comparing err to a central chi - square distribution x123
rdf s tr i y s i y s 123
err ; x
we now return to the poisson situation where we observe
and estimate m by some estimator m s m s , not necessarily of the sef form . 123 , having expectation the observed error err s err s in the poisson context is
err s s dev s , m ,
s ; po m
n e m s
efron and r .
tibshirani
dev m , n s 123 ( cid : 123 ) log m rn y m y n .
where dev indicates the total poisson deviance
the poisson equivalents of k , df and tv in 123 are
k m s e dev s , m ,
df m s e
s y m log m ,
trv m s 123 e m log n rm ,
all expectations being with respect to s ; po m .
in the poisson situation 123 ) 123 , e err s y k m y 123df m s e dev m , m
e err s y k m y 123df m q trv m s dev m , n .
123a , b .
the proof is given below .
formulas 123a , b are the poisson versions of
in order to use lemma 123 , we can approximate k m , df m and trv m by their plug - in estimates k m , df m and trv m .
using bootstrap notation , let s* given s have poisson distribution and let e# indicate expectations with respect to 123 , with s and m s xed .
s* s ; po m s
k m s e# dev su , m
s e# 123 s log s rm y s y m
it is easy to evaluate 123 numerically since it is the sum of univariate poisson deviances , the m being just xed constants in the e# expectations .
using this same notation we can write the degrees of freedom estimate
df m as
df m s e#
su y m log mu s e# s* y m 123h* ,
where h* s log m* s log m s* , the vector with kth component log m .
since by 123 , dhrds s o , 123 has the taylor series approximation see remark k .
an alternative estimate is dfs tr do as in 123 .
df m 123 e# s* y m 123o s* y m s tr do df .
( cid : 123 ) mk
trv m s
exponential families for density estimation
the quadratic expansion log mrn 123 mrny 123 y mrny 123
m var m permitting us to approximate 123 by
e m log
the quantities trv and trv in 123 are the obvious plug - in estimates for 123 .
the substitution of m for n in the denominator of 123 looks worrisome , but remark k of section 123 shows that the resulting error is
the trv estimates 123 can be written as
df s tr do or df s tr do .
trv s tr do123do or trv s tr do123do ,
using 123 , compared to the df estimates notice the similarity to the normal - theory denitions in 123 .
comparing 123b with 123b , we can also dene residual degrees of freedom rdf for the poisson situation , estimated by see remark l .
the quantity graphed in figure 123 was the expected deviance estimate obtained from 123a , 123 and 123 :
rdf s k m y 123df q trv or rdf s k m y 123df q trv .
edev s err s y k m q 123df .
the vertical scale in figure 123 is misleading .
for the galaxy data , reducing l from 123 to 123 reduces edev for the quadratic sef 123 from 123 to 123 , a considerable amount .
is this signicant ? the observed deviance error 123 , $ ( cid : 123 ) err , decreases by 123 , while the residual degrees of freedom rdf , 123 , decreases by 123 .
this gives the naive chi - square signicance value prob x ) 123 s 123 .
a more trustworthy signicance level could be
obtained by monte carlo methods , bootstrapping with s* ; prob m , with m the 123 , 123 sef vector .
the quantitative aspects of figure 123 cannot be taken too literally .
for instance , using edev instead of edev moved the smoothing - only curve below the others for l - 123 in the galaxy data .
in general the careted hat formulas gave less erratic answers than the bar formulas , but there are really no strong reasons for preferring edev .
the fact is that it is usually difcult to estimate the performance of competing decision rules , and the sef density estimates are no exception .
formulas like 123 and 123 help with model selection , but considerable subjectivity remains .
the main point of figure 123 is the qualitative one that the sef estimates permit extensive oversmoothing .
proof of lemma 123
dene ee m s e dev s , m , where s ; po m inde - pendent of s , so ee m is the expected prediction error of m s .
efron 123
efron and r .
tibshirani
ee m s e err s q 123df m .
we also have the identities all three of these relationships are easy to prove directly from the denition of the total poisson deviance .
substituting 123b into 123a gives 123a .
substituting 123c on the right side of 123a gives 123b
e dev m , m s dev m , n q trv m .
ee m s k m q e dev m , m
multisample problems .
so far we have only considered one - sample problems .
sef estimates are particularly useful for investigating density differences in multisample situations .
we use the exponential family model . 123 for the different densities , with a shared carrier g estimated nonpara - metrically , but with possibly different values of the exponential b parame - ters .
an example will precede the theory .
figure 123 concerns a two - sample application of sef modelling .
the data are the compliances of men in the stanford arm of a randomized trial of the cholesterol - lowering drug cholostyramine; see efron and feldman 123 .
there were n s 123 men in the control group and n s 123 men in the treatment group .
compliance ran from 123 to 123% , so yy s 123 , 123 .
a dis - cretization yy s d yy partitioned yy into k s 123 intervals of equal length .
the left panel of figure 123 shows the counts in the two groups .
compliance is signicantly worse in the treatment group , as shown by standard two - sample
an application of sef modelling to the cholostyramine trial compliance data of efron and feldman 123 .
left panel : the count vectors for the control group and the treatment group; k s 123 equal divisions of yy s 123 , 123 .
right panel : sef density estimates 123 for the two groups; m and x as in 123 .
the poorer compliance in the treatment group is graphically
exponential families for density estimation
the right panel of figure 123 is the output of a two - sample sef analysis .
it neatly displays the compliance differences between the two groups in terms of
their estimated densities .
the ratio g y decreases almost lin - early as y goes from 123 to 123% , but both densities are greatest at y s 123% .
these densities were derived from a simple extension of the previous theory .
in the multisample situation we observe independent random sam - ples from l possibly different densities g , g , .
, g on the same sample
y l ; g y
we discretize the problem as in section 123 , obtaining count vector s l
for i s 123 , 123 , .
, n and l s 123 , 123 ,
the lth sample , the many - sample version of the poisson regression model 123 is
for k s 123 , 123 , .
, k and l s 123 , 123 ,
l s a y l g yy
s l ; po m l
for l s 123 , 123 , .
, l , with m l s m e
the sef estimates corresponding to 123 are
in this model , a common carrier mo , estimated from all of the data ( cid : 123 ) ( cid : 123 ) .
s 123 , s 123 , .
, s l is modied by an exponential factor e which can vary
m s m s 123 , s 123 ,
and b l : x 123 s l y m e
m l s m e
the many - sample version of lemma 123 in section 123 is the following lemma :
lemma 123
s l ( cid : 123 ) .
be the k = k matrix with kjth entry log m rds l .
then the p q 123 = k derivative matrix of b l with respect to s j
b l ( cid : 123 ) .
d l d m l
g l s x 123d l x and z s x d i y d l h j
, d equalling 123 or 123 as l does or does not equal j
mo s ms
s s ,
efron and r .
tibshirani
z s x d i y d e
the proof is nearly the same as for lemma 123 and will not be given here .
lemma 123 leads to delta - method estimates of the covariance matrix for
b l , just as in 123 ,
cov b l s g l
cov b l s g l
z d j z g l
z d j z g l
d j d s .
we can also obtain covariance estimates for functions of the
for example , if gs b 123 y b 123 , then
z y g 123
z y g 123
cov g s
cov g s
sef model 123 was used to estimate the two compliance densities in
x s 123 , y
being the midpoint of yy ; mo s ms as in
figure 123
the kth row of x was quadratic in compliance ,
where y s y y 123r123 , y . 123 , with as in 123 .
the estimated difference gs b treatment y b control was with the standard errors taken from cov g , 123b .
we see that the linear coefcient is signicantly negative , t - value s y123 , but the quadratic coef - cient is not .
123 , y123 , y123 " 123 , 123 , 123 ,
m s m 123 ,
other types of estimators .
in the sef methodology the application of an initial nonparametric smoother is followed by the tting of an exponen - tial family parametric model .
hjort and glads 123 semiparametric density
exponential families for density estimation
estimator reverses this order , with the parametric model coming before the smoother .
backtting , as applied to generalized linear models like 123 , repeatedly iterates between the parametric and nonparametric tting meth - ods .
this section discusses the sef methodology in the context of these other
as in section 123 , we begin with the normal case where the statistician
z ; n m , i .
m s pp z; j j q p z y j .
wishes to estimate m having observed the linear model m s j q xb , with j a known origin vector and x a known k = p q 123 structure matrix , gives the estimate here p s x x 123x x 123 is the k = k projection matrix into ll x , the col - umn space of x .
in the normal case , pp z; j plays the role of the parametric exponential family model estimator .
the role of the nonparametric smoother is played by where m is some xed k = k smoothing matrix such as m l in 123 .
once again j represents a xed and known origin .
often we take j s 123 , but it will be important here to consider more general choices of the origin .
mm z; j s j q m z y j ,
the normal - theory analog of the sef estimate 123 is m s pp z; mm z; 123 s p q m y pm z
s p q qm z .
here q s i y p is the projection matrix into ll x , the orthocomplement to ll x .
in other words , we begin with 123 as the origin , apply mm to get an updated origin m s mm z , 123 s mz and nally take the estimate of m to be
m s pp z; m s m q p z y m .
notice that
x 123z s x 123m
according to 123 , which says that z and m have the same projection into ll x , namely , pz .
equality 123 is the normal - theory analog of the moment - matching property 123 .
reversing the order of pp and mm , as hjort and glad do in the density
m s mm z; pp z; 123 s p q mq z .
estimation problem , gives the estimator this no longer enjoys the moment - matching property 123 , but we can restore it with one further application of pp .
this denes the symmetrized estimator if m is a symmetric matrix with eigenvalues between 123 and 123 , then so is p q qmq .
this makes m a formal bayes estimator for m in situation 123 , versus a normal prior distribution on m possibly having innite variance in
s p q qmq z .
m s pp z; m
efron and r .
tibshirani
the backtting estimator m
is dened as follows in chapter 123 of hastie
mo s mm z; m123 y m123 and m123 s pp z; mo y mo .
and tibshirani 123 : suppose we can nd vectors m and m such that
so m s mm z; m s p z; m .
letting v s z y m , it is easy to show that which implies the moment - matching property 123 .
hastie and tibshirani show that m s s that is symmetric and with eigenvalues in 123 , 123 if m has these same properties .
m s mo q m123
v y mv s z y m y m g ll x ,
z for a matrix s
further insight into the backtting estimator can be gained by expressing
it in an explicit form .
the backtting estimator satises
bs x 123x x 123 z y m ,
m s m z y xb .
solving these equations yields the explicit expression
bs x 123 i y m x
x 123 i y m z .
this looks like a weighted least squares estimate with weight matrix i y m or , equivalently , variance matrix i y m .
it has the same form as hjort and glads estimator except that they use an unweighted least squares estimator .
the weights i y m can be justied from a mixed effects model in which m123 is a random effect .
we have discussed four linear estimators m s sz , three of which have the moment - matching property 123 .
these three can be described as follows : the ll x component of m matches the ll x component of z; the ll x compo - nent of m equals the ll x component of a point v in the at space z ( ll x ; v s z for m , v s qz for m and v equals the point or points satisfying the orthogonality condition 123 for m .
note : all four estimators are the same if m and p commute , mp s pm .
we calculated the equivalent kernels for each of the four estimators , as in figure 123 of hastie and tibshirani 123 .
the calculation was done for the situation that produced the quadratic estimator in figure 123 : 123 equally spaced x values , p based on a matrix x with rows 123 , x , x and m of the form 123 .
the smoothing parameter l was chosen to give tr s s 123 in each case .
the sef and backtting kernels were remarkably similar , with both the hg and symmetrical kernels being slightly different .
for the poisson situation s ; po m .
given a positive origin vector j and structure matrix x , we let the analog of 123 be
we can dene analogs to m , m , m
, where x 123 s y d j e
pp s; j d j e
exponential families for density estimation
( cid : 123 )
the analog of 123 is
m s pp s; j is the mle estimate of m in the offset generalized linear model m s d j e
m s mm s; j is a discretized version of what hjort and glad 123 call a nonparametric density estimate with a parametric start , the start being the choice of j .
mm s; j s d j md 123rj s s d j m srj ;
m s pp s; mm s; 123 ,
the poisson analogs of m , m
m s mm s; pp s; 123 ,
the backtting estimate 123 is now dened by m s m q m , where m
and m123 satisfy the xed - point relationships
chapter 123 of hastie and tibshirani 123 discusses an iterative algorithm for computing m
and m123 s d 123rmo pp s; mo .
123 ) 123 are
m s pp s; m
mo s d 123rm123 mm s; m123
mo s ms ,
the moment - matching property 123 is satised by m , m
and mo s m srm123 .
each of these estimators can be written in the form m e we have used m in all of our examples because it makes the computation of
h s d log m rds , a crucial part of the formulas in sections 123 ) 123 , so simple .
an even simpler choice , log m s hs for some xed matrix h , does not satisfy 123 and seems to have undesirable small - sample properties .
in theory at least we can compute h for any choice of m s m s
o xb ( cid : 123 ) , 123 , with
mo s m
without proof , is h for m :
lemma 123
for m s m s mm s; pp s , 123 ,
the derivative matrix h s
d log m rds is
123a h s d m rm md 123rm q i y d m rm md srm
and p s x x 123d m
m s pp s; 123
in figure 123 , m s pp s; mm s; m
the symmetrized version of the m
gave similar but somewhat rougher contours than those in the left panel of figure 123
there is no compelling theoretical reason for preferring m
to m , though they seem closer in structure to bayes and maximum likelihood estimators .
in practice , the specic choices of m and x seem more
crucial to successful estimation than does the choice between m , m
remarks .
the following remarks apply to the indicated sections .
remark a section 123 .
there is an interesting connection between moment - matching and function - preserving properties of smoothers .
for a smoother
ms ms , the condition ( cid : 123 ) m s ( cid : 123 ) s requires 123m s 123 where 123 is a column of
efron and r .
tibshirani
ones , or equivalently m123 s 123
now most smoothers preserve constants so that m123 s 123
for symmetric m we see that matching the zeroth moment is the same as preserving the constant vector .
however , most smoothers such as kernels are not symmetric , and hence will not match the zeroth moment exactly .
similarly , a smoother may preserve a vector t so mt s t without satisfying the moment - matching property m123t s t .
in general , moment - matching is equivalent to function - preserving for the transpose of the
remark b section 123 .
what is the true parameter b being estimated by
b ? suppose that s ; po m , but that m is not necessarily of the form m b in 123 .
from m we determine m s m m and then b the solution vector to the equations x 123 m y m e s 123
under reasonable conditions b will be an asymptotically normal estimate for b , in the usual manner of an mle .
for instance if m s np , with p xed and n , and if the mo estimate is homogeneous , m cs s cm s , then it is easy to show that where , with z123 s x 123 i y d e
n by b n
dm rdm ,
123 , a ba
a s lim x 123d
the bias of b as an estimate of b is of order 123rn .
x and b s lim z123d
remark c section 123 .
in case 123 , m s ms , the matrix z in 123a is a
function of b but not of m , say z s z b .
if ds is a k vector orthogonal to the columns of z b , then
this implies that the level surfaces of constant b value are k y p q 123 - dimensional at subspaces in the k - dimensional s space .
flat level surfaces tend to make delta - method covariance estimates such as 123 more accurate .
dbs x 123dx
z123 b ds s 123
remark d section 123 .
here are the total poisson deviances :
dev s , m s 123 s log
y s y m
for four choices of m : m s m 123 s; m the sef based on this m and x s 123 , i , j ;
m the sef 123 ; m the sef 123 :
123 123 123 123 .
the deviance decrease d s , m y d sm s 123 is much smaller than 123 , the square of the corresponding t - value in table 123
the naive t - value , calculated using the standard error estimate from 123 , is the right one for approximating the deviance decrease .
in this case the naive t - value is 123r123 s 123 , predicting 123 s 123 for the deviance decrease .
exponential families for density estimation
remark e section 123 .
results like 123 can be directly derived for continuous special exponential families 123 without going through the pois - son discretization argument .
suppose we estimate g y in 123 by a continu - ous version of 123 ,
m y y ,
where , for any y , m y y is a distribution over yy not necessarily satisfying h m y y dy s 123 .
we dene the sef density estimate to be g g y exp b q t y b , where bs b , b satises the maximum likeli -
y dy s t s
as well as the constraint h g
y dy s 123
z s t y y t y and let cov t corresponding to g
t y y t exp b q t y y t b m y y dy
indicate the covariance matrix of t y for the distribution on yy
then the continuous analog of 123a is
cov b s cov t
123 is the limit of 123a as the discretization 123 becomes innitely ne , after the superuous parameter b is removed .
in order to use 123 , we need to evaluate the integrals over yy involved in 123 , 123 and cov t .
the discretization argument effectively does such integrals by summation over k s 123 , 123 , .
if yy is high dimensional , we might prefer to work in the continuous mode , doing the integrals by some more efcient algorithm such as componentwise simpson rules .
remark f section 123 .
we will often be interested in the probability
p s mrm s mrn ,
m cs s cm s
rather than in m itself .
suppose that m s m s is scale homogeneous ,
the m s sef s; m , x is also scale homogeneous and p cs s p s .
familiar homogeneity properties give
s s m and
i y p123
c ) 123 .
efron and r .
tibshirani
using 123 it is not difcult to show that
cov m y
and that the diagonal elements of cov p satisfy
trv y 123
( cid : 123 ) p k
thus the comparisons in table 123 remain valid for trv .
the results for the equivalent of trv are a little less neat .
remark g section 123 .
let p s srn , the vector of empirical probabilities ,
d d p s drn ,
p x x 123dx
x 123 s np ,
d d p s drn ,
q d y p s nq ,
under the homogeneity condition 123 , h s d log p rdp , for p m rn .
then 123 can be written as
or trv s p q 123 q tr hdh dqd not depending on n .
this shows that trv and trv are o 123 as n
trv s tr dp q hdh 123 dqd
$ ( cid : 123 ) .
remark h section 123 .
suppose that the discretization of yy becomes innitely ne , with k going to innity and the kth cell yy having volume d going to zero .
under sufcient regularity conditions , p y rd will approach , an estimate of the original continuous density with approximate
var g y
var p y ( cid : 123 )
then trv , 123 , will approach var g y with cv y var g y rg y , a coefcient of variation measure for g y .
from remark g we see that cv y is typically o 123r n as n .
the estimated average value for the coefcient of variation of sef 123 was 123 .
dy s n cv y
g y dy ,
remark i section 123 .
a computationally more efcient expression than
trv s tr d p q h123dh y h123dpdh .
exponential families for density estimation
each of the three terms in 123 can be evaluated using o k multiplica - tions rather than o k .
this makes values of k as large as 123 practical .
letting g s x 123dx , the o k computing formula is
x 123dx q d hd
q tr g
a ( cid : 123 ) ( cid : 123 ) a for matrix a , and similarly for trv , substituting d for d .
when m s ms the middle term in 123 equals ( cid : 123 ) m a , where
trv s tr g
a s m s r
m m rm m ,
and s equals s for trv or m for trv .
for the trv case a f 123 , but a can blow up for trv .
the calculations of section 123 replace a with in the trv case .
min a , 123
remark j section 123 .
the degrees of freedom formula 123 can be
123 df s tr d p q h y pdh or df s p q 123 q tr d h y pdh .
the p term corresponds to the e part of the sef denition 123 , while the
h term corresponds to the choice of the carrier m .
the pdh subtraction term corrects for collinearity between the carrier and the exponential family terms .
the degrees of freedom denition for p rather than m , as in remark f , subtracts 123 from formula 123 or 123 .
it takes o k multiplications to compute df or df from either formula .
remark k section 123 .
write s s np as in remark g and consider expres - sion 123 for df m as n with p xed .
assuming m s s cm s , 123 , the approximation dfs do does not depend on n and so is o 123 just as in 123 .
using higher - order expansions it is easy to show that so at least in this sense df is a good approximation to df m .
the corre - sponding results hold for df , trv , and trv .
df m y df s o 123rn ,
( cid : 123 ) .
( cid : 123 )
remark l section 123 .
the poisson residual degrees of freedom formula
rdf m s k m y 123df m q trv m
is always nonnegative .
to prove this we use 123b to write and note that e s , m s m , n .
the proof is completed with the fact that the poisson deviance dev m , n is a jointly convex function of m , n .
rdf m s e dev s , m y dev m , n ,
efron and r .
tibshirani
the rdf estimates 123 are not necessarily nonnegative .
they become so
if we replace k m in 123 with the taylor series estimates
k s tr dd
or k s tr dd s p q 123 ,
but these were poor approximations in our examples .
remark m section 123 .
we tried a loess smoother cleveland and gross for the galaxy data , in the form of a generalized additive poisson model hastie and tibshirani 123 .
we used a degree 123 loess model , that is , one that ts second degree polynomials locally .
this gave an expected de - viance picture similar to figure 123
this is not surprising given the similarity between the moment - matching and function - preserving properties mentioned in remark a .
