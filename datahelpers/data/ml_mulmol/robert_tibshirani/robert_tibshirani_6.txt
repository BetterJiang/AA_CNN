introduction .
automatic model - building algorithms are familiar , and sometimes notorious , in the linear model literature : forward selection , backward elimination , all subsets regression and various combinations are used to auto - matically produce good linear models for predicting a response y on the basis of some measured covariates x123 , x123 , .
goodness is often dened in terms of prediction accuracy , but parsimony is another important criterion : simpler mod - els are preferred for the sake of scientic insight into the x y relationship .
two promising recent model - building algorithms , the lasso and forward stagewise lin -
received march 123; revised january 123
123supported in part by nsf grant dms - 123 - 123 and nih grant 123r123 - eb123
123supported in part by nsf grant dms - 123 - 123 and nih grant r123 - eb123 - 123
123supported in part by nsf grant dms - 123 - 123 and nih grant r123 - eb123 - 123
123supported in part by nsf grant dms - 123 - 123 and nih grant 123r123 - ca123
ams 123 subject classication .
key words and phrases .
lasso , boosting , linear regression , coefcient paths , variable selection .
efron , hastie , johnstone and tibshirani
ear regression , will be discussed here , and motivated in terms of a computationally simpler method called least angle regression .
least angle regression ( lars ) relates to the classic model - selection method known as forward selection , or forward stepwise regression , described in weisberg ( ( 123 ) , section 123 ) : given a collection of possible predictors , we select the one having largest absolute correlation with the response y , say xj123 , and perform simple linear regression of y on xj123
this leaves a residual vector orthogonal to xj123 , now considered to be the response .
we project the other predictors orthogonally to xj123 and repeat the selection process .
after k steps this results in a set of predictors xj123 , xj123 , .
, xjk that are then used in the usual way to construct a k - parameter linear model .
forward selection is an aggressive tting technique that can be overly greedy , perhaps eliminating at the second step useful predictors that happen to be correlated with xj123
forward stagewise , as described below , is a much more cautious version of forward selection , which may take thousands of tiny steps as it moves toward a nal model .
it turns out , and this was the original motivation for the lars algorithm , that a simple formula allows forward stagewise to be implemented using fairly large steps , though not as large as a classic forward selection , greatly reducing the computational burden .
the geometry of the algorithm , described in section 123 , suggests the name least angle regression .
it then happens that this same geometry applies to another , seemingly quite different , selection method called the lasso ( tibshirani ( 123 ) ) .
the larslassostagewise connection is conceptually as well as computationally useful .
the lasso is described next , in terms of the main example used in this paper .
table 123 shows a small part of the data for our main example .
ten baseline variables , age , sex , body mass index , average blood pressure and six blood serum measurements , were obtained for each of n = 123 diabetes
diabetes study : 123 diabetes patients were measured on 123 baseline variables; a prediction model
was desired for the response variable , a measure of disease progression one year after baseline
least angle regression
yi = 123 ,
xij = 123 ,
patients , as well as the response of interest , a quantitative measure of disease progression one year after baseline .
the statisticians were asked to construct a model that predicted response y from covariates x123 , x123 , .
two hopes were evident here , that the model would produce accurate baseline predictions of response for future patients and that the form of the model would suggest which covariates were important factors in disease progression .
the lasso is a constrained version of ordinary least squares ( ols ) .
let x123 , x123 , .
, xm be n - vectors representing the covariates , m = 123 and n = 123 in the diabetes study , and let y be the vector of responses for the n cases .
by location and scale transformations we can always assume that the covariates have been standardized to have mean 123 and unit length , and that the response has mean 123 , for j = 123 , 123 ,
a candidate vector of regression coefcients ( cid : 123 ) prediction vector ( cid : 123 ) , ( cid : 123 ) = m ( cid : 123 )
this is assumed to be the case in the theory which follows , except that numerical results are expressed in the original units of the diabetes example .
j = x ( cid : 123 ) ) = ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) 123 = n ( cid : 123 ) ( yi ( cid : 123 ) i ) 123
( cid : 123 ) ) be the absolute norm of ( cid : 123 ) , ) = m ( cid : 123 ) ( cid : 123 ) ) subject to a bound t on t ( the lasso chooses ( cid : 123 ) by minimizing s (
with total squared error
( xnm = ( x123 , x123 , .
, xm ) )
lasso : minimize s (
subject to t (
let t (
quadratic programming techniques can be used to solve ( 123 ) though we will present an easier method here , closely related to the homotopy method of osborne , presnell and turlach ( 123a ) .
the left panel of figure 123 shows all lasso solutions ( cid : 123 ) as t increases from 123 , where ( cid : 123 )
( t ) for the diabetes study , equals the ols regression vector , the constraint in ( 123 ) no longer binding .
we see that the lasso tends to shrink the ols coefcients toward 123 , more so for small values of t .
shrinkage often improves prediction accuracy , trading off decreased variance for increased bias as discussed in hastie , tibshirani and friedman ( 123 ) .
= 123 , to t = 123 , where ( cid : 123 )
efron , hastie , johnstone and tibshirani
estimates of regression coefcients ( cid : 123 ) panel ) lasso estimates , as a function of t = ( cid : 123 ) j , j = 123 , 123 , .
, 123 , for the diabetes study .
( left j| .
the covariates enter the regression equation sequentially as t increases , in order j = 123 , 123 , 123 , 123 , .
( right panel ) the same plot for forward stagewise linear regression .
the two plots are nearly identical , but differ slightly for large t as shown in the track of covariate 123
a subset of the covariates have nonzero values of ( cid : 123 )
forward stagewise linear regression , henceforth called stagewise , is an
the lasso also has a parsimony property : for any given constraint value t , only j .
at t = 123 , for example , only variables 123 , 123 , 123 and 123 enter the lasso regression model ( 123 ) .
if this model provides adequate predictions , a crucial question considered in section 123 , the statisticians could report these four variables as the important ones .
iterative technique that begins with ( cid : 123 ) = 123 and builds up the regression function in successive small steps .
if ( cid : 123 ) is the current stagewise estimate , let c ( ( cid : 123 ) ) be the vector of current correlations ( cid : 123 ) c = c ( ( cid : 123 ) ) = x so that ( cid : 123 ) cj is proportional to the correlation between covariate xj and the current
j = arg max| ( cid : 123 ) cj|
and ( cid : 123 ) ( cid : 123 ) + sign ( ( cid : 123 ) c j ) x j ,
residual vector .
the next step of the stagewise algorithm is taken in the direction of the greatest current correlation ,
with some small constant .
small is important here : the big choice = | ( cid : 123 ) c j
leads to the classic forward selection technique , which can be overly greedy , impulsively eliminating covariates which are correlated with x j .
the stagewise procedure is related to boosting and also to friedmans mart algorithm
least angle regression
( friedman ( 123 ) ) ; see section 123 , as well as hastie , tibshirani and friedman ( ( 123 ) , chapter 123 and algorithm 123 ) .
the right panel of figure 123 shows the coefcient plot for stagewise applied to the diabetes data .
the estimates were built up in 123 stagewise steps ( making in ( 123 ) small enough to conceal the etch - a - sketch staircase seen in figure 123 , section 123 ) .
the striking fact is the similarity between the lasso and stagewise estimates .
although their denitions look completely different , the results are nearly , but not exactly , identical .
the main point of this paper is that both lasso and stagewise are variants of a basic procedure called least angle regression , abbreviated lars ( the s suggesting lasso and stagewise ) .
section 123 describes the lars algorithm while section 123 discusses modications that turn lars into lasso or stagewise , reducing the computational burden by at least an order of magnitude for either one .
sections 123 and 123 verify the connections stated in section 123
least angle regression is interesting in its own right , its simple structure lending itself to inferential analysis .
section 123 analyzes the degrees of freedom of a lars regression estimate .
this leads to a cp type statistic that suggests which estimate we should prefer among a collection of possibilities like those in figure 123
a particularly simple cp approximation , requiring no additional computation
beyond that for the ( cid : 123 )
vectors , is available for lars .
section 123 briey discusses computational questions .
an efcient s program for all three methods , lars , lasso and stagewise , is available .
section 123 elaborates on the connections with boosting .
the lars algorithm .
least angle regression is a stylized version of the stagewise procedure that uses a simple mathematical formula to accelerate the computations .
only m steps are required for the full set of solutions , where m is the number of covariates : m = 123 in the diabetes example compared to the 123 steps used in the right panel of figure 123
this section describes the lars algorithm .
modications of lars that produce lasso and stagewise solutions are discussed in section 123 , and veried in sections 123 and 123
section 123 uses the simple structure of lars to help analyze its estimation properties .
the lars procedure works roughly as follows .
as with classic forward selection , we start with all coefcients equal to zero , and nd the predictor most correlated with the response , say xj123
we take the largest step possible in the direction of this predictor until some other predictor , say xj123 , has as much correlation with the current residual .
at this point lars parts company with forward selection .
instead of continuing along xj123 , lars proceeds in a direction equiangular between the two predictors until a third variable xj123 earns its way into the most correlated set .
lars then proceeds equiangularly between xj123 , xj123 and xj123 , that is , along the least angle direction , until a fourth variable enters , and
efron , hastie , johnstone and tibshirani
c ( ( cid : 123 ) ) = x
( y ( cid : 123 ) ) = x
the remainder of this section describes the algebra necessary to execute the equiangular strategy .
as usual the algebraic details look more complicated than the simple underlying geometry , but they lead to the highly efcient computational algorithm described in section 123
( cid : 123 ) , ( 123 ) , in successive steps , each step adding lars builds up estimates ( cid : 123 ) = x one covariate to the model , so that after k steps just k of the ( cid : 123 ) j s are nonzero .
figure 123 illustrates the algorithm in the situation with m = 123 covariates , x = ( x123 , x123 ) .
in this case the current correlations ( 123 ) depend only on the projection y123 of y into the linear space l ( x ) spanned by x123 and x123 , the algorithm begins at ( cid : 123 ) 123 = 123 ( remembering that the response has had its mean subtracted off , as in ( 123 ) ) .
figure 123 has y123 ( cid : 123 ) 123 making a smaller angle with x123 than x123 , that is , c123 ( ( cid : 123 ) 123 ) > c123 ( ( cid : 123 ) 123 ) .
lars then augments ( cid : 123 ) 123 in the direction stagewise would choose ( cid : 123 ) 123 equal to some small value , and then repeat the process many times .
classic forward selection would take ( cid : 123 ) 123 large enough to make ( cid : 123 ) 123 equal y123 , the projection of y into l ( x123 ) .
lars uses an intermediate value of ( cid : 123 ) 123 , the value that makes y123 ( cid : 123 ) , equally correlated with x123 and x123; that is , y123 ( cid : 123 ) 123 bisects the angle between x123 and x123 , so c123 ( ( cid : 123 ) 123 ) = c123 ( ( cid : 123 ) 123 ) .
( cid : 123 ) 123 = ( cid : 123 ) 123 + ( cid : 123 ) 123x123
of x123 , to
l ( x123 , x123 ) .
beginning at ( cid : 123 ) 123 = 123 , the residual vector y123 ( cid : 123 ) 123 has greater correlation with x123 than x123; fig .
the lars algorithm in the case of m = 123 covariates; y123 is the projection of y into the next lars estimate is ( cid : 123 ) 123 = ( cid : 123 ) 123 + ( cid : 123 ) 123x123 , where ( cid : 123 ) 123 is chosen such that y123 ( cid : 123 ) 123 bisects the angle between x123 and x123; then ( cid : 123 ) 123 = ( cid : 123 ) 123 + ( cid : 123 ) 123u123 , where u123 is the unit bisector; ( cid : 123 ) 123 = y123 in the case m = 123 , but not for the case m > 123; see figure 123
the staircase indicates a typical stagewise path .
here lars gives the stagewise track as 123 , but a modication is necessary to guarantee agreement in higher dimensions; see section 123 .
least angle regression
( cid : 123 ) 123 = ( cid : 123 ) 123 + ( cid : 123 ) 123u123 ,
let u123 be the unit vector lying along the bisector .
the next lars estimate is
with ( cid : 123 ) 123 chosen to make ( cid : 123 ) 123 = y123 in the case m = 123
with m > 123 covariates , ( cid : 123 ) 123 would be smaller , leading to another change of direction , as illustrated in is motivated by the fact that it is easy to calculate the step sizes ( cid : 123 ) 123 , ( cid : 123 ) 123 ,
figure 123
the staircase in figure 123 indicates a typical stagewise path
theoretically , short - circuiting the small stagewise steps .
xa = ( sj xj ) ja ,
subsequent lars steps , beyond two covariates , are taken along equiangular vectors , generalizing the bisector u123 in figure 123
we assume that the covariate vectors x123 , x123 , .
, xm are linearly independent .
for a a subset of the indices ( 123 , 123 , .
, m ) , dene the matrix where the signs sj equal 123
let 123a being a vector of 123s of length equaling |a| , the size of a .
the where wa = aag
equiangular vector ua = xawa
axa and aa = ( 123
ga = x
is the unit vector making equal angles , less than 123
, with the columns of xa ,
aua = aa123a and ( cid : 123 ) ua ( cid : 123 ) 123 = 123
procedure we begin at ( cid : 123 ) 123 = 123 and build up ( cid : 123 ) by steps , larger steps in the lars case .
suppose that ( cid : 123 ) a is the current lars estimate and that
we can now fully describe the lars algorithm .
as with the stagewise
is the vector of current correlations ( 123 ) .
the active set a is the set of indices corresponding to covariates with the greatest absolute current correlations ,
we compute xa , aa and ua as in ( 123 ) ( 123 ) , and also the inner product vector
then the next step of the lars algorithm updates ( cid : 123 ) a , say to
c = max
( cid : 123 ) c = x and a = ( j : | ( cid : 123 ) cj| = ( cid : 123 ) sj = sign ( ( cid : 123 ) cj )
for j a ,
efron , hastie , johnstone and tibshirani
aa + aj
( cid : 123 ) = min
( ) = ( cid : 123 ) a y ( ) |cj ( ) | = ( cid : 123 )
indicates that the minimum is taken over only positive components within
each choice of j in ( 123 ) .
formulas ( 123 ) and ( 123 ) have the following interpretation : dene
cj ( ) = x
( cid : 123 ) = ( cid : 123 ) cj aj .
for > 123 , so that the current correlation for j a , ( 123 ) ( 123 ) yield c ( cid : 123 ) cj ) / ( aa aj ) .
likewise cj ( ) , the current correlation for the showing that all of the maximal absolute current correlations decline equally .
for j ac , equating ( 123 ) with ( 123 ) shows that cj ( ) equals the maximal c + ( cid : 123 ) cj ) / ( aa + aj ) .
therefore ( cid : 123 ) value at = ( in ( 123 ) is the smallest positive value of such that some new index ( cid : 123 ) reversed covariate xj , achieves maximality at ( the active set; ( cid : 123 ) j ) ; the new maximum absolute correlation is ( cid : 123 ) figure 123 concerns the lars analysis of the diabetes data .
the complete algorithm required only m = 123 steps of procedure ( 123 ) ( 123 ) , with the variables
j is the minimizing index in ( 123 ) , and the new active set a+ is
c ( cid : 123 ) aa .
lars analysis of the diabetes study : ( left ) estimates of regression coefcients ( cid : 123 ) j|; plot is slightly different than either lasso or stagewise , j = 123 , 123 , .
, 123; plotted versus set ( 123 ) in order 123 , 123 , 123 , 123 , .
, 123; heavy curve shows maximum current correlation ( cid : 123 ) figure 123; ( right ) absolute current correlations as function of lars step; variables enter active
least angle regression
of the regression coefcients ( cid : 123 )
joining the active set a in the same order as for the lasso : 123 , 123 , 123 , 123 , .
tracks j are nearly but not exactly the same as either the
active set increases .
lasso or stagewise tracks of figure 123
the right panel shows the absolute current correlations
j ( y ( cid : 123 ) k123 ) |
section 123 makes use of the relationship between least angle regression and ordinary least squares illustrated in figure 123
suppose lars has just completed
| ( cid : 123 ) ckj| = |x for variables j = 123 , 123 , .
, 123 , as a function of the lars step k .
the maximum ck = max ( | ( cid : 123 ) ckj| ) = ( cid : 123 ) henceforth having | ( cid : 123 ) ckj| = ( cid : 123 ) declines with k , as it must .
at each step a new variable j joins the active set , ck .
the sign sj of each xj in ( 123 ) stays constant as the step k 123 , giving ( cid : 123 ) k123 , and is embarking upon step k .
the active set ak , ( 123 ) , ( cid : 123 ) k123 l ( xk123 ) , is will have k members , giving xk , gk , ak and uk as in ( 123 ) ( 123 ) ( here replacing subscript a with k ) .
let yk indicate the projection of y into l ( xk ) , which , since yk = ( cid : 123 ) k123 + xkg correlations in ak all equal ( cid : 123 ) since uk is a unit vector , ( 123 ) says that yk ( cid : 123 ) k123 has length comparison with ( 123 ) shows that the lars estimate ( cid : 123 ) k lies on the line
k ( y ( cid : 123 ) k123 ) = ( cid : 123 )
k ( y ( cid : 123 ) k123 ) = ( cid : 123 ) k123 +
the last equality following from ( 123 ) and the fact
the signed current
at each stage the lars estimate ( cid : 123 ) k approaches , but does not reach , the corresponding ols estimate yk .
efron , hastie , johnstone and tibshirani
( cid : 123 ) k123 = ( cid : 123 ) kk
from ( cid : 123 ) k123 to yk , it is easy to see that ( cid : 123 ) k , ( 123 ) , is always less than k , so that ( cid : 123 ) k lies closer than yk to ( cid : 123 ) k123
figure 123 shows the successive lars estimates ( cid : 123 ) k always approaching but never reaching the ols estimates yk .
by convention the algorithm takes ( cid : 123 ) m = m = ( cid : 123 ) the exception is at the last stage : since am contains all covariates , ( 123 ) is not
m equal the ols estimate for the full set of m covariates .
the lars algorithm is computationally thrifty .
organizing the calculations correctly , the computational cost for the entire m steps is of the same order as that required for the usual least squares solution for the full set of m covariates .
section 123 describes an efcient lars program available from the authors .
with the modications described in the next section , this program also provides economical lasso and stagewise solutions .
modied versions of least angle regression .
figures 123 and 123 show lasso , stagewise and lars yielding remarkably similar estimates for the diabetes data .
the similarity is no coincidence .
this section describes simple modications of the lars algorithm that produce lasso or stagewise estimates .
besides improved computational efciency , these relationships elucidate the methods rationale : all three algorithms can be viewed as moderately greedy forward stepwise procedures whose forward progress is determined by compromise among the currently most correlated covariates .
lars moves along the most obvious compromise direction , the equiangular vector ( 123 ) , while lasso and stagewise put some restrictions on the equiangular strategy .
the larslasso relationship .
the full set of lasso solutions , as shown for the diabetes study in figure 123 , can be generated by a minor modication of the lars algorithm ( 123 ) ( 123 ) .
our main result is described here and veried in section 123
it closely parallels the homotopy method in the papers by osborne , presnell and turlach ( 123a , b ) , though the lars approach is somewhat more
be a lasso solution ( 123 ) , with ( cid : 123 ) = x the sign of any nonzero coordinate ( cid : 123 ) correlation ( cid : 123 ) cj = x j ) = sign ( ( cid : 123 ) cj ) = sj;
j ( y ( cid : 123 ) ) ,
then it is easy to show that j must agree with the sign sj of the current
see lemma 123 of section 123
the lars algorithm does not enforce restriction ( 123 ) , but it can easily be modied to do so .
least angle regression
where j ( ) = ( cid : 123 )
suppose we have just completed a lars step , giving a new active set a as
in ( 123 ) , and that the corresponding lars estimate ( cid : 123 ) a corresponds to a lasso solution ( cid : 123 ) = x dene ( cid : 123 ) d to be the m - vector equaling sj waj for j a and zero elsewhere .
moving
a vector of length the size of a , and ( somewhat abusing subscript notation )
wa = aag
( ) = x ( ) ,
in the positive direction along the lars line ( 123 ) , we see that j = ( cid : 123 ) for j a .
therefore j ( ) will change sign at the rst such change occurring at ( cid : 123 ) = min say for covariate x j ; ( cid : 123 ) equals innity by denition if there is no j > 123
if ( cid : 123 ) is less than ( cid : 123 ) , ( 123 ) , then j ( ) cannot be a lasso solution for > ( cid : 123 ) since since |c j ( ) | = ( cid : 123 )
the sign restriction ( 123 ) must be violated : j ( ) has changed sign while c j ( ) has not .
( the continuous function c j ( ) cannot change sign within a single lars step
if ( cid : 123 ) < ( cid : 123 ) , stop the ongoing lars step at = ( cid : 123 ) and + ( cid : 123 ) ua and a+ = a ( ( cid : 123 )
c aa > 123 , ( 123 ) . )
j from the calculation of the next equiangular direction .
that is ,
rather than ( 123 ) .
theorem 123
under the lasso modication , and assuming the one at a time
condition discussed below , the lars algorithm yields all lasso solutions .
the active sets a grow monotonically larger as the original lars algorithm progresses , but the lasso modication allows a to decrease .
one at a time means that the increases and decreases never involve more than a single index j .
this is the usual case for quantitative data and can always be realized by adding a little jitter to the y values .
section 123 discusses tied situations .
the lasso diagram in figure 123 was actually calculated using the modied lars algorithm .
modication ( 123 ) came into play only once , at the arrowed point in the left panel .
there a contained all 123 indices while a+ = a ( 123 ) .
variable 123 had an effect on the tracks of the others , noticeably ( cid : 123 ) was restored to the active set one lars step later , the next and last step then all the way to the full ols solution .
the brief absence of variable 123 123
the price of using lasso instead of unmodied lars comes in the form of added steps , 123 instead of 123 in this example .
for the more complicated quadratic model of section 123 , the comparison was 123 lasso steps versus 123 for lars .
efron , hastie , johnstone and tibshirani
the larsstagewise relationship .
the staircase in figure 123 indicates
how the stagewise algorithm might proceed forward from ( cid : 123 ) 123 , a point of equal current correlations ( cid : 123 ) c123 = ( cid : 123 ) c123 , ( 123 ) .
the rst small step has ( randomly ) selected index j = 123 , taking us to ( cid : 123 ) 123 + x123
now variable 123 is more correlated , 123 ( y ( cid : 123 ) 123 x123 ) , forcing j = 123 to be the next stagewise choice and so on .
123 ( y ( cid : 123 ) 123 x123 ) > x
suppose that the stagewise procedure has taken n steps of innitesimal size
we will consider an idealized stagewise procedure in which the step size goes to zero .
this collapses the staircase along the direction of the bisector u123 in figure 123 , making the stagewise and lars estimates agree .
they always agree for m = 123 covariates , but another modication is necessary for lars to produce from some previous estimate ( cid : 123 ) , with stagewise estimates in general .
section 123 veries the main result described next .
it is easy to show , as in lemma 123 of section 123 , that nj = 123 for j not in the active set a dened by the current correlations x
j = 123 , 123 , .
j ( y ( cid : 123 ) ) , ( 123 ) .
letting
nj # ( steps with selected index j ) ,
with pa indicating the coordinates of p for j a , the new estimate is
p ( n123 , n123 , .
, nm ) / n , = ( cid : 123 ) + n xapa
( notice that the stagewise steps are taken along the directions sj xj . )
the lars algorithm ( 123 ) progresses along where wa = aag
comparing ( 123 ) with ( 123 ) shows that lars cannot agree with stagewise if wa has negative components , since pa is nonnegative .
to put it another way , the direction of stagewise progress xapa must lie in the convex cone generated by the columns of xa ,
sj xj pj , pj 123
v = ( cid : 123 )
if ua ca then there is no contradiction between ( 123 ) and ( 123 ) .
if not it seems natural to replace ua with its projection into ca , that is , the nearest point in the convex cone .
stagewise modification .
proceed as in ( 123 ) ( 123 ) , except with ua replaced by u b , the unit vector lying along the projection of ua into ca .
( see figure 123 in section 123 )
least angle regression
theorem 123
under the stagewise modication , the lars algorithm yields
all stagewise solutions .
the vector u b in the stagewise modication is the equiangular vector ( 123 ) for
the subset ( cid : 123 ) b a corresponding to the face of ca into which the projection falls .
there the set a = ( 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 ) was decreased to ( cid : 123 ) b = a ( 123 , 123 ) .
it took a total of 123 modied lars steps to reach the full ols solution m
stagewise is a lars type algorithm that allows the active set to decrease by one or more indices .
this happened at the arrowed point in the right panel of figure 123 :
the three methods , lars , lasso and stagewise , always reach ols eventually , but lars does so in only m steps while lasso and , especially , stagewise can take longer .
for the m = 123 quadratic model of section 123 , stagewise took 123 steps .
according to theorem 123 the difference between successive stagewisemodied
lars estimates is
= ( cid : 123 ) u b
= ( cid : 123 ) x b w b ,
j ) = sj ,
we can now make a useful comparison of the three methods :
as in ( 123 ) .
since u b exists in the convex cone ca , w b must have nonnegative components .
this says that the difference of successive coefcient estimates for
coordinate j ( cid : 123 ) b satises j ( y ( cid : 123 ) ) ) .
where sj = sign ( x 123
stagewisesuccessive differences of ( cid : 123 ) j ( y ( cid : 123 ) ) ; correlation ( cid : 123 ) cj = x the successive difference property ( 123 ) makes the stagewise ( cid : 123 ) move monotonically away from 123
reversals are possible only if ( cid : 123 ) cj changes sign
from this point of view , lasso is intermediate between the lars and stagewise
j agrees in sign with ( cid : 123 ) cj ;
j is resting between two periods of change .
this happened to variable 123
larsno sign restrictions ( but see lemma 123 of section 123 ) .
j agree in sign with the current
in figure 123 between the 123th and 123th stagewise - modied lars steps .
simulation study .
a small simulation study was carried out comparing the lars , lasso and stagewise algorithms .
the x matrix for the simulation was based on the diabetes example of table 123 , but now using a quadratic model having m = 123 predictors , including interactions and squares of the 123 original
quadratic model 123 main effects , 123 interactions , 123 squares ,
efron , hastie , johnstone and tibshirani
the last being the squares of each xj except the dichotomous variable x123
the true mean vector for the simulation was = x , where was obtained by running lars for 123 steps on the original ( x , y ) diabetes data ( agreeing in this case with the 123 - step lasso or stagewise analysis ) .
subtracting from a centered version of the original y vector of table 123 gave a vector = y of n = 123 residuals .
the true r123 for this model , ( cid : 123 ) ( cid : 123 ) 123 / ( ( cid : 123 ) ( cid : 123 ) 123 + ( cid : 123 ) ( cid : 123 ) 123 ) , equaled 123 .
123 simulated response vectors y
were generated from the model
figure 123 compares the lars , lasso and stagewise estimates .
for a given
n ) a random sample , with replacement , from the compo - ) , yielding a sequence of estimates ( cid : 123 ) ( k ) nents of .
the lars algorithm with k = 123 steps was run for each simulated , k = 123 , 123 , .
, 123 , and like - data set ( x , y estimate ( cid : 123 ) dene the proportion explained pe ( ( cid : 123 ) ) to be wise using the lasso and stagewise algorithms .
pe ( ( cid : 123 ) ) = 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 / ( cid : 123 ) ( cid : 123 ) 123 , so pe ( 123 ) = 123 and pe ( ) = 123
the solid curve graphs the average of pe ( ( cid : 123 ) ( k ) over the 123 simulations , versus step number k for lars , k = 123 , 123 , .
horizontal axis is now the average number of nonzero ( cid : 123 ) j terms composing ( cid : 123 ) ( k ) the corresponding curves are graphed for lasso and stagewise , except that the averaged 123 nonzero terms with stagewise , compared to
123 for lasso and 123 for lars .
figure 123s most striking message is that the three algorithms performed almost identically , and rather well .
the average proportion explained rises quickly ,
simulation study comparing lars , lasso and stagewise algorithms; 123 replications of model ( 123 ) ( 123 ) .
solid curve shows average proportion explained , ( 123 ) , for lars estimates as function of number of steps k = 123 , 123 , .
, 123; lasso and stagewise give nearly identical results; small dots indicate plus or minus one standard deviation over the 123 simulations .
classic forward selection ( heavy dashed curve ) rises and falls more abruptly .
least angle regression
the light dots display the small standard deviation of pe ( ( cid : 123 ) ( k ) reaching a maximum of 123 at k = 123 , and then declines slowly as k grows ) over the 123 simulations , roughly 123 .
stopping at any point between k = 123 and 123 typically with true predictive r123 about 123 , compared to the ideal value 123 the dashed curve in figure 123 tracks the average proportion explained by classic forward selection .
it rises very quickly , to a maximum of 123 after k = 123 steps , and then falls back more abruptly than the larslassostagewise curves .
this behavior agrees with the characterization of forward selection as a dangerously
other lars modications .
here are a few more examples of lars type
positive lasso .
constraint ( 123 ) can be strengthened to
subject to t (
) t and all ( cid : 123 )
this would be appropriate if the statisticians or scientists believed that the variables xj must enter the prediction equation in their dened directions .
situation ( 123 ) is a more difcult quadratic programming problem than ( 123 ) , but it can be solved by a further modication of the lasso - modied lars
algorithm : change | ( cid : 123 ) cj| to ( cid : 123 ) cj at both places in ( 123 ) , set sj = 123 instead of ( 123 )
and change ( 123 ) to
( cid : 123 ) = min
very large choices of t .
the positive lasso usually does not converge to the full ols solution m , even for in the same way , and has the property that the ( cid : 123 ) the changes above amount to considering the xj as generating half - lines rather than full one - dimensional spaces .
a positive stagewise version can be developed of covariates , for example , a123 = ( 123 , 123 , 123 , 123 ) in the diabetes study .
instead of ( cid : 123 ) larsols hybrid .
after k steps the lars algorithm has identied a set ak might prefer k , the ols coefcients based on the linear model with covariates in akusing lars to nd the model but not to estimate the coefcients .
besides looking more familiar , this will always increase the usual empirical r123 measure of ( cid : 123 ) k ) r123 ( t ( though not necessarily the true tting accuracy ) ,
j tracks are always monotone .
( cid : 123 ) k ) = 123 123
where k = ( cid : 123 ) k / k as in ( 123 ) .
efron , hastie , johnstone and tibshirani
the increases in r123 were small in the diabetes example , on the order of 123 for k 123 compared with r123 = 123 , which is expected from ( 123 ) since we reason k and ( cid : 123 ) k123 ) was small .
for the same would usually continue lars until r123 ( k are likely to lie near each other as they did in the diabetes
main effects rst .
it is straightforward to restrict the order in which variables are allowed to enter the lars algorithm .
for example , having obtained a123 = do this we begin lars again , replacing y with y ( cid : 123 ) 123 and x with the n 123 matrix ( 123 , 123 , 123 , 123 ) for the diabetes study , we might then wish to check for interactions .
to whose columns represent the interactions x123 : 123 , x123 : 123 , .
, x123 : 123
backward lasso .
the lassomodied lars algorithm can be run backward ,
are nonzero , their signs must agree with the signs sj that the current correlations had during the nal lars step .
this allows us to calculate the last equiangular
starting from the full ols solution m .
assuming that all the coordinates of m direction ua , ( 123 ) ( 123 ) .
moving backward from ( cid : 123 ) m ua , we eliminate from the active set the index of the rst ( cid : 123 ) ( ) = ( cid : 123 ) m m along the line that becomes zero .
continuing backward , we keep track of all coefcients ( cid : 123 ) current correlations ( cid : 123 ) cj , following essentially the same rules for changing a as in section 123 .
as in ( 123 ) , ( 123 ) the calculation of ( cid : 123 ) and ( cid : 123 ) is easy .
the crucial property of the lasso that makes backward navigation possible is ( 123 ) , which permits calculation of the correct equiangular direction ua at each step .
in this sense lasso can be just as well thought of as a backward - moving algorithm .
this is not the case for lars or stagewise , both of which are inherently
degrees of freedom and cp estimates .
figures 123 and 123 show all possible lasso , stagewise or lars estimates of the vector for the diabetes data .
the of course , so we need some rule for selecting among the possibilities .
this section concerns a cp - type selection criterion , especially as it applies to the choice of lars estimate .
scientists want just a single ( cid : 123 ) let ( cid : 123 ) = g ( y ) represent a formula for estimating from the data vector y .
here , as usual in regression situations , we are considering the covariate vectors x123 , x123 , .
, xm xed at their observed values .
we assume that given the xs , y is generated according to an homoskedastic model y ( , 123i ) ,
meaning that the components yi are uncorrelated , with mean i and variance 123
taking expectations in the identity
( ( cid : 123 ) i i ) 123 = ( yi ( cid : 123 ) i ) 123 ( yi i ) 123 + 123 ( ( cid : 123 ) i i ) ( yi i ) ,
least angle regression
cov ( ( cid : 123 ) i , yi )
the last term of ( 123 ) leads to a convenient denition of the degrees of freedom
and summing over i , yields
for an estimator ( cid : 123 ) = g ( y ) ,
and a cp - type risk estimation formula ,
cov ( ( cid : 123 ) i , yi ) / 123 ,
df , 123 = n ( cid : 123 ) cp ( ( cid : 123 ) ) = ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) 123
n + 123df , 123
if 123 and df , 123 are known , cp ( ( cid : 123 ) ) is an unbiased estimator of the true risk e ( ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 / 123 ) .
for linear estimators ( cid : 123 ) = my , model ( 123 ) makes df , 123 =
trace ( m ) , equaling the usual denition of degrees of freedom for ols , and coinciding with the proposal of mallows ( 123 ) .
section 123 of efron and tibshirani ( 123 ) and section 123 of efron ( 123 ) discuss formulas ( 123 ) and ( 123 ) and their role in cp , akaike information criterion ( aic ) and steins unbiased risk estimated ( sure ) estimation theory , a more recent reference being ye ( 123 ) .
practical use of cp formula ( 123 ) requires preliminary estimates of , 123 and df , 123
in the numerical results below , the usual ols estimates and 123 from the full ols model were used to calculate bootstrap estimates of df , 123; bootstrap
were then generated according to
n ( , 123 )
independently repeating ( 123 ) say b times gives straightforward estimates for the covariances in ( 123 ) ,
i ( b ) ( y i ( b ) y
df = n ( cid : 123 ) ( cid : 123 ) covi / 123
the left panel of figure 123 shows ( cid : 123 ) normality is not crucial in ( 123 ) .
nearly the same results were obtained using were resampled from e = y .
mates ( cid : 123 ) k , k = 123 , 123 , .
, m = 123
it portrays a startlingly simple situation that we dfk for the diabetes data lars esti - df ( ( cid : 123 ) k ) = k .
will call the simple approximation ,
, where the components of e
efron , hastie , johnstone and tibshirani
degrees of freedom for lars estimates ( cid : 123 ) k : ( left ) diabetes study , table 123 , k = 123 , 123 , .
, m = 123; ( right ) quadratic model ( 123 ) for the diabetes data , m = 123
solid line is simple approximation dfk = k .
dashed lines are approximate 123% condence intervals for the bootstrap estimates .
each panel based on b = 123 bootstrap replications .
the right panel also applies to the diabetes data , but this time with the quadratic model ( 123 ) , having m = 123 predictors .
we see that the simple approxima - tion ( 123 ) is again accurate within the limits of the bootstrap computation ( 123 ) , where b = 123 replications were divided into 123 groups of 123 each in order to estimate the risk of a k - step lars estimator ( cid : 123 ) k by calculate student - t condence intervals .
if ( 123 ) can be believed , and we will offer some evidence in its behalf , we can
cp ( ( cid : 123 ) k ) = ( cid : 123 ) y ( cid : 123 ) k
( cid : 123 ) 123 / 123 n + 123k .
the formula , which is the same as the cp estimate of risk for an ols estimator based on a subset of k preselected predictor vectors , has the great advantage of not figure 123 displays cp ( ( cid : 123 ) k ) as a function of k for the two situations of figure 123
requiring any further calculations beyond those for the original lars estimates .
the formula applies only to lars , and not to lasso or stagewise .
minimum cp was achieved at steps k = 123 and k = 123 , respectively .
both of the minimum cp models looked sensible , their rst several selections of important covariates agreeing with an earlier model based on a detailed inspection of the data assisted by medical expertise .
the simple approximation becomes a theorem in two cases .
then the k - step lars estimate k has df ( k ) = k .
if the covariate vectors x123 , x123 , .
, xm are mutually orthogonal ,
to state the second more general setting we introduce the following condition .
least angle regression
cp estimates of risk ( 123 ) for the two situations of figure 123 : ( left ) m = 123 model has smallest cp at k = 123; ( right ) m = 123 model has smallest cp at k = 123
positive cone condition .
for all possible subsets xa of the full design
a 123a > 123 ,
where the inequality is taken element - wise .
the positive cone condition holds if x is orthogonal .
it is strictly more general than orthogonality , but counterexamples ( such as the diabetes data ) show that not all design matrices x satisfy it .
it is also easy to show that lars , lasso and stagewise all coincide under the positive cone condition , so the degrees - of - freedom formula applies to them too in
theorem 123
under the positive cone condition , df ( k ) = k .
differentiable ( see remark a . 123 in the appendix ) and set g = ( cid : 123 ) the proof , which appears later in this section , is an application of steins unbiased risk estimate ( sure ) ( stein ( 123 ) ) .
suppose that g : rn rn is almost i=123 gi / xi .
if y nn ( , 123i ) , then steins formula states that
cov ( gi , yi ) / 123 = e ( g ( y ) ) .
the left - hand side is df ( g ) for the general estimator g ( y ) .
focusing specically on lars , it will turn out that k ( y ) = k in all situations with probability 123 , but that the continuity assumptions underlying ( 123 ) and sure can fail in certain nonorthogonal cases where the positive cone condition does not hold .
efron , hastie , johnstone and tibshirani
effort at pathology to make df ( ( cid : 123 ) k ) much different than k .
a range of simulations suggested that the simple approximation is quite accurate even when the xj s are highly correlated and that it requires concerted steins formula assumes normality , y n ( , 123i ) .
a cruder delta method rationale for the simple approximation requires only homoskedasticity , ( 123 ) .
the geometry of figure 123 implies ( cid : 123 ) k
where cotk is the cotangent of the angle between uk and uk+123 ,
= yk cotk ( cid : 123 ) yk+123 yk ( cid : 123 ) ,
let vk be the unit vector orthogonal to l ( xb ) , the linear space spanned by the rst k covariates selected by lars , and pointing into l ( xk+123 ) along the direction of yk+123 yk .
for y near y we can reexpress ( 123 ) as a locally linear transformation ,
pk being the usual projection matrix from rn into l ( xk ) ; ( 123 ) holds within a neighborhood of y such that the lars choices l ( xk ) and vk remain the same .
the matrix mk has trace ( mk ) = k .
since the trace equals the degrees of freedom it is clear that ( 123 ) df ( ( cid : 123 ) k ) = k cannot hold for the lasso , since the degrees of for linear estimators , the simple approximation ( 123 ) is seen to be a delta method approximation to the bootstrap estimates ( 123 ) and ( 123 ) .
with mk = pk cotk ukv
freedom is m for the full model but the total number of steps taken can exceed m .
however , we have found empirically that an intuitively plausible result holds : the degrees of freedom is well approximated by the number of nonzero predictors in the model .
specically , starting at step 123 , let ( cid : 123 ) ( k ) be the index of the last model
in the lasso sequence containing k predictors .
then df ( ( cid : 123 ) ( cid : 123 ) ( k ) ) = k .
we do not yet in the orthogonal case , we assume that xj = ej for j = 123 , .
the lars algorithm then has a particularly simple form , reducing to soft thresholding at the order statistics of the data .
have any mathematical support for this claim .
orthogonal designs .
to be specic , dene the soft thresholding operation on a scalar y123 at threshold t
y123 + t ,
( y123; t ) =
if y123 > t , if |y123| t , if y123 < t .
the order statistics of the absolute values of the data are denoted by
|y| ( 123 ) |y| ( 123 ) |y| ( n ) |y| ( n+123 ) : = 123
we note that ym+123 , .
, yn do not enter into the estimation procedure , and so we may as well assume that m = n .
least angle regression
lemma 123
for an orthogonal design with xj = ej , j = 123 , .
, n , the kth lars
estimate ( 123 k n ) is given by yi + |y| ( k+123 ) ,
if yi > |y| ( k+123 ) , if |yi| |y| ( k+123 ) , if yi < |y| ( k+123 ) ,
aa = |a|123 / 123 ,
ua = |a|123 / 123a ,
the proof is by induction , stepping through the lars sequence .
first note that the lars parameters take a simple form in the orthogonal setting : j / ak .
ga = ia , we assume for the moment that there are no ties in the order statistics ( 123 ) , so that the variables enter one at a time .
let j ( l ) be the index corresponding to the lth order statistic , |y| ( l ) = slyj ( l ) : we will see that ak = ( j ( 123 ) , .
, j ( k ) ) .
c123 = |y| ( 123 ) .
it is easily seen that 123 = min
j y = yj , and so at the rst step lars picks variable j ( 123 ) and sets
ak , j = 123 ,
we have x
( cid : 123 ) |y| ( 123 ) |yj| ( cid : 123 ) = |y| ( 123 ) |y| ( 123 ) 123 = ( cid : 123 ) |y| ( 123 ) |y| ( 123 )
which is precisely ( 123 ) for k = 123
suppose now that step k 123 has been completed , so that ak = ( j ( 123 ) , .
, j ( k ) ) and ( 123 ) holds for k123
the current correlations ck = |y| ( k ) and ck , j = yj for j / ak .
since ak ak , j = k
123 / 123 , we have
k = min
kuk = ( cid : 123 ) |y| ( k ) |y| ( k+123 ) adding this term to k123 yields ( 123 ) for step k .
the argument clearly extends to the case in which there are ties in the order statistics ( 123 ) : if |y| ( k+123 ) = = |y| ( k+r ) , then ak ( y ) expands by r variables at step k + 123 and k+ ( y ) , = 123 , .
, r , are all determined at the same time and are equal to k+123 ( y ) .
( cid : 123 )
proof of theorem 123 ( orthogonal case ) .
the argument is particularly simple in this setting , and so worth giving separately .
first we note from ( 123 ) that k is continuous and lipschitz ( 123 ) and so certainly almost differentiable .
efron , hastie , johnstone and tibshirani
hence ( 123 ) shows that we simply have to calculate k .
inspection of ( 123 )
( cid : 123 ) |yi| > |y| ( k+123 )
( cid : 123 ) = k
almost surely , that is , except for ties .
this completes the proof .
( cid : 123 )
the divergence formula .
while for the most general design matrices x , it can happen that k fails to be almost differentiable , we will see that the divergence
k ( y ) = k
does hold almost everywhere .
indeed , certain authors ( e . g . , meyer and woodroofe
( 123 ) ) have argued that the divergence ( cid : 123 ) of an estimator provides itself a useful measure of the effective dimension of a model .
turning to lars , we shall say that ( y ) is locally linear at a data point y123 if there is some small open neighborhood of y123 on which ( y ) = my is exactly linear .
of course , the matrix m = m ( y123 ) can depend on y123in the case of lars , it will be seen to be constant on the interior of polygonal regions , with jumps across the boundaries .
we say that a set g has full measure if its complement has lebesgue measure zero .
lemma 123
there is an open set gk of full measure such that , at all y gk ,
k ( y ) is locally linear and k ( y ) = k .
we give here only the part of the proof that relates to actual calculation of the divergence in ( 123 ) .
the arguments establishing continuity and local linearity are delayed to the appendix .
so , let us x a point y in the interior of gk .
from lemma 123 in the appendix , this means that near y the active set ak ( y ) is locally constant , that a single variable enters at the next step , this variable being the same near y .
in addition , k ( y ) is locally linear , and hence in particular differentiable .
since gk gl for l < k , the same story applies at all previous steps and we have
differentiating the j th component of vector k ( y ) yields
k ( y ) = k ( cid : 123 ) ( y ) = k ( cid : 123 )
least angle regression
in particular , for the divergence
k ( y ) = n ( cid : 123 )
the brackets indicating inner product .
the active set is ak = ( 123 , 123 , .
, k ) and xk+123 is the variable to enter next .
for k 123 , write k = xlxk for any choice l < kas remarked in the conventions in the appendix , the choice of l is immaterial ( e . g . , l = 123 for deniteness ) .
let bk+123 = ( cid : 123 ) k+123 , uk ( cid : 123 ) , which is nonzero , as argued in the proof of lemma 123
as shown in ( a . 123 ) in the appendix , ( 123 ) can be rewritten
k ( y ) = b
( cid : 123 ) k+123 , y k123 ( cid : 123 ) .
for k 123 , dene the linear space of vectors equiangular with the active set u : ( cid : 123 ) x123 , u ( cid : 123 ) = = ( cid : 123 ) xk , u ( cid : 123 ) for xl with l ak ( y )
lk = lk ( y ) = ( cid : 123 )
( we may drop the dependence on y since ak ( y ) is locally xed . ) clearly dim lk = n k + 123 and
we shall now verify that , for each k 123 ,
( cid : 123 ) k , uk ( cid : 123 ) = 123
first , for k = 123 we have 123 ( y ) = b
( cid : 123 ) k , u ( cid : 123 ) = 123
formula ( 123 ) shows that this sufces to prove lemma 123
for u lk+123
( cid : 123 ) 123 , u ( cid : 123 ) , and that
now , for general k , combine ( 123 ) and ( 123 ) :
( cid : 123 ) 123 , u ( cid : 123 ) = ( cid : 123 ) x123 x123 , u ( cid : 123 ) =
( cid : 123 ) 123 , y ( cid : 123 ) and ( cid : 123 ) 123 , u ( cid : 123 ) = b if u = u123 , if u l123
bk+123k ( y ) = ( cid : 123 ) k+123 , y ( cid : 123 ) k123 ( cid : 123 ) bk+123 ( cid : 123 ) k , u ( cid : 123 ) = ( cid : 123 ) k+123 , u ( cid : 123 ) k123 ( cid : 123 )
( cid : 123 ) k+123 , ul ( cid : 123 ) ( cid : 123 ) l , u ( cid : 123 ) .
from the denitions of bk+123 and lk+123 we have
( cid : 123 ) k+123 , u ( cid : 123 ) = ( cid : 123 ) xl xk+123 ( cid : 123 ) =
if u = uk , if u lk+123
hence the truth of ( 123 ) for step k follows from its truth at step k 123 because of the containment properties ( 123 ) .
( cid : 123 )
efron , hastie , johnstone and tibshirani
proof of theorem 123
to complete the proof of theorem 123 , we state the
following regularity result , proved in the appendix .
lemma 123
under the positive cone condition , k ( y ) is continuous and almost
this guarantees that steins formula ( 123 ) is valid for k under the positive
cone condition , so the divergence formula of lemma 123 then immediately yields
lars and lasso properties .
the lars and lasso algorithms are described more carefully in this section , with an eye toward fully understanding their relationship .
theorem 123 of section 123 will be veried .
the latter material overlaps results in osborne , presnell and turlach ( 123a ) , particularly in their section 123
our point of view here allows the lasso to be described as a quite simple modication of lars , itself a variation of traditional forward selection methodology , and in this sense should be more accessible to statistical audiences .
in any case we will stick to the language of regression and correlation rather than convex optimization , though some of the techniques are familiar from the
the results will be developed in a series of lemmas , eventually lending to a proof of theorem 123 and its generalizations .
the rst three lemmas refer to attributes of giving estimate ( cid : 123 ) k123 and active set ak for step k , with covariate xk the newest the lars procedure that are not specic to its lasso modication .
using notation as in ( 123 ) ( 123 ) , suppose lars has completed step k 123 ,
addition to the active set .
if xk is the only addition to the active set at
the end of step k 123 , then the coefcient vector wk = akg k 123k for the equiangular vector uk = xkwk , ( 123 ) , has its kth component wkk agreeing in sign with the current has its kth component ( cid : 123 ) correlation ckk = x
k ( y ( cid : 123 ) k123 ) .
moreover , the regression vector ( cid : 123 )
kk agreeing in sign with ckk .
lemma 123 says that new variables enter the lars active set in the correct direction , a weakened version of the lasso requirement ( 123 ) .
this will turn out to be a crucial connection for the larslasso relationship .
proof of lemma 123
the case k = 123 is apparent .
note that since
( 123 ) , from ( 123 ) we have
wk = ak
k ( y ( cid : 123 ) k123 ) = ( cid : 123 ) k ( y ( cid : 123 ) k123 ) ) : = ak
least angle regression
the term in square braces is the least squares coefcient vector in the regression of the current residual on xk , and the term preceding it is positive .
note also that
k ( y yk123 ) = ( 123 , )
with > 123 ,
k123 ( y yk123 ) = 123 by denition ( this 123 has k 123 elements ) , and ck ( ) =
< cj ( ) , k ( y uk123 ) decreases more slowly in than cj ( ) for j ak123 : = cj ( ) = ( cid : 123 ) for ( cid : 123 ) k123 < < k123
k ( y yk123 + yk123 ( cid : 123 ) k123 )
> cj ( ) ,
k is positive , because it is in the rst term in ( 123 ) ( ( x
( ( k123 ( cid : 123 ) k123 ) uk123 ) .
the kth element of ( cid : 123 ) w positive denite ) , and in the second term it is 123 since uk123 l ( xk123 ) .
this proves the rst statement in lemma 123
the second follows from k123 , k = 123 , xk not being active before step k .
( cid : 123 ) our second lemma interprets the quantity aa = ( 123
let sa indicate the extended simplex generated by the columns of xa ,
v = ( cid : 123 )
pj = 123
sj xj pj :
123 / 123 , ( 123 ) and ( 123 ) .
extended meaning that the coefcients pj are allowed to be negative .
lemma 123
the point in sa nearest the origin is
va = aaua = aaxawa
where wa = aag
with length ( cid : 123 ) va ( cid : 123 ) = aa .
if a b , then aa ab , the largest possible value being aa = 123 for a a singleton .
for any v sa , the squared distance to the origin is ( cid : 123 ) xap ( cid : 123 ) 123 = gap .
introducing a lagrange multiplier to enforce the summation constraint ,
efron , hastie , johnstone and tibshirani
pa = a123
a 123a = 123 ,
and nd that the minimizing pa = g a 123a .
summing , we get 123 a 123a = aawa .
hence va = xapa sa and ( cid : 123 ) va ( cid : 123 ) 123 = p verifying ( 123 ) .
if a b , then sa sb , so the nearest distance ab must be equal to or less than the nearest distance aa .
aa obviously equals 123 if and only if a has only one member .
( cid : 123 )
a pa = a123
a 123a = a123
and d , let
+ d and s ( ) = ( cid : 123 ) y x ( ) ( cid : 123 ) 123
the lars algorithm and its various modications proceed in piecewise linear
for m - vectors ( cid : 123 ) ( ) = ( cid : 123 ) lemma 123
letting ( cid : 123 ) c = x at ( cid : 123 ) = x ) be the current correlation vector s ( ) s ( 123 ) = 123 ( cid : 123 ) c d + d s ( 123 ) = 123 ( cid : 123 ) c s ( 123 ) = 123d ) are both convex functions of ( cid : 123 )
s ( ) is a quadratic function of , with rst two derivatives at = 123 , standard results show that ( cid : 123 ) ( t ) and ( cid : 123 ) ( t ) are unique and continuous functions of t .
the remainder of this section concerns the larslasso relationship .
now , with s strictly convex ,
( t ) will indicate a lasso solution ( 123 ) , and likewise ( cid : 123 ) = ( cid : 123 ) ( t ) = x
) and t (
for a given value of t let
a = ( j : ( cid : 123 )
j ( t ) ( cid : 123 ) = 123 ) .
interval of the t axis , with inmum t123 , within which the set a of nonzero lasso
we will show later that a is also the active set that determines the equiangular direction ua , ( 123 ) , for the larslasso computations .
we wish to characterize the track of the lasso solutions ( cid : 123 ) of ( cid : 123 ) ( t ) as t increases from 123 to its maximum effective value .
let t be an open lemma 123
the lasso estimates ( cid : 123 ) ( t ) satisfy for t t , where ua is the equiangular vector xawa , wa = aag
( cid : 123 ) ( t ) = ( cid : 123 ) ( t123 ) + aa ( t t123 ) ua
j ( t ) remains constant .
( t ) or equivalently
a 123a , ( 123 ) .
least angle regression
equiangular vector ua determined by a .
we can also state this in terms of
the lemma says that , for t in t , ( cid : 123 ) ( t ) moves linearly along the the nonzero regression coefcients ( cid : 123 ) a ( t123 ) + saaa ( t t123 ) wa , in ( 123 ) because denitions ( 123 ) , ( 123 ) require ( cid : 123 ) ( t ) = x where sa is the diagonal matrix with diagonal elements sj , j a .
( sa is needed since ( cid : 123 ) ( t ) satises ( 123 ) and has nonzero set a , it also minimizes ( t ) = xasa subject to ( cid : 123 ) ( cid : 123 ) | j| for the full m - variable ols solution m . ) moreover , the fact that the minimizing point ( cid : 123 ) so that ( cid : 123 )
j ) = sj ) = t as long as t is less than
a ( t ) occurs strictly inside the simplex ( 123 ) , combined with a ) , implies we can drop the second condition in ( 123 )
a ) = ( cid : 123 ) y xasa
( the inequality in ( 123 ) can be replaced by t (
the strict convexity of s (
for j a .
introducing a lagrange multiplier , ( 123 ) becomes
a ) + sa123a = 123
differentiating we get
j = t .
a ( t123 ) and ( cid : 123 )
consider two values t123 and t123 in t with t123 < t123 < t123
corresponding to each of these are values for the lagrange multiplier such that 123 > 123 , and solutions a ( t123 ) .
inserting these into ( 123 ) , differencing and premultiplying
( cid : 123 ) = ( 123 123 ) 123a .
a ( t123 ) = ( 123 123 ) sag a ( t123 ) ) = t123 t123 according to the lasso denition , so t123 t123 = ( 123 123 ) s a 123a = ( 123 123 ) a
a 123a = ( 123 123 ) 123
a 123a = saaa ( t t123 ) wa .
a ( t123 ) = saa123
by sa we get
d ( t ) =
efron , hastie , johnstone and tibshirani
of the lasso parameter t .
( cid : 123 )
c ( t ) is a piecewise linear decreasing function
a ( t t123 ) , so that ( cid : 123 )
letting t123 = t and t123 t123 gives ( 123 ) by the continuity of ( cid : 123 ) ( t ) , and nally ( 123 ) .
note that ( 123 ) implies that the maximum absolute correlation ( cid : 123 ) the lasso solution ( cid : 123 ) ( t ) occurs on the surface of the diamond - shaped convex d ( t ) increasing with t .
lemma 123 says that , for t t , ( cid : 123 ) estimates ( cid : 123 ) ( t ) move in the lars equiangular direction ua , ( 123 ) .
it remains to ( t ) moves linearly along edge a of the polytope , the edge having j = 123 for j / a .
moreover the regression show that a changes according to the rules of theorem 123 , which is the purpose lemma 123
a lasso solution ( cid : 123 ) has of the next three lemmas .
where ( cid : 123 ) cj equals the current correlation x j ) = sign ( ( cid : 123 ) cj )
for j a , j ( y ( cid : 123 ) ) = x j ( y x for j a .
this implies that
in particular ,
|cj ( t ) | = ( cid : 123 )
of the left - hand side is ( cid : 123 ) cj , and the right - hand side is sign ( likewise = | ( cid : 123 ) cj| = ( cid : 123 ) proof .
this follows immediately from ( 123 ) by noting that the j th element j ) for j a .
j ( y ( cid : 123 ) ( t ) ) satisfy lemma 123
within an interval t of constant nonzero set a , and also at t123 = inf ( t ) , the lasso current correlations cj ( t ) = x for j a |cj ( t ) | ( cid : 123 ) proof .
equation ( 123 ) says that the |cj ( t ) | have identical values , say ( cid : 123 ) for j a .
it remains to show that ( cid : 123 ) in ( 123 ) .
for an m - vector d we dene ( ) = ( cid : 123 ) likewise t ( ) = ( cid : 123 ) |j ( ) | , and ct has the extremum properties indicated ( t ) + d and s ( ) as in ( 123 ) , rt ( d ) = s ( 123 ) / t ( 123 ) .
again assuming ( cid : 123 ) rt ( d ) = 123
j > 123 for j a , by redenition of xj if necessary , ( 123 ) and
for j / a .
if dj = 123 for j / a , and
dj ( cid : 123 ) = 123 ,
rt ( d ) = 123 ( cid : 123 ) rt ( d ) = 123|cj ( t ) | .
least angle regression
while if d has only component j nonzero we can make
rt rt ( wa )
according to lemma 123 the lasso solutions for t t use da proportional to wa with dj = 123 for j / a , so the lasso must maximize rt ( d ) .
this shows that ( cid : 123 ) is the downward slope of the curve ( t , s ( t ) ) at t = t , and by the denition of c ( t ) , and veries ( 123 ) , which also holds at t123 = inf ( t ) by the continuity of the current correlations .
( cid : 123 ) we note that lemmas 123 follow relatively easily from the karushkuhn tucker conditions for optimality for the quadratic programming lasso problem ( osborne , presnell and turlach ( 123a ) ) ; we have chosen a more geometrical argument here to demonstrate the nature of the lasso path .
figure 123 shows the ( t , s ) curve corresponding to the lasso estimates in figure 123
the arrow indicates the tangent to the curve at t = 123 , which has
ct = ( cid : 123 )
plot of s versus t for lasso applied to diabetes data; points indicate the 123 modied lars steps of figure 123; triangle is ( t , s ) boundary point at t = 123; dashed arrow is tangent at t = 123 , negative slope rt , ( 123 ) .
the ( t , s ) curve is a decreasing , convex , quadratic spline .
efron , hastie , johnstone and tibshirani
a123 = ( j : ( cid : 123 )
downward slope r123
the argument above relies on the fact that rt ( d ) cannot be greater than rt , or else there would be ( t , s ) values lying below the optimal curve .
using lemmas 123 and 123 it can be shown that the ( t , s ) curve is always convex , as
in figure 123 , being a quadratic spline with s ( t ) = 123 ( cid : 123 ) c ( t ) and s ( t ) = 123a123 lemma 123 , with lasso regression vector ( cid : 123 ) , prediction estimate ( cid : 123 ) = x we now consider in detail the choice of active set at a breakpoint of the piecewise linear lasso path .
let t = t123 indicate such a point , t123 = inf ( t ) as in ( y ( cid : 123 ) ) , sj = sign ( ( cid : 123 ) cj ) and maximum absolute correlation ( cid : 123 ) correlations ( cid : 123 ) c = x j = 123 and | ( cid : 123 ) cj| = ( cid : 123 ) a123 = ( j : ( cid : 123 ) j ( cid : 123 ) = 123 ) , 123 , and take ( ) = ( cid : 123 ) + d for some m - vector d; also s ( ) = ( cid : 123 ) y x ( ) ( cid : 123 ) 123 and t ( ) = ( cid : 123 ) |j ( ) | .
a123 = a123 a123 and a123 = ac lemma 123
the negative slope ( 123 ) at t123 is bounded by 123 ( cid : 123 ) r ( d ) = s ( 123 ) / t ( 123 ) 123 ( cid : 123 ) with equality only if dj = 123 for j a123
if so , the differences s = s ( ) s ( 123 ) and t = t ( ) t ( 123 ) satisfy s = 123 ( cid : 123 )
ct + l ( d ) 123 ( t ) 123 ,
according to lemma 123
proceeding as in ( 123 ) ,
we can assume ( cid : 123 ) cj 123 for all j , by redenition if necessary , so ( cid : 123 )
l ( d ) = ( cid : 123 ) xd / d+ ( cid : 123 ) .
r ( d ) = 123 ( cid : 123 ) r ( d ) = 123 ( cid : 123 ) c unless dj = 123 for j a123 , verifying ( 123 ) , and also implying
we need dj 123 for j a123 a123 in order to maximize ( 123 ) , in which case this is < 123 ( cid : 123 ) the rst term on the right - hand side of ( 123 ) is then 123 ( cid : 123 )
dj + ( cid : 123 )
t ( ) = t ( 123 ) +
c ( t ) , while the second
term equals ( d / d+ )
x ( d / d+ ) ( t ) 123 = l ( d ) 123
( cid : 123 )
least angle regression
lemma 123 has an important consequence .
suppose that a is the current active set for the lasso , as in ( 123 ) , and that a a123
then lemma 123 says that l ( d ) is aa , and ( 123 ) gives with equality if d is chosen to give the equiangular vector ua , da = sawa , dac = 123
the lasso operates to minimize s ( t ) so we want s to be as negative s ( 123 ) exceeds the optimum value 123 ( cid : 123 ) as possible .
lemma 123 says that if the support of d is not conned to a123 , then c but s ( 123 ) exceeds the minimum value 123aa unless da is proportional to sawa as in ( 123 ) .
obtained from the lasso - modied lars algorithm , henceforth called larslasso , as at t = 123 in follow a linear track determined by some subset a , ( ) = ( cid : 123 ) + ua , and so will figures 123 and 123
we know from lemma 123 that subsequent lasso estimates will
c t + a123 c; if it is conned , then s ( 123 ) = 123 ( cid : 123 )
, a lasso solution , exactly equals a ( cid : 123 )
the larslasso estimates , but to verify theorem 123 we need to show that a is the same set in both cases .
lemmas 123 put four constraints on the lasso choice of a .
dene a123 , a123 and
a123 as at ( 123 ) .
a + sawa ,
small the subsequent lasso coefcients ( 123 ) ,
a ( ) = ( cid : 123 ) j ( ) ( cid : 123 ) = 123 , j a123
constraint 123
this follows from lemma 123 since for sufciently constraint 123
a a123
lemma 123 , ( 123 ) shows that the lasso choice ( cid : 123 ) d in ( ) = ( cid : 123 ) + ( cid : 123 ) d must have its nonzero support in a123 , or equivalently that ( cid : 123 ) ( ) = ( cid : 123 ) + ua must have ua l ( xa123 ) .
( it is possible that ua happens to equal ub for some b a123 , but that does not affect the argument below . ) a 123a cannot have sign ( wj ) ( cid : 123 ) = sign ( ( cid : 123 ) cj ) for any j ( ) ) ( cid : 123 ) = sign ( ( cid : 123 ) cj ( ) ) for sufciently constraint 123
wa = aag
coordinate j a123
if it does , then sign ( small , violating lemma 123
constraint 123
subject to constraints 123 , a must minimize aa
follows from lemma 123 as in ( 123 ) , and the requirement curve s ( t ) declines at the fastest possible rate .
theorem 123 follows by induction : beginning at ( cid : 123 ) 123 = 123 , we follow the lars with the lasso denition ( 123 ) .
first of all , suppose that ( cid : 123 ) , our hypothesized lasso
lasso algorithm and show that at every succeeding step it must continue to agree
and larslasso solution , has occurred strictly within a larslasso step
efron , hastie , johnstone and tibshirani
a123 is empty so that constraints 123 and 123 imply that a cannot change its current value : the equivalence between lasso and larslasso must continue at least to the end of the step .
there are two cases : if j123 has just been added to the set ( | ( cid : 123 ) cj| = ( cid : 123 ) the one - at - a - time assumption of theorem 123 says that at a larslasso says that sign ( wj123 ) = sign ( ( cid : 123 ) cj123 ) , so that constraint 123 is not violated; the other breakpoint , a123 has exactly one member , say j123 , so a must equal a123 or a123
c ) , then lemma 123 three constraints and lemma 123 imply that the lasso choice a = a123 agrees with the larslasso algorithm .
the other case has j123 deleted from the active set as in ( 123 ) .
now the choice a = a123 is ruled out by constraint 123 : it would keep wa the same as in the previous larslasso step , and we know that that was stopped in ( 123 ) to prevent a sign contradiction at coordinate j123
in other words , a = a123 , in accordance with the lasso modication of lars .
this completes the proof of theorem 123
two new members j123 and j123 are added to the set ( | ( cid : 123 ) cj| = ( cid : 123 ) a larslasso algorithm is available even if the one - at - a - time condition does not hold , but at the expense of additional computation .
suppose , for example , c ) , so a123 = ( j123 , j123 ) .
it is possible but not certain that a123 does not violate constraint 123 , in which case a = a123
however , if it does violate constraint 123 , then both possibilities a = a123 ( j123 ) and a = a123 ( j123 ) must be examined to see which one gives the smaller value of aa .
since one - at - a - time computations , perhaps with some added y jitter , apply to all practical situations , the lars algorithm described in section 123 is not equipped to handle many - at - a - time problems .
coefcients , for example , as indicated at
and stagewise procedures .
assume that ( cid : 123 ) 123
stagewise properties .
the main goal of this section is to verify theorem 123
doing so also gives us a chance to make a more detailed comparison of the lars ( cid : 123 ) , current correlations ( cid : 123 ) c = x figure 123 , with prediction vector ( cid : 123 ) = x is a stagewise estimate of the regression j| = 123 in the right panel of c = max ( | ( cid : 123 ) cj| ) and maximal set a = ( j : | ( cid : 123 ) cj| = ( cid : 123 ) c ) .
we must show that successive by redenition of xj as xj , if necessary , that the signs sj = sign ( ( cid : 123 ) cj ) are all non - stagewise estimates of develop according to the modied lars algorithm of theorem 123 , henceforth called larsstagewise .
for convenience we can assume , , giving new prediction vector ( cid : 123 ) ( n ) .
n additional - steps forward from ( cid : 123 ) = x lemma 123
for sufciently small , only j a can have pj = nj / n > 123
letting n , ( cid : 123 ) ( cid : 123 ) ( n ) ( cid : 123 ) ( cid : 123 ) so that ( cid : 123 ) c ( n ) = x ( y ( cid : 123 ) ( n ) ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xj ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( n ) ( cid : 123 ) ( cid : 123 ) .
as in ( 123 ) ( 123 ) we suppose that the stagewise procedure ( 123 ) has taken
| ( cid : 123 ) cj ( n ) ( cid : 123 ) cj| = ( cid : 123 ) ( cid : 123 ) x
c maxac ( ( cid : 123 ) cj ) ) , j in ac cannot have maximal current correlation and
least angle regression
for < 123 can never be involved in the n steps .
( cid : 123 )
( cid : 123 ) ( ) = ( cid : 123 ) + v ,
lemma 123 says that we can write the developing stagewise prediction vector as
pa a vector of length |a| , with components nj / n for j a .
the nature of the stagewise procedure puts three constraints on v , the most obvious of which is
where v = xapa ,
constraint i .
the vector v s
v : v = ( cid : 123 )
a , the nonnegative simplex xj pj , pj 123 ,
pj = 123
equivalently , v ca , the convex cone ( 123 ) .
the stagewise procedure , unlike lars , is not required to use all of the maximal set a as the active set , and can instead restrict the nonzero coordinates pj to a subset b a .
then v l ( xb ) , the linear space spanned by the columns of xb , but not all such vectors v are allowable stagewise forward directions .
constraint ii .
the vector v must be proportional to the equiangular vector
ub , ( 123 ) , that is , v = vb , ( 123 ) , vb = a123
constraint ii amounts to requiring that the current correlations in b decline at
b 123b = abub .
j ( y ( cid : 123 ) v ) = ( cid : 123 ) cj x bv = 123b for some > 123 , implying v = g
an equal rate : since ( cid : 123 ) cj ( ) = x satises constraint ii .
violating constraint ii makes the current correlations ( cid : 123 ) cj ( ) b 123b; choosing = a123
we need x
unequal so that the stagewise algorithm as dened at ( 123 ) could not proceed in
bvb = a123 j vb = a123
equation ( 123 ) gives x
for j b .
constraint iii .
the vector v = vb must satisfy for j a b .
j vb a123
efron , hastie , johnstone and tibshirani
j vb = a123
it is easy to show that vbbo
members of a = ( j : | ( cid : 123 ) cj| = ( cid : 123 ) constraint iii follows from ( 123 ) .
it says that the current correlations for c ) not in b must decline at least as quickly as those in b .
if this were not true , then vb would not be an allowable direction for stagewise development since variables in a b would immediately reenter ( 123 ) .
to obtain strict inequality in ( 123 ) , let b123 a b be the set of indices for = vb .
in other words , if we take b to be the largest set having a given vb proportional to its equiangular writing ( cid : 123 ) ( ) = ( cid : 123 ) + v as in ( 123 ) presupposes that the stagewise solutions vector , then x always express the family of stagewise solutions as ( cid : 123 ) ( z ) , where the real - valued value as ( cid : 123 ) ( z ) goes from 123 to the full ols estimate .
( the choice z = t used in parameter z plays the role of t for the lasso , increasing from 123 to some maximum figure 123 may not necessarily yield a one - to - one mapping; z = s ( 123 ) s ( reduction in residual squared error , always does . ) we suppose that the stagewise ( z ) is everywhere right differentiable with respect to z .
then the right
follow a piecewise linear track .
however , the presupposition can be reduced to one of piecewise differentiability by taking innitesimally small .
we can
b for j a b .
j vb > a123
( cid : 123 ) v = d
if ua ca ,
must obey the three constraints .
the denition of the idealized stagewise procedure in section 123 , in which 123 in rule ( 123 ) , is somewhat vague but the three constraints apply to any reasonable interpretation .
it turns out that the larsstagewise algorithm satises the constraints and is unique in doing so .
this is the meaning of theorem 123
( of course the larsstagewise algorithm is also supported by direct numerical comparisons with ( 123 ) , as in figure 123s right panel . ) then v = va obviously satises the three constraints .
the interesting situation for theorem 123 is ua / ca , which we now assume to be the case .
any subset b a determines a face of the convex cone of dimension |b| , the face having pj > 123 in ( 123 ) for j b and pj = 123 for j a b .
the orthogonal projection of ua into the linear subspace l ( xb ) , say projb ( ua ) , is proportional to bs equiangular vector ub : using ( 123 ) ,
projb ( va ) = ( aa / ab ) 123vb .
the nearest point to ua in ca , say ( cid : 123 ) ua , is of the form axj therefore ( cid : 123 ) ua exists strictly within face ( cid : 123 ) b , where ( cid : 123 ) b = ( j : ( cid : 123 ) proj b ( ua ) .
according to ( 123 ) , ( cid : 123 ) ua is proportional to ( cid : 123 ) bs equiangular vector u b , pj > 123 ) , and must equal = abub .
in other words v b satises constraint ii , and it obviously
b aa123b = ( aa / ab ) ub , pj with ( cid : 123 )
and also to v b also satises constraint i .
figure 123 schematically illustrates the geometry .
projb ( ua ) = xbg
bua = xbg
least angle regression
the geometry of the larsstagewise modication .
lemma 123
the vector v b satises constraints iiii , and conversely if v satises the three constraints , then v = v b .
let cos aa / ab and sin = ( 123 cos123 ) 123 / 123 , the latter being greater
than zero by lemma 123
for any face b a , ( 123 ) implies where zb is a unit vector orthogonal to l ( xb ) , pointing away from ca .
by an n - dimensional coordinate rotation we can make l ( xb ) = l ( c123 , c123 , .
, cj ) , j = |b| , the space of n - vectors with last n j coordinates zero , and also the rst 123 having length j 123 , the second 123 length n j 123
then we can write
ua = ( cos , 123 , sin , 123 ) ,
ua = cosub + sinzb ,
ub = ( 123 , 123 , 123 , 123 ) ,
so ( 123 ) yields
( cid : 123 ) ua = cosx ( cid : 123 ) 123
aa = x
x ( cid : 123 ) 123 , x ( cid : 123 ) 123 , x ( cid : 123 ) 123 , x ( cid : 123 ) 123
+ sinx ( cid : 123 ) 123 .
ab , xj123 , 123 , 123
the rst coordinate ab being required since x j ua = cosab = aa , as also required by ( 123 ) .
for ( cid : 123 ) a b denote x ( cid : 123 ) as
for j b , j ub = ab , ( 123 ) .
notice that
efron , hastie , johnstone and tibshirani
now assume b = ( cid : 123 ) b .
in this case a separating hyperplane h orthogonal to z b in ( 123 ) passes between the convex cone ca and ua , through ( cid : 123 ) ua = cosu b , 123 ( i . e . , x ( cid : 123 ) and ua are on opposite sides of h , x ( cid : 123 ) 123 being negative since the corresponding coordinate of ua , sin in ( 123 ) , is positive ) .
equation ( 123 ) gives cosx ( cid : 123 ) 123 aa = cosa b or ( cid : 123 ) ( a bu b ) = a b x ( cid : 123 ) 123
verifying that constraint iii is satised .
a and v = vb conversely suppose that v satises constraints iiii so that v s for the nonzero coefcient set b : vb = bxj pj , pj > 123
let h be the hyperplane passing through cos ub orthogonally to zb , ( 123 ) , ( 123 ) .
if vb ( cid : 123 ) = v b , then at least one of the vectors x ( cid : 123 ) , ( cid : 123 ) a b , must lie on the same side of h as ua , so and vb would be proportional to ( cid : 123 ) ua , the nearest point to ua in ca , implying that x ( cid : 123 ) 123 > 123 ( or else h would be a separating hyperplane between ua and ca , vb = v b ) .
now ( 123 ) gives cos x ( cid : 123 ) 123 < aa = cos ab , or ( cid : 123 ) ( abub ) = abx ( cid : 123 ) 123 < a123
this violates constraint iii , showing that v must equal v b .
( cid : 123 ) notice that the direction of advance ( cid : 123 ) v = v b of the idealized stagewise procedure is a function only of the current maximal set ( cid : 123 ) a = ( j : | ( cid : 123 ) cj| = ( cid : 123 ) say ( cid : 123 ) v = (
( cid : 123 ) a ) .
in the language of ( 123 ) ,
( cid : 123 ) vb = x
the larsstagewise algorithm of theorem 123 produces an evolving family of that everywhere satises ( 123 ) .
this is true at every larsstagewise breakpoint by the denition of the stagewise modication .
it is also true between
breakpoints .
let ( cid : 123 ) a be the maximal set at the breakpoint , giving ( cid : 123 ) v = v b in the succeeding larsstagewise interval ( cid : 123 ) ( ) = ( cid : 123 ) + v b , the maximal set is immediately reduced to ( cid : 123 ) b , according to properties ( 123 ) , ( 123 ) of v b , at which it ( cid : 123 ) a ) = v b since v b so the larsstagewise procedure , which continues in the direction ( cid : 123 ) v until a
stays during the entire interval .
however , (
( cid : 123 ) b ) = (
new member is added to the active set , continues to obey the idealized stagewise
all of this shows that the larsstagewise algorithm produces a legitimate version of the idealized stagewise track .
the converse of lemma 123 says that there are no other versions , verifying theorem 123
the stagewise procedure has its potential generality as an advantage over lars and lasso : it is easy to dene forward stagewise methods for a wide variety of nonlinear tting problems , as in hastie , tibshirani and friedman ( ( 123 ) , chapter 123 , which begins with a stagewise analysis of boosting ) .
comparisons
least angle regression
with lars and lasso within the linear model framework , as at the end of section 123 , help us better understand stagewise methodology .
this sections results permit further comparisons .
consider proceeding forward from ( cid : 123 ) along unit vector u , ( cid : 123 ) ( ) = ( cid : 123 ) + u , two interesting choices being the lars direction u a and the stagewise direction ( cid : 123 ) b .
for u l ( x a ) , the rate of change of s ( ) = ( cid : 123 ) y ( cid : 123 ) ( ) ( cid : 123 ) 123 is
( 123 ) following quickly from ( 123 ) .
this shows that the lars direction u a maximizes the instantaneous decrease in s .
the ratio
equaling the quantity cos in ( 123 ) .
the comparison goes the other way for the maximum absolute correlation ( cid : 123 )
proceeding as in ( 123 ) ,
the argument for lemma 123 , using constraints ii and iii , shows that u b maxi - mizes ( 123 ) at a b , and that
the original motivation for the stagewise procedure was to minimize residual squared error within a framework of parsimonious forward search .
however , ( 123 ) shows that stagewise is less greedy than lars in this regard , it being more accurate to describe stagewise as striving to minimize the maximum absolute
computations .
the entire sequence of steps in the lars algorithm with m < n variables requires o ( m123 + nm123 ) computationsthe cost of a least squares t on m variables .
in detail , at the kth of m steps , we compute m k inner products cj k of the nonactive xj with the current residuals to identify the next active variable , and then invert the k k matrix gk = x kxk to nd the next lars direction .
we do this by updating the cholesky factorization rk123 of gk123 found at the previous step ( golub and van loan ( 123 ) ) .
at the nal step m , we have computed the cholesky r = rm for the full cross - product matrix , which is the dominant calculation for a least squares t .
hence the lars sequence can be seen as a cholesky factorization with a guided ordering of the variables .
efron , hastie , johnstone and tibshirani
the computations can be reduced further by recognizing that the inner products x and the above can be updated at each iteration using the cross - product matrix x current directions .
for m ( cid : 123 ) n , this strategy is counterproductive and is not used .
for the lasso modication , the computations are similar , except that occasion - ally one has to drop a variable , and hence downdate rk ( costing at most o ( m123 ) operations per downdate ) .
for the stagewise modication of lars , we need to check at each iteration that the components of w are all positive .
if not , one or more variables are dropped ( using the inner loop of the nnls algorithm described in lawson and hanson ( 123 ) ) , again requiring downdating of rk .
with many correlated variables , the stagewise version can take many more steps than lars because of frequent dropping and adding of variables , increasing the computations by a factor up to 123 or more in extreme cases .
the lars algorithm ( in any of the three states above ) works gracefully for the case where there are many more variables than observations : m ( cid : 123 ) n .
in this case lars terminates at the saturated least squares t after n 123 variables have entered the active set ( at a cost of o ( n123 ) operations ) .
( this number is n 123 rather than n , because the columns of x have been mean centered , and hence it has row - rank n 123 ) we make a few more remarks about the m ( cid : 123 ) n case in the lasso state : 123
the lars algorithm continues to provide lasso solutions along the way , and the nal solution highlights the fact that a lasso t can have no more than n 123 ( mean centered ) variables with nonzero coefcients .
although the model involves no more than n 123 variables at any time , the number of different variables ever to have entered the model during the entire sequence can beand typically isgreater than n 123
the model sequence , particularly near the saturated end , tends to be quite
variable with respect to small changes in y .
the estimation of 123 may have to depend on an auxiliary method such as nearest neighbors ( since the nal model is saturated ) .
we have not investigated the accuracy of the simple approximation formula ( 123 ) for the case m > n .
documented s - plus implementations of lars and associated functions are available from www - stat . stanford . edu / hastie / papers / ; the diabetes data also
boosting procedures .
one motivation for studying the forward stagewise algorithm is its usefulness in adaptive tting for data mining .
in particular , forward stagewise ideas are used in boosting , an important class of tting methods for data mining introduced by freund and schapire ( 123 ) .
these methods are one of the hottest topics in the area of machine learning , and one of the most effective prediction methods in current use .
boosting can use any adaptive tting procedure as its base learner ( model tter ) : trees are a popular choice , as implemented in cart ( breiman , friedman , olshen and stone ( 123 ) ) .
least angle regression
friedman , hastie and tibshirani ( 123 ) and friedman ( 123 ) studied boosting and proposed a number of procedures , the most relevant to this discussion being least squares boosting .
this procedure works by successive tting of regression trees to the current residuals .
specically we start with the residual r = y and the t y = 123
we t a tree in x123 , x123 , .
, xm to the response y giving a tted tree t123 ( an n - vector of tted values ) .
then we update y to y + t123 , r to y y and continue for many iterations .
here is a small positive constant .
empirical studies show that small values of work better than = 123 : in fact , for prediction accuracy the smaller the better .
the only drawback in taking very small values of is
a major research question has been why boosting works so well , and specically why is - shrinkage so important ? to understand boosted trees in the present context , we think of our predictors not as our original variables x123 , x123 , .
, xm , but instead as the set of all trees tk that could be tted to our data .
there is a strong similarity between least squares boosting and forward stagewise regression as dened earlier .
fitting a tree to the current residual is a numerical way of nding the predictor most correlated with the residual .
note , however , that the greedy algorithms used in cart do not search among all possible trees , but only a subset of them .
in addition the set of all trees , including a parametrization for the predicted values in the terminal nodes , is innite .
nevertheless one can dene idealized versions of least - squares boosting that look much like forward stagewise
hastie , tibshirani and friedman ( 123 ) noted the the striking similarity between forward stagewise regression and the lasso , and conjectured that this may help explain the success of the forward stagewise process used in least squares boosting .
that is , in some sense least squares boosting may be carrying out a lasso t on the innite set of tree predictors .
note that direct computation of the lasso via the lars procedure would not be feasible in this setting because the number of trees is innite and one could not compute the optimal step length .
however , forward stagewise regression is feasible because it only need nd the the most correlated predictor among the innite set , where it approximates by numerical
in this paper we have established the connection between the lasso and forward stagewise regression .
we are now thinking about how these results can help to understand and improve boosting procedures .
one such idea is a modied form of forward stagewise : we nd the best tree as usual , but rather than taking a small step in only that tree , we take a small least squares step in all trees currently in our model .
one can show that for small step sizes this procedure approximates lars; its advantage is that it can be carried out on an innite set of predictors such as
efron , hastie , johnstone and tibshirani
local linearity and lemma 123
l ( y k123 ( y ) ) = ck ( y ) and x
conventions .
we write xl with subscript l for members of the active set ak .
thus xl denotes the lth variable to enter , being an abuse of notation for slxj ( l ) = sgn ( cj ( l ) ) xj ( l ) .
expressions x luk = ak clearly do not depend on which xl ak we choose .
by writing j / ak , we intend that both xj and xj are candidates for inclusion at the next step .
one could think of negative indices j corresponding to new variables xj = xj .
a neighborhood of y123 , we say that ak ( y ) is locally xed ( at ak = ak ( y123 ) ) .
the active set ak ( y ) depends on the data y .
when ak ( y ) is the same for all y in
a function g ( y ) is locally lipschitz at y if for all sufciently small vectors y ,
( cid : 123 ) g ( cid : 123 ) = ( cid : 123 ) g ( y + y ) g ( y ) ( cid : 123 ) l ( cid : 123 ) y ( cid : 123 ) .
if the constant l applies for all y , we say that g is uniformly locally lipschitz ( l ) , and the word locally may be dropped .
lemma 123
for each k , 123 k m , there is an open set gk of full measure on which ak ( y ) and ak+123 ( y ) are locally xed and differ by 123 , and k ( y ) is locally linear .
the sets gk are decreasing as k increases .
the argument is by induction .
the induction hypothesis states that for each y123 gk123 there is a small ball b ( y123 ) on which ( a ) the active sets ak123 ( y ) and ak ( y ) are xed and equal to ak123 and ak , respectively , ( b ) |ak \ ak123| = 123 so that the same single variable enters locally at stage k 123 and ( c ) k123 ( y ) = my is linear .
we construct a set gk with the same property .
fix a point y123 and the corresponding ball b ( y123 ) gk123 , on which y k123 ( y ) = y my = ry , say .
for indices j123 , j123 / a , let n ( j123 , j123 ) be the set of y
for which there exists a such that ( ry uk ) = x j123 ( ry uk ) = x
setting 123 = xl xj123 , the rst equality may be written
123uk and so
123uk ( cid : 123 ) = 123 determines
123uk = 123 , there are no qualifying y , and n ( j123 , j123 ) is empty . ) now using the second equality and setting 123 = xl xj123 , we see that n ( j123 , j123 ) is contained in the set of y for which
least angle regression
in other words , setting 123 = r
n ( j123 , j123 ) ( y :
123uk ) 123 , we have 123y = 123 ) .
n ( y123 ) = ( cid : 123 ) ( n ( j123 , j123 ) : j123 , j123 / a , j123 ( cid : 123 ) = j123 ) ,
if we dene
it is evident that n ( y123 ) is a nite union of hyperplanes and hence closed .
for y b ( y123 ) \ n ( y123 ) , a unique new variable joins the active set at step k .
near each such y the joining variable is locally the same and k ( y ) uk is locally linear .
we then dene gk gk123 as the union of such sets b ( y ) \ n ( y ) over y gk123
thus gk is open and , on gk , ak+123 ( y ) is locally constant and k ( y ) is locally linear .
thus properties ( a ) ( c ) hold for gk .
the same argument works for the initial case k = 123 : since 123 = 123 , there is no finally , since the intersection of gk with any compact set is covered by a nite
number of b ( yi ) \ n ( yi ) , it is clear that gk has full measure .
( cid : 123 )
continuous ( resp .
linear ) and uniformly lipschitz .
suppose that , for y near y123 , k123 ( y ) is continuous ( resp .
linear ) and that ak ( y ) = ak .
suppose also that , at y123 , ak+123 ( y123 ) = a ( k + 123 ) .
then for y near y123 , ak+123 ( y ) = ak ( k + 123 ) and k ( y ) and hence k ( y ) are ck and ( cid : 123 ) ckj dened in ( 123 ) and ( 123 ) , respectively .
since k + 123 / ak , we have | ck ( y123 ) | > ck , k+123 ( y123 ) , and k ( y123 ) > 123 satises
the situation at y123 , with ( cid : 123 )
consider rst
j = k + 123 j > k + 123 .
ck , j ( y123 ) k ( y123 ) ak , j
in particular , it must be that ak ( cid : 123 ) = ak , k+123 , and hence k ( y123 ) = ck ( y123 ) ck , k+123 ( y123 )
call an index j admissible if j / ak and ak , j ( cid : 123 ) = ak .
for y near y123 , this property
is independent of y .
for admissible j , dene
which is continuous ( resp .
linear ) near y123 from the assumption on k123
rk , j ( y ) = ck ( y ) ck , j ( y )
k ( y ) = min
pk ( y ) = ( j admissible and rk , j ( y ) > 123 ) .
for admissible j , rk , j ( y123 ) ( cid : 123 ) = 123 , and near y123 the functions y rk , j ( y ) are continuous and of xed sign .
thus , near y123 the set pk ( y ) stays xed at pk ( y123 ) and ( a . 123 ) implies that
rk , k+123 ( y ) < rk , j ( y ) ,
j > k + 123 , j pk ( y ) .
consequently , for y near y123 , only variable k + 123 joins the active set , and so ak+123 ( y ) = ak ( k + 123 ) , and
k ( y ) = rk , k+123 ( y ) = ( xl xk+123 )
this representation shows that both k ( y ) and hence k ( y ) = k123 ( y ) + k ( y ) uk are continuous ( resp .
linear ) near y123
to show that k is locally lipschitz at y , we set = w xk+123 and write , using
notation from ( a . 123 ) ,
efron , hastie , johnstone and tibshirani
as y varies , there is a nite list of vectors ( xl , xk+123 , uk ) that can occur in uk , and since all such terms are positive ( as observed the denominator term below ( a . 123 ) ) , they have a uniform positive lower bound , amin say .
since ( cid : 123 ) ( cid : 123 ) 123 and k123 is lipschitz ( lk123 ) by assumption , we conclude that
min ( 123 + lk123 ) = : lk .
consequences of the positive cone condition .
x+ = sj xj for some j / a ) .
let pa = xag so that a = x
suppose that |a+| = |a| + 123 and that xa+ = ( xa x+ ) ( where a denote projection on span ( xa ) , ( cid : 123 ) +pax+ < 123
the + - component of g
a+123a+ ) + = ( 123 a )
consequently , under the positive cone condition ( 123 ) ,
( cid : 123 ) +ua < aa .
write ga+ as a partitioned matrix
least angle regression
applying the formula for the inverse of a partitioned matrix ( e . g . , rao ( 123 ) , page
a+123a+ ) + = e
e = d b f = a
123b = g
123b = 123 x
from which ( a . 123 ) follows .
the positive cone condition implies that g and so ( a . 123 ) is immediate .
( cid : 123 )
a+123a+ > 123 ,
global continuity and lemma 123
we shall call y123 a multiple point at step k if two or more variables enter at the same time .
lemma 123 shows that such points form a set of measure zero , but they can and do cause discontinuities
in k+123 at y123 in general .
we will see , however , that the positive cone condition
prevents such discontinuities .
we conne our discussion to double points , hoping that these arguments will be sufcient to establish the same pattern of behavior at points of multiplicity 123 or higher .
in addition , by renumbering , we shall suppose that indices k + 123 and k + 123 are those that are added at double point y123
similarly , for convenience only , we assume that ak ( y ) is constant near y123
our task then is to show that , for y near a double point y123 , both k ( y ) and k+123 ( y ) are continuous and uniformly locally
suppose that ak ( y ) = ak
is constant near y123 and that ak+ ( y123 ) = ak ( k + 123 , k + 123 ) .
then for y near y123 , ak+ ( y ) \ ak can only be one of three possibilities , namely ( k + 123 ) , ( k + 123 ) or ( k + 123 , k + 123 ) .
in all cases k ( y ) = k123 ( y ) + k ( y ) uk as usual , and both k ( y ) and k ( y ) are continuous
and locally lipschitz .
double point and the positivity set pk ( y ) = pk near y123 , we have
we use notation and tools from the proof of lemma 123
since y123 is a 123 < rk , k+123 ( y123 ) = rk , k+123 ( y123 ) < rk , j ( y123 )
for j pk \ ( k + 123 , k + 123 ) .
continuity of rk , j implies that near y123 we still have
123 < rk , k+123 ( y ) , rk , k+123 ( y ) < min
rk , j ( y ) ; j pk \ ( k + 123 , k + 123 ) ( cid : 123 )
hence ak+ \ ak must equal ( k + 123 ) or ( k + 123 ) or ( k + 123 , k + 123 ) according as rk , k+123 ( y ) is less than , greater than or equal to rk , k+123 ( y ) .
the continuity of
k ( y ) = min ( rk , k+123 ( y ) , rk , k+123 ( y ) )
is immediate , and the local lipschitz property follows from the arguments of lemma 123
( cid : 123 )
efron , hastie , johnstone and tibshirani
lemma 123
assume the conditions of lemma 123 and in addition that the
positive cone condition ( 123 ) holds .
then k+123 ( y ) is continuous and locally lipschitz near y123
since y123 is a double point , property ( a . 123 ) holds , but now with equality when j = k+ 123 or k+ 123 and strict inequality otherwise .
in other words , there exists 123 > 123 for which
ck+123 ( y123 ) ck+123 , j ( y123 )
if j = k + 123 , if j > k + 123
consider a neighborhood b ( y123 ) of y123 and let n ( y123 ) be the set of double points in b ( y123 ) , that is , those for which ak+123 ( y ) \ ak = ( k + 123 , k + 123 ) .
we establish the convention that at such double points k+123 ( y ) = k ( y ) ; at other points y in b ( y123 ) , k+123 ( y ) is dened by k ( y ) + k+123 ( y ) uk+123 as usual .
now consider those y near y123 for which ak+123 ( y ) \ ak = ( k + 123 ) , and so , from the previous lemma , ak+123 ( y ) \ ak+123 = ( k + 123 ) .
for such y , continuity and the ( cid : 123 ) = o ( ( cid : 123 ) y y123 ( cid : 123 ) ) , local lipschitz property for k imply that
ck+123 ( y ) ck+123 , j ( y )
if j = k + 123 , if j > k + 123
it is at this point that we use the positive cone condition ( via lemma 123 ) to guarantee that ak+123 > ak+123 , k+123
also , since ak+123 ( y ) \ ak = ( k + 123 ) , we have
ck+123 ( y ) > ck+123 , k+123 ( y ) .
these two facts together show that k + 123 pk+123 ( y ) and hence that = o ( ( cid : 123 ) y y123 ( cid : 123 ) )
k+123 ( y ) = ck+123 ( y ) ck+123 , k+123 ( y )
is continuous and locally lipschitz .
in particular , as y approaches n ( y123 ) , we have k+123 ( y ) 123
( cid : 123 )
remark a . 123
we say that a function g : rn r is almost differentiable if it is absolutely continuous on almost all line segments parallel to the coordinate axes , and its partial derivatives ( which consequently exist a . e . ) are locally integrable .
this denition of almost differentiability appears supercially to be weaker than that given by stein , but it is in fact precisely the property used in his proof .
furthermore , this denition is equivalent to the standard denition of weak differentiability used in analysis .
proof of lemma 123
we have shown explicitly that k ( y ) is continuous and uniformly locally lipschitz near single and double points .
similar arguments
least angle regression
extend the property to points of multiplicity 123 and higher , and so all points y are covered .
finally , absolute continuity of y k ( y ) on line segments is a simple consequence of the uniform lipschitz property , and so k is almost differentiable .
acknowledgments .
the authors thank jerome friedman , bogdan popescu ,
saharon rosset and ji zhu for helpful discussions .
