in this paper we propose a novel method for learning a mahalanobis distance measure to be used in the knn classication algorithm .
the algorithm directly maximizes a stochastic variant of the leave - one - out knn score on the training set .
it can also learn a low - dimensional lin - ear embedding of labeled data that can be used for data visualization and fast classication .
unlike other methods , our classication model is non - parametric , making no assumptions about the shape of the class distributions or the boundaries between them .
the performance of the method is demonstrated on several data sets , both for metric learning and linear dimensionality reduction .
nearest neighbor ( knn ) is an extremely simple yet surprisingly effective method for clas - its appeal stems from the fact that its decision surfaces are nonlinear , there is only a single integer parameter ( which is easily tuned with cross - validation ) , and the expected quality of predictions improves automatically as the amount of training data in - creases .
these advantages , shared by many non - parametric methods , reect the fact that although the nal classication machine has quite high capacity ( since it accesses the entire reservoir of training data at test time ) , the trivial learning procedure rarely causes overtting
however , knn suffers from two very serious drawbacks .
the rst is computational , since it must store and search through the entire training set in order to classify a single test point .
( storage can potentially be reduced by editing or thinning the training data; and in low dimensional input spaces , the search problem can be mitigated by employing data structures such as kd - trees or ball - trees ( 123 ) . ) the second is a modeling issue : how should the distance metric used to dene the nearest neighbours of a test point be dened ? in this paper , we attack both of these difculties by learning a quadratic distance metric which optimizes the expected leave - one - out classication error on the training data when used with a stochastic neighbour selection rule .
furthermore , we can force the learned distance metric to be low rank , thus substantially reducing storage and search costs at test time .
123 stochastic nearest neighbours for distance metric learning
we begin with a labeled data set consisting of n real - valued input vectors x123 , .
, xn in rd and corresponding class labels c123 , . . . , cn .
we want to nd a distance metric that maximizes
the performance of nearest neighbour classication .
ideally , we would like to optimize performance on future test data , but since we do not know the true data distribution we instead attempt to optimize leave - one - out ( loo ) performance on the training data .
in what follows , we restrict ourselves to learning mahalanobis ( quadratic ) distance metrics , which can always be represented by symmetric positive semi - denite matrices .
we esti - mate such metrics through their inverse square roots , by learning a linear transformation of the input space such that in the transformed space , knn performs well .
if we denote the transformation by a matrix a we are effectively learning a metric q = a>a such that d ( x , y ) = ( x y ) >q ( x y ) = ( ax ay ) > ( ax ay ) .
the actual leave - one - out classication error of knn is quite a discontinuous function of the transformation a , since an innitesimal change in a may change the neighbour graph and thus affect loo classication performance by a nite amount .
instead , we adopt a more well behaved measure of nearest neighbour performance , by introducing a differentiable cost function based on stochastic ( soft ) neighbour assignments in the transformed space .
in particular , each point i selects another point j as its neighbour with some probability pij , and inherits its class label from the point it selects .
we dene the pij using a softmax over euclidean distances in the transformed space :
pk123=i exp ( kaxi axkk123 )
pii = 123
under this stochastic selection rule , we can compute the probability pi that point i will be correctly classied ( denote the set of points in the same class as i by ci = ( j|ci = cj ) ) :
pi = xjci
the objective we maximize is the expected number of points correctly classied under this
f ( a ) =xi xjci
differentiating f with respect to the transformation matrix a yields a gradient rule which we can use for learning ( denote xij = xi xj ) :
= 123axi xjci
reordering the terms we obtain a more efciently computed expression :
our algorithm which we dub neighbourhood components analysis ( nca ) is extremely simple : maximize the above objective ( 123 ) using a gradient based optimizer such as delta - bar - delta or conjugate gradients .
of course , since the cost function above is not convex , some care must be taken to avoid local maxima during training .
however , unlike many other objective functions ( where good optima are not necessarily deep but rather broad ) it has been our experience that the larger we can drive f during training the better our test performance will be .
in other words , we have never observed an overtraining effect .
notice that by learning the overall scale of a as well as the relative directions of its rows we are also effectively learning a real - valued estimate of the optimal number of neighbours ( k ) .
this estimate appears as the effective perplexity of the distributions pij .
if the learning
procedure wants to reduce the effective perplexity ( consult fewer neighbours ) it can scale up a uniformly; similarly by scaling down all the entries in a it can increase the perplexity of and effectively average over more neighbours during the stochastic selection .
maximizing the objective function f ( a ) is equivalent to minimizing the l123 norm between the true class distribution ( having probability one on the true class ) and the stochastic class distribution induced by pij via a .
a natural alternative distance is the kl - divergence which induces the following objective function :
maximizing this objective would correspond to maximizing the probability of obtaining a perfect ( error free ) classication of the entire training set .
the gradient of g ( a ) is even simpler than that of f ( a ) :
= 123axi xk
we have experimented with optimizing this cost function as well , and found both the trans - formations learned and the performance results on training and testing data to be very similar to those obtained with the original cost function .
to speed up the gradient computation , the sums that appear in equations ( 123 ) and ( 123 ) over the data points and over the neigbours of each point , can be truncated ( one because we can do stochastic gradient rather than exact gradient and the other because pij drops off
123 low rank distance metrics and nonsquare projection
often it is useful to reduce the dimensionality of input data , either for computational sav - ings or for regularization of a subsequent learning algorithm .
linear dimensionality re - duction techniques ( which apply a linear operator to the original data in order to arrive at the reduced representation ) are popular because they are both fast and themselves rela - tively immune to overtting .
because they implement only afne maps , linear projections also preserve some essential topology of the original data .
many approaches exist for lin - ear dimensionality reduction , ranging from purely unsupervised approaches ( such as factor analysis , principal components analysis and independent components analysis ) to methods which make use of class labels in addition to input features such as linear discriminant analysis ( lda ) ( 123 ) possibly combined with relevant components analysis ( rca ) ( 123 ) .
by restricting a to be a nonsquare matrix of size dd , nca can also do linear dimension - ality reduction .
in this case , the learned metric will be low rank , and the transformed inputs will lie in rd .
( since the transformation is linear , without loss of generality we only con - sider the case d d .
) by making such a restriction , we can potentially reap many further benets beyond the already convenient method for learning a knn distance metric .
in par - ticular , by choosing d ( cid : 123 ) d we can vastly reduce the storage and search - time requirements of knn .
selecting d = 123 or d = 123 we can also compute useful low dimensional visual - izations on labeled datasets , using only a linear projection .
the algorithm is exactly the same : optimize the cost function ( 123 ) using gradient descent on a nonsquare a .
our method requires no matrix inversions and assumes no parametric model ( gaussian or otherwise ) for the class distributions or the boundaries between them .
for now , the dimensionality of the reduced representation ( the number of rows in a ) must be set by the user .
by using an highly rectangular a so that d ( cid : 123 ) d , we can signicantly reduce the com - putational load of knn at the expense of restricting the allowable metrics to be those of
rank at most d .
to achieve this , we apply the nca learning algorithm to nd the optimal transformation a , and then we store only the projections of the training points yn = axn ( as well as their labels ) .
at test time , we classify a new point xtest by rst computing its projection ytest = axtest and then doing knn classication on ytest using the yn and a simple euclidean metric .
if d is relatively small ( say less than 123 ) , we can preprocess the yn by building a kd - tree or a ball - tree to further increase the speed of search at test time .
the storage requirements of this method are o ( dn ) + dd compared with o ( dn ) for knn in the original input space .
123 experiments in metric learning and dimensionality reduction
we have evaluated the nca algorithm against standard distance metrics for knn and other methods for linear dimensionality reduction .
in our experiments , we have used 123 data sets ( 123 from the uc irvine repository ) .
we compared the nca transformation obtained from optimizing f ( for square a ) on the training set with the default euclidean distance a = i , the whitening transformation , a = 123 123 ( where is the sample data covariance matrix ) , and the rca ( 123 ) transformation a = w ( where w is the average of the within - class covariance matrices ) .
we also investigated the behaviour of nca when a is restricted to be diagonal , allowing only axis aligned mahalanobis measures .
figure 123 shows that the training and ( more importantly ) testing performance of nca is consistently the same as or better than that of other mahalanobis distance measures for knn , despite the relative simplicity of the nca objective function and the fact that the distance metric being learned is nothing more than a positive denite matrix a>a .
we have also investigated the use of linear dimensionality reduction using nca ( with non - square a ) for visualization as well as reduced - complexity classication on several datasets .
in gure 123 we show 123 examples of 123 - d visualization .
first , we generated a synthetic three - dimensional dataset ( shown in top row of gure 123 ) which consists of 123 classes ( shown by different colors ) .
in two dimensions , the classes are distributed in concentric circles , while the third dimension is just gaussian noise , uncorrelated with the other dimensions or the class label .
if the noise variance is large enough , the projection found by pca is forced to include the noise ( as shown on the top left of gure 123 ) .
( a full rank euclidean metric would also be misled by this dimension . ) the classes are not convex and cannot be lin - early separated , hence the results obtained from lda will be inappropriate ( as shown in gure 123 ) .
in contrast , nca adaptively nds the best projection without assuming any para - metric structure in the low dimensional representation .
we have also applied nca to the uci wine dataset , which consists of 123 points labeled into 123 classes and to a database of gray - scale images of faces consisting of 123 classes ( each a separate individual ) and 123 dimensions ( image size is 123 123 ) .
the face dataset consists of 123 images ( 123 for each person ) .
finally , we applied our algorithm to a subset of the usps dataset of handwritten digit images , consisting of the rst ve digit classes ( one through ve ) .
the grayscale images were downsampled to 123 123 pixel resolution resulting in 123 dimensions .
as can be seen in gure 123 when a two - dimensional projection is used , the classes are con - sistently much better separated by the nca transformation than by either pca ( which is unsupervised ) or lda ( which has access to the class labels ) .
of course , the nca transfor - mation is still only a linear projection , just optimized with a cost function which explicitly encourages local separation .
to further quantify the projection results we can apply a nearest - neighbor classication in the projected space .
using the same projection learned at training time , we project the training set and all future test points and perform knn in the low - dimensional space using the euclidean measure .
the results under the pca , lda , lda followed by rca and nca transformations ( using k=123 ) appear in gure 123
the nca projection consistently gives superior performance in this highly constrained low -
distance metric learning training
distance metric learning testing
rank 123 transformation training
rank 123 transformation testing
figure 123 : knn classication accuracy ( left train , right test ) on uci datasets balance , iono - sphere , iris , wine and housing and on the usps handwritten digits .
results are averages over 123 realizations of splitting each dataset into training ( 123% ) and testing ( 123% ) subsets ( for usps 123 images for each of the 123 digit classes were used for training and 123 for testing ) .
top panels show distance metric learning ( square a ) and bottom panels show linear dimensionality reduction down to d = 123
rank knn setting .
in summary , we have found that when labeled data is available , nca performs better both in terms of classication performance in the projected representation and in terms of visualization of class separation as compared to the standard methods of pca and lda .
123 extensions to continuous labels and semi - supervised learning
although we have focused here on discrete classes , linear transformations and fully su - pervised learning , many extensions of this basic idea are possible .
clearly , a nonlinear transformation function a ( ) could be learned using any architecture ( such as a multilayer perceptron ) trainable by gradient methods .
furthermore , it is possible to extend the clas - sication framework presented above to the case of a real valued ( continuous ) supervision signal by dening the set of correct matches ci for point i to be those points j having similar ( continuous ) targets .
this naturally leads to the idea of soft matches , in which the objective function becomes a sum over all pairs , each weighted by their agreement ac - cording to the targets .
learning under such an objective can still proceed even in settings where the targets are not explicitly provided as long as information identifying close pairs
figure 123 : dataset visualization results of pca , lda and nca applied to ( from top ) the concentric rings , wine , faces and digits datasets .
the data are reduced from their original dimensionalities ( d=123 , d=123 , d=123 , d=123 respectively ) to the d=123 dimensions
figure 123 : the two dimensional outputs of the neural network on a set of test cases .
on the left , each point is shown using a line segment that has the same orientation as the input face .
on the right , the same points are shown again with the size of the circle representing the size of the face .
is available .
such semi - supervised tasks often arise in domains with strong spatial or tem - poral continuity constraints on the supervision , e . g .
in a video of a persons face we may assume that pose , and expression vary slowly in time even if no individual frames are ever labeled explicitly with numerical pose or expression values .
to illustrate this , we generate pairs of faces in the following way : first we choose two faces at random from the feret - b dataset ( 123 isolated faces that have a standard orientation and scale ) .
the rst face is rotated by an angle uniformly distributed between 123o and scaled to have a height uniformly distributed between 123 and 123 pixels .
the second face ( which is of a different person ) is given the same rotation and scaling but with gaussian noise of 123o and 123 pixels .
the pair is given a weight , wab , which is the probability density of the added noise divided by its maximum possible value .
we then trained a neural network with one hidden layer of 123 logistic units to map from the 123 pixel intensities of a face to a point , y , in a 123 - d output space .
backpropagation was used to minimize the cost function in eq .
123 which encourages the faces in a pair to be placed close together :
wab log exp ( ||ya yb||123 )
pc , d exp ( ||yc yd||123 ) ! ( 123 )
cost = xpair ( a , b )
where c and d are indices over all of the faces , not just the ones that form a pair .
four example faces are shown to the right; hori - zontally the pairs agree and vertically they do not .
figure 123 above shows that the feedforward neural network discovered polar coor - dinates without the user having to decide how to represent scale and orientation in the output space .
123 relationships to other methods and conclusions
several papers recently addressed the problem of learning mahalanobis distance functions given labeled data or at least side - information of the form of equivalence constraints .
two related methods are rca ( 123 ) and a convex optimization based algorithm ( 123 ) .
rca is implicitly assuming a gaussian distribution for each class ( so it can be described using only the rst two moments of the class - conditional distribution ) .
xing et .
al attempt to nd a transformation which minimizes all pairwise squared distances between points in the
same class; this implicitly assumes that classes form a single compact connected set .
for highly multimodal class distributions this cost function will be severely penalized .
lowe ( 123 ) proposed a method similar to ours but used a more limited idea for learning a nearest neighbour distance metric .
in his approach , the metric is constrained to be diagonal ( as well , it is somewhat redundantly parameterized ) , and the objective function corresponds to the average squared error between the true class distribution and the predicted distribution , which is not entirely appropriate in a more probabilistic setting .
in parallel there has been work on learning low rank transformations for fast classication and visualization .
the classic lda algorithm ( 123 ) is optimal if all class distributions are gaussian with a single shared covariance; this assumption , however is rarely true .
lda also suffers from a small sample size problem when dealing with high - dimensional data when the within - class scatter matrix is nearly singular ( 123 ) .
recent variants of lda ( e . g .
( 123 ) , ( 123 ) ) make the transformation more robust to outliers and to numerical instability when not enough datapoints are available .
( this problem does not exist in our method since there is no need for a matrix inversion . )
in general , there are two classes of regularization assumption that are common in linear methods for classication .
the rst is a strong parametric assumption about the structure of the class distributions ( typically enforcing connected or even convex structure ) ; the second is an assumption about the decision boundary ( typically enforcing a hyperplane ) .
our method makes neither of these assumptions , relying instead on the strong regularization imposed by restricting ourselves to a linear transformation of the original inputs .
future research on the nca model will investigate using local estimates of k as derived from the entropy of the distributions pij; the possible use of a stochastic classication rule at test time; and more systematic comparisons between the objective functions f and g .
to conclude , we have introduced a novel non - parametric learning method nca that handles the tasks of distance learning and dimensionality reduction in a unied manner .
although much recent effort has focused on non - linear methods , we feel that linear em - bedding has still not fully fullled its potential for either visualization or learning .
thanks to david heckerman and paul viola for suggesting that we investigate the alterna - tive cost g ( a ) and the case of diagonal a .
