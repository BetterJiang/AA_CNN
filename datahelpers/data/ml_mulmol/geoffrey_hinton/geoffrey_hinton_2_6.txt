most of the existing approaches to collab - orative ( cid : 123 ) ltering cannot handle very large in this paper we show how a class of two - layer undirected graphical mod - els , called restricted boltzmann machines ( rbms ) , can be used to model tabular data , such as users ratings of movies .
we present e ( cid : 123 ) cient learning and inference procedures for this class of models and demonstrate that rbms can be successfully applied to the net ( cid : 123 ) ix data set , containing over 123 mil - lion user / movie ratings .
we also show that rbms slightly outperform carefully - tuned svd models .
when the predictions of mul - tiple rbm models and multiple svd models are linearly combined , we achieve an error rate that is well over 123% better than the score of net ( cid : 123 ) ixs own system .
a common approach to collaborative ( cid : 123 ) ltering is to as - sign a low - dimensional feature vector to each user and a low - dimensional feature vector to each movie so that the rating that each user assigns to each movie is mod - eled by the scalar - product of the two feature vectors .
this means that the n ( cid : 123 ) m matrix of ratings that n users assign to m movies is modeled by the matrix x which is the product of an n ( cid : 123 ) c matrix u whose rows are the user feature vectors and a c ( cid : 123 ) m matrix v 123 whose columns are the movie feature vectors .
the rank of x is c ( the number of features assigned to each user or movie .
appearing in proceedings of the 123 th international confer - ence on machine learning , corvallis , or , 123
copyright 123 by the author ( s ) / owner ( s ) .
low - rank approximations based on minimizing the sum - squared distance can be found using singular value decomposition ( svd ) .
in the collaborative ( cid : 123 ) l - tering domain , however , most of the data sets are sparse , and as shown by srebro and jaakkola ( 123 ) , this creates a di ( cid : 123 ) cult non - convex problem , so a naive solution is not going work . 123
in this paper we describe a class of two - layer undi - rected graphical models that generalize restricted boltzmann machines to modeling tabular or count data ( welling et al . , 123 ) .
maximum likelihood learn - ing is intractable in these models , but we show that learning can be performed e ( cid : 123 ) ciently by following an approximation to the gradient of a di ( cid : 123 ) erent objec - tive function called \contrastive divergence " ( hinton ,
restricted boltzmann machines
suppose we have m movies , n users , and integer rat - ing values from 123 to k .
the ( cid : 123 ) rst problem in applying rbms to movie ratings is how to deal e ( cid : 123 ) ciently with the missing ratings .
if all n users rated the same set of m movies , we could treat each user as a single training case for an rbm which had m \softmax " vis - ible units symmetrically connected to a set of binary hidden units .
each hidden unit could then learn to model a signi ( cid : 123 ) cant dependency between the ratings of di ( cid : 123 ) erent movies .
when most of the ratings are miss - ing , we use a di ( cid : 123 ) erent rbm for each user ( see fig .
every rbm has the same number of hidden units , but an rbm only has visible softmax units for the movies rated by that user , so an rbm has few connec - tions if that user rated few movies .
each rbm only has a single training case , but all of the corresponding
123we describe the details of the svd training procedure
in section 123
restricted boltzmann machines for collaborative filtering
figure 123
a restricted boltzmann machine with binary hidden units and softmax visible units .
for each user , the rbm only includes softmax units for the movies that user has rated .
in addition to the symmetric weights between each hidden unit and each of the k = 123 values of a soft - max unit , there are 123 biases for each softmax unit and one for each hidden unit .
when modeling user ratings with an rbm that has gaussian hidden units , the top layer is composed of linear units with gaussian noise .
weights and biases are tied together , so if two users have rated the same movie , their two rbms must use the same weights between the softmax visible unit for that movie and the hidden units .
the binary states of the hidden units , however , can be quite di ( cid : 123 ) erent for di ( cid : 123 ) erent users .
from now on , to simplify the nota - tion , we will concentrate on getting the gradients for the parameters of a single user - speci ( cid : 123 ) c rbm .
the full gradients with respect to the shared weight parameters can then be obtained by averaging over all n users .
suppose a user rated m movies .
let v be a k ( cid : 123 ) m observed binary indicator matrix with vk i = 123 if the user rated movie i as k and 123 otherwise .
we also let hj , j = 123; : : : ; f , be the binary values of hidden ( la - tent ) variables , that can be thought of as representing stochastic binary features that have di ( cid : 123 ) erent values for
the model
we use a conditional multinomial distribution ( a \soft - max " ) for modeling each column of the observed \visible " binary rating matrix v and a conditional bernoulli distribution for modeling \hidden " user fea - tures h ( see fig
p ( hj = 123jv ) = ( cid : 123 ) ( bj +
where ( cid : 123 ) ( x ) = 123= ( 123 + e ( cid : 123 ) x ) is the logistic function , w k is a symmetric interaction parameter between feature j and rating k of movie i , bk i is the bias of rating k for movie i , and bj is the bias of feature j .
note that the i can be initialized to the logs of their respective base rates over all users .
the marginal distribution over the visible ratings v
p ( v ) = xh
exp ( ( cid : 123 ) e ( v; h ) )
pv123;h123 exp ( ( cid : 123 ) e ( v123; h123 ) )
with an \energy " term given by :
e ( v; h ) = ( cid : 123 )
where zi = pk i +pj hjw l ization term that ensures that pk
i = 123jh ) = 123
the movies with missing ratings do not make any con - tribution to the energy function .
ij ( cid : 123 ) is the normal -
the parameter updates required to perform gradient ascent in the log - likelihood can be obtained from eq .
ij = ( cid : 123 )
@ log p ( v )
= ( cid : 123 ) ( cid : 123 ) <vk
i hj>data ( cid : 123 ) <vk
i hj>model ( cid : 123 )
where ( cid : 123 ) is the learning rate .
the expectation i hj>data de ( cid : 123 ) nes the frequency with which movie i with rating k and feature j are on together when the features are being driven by the observed user - rating data from the training set using eq .
123 , and < ( cid : 123 ) >model is an expectation with respect to the distribution de ( cid : 123 ) ned by the model .
the expectation < ( cid : 123 ) >model cannot be computed analytically in less than exponential time .
mcmc methods ( neal , 123 ) can be employed to ap - proximate this expectation .
these methods , however , are quite slow and su ( cid : 123 ) er from high variance in their
i = 123jh ) =
j=123 hjw k j=123 hjw l
to avoid computing < ( cid : 123 ) >model , we follow an approxi - mation to the gradient of a di ( cid : 123 ) erent objective function
restricted boltzmann machines for collaborative filtering
called \contrastive divergence " ( cd ) ( hinton , 123 ) :
over k ratings for a movie q :
ij = ( cid : 123 ) ( <vk
i hj>data ( cid : 123 ) <vk
i hj>t )
the expectation < ( cid : 123 ) >t represents a distribution of samples from running the gibbs sampler ( eqs .
123 , 123 ) , initialized at the data , for t full steps .
t is typi - cally set to one at the beginning of learning and in - creased as the learning converges .
by increasing t to a su ( cid : 123 ) ciently large value , it is possible to approx - imate maximum likelihood learning arbitrarily well ( carreira - perpinan & hinton , 123 ) , but large values of t are seldom needed in practice .
when running the gibbs sampler , we only reconstruct ( eq .
123 ) the distri - bution over the non - missing ratings .
the approximate gradients of cd with respect to the shared weight pa - rameters of eq .
123 can be then be averaged over all n
it was shown ( hinton , 123 ) that cd learning is quite e ( cid : 123 ) cient and greatly reduces the variance of the es - timates used for learning .
the learning rule for the biases is just a simpli ( cid : 123 ) ed version of eq
making predictions
given the observed ratings v , we can predict a rating for a new query movie q in time linear in the number of hidden units :
q = 123jv ) / xh123; : : : ;hp
q ; v; h ) )
( cid : 123 ) 123 + exp ( cid : 123 ) xil
ij + vk
q hjw k
qj + hjbj ( cid : 123 )
ij + vk
qj + bj ( cid : 123 ) ( cid : 123 )
q = exp ( vk
once we obtain unnormalized scores , we can either pick the rating with the maximum score as our prediction , or perform normalization over k values to get probabilities p ( vq = kjv ) and take the expectation e ( vq ) as our prediction .
the latter method works better .
when asked to predict ratings for n movies q123 , q123 , . . . , qn , we can also compute
q123 = 123; vk123
q123 = 123; : : : ; vkn
qn = 123jv )
this , however , requires us to make k n evaluations for
alternatively , we can perform one iteration of the mean ( cid : 123 ) eld updates to get the probability distribution
^pj = p ( hj = 123jv ) = ( cid : 123 ) ( bj +
j=123 ^pjw k j=123 ^pjw l
q = 123j^p ) =
and take an expectation as our prediction .
in our expe - rience , eq .
123 makes slightly more accurate predictions , although one iteration of the mean ( cid : 123 ) eld equations is considerably faster .
we use the mean ( cid : 123 ) eld method in the experiments described below .
rbms with gaussian hidden units
we can also model \hidden " user features h as gaus - sian latent variables ( welling et al . , 123 ) .
this model represents an undirected counterpart of plsi ( hof -
i = 123jh ) =
p ( hj = hjv ) = 123p123 ( cid : 123 ) ( cid : 123 ) j
hj w k
exp ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) h ( cid : 123 ) bj ( cid : 123 ) ( cid : 123 ) jpik
hj w l
j is the variance of the hidden unit j .
the marginal distribution over visible units v is given by eq .
with an energy term :
e ( v; h ) = ( cid : 123 ) xijk
( hj ( cid : 123 ) bj ) 123
we ( cid : 123 ) x variances at ( cid : 123 ) 123 j = 123 for all hidden units j , in which case the parameter updates are the same as de - ( cid : 123 ) ned in eq
conditional rbms
suppose that we add w to each of the k weights from the k possible ratings to each hidden feature and we subtract w from the bias of the hidden feature .
so long as one of the k ratings is present , this does not have any e ( cid : 123 ) ect on the behaviour of the hidden or visible units because the \softmax " is over - parameterized .
if , however , the rating is missing , there is an e ( cid : 123 ) ect of ( cid : 123 ) w on the total input to the hidden feature .
so by using the over - parametrization of the softmax , the rbm can learn to use missing ratings to in ( cid : 123 ) uence its hidden fea - tures , even though it does not try to reconstruct these
restricted boltzmann machines for collaborative filtering
figure 123
conditional rbm .
the binary vector r , indi - cating rated / unrated movies , a ( cid : 123 ) ects binary states of the
missing ratings and it does not perform any computa - tions that scale with the number of missing ratings .
there is a more subtle source of information in the net ( cid : 123 ) ix database that cannot be captured by the \stan - dard " multinomial rbm .
net ( cid : 123 ) ix tells us in advance which user / movie pairs occur in the test set , so we have a third category : movies that were viewed but for which the rating is unknown .
this is a valuable source of information about users who occur several times in the test set , especially if they only gave a small number of ratings in the training set .
if , for ex - ample , a user is known to have rated \rocky 123 " , we already have a good bet about the kinds of movies he
the conditional rbm model takes this extra informa - tion into account .
let r 123 f123; 123gm be a binary vec - tor of length m ( total number of movies ) , indicating which movies the user rated ( even if these ratings are unknown ) .
the idea is to de ( cid : 123 ) ne a joint distribution over ( v; h ) conditional on r .
in the proposed condi - tional model , a vector r will a ( cid : 123 ) ect the states of the hidden units ( see fig
i = 123jh ) =
p ( hj = 123jv; r ) = ( cid : 123 ) ( cid : 123 ) bj +
j=123 hjw k j=123 hjw l
where dij is an element of a learned matrix that mod - els the e ( cid : 123 ) ect of r on h .
learning d using cd is similar
to learning biases and takes the form :
( cid : 123 ) dij = ( cid : 123 ) ( cid : 123 ) <hj>data ( cid : 123 ) <hj>t ( cid : 123 ) ri
we could instead de ( cid : 123 ) ne an arbitrary nonlinear func - tion f ( rj ( cid : 123 ) ) .
provided f is di ( cid : 123 ) erentiable with respect to ( cid : 123 ) , we could use backpropagation to learn ( cid : 123 ) :
( cid : 123 ) ( cid : 123 ) = ( cid : 123 ) ( cid : 123 ) <hj>data ( cid : 123 ) <hj>t ( cid : 123 ) @f ( rj ( cid : 123 ) )
in particular , f ( rj ( cid : 123 ) ) can be parameterized as a multi - layer neural network .
conditional rbm models have been successfully used for modeling temporal data , such as motion cap - ture data ( taylor et al . , 123 ) , or video sequences ( sutskever & hinton , 123 ) .
for the net ( cid : 123 ) ix task , con - ditioning on a vector of rated / unrated movies proves to be quite helpful ( it signi ( cid : 123 ) cantly improves perfor -
instead of using a conditional rbm , we can impute the missing ratings from the ordinary rbm model .
suppose a user rated a movie t , but his / her rating is missing ( i . e .
it was provided as a part of the test set ) .
we can initialize vt to the base rate of movie t , and compute the gradient of the log - probability of the data with respect to this input ( eq .
the cd learning
t = ( cid : 123 ) ( cid : 123 ) <xj
t hj>t ( cid : 123 )
t hj>data ( cid : 123 ) <xj t , for k = 123; : : ; k , vk
after updating vk t are renormalized to obtain probability distribution over k values .
the imputed values vt will now contribute to the energy term of eq .
123 and will a ( cid : 123 ) ect the states of the hidden imputing missing values by following an ap - proximate gradient of cd works quite well on a small subset of the net ( cid : 123 ) ix data set , but is slow for the com - plete data set .
alternatively , we can use a set of mean ( cid : 123 ) eld equations eqs .
123 , 123 to impute the missing val - ues .
the imputed values will be quite noisy , especially at the early stages of training .
nevertheless , in our experiments , the model performance was signi ( cid : 123 ) cantly improved by using imputations and was comparable to the performance of the conditional rbm .
conditional factored rbms
one disadvantage of the rbm models we have de - scribed so far is that their current parameterization of w 123 rm ( cid : 123 ) k ( cid : 123 ) f results in a large number of free param - eters .
in our current implementation , with f = 123
restricted boltzmann machines for collaborative filtering
rbm with gaussian
cd t=123 cd t=123
figure 123
performance of various models on the validation data .
left panel : rbm vs .
rbm with gaussian hidden units .
middle panel : rbm vs .
conditional rbm .
right panel : conditional rbm vs .
conditional factored rbm .
the y - axis displays rmse ( root mean squared error ) , and the x - axis shows the number of epochs , or passes through the entire
( the number of hidden units ) , m = 123 , and k = 123 , we end up with about 123 million free parameters .
by using proper weight - decay to regularize the model , we are still able to avoid serious over ( cid : 123 ) tting .
however , if we increase the number of hidden features or the num - ber of movies , 123 learning this huge parameter matrix w becomes problematic .
reducing the number of free parameters by simply reducing the number of hidden units does not lead to a good model because the model cannot express enough information about each user in its hidden state .
we address this problem by factorizing the parameter matrix w into a product of two lower - rank matrices a and b .
in particular :
where typically c ( cid : 123 ) m and c ( cid : 123 ) f .
for example , setting c = 123 , we reduce the number of free parame - ters by a factor of three .
we call this model a factored rbm .
learning matrices a and b is quite similar to learning w of eq
ic = ( cid : 123 ) ( cid : 123 ) < ( cid : 123 ) xj
( cid : 123 ) bcj = ( cid : 123 ) ( cid : 123 ) < ( cid : 123 ) xik
i >data ( cid : 123 )
i ( cid : 123 ) hj>data ( cid : 123 )
i >t ( cid : 123 )
i ( cid : 123 ) hj>t ( cid : 123 )
123net ( cid : 123 ) ixs own database contains about 123 movie ti -
in our experimental results section we show that a con - ditional factored rbm converges considerably faster than a conditional unfactored rbm .
experimental results
description of the net ( cid : 123 ) ix data
according to net ( cid : 123 ) ix , the data were collected between october , 123 and december , 123 and represent the distribution of all ratings net ( cid : 123 ) ix obtained during this period .
the training data set consists of 123 , 123 , 123 ratings from 123 , 123 randomly - chosen , anonymous users on 123 , 123 movie titles .
as part of the training data , net ( cid : 123 ) ix also provides validation data , containing 123 , 123 , 123 ratings .
in addition to the training and vali - dation data , net ( cid : 123 ) ix also provides a test set containing 123 , 123 , 123 user / movie pairs with the ratings withheld .
the pairs were selected from the most recent ratings from a subset of the users in the training data set , over a subset of the movies .
to reduce the uninten - tional ( cid : 123 ) ne - tuning on the test set that plagues many empirical comparisons in the machine learning litera - ture , performance is assessed by submitting predicted ratings to net ( cid : 123 ) ix who then post the root mean squared error ( rmse ) on an unknown half of the test set .
as a baseline , net ( cid : 123 ) ix provided the score of its own system trained on the same data , which is 123 .
details rbm training
we train the rbm with f = 123 , and the condi - tional factored rbm with f = 123 , and c = 123
to speed - up the training , we subdivided the net ( cid : 123 ) ix dataset into small mini - batches , each containing 123 cases ( users ) , and updated the weights after each mini - batch .
all models were trained for between 123 and 123 passes ( epochs ) through the entire training dataset .
restricted boltzmann machines for collaborative filtering
the weights were updated using a learning rate of 123 / batch - size , momentum of 123 , and a weight de - cay of 123 .
the weights were initialized with small random values sampled from a zero - mean normal dis - tribution with standard deviation 123 .
cd learning was started with t = 123 and increased in small steps
we compare di ( cid : 123 ) erent models based on their perfor - mance on the validation set .
the error that net ( cid : 123 ) ix reports on the test set is typically larger than the er - ror we get on the validation set by about 123 .
when the validation set is added to the training set , rmse on the test set is typically reduced by about 123 .
figure 123 ( left panel ) shows performance of the rbm and the rbm with gaussian hidden units .
the y - axis displays rmse , and the x - axis shows the number of epochs .
clearly , the nonlinear model substantially outperforms its linear counterpart .
figure 123 ( middle panel ) also reveals that conditioning on rated / unrated information signi ( cid : 123 ) cantly improves model performance .
it also shows ( right panel ) that , when using a condi - tional rbm , factoring the weight matrix leads to much
singular value decomposition ( svd )
svd seeks a low - rank matrix x = u v 123 , where u 123 rn ( cid : 123 ) c and v 123 rm ( cid : 123 ) c , that minimizes the sum - squared distance to the fully observed target matrix y .
the solution is given by the leading singular vec - tors of y .
in the collaborative ( cid : 123 ) ltering domain , most of the entries in y will be missing , so the sum - squared distance is minimized with respect to the partially ob - served entries of the target matrix y .
unobserved en - tries of y are then predicted using the corresponding entries of x .
let x = u v 123 , where u 123 rn ( cid : 123 ) c and v 123 rm ( cid : 123 ) c de - note the low - rank approximation to the partially ob - served target matrix y 123 rn ( cid : 123 ) m .
matrices u and v are initialized with small random values sampled from a zero - mean normal distribution with standard deviation 123 .
we minimize the following objective
iij ( cid : 123 ) uivj123 ( cid : 123 ) yij ( cid : 123 ) 123 iij ( cid : 123 ) k ui k123
f ro + k vj k123
f ro ( cid : 123 )
where k ( cid : 123 ) k123
f ro denotes the frobenius norm , and iij is
figure 123
performance of the conditional factored rbm svd with c = 123 factors .
the y - axis displays rmse ( root mean squared error ) , and the x - axis shows the number of epochs , or passes through the entire train -
the indicator function , taking on value 123 if user i rated movie j , and 123 otherwise .
we then perform gradient descent in u and v to minimize the objective function of eq
to speed - up the training , we subdivided the net ( cid : 123 ) ix data into mini - batches of size 123 , 123 ( user / movie pairs ) , and updated the weights after each mini - batch .
the weights were updated using a learning rate of 123 , momentum of 123 , and regularization parameter ( cid : 123 ) = 123 : 123
regularization , particularly for the net ( cid : 123 ) ix dataset , makes quite a signi ( cid : 123 ) cant di ( cid : 123 ) erence in model performance .
we also experimented with various val - ues of c and report the results with c = 123 , since it resulted in the best model performance on the valida - tion set .
values of c in the range of ( 123; 123 ) also give
factored rbm with we compared the conditional an svd model ( see fig .
the conditional fac - tored rbm slightly outperforms svd , but not by much .
both models could potentially be improved by more careful tuning of learning rates , batch sizes , and weight - decay .
more importantly , the errors made by various versions of the rbm are signi ( cid : 123 ) cantly di ( cid : 123 ) erent from the errors made by various versions of svd , so linearly combining the predictions of several di ( cid : 123 ) erent versions of each method , using coe ( cid : 123 ) cients tuned on the validation data , produces an error rate that is well over 123% better than the net ( cid : 123 ) ixs own baseline score .
restricted boltzmann machines for collaborative filtering
future extensions
there are several extensions to our model that we are
learning autoencoders
an alternative way of using an rbm is to treat this learning as a pretraining stage that ( cid : 123 ) nds a good re - gion of the parameter space ( hinton & salakhut - dinov , 123 ) .
after pretraining , the rbm is \un - rolled " as shown in ( cid : 123 ) gure 123 to create an autoencoder network in which the stochastic activities of the bi - nary \hidden " features are replaced by deterministic , real - valued probabilities .
backpropagation , using the squared error objective function , is then used to ( cid : 123 ) ne - tune the weights for optimal reconstruction of each users ratings .
however , over ( cid : 123 ) tting becomes an issue and more careful model regularization is required .
learning deep generative models
recently , ( hinton et al . , 123 ) derived a way to per - form fast , greedy learning of deep belief networks one layer at a time , with the top two layers forming an undirected bipartite graph which acts as an associa -
the learning procedure consists of training a stack of rbms each having only one layer of latent ( hidden ) feature detectors .
the learned feature activations of one rbm are used as the \data " for training the next rbm in the stack .
an important aspect of this layer - wise training proce - dure is that , provided the number of features per layer does not decrease , each extra layer increases a lower bound on the log probability of data .
so layer - by - layer training can be recursively applied several times123 to learn a deep , hierarchical model in which each layer of features captures strong high - order correlations be - tween the activities of features in the layer below .
learning multi - layer models has been successfully ap - plied in the domain of dimensionality reduction ( hin - ton & salakhutdinov , 123 ) , with the resulting mod - els signi ( cid : 123 ) cantly outperforming latent semantic anal - ysis , a well - known document retrieval method based on svd ( deerwester et al . , 123 ) .
it has also been used for modeling temporal data ( taylor et al . , 123; sutskever & hinton , 123 ) and learning nonlinear em - beddings ( salakhutdinov & hinton , 123 ) .
we are currently exploring this kind of learning for the net - ( cid : 123 ) ix data .
for classi ( cid : 123 ) cation of the mnist digits ,
123in fact , one can proceed learning recursively for as
many layers as desired .
figure 123
the \unrolled " rbm used to create an autoen - coder network which is then ( cid : 123 ) ne - tuned using backpropa - gation of error derivatives .
deep networks reduce the error signi ( cid : 123 ) cantly ( hinton & salakhutdinov , 123 ) and our hope is that they will be similarly helpful for the net ( cid : 123 ) ix data .
summary and discussion
we introduced a class of two - layer undirected graph - ical models ( rbms ) , suitable for modeling tabular or count data , and presented e ( cid : 123 ) cient learning and inference procedures for this class of models .
we also demonstrated that rbms can be successfully ap - plied to a large dataset containing over 123 million
a variety of models have recently been proposed for minimizing the loss corresponding to a speci ( cid : 123 ) c prob - abilistic model ( hofmann , 123; canny , 123; marlin & zemel , 123 ) .
all these probabilistic models can be viewed as graphical models in which hidden factor variables have directed connections to variables that represent user ratings .
their major drawback ( welling et al . , 123 ) is that exact inference is intractable due to explaining away , so they have to resort to slow or inaccurate approximations to compute the posterior distribution over hidden factors .
instead of constraining the rank or dimensionality of the factorization x = u v 123 , i . e .
the number of factors , ( srebro et al . , 123 ) proposed constraining the norms of u and v .
this problem formulation termed \max - imum margin matrix factorization " could be seen as constraining the overall \strength " of factors rather than their number .
however , learning mmmf re - quires solving a sparse semi - de ( cid : 123 ) nite program ( sdp ) .
generic sdp solvers run into di ( cid : 123 ) culties with more than about 123 , 123 observations ( user / movie pairs ) , so
restricted boltzmann machines for collaborative filtering
salakhutdinov , r . , & hinton , g .
( 123 ) .
learning a nonlinear embedding by preserving class neighbour - hood structure .
ai and statistics .
srebro , n . , & jaakkola , t .
( 123 ) .
weighted low - rank approximations .
machine learning , proceedings of the twentieth international conference ( icml 123 ) , august 123 - 123 , 123 , washington , dc , usa ( pp .
123 ( 123 ) .
aaai press .
srebro , n . , rennie , j .
m . , & jaakkola , t .
( 123 ) .
maximum - margin matrix factorization .
advances in neural information processing systems .
i . , & hinton , g .
( 123 ) .
ing multilevel distributed representations for high - dimensional sequences ( technical report utml tr 123 - 123 ) .
of computer science , univer - sity of toronto .
taylor , g .
w . , hinton , g .
e . , & roweis , s .
( 123 ) .
modeling human motion using binary latent vari - ables .
advances in neural information processing systems .
mit press .
welling , m . , rosen - zvi , m . , & hinton , g .
( 123 ) .
ex - ponential family harmoniums with an application to information retrieval .
nips 123 ( pp .
123 ( 123 ) .
cambridge , ma : mit press .
direct gradient - based optimization methods have been proposed in an attempt to make mmmf scale up to larger problems .
the net ( cid : 123 ) ix data set , however , con - tains over 123 million observations and none of the above - mentioned approaches can easily deal with such large data sets .
we thank vinod nair , tijmen tieleman and ilya sutskever for many helpful discussions .
we thank net - ( cid : 123 ) ix for making such nice data freely available and for providing a free and rigorous model evaluation service .
