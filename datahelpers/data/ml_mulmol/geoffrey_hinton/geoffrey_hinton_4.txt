most current speech recognition systems use hidden markov models ( hmms ) to deal with the temporal variability of speech and gaussian mixture models to determine how well each state of each hmm ts a frame or a short window of frames of coefcients that represents the acoustic input .
an alternative way to evaluate the t is to use a feed - forward neural network that takes several frames of coefcients as input and produces posterior probabilities over hmm states as output .
deep neural networks with many hidden layers , that are trained using new methods have been shown to outperform gaussian mixture models on a variety of speech recognition benchmarks , sometimes by a large margin .
this paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition .
new machine learning algorithms can lead to signicant advances in automatic speech recognition .
the biggest
single advance occured nearly four decades ago with the introduction of the expectation - maximization ( em )
algorithm for training hidden markov models ( hmms ) ( see ( 123 ) , ( 123 ) for informative historical reviews of the
introduction of hmms ) .
with the em algorithm , it became possible to develop speech recognition systems for
real world tasks using the richness of gaussian mixture models ( gmm ) ( 123 ) to represent the relationship between
hmm states and the acoustic input .
in these systems the acoustic input is typically represented by concatenating
mel frequency cepstral coefcients ( mfccs ) or perceptual linear predictive coefcients ( plps ) ( 123 ) computed
from the raw waveform , and their rst - and second - order temporal differences ( 123 ) .
this non - adaptive but highly -
engineered pre - processing of the waveform is designed to discard the large amount of information in waveforms that
is considered to be irrelevant for discrimination and to express the remaining information in a form that facilitates
discrimination with gmm - hmms .
gmms have a number of advantages that make them suitable for modeling the probability distributions over
vectors of input features that are associated with each state of an hmm .
with enough components , they can model
hinton , dahl , mohamed , and jaitly are with the university of toronto .
deng and yu are with microsoft research .
senior , vanhoucke and nguyen are with google research .
sainath and kingsbury are with ibm research .
april 123 , 123
probability distributions to any required level of accuracy and they are fairly easy to t to data using the em
algorithm .
a huge amount of research has gone into ways of constraining gmms to increase their evaluation speed
and to optimize the trade - off between their exibility and the amount of training data available to avoid serious
the recognition accuracy of a gmm - hmm system can be further improved if it is discriminatively ne - tuned
after it has been generatively trained to maximize its probability of generating the observed data , especially if
the discriminative objective function used for training is closely related to the error rate on phones , words or
sentences ( 123 ) .
the accuracy can also be improved by augmenting ( or concatenating ) the input features ( e . g . , mfccs )
with tandem or bottleneck features generated using neural networks ( 123 ) , ( 123 ) .
gmms are so successful that it is
difcult for any new method to outperform them for acoustic modeling .
despite all their advantages , gmms have a serious shortcoming they are statistically inefcient for modeling
data that lie on or near a non - linear manifold in the data space .
for example , modeling the set of points that lie very
close to the surface of a sphere only requires a few parameters using an appropriate model class , but it requires a
very large number of diagonal gaussians or a fairly large number of full - covariance gaussians .
speech is produced
by modulating a relatively small number of parameters of a dynamical system ( 123 ) , ( 123 ) and this implies that its true
underlying structure is much lower - dimensional than is immediately apparent in a window that contains hundreds
of coefcients .
we believe , therefore , that other types of model may work better than gmms for acoustic modeling
if they can more effectively exploit information embedded in a large window of frames .
articial neural networks trained by backpropagating error derivatives have the potential to learn much better
models of data that lie on or near a non - linear manifold .
in fact two decades ago , researchers achieved some success
using articial neural networks with a single layer of non - linear hidden units to predict hmm states from windows
of acoustic coefcients ( 123 ) .
at that time , however , neither the hardware nor the learning algorithms were adequate
for training neural networks with many hidden layers on large amounts of data and the performance benets of
using neural networks with a single hidden layer were not sufciently large to seriously challenge gmms
result , the main practical contribution of neural networks at that time was to provide extra features in tandem or
over the last few years , advances in both machine learning algorithms and computer hardware have led to more
efcient methods for training deep neural networks ( dnns ) that contain many layers of non - linear hidden units and
a very large output layer .
the large output layer is required to accommodate the large number of hmm states that
arise when each phone is modelled by a number of different triphone hmms that take into account the phones on
either side .
even when many of the states of these triphone hmms are tied together , there can be thousands of tied
states .
using the new learning methods , several different research groups have shown that dnns can outperform
gmms at acoustic modeling for speech recognition on a variety of datasets including large datasets with large
this review paper aims to represent the shared views of research groups at the university of toronto , microsoft
research ( msr ) , google and ibm research , who have all had recent successes in using dnns for acoustic
april 123 , 123
modeling .
the paper starts by describing the two - stage training procedure that is used for tting the dnns .
in the
rst stage , layers of feature detectors are initialized , one layer at a time , by tting a stack of generative models ,
each of which has one layer of latent variables .
these generative models are trained without using any information
about the hmm states that the acoustic model will need to discriminate .
in the second stage , each generative model
in the stack is used to initialize one layer of hidden units in a dnn and the whole network is then discriminatively
ne - tuned to predict the target hmm states .
these targets are obtained by using a baseline gmm - hmm system to
produce a forced alignment .
in this paper we review exploratory experiments on the timit database ( 123 ) , ( 123 ) that were used to demonstrate
the power of this two - stage training procedure for acoustic modeling .
the dnns that worked well on timit were
then applied to ve different large vocabulary , continuous speech recognition tasks by three different research groups
whose results we also summarize .
the dnns worked well on all of these tasks when compared with highly - tuned
gmm - hmm systems and on some of the tasks they outperformed the state - of - the - art by a large margin .
we also
describe some other uses of dnns for acoustic modeling and some variations on the training procedure .
training deep neural networks
a deep neural network ( dnn ) is a feed - forward , articial neural network that has more than one layer of hidden units between its inputs and its outputs .
each hidden unit , j , typically uses the logistic function123 to map its total
input from the layer below , xj , to the scalar state , yj that it sends to the layer above .
yj = logistic ( xj ) =
123 + exj
xj = bj +xi
where bj is the bias of unit j , i is an index over units in the layer below , and wij is a the weight on a connection
to unit j from unit i in the layer below .
for multiclass classication , output unit j converts its total input , xj , into
a class probability , pj , by using the softmax non - linearity :
where k is an index over all classes .
dnns can be discriminatively trained by backpropagating derivatives of a cost function that measures the
discrepancy between the target outputs and the actual outputs produced for each training case ( 123 ) .
when using the
softmax output function , the natural cost function c is the cross - entropy between the target probabilities d and the
outputs of the softmax , p :
where the target probabilities , typically taking values of one or zero , are the supervised information provided to
c = xj
dj log pj ,
train the dnn classier .
123the closely related hyberbolic tangent is also often used and any function with a well - behaved derivative can be used .
april 123 , 123
for large training sets , it is typically more efcient to compute the derivatives on a small , random mini - batch
of training cases , rather than the whole training set , before updating the weights in proportion to the gradient
stochastic gradient descent method can be further improved by using a momentum coefcient , 123 < < 123 , that
smooths the gradient computed for mini - batch t , thereby damping oscillations across ravines and speeding progress
the update rule for biases can be derived by treating them as weights on connections coming from units that always
wij ( t ) = wij ( t 123 )
have a state of 123
to reduce overtting , large weights can be penalized in proportion to their squared magnitude , or the learning
can simply be terminated at the point at which performance on a held - out validation set starts getting worse ( 123 )
dnns with full connectivity between adjacent layers , the initial weights are given small random values to prevent
all of the hidden units in a layer from getting exactly the same gradient .
dnns with many hidden layers are hard to optimize .
gradient descent from a random starting point near the
origin is not the best way to nd a good set of weights and unless the initial scales of the weights are carefully
chosen ( 123 ) , the backpropagated gradients will have very different magnitudes in different layers .
in addition to
the optimization issues , dnns may generalize poorly to held - out test data .
dnns with many hidden layers and
many units per layer are very exible models with a very large number of parameters .
this makes them capable of
modeling very complex and highly non - linear relationships between inputs and outputs .
this ability is important
for high - quality acoustic modeling , but it also allows them to model spurious regularities that are an accidental
property of the particular examples in the training set , which can lead to severe overtting .
weight penalties or
early - stopping can reduce the overtting but only by removing much of the modeling power .
very large training sets
( 123 ) can reduce overtting whilst preserving modeling power , but only by making training very computationally
expensive .
what we need is a better method of using the information in the training set to build multiple layers of
non - linear feature detectors .
generative pre - training
instead of designing feature detectors to be good for discriminating between classes , we can start by designing
them to be good at modeling the structure in the input data .
the idea is to learn one layer of feature detectors at
a time with the states of the feature detectors in one layer acting as the data for training the next layer .
after this
generative pre - training , the multiple layers of feature detectors can be used as a much better starting point for
a discriminative ne - tuning phase during which backpropagation through the dnn slightly adjusts the weights
found in pre - training ( 123 ) .
some of the high - level features created by the generative pre - training will be of little
use for discrimination , but others will be far more useful than the raw inputs .
the generative pre - training nds a
region of the weight - space that allows the discriminative ne - tuning to make rapid progress , and it also signicantly
reduces overtting ( 123 ) .
april 123 , 123
a single layer of feature detectors can be learned by tting a generative model with one layer of latent variables
to the input data .
there are two broad classes of generative model to choose from .
a directed model generates
data by rst choosing the states of the latent variables from a prior distribution and then choosing the states of the
observable variables from their conditional distributions given the latent states .
examples of directed models with
one layer of latent variables are factor analysis , in which the latent variables are drawn from an isotropic gaussian ,
and gmms , in which they are drawn from a discrete distribution .
an undirected model has a very different way of
generating data .
instead of using one set of parameters to dene a prior distribution over the latent variables and a
separate set of parameters to dene the conditional distributions of the observable variables given the values of the
latent variables , an undirected model uses a single set of parameters , w , to dene the joint probability of a vector
of values of the observable variables , v , and a vector of values of the latent variables , h , via an energy function ,
p ( v , h; w ) =
where z is called the partition function .
ee ( v , h;w ) , z = xv , h
if many different latent variables interact non - linearly to generate each data vector , it is difcult to infer the states
of the latent variables from the observed data in a directed model because of a phenomenon known as explaining
away ( 123 ) .
in undirected models , however , inference is easy provided the latent variables do not have edges linking
such a restricted class of undirected models is ideal for layerwise pre - training because each layer will have
an easy inference procedure .
we start by describing an approximate learning algorithm for a restricted boltzmann machine ( rbm ) which
consists of a layer of stochastic binary visible units that represent binary input data connected to a layer of
stochastic binary hidden units that learn to model signicant non - independencies between the visible units ( 123 )
are undirected connections between visible and hidden units but no visible - visible or hidden - hidden connections .
an rbm is a type of markov random field ( mrf ) but differs from most mrfs in several ways : it has a bipartite
connectivity graph; it does not usually share weights between different units; and a subset of the variables are
unobserved , even during training .
an efcient learning procedure for rbms
a joint conguration , ( v , h ) of the visible and hidden units of an rbm has an energy given by :
e ( v , h ) = xivisible
where vi , hj are the binary states of visible unit i and hidden unit j , ai , bj are their biases and wij is the weight
between them .
the network assigns a probability to every possible pair of a visible and a hidden vector via this
energy function as in eqn .
( 123 ) and the probability that the network assigns to a visible vector , v , is given by
summing over all possible hidden vectors :
april 123 , 123
the derivative of the log probability of a training set with respect to a weight is surprisingly simple :
where n is the size of the training set and the angle brackets are used to denote expectations under the distribution
specied by the subscript that follows .
the simple derivative in eqn . ( 123 ) leads to a very simple learning rule for
performing stochastic steepest ascent in the log probability of the training data :
wij = ( <vihj>data <vihj>model )
where is a learning rate .
the absence of direct connections between hidden units in an rbm makes it is very easy to get an unbiased
sample of <vihj>data .
given a randomly selected training case , v , the binary state , hj , of each hidden unit , j , is
set to 123 with probability
and vihj is then an unbiased sample .
the absence of direct connections between visible units in an rbm makes
p ( hj = 123 | v ) = logistic ( bj +xi
it very easy to get an unbiased sample of the state of a visible unit , given a hidden vector
p ( vi = 123 | h ) = logistic ( ai +xj
getting an unbiased sample of < vihj >model , however , is much more difcult .
it can be done by starting at
any random state of the visible units and performing alternating gibbs sampling for a very long time .
alternating
gibbs sampling consists of updating all of the hidden units in parallel using eqn . ( 123 ) followed by updating all of
the visible units in parallel using eqn . ( 123 ) .
a much faster learning procedure called contrastive divergence ( cd ) was proposed in ( 123 ) .
this starts by
setting the states of the visible units to a training vector .
then the binary states of the hidden units are all computed
in parallel using eqn . ( 123 ) .
once binary states have been chosen for the hidden units , a reconstruction is produced
by setting each vi to 123 with a probability given by eqn . ( 123 ) .
finally , the states of the hidden units are updated
the change in a weight is then given by
wij = ( <vihj>data <vihj>recon )
a simplied version of the same learning rule that uses the states of individual units instead of pairwise products
is used for the biases .
contrastive divergence works well even though it is only crudely approximating the gradient of the log probability
of the training data ( 123 ) .
rbms learn better generative models if more steps of alternating gibbs sampling are used
before collecting the statistics for the second term in the learning rule , but for the purposes of pre - training feature
detectors , more alternations are generally of little value and all the results reviewed here were obtained using
cd123 which does a single full step of alternating gibbs sampling after the initial update of the hidden units .
to suppress noise in the learning , the real - valued probabilities rather than binary samples are generally used for the
april 123 , 123
reconstructions and the subsequent states of the hidden units , but it is important to use sampled binary values for the
rst computation of the hidden states because the sampling noise acts as a very effective regularizer that prevents
modeling real - valued data
real - valued data , such as mfccs , are more naturally modeled by linear variables with gaussian noise and the
rbm energy function can be modied to accommodate such variables , giving a gaussian - bernoulli rbm ( grbm ) :
e ( v , h ) = xivis
where i is the standard deviation of the gaussian noise for visible unit i .
the two conditional distributions required for cd123 learning are :
p ( hj|v ) = logistic bj +xi p ( vi|h ) = n ai + ixj
where n ( , 123 ) is a gaussian .
learning the standard deviations of a grbm is problematic for reasons described
in ( 123 ) , so for pre - training using cd123 , the data are normalized so that each coefcient has zero mean and unit variance , the standard deviations are set to 123 when computing p ( v|h ) , and no noise is added to the reconstructions .
this avoids the issue of deciding the right noise level .
stacking rbms to make a deep belief network
after training an rbm on the data , the inferred states of the hidden units can be used as data for training
another rbm that learns to model the signicant dependencies between the hidden units of the rst rbm
can be repeated as many times as desired to produce many layers of non - linear feature detectors that represent
progressively more complex statistical structure in the data .
the rbms in a stack can be combined in a surprising
way to produce a single , multi - layer generative model called a deep belief net ( dbn ) ( 123 ) .
even though each rbm is an undirected model , the dbn 123 formed by the whole stack is a hybrid generative model whose top two layers
are undirected ( they are the nal rbm in the stack ) but whose lower layers have top - down , directed connections
( see gure 123 ) .
to understand how rbms are composed into a dbn it is helpful to rewrite eqn . ( 123 ) and to make explicit the
dependence on w :
p ( v; w ) =xh
p ( h; w ) p ( v|h; w ) ,
123not to be confused with a dynamic bayesian net which is a type of directed model of temporal data that unfortunately has the same
april 123 , 123
the sequence of operations used to create a dbn with three hidden layers and to convert it to a pre - trained dbn - dnn .
first a
grbm is trained to model a window of frames of real - valued acoustic coefcients .
then the states of the binary hidden units of the grbm
are used as data for training an rbm .
this is repeated to create as many hidden layers as desired .
then the stack of rbms is converted
to a single generative model , a dbn , by replacing the undirected connections of the lower level rbms by top - down , directed connections .
finally , a pre - trained dbn - dnn is created by adding a softmax output layer that contains one unit for each possible state of each hmm .
the dbn - dnn is then discriminatively trained to predict the hmm state corresponding to the central frame of the input window in a forced
where p ( h; w ) is dened as in eqn . ( 123 ) but with the roles of the visible and hidden units reversed .
now it is
clear that the model can be improved by holding p ( v|h; w ) xed after training the rbm , but replacing the prior
over hidden vectors p ( h; w ) by a better prior , i . e .
a prior that is closer to the aggregated posterior over hidden
vectors that can be sampled by rst picking a training case and then inferring a hidden vector using eqn . ( 123 )
aggregated posterior is exactly what the next rbm in the stack is trained to model .
as shown in ( 123 ) , there is a series of variational bounds on the log probability of the training data , and furthermore ,
each time a new rbm is added to the stack , the variational bound on the new and deeper dbn is better than the
previous variational bound , provided the new rbm is initialized and learned in the right way .
while the existence
of a bound that keeps improving is mathematically reassuring , it does not answer the practical issue , addressed in
this review paper , of whether the learned feature detectors are useful for discrimination on a task that is unknown
while training the dbn .
nor does it guarantee that anything improves when we use efcient short - cuts such as
cd123 training of the rbms .
april 123 , 123
one very nice property of a dbn that distinguishes it from other multilayer , directed , non - linear generative
models , is that it is possible to infer the states of the layers of hidden units in a single forward pass .
this inference ,
which is used in deriving the variational bound , is not exactly correct but it is fairly accurate .
so after learning a
dbn by training a stack of rbms , we can jettison the whole probabilistic framework and simply use the generative
weights in the reverse direction as a way of initializing all the feature detecting layers of a deterministic feed - forward dnn .
we then just add a nal softmax layer and train the whole dnn discriminatively123
interfacing a dnn with an hmm
a dnn outputs
p ( hm m state|acousticinput ) .
but to compute a viterbi alignment or to run the forward - backward algorithm
within the hmm framework we require the likelihood p ( acousticinput|hm m state ) .
the posterior probabilities
that the dnn outputs can be converted into the scaled likelihood by dividing them by the frequencies of the
hmm - states in the forced alignment that is used for ne - tuning the dnn ( 123 ) .
all of the likelihoods produced in
this way are scaled by the same unknown factor of p ( acousticinput ) , but this has no effect on the alignment .
although this conversion appears to have little effect on some recognition tasks , it can be important for tasks
where training labels are highly unbalanced ( e . g . , with many frames of silences ) .
phonetic classification and recognition on timit
the timit dataset provides a simple and convenient way of testing new approaches to speech recognition .
the training set is small enough to make it feasible to try many variations of a new method and many existing
techniques have already been benchmarked on the core test set so it is easy to see if a new approach is promising
by comparing it with existing techniques that have been implemented by their proponents ( 123 ) .
experience has
shown that performance improvements on timit do not necessarily translate into performance improvements on
large vocabulary tasks with less controlled recording conditions and much more training data .
nevertheless , timit
provides a good starting point for developing a new approach , especially one that requires a challenging amount of
mohamed et .
( 123 ) showed that a dbn - dnn acoustic model outperformed the best published recognition
results on timit at about the same time as sainath et .
( 123 ) achieved a similar improvement on timit by
applying state - of - the - art techniques developed for large vocabulary recognition .
subsequent work combined the
two approaches by using state - of - the - art , discriminatively trained ( dt ) speaker - dependent features as input to the
dbn - dnn ( 123 ) , but this produced little further improvement , probably because the hidden layers of the dbn - dnn
were already doing quite a good job of progressively eliminating speaker differences ( 123 ) .
the dbn - dnns that worked best on the timit data formed the starting point for subsequent experiments
on much more challenging , large vocabulary tasks that were too computationally intensive to allow extensive
123unfortunately , a dnn that is pre - trained generatively as a dbn is often still called a dbn in the literature .
for clarity we call it a dbn - dnn .
april 123 , 123
comparisons among the reported speaker - independent phonetic recognition accuracy results on timit core test set with 123 sentences
augmented conditional random fields ( 123 )
randomly initialized recurrent neural nets ( 123 )
bayesian triphone gmm - hmm ( 123 )
monophone htms ( 123 )
heterogeneous classiers ( 123 )
monophone randomly initialized dnns ( 123 layers ) ( 123 )
monophone dbn - dnns ( 123 layers ) ( 123 )
monophone dbn - dnns with mmi training ( 123 )
triphone gmm - hmms discriminatively trained w / bmmi ( 123 )
monophone dbn - dnns on fbank ( 123 layers ) ( 123 )
monophone mcrbm - dbn - dnns on fbank ( 123 layers ) ( 123 )
monophone convolutional dnns on fbank ( 123 layers ) ( 123 )
exploration of variations in the architecture of the neural network , the representation of the acoustic input or the
for simplicity , all hidden layers always had the same size , but even with this constraint it was impossible to
train all possible combinations of number of hidden layers ( 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 ) , number of units per layer ( 123 ,
123 , 123 , 123 ) and number of frames of acoustic data in the input layer ( 123 , 123 , 123 , 123 , 123 , 123 ) .
fortunately ,
the performance of the networks on the timit core test set was fairly insensitive to the precise details of the
architecture and the results in ( 123 ) suggest that any combination of the numbers in boldface probably has an error
rate within about 123% of the very best combination .
this robustness is crucial for methods such as dbn - dnns
that have a lot of tuneable meta - parameters .
our consistent nding is that multiple hidden layers always worked
better than one hidden layer and , with multiple hidden layers , pre - training always improved the results on both the
development and test sets in the timit task .
details of the learning rates , stopping criteria , momentum , l123 weight
penalties and mini - batch size for both the pre - training and ne - tuning are given in ( 123 ) .
table i compares dbn - dnns with a variety of other methods on the timit core test set .
for each type of
dbn - dnn the architecture that performed best on the development set is reported .
all methods use mfccs as
inputs except for the three marked fbank that use log mel - scale lter - bank outputs .
pre - processing the waveform for deep neural networks
state - of - the - art asr systems do not use lter - bank coefcients as the input representation because they are
strongly correlated so modeling them well requires either full covariance gaussians or a huge number of diagonal
gaussians .
mfccs offer a more suitable alternative as their individual components are roughly independent so they
are much easier to model using a mixture of diagonal covariance gaussians .
dbn - dnns do not require uncorrelated
april 123 , 123
data and , on the timit database , the work reported in ( 123 ) showed that the best performing dbn - dnns trained
with lter - bank features had a phone error rate 123% lower than the best performing dbn - dnns trained with
mfccs ( see table i ) .
fine - tuning dbn - dnns to optimize mutual information
in the experiments using timit discussed above , the dnns were ne - tuned to optimize the per frame cross -
entropy between the target hmm state and the predictions .
the transition parameters and language model scores
were obtained from an hmm - like approach and were trained independently of the dnn weights .
however , it has
long been known that sequence classication criteria , which are more directly correlated with the overall word or
phone error rate , can be very helpful in improving recognition accuracy ( 123 ) , ( 123 ) and the benet of using such
sequence classication criteria with shallow neural networks has already been shown by ( 123 ) , ( 123 ) , ( 123 ) .
in the more
recent work reported in ( 123 ) , one popular type of sequence classication criterion , maximum mutual information
or mmi , proposed as early as 123 ( 123 ) , was successfully applied to learn dbn - dnn weights for the timit phone
recognition task .
mmi optimizes the conditional probability p ( l123 : t |v123 : t ) of the whole sequence of labels , l123 : t , with length t , given the whole visible feature utterance v123 : t , or equivalently the hidden feature sequence h123 : t extracted by the dnn :
p ( l123 : t |v123 : t ) = p ( l123 : t |h123 : t ) =
t=123 ijij ( lt123 , lt ) +pt
where the transition feature ij ( lt123 , lt ) takes on a value of one if lt123 = i and lt = j , and otherwise takes on a value of zero , where ij is the parameter associated with this transition feature , htd is the d - th dimension of
the hidden unit value at the t - th frame at the nal layer of the dnn , and where d is the number of units in the
nal hidden layer .
note the objective function of eqn . ( 123 ) derived from mutual information ( 123 ) is the same as
the conditional likelihood associated with a specialized linear - chain conditional random eld .
here , it is the top
most layer of the dnn below the softmax layer , not the raw speech coefcients of mfcc or plp , that provides
features to the conditional random eld .
to optimize the log conditional probability p ( ln
123 : t ) of the n - th utterance , we take the gradient over the
activation parameters kd , transition parameters ij , and the lower - layer weights of the dnn , wij , according to
t = k ) p ( ln
t = k|vn
t123 = i , ln
t = j ) p ( ln
t123 = i , ln
t = j|vn
t = k|vn
123 : t ) kd ) hn
note that the gradient log p ( ln 123 : t ) , vs
t = k ) p ( ln
t = k|vn
t ) in the frame - based training algorithm .
above can be viewed as back - propagating the error ( ln
t = k ) p ( ln
april 123 , 123
in implementing the above learning algorithm for a dbn - dnn , the dnn weights can rst be ne - tuned to
optimize the per frame cross entropy .
the transition parameters can be initialized from the combination of the
hmm transition matrices and the phone language model scores , and can be further optimized by tuning the
transition features while xing the dnn weights before the joint optimization .
using the joint optimization with
careful scheduling , we observe that the sequential mmi training can outperform the frame - level training by about
123% relative within the same system in the same laboratory .
convolutional dnns for phone classication and recognition
the previously cited work reported phone recognition results on the timit database .
in recognition
experiments , the input is the acoustic input for the whole utterance while the output is the spoken phonetic sequence .
a decoding process using a phone language model is used to produce this output sequence .
phonetic classication
is a different task where the acoustic input has already been labeled with the correct boundaries between different
phonetic units and the goal is to classify these phones conditioned on the given boundaries .
in ( 123 ) convolutional
dbn - dnns were introduced and successfully applied to various audio tasks including phone classication on the
timit database .
in this model , the rbm was made convolutional in time by sharing weights between hidden
units that detect the same feature at different times .
a max - pooling operation was then performed which takes the
maximal activation over a pool of adjacent hidden units that share the same weights but apply them at different
this yields some temporal invariance .
although convolutional models along the temporal dimension achieved good classication results ( 123 ) , applying
them to phone recognition is not straightforward .
this is because temporal variations in speech can be partially
handled by the dynamic programing procedure in the hmm component and those aspects of temporal variation that
cannot be adequately handled by the hmm can be addressed more explicitly and effectively by hidden trajectory
the work reported in ( 123 ) applied local convolutional lters with max - pooling to the frequency rather than
time dimension of the spectrogram .
sharing - weights and pooling over frequency was motivated by the shifts in
formant frequencies caused by speaker variations .
it provides some speaker invariance while also offering noise
robustness due to the band - limited nature of the lters .
( 123 ) only used weight - sharing and max - pooling across
nearby frequencies because , unlike features that occur at different positions in images , acoustic features occuring
at very different frequencies are very different .
a summary of the differences between dnns and gmms
here we summarize the main differences between the dnns and gmms used in the timit experiments described
so far in this paper .
first , one major element of the dbn - dnn , the rbm which serves as the building block for
pre - training , is an instance of product of experts ( 123 ) , in contrast to mixture models that are a sum of experts .
mixture models with a large number of components use their parameters inefciently because each parameter
123product models have only very recently been explored in speech processing; e . g . , ( 123 ) .
april 123 , 123
only applies to a very small fraction of the data whereas each parameter of a product model is constrained by a
large fraction of the data .
second , while both dnns and gmms are nonlinear models , the nature of the nonlinearity
is very different .
third , dnns are good at exploiting multiple frames of input coefcients whereas gmms that
use diagonal covariance matrices benet much less from multiple frames because they require decorrelated inputs .
finally , dnns are learned using stochastic gradient descent , while gmms are learned using the em algorithm or
its extensions ( 123 ) which makes gmm learning much easier to parallelize on a cluster machine .
comparing dbn - dnns with gmms for large - vocabulary speech recognition
the success of dbn - dnns on timit tasks starting in 123 motivated more ambitious experiments with much
larger vocabularies and more varied speaking styles .
in this section , we review experiments by three different speech
groups on ve different benchmark tasks for large vocabulary speech recognition .
to make dbn - dnns work really
well on large vocabulary tasks it is important to replace the monophone hmms used for timit ( and also for
early neural network / hmm hybrid systems ) with triphone hmms that have many thousands of tied states ( 123 ) .
predicting these context - dependent states provides several advantages over monophone targets .
they supply more
bits of information per frame in the labels .
they also make it possible to use a more powerful triphone hmm
decoder and to exploit the sensible classes discovered by the decision tree clustering that is used to tie the states of
different triphone hmms .
using context - dependent hmm states , it is possible to outperform state - of - the - art bmmi
trained gmm - hmm systems with a two - hidden - layer neural network without using any pre - training ( 123 ) , though
using more hidden layers and pre - training works even better .
bing - voice - search speech recognition task
the rst successful use of acoustic models based on dbn - dnns for a large vocabulary task used data collected
from the bing mobile voice search application ( bmvs ) .
the task used 123 hours of training data with a high degree
of acoustic variability caused by noise , music , side - speech , accents , sloppy pronunciation , hesitation , repetition ,
interruptions , and mobile phone differences .
the results reported in ( 123 ) demonstrated that the best dnn - hmm
acoustic model trained with context - dependent states as targets achieved a sentence accuracy of 123% on the test
set , compared with 123% for a strong , mpe trained gmm - hmm baseline .
the dbn - dnn used in the experiments was based on one of the dbn - dnns that worked well for the timit
it used ve pre - trained layers of hidden units with 123 , 123 units per layer and was trained to classify the central
frame of an 123 frame acoustic context window using 123 possible context - dependent states as targets .
in addition
to demonstrating that a dbn - dnn could provide gains on a large vocabulary task , several other important issues
were explicitly investigated in ( 123 ) .
it was found that using tied triphone context - dependent state targets was crucial
and clearly superior to using monophone state targets , even when the latter were derived from the same forced
alignment with the same baseline .
it was also conrmed that the lower the error rate of the system used during forced
alignment to generate frame level training labels for the neural net , the lower the error rate of the nal neural - net
based system .
this effect was consistent across all the alignments they tried , including monophone alignments ,
april 123 , 123
alignments from maximum likelihood trained gmm - hmm systems , and alignments from discriminatively trained
further work after that of ( 123 ) extended the dnn - hmm acoustic model from 123 hours of training data to 123
hours , and explored the respective roles of pre - training and ne - tuning the dbn - dnn ( 123 ) .
as expected , pre - training
is helpful in training the dbn - dnn because it initializes the dbn - dnn weights to a point in the weight - space
from which ne - tuning is highly effective .
however , a moderate increase of the amount of unlabeled pre - training
data has an insignicant effect on the nal recognition results ( 123% to 123% ) , as long as the original training set
is fairly large .
by contrast , the same amount of additional labeled ne - tuning training data signicantly improves
the performance of the dnn - hmms ( accuracy from 123% to 123% ) .
switchboard speech recognition task
the dnn - hmm training recipe developed for the bing voice search data was applied unaltered to the switchboard
speech recognition task ( 123 ) to conrm the suitability of dnn - hmm acoustic models for large vocabulary tasks .
before this work , dnn - hmm acoustic models had only been trained with up to 123 hours of data ( 123 ) and hundreds
of tied triphone states as targets , whereas this work used over 123 hours of training data and thousands of tied
triphone states as targets .
furthermore , switchboard is a publicly available speech - to - text transcription benchmark
task that allows much more rigorous comparisons among techniques .
the baseline gmm - hmm system on the switchboard task was trained using the standard 123 - hour switchboard - i
training set .
123 - dimensional plp features with windowed mean - variance normalization were concatenated with up
to third - order derivatives and reduced to 123 dimensions by hdla , a form of linear discriminant analysis ( lda ) .
the speaker - independent crossword triphones used the common left - to - right 123 - state topology and shared 123 tied
the baseline gmm - hmm system had a mixture of 123 gaussians per ( tied ) hmm state that were rst trained
generatively to optimize a maximum likelihood ( ml ) criterion and then rened discriminatively to optimize a
boosted maximum - mutual - information ( bmmi ) criterion .
a seven - hidden - layer dbn - dnn with 123 units in each
layer and full connectivity between adjacent layers replaced the gmm in the acoustic model .
the trigram language
model , used for both systems , was trained on the training transcripts of the 123 - hours of the fisher corpus and
interpolated with a trigram model trained on written text .
the primary test set is the fsh portion of the 123 - hour spring 123 nist rich transcription set ( rt123s )
ii extracted from the literature shows a summary of the core results .
using a dnn reduced the word - error rate
( wer ) from the 123% of the baseline gmm - hmm ( trained with bmmi ) to 123% a 123% relative reduction .
the dnn - hmm system trained on 123 hours performs as well as combining several speaker - adaptive , multi - pass
systems which use vocal tract length normalization ( vtln ) and nearly seven times as much acoustic training
data ( the 123h fisher corpus ) ( 123% , last row ) .
detailed experiments ( 123 ) on the switchboard task conrmed that the remarkable accuracy gains from the dnn -
hmm acoustic model are due to the direct modeling of tied triphone states using the dbn - dnn , the effective
april 123 , 123
comparing ve different dbn - dnn acoustic models with two strong gmm - hmm baseline systems that are discriminatively trained ( dt ) .
speaker - independent ( si ) training on 123 hours of data and single - pass decoding were used for all models except for the gmm - hmm system
shown on the last row which used speaker adaptive ( sa ) training with 123 hours of data and multi - pass decoding including hypotheses
combination .
in the table , 123 mix means a mixture of 123 gaussians per hmm state and 123 nz means 123 million , non - zero weights .
word - error rates ( wer ) in % are shown for two separate test sets , hub123 - swb and rt123s - fsh .
gmm , 123 mix dt 123h si
nn 123 hidden - layer123 units
+ 123 neighboring frames
dbn - dnn 123 hidden layers123 units 123
+ updated state alignment
gmm 123 mix dt 123h sa
exploitation of neighboring frames by the dbn - dnn , and the strong modeling power of deeper networks , as was
discovered in the bing voice search task ( 123 ) , ( 123 ) .
pre - training the dbn - dnn leads to the best results but it is
not critical : for this task , it provides an absolute wer reduction of less than 123% and this gain is even smaller
when using ve or more hidden layers .
for under - resourced languages that have smaller amounts of labeled data ,
pre - training is likely to be far more helpful .
further study ( 123 ) suggests that feature - engineering techniques such as hlda and vtln , commonly used in
gmm - hmms , are more helpful for shallow neural nets than for dbn - dnns , presumably because dbn - dnns are
able to learn appropriate features in their lower layers .
google voice input speech recognition task
google voice input transcribes voice search queries , short messages , emails and user actions from mobile devices .
this is a large vocabulary task that uses a language model designed for a mixture of search queries and dictation .
googles full - blown model for this task , which was built from a very large corpus , uses a speaker - independent
gmm - hmm model composed of context dependent cross - word triphone hmms that have a left - to - right , three -
state topology .
this model has a total of 123 senone states and uses as acoustic input plp features that have been
transformed by lda .
semi - tied covariances ( stc ) are used in the gmms to model the lda transformed features
and bmmi ( 123 ) was used to train the model discriminatively .
jaitly et .
( 123 ) used this model to obtain approximately 123 , 123 hours of aligned training data for a dbn - dnn
acoustic model that predicts the 123 , 123 hmm state posteriors from the acoustic input .
the dbn - dnn was loosely
based on one of the dbn - dnns used for the timit task .
it had four hidden layers with 123 , 123 fully connected
units per layer and a nal softmax layer with 123 , 123 alternative states .
its input was 123 contiguous frames of 123
log lter - bank outputs with no temporal derivatives .
each dbn - dnn layer was pre - trained for one epoch as an
rbm and then the resulting dnn was discriminatively ne - tuned for one epoch .
weights with magnitudes below
april 123 , 123
a threshold were then permanently set to zero before a further quarter epoch of training .
one third of the weights
in the nal network were zero .
in addition to the dbn - dnn training , sequence level discriminative ne - tuning
of the neural network was performed using mmi , similar to the method proposed in ( 123 ) .
model combination
was then used to combine results from the gmm - hmm system with the dnn - hmm hybrid , using the scarf
framework ( 123 ) .
viterbi decoding was done using the google system ( 123 ) with modications to compute the scaled
log likelihoods from the estimates of the posterior probabilities and the state priors .
unlike the other systems , it was
observed that for voice input it was essential to smooth the estimated priors for good performance .
this smoothing
of the priors was performed by rescaling the log priors with a multiplier that was chosen by using a grid search to
nd a joint optimum of the language model weight , the word insertion penalty and the smoothing factor .
on a test set of anonymized utterances from the live voice input system , the dbn - dnn - based system achieved
a word error rate of 123% a 123% relative reduction compared to the best gmm - based system for this task .
mmi sequence discriminative training gave an error rate of 123% and model combination with the gmm system
youtube speech recognition task
in this task , the goal is to transcribe youtube data .
unlike the mobile voice input applications described above ,
this application does not have a strong language model to constrain the interpretation of the acoustic information
so good discrimination requires an accurate acoustic model .
googles full - blown baseline , built with a much larger training set , was used to create approximately 123 hours
of aligned training data .
this was used to create a new baseline system for which the input was 123 frames of mfccs
that were transformed by lda .
speaker adaptive training was performed , and decision tree clustering was used to
obtain 123 , 123 triphone states .
semi - tied covariances were used in the gmms to model the features .
the acoustic
models were further improved with bmmi .
during decoding , feature space maximum likelihood linear regression
( fmllr ) and maximum likelihood linear regression ( mllr ) transforms were applied .
the acoustic data used for training the dbn - dnn acoustic model were the fmllr transformed features
large number of hmm states added signicantly to the computational burden , since most of the computation is
done at the output layer .
to reduce this burden , the dnn used only four hidden layers with 123 units in the rst
hidden layer and only 123 in each of the layers above .
about ten epochs of training were performed on this data before sequence level training and model combination .
the dbn - dnn gave an absolute improvement of 123% over the baseline systems wer of 123% .
sequence level
ne - tuning of the dbn - dnn further improved results by 123% and model combination produced an additional gain
english - broadcast - news speech recognition task
dnns have also been successfully applied to an english broadcast news task .
since a gmm - hmm baseline creates
the initial training labels for the dnn , it is important to have a good baseline system .
all gmm - hmm systems
april 123 , 123
a comparison of the percentage word error rates using dnn - hmms and gmm - hmms on ve different large vocabulary tasks .
with same data
with more data
switchboard ( test set 123 )
switchboard ( test set 123 )
english broadcast news
bing voice search
( sentence error rates )
google voice input
123 ( 123 hrs )
123 ( 123 hrs )
created at ibm use the following recipe to produce a state - of - the - art baseline system .
first speaker - independent
( si ) features are created , followed by speaker - adaptively trained ( sat ) and discriminatively trained ( dt ) features .
specically , given initial plp features , a set of si features are created using linear discriminative analysis ( lda ) .
further processing of lda features is performed to create sat features using vocal tract length normalization
( vtln ) followed by feature space maximum likelihood linear regression ( fmllr ) .
finally , feature and model -
space discriminative training is applied using the the boosted maximum mutual information ( bmmi ) or minimum
phone error ( mpe ) criterion .
using alignments from a baseline system , ( 123 ) trained a dbn - dnn acoustic model on 123 hours of data from the
123 and 123 english broadcast news speech corpora ( 123 ) .
the dbn - dnn was trained with the best - performing
lvcsr features , namely sat + dt features .
the dbn - dnn architecture consisted of 123 hidden layers with 123 , 123
units per layer and a nal softmax layer of 123 , 123 context - dependent states .
the sat+dt feature input into the rst
layer used a context of 123 frames .
pre - training was performed following a recipe similar to ( 123 ) .
two phases of ne - tuning were performed .
during the rst phase , the cross - entropy loss was used .
for cross -
entropy training , after each iteration through the whole training set , loss is measured on a held - out set and the
learning rate is annealed ( i . e .
reduced ) by a factor of 123 if the held - out loss has grown or improves by less than
a threshold of 123% from the previous iteration .
once the learning rate has been annealed ve times , the rst
phase of ne - tuning stops .
after weights are learned via cross - entropy , these weights are used as a starting point
for a second phase of ne - tuning using a sequence criterion ( 123 ) which utilizes the mpe objective function , a
discriminative objective function similar to mmi ( 123 ) but which takes into account phoneme error rate .
a strong sat+dt gmm - hmm baseline system , which consisted of 123 , 123 context - dependent states and 123 , 123
gaussians , gave a wer of 123% on the ears dev - 123f set , whereas the dnn - hmm system gave 123% ( 123 ) .
summary of the main results for dbn - dnn acoustic models on lvcsr tasks
table iii summarizes the acoustic modeling results described above .
it shows that dnn - hmms consistently
outperform gmm - hmms that are trained on the same amount of data , sometimes by a large margin .
for some
tasks , dnn - hmms also outperform gmm - hmms that are trained on much more data .
april 123 , 123
speeding up dnns at recognition time
state pruning or gaussian selection methods can be used to make gmm - hmm systems computationally efcient
at recognition time .
a dnn , however , uses virtually all its parameters at every frame to compute state likelihoods ,
making it potentially much slower than a gmm with a comparable number of parameters .
fortunately , the time that
a dnn - hmm system requires to recognize 123s of speech can be reduced from 123s to 123ms , without decreasing
recognition accuracy , by quantizing the weights down to 123 bits and using the very fast simd primitives for xed -
point computation that are provided by a modern x123 cpu ( 123 ) .
alternatively , it can be reduced to 123ms by using
alternative pre - training methods for dnns
pre - training dnns as generative models led to better recognition results on timit and subsequently on a variety
of lvcsr tasks .
once it was shown that dbn - dnns could learn good acoustic models , further research revealed
that they could be trained in many different ways .
it is possible to learn a dnn by starting with a shallow neural
net with a single hidden layer .
once this net has been trained discriminatively , a second hidden layer is interposed
between the rst hidden layer and the softmax output units and the whole network is again discriminatively trained .
this can be continued until the desired number of hidden layers is reached , after which full backpropagation
ne - tuning is applied .
this type of discriminative pre - training works well in practice , approaching the accuracy achieved by generative
dbn pre - training and further improvement can be achieved by stopping the discriminative pre - training after a single
epoch instead of multiple epochs as reported in ( 123 ) .
discriminative pre - training has also been found effective
for the architectures called deep convex network ( 123 ) and deep stacking network ( 123 ) , where pre - training is
accomplished by convex optimization involving no generative models .
purely discriminative training of the whole dnn from random initial weights works much better than had been
thought , provided the scales of the initial weights are set carefully , a large amount of labeled training data is available ,
and mini - batch sizes over training epochs are set appropriately ( 123 ) , ( 123 ) .
nevertheless , generative pre - training still
improves test performance , sometimes by a signicant amount .
layer - by - layer generative pre - training was originally done using rbms , but various types of autoencoder with
one hidden layer can also be used ( see gure 123 ) .
on vision tasks , performance similar to rbms can be achieved
by pre - training with denoising autoencoders ( 123 ) that are regularized by setting a subset of the inputs to zero
or contractive autoencoders ( 123 ) that are regularized by penalizing the gradient of the activities of the hidden
units with respect to the inputs .
for speech recognition , improved performance was achieved on both timit and
broadcast news tasks by pre - training with a type of autoencoder that tries to nd sparse codes ( 123 ) .
alternative ne - tuning methods for dnns
very large gmm acoustic models are trained by making use of the parallelism available in compute clusters .
it is more difcult to use the parallelism of cluster systems effectively when training dbn - dnns .
at present , the
april 123 , 123
an autoencoder is trained to minimize the discrepancy between the input vector and its reconstruction of the input vector on its output
if the code units and the output units are both linear and the discrepancy is the squared reconstruction error , an autoencoder nds the
same solution as principal components analysis ( up to a rotation of the components ) .
if the output units and the code units are logistic , an
autoencoder is quite similar to an rbm that is trained using contrastive divergence , but it does not work as well for pre - training dnns unless it
is strongly regularized in an appropriate way .
if extra hidden layers are added before and / or after the code layer , an autoencoder can compress
data much better than principal components analysis ( 123 ) .
most effective parallelization method is to parallelize the matrix operations using a gpu .
this gives a speed - up of
between one and two orders of magnitude , but the ne - tuning stage remains a serious bottleneck and more effective
ways of parallelizing training are needed .
some recent attempts are described in ( 123 ) , ( 123 ) .
most dbn - dnn acoustic models are ne - tuned by applying stochastic gradient descent with momentum to
small mini - batches of training cases .
more sophisticated optimization methods that can be used on larger mini -
batches include non - linear conjugate - gradient ( 123 ) , lbfgs ( 123 ) and hessian free methods adapted to work for
deep neural networks ( 123 ) .
however , the ne - tuning of dnn acoustic models is typically stopped early to prevent
overtting and it is not clear that the more sophisticated methods are worthwhile for such incomplete optimization .
other ways of using deep neural networks for speech recognition
the previous section reviewed experiments in which gmms were replaced by dbn - dnn acoustic models to
give hybrid dnn - hmm systems in which the posterior probabilities over hmm states produced by the dbn - dnn
replace the gmm output model .
in this section , we describe two other ways of using dnns for speech recognition .
using dbn - dnns to provide input features for gmm - hmm systems
here we describe a class of methods where neural networks are used to provide the feature vectors that the
gmm in a gmm - hmm system is trained to model .
the most common approach to extracting these feature vectors
is to discriminatively train a randomly initialized neural net with a narrow bottleneck middle layer and to use the
activations of the bottleneck hidden units as features .
for a summary of such methods , commonly known as the
tandem approach , see ( 123 ) , ( 123 ) .
april 123 , 123
recently , ( 123 ) investigated a less direct way of producing feature vectors for the gmm .
first , a dnn with
six hidden layers of 123 units each was trained to achieve good classication accuracy for the 123 hmm states
represented in its softmax output layer .
this dnn did not have a bottleneck layer and it was therefore able to
classify better than a dnn with a bottleneck .
then the 123 logits computed by the dnn as input to its softmax
layer were compressed down to 123 values using a 123 - 123 - 123 - 123 autoencoder .
this method of producing feature
vectors is called ae - bn because the bottleneck is in the autoencoder rather than in the dnn that is trained to
classify hmm states .
bottleneck feature experiments were conducted on 123 hours and 123 hours of data from the 123 and 123
english broadcast news speech collections and english broadcast audio from tdt - 123
the baseline gmm - hmm
acoustic model trained on 123 hours was the same acoustic model described in section iv - e .
the acoustic model
trained on 123 hours had 123 , 123 states and 123 , 123 gaussians .
again , the standard ibm lvcsr recipe described
in section iv - e was used to create a set of speaker - adapted , discriminatively trained features and models .
all dbn - dnns used sat features as input .
they were pre - trained as dbns and then discriminatively ne - tuned
to predict target values for 123 hmm states that were obtained by clustering the context - dependent states in the
baseline gmm - hmm system .
as in section iv - e , the dbn - dnn was trained using the cross - entropy criterion ,
followed by the sequence criterion with the same annealing and stopping rules .
after the training of the rst dbn - dnn terminated , the nal set of weights was used for generating the 123
logits at the output layer .
a second 123 - 123 - 123 - 123 dbn - dnn was then trained as an auto - encoder to reduce the
dimensionality of the output logits .
the gmm - hmm system that used the feature vectors produced by the ae - bn
was trained using feature and model space discriminative training .
both pre - training and the use of deeper networks
made the ae - bn features work better for recognition .
to fairly compare the performance of the system that used
the ae - bn features with the baseline gmm - hmm system , the acoustic model of the ae - bn features was trained
with the same number of states and gaussians as the baseline system .
table iv shows the results of the ae - bn and baseline systems on both 123 and 123 hours , for different steps in
the lvcsr recipe described in section iv - e .
on 123 hours , the ae - bn system offers a 123% absolute improvement
over the baseline gmm - hmm system which is the same improvement as the dbn - dnn , while on 123 hours the
ae - bn system provides a 123% improvement over the baseline .
the 123% wer is the best result to date on the
dev - 123f task , using an acoustic model trained on 123 hours of data .
finally , the complementarity of the ae - bn
and baseline methods is explored by performing model combination on both the 123 and 123 hour tasks .
table iv
shows that model - combination provides an additional 123% absolute improvement over individual systems on the
123 hour task , and a 123% absolute improvement over the individual systems on the 123 hour task , conrming the
complementarity of the ae - bn and baseline systems .
instead of replacing the coefcients usually modeled by gmms , neural networks can also be used to provide
additional features for the gmm to model ( 123 ) , ( 123 ) , ( 123 ) .
dbn - dnns have recently been shown to be very effective
in such tandem systems .
on the aurora123 test set , pre - training decreased word error rates by more than one third
for speech with signal - to - noise levels of 123db or more , though this effect almost disappeared for very high noise
april 123 , 123
wer in % on english broadcast news
using dnns to estimate articulatory features for detection - based speech recognition
a recent study ( 123 ) demonstrated the effectiveness of dbn - dnns for detecting sub - phonetic speech attributes
( also known as phonological or articulatory features ( 123 ) ) in the widely used wall street journal speech database
( 123k - wsj123 ) .
123 mfccs plus rst and second temporal derivatives were used as the short - time spectral representation
of the speech signal .
the phone labels were derived from the forced alignments generated using a gmm - hmm
system trained with maximum likelihood , and that hmm system had 123 tied - state , cross - word tri - phones , each
modeled by a mixture of 123 gaussians .
the attribute labels were generated by mapping phone labels to attributes ,
simplifying the overlapping characteristics of the articulatory features .
the 123 attributes used in the recent work ,
as reported in ( 123 ) , are a subset of the articulatory features explored in ( 123 ) , ( 123 ) .
dbn - dnns achieved less than half the error rate of shallow neural nets with a single hidden layer
architectures with 123 to 123 hidden layers and up to 123 hidden units per layer were explored , producing greater
than 123% frame - level accuracy for all 123 attributes tested in the full dnn system .
on the same data , dbn - dnns
also achieved a very high per frame phone classication accuracy of 123% .
this level of accuracy for detecting
sub - phonetic fundamental speech units may allow a new family of exible speech recognition and understanding
systems that make use of phonological features in the full detection - based framework discussed in ( 123 ) .
summary and future directions
when gmms were rst used for acoustic modeling they were trained as generative models using the em algorithm
and it was some time before researchers showed that signicant gains could be achieved by a subsequent stage of
discriminative training using an objective function more closely related to the ultimate goal of an asr system ( 123 ) ,
when neural nets were rst used they were trained discriminatively and it was only recently that researchers
showed that signicant gains could be achieved by adding an initial stage of generative pre - training that completely
ignores the ultimate goal of the system .
the pre - training is much more helpful in deep neural nets than in shallow
ones , especially when limited amounts of labeled training data are available .
it reduces overtting and it also reduces
the time required for discriminative ne - tuning with backpropagation which was one of the main impediments to
using dnns when neural networks were rst used in place of gmms in the 123s .
the successes achieved using
april 123 , 123
pre - training led to a resurgence of interest in dnns for acoustic modeling .
retrospectively , it is now clear that
most of the gain comes from using deep neural networks to exploit information in neighboring frames and from
modeling tied context - dependent states .
pre - training is helpful in reducing overtting , and it does reduce the time
taken for ne - tuning , but similar reductions in training time can be achieved with less effort by careful choice of
the scales of the initial random weights in each layer .
the rst method to be used for pre - training dnns was to learn a stack of rbms , one per hidden layer of the
an rbm is an undirected generative model that uses binary latent variables , but training it by maximum
likelihood is expensive so a much faster , approximate method called contrastive divergence is used .
this method has
strong similarities to training an autoencoder network ( a non - linear version of pca ) that converts each datapoint
into a code from which it is easy to approximately reconstruct the datapoint .
subsequent research showed that
autoencoder networks with one layer of logistic hidden units also work well for pre - training , especially if they are
regularized by adding noise to the inputs or by constraining the codes to be insensitive to small changes in the
rbms do not require such regularization because the bernoulli noise introduced by using stochastic binary
hidden units acts as a very strong regularizer .
we have described how three major speech research groups achieved signicant improvements in a variety
of state - of - the - art asr systems by replacing gmms with dnns , and we believe that there is the potential for
considerable further improvement .
there is no reason to believe that we are currently using the optimal types of
hidden units or the optimal network architectures and it is highly likely that both the pre - training and ne - tuning
algorithms can be modied to reduce the amount of overtting and the amount of computation .
we therefore expect
that the performance gap between acoustic models that use dnns and ones that use gmms will continue to increase
for some time .
currently , the biggest disadvantage of dnns compared with gmms is that it is much harder to make good use of
large cluster machines to train them on massive datasets .
this is offset by the fact that dnns make more efcient
use of data so they do not require as much data to achieve the same performance , but better ways of parallelizing
the ne - tuning of dnns is still a major issue .
