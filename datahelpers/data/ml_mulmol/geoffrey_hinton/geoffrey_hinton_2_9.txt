discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning .
one fruitful approach is to build a parame - terised stochastic generative model , independent draws from which are likely to produce the patterns .
for all but the simplest generative models , each pat - tern can be generated in exponentially many ways .
it is thus intractable to ad - just the parameters to maximize the probability of the observed patterns , we describe a way of nessing this combinatorial explosion by maximising an eas - ily computed lower bound on the probability of the observations .
our method can be viewed as a form of hierarchical self - supervised learning that may re - late to the function of bottom - up and top - down cortical processing pathways .
following helmholtz , we view the human perceptual system as a statistical infer - ence engine whose function is to infer the probable causes of sensory input .
we show that a device of this kind can learn how to perform these inferences without requiring a teacher to label each sensory input vector with its underlying causes .
a recognition model is used to infer a probability distribution over the underly - ing causes from the sensory input , and a separate generative model , which is also learned , is used to train the recognition model ( zemel , 123; hinton & zemel , 123; zemel & hinton , 123 ) .
as an example of the generative models in which we are interested , consider the shift patterns in figure 123 , which are on four 123 123 rows of binary pixels .
these were produced by a two - level stochastic hierarchical generative process described in the gure caption .
the task of learning is to take a set of examples generated by
such a process and induce the model .
note that underlying any pattern there are multiple simultaneous causes .
we call each possible set of causes an explanation of the pattern .
for this particular example , it is possible to infer a unique set of causes for most patterns , but this need not always be the case .
for general generative models , the causes need not be immediately evident from the surface form of patterns .
worse still , there can be an exponential number of possible explanations underlying each pattern .
the computational cost of consid - ering all of these explanations makes standard maximum likelihood approaches such as the expectationmaximisation algorithm ( dempster et al , 123 ) intractable .
in this paper we describe a tractable approximation to maximum likelihood learn - ing implemented in a layered hierarchical connectionist network .
123 the recognition distribution
the log probability of generating a particular example , d , from a model with pa - rameters ( cid : 123 ) is
g dj ( cid : 123 ) = g x ( cid : 123 )
where the ( cid : 123 ) are explanations .
if we view the alternative explanations of an ex - ample as alternative congurations of a physical system there is a precise analogy with statistical physics .
we dene the energy of explanation ( cid : 123 ) to be
e ( cid : 123 ) ( cid : 123 ) ; d = g ( cid : 123 ) j ( cid : 123 ) dj ( cid : 123 ) ; ( cid : 123 )
the posterior probability of an explanation given d and ( cid : 123 ) is related to its energy by the equilibrium or boltzmann distribution , which at a temperature of 123 gives :
( cid : 123 ) ( cid : 123 ) ; d =
123 ( cid : 123 ) 123j ( cid : 123 ) dj ( cid : 123 ) 123; ( cid : 123 )
where indices ( cid : 123 ) and d in the last expression have been omitted for clarity .
using e ( cid : 123 ) and ( cid : 123 ) equation 123 can be rewritten in terms of the helmholtz free energy , which is the difference between the expected energy of an explanation and the entropy of the probability distribution across explanations .
g dj ( cid : 123 ) = " x ( cid : 123 )
( cid : 123 ) g ( cid : 123 ) ! #
so far , we have not gained anything in terms of computational tractability because we still need to compute expectations under the posterior distribution which , in
general , has exponentially many terms and cannot be factored into a product of simpler distributions .
however , we know ( thompson , 123 ) that any probability distribution over the explanations will have at least as high a free energy as the boltzmann distribution ( equation 123 ) .
therefore we can restrict ourselves to some class of tractable distributions and still have a lower bound on the log probability of the data .
instead of using the true posterior probability distribution , , for averaging over explanations , we use a more convenient probability distribution , .
the log probability of the data can then be written as
g dj ( cid : 123 ) = ( cid : 123 ) ( cid : 123 ) e ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) g ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) g ( ( cid : 123 ) = ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) g ( ( cid : 123 ) = ( cid : 123 ) )
f d; ( cid : 123 ) ;
where f is the free energy based on the incorrect or non - equilibrium posterior .
making the dependencies explicit , the last term in equation 123 is the kullback - leibler divergence between d and the posterior distribution , ( cid : 123 ) ; d ( kullback , 123 ) .
this term cannot be negative , so by ignoring it we get a lower bound on the log probability of the data given the model .
in our work , distribution is produced by a separate recognition model that has its own parameters , ( cid : 123 ) .
these parameters are optimized at the same time as the parameters of the generative model , ( cid : 123 ) , to maximise the overall t function f d; ( cid : 123 ) ; ( cid : 123 ) = f d; ( cid : 123 ) ; ( cid : 123 ) .
figure 123 shows graphically the nature of the approx - imation we are making and the relationship between our procedure and the em algorithm .
from equation 123 , maximising f is equivalent to maximising the log probability of the data minus the kullback - leibler divergence , showing that this divergence acts like a penalty on the traditional log probability .
the recognition model is thus encouraged to be a good approximation to the true posterior dis - tribution .
however , the same penalty also encourages the generative model to change so that the true posterior distributions will be close to distributions that can be represented by the recognition model .
123 the deterministic helmholtz machine
a helmholtz machine ( gure 123 ) is a simple implementation of these principles .
it is a connectionist system with multiple layers of neuron - like binary stochastic pro - cessing units connected hierarchically by two sets of weights .
top - down connec - tions ( cid : 123 ) implement the generative model .
bottom - up connections ( cid : 123 ) implement the
the key simplifying assumption is that the recognition distribution for a particular example d , ( cid : 123 ) ; d , is factorial ( separable ) in each layer .
if there are h stochastic
binary units in a layer , the portion of the distribution ( cid : 123 ) ; d due to that layer is determined by 123h 123 probabilities .
however , ( cid : 123 ) ; d makes the assumption that the actual activity of any one unit in layer is independent of the activities of all the other units in that layer , given the activities of all the units in the lower layer , 123 , so the recognition model needs only specify h probabilities rather than 123h 123
the independence assumption allows f d; ( cid : 123 ) ; ( cid : 123 ) to be evaluated efciently , but this computational tractability is bought at a price , since the true posterior is unlikely to be factorial : the log probability of the data will be underestimated by an amount equal to the kullback - leibler divergence between the true posterior and the recognition distribution .
the generative model is taken to be factorial in the same way , although one should note that factorial generative models rarely have recognition distributions that are themselves exactly factorial .
recognition for input example d entails using the bottom - up connections ( cid : 123 ) to de - j = 123
the termine the probability recognition model is inherently stochastic these probabilities are functions of the 123; 123 activities 123
j ( cid : 123 ) ; d that the jth unit in layer has activity
of the units in layer 123
we use :
123 = ( cid : 123 ) xi
where ( cid : 123 ) x = 123=123 exx is the conventional sigmoid function , and 123 is the vector of activities of the units in layer 123
all units have recognition biases as one element of the sums , all the activities at layer are calculated after all the activities at layer 123 , and 123 i are the activities of the input units .
it is essential that there are no feedback connections in the recognition model .
in the terms of the previous section , ( cid : 123 ) is a complete assignment of j for all the units in all the layers other than the input layer ( for which = 123 ) .
the multi - plicative contributions to the probability of choosing that assignment using the recognition weights are
j for units that are on and 123
j for units that are off :
( cid : 123 ) ( cid : 123 ) ; d = y>123yj ( cid : 123 )
the helmholtz free energy f depends on the generative model through e ( cid : 123 ) ( cid : 123 ) ; d in equation 123
the top - down connections ( cid : 123 ) use the activities 123 of the units in layer 123 to determine the factorial generative probabilities j ( cid : 123 ) ; 123 over the activities of the units in layer .
the obvious rule to use is the sigmoid :
123 = ( cid : 123 ) xi
including a generative bias ( which is the only contribution to units in the topmost layer ) .
unfortunately this rule did not work well in practice for the sorts of inputs we tried .
appendix a discusses the more complicated method that we actually used to determine j ( cid : 123 ) ; 123
given this , the overall generative probability of ( cid : 123 ) is :
( cid : 123 ) j ( cid : 123 ) = y>123yj ( cid : 123 )
we extend the factorial assumption to the input layer = 123
the activities layer 123 determine the probabilities 123
123 of the activities in the input layer
dj ( cid : 123 ) ; ( cid : 123 ) = yj ( cid : 123 ) 123
j ( cid : 123 ) 123 123
combining equations 123 , 123 and 123 , and omitting dependencies for clarity ,
e ( cid : 123 ) ( cid : 123 ) ; d = g ( cid : 123 ) j ( cid : 123 ) dj ( cid : 123 ) ; ( cid : 123 )
putting together the two components of f , an unbiased estimate of the value of f d; ( cid : 123 ) ; ( cid : 123 ) based on an explanation ( cid : 123 ) drawn from ( cid : 123 ) is :
f ( cid : 123 ) d; ( cid : 123 ) ; ( cid : 123 ) = e ( cid : 123 ) g ( cid : 123 )
= x xj
one could perform stochastic gradient ascent in the negative free energy across all
the data f ( cid : 123 ) ; ( cid : 123 ) = d f d; ( cid : 123 ) ; ( cid : 123 ) using equation 123 and a form of reinforce
algorithm ( barto & anandan , 123; williams , 123; dayan et al , in preparation ) .
however , for the simulations in this paper , we made a number of mean - eld in - spired approximations , in that we replaced the stochastic binary activities their mean values under the recognition model
we took :
123 = ( cid : 123 ) xi
j which we discuss in appendix a , and we we made a similar approximation for then averaged the expression in equation 123 over ( cid : 123 ) to give the overall free energy :
f ( cid : 123 ) ; ( cid : 123 ) = xd x xj
where the innermost term in the sum is the kullback - leibler divergence between generative and recognition distributions for unit j in layer for example d :
kl ( ; ) = g
weights ( cid : 123 ) and ( cid : 123 ) are trained by following the derivatives of f ( cid : 123 ) ; ( cid : 123 ) in equation 123
since the generative weights ( cid : 123 ) do not affect the actual activities of the units , there are no cycles , and so the derivatives can be calculated in closed form using the chain rule .
appendix b gives the appropriate recursive formul .
note that this deterministic version introduces a further approximation by ignor - ing correlations arising from the fact that under the real recognition model , the actual activities at layer 123 are a function of the actual activities at layer rather than their mean values .
figure 123 demonstrates the performance of the helmholtz machine in a hierarchical learning task ( becker & hinton , 123 ) , showing that it is capable of extracting the structure underlying a complicated generative model .
the example shows clearly the difference between the generative ( ( cid : 123 ) ) and the recognition ( ( cid : 123 ) ) weights , since the latter often include negative side - lobes around their favoured shifts , which are needed to prevent incorrect recognition .
123 the stochastic helmholtz machine
the derivatives required for learning in the deterministic helmholtz machine are quite complicated because they have to take into account the effects that changes in an activity at one layer will have on activities in higher layers .
however , by bor - rowing an idea from the boltzmann machine ( hinton & sejnowski , 123; ackley , hinton & sejnowski , 123 ) , we get a very simple learning scheme for layered net - works of stochastic binary units that approximates the correct derivatives ( hinton et al , in preparation ) .
learning in this scheme is separated into two phases .
during the wake phase , data d from the world are presented at the lowest layer and binary activations of units at successively higher layers are picked according to the recognition proba - j ( cid : 123 ) ; 123 , determined by the bottom - up weights .
the top - down genera - tive weights from layer 123 to layer are then altered to reduce the kullback - leibler divergence between the actual activations and the generative probabilities j ( cid : 123 ) ; 123
in the sleep phase , the recognition weights are turned off and the top -
down weights are used to activate the units .
starting at the top layer , activities are generated at successively lower layers based on the current top - down weights ( cid : 123 ) .
the network thus generates a random instance from its generative model .
since it has generated the instance , it knows the true underlying causes , and therefore has available the target values for the hidden units that are required to train the bottom - up weights .
if the bottom - up and the top - down activation functions are both sigmoid ( equations 123 and 123 ) , then both phases use exactly the same learning rule , the purely local delta rule ( widrow & stearns , 123 ) .
unfortunately , there is no single cost function that is reduced by these two proce - dures .
this is partly because the sleep phase trains the recognition model to invert the generative model for input vectors that are distributed according to the gen - erative model rather than according to the real data and partly because the sleep phase learning does not follow the correct gradient .
nevertheless , ( cid : 123 ) = ( cid : 123 ) at the optimal end point , if it can be reached .
preliminary results by brendan frey ( per - sonal communication ) show that this algorithm works well on some non - trivial
the helmholtz machine can be viewed as a hierarchical generalization of the type of learning procedure described by zemel ( 123 ) and hinton and zemel ( 123 ) .
in - stead of using a xed independent prior distribution for each of the hidden units in a layer , the helmholtz machine makes this prior more exible by deriving it from the bottom - up activities of units in the layer above .
in related work , zemel and hinton ( 123 ) show that a system can learn a redundant population code in a layer of hidden units , provided the activities of the hidden units are represented by a point in a multidimensional constraint space with pre - specied dimensionality .
the role of their constraint space is to capture statistical dependencies among the hidden unit activities and this can again be achieved in a more uniform way by us - ing a second hidden layer in a hierarchical generative model of the type described
the old idea of analysis - by - synthesis assumes that the cortex contains a gener - ative model of the world and that recognition involves inverting the generative model in real time .
this has been attempted for non - probabilistic generative mod - els ( mackay , 123; pece , 123 ) .
however , for stochastic ones it typically involves markov chain monte carlo methods ( neal , 123 ) .
these can be computationally unattractive , and their requirement for repeated sampling renders them unlikely to be employed by the cortex .
in addition to making learning tractable , its sepa - rate recognition model allows a helmholtz machine to recognise without iterative sampling , and makes it much easier to see how generative models could be im - plemented in the cortex without running into serious time constraints .
during
recognition , the generative model is superuous , since the recognition model con - tains all the information that is required .
nevertheless , the generative model plays an essential role in dening the objective function f that allows the parameters ( cid : 123 ) of the recognition model to be learned .
the helmholtz machine is closely related to other schemes for self - supervised learning that use feedback as well as feedforward weights ( carpenter & gross - berg , 123; luttrell , 123; 123; ullman , 123; kawato et al , 123; mumford , 123 ) .
by contrast with adaptive resonance theory ( carpenter & grossberg , 123 ) and the counter - streams model ( ullman , 123 ) , the helmholtz machine treats self - supervised learning as a statistical problem one of ascertaining a generative model which accurately captures the structure in the input examples .
luttrell ( 123; 123 ) discusses multilayer self - supervised learning aimed at faithful vec - tor quantisation in the face of noise , rather than our aim of maximising the like - lihood .
the outputs of his separate low level coding networks are combined at higher levels , and thus their optimal coding choices become mutually dependent .
these networks can be given a coding interpretation that is very similar to that of the helmholtz machine .
however , we are interested in distributed rather than lo - cal representations at each level ( multiple cause rather than single cause models ) , forcing the approximations that we use .
kawato et al ( 123 ) consider forward ( gen - erative ) and inverse ( recognition ) models ( jordan & rumelhart , 123 ) in a similar fashion to the helmholtz machine , but without this probabilistic perspective .
the recognition weights between two layers do not just invert the generation weights between those layers , but also take into account the prior activities in the upper layer .
the helmholtz machine ts comfortably within the framework of grenan - ders pattern theory ( grenander , 123 ) in the form of mumfords ( 123 ) proposals for the mapping onto the brain .
as described , the recognition process in the helmholtz machine is purely bottom - up the top - down generative model plays no direct role and there is no interac - tion between units in a single layer .
however , such effects are important in real perception and can be implemented using iterative recognition , in which the gen - erative and recognition activations interact to produce the nal activity of a unit .
this can introduce substantial theoretical complications in ensuring that the acti - vation process is stable and converges adequately quickly , and in determining how the weights should change so as to capture input examples more accurately .
an in - teresting rst step towards towards interaction within layers would be to organize their units into small clusters with local excitation and longer - range inhibition , as is seen in the columnar structure of the brain .
iteration would be conned within layers , easing the complications .
we are very grateful to drew van camp , brendan frey , geoff goodhill , mike jor - dan , david mackay , mike revow , virginia de sa , nici schraudolph , terry se - jnowski and chris williams for helpful discussions and comments , and particu - larly to mike jordan for extensive criticism of an earlier version of this paper .
this work was supported by nserc and iris .
geh is the noranda fellow of the cana - dian institute for advanced research .
the current address for rsz is baker hall 123 , department of psychology , carnegie mellon university , pittsburgh , pa 123
a the imaging model
the sigmoid activation function given in equation 123 turned out not to work well for the generative model for the input examples we tried , such as the shifter prob - lem ( gure 123 ) .
learning almost invariably got caught in one of a variety of local minima .
in the context of a one layer generative model and without a recognition model , saund ( 123a;b ) discussed why this might happen in terms of the under - lying imaging model which is responsible for turning binary activities in what we call layer 123 into probabilities of activation of the units in the input layer .
he suggested using a noisy - or imaging model ( pearl , 123 ) , for which the weights k = 123 , and are 123 ( cid : 123 ) ( cid : 123 ) 123;
j = 123 if unit 123
;j ( cid : 123 ) 123 are interpreted as probabilities that 123 = 123 yk ( cid : 123 ) 123 123
the noisy - or imaging model worked somewhat better than the sigmoid model of equation 123 , but it was still prone to fall into local minima .
dayan & zemel ( 123 ) suggested a yet more competitive rule based on the integrated segmentation and recognition architecture of keeler et al ( 123 ) .
in this , the weights 123 ( cid : 123 ) ( cid : 123 ) 123; interpreted as the odds that
k = 123 , and are combined as :
j = 123 if unit 123
for the deterministic helmholtz machine , we need a version of this activation rule that uses the probabilities 123 rather than the binary samples 123
this is some - ;j turns out not to work .
in the end ( dayan & zemel , 123 ) we used a product of this term and the deterministic version of the noisy - or :
what complicated , since the obvious expression 123 123=123 k 123
appendix b gives the derivatives of this .
we used the exact expected value of equation 123 if there were only three units in layer 123 because it is computationally inexpensive to work it out .
for convenience , we used the same imaging model ( equations 123 and 123 ) for all the generative connections .
in general one could use different types of connections between different levels .
b the derivatives
write f d; ( cid : 123 ) ; ( cid : 123 ) for the contribution to the overall error in equation 123 for input example d , including the input layer :
f d; ( cid : 123 ) ; ( cid : 123 ) = x xj
then the total derivative for input example d with respect to the activation of a unit in layer is :
@f d; ( cid : 123 ) ; ( cid : 123 )
@f d; ( cid : 123 ) ; ( cid : 123 )
j affects the generative priors at layer 123 , and the recognition activities at all layers higher than .
these derivatives can be calculated in a single backward propagation pass through the network , accumulating @f d; ( cid : 123 ) ; ( cid : 123 ) =@ ) as it goes .
the use of standard sigmoid units in the recognition direction makes
j completely conventional .
using equation 123 makes :
j ;i ya123=j
123 ( cid : 123 ) ;123
123 ( cid : 123 ) ;123
one also needs the derivative :
123 ( cid : 123 ) ;123
j ;i ( cid : 123 ) 123 ya123=j
123 ( cid : 123 ) ;123
this is exactly what we used for the imaging model in equation 123
however , it is j ( cid : 123 ) ; 123 should really be a function of the stochas - important to bear in mind that tic choices of the units in layer 123
the contribution to the expected cost f is a i indicates aver -
function of dg aging over the recognition distribution .
these are not the same as gd and g ( cid : 123 ) 123 d
j ( cid : 123 ) ; 123e and dg ( cid : 123 ) 123 j ( cid : 123 ) ; 123e ( cid : 123 ) , which is what the deterministic machine uses .
for other
imaging models , it is possible to take this into account .
j ( cid : 123 ) ; 123 ( cid : 123 ) e , where h
