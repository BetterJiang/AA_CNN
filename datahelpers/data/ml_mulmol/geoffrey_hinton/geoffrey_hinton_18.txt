deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction .
these methods have dramatically improved the state - of - the - art recognition , visual object recognition , object detection and many other domains such as drug discovery and genomics .
deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer .
deep convolutional nets have brought about breakthroughs in processing images , video , speech and audio , whereas recurrent nets have shone light on sequential data such as text and speech .
deep convolutional nets ( cnn ) deep recurrent nets ( rnn )
visual object recognition
imagenet ( car , dog )
predict drug activity
deep genomics company
conventional machine learning
limited in their ability to process natural data in their raw form .
coming up with features is difficult , time - consuming , requires expert knowledge .
when working applications of learning , we spend a lot of time tuning the features .
a machine be fed with raw data automatically discover representations
deep - learning methods are representation - learning methods
with multiple levels of representation simple but non - linear modules higher and abstract representation with the composition of enough such transformations , very
complex functions can be learned .
layers of features are not designed by human engineers .
learn features from data using a general - purpose learning procedure .
123 basic audio structure
hinton , 123 , ref .
123; lecun , 123 , ref .
123; lecun , 123 , ref .
123; szegedy , 123 , ref
mikolov , 123 , ref .
123; hinton , 123 , ref .
123; sainath , 123 , ref
predicting the activity of potential drug molecules .
ma , j . , 123 , ref .
123 analysing particle accelerator data .
ciodaro , 123 , ref .
123; kaggle , 123 , ref .
123 reconstructing brain circuits .
helmstaedter , 123 , ref .
123 predicting the effects of mutations in non - coding dna on gene expression and disease .
leung , 123 , ref .
123; xiong ,
123 , ref
natural language understanding ( collobert , 123 , ref
question answering .
bordes , 123 , ref .
123 language translation .
jean , 123 , ref .
123; sutskever , 123 , ref
backpropagation to train multilayer architectures convolutional neural networks image understanding with deep convolutional networks distributed representations and language processing recurrent neural networks the future of deep learning
dataset labeling training ( errors , tuning parameters , gradient descent ) testing
stochastic gradient descent ( sgd , bottou , 123 , ref
showing input vector , computing outputs and errors , computing average gradient , adjusting weights accordingly repeating for many small sets until objective function stop decreasing why stochastic : small set gives a noisy estimate of the average gradient over all examples
linear classifiers or shallow classifiers ( must have good features )
input space half - spaces ( duda , 123 , ref .
123 ) : cannot distinguish wolf and samoyed in same position and background
kernel methods ( scholkopf , 123 , ref .
123; bengio , 123 , ref .
123 ) : do not generalize well
deep learning architecture
multiple non - linear layers ( 123 - 123 ) : can distinguish samoyeds from white wolves
backpropagation to train multilayer architectures
key insight : ( ref .
123 - 123 )
nothing more than a practical application of the chain rule for derivatives .
feedforward neural networks
non - linear function : max ( z , 123 ) ( relu , begio , 123 , ref .
123 ) , tanh ( z ) , 123 / ( 123+exp ( - z ) )
forsaken because poor local minima revived around 123 by unsupervised learning procedures with unlabeled data
in practice , local minima are rarely a problem .
( dauphin , 123 , ref .
123; lecun , 123 , ref .
123 ) cifar match : hinton , 123 ref .
123; hinton , 123 , ref .
123; bengio , 123 , ref .
123; lecun , 123 , ref .
123; hinton ,
123 , ref
recognizing handwritten digits or detecting pedestrians ( lecun , 123 , ref .
123 ) speech recognition by gpus with 123 or 123 times faster ( raina , 123 , ref .
123; hinton , 123 , ref .
123; dahl , 123 , ref .
123; bengio , 123 , ref
convolutional neural network ( convnet , cnn )
widely adopted by computer - vision community : lecun , 123 , ref .
123; lecun , 123 , ref
bp key insight
the derivative ( or gradient ) of the objective with respect to the input of a module can be computed by working backwards from the gradient with respect to the output of that module ( or the input of the subsequent module ) .
the chain rule of derivatives tells us how two small effects ( that of a small change
of on , and that of on ) are composed .
a small change in gets transformed first into a small change in by getting multiplied by definition of partial derivative ) .
similarly , the change creates a change in .
substituting one equation into the other gives the chain rule of derivatives how gets turned into through multiplication by the product of also works when , and are vectors ( and the derivatives are jacobian matrices ) .
( that is , the
multilayer neural network
a multilayer neural network ( shown by the connected dots ) can distort the input space to make the classes of data ( examples of which are on the red and blue lines ) linearly separable .
note how a regular grid ( shown on the left ) in input space is also transformed ( shown in the middle panel ) by hidden units .
this is an illustrative example with only two input units , two hidden units and one output unit , but the networks used for object recognition or natural language processing contain tens or hundreds of thousands of units .
reproduced with permission from c
the equations used for computing the forward pass in a neural net with two hidden layers and one output layer , each constituting a module through which one can backpropagate gradients .
at each layer , we first compute the total input z to each unit , which is a weighted sum of the outputs of the units in the layer below .
then a non - linear function f ( . ) is applied to z to get the output of the unit .
for simplicity , we have omitted bias terms .
the non - linear functions used in neural networks include the rectified linear unit ( relu ) , commonly used in recent years , as well as the more conventional sigmoids , such as the hyberbolic tangent ( tanh ) , logistic function .
hyberbolic tangent : = . / 123 / 123 / 123 logistic function : =
the equations used for computing the backward pass .
at each hidden layer we compute the error derivative with respect to the output of each unit , which is a weighted sum of the error derivatives with respect to the total inputs to the units in the layer above .
we then convert the error derivative with respect to the output into the error derivative with respect
to the input by multiplying it by the gradient of ( ) .
at the output layer , differentiating the cost function .
this gives123 if the cost function for
unit is123 > , where123 is the target value .
once the @ known , the error - derivative for the weightb@ on the connection from unit in the layer below is justb @
the error derivative with respect to the output of a unit is computed by
convolutional neural networks
local values are correlated
local statistics are invariant to location
merge similar features into one
the use of many layers
many layers of convolutional , non -
linearity and pooling
mathematically , the filtering operation performed by a feature map is a discrete convolution , hence the name .
inside a convolutional network
the outputs ( not the filters ) of each layer ( horizontally ) of a typical convolutional network architecture applied to the image of a samoyed dog ( bottom left; and rgb ( red , green , blue ) inputs , bottom right ) .
each rectangular image is a feature map corresponding to the output for one of the learned features , detected at each of the image positions .
information flows bottom up , with lower - level features acting as oriented edge detectors , and a score is computed for each image class in output .
relu , rectified linear unit .
lenet - 123 ( lecun , 123 )
c123 : 123*123 units 123 unit; 123*123 123*123; 123*123*123*123+*123=123 parameters;
( 123*123+123 ) *123* ( 123*123 ) =123 , 123 connections; 123 map 123 maps
s123 : 123*123 units 123 unit; 123*123 123*123 ( no overlap ) ; 123*123=123 parameters;
( 123*123+123 ) *123* ( 123*123 ) =123 , 123 connections; 123 map 123 map
c123 : 123 or some maps 123map
convnets vs visual neuroscience
lgn - v123 - v123 - v123 - it ventral pathway ( hubel , 123 , ref .
123; felleman , 123 , ref .
123 ) time - delay neural networks ( ref .
123 - 123 ) document reading , object detection , ( ref .
123 - 123 )
lfw 123 - fold average precision
deepface ( taigman , cvpr123 )
deepid ( sun , cvpr123 )
deepid123 ( sun , nips123 )
deepid123+ ( sun , cvpr123 )
wstfusion ( taigman , cvpr123 )
vggface ( parkhi , bmvc123 )
facenet ( schroff , cvpr 123 )
image understanding with deep convolutional networks
traffic sign recognition , detect of pedestrians , ( ref .
123 , ref .
123 - 123 , ref .
123 - 123 ) face recognition ( deepface , facebook , taigman , cvpr , 123 , ref .
123 ) autonomous mobile robots and self - driving cars ( hadsell , 123 , ref .
123; farabet , 123 , ref .
123 ) natural language understanding ( collobert , 123 , ref .
123 ) speech recognition ( sainath , 123 , ref
convnets in computer version
imagenet ( hinton , 123 , ref .
123; srivastava , 123 , ref
other detection tasks ( ref .
123 , ref .
123 - 123 , ref .
123 - 123 )
convnets - based product and service
google , facebook , microsoft , ibm , yahoo ! , twitter and adobe
nvidia , mobileye , intel , qualcomm and samsung
from image to text
captions generated by a recurrent neural network ( rnn ) taking , as extra input , the representation extracted by a deep convolution neural network ( cnn ) from a test image , with the rnn trained to translate high - level representations of images into captions ( top ) .
reproduced with permission from ref .
when the rnn is given the ability to focus its attention on a different location in the input image ( middle and bottom; the lighter patches were given more attention ) as it generates each word ( bold ) , we found that it exploits this to achieve better translation of images into captions .
distributed representations and language processing
learning distributed representations enable generalization to new combinations of the values of learned features
beyond those seen during training ( for example , 123n combinations are possible with n binary features ) .
( bengio , 123 , ref .
123; montufar , 123 , ref
composing layers of representation in a deep net brings the potential for another exponential advantage ( exponential
in the depth ) .
( montufar , 123 , ref
neural language models predict the next word in a sequence ( begnio , 123 , ref .
123 ) such representations are called distributed representations because their elements ( the features ) are not mutually
exclusive and their many configurations correspond to the variations seen in the observed data .
123 , ref .
123 , ref
visualizing the learned words vectors
on the left is an illustration of word representations learned for modelling language , non - linearly projected to 123d for visualization using the t - sne algorithm ( ref .
on the right is a 123d representation of phrases learned by an english - to - french encoderdecoder recurrent neural network ( ref .
one can observe that semantically similar words or sequences of words are mapped to nearby representations .
the distributed representations of words are obtained by using backpropagation to jointly learn a representation for each word and a function that predicts a target quantity such as the next word in a sequence ( for language modelling ) or a whole sequence of translated words ( for machine translation ) ( ref . 123 , ref
recurrent neural networks
it is better to use rnns for tasks that involve sequential inputs , such as speech and language .
trained by backpropagation ( ref .
123 - 123 ) backpropagation gradients typically explode or vanish ( ref .
123 - 123 )
predict next character in the text ( stutskever , 123 , ref .
123 ) predict next word in a sequence ( lakoff , 123 , ref .
123 ) english - french encoder - decoder network ( ref .
123 , ref .
123 , ref .
123 , ref . 123 - 123 )
very deep feedforward networks : difficult to learn story information very long .
123 ) long short - term memory ( lstm ) networks : memory cell ( hochreither , 123 , ref
more effective than conventional rnns ( hinton , ref
neural turning machine ( graves , 123 , ref .
123 ) and memory networks ( weston , 123 , ref
memory networks can answer questions that require complex inference ( weston , 123 , ref
memory networks answer questions
123 - sentence version of the lord of the rings
bilbo travelled to the cave .
gollum dropped the ring there .
bilbo took the ring .
bilbo went back to the shire .
bilbo left the ring there .
frodo got the ring .
frodo journeyed to mount - doom .
frodo dropped the ring there .
sauron died .
frodo went back to the shire .
bilbo travelled to the grey - havens .
the end .
question123 : where is the ring ?
question123 : where is bilbo now ?
question123 : where is frodo now ?
recurrent neural network ( rnn ) and unfolding
a recurrent neural network and the unfolding in time of the computation involved in its forward computation .
the artificial neurons ( for example , hidden units grouped under node s with values st at time t ) get inputs from other neurons at previous time steps ( this is represented with the black square , representing a delay of one time step , on the left ) .
in this way , a recurrent neural network can map an input sequence with elements xt into an output sequence with elements ot , with each ot depending on all the previous xt ( for t t ) .
the same parameters ( matrices u , v , w ) are used at each time step .
many other architectures are possible , including a variant in which the network can generate a sequence of outputs ( for example , words ) , each of which is used as inputs for the next time step .
the backpropagation algorithm can be directly applied to the computational graph of the unfolded network on the right , to compute the derivative of a total error ( for example , the log - probability of generating the right sequence of outputs ) with respect to all the states st and all the parameters .
the future of deep learning
had a catalytic effect in reviving interest in deep learning , but has since been overshadowed by the
successes of purely supervised learning .
123 - 123 )
human and animal learning is largely unsupervised , we expect unsupervised learning to become
far more important in the longer term .
combine convnets and rnns and use reinforcement learning ( ref .
123 - 123 )
natural language understanding
rnns : when they learn strategies for selectively attending to one part at a time .
123 , ref
representation learning with complex reasoning
new paradigms are needed to replace rule - based manipulation of symbolic expressions by
operations on large vectors
krizhevsky , a . , sutskever , i .
& hinton , g .
imagenet classification with deep convolutional neural networks .
in proc .
advances in neural information processing systems 123 123 this report was a breakthrough that used convolutional nets to almost halve the error rate for object
recognition , and precipitated the rapid adoption of deep learning by the computer vision community .
hinton , g .
deep neural networks for acoustic modeling in speech recognition
signal processing magazine 123 , 123 ( 123 ) .
this joint paper from the major speech recognition laboratories , summarizing the breakthrough achieved with deep learning on the task of phonetic classification for automatic speech recognition , was the first major industrial application of deep learning .
sutskever , i .
vinyals , o .
sequence to sequence learning with neural networks
advances in neural information processing systems 123 123 ( 123 ) .
this paper showed state - of - the - art machine translation results with the architecture introduced in ref .
123 , with a recurrent network trained to read a sentence in one language , produce a semantic representation of its meaning , and generate a translation in another language .
important reference ii
glorot , x . , bordes , a .
& bengio .
deep sparse rectifier neural networks .
in proc
international conference on artificial intelligence and statistics 123 ( 123 ) .
this paper showed that supervised training of very deep neural networks is much faster if the hidden layers are
composed of relu .
hinton , g . e . , osindero , s . &teh , y . - w . afastlearningalgorithmfordeepbelief nets .
neural comp .
123 , 123 ( 123 ) .
this paper introduced a novel and effective way of training very deep neural networks by pre - training one
hidden layer at a time using the unsupervised learning procedure for restricted boltzmann machines .
bengio , y . , lamblin , p . , popovici , d . &larochelle , h . greedylayer - wisetraining of deep networks .
in proc .
advances in neural information processing systems 123 123 ( 123 ) .
this report demonstrated that
the unsupervised pre - training method introduced in ref .
123 significantly improves performance on test data and generalizes the method to other unsupervised representation - learning techniques , such as auto - encoders .
important reference iii
lecun , y . etal .
handwrittendigitrecognitionwithaback - propagationnetwork .
in proc .
advances in
neural information processing systems 123 ( 123 ) .
this is the first paper on convolutional networks trained by backpropagation for the task of classifying low -
resolution images of handwritten digits .
lecun , y . , bottou , l . , bengio , y .
& haffner , p .
gradient - based learning applied to document
recognition .
ieee 123 , 123 ( 123 ) .
this overview paper on the principles of end - to - end training of modular systems such as deep neural networks using gradient - based optimization showed how neural networks ( and in particular convolutional nets ) can be combined with search or inference mechanisms to model complex outputs that are interdependent , such as sequences of characters associated with the content of a document .
bengio , y . , ducharme , r . &vincent , p . aneuralprobabilisticlanguagemodel . in proc .
advances in
neural information processing systems 123 123 ( 123 ) .
this paper introduced neural language models , which learn to convert a word symbol into a word vector or
word embedding composed of learned semantic features in order to predict the next word in a sequence .
important reference iv
with recurrent networks because they are good at learning long - range dependencies .
hochreiter , s .
& schmidhuber , j .
long short - term memory .
neural comput .
123 , 123
this paper introduced lstm recurrent networks , which have become a crucial ingredient in recent advances
