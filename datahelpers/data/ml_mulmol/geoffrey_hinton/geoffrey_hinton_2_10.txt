abstractgaussian mixture models are currently the domi - nant technique for modeling the emission distribution of hidden markov models for speech recognition .
we show that better phone recognition on the timit dataset can be achieved by replacing gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters .
these networks are rst pre - trained as a multi - layer generative model of a window of spectral feature vectors without making use of any discriminative information .
once the generative pre - training has designed the features , we perform discriminative ne - tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden markov models .
using a feature vector that describes segments of the temporal evolution of critical - band spectral densities within a single critical band .
sub - word posterior probabilities are estimated using feedforward neural networks for each critical band and these probabilities are merged to produce the nal estimate of the posterior probabilities using another feedforward neural network .
in ( 123 ) , the split temporal context system is introduced which modies the trap system by including , in the middle layer of the system , splits over time as well as over frequency bands before the nal merger neural network .
feedforward neural networks offer several potential advan -
index termsacoustic modeling , deep belief networks , neural
networks , phone recognition
a utomatic speech recognition ( asr ) has evolved signi -
cantly over the past few decades .
early systems typically discriminated isolated digits or yes / no , whereas current sys - tems can do quite well at recognizing telephone - quality , spon - taneous speech .
a huge amount of progress has been made in improving word recognition rates , but the core acoustic modeling has remained fairly stable , despite many attempts to develop better alternatives .
a typical asr system uses hidden markov models ( hmms ) to model the sequential structure of speech signals , with each hmm state using a mixture of gaussians to model a spectral representation of the sound wave .
the most common spectral representation is a set of mel frequency cepstral coefcients ( mfccs ) derived from a window of about 123 ms of speech .
the window is typically advanced by about 123 ms per frame , and each frame of coefcients is augmented with differences and differences of differences with nearby frames .
one research direction involves using deeper acoustic mod - els that contain many layers of features .
the work in ( 123 ) pro - poses a hierarchical framework where each layer is designed to capture a set of distinctive feature landmarks .
for each feature , a specialized acoustic representation is constructed in which that feature is easy to detect .
in ( 123 ) , a probabilistic generative model is introduced where the dynamic structure in the hidden vocal tract resonance space is used to characterize long - span contextual inuence across phonetic units .
feedforward neural networks have been used in many asr systems ( 123 ) , ( 123 ) , ( 123 ) .
inspired by insights from ( 123 ) , the trap architecture ( 123 ) models a whole second of speech
copyright ( c ) 123 ieee .
personal use of this material
however , permission to use this material for any other purposes must be obtained from the ieee by sending a request to pubs - permissions@ieee . org .
the authors are with the university of toronto , toronto , on , m123s 123g123 , canada .
( e - mail : asamir@cs . toronto . edu; gdahl@cs . toronto . edu; hin -
tages over gmms :
their estimation of the posterior probabilities of hmm states does not require detailed assumptions about the
they allow an easy way of combining diverse features ,
including both discrete and continuous features .
they use far more of the data to constrain each parameter because the output on each training case is sensitive to a large fraction of the weights .
the benet of each weight
in a neural network being constrained by a larger fraction of training cases than each parameter in a gmm has been masked by other differences in training .
neural networks have traditionally been trained purely discriminatively , whereas gmms are typically trained as generative models ( even if discriminative training is per - formed later in the training procedure ) .
generative training allows the data to impose many more bits of constraint on the parameters ( see below ) , thus partially compensating for the fact that each component of a large gmm must be trained on a very small fraction of the data .
mfccs , gmms , and hmms co - evolved as a way of doing speech recognition when computers were too slow to explore more computationally intensive approaches .
mfccs throw away a lot of the information in the sound wave , but pre - serve most of the information required for discrimination .
by including temporal differences , mfccs partially overcome the very strong conditional independence assumption of hmms , namely that successive frames are independent given the hidden state of the hmm .
the temporal differences also allow diagonal covariance gaussians to model the strong temporal covariances by reducing these particular pairwise covariances to individual coefcients .
as we shall see , a fourier transform based lterbank , densely distributed on a mel - scale , potential alternative to mfccs for models that can easily model correlated features ( 123 ) .
gmms are easy to t to data using the em algorithm , especially when they have diagonal covariance matrices , and with enough components they can model any distribution .
they are , however , statistically inefcient at modeling high - dimensional data that has any kind of componential structure .
submitted to ieee trans .
on audio , speech , and language processing
suppose , for example , that n signicantly different patterns can occur in one sub - band and m signicantly different pat - terns can occur in another sub - band .
suppose also that which pattern occurs in each sub - band is approximately independent .
a gmm requires n m components to model this structure because each component must generate both sub - bands ( each piece of data has only a single latent cause ) .
on the other hand , a model that explains the data using multiple causes only requires n + m components , each of which is specic to a particular sub - band .
this exponential inefciency of gmms for modeling factorial structure leads to asr systems that have a very large number of gaussians , most of which must be estimated from a very small fraction of the data .
in this paper , we focus on a more powerful alternative to gmms for relating hmm states to feature vectors .
in particular , we reconsider the use of multi - layer , feed - forward neural networks that take a window of feature vectors as input and produce posterior probabilities of hmm states as output .
previous instantiations of the neural network approach have used the backpropagation algorithm to train the neural networks discriminatively .
these approaches coincided nicely with a trend initiated by ( 123 ) in which generative modeling is replaced by discriminative training .
discriminative training is a very sensible thing to do when using computers that are too slow to learn a really good generative model of the data .
as generative models get better , however , the advantage of discriminative training gets smaller123 and is eventually outweighed by a major disadvantage : the amount of constraint that the data imposes on the parameters of a discriminative model is equal to the number of bits required to specify the correct labels of the training cases , whereas the amount of constraint for a generative model is equal to the number of bits required to specify the input vectors of the training cases .
so when the input vectors contain much more structure than the labels , a generative model can learn many more parameters before it overts .
the benet of learning a generative model is greatly mag - nied when there is a large supply of unlabeled speech in addition to the training data that has been labeled by a forced hmm alignment .
we do not make use of unlabeled data in this paper , but it could only improve our results relative to purely discriminative approaches .
naturally , many of the high - level features learned by the generative model may be irrelevant for making the required discriminations , even though they are important for explaining the input data .
however , this is a price worth paying if computation is cheap and some of the high - level features are very good for discriminating between the classes of interest .
the main novelty of our work is to show that we can achieve consistently better phone recognition performance by pre - training a multi - layer neural network , one layer at a time , as a generative model of the window of speech coefcients .
this pre - training makes it easier to optimize deep neural networks that have many layers of hidden units and it also allows
many more parameters to be used before overtting occurs .
the generative pre - training creates many layers of feature detectors that become progressively more complex .
a subse - quent phase of discriminative ne - tuning , using the standard backpropagation algorithm , then slightly adjusts the features in every layer to make them more useful for discrimination .
the big advantage of this new way of training multi - layer neural networks is that the limited amount of information in the labels is not used to design features from scratch .
it is only used to change the features ever so slightly in order to adjust the class boundaries .
the features themselves are discovered by building a multi - layer generative model of the much richer information in the window of speech coefcients , and this does not require labeled data .
our approach makes two major assumptions about
nature of the relationship between the input data , which in this case is a window of speech coefcients , and the labels , which are hmm states produced by a forced alignment using a pre - existing asr model .
first , we assume that the discrimination we want to perform is more directly related to the underlying causes of the data than to the individual elements of the data itself .
second , we assume that a good feature - vector representation of the underlying causes can be recovered from the input data by modeling its higher - order statistical structure .
it is easy to dream up articial tasks in which our two assumptions are entirely wrong , but a purely discriminative machine learning approach , such as a support vector machine with a polynomial kernel , would still work very well .
suppose , for example , that the label assigned to a window of speech coefcients is simply the parity of two particular components of the binarized data .
our approach would almost certainly fail because this information is unlikely to be preserved in the high - level features and , even if it is preserved , it will be in a much more complex form than the simple form it had in the data .
our claim is that , because of the highly structured way in which speech is generated , this articial task is exactly what asr is not like , and machine learning methods that do not make use of the huge difference between these two tasks have no long term future in asr .
learning a multilayer generative model
there are two very different ways to understand our ap - proach to learning a multi - layer generative model of a window of speech coefcients .
in the directed view , we t a multilayer that has innitely many layers of latent variables , but uses weight sharing among the higher layers to keep the number of parameters under control .
in the undirected , energy - based view , we t a relatively simple type of learning module that only has one layer of latent variables , but then we treat the activities of the latent variables as data and t the same type of module again to this new data .
this can be repeated as many times as we like to learn as many layers of latent variables as we desire .
the undirected view
123it is impossible for a discriminatively trained system to produce better estimates of the posterior probability ratio of two classes than the ratio of the probabilities of generating the data from a mixture of two , class - specic , generative models if these models and their mixing proportions are correct .
the simple learning module used in the undirected view is called a restricted boltzmann machine ( rbm ) .
it is a bi - partite graph in which visible units that represent observations
mohamed , dahl and hinton : acoustic modeling using deep belief networks
are connected to hidden units that learn to represent features using undirected weighted connections .
an rbm is restricted in the sense that there are no visible - visible or hidden - hidden connections .
in the simplest type of rbm , the binary rbm , both the hidden and visible units are binary and stochastic .
to deal with real - valued input data , we use a gaussian - bernoulli rbm in which the hidden units are binary but the input units are linear with gaussian noise .
we will explain the gaussian - bernoulli rbm later after rst explaining the simpler binary
in a binary rbm , the weights on the connections and the biases of the individual units dene a probability distribution over the joint states of the visible and hidden units via an energy function .
the energy of a joint conguration is :
e ( v , h| ) =
where = ( w , b , a ) and wij represents the symmetric interaction term between visible unit i and hidden unit j while bi and aj are their bias terms .
v and h are the numbers of visible and hidden units .
the probability that an rbm assigns to a visible vector v
p ( v| ) = ph ee ( v , h ) pu ph ee ( u , h )
since there are no hidden - hidden connections , the conditional distribution p ( h|v , ) is factorial and is given by :
p ( hj = 123|v , ) = ( aj +
where ( x ) = ( 123 + ex ) no visible - visible connections , p ( v|h , ) is factorial and is given by :
similarly , since there are the conditional distribution
p ( vi = 123|h , ) = ( bi +
exact maximum likelihood learning is infeasible in a large rbm because it is exponentially expensive to compute the derivative of the log probability of the training data .
never - theless , rbms have an efcient approximate training proce - dure called contrastive divergence ( 123 ) which makes them suitable as building blocks for learning dbns .
we repeatedly update each weight , wij , using the difference between two measured , pairwise correlations :
wij hvihjidata hvihjireconstruction
the rst term on the rhs of eq .
123 is the measured frequency with which visible unit i and hidden unit j are on together when the visible vectors are samples from the training set and the states of the hidden units are determined by eq .
the second term is the measured frequency with which i and j are both on when the visible vectors are reconstructions of the data vectors and the states of the hidden units are determined by applying eq .
123 to the reconstructions .
reconstructions are produced by applying eq .
123 to the hidden states that were
computed from the data when computing the rst term on the rhs of eq
the contrastive divergence learning rule does not follow the maximum likelihood gradient .
understanding why it works at all is much easier using the directed view , so we defer the explanation to the next section .
after learning the weights in an rbm module , we use the states of the hidden units , when driven by real data , as the data for training another module of the same kind .
this process can be repeated to learn as many layers of features as we desire .
again , understanding why this greedy approach works is much easier using the directed view .
for gaussian - bernoulli rbms123 the energy of a joint con -
e ( v , h| ) =
since there are no visible - visible connections , the conditional distribution p ( v|h , ) is factorial and is given by :
p ( vi|h , ) = n
where n ( , v ) is a gaussian with mean and variance v .
apart from these differences , the inference and learning rules for a gaussian - bernoulli rbm are the same as for a binary rbm , though the learning rate needs to be smaller .
the directed view
in the undirected view , it is easy to say what we do , but hard to justify it .
in the alternative directed view , the learning algorithm is more complicated to explain , but much easier to
a multi - layer sigmoid belief net composed of stochastic binary units .
consider a sigmoid belief net ( 123 ) that consists of multiple layers of binary stochastic units as shown in gure 123
the higher hidden layers represent binary features and the bot - tom , visible , layer represents a binary data vector ( we will
123to keep the equation simple , we assume that the gaussian noise level of all the visible units is xed at 123
we also normalize the input data to have a xed variance of 123 for each component over the whole training set .
submitted to ieee trans .
on audio , speech , and language processing
123 ) learning with tied weights : consider a sigmoid belief net with an innite number of layers and with tied symmetric weights between layers as shown in gure 123
in this net , the posterior in the rst hidden layer is factorial : the hidden units are independent given the states of the visible units .
this occurs because the correlations created by the prior coming from all of the layers above exactly cancel the anti - correlations in the likelihood term coming from the layer below ( 123 ) .
moreover , the factorial posterior can be computed by simply multiplying the visible vector by the transposed weight matrix and then applying the logistic function to each element :
j =123|v , w ) = ( b ( 123 )
notice that this simple operation computes the normalized product of the prior and the likelihood terms , not the likelihood term , which is considerably more complicated .
show how to handle real - valued data later ) .
generating data from the model is easy .
first , binary states are chosen for the top layer of hidden units using their biases to determine the log odds of choosing 123 or 123
given the binary states of the units in layer k , binary states are chosen in parallel for all of the units in layer k 123 by applying the logistic sigmoid 123 to the total input received from function ( x ) = ( 123 + ex ) the layer above plus the units own bias :
=123|h ( k ) , w ( k ) ) = ( b ( k123 )
where h ( k ) is the vector of binary states for layer k and h ( k ) is its jth element .
w ( k ) is the matrix of weights from layer k to layer k 123 , w ( k ) the bias of unit j in layer k .
the vector of states of the visible units , v , is also called h ( 123 ) .
is an element of that matrix , and b ( k )
now consider the problem of adjusting the weights on the top - down connections so that the model is more likely to gen - erate the binary training data on its visible units .
performing gradient ascent in the expected log probability of generating the training data is very simple if we can get unbiased samples of the hidden states from their posterior distribution given an observed data vector :
where the angle brackets denote an expectation over the training data .
if we want to ensure that every weight update increases the log probability of the data , we need to use a very small learning rate and we need to average over many samples from the posterior .
in practice , it is much more efcient to use a larger learning rate on small mini - batches of the data .
unfortunately , getting unbiased samples from the exponen - tially large posterior appears to be intractable for all but the smallest models . 123 consider , for example , the posterior in the rst hidden layer .
this posterior is the normalized product of a complicated , non - factorial prior created by the layers above and a complicated non - factorial likelihood term created by the observed states of the visible units .
when generating data from the model , the units in the rst hidden layer are , by denition , conditionally independent given the states of the units in the layer above .
when inferring the posterior , however , they are not conditionally independent given the states of the units in the layer below , even with a uniform prior , due to the phenomenon of explaining away ( 123 ) .
approximate samples from the posterior can be obtained by using a slow markov chain monte carlo method ( 123 ) or a fast but crude approximation ( 123 ) .
there is , however , one very special form of sigmoid belief net in which sampling from the posterior distribution in every hidden layer is just as easy as generating data from the model .
in fact , inference and generation are the same process , but running in opposite
123in this respect , mixture models appear to be far superior .
the exact posterior over the mixture components is easy to compute because it only has as many terms as the number of components .
this computational simplicity , however , comes at a terrible price : the whole data vector must be generated by a single component .
as we shall see later , it is possible to achieve an equally simple posterior whilst allowing multiple simultaneous causes .
an innite sigmoid belief net with tied weights .
alternate layers must have the same number of units .
the tied weights make inference much simpler in this net than in a general sigmoid belief net .
once the posterior has been sampled for the rst hidden layer , exactly the same process can be used for the next hidden layer .
so inference is extremely easy in this special kind of network .
learning is a little more difcult because every copy of the tied weight matrix gets different derivatives .
however , we know in advance that the expected derivatives will be zero for very high level layers .
this is because the bottom - up inference process is really a markov chain that eventually converges to its stationary distribution in the higher layers .
when it is sampling from its stationary distribution , the current weights are perfect for explaining the samples , so , on average , there is no derivative .
when the weights and biases are small , this markov chain converges rapidly and we can approximate
mohamed , dahl and hinton : acoustic modeling using deep belief networks
gradient ascent in the log likelihood quite well by just adding the derivatives for the rst two layers ( 123 ) .
the tied weights mean that the process of inferring h ( 123 ) from h ( 123 ) is the same as the process of generating v from h ( 123 ) .
consequently , h ( 123 ) can be viewed as a noisy but unbiased estimate of the probabilities for the visible units predicted by h ( 123 ) .
similarly h ( 123 ) can be viewed as a noisy estimate of the probabilities for the units in the rst hidden layer predicted by h ( 123 ) .
we can use these two facts and equation 123 to get an unbiased estimate of the sum of the derivatives for the rst two layers of weights .
this gives the following learning rule which is known as contrastive divergence ( 123 ) :
j ( vi h ( 123 ) j i hh ( 123 )
) + h ( 123 )
where the angle brackets denote expectations over the training data ( or a representative mini - batch ) .
as the weights and biases grow , it makes sense to add the derivatives for more layers ( 123 ) and for learning really good generative models this is essential ( 123 ) .
for the purpose of creating sensible feature detectors , however , even a rather poorly tuned generative model is good enough , and the learning rule in equation 123 is sufcient even when the weights get quite large .
the approximate maximum likelihood derivatives produced by this learning rule become highly biased , but they are cheap to compute and they also have very low variance ( 123 ) which is important for allowing a high learning rate when the derivatives are estimated from small mini - batches of data .
these issues are discussed further in
123 ) learning different weights in each layer : now that we have efcient inference and learning procedures for an innite sigmoid belief net with tied weights , the next step is to make the generative model more powerful by allowing different weights in different layers .
first , we learn with all of the weight matrices tied together .
then we untie the bottom weight matrix from the other matrices and freeze the values of its weights .
this frozen matrix is now called w ( 123 ) .
then , keeping all the remaining matrices tied together , we continue to learn the higher matrices , treating the inferred state vector h ( 123 ) in just the same way as we previously treated v .
this involves rst inferring h ( 123 ) from v by using ( w ( 123 ) ) t and then inferring h ( 123 ) , h ( 123 ) , and h ( 123 ) in a similar bottom - up manner using w or w t .
the inference for the higher hidden layers produces unbiased samples given h ( 123 ) , but the simple inference method no longer gives an exactly unbiased sample for h ( 123 ) because the higher weight matrices are no longer equal to w ( 123 ) so they no longer create a prior that exactly cancels the correlations in the likelihood term .
however , the posterior for h ( 123 ) is still approximately factorial and it can be proved that if we continue to infer h ( 123 ) as if the higher matrices had not changed , then learning improves a variational lower bound on the log probability of the training data ( 123 ) 123
123the proof assumes that each layer of weights is learned by following the maximum likelihood derivative , rather than using the contrastive divergence
we can repeat the process of freezing and untying the lowest copy of the currently tied weight matrices as many times as we like , so we can learn as many layers of features as we desire .
when we have learned k layers of features , we are left with a directed generative model called a deep belief net ( dbn ) that has k different weight matrices between the lower layers and an innite number of higher layers that all use the k th weight matrix or its transpose .
we then simply throw away all layers above the k th and add a nal softmax layer of label units representing hmm states .
we also jettison the probabilistic interpretation that was used to justify the generative learning , and we treat the whole system as a feedforward , deterministic neural network .
this network is then discriminatively ne - tuned by using backpropagation to maximize the log probability of the correct hmm state123
for the softmax nal layer , the probability of label l given the real - valued activations of the nal layer of features ( which we call h ( k ) even though they are no longer binary sampled values ) is dened to be
exp ( bl + pi h ( k ) pm exp ( bm + pi h ( k )
where bl is the bias of the label and wil is the weight from hidden unit i in layer k to label l .
the discriminative training must learn the weights from the last layer of features to the label units , but it does not need to create any new feature detectors .
it simply ne - tunes existing feature detectors that were discovered by the unsupervised pre - training .
to model real values in the visible layer , we simply replace the binary unit by linear units with gaussian noise that has a variance of 123
this does not change p ( h ( 123 ) |v ) and the distribution for visible unit i given h ( 123 ) is a gaussian with unit variance and mean i given by :
i = b ( 123 )
this type of generative pre - training followed by discrimi - native ne - tuning has been used successfully for hand - written character recognition ( 123 ) , ( 123 ) , ( 123 ) , dimensionality reduction ( 123 ) , 123 - d object recognition ( 123 ) , ( 123 ) , extracting road maps from cluttered aerial images ( 123 ) , information retrieval ( 123 ) , ( 123 ) and machine transliteration ( 123 ) .
as we shall see , it is also very good for phone recognition .
using deep belief nets for phone recognition in order to apply dbns with xed input and output dimen - sionality to phone recognition , we use a context window of n successive frames of speech coefcients to set the states of the visible units of the lowest layer of the dbn .
once it has been pre - trained as a generative model , the resulting feedfoward neural network is discriminatively trained to output a probability distribution over the possible labels of the central frame .
to generate phone sequences , the sequence of predicted
123in a convenient abuse of the correct terminology , we sometimes use dbn to refer to a feedforward neural network that was initialized using a generatively trained deep belief net , even though the feedforward neural network is clearly very different from a belief net .
submitted to ieee trans .
on audio , speech , and language processing
probability distributions over the possible labels for each frame is fed into a standard viterbi decoder .
strictly speaking , since the hmm implements a prior over states , we should divide out the prior from the posterior distribution over hmm states produced by the dbn , although for our particular task it made no difference .
experimental setup
timit corpus
phone recognition experiments were performed on the timit corpus . 123 we used the 123 speaker training set and re - moved all sa records ( i . e . , identical sentences for all speakers in the database ) since they could bias the results .
a separate development set of 123 speakers was used for tuning all of the meta parameters , such as the number of layers and the size of each layer .
results are reported using the 123 - speaker core test set , which excludes the development set .
the speech was analyzed using a 123 - ms hamming window with a 123 - ms xed frame rate .
in most of the experiments , we represented the speech using 123th - order mel frequency cepstral coefcients ( mfccs ) and energy , along with their rst and second temporal derivatives .
for some experiments , we used a fourier - transform - based lter - bank with 123 coefcients distributed on a mel - scale ( and energy ) together with their rst and second temporal derivatives .
the data were normalized so that , averaged over the training cases , each coefcient or rst derivative or second derivative had zero mean and unit variance .
we used 123 target class labels ( i . e . , 123 states for each one of the 123 phones ) .
after decoding , the 123 phone classes were mapped to a set of 123 classes as in ( 123 ) for scoring .
all of our experiments used a bigram language model over phones , estimated from the
computational setup
training dbns of the sizes used in this paper is quite com - putationally expensive .
training was accelerated by exploiting graphics processors , in particular gpus in a nvidia tesla s123 system , using the cudamat library ( 123 ) .
a single pass over the entire training set ( an epoch ) for a model with 123 hidden layers and 123 units per layer took about 123 minutes during pre - training of the topmost layer and about 123 minutes during ne - tuning the whole network with backpropagation .
a single gpu learns at 123 times faster than a single 123 ghz
for all experiments , we xed the parameters for the viterbi decoder .
specically , we used a zero word insertion probabil - ity and a language model scale factor of 123 .
all dbns were pre - trained with a xed recipe using stochastic gradient decent with a mini - batch size of 123 training cases .
for gaussian - binary rbms , we ran 123 epochs with a xed learning rate of 123 while for binary - binary rbms we used 123 epochs with a learning rate of 123 .
the theory used to justify the pre - training algorithm as - sumes that when the states of the visible units are reconstructed from the inferred binary activities in the rst hidden layer , they are reconstructed stochastically .
to reduce noise in the learning , we actually reconstructed them deterministically and used the real values ( see ( 123 ) for more details ) .
for ne - tuning , we used stochastic gradient descent with the same mini - batch size as in pre - training .
the learning rate started at 123 .
at the end of each epoch , if the substitution error on the development set increased , the weights were returned to their values at the beginning of the epoch and the learning rate was halved .
this continued until the learning rate fell below
during both pre - training and ne - tuning , a small weight - cost of 123 was used and the learning was accelerated by using a momentum of 123 ( except for the rst epoch of ne - tuning which did not use momentum ) .
( 123 ) gives a detailed explanation of weight - cost and momentum and sensible ways to set them .
figure 123 and gure 123 show the effect of varying the size of each hidden layer and the number of hidden layers .
for simplicity we used the same size for every hidden layer in a network .
for these comparisons , the number of input frames was xed at 123
number of layers
phone error rate on the development set as a function of the number of layers and size of each layer , using 123 input frames .
the main trend visible in gures 123 and 123 is that adding more hidden layers gives better performance , though the gain diminishes as the number of layers increases .
using more hidden units per layer also improves performance when the number of hidden layers is less than 123 , but with more hidden layers the number of units has little effect provided it is at least 123
the advantage of using a deep architecture is clear if we consider the best way to use a total of 123 hidden units : it is better to use two layers of 123 or four layers of 123 than one layer of 123
to benet from having many hidden layers , it is necessary to do generative pre - training .
with a single hidden layer of 123 units , generative pre - training gives a phone error rate of 123% and exactly the same ne - tuning algorithm started from
mohamed , dahl and hinton : acoustic modeling using deep belief networks
number of layers
number of layers
phone error rate on the core test set as a function of the number of layers and size of each layer , using 123 input frames .
phone error rate on the core test set as a function of the number of hidden layers using randomly initialized and pretrained networks .
small random weights gives 123% .
so generative pre - training does not help .
adding a second hidden layer causes a larger proportional increase in the number of trainable parameters than adding a third hidden layer because the input and output layers are much smaller than the hidden layers and because adjacent hidden layers are fully connected .
this large increase in capacity makes the model far more exible , but it also makes it overt far more easily .
figure 123 and gure 123 show that for networks that are not pre - trained ( but still use early stopping ) , these two effects apparently cancel out , whereas for pre - trained networks there is less overtting so extra layers help .
although the advantage of pre - training is not as large as for some other tasks ( 123 ) , ( 123 ) , it is still required to gain an advantage from using extra hidden layers .
the development set is achieved using 123 , 123 , or 123 frames and this is true whatever the number of hidden layers .
much smaller ( 123 frames ) and much bigger ( 123 frames ) windows give signicantly worse performance .
the range from 123ms to 123ms covers the average size of phones or syllables .
smaller input windows miss important discriminative information in the context , while networks with larger windows are probably getting distracted by the almost from the center of the window .
the trap ( 123 ) architecture successfully uses 123 second long windows , but it dedicates separate networks to model different parts of the spectrum which simplies the learning task .
larger windows would probably work better using triphone targets which provide the network with more information about the context and make the peripheral frames more relevant .
number of layers
number of layers
phone error rate on the development set as a function of the number of hidden layers using randomly initialized and pretrained networks .
phone error rate on the development set as a function of the number of layers , using 123 hidden units per layer .
fixing the number of hidden units per layer to 123 and varying the number of frames in the input window ( gures 123 and 123 and also 123 and 123 ) shows that the best performance on
since all but the output layer weights are pre - trained , it could be helpful to introduce a bottleneck at the last layer
submitted to ieee trans .
on audio , speech , and language processing
number of layers
number of layers
phone error rate on the core test set as a function of the number of layers , using 123 hidden units per layer .
context window of lter bank coefcients as input features to the dbn .
phone error rates as a function of the number of layers , using a
of the dbn to combat overtting by reducing the number of weights that are not pre - trained .
a bottleneck layer followed by a softmax is exactly equivalent to using a distributed output code for each class and then making the class probabilities proportional to the exponentiated squared difference between the code for a class and the activity pattern in the bottleneck layer ( 123 ) .
figure 123 shows that having a bottleneck layer does not actually improve per for a typical network with 123 hidden layers of 123 units and an input window of 123 frames of
the effect of the size of the bottleneck on the phone error rate for a typical network with 123 input frames and 123 hidden layers of 123 units per layer ( except for the last hidden layer ) .
in all of the previous experiments mfccs were used as the input representation .
mfccs attempt to reduce the dimensionality of the input by eliminating variations that are irrelevant for recognition and spoon - feeding the recognizer with a modest - sized input representation that to make recognition easy .
with a more powerful learning procedure , better recognition performance can be achieved
by using a less pre - processed input representation consisting of a large number of lter - bank coefcients augmented with temporal differences and differences of differences .
figure 123 shows that a dbn is capable of making good use of the more detailed information available in this larger input representation .
for the dbn system that perfomed best on the development set , the phone error rate on the timit core test set was 123% .
reported results on timit core test set
stochastic segmental models ( 123 )
conditional random field ( 123 )
large - margin gmm ( 123 )
augmented conditional random fields ( 123 )
recurrent neural nets ( 123 )
bayesian triphone hmm ( 123 )
monophone htms ( 123 )
heterogeneous classiers ( 123 )
triphone hmms discriminatively trained w / bmmi ( 123 ) monophone deep belief networks ( dbns ) ( this work )
table i compares the best performing dbn model with
previously reported results on the timit core test set 123
conclusions and future work
so far as we know , the work reported in this paper was the rst application to acoustic modeling of neural networks in which multiple layers of features are generatively pre - trained .
since then , our approach has been extended to explicitly model the covariance structure of the input features ( 123 ) .
it has been also used to jointly train the acoustic and language models using the full utterance rather than a local window of frames
123in ( 123 ) a per of 123% is reported on the complete test set of timit .
the speech units used are not the same as the standard timit denitions and their method would very probably give a worse result using the standard
mohamed , dahl and hinton : acoustic modeling using deep belief networks
it has been also applied to a large vocabulary task ( 123 ) where the competing gmm approach uses a very large number of components .
in this latter task it gives a very large advantage relative to the gmm .
we are currently exploring alternatives input representations that allow deep neural networks to see more of the relevant information in the sound - wave , such as very precise coinci - dences of onset times in different frequency bands .
we are also exploring ways of using recurrent neural networks to greatly increase the amount of detailed information about the past that can be carried forward to help in the interpretation of the
we thank jim glass , john bridle , li deng and gerald penn for helpful discussions and two anonymous reviewers for improving the paper .
the research was funded by a gift from microsoft research and by grants from the natural sciences and engineering research council of canada and the canadian institute for advanced research .
