leads to clustering of documents according to sharing of topics at multiple levels of abstraction .
given a corpus of documents , a posterior inference algorithm nds an approximation to a posterior distribution over trees , topics and allocations of words to levels of the tree .
we demonstrate this algorithm on collections of scientic abstracts from several journals .
this model exemplies a recent trend in statistical machine learningthe use of bayesian nonparametric methods to infer distributions on exible data structures .
categories and subject descriptors : g . 123 ( probability and statistics ) : stochastic processes; i . 123 ( artificial intelligence ) : text analysis
general terms : algorithms , experimentation
additional key words and phrases : bayesian nonparametric statistics , unsupervised learning
( to appear in the journal of the acm )
for much of its history , computer science has focused on deductive formal methods , allying itself with deductive traditions in areas of mathematics such as set theory , logic , algebra , and combinatorics .
there has been accordingly less focus on eorts to develop inductive , empirically - based formalisms in computer science , a gap which became increasingly visible over the years as computers have been required to in - teract with noisy , dicult - to - characterize sources of data , such as those deriving from physical signals or from human activity .
in more recent history , the eld of
authors address : d .
blei , computer science department , princeton university , 123 olden street , princeton , nj , 123 permission to make digital / hard copy of all or part of this material without fee for personal or classroom use provided that the copies are not made or distributed for prot or commercial advantage , the acm copyright / server notice , the title of the publication , and its date appear , and notice is given that copying is by permission of the acm , inc .
to copy otherwise , to republish , to post on servers , or to redistribute to lists requires prior specic permission and / or a fee .
c ( cid : 123 ) 123yy acm 123 - 123 / 123yy / 123 - 123 $123
journal of the acm , vol .
n , month 123yy , pages 123 ? ? .
david m .
blei et al .
machine learning has aimed to ll this gap , allying itself with inductive traditions in probability and statistics , while focusing on methods that are amenable to analysis as computational procedures .
machine learning methods can be divided into supervised learning methods and unsupervised learning methods .
supervised learning has been a major focus of machine learning research .
in supervised learning , each data point is associated with a label ( e . g . , a category , a rank or a real number ) and the goal is to nd a function that maps data into labels ( so as to predict the labels of data that have not yet been labeled ) .
a canonical example of supervised machine learning is the email spam lter , which is trained on known spam messages and then used to mark incoming unlabeled email as spam or non - spam .
while supervised learning remains an active and vibrant area of research , more recently the focus in machine learning has turned to unsupervised learning meth - in unsupervised learning the data are not labeled , and the broad goal is to nd patterns and structure within the data set .
dierent formulations of un - supervised learning are based on dierent notions of pattern and structure .
canonical examples include clustering , the problem of grouping data into mean - ingful groups of similar points , and dimension reduction , the problem of nding a compact representation that retains useful information in the data set .
one way to render these notions concrete is to tie them to a supervised learning problem; thus , a structure is validated if it aids the performance of an associated supervised learning system .
often , however , the goal is more exploratory .
tures and patterns might be used , for example , to visualize or organize the data according to subjective criteria .
with the increased access to all kinds of unlabeled datascientic data , personal data , consumer data , economic data , government data , text dataexploratory unsupervised machine learning methods have become
another important dichotomy in machine learning distinguishes between para - metric and nonparametric models .
a parametric model involves a xed representa - tion that does not grow structurally as more data are observed .
examples include linear regression and clustering methods in which the number of clusters is xed a priori .
a nonparametric model , on the other hand , is based on representations that are allowed to grow structurally as more data are observed . 123 nonparametric approaches are often adopted when the goal is to impose as few assumptions as possible and to let the data speak .
the nonparametric approach underlies many of the most signicant developments in the supervised learning branch of machine learning over the past two decades .
in particular , modern classiers such as decision trees , boosting and nearest neighbor methods are nonparametric , as are the class of supervised learning systems built on kernel methods , including the support vector machine .
( see ( hastie et al .
123 ) for a good review of these methods . ) theoretical developments in supervised learning have shown that as the number of data points grows , these methods can converge to the true labeling function underlying the data , even when the data lie in an uncountably innite space and the labeling function is arbitrary ( devroye
123in particular , despite the nomenclature , a nonparametric model can involve parameters; the issue is whether or not the number of parameters grows as more data are observed .
journal of the acm , vol .
n , month 123yy .
the nested chinese restaurant process
this would clearly not be possible for parametric classiers .
the assumption that labels are available in supervised learning is a strong as - sumption , but it has the virtue that few additional assumptions are generally needed to obtain a useful supervised learning methodology .
in unsupervised learning , on the other hand , the absence of labels and the need to obtain operational denitions of pattern and structure generally makes it necessary to impose additional as - sumptions on the data source .
in particular , unsupervised learning methods are often based on generative models , which are probabilistic models that express hypotheses about the way in which the data may have been generated .
proba - bilistic graphical models ( also known as bayesian networks and markov random elds ) have emerged as a broadly useful approach to specifying generative mod - els ( lauritzen 123; jordan 123 ) .
the elegant marriage of graph theory and prob - ability theory in graphical models makes it possible to take a fully probabilistic ( i . e . , bayesian ) approach to unsupervised learning in which ecient algorithms are available to update a prior generative model into a posterior generative model once data have been observed .
although graphical models have catalyzed much research in unsupervised learn - ing and have had many practical successes , it is important to note that most of the graphical model literature has been focused on parametric models .
in particu - lar , the graphs and the local potential functions comprising a graphical model are viewed as xed objects; they do not grow structurally as more data are observed .
thus , while nonparametric methods have dominated the literature in supervised learning , parametric methods have dominated in unsupervised learning .
this may seem surprising given that the open - ended nature of the unsupervised learning problem seems particularly commensurate with the nonparametric philosophy .
but it reects an underlying tension in unsupervised learningto obtain a well - posed learning problem it is necessary to impose assumptions , but the assumptions should not be too strong or they will inform the discovered structure more than the data
it is our view that the framework of bayesian nonparametric statistics pro - vides a general way to lessen this tension and to pave the way to unsupervised learning methods that combine the virtues of the probabilistic approach embod - ied in graphical models with the nonparametric spirit of supervised learning .
in bayesian nonparametric ( bnp ) inference , the prior and posterior distributions are no longer restricted to be parametric distributions , but are general stochastic pro - cesses ( hjort et al .
recall that a stochastic process is simply an indexed col - lection of random variables , where the index set is allowed to be innite .
thus , using stochastic processes , the objects of bayesian inference are no longer restricted to nite - dimensional spaces , but are allowed to range over general innite - dimensional spaces .
for example , objects such as trees of arbitrary branching factor and arbi - trary depth are allowed within the bnp framework , as are other structured objects of open - ended cardinality such as partitions and lists .
it is also possible to work with stochastic processes that place distributions on functions and distributions on distributions .
the latter fact exhibits the potential for recursive constructions that is available within the bnp framework .
in general , we view the representational exibility of the bnp framework as a statistical counterpart of the exible data
journal of the acm , vol .
n , month 123yy .
david m .
blei et al .
structures that are ubiquitous in computer science .
in this paper , we aim to introduce the bnp framework to a wider computational audience by showing how bnp methods can be deployed in a specic unsupervised machine learning problem of signicant current interestthat of learning topic models for collections of text , images and other semi - structured corpora ( blei et al .
123; griths and steyvers 123; blei and laerty 123 ) .
let us briey introduce the problem here; a more formal presentation appears in section 123
a topic is dened to be a probability distribution across words from a vocabulary .
given an input corpusa set of documents each consisting of a sequence of wordswe want an algorithm to both nd useful sets of topics and learn to organize the topics according to a hierarchy in which more abstract topics are near the root of the hierarchy and more concrete topics are near the leaves .
while a classical unsupervised analysis might require the topology of the hierarchy ( branching factors , etc ) to be chosen in advance , our bnp approach aims to infer a distribution on topologies , in particular placing high probability on those hierarchies that best explain the data .
moreover , in accordance with our goals of using exible models that let the data speak , we wish to allow this distribution to have its support on arbitrary topologiesthere should be no limitations such as a maximum depth or maximum branching factor .
we provide an example of the output from our algorithm in figure 123
the input corpus in this case was a collection of abstracts from the journal of the acm ( jacm ) from the years 123 to 123
the gure depicts a topology that is given highest probability by our algorithm , along with the highest probability words from the topics associated with this topology ( each node in the tree corresponds to a single topic ) .
as can be seen from the gure , the algorithm has discovered the category of function words at level zero ( e . g . , the and of ) , and has discovered a set of rst - level topics that are a reasonably faithful representation of some of the main areas of computer science .
the second level provides a further subdivision into more concrete topics .
we emphasize that this is an unsupervised problem .
the algorithm discovers the topic hierarchy without any extra information about the corpus ( e . g . , keywords , titles or authors ) .
the documents are the only inputs to the algorithm .
a learned topic hierarchy can be useful for many tasks , including text catego - rization , text compression , text summarization and language modeling for speech recognition .
a commonly - used surrogate for the evaluation of performance in these tasks is predictive likelihood , and we use predictive likelihood to evaluate our meth - ods quantitatively .
but we also view our work as making a contribution to the de - velopment of methods for the visualization and browsing of documents .
the model and algorithm we describe can be used to build a topic hierarchy for a document collection , and that hierarchy can be used to sharpen a users understanding of the contents of the collection .
a qualitative measure of the success of our approach is that the same tool should be able to uncover a useful topic hierarchy in dierent domains based solely on the input data .
by dening a probabilistic model for documents , we do not dene the level of abstraction of a topic formally , but rather dene a statistical procedure that allows a system designer to capture notions of abstraction that are reected in
journal of the acm , vol .
n , month 123yy .
the nested chinese restaurant process
the topic hierarchy learned from 123 abstracts of the journal of the acm ( jacm ) from 123
the vocabulary was restricted to the 123 , 123 terms that occurred in more than ve documents , yielding a corpus of 123k words .
the learned hierarchy contains 123 topics , and each topic node is annotated with its top ve most probable terms .
we also present examples of documents associated with a subset of the paths in the hierarchy .
usage patterns of the specic corpus at hand .
while the content of topics will vary across corpora , the ways in which abstraction interacts with usage will not .
a corpus might be a collection of images , a collection of html documents or a collection of dna sequences .
dierent notions of abstraction will be appropriate in these dierent domains , but each are expressed and discoverable in the data , making it possible to automatically construct a hierarchy of topics .
this paper is organized as follows .
we begin with a review of the necessary background in stochastic processes and bayesian nonparametric statistics in sec - tion 123
in section 123 , we develop the nested chinese restaurant process , the prior on topologies that we use in the hierarchical topic model of section 123
we derive
journal of the acm , vol .
n , month 123yy .
the , ofa , is , and , in , to , for , that , wenalgorithmtimelogproblemgraphgraphsverticesedgeedgesproperty testing and its connection to learning and approximationfully dynamic planarity testing with applicationsrecognizing planar perfect graphsthe coloring and maximum independent set problems on planar perfect graphsbiconnectivity approximations and graph carvingsfunctionsfunctionpolynomialegrsettreestreesearchregularstringon the sorting - complexity of sufx tree constructionefcient algorithms for inverting evolutiontheory of neuromatapatricia tries again revisiteddecision tree reductionschedulingonlinecompetitivemachineparallelprogramslanguagelanguagessetsprogramlogicformulaslogicstemporalrelationalalternating - time temporal logicfixpoint logics , relational machines , and computational complexitydenable relations and rst - order query languages over stringsautoepistemic logicexpressiveness of structured document query languages . . . rulesresolutionproofrewritingcompletenessquestionalgebradependenciesbooleanalgebrasnetworksnetworknprotocolboundsasynchronoustobjectsconsensusobjectroutingsortingnetworksadaptiveschemeplanar - adaptive routing : low - cost adaptive networks for multiprocessorson - line analysis of the tcp acknowledgment delay problema trade - off between space and efciency for routing tablesuniversal - stability results and performance bounds for greedy contention - resolution protocolsperiodication scheme : constructing sorting networks with constant periodqueuingclosedthroughputproduct - formasymptoticstatesautomatavericationautomatonstatesystemsystemsdatabaseprocessingschemestransactionsdistributedperformancemeasuresavailabilityconsistencyconstraintconstraintslocaldconstraint tightness and looseness versus local and global consistencyan optimal on - line algorithm for metrical task systemon the minimality and global consistency of row - convex constraint networksusing temporal hierarchies to efciently maintain large temporal databasesmaintaining state constraints in relational databases : a proof theoretic basismethodsretrievaldecompositionseveralextraknowledgeclassesinferencetheoryquerieslearningprobabilisticformulasquantumlearnablelearning to reasonlearning boolean formulaslearning functions represented as multiplicity automatadense quantum coding and quantum nite automataa neuroidal architecture for cognitive computation
david m .
blei et al .
a conguration of the chinese restaurant process .
there are an innite number of tables , each associated with a parameter i .
the customers sit at the tables according to eq .
( 123 ) and each generate data with the corresponding parameter .
in this conguration , ten customers have been seated in the restaurant , populating four of the innite set of tables .
an approximate posterior inference algorithm in section 123 to learn topic hierarchies from text data .
examples and an empirical evaluation are provided in section 123
finally , we present related work and a discussion in section 123
our approach to topic modeling reposes on several building blocks from stochas - tic process theory and bayesian nonparametric statistics , specically the chinese restaurant process ( aldous 123 ) , stick - breaking processes ( pitman 123 ) , and the dirichlet process mixture ( antoniak 123 ) .
in this section we briey review these ideas and the connections between them .
123 dirichlet and beta distributions recall that the dirichlet distribution is a probability distribution on the simplex of nonnegative real numbers that sum to one .
we write u dir ( 123 , 123 ,
for a random vector u distributed as a dirichlet random variable on the k - simplex , where i > 123 are parameters .
the mean of u is proportional to the parameters
and the magnitude of the parameters determines the concentration of u around the mean .
the specic choice 123 = = k = 123 yields the uniform distribution on the simplex .
letting i > 123 yields a unimodal distribution peaked around the mean , and letting i < 123 yields a distribution that has modes at the corners of the simplex .
the beta distribution is a special case of the dirichlet distribution for k = 123 , in which case the simplex is the unit interval ( 123 , 123 ) .
in this case we write u beta ( 123 , 123 ) , where u is a scalar .
123 chinese restaurant process the chinese restaurant process ( crp ) is a single parameter distribution over par - titions of the integers .
the distribution can be most easily described by specifying how to draw a sample from it .
consider a restaurant with an innite number of ta - bles each with innite capacity .
a sequence of n customers arrive , labeled with the integers ( 123 , .
the rst customer sits at the rst table; the nth subsequent
journal of the acm , vol .
n , month 123yy .
123 customer sits at a table drawn from the following distribution :
the nested chinese restaurant process
p ( occupied table i| previous customers ) = ni
p ( next unoccupied table| previous customers ) =
where ni is the number of customers currently sitting at table i , and is a real - valued parameter which controls how often , relative to the number of customers in the restaurant , a customer chooses a new table versus sitting with others .
after n customers have been seated , the seating plan gives a partition of those customers as illustrated in figure 123
with an eye towards bayesian statistical applications , we assume that each table is endowed with a parameter vector drawn from a distribution g123
each cus - tomer is associated with the parameter vector at the table at which he sits .
the resulting distribution on sequences of parameter vectors is referred to as a polya urn model ( johnson and kotz 123 ) .
the polya urn distribution can be used to dene a exible clustering model .
let the parameters at the tables index a family of probability distributions ( for example , the distribution might be a multivariate gaussian in which case the parameter would be a mean vector and covariance matrix ) .
associate customers to data points , and draw each data point from the probability distribution associated with the table at which the customer sits .
this induces a probabilistic clustering of the generated data because customers sitting around each table share the same
this model is in the spirit of a traditional mixture model ( titterington et al .
123 ) , but is critically dierent in that the number of tables is unbounded .
data analysis amounts to inverting the generative process to determine a probability distribution on the seating assignment of a data set .
the underlying crp lets the data determine the number of clusters ( i . e . , the number of occupied tables ) and further allows new data to be assigned to new clusters ( i . e . , new tables ) .
123 stick - breaking constructions the dirichlet distribution places a distribution on nonnegative k - dimensional vec - tors whose components sum to one .
in this section we discuss a stochastic process that allows k to be unbounded .
consider a collection of nonnegative real numbers ( i )
i i = 123
we wish to place a probability distribution on such sequences .
given that each such sequence can be viewed as a probability distribution on the positive integers , we obtain a distribution on distributions , i . e . , a random probability distribution .
to do this , we use a stick - breaking construction .
view the interval ( 123 , 123 ) as a unit - length stick .
draw a value v123 from a beta ( 123 , 123 ) distribution and break o a fraction v123 of the stick .
let 123 = v123 denote this rst fragment of the stick and let 123 123 denote the remainder of the stick .
continue this procedure recursively , letting 123 = v123 ( 123 123 ) , and in general dene
i = vi
where ( vi ) are an innite sequence of independent draws from the beta ( 123 , 123 )
journal of the acm , vol .
n , month 123yy .
david m .
blei et al .
distribution .
sethuraman ( 123 ) shows that the resulting sequence ( i ) satises
i i = 123 with probability one .
in the special case 123 = 123 we obtain a one - parameter stochastic process known as the gem distribution ( pitman 123 ) .
let = 123 denote this parameter and denote draws from this distribution as gem ( ) .
large values of skew the beta distribution towards zero and yield random sequences that are heavy - tailed , i . e . , signicant probability tends to be assigned to large integers .
small values of yield random sequences that decay more quickly to zero .
the gem distribution and the crp are closely related .
let gem ( ) and let ( z123 , z123 , .
, zn ) be a sequence of integer - valued variables drawn independently from , i . e . ,
p ( zn = i| ) = i .
this distribution induces a random partition on the integers ( 123 , 123 , .
, n ) , where the partition groups together those indices n whose values of zn are equal .
it can be shown that this distribution on partitions is the same as the distribution on partitions induced by the crp ( pitman 123 ) .
as implied by this result , the gem parameter controls the partition in the same way as the crp parameter .
as with the crp , we can augment the gem distribution to consider draws of parameter vectors .
let ( i ) be an innite sequence of independent draws from a distribution g123 dened on a sample space
where i is an atom at location i and where gem ( ) .
the object g is a distribution on ; it is a random distribution .
consider now a nite partition of .
sethuraman ( 123 ) showed that the prob - ability assigned by g to the cells of this partition follows a dirichlet distribution .
moreover , if we consider all possible nite partitions of , the resulting dirichlet distributions are consistent with each other .
this suggests , by an appeal to the kolmogorov consistency theorem ( billingsley 123 ) , that we can view g as a draw from an underlying stochastic process , where the index set is the set of borel sets of .
although this naive appeal to the kolmogorov consistency theorem runs aground on measure - theoretic diculties , the basic idea is correct and can be made rigorous via a dierent approach ( ferguson 123 ) .
the resulting stochastic process is known as the dirichlet process .
note that if we truncate the stick - breaking process after l123 breaks , we obtain a dirichlet distribution on an l - dimensional vector .
the rst l123 components of this vector manifest the same kind of bias towards larger values for earlier components as the full stick - breaking distribution .
however , the last component l represents the portion of the stick that remains after l123 breaks and has less of a bias toward small values than in the untruncated case .
finally , we will nd it convenient to dene a two - parameter variant of the gem distribution that allows control over both the mean and variance of stick lengths .
we denote this distribution as gem ( m , ) , in which > 123 and m ( 123 , 123 ) .
in this
journal of the acm , vol .
n , month 123yy .
the nested chinese restaurant process
a conguration of the nested chinese restaurant process illustrated to three levels .
each box represents a restaurant with an innite number of tables , each of which refers to a unique table in the next level of the tree .
in this conguration , ve tourists have visited restaurants along four unique paths .
their paths trace a subtree in the innite tree .
( note that the conguration of customers within each restaurant can be determined by observing the restaurants chosen by customers at the next level of the tree . ) in the hlda model of section 123 , each restaurant is associated with a topic distribution .
each document is assumed to choose its words from the topic distributions along a randomly chosen path .
variant , the stick lengths are dened as vi beta ( m , ( 123 m ) ) .
the standard gem ( ) is the special case when m = 123 and = ( 123 m ) .
note that the mean and variance of the standard gem are tied through its single parameter .
the nested chinese restaurant process the chinese restaurant process and related distributions are widely used in bayesian nonparametric statistics because they make it possible to dene statistical models in which observations are assumed to be drawn from an unknown number of classes .
however , this kind of model is limited in the structures that it allows to be expressed in data .
analyzing the richly structured data that are common in computer science requires extending this approach .
in this section we discuss how similar ideas can be used to dene a probability distribution on innitely - deep , innitely - branching trees .
this distribution is subsequently used as a prior distribution in a hierarchical topic model that identies documents with paths down the tree .
a tree can be viewed as a nested sequence of partitions .
we obtain a distribu - tion on trees by generalizing the crp to such sequences .
specically , we dene a nested chinese restaurant process ( ncrp ) by imagining the following scenario for generating a sample .
suppose there are an innite number of innite - table chinese restaurants in a city .
one restaurant is identied as the root restaurant , and on each of its innite tables is a card with the name of another restaurant .
on each of the tables in those restaurants are cards that refer to other restaurants , and this structure repeats innitely many times . 123 each restaurant is referred to exactly once; thus , the restaurants in the city are organized into an innitely - branched ,
123a nite - depth precursor of this model was presented in blei et al .
( 123 ) .
journal of the acm , vol .
n , month 123yy .
david m .
blei et al .
innitely - deep tree .
note that each restaurant is associated with a level in this tree .
the root restaurant is at level 123 , the restaurants referred to on its tables cards are at level 123 , and so on .
a tourist arrives at the city for an culinary vacation .
on the rst evening , he enters the root chinese restaurant and selects a table using the crp distribution in eq .
on the second evening , he goes to the restaurant identied on the rst nights table and chooses a second table using a crp distribution based on the occupancy pattern of the tables in the second nights restaurant .
he repeats this process forever .
after m tourists have been on vacation in the city , the collection of paths describes a random subtree of the innite tree; this subtree has a branching factor of at most m at all nodes .
see figure 123 for an example of the rst three levels from such a random tree .
there are many ways to place prior distributions on trees , and our specic choice is based on several considerations .
first and foremost , a prior distribution combines with a likelihood to yield a posterior distribution , and we must be able to compute this posterior distribution .
in our case , the likelihood will arise from the hierarchical topic model to be described in section 123
as we will show in section 123 , the specic prior that we propose in this section combines with the likelihood to yield a posterior distribution that is amenable to probabilistic inference .
second , we have retained important aspects of the crp , in particular the preferential attachment dynamics that are built into eq .
probability structures of this form have been used as models in a variety of applications ( barabasi and reka 123; krapivsky and redner 123; albert and barabasi 123; drinea et al .
123 ) , and the clustering that they induce makes them a reasonable starting place for a hierarchical topic model .
in fact , these two points are intimately related .
the crp yields an exchangeable distribution across partitions , i . e . , the distribution is invariant to the order of the arrival of customers ( pitman 123 ) .
this exchangeability property makes crp - based models amenable to posterior inference using monte carlo methods ( escobar and west 123; maceachern and muller 123; neal 123 ) .
the ncrp is closely related to a stochastic process known as the nested dirichlet process ( ndp ) , which has been proposed independently of our work by ( rodrguez et al .
indeed , just as the crp can be obtained be obtained by integrating out the dirichlet process ( blackwell and macqueen 123 ) , a k - level ncrp can be obtained by integrating out the dirichlet processes in a k - level ndp .
hierarchical latent dirichlet allocation the nested crp provides a way to dene a prior on tree topologies that does not limit the branching factor or depth of the trees .
we can use this distribution as a component of a probabilistic topic model .
the goal of topic modeling is to identify subsets of words that tend to co - occur within documents .
some of the early work on topic modeling derived from latent semantic analysis , an application of the singular value decomposition in which top - ics are viewed post hoc as the basis of a low - dimensional subspace ( deerwester et al .
subsequent work treated topics as probability distributions over words and used likelihood - based methods to estimate these distributions from a corpus ( hof - mann 123b ) .
in both of these approaches , the interpretation of topic diers in
journal of the acm , vol .
n , month 123yy .
the nested chinese restaurant process
key ways from the clustering metaphor because the same word can be given high probability ( or weight ) under multiple topics .
this gives topic models the capabil - ity to capture notions of polysemy ( e . g . , bank can occur with high probability in both a nance topic and a waterways topic ) .
probabilistic topic models were given a fully bayesian treatment in the latent dirichlet allocation ( lda ) model ( blei et al .
123 ) .
topic models such as lda treat topics as a at set of probability distributions , with no direct relationship between one topic and another .
while these models can be used to recover a set of topics from a corpus , they fail to indicate the level of abstraction of a topic , or how the various topics are related .
the model that we present in this section builds on the ncrp to dene a hierarchical topic model .
this model arranges the topics into a tree , with the desideratum that more general topics should appear near the root and more specialized topics should appear near the leaves ( hofmann 123a ) .
having dened such a model , we use probabilistic inference to simultaneously identify the topics and the relationships between them .
our approach to dening a hierarchical topic model is based on identifying doc - uments with the paths generated by the ncrp .
we augment the ncrp in two ways to obtain a generative model for documents .
first , we associate a topic , i . e . , a probability distribution across words , with each node in the tree .
a path in the tree thus picks out an innite collection of topics .
second , given a choice of path , we use the gem distribution to dene a probability distribution on the topics along this path .
given a draw from a gem distribution , a document is generated by repeatedly selecting topics according to the probabilities dened by that draw , and then drawing each word from the probability distribution dened by its selected
more formally , consider the innite tree dened by the ncrp and let cd denote the path through that tree for the dth customer ( i . e . , document ) .
in the hierarchi - cal lda ( hlda ) model , the documents in a corpus are assumed drawn from the following generative process : ( 123 ) for each table k t in the innite tree ,
( a ) draw a topic k dirichlet ( ) .
( 123 ) for each document , d ( 123 , 123 ,
( a ) draw cd ncrp ( ) .
( b ) draw a distribution over levels in the tree , d | ( m , ) gem ( m , ) .
( c ) for each word ,
choose level zd , n | d discrete ( d ) .
choose word wd , n | ( zd , n , cd , ) discrete ( cd ( zd , n ) ) , which is param -
eterized by the topic in position zd , n on the path cd .
here we use z discrete ( ) to denote the discrete distribution that sets z = i with probability i .
this generative process denes a probability distribution across
the goal of nding a topic hierarchy at dierent levels of abstraction is distinct from the problem of hierarchical clustering ( zamir and etzioni 123; larsen and aone 123; vaithyanathan and dom 123; duda et al .
123; hastie et al .
123; heller and ghahramani 123 ) .
hierarchical clustering treats each data point as a
journal of the acm , vol .
n , month 123yy .
david m .
blei et al .
leaf in a tree , and merges similar data points up the tree until all are merged into a root node .
thus , internal nodes represent summaries of the data below which , in this setting , would yield distributions across words that share high probability words with their children .
in the hierarchical topic model , the internal nodes are not summaries of their children .
rather , the internal nodes reect the shared terminology of the documents assigned to the paths that contain them .
this can be seen in figure 123 , where the high probability words of a node are distinct from the high probability words of its
it is important to emphasize that our approach is an unsupervised learning ap - proach in which the probabilistic components that we have dened are latent vari - ables .
that is , we do not assume that topics are predened , nor do we assume that the nested partitioning of documents or the allocation of topics to levels are pre - dened .
we infer these entities from a bayesian computation in which a posterior distribution is obtained from conditioning on a corpus and computing probabilities for all latent variables .
as we will see experimentally , there is statistical pressure in the posterior to place more general topics near the root of the tree and to place more specialized topics further down in the tree .
to see this , note that each path in the tree includes the root node .
given that the gem distribution tends to assign relatively large prob - abilities to small integers , there will be a relatively large probability for documents to select the root node when generating words .
therefore , to explain an observed corpus , the topic at the root node will place high probability on words that are useful across all the documents .
moving down in the tree , recall that each document is assigned to a single path .
thus , the rst level below the root induces a coarse partition on the documents , and the topics at that level will place high probability on words that are useful within the corresponding subsets .
as we move still further down , the nested partitions of documents become ner .
consequently , the corresponding topics will be more specialized to the particular documents in those paths .
we have presented the model as a two - phase process : an innite set of topics are generated and assigned to all of the nodes of an innite tree , and then documents are obtained by selecting nodes in the tree and drawing words from the corresponding topics .
it is also possible , however , to conceptualize a lazy procedure in which a topic is generated only when a node is rst selected .
in particular , consider an empty tree ( i . e . , containing no topics ) and consider generating the rst document .
we select a path and then repeatedly select nodes along that path in order to generate words .
a topic is generated at a node when that node is rst selected and subsequent selections of the node reuse the same topic .
after n words have been generated , at most n nodes will have been visited and at most n topics will have been generated .
the ( n + 123 ) th word in the document can come from one of previously generated topics or it can come from a new topic .
similarly , suppose that d documents have previously been generated .
the ( d + 123 ) th document can follow one of the paths laid down by an earlier document and select only old topics , or it can branch o at any point in the tree and generate new topics along the new branch .
journal of the acm , vol .
n , month 123yy .
the nested chinese restaurant process
this discussion highlights the nonparametric nature of our model .
rather than describing a corpus by using a probabilistic model involving a xed set of param - eters , our model assumes that the number of parameters can grow as the corpus grows , both within documents and across documents .
new documents can spark new subtopics or new specializations of existing subtopics .
given a corpus , this ex - ibility allows us to use approximate posterior inference to discover the particular tree of topics that best describes its documents .
it is important to note that even with this exibility , the model still makes as - its size , shape , and character will be aected by the sumptions about the tree .
settings of the hyperparameters .
the most inuential hyperparameters in this re - gard are the dirichlet parameter for the topics and the stick - breaking parameters for the topic proportions ( m , ) .
the dirichlet parameter controls the sparsity of the topics; smaller values of will lead to topics with most of their probability mass on a small set of words .
with a prior bias to sparser topics , the posterior will pre - fer more topics to describe a collection and thus place higher probability on larger trees .
the stick - breaking parameters control how many words in the documents are likely to come from topics of varying abstractions .
if we set m to be large ( e . g . , m = 123 ) then the posterior will more likely assign more words from each document to higher levels of abstraction .
setting to be large ( e . g . , = 123 ) means that word allocations will not likely deviate from such a setting .
how we set these hyperparameters depends on the goal of the analysis .
when we analyze a document collection with hlda for discovering and visualizing a hierarchy embedded within it , we might examine various settings of the hyperparameters to nd a tree that meets our exploratory needs .
we analyze documents with this purpose in mind in section 123 .
in a dierent setting , when we are looking for a good predictive model of the data , e . g . , to compare hlda to other statistical models of text , then it makes sense to t the hyperparameters by placing priors on them and computing their posterior .
we describe posterior inference for the hyperparameters in section 123 and analyze documents using this approach in section 123 .
finally , we note that hlda is the simplest model that exploits the nested crp , i . e . , a exible hierarchy of distributions , in the topic modeling framework .
in a more complicated model , one could consider a variant of hlda where each document exhibits multiple paths through the tree .
this can be modeled using a two - level distribution for word generation : rst choose a path through the tree , and then choose a level for the word .
recent extensions to topic models can also be adapted to make use of a exible topic hierarchy .
as examples , in the dynamic topic model the documents are time stamped and the underlying topics change over time ( blei and laerty 123 ) ; in the author - topic model the authorship of the documents aects which topics they ex - hibit ( rosen - zvi et al .
this said , some extensions are more easily adaptable than others .
in the correlated topic model , the topic proportions exhibit a covari - ance structure ( blei and laerty 123 ) .
this is achieved by replacing a dirichlet distribution with a logistic normal , and the application of bayesian nonparametric extensions is less direct .
journal of the acm , vol .
n , month 123yy .
david m .
blei et al .
123 related work in previous work , researchers have developed a number of methods that employ hierarchies in analyzing text data .
in one line of work , the algorithms are given a hierarchy of document categories , and their goal is to correctly place documents within it ( koller and sahami 123; chakrabarti et al .
123; mccallum et al .
123; dumais and chen 123 ) .
other work has focused on deriving hierarchies of indi - vidual terms using side information , such as a grammar or a thesaurus , that are sometimes available for text domains ( sanderson and croft 123; stoica and hearst 123; cimiano et al .
123 ) .
our method provides still another way to employ a notion of hierarchy in text analysis .
first , rather than learn a hierarchy of terms we learn a hierarchy of topics , where a topic is a distribution over terms that describes a signicant pattern of word co - occurrence in the data .
moreover , while we focus on text , a topic is simply a data - generating distribution; we do not rely on any text - specic side information such as a thesaurus or grammar .
thus , by using other data types and distributions , our methodology is readily applied to biological data sets , purchasing data , collections of images , or social network data .
( note that applications in such domains have already been demonstrated for at topic models ( pritchard et al .
123; marlin 123; fei - fei and perona 123; blei and jordan 123; airoldi et al .
123 ) . ) finally , as a bayesian nonparametric model , our approach can accommodate future data that might lie in new and previously undiscovered parts of the tree .
previous work commits to a single xed tree for all future data .
probabilistic inference with the hlda model in hand , our goal is to perform posterior inference , i . e . , to invert the generative process of documents described above for estimating the hidden topical structure of a document collection .
we have constructed a joint distribution of hidden variables and observationsthe latent topic structure and observed documentsby combining prior expectations about the kinds of tree topologies we will encounter with a generative process for producing documents given a particular topology .
we are now interested in the distribution of the hidden structure conditioned on having seen the data , i . e . , the distribution of the underlying topic structure that might have generated an observed collection of documents .
finding this posterior distribution for dierent kinds of data and models is a central problem in bayesian statistics .
see bernardo and smith ( 123 ) and gelman et al .
( 123 ) for general introductions to bayesian statistics .
in our nonparametric setting , we must nd a posterior distribution on countably innite collections of objectshierarchies , path assignments , and level allocations of wordsgiven a collection of documents .
moreover , we need to be able to do this using the nite resources of the computer .
not surprisingly , the posterior distribu - tion for hlda is not available in closed form .
we must appeal to an approximation .
we develop a markov chain monte carlo ( mcmc ) algorithm to approximate the posterior for hlda .
in mcmc , one samples from a target distribution on a set of variables by constructing a markov chain that has the target distribution as its stationary distribution ( robert and casella 123 ) .
one then samples from the chain for suciently long that it approaches the target , collects the sampled states
journal of the acm , vol .
n , month 123yy .
the nested chinese restaurant process
thereafter , and uses those collected states to estimate the target .
this approach is particularly straightforward to apply to latent variable models , where we take the state space of the markov chain to be the set of values that the latent variables can take on , and the target distribution is the conditional distribution of these latent variables given the observed data .
the particular mcmc algorithm that we present in this paper is a gibbs sampling algorithm ( geman and geman 123; gelfand and smith 123 ) .
in a gibbs sampler each latent variable is iteratively sampled conditioned on the observations and all the other latent variables .
we employ collapsed gibbs sampling ( liu 123 ) , in which we marginalize out some of the latent variables to speed up the convergence of the chain .
collapsed gibbs sampling for topic models ( griths and steyvers 123 ) has been widely used in a number of topic modeling applications ( mccallum et al .
123; rosen - zvi et al .
123; mimno and mccallum 123; dietz et al .
123; newman et al .
in hlda , we sample the per - document paths cd and the per - word level allocations to topics in those paths zd , n .
we marginalize out the topic parameters i and the per - document topic proportions d .
the state of the markov chain is illustrated , for a single document , in figure 123
( the particular assignments illustrated in the gure are taken at the approximate mode of the hlda model posterior conditioned on abstracts from the jacm . ) thus , we approximate the posterior p ( c123 : d , z123 : d | , , m , , w123 : d ) .
the hyper - parameter reects the tendency of the customers in each restaurant to share tables , reects the expected variance of the underlying topics ( e . g , ( cid : 123 ) 123 will tend to choose topics with fewer high - probability words ) , and m and reect our expectation about the allocation of words to levels within a document .
the hy - perparameters can be xed according to the constraints of the analysis and prior expectation about the data , or inferred as described in section 123 .
intuitively , the crp parameter and topic prior provide control over the size of the inferred tree .
for example , a model with large and small will tend to nd a tree with more topics .
the small encourages fewer words to have high probability in each topic; thus , the posterior requires more topics to explain the data .
the large increases the likelihood that documents will choose new paths when traversing the nested crp .
the gem parameter m reects the proportion of general words relative to specic words , and the gem parameter reects how strictly we expect the documents to adhere to these proportions .
a larger value of enforces the notions of generality and specicity that lead to more interpretable trees .
the remainder of this section is organized as follows .
first , we outline the two main steps in the algorithm : the sampling of level allocations and the sampling of path assignments .
we then combine these steps into an overall algorithm .
next , we present prior distributions for the hyperparameters of the model and describe posterior inference for the hyperparameters .
finally , we outline how to assess the convergence of the sampler and approximate the mode of the posterior distribution .
123 sampling level allocations given the current path assignments , we need to sample the level allocation variable zd , n for word n in document d from its distribution given the current values of all journal of the acm , vol .
n , month 123yy .
david m .
blei et al .
a single state of the markov chain in the gibbs sampler for the abstract of a new approach to the maximum - ow problem ( goldberg and tarjan , 123 ) .
the document is associated with a path through the hierarchy cd , and each node in the hierarchy is associated with a distribution over terms .
( the ve most probable terms are illustrated . ) finally , each word in the abstract wd , n is associated with a level in the path through the hierarchy zd , n , with 123 being the highest level and 123 being the lowest .
the gibbs sampler iteratively draws cd and zd , n for all words in all documents ( see section 123 ) .
p ( zd , n | z ( d , n ) , c , w , m , , ) p ( zd , n | zd , n , m , ) p ( wd , n | z , c , w ( d , n ) , ) ,
where z ( d , n ) and w ( d , n ) are the vectors of level allocations and observed words leaving out zd , n and wd , n respectively .
we will use similar notation whenever items are left out from an index set; for example , zd , n denotes the level allocations in document d , leaving out zd , n .
journal of the acm , vol .
n , month 123yy .
the , ofa , isandnalgorithmtimelogproblemprogramslanguagelanguagessetsprogramgraphgraphsverticesedgeedgesfunctionsfunctionpolynomialegrtreestreesearchregularstringcdzd , nwd , n the nested chinese restaurant process
the rst term in eq .
( 123 ) is a distribution over levels .
this distribution has an innite number of components , so we sample in stages .
first , we sample from the distribution over the space of levels that are currently represented in the rest of the document , i . e . , max ( zd , n ) , and a level deeper than that level .
the rst components of this distribution are , for k max ( zd , n ) ,
p ( zd , n = k | zd , n , m , ) = e
( 123 vj ) | zd , n , m ,
= e ( vk | zd , n , m , )
e ( 123 vj | zd , n , m , )
= m + # ( zd , n = k ) + # ( zd , n k )
( 123 m ) + # ( zd , n > j )
+ # ( zd , n j )
where # ( ) counts the elements of an array satisfying a given condition .
the second term in eq .
( 123 ) is the probability of a given word based on a possible assignment .
from the assumption that the topic parameters i are generated from a symmetric dirichlet distribution with hyperparameter we obtain p ( wd , n | z , c , w ( d , n ) , ) # ( z ( d , n ) = zd , n , czd , n = cd , zd , n , w ( d , n ) = wd , n ) + ( 123 ) which is the smoothed frequency of seeing word wd , n allocated to the topic at level zd , n of the path cd .
the last component of the distribution over topic assignments is
p ( zd , n > max ( zd , n ) | zd , n , w , m , , ) = 123
p ( zd , n = j | zd , n , w , m , , ) .
if the last component is sampled then we sample from a bernoulli distribution for increasing values of ( cid : 123 ) , starting with ( cid : 123 ) = max ( zd , n ) + 123 , until we determine zd , n , p ( zd , n = ( cid : 123 ) | zd , n , zd , n > ( cid : 123 ) 123 , w , m , , ) = ( 123 m ) p ( wd , n | z , c , w ( d , n ) , )
p ( zd , n > ( cid : 123 ) | zd , n , zd , n > ( cid : 123 ) 123 ) = 123 p ( zd , n = ( cid : 123 ) | zd , n , zd , n > ( cid : 123 ) 123 , w , m , , ) .
note that this changes the maximum level when resampling subsequent level as -
123 sampling paths given the level allocation variables , we need to sample the path associated with each document conditioned on all other paths and the observed words .
we appeal to the fact that max ( zd ) is nite , and are only concerned with paths of that length : this expression is an instance of bayess theorem with p ( wd | c , wd , z , ) as the probability of the data given a particular choice of path , and p ( cd | cd , ) as the prior on paths implied by the nested crp .
the probability of the data is obtained by integrating over the multinomial parameters , which gives a ratio of normalizing
p ( cd | w , cd , z , , ) p ( cd | cd , ) p ( wd | c , wd , z , ) .
journal of the acm , vol .
n , month 123yy .
david m .
blei et al .
constants for the dirichlet distribution , p ( wd | c , wd , z , ) =
w # ( zd = ( cid : 123 ) , cd , ( cid : 123 ) = cd , ( cid : 123 ) , wd = w ) + v
w ( # ( zd = ( cid : 123 ) , cd , ( cid : 123 ) = cd , ( cid : 123 ) , wd = w ) + )
w ( # ( z = ( cid : 123 ) , c ( cid : 123 ) = cd , ( cid : 123 ) , w = w ) + )
w # ( z = ( cid : 123 ) , c ( cid : 123 ) = cd , ( cid : 123 ) , w = w ) + v ,
where we use the same notation for counting over arrays of variables as above .
note that the path must be drawn as a block , because its value at each level depends on its value at the previous level .
the set of possible paths corresponds to the union of the set of existing paths through the tree , each represented by a leaf , with the set of possible novel paths , each represented by an internal node .
123 summary of gibbs sampling algorithm with these conditional distributions in hand , we specify the full gibbs sampling algorithm .
given the current state of the sampler , ( c ( t ) 123 : d ) , we iteratively sample each variable conditioned on the rest .
( 123 ) for each document d ( 123 ,
( a ) randomly draw c ( t+123 ) ( b ) randomly draw z ( t+123 )
from eq .
from eq .
( 123 ) for each word , n ( 123 ,
the stationary distribution of the corresponding markov chain is the conditional distribution of the latent variables in the hlda model given the corpus .
after run - ning the chain for suciently many iterations that it can approach its stationary distribution ( the burn - in ) we can collect samples at intervals selected to mini - mize autocorrelation , and approximate the true posterior with the corresponding
although this algorithm is guaranteed to converge in the limit , it is dicult to say something more denitive about the speed of the algorithm independent of the data being analyzed .
in hlda , we sample a path from the tree for each document cd and a level assignment for each word zd , n .
as described above , the number of items from which each is sampled depends on the current state of the hierarchy and other level assignments in the document .
two data sets of equal size may induce dierent trees and yield dierent running times for each iteration of the sampler .
for the corpora analyzed below in section 123 , the gibbs sampler averaged 123 seconds per document for the jacm data and psychological review data , and 123 seconds per document for the proceedings of the national academy of sciences data . 123
123 sampling the hyperparameters the values of hyperparameters are generally unknown a priori .
we include them in the inference process by endowing them with prior distributions ,
m beta ( 123 , 123 )
123timings were measured with the gibbs sampler running on a 123ghz opteron 123 processor .
journal of the acm , vol .
n , month 123yy .
the nested chinese restaurant process
( left ) the complete log likelihood of eq .
( 123 ) for the rst 123 iterations of the gibbs sampler run on the jacm corpus of section 123 .
( right ) the autocorrelation function ( acf ) of the log complete log likelihood ( with condence interval ) for the remaining 123 iterations .
the autocorrelation decreases rapidly as a function of the lag between samples .
these priors also contain parameters ( hyper - hyperparameters ) , but the resulting inferences are less inuenced by these hyper - hyperparameters than they are by xing the original hyperparameters to specic values ( bernardo and smith 123 ) .
to incorporate this extension into the gibbs sampler , we interleave metropolis - hastings ( mh ) steps between iterations of the gibbs sampler to obtain new values of m , , , and .
this preserves the integrity of the markov chain , although it may mix slower than the collapsed gibbs sampler without the mh updates ( robert and casella 123 ) .
123 assessing convergence and approximating the mode practical applications must address the issue of approximating the mode of the dis - tribution on trees and assessing convergence of the markov chain .
we can obtain information about both by examining the log probability of each sampled state .
for a particular sample , i . e . , a conguration of the latent variables , we compute the log probability of that conguration and observations , conditioned on the hy -
l ( t ) = log p ( c ( t )
123 : d , w123 : d | , , m , ) .
with this statistic , we can approximate the mode of the posterior by choosing the state with the highest log probability .
moreover , we can assess convergence of the chain by examining the autocorrelation of l ( t ) .
figure 123 ( right ) illustrates the autocorrelation as a function of the number of iterations between samples ( the lag ) when modeling the jacm corpus described in section 123 .
the chain was run for 123 , 123 iterations; 123 iterations were discarded as burn - in .
figure 123 ( left ) illustrates eq .
( 123 ) for the burn - in iterations .
gibbs samplers stochastically climb the posterior distribution surface to nd an area of high pos -
journal of the acm , vol .
n , month 123yy .
123log complete probability for the jacm corpusiterationlog complete probability123 . 123 . 123 . 123lagacfautocorrelation function for the jacm corpus
david m .
blei et al .
terior probability , and then explore its curvature through sampling .
in practice , one usually restarts this procedure a handful of times and chooses the local mode which has highest posterior likelihood ( robert and casella 123 ) .
despite the lack of theoretical guarantees , gibbs sampling is appropriate for the kind of data analysis for which hlda and many other latent variable models are tailored .
rather than try to understand the full surface of the posterior , the goal of latent variable modeling is to nd a useful representation of complicated high - dimensional data , and a local mode of the posterior found by gibbs sampling often provides such a representation .
in the next section , we will assess hlda qualitatively , through visualization of summaries of the data , and quantitatively , by using the latent variable representation to provide a predictive model of text .
examples and empirical results we present experiments analyzing both simulated and real text data to demonstrate the application of hlda and its corresponding gibbs sampler .
123 analysis of simulated data in figure 123 , we depict the hierarchies and allocations for ten simulated data sets drawn from an hlda model .
for each data set , we draw 123 documents of 123 words each .
the vocabulary size is 123 , and the hyperparameters are xed at = . 123 , and = 123
in these simulations , we truncated the stick - breaking procedure at three levels , and simply took a dirichlet distribution over the proportion of words allocated to those levels .
the resulting hierarchies shown in figure 123 illustrate the range of structures on which the prior assigns probability .
in the same gure , we illustrate the estimated mode of the posterior distribution across the hierarchy and allocations for the ten data sets .
we exactly recover the correct hierarchies , with only two errors .
in one case , the error is a single wrongly allocated path .
in the other case , the inferred mode has higher posterior probability than the true tree structure ( due to nite data ) .
in general we cannot expect to always nd the exact tree .
this is dependent on the size of the data set , and how identiable the topics are .
our choice of small yields topics that are relatively sparse and ( probably ) very dierent from each other .
trees will not be as easy to identify in data sets which exhibit polysemy and similarity between topics .
123 hierarchy discovery in scientic abstracts given a document collection , one is typically interested in examining the underlying tree of topics at the mode of the posterior .
as described above , our inferential procedure yields a tree structure by assembling the unique subset of paths contained in ( c123 , .
, cd ) at the approximate mode of the posterior .
for a given tree , we can examine the topics that populate the tree .
given the assignment of words to levels and the assignment of documents to paths , the prob - ability of a particular word at a particular node is roughly proportional to the number of times that word was generated by the topic at that node .
more specif - ically , the mean probability of a word w in a topic at level ( cid : 123 ) of path p is given
journal of the acm , vol .
n , month 123yy .
the nested chinese restaurant process
inferring the mode of the posterior hierarchy from simulated data .
see section 123 .
journal of the acm , vol .
n , month 123yy .
123true dataset hierarchyposterior modetrue dataset hierarchyposterior mode
david m .
blei et al .
p ( w | z , c , w , ) =
# ( z = ( cid : 123 ) , c = p , w = w ) +
# ( z = ( cid : 123 ) , c = p ) + v
using these quantities , the hlda model can be used for analyzing collections of scientic abstracts , recovering the underlying hierarchical structure appropriate to a collection , and visualizing that hierarchy of topics for a better understanding of the structure of the corpora .
we demonstrate the analysis of three dierent collections of journal abstracts under hlda .
in these analyses , as above , we truncate the stick - breaking procedure at three levels , facilitating visualization of the results .
the topic dirichlet hyperparameters were xed at = ( 123 , 123 , 123 ) , which encourages many terms in the high - level distributions , fewer terms in the mid - level distributions , and still fewer terms in the low - level distributions .
the nested crp parameter was xed at 123 .
the gem parameters were xed at = 123 and m = 123 .
this strongly biases the level proportions to place more mass at the higher levels of the hierarchy .
in figure 123 , we illustrate the approximate posterior mode of a hierarchy estimated from a collection of 123 abstracts from the jacm .
the tree structure illustrates the ensemble of paths assigned to the documents .
in each node , we illustrate the top ve words sorted by expected posterior probability , computed from eq .
several leaves are annotated with document titles .
for each leaf , we chose the ve documents assigned to its path that have the highest numbers of words allocated to the bottom level .
the model has found the function words in the data set , assigning words like the , of , or , and and to the root topic .
in its second level , the posterior hierarchy appears to have captured some of the major subelds in computer sci - ence , distinguishing between databases , algorithms , programming languages and networking .
in the third level , it further renes those elds .
for example , it delin - eates between the verication area of networking and the queuing area .
in figure 123 , we illustrate an analysis of a collection of 123 , 123 psychology abstracts from psychological review from 123 to 123
again , we have discovered an under - lying hierarchical structure of the eld .
the top node contains the function words; the second level delineates between large subelds such as behavioral , social and cognitive psychology; the third level further renes those subelds .
finally , in figure 123 , we illustrate a portion of the analysis of a collection of 123 , 123 abstracts from the proceedings of the national academy of sciences from 123 to 123
an underlying hierarchical structure of the content of the journal has been discovered , dividing articles into groups such as neuroscience , immunology , population genetics and enzymology .
in all three of these examples , the same posterior inference algorithm with the same hyperparameters yields very dierent tree structures for dierent corpora .
models of xed tree structure force us to commit to one in advance of seeing the data .
the nested chinese restaurant process at the heart of hlda provides a exible solution to this dicult problem .
journal of the acm , vol .
n , month 123yy .
the nested chinese restaurant process
a portion of the hierarchy learned from the 123 , 123 abstracts of psychological review from 123
the vocabulary was restricted to the 123 , 123 terms that occurred in more than ve documents , yielding a corpus of 123k words .
the learned hierarchy , of which only a portion is illustrated , contains 123 topics .
journal of the acm , vol .
n , month 123yy .
david m .
blei et al .
a portion of the hierarchy learned from the 123 , 123 abstracts of the proceedings of the national academy of sciences from 123
the vocabulary was restricted to the 123 , 123 terms that occurred in more than ve documents , yielding a corpus of 123m words .
the learned hierarchy , of which only a portion is illustrated , contains 123 topics .
note that the parameter is xed at a smaller value , to provide a reasonably sized topic hierarchy with the signicantly larger corpus .
journal of the acm , vol .
n , month 123yy .
theofinandastructurefoldingstatestructuresreactionneuronsbrainneuronalcortexmemoryspeciesevolutiongeneticpopulationspopulationenzymebiosynthesisacidcolisynthasetcdcellsantigenilenergytimemethodtheoryuorescenceresidueshelixenzymecatalyticsitegenesgenomesequencesgenomeslocidnareplicationpolymerasestrandrecombinationsynapticreceptorsglutamategabacaclimateglobalcarbonfossilyearsplantsplantarabidopsisleaveshspvisualcortextaskauditorystimulustcrclasshlankmhchivvirusviralinfectionccrironfeoxygenohemehostvirulenceparasitemalariaparasitestumorprostateantibodyanticancermalefemalemalesfemalessexualpaxfgfretinalrodphotoreceptorapppsnotchamyloidalzheimercholesterolrararsterolldlglobinprpprionhdgatacapsidicamcrystallinheparinfnperoxisomalureachopexptsapoxationplasminogenhtapu the nested chinese restaurant process
123 comparison to lda in this section we present experiments comparing hlda to its non - hierarchical pre - cursor , lda .
we use the innite - depth hlda model; the per - document distribution over levels is not truncated .
we use predictive held - out likelihood to compare the two approaches quantitatively , and we present examples of lda topics in order to provide a qualitative comparison of the methods .
lda has been shown to yield good predictive performance relative to competing unigram language models , and it has also been argued that the topic - based analysis provided by lda represents a qualitative improvement on competing language models ( blei et al .
123; griths and steyvers 123 ) .
thus lda provides a natural point of comparison .
there are several issues that must be borne in mind in comparing hlda to lda .
first , in lda the number of topics is a xed parameter , and a model selection procedure is required to choose the number of topics .
( a bayesian nonparametric solution to this can be obtained with the hierarchical dirichlet process ( teh et al .
123 ) . ) second , given a set of topics , lda places no constraints on the usage of the topics by documents in the corpus; a document can place an arbitrary probability distribution on the topics .
in hlda , on the other hand , a document can only access the topics that lie along a single path in the tree .
in this sense , lda is signicantly more exible than hlda .
this exibility of lda implies that for large corpora we can expect lda to dom - inate hlda in terms of predictive performance ( assuming that the model selection problem is resolved satisfactorily and assuming that hyperparameters are set in a manner that controls overtting ) .
thus , rather than trying to simply optimize for predictive performance within the hlda family and within the lda family , we have instead opted to rst run hlda to obtain a posterior distribution over the number of topics , and then to conduct multiple runs of lda for a range of topic cardinalities bracketing the hlda result .
this provides an hlda - centric assess - ment of the consequences ( for predictive performance ) of using a hierarchy versus a at model .
we used predictive held - out likelihood as a measure of performance .
the pro - cedure is to divide the corpus into d123 observed documents and d123 held - out doc - uments , and approximate the conditional probability of the held - out set given the
where m represents a model , either lda or hlda .
we employed collapsed gibbs sampling for both models and integrated out all the hyperparameters with priors .
we used the same prior for those hyperparameters that exist in both models .
, wheld - out
, wobs
to approximate this predictive quantity , we run two samplers .
first , we collect 123 samples from the posterior distribution of latent variables given the observed documents , taking samples 123 iterations apart and using a burn - in of 123 samples .
for each of these outer samples , we collect 123 samples of the latent variables given the held - out documents and approximate their conditional probability given the outer sample with the harmonic mean ( kass and raftery 123 ) .
finally , these conditional probabilities are averaged to obtain an approximation to eq
figure 123 illustrates the ve - fold cross - validated held - out likelihood for hlda and lda on the jacm corpus .
the gure also provides a visual indication of the
journal of the acm , vol .
n , month 123yy .
david m .
blei et al .
the held - out predictive log likelihood for hlda compared to the same quantity for lda as a function of the number of topics .
the shaded blue region is centered at the mean number of topics in the hierarchies found by hlda ( and has width equal to twice the standard error ) .
the ve most probable words for each of ten randomly chosen topics from an lda model t to fty topics .
mean and variance of the posterior distribution over topic cardinality for hlda; the mode is approximately a hierarchy with 123 topics .
for lda , we plot the predictive likelihood in a range of topics around this value .
we see that at each xed topic cardinality in this range of topics , hlda pro - vides signicantly better predictive performance than lda .
as discussed above , we eventually expect lda to dominate hlda for large numbers of topics .
in a large range near the hlda mode , however , the constraint that documents pick topics along single paths in a hierarchy yields superior performance .
this suggests that
journal of the acm , vol .
n , month 123yy .
number of topicsmean heldout log likelihood123lllllllllllllllhldaldaofobjectstoand thetheofatowetheof aisinmethodstheaofproblemsktheofalgorithmforforthe linear problem problemstheofweandatheandoftothatoperationsthefunctionalrequiresandtheofaisin the nested chinese restaurant process
the hierarchy is useful not only for interpretation , but also for capturing predictive
to give a qualitative sense of the relative degree of interpretability of the topics that are found using the two approaches , figure 123 illustrates ten lda topics chosen randomly from a 123 - topic model .
as these examples make clear , the lda topics are generally less interpretable than the hlda topics .
in particular , function words are given high probability throughout .
in practice , to sidestep this issue , corpora are often stripped of function words before tting an lda model .
while this is a reasonable ad - hoc solution for ( english ) text , it is not a general solution that can be used for non - text corpora , such as visual scenes .
even more importantly , there is no notion of abstraction in the lda topics .
the notion of multiple levels of abstraction requires a model such as hlda .
in summary , if interpretability is the goal , then there are strong reasons to prefer hlda to lda .
if predictive performance is the goal , then hlda may well remain the preferred method if there is a constraint that a relatively small number of topics should be used .
when there is no such constraint , lda may be preferred .
these comments also suggest , however , that an interesting direction for further research is to explore the feasibility of a model that combines the dening features of the lda and hlda models .
as we described in section 123 , it may be desirable to consider an hlda - like hierarchical model that allows each document to exhibit multiple paths along the tree .
this might be appropriate for collections of long documents , such as full - text articles , which tend to be more heterogeneous than short abstracts .
in this paper , we have shown how the nested chinese restaurant process can be used to dene prior distributions on recursive data structures .
we have also shown how this prior can be combined with a topic model to yield a bayesian nonpara - metric methodology for analyzing document collections in terms of hierarchies of topics .
given a collection of documents , we use mcmc sampling to learn an un - derlying thematic structure that provides a useful abstract representation for data visualization and summarization .
we emphasize that no knowledge of the topics of the collection or the structure of the tree are needed to infer a hierarchy from data .
we have demonstrated our methods on collections of abstracts from three dierent scientic journals , showing that while the content of these dierent domains can vary signicantly , the sta - tistical principles behind our model make it possible to recover meaningful sets of topics at multiple levels of abstraction , and organized in a tree .
the bayesian nonparametric framework underlying our work makes it possible to dene probability distributions and inference procedures over countably innite collections of objects .
there has been other recent work in articial intelligence in which probability distributions are dened on innite objects via concepts from rst - order logic ( milch et al .
123; pasula and russell 123; poole 123 ) .
while pro - viding an expressive language , this approach does not necessarily yield structures that are amenable to ecient posterior inference .
our approach reposes instead on combinatorial structurethe exchangeability of the dirichlet process as a distribu - tion on partitionsand this leads directly to a posterior inference algorithm that
journal of the acm , vol .
n , month 123yy .
david m .
blei et al .
can be applied eectively to large - scale learning problems .
the hlda model draws on two complementary insightsone from statistics , the other from computer science .
from statistics , we take the idea that it is possible to work with general stochastic processes as prior distributions , thus accommodating latent structures that vary in complexity .
this is the key idea behind bayesian non - parametric methods .
in recent years , these models have been extended to include spatial models ( duan et al .
123 ) and grouped data ( teh et al .
123 ) , and bayesian nonparametric methods now enjoy new applications in computer vision ( sudderth et al .
123 ) , bioinformatics ( xing et al .
123 ) , and natural language processing ( li et al .
123; teh et al .
123; goldwater et al .
123b; 123a; johnson et al .
123; liang et al .
123 ) .
from computer science , we take the idea that the representations we infer from data should be richly structured , yet admit ecient computation .
this is a growing theme in bayesian nonparametric research .
for example , one line of recent research has explored stochastic processes involving multiple binary features rather than clusters ( griths and ghahramani 123; thibaux and jordan 123; teh et al .
a parallel line of investigation has explored alternative posterior inference techniques for bayesian nonparametric models , providing more ecient algorithms for extracting this latent structure .
specically , variational methods , which replace sampling with optimization , have been developed for dirichlet process mixtures to further increase their applicability to large - scale data analysis problems ( blei and jordan 123; kurihara et al .
123 ) .
the hierarchical topic model that we explored in this paper is just one example of how this synthesis of statistics and computer science can produce powerful new tools for the analysis of complex data .
however , this example showcases the two major strengths of the bayesian nonparametric approach .
first , the use of the nested crp means that the model does not start with a xed set of topics or hypotheses about their relationship , but grows to t the data at hand .
thus , we learn a topology but do not commit to it; the tree can grow as new documents about new topics and subtopics are observed .
second , despite the fact that this results in a very rich hypothesis space , containing trees of arbitrary depth and branching factor , it is still possible to perform approximate probabilistic inference using a simple algorithm .
this combination of exible , structured representations and ecient inference makes nonparametric bayesian methods uniquely promising as a formal framework for learning with exible data structures .
we thank edo airoldi , tamara broderick , josh tenenbaum , and the anonymous reviewers .
david m .
blei is supported by onr 123 - 123 , nsf career 123 , and grants from google and microsoft research .
thomas l .
griths is supported by nsf grant bcs - 123 and the darpa calo project .
michael i .
jordan is supported by grants from google and microsoft research .
