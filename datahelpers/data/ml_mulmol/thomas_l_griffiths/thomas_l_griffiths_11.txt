relationships between concepts account for a large pro - portion of semantic knowledge .
we present a nonpara - metric bayesian model that discovers systems of related concepts .
given data involving several sets of entities , our model discovers the kinds of entities in each set and the relations between kinds that are possible or likely .
we apply our approach to four problems : clustering ob - jects and features , learning ontologies , discovering kin - ship systems , and discovering structure in political data .
philosophers , psychologists and computer scientists have proposed that semantic knowledge is best understood as a system of relations .
two questions immediately arise : how can these systems be represented , and how are these representations acquired ? researchers who start with the rst question often devise complex representational schemes ( e . g .
minskys ( 123 ) classic work on frames ) , but explain - ing how these representations are learned is a challenging problem .
we take the opposite approach .
we consider only simple relational systems , but show how these systems can be acquired by unsupervised learning .
the systems we wish to discover are simple versions of the domain theories discussed by cognitive scientists and ai researchers ( davis 123 ) .
suppose that a domain in - cludes several types , or sets of entities .
one role of a domain theory is to specify the kinds of entities that exist in each set , and the possible or likely relationships between those kinds .
consider the domain of medicine , and a single type dened as the set of terms that might appear on a medical chart .
a theory of this domain might specify that cancer and diabetes are both disorders , asbestos and arsenic are both chemicals , and that chemicals can cause disorders .
our model assumes that each entity belongs to exactly one kind , or cluster , and simultaneously discovers the clusters and the relationships between clusters that are best supported by the data .
a key feature of our approach is that it does not require the number of clusters to be xed in advance .
the number of clusters used by a theory should be able to grow as more and more data are encountered , but a theory - learner should intro - duce no more clusters than are necessary to explain the data .
our approach automatically chooses an appropriate number copyright c ( cid : 123 ) 123 , american association for articial intelli - gence ( www . aaai . org ) .
all rights reserved .
of clusters using a prior that favors small numbers of clus - ters , but has access to a countably innite collection of clus - ters .
we therefore call our approach the innite relational model ( irm ) .
previous innite models ( rasmussen 123; antoniak 123 ) have focused on feature data , and the irm extends these approaches to work with arbitrary systems of
our framework can discover structure in relational data sets that appear quite different on the surface .
we demon - strate its range by applying it to four problems .
first we suggest that object - feature data can be protably viewed as a relation between two sets of entities the objects and the features and show how the irm simultaneously clus - ters both .
we then use the irm to learn a biomedical on - tology .
ontologies are classic examples of the theories we have described , since they group entities into higher - level concepts and specify how these high - level concepts relate to each other .
next we show that the irm discovers aspects of the kinship structure of an australian tribe .
our nal exam - ple considers a political data set , and we discover a system with clusters of countries , clusters of interactions between countries , and clusters of country features .
the innite relational model
suppose we are given one or more relations involving one or more types .
the goal of the irm is to partition each type into clusters , where a good set of partitions allows relation - ships between entities to be predicted by their cluster assign - ments .
for example , we may have a single type people and a single relation likes ( i , j ) which indicates whether person i likes person j .
our goal is to organize the entities into clusters that relate to each other in predictable ways ( fig - ure 123a ) .
we also allow predicate types : if there are multiple relations dened over the same domain , we will group them into a type and refer to them as predicates .
for instance , we may have several social predicates dened over the domain people people : likes ( , ) , admires ( , ) , respects ( , ) , and hates ( , ) .
we can introduce a type for these social pred - icates , and dene a ternary relation applies ( i , j , p ) which is true if predicate p applies to the pair ( i , j ) .
our goal is now to simultaneously cluster the people and the predi - cates ( figure 123c ) .
the irm can handle arbitrarily complex systems of attributes , entities and relations : if we include demographic attributes for the people , for example , we can
123 123 123 123 123 123 123 123 123
123 123 123 123 123 123 123 123 123
123 123 123 123 123 123 123 123 123
figure 123 : ( a ) input and output when the irm is applied to a binary relation r : t 123 t 123 ( 123 , 123 ) .
the irm discovers a partition of the entities , and the input matrix takes on a relatively clean block structure when sorted according to this partition .
( b ) the irm assumes that relation r is generated from two latent structures : a partition z and a parameter matrix .
entry r ( i , j ) is generated by tossing a coin with bias ( zi , zj ) , where zi and zj are the cluster assignments of entities i and j .
the irm inverts this generative model to discover the z and the that best explain relation r .
( c ) clustering a three place relation r : t 123 t 123 t 123 ( 123 , 123 ) .
t 123 might be a set of people , t 123 a set of social predicates , and r might specify whether each predicate applies to each pair of people .
the irm looks for solutions where each three dimensional sub - block includes mostly 123s or mostly 123s .
( d ) clustering three relations simultaneously .
t 123 might be a set of people , t 123 a set of demographic features , and t 123 a set of questions on a personality test .
note that the partition for t 123 is the same wherever this type appears .
simultaneously cluster people , social predicates , and demo -
formally , suppose that the observed data are m rela - tions involving n types .
let ri be the ith relation , t j be the jth type , and zj be a vector of cluster assignments for t j .
our task is to infer the cluster assignments , and we are ultimately interested in the posterior distribution p ( z123 , .
, zn|r123 , .
we specify this distribution by dening a generative model for the relations and the cluster
p ( r123 , .
, rm , z123 , .
, zn ) =
p ( ri|z123 ,
where we assume that the relations are conditionally inde - pendent given the cluster assignments , and that the cluster assignments for each type are independent .
to complete the generative model we rst describe the prior on the cluster assignment vectors , p ( zj ) , then show how the relations are generated given a set of these vectors .
to allow the irm the ability to discover the number of clus - ters in type t , we use a prior that assigns some probabil - ity mass to all possible partitions of the type .
a reasonable prior should encourage the model to introduce only as many clusters as are warranted by the data .
following previous work on nonparametric bayesian models ( rasmussen 123; antoniak 123 ) , we use a distribution over partitions in - duced by a chinese restaurant process ( crp , pitman 123 ) .
imagine building a partition from the ground up : starting with a single cluster containing a single object , and adding objects until all the objects belong to clusters .
under the crp , each cluster attracts new members in proportion to its size .
the distribution over clusters for object i , conditioned
on the cluster assignments of objects 123 , .
, i 123 is p ( zi = a|z123 , .
, zi123 ) = ( cid : 123 ) na na > 123
a is a new cluster
where na is the number of objects already assigned to clus - ter a , and is a parameter .
the distribution on z induced by the crp is exchangeable : the order in which objects are assigned to clusters can be permuted without changing the probability of the resulting partition .
p ( z ) can therefore be computed by choosing an arbitrary ordering and multiplying conditional probabilities as specied above .
since new ob - jects can always be assigned to new clusters , the irm effec - tively has access to a countably innite collection of clusters , hence the rst part of its name .
a crp prior on partitions is mathematically convenient , and consistent with the intuition that the prior should favor partitions with small numbers of clusters .
yet it is not a uni - versal solution to the problem of choosing the right number of clusters .
in some settings we may have prior knowledge that is not captured by the crp : for instance , we may expect that the clusters will be roughly equal in size .
even so , the crp provides a useful starting point for structure discovery in novel domains .
generating relations from clusters we assume that relations are binary - valued functions , al - though extensions to frequency data and continuous data are straightforward .
consider rst a problem with a single type t and a single two - place relation r : t t ( 123 , 123 ) .
type t , for example , could be a collection of people , and r ( i , j ) might indicate whether person i likes person j .
the complete generative model for this problem is :
z | crp ( )
( a , b ) | beta ( , ) r ( i , j ) | z , bernoulli ( ( zi , zj ) ) ,
123 where a , b n .
the model is represented graphically in here we assume that an entitys tendency to participate in relations is determined entirely by its cluster assignment .
the parameter ( a , b ) species the probability that a link ex - ists between any given pair ( i , j ) where i belongs to cluster a and j belongs to cluster b .
we place symmetric conjugate priors ( with hyperparameter ) on each entry in the matrix .
to specify the most general version of the irm , we extend equation 123 to relations of arbitrary arity .
consider an m di - mensional relation r involving n different types .
let dk be the label of the type that occupies dimension k : for exam - ple , the three place relation r : t 123 t 123 t 123 ( 123 , 123 ) has d123 = d123 = 123 , and d123 = 123
as before , the probability that the relation holds between a group of entities depends only on the clusters of those entities : r ( i123 , .
, im ) |z123 , .
, zn , bernoulli ( ( zd123 in settings with multiple relations , we introduce a parameter matrix i for each relation ri .
consider again a binary relation r over a single type t .
since we use conjugate priors on the entries in , it is simple
to compute p ( r|z ) = ( cid : 123 ) p ( r| , z ) p ( ) d :
beta ( m ( a , b ) + , m ( a , b ) + )
p ( r|z ) = ( cid : 123 ) a , bn
where m ( a , b ) is the number of pairs ( i , j ) where i a and j b and r ( i , j ) = 123 , m ( a , b ) is the number of pairs where r ( i , j ) = 123 , and beta ( , ) is the beta function .
if some en - tries in r are missing at random , we can ignore them and maintain counts m ( a , b ) and m ( a , b ) over only the observed values .
even though is integrated out , it is simple to re - cover the relationships between clusters given z .
the maxi - mum a posteriori value of ( a , b ) given z is :
m ( a , b ) +
m ( a , b ) + m ( a , b ) + 123
since we integrate out ,
inference can be carried out using markov chain monte carlo methods to sam - ple from the posterior on cluster assignments p ( z|r ) p ( r|z ) p ( z ) ( jain & neal 123 ) , or by searching for the mode of this distribution .
we are interested in discover - ing the single best representation for each data set men - tioned in this paper , and we search for the best partition z by repeatedly running hill climbing from an initial cong - uration where a single cluster is used for each type .
we also search for the best values of the parameters and us - ing an exponential prior p ( ) e and an improper prior the search uses proposals that move an object from one cluster to another , split a cluster , or merge two clusters .
the goal of the irm can be understood intuitively by represent - ing the relation r as an adjacency matrix .
our search pro - cedure tries to shufe the rows and columns of this matrix so that it assumes a clean block structure like the matrix in
figure 123a .
the same idea applies to relations with more than two dimensions : figure 123c shows a ternary relation , and here the aim is to shufe the dimensions so that the matrix takes on a 123 - dimensional block structure .
figure 123d shows three relations involving three types .
the goal is again to create matrices with clean block structures , but now the partition for t 123 must be the same wherever this type appears .
statisticians and sociologists have used the stochastic block - model to discover social roles in network data .
this model relies on a generative process identical to equation 123 , ex - cept that the zi are drawn from a multinomial distribution over a xed , nite number of clusters ( nowicki & snijders 123 ) .
several alternative approaches to relational learning ( e . g .
kubica et al .
( 123 ) ) focus on clique structures , where relational links are expected primarily between members of the same cluster .
an advantage of the blockmodel is that it also handles other kinds of relational structures hier - archies , for example , where members of one cluster tend to send links to individuals from higher - status clusters .
recent work in machine learning has extended the intu - ition behind the blockmodel in several directions .
there are approaches that learn overlapping clusters for a single type ( wolfe & jensen 123 ) and approaches that handle multiple relations and types using probabilistic relational models ( taskar , segal , & koller 123; getoor et al .
existing models often focus on data sets with some specic for example , the group - topic model ( wang , mo - hanty , & mccallum 123 ) simultaneously clusters entities ( e . g .
people ) and attributes associated with links between those entities ( e . g .
words ) .
compared to much of this work , a distinctive feature of the irm is its ability to automatically handle arbitrary collections of relations , each of which may take any number of arguments .
the irm is a lightweight framework that can be applied to data sets with qualitatively different forms : note that a single generic piece of code was used to analyze all of the data sets in this paper .
another distinctive feature of the irm is its ability to learn increasingly complex representations as more data are encountered .
this ability allows the model to choose a size vector specifying the number of clusters in each type , and is particularly important for structure discovery in novel do - mains , where we may have little or no prior knowledge about the number of clusters in each type .
other approaches to choosing the number of clusters are also possible : for ex - ample , we could learn systems of many different sizes and choose the best using cross - validation or bayesian model se - lection .
these alternatives may be practical when there is only one type , but scale badly as the number of types in - creases : if there are n types , each point in an n - dimensional space of size vectors must be separately considered .
finally , most previous approaches to relational clustering discover only clusters of objects , and our emphasis on clus - tering predicates is somewhat unusual .
an intelligent system should attempt to nd patterns at all levels , and clustering entities , features and relations is one step towards this goal .
ri = 123
ri = 123
ri = 123
ri = 123
ri = 123
ri = 123
ri = 123
ri = 123
ri = 123
ri = 123
ri = 123
ri = 123
123 123 123 123 123
123 123 123 123 123
123 123 123 123 123
123 123 123 123 123
123 123 123 123 123
123 123 123 123 123
figure 123 : each sub - plot shows the number of clusters recovered by the irm ( y axis ) against ( a ) the true number of clusters or ( b ) the number of entities per cluster .
in ( a ) , the total number of entities is xed at 123 and the true dimensionality varies between 123 and 123
in ( b ) , the number of clusters is xed at 123 and the total number of entities varies between 123 ( 123 per cluster ) and 123 ( 123 per cluster ) .
the columns represent results for three relational systems described in the text ( s123 , s123 , s123 ) and the rows show performance for clean data ( top ) and noisy data ( bottom ) .
each datapoint is an average across 123 randomly generated data sets and across all the types in each system , and error bars show standard deviations .
ri is the mean adjusted rand index , which measures the quality of the clusters discovered .
we generated synthetic data to explore the irms ability to infer the number of clusters in each type .
we considered data sets with three different forms .
system s123 has two types t 123 and t 123 and a single binary relation r : t 123t 123 ( 123 , 123 ) .
system s123 uses four types and three binary rela - tions with domains t 123 t 123 , t 123 t 123 and t 123 t 123 , and tests the irms ability to work with multiple types and re - lations .
system s123 is a single ternary relation with domain t 123 t 123 t 123 , and tests the irms ability to work with higher - order relations .
for the rst set of simulations , each type included 123 objects , and we generated data sets where the dimensionality d the true number of clusters in each type varied between 123 and 123
for each setting of d the clusters were approximately equal in size .
for each system and each setting of d , we varied the parameter ( see equa - tion 123 ) used to generate the data .
when is small , each relation has sub - blocks that are relatively clean , but the data become noisier as increases .
the top row of figure 123a shows that the irm accurately recovers the true number of clusters in each type when the data are clean ( = 123 ) .
the results are averaged across all the types in each system : note that the true dimension - ality is always the same for all the types in any given data set .
the second row suggests that performance remains sta - ble when = 123 and the data are noisier .
performance for the ternary relation ( system s123 ) is still perfect when = 123 , and experiments with d = 123 showed that performance only begins to suffer once reaches 123 and the data are extremely noisy .
this result suggests that nonparametric bayesian ap - proaches may be particularly useful for relational problems : the more that is known about relationships between types , the easier it should be to discover the number of clusters in
to assess the quality of the clusters found by the irm ,
we used the adjusted rand index ( hubert & arabie 123 ) .
compared to a ground - truth partition , a randomly generated partition has an expected score of zero , and the correct par - tition has a score of 123
mean scores are shown above each plot in figure 123a , and we see that the irm accurately recov - ers both the true number of clusters and the composition of
as more and more entities are observed , the expected number of clusters in a data set should probably increase .
the irm has this property : under the crp prior , the ex - pected number of clusters grows as o ( log ( n ) ) .
yet this as - pect of the prior should not exert too strong an inuence : even if the number of entities is large , a successful method should be able to recognize when the true dimensionality is small .
we tested this aspect of our model by generat - ing data sets where the true dimensionality was always 123 , and the number of entities in each type varied between 123 and 123
figure 123b shows that when the data are clean , the irm successfully recovers the dimensionality regardless of the number of entities used .
for noisier data sets , there is little statistical evidence for the true clusters when there are only a handful of entities per cluster , but the model reaches the right dimensionality and stays there as the number of
clustering objects and features
even though the irm is primarily intended for relational data , it can also discover structure in object - feature data .
any object - feature matrix can be viewed as a relation r : t 123 t 123 ( 123 , 123 ) between a set of objects ( t 123 ) and a set of features ( t 123 ) , and the irm provides a strategy for co - clustering , or simultaneously clustering both sets .
since features can be viewed as unary predicates , co - clustering is a simple instance of predicate clustering .
of the many ex - isting approaches to co - clustering , the irm is closest to the
123 killer whale , blue whale , humpback , seal , walrus , dolphin antelope , horse , giraffe , zebra , deer
o123 monkey , gorilla , chimp hippo , elephant , rhino grizzly bear , polar bear
ippers , strain teeth , swims , arctic , coastal , ocean , water hooves , long neck , horns hands , bipedal , jungle , tree bulbous body shape , slow , inactive
f123 meat teeth , eats meat , hunter , erce
walks , quadrapedal , ground
figure 123 : animal clusters , feature clusters , and a sorted matrix showing the relationships between them .
the matrix includes seven of the twelve animal clusters and all of the feature clusters .
some features refer to habitat ( jungle , tree , coastal ) , and others are anatomical ( bulbous body shape , has teeth for straining food from the water ) or behavioral ( swims , slow ) .
animals ( 123 ) medicine ( 123 ) alyawarra ( 123 )
table 123 : adjusted rand indices comparing the best irm so - lution and the best imm solution to ground truth partitions .
in parentheses are the true number of clusters ( top row ) and the number of clusters found by each model ( bottom rows ) .
work of hofmann & puzicha ( 123 ) .
figure 123 shows that coherent clusters emerge when the irm is applied to a 123 by 123 animal - feature matrix collected in a psychological experiment ( osherson et al .
fea - ture ratings were collected on a scale from 123 to 123 , and we created a binary matrix by thresholding at the global mean .
the feature clusters capture the coherent covariation of features across the objects in the data set .
importantly , the model also discovers relationships between feature and object clusters : for example , aquatic mammals tend to have
the irm reduces to the innite mixture model ( imm , rasmussen 123 ) if we choose not to cluster the features , as - suming instead that each feature is generated independently over the animal partition .
applied to the osherson data , the imm nds 123 animal clusters and the irm nds 123
any sin - gle feature may provide weak evidence for the additional structure discovered by the irm , but grouping several of these features allows the irm to discover a ner - grained
we asked two human subjects to sort the animals into groups .
one used 123 groups and the other used 123 , and we compared the model solutions to these partitions using the adjusted rand index .
the irm matched both human so - lutions better than the imm , and table 123 reports the mean
although there are many kinds of domain theories , ontolo - gies have played a particularly important role in the devel - opment of ai .
many researchers have developed ontolo - gies and used them to support learning and inference , but the acquisition of ontological knowledge itself has received
less attention .
we demonstrate here that the irm discovers a simple biomedical ontology given data from the unied medical language system ( umls , mccray 123 ) .
the umls includes a semantic network with 123 con - cepts and 123 binary predicates .
the concepts are high - level concepts like disease or syndrome , diagnostic proce - dure , and mammal .
the predicates include verbs like complicates , affects and causes .
we applied the irm to the ternary relation r : t 123 t 123 t 123 ( 123 , 123 ) , where t 123 is the set of concepts and t 123 is the set of binary predicates ( see figure 123c ) .
we have already seen that features ( unary predicates ) can be clustered , and here we see that predicates of higher orders can also be clustered .
our general philos - ophy is that every type is potentially a candidate for clus - tering , although there may be problem - specic reasons why we choose not to cluster some of them .
figure 123 shows some of the clusters that emerge when we cluster both concepts and predicates .
123 concept clusters and 123 predicate clusters are found in total .
we assessed the quality of the concept clusters using a 123 cluster parti - tion created by domain experts ( mccray et al .
the expert - designed partition includes clusters labeled living things , chemicals and drugs and disorders that match some of the clusters shown in figure 123
again , the irm discovers not just clusters , but relationships between these clusters .
by computing maximum a posteriori values of ( a , b ) , we identify the pairs of clusters ( a , b ) that are most strongly linked , and the predicates that link them .
some of the strongest relationships tell us that biological functions affect organisms , that chemicals cause diseases , and that bi - ologically active substances complicate diseases .
if we are interested only in discovering concept clusters , the imm can be applied to a attened version of the rela - tional data .
suppose that a is an element of t 123 , and we wish to atten the ternary relation r : t 123 t 123 t 123 ( 123 , 123 ) .
the features of a correspond to all values of r ( a , x123 , x123 ) where x123 t 123 and x123 t 123 and all values of r ( x123 , a , x123 ) .
any relational system can be similarly converted into an ob - ject feature matrix involving just one of its component di - mensions .
table 123 suggests that the irm solution for the relational data matches the expert partition somewhat better than the best solution for the imm on the attened data .
123organisms 123chemicals 123biological functions 123bio - active substances
assesses effect of
model of disease
assesses effect of
figure 123 : ( a ) predicate and concept clusters found using the umls data .
we have labeled the concept clusters and shown only six members of each .
( b ) adjacency matrices for six predicates , where the rows and columns are sorted according to the 123 cluster concept partition .
the rst ve clusters are the clusters shown in ( a ) : we see , for instance , that chemicals affect biological functions and that organisms interact with organisms .
learning kinship systems
australian tribes are renowned among anthropologists for the complex relational structure of their kinship systems .
we focus here on the alyawarra , a tribe from central aus - tralia ( denham 123 ) .
to a rst approximation , alyawarra kinship is captured by the kariera system shown in fig - ure 123a .
the tribe has four kinship sections , and figure 123 shows how the sections of individuals are related to the kin - ship sections of their parents .
for example , every member of section 123 has a mother in section 123 and a father in section 123
we show here that the irm discovers some of the properties of this system .
denham asked 123 tribe members to provide kinship terms for each other .
figure 123c shows six of the 123 differ - ent kinship terms recorded : for each term , the ( i , j ) cell in the corresponding matrix indicates whether person i used that term to refer to person j .
the four kinship sections are clearly visible in the rst two matrices .
adiadya refers to a classicatory younger brother or sister : that is , to a younger person in ones own section , even if he or she is not a bi - ological sibling .
umbaidya is used by female speakers to refer to a classicatory son or daughter , and by male speak - ers to refer to the child of a classicatory sister .
we see from the matrix that women in section 123 have children in section 123 , and vice versa .
anowadya refers to a preferred marriage partner .
the eight rough blocks indicate that men must marry women , that members of section 123 are expected to marry members of section 123 , and that members of section 123 are expected to marry members of section 123
we applied the irm to the ternary relation r : t 123 t 123 t 123 ( 123 , 123 ) where t 123 is the set of 123 people and t 123 is the set of kinship terms ( see figure 123c ) .
denham recorded demographic information for each of his informants , and we created a ground truth partition by assigning each person to one of 123 clusters depending on gender , kinship section ,
and a binary age feature ( older than 123 ) .
the best solution for the irm uses 123 clusters , and figure 123b shows that these clusters are relatively clean with respect to the dimensions of age , gender , and kinship section .
as for the biomedical data , we can apply the imm to a attened version of the data if we are interested only in clus - tering the people .
table 123 suggests that the irm solution captures the true structure substantially better than the imm .
clustering with multiple types and relations
the problem of theory discovery is especially interesting when there are multiple types and relations .
our nal ex - ample shows that the irm discovers structure in a political data set including 123 countries , 123 binary predicates repre - senting interactions between countries , and 123 features of the countries ( rummel 123 ) .
to create a binary data set , we thresholded each continuous variable at its mean and used one - of - n coding for the categorical variables .
the re - sulting data set has three types : countries ( t 123 ) , interaction predicates ( t 123 ) and country features ( t 123 ) , and two relations : r123 : t 123 t 123 t 123 ( 123 , 123 ) , and r123 : t 123 t 123 ( 123 , 123 ) .
the irm analyzes r123 and r123 simultaneously and discovers partitions of all three types .
the model partitions the 123 countries into the ve groups shown in figure 123a .
the data come from 123 and there are two groups from the western bloc , a group from the commu - nist bloc , and two groups from the so - called neutral bloc .
the model discovers 123 clusters of interaction predicates , and figures 123b through 123i represent some of the clusters that emerge .
note that the countries in each matrix are sorted according to the order in 123a .
the clusters in 123b and 123e repre - sent positive and negative interactions , and the cluster in 123i conrms that the country partition is well explained by bloc
the irm divides the country features into the ve groups
figure 123 : ( a ) the kariera kinship system .
each person belongs to one of four kinship sections , and the section of any person predicts the sections of his or her parents .
( b ) composition of the 123 clusters found by the irm .
the six age categories were chosen by denham , and are based in part on alyawarra terms for age groupings ( denham 123 ) .
( c ) data for six alyawarra kinship terms .
the 123 individuals are sorted by the clusters shown in ( b ) .
shown in figure 123a .
the rst group includes a single fea - ture non - communist which captures one of the most important aspects of this cold - war data set .
the second and third clusters include features that are characteristic of west - ern and communist countries respectively , and the fourth cluster includes features that are often true of developing countries but never true of the uk and the usa
we presented the innite relational model , a framework for simultaneously clustering one or more sets of entities and discovering the relationships between clusters that are pos - sible or likely .
our framework supports the discovery of simple theories that specify the kinds of entities in a domain and the relations that hold between them .
these theories capture important aspects of semantic knowledge , but we are ultimately interested in more expressive representations that can capture a greater proportion of human knowledge .
log - ical representations , for example , will probably be needed to fully capture knowledge about kinship .
there are many ways to formalize the notion of a rela - tional system , and it is useful to arrange these formalizations along a spectrum from simple to complex .
we considered relatively simple systems , which allowed us to give a princi - pled account of how these systems might be learned .
meth - ods for learning logical theories ( muggleton & de raedt 123; kok & domingos 123 ) consider relational systems at the more complex end of the spectrum , and it may be worth thinking about hybrid approaches where the clusters discovered by the irm serve as primitives in more complex
123figure 123 reects some inconsistencies in the original data : for instance , 123i suggests that israel is part of the neutral bloc , but the second labeled feature in 123a suggests that israel is part of the west -
theories .
adding representational power while preserving learnability is an imposing challenge , but we hope that ap - proaches like ours will help bring algorithms for relational learning closer to theories of the organization and develop - ment of human semantic knowledge .
acknowledgments supported in part by afosr muri contract fa123 - 123 - 123 - 123 , the william albert asbjornsen memorial fellowship ( ck ) and the paul e .
newton chair ( jbt ) .
we thank steven sloman for providing the animal - feature data and woodrow denham for providing the
