abstract .
dirichlet process ( dp ) mixture models are the cornerstone of non - parametric bayesian statistics , and the development of monte - carlo markov chain ( mcmc ) sampling methods for dp mixtures has enabled the application of non - parametric bayesian methods to a variety of practical data analysis problems .
however , mcmc sampling can be prohibitively slow , and it is important to ex - plore alternatives .
one class of alternatives is provided by variational methods , a class of deterministic algorithms that convert inference problems into optimization problems ( opper and saad 123; wainwright and jordan 123 ) .
thus far , varia - tional methods have mainly been explored in the parametric setting , in particular within the formalism of the exponential family ( attias 123; ghahramani and beal 123; blei et al .
in this paper , we present a variational inference algorithm for dp mixtures .
we present experiments that compare the algorithm to gibbs sampling algorithms for dp mixtures of gaussians and present an application to a large - scale image analysis problem .
keywords : dirichlet processes , hierarchical models , variational inference , image processing , bayesian computation
the methodology of monte carlo markov chain ( mcmc ) sampling has energized bayesian statistics for more than a decade , providing a systematic approach to the computation of likelihoods and posterior distributions , and permitting the deployment of bayesian methods in a rapidly growing number of applied problems .
however , while an unques - tioned success story , mcmc is not an unqualied success storymcmc methods can be slow to converge and their convergence can be dicult to diagnose .
while further research on sampling is needed , it is also important to explore alternatives , particularly in the context of large - scale problems .
one such class of alternatives is provided by variational inference methods ( ghahra - mani and beal 123; jordan et al .
123; opper and saad 123; wainwright and jordan 123; wiegerinck 123 ) .
like mcmc , variational inference methods have their roots in statistical physics , and , in contradistinction to mcmc methods , they are deterministic .
the basic idea of variational inference is to formulate the computation of a marginal
c ( cid : 123 ) 123 international society for bayesian analysis
variational inference for dirichlet process mixtures
or conditional probability in terms of an optimization problem .
this ( generally in - tractable ) problem is then relaxed , yielding a simplied optimization problem that depends on a number of free parameters , known as variational parameters .
solving for the variational parameters gives an approximation to the marginal or conditional probabilities of interest .
variational inference methods have been developed principally in the context of the exponential family , where the convexity properties of the natural parameter space and the cumulant function yield an elegant general variational formalism ( wainwright and jordan 123 ) .
for example , variational methods have been developed for parametric hi - erarchical bayesian models based on general exponential family specications ( ghahra - mani and beal 123 ) .
mcmc methods have seen much wider application .
in particular , the development of mcmc algorithms for nonparametric models such as the dirichlet process has led to increased interest in nonparametric bayesian methods .
in the current paper , we aim to close this gap by developing variational methods for dirichlet process
the dirichlet process ( dp ) , introduced in ferguson ( 123 ) , is a measure on measures .
the dp is parameterized by a base distribution g123 and a positive scaling parameter . 123 suppose we draw a random measure g from a dirichlet process , and independently draw n random variables n from g :
g | ( g123 , ) dp ( g123 , )
n g , n ( 123 , .
, n ) .
marginalizing out the random measure g , the joint distribution of ( 123 , .
, n ) follows a polya urn scheme ( blackwell and macqueen 123 ) .
positive probability is assigned to congurations in which dierent n take on identical values; moreover , the underlying random measure g is discrete with probability one .
this is seen most directly in the stick - breaking representation of the dp , in which g is represented explicitly as an innite sum of atomic measures ( sethuraman 123 ) .
the dirichlet process mixture model ( antoniak 123 ) adds a level to the hierarchy by treating n as the parameter of the distribution of the nth observation .
given the discreteness of g , the dp mixture has an interpretation as a mixture model with an unbounded number of mixture components .
given a sample ( x123 , .
, xn ) from a dp mixture , our goal is to compute the predic -
p ( x | x123 , .
, xn , , g123 ) =z p ( x | ) p ( | x123 , .
, xn , , g123 ) d ,
as in many hierarchical bayesian models , the posterior distribution p ( | x123 , .
, xn , g123 , ) is complicated and is not available in a closed form .
mcmc provides one class of ap - proximations for this posterior and the predictive density ( maceachern 123; escobar and west 123; neal 123 ) .
123ferguson ( 123 ) parameterizes the dirichlet process by a single base measure , which is g123 in our
blei and m .
jordan
in this paper , we present a variational inference algorithm for dp mixtures based on the stick - breaking representation of the underlying dp .
the algorithm involves two probability distributionsthe posterior distribution p and a variational distribution q .
the latter is endowed with free variational parameters , and the algorithmic problem is to adjust these parameters so that q approximates p .
we also use a stick - breaking representation for q , but in this case we truncate the representation to yield a nite - dimensional representation .
while in principle we could also truncate p , turning the model into a nite - dimensional model , it is important to emphasize at the outset that this is not our approachwe truncate only the variational distribution .
the paper is organized as follows .
in section 123 we provide basic background on dp mixture models , focusing on the case of exponential family mixtures .
in section 123 we present a variational inference algorithms for dp mixtures .
section 123 overviews mcmc algorithms for the dp mixture , discussing algorithms based both on the polya urn representation and the stick - breaking representation .
section 123 presents the results of experimental comparisons , section 123 presents an analysis of natural image data , and section 123 presents our conclusions .
123 dirichlet process mixture models
let be a continuous random variable , let g123 be a non - atomic probability distribution for , and let be a positive , real - valued scalar .
a random measure g is distributed according to a dirichlet process ( dp ) ( ferguson 123 ) , with scaling parameter and base distribution g123 , if for all natural numbers k and k - partitions ( b123 , .
, bk ) :
( g ( b123 ) , g ( b123 ) , .
, g ( bk ) ) dir ( g123 ( b123 ) , g123 ( b123 ) , .
, g123 ( bk ) ) .
integrating out g , the joint distribution of the collection of variables ( 123 , .
, n ) ex - hibits a clustering eect; conditioning on n 123 draws , the nth value is , with positive probability , exactly equal to one of those draws :
p ( | 123 , .
, n123 ) g123 ( ) +
thus , the variables ( 123 , .
, n123 ) are randomly partitioned according to which variables are equal to the same value , with the distribution of the partition obtained from a polya urn scheme ( blackwell and macqueen 123 ) .
let ( |c| ) denote the distinct values of ( 123 , .
, n123 ) , let c = ( c123 , .
, cn123 ) be assignment variables such that i =
ci , and let |c| denote the number of cells in the partition .
the distribution of n follows the urn distribution :
, g123 with prob .
| ( j : cj =i ) |
where | ( j : cj = i ) | is the number of times the value
i occurs in ( 123 , .
, n123 ) .
variational inference for dirichlet process mixtures
in the dirichlet process mixture model , the dp is used as a nonparametric prior in
a hierarchical bayesian specication ( antoniak 123 ) :
g | ( , g123 ) dp ( , g123 )
n | g g
xn | n p ( xn | n ) .
data generated from this model can be partitioned according to the distinct values of the parameter .
taking this view , the dp mixture has a natural interpretation as a exible mixture model in which the number of components ( i . e . , the number of cells in the partition ) is random and grows as new data are observed .
the denition of the dp via its nite dimensional distributions in equation ( 123 ) reposes on the kolmogorov consistency theorem ( ferguson 123 ) .
sethuraman ( 123 ) provides a more explicit characterization of the dp in terms of a stick - breaking construc - tion .
consider two innite collections of independent random variables , vi beta ( 123 , )
i g123 , for i = ( 123 , 123 , .
the stick - breaking representation of g is as follows :
i ( v ) = vi
this representation of the dp makes clear that g is discrete ( with probability one ) ; the support of g consists of a countably innite set of atoms , drawn iid from g123
the mixing proportions i ( v ) are given by successively breaking a unit length stick into an innite number of pieces .
the size of each successive piece , proportional to the rest of the stick , is given by an independent draw from a beta ( 123 , ) distribution .
in the dp mixture , the vector ( v ) comprises the innite vector of mixing pro - portions and ( 123 , .
. ) are the atoms representing the mixture components .
let zn be an assignment variable of the mixture component with which the data point xn is associated .
the data can be described as arising from the following process :
draw vi | beta ( 123 , ) ,
i = ( 123 , 123 ,
i | g123 g123 ,
i = ( 123 , 123 ,
for the nth data point :
( a ) draw zn | ( v123 , v123 , .
. ) mult ( ( v ) ) .
( b ) draw xn | zn p ( xn |
in this paper , we restrict ourselves to dp mixtures for which the observable data are drawn from an exponential family distribution , and where the base distribution for the dp is the corresponding conjugate prior .
blei and m .
jordan
figure 123 : graphical model representation of an exponential family dp mixture .
nodes denote random variables , edges denote possible dependence , and plates denote replica -
the stick - breaking construction for the dp mixture is depicted as a graphical model in figure 123
the conditional distributions of vk and zn are as described above .
the distribution of xn conditional on zn and (
. ) is :
p ( xn | zn ,
t xn a (
is the sucient statistic for the natural parameter .
i ) is the appropriate cumulant function and we assume for simplicity that x
the vector of sucient statistics of the corresponding conjugate family is ( t , a ( ) ) t .
the base distribution is :
p ( | ) = h ( ) exp ( t
123 + 123 ( a ( ) ) a ( ) ) ,
where we decompose the hyperparameter such that 123 contains the rst dim ( ) components and 123 is a scalar .
123 variational inference for dp mixtures
there is no direct way to compute the posterior distribution under a dp mixture prior .
approximate inference methods are required for dp mixtures and markov chain monte carlo ( mcmc ) sampling methods have become the methodology of choice ( maceachern 123; escobar and west 123; maceachern 123; neal 123; ishwaran and james 123 ) .
variational inference provides an alternative , deterministic methodology for ap - proximating likelihoods and posteriors ( wainwright and jordan 123 ) .
consider a model with hyperparameters , latent variables w = ( w123 , .
, wm ) , and observations x = ( x123 , .
, xn ) .
the posterior distribution of the latent variables is :
p ( w | x , ) = exp ( log p ( x , w | ) log p ( x | ) ) .
variational inference for dirichlet process mixtures
working directly with this posterior is typically precluded by the need to compute
the normalizing constant .
the log marginal probability of the observations is :
log p ( x | ) = logz p ( w , x | ) dw ,
which may be dicult to compute given that the latent variables become dependent when conditioning on observed data .
mcmc algorithms circumvent this computation by constructing an approximate posterior based on samples from a markov chain whose stationary distribution is the posterior of interest .
gibbs sampling is the simplest mcmc algorithm; one iteratively samples each latent variable conditioned on the previously sampled values of the other
p ( wi | wi , x , ) = exp ( log p ( w , x | ) log p ( wi , x | ) ) .
the normalizing constants for these conditional distributions are assumed to be available analytically for settings in which gibbs sampling is appropriate .
variational inference is based on reformulating the problem of computing the poste - rior distribution as an optimization problem , perturbing ( or , relaxing ) that problem , and nding solutions to the perturbed problem ( wainwright and jordan 123 ) .
in this paper , we work with a particular class of variational methods known as mean - eld meth - ods .
these are based on optimizing kullback - leibler ( kl ) divergence with respect to a so - called variational distribution .
in particular , let q ( w ) be a family of distributions indexed by a variational parameter .
we aim to minimize the kl divergence between q ( w ) and p ( w | x , ) :
d ( q ( w ) ||p ( w | x , ) ) = eq ( log q ( w ) ) eq ( log p ( w , x | ) ) + log p ( x | ) ,
where here and elsewhere in the paper we omit the variational parameters when using q as a subscript of an expectation .
notice that the problematic marginal probability does not depend on the variational parameters; it can be ignored in the optimization .
the minimization in equation ( 123 ) can be cast alternatively as the maximization of
a lower bound on the log marginal likelihood :
log p ( x | ) eq ( log p ( w , x | ) ) eq ( log q ( w ) ) .
the gap in this bound is the divergence between q ( w ) and the true posterior .
for the mean - eld framework to yield a computationally eective inference method , it is necessary to choose a family of distributions q ( w ) such that we can tractably optimize equation ( 123 ) .
in constructing that family , one typically breaks some of de - pendencies between latent variables which make the true posterior dicult to compute .
in the next sections , we consider fully - factorized variational distributions which break all of the dependencies .
blei and m .
jordan
123 mean eld variational inference in exponential families
for each latent variable , let us assume that the conditional distribution p ( wi | wi , x , ) is a member of the exponential family123 :
p ( wi | wi , x , ) = h ( wi ) exp ( gi ( wi , x , ) t wi a ( gi ( wi , x , ) ) ) ,
where gi ( wi , x , ) is the natural parameter for wi when conditioning on the remaining latent variables and the observations .
in this setting it is natural to consider the following family of distributions as mean -
eld variational approximations ( ghahramani and beal 123 ) :
q ( w ) =
i wi a ( wi ) ) ,
where = ( 123 , 123 , .
, m ) are variational parameters .
indeed , it turns out that the variational algorithm that we obtain using this fully - factorized family is reminiscent of gibbs sampling .
in particular , as we show in appendix 123 , the optimization of kl divergence with respect to a single variational parameter i is achieved by computing the following expectation :
i = eq ( gi ( wi , x , ) ) .
repeatedly updating each parameter in turn by computing this expectation amounts to performing coordinate ascent in the kl divergence .
notice the interesting relationship of this algorithm to the gibbs sampler .
in gibbs
sampling , we iteratively draw the latent variables wi from the distribution p ( wi | wi , x , ) .
in mean - eld variational inference , we iteratively update the variational parameter i by setting it equal to the expected value of gi ( wi , x , ) .
this expectation is computed under the variational distribution .
123 dp mixtures
in this section we develop a mean - eld variational algorithm for the dp mixture .
our algorithm is based on the stick - breaking representation of the dp mixture ( see figure 123 ) .
in this representation the latent variables are the stick lengths , the atoms , and the cluster assignments : w = ( v , , z ) .
the hyperparameters are the scaling parameter and the parameter of the conjugate base distribution : = ( , ) .
following the general recipe in equation ( 123 ) , we write the variational bound on the
123examples of models in which p ( wi | w
i , x , ) is an exponential family distribution include hidden markov models , mixture models , state space models , and hierarchical bayesian models with conjugate and mixture of conjugate priors .
variational inference for dirichlet process mixtures
log marginal probability of the data :
log p ( x | , ) eq ( log p ( v | ) ) + eq ( log p (
( eq ( log p ( zn | v ) ) + eq ( log p ( xn | zn ) ) )
eq ( log q ( v ,
, z ) ) .
to exploit this bound , we must nd a family of variational distributions that approxi - mates the distribution of the innite - dimensional random measure g , where the random measure is expressed in terms of the innite sets v = ( v123 , v123 , .
. ) and 123 , .
we do this by considering truncated stick - breaking representations .
thus , we x a value t and let q ( vt = 123 ) = 123; this implies that the mixture proportions t ( v ) are equal to zero for t > t ( see equation 123 ) .
truncated stick - breaking representations have been considered previously by ish - waran and james ( 123 ) in the context of sampling - based inference for an approxima - tion to the dp mixture model .
note that our use of truncation is rather dierent .
in our case , the model is a full dirichlet process and is not truncated; only the variational distribution is truncated .
the truncation level t is a variational parameter which can be freely set; it is not a part of the prior model specication ( see section 123 ) .
we thus propose the following factorized family of variational distributions for mean -
eld variational inference :
, z ) =
where qt ( vt ) are beta distributions , qt ( t ) are exponential family distributions with natural parameters t , and qn ( zn ) are multinomial distributions .
in the notation of section 123 , the free variational parameters are :
= ( 123 , .
, t 123 , 123 , .
, t , 123 , .
, n ) .
it is important to note that there is a dierent variational parameter for each latent variable under the variational distribution .
for example , the choice of the mixture com - ponent zn for the nth data point is governed by a multinomial distribution indexed by a variational parameter n .
this reects the conditional nature of variational inference .
coordinate ascent algorithm
in this section we present an explicit coordinate ascent algorithm for optimizing the bound in equation ( 123 ) with respect to the variational parameters .
all of the terms in the bound involve standard computations in the exponential family , except for the third term .
we rewrite the third term using indicator random
blei and m .
jordan
eq ( log p ( zn | v ) ) = eqhlog ( cid : 123 ) q
i=123 ( 123 vi ) 123 ( zn>i ) v 123 ( zn=i )
i=123 q ( zn > i ) eq ( log ( 123 vi ) ) + q ( zn = i ) eq ( log vi ) .
recall that eq ( log ( 123 vt ) ) = 123 and q ( zn > t ) = 123
consequently , we can truncate this summation at t = t :
eq ( log p ( zn | v ) ) =
q ( zn > i ) eq ( log ( 123 vi ) ) + q ( zn = i ) eq ( log vi ) ,
q ( zn = i ) = n , i
q ( zn > i ) = pt
eq ( log vi ) = ( i , 123 ) ( i , 123 + i , 123 ) eq ( log ( 123 vi ) ) = ( i , 123 ) ( i , 123 + i , 123 ) .
the digamma function , denoted by , arises from the derivative of the log normalization factor in the beta distribution .
we now use the general expression in equation ( 123 ) to derive a mean - eld coordinate
ascent algorithm .
this yields :
t , 123 = 123 +pn n , t t , 123 = +pnpt t , 123 = 123 +pn n , txn t , 123 = 123 +pn n , t .
for t ( 123 , .
, t ) and n ( 123 , .
, n ) , where :
st = eq ( log vt ) +pt123
i=123 eq ( log ( 123 vi ) ) + eq (
t ) t xn eq ( a (
t ) ) .
iterating these updates optimizes equation ( 123 ) with respect to the variational param - eters dened in equation ( 123 ) .
practical applications of variational methods must address initialization of the vari - ational distribution .
while the algorithm yields a bound for any starting values of the variational parameters , poor choices of initialization can lead to local maxima that yield poor bounds .
we initialize the variational distribution by incrementally updating the parameters according to a random permutation of the data points .
( this can be viewed as a variational version of sequential importance sampling ) .
we run the algorithm mul - tiple times and choose the nal parameter settings that give the best bound on the
variational inference for dirichlet process mixtures
to compute the predictive distribution , we use the variational posterior in a manner analogous to the way that the empirical approximation is used by an mcmc sampling algorithm .
the predictive distribution is :
p ( xn +123 | x , , ) =z
t ( v ) p ( xn +123 |
t ) ! dp ( v ,
| x , , ) .
under the factorized variational approximation to the posterior , the distribution of the atoms and the stick lengths are decoupled and the innite sum is truncated .
conse - quently , we can approximate the predictive distribution with a product of expectations which are straightforward to compute under the variational approximation :
p ( xn +123 | x , , )
eq ( t ( v ) ) eq ( p ( xn +123 |
t ) ) ,
where q depends implicitly on x , , and .
finally , we remark on two possible extensions .
first , when g123 is not conjugate , a
simple coordinate ascent update for i may not be available , particularly when p ( is not in the exponential family .
however , such an update is available for the special case of g123 being a mixture of conjugate distributions .
second , it is often important in applications to integrate over a diuse prior on the scaling parameter .
as we show in appendix 123 , it is straightforward to extend the variational algorithm to include a gamma prior on .
i | z , x , )
123 gibbs sampling
for comparison to variational inference , we review the collapsed gibbs sampler and blocked gibbs sampler for dp mixtures .
123 collapsed gibbs sampling
the collapsed gibbs sampler for a dp mixture with conjugate base distribution ( maceach - ern 123 ) integrates out the random measure g and distinct parameter values ( 123 , .
, the markov chain is thus dened only on the latent partition c = ( c123 , .
, cn ) .
( recall that |c| denotes the number of cells in the partition . )
the algorithm iteratively samples each assignment variable cn , for n ( 123 , .
, n ) , conditional on the other cells in the partition , cn .
the assignment cn can be one of |cn| + 123 values : either the nth data point is in a cell with other data points , or in a cell by itself .
exchangeability implies that cn has the following multinomial distribution :
p ( cn = k | x , cn , , ) p ( xn | xn , cn , cn = k , ) p ( cn = k | cn , ) .
blei and m .
jordan
the rst term is a ratio of normalizing constants of the posterior distribution of the kth parameter , one including and one excluding the nth data point :
p ( xn | xn , cn , cn = k , ) =
expna ( 123 +pm123=n 123 ( cm = k ) xm + xn , 123 +pm123=n 123 ( cm = k ) + 123 ) o
expna ( 123 +pm123=n 123 ( cm = k ) xm , 123 +pm123=n 123 ( cm = k ) ) o
the second term is given by the polya urn scheme :
p ( cn = k | cn ) ( cid : 123 ) | ( j : cn , j = k ) |
if k is an existing cell in the partition if k is a new cell in the partition ,
where | ( j : cn , j = k ) | denotes the number of data points in the kth cell of the partition
once this chain has reached its stationary distribution , we collect b samples ( c123 ,
to approximate the posterior .
the approximate predictive distribution is an average of the predictive distributions across the monte carlo samples :
p ( xn +123 | x123 , .
, xn , , ) =
p ( xn +123 | cb , x , , ) .
for a given sample , that distribution is :
p ( xn +123 | cb , x , , ) =
p ( cn +123 = k | cb , ) p ( xn +123 | cb , x , cn +123 = k , ) .
when g123 is not conjugate , the distribution in equation ( 123 ) does not have a simple
closed form .
eective algorithms for handling this case are given in neal ( 123 ) .
123 blocked gibbs sampling
in the collapsed gibbs sampler , the assignment variable cn is drawn from a distribution that depends on the most recently sampled values of the other assignment variables .
consequently , these variables must be updated one at a time which can potentially slow down the algorithm when compared to a blocking strategy .
to this end , ishwaran and james ( 123 ) developed a blocked gibbs sampling algorithm based on the stick - breaking representation of figure 123
the main issue to face in developing a blocked gibbs sampler for the stick - breaking dp mixture is that one needs to sample the innite collection of stick lengths v before sampling the nite collection of cluster assignments z .
ishwaran and james ( 123 ) face this issue by dening a truncated dirichlet process ( tdp ) in which vk123 is set equal to one for some xed value k .
this yields i ( v ) = 123 for i k , and converts the innite sum in equation ( 123 ) into a nite sum .
ishwaran and james ( 123 ) justify substituting a
variational inference for dirichlet process mixtures
tdp mixture model for a full dp mixture model by showing that the truncated process closely approximates a true dirichlet process when the truncation level is chosen large relative to the number of data points .
in the tdp mixture , the state of the markov chain consists of the beta variables v = ( v123 , .
, vk123 ) , the mixture component parameters k ) , and the indicator variables z = ( z123 , .
, zn ) .
the blocked gibbs sampler iterates between the following three steps :
for n ( 123 , .
, n ) , independently sample zn from :
p ( zn = k | v ,
, x ) = k ( v ) p ( xn |
for k ( 123 , .
, k ) , independently sample vk from beta ( k , 123 , k , 123 ) , where :
k , 123 = 123 +pn k , 123 = +pk
n=123 123 ( zn = k )
n=123 123 ( zn = i ) .
this step follows from the conjugacy between the multinomial distribution and the truncated stick - breaking construction , which is a generalized dirichlet distri - bution ( connor and mosimann 123 ) .
for k ( 123 , .
, k ) , independently sample
k from p (
k | k ) .
this distribution is
in the same family as the base distribution , with parameters :
k , 123 = 123 +pi123=n 123 ( zi = k ) xi k , 123 = 123 +pi123=n 123 ( zi = k ) .
after the chain has reached its stationary distribution , we collect b samples and construct an approximate predictive distribution .
again , this distribution is an aver - age of the predictive distributions for each of the collected samples .
the predictive distribution for a particular sample is :
p ( xn +123 | z , x , , ) =
e ( i ( v ) | 123 , .
, k ) p ( xn +123 | k ) ,
where e ( i | 123 , .
, k ) is the expectation of the product of independent beta variables given in equation ( 123 ) .
this distribution only depends on z; the other variables are needed in the gibbs sampling procedure , but can be integrated out here .
note that this approximation has a form similar to the approximate predictive distribution under the variational distribution in equation ( 123 ) .
in the variational case , however , the averaging is done parametrically via the variational distribution rather than by a monte carlo
the tdp sampler readily handles non - conjugacy of g123 , provided that there is a
method of sampling
i from its posterior .
blei and m .
jordan
figure 123 : the approximate predictive distribution given by variational inference at dierent stages of the algorithm .
the data are 123 points generated by a gaussian dp mixture model with xed diagonal covariance .
123 empirical comparison
qualitatively , variational methods oer several potential advantages over gibbs sam - pling .
they are deterministic , and have an optimization criterion given by equa - tion ( 123 ) that can be used to assess convergence .
in contrast , assessing convergence of a gibbs samplernamely , determining when the markov chain has reached its sta - tionary distributionis an active eld of research .
theoretical bounds on the mixing time are of little practical use , and there is no consensus on how to choose among the several empirical methods developed for this purpose ( robert and casella 123 ) .
but there are several potential disadvantages of variational methods as well .
first , the optimization procedure can fall prey to local maxima in the variational parameter space .
local maxima can be mitigated with restarts , or removed via the incorporation of additional variational parameters , but these strategies may slow the overall conver - gence of the procedure .
second , any given xed variational representation yields only an approximation to the posterior .
there are methods for considering hierarchies of variational representations that approach the posterior in the limit , but these methods may again incur serious computational costs .
lacking a theory by which these issues can be evaluated in the general setting of dp mixtures , we turn to experimental evaluation .
we studied the performance of the variational algorithm of section 123 and the gibbs samplers of section 123 in the setting of dp mixtures of gaussians with xed inverse covariance matrix ( i . e . , the dp mixes over the mean of the gaussian ) .
the natural conjugate base distribution for the dp is gaussian , with covariance given by / 123 ( see
figure 123 provides an illustrative example of variational inference on a small problem involving 123 data points sampled from a two - dimensional dp mixture of gaussians with diagonal covariance .
each panel in the gure plots the data and presents the
variational inference for dirichlet process mixtures
figure 123 : mean convergence time and standard error across ten data sets per dimension for variational inference , tdp gibbs sampling , and the collapsed gibbs sampler .
predictive distribution given by the variational inference algorithm at a given iteration ( see equation ( 123 ) ) .
the truncation level was set to 123
as seen in the rst panel , the initialization of the variational parameters yields a largely at distribution .
after one iteration , the algorithm has found the modes of the predictive distribution and , after convergence , it has further rened those modes .
even though 123 mixture components are represented in the variational distribution , the tted approximate posterior only uses ve of them .
to compare the variational inference algorithm to the gibbs sampling algorithms , we conducted a systematic set of simulation experiments in which the dimensionality of the data was varied from 123 to 123
the covariance matrix was given by the autocorrelation matrix for a rst - order autoregressive process , chosen so that the components are highly dependent ( = 123 ) .
the base distribution was a zero - mean gaussian with covariance appropriately scaled for comparison across dimensions .
the scaling parameter was set equal to one .
in each case , we generated 123 data points from a dp mixture of gaussians model of the chosen dimensionality and generated 123 additional points as held - out data .
in testing on the held - out data , we treated each point as the 123st data point in the collection and computed its conditional probability using each algorithms approximate
blei and m .
jordan
mean held out log probability ( std err )
collapsed gibbs truncated gibbs
table 123 : average held - out log probability for the predictive distributions given by vari - ational inference , tdp gibbs sampling , and the collapsed gibbs sampler .
figure 123 : ( left ) the optimal bound on the log probability as a function of the truncation level .
there are ve clusters in the simulated 123 - dimensional dp mixture of gaussians data set which was used .
( right ) held - out probability as a function of iteration of variational inference for the same simulated data set .
the relative change in the log probability bound of the observations is labeled at selected iterations .
variational inference for dirichlet process mixtures
figure 123 : autocorrelation plots on the size of the largest component for the truncated dp gibbs sampler ( left ) and collapsed gibbs sampler ( right ) in an example dataset of 123 - dimensional gaussian data .
the tdp approximation was truncated at k = 123 components .
for the variational algorithm , the truncation level was also t = 123 components .
note that in the latter case , the truncation level is simply another variational parameter .
while we held t xed in our simulations , it is also possible to optimize t with respect to the kl divergence .
indeed , figure 123 ( left ) shows how the optimal kl divergence changes as a function of the truncation level for one of the simulated data sets .
we ran all algorithms to convergence and measured the computation time . 123 for the collapsed gibbs sampler , we assessed convergence to the stationary distribution with the diagnostic given by raftery and lewis ( 123 ) , and collected 123 additional samples to estimate the predictive distribution ( the same diagnostic provides an appropriate lag at which to collect uncorrelated samples ) .
we assessed convergence of the blocked gibbs sampler using the same statistic as for the collapsed gibbs sampler and used the same number of samples to form the approximate predictive distribution . 123
finally , for variational inference , we measured convergence using the relative change in the log marginal probability bound ( equation 123 ) , stopping the algorithm when it was less than 123e123
there is a certain inevitable arbitrariness in these choices; in general it is dicult to envisage measures of computation time that allow stochastic mcmc algorithms and deterministic variational algorithms to be compared in a standardized way .
nonetheless , we have made what we consider to be reasonable , pragmatic choices .
in particular , our choice of stopping time for the variational algorithm is quite conservative , as illustrated in figure 123 ( right ) .
figure 123 illustrates the average convergence time across ten datasets per dimension .
with the caveats in mind regarding convergence time measurement , it appears that the variational algorithm is quite competitive with the mcmc algorithms .
the variational
123all timing computations were made on a pentium iii 123ghz desktop machine .
123typically , hundreds or thousands of samples are used in mcmc algorithms to form the approxi - mate posterior .
however , we found that such approximations did not oer any additional predictive performance in the simulated data .
to be fair to mcmc in the timing comparisons , we used a small number of samples to estimate the predictive distributions .
blei and m .
jordan
figure 123 : four sample clusters from a dp mixture analysis of 123 images from the associated press .
the left - most column is the posterior mean of each cluster followed by the top ten images associated with it .
these clusters capture patterns in the data , such as basketball shots , outdoor scenes on gray days , faces , and pictures with blue
algorithm was faster and exhibited signicantly less variance in its convergence time .
moreover , there is little evidence of an increase in convergence time across dimensionality for the variational algorithm over the range tested .
note that the collapsed gibbs sampler converged faster than the tdp gibbs sampler .
though an iteration of collapsed gibbs is slower than an iteration of tdp gibbs , the tdp gibbs sampler required a longer burn - in and greater lag to obtain uncorrelated samples .
this is illustrated in the autocorrelation plots of figure 123
comparing the two mcmc algorithms , we found no advantage to the truncated approximation .
table 123 illustrates the average log likelihood assigned to the held - out data by the approximate predictive distributions .
first , notice that the collapsed dp gibbs sam - pler assigned the same likelihood as the posterior from the tdp gibbs sampleran indication of the quality of a tdp for approximating a dp .
more importantly , however , the predictive distribution based on the variational posterior assigned a similar score as those based on samples from the true posterior .
though it is based on an approximation to the posterior , the resulting predictive distributions are very accurate for this class of
123 image analysis
finite gaussian mixture models are widely used in computer vision to model natural im - ages for the purposes of automatic clustering , retrieval , and classication ( barnard et al .
123; jeon et al .
these applications are often large - scale data analysis problems , involving thousands of data points ( images ) in hundreds of dimensions ( pixels ) .
the ap -
variational inference for dirichlet process mixtures
figure 123 : ( left ) the expected number of images allocated to each component in the variational posterior .
the posterior uses 123 components to describe the data .
( right ) the prior for the scaling parameter and the approximate posterior given by its vari -
propriate number of mixture components to use in these problems is generally unknown , and dp mixtures provide an attractive alternative to current methods .
however , a de - ployment of dp mixtures in such problems crucially requires inferential methods that are computationally ecient .
to demonstrate the applicability of our variational ap - proach to dp mixtures in the setting of large datasets , we analyzed a collection of 123 images from the associated press under the assumptions of a dp mixture of gaussians
each image was reduced to a 123 - dimensional real - valued vector given by an 123 grid of average red , green , and blue values .
we t a dp mixture model in which the mixture components are gaussian with mean and covariance matrix 123i .
the base distribution g123 was a product measuregamma ( 123 , 123 ) for 123 / 123 and n ( 123 , 123 ) for .
furthermore , we placed a gamma ( 123 , 123 ) prior on the dp scaling parameter , as described in appendix 123
we used a truncation level of 123 for the variational distribution .
the variational algorithm required approximately four hours to converge .
the re - sulting approximate posterior used 123 mixture components to describe the collection .
for a rough comparison to gibbs sampling , an iteration of collapsed gibbs takes 123 minutes with this data set .
in the same four hours , one could perform only 123 itera - tions .
this is not enough for a chain to converge to its stationary distribution , let alone provide a sucient number of uncorrelated samples to construct an empirical estimate of the posterior .
figure 123 ( left ) illustrates the expected number of images allocated to each compo - nent under the variational approximation to the posterior .
figure 123 illustrates the ten pictures with highest approximate posterior probability associated with each of four of the components .
these clusters appear to capture basketball shots , outdoor scenes on gray days , faces , and blue backgrounds .
figure 123 ( right ) illustrates the prior for the scaling parameter as well as the approximate posterior given by the tted variational distribution .
we see that the
blei and m .
jordan
approximate posterior is peaked and rather dierent from the prior , indicating that the data have provided information regarding .
we have developed a mean - eld variational inference algorithm for the dirichlet pro - cess mixture model and demonstrated its applicability to the kinds of multivariate data for which gibbs sampling algorithms can exhibit slow convergence .
variational infer - ence was faster than gibbs sampling in our simulations , and its convergence time was independent of dimensionality for the range which we tested .
both variational and mcmc methods have strengths and weaknesses , and it is un - likely that one methodology will dominate the other in general .
while mcmc sampling provides theoretical guarantees of accuracy , variational inference provides a fast , deter - ministic approximation to otherwise unattainable posteriors .
moreover , both mcmc and variational methods are computational paradigms , providing a wide variety of spe - cic algorithmic approaches which trade o speed , accuracy and ease of implementation in dierent ways .
we have investigated the deployment of the simplest form of varia - tional method for dp mixturesa mean - eld variational algorithmbut it worth noting that other variational approaches , such as those described in wainwright and jordan ( 123 ) , are also worthy of consideration in the nonparametric context .
variational inference in exponential families
in this appendix , we derive the coordinate ascent algorithm for variational inference described in section 123 .
recall that we are considering a latent variable model with hyperparameters , observed variables x = ( x123 , .
, xn ) , and latent variables w = ( w123 , .
, wm ) .
the posterior can be written as :
p ( w | x , ) = exp ( log p ( w , x | ) log p ( x | ) ) .
the variational bound on the log marginal probability is :
log p ( x | ) eq ( log p ( x , w | ) ) eq ( log q ( w ) ) .
this bound holds for any distribution q ( w ) .
selves to fully - factorized variational distributions of the form q ( w ) = qm
for the optimization of this bound to be computationally tractable , we restrict our - where = ( 123 , 123 , .
, m ) are variational parameters and each distribution is in the exponential family ( ghahramani and beal 123 ) .
we derive a coordinate ascent algo - rithm in which we iteratively maximize the bound with respect to each i , holding the other variational parameters xed .
variational inference for dirichlet process mixtures
let us rewrite the bound in equation ( 123 ) using the chain rule :
eq ( log p ( wm | x , w123 , .
, wm123 , ) )
eq ( log qm ( wm ) ) .
to optimize with respect to i , reorder w such that wi is last in the list .
the portion of equation ( 123 ) depending on i is :
log p ( x | ) log p ( x | ) +
`i = eq ( log p ( wi | wi , x , ) ) eq ( log qi ( wi ) ) .
given that the variational distribution qi ( wi ) is in the exponential family , we have :
qi ( wi ) = h ( wi ) exp ( t
i wi a ( i ) ) ,
and equation ( 123 ) simplies as follows :
`i = eq ( cid : 123 ) log p ( wi | wi , x , ) log h ( wi ) t
= eq ( log p ( wi | wi , x , ) ) eq ( log h ( wi ) ) t
i wi + a ( i ) ( cid : 123 )
i a123 ( i ) + a ( i ) ,
because eq ( wi ) = a123 ( i ) .
the derivative with respect to i is :
( eq ( log p ( wi | wi , x , ) ) eq ( log h ( wi ) ) ) t
the optimal i satises :
i = ( a123 ( i ) ) 123 ( cid : 123 )
eq ( log p ( wi | wi , x , ) )
eq ( log h ( wi ) ) ( cid : 123 ) .
the result in equation ( 123 ) is general .
in many applications of mean eld methods , including those in the current paper , a further simplication is achieved .
in particu - lar , suppose that the conditional distribution p ( wi | wi , x , ) is an exponential family distribution .
we have :
p ( wi | wi , x , ) = h ( wi ) exp ( gi ( wi , x , ) t wi a ( gi ( wi , x , ) ) ) ,
where gi ( wi , x , ) denotes the natural parameter for wi when conditioning on the remaining latent variables and the observations .
this yields simplied expressions for the expected log probability of wi and its rst derivative :
eq ( log p ( wi | wi , x , ) ) = eq ( log h ( wi ) ) + eq ( gi ( wi , x , ) ) t a123 ( i ) eq ( a ( gi ( wi , x , ) ) )
eq ( log p ( wi | wi , x , ) ) =
eq ( log h ( wi ) ) + eq ( gi ( wi , x , ) ) t a123 ( i ) .
using the rst derivative in equation ( 123 ) , the maximum is attained at :
i = eq ( gi ( wi , x , ) ) .
blei and m .
jordan
we dene a coordinate ascent algorithm based on equation ( 123 ) by iteratively updating i for i ( 123 , .
such an algorithm nds a local maximum of equation ( 123 ) by proposition 123 . 123 of bertsekas ( 123 ) , under the condition that the right - hand side of equation ( 123 ) is strictly convex .
relaxing the two assumptions complicates the algorithm , but the basic idea re - if p ( wi | wi , x , ) is not in the exponential family , then there may mains the same .
not be an analytic expression for the update in equation ( 123 ) .
if q ( w ) is not a fully factorized distribution , then the second term of the bound in equation ( 123 ) becomes eq ( log q ( wi | wi ) ) and the subsequent simplications may not be applicable .
further perspectives on algorithms of this kind can be found in xing et al .
( 123 ) , ghahramani and beal ( 123 ) , and wiegerinck ( 123 ) .
for a more general treatment of variational methods for statistical inference , see wainwright and jordan ( 123 ) .
placing a prior on the scaling parameter
the scaling parameter can have a signicant eect on the growth of the number of components grows with the data , and it is generally important to consider extended models which integrate over .
for the urn - based samplers , escobar and west ( 123 ) place a gamma ( s123 , s123 ) prior on and implement the corresponding gibbs updates with auxiliary variable methods .
in the stick - breaking representation , the gamma distribution is convenient because it is conjugate to the stick lengths .
writing the gamma distribution in its canonical form we have :
p ( | s123 , s123 ) = ( 123 / ) exp ( s123 + s123 log a ( s123 , s123 ) ) ,
where s123 is the shape parameter and s123 is the inverse scale parameter .
this distribution is conjugate to beta ( 123 , ) .
the log normalizer is :
and the posterior parameters conditional on data ( v123 , .
, vk ) are :
a ( s123 , s123 ) = log ( s123 ) s123 log s123 ,
s123 = s123 pk
s123 = s123 + k .
i=123 log ( 123 vi )
we extend the variational inference algorithm to include posterior updates for the scaling parameter .
the variational distribution is gamma ( w123 , w123 ) .
the variational parameters are updated as follows :
w123 = s123 + t 123
and we replace with its expectation eq ( ) = w123 / w123 in the updates for t , 123 in equa -
w123 = s123
eq ( log ( 123 vi ) ) ) ,
variational inference for dirichlet process mixtures
