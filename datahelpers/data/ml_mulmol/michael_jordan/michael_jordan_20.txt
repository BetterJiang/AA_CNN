we introduce supervised latent dirichlet allocation ( slda ) , a statistical model of labelled documents .
the model accommodates a variety of response types .
we derive a maximum - likelihood procedure for parameter estimation , which relies on variational approximations to handle intractable posterior expectations .
prediction problems motivate this research : we use the tted model to predict response values for new documents .
we test slda on two real - world problems : movie ratings predicted from reviews , and web page popularity predicted from text descriptions .
we illustrate the benets of slda versus modern regularized regression , as well as versus an unsupervised lda analysis followed by a separate regression .
there is a growing need to analyze large collections of electronic text .
the complexity of document corpora has led to considerable interest in applying hierarchical statistical models based on what are called topics .
formally , a topic is a probability distribution over terms in a vocabulary .
informally , a topic represents an underlying semantic theme; a document consisting of a large number of words might be concisely modelled as deriving from a smaller number of topics .
such topic models provide useful descriptive statistics for a collection , which facilitates tasks like browsing , searching , and assessing document similarity .
most topic models , such as latent dirichlet allocation ( lda ) ( 123 ) , are unsupervised : only the words in the documents are modelled .
the goal is to infer topics that maximize the likelihood ( or the pos - terior probability ) of the collection .
in this work , we develop supervised topic models , where each document is paired with a response .
the goal is to infer latent topics predictive of the response .
given an unlabeled document , we infer its topic structure using a tted model , then form its pre - diction .
note that the response is not limited to text categories .
other kinds of document - response corpora include essays with their grades , movie reviews with their numerical ratings , and web pages with counts of how many online community members liked them .
unsupervised lda has previously been used to construct features for classication .
the hope was that lda topics would turn out to be useful for categorization , since they act to reduce data di - mension ( 123 ) .
however , when the goal is prediction , tting unsupervised topics may not be a good choice .
consider predicting a movie rating from the words in its review .
intuitively , good predictive topics will differentiate words like excellent , terrible , and average , without regard to genre .
but topics estimated from an unsupervised model may correspond to genres , if that is the dominant structure in the corpus .
the distinction between unsupervised and supervised topic models is mirrored in existing dimension - reduction techniques .
for example , consider regression on unsupervised principal com - ponents versus partial least squares and projection pursuit ( 123 ) , which both search for covariate linear combinations most predictive of a response variable .
these linear supervised methods have non -
parametric analogs , such as an approach based on kernel ica ( 123 ) .
in text analysis , mccallum et al .
developed a joint topic model for words and categories ( 123 ) , and blei and jordan developed an lda model to predict caption words from images ( 123 ) .
in chemogenomic proling , flaherty et al .
( 123 ) proposed labelled lda , which is also a joint topic model , but for genes and protein function categories .
it differs fundamentally from the model proposed here .
this paper is organized as follows .
we rst develop the supervised latent dirichlet allocation model ( slda ) for document - response pairs .
we derive parameter estimation and prediction algorithms for the real - valued response case .
then we extend these techniques to handle diverse response types , using generalized linear models .
we demonstrate our approach on two real - world problems .
first , we use slda to predict movie ratings based on the text of the reviews .
second , we use slda to predict the number of diggs that a web page will receive in the www . digg . com community , a forum for sharing web content of mutual interest .
the digg count prediction for a page is based on the pages description in the forum .
in both settings , we nd that slda provides much more predictive power than regression on unsupervised lda features .
the slda approach also improves on the lasso , a modern regularized regression technique .
123 supervised latent dirichlet allocation
in topic models , we treat the words of a document as arising from a set of latent topics , that is , a set of unknown distributions over the vocabulary .
documents in a corpus share the same set of k topics , but each document uses a mix of topics unique to itself .
thus , topic models are a relaxation of classical document mixture models , which associate each document with a single unknown topic .
here we build on latent dirichlet allocation ( lda ) ( 123 ) , a topic model that serves as the basis for many others .
in lda , we treat the topic proportions for a document as a draw from a dirichlet distribution .
we obtain the words in the document by repeatedly choosing a topic assignment from those proportions , then drawing a word from the corresponding topic .
in supervised latent dirichlet allocation ( slda ) , we add to lda a response variable associated with each document .
as mentioned , this variable might be the number of stars given to a movie , a count of the users in an on - line community who marked an article interesting , or the category of a document .
we jointly model the documents and the responses , in order to nd latent topics that will best predict the response variables for future unlabeled documents .
we emphasize that slda accommodates various types of response : unconstrained real values , real values constrained to be positive ( e . g . , failure times ) , ordered or unordered class labels , nonnegative integers ( e . g . , count data ) , and other types .
however , the machinery used to achieve this generality complicates the presentation .
so we rst give a complete derivation of slda for the special case of an unconstrained real - valued response .
then , in section 123 , we present the general version of slda , and explain how it handles diverse response types .
focus now on the case y r .
fix for a moment the model parameters : the k topics 123 : k ( each k a vector of term probabilities ) , the dirichlet parameter , and the response parameters and 123
under the slda model , each document and response arises from the following generative process :
draw topic proportions | dir ( ) .
for each word
( a ) draw topic assignment zn | mult ( ) .
draw response variable y | z123 : n , , 123 n ( cid : 123 ) >z , 123 ( cid : 123 ) .
( b ) draw word wn | zn , 123 : k mult ( zn
here we dene z : = ( 123 / n ) pn
n=123 zn .
the family of probability distributions corresponding to this
generative process is depicted as a graphical model in figure 123
notice the response comes from a normal linear model .
the covariates in this model are the ( un - observed ) empirical frequencies of the topics in the document .
the regression coefcients on those frequencies constitute .
note that a linear model usually includes an intercept term , which amounts to adding a covariate that always equals one .
here , such a term is redundant , because the compo - nents of z always sum to one .
by regressing the response on the empirical topic frequencies , we treat the response as non - exchangeable with the words .
the document ( i . e . , words and their topic assignments ) is generated rst , under full word exchangeability; then , based on the document , the response variable is gen - erated .
in contrast , one could formulate a model in which y is regressed on the topic proportions .
this treats the response and all the words as jointly exchangeable .
but as a practical matter , our chosen formulation seems more sensible : the response depends on the topic frequencies which actually occurred in the document , rather than on the mean of the distribution generating the topics .
moreover , estimating a fully exchangeable model with enough topics allows some topics to be used entirely to explain the response variables , and others to be used to explain the word occurrences .
this degrades predictive performance , as demonstrated in ( 123 ) .
we treat , 123 : k , , and 123 as unknown constants to be estimated , rather than random variables .
we carry out approximate maximum - likelihood estimation using a variational expectation - maximization ( em ) procedure , which is the approach taken in unsupervised lda as well ( 123 ) .
123 variational e - step
given a document and response , the posterior distribution of the latent variables is
p ( , z123 : n | w123 : n , y , , 123 : k , , 123 ) = p ( y | z123 : n , , 123 ) n=123 p ( zn | ) p ( wn | zn , 123 : k ) r d p ( | ) p n=123 p ( zn | ) p ( wn | zn , 123 : k )
p ( | )
p ( y | z123 : n , , 123 )
the normalizing value is the marginal probability of the observed data , i . e . , the document w123 : n and response y .
this normalizer is also known as the likelihood , or the evidence .
as with lda , it is not efciently computable .
thus , we appeal to variational methods to approximate the posterior .
variational objective function .
we maximize the evidence lower bound ( elbo ) l ( ) , which for a single document has the form
log p ( cid : 123 ) w123 : n , y | , 123 : k , , 123 ( cid : 123 ) l ( , 123 : n; , 123 : k , , 123 ) = e ( log p ( | ) ) +
e ( log p ( wn | zn , 123 : k ) ) + e ( log p ( y | z123 : n , , 123 ) ) + h ( q ) .
e ( log p ( zn | ) ) + nx
here the expectation is taken with respect to a variational distribution q .
we choose the fully factor -
q ( , z123 : n | , 123 : n ) = q ( | ) qn
n=123 q ( zn | n ) ,
figure 123 : ( left ) a graphical model representation of supervised latent dirichlet allocation .
( bottom ) the topics of a 123 - topic slda model t to the movie review data of section 123
bothmotionsimpleperfectfascinatingpowercomplexhowevercinematographyscreenplayperformancespictureseffectivepicturehistheircharactermanywhileperformancebetween ! 123 ! 123 ! 123 ! ! ! ! ! ! ! ! ! ! morehasthanlmsdirectorwillcharactersonefromtherewhichwhomuchwhatawfulfeaturingroutinedryofferedcharlieparisnotaboutmovieallwouldtheyitshavelikeyouwasjustsomeoutbadguyswatchableitsnotonemovieleastproblemunfortunatelysupposedworseatdulldzd , nwd , nndkkyd , 123 ( 123 )
where is a k - dimensional dirichlet parameter vector and each n parametrizes a categorical dis - tribution over k elements .
notice e ( zn ) = n .
the rst three terms and the entropy of the variational distribution are identical to the corresponding log ( cid : 123 ) 123 123 ( cid : 123 ) ( cid : 123 ) terms in the elbo for unsupervised lda ( 123 ) .
the fourth term is the expected log probability of the response variable given the latent topic assignments , e ( log p ( y | z123 : n , , 123 ) ) = = 123 the rst expectation is e ( cid : 123 ) z ( cid : 123 ) = : = ( 123 / n ) pn > ( cid : 123 ) = ( 123 / n 123 ) to see ( 123 ) , notice that for m 123= n , e ( zn z> m ) = e ( zn ) e ( zm ) > = n> distribution is fully factorized .
on the other hand , e ( zn z> is an indicator vector .
for a single document - response pair , we maximize ( 123 ) with respect to 123 : n and to obtain an estimate of the posterior .
we use block coordinate - ascent variational inference , maximizing with respect to each variational parameter vector in turn .
optimization with respect to .
the terms that involve the variational dirichlet are identical to those in unsupervised lda , i . e . , they do not involve the response variable y .
thus , the coordinate ascent update is as in ( 123 ) ,
m because the variational n ) = diag ( e ( zn ) ) = diag ( n ) because zn
e ( cid : 123 ) z z e ( cid : 123 ) z ( cid : 123 ) + >
n=123 n , and the second expectation is
e ( cid : 123 ) z z
n123= j n .
given j ( 123 , .
in ( 123 ) , we maximize the lagrangian of the elbo , which incorporates the constraint that the components of j sum to one , and obtain the coordinate update
optimization with respect to j .
dene j : = p e ( log | ) + e ( log p ( w j | 123 : k ) ) + ( cid : 123 ) y that e ( log i | ) = ( i ) ( p j ) , where ( ) is the digamma function .
exponentiating a vector means forming the vector of exponentials .
the proportionality symbol means the components of new are computed according to ( 123 ) , then normalized to sum to one
( cid : 123 ) + ( ) ( cid : 123 )
123n 123 123
the central difference between lda and slda lies in this update .
as in lda , the jth words variational distribution over topics depends on the words topic probabilities under the actual model ( determined by 123 : k ) .
but w j s variational distribution , and those of all other words , affect the probability of the response , through the expected residual sum of squares ( rss ) , which is the second term in ( 123 ) .
the end result is that the update ( 123 ) also encourages j to decrease this expected rss .
the update ( 123 ) depends on the variational parameters j of all other words .
thus , unlike lda , the j cannot be updated in parallel .
distinct occurrences of the same term are treated separately .
123 m - step and prediction
the corpus - level elbo lower bounds the joint log likelihood across documents , which is the sum of the per - document log - likelihoods .
in the e - step , we estimate the approximate posterior distribution for each document - response pair using the variational inference algorithm described above .
in the m - step , we maximize the corpus - level elbo with respect to the model parameters 123 : k , , and 123
for our purposes , it sufces simply to x to 123 / k times the ones vector .
in this section , we add document indexes to the previous sections quantities , so y becomes yd and z becomes zd .
estimating the topics .
the m - step updates of the topics 123 : k are the same as for unsupervised lda , where the probability of a word under a topic is proportional to the expected number of times that it was assigned to that topic ( 123 ) ,
123 ( wd , n = w ) k
here again , proportionality means that each new estimating the regression parameters .
the only terms of the corpus - level elbo involving and 123 come from the corpus - level analog of ( 123 ) .
dene y = y123 : d as the vector of response values across documents .
let a be the d ( k + 123 ) matrix whose rows are the vectors z>
then the corpus - level version of ( 123 ) is
is normalized to sum to one .
( y a ) > ( y a )
e ( log p ( y | a , , 123 ) ) = d
log ( 123 123 ) 123
here the expectation is over the matrix a , using the variational distribution parameters chosen in the previous e - step .
expanding the inner product , using linearity of expectation , and applying the rst - order condition for , we arrive at an expected - value version of the normal equations :
a ( cid : 123 ) = e ( a ) ( cid : 123 ) , with each term having a xed value from the previous e - step step .
also , e ( cid : 123 ) a> a ( cid : 123 ) =p d e ( cid : 123 ) zd note that the dth row of e ( a ) is just d , and all these average vectors were xed in the previous e - as well , given by ( 123 ) .
we caution again : formulas in the previous section , such as ( 123 ) , suppress the document indexes which appear here .
we now apply the rst - order condition for 123 to ( 123 ) and evaluate the solution at new , obtaining :
prediction .
our focus in applying slda is prediction .
specically , we wish to compute the ex - pected response value , given a new document w123 : n and a tted model ( , 123 : k , , 123 ) :
e ( y | w123 : n , , 123 : k , , 123 ) = >
e ( z | w123 : n , , 123 : k ) .
the identity follows easily from iterated expectation .
we approximate the posterior mean of z using the variational inference procedure of the previous section .
but here , the terms depending on y are removed from the j update in ( 123 ) .
notice this is the same as variational inference for unsupervised lda : since we averaged the response variable out of the right - hand side in ( 123 ) , what remains is the standard unsupervised lda model for z123 : n and .
thus , given a new document , we rst compute eq ( z123 : n ) , the variational posterior distribution of the latent variables zn .
then , we estimate the response with e ( y | w123 : n , , 123 : k , , 123 ) > 123 diverse response types via generalized linear models
eq ( z ) = > .
up to this point , we have conned our attention to an unconstrained real - valued response variable .
in many applications , however , we need to predict a categorical label , or a non - negative integral count , or a response with other kinds of constraints .
sometimes it is reasonable to apply a normal linear model to a suitably transformed version of such a response .
when no transformation results in approximate normality , statisticians often make use of a generalized linear model , or glm ( 123 ) .
in this section , we describe slda in full generality , replacing the normal linear model of the earlier exposition with a glm formulation .
as we shall see , the result is a generic framework which can be specialized in a straightforward way to supervised topic models having a variety of response types .
there are two main ingredients in a glm : the random component and the systematic compo - nent .
for the random component , one takes the distribution of the response to be an exponential dispersion family with natural parameter and dispersion parameter :
p ( y | , ) = h ( y , ) exp
( cid : 123 ) y a ( )
for each xed , ( 123 ) is an exponential family , with base measure h ( y , ) , sufcient statistic y , and log - normalizer a ( ) .
the dispersion parameter provides additional exibility in modeling the variance of y .
note that ( 123 ) need not be an exponential family jointly in ( , ) .
in the systematic component of the glm , we relate the exponential - family parameter of the ran - dom component to a linear combination of covariates the so - called linear predictor .
for slda , the linear predictor is >z .
in fact , we simply set = >z .
thus , in the general version of slda , the previous specication in step 123 of the generative process is replaced with
p ( y | z123 : n , , ) = h ( y , ) exp
y | z123 : n , , glm ( z , , ) ,
( cid : 123 ) > ( zy ) a ( >z )
e ( log p ( y | z123 : n , , ) ) = log h ( y , ) + 123
the reader familiar with glms will recognize that our choice of systematic component means slda uses only canonical link functions .
in future work , we will relax this constraint .
we now have the exibility to model any type of response variable whose distribution can be written in exponential dispersion form ( 123 ) .
as is well known , this includes many commonly used distribu - tions : the normal; the binomial ( for binary response ) ; the poisson and negative binomial ( for count data ) ; the gamma , weibull , and inverse gaussian ( for failure time data ) ; and others .
each of these
distributions corresponds to a particular choice of h ( y , ) and a ( ) .
for example , it is easy to show that the normal distribution corresponds to h ( y , ) = ( 123 / 123 ) exp ( y123 / ( 123 ) ) and a ( ) = 123 / 123
in this case , the usual parameters and 123 just equal and , respectively .
> ( cid : 123 ) e ( cid : 123 ) z ( cid : 123 ) y ( cid : 123 ) e ( cid : 123 ) a ( > z ) ( cid : 123 ) i variational e - step .
the distribution of y appears only in the cross - entropy term ( 123 ) .
its form under the glm is = e ( log | ) +e ( log p ( w j | 123 : k ) ) log j +123+ ( cid : 123 ) y
this changes the coordinate ascent step for each j , but the variational optimization is otherwise unaffected .
in particular , the gradient of the elbo with respect to j becomes thus , the key to variational inference in slda is obtaining the gradient of the expected glm log - normalizer .
sometimes there is an exact expression , such as the normal case of section 123
as another example , the poisson glm leads to an exact gradient , which we omit for brevity .
other times , no exact gradient is available .
in a longer paper ( 123 ) , we study two methods for this situation .
first , we can replace e ( a ( > z ) ) with an adjustable lower bound whose gradient is known exactly; then we maximize over the original variational parameters plus the parameter con - trolling the bound .
alternatively , an application of the multivariate delta method for moments ( 123 ) , plus standard exponential family theory , shows
e ( cid : 123 ) a ( > z ) ( cid : 123 ) o
e ( cid : 123 ) a ( > z ) ( cid : 123 ) a ( > ) + varglm ( y | = > ) >
here , varglm denotes the response variance under the glm , given a specied value of the natu - ral parameterin all standard cases , this variance is a closed - form function of j .
the variance - covariance matrix of z under q is already known in closed from from e ( z ) and ( 123 ) .
thus , computing / j of ( 123 ) exactly is mechanical .
however , using this approximation gives up the usual guaran - tee that the elbo lower bounds the marginal likelihood .
we forgo details and further examples due to space constraints .
the glm contribution to the gradient determines whether the j coordinate update itself has a closed form , as it does in the normal case ( 123 ) and the poisson case ( omitted ) .
if the update is not closed - form , we use numerical optimization , supplying a gradient obtained from one of the methods described in the previous paragraph .
parameter estimation ( m - step ) .
the topic parameter estimates are given by ( 123 ) , as before .
for the corpus - level elbo , the gradient with respect to becomes
varq ( z ) .
> d yd e ( cid : 123 ) a ( > zd ) ( cid : 123 ) o = ( cid : 123 ) 123
d yd dx
the appearance of ( ) = eglm ( y | = ) follows from exponential family properties .
this glm mean response is a known function of > zd in all standard cases .
however , eq ( ( > zd ) zd ) has
( cid : 123 ) ( > zd ) zd
figure 123 : predictive r123 and per - word likelihood for the movie and digg data ( see section 123 ) .
an exact solution only in some cases ( e . g .
normal , poisson ) .
in other cases , we approximate the expectation with methods similar to those applied for the j coordinate update .
reference ( 123 ) has details , including estimation of and prediction , where we encounter the same issues .
the derivative with respect to , evaluated at new , is
zd ) zd
h ( yd , ) /
h ( yd , )
d yd dx
given that the rightmost summation has been evaluated , exactly or approximately , during the optimization , ( 123 ) has a closed form .
depending on h ( y , ) and its partial with respect to , we obtain new either in closed form or via one - dimensional numerical optimization .
prediction .
we form predictions just as in section 123 .
the difference is that we now approximate the expected response value of a test document as
e ( y | w123 : n , , 123 : k , , ) eq ( ( > z ) ) .
again , this follows from iterated expectation plus the variational approximation .
when the varia - tional expectation cannot be computed exactly , we apply the approximation methods we relied on for the glm e - step and m - step .
we defer specics to ( 123 ) .
123 empirical results
we evaluated slda on two prediction problems .
first , we consider sentiment analysis of news - paper movie reviews .
we use the publicly available data introduced in ( 123 ) , which contains movie reviews paired with the number of stars given .
while pang and lee treat this as a classication problem , we treat it as a regression problem .
with a 123 - term vocabulary chosen by tf - idf , the corpus contains 123 documents and comprises 123m words .
second , we introduce the problem of predicting web page popularity on digg . com .
digg is a com - munity of users who share links to pages by submitting them to the digg homepage , with a short description .
once submitted , other users digg the links they like .
links are sorted on the digg homepage by the number of diggs they have received .
our digg data set contains a year of link descriptions , paired with the number of diggs each received during its rst week on the homepage .
( this corpus will be made publicly available at publication . ) we restrict our attention to links in the technology category .
after trimming the top ten outliers , and using a 123 - term vocabulary chosen by tf - idf , the digg corpus contains 123 documents and comprises 123k words .
for both sets of response variables , we transformed to approximate normality by taking logs .
this makes the data amenable to the continuous - response model of section 123; for these two problems , generalized linear modeling turned out to be unnecessary .
we initialized 123 : k to uniform topics , 123 to the sample variance of the response , and to a grid on ( 123 , 123 ) in increments of 123 / k .
we ran em until the relative change in the corpus - level likelihood bound was less than 123% .
in the e - step , we ran coordinate - ascent variational inference for each document until the relative change in the
123 . 123 . 123 . 123number of topicspredictive r123 . 123 . 123 . 123number of topicsperword held out log likelihood123 . 123 . 123 . 123number of topicsperword held out log likelihood123 . 123 . 123 . 123number of topicspredictive r123sldaldamovie corpusdigg corpus per - document elbo was less than 123% .
for the movie review data set , we illustrate in figure 123 a matching of the top words from each topic to the corresponding coefcient k .
captured by the out - of - fold predictions : pr123 : = 123 ( p ( y y ) 123 ) / ( p ( y y ) 123 ) .
we assessed the quality of the predictions with predictive r123
in our 123 - fold cross - validation ( cv ) , we dened this quantity as the fraction of variability in the out - of - fold response values which is we compared slda to linear regression on the d from unsupervised lda .
this is the regression equivalent of using lda topics as classication features ( 123 ) . figure 123 ( l ) illustrates that slda pro - vides improved predictions on both data sets .
moreover , this improvement does not come at the cost of document model quality .
the per - word hold - out likelihood comparison in figure 123 ( r ) shows that slda ts the document data as well or better than lda .
note that digg prediction is signicantly harder than the movie review sentiment prediction , and that the homogeneity of digg technology content leads the model to favor a small number of topics .
finally , we compared slda to the lasso , which is l123 - regularized least - squares regression .
the lasso is a widely used prediction method for high - dimensional problems .
we used each documents empirical distribution over words as its lasso covariates , setting the lasso complexity parameter with 123 - fold cv .
on digg data , the lassos optimal model complexity yielded a cv pr123 of 123 .
the best slda pr123 was 123 , an 123% relative improvement .
on movie data , the best lasso pr123 was 123 versus 123 for slda , a 123% relative improvement .
note moreover that the lasso provides only a prediction rule , whereas slda models latent structure useful for other purposes .
we have developed slda , a statistical model of labelled documents .
the model accommodates the different types of response variable commonly encountered in practice .
we presented a variational procedure for approximate posterior inference , which we then incorporated in an em algorithm for maximum - likelihood parameter estimation .
we studied the models predictive performance on two real - world problems .
in both cases , we found that slda moderately improved on the lasso , a state - of - the - art regularized regression method .
moreover , the topic structure recovered by slda had higher hold - out likelihood than lda on one problem , and equivalent hold - out likelihood on the other .
these results illustrate the benets of supervised dimension reduction when prediction is the
david m .
blei is supported by grants from google and the microsoft corporation .
