bayesian methods have a number of virtues , particularly their uniform treatment of uncertainty at all levels of the modeling process .
the formalism also allows ready incorporation of prior knowledge and the seamless combination of such knowledge with observed data ( bernardo and smith 123 , gelman 123 , heckerman , geiger and chickering 123 ) .
the elegant seman - tics , however , often comes at a sizable computational cost pos - terior distributions resulting from the incorporation of observed data must be represented and updated , and this generally involves high - dimensional integration .
the computational cost involved in carrying out these operations can call into question the viabil - ity of bayesian methods even in relatively simple settings , such as generalized linear models ( mccullagh and nelder 123 ) .
we concern ourselves in this paper with a particular generalized linear model logistic regression and we focus on bayesian calculations that are computationally tractable .
in particular we describe a exible deterministic approximation procedure that allows the posterior distribution in logistic regression to be rep - resented and updated efciently .
we also show how our meth - ods permit a bayesian treatment of a more complex model a 123 - 123 c ( cid : 123 ) 123 kluwer academic publishers
directed graphical model ( a belief network ) in which each node is a logistic regression model .
the deterministic approximation methods that we develop in this paper are known generically as variational methods .
variational techniques have been used extensively in the physics literature ( see , e . g . , parisi 123 , sakurai 123 ) and have also found applications in statistics ( rustagi 123 ) .
roughly speak - ing , the objective of these methods is to transform the problem of interest into an optimization problem via the introduction of extra degrees of freedom known as variational parameters .
for xed values of the variational parameters the transformed prob - lem often has a closed form solution , providing an approximate solution to the original problem .
the variational parameters are adjusted via an optimization algorithm to yield an improving sequence of approximations .
for an introduction to variational methods in the context of graphical models see jordan et al .
let us briey sketch the variational method that we develop in this paper .
we study a logistic regression model with a gaussian prior on the parameter vector .
our variational transformation replaces the logistic function with an adjustable lower bound that has a gaussian form; that is , an exponential of a quadratic
jaakkola and jordan
function of the parameters .
the product of the prior and the variationally transformed likelihood thus yields a gaussian ex - pression for the posterior ( conjugacy ) , which we optimize varia - tionally .
this procedure is iterated for each successive data point .
our methods can be compared to the laplace approximation for logistic regression ( cf .
spiegelhalter and lauritzen 123 ) , a closely related method which also utilizes a gaussian approxi - mation to the posterior .
to anticipate the discussion in following sections , we will see that the variational approach has an ad - vantage over the laplace approximation; in particular , the use of variational parameters gives the variational approach greater exibility .
we will show that this exibility translates into im - proved accuracy of the approximation .
variational methods can also be contrasted with sampling techniques , which have become the method of choice in bayesian statistics ( thomas , spiegelhalter and gilks 123 , neal 123 , gilks , richardson and spiegelhalter 123 ) .
sampling techniques enjoy wide applicability and can be powerful in eval - uating multi - dimensional integrals and representing posterior distributions .
they do not , however , yield closed form solu - tions nor do they guarantee monotonically improving approxi - mations .
it is precisely these features that characterize variational
the paper is organized as follows .
first we describe in some detail a variational approximation method for bayesian logistic regression .
this is followed by an evaluation of the accuracy of the method and a comparison to laplace approximation .
we then extend the framework to belief networks , considering both complete data and incomplete data .
finally , we consider the dual of the regression problem and show that our techniques lead to exactly solvable em updates .
bayesian logistic regression
we begin with a logistic regression model given by :
p ( s d 123j x; ( cid : 123 ) ) d g ( ( cid : 123 ) t x ) ;
where g ( x ) d ( 123c e 123 is the logistic function , s the binary response variable , and x d fx123; : : : ; xrg the set of explana - tory variables .
we represent the uncertainty in the parameter values ( cid : 123 ) via a prior distribution p ( ( cid : 123 ) ) which we assume to be a gaussian with possibly full covariance structure .
our predictive distribution is therefore : p ( s j x ) d
p ( s j x; ( cid : 123 ) ) p ( ( cid : 123 ) ) d ( cid : 123 ) :
in order to utilize this distribution we need to be able to compute the posterior parameter distribution p ( ( cid : 123 ) j d123; : : : ; dt ) , where we assume that each dt d fst ; x t g is a complete ob - servation .
this calculation is intractable for larger n or t , thus we consider a variational approximation .
; : : : ; x t
our approach involves nding a variational transformation of the logistic function and using this transformed function as an approximate likelihood .
in particular we wish to consider
a convex function f and its two tangent lines .
the locations of the tangents are indicated with short vertical line segments
transformations that combine readily with a gaussian prior in the sense that the gaussian prior becomes the conjugate prior to the transformed likelihood .
we begin by introducing the type of variational transformations we will use for this purpose .
a brief introduction to variational methods
consider any continuously differentiable convex function f ( z ) .
figure 123 provides an example of a convex function that we will make use of later on .
convexity of this function guarantees by denition that any tangent line always remains below the func - tion itself .
we may thus interpret the collection of all the tangent lines as a parameterized family of lower bounds for this convex function ( cf .
convex duality , rockafellar 123 ) .
the tangents in this family are naturally parameterized by their locations .
from the point of view of approximating the convex non - linear func - tion f , it seems natural to use one of the simpler tangent lines as a lower bound .
to formulate this a little more precisely , let l ( z; z123 ) be the tangent line at z d z123 ,
f ( z ) jzdz123 ( z z123 ) ;
l ( z; z123 ) d f ( z123 ) c @
f ( z ) l ( z; z123 ) for all z; z123 and f ( z123 ) d then it follows that l ( z123; z123 ) .
in the terminology of variational methods , l ( z; z123 ) is a variational lower bound of f ( z ) where the parameter z123 is known as the variational parameter .
since the lower bound l ( z; z123 ) is considerably simpler ( linear in this case ) than the non - linear function f ( z ) , it may be attractive to substitute the lower bound for f .
note that we are free to adjust the variational para - meter z123 , the location of the tangent , so as to make l ( z; z123 ) as accurate an approximation of f ( z ) as possible around the point of interest , i . e . , when z z123
the quality of this approximation degrades as z receeds from z123; the rate at which this happens depends on the curvature of f ( z ) .
whenever the function f has relatively low curvature as is the case in fig .
123 , the adjustable linear approximation seems quite attractive .
bayesian parameter estimation
variational methods in bayesian logistic regression
log g ( x ) d log ( 123 c e
here we illustrate how variational methods , of the type described above , can be used to transform the logistic likelihood function into a form that readily combines with the gaussian prior ( conju - gacy ) .
more precisely , the transformed logistic function should depend on the parameters ( cid : 123 ) at most quadratically in the expo - nent .
we begin by symmetrizing the log logistic function :
and noting that f ( x ) d log ( ex=123ce x=123 ) , is a convex function in the variable x 123
( this is readily veried by taking second derivatives; the behavior of f ( x ) as a function of x 123 is shown in fig .
as discussed above , a tangent surface to a convex function is a global lower bound for the function and thus we can bound f ( x ) globally with a rst order taylor expansion in the variable x 123 :
log ( ex=123 c e
x ) d x
f ( x ) f ( ) c @ f ( )
( x 123 123 )
c log g ( ) c 123
note that this lower bound is exact whenever 123 d x 123
combin - ing this result with equation ( 123 ) and exponentiating yields the desired variational transformation of the logistic function :
( x 123 123 ) :
p ( s j x; ( cid : 123 ) ) d g ( hs )
g ( ) exp
where hs d ( 123s 123 ) ( cid : 123 ) t x and ( ) d tanh ( =123 ) = ( 123 ) .
we also introduce the following notation : p ( s j x; ( cid : 123 ) ; ) g ( ) exp that is , p ( s j x; ( cid : 123 ) ; ) denotes the variational lower bound on the logistic function g ( hs ) .
as a lower bound it is no longer normalized .
we refer to equation ( 123 ) as a - transformation of the conditional probability .
for each xed value of hs we can in fact recover the ex - act value of the logistic function via a particular choice of the variational parameter .
indeed , maximizing the lower bound with respect to yields d hs; substituting this value back into the lower bound recovers the original conditional probability .
for all other values of we obtain a lower bound .
the true posterior p ( ( cid : 123 ) j d ) can be computed by normalizing p ( s j x; ( cid : 123 ) ) p ( ( cid : 123 ) ) .
given that this calculation is not feasible in general , we instead form the bound :
p ( s j x; ( cid : 123 ) ) p ( ( cid : 123 ) ) p ( s j x; ( cid : 123 ) ; ) p ( ( cid : 123 ) )
and normalize the variational approximation p ( s j x; ( cid : 123 ) ; ) p ( ( cid : 123 ) ) .
given that p ( ( cid : 123 ) ) is gaussian and given our choice of a gaussian variational form for p ( s j x; ( cid : 123 ) ; ) , the normali - zed variational distribution is a gaussian .
note that although
p ( s j x; ( cid : 123 ) ; ) is a lower bound on the true conditional probabil - ity , our variational posterior approximation is a proper density and thus no longer a bound .
this approximate bayesian up - date amounts to updating the prior mean and the prior co - variance matrix 123 into the posterior mean and the posterior covariance matrix .
omitting the algebra we nd that the updates take the following form :
d 123 c 123 ( ) x x t
pos d 123pos
for a single observation ( s; x ) , where x d ( x123 : : xr ) t .
suc - cessive observations can be incorporated into the posterior by applying these updates recursively .
our work is not nished , however , because the posterior co - variance matrix depends on the variational parameter through ( ) and we have yet to specify .
we choose via an optimiza - tion procedure; in particular , we nd a value of that yields a tight lower bound in equation ( 123 ) .
the fact that the variational expression in equation ( 123 ) is a lower bound is important it allows us to use the em algorithm to perform the optimization .
we derive such an em algorithm in appendix a; the result is the following ( closed form ) update equation for : 123 d ef ( ( cid : 123 ) t x ) 123g d x t 123post x c ( x t post ) 123;
where the expectation is taken with respect to p ( ( cid : 123 ) j d; old ) , the variational posterior distribution based on the previous value of .
owing to the em formulation , each update for corres - ponds to a monotone improvement to the posterior approxima - tion .
empirically we nd that this procedure converges rapidly; only a few iterations are needed .
the accuracy of the approxi - mation is considered in the following two sections .
p ( s j x; ( cid : 123 ) ) p ( ( cid : 123 ) jd ) d ( cid : 123 ) ;
to summarize , the variational approach allows us to obtain a closed form expression for the posterior predictive distribution in logistic regression : p ( s j x;d ) d
where the posterior distribution p ( ( cid : 123 ) jd ) comes from making a single pass through the data setd d fd123; : : : ; dtg , applying the updates in equations ( 123 ) and ( 123 ) after optimizing the associated variational parameters at each step .
the predictive lower bound p ( st j x t ;d ) takes the form : log p ( st j x t ;d ) d log g ( t ) t t c 123
c ( t ) 123
for any complete observation dt , where and 123 signify the parameters in p ( ( cid : 123 ) jd ) and the subscript t refers to the posterior p ( ( cid : 123 ) jd; dt ) found by augmenting the data set to include the point dt .
we note nally that the variational bayesian calculations pre - sented above need not be carried out sequentially .
we could compute a variational approximation to the posterior probabil - ity p ( ( cid : 123 ) jd ) by introducing ( separate ) transformations for each of the logistic functions in p ( d j ( cid : 123 ) ) d
g ( ( 123st 123 ) ( cid : 123 ) t x t )
p ( st j x t ; ( cid : 123 ) ) d
the resulting variational parameters would have to be optimized jointly rather than one at a time .
we believe the sequential approach provides a cleaner solution .
accuracy of the variational method
the logistic function is shown in fig .
123 ( a ) , along with a vari - ational approximation for d 123
as we have noted , for each value of the variational parameter , there is a particular point x where the approximation is exact; for the remaining values of x the approximation is a lower bound .
integrating equation ( 123 ) over the parameters we obtain a lower bound on the predictive probability of an observation .
the tight - ness of this lower bound is a measure of accuracy of the approxi - mation .
to assess the variational approximation according to this measure , we compared the lower bound to the true predictive likelihood that was evaluated numerically .
note that for a single observation , the evaluation of the predictive likelihood can be reduced to a one - dimensional integration problem :
g ( ( 123s 123 ) ( cid : 123 ) t x ) p ( ( cid : 123 ) ) d ( cid : 123 )
p ( s j x; ( cid : 123 ) ) p ( ( cid : 123 ) ) d ( cid : 123 ) d
) is a gaussian with mean where the effective prior p 123 d ( 123s 123 ) t x and variance ( cid : 123 ) 123 d x t 123 x where the actual prior distribution p ( ( cid : 123 ) ) has mean and covariance 123
this reduction has no effect on the accuracy of the varia - tional transformation and thus it can be used in evaluating the overall accuracy .
figure 123 ( b ) shows the difference between the true predictive probability and the variational lower bound for
jaakkola and jordan
various settings of the effective mean 123 and variance ( cid : 123 ) 123 , with optimized separately for each different values of 123 and ( cid : 123 ) 123
the fact that the variational approximation is a lower bound means that the difference in the predictive likelihood is always
we emphasize that the tightness of the lower bound is not the only relevant measure of accuracy .
indeed , while a tight lower bound on the predictive probability assures us that the associated posterior distribution is highly accurate , the converse is not true in general .
in other words , a poor lower bound does not neces - sarily imply a poor approximation to the posterior distribution at the point of interest , only that we no longer have any guaran - tees of good accuracy .
in practice , we expect the accuracy of the posterior to be more important than that of the predictive prob - ability since errors in the posterior run the risk of accumulating in the course of the sequential estimation procedure .
we defer the evaluation of the posterior accuracy to the following section where comparisons are made to related methods .
comparison to other methods
there are other sequential approximation methods that yield closed form posterior parameter distributions in logistic regres - sion models .
the method most closely related to ours is that of spiegelhalter and lauritzen ( 123 ) , which we refer to as the s - l approximation in this paper .
their method is based on the laplace approximation; that is , they utilize a local quadratic approximation to the complete log - likelihood centered at the prior mean .
the parameter updates that implement this approximation are similar in spirit to the variational updates of equations ( 123 ) and ( 123 ) :
d 123 c p ( 123 p ) x x t post d c ( s p ) 123post x
where p d g ( t x ) .
since there are no additional adjustable parameters in this approximation , it is simpler than the varia - tional method; however , we would expect this lack of exibility to translate into less accurate posterior estimates .
a ) the logistic function ( solid line ) and its variational form ( dashed line ) for d 123 : b ) the difference between the predictive likelihood and its variational approximation as a function of g ( 123
) , as described in the text
bayesian parameter estimation
a ) the errors in the posterior means as a function of g ( 123 the posterior standard deviations as a function of g ( 123
again ( cid : 123 ) d 123 for the prior distribution
) , where 123
is the prior mean .
here ( cid : 123 ) d 123 for the prior .
b ) the relative errors in
the plots are the same as in fig .
123 , but now ( cid : 123 ) d 123 for the prior distribution
we compared the accuracy of the posterior estimates for the two methods in the context of a single observation .
to sim - plify the comparison we utilized the reduction described in the previous section .
since the accuracy of neither method is affected by this reduction , it sufces for our purposes here to carry out the comparison in this simpler setting . 123 the poste - rior probability of interest was therefore p ( ( cid : 123 ) j d ) / g ( ( cid : 123 ) 123 computed for various choices of values for the prior mean 123 and the prior standard deviation ( cid : 123 ) .
the correct posterior mean and standard deviations were obtained numerically .
figures 123 and 123 present the results .
we plot signed differences in com - paring the obtained posterior means to the correct ones; relative errors were used for the posterior standard deviations .
the error measures were left signed to reveal any systematic biases .
note that the posterior mean from the variational method is not guar - anteed to be a lower bound on the true mean .
such guarantees can be given only for the predictive likelihood .
as can be seen in figs .
123 ( a ) and 123 ( a ) the variational method yields signicantly more accurate estimates of the posterior means , for both val - ues of the prior variance .
for the posterior variance , the s - l estimate and the variational estimate appear to yield roughly comparable accuracy for the small value of the prior variance ( fig .
123 ( b ) ) ; however , for the larger prior variance , the variational approximation is superior ( fig .
we note that the variational method consistently underestimates the true posterior variance;
a fact that could be used to rene the approximation .
finally , in terms of the kl - divergences between the approximate and true posteriors , the variational method and the s - l approximation are roughly equivalent for the small prior variance; and again the variational method is superior for the larger value of the prior variance .
this is shown in fig
extension to belief networks
a belief network is a probabilistic model over a set of variables fsig that are identied with the nodes in an acyclic directed graph .
letting ( i ) denote the set of parents of node si in the graph , we dene the joint distribution associated with the belief network as the following product : p ( s123; : : : ; sn ) d
we refer to the conditional probabilities p ( si j s ( i ) ) as the local probabilities associated with the belief network .
in this section we extend our earlier work in this paper and consider belief networks in which logistic regression is used to dene the local probabilities ( such models have been studied in a non - bayesian setting by neal 123 and by saul , jaakkola , and jordan 123 ) .
thus we introduce parameter vectors ( cid : 123 ) i , one for each binary variable si , and consider models in which each local
jaakkola and jordan
kl - divergences between the approximate and the true posterior distribution as a function of g ( 123 the two approximation methods have ( visually ) identical curves for ( cid : 123 ) d 123
) : a ) ( cid : 123 ) d 123 for the prior .
b ) ( cid : 123 ) d 123
a ) a complete observation ( shaded variables ) and the markov blanket ( dashed line ) associated with the parameters ( cid : 123 ) 123
b ) an observation where the value of s123 is missing ( unshaded in the gure ) probability p ( si j s ( i ) ; ( cid : 123 ) i ) is a logistic regression of node si on its parents s ( i ) .
problem .
we apply the methods developed in the previous sec -
to simplify the arguments in the following sections , we will consider augmented belief networks in which the parameters themselves are treated as nodes in the belief network ( see fig .
this is a standard device in the belief network literature and is of course natural within the bayesian formalism .
complete cases
a complete case refers to a data point in which all of the vari - ables fsig are observed .
if all of the data points are complete cases , then the methods that we developed in the previous sec - tion apply immediately to belief networks .
this can be seen as follows .
consider the markov blankets associated with each of the parameters ( fig .
for complete cases each of the nodes within the markov blanket for each of the parameters is observed ( shaded in the diagram ) .
by the independence semantics of be - lief networks , this implies that the posterior distributions for the parameters are independent of one another ( conditioned on the observed data ) .
thus the problem of estimating the posterior dis - tributions for the parameters reduces to a set of n independent subproblems , each of which is a bayesian logistic regression
incomplete cases
the situation is substantially more complex when there are in - complete cases in the data set .
incomplete cases imply that we no longer have all the markov blankets for the parameters in the network .
thus dependencies can arise between the parame - ter distributions in different conditional models .
let us consider this situation in some detail .
a missing value implies that the observations arise from a marginal distribution obtained by sum - ming over the missing values of the unobserved variables .
the marginal distribution is thus a mixture distribution , where each mixture component corresponds to a particular conguration of the missing variables .
the weight assigned to that compo - nent is essentially the posterior probability of the associated conguration ( spiegelhalter and lauritzen 123 ) .
note that the dependencies arising from the missing values in the observa - tions can make the network quite densely connected ( a miss - ing value for a node effectively connects all of the neighboring nodes in the graph ) .
the dense connectivity leaves little struc - ture to be exploited in the exact probabilistic computations in
bayesian parameter estimation
these networks and tends to make exact probabilistic calculations
our approach to developing bayesian methods for belief net - works with missing variables combines two variational tech - niques .
in particular , we augment the - transformation intro - duced earlier with a second variational transformation that we refer to as a q - transformation .
while the purpose of the - transformation is to convert a local conditional probability into a form that can be integrated analytically , the purpose of the q - transformation is to approximate the effect of marginaliz - ing across missing values associated with one or more parents .
intuitively , the q - transformation lls in the missing values , allowing the variational transformation for complete data to be invoked .
the overall result is a closed - form approximation to the marginal posterior .
the correct marginalization across missing variables is a global operation that affects all of the conditional models that depend on the variables being marginalized over .
under the vari - ational approximation that we describe below , marginalization is a local operation that acts individually on the relevant condi -
approximate marginalization consider the problem of marginalizing over a set of variables s under a joint distribution :
p ( s123; : : : ; sn
if we performed the marginalization exactly , then the result - ing distribution would not retain the same factorization as the is involved in more than one of the original joint ( assuming s conditionals ) ; this can be seen from :
( indexed by i
where we have partitioned the product into the set of factors ) and those that do not ( indexed that depend on s ) .
marginalization is not generally a local operation on the individual node probabilities p ( si j s ( si ) ; ( cid : 123 ) i ) .
maintaining such locality , a desirable goal for computational reasons , can be achieved if we forgo exact marginalization and instead consider approximations .
in particular , we describe a variational approxi - mation that preserves locality at the expense of providing a lower bound on the marginal probability instead of an exact result .
to obtain the desired variational transformation , we again ex - ploit a convexity property .
in particular , for a given sequence pi ; i 123f123; : : : ; ng , consider where qi is a probability distribution .
it is well known that the geometric average is less than or equal to the arithmetic aver - age 123i qi pi .
( this can be easily established via an invocation of jensens inequality ) .
we can exploit this fact as follows .
consider ) , and rewrite the marginalization an arbitrary distribution q ( s
the geometric average
operation in the following way :
p ( s123; : : : ; sn j ( cid : 123 ) ) d
p ( s123; : : : ; sn j ( cid : 123 ) )
p ( s123; : : : ; sn j ( cid : 123 ) )
where the inequality comes from transforming the average over the bracketed term ( with respect to the distribution q ) into a geometric average .
the third line follows from plugging in the form of the joint distribution and exchanging the order of the products .
the logarithm of the multiplicative constant c ( q ) is the entropy of the variational distribution q :
log c ( q ) d
) log q ( s
let us now make a few observations about the result in equation ( 123 ) .
first , note that the lower bound in this equation has the same factored form as the original joint probability .
in particu - lar , we dene the q - transformation of the ith local conditional probability as follows :
s ( i ) ; ( cid : 123 ) i ; q
the lower bound in equation ( 123 ) is then a product of these q - transformations .
second , note that all the conditionals are trans - formed by the same distribution q .
a change in q can thus affect all the transformed conditionals .
this means that the dependen - cies between variables s that would have resulted from exact have been replaced with effective de - marginalization over s pendencies through a shared variational distribution q .
while the bound in equation ( 123 ) holds for an arbitrary vari - ) , to obtain a tight bound we need to ational distribution q ( s ) .
in practice this involves choosing a con - optimize across q ( s strained class of distributions and optimizing across the class .
the simplest form of variational distribution is the completely
) d my
which yields a variational bound which is traditionally referred to as the mean eld approximation .
this simplied approxi - mation is appropriate in dense models with a relatively large number of missing values .
more generally , one can consider structured variational distributions involving partial factoriza - tions that correspond to tractable substructures in the graphical
model ( cf .
saul and jordan 123 ) .
we consider this topic further in the following two sections .
although the main constraint on the choice of q ( s
) is the computational one associated with evaluation and optimization , there is one additional constraint that must be borne in mind .
in particular , the q - transformed conditional probabilities must be in a form such that a subsequent - transformation can be invoked , yielding as a result a tractable bayesian integral .
a simple way to meet this constraint is to require that the variational distribution ) should not depend on the parameters ( cid : 123 ) .
as we discuss in the following section , in this case all of the q - transformations simply involve products of logistic functions , which behave well under the - transformation .
bayesian parameter updates the derivation presented in the previous section shows that ap - proximate variational marginalization across a set of variables can be viewed as a geometric average of the local conditional
p ( s j s ; ( cid : 123 ) ) !
p ( s j s ; ( cid : 123 ) ) q ( s
) is the variational distribution over the missing val - ues .
note that while the - transformations are carried out sepa - rately for each relevant conditional model , the variational distri - bution q associated with the missing values is the same across all the q - transformations .
given the transformation in equation ( 123 ) , the approximate bayesian updates are obtained readily .
in particular , when condi - tioning on a data point that has missing components we rst apply the q - transformation .
this effectively lls in the missing values , resulting in a transformed joint distribution that factorizes as in the case of complete observations .
the posterior parameter distributions therefore can be obtained independently for the parameters associated with the transformed local probabilities .
two issues need to be considered .
first , the transformed con - ditional probabilities ( cf .
equation ( 123 ) ) are products of logistic functions and therefore more complicated than before .
the - transformation method , however , transforms each logistic func - tion into an exponential with quadratic dependence on the para - meters .
products of such transforms are also exponential with quadratic dependence on the parameters .
thus the approximate likelihood will again be gaussian and if the prior is a multivariate gaussian the approximate posterior will also be gaussian .
the second issue is the dependence of the posterior parame - ter distributions on the variational distribution q .
once again we have to optimize the variational parameters ( a distribution in this case ) to make our bounds as tight as possible; in particular , we set q to the distribution that maximizes our lower bound .
this optimization is carried out in conjunction with the optimization of the parameters for the transformations of the logistic func - tions , which are also lower bounds .
as we show in appendix b . 123 , the fact that all of our approximations are lower bounds implies that we can again devise an em algorithm to perform the maximization .
the updates that are derived in the appendix
jaakkola and jordan
are as follows :
c 123 ( i ) e
where s ( i ) is the vector of parents of si , and the expectations are taken with respect to the variational distribution q .
numerical evaluation in this section , we provide a numerical evaluation of our pro - posed combination of q - transformation and - transformation .
we study a simple graph that consists of a single node s and its parents s .
in contrast to the simple logistic regression case analyzed earlier , the parents s are not observed but instead are distributed according to a distribution p ( s ) .
this distribution , which we manipulate directly in our experiments , essentially provides a surrogate for the effects of a pattern of evidence in the ancestral graph associated with node s ( cf .
spiegelhalter and ( cid : 123 ) associated with the conditional probability p ( s j s ; ( cid : 123 ) ) .
our interest is in the posterior probability over the parameters suppose now that we observe s d 123
the exact posterior
probability over the parameters ( cid : 123 ) in this case is given by
p ( ( cid : 123 ) j d ) /
p ( s d 123j s ; ( cid : 123 ) ) p ( s )
our variational method focuses on lower bounding the evidence term in brackets .
it is natural to evaluate the overall accuracy of the approximation by evaluating the accuracy of the marginal
p ( s d 123j s ; ( cid : 123 ) ) p ( s )
( cid : 123 ) ( ( cid : 123 ) t s ) p ( s )
we consider two different variational approximations .
in the rst approximation the variational distribution q is left uncon - strained; in the second we use an approximation that factorizes across the parents s ( the mean eld approximation ) .
we em - phasize that in both cases the variational posterior approximation over the parameters is a single gaussian .
the results of our experiment are shown in figs .
123 and 123
each gure displays three curves , corresponding to the exact evaluation of the data likelihood p ( d ) and the two variational lower bounds .
the number of parents in s was 123 and the prior distribution p ( ( cid : 123 ) ) was taken to be a zero mean gaussian with a variable covariance matrix .
by the symmetry of the gaussian distribution and the sigmoid function , the exact value of p ( d ) was 123 in all cases .
we considered several choices of p ( s ) and p ( ( cid : 123 ) ) .
in the rst case , the p ( s ) were assumed to factorize
bayesian parameter estimation
exact data likelihood ( solid line ) , variational lower bound 123 ( dashed line ) , and variational lower 123 ( dotted line ) as a function of the stochasticity parameter p of p ( s ) .
in ( a ) p ( ( cid : 123 ) ) d n ( 123; i =123 ) and in ( b ) p ( ( cid : 123 ) ) d n ( 123; 123 ) where 123 is a sample covariance of 123 random vectors distributed according to n ( 123; i =123 )
exact data likelihood ( solid line ) and the two variational lower bounds ( dashed and dotted lines respectively ) as a function of the mixture parameter pm .
in ( a ) p d 123 : 123 and in ( b ) p d 123 : 123
across the parents and for each s j 123 s ; p ( s j d 123 ) d p leaving a single parameter p that species the stochasticity of p ( s ) .
a similar setting would arise when applying the mean eld ap - proximation in the context of a more general graph .
figure 123 shows the accuracy of the variational lower bounds as a function of p where in fig .
123 ( a ) p ( ( cid : 123 ) ) d n ( 123; i =123 ) , i . e . , the covariance matrix is diagonal with diagonal components set to 123 / 123 , and in fig .
123 ( b ) p ( ( cid : 123 ) ) d n ( 123; 123 ) where 123 is a sample covariance matrix of 123 gaussian random vectors distributed according to n ( 123; i =123 ) .
the results of fig .
123 ( b ) are averaged over 123 inde - pendent runs .
the choice of scaling in n ( 123; i =123 ) is made to in - ( cid : 123 ) jj 123
both gures indicate that the variational approximations are reasonably accurate and that there is little difference between the two methods .
sure that jp
in fig .
123 we see how the mean eld approximation ( which is unimodal ) deteriorates as the distribution p ( s ) changes from a factorized distribution toward a mixture distribution .
more specically , let p f ( s j p ) be the ( uniform ) factorized distri - bution discussed above with parameter p and let pm ( s ) be a pure mixture distribution that assigns a probability mass 123 / 123 to three different ( randomly chosen ) congurations of the parents s .
we let p ( s ) d ( 123 pm ) p f ( s j p ) c pm pm ( s ) , where
the parameter pm controls the extent to which p ( s ) resembles a ( pure ) mixture distribution .
figure 123 illustrates the accuracy of the two variational methods as a function of pm where in fig .
123 ( a ) p d 123 : 123 and in 123 ( b ) p d 123 : 123
as expected , the mean eld approximation deteriorates with an increasing pm whereas our rst variational approximation remains accurate .
the dual problem
in the logistic regression formulation ( equation ( 123 ) ) , the para - meters ( cid : 123 ) and the explanatory variables x play a dual or sym - metric role ( cf .
nadal and parga 123 ) .
in the bayesian logis - tic regression setting , the symmetry is broken by associating the same parameter vector ( cid : 123 ) with multiple occurrences of the explanatory variables x as shown in fig .
alternatively , we may break the symmetry by associating a single instance of the explanatory variable x with multiple realizations of ( cid : 123 ) .
in this sense the explanatory variables x play the role of parameters while ( cid : 123 ) functions as a continuous latent variable .
the dual of the bayesian regression model is thus a latent variable density model over a binary response variable s .
graphically , in the
a ) bayesian regression problem .
b ) the dual problem .
dual interpretation we have a single parameter node for x whereas separate nodes are required for different realizations of ( cid : 123 ) ( illustrated as ( cid : 123 ) ( i ) in the gure ) to explain successive obser - vations s ( i ) .
while a latent variable density model over a sin - gle binary variable is not particularly interesting , we can gen - eralize the response variable s to a vector of binary variables s d ( s123; : : : ; sn ) t where each component si has a distinct set of parameters xi d ( xi123; : : : ; xim ) t associated with it .
the latent variables ( cid : 123 ) , however , remain in this dual interpretation the same for all si .
we note that strictly speaking the dual interpre - tation would require us to assign a prior distribution over the new parameters vectors xi .
for simplicity , however , we omit this consideration and treat xi simply as adjustable parameters .
the resulting latent variable density model over binary vectors is akin to the standard factor analysis model ( see e . g .
everitt 123 ) .
this model has already been used to facilitate visualization of high dimensional binary vectors ( tipping 123 ) .
we now turn to a more technical treatment of this latent vari -
able model .
the joint distribution is given by
p ( s123; : : : ; sn j x ) d
p ( si j xi ; ( cid : 123 ) )
where the conditional probabilities for the binary observables are logistic regression models
p ( si j xi ; ( cid : 123 ) ) d g ( ( 123si 123 ) 123 j xi j ( cid : 123 ) j )
; : : : ; st
we would like to use the em - algorithm for parameter estimation .
to achieve this we again exploit the variational transformations .
the transformations can be introduced for each of the condi - tional probability in the joint distribution and optimized sepa - rately for every observation dt dfst g in the database consisting only of the values of the binary output variables .
as in the logistic regression case , the transformations change the unwieldy conditional models into simpler ones that depend on the parameters only quadratically in the exponent .
the vari - ational evidence , which is a product of the transformed con - ditional probabilities , retains the same property .
consequently , under the variational approximation , we can compute the pos - terior distribution over the latent variables ( cid : 123 ) in closed form .
the mean and the covariance of this posterior can be obtained
analogously to the regression case giving
d 123 c
xi x t
jaakkola and jordan
t d 123t
the variational parameters t i associated with each observation and the conditional model can be updated using equation ( 123 ) where x is replaced with xi , now the vector of parameters associated with the ith conditional model .
we can solve the m - step of the em - algorithm by accumu - lating sufcient statistics for the parameters xi ; ; 123 based on the closed form posterior distributions corresponding to the ob - servations in the data set .
omitting the algebra , we obtain the following explicit updates for the parameters :
123t c t t
and the subscript t denotes the quantities pertaining to the obser - vation dt .
note that since the variational transformations that we expoited to arrive at these updates are all lower bounds , the m - step necessarily results in a monotonically increasing lower bound on the log - probability of the observations .
this desirable monotonicity property is unlikely to arise with other types of approximation methods , such as the laplace approximation .
we have exemplied the use of variational techniques in the set - ting of bayesian parameter estimation .
we found that variational methods can be exploited to yield closed form expressions that approximate the posterior distributions for the parameters in lo - gistic regression .
the methods apply immediately to a bayesian treatment of logistic belief networks with complete data .
we also showed how to combine mean eld theory with our variational transformation and thereby treat belief networks with missing data .
finally , our variational techniques lead to an exactly solv - able em algorithm for a latent variable density model the dual of the logistic regression problem .
bayesian parameter estimation
it is also of interest to note that our variational method pro - vides an alternative to the standard iterative newton - raphson method for maximum likelihood estimation in logistic regres - sion ( an algorithm known as iterative reweighted least squares or irls ) .
the advantage of the variational approach is that it guarantees monotone improvement in likelihood .
we present the derivation of this algorithm in appendix c .
finally , for an alternative perspective on the application of variational methods to bayesian inference , see hinton and van camp ( 123 ) and mackay ( 123 ) .
these authors have devel - oped a variational method known as ensemble learning , which can be viewed as a mean eld approximation to the marginal
appendix a : optimization of the variational
to optimize the variational approximation of equation ( 123 ) in the context of an observation d d fs; x123; : : : ; xrg we formulate an em algorithm to maximize the predictive likelihood of this observation with respect to .
in other words , we nd that maximizes the right hand side of
p ( s j x; ( cid : 123 ) ) p ( ( cid : 123 ) ) d ( cid : 123 )
p ( s j x; ( cid : 123 ) ; ) p ( ( cid : 123 ) ) d ( cid : 123 )
in the em formalism this is achieved by iteratively maximizing the expected complete log - likelihood given by q ( j old ) d eflog p ( s j x; ( cid : 123 ) ; ) p ( ( cid : 123 ) ) g
where the expectation is over p ( ( cid : 123 ) j d; old ) .
taking the deriva - tive of q with respect to and setting it to zero leads to
q ( j old ) d @ ( )
( e ( ( cid : 123 ) t x ) 123 123 ) d 123
as ( ) is a monotonically decreasing function123 the maximum is obtained at
123 d e ( ( cid : 123 ) t x ) 123
by substituting for old above , the procedure can be repeated .
each such iteration yields a better approximation in the sense of
p ( ( cid : 123 ) i j dt ; q ) /
the form of this posterior , however , remains at least as unwieldy as the bayesian logistic regression problem considered earlier in the paper .
proceeding analogously , we transform the logis - tic functions as in equation ( 123 ) corresponding to each of the conditional probabilities in the product and obtain
p ( ( cid : 123 ) i j dt ; q; i ) /
s ( i ) ; ( cid : 123 ) i ; i
g ( i ) e ( hsi
i ) =123 ( i ) ( h 123
g ( i ) e123s123 q ( s
i ) =123 ( i ) ( h 123
gi ) =123 ( i ) ( efh 123
g ( i ) e ( efhsi
s ( i ) ; ( cid : 123 ) i ; i ; q
d ( 123si 123 ) ( cid : 123 ) t
i s ( i ) ; s ( i ) is the vector of parents of si , and the expectations are with respect to the variational distri - bution q .
for simplicity , we have not let the variational parameter i vary independently with the congurations of the missing val - ues but assumed it to be the same for all such congurations .
this choice is naturally suboptimal but is made here primarily for notational simplicity ( the choice may also be necessary in cases where the number of missing values is large ) .
now , since hsi is linear in the parameters ( cid : 123 ) i , the exponent in equation ( 123 ) consisting of averages over hsi and its square with respect to the variational distribution q , stays at most quadratic in the parameters ( cid : 123 ) i .
a multivariate gaussian prior will be conjugate to this likelihood , and therefore the posterior will also be gaus - sian .
the mean posi and covariance 123posi of such posterior are given by ( we omit the algebra )
c 123 ( i ) e i c e
; : : : ; st
appendix b : parameter posteriors to ll - in the possible missing values in an observation dt d g we employ the q - transformations described in the text .
as a result , the joint distribution after the approximate marginalization factorizes as with complete observations .
thus the posterior distributions for the parameters remain independent across the different conditional models and can be computed
note that this posterior depends both on the distribution q and the parameters .
the optimization of these parameters is shown in appendix b . 123
optimization of the variational parameters
we have introduced two variational parameters : the distribu - tion q over the missing values , and the parameters correspond - ing to the logistic or - transformations .
the metric for optimiz - ing the parameters comes from the fact that the transformations
jaakkola and jordan p ( si j s ( i ) ; ( cid : 123 ) i ; i ; q ) , into the above denition of the q function .
for clarity we will omit all the terms with no dependence on the variational distribution q .
we obtain :
q ( q j qold ) d
c log c ( q ) c
where eq refers to the expectation with respect to the variational distribution q .
the second equation follows by exchanging the order of the ( mutually independent ) expectations e ( cid : 123 ) i and eq .
we have also used the fact that log c ( q ) is the entropy h ( q ) of q i s ( i ) , where ( see the text ) .
recall the notation hsi s ( i ) , is a binary vector of parents of si .
before proceeding to maximize the q function with respect to q , we explicate the averages e ( cid : 123 ) i in the above formula :
d ( 123si 123 ) ( cid : 123 ) t
d ( 123si 123 ) t 123 c st
123 pi s ( i )
here pi and 123 pi are the mean and the covariance , respectively , of the posterior p ( ( cid : 123 ) i j i ; qold ) associated with the ith conditional model .
simply inserting these back into the expression for the q function we get q ( q j qold ) d eq
pi s ( i ) ( i )
c h ( q ) c
123 pi s ( i )
now , some of the binary variables si have a value assignment based on the observation dt and the remaining variables will be averaged over the variational distribution q .
assuming no a priori constraints on the form of the q distribution , the maximizing q is the boltzmann distribution ( see e . g .
parisi 123 ) :
) / exp
pi s ( i ) ( i )
associated with these parameters introduce a lower bound on the probability of the observations .
thus by maximizing this lower bound we nd the parameter values that yield the most accurate approximations .
we therefore attempt to maximize the right hand side of
log p ( dt ) log p ( dt j ; q )
p ( dt j ( cid : 123 ) ; ; q ) p ( ( cid : 123 ) ) d ( cid : 123 )
s ( i ) ; ( cid : 123 ) i ; i ; q
c log c ( q )
p ( ( cid : 123 ) i ) d ( cid : 123 ) i
where dt contains the observed settings of the variables .
we have used the fact that the joint distribution under our approximations factorizes as with complete cases .
similarly to the case of the simple bayesian logistic regression considered previously ( see appendix a ) , we can devise an em - algorithm to maximize the variational lower bound with respect to the parameters q and ; the parameters ( cid : 123 ) can be considered as latent variables in this formulation .
the e - step of the em - algorithm , i . e . , nding the posterior distribution over the latent variables , has already been described in appendix b .
here we will consider in detail only the m - step .
for simplicity , we solve the m - step in two phases : the rst where the variational distribution is kept xed and the maximization is over , and the second where these roles are reversed .
we start with the rst phase .
as the variational joint distribution factorizes , the problem of nding the optimal parameters separates into independent problems concerning each of the transformed conditionals .
thus the optimization becomes analogous to the simple bayesian logistic regression considered earlier .
two differences exist : rst , the posterior over each ( cid : 123 ) i is now obtained from equation ( 123 ) ; second , we have an additional expectation with respect to the variational distribution q .
with these differences the optimiza - tion is analogous to the one presented in appendix a above and we wont repeat it here .
the latter part of our two - stage m - step is new , however , and we will consider it in detail .
the objective is to optimize q while keeping the parameters xed to their previously obtained val - ues .
similarly to the case we construct an em - algorithm to perform this inner loop optimization :
q ( q j qold ) d e ( cid : 123 ) flog p ( dt ; ( cid : 123 ) j ; q ) g
s ( i ) ; ( cid : 123 ) i ; i ; q
c log c ( q )
where the rst expectation is with respect to p ( ( cid : 123 ) j ; qold ) , which factorizes across the conditional probabilities as explained pre - viously; the expectations e ( cid : 123 ) i are over the component distribu - tions p ( ( cid : 123 ) i j i ; qold ) , obtained directly from equation ( 123 ) .
let us now insert the form of the transformed conditional probabilities ,
123 pi s ( i )
whenever the variational distribution is constrained , however , such as in the case of a completely factorized distribution , we may no longer expect to nd the q that maximizes equation ( 123 ) .
nevertheless , a locally optimal solution can be found by , for example , sequentially solving
q ( q j qold ) d 123
bayesian parameter estimation with respect to each of the components qk d qk ( sk d 123 ) in the
this holds for 123
however , since p ( s j x; ( cid : 123 ) ; ) is a symmetric function of , assuming 123 has no effect on the quality of the
appendix c : technical note : ml estimation
