abstract .
given a covariance matrix , we consider the problem of maximizing the variance explained by a particular linear combination of the input variables while constraining the number of nonzero coecients in this combination .
this problem arises in the decomposition of a covari - ance matrix into sparse factors or sparse pca , and has wide applications ranging from biology to nance .
we use a modication of the classical variational representation of the largest eigenvalue of a symmetric matrix , where cardinality is constrained , and derive a semidenite programming based relaxation for our problem .
we also discuss nesterovs smooth minimization technique applied to the semidenite program arising in the semidenite relaxation of the sparse pca problem .
the method
has complexity o ( n123plog ( n ) / ) , where n is the size of the underlying covariance matrix , and is
the desired absolute accuracy on the optimal value of the problem .
key words .
principal component analysis , karhunen - lo`eve transform , factor analysis , semidef -
inite relaxation , moreau - yosida regularization , semidenite programming .
ams subject classications .
123c123 , 123h123 , 123c123
introduction .
principal component analysis ( pca ) is a popular tool for data analysis , data compression and data visualization .
it has applications throughout science and engineering .
in essence , pca nds linear combinations of the variables ( the so - called principal components ) that correspond to directions of maximal variance in the data .
it can be performed via a singular value decomposition ( svd ) of the data matrix a , or via an eigenvalue decomposition if a is a covariance matrix .
the importance of pca is due to several factors .
first , by capturing directions of maximum variance in the data , the principal components oer a way to compress the data with minimum information loss .
second , the principal components are un - correlated , which can aid with interpretation or subsequent statistical analysis .
on the other hand , pca has a number of well - documented disadvantages as well .
a par - ticular disadvantage that is our focus here is the fact that the principal components are usually linear combinations of all variables .
that is , all weights in the linear com - bination ( known as loadings ) are typically non - zero .
in many applications , however , the coordinate axes have a physical interpretation; in biology for example , each axis might correspond to a specic gene .
in these cases , the interpretation of the princi - pal components would be facilitated if these components involved very few non - zero loadings ( coordinates ) .
moreover , in certain applications , e . g . , nancial asset trading strategies based on principal component techniques , the sparsity of the loadings has important consequences , since fewer non - zero loadings imply fewer transaction costs .
it would thus be of interest to discover sparse principal components , i . e . , sets of sparse vectors spanning a low - dimensional space that explain most of the variance present in the data .
to achieve this , it is necessary to sacrice some of the explained
a preliminary version of this paper appeared in the proceedings of the neural information processing systems ( nips ) 123 conference and the associated preprint is on arxiv as cs . ce / 123
orfe dept . , princeton university , princeton , nj 123
aspremon@princeton . edu eecs dept . , u . c .
berkeley , berkeley , ca 123
elghaoui@eecs . berkeley . edu eecs and statistics depts . , u . c .
berkeley , berkeley , ca 123
jordan@cs . berkeley . edu ece dept . , u . c .
san diego , la jolla , ca 123
gert@ece . ucsd . edu
daspremont , l .
el ghaoui , m .
jordan and g . r . g .
lanckriet
variance and the orthogonality of the principal components , albeit hopefully not too
rotation techniques are often used to improve interpretation of the standard principal components ( see ( 123 ) for example ) .
vines or kolda and oleary ( 123 , 123 ) considered simple principal components by restricting the loadings to take values from a small set of allowable integers , such as 123 , 123 , and 123
cadima and jollie ( 123 ) proposed an ad hoc way to deal with the problem , where the loadings with small absolute value are thresholded to zero .
we will call this approach simple thresholding .
later , algorithms such as scotlass ( 123 ) and slra ( 123 , 123 ) were introduced to nd modied principal components with possible zero loadings .
finally , zou , hastie and tibshirani ( 123 ) propose a new approach called sparse pca ( spca ) to nd modied components with zero loading in very large problems , by writing pca as a regression - type optimization problem .
this allows the application of lasso ( 123 ) , a penalization technique based on the l123 norm .
all these methods are either signicantly suboptimal ( thresholding ) or nonconvex ( scotlass , slra , spca ) .
in this paper , we propose a direct approach ( called dspca in what follows ) that improves the sparsity of the principal components by directly incorporating a sparsity criterion in the pca problem formulation , then forming a convex relaxation of the problem .
this convex relaxation turns out to be a semidenite program .
semidenite programs ( sdp ) can be solved in polynomial time via general - purpose interior - point methods ( 123 , 123 ) .
this suces for an initial empirical study of the prop - erties of dspca and for comparison to the algorithms discussed above on small prob - lems .
for high - dimensional problems , however , the general - purpose methods are not viable and it is necessary to exploit problem structure .
our particular problem can be expressed as a saddle - point problem which is well suited to recent algorithms based on a smoothing argument combined with an optimal rst - order smooth minimization algorithm ( 123 , 123 , 123 ) .
these algorithms oer a signicant reduction in computational time compared to generic interior - point sdp solvers .
this also represents a change in the granularity of the solver , requiring a larger number of signicantly cheaper iter - ations .
in many practical problems this is a desirable tradeo; interior - point solvers often run out of memory in the rst iteration due to the necessity of forming and solving large linear systems .
the lower per - iteration memory requirements of the rst - order algorithm described here means that considerably larger problems can be solved , albeit more slowly .
this paper is organized as follows .
in section 123 , we show how to eciently maxi - mize the variance of a projection while constraining the cardinality ( number of nonzero coecients ) of the vector dening the projection .
we achieve this via a semidenite relaxation .
we briey explain how to generalize this approach to non - square matrices and formulate a robustness interpretation of our technique .
we also show how this interpretation can be used in the decomposition of a matrix into sparse factors .
sec - tion 123 describes how nesterovs smoothing technique ( see ( 123 ) , ( 123 ) ) can be used to solve large problem instances eciently .
finally , section 123 presents applications and numerical experiments comparing our method with existing techniques .
notation .
in this paper , sn is the set of symmetric matrices of size n , and n the spectahedron ( set of positive semidenite matrices with unit trace ) .
we denote by 123 a vector of ones , while card ( x ) denotes the cardinality ( number of non - zero elements ) of a vector x and card ( x ) is the number of non - zero coecients in the matrix x .
for x sn , x ( cid : 123 ) 123 means that x is positive semidenite , kxkf is the frobenius norm of x , i . e . , kxkf = ptr ( x 123 ) , max ( x ) is the maximum eigenvalue
sparse pca using semidefinite programming
of x and kxk = max ( i , j=123 , . . . , n ) |xij| , while |x| is the matrix whose elements are the absolute values of the elements of x .
finally , for matrices x , y sn , x y is the hadamard ( componentwise or schur ) product of x and y .
semidenite relaxation .
in this section , we derive a semidenite program - ming ( sdp ) relaxation for the problem of maximizing the variance explained by a vector while constraining its cardinality .
we formulate this as a variational problem , then obtain a lower bound on its optimal value via an sdp relaxation ( we refer the reader to ( 123 ) or ( 123 ) for an overview of semidenite programming ) .
sparse variance maximization .
let a sn be a given symmetric ma - trix and k be an integer with 123 k n .
given the matrix a and assuming without loss of generality that a is a covariance matrix ( i . e .
a is positive semidenite ) , we consider the problem of maximizing the variance of a vector x rn while constraining
subject to kxk123 = 123
because of the cardinality constraint , this problem is hard ( in fact , np - hard ) and we look here for a convex , hence ecient , relaxation .
semidenite relaxation .
following the lifting procedure for semidenite
relaxation described in ( 123 ) , ( 123 ) , ( 123 ) for example , we rewrite ( 123 ) as :
subject to tr ( x ) = 123
x ( cid : 123 ) 123 , rank ( x ) = 123 ,
in the ( matrix ) variable x sn .
programs ( 123 ) and ( 123 ) are equivalent , indeed if x is a solution to the above problem , then x ( cid : 123 ) 123 and rank ( x ) = 123 mean that we have x = xxt , while tr ( x ) = 123 implies that kxk123 = 123
finally , if x = xxt then card ( x ) k123 is equivalent to card ( x ) k .
we have made some progress by turning the convex maximization objective xt ax and the nonconvex constraint kxk123 = 123 into a linear constraint and linear objective .
problem ( 123 ) is , however , still nonconvex and we need to relax both the rank and cardinality constraints .
since for every u rn , card ( u ) = q implies kuk123 qkuk123 , we can replace the nonconvex constraint card ( x ) k123 , by a weaker but convex constraint : 123t|x|123 k , where we exploit the property that kxkf = xt x = 123 when x = xxt and tr ( x ) = 123
if we drop the rank constraint , we can form a relaxation of ( 123 ) and ( 123 ) as :
subject to tr ( x ) = 123 x ( cid : 123 ) 123 ,
which is a semidenite program in the variable x sn , where k is an integer param - eter controlling the sparsity of the solution .
the optimal value of this program will be an upper bound on the optimal value of the variational problem in ( 123 ) .
here , the relaxation of card ( x ) in 123t|x|123 corresponds to a classic technique which replaces the ( non - convex ) cardinality or l123 norm of a vector x with its largest convex lower bound on the unit box : |x| , the l123 norm of x ( see ( 123 ) or ( 123 ) for other applications ) .
daspremont , l .
el ghaoui , m .
jordan and g . r . g .
lanckriet
extension to the non - square case .
similar reasoning applies to the
case of a non - square matrix a rmn , and the problem :
kuk123 = kvk123 = 123 card ( u ) k123 , card ( v ) k123 ,
in the variables ( u , v ) rm rn where k123 m , k123 n are xed integers controlling the sparsity .
this can be relaxed to :
maximize tr ( at x 123 ) subject to x ( cid : 123 ) 123 , tr ( x ii ) = 123 123t|x ii|123 ki , 123t|x 123|123 k123k123 ,
i = 123 , 123
in the variable x sm+n with blocks x ij for i , j = 123 , 123
of course , we can consider several variations on this , such as constraining card ( u , v ) = card ( u ) + card ( v ) .
a robustness interpretation .
in this section , we show that problem ( 123 ) can be interpreted as a robust formulation of the maximum eigenvalue problem , with additive , componentwise uncertainty in the input matrix a .
we again assume a to be symmetric and positive semidenite .
in the previous section , we considered a cardinality - constrained variational for -
mulation of the maximum eigenvalue problem :
kxk123 = 123
here we look instead at a variation in which we penalize the cardinality and solve :
xt ax card123 ( x ) kxk123 = 123 ,
in the variable x rn , where the parameter > 123 controls the magnitude of the penalty .
this problem is again nonconvex and very dicult to solve .
as in the last section , we can form the equivalent program :
maximize tr ( ax ) card ( x ) subject to tr ( x ) = 123 x ( cid : 123 ) 123 , rank ( x ) = 123 ,
in the variable x sn .
again , we get a relaxation of this program by forming :
maximize tr ( ax ) 123t|x|123 subject to tr ( x ) = 123
x ( cid : 123 ) 123 ,
which is a semidenite program in the variable x sn , where > 123 controls the magnitude of the penalty .
we can rewrite this problem as :
tr ( x ( a + u ) )
sparse pca using semidefinite programming
in the variables x sn and u sn .
this yields the following dual to ( 123 ) :
max ( a + u )
i , j = 123 ,
which is a maximum eigenvalue problem with variable u sn .
this gives a natural robustness interpretation to the relaxation in ( 123 ) : it corresponds to a worst - case maximum eigenvalue computation , with componentwise bounded noise of intensity imposed on the matrix coecients .
let us rst remark that in ( 123 ) corresponds to the optimal lagrange multiplier in ( 123 ) .
also , the kkt conditions ( see ( 123 , 123 . 123 ) ) for problem ( 123 ) and ( 123 ) are
( a + u ) x = max ( a + u ) x u x = |x| tr ( x ) = 123 , x ( cid : 123 ) 123
i , j = 123 ,
if the eigenvalue max ( a+ u ) is simple ( when , for example , max ( a ) is simple and is suciently small ) , the rst condition means that rank ( x ) = 123 and the semidenite relaxation is tight , with in particular card ( x ) = card ( x ) 123 if x is the dominant eigen - vector of x .
when the optimal solution x is not of rank one because of degeneracy ( i . e .
when max ( a + u ) has multiplicity strictly larger than one ) , we can truncate x as in ( 123 , 123 ) , retaining only the dominant eigenvector x as an approximate solution to the original problem .
in that degenerate scenario however , the dominant eigenvector of x is not guaranteed to be as sparse as the matrix itself .
sparse decomposition .
using the results obtained in the previous two sec - tions we obtain a sparse equivalent to the pca decomposition .
given a matrix a123 sn , our objective is to decompose it in factors with target sparsity k .
we solve the relaxed problem in ( 123 ) :
subject to tr ( x ) = 123 x ( cid : 123 ) 123
letting x123 denote the solution , we truncate x123 , retaining only the dominant ( sparse ) eigenvector x123
finally , we deate a123 to obtain
a123 = a123 ( xt
and iterate to obtain further components .
the question is now : when do we stop the
in the pca case , the decomposition stops naturally after rank ( a ) factors have in the case of the sparse de - been found , since arank ( a ) +123 is then equal to zero .
composition , we have no guarantee that this will happen .
of course , we can add an additional set of linear constraints xt i xxi = 123 to problem ( 123 ) to explicitly enforce the orthogonality of x123 , .
, xn and the decomposition will then stop after a maxi - mum of n iterations .
alternatively , the robustness interpretation gives us a natural if all the coecients in |ai| are smaller than the noise level ( computed in the last section ) then we must stop since the matrix is essentially indis - tinguishable from zero .
thus , even though we have no guarantee that the algorithm will terminate with a zero matrix , in practice the decomposition will terminate as soon as the coecients in a become indistinguishable from the noise .
daspremont , l .
el ghaoui , m .
jordan and g . r . g .
lanckriet
algorithm .
for small problems , the semidenite program ( 123 ) can be solved eciently using interior - point solvers such as sedumi ( 123 ) or sdpt123 ( 123 ) .
for larger - scale problems , we need to resort to other types of convex optimization algorithms because the o ( n123 ) constraints implicitly contained in 123t|x|123 k make the memory requirements of newtons method prohibitive .
of special interest are the algorithms recently presented in ( 123 , 123 , 123 ) .
these are rst - order methods specialized to prob - lems such as ( 123 ) having a specic saddle - point structure .
these methods have a signicantly smaller memory cost per iteration than interior - point methods and en - able us to solve much larger problems .
of course , there is a price : for xed problem size , the rst - order methods mentioned above converge in o ( 123 / ) iterations , where is the required accuracy on the optimal value , while interior - point methods converge as o ( log ( 123 / ) ) .
since the problem under consideration here does not require a high degree of precision , this slow convergence is not a major concern .
in what follows , we adapt the algorithm in ( 123 ) to our particular constrained eigenvalue problem .
a smoothing technique .
the numerical diculties arising in large scale semidenite programs stem from two distinct origins .
first , there is an issue of memory : beyond a certain problem size n , it becomes essentially impossible to form and store any second order information ( hessian ) on the problem , which is the key to the numerical eciency of interior - point sdp solvers .
second , smoothness is an issue : the constraint x ( cid : 123 ) 123 is not smooth , hence the number of iterations required to solve problem ( 123 ) using rst - order methods such as the bundle code of ( 123 ) ( which do not form the hessian ) to an accuracy is given by o ( 123 / 123 ) .
in general , this complexity bound is tight and cannot be improved without additional structural information on the problem .
fortunately , in our case we do have structural information available that can be used to bring the complexity down from o ( 123 / 123 ) to o ( 123 / ) .
furthermore , the cost of each iteration is equivalent to that of computing a matrix exponential ( roughly
recently , ( 123 ) and ( 123 ) ( see also ( 123 ) ) proposed an ecient rst - order scheme for convex minimization based on a smoothing argument .
the main structural assump - tion on the function to minimize is that it has a saddle - function format :
f ( x ) = f ( x ) + max
u ( ht x , ui ( u ) : u q123 )
where f is dened over a compact convex set q123 rn , f ( x ) is convex and dieren - tiable and has a lipschitz continuous gradient with constant m 123 , t is an element of rnn and ( u ) is a continuous convex function over some closed compact set q123 rn .
this assumes that the function ( u ) and the set q123 are simple enough so that the optimization subproblem in u can be solved very eciently .
when a function f can be written in this particular format , ( 123 ) uses a smoothing technique to show that the complexity ( number of iterations required to obtain a solution with absolute precision ) of solving :
falls from o ( 123 / 123 ) to o ( 123 / ) .
this is done in two steps .
regularization .
by adding a strongly convex penalty to the saddle function representation of f in ( 123 ) , the algorithm rst computes a smooth - approximation of f with lipschitz continuous gradient .
this can be seen as a generalized moreau - yosida regularization step ( see ( 123 ) for example ) .
sparse pca using semidefinite programming
optimal rst - order minimization .
the algorithm then applies the optimal rst - order scheme for functions with lipschitz continuous gradient detailed in ( 123 ) to the regularized function .
each iteration requires an ecient computation of the regularized function value and its gradient .
as we will see , this can be done explicitly in our case , with a complexity of o ( n123 ) and memory requirements of o ( n123 ) .
application to sparse pca .
given a symmetric matrix a sn , we consider the problem given in ( 123 ) ( where we can assume without loss of generality that = 123 ) :
maximize tr ( ax ) 123t|x|123 subject to tr ( x ) = 123
x ( cid : 123 ) 123 ,
in the variable x sn .
duality allows us to rewrite this in the saddle - function format :
f ( u ) ,
q123 = ( u sn : |uij| 123 , i , j = 123 , .
, n ) , q123 = ( x sn : tr x = 123 , x ( cid : 123 ) 123 )
f ( u ) : = maxxq123ht u , xi ( x ) , with t = in123 , ( x ) = tr ( ax ) .
as in ( 123 ) , to q123 and q123 we associate norms and so - called prox - functions .
to q123 , we associate the frobenius norm in sn , and a prox - function dened for u q123 by :
d123 ( u ) =
u t u .
with this choice , the center u123 of q123 , dened as :
u123 : = arg min
is u123 = 123 , and satises d123 ( u123 ) = 123
moreover , we have :
d123 : = max
d123 ( u ) = n123 / 123
furthermore , the function d123 is strongly convex on its domain , with convexity param - eter of 123 = 123 with respect to the frobenius norm .
next , for q123 we use the dual of the standard matrix norm ( denoted k k
123 ) , and a prox - function
d123 ( x ) = tr ( x log x ) + log ( n ) ,
where log x refers to the matrix ( and not componentwise ) logarithm , obtained by replacing the eigenvalues of x by their logarithm .
the center of the set q123 is x123 = n123in , where d123 ( x123 ) = 123
we have
d123 ( x ) log n : = d123
the convexity parameter of d123 with respect to k k ( this non - trivial result is proved in ( 123 ) . )
123 , is bounded below by 123 = 123
daspremont , l .
el ghaoui , m .
jordan and g . r . g .
lanckriet
next we compute the ( 123 , 123 ) norm of the operator t introduced above , which is
ktk123 , 123 : = max
x , u ht x , ui : kukf = 123 , kxk x kxk123 : kxkf 123
to summarize , the parameters dened above are set as follows :
d123 = n123 / 123 , 123 = 123 , d123 = log ( n ) , 123 = 123 , ktk123 , 123 = 123
let us now explicitly formulate how the regularization and smooth minimization tech - niques can be applied to the variance maximization problem in ( 123 ) .
regularization .
the method in ( 123 ) rst sets a regularization parameter
the method then produces an - suboptimal optimal value and corresponding subop - timal solution in a number of steps not exceeding
the non - smooth objective f ( x ) of the original problem is replaced with
where f is the penalized function involving the prox - function d123 :
f ( u ) : = max
xq123ht u , xi ( x ) d123 ( x ) .
note that in our case , the function f and its gradient are readily computed; see below .
the function f is a smooth uniform approximation to f everywhere on q123 , with maximal error d123 = / 123
furthermore , f has a lipschitz continuous gradient , with lipschitz constant given by :
in our specic case , the function f can be computed explicitly as :
f ( u ) = log ( tr exp ( ( a + u ) / ) ) log n ,
which can be seen as a smooth approximation to the function f ( u ) = max ( a + u ) .
this function f has a lipshitz - continuous gradient and is a uniform approximation of the function f .
sparse pca using semidefinite programming
first - order minimization .
an optimal gradient algorithm for mini - mizing convex functions with lipschitz continuous gradients is then applied to the smooth convex function f dened above .
the key dierence between the minimiza - tion scheme developed in ( 123 ) and classical gradient minimization methods is that it is not a descent method but achieves a complexity of o ( l / k123 ) instead of o ( 123 / k ) for gra - dient descent , where k is the number of iterations and l the lipschitz constant of the gradient .
furthermore , this convergence rate is provably optimal for this particular class of convex minimization problems ( see ( 123 , th .
123 . 123 ) ) .
thus , by sacricing the ( local ) properties of descent directions , we improve the ( global ) complexity estimate by an order of magnitude .
for our problem here , once the regularization parameter is set , the algorithm
proceeds as follows .
compute f ( uk ) and f ( uk ) 123
find yk = arg miny q123 hf ( uk ) , y i + 123 123
find wk = arg minw q123n ld123 ( w )
set uk+123 = 123
k+123 wk + k+123
123 lkuk y k123 123 ( f ( ui ) + hf ( ui ) , w uii ) o
until gap .
step one above computes the ( smooth ) function value and gradient .
the second step computes the gradient mapping , which matches the gradient step for uncon - strained problems ( see ( 123 , p . 123 ) ) .
step three and four update an estimate sequence see ( ( 123 , p . 123 ) ) of f whose minimum can be computed explicitly and gives an in - creasingly tight upper bound on the minimum of f .
we now present these steps in detail for our problem ( we write u for uk and x for xk ) .
step 123
the most expensive step in the algorithm is the rst , the computation of
f and its gradient .
setting z = a + u , the problem boils down to computing
u ( z ) : = arg max
xq123hz , xi d123 ( x )
and the associated optimal value f ( u ) .
it turns out that this problem has a very simple solution , requiring only an eigenvalue decomposition for z = a + u .
the gradient of the objective function with respect to z is set to the maximizer u ( z ) itself , so the gradient with respect to u is f ( u ) = u ( a + u ) .
diag ( d ) the matrix with diagonal d , then set
to compute u ( z ) , we form an eigenvalue decomposition z = v dv t , with d =
j=123 exp ( djdmax
i = 123 ,
where dmax : = max ( j=123 , . . . , n ) dj is used to mitigate problems with large numbers .
we then let u ( z ) = v hv t , with h = diag ( h ) .
the corresponding function value is
f ( u ) = log ( cid : 123 ) tr exp ( cid : 123 ) ( a + u )
which can be reliably computed as :
( cid : 123 ) ( cid : 123 ) = log n
( cid : 123 ) ! log n ,
f ( u ) = dmax + log n
) ! log n .
daspremont , l .
el ghaoui , m .
jordan and g . r . g .
lanckriet
step 123
this step involves a problem of the form :
y q123 hf ( u ) , y i +
lku y k123
where u is given .
the above problem can be reduced to a euclidean projection :
ky k123 ky v kf ,
where v = u l123f ( u ) is given .
the solution is given by : i , j = 123 ,
yij = sgn ( vij ) min ( |vij| , 123 ) ,
step 123
the third step involves solving a euclidean projection problem similar to
( 123 ) , with v dened by :
stopping criterion .
we can stop the algorithm when the duality gap is smaller
gapk = max ( a + uk ) tr axk + 123t|xk|123 ,
where xk = u ( ( a + uk ) / ) is our current estimate of the dual variable ( the function u is dened by ( 123 ) ) .
the above gap is necessarily non - negative , since both xk and uk are feasible for the primal and dual problem , respectively .
this is checked periodically , for example every 123 iterations .
complexity .
since each iteration of the algorithm requires computing a matrix exponential ( which requires an eigenvalue decomposition and o ( n123 ) ops in our code ) , the predicted worst - case complexity to achieve an objective with absolute accuracy less than is ( 123 ) :
= o ( n123plog n / ) .
in some cases , this complexity estimate can be improved by using specialized algo - rithms for computing the matrix exponential ( see ( 123 ) for a discussion ) .
for example , computing only a few eigenvalues might be sucient to obtain this exponential with the required precision ( see ( 123 ) ) .
in our preliminary experiments , the standard tech - nique using pade approximations , implemented in packages such as expokit ( see ( 123 ) ) , required too much scaling to be competitive with a full eigenvalue decomposition .
numerical results & applications .
in this section , we illustrate the ef - fectiveness of the proposed approach ( called dspca in what follows ) both on an articial data set and a real - life data set .
we compare with the other approaches mentioned in the introduction : pca , pca with simple thresholding , scotlass and spca .
the results show that our approach can achieve more sparsity in the principal components than spca ( the current state - of - the - art method ) does , while explaining as much variance .
the other approaches can explain more variance , but result in prin - cipal components that are far from sparse .
we begin by a simple example illustrating the link between k and the cardinality of the solution .
sparse pca using semidefinite programming
articial data .
to compare the numerical performance with that of exist - ing algorithms , we consider the simulation example proposed by ( 123 ) .
in this example , three hidden factors are created :
v123 n ( 123 , 123 ) , v123 n ( 123 , 123 ) , v123 = 123v123 + 123v123 + ,
n ( 123 , 123 )
with v123 , v123 and independent .
afterward , 123 observed variables are generated as
xi = vj + j
i n ( 123 , 123 ) ,
with j = 123 for i = 123 , .
, 123 , j = 123 for i = 123 , .
, 123 and j = 123 for i = 123 , 123 and j independent for j = 123 , 123 , 123 , i = 123 , .
we use the exact covariance matrix to compute principal components using the dierent approaches .
since the three underlying factors have roughly the same variance , and the rst two are associated with four variables while the last one is associated with only two variables , v123 and v123 are almost equally important , and they are both signicantly more important than v123
this , together with the fact that the rst two principal components explain more than 123% of the total variance , suggests that considering two sparse linear combinations of the original variables should be sucient to explain most of the variance in data sampled from this model ( 123 ) .
the ideal solution would thus be to use only the variables ( x123 , x123 , x123 , x123 ) for the rst sparse principal component , to recover the factor v123 , and only ( x123 , x123 , x123 , x123 ) for the second sparse principal component to recover v123
using the true covariance matrix and the oracle knowledge that the ideal spar - sity is four , ( 123 ) performed spca ( with = 123 ) .
we carry out our algorithm with k = 123
the results are reported in table 123 , together with results for pca , simple thresholding and scotlass ( t = 123 ) .
notice that dspca , spca and scotlass all nd the correct sparse principal components , while simple thresholding yields inferior performance .
the latter wrongly includes the variables x123 and x123 ( likely being mis - led by the high correlation between v123 and v123 ) , moreover , it assigns higher loadings to x123 and x123 than to each of the variables ( x123 , x123 , x123 , x123 ) that are clearly more important .
simple thresholding correctly identies the second sparse principal com - ponent , probably because v123 has a lower correlation with v123
simple thresholding also explains a bit less variance than the other methods .
loadings and explained variance for the rst two principal components of the articial exam - ple .
here , st denotes the simple thresholding method , other is all the other methods : spca , dspca and scotlass .
pc123 and pc123 denote the rst and second principal components .
x123 x123 x123 x123 x123 x123 x123 x123 x123 x123 explained variance
pca , pc123 . 123 . 123 . 123 . 123 - . 123 - . 123 - . 123 - . 123 - . 123 - . 123 pca , pc123 - . 123 - . 123 - . 123 - . 123 - . 123 - . 123 - . 123 - . 123 . 123 . 123 123 - . 123 - . 123 - . 123 - . 123
pit props data .
the pit props data ( consisting of 123 observations and 123 measured variables ) was introduced by ( 123 ) and is another benchmark example used to test sparse pca codes .
both scotlass ( 123 ) and spca ( 123 ) have been tested on this data set .
as reported in ( 123 ) , spca performs better than scotlass in the sense
daspremont , l .
el ghaoui , m .
jordan and g . r . g .
lanckriet
that it identies principal components with 123 , 123 , 123 , 123 , 123 , and 123 non - zero loadings , respectively , as shown in table 123 .
this is much sparser than the modied principal components by scotlass , while explaining nearly the same variance ( 123% versus 123% for the 123 rst principal components ) ( 123 ) .
also , simple thresholding of pca , with a number of non - zero loadings that matches the result of spca , does worse than spca in terms of explained variance .
following this previous work , we also consider the rst 123 principal components .
we try to identify principal components that are sparser than those of spca , but explain the same variance .
therefore , we choose values for k of 123 , 123 , 123 , 123 , 123 , 123 ( two less than the values of spca reported above , but no less than 123 ) .
figure 123 shows the cumulative number of non - zero loadings and the cumulative explained variance ( measuring the variance in the subspace spanned by the rst i eigenvectors ) .
can be seen that our approach is able to explain nearly the same variance as the spca method , while clearly reducing the number of non - zero loadings for the rst six principal components .
adjusting the rst value of k from 123 to 123 ( relaxing the sparsity ) , we obtain results that are still better in terms of sparsity , but with a cumulative explained variance that is uniformly larger than spca .
moreover , as in the spca approach , the important variables associated with the six principal components do not overlap , which leads to a clearer interpretation .
table 123 shows the rst three corresponding principal components for the dierent approaches ( dspcaw123 denotes runs with k123 = 123 and dspcaw123 uses k123 = 123 ) .
loadings for rst three principal components , for the pit props data .
dspcaw123 ( resp
caw123 ) shows the results for our technique with k123 equal to 123 ( resp
topd length moist testsg ovensg ringt ringb bowm bowd whorls clear knots diaknot dspcaw123 123 - . 123 dspcaw123 123 - . 123
123 - . 123 - . 123 123 - . 123 - . 123 123 - . 123 - . 123
controlling sparsity with k .
we present a simple example to illustrate how the sparsity of the solution to our relaxation evolves as k varies from 123 to n .
we generate a 123 123 matrix u with uniformly distributed coecients in ( 123 , 123 ) .
we let v be a sparse vector with :
v = ( 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 ) .
we then form a test matrix a = u t u + vvt , where is a signal - to - noise ratio that we set equal to 123
we sample 123 dierent matrices a using this technique .
for each value of k between 123 and 123 and each a , we solve the following sdp :
subject to tr ( x ) = 123 x ( cid : 123 ) 123
we then extract the rst eigenvector of the solution x and record its cardinality .
in figure 123 , we show the mean cardinality ( and standard deviation ) as a function of
sparse pca using semidefinite programming
number of principal components
number of principal components
cumulative cardinality and percentage of total variance explained versus number of principal components , for spca and dspca on the pit props data .
the dashed lines are spca and the solid ones are dspca with k123 = 123 and k123 = 123
on the right , the dotted line also shows the percentage of variance explained by standard ( non sparse ) pca .
while explaining the same cumulative variance , our method ( dspca ) produces sparser factors .
we observe that k + 123 is actually a good predictor of the cardinality , especially when k + 123 is close to the actual cardinality ( 123 in this case ) .
in fact , in the random examples tested here , we always recover the original cardinality of 123 when k + 123 is set
problem size n
left : plot of average cardinality ( and its standard deviation ) versus k for 123 random examples with original cardinality 123
right : plot of cpu time ( in seconds ) versus problem size for randomly chosen problems .
computing time versus problem size .
in figure 123 we plot the total cpu time used for randomly chosen problems of size n ranging from 123 to 123
the required precision was set to = 123 , which was always reached in fewer than 123 iterations .
in these examples , the empirical complexity appears to grow as o ( n123 ) .
sparse pca for gene expression data analysis .
we are given m data vectors xj rn , with n = 123
each coecient xij corresponds to the ex - pression of gene i in experiment j .
for each vector xj we are also given a class cj ( 123 , 123 , 123 , 123 ) .
we form a = xxt , the covariance matrix of the experiment .
our objective is to use pca to rst reduce the dimensionality of the problem and then look for clustering when the data are represented in the basis formed by the rst three
daspremont , l .
el ghaoui , m .
jordan and g . r . g .
lanckriet
principal components .
here , we do not apply any clustering algorithm to the data points , we just assign a color to each sample point in the three dimensional scatter plot , based on known experimental data .
the sparsity of the factors in sparse pca implies that the clustering can be attributed to fewer genes , making interpretation easier .
in figure 123 , we see clustering in the pca representation of the data and in the dspca representation .
although there is a slight drop in the resolution of the clusters for dspca , the key feature here is that the total number of nonzero gene coecients in the dspca factors is equal to 123 while standard pca produces three dense factors , each with 123 nonzero
clustering of the gene expression data in the pca versus sparse pca basis with 123 genes .
the factors f on the left are dense and each use all 123 genes while the sparse factors g123 , g123 and g123 on the right involve 123 , 123 and 123 genes respectively .
( data : iconix pharmaceuticals )
acknowledgments .
thanks to andrew mullhaupt and francis bach for use - ful suggestions .
we would like to acknowledge support from nsf grant 123 , onr muri n123 - 123 - 123 - 123 , eurocontrol - c123e / bm / 123 and c123e / bm / 123 , nasa - ncc123 - 123 and a gift from google , inc .
