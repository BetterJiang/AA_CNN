while classical kernel - based classiers are based on a single kernel , in practice it is often desirable to base classiers on combinations of multiple kernels .
lanckriet et al .
( 123 ) considered conic combinations of kernel ma - trices for the support vector machine ( svm ) , and showed that the optimization of the co - ecients of such a combination reduces to a convex optimization problem known as a quadratically - constrained quadratic program ( qcqp ) .
unfortunately , current convex op - timization toolboxes can solve this problem only for a small number of kernels and a small number of data points; moreover , the sequential minimal optimization ( smo ) tech - niques that are essential in large - scale imple - mentations of the svm cannot be applied be - cause the cost function is non - dierentiable .
we propose a novel dual the qcqp as a second - order cone program - ming problem , and show how to exploit the technique of moreau - yosida regularization to yield a formulation to which smo techniques can be applied .
we present experimental re - sults that show that our smo - based algo - rithm is signicantly more ecient than the general - purpose interior point methods avail - able in current optimization toolboxes .
one of the major reasons for the rise to prominence of the support vector machine ( svm ) is its ability to cast nonlinear classication as a convex optimization problem , in particular a quadratic program ( qp )
appearing in proceedings of the 123 st international confer - ence on machine learning , ban , canada , 123
copyright 123 by the authors .
vexity implies that the solution is unique and brings a suite of standard numerical software to bear in nding the solution .
convexity alone , however , does not imply that the available algorithms scale well to problems of interest .
indeed , o - the - shelf algorithms do not suce in large - scale applications of the svm , and a second major reason for the rise to prominence of the svm is the development of special - purpose algorithms for solving the qp ( platt , 123; joachims , 123; keerthi et al . , 123 ) .
recent developments in the literature on the svm and other kernel methods have emphasized the need to consider multiple kernels , or parameterizations of kernels , and not a single xed kernel .
this provides needed exibility and also reects the fact that prac - tical learning problems often involve multiple , hetero - geneous data sources .
while this so - called multiple kernel learning problem can in principle be solved via cross - validation , several recent papers have focused on more ecient methods for kernel learning ( chapelle et al . , 123; grandvalet & canu , 123; lanckriet et al . , 123; ong et al . , 123 ) .
in this paper we focus on the framework proposed by lanckriet et al .
( 123 ) , which involves joint optimization of the coecients in a conic combination of kernel matrices and the coecients of a discriminative classier .
in the svm setting , this problem turns out to again be a convex optimization problema quadratically - constrained quadratic pro - gram ( qcqp ) .
this problem is more challenging than a qp , but it can also be solved in principle by general - purpose optimization toolboxes such as mosek ( ander - sen & andersen , 123 ) .
again , however , this existing algorithmic solution suces only for small problems ( small numbers of kernels and data points ) , and im - proved algorithmic solutions akin to sequential mini - mization optimization ( smo ) are needed .
while the multiple kernel learning problem is convex , it is also non - smoothit can be cast as the minimiza - tion of a non - dierentiable function subject to linear
constraints ( see section 123 ) .
unfortunately , as is well known in the non - smooth optimization literature , this means that simple local descent algorithms such as smo may fail to converge or may converge to incor - rect values ( bertsekas , 123 ) .
indeed , in preliminary attempts to solve the qcqp using smo we ran into exactly these convergence problems .
one class of solutions to non - smooth optimization problems involves constructing a smooth approximate problem out of a non - smooth problem .
lar , moreau - yosida ( my ) regularization is an eec - tive general solution methodology that is based on inf - convolution ( lemarechal & sagastizabal , 123 ) .
it can be viewed in terms of the dual problem as simply adding a quadratic regularization term to the dual ob - jective function .
unfortunately , in our setting , this creates a new dicultywe lose the sparsity that makes the svm amenable to smo optimization .
in particular , the qcqp formulation of lanckriet et al .
( 123 ) does not lead to an my - regularized problem that can be solved eciently by smo techniques .
in this paper we show how these problems can be re - solved by considering a novel dual formulation of the qcqp as a second - order cone programming ( socp ) problem .
this new formulation is of interest on its own merit , because of various connections to existing algorithms .
in particular , it is closely related to the classical maximum margin formulation of the svm , diering only by the choice of the norm of the in - verse margin .
moreover , the kkt conditions arising in the new formulation not only lead to support vec - tors as in the classical svm , but also to a dual notion of support kernelsthose kernels that are active in the conic combination .
we thus refer to the new for - mulation as the support kernel machine ( skm ) .
as we will show , the conic dual problem dening the skm is exactly the multiple kernel learning problem of lanckriet et al .
( 123 ) . 123 moreover , given this new formulation , we can design a moreau - yosida regular - ization which preserves the sparse svm structure , and therefore we can apply smo techniques .
making this circle of ideas precise requires a number of tools from convex analysis .
in particular , section 123 denes appropriate approximate optimality conditions for the skm in terms of subdierentials and approxi - mate subdierentials .
these conditions are then used in section 123 in the design of an my regularization for the skm and an smo - based algorithm .
we present
123it is worth noting that this dual problem cannot be obtained directly as the lagrangian dual of the qcqp problemlagrangian duals of qcqps are semidenite
the results of numerical experiments with the new method in section 123
learning the kernel matrix
in this section , we ( 123 ) begin with a brief review of the multiple kernel learning problem of lanckriet ( 123 ) , ( 123 ) introduce the support kernel ma - chine ( skm ) , and ( 123 ) show that the dual of the skm is equivalent to the multiple kernel learning primal .
multiple kernel learning problem
in the multiple kernel learning problem , we assume that we are given n data points ( xi , yi ) , where xi x for some input space x , and where yi ( 123 , 123 ) .
we also assume that we are given m matrices kj rnn , which are assumed to be symmetric positive semidef - inite ( and might or might not be obtained from eval - uating a kernel function on the data ( xi ) ) .
we con - sider the problem of learning the best linear combi - j=123 jkj of the kernels kj with nonnega - tive coecients j > 123 and with a trace constraint j=123 j tr kj = c , where c > 123 is ( 123 ) show that this setup
j=123 jkj = pm
lanckriet et al .
yields the following optimization problem :
123 123 123 c , >y = 123
c , j ( 123 , .
, m ) , where d ( y ) is the diagonal matrix with diagonal y , e rn the vector of all ones , and c a positive constant .
the coecients j are recovered as lagrange multipli - ers for the constraints >d ( y ) kjd ( y ) 123
support kernel machine
we now introduce a novel classication algorithm that we refer to as the support kernel machine ( skm ) .
it will be motivated as a block - based variant of the svm and related margin - based classication algo - rithms .
but our underlying motivation is the fact that the dual of the skm is exactly the problem ( l ) .
we establish this equivalence in the following section .
linear classification
in this section we let x = rk .
we also assume we are given a decomposition of rk as a product of m blocks : rk = rk123 rkm , so that each data point xi can be decomposed into m block components , i . e .
xi = ( x123i , .
, xmi ) , where each xji is in general a vector .
is to nd a linear classier of the form
y = sign ( w>x + b ) where w has the same block de - composition w = ( w123 , .
, wm ) rk123++km .
in the spirit of the soft margin svm , we achieve this by min - imizing a linear combination of the inverse of the mar - gin and the training error .
various norms can be used to combine the two terms , and indeed many dierent algorithms have been explored for various combina - tions of `123 - norms and `123 - norms .
in this paper , our goal is to encourage the sparsity of the vector w at the level of blocks; in particular , we want most of its ( multivariate ) components wi to be zero .
a natural way to achieve this is to penalize the `123 - norm of w .
since w is dened by blocks , we minimize the square of a weighted block `123 - norm , ( pm j=123 dj||wj||123 ) 123 , where within every block , an `123 - norm is used .
note that a standard `123 - based svm is obtained if we minimize the square of a block `123 - norm , pm 123 , which corre - sponds to ||w||123 123 , i . e . , ignoring the block structure .
on the other hand , if m = k and dj = 123 , we minimize the square of the `123 - norm of w , which is very similar to the lp - svm proposed by bradley and mangasarian ( 123 ) .
the primal problem for the skm is thus :
j=123 dj||wj||123 ) 123 + c pn ( p ) w . r . t .
w rk123 rkm , rn + , b r
yi ( pj w>
j xji + b ) > 123 i , i ( 123 ,
conic duality and optimality
for a given optimization problem there are many ways of deriving a dual problem .
in our particular case , we treat problem ( p ) as a second - order cone program ( socp ) ( lobo et al . , 123 ) , which yields the following dual ( see appendix a for the derivation ) :
123 123 >e
( d ) w . r . t .
r , rn
123 123 123 c , >y = 123
||pi iyixji||123 123 dj , j ( 123 , .
in addition , the karush - kuhn - tucker ( kkt ) optimal - ity conditions give the following complementary slack -
j xji + b ) 123 + i ) = 123 , i
( a ) i ( yi ( pj w> ( b ) ( c i ) i = 123 , i ||wj ||123 ( cid : 123 ) > ( cid : 123 ) p i iyixji ( c ) ( cid : 123 ) wj ( d ) ( p djtj ) = 123
( cid : 123 ) = 123 , j
equations ( a ) and ( b ) are the same as in the classi - cal svm , where they dene the notion of a support vector .
that is , at the optimum , we can divide the
figure 123
orthogonality of elements of the second - order cone k123 = ( w = ( u , v ) , u r123 , v r , ||u||123 123 v ) : two ele - 123 of k123 are orthogonal and nonzero if and only ments w , w if they belong to the boundary and are anti - proportional .
data points into three disjoint sets : i123 = ( i , i = 123 ) , im = ( i , i ( 123 , c ) ) , and ic = ( i , i = c ) , such that points belonging to i123 are correctly classied points not on the margin and such that i = 123; points in im are correctly classied points on the margin such that i = 123 and yi ( pj w> j xji + b ) = 123 , and points in ic are points on the wrong side of the margin for which i > 123 ( incorrectly classied if i > 123 ) and j xji + b ) = 123 i .
the points whose indices i are in im or ic are the support vectors .
while the kkt conditions ( a ) and ( b ) refer to the in - dex i over data points , the kkt conditions ( c ) and ( d ) refer to the index j over components of the input vec - tor .
these conditions thus imply a form of sparsity not over data points but over input dimensions .
indeed , two non - zero elements ( u , v ) and ( u123 , v123 ) of a second - order cone kd = ( ( u , v ) rd r , ||u||123 123 v ) are or - thogonal if and only if they both belong to the bound - ary , and they are anti - proportional ( lobo et al . , 123 ) ; that is , > 123 such that ||u||123 = v , v123 , ( u , v ) = ( u123 , v123 ) ( see figure 123 ) .
thus , if > 123 , we have : if ||pi iyixji||123 < dj , then wj = 123 , if ||pi iyixji||123 = dj , then j > 123 , such that wj = j pi iyixji , ||wj||123 = jdj .
sparsity thus emerges from the optimization prob - lem .
let j denote the set of active dimensions , i . e . , j ( , ) = ( j : ||pi iyixji||123 = dj ) .
we can rewrite the optimality conditions as
j , wj = j pi iyixji , with j = 123 if j / j .
equation ( d ) implies that = pj dj||wj||123 = pj dj ( jdj ) , which in turn implies pjj d123 j j = 123
we now remove the assumption that x is a euclidean space , and consider embeddings of the data points xi in a euclidean space via a mapping : x rf .
in correspondence with our block - based formulation of
the classication problem , we assume that ( x ) has m distinct block components ( x ) = ( 123 ( x ) , .
, m ( x ) ) .
following the usual recipe for kernel methods , we as - sume that this embedding is performed implicitly , by specifying the inner product in rf using a kernel func - tion , which in this case is the sum of individual kernel functions on each block component :
k ( xi , xj ) = ( xi ) > ( xj ) = pm
s=123 ks ( xi , xj ) .
we now kernelize the problem ( p ) using this ker - in particular , we consider the dual of ( p ) and substitute the kernel function for the inner products in ( d ) :
123 123 e>
( dk ) w . r . t .
r , rn
123 123 123 c , >y = 123
( >d ( y ) kjd ( y ) ) 123 / 123 123 dj , j ,
where kj is the j - th gram matrix of the points ( xi ) corresponding to the j - th kernel .
the sparsity that emerges via the kkt conditions ( c ) and ( d ) now refers to the kernels kj , and we refer to the kernels with nonzero j as support kernels .
the resulting classier has the same form as the svm classier , but is based on the kernel matrix combina - tion k = pj jkj , which is a sparse combination of
equivalence of the two formulations by simply taking dj = q tr kj , we see that problem ( dk ) and ( l ) are indeed equivalentthus the dual of the skm is the multiple kernel learning primal .
care must be taken here thoughthe weights j are dened for ( l ) as lagrange multipliers and for ( dk ) through the anti - proportionality of orthogonal elements of a second - order cone , and a priori they might not coin - cide : although ( dk ) and ( l ) are equivalent , their dual problems have dierent formulations .
it is straightfor - ward , however , to write the kkt optimality condi - tions for ( , ) for both problems and verify that they are indeed equivalent .
one direct consequence is that for an optimal pair ( , ) , is an optimal solution of the svm with kernel matrix pj jkj .
optimality conditions
in this section , we formulate our problem ( in either of its two equivalent forms ) as the minimization of a non - dierentiable convex function subject to linear
constraints .
exact and approximate optimality condi - tions are then readily derived using subdierentials .
in later sections we will show how these conditions lead to an my - regularized algorithmic formulation that will be amenable to smo techniques .
max - function formulation
a rearrangement of the problem ( dk ) yields an equiv - alent formulation in which the quadratic constraints are moved into the objective function :
j n 123 ( s ) w . r . t
123 123 123 c , >y = 123
>d ( y ) kjd ( y ) >e and we let jj ( ) denote j ( ) = maxj jj ( ) .
problem ( s ) is the minimization of the non - dierentiable convex function j ( ) subject to linear constraints .
let j ( ) be the set of active kernels , i . e . , the set of indices j such that jj ( ) = j ( ) .
we let fj ( ) rn denote the gradient of jj , that is , fj = jj
optimality conditions and subdierential
given any function j ( ) , the subdierential of j at j ( ) is dened as ( bertsekas , 123 ) : j ( ) = ( g rn , 123 , j ( 123 ) > j ( ) + g> ( 123 ) ) .
elements of the subdierential j ( ) are called sub - gradients .
when j is convex and dierentiable at , then the subdierential is a singleton and reduces to the gradient .
the notion of subdierential is especially useful for characterizing optimality conditions of non - smooth problems ( bertsekas , 123 ) .
the function j ( ) dened in the earlier section is a pointwise maximum of convex dierentiable functions , and using subgradient calculus we can easily see that the subdierential j ( ) of j at is equal to the convex hull of the gradients fj of jj for the active kernels .
that is :
j ( ) = convex hull ( fj ( ) , j j ( ) ) .
the lagrangian for ( s ) is equal to l ( ) = j ( ) > + > ( ce ) + b>y , where b r , , rn the global minimum of l ( , , , b ) with respect to is characterized by the equation
123 l ( ) by j ( ) .
the optimality conditions are thus the following : , ( b , , ) is a pair of optimal primal / dual variables
if and only if :
i , ii = 123 , i ( c i ) = 123 >y = 123 , 123 123 123 c .
as before , we dene im ( ) = ( i , 123 < i < c ) , i123 ( ) = ( i , i = 123 ) , ic ( ) = ( i , i = c ) .
we also de - ne , following ( keerthi et al . , 123 ) , i123+ = i123 ( i , yi = 123 ) and i123 = i123 ( i , yi = 123 ) , ic+ = ic ( i , yi = 123 ) , ic = ic ( i , yi = 123 ) .
we can then rewrite the optimality conditions as
j j = 123
be = d ( y ) pjj ( ) d123 > 123 , pj d123 i im i123+ ic , i > 123 i im i123+ ic , i 123 123
approximate optimality conditions
exact optimality conditions ( op t123 ) or ( op t123 ) are generally not suitable for numerical op - in non - smooth optimization theory , one instead formulates optimality criteria in terms of the - subdierential , which is dened as j ( ) = ( g rn , 123 , j ( 123 ) > j ( ) +g> ( 123 ) ) .
when j ( ) = maxj jj ( ) , then the - subdierential contains ( potentially strictly ) the convex hull of the gradients fj ( ) , for all - active functions , i . e . , for all j such that maxi ji ( ) 123 jj ( ) .
we let j ( ) denote the set of all such kernels .
so , we have c ( ) = convex hull ( fj ( ) , j j ( ) ) j ( ) .
within 123 of zero , and that the usual kkt conditions are met .
that is , we stop whenever there exist , b , g
referred to as
i im i123+ ic , i > 123 i im i123+ ic , i 123 123 || be d ( y ) g|| 123 123
note that for one kernel , i . e . , when the skm re - duces to the svm , this corresponds to the approxi - mate kkt conditions usually employed for the stan - dard svm ( platt , 123; keerthi et al . , 123; joachims , 123 ) .
for a given , checking optimality is hard , since even computing 123j ( ) is hard in closed form .
how - ever , a sucient condition for optimality can be ob - tained by using the inner approximation c123 ( ) of this
123 - subdierential , i . e . , the convex hull of gradients of 123 - active kernels .
checking this sucient condition is a linear programming ( lp ) existence problem , i . e . , nd such that :
> 123 , j = 123 if j / j123 ( ) , pj d123
iim i123ic+ ( ( k ( ) d ( y ) ) i yi )
j j = 123
iim i123+ic ( ( k ( ) d ( y ) ) i yi ) + 123 , where k ( ) = pjj123 ( ) jkj .
given , we can de - termine whether it is ( 123 , 123 ) - optimal by solving the potentially large lp ( op t123 ) .
if in addition to having , we know a potential candidate for , then a suf - cient condition for optimality is that this veries ( op t123 ) , which doesnt require solving the lp .
indeed , the iterative algorithm that we present in section 123 outputs a pair ( , ) and only these sucient optimal - ity conditions need to be checked .
improving sparsity
once we have an approximate solution , i . e . , values and that satisfy ( op t123 ) , we can ask whether can be made sparser .
indeed , if some of the kernels are close to identical , then some of the s can potentially be removedfor a general svm , the optimal is not unique if data points coincide , and for a general skm , the optimal and are not unique if data points or kernels coincide .
when searching for the minimum `123 - norm which satises the constraints ( op t123 ) , we can thus consider a simple heuristic approach where we loop through all the nonzero j and check whether each such component can be removed .
that is , for all j j123 ( ) , we force j to zero and solve the lp .
if it is feasible , then the j - th kernel can be removed .
regularized support kernel machine
the function j ( ) is convex but not dierentiable .
it is well known that in this situation , steepest de - scent and coordinate descent methods do not necessar - ily converge to the global optimum ( bertsekas , 123 ) .
smo unfortunately falls into this class of methods .
therefore , in order to develop an smo - like algorithm for the skm , we make use of moreau - yosida regu - in our specic case , this simply involves adding a second regularization term to the objective function of the skm , as follows : 123 ( pj dj||wj||123 ) 123 + 123
123 pj a123 ( r ) w . r . t .
w rk123 rkm , rn
+ , b r
123 + c pi i
yi ( pj w>
j xji + b ) > 123 i , i ( 123 ,
where ( aj ) are the my - regularization parameters .
dual problem
the conic dual is readily computed as :
r+ , rm , rn
123 123 i 123 c , >y = 123
||pi iyixji||123 123 j , j .
if we dene the function g ( ) as
( j dj ) 123
123 123 + 123
g ( ) = minr+ , rm ( 123
||pi iyixji||123 123 j , j ) , then the dual problem is equivalent to minimizing g ( ) subject to 123 123 123 c and >y = 123
we prove in appendix b that g ( ) is dierentiable and we show how to compute g ( ) and its derivative in time o ( m log m ) .
solving the my - regularized skm using
since the objective function g ( ) is dierentiable , we can now safely envisage an smo - like approach , which consists in a sequence of local optimizations over only two components of .
since the - optimality con - ditions for the my - regularized skm are exactly the same as for the svm , but with a dierent objective function ( platt , 123; keerthi et al . , 123 ) :
iim i123+ic ( yig ( ) i ) + 123 ,
choosing the pair of indices can be done in a manner similar to that proposed for the svm , by using the fast heuristics of platt ( 123 ) and keerthi et al .
( 123 ) .
in addition , caching and shrinking techniques ( joachims , 123 ) that prevent redundant computations of kernel matrix values can also be employed .
a dierence between our setting and the svm set - ting is the line search , which cannot be performed in closed form for the my - regularized skm .
however , since each line search is the minimization of a con - vex function , we can use ecient one - dimensional root nding , such as brents method ( brent , 123 ) .
theoretical bounds
in order to be able to check eciently the approxi - mate optimality condition ( op t123 ) of section 123 , we need estimates for and from the solution of the
my - regularized skm obtained by smo .
it turns out that the kkt conditions for the my - regularized skm also lead to support kernels , i . e . , there is a sparse non - negative weight vector such that is a solution of the svm with the kernel k = pj jkj .
however , in the regularized case , those weights can be obtained directly from as a byproduct of the computation of g ( ) and its derivative .
those weights ( ) do not satisfy pj d123 j j = 123 , but can be used to dene weights ( ) that do ( we give expressions for ( ) and ( ) in
let 123 , 123 be the two tolerances for the approximate optimality conditions for the skm .
in this section , we show that if ( aj ) are small enough , then an 123 / 123 - optimal solution of the my - regularized skm , to - gether with ( ) , is an ( 123 , 123 ) - optimal solution of the skm , and an a priori bound on ( aj ) is obtained that does not depend on the solution .
theorem 123 let 123 < < 123
let y ( 123 , 123 ) n and kj , j = 123 , .
, m be m positive semidenite kernel matri - ces .
let dj and aj , j = 123 , .
, m , be 123m strictly posi - tive numbers .
if is an - optimal solution of the my - regularized skm , then ( , ( ) ) is an ( 123 , 123 ) - optimal solution of the skm , with
123 = nc max
) and 123 = +c max
where mj = max
corollary 123 with the same assumptions and
123 min ( cid : 123 ) min
123 + ( 123 + 123
nc ) 123 / 123 ,
if is an 123 / 123 - optimal solution of the my - regularized skm , then ( , ( ) ) is an ( 123 , 123 ) - optimal solution of
a minimization algorithm
we solve the skm by solving the my - regularized skm with decreasing values of the regularization parameters in our simulations , the kernel matrices are all normalized , i . e . , have unit diagonal , so we can choose all dj equal .
we use aj ( ) = and dj ( ) = ( 123 ) , where is a constant in ( 123 , 123 ) .
when = 123 , the my - regularized skm is exactly the svm based on the sum of the kernels , while when = 123 , it is the non - my -
the algorithm is as follows : given the data and pre - cision parameters 123 , 123 , we start with = 123 , which
solves the svm up to precision 123 / 123 using standard smo , and then update to ( where < 123 ) and solve the my - regularized skm with constant using the adjusted smo up to precision 123 / 123 , and so on .
at the end of every smo optimization , we can extract weights j ( ) from the solution , as shown in ap - pendix b , and check the ( 123 , 123 ) - optimality conditions ( op t123 ) of the original problem ( without solving the lp ) .
once they are satised , the algorithm stops .
since each smo optimization is performed on a dierentiable function with lipschitz gradient and smo is equivalent to steepest descent for the `123 - norm ( joachims , 123 ) , classical optimization results show that each of those smo optimizations is nitely convergent ( bertsekas , 123 ) .
corollary 123 directly im - plies there are only a nite number of such optimiza - tions; thus , the overall algorithm is nitely convergent .
additional speed - ups can be easily achieved here .
for example , if for successive values of , some kernels have a zero weight , we might as well remove them from the algorithm and check after convergence if they can be safely kept out .
in simulations , we use the following values for the free parameters : = 123 , 123 / n = 123 , 123 = 123 , where the value for 123 / n corresponds to the average value this quantity attains when solving the qcqp ( l ) directly using mosek .
we compare the algorithm presented in section 123 with solving the qcqp ( l ) using mosek for two datasets , ionosphere and breast cancer , from the uci repository , and nested subsets of the adult dataset from platt ( 123 ) .
the basis kernels are gaussian kernels on random subsets of features , with varying widths .
we vary the number of kernels m for xed number of data points n , and vice versa .
we report running time results ( athlon mp 123+ processor , 123g ram ) in figure 123
empirically , we obtain an average scaling of m123 and n123 for the smo - based approach and m123 and n123 for mosek .
thus the al - gorithm presented in this paper appears to provide a signicant improvement over mosek in computational complexity , both in terms of the number of kernels and the number of data points .
we have presented an algorithm for ecient learning of kernels for the support vector machine .
our al - gorithm is based on applying sequential minimization techniques to a smoothed version of a convex non - smooth optimization problem .
the good scaling with
ionosphere , n = 123
adult , n = 123
m smo mosek
breast cancer , n = 123 m smo mosek
adult , m = 123
figure 123
running times in seconds for mosek and smo .
( top ) ionosphere and breast cancer data , with xed num - ber of data points n and varying number of kernels m .
( bottom ) adult dataset : ( left ) with xed n and varying m , ( right ) with xed m and varying n ( means mosek ran out of memory ) .
respect to the number of data points makes it pos - sible to learn kernels for large scale problems , while the good scaling with respect to the number of basis kernels opens up the possibility of application to large - scale feature selection , in which the algorithm selects kernels that dene non - linear mappings on subsets of
appendix a .
dual of the skm
the primal problem ( p ) can be put in the following equivalent form , where kk = ( ( u , v ) rk+123 , ||u||123 123 v ) is the second - order cone of order k ( we now omit the summation intervals , with the convention that index i goes from 123 to n and index j goes from 123 to m ) :
u r , t rm , b r , rn j xji + b ) > 123 i , i
yi ( pj w>
+ , ( wj , tj ) kkj , j
123 u123 + c pi i
pj djtj 123 u .
the cone kk is self - dual , so the conic lagrangian cor - responding to the problem is l = 123
123 u123 +c pi ipii ( yi ( pj w> pi ii + ( pjdjtju ) pj ( >
j xji +b ) 123+i ) j wj +jtj ) ,
with i r+ , i r+ , r+ , ( j , j ) kkj .
after computing derivatives with respect to the pri - mal variables and setting them to zero , we readily get the dual function : g ( , , , , ) = 123 123 123 + pi i de - ned on the domain dened by i > 123 , i > 123 , > 123 , ||j||123 123 j , dj j = 123 , pi iyixji + j = 123 , pi iyi = 123 , c i i = 123
after elimination of dummy variables , we obtain problem ( d ) .
appendix b .
computation of g ( ) let j ( ) = 123 over each i; a short calculation reveals :
dj ||pi iyixji||123
we can rst maximize
j >|| p i iyixji||123 which implies that
( j j ) 123 = d123
j max ( 123 , j ) 123 ,
g ( ) = min ( 123
123 123 + 123
j , ) pi i ) ,
where ( x , y ) = max ( 123 , x y ) 123
the function is continuously dierentiable , with partial derivatives y ( cid : 123 ) = ( 123 y / x , 123y 123x ) if y 123 x , equal to ( cid : 123 ) and zero otherwise .
also , for given x , it is a piecewise quadratic function of y .
we thus need to minimize a piecewise quadratic dierentiable strictly convex func - tion of , which can be done easily by inspecting all points of non - dierentiability , which requires sorting the sequence ( j ) .
the complexity of such an algo - rithm is o ( m log m ) .
because of strict convexity the minimum with respect to is unique and denoted ( ) .
in addition , this uniqueness implies that g ( ) is dierentiable and that its derivative is equal to :
g ( ) = 123
j ( ) , ( ) ) 123
j ( ) e
j ( cid : 123 ) 123 ( ) j ( cid : 123 ) 123 ( )
j ( ) ( cid : 123 ) d ( y ) kjd ( y ) e .
= pjj ( ) j ( ) ( cid : 123 ) if j ( ) > ( ) , and we dene j ( ) = 123 zero otherwise .
we also dene j ( ) = j ( ) / ( 123 j j ( ) ) .
using the optimality conditions for ( ) , it is easy to prove that pj d123 j j ( ) = 123
the weights j ( ) provide an estimate of the weights for the skm , and can be used to check optimality .
corollary 123 shows that if ( aj ) is small enough , then if is ap - proximately optimal for the my - regularized skm , the pair ( , ( ) ) is approximately optimal for the skm .
we wish to acknowledge support from a grant from intel corporation , and a graduate fellowship to francis bach from microsoft research .
