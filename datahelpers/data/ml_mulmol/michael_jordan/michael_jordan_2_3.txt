we build up the mathematical connection between the \expectation - maximization " ( em ) algorithm and
gradient - based approaches for maximum likelihood learning of ( cid : 123 ) nite gaussian mixtures .
we show that the em step in parameter space is obtained from the gradient via a projection matrix p , and we provide an explicit expression for the matrix .
we then analyze the convergence of em in terms of special properties of p and provide new results analyzing the e ( cid : 123 ) ect that p has on the likelihood surface .
based on these mathematical results , we present a comparative discussion of the advantages and disadvantages of em
and other algorithms for the learning of gaussian mixture models .
copyright c ( cid : 123 ) massachusetts institute of technology ,
this report describes research done at the center for biological and computational learning and the arti ( cid : 123 ) cial intelligence laboratory of the massachusetts institute of technology .
support for the center is provided in part by a grant from the national science foundation under contract asc ( .
support for the laboratorys arti ( cid : 123 ) cial intelligence research is provided in part by the advanced research projects agency of the department of defense under o ( cid : 123 ) ce of naval research contract n - - a - .
the authors were also supported by the hk rgc earmarked grant cuhk / e , by a grant from the mcdonnell - pew foundation , by a grant from atr human information processing research laboratories , by a grant from siemens corporation , and by grant n - - - from the o ( cid : 123 ) ce of naval research .
michael i .
jordan is an nsf presidential young investigator .
the component populations in a mixture are poorly sep -
arated , the em algorithm can be expected to produce
in a very small number of iterations parameter values
such that the mixture density determined by them re -
( cid : 123 ) ects the sample data very well . " in the context of the
current literature on learning , in which the predictive
aspect of data modeling is emphasized at the expense of
the traditional fisherian statisticians concern over the
\true " values of parameters , such rapid convergence in
likelihood is a major desideratum of a learning algorithm
and undercuts the critique of em as a \slow " algorithm .
in the current paper , we provide a comparative anal -
ysis of em and other optimization methods .
we empha -
size the comparison between em and other ( cid : 123 ) rst - order
methods ( gradient ascent , conjugate gradient methods ) ,
because these have tended to be the methods of choice
in the neural network literature .
however , we also com -
pare em to superlinear and second - order methods
argue that em has a number of advantages , including its
naturalness at handling the probabilistic constraints of
mixture problems and its guarantees of convergence
also provide new results suggesting that under appropri -
ate conditions em may in fact approximate a superlin -
ear method; this would explain some of the promising
empirical results that have been obtained ( jordan & ja -
cobs , ) , and would further temper the critique of em
o ( cid : 123 ) ered by redner and walker .
the analysis in the cur -
rent paper focuses on unsupervised learning; for related
results in the supervised learning domain see jordan and
xu ( in press ) .
the remainder of the paper is organized as follows .
we ( cid : 123 ) rst brie ( cid : 123 ) y review the em algorithm for gaussian
mixtures .
the second section establishes a connection
between em and the gradient of the log likelihood
then present a comparative discussion of the advantages
and disadvantages of various optimization algorithms in
the gaussian mixture setting .
we then present empir -
ical results suggesting that em regularizes the condi -
tion number of the e ( cid : 123 ) ective hessian .
the fourth section
presents a theoretical analysis of this empirical ( cid : 123 ) nding .
the ( cid : 123 ) nal section presents our conclusions .
the em algorithm for gaussian
we study the following probabilistic model :
p ( xj ( cid : 123 ) ) =
( cid : 123 ) jp ( xjmj; ( cid : 123 ) j ) ;
the \expectation - maximization " ( em ) algorithm is a
general technique for maximum likelihood ( ml ) or max -
imum a posteriori ( map ) estimation .
the recent em -
phasis in the neural network literature on probabilistic
models has led to increased interest in em as a possible
alternative to gradient - based methods for optimization .
em has been used for variations on the traditional theme
of gaussian mixture modeling ( ghahramani & jordan ,
; nowlan , ; xu & jordan , a , b; tresp , ah -
mad & neuneier , ; xu , jordan & hinton , ) and
has also been used for novel chain - structured and tree -
structured architectures ( bengio & frasconi , ; jor -
dan & jacobs , ) .
the empirical results reported in
these papers suggest that em has considerable promise
as an optimization method for such architectures
over , new theoretical results have been obtained that
link em to other topics in learning theory ( amari , ;
jordan & xu , ; neal & hinton , ; xu & jordan ,
c; yuille , stolorz & utans , ) .
despite these developments , there are grounds for
caution about the promise of the em algorithm
reason for caution comes from consideration of theoret -
ical convergence rates , which show that em is a ( cid : 123 ) rst
more precisely , there are two key re -
sults available in the statistical literature on the con -
vergence of em .
first , it has been established that un -
der mild conditions em is guaranteed to converge to a local maximum of the log likelihood l ( boyles , ; dempster , laird & rubin , ; redner & walker ,
; wu , ) .
( indeed the convergence is monotonic : is the value of the pa - rameter vector ( cid : 123 ) at iteration k . ) second , considering
) ( cid : 123 ) l ( ( cid : 123 )
) , where ( cid : 123 )
em as a mapping ( cid : 123 )
= m ( ( cid : 123 )
) , we have ( cid : 123 )
= m ( ( cid : 123 )
) with ( cid : 123 ) xed point
is near ( cid : 123 )
, and thus
( k+ ) ( cid : 123 ) ( cid : 123 )
( cid : 123 ) k ( cid : 123 ) k
k ( cid : 123 ) k ( cid : 123 )
( k ) ( cid : 123 ) ( cid : 123 )
almost surely .
that is , em is a ( cid : 123 ) rst order algorithm .
the ( cid : 123 ) rst - order convergence of em has been cited in
the statistical literature as a major drawback
ner and walker ( ) , in a widely - cited article , argued
that superlinear ( quasi - newton , method of scoring ) and
second - order ( newton ) methods should generally be pre -
ferred to em .
they reported empirical results demon -
strating the slow convergence of em on a gaussian mix -
ture model problem for which the mixture components
were not well separated .
these results did not include
tests of competing algorithms , however .
moreover , even
though the convergence toward the \optimal " parameter
values was slow in these experiments , the convergence in
likelihood was rapid .
indeed , redner and walker ac -
knowledge that their results show that \ . . .
even when
an iterative algorithm is said to have a local convergence rate of order q ( cid : 123 ) if k ( cid : 123 ) ( k+ ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) k=k ( cid : 123 ) ( k ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) kq ( cid : 123 ) r + o ( k ( cid : 123 ) ( k ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) k ) for k su ( cid : 123 ) ciently large .
( x ( cid : 123 ) mj ) t ( cid : 123 ) ( cid : 123 )
p ( xjmj; ( cid : 123 ) j ) = where ( cid : 123 ) j ( cid : 123 ) and pk
j= ( cid : 123 ) j = and d is the dimension - ality of the vector x .
the parameter vector ( cid : 123 ) consists of the mixing proportions ( cid : 123 ) j , the mean vectors mj , and the covariance matrices ( cid : 123 ) j .
given k and given n independent , identically dis - , we obtain the following log
tributed samples fx ( t ) gn
l ( ( cid : 123 ) ) = log
p ( x ( t ) j ( cid : 123 ) ) =
log p ( x ( t ) j ( cid : 123 ) ) ;
which can be optimized via the following iterative algo -
rithm ( see , e . g , dempster , laird & rubin , ) :
of the matrix b , and \ ( cid : 123 ) " denotes the kronecker prod - uct .
moreover , given the constraints pk j = and j ( cid : 123 ) , p ( k ) a is a positive de ( cid : 123 ) nite matrix and the ma - trices p ( k ) ( cid : 123 ) j are positive de ( cid : 123 ) nite with probability one for n su ( cid : 123 ) ciently large .
mj and p ( k )
( ) we begin by considering the em update for the mixing proportions ( cid : 123 ) i .
from eqs .
( ) and ( ) ,
j ( t ) ( x ( t ) ( cid : 123 ) m ( k+ )
) ( x ( t ) ( cid : 123 ) m ( k+ )
( p ( x ( t ) ; ( cid : 123 ) ( k )
) ; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; p ( x ( t ) ; ( cid : 123 ) ( k )
i p ( x ( t ) ; ( cid : 123 ) ( k )
premultiplying by p ( k )
a , we obtain
where the posterior probabilities h ( k )
are de ( cid : 123 ) ned as fol -
j p ( x ( t ) jm ( k )
i p ( x ( t ) jm ( k )
j ( t ) =
connection between em and
in the following theorem we establish a relationship be -
tween the gradient of the log likelihood and the step in
parameter space taken by the em algorithm .
in par -
ticular we show that the em step can be obtained by
premultiplying the gradient by a positive de ( cid : 123 ) nite ma -
we provide an explicit expression for the matrix .
theorem at each iteration of the em algorithm eq .
( ) , we have
a ( k+ ) ( cid : 123 ) a ( k )
= p ( k )
= p ( k )
) ( cid : 123 ) vec ( ( cid : 123 )
) = p ( k )
@mj jmj =m ( k )
; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; ( cid : 123 ) ( k )
k ) ( cid : 123 ) a ( k )
j ( cid : 123 ) ( cid : 123 )
where a denotes the vector of mixing proportions t , j indexes the mixture components ( j = ( ( cid : 123 ) ; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; ( cid : 123 ) k ) ; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; k ) , k denotes the iteration number , \vec ( b ) " de - notes the vector obtained by stacking the column vectors
although we focus on maximum likelihood ( ml ) estima - tion in this paper , it is straightforward to apply our results to maximum a posteriori ( map ) estimation by multiplying the likelihood by a prior .
p ( x ( t ) ; ( cid : 123 ) ( k )
i p ( x ( t ) ; ( cid : 123 ) ( k )
i p ( x ( t ) ; ( cid : 123 ) ( k )
t ( cid : 123 ) a ( k ) pk ) ; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ) t ( cid : 123 ) a ( k ) :
( t ) ; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; h ( k )
the update formula for a in eq .
( ) can be rewritten as t ( cid : 123 ) a ( k ) :
( t ) ; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; h ( k )
combining the last two equations establishes the update
by jensens inequality we have
rule for a ( eq .
furthermore , for an arbitrary vec - tor u , we have n ut p ( k ) k ) u ( cid : 123 )
; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; ( cid : 123 ) ( k )
a u = ut
k ) u =
; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; ( cid : 123 ) ( k )
j uj )
thus , ut p ( k )
a u > and p ( k )
a is positive de ( cid : 123 ) nite given
( ) we now consider the em update for the means
j = and ( cid : 123 ) ( k )
j ( cid : 123 ) for all j .
the constraints pk
it follows from eqs .
( ) and ( ) that
@mj jmj =m ( k )
( x ( t ) ( cid : 123 ) m ( k )
premultiplying by p ( k )
@mj jmj =m ( k )
from eq .
( ) , we have pn
is positive de ( cid : 123 ) nite with probability one assuming that n
( t ) > ; moreover , ( cid : 123 )
j ( t ) x ( t ) ( cid : 123 ) m ( k )
is large enough such that the matrix is of full rank .
thus , it follows from eq .
( ) that p ( k )
mj is positive de ( cid : 123 ) nite with
( ) finally , we prove the third part of the theorem .
it follows from eqs .
( ) and ( ) that
using the notation ( cid : 123 ) = ( mt
; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; mt
k ; vec ( ( cid : 123 ) )
t ; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; vec ( ( cid : 123 ) k )
t ; at
and p ( ( cid : 123 ) ) = diag ( pm ; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; pmk ; p ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; p ( cid : 123 ) k ; pa ) , we
can combine the three updates in theorem into a single
+ p ( ( cid : 123 )
@ ( cid : 123 ) j j ( cid : 123 ) j = ( cid : 123 ) ( k )
j ( cid : 123 ) ( x ( t ) ( cid : 123 ) m ( k )
) ( x ( t ) ( cid : 123 ) m ( k )
with this in mind , we rewrite the em update formula
under the conditions of theorem , p ( ( cid : 123 ) de ( cid : 123 ) nite matrix with probability one .
recalling that for a positive de ( cid : 123 ) nite matrix b , we have @ ( cid : 123 ) > , we have the following corollary :
) is a positive
( x ( t ) ( cid : 123 ) m ( k )
t ( cid : 123 ) ( cid : 123 )
j ( t ) ( x ( t ) ( cid : 123 ) m ( k )
corollary for each iteration of the em algorithm given by eq . ( ) , the search direction ( cid : 123 ) a positive projection on the gradient of l .
( k+ ) ( cid : 123 ) ( cid : 123 )
that is , the em algorithm can be viewed as a variable
j ) vec ( u )
j ( cid : 123 ) ( cid : 123 ) j u ( cid : 123 ) j u ) j u )
j u ) ) j u )
where equality holds only when ( cid : 123 )
j u = for all u .
equality is impossible , however , since ( cid : 123 ) de ( cid : 123 ) nite with probability one n is su ( cid : 123 ) ciently large .
thus j ( t ) > that p ( k )
it follows from eq .
( ) and pn
is positive de ( cid : 123 ) nite with probability one .
v ( cid : 123 ) j = ( cid : 123 )
j ( cid : 123 ) ( x ( t ) ( cid : 123 ) m ( k )
that is , we have
) ( x ( t ) ( cid : 123 ) m ( k )
utilizing the identity vec ( abc ) = ( c t ( cid : 123 ) a ) vec ( b ) , we
) = vec ( ( cid : 123 )
j ( cid : 123 ) ( cid : 123 )
@ ( cid : 123 ) j j ( cid : 123 ) j = ( cid : 123 ) ( k )
thus p ( k )
j ( cid : 123 ) ( cid : 123 )
moreover , for an
arbitrary matrix u , we have
metric gradient ascent algorithm for which the projection matrix p ( ( cid : 123 )
) changes at each iteration as a function
of the current parameter value ( cid : 123 )
our results extend earlier results due to baum and
sell ( ) .
baum and sell studied recursive equations
of the following form :
= t ( x ( k ) ) = ( t ( x ( k )
) ; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; t ( x ( k )
having positive coe ( cid : 123 ) cients .
they showed
i = , where j is a polyno -
i ( cid : 123 ) ; pk ) ( cid : 123 ) x ( k )
mial in x ( k ) that the search direction of this recursive formula , i . e . , of of j with respect to the x ( k ) biner & sondhi , ) .
it can be shown that baum and
, has a positive projection on the gradient
( see also levinson , ra -
sells recursive formula implies the em update formula
for a in a gaussian mixture .
thus , the ( cid : 123 ) rst statement
in theorem is a special case of baum and sells earlier
however , baum and sells theorem is an existence
theorem and does not provide an explicit expression for the matrix pa that transforms the gradient direction into the em direction .
our theorem provides such an explicit form for pa .
moreover , we generalize baum and sells results to handle the updates for mj and ( cid : 123 ) j , and we provide explicit expressions for the positive de ( cid : 123 ) nite transformation matrices pmj and p ( cid : 123 ) j as well .
it is also worthwhile to compare the em algorithm to other gradient - based optimization methods .
newtons method is obtained by premultiplying the gradient by the inverse of the hessian of the log likelihood :
+ h ( ( cid : 123 )
newtons method is the method of choice when it can
be applied , but the algorithm is often di ( cid : 123 ) cult to use
in particular , the algorithm can diverge
when the hessian becomes nearly singular; moreover ,
the computational costs of computing the inverse hes -
sian at each step can be considerable .
an alternative
is to approximate the inverse by a recursively updated .
such a modi ( cid : 123 ) cation is called a quasi - newton method .
conventional quasi - newton methods are unconstrained optimization meth -
on unconstrained convergence rates problematic
over , it is not easy to meet the constraints on the covari -
ance matrices in the mixture using such techniques .
a second appealing property of p ( ( cid : 123 )
) is that each
ods , however , and must be modi ( cid : 123 ) ed in order to be used
in the mixture setting ( where there are probabilistic con -
straints on the parameters ) .
in addition , quasi - newton
iteration of em is guaranteed to increase the likelihood of the likelihood is achieved without step - size parameters
this monotonic convergence
) ( cid : 123 ) l ( ( cid : 123 )
methods generally require that a one - dimensional search
or line searches .
other gradient - based optimization tech -
be performed at each iteration in order to guarantee con -
niques , including gradient descent , quasi - newton , and
vergence .
the em algorithm can be viewed as a special
newtons method , do not provide such a simple theo -
form of quasi - newton method in which the projection matrix p ( ( cid : 123 ) we discuss in the remainder of the paper , this partic -
) in eq .
( ) plays the role of b ( k )
ular matrix has a number of favorable properties that
make em particularly attractive for optimization in the
constrained optimization and general
an important property of the matrix p is that the em step in parameter space automatically satis ( cid : 123 ) es the prob -
abilistic constraints of the mixture model in eq .
the domain of ( cid : 123 ) contains two regions that embody the
probabilistic constraints : d = f ( cid : 123 ) : pk j = g and d = f ( cid : 123 ) : ( cid : 123 ) ( k ) j ( cid : 123 ) , ( cid : 123 ) j positive de ( cid : 123 ) niteg .
for the em algorithm the update for the mixing proportions ( cid : 123 ) j can be rewritten as follows :
j p ( x ( t ) jm ( k )
i p ( x ( t ) jm ( k )
it is obvious that the iteration stays within d
larly , the update for ( cid : 123 ) j can be rewritten as :
( x ( t ) ( cid : 123 ) m ( k )
) ( x ( t ) ( cid : 123 ) m ( k )
j p ( x ( t ) jm ( k )
i p ( x ( t ) jm ( k )
which stays within d for n su ( cid : 123 ) ciently large .
whereas em automatically satis ( cid : 123 ) es the probabilistic
constraints of a mixture model , other optimization tech -
niques generally require modi ( cid : 123 ) cation to satisfy the con -
straints .
one approach is to modify each iterative step
to keep the parameters within the constrained domain .
a number of such techniques have been developed , in -
cluding feasible direction methods , active sets , gradient
projection , reduced - gradient , and linearly - constrained
quasi - newton .
these constrained methods all incur ex -
tra computational costs to check and maintain the con -
straints and , moreover , the theoretical convergence rates
for such constrained algorithms need not be the same as
retical guarantee , even assuming that the constrained
( k+ ) ( cid : 123 ) ( cid : 123 )
problem has been transformed into an unconstrained one .
for gradient ascent , the step size ( cid : 123 ) must be chosen to ensure that k ( cid : 123 ) ) ) k < .
this requires a one - dimensional ki + ( cid : 123 ) h ( ( cid : 123 ) line search or an optimization of ( cid : 123 ) at each iteration , which requires extra computation which can slow down the convergence .
an alternative is to ( cid : 123 ) x ( cid : 123 ) to a very small value which generally makes ki + ( cid : 123 ) h ( ( cid : 123 )
close to one and results in slow convergence .
for new -
( k ) ( cid : 123 ) ( cid : 123 )
tons method , the iterative process is usually required
to be near a solution , otherwise the hessian may be in -
de ( cid : 123 ) nite and the iteration may not converge .
levenberg -
marquardt methods handle the inde ( cid : 123 ) nite hessian ma -
trix problem; however , a one - dimensional optimization
or other form of search is required for a suitable scalar
to be added to the diagonal elements of hessian .
fisher
scoring methods can also handle the inde ( cid : 123 ) nite hessian
matrix problem , but for non - quadratic nonlinear opti - mization fisher scoring requires a stepsize ( cid : 123 ) that obeys ) ) k < , where b is the fisher infor - ki + ( cid : 123 ) bh ( ( cid : 123 )
mation matrix .
thus , problems similar to those of gra -
dient ascent arise here as well .
finally , for the quasi -
newton methods or conjugate gradient methods , a one -
dimensional line search is required at each iteration
summary , all of these gradient - based methods incur ex -
tra computational costs at each iteration , rendering sim -
ple comparisons based on local convergence rates unre -
for large scale problems , algorithms that change the
parameters immediately after each data point ( \on - line
algorithms " ) are often signi ( cid : 123 ) cantly faster in practice
than batch algorithms .
the popularity of gradient de -
scent algorithms for neural networks is in part to the
ease of obtaining on - line variants of gradient descent .
it is worth noting that on - line variants of the em algo -
rithm can be derived ( neal & hinton , , titterington ,
) , and this is a further factor that weighs in favor
of em as compared to conjugate gradient and newton
convergence rate comparisons
in this section , we provide a comparative theoretical dis -
cussion of the convergence rates of constrained gradient
that for the corresponding unconstrained algorithms
ascent and em .
second approach is to transform the constrained opti -
mization problem into an unconstrained problem before
using the unconstrained method .
this can be accom -
plished via penalty and barrier functions , lagrangian
terms , or re - parameterization .
once again , the extra al -
gorithmic machinery renders simple comparisons based
for gradient ascent a local convergence result can by
obtained by taylor expanding the log likelihood around
the maximum likelihood estimate ( cid : 123 ) large k we have : ( k+ ) ( cid : 123 ) ( cid : 123 )
( cid : 123 ) k ( cid : 123 ) ki + ( cid : 123 ) h ( ( cid : 123 )
for su ( cid : 123 ) ciently
( k ) ( cid : 123 ) ( cid : 123 )
since h ( ( cid : 123 )
) is negative de ( cid : 123 ) nite , we obtain
ki + ( cid : 123 ) h ( ( cid : 123 )
) k ( cid : 123 ) ( cid : 123 ) m ( i + ( cid : 123 ) h ( ( cid : 123 )
where h is the hessian of l , ( cid : 123 ) is the step size , and r = maxfj ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) m ( ( cid : 123 ) h ( ( cid : 123 ) where ( cid : 123 ) m ( a ) and ( cid : 123 ) m ( a ) denote the largest and small - est eigenvalues of a , respectively .
j ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) m ( ( cid : 123 ) h ( ( cid : 123 )
) ) = r;
smaller values of r correspond to faster convergence rates .
to guarantee convergence , we require r < or < ( cid : 123 ) < = ( cid : 123 ) m ( ( cid : 123 ) h ( ( cid : 123 ) ) ) .
the minimum possible value of r is obtained when ( cid : 123 ) = = ( cid : 123 ) m ( h ( ( cid : 123 )
rmin = ( cid : 123 ) ( cid : 123 ) m ( h ( ( cid : 123 )
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
) ) = ( cid : 123 ) m ( h ( ( cid : 123 )
where ( cid : 123 ) ( h ) = ( cid : 123 ) m ( h ) = ( cid : 123 ) m ( h ) is the condition number of h .
larger values of the condition number correspond to slower convergence .
when ( cid : 123 ) ( h ) = we have rmin = , which corresponds to a superlinear rate of convergence .
indeed , newtons method can be viewed as a method
for obtaining a more desirable condition number|the inverse hessian h ( cid : 123 ) balances the hessian h such that the resulting condition number is one .
e ( cid : 123 ) ectively , new -
rc ( cid : 123 ) q + ( cid : 123 ) ( cid : 123 )
m ( ( cid : 123 ) hc ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) m ( ( cid : 123 ) hc ) :
we see from this derivation that the convergence
in this equation hc = et h ( ( cid : 123 ) ) e is the hessian of l restricted to d .
speed depends on ( cid : 123 ) ( hc ) = ( cid : 123 ) m ( ( cid : 123 ) hc ) = ( cid : 123 ) m ( ( cid : 123 ) hc ) .
when ( cid : 123 ) ( hc ) = , we have p + ( cid : 123 ) ( cid : 123 ) m ( ( cid : 123 ) hc ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) m ( ( cid : 123 ) hc ) = ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( ( cid : 123 ) hc ) , which in principle can be made to equal zero if ( cid : 123 ) is selected appropriately .
in this case , a super - linear rate is obtained .
generally , however , ( cid : 123 ) ( hc ) = , with smaller values of ( cid : 123 ) ( hc ) corresponding to faster con -
we now turn to an analysis of the em algorithm
we have seen em keeps the parameter vector within d
automatically .
thus , in the new basis the connection
between em and gradient ascent ( cf .
( ) ) becomes
c + et p ( ( cid : 123 )
ton can be regarded as gradient ascent on a new func -
and we have
tion with an e ( cid : 123 ) ective hessian that is the identity matrix : hef f = h ( cid : 123 ) h = i .
in practice , however , ( cid : 123 ) ( h ) is usually quite large .
the larger ( cid : 123 ) ( h ) is , the more di ( cid : 123 ) cult it is to compute h ( cid : 123 ) accurately .
hence it is di ( cid : 123 ) cult to balance the hessian as desired .
in addition , as we mentioned
in the previous section , the hessian varies from point
to point in the parameter space , and at each iteration
we need recompute the inverse hessian .
quasi - newton methods approximate h ( ( cid : 123 )
that is easy to compute .
by a positive matrix
the discussion thus far has treated unconstrained op -
in order to compare gradient ascent with
the em algorithm on the constrained mixture estima -
tion problem , we consider a gradient projection method :
where ( cid : 123 ) k is the projection matrix that projects the gra -
into d .
this gradient projection iteration will remain in d as long as the initial parameter vector is in d .
to keep the iteration within d , we choose an ( ) d and keep ( cid : 123 ) su ( cid : 123 ) ciently small at each suppose that e = ( e; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; em ) are a set of indepen - dent unit basis vectors that span the space d .
in this
@ ( cid : 123 ) ( k ) become ( cid : 123 )
c = et
( k ) ( cid : 123 ) ( cid : 123 )
= et @l
@ ( cid : 123 ) ( k ) , respectively , with k ( cid : 123 )
( cid : 123 ) k .
in this representation the projective gradi -
c ( cid : 123 ) ( cid : 123 )
ent algorithm eq .
( ) becomes simple gradient ascent :
a result , the convergence rate is bounded by
( i + ( cid : 123 ) h ( ( cid : 123 )
( k ) ( cid : 123 ) ( cid : 123 )
( i + ( cid : 123 ) h ( ( cid : 123 )
( i + ( cid : 123 ) h ( ( cid : 123 )
) ) ( i + ( cid : 123 ) h ( ( cid : 123 )
( i + ( cid : 123 ) h ( ( cid : 123 )
) + ( cid : 123 ) h ( ( cid : 123 )
c + ( cid : 123 ) @l ( cid : 123 ) k ( cid : 123 ) ket
( k+ ) ( cid : 123 ) ( cid : 123 ) rc = ket
( cid : 123 ) q ( cid : 123 ) m ( et = q ( cid : 123 ) m ( et
( k+ ) ( cid : 123 ) ( cid : 123 )
( cid : 123 ) k ( cid : 123 ) ket
( i + p h ( ( cid : 123 )
( k ) ( cid : 123 ) ( cid : 123 )
( i + p h ( ( cid : 123 )
( i + p h ( ( cid : 123 )
) ) ( i + p h ( ( cid : 123 )
rc = ket
( cid : 123 ) q ( cid : 123 ) m ( et rc ( cid : 123 ) q + ( cid : 123 )
the latter equation can be further manipulated to yield :
m ( et p he ) ( cid : 123 ) ( cid : 123 ) m ( ( cid : 123 ) et p he ) :
thus we see that the convergence speed of em de - pends on ( cid : 123 ) ( et p he ) = ( cid : 123 ) m ( et p he ) = ( cid : 123 ) m ( et p he ) .
when ( cid : 123 ) ( et p he ) = , ( cid : 123 ) m ( et p he ) = , we m ( et p he ) ( cid : 123 ) ( cid : 123 ) m ( ( cid : 123 ) et p he ) = ( ( cid : 123 )
have p + ( cid : 123 ) ( cid : 123 ) m ( ( cid : 123 ) et p he ) ) = .
in this case , a superlinear rate
is obtained .
we discuss the possibility of obtaining su -
perlinear convergence with em in more detail below .
these results show that the convergence of gradient
ascent and em both depend on the shape of the log likeli - hood as measured by the condition number .
when ( cid : 123 ) ( h ) is near one , the con ( cid : 123 ) guration is quite regular , and the
update direction points directly to the solution yielding fast convergence .
when ( cid : 123 ) ( h ) is very large , the l sur - face has an elongated shape , and the search along the
update direction is a zigzag path , making convergence
very slow .
the key idea of newton and quasi - newton
methods is to reshape the surface .
the nearer it is to a
ball shape ( newtons method achieves this shape in the
methods aim to achieve an e ( cid : 123 ) ective hessian whose con -
dition number is as close as possible to one .
ingly , the results that we now present suggest that the projection matrix p for the em algorithm also serves to e ( cid : 123 ) ectively reshape the likelihood yielding an e ( cid : 123 ) ective
condition number that tends to one .
we ( cid : 123 ) rst present
empirical results that support this suggestion and then
present a theoretical analysis .
moreover , eq .
( ) becomes
ideal case ) , the better the convergence .
quasi - newton
solid - the original hessian dash - dot - .
the constrained hessian dashed - - the em - equivalent hessian
the learning steps
solid - the original hessian
dash - dot - .
the constrained hessian
dashed - - the em - equivalent hessian
the original hessian
the constrained hessian
the em - equivalent hessian
the learning steps
the learning steps
solid - the original hessian
the constrained hessian
the em - equivalent hessian
the learning steps
figure : experimental results for the estimation of the
parameters of a two - component gaussian mixture
the condition numbers as a function of the iteration
number .
( b ) a zoomed version of ( a ) after discarding
the ( cid : 123 ) rst iterations .
the terminology original , con -
strained , and em - equivalent hessians refers to the ma - trices h; et he , and et p he respectively .
we sampled points from a simple ( cid : 123 ) nite mixture
model given by
p ( x ) = ( cid : 123 ) p ( x ) + ( cid : 123 ) p ( x )
( x ( cid : 123 ) mi )
the parameter values were as
we ran both the em algorithm and gradient ascent
: ; ( cid : 123 ) = : ; m = ( cid : 123 ) ; m = ; ( cid : 123 )
= ; ( cid : 123 )
on the data .
at each step of the simulation , we calcu - lated the condition number of the hessian ( ( cid : 123 ) ( h ( ( cid : 123 ) the condition number determining the rate of conver - gence of the gradient algorithm ( ( cid : 123 ) ( et h ( ( cid : 123 ) the condition number determining the rate of conver - gence of em ( ( cid : 123 ) ( et p ( ( cid : 123 ) ) e ) ) .
we also calcu - lated the largest eigenvalues of the matrices h ( ( cid : 123 ) et h ( ( cid : 123 ) ) e .
the results are shown in fig .
as can be seen in fig .
( a ) , the con -
) e , and et p ( ( cid : 123 )
dition numbers change rapidly in the vicinity of the th
iteration and the corresponding hessian matrices be -
come inde ( cid : 123 ) nite .
afterward , the hessians quickly become
de ( cid : 123 ) nite and the condition numbers converge .
) ) = : , ( cid : 123 ) ( et h ( ( cid : 123 )
in fig .
( b ) , the condition numbers converge toward the values ( cid : 123 ) ( h ( ( cid : 123 ) ) e ) = : , and ( cid : 123 ) ( et p ( ( cid : 123 ) ) e ) = : .
that is , the matrix p has greatly reduced the condition number , by factors of and .
this signi ( cid : 123 ) cantly improves the shape of l and speeds up the convergence .
we ran a second experiment in which the means of the
component gaussians were m = ( cid : 123 ) and m =
results are similar to those shown in fig .
since the
distance between two distributions is reduced into half ,
interestingly , the em algorithm converges soon afterward as well , showing that for this problem em spends little time in the region of parameter space in which a local analysis is
figure : experimental results for the estimation of the
parameters of a two - component gaussian mixture ( cf .
the separation of the gaussians is half the
separation in fig
the original hessian
the constrained hessian
the em - equivalent hessian
the learning steps
the original hessian
the constrained hessian
the em - equivalent hessian
the learning steps
figure : the largest eigenvalues of the matrices h; et he , and et p he plotted as a function of the number of iterations .
the plot in ( a ) is for the experi -
ment in fig .
; ( b ) is for the experiment reported in fig .
the shape of l becomes more irregular .
the condition number ( cid : 123 ) ( h ( ( cid : 123 ) creases to , and ( cid : 123 ) ( et p ( ( cid : 123 ) ) e ) increases to .
we see once again a signi ( cid : 123 ) cant improvement in the
) ) increases to , ( cid : 123 ) ( et h ( ( cid : 123 )
case of em , by factors of and .
shows that the matrix p has also reduced the largest eigenvalues of the hessian from between to
to around .
this demonstrates clearly the sta -
ble convergence that is obtained via em , without a line
search or the need for external selection of a learning
in the remainder of the paper we provide some theo -
retical analyses that attempt to shed some light on these
empirical results .
to illustrate the issues involved , con -
sider a degenerate mixture problem in which the mixture has a single component .
( in this case ( cid : 123 ) = . ) let us fur - thermore assume that the covariance matrix is ( cid : 123 ) xed ( i . e . , only the mean vector m is to be estimated ) .
the hes - sian with respect to the mean m is h = ( cid : 123 ) n ( cid : 123 ) em projection matrix p is ( cid : 123 ) =n .
for gradient ascent , we have ( cid : 123 ) ( et he ) = ( cid : 123 ) ( ( cid : 123 ) ) , which is larger than one when - ever ( cid : 123 ) = ci .
em , on the other hand , achieves a condi - tion number of one exactly ( ( cid : 123 ) ( et p he ) = ( cid : 123 ) ( p h ) = ( cid : 123 ) ( i ) = and ( cid : 123 ) m ( et p he ) = ) .
thus , em and new -
tons method are the same for this simple quadratic
that the theorem can be extended to apply more widely ,
problem .
for general non - quadratic optimization prob -
in particular to the case of the full em update in which
lems , newton retains the quadratic assumption , yield -
the mixing proportions and covariances are estimated ,
ing fast convergence but possible divergence
and also , within limits , to cases in which the means are
a more conservative algorithm that retains the conver -
not well separated .
to obtain an initial indication as to
gence guarantee but also maintains quasi - newton be -
possible conditions that can be usefully imposed on the
havior .
we now analyze this behavior in more detail .
separation of the mixture components , we have stud -
we consider the special case of estimating the means in
a gaussian mixture when the gaussians are well sepa -
theorem consider the em algorithm in eq .
where the parameters ( cid : 123 ) j and ( cid : 123 ) j are assumed to be known .
assume that the k gaussian distributions are well separated , such that for su ( cid : 123 ) ciently large k the pos - terior probabilities h ( k ) ( t ) are nearly zero or one .
for such k , the condition number associated with em is al - ways smaller than the condition number associated with gradient ascent .
that is :
( cid : 123 ) ( et p ( ( cid : 123 )
) e ) < ( cid : 123 ) ( et h ( ( cid : 123 )
furthermore , ( cid : 123 ) m ( et p ( ( cid : 123 ) as k goes to in ( cid : 123 ) nity .
) e ) approaches one
( ) is ied the case in which the second term in eq .
neglected only for hii and is retained for the hij com - ponents , where j = i .
consider , for example , the case
of a univariate mixture having two mixture components .
for ( cid : 123 ) xed mixing proportions and ( cid : 123 ) xed covariances , the
hessian matrix ( eq .
) becomes :
h = ( cid : 123 ) h h h h ( cid : 123 ) ;
and the projection matrix ( eq .
) becomes :
p = ( cid : 123 ) ( cid : 123 ) h ( cid : 123 )
hii = ( cid : 123 )
( t ) ; i = ;
j ( t ) ( x ( t ) ( cid : 123 ) mj )
for i = j = ; .
if h is negative de ( cid : 123 ) nite , ( i . e . , hh ( cid : 123 ) hh < ) , then we can show that the conclusions of theorem remain true , even for gaussians that are not
necessarily well - separated .
the proof is achieved via the
lemma consider the positive de ( cid : 123 ) nite matrix
the hessian is
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) hk ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) hk
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) hkk
j ( t ) + ( ( cid : 123 )
) ( x ( t ) ( cid : 123 ) mj ) ( x ( t ) ( cid : 123 ) mi )
) = ( ( cid : 123 ) ij ( cid : 123 ) h ( k )
with ( cid : 123 ) ij ( x ( t ) matrix p is
the projection
= diag ( p ( k )
; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; p ( k )
( t ) ( ( cid : 123 ) h ( k )
given that h ( k ) j ( t ) ) is negligible for su ( cid : 123 ) - ciently large k , the second term in eq .
( ) can be neglected , yielding hii = ( cid : 123 ) ( ( cid : 123 ) h = diag ( h; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; hkk ) .
this implies that p h = ( cid : 123 ) i , and thus ( cid : 123 ) ( p h ) = , whereas ( cid : 123 ) ( h ) = .
this theorem , although restrictive in its assumptions ,
gives some indication as to why the projection matrix
in the em algorithm appears to condition the hessian ,
yielding improved convergence .
in fact , we conjecture
( cid : 123 ) = ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
for the diagonal matrix b = diag ( ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( b ( cid : 123 ) ) < ( cid : 123 ) ( ( cid : 123 ) ) .
) , we have
the eigenvalues of ( cid : 123 ) are the roots of ( ( cid : 123 ) ( cid : 123 )
( cid : 123 ) ) ( ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) = , which gives
( cid : 123 ) + ( cid : 123 ) + ( cid : 123 )
( cid : 123 ) + ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
( cid : 123 ) = p ( ( cid : 123 ) + ( cid : 123 ) ) ( cid : 123 ) ( ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) )
( cid : 123 ) + ( cid : 123 ) + ( cid : 123 ) ( cid : 123 ) + ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
the condition number ( cid : 123 ) ( ( cid : 123 ) ) can be written as ( cid : 123 ) ( ( cid : 123 ) ) = ( + s ) = ( ( cid : 123 ) s ) ( cid : 123 ) f ( s ) , where s is de ( cid : 123 ) ned as follows :
s =s ( cid : 123 )
( ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) )
( ( cid : 123 ) + ( cid : 123 ) )
furthermore , the eigenvalues of b ( cid : 123 ) are the roots of ( ( cid : 123 ) ( cid : 123 ) ) ( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 ) ( ( cid : 123 ) ( cid : 123 ) ) = ( ( cid : 123 ) ( cid : 123 ) ) = , which gives ( cid : 123 ) m = + p ( ( cid : 123 ) ( cid : 123 ) ) = ( ( cid : 123 ) ( cid : 123 ) ) and ( cid : 123 ) m = p ( ( cid : 123 ) ( cid : 123 ) ) = ( ( cid : 123 ) ( cid : 123 ) ) , we have ( cid : 123 ) ( b ( cid : 123 ) ) = ( + r ) = ( ( cid : 123 ) r ) =
thus , de ( cid : 123 ) ning r =
we now examine the quotient s=r :
( ( cid : 123 ) r )
( ( cid : 123 ) + ( cid : 123 ) ) = ( ( cid : 123 ) ( cid : 123 ) )
given that ( ( cid : 123 ) + ( cid : 123 ) )
= ( ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 ) , we have
rp ( cid : 123 ) ( ( cid : 123 ) r ) = .
that is , s > r .
since f ( x ) = ( + x ) = ( ( cid : 123 ) x ) is a monotonically increasing function for x > , we have f ( s ) > f ( r ) .
therefore , ( cid : 123 ) ( b ( cid : 123 ) ) < ( cid : 123 ) ( ( cid : 123 ) ) .
hessian of the em algorithm tends toward one , showing
that em can approximate a superlinear method .
finally ,
in cases of a poorly conditioned hessian , superlinear con -
vergence is not necessarily a virtue .
in such cases many
optimization schemes , including em , essentially revert
to gradient ascent .
we feel that em will continue to play an important
role in the development of learning systems that empha -
size the predictive aspect of data modeling .
em has in -
deed played a critical role in the development of hidden
markov models ( hmms ) , an important example of pre -
dictive data modeling .
em generally converges rapidly
in this setting .
similarly , in the case of hierarchical mix -
tures of experts the empirical results on convergence in
likelihood have been quite promising ( jordan & jacobs ,
; waterhouse & robinson , ) .
finally , em can
play an important conceptual role as an organizing prin -
ciple in the design of learning algorithms .
its role in this
we think that it should be possible to generalize
case is to focus attention on the \missing variables " in
this lemma beyond the univariate , two - component case ,
the problem .
this clari ( cid : 123 ) es the structure of the algorithm
thereby weakening the conditions on separability in the -
and invites comparisons with statistical physics , where
orem in a more general setting .
missing variables often provide a powerful analytic tool .
