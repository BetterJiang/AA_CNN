we present a framework for learning in hidden markov models with distributed state representations .
within this framework , we derive a learning algorithm based on the expectation ( maximization ( em ) procedure for maximum likelihood estimation .
anal - ogous to the standard baum - welch update rules , the m - step of our algorithm is exact and can be solved analytically .
however , due to the combinatorial nature of the hidden state representation , the exact e - step is intractable .
a simple and tractable mean ( cid : 123 ) eld approximation is derived .
empirical results on a set of problems suggest that both the mean ( cid : 123 ) eld approximation and gibbs sampling are viable alternatives to the computa - tionally expensive exact algorithm .
copyright c ( cid : 123 ) massachusetts institute of technology ,
this report describes research done at the center for biological and computational learning and the arti ( cid : 123 ) cial intelligence laboratory of the massachusetts institute of technology .
support for the center is provided in part by a grant from the national science foundation under contract asc ( .
this project was supported in part by a grant from the mcdonnell - pew foundation , by a grant from atr human information processing research laboratories , by a grant from siemens corporation , and by grant n - - - from the o ( cid : 123 ) ce of naval research .
a problem of fundamental interest to machine learning is time series modeling .
due to the sim - plicity and e ( cid : 123 ) ciency of its parameter estimation algorithm , the hidden markov model ( hmm ) has emerged as one of the basic statistical tools for modeling discrete time series , ( cid : 123 ) nding widespread application in the areas of speech recognition ( rabiner and juang , ) and computational molec - ular biology ( baldi et al . , ) .
an hmm is essentially a mixture model , encoding information about the history of a time series in the value of a single multinomial variable ( the hidden state ) .
this multinomial assumption allows an e ( cid : 123 ) cient parameter estimation algorithm to be derived ( the baum - welch algorithm ) .
however , it also severely limits the representational capacity of hmms .
for example , to represent bits of information about the history of a time sequence , an hmm would need distinct states .
on the other hand an hmm with a distributed state representa - tion could achieve the same task with binary units ( williams and hinton , ) .
this paper addresses the problem of deriving e ( cid : 123 ) cient learning algorithms for hidden markov models with distributed state representations .
the need for distributed state representations in hmms can be motivated in two ways .
first , such representations allow the state space to be decomposed into features that naturally decouple the dynamics of a single process generating the time series .
second , distributed state representations simplify the task of modeling time series generated by the interaction of multiple independent processes .
for example , a speech signal generated by the superposition of multiple simultaneous speakers can be potentially modeled with such an architecture .
williams and hinton ( ) ( cid : 123 ) rst formulated the problem of learning in hmms with distributed state representation and proposed a solution based on deterministic boltzmann learning .
the ap - proach presented in this paper is similar to williams and hintons in that it is also based on a statistical mechanical formulation of hidden markov models .
however , our learning algorithm is quite di ( cid : 123 ) erent in that it makes use of the special structure of hmms with distributed state rep - resentation , resulting in a more e ( cid : 123 ) cient learning procedure .
anticipating the results in section , this learning algorithm both obviates the need for the two - phase procedure of boltzmann machines , and has an exact m - step .
a di ( cid : 123 ) erent approach comes from saul and jordan ( ) , who derived a set of rules for computing the gradients required for learning in hmms with distributed state spaces .
however , their methods can only be applied to a limited class of architectures .
factorial hidden markov models
hidden markov models are a generalization of mixture models .
at any time step , the probability density over the observables de ( cid : 123 ) ned by an hmm is a mixture of the densities de ( cid : 123 ) ned by each state in the underlying markov model .
temporal dependencies are introduced by specifying that the prior probability of the state at time t depends on the state at time t ( cid : 123 ) through a transition matrix , p ( figure a ) .
another generalization of mixture models , the cooperative vector quantizer ( cvq; hinton and zemel , ) , provides a natural formalism for distributed state representations in hmms .
whereas in simple mixture models each data point must be accounted for by a single mixture component , in cvqs each data point is accounted for by the combination of contributions from many mixture components , one from each separate vector quantizer .
the total probability density modeled by a cvq is also a mixture model; however this mixture density is assumed to factorize into a product of densities , each density associated with one of the vector quantizers .
thus , the cvq is a mixture
model with distributed representations for the mixture components .
factorial hidden markov models combine the state transition structure of hmms with the dis - tributed representations of cvqs ( figure b ) .
each of the d underlying markov models has a discrete state s i at time t and transition probability matrix pi .
as in the cvq , the states are mu - tually exclusive within each vector quantizer and we assume real - valued outputs .
the sequence of observable output vectors is generated from a normal distribution with mean given by the weighted combination of the states of the underlying markov models :
( cid : 123 ) n d
i; c ! ;
where c is a common covariance matrix .
the k - valued states si are represented as discrete column vectors with a in one position and everywhere else; the mean of the observable is therefore a combination of columns from each of the wi matrices .
figure .
a ) hidden markov model .
b ) factorial hidden markov model .
we capture the above probability model by de ( cid : 123 ) ning the energy of a sequence of t states and
t= , which we abbreviate to fs; yg , as :
h ( fs; yg ) =
il ) such that pk
where ( ai ) jl = log p ( st for the initial state , s probability model is de ( cid : 123 ) ned from this energy by the boltzmann distribution
, are introduced by setting the second term in ( ) to ( cid : 123 ) pd
j= e ( ai ) jl = , and denotes matrix transpose .
priors
i log ( cid : 123 ) i
p ( fs; yg ) =
we refer to hmms with distributed state as factorial hmms as the features of the distributed state factorize the
total state representation .
note that like in the cvq ( ghahramani , ) , the unclamped partition function
z = z dfygxfsg
evaluates to a constant , independent of the parameters .
this can be shown by ( cid : 123 ) rst integrating the gaussian variables , removing all dependency on fyg , and then summing over the states using the constraint on e ( ai ) jl .
the em algorithm for factorial hmms
as in hmms , the parameters of a factorial hmm can be estimated via the em ( baum - welch ) algorithm .
this procedure iterates between assuming the current parameters to compute proba - bilities over the hidden states ( e - step ) , and using these probabilities to maximize the expected log likelihood of the parameters ( m - step ) .
using the likelihood ( ) , the expected log likelihood of the parameters is
j ( cid : 123 ) ) = h ( cid : 123 ) h ( fs; yg ) ( cid : 123 ) log z ic;
where ( cid : 123 ) = fwi; pi; c g i= denotes the current parameters , and h ( cid : 123 ) ic denotes expectation given the clamped observation sequence and ( cid : 123 ) .
given the observation sequence , the only random variables are the hidden states .
expanding equation ( ) and limiting the expectation to these random variables we ( cid : 123 ) nd that the statistics that need to be computed for the e - step are hs i ic , hs note that in standard hmm notation ( rabiner and juang , ) , hs i ic corresponds to ( cid : 123 ) t and j ic has no analogue when there is only a single underlying markov model .
the m - step uses these expectations to maximize q with respect to the parameters .
the constant partition function allowed us to drop the second term in ( ) .
therefore , unlike the boltzmann machine , the expected log likelihood does not depend on statistics collected in an unclamped phase of learning , resulting in much faster learning than the traditional boltzmann machine ( neal , ) .
ic corresponds to ( cid : 123 ) t , whereas hs
j ic , and hs
setting the derivatives of q with respect to the output weights to zero , we obtain a linear system of equations for w :
w new =
summation over a data set of n sequences , and y is the moore - penrose pseudo - inverse .
to estimate
where s and w are the vector and matrix of concatenated si and wi , respectively , pn denotes the log transition probabilities we solve @q=@ ( ai ) jl = subject to the constraint pj e ( ai ) jl = ,
the covariance matrix can be similarly estimated :
il ic ! :
jl = log pn;thst
c new = xn;t
the m - step equations can therefore be solved analytically; furthermore , for a single underlying markov chain , they reduce to the traditional baum - welch re - estimation equations .
unfortunately , as in the simpler cvq , the exact e - step for factorial hmms is computationally intractable .
for example , the expectation of the j th unit in vector i at time step t , given fyg , is :
ij ic = p ( st
ij = jfyg; ( cid : 123 ) )
j= ; : : : ;st
ij = ; : : : ; st
d;jd= jfyg; ( cid : 123 ) )
although the markov property can be used to obtain a forward - backward ( like factorization of this expectation across time steps , the sum over all possible con ( cid : 123 ) gurations of the other hidden units within each time step is unavoidable .
for a data set of n sequences of length t , the full e - step calculated through the forward - backward procedure has time complexity o ( n t kd ) .
although more careful bookkeeping can reduce the complexity to o ( n t dkd+ ) , the exponential time cannot be avoided .
this intractability of the exact e - step is due inherently to the cooperative nature of the model|the setting of one vector only determines the mean of the observable if all the other vectors are ( cid : 123 ) xed .
rather than summing over all possible hidden state patterns to compute the exact expectations , a natural approach is to approximate them through a monte carlo method such as gibbs sampling .
the procedure starts with a clamped observable sequence fyg and a random setting of the hidden j g .
at each time step , each state vector is updated stochastically according to its probability distribution conditioned on the setting of all the other state vectors : s : j = i ( cid : 123 ) p ( s i jfyg; fs i or ( cid : 123 ) = tg; ( cid : 123 ) ) : these conditional distributions are straightforward to compute and a full pass of gibbs sampling requires o ( n t kd ) operations .
the ( cid : 123 ) rst and second - order statistics needed to estimate hs ijs visited and the probabilities estimated during this sampling process .
ic are collected using the st
j ic and hs
i ic , hs
mean ( cid : 123 ) eld approximation
a di ( cid : 123 ) erent approach to computing the expectations in an intractable system is given by mean ( cid : 123 ) eld theory .
a mean ( cid : 123 ) eld approximation for factorial hmms can be obtained by de ( cid : 123 ) ning the energy
h ( fs; yg ) =
i log m
which results in a completely factorized approximation to probability density ( ) :
~ p ( fs; yg ) / yt
in this approximation , the observables are independently gaussian distributed with mean ( cid : 123 ) each hidden state vector is multinomially distributed with mean m i .
this approximation is made as tight as possible by chosing the mean ( cid : 123 ) eld parameters ( cid : 123 ) i that minimize the kullback - liebler
t and m
kl ( ~ p kp ) ( cid : 123 ) hlog p i ~ p ( cid : 123 ) hlog ~ p i ~ p
where h ( cid : 123 ) i ~ p denotes expectation over the mean ( cid : 123 ) eld distribution ( ) .
with the observables clamped , t .
minimizing kl ( ~ p kp ) with respect to the mean ( cid : 123 ) eld
t can be set equal to the observable y
parameters for the states results in a ( cid : 123 ) xed - point equation which can be iterated until convergence :
i c ( cid : 123 ) hy
ti + w
i c ( cid : 123 ) wim
i c ( cid : 123 ) wig ( cid : 123 )
( cid : 123 ) pi wim
i and ( cid : 123 ) f ( cid : 123 ) g is the softmax exponential , normalized over each hidden state vector .
the ( cid : 123 ) rst term is the projection of the error in the observable onto the weights of state vector i|the more a hidden unit can reduce this error , the larger its mean ( cid : 123 ) eld parameter .
the next three terms arise from the fact that hs ij .
the last two terms introduce dependencies forward and backward in time .
each state vector is asynchronously updated using ( ) , at a time cost of o ( n t kd ) per iteration .
convergence is diagnosed by monitoring the kl divergence in the mean ( cid : 123 ) eld distribution between successive time steps; in practice convergence is very rapid ( about to iterations of ( ) ) .
ij i ~ p is equal to mij and not m
we compared three em algorithms for learning in factorial hmms|using gibbs sampling , mean ( cid : 123 ) eld approximation , and the exact ( exponential ) e step|on the basis of performance and speed on randomly generated problems .
problems were generated from a factorial hmm structure , the parameters of which were sampled from a uniform ( ; ) distribution , and appropriately normalized to satisfy the sum - to - one constraints of the transition matrices and priors .
also included in the comparison was a traditional hmm with as many states ( kd ) as the factorial hmm .
table summarizes the results .
even for moderately large state spaces ( d ( cid : 123 ) and k ( cid : 123 ) ) the standard hmm with kd states su ( cid : 123 ) ers from severe over ( cid : 123 ) tting .
furthermore , both the standard hmm and the exact e - step factorial hmm are extremely slow on the larger problems .
the gibbs sampling and mean ( cid : 123 ) eld approximations o ( cid : 123 ) er roughly comparable performance at a great increase
the basic contribution of this paper is a learning algorithm for hidden markov models with dis - tributed state representations .
the standard baum - welch procedure is intractable for such archi - tectures as the size of the state space generated from the cross product of d k - valued features is o ( kd ) , and the time complexity of baum - welch is quadratic in this size .
more importantly , unless special constraints are applied to this cross - product hmm architecture , the number of parameters also grows as o ( kd ) , which can result in severe over ( cid : 123 ) tting .
the architecture for factorial hmms presented in this paper did not include any coupling between the underlying markov chains .
it is possible to extend the algorithm presented to architectures which incorporate such couplings .
however , these couplings must be introduced with caution as they may result either in an exponential growth in parameters or in a loss of the constant partition function
the learning algorithm derived in this paper assumed real - valued observables .
the algorithm can also be derived for hmms with discrete observables , an architecture closely related to sigmoid belief networks ( neal , ) .
however , the nonlinearities induced by discrete observables make both the e - step and m - step of the algorithm more di ( cid : 123 ) cult .
table : comparison of factorial hmm on four problems of varying size
table .
data was generated from a factorial hmm with d underlying markov models of k states each .
the training set was sequences of length where the observable was a - dimensional vector; the test set was such sequences .
hmm indicates a hidden markov model with kd states; the other algorithms are factorial hmms with d underlying k - state models .
gibbs sampling used samples of each state .
the algorithms were run until convergence , as monitored by relative change in the likelihood , or a maximum of cycles .
the # column indicates number of runs .
the train and test columns show the log likelihood ( cid : 123 ) one standard deviation on the two data sets .
the last column indicates approximate time per cycle on a silicon graphics r processor running matlab .
in conclusion , we have presented gibbs sampling and mean ( cid : 123 ) eld learning algorithms for factorial hidden markov models .
such models incorporate the time series modeling capabilities of hidden markov models and the advantages of distributed representations for the state space .
future work will concentrate on a more e ( cid : 123 ) cient mean ( cid : 123 ) eld approximation in which the forward - backward algo - rithm is used to compute the e - step exactly within each markov chain , and mean ( cid : 123 ) eld theory is used to handle interactions between chains ( saul and jordan , ) .
