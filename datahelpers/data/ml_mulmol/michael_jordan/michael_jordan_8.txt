123 abstract ( maximw 123wb word )
w e present a tree - structured architecture for supervised learning .
ile statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture comnponents are generalized linear models ( guim ' s ) .
learning is treated as a maximum liklihood problem; in particular , we present an expectation - maximization ( em algorithm for adjusting the parameters of the architecture .
we also develop an on - line learning algorithm in which the parameters are updated incrementally .
comparative simulation results are presented in the robot dy namics domain .
123 subject terms
123number of pages
price code
123 security classification
security classification
of this page
security classification
uimitatkon of
dtic quality inspec ' ted 123
massachusetts institute of technology
artificial intelligence laboratory
center for biological and computational learning
department of brain and cognitive sciences
memo no .
123 c . b . c . l .
memo no
august 123 , 123
hierarchical mixtures of experts and the em
michael i .
jordan and robert a .
jacobs
we present a tree - structured architecture for supervised learning .
the statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture com - ponents are generalized linear models ( glim ' s ) .
learning is treated as a maximum likelihood problem; in particular , we present an expectation - maximization ( em ) algorithm for adjusting the parameters of the architecture .
we also develop an on - line learning algorithm in which the parameters are updated incrementally .
comparative simulation results are presented in the robot dynamics domain .
copyright @ massachusetts institute of technology , 123
this report describes research done at the dept .
of brain and cognitive sciences , the center for biological and computational learning , and the artificial intelligence laboratory of the massachusetts institute of technology .
support for cbcl is provided in part by a grant from the nsf ( asc - 123 ) .
support for the laboratory ' s artificial intelligence research is provided in part by the advanced research projects agency of the dept .
of defense .
the authors were supported by a grant from the mcdonnell - pew foundation , by a grant from atr human information processing research laboratories , by a grant from siemens corporation , by by grant iri - 123 from the national science foundation , by grant n123 - 123 - j - 123 from the office of naval research , and by nsf grant ecs - 123 to support an initiative in intelligent control at mit .
michael 123
jordan is a nsf presidential young investigator .
the principle of divide - and - conquer is a principle with wide applicability throughout applied mathematics .
divide - and - conquer algorithms attack a complex problem by dividing it into simpler problems whose solutions can be combined to yield a solution to the complex problem .
this approach can often lead to simple .
elegant and efficient algorithms .
in this paper we explore a particular application of the divide - and - conquer principle to the problem of learning from examples .
we describe a network architecture and a learning algorithm for the architecture .
both of which are inspired by the philosophy of divide - and - conquer .
in the statistical literature and in the machine learning literature , divide - and - conquer ap - proaches have become increasingly popular .
the cart algorithm of breiman , friedman .
123 - shen , and stone ( 123 ) , the mars algorithm of friedman ( 123 ) , and the id123 algorithm of quinlan ( 123 ) are well - known examples .
these algorithms fit surfaces to data by explicitly dividing the input space into a nested sequence of regions , and by fitting simple surfaces ( e . g . , constant functions ) within these regions .
they have convergence times that are often orders of magnitude faster than gradient - based neural network algorithms .
although divide - and - conquer algorithms have much to recommend them , one should be concerned about the statistical consequences of dividing the input space .
dividing the data can have favorable consequences for the bias of an estimator , but it generally increases the variance .
consider linear regression , for example , in which the variance of the estimates of the slope and intercept depend quadratically on the spread of data on the x - axis .
the points that are the most peripheral in the input space are those that have the maximal effect in decreasing the variance of the parameter estimates .
the foregoing considerations suggest that divide - and - conquer algorithms generally tend to be variance - increasing algorithms .
this is indeed the case and is particularly problematic in high - dimensional spaces where data become exceedingly sparse ( scott , 123 ) .
one response to this dilemma - that adopted by cart , mars , and id123 , and also adopted here - is to utilize piecewise constant or piecewise linear functions .
these functions minimize variance at a cost of increased bias .
we also make use of a second variance - decreasing device; a device familiar in the neural network literature .
we make use of " soft " splits of data ( bridle , 123; nowlan .
123 : wahba , gu , wang , k chappell , 123 ) , allowing data to lie simultaneously in multiple regions .
this approach allows the parameters in one region to be influenced by data in neighboring regions .
cart , mars , and id123 rely on " hard " splits , which , as we remarked above , have particularly severe effects on variance .
by allowing soft splits the severe effects of lopping off distant data can be ameliorated .
we also attempt to minimize the bias that is incurred by using piecewise linear functions , by allowing the splits to be formed along hyperplanes at arbitrary orientations in the input space .
this lessens the bias due to high - order interactions among the inputs and allows the algorithm to be insensitive to the particular choice of coordinates used to encode the data ( an improvement over methods such as mars and id123 , which are
the work that we describe here makes contact with a .
number of branches of statistical theory .
first , as in our earlier work ( jacobs , jordan , nowlan , & , hinton , 123 ) , we formulate the learning problem as a mixture estimation problem ( cf .
cheeseman , et al , 123; duda &k : hart .
123; nowlan .
123; redner & walker , 123; titterington , smith , & makov , 123 ) .
we show that the algorithm that is generally employed for the unsupervised learning of mixture ( em ) algorithm of dempster , laird and rubin ( 123 ) - can also be exploited for supervised learning .
second .
we utilize generalized linear model ( glim ) theory ( mccullagh & nelder , 123 ) to provide the basic statistical structure oea
for the components of the architecture .
in particular .
the " soft splits " referred to above are modeled as multinomial logit models - a specific form of glim .
we also show that the algorithm developed for fitting glim ' s - the iteratively reweighted least squares ( irls ) algorithm - can be usefully employed in our model , in particular as the m step of the em algorithm .
finally .
we show that these ideas can be developed in a recursive manner , yielding a tree - structured approach to estimation that is reminiscent of cart , mars .
and id123
the remainder of the paper proceeds as follows .
we first introduce the hierarchical mixture - of - experts architecture and present the likelihood function for the architecture .
after describing a gradient descent algorithm , we develop a more powerful learning algorithm for the architecture that is a special case of the general expectation - maximization ( em ) framework of dempster .
laird and rubin ( 123 ) .
we also describe a least - squares version of this algorithm that leads to a particularly efficient implementation .
both of the latter algorithms are batch learning algorithms .
in the final section , we present an on - line version of the least - squares algorithm that in practice appears to be the most efficient of the algorithms that we have studied .
hierarchical mixtures of experts
the algorithms that we discuss in this paper are supervised learning algorithms .
we explicitly address the case of regression , in which the input vectors are elements of r ' and the output vectors are elements of rn .
we also consider classification models and counting models in which the outputs are integer - valued .
the data are assumed to form a countable set of paired observations a ' = ( ( x ( t ) , y ( t ) ) ) .
in the case of the batch algorithms discussed below , this set is assumed to be finite; in the case of the on - line algorithms , the set may be infinite .
we propose to solve nonlinear supervised learning problems by dividing the input space into a nested set of regions and fitting simple surfaces to the data that fall in these regions .
the regions have " soft " boundaries , meaning that data points may lie simultaneously in multiple regions .
the boundaries between regions are themselves simple parameterized surfaces that are adjusted by the learning algorithm .
the hierarchical mixture - of - experts ( hme ) architecture is shown in figure 123 the ar - chitecture is a tree in which the gating networks sit at the nonterminals of the tree .
these networks receive the vector x as input and produce scalar outputs that are a partition of unity at each point in the input , space .
the expert networks sit at the leaves of the tree .
each expert produces an output vector lij for each input vector .
these output vectors proceed up the tree , being blended by the gating network outputs .
all of the expert networks in the tree are linear with a single output nonlinearity .
we will refer to such a network as " generalized linear , " borrowing the terminology from statistics ( mccullagh & nelder , 123 ) .
expert network ( i , j ) produces its output / pi , as a generalized linear function of the input x :
where ( tij is a weight matrix and f is a fixed continuous nonlinearity .
the vector x is assumed to include a fixed component of one to allow for an intercept term .
l ( cid : 123 ) ij = f ( uijx ) ,
for regression problems , f ( . ) is generally chosen to be the identity function ( i . e . , the ex - perts are linear ) .
for binary classification problems , f ( . ) is generally taken to be the logistic function , in which case the expert outputs are interpreted as the log odds of " success " under a
' to simplify the presentation , we restrict ourselves to a two - level hierarchy throughout the paper .
all of the algorithms that we describe , however , generalize readily to hierarchies of arbitrary depth .
see jordan and xu ( 123 ) for a recursive formalism that handles arbitrary hierarchies .
figure 123 : a two - level hierarchical mixture of experts .
to form a deeper tree , each expert is expanded recursively into a gating network and a set of sub - experts .
bernoulli probability model ( see below ) .
other models ( e . g . , multiway classification , counting , rate estimation and survival estimation ) are handled by making other choices for f ( - ) .
these models are smoothed piecewise analogs of the corresponding glim models ( cf .
mccullagh &
the gating networks are also generalized linear .
define intermediate variables & , as follows :
where vi is a weight vector .
then the ith output of the top - level gating network is the " softmax " function of the & ( bridle , 123; mccullagh & nelder , 123 ) :
123i = ( cid : 123 ) k
note that the gi are positive and sum to one for each x .
they can be interpreted as providing a " soft " partitioning of the input space .
similarly , the gating networks at lower levels are also generalized linear systems .
define
is the output of the jth unit in the ith gating network at the second level of the architecture .
once again , the gjli are positive and sum to one for each x .
they can be interpreted as providing a nested " soft " partitioning of the input space within the partitioning providing by the higher - level gating network .
the output vector at each nonterminal of the tree is the weighted output of the experts below that nonterminal .
that is , the output at the ith nonterminal in the second layer of the two - level tree is :
and the output at the top level of the tree is :
p = e123 ~ i
note that both the g ' s and the t ' s depend on the input x , thus the total output is a nonlinear function of the input .
given the definitions of the expert networks and the gating networks , the regression surface defined by the hierarchy is a piecewise blend of the regression surfaces defined by the experts .
the gating networks provide a nested , " soft " partitioning of the input space and the expert networks provide local regression surfaces within the partition .
there is overlap between neigh - boring regions .
to understand the nature of the overlap , consider a one - level hierarchy with two expert networks .
in this case , the gating network has two outputs , g , and g123
the gating output g , is given by :
~ ef + e123
which is a logistic ridge function whose orientation is determined by the direction of the vector
the gating output g123 is equal to 123 - gi .
for a given x , the total output i is the convex combination 123 , 123 + 123p123 , this is a weighted average of the experts .
where the weights are determined by the values of the ridge function .
along the ridge , g , = g123 = , and both experts contribute equally .
away from the ridge , one expert or the other dominates .
the amount of smoothing across the ridge is determined by the magnitude of the vector v 123 - v 123 .
if v 123 - v123 is large , then the ridge function becomes a sharp split and the weighted output of the experts becomes piecewise ( generalized ) linear .
if v 123 - vi is small , then each expert contributes to a significant degree on each side of the ridge .
thereby smoothing the piecewise map .
in the limit of a zero difference vector , gi = 123 = for all x , and the total output is the same fixed average of the experts on both sides of the fictitious " split . "
in general , a given gating network induces a smoothed planar partitioning of the input space .
lower - level gating networks induce a partition within the partition induced by higher - level gating networks .
the weights in a given gating network determine the amount of smoothing
across the partition at that particular level of resolution : large weight vectors imply sharp changes in the regression surface across a ridge and small weights imply a smoother surface .
in the limit of zero weights in all gating networks , the entire hierarchy reduces to a fixed average ( a linear system in the case of regression ) .
a probability model
the hierarchy can be given a probabilistic interpretation .
we suppose that the mechanism by which data are generated by the environment involves a nested sequence of decisions that terminates in a regressive process that maps x to y .
the decisions are modeled as multinomial random variables .
that is , for each x , we interpret the values gi ( x , v ) as the multinomial probabilities associated with the first decision and the gjli ( x , v% ) as the ( conditional ) multino - mial probabilities associated with the second decision , where the superscript " 123 " refers to the " true " values of the parameters .
the decisions form a decision tree .
we use a statistical model to model this decision tree; in particular , our choice of parameterization ( cf .
equations 123 , 123 , 123 and 123 ) corresponds to a multinomial legit probability model at each nonterminal of the tree ( see appendix 123 ) .
a multinomial logit model is a special case of a glim that is commonly used for , . soft " multiway classification ( mccullagh & nelder , 123 ) .
under the multinomial logit model , we interpret the gating networks as modeling the input - dependent , multinomial probabilities associated with decisions at particular levels of resolution in a tree - structured model of the
once a particular sequence of decisions has been made , resulting in a choice of regressive process ( ij ) , output y is assumed to be generated according to the following statistical model .
first , a linear predictor nij is formed :
the expected value of y is obtained by passing the linear predictor through the link function
the output y is then chosen from a probability density p , with mean ( cid : 123 ) a and " dispersion " parameter 123
we denote the density of y as :
where the parameter vector 123 includes the weights u and the dispersion parameter 123 :
we assume the density p to be a member of the exponential family of densities ( mccullagh & nelder , 123 ) .
the interpretation of the dispersion parameter depends on the particular choice of density .
for example , in the case of the n - dimensional gaussian , the dispersion parameter is the covariance matrix 123
123 we utilize the neural network convention in defining links .
in glim theory , the convention is that .
the link
function relates r to p; thus , q = h ( p ) , where h is equivalent to our f - 123
' not all exponential family densities have a dispersion parameter; in particular , the bernoulli density discussed
below has no dispersion parameter .
given these assumptions , the total probability of generating y from x is the mixture of the probabilities of generating y from each of the component densities , where the mixing proportions are multinomial probabilities :
p ( ylx , 123 ) = - gi ( x , v ) egjli ( x , v ) p ( yjx , 123 ) ,
note that 123o includes the expert network parameters 123 as well as the gating network parame - ters v123 and v123
note also that we have explicitly indicated the dependence of the probabilities gi and gjli on the input x and on the parameters .
in the remainder of the paper we drop the explicit reference to the input and the parameters to simplify the notation :
p ( ylx , 123 ) = - i
we also utilize equation 123 without the superscripts to refer to the probability model defined by a particular hme architecture , irrespective of any reference to a " true " model .
in the case of regression the probabilistic component of the model is generally assumed to be gaussian .
assuming identical covariance matrices of the form a123i for each of the experts yields the following hierarchical probability model :
p ( y x , 123 ) = ( 123r ) n / 123 n
example ( binary classification )
in binary classification problems the output y is a discrete random variable having possible outcomes of " failure " and " success . " the probabilistic component of the model is generally assumed to be the bernoulli distribution ( cox , 123 ) .
in this case , the mean pii is the condi - tional probability of classifying the input as " success . " the resulting hierarchical probability model is a mixture of bernoulli densities :
p ( ylx , 123 ) = f g , ( cid : 123 ) gjliy ( cid : 123 ) ( 123 -
in developing the learning algorithms to be presented in the remainder of the paper , it will prove useful to define posterior probabilities associated with the nodes of the tree .
the terms " posterior " and " prior " have meaning in this context during the training of the system .
we refer to the probabilities gi and gili as prior probabilities , because they are computed based only on the input x , without knowledge of the corresponding target output y .
a posterior probability is defined once both the input and the target output are known .
using bayes ' rule , we define the posterior probabilities at the nodes of the tree as follows :
hi= gi ej gjlipij ( y )
ei gi ej gjlipij ( )
hji= y . jg123 , p ( cid : 123 ) 123 ( y ) .
we will also find it useful to define the joint posterior probability hij , the product of hi and
j gj123 ipij ( y )
h i gi
this quantity is the probability that expert network ( i , j ) can be considered to have generated the data , based on knowledge of both the input and the output .
once again , we emphasize that all of these quantities are conditional on the input x .
in deeper trees , the posterior probability associated with an expert network is simply the product of the conditional posterior probabilities along the path from the root of the tree to
the likelihood and a gradient descent learning algorithm
jordan and jacobs ( 123 ) presented a gradient descent learning algorithm for the hierarchical architecture .
the algorithm was based on earlier work by jacobs , jordan , nowlan , and hinton ( 123 ) , who treated the problem of learning in mixture - of - experts architectures as a maximum likelihood estimation problem .
the log likelihood of a data set a ' = ( ( x ( t ) , y ( t ) ) ) jn is obtained by taking the log of the product of n densities of the form of equation 123 , which yields the following log likelihood :
123 ( 123; a ' ) =
i n ' 123t ) e
let us assume that the probability density p is gaussian with an identity covariance matrix and that the link function is the identity .
in this case , by differentiating 123 ( e; a ' ) with respect to the parameters , we obtain the following gradient descent learning rule for the weight matrix
allij = p l h ~ t ) h ( t ) ( y ( t ) -
where p is a learning rate .
the gradient descent learning rule for the ith weight vector in the top - level gating network is given by :
avi = p - ' ( h ) - g123t ) ) x ( i ) ,
and the gradient descent rule for the jth weight vector in the i ' h lower - level gating network is
updates can also be obtained for covariance matrices ( jordan & jacobs , 123 ) .
the algorithm given by equations 123 , 123 , and 123 is a batch learning algorithm .
the corre - sponding on - line algorithm is obtained by simply dropping the summation sign and updating the parameters after each stimulus presentation .
thus , for example ,
is the stochastic update rule for the weights in the ( i , j ) th expert network based on the tth
+ pht ) ht ) t ( y ( t ) - pi ) ) xt ) t ( 123 )
the em algorithm
in the following sections we develop a learning algorithm for the hme architecture based on the expectation - maximization ( em ) framework of dempster , laird , and rubin ( 123 ) .
we derive an em algorithm for the architecture that consists of the iterative solution of a coupled set of iteratively - reweighted least - squares problems .
the em algorithm is a general technique for maximum likelihood estimation .
in practice em has been applied almost exclusively to unsupervised learning problems .
this is true of the neural network literature and machine learning literature , in which em has appeared in the context of clustering ( cheeseman , et al .
123; nowlan , 123 ) and density estimation ( specht , 123 ) , as well as the statistics literature .
in which applications include missing data problems ( little & , rubin , 123 ) .
mixture density estimation ( redner & walker , 123 ) , and factor analysis ( dempster , laird .
& rubin .
123 ) .
another unsupervised learning application is the learning problem for hidden markov models , for which the baum - welch reestimation formulas are a special case of em .
there is nothing in the em framework that precludes its application to regression or classification problems; however , such applications have been few
em is an iterative approach to maximum likelihood estimation .
each iteration of an em algorithm is composed of two steps : an estimation ( e ) step and a maximization ( m ) step .
the m step involves the maximization of a likelihood function that is redefined in each itera - tion by the e step .
if the algorithm simply increases the function during the m step , rather than maximizing the function , then the algorithm is referred to as a generalized em ( gem ) algorithm .
the boltzmann learning algorithm ( hinton & sejnowski , 123 ) is a neural network example of a gem algorithm .
gem algorithms are often significantly slower to converge than
an application of em generally begins with the observation that the optimization of the likelihood function 123 ( 123; a ' ) would be simplified if only a set of additional variables , called " miss - ing " or - hidden " variables , were known .
in this context , we refer to the observable data a ' as the " incomplete data " and posit a " complete data " set y that includes the missing variables z .
we specify a probability model that links the fictive missing variables to the actual data : p ( y .
zlx , 123 ) .
the logarithm of the density p defines the " complete - data likelihood , " l , ( 123; y ) .
the original likelihood , 123 ( 123; a ' ) , is referred to in this context as the " incomplete - data likelihood . " it is the relationship between these two likelihood functions that motivates the em algorithm .
note that the complete - data likelihood is a random variable , because the missing variables z are in fact unknown .
an em algorithm first finds the expected value of the complete - data likelihood , given the observed data and the current model .
this is the e step :
q ( 123 , 123 ( pj ) = e ( 123 ( o; y ) i x ) ,
where 123 ( p ) is the value of the parameters at the pth iteration and the expectation is taken with respect to 123 ( p ) .
this step yields a deterministic function q .
the m step maximizes this function with respect to 123 to find the new parameter estimates o ( p+ ' ) :
123 ( p+l ) = argmax ! q ( o , o ( p ) ) .
the e step is then repeated to yield an improved estimate of the complete likelihood and the
" 123an exception is the " switching regression " model of quandt and ramsey ( 123 ) .
for further discussion of
switching regression , see jordan and xu ( 123 ) .
an iterative step of em chooses a parameter value that increases the value of q , the expec - tation of the complete likelihood .
what is the effect of such a step on the incomplete likelihood ? dempster , et al .
proved that an increase in q implies an increase in the incomplete likelihood :
123 ( 123 ( p+i ) ; x ) _ 123 ( 123 ( p ) ; xt ) .
equality obtains only at the stationary points of i ( wu , 123 ) .
thus the likelihood i increases monotonically along the sequence of parameter estimates generated by an em algorithm .
in practice this implies convergence to a local maximum .
applying em to the hme architecture
to develop an em algorithm for the hme architecture , we must define appropriate " missing data " so as to simplify the likelihood function .
we define indicator variables zi and z .
123i , such that one and only one of the zi is equal to one , and one and only one of the z ~ ji is equal to one .
these indicator variables have an interpretation as the labels that correspond to the decisions in the probability model .
we also define the indicator variable zij , which is the product of zi and zjlj .
this variable has an interpretation as the label that specifies the expert ( the regressive process ) in the probability model .
if the labels zi , zjli and zij were known , then the maximum likelihood problem would decouple into a separate set of regression problems for each expert network and a separate set of multiway classification problems for the gating networks .
these problems would be solved independently of each other , yielding a rapid one - pass learning algorithm .
of course , the missing variables are not known , but we can specify a probability model that links them to the observable data .
this probability model can be written in terms of the zj , as follows :
j - r j " g t )
g pij ( y ( ) ) zi ,
using the fact that z 123t ) is an indicator variable .
taking the logarithm of this probability model yields the following complete - data likelihood :
l , ( e; y ) =
j y z ) nfg ~ t ( ) gpj ~ y ( t ) )
ziv ln gijli ij ( cid : 123 ) #i
, , ( + in p , 123 ( y ( t ) ) .
note the relationship of the complete - data likelihood in equation 123 to the incomplete - data likelihood in equation 123
the use of the indicator variables zij has allowed the logarithm to be brought inside the summation signs , substantially simplifying the maximization problem .
we now define the e step of the em algorithm by taking the expectation of the complete - data
b ( l ) ng123t + lng123i + in pa123 ( y ( t ) ) ) ,
where we have used the fact that :
- 123 _x ( t ) _ ( p ) ) p ( t
p y ( t ) x ( t ) o .
. ( p ) ) g y , ( t ) , (
j , ( olp w oix ~ lvt ) o ( p ) )
( note also that e ( zpt ) ix )
123 h123 t ) and e ( z ( jia ' ) = h ( wt )
the m step requires maximizing q ( 123 , 123 ( p ) ) with respect to the expert network parameters and the gating network parameters .
examining equation 123 , we see that the expert nctwork parameters influence the q function only through the terms ht ) in pij ( y ( t ) ) , and the gating network parameters influence the q function only through the tefms 123 ) in gi ) and 123 thus the m step reduces to the following separate maximization problems :
arg max e 123 ) in pij ( y ( t ) ) ,
vp+ = argrmaxie
123 ) in g ~ t )
vp+123 ) = argm .
123 e 123 ) in
each of these maximization problems are themselves maximum likelihood problems .
equa - tion 123 is simply the general form of a weighted maximum likelihood problem in the probability density p ~ j .
given our parameterization of pij , the log likelihood in equation 123 is a weighted log likelihood for a glim .
an efficient algorithm known as iteratively reweighted least - squares ( irls ) is available to solve the maximum likelihood problem for such models ( mccullagh & nelder , 123 ) .
we discuss irls in appendix a .
equation 123 involves maximizing the cross - entropy between the posterior probabilities h ( ) and the prior probabilities g123k .
this cross - entropy is the log likelihood associated with a multi - nomial logit probability model in which the h ( t ) act as the output observations ( see appendix b ) .
thus the maximization in equation 123 is also a maximum likelihood problem for a glim and can be solved using irls .
the same is true of equation 123 , which is a weighted maximum likelihood problem with output observations hi ) k and observation weights h123k .
in summary , the em algorithm that we have obtained involves a calculation of posterior probabilities in the outer loop ( the e step ) , and the solution of a set of irls problems in the inner loop ( the m step ) .
we summarize the algorithm as follows :
for each data pair ( x ( t ) , y ( t ) ) , compute the posterior probabilities hlt ) and h ( ' ) using the
current values of the parameters .
for each expert ( i , j ) , solve an irls problem with observations ( ( x ( t ) , y ( t ) ) ) n and obser -
vation weights ( hi h123
for each top - level gating network , solve an irls problem with observations ( ( xit ) , h ( t ) . ) in
for each lower - level gating network , solve a weighted irls problem with observations
) ) i and observation weights ( h ( t ) ) n .
iterate using the updated parameter values .
a least - squares algorithm
in the case of regression , in which a gaussian probability model and an identity link function are used , the irls loop for the expert networks reduces to weighted least squares , which can be solved ( in one pass ) by any of the standard least - squares algorithms ( golub & van loan .
the gating networks still require iterative processing .
suppose , however , that we fit the parameters of the gating uctworks using least squares rather than maximum likelihood .
in this case , we might hope to obtain an algorithm in which the gating network parameters are fit by a one - pass algorithm .
to motivate this approach , note that we can express the irls problem for the gating networks as follows .
differentiating the cross - entropy ( equation 123 ) with respect to the parameters vi ( using the fact that ogi / loj = gi ( ij - gj ) , where bij is the kronecker delta ) and setting the derivatives to zero yields the following equations :
- gi ( x123t ) vi ) ) x123 = 123 ,
which are a coupled set of equations that must be solved for each i .
similarly , for each gating network at the second level of the tree , we obtain the following equations :
" i123 ) ( 123 ) - gjli ( x ( t ) , vij ) ) x ( t ) = 123 ,
which must be solved for each i and j .
there is one aspect of these equations that renders them unusual .
recall that if the labels z and p ) were known , then the gating networks would be
essentially solving a set of multiway classification problems .
the supervised errors ( z : t ) - g ( cid : 123 ) t ) ) and ( - ( i ) - g ( it ) would appear in the algorithm for solving these problems .
note that these errors are differences between indicator variables and probabilities .
in equations 123 and 123 , on the other hand , the errors that drive the algorithm are the differences ( h123 - g ~ i ) ) and ( h ( which are differences between probabilities .
the em algorithm effectively " fills in " the missing labels with estimated probabilities hi and hjli .
these estimated probabilities can be thought of as targets for the gi and the gili .
this suggests that we can compute " virtual targets " for the underlying linear predictors & , and jlj , by inverting the softmax function .
( note that this
option would not be available for the zi and zjli , even if they were known , because zero and one are not in the range of the softmax function . ) thus the targets for the i are the values :
in ht ) - - in c ,
where c = ek ctk is the normalization constant in the softmax function .
note , however , that constants that are common to all of the i can be omitted , because such constants disappear when i are converted to gi .
thus the values in h ! ' can be used as targets for the i .
a similar argument shows that the values in h ( ' ) can be used as targets for the cij , with observation
the utility of this approach is that once targets are available for the linear predictors ( cid : 123 ) i and ij , the problem of finding the parameters vi and vii reduces to a coupled set of weighted least - squares problems .
thus we obtain an algorithm in which all of the parameters in the hierarchy , both in the expert networks and the gating networks , can be obtained by solving least - squares problems .
this yields the following learning algorithm :
for each data pair ( x ( o ) , y ( t ) ) , compute the posterior probabilities h t ) and h 123t ) using the
current values of the parameters .
for each expert ( ij ) ,
solve a weighted
least - squares problem with observations
and observation weights ( h ( t ) n
for each top - level gating network , solve a least - squares problem with observations
( ( x ( 123 , in h ( ' 123 ) ) ) i .
for each lower - level gating network , solve a weighted least - squares problem with obser -
vations f ( x ( t ) .
rlnkh ) ) n and observation weights ( ht ) ) n
iterate using the updated parameter values .
it is important to note that this algorithm does not yield the same parameter estimates as algorithm 123; the gating network residuals ( ht ) - g ~ t ) ) are being fit by least squares rather than maximum likelihood .
the algorithm can be thought of as an approximation to algorithm 123 , an approximation based on the assumption that the differences between hlt ) and g ( t ) are small .
this assumption is equivalent to the assumption that the architecture can fit the underlying regression surface ( a consistency condition ) and the assumption that the noise is small .
practice we have found that the least squares algorithm works reasonably well , even in the early stages of fitting when the residuals can be large .
the ability to use least squares is certainly appealing from a computational point of view .
one possible hybrid algorithm involves using the least squares algorithm to converge quickly to the neighborhood of a solution and then using irls to refine the solution .
we tested algorithm 123 and algorithm 123 on a nonlinear system identification problem .
the data were obtained from a simulation of a four - joint robot arm moving in three - dimensional
hme ( algorithm 123 ) hme ( algorithm 123 )
relative error # epochs
table 123 : average values of relative error and number of epochs required for convergence for the
space ( fun & jordan , 123 ) .
the network must learn the forward dynamics of the arm; a state - dependent mapping from joint torques to joint accelerations .
the state of the arm is encoded by eight real - valued variables : four positions ( rad ) and four angular velocities ( rad / sec ) .
the torque was encoded as four real - valued variables ( n .
thus there were twelve inputs to the learning system .
given these twelve input variables , the network must predict the four accelerations at the joints ( rad / sec123 ) .
this mapping is highly nonlinear due to the rotating coordinate systems and the interaction torques between the links of the arm .
we generated 123 , 123 data points for training and 123 , 123 points for testing .
for each epoch ( i . e . , each pass through the training set ) , we computed the relative error on the test set .
relative error is computed as a ratio between the mean squared error and the mean squared error that would be obtained if the learner were to output the mean value of the accelerations for all data
we compared the performance of a binary hierarchy to that of a backpropagation network .
the hierarchy was a four - level hierarchy with 123 expert networks and 123 gating networks .
each expert network had 123 output units and each gating network had 123 output unit .
the backpropagation network had 123 hidden units , which yields approximately the same number of parameters in the network as in the hierarchy .
the hme architecture was trained by algorithms 123 and 123 , utilizing cholesky decomposition to solve the weighted least - squares problems ( golub & van loan , 123 ) .
note that the hme algorithms have no free parameters .
the free parameters for the backpropagation network ( the learning rate and the momentum term ) were chosen based on a coarse search of the parameter space .
( values of 123 and 123 were chosen for these parameters . ) there were difficulties with local minima ( or plateaus ) using the backpropagation algorithm : five of ten runs failed to converge to " reasonable " error values .
( as we report in the next section , no such difficulties were encountered in the case of on - line backpropagation ) .
we report average convergence times and average relative errors only for those runs that converged to " reasonable " error values .
all ten runs for both of the hme algorithms converged to " reasonable " error values .
figure 123 shows the performance of the hierarchy and the backpropagation network .
the hor - izontal axis of the graph gives the training time in epochs .
the vertical axis gives generalization performance as measured by the average relative error on the test set .
table 123 reports the average relative errors for both architectures measured at the minima of the relative error curves .
( minima were defined by a sequence of three successive increases in the relative error . ) we also report values of relative error for the best linear approximation , the cart algorithm , and the mars algorithm .
both cart and mars were run four times ,
. . . . . . . . .
hme ( algorithm 123 )
figure 123 : relative error on the test set for a backpropagation network and a four - level hme architecture trained with batch algorithms .
the standard errors at the minima of the curves are 123 for backprop and 123 for hme .
once for each of the output variables .
we combined the results from these four computations to compute the total relative error .
two versions of cart were run; one in which the splits were restricted to be parallel to the axes and one in which linear combinations of the input variables
the mars algorithm requires choices to be made for the values of two structural parame - ters : the maximum number of basis functions and the maximum number of interaction terms .
each basis function in mars yields a linear surface defined over a rectangular region of the input space , corresponding roughly to the function implemented by a single expert in the hme architecture .
therefore we chose a maximum of 123 basis functions to correspond to the 123 experts in the four - level hierarchy .
to choose the maximum number of interactions ( mi ) , we compared the performance of mars for mi = 123 , 123 , 123 , 123 , and 123 , and chose the value that yielded the best performance ( mi = 123 ) .
for the iterative algorithms , we also report the number of epochs required for convergence .
because the learning curves for these algorithms generally have lengthy tails , we defined con - vergence as the first epoch at which the relative error drops within five percent of the minimum .
all of the architectures that we studied performed significantly better than the best linear approximation .
as expected , the cart architecture with linear combinations performed better than ( ' art with axis - parallel splits . ' the hme architecture yielded a modest improvement
' it should be noted that cart is at an advantage relative to the other algorithms in this comparison , because no structural parameters were fixed for cart .
that is , cart is allowed to find the best .
tree of any size to fit
aa a a a
a a a wllui ll i wl lau
i lj la l lj
figure 123 : a sequence of histogram trees for the hme architecture .
each histogram displays the distribution of posterior probabilities across the training set at each node in the tree .
over mars and cart .
backpropagation produced the lowest relative error of the algorithms tested ( ignoring the difficulties with convergence ) .
these differences in relative error should be treated with some caution .
the need to set free parameters for some of the architectures ( e . g . , backpropagation ) and the need to make structural choices ( e . g . , number of hidden units , number of basis functions , number of experts ) makes it difficult to match architectures .
the hme architecture , for example , involves parameter dependencies that are not present in a backpropagation network .
a gating network at a high level in the tree can " pinch off " a branch of the tree , rendering useless the parameters in that branch of the tree .
raw parameter count is therefore only a very rough guide to architecture capacity; more precise measures are needed ( e . g . , vc dimension ) before definitive quantitative comparisons can be made .
the differences between backpropagation and hme in terms of convergence time are more definitive .
both hme algorithms reliably converge more than two orders of magnitude faster
as shown in figure 123 , the hme architecture lends itself well to graphical investigation .
this figure displays the time sequence of the distributions of posterior probabilities across the training set at each node of the tree .
at epoch 123 , before any learning has taken place , most of the posterior probabilities at each node are approximately 123 across the training set .
as the training proceeds , the histograms flatten out , eventually approaching bimodal distributions in
iii li . ji
uh wi li
figure 123 : a deviance tree for the hme architecture .
each plot displays the mean squared error ( mse ) for the four output units of the clipped tree .
the plots are on a log scale covering approximately three orders of magnitude .
which the posterior probabilities are either one or zero for most of the training patterns .
this evolution is indicative of increasingly sharp splits being fit by the gating networks .
note that there is a tendency for the splits to be formed more rapidly at higher levels in the tree than at
figure 123 shows another graphical device that can be useful for understanding the way in which a hme architecture fits a data set .
this figure , which we refer to as a " deviance tree , " shows the deviance ( mean squared error ) that would be obtained at each level of the tree if the tree were clipped at that level .
we construct a clipped tree at a given level by replacing each nonterminal at that level with a matrix that is a weighted average of the experts below that nonterminal .
the weights are the total prior probabilities associated with each expert across the training set .
the error for each output unit is then calculated by passing the test set through the clipped tree .
as can be seen in the figure , the deviance is substantially smaller for deeper trees ( note that the ordinate of the plots is on a log scale ) .
the deviance in the right branch of the tree is larger than in the left branch of the tree .
information such as this can be useful for purposes of exploratory data analysis and for model selection .
an on - line algorithm
the batch least - squares algorithm that we have described ( algorithm 123 ) can be converted into an on - line algorithm by noting that linear least squares and weighted linear least squares problems can be solved by recursive procedures that update the parameter estimates with each successive data point ( ljung & s123derstr123m , 123 ) .
our application of these recursive algorithms is straightforward; however , care must be taken to handle the observation weights ( the posterior probabilities ) correctly .
these weights change as a function of the changing parameter values .
this implies that the recursive least squares algorithm must include a , decay parameter that allows the system to " forget " older values of the posterior probabilities .
in this section we present the equations for the on - line algorithm .
these equations involve an update not only of the parameters in each of the networks .
123 but also the storage and updating of an inverse covariance matrix for each network .
each matrix has dimensionality mxrn , where m is the dimensionality of the input vector .
( note that the size of these matrices depends on the square of the number of input variables , not the square of the number of paramretrs .
note also that the update equation for the inverse covariance matrix updates the inverse matrix directly; there is never a need to invert matrices . )
the on - line update rule for the parameters of the expert networks is given by the following
123 ( t+123 ) = 123 ( ) 123 + rh ( t ) h , ( t123tv ( t ) - a ( t ) x ( t ) tgt )
where rij is the inverse covariance matrix for expert network ( ij ) .
this matrix is updated via
\ 123r ( t , 123 ) - a - ' -
\ ( h - t ) ) - + x ( t ) tr l - 123 ) x ( t ) 123
where a is the decay parameter .
it is interesting to note the similarity between the parameter update rule in equation 123 and the gradient rule presented earlier ( cf .
equation 123 ) .
these updates are essentially the same , except that the scalar p is replaced by the matrix r / . ) it can be shown , however , that r ' )
is an estimate of the inverse hessian of the least - squares cost function ( ljung & s123derstr123m , 123 ) , thus equation 123 is in fact a stochastic approximation to a newton - raphson method rather than a gradient method . '
similar equations apply for the updates of the gating networks .
the update rule for the parameters of the top - level gating network is given by the following equation ( for the ith output of the gating network ) :
where the inverse covariance matrix si is updated by :
vlt+i ) = vlt ) + s ) ( inhlt ) -
s ( t ) = a_ , s ! , _ , ) _
a + x ( t ) ts ( - 123 - ) x ( t )
finally , the update rule for the parameters of the lower - level gating network are as follows :
( t ) sh t ) h ( in
lnh ( 123 ) - ct ) 123x ( t )
' note that in this section we use the term " parameters " for the variables that are traditionally called " weights "
in the neural network literature .
we reserve the term " weights " for the observation weights .
" 123this is true for fixed values of the posterior probabilities .
these posterior probabilities are also changing over time , however , as required by the em algorithm .
the overall convergence rate of the algorithm is determined by the convergence rate of em , not the convergence rate of newton - raphson .
- - o - - - - e - o
figure 123 : relative error on the test set for a backpropagation network and a four - level hierarchy trained with on - line algorithms .
the standard errors at the minima of the curves are 123 for backprop and 123 for hme .
where the inverse covariance matrix si is updated by :
( h123 ) ) - 123 + x ( t ) tst . _ ) x - )
the on - line algorithm was tested on the robot dynamics problem described in the previous section .
preliminary simulations convinced us of the necessity of the decay parameter ( a ) .
we also found that this parameter should be slowly increased as training proceeds - - on the early trials the posterior probabilities are changing rapidly so that the covariances should be decayed rapidly , whereas on later trials the posterior probabilities have stabilized and the covariances should be decayed less rapidly .
we used a simple fixed schedule : a was initialized to 123 and increased a fixed fraction ( 123 ) of the remaining distance to 123 every 123 time steps .
the performance of the on - line algorithm was compared to an on - line backpropagation network .
parameter settings for the backpropagation network were obtained by a coarse search through the parameter space , yielding a value of 123 for the learning rate and 123 for the momentum .
the results for both architectures are shown in figure 123
as can be seen , the on - line algorithm for backpropagation is significantly faster than the corresponding batch algorithm ( cf .
figure 123 ) .
this is also true of the on - line hme algorithm , which has nearly converged within the first epoch .
relative error ( # epochs
table 123 : average values of relative error and number of epochs required for convergence for the
the minimum values of relative error and the convergence times for both architectures are provided in table 123
we also provide the corresponding values for a simulation of the on - line gradient algorithm for the hme architecture ( equation 123 ) .
we also performed a set of simulations which tested a variety of different hme architectures .
we compared a one - level hierarchy with 123 experts to hierarchies with five levels ( 123 experts ) , and six levels ( 123 experts ) .
we also simulated two three - level hierarchies , one with branching factors of 123 , 123 , and 123 ( proceeding from the top of the tree to the bottom ) , and one with branching factors of 123 , 123 , and 123
( each three - level hierarchy contained 123 experts . ) the results are shown in figure 123
as can be seen , there was a significant difference between the one - level hierarchy and the other architectures .
there were smaller differences among the multi - level hierarchies .
no significant difference was observed between the two different 123 - level architectures .
utilizing the hme approach requires that choices be made regarding the structural parameters of the model , in particular the number of levels and the branching factor of the tree .
as with other flexible estimation techniques , it is desirable to allow these structural parameters to be chosen based at least partly on the data .
this model selection problem can be addressed in a variety of ways .
in this paper we have utilized a test set approach to model selection , stopping the training when the error on the test set reaches a minimum .
as is the case with other neural network algorithms , this procedure can be justified as a complexity control measure .
as we have noted , when the parameters in the gating networks of an hme architecture are small , the entire system reduces to a single " averaged " glim at the root of the tree .
as the training proceeds , the parameters in the gating networks begin to grow in magnitude and splits are formed .
when a split is formed the parameters in the branches of the tree on either side of the split are decoupled and the effective number of degrees of freedom in the system increases .
this increase in complexity takes place gradually as the values of the parameters increase and the splits sharpen .
by stopping the training of the system based on the performance on a test set , we obtain control over the effective number of degrees of freedom in the architecture .
other approaches to model selection can also be considered .
one natural approach is to use ridge regression in each of the expert networks and the gating networks .
this approach extends naturally to the on - line setting in the form of a " weight decay . " it is also worth considering bayesian techniques of the kind considered in the decision tree literature by buntine ( 123 ) , as well as the mdl methods of quinlan and rivest ( 123 ) .
. . . . . . .
123 - level ( a )
figure 123 : relative error on the test set for hme hierarchies with different structures .
" 123 - level ( a ) - refers to a 123 - level hierarchy with branching factors of 123 , 123 , and 123 , and " 123 - level ( b ) " refers to a 123 - level hierarchy with branching factors of 123 , 123 , and 123
the standard errors for all curves at their respective minima were approximately 123 .
there are a variety of ties that can be made between the hme architecture and related work in statistics , machine learning , and neural networks .
in this section we briefly mention some of these ties and make some comparative remarks .
our architecture is not the only nonlinear approximator to make substantial use of glim ' s and the irls algorithm .
irls also figures prominently in a branch of nonparametric statistics known as generalized additive models ( gam ' s; hastie & tibshirani , 123 ) .
it is interesting to note the complementary roles of irls in these two architectures .
in the gam model , the irls algorithm appears in the outer loop , providing an adjusted dependent variable that is fit by a backfitting procedure in the inner loop .
in the hme approach , on the other hand , the outer loop is the e step of em and irls is in the inner loop .
this complementarity suggests that it might be of interest to consider hybrid models in which a hme is nested inside a gam or vice
we have already mentioned the close ties between the hme approach and other tree - structured estimators such as cart and mars .
our approach differs from mars and related architectures - such as the basis - function trees of sanger ( 123 ) - by allowing splits that are oblique with respect to the axes .
we also differ from these architectures by using a statistical model - the multinomial logit model - for the splits .
we believe that both of these features can play a role in increasing predictive ability - the use of oblique splits should tend to decrease
bias , and the use of smooth multinomial logit splits should generally decrease variance .
oblique splits also render the hme architecture insensitive to the particular choice of coordinates used to encode the data .
finally , it is worth emphasizing the difference in philosophy behind these architectures .
whereas cart and mars are entirely nonparametric , the hme approach has a strong flavor of parametric statistics , via its use of generalized linear models , mixture models and maximum likelihood .
similar comments can be made with respect to the decision tree methodology in the machine learning literature .
algorithms such as id123 build trees that have axis - parallel splits and use heuristic splitting algorithms ( quinlan , 123 ) .
more recent research has studied decision trees with oblique splits ( murthy , kasif & salzberg , 123; utgoff & brodley , 123 ) .
none of these papers , however , have treated the problem of splitting data as a statistical problem .
nor have they provided a global goodness - of - fit measure for their trees .
there are a variety of neural network architectures that are related to the hme architec - ture .
the multi - resolution aspect of hme is reminiscent of moody ' s ( 123 ) multi - resolution cmac hierarchy , differing in that moody ' s levels of resolution are handled explicitly by sepa - rate networks .
the " neural tree " algorithm ( strbmberg , zrida , & isaksson , 123 ) is a decision tree with multi - layer perceptions ( mlp ' s ) at the non - terminals .
this architecture can form oblique ( or curvilinear ) splits , however the mlp ' s are trained by a heuristic that has no clear relationship to overall classification performance .
finally , hinton and nowlan ( see nowlan , 123 ) have independently proposed extending the jacobs et al .
( 123 ) modular architecture to a tree - structured system .
they did not develop a likelihood approach to the problem , however , proposing instead a heuristic splitting scheme .
we have presented a tree - structured architecture for supervised learning .
we have developed the learning algorithm for this architecture within the framework of maximum likelihood esti - mation , utilizing ideas from mixture model estimation and generalized linear model theory .
the maximum likelihood framework allows standard tools from statistical theory to be brought to bear in developing inference procedures and measures of uncertainty for the architecture ( cox & hinkley , 123 ) .
it also opens the door to the bayesian approaches that have been found to be useful in the context of unsupervised mixture model estimation ( cheeseman , et al . , 123 ) .
although we have not emphasized theoretical issues in this paper , there are a number of points that are worth mentioning .
first , the set of exponentially - smoothed piecewise linear functions that we have utilized are clearly dense in the set of piecewise linear functions on com - pact sets in *i , thus it is straightforward to show that the hierarchical architecture is dense in the set of continuous functions on compact sets in wm .
that is , the architecture is " univer - sal " in the sense of hornik , stinchcombe , and white ( 123 ) .
from this result it would seem straightforward to develop consistency results for the architecture ( cf .
geman , bienenstock , & doursat , 123; stone , 123 ) .
we are currently developing this line of argument and are study - ing the asymptotic distributional properties of fixed hierarchies .
second , convergence results are available for the architecture .
we have shown that the convergence rate of the algorithm is linear in the condition number of a matrix that is the product of an inverse covariance matrix and the hessian of the log likelihood for the architecture ( jordan & xu , 123 ) .
finally , it is worth noting a number of possible extensions of the work reported here .
our earlier work on hierarchical mixtures of experts utilized the multilayer perceptron as the prim - itive function for the expert networks and gating networks ( jordan & jacobs , 123 )
option is still available , although we lose the em proof of convergence ( cf .
jordan & xu , 123 ) and we lose the ability to fit the sub - networks efficiently with irls .
one interesting example of such an application is the case where the experts are auto - associators ( bourlard & kamp , 123 ) .
in which case the architecture fits hierarchically - nested local principal component de - compositions .
another area in unsupervised learning worth exploring is the non - associative version of the hierarchical architecture .
such a model would be a recursive version of classical mixture - likelihood clustering and may have interesting ties to hierarchical clustering models .
finally .
it is also of interest to note that the recursive least squares algorithm that we utilized in obtaining an on - line variant of algorithm 123 is not the only possible on - line approach .
any of the fast filter algorithms ( haykin , 123 ) could also be utilized , giving rise to a family of on - line algorithms .
it is worth studying the application of the recursive algorithms to press - like cross - validation calculations to efficiently compute the changes in likelihood that arise from adding or deleting parameters or data points .
acknowledgements : we want to thank geoffrey hinton , tony robinson , mitsuo kawato , and daniel wolpert for helpful comments on the manuscript .
