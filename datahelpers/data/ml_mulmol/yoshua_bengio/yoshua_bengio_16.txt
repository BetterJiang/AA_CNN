one long - term goal of machine learning research is to produce methods that are applicable to highly complex tasks , such as perception ( vision , audition ) , rea - soning , intelligent control , and other articially intelligent behaviors .
we argue that in order to progress toward this goal , the machine learning community must endeavor to discover algorithms that can learn highly complex functions , with min - imal need for prior knowledge , and with minimal human intervention .
we present mathematical and empirical evidence suggesting that many popular approaches to non - parametric learning , particularly kernel methods , are fundamentally lim - ited in their ability to learn complex high - dimensional functions .
our analysis focuses on two problems .
first , kernel machines are shallow architectures , in which one large layer of simple template matchers is followed by a single layer of trainable coefcients .
we argue that shallow architectures can be very inef - cient in terms of required number of computational elements and examples .
sec - ond , we analyze a limitation of kernel machines with a local kernel , linked to the curse of dimensionality , that applies to supervised , unsupervised ( manifold learn - ing ) and semi - supervised kernel machines .
using empirical results on invariant image recognition tasks , kernel methods are compared with deep architectures , in which lower - level features or concepts are progressively combined into more ab - stract and higher - level representations .
we argue that deep architectures have the potential to generalize in non - local ways , i . e . , beyond immediate neighbors , and that this is crucial in order to make progress on the kind of complex tasks required for articial intelligence .
statistical machine learning research has yielded a rich set of algorithmic and mathe - matical tools over the last decades , and has given rise to a number of commercial and scientic applications .
however , some of the initial goals of this eld of research re - main elusive .
a long - term goal of machine learning research is to produce methods that will enable articially intelligent agents capable of learning complex behaviors with minimal human intervention and prior knowledge .
examples of such complex behaviors are found in visual perception , auditory perception , and natural language
the main objective of this chapter is to discuss fundamental limitations of cer - tain classes of learning algorithms , and point towards approaches that overcome these limitations .
these limitations arise from two aspects of these algorithms : shallow ar - chitecture , and local estimators .
we would like our learning algorithms to be efcient in three respects :
computational : number of computations during training and during recognition ,
statistical : number of examples required for good generalization , especially la -
beled data , and
human involvement : amount of human labor necessary to tailor the algorithm to a task , i . e . , specify the prior knowledge built into the model before training .
( explicitly , or implicitly through engineering designs with a human - in - the - loop ) .
the last quarter century has given us exible non - parametric learning algorithms that can learn any continuous input - output mapping , provided enough computing resources and training data .
a crucial question is how efcient are some of the popular learn - ing methods when they are applied to complex perceptual tasks , such a visual pattern recognition with complicated intra - class variability .
the chapter mostly focuses on computational and statistical efciency .
among exible learning algorithms , we establish a distinction between shallow architectures , and deep architectures .
shallow architectures are best exemplied by modern kernel machines ( scholkopf et al . , 123 ) , such as support vector machines ( svms ) ( boser et al . , 123 , cortes and vapnik , 123 ) .
they consist of one layer of xed kernel functions , whose role is to match the incoming pattern with templates ex - tracted from a training set , followed by a linear combination of the matching scores .
since the templates are extracted from the training set , the rst layer of a kernel ma - chine can be seen as being trained in a somewhat trivial unsupervised way .
the only components subject to supervised training are the coefcients of the linear combina -
deep architectures are perhaps best exemplied by multi - layer neural networks with several hidden layers .
in general terms , deep architectures are composed of mul - tiple layers of parameterized non - linear modules .
the parameters of every module are
123in svms only a subset of the examples are selected as templates ( the support vectors ) , but this is equiv -
alent to choosing which coefcients of the second layer are non - zero .
subject to learning .
deep architectures rarely appear in the machine learning litera - ture; the vast majority of neural network research has focused on shallow architectures with a single hidden layer , because of the difculty of training networks with more than 123 or 123 layers ( tesauro , 123 ) .
notable exceptions include work on convolutional networks ( lecun et al . , 123 , lecun et al . , 123 ) , and recent work on deep belief networks ( hinton et al . , 123 ) .
while shallow architectures have advantages , such as the possibility to use convex loss functions , we show that they also have limitations in the efciency of the represen - tation of certain types of function families .
although a number of theorems show that certain shallow architectures ( gaussian kernel machines , 123 - hidden layer neural nets , etc ) can approximate any function with arbitrary precision , they make no statements as to the efciency of the representation .
conversely , deep architectures can , in prin - ciple , represent certain families of functions more efciently ( and with better scaling properties ) than shallow ones , but the associated loss functions are almost always non
the chapter starts with a short discussion about task - specic versus more general types of learning algorithms .
although the human brain is sometimes cited as an ex - istence proof of a general - purpose learning algorithm , appearances can be deceiving : the so - called no - free - lunch theorems ( wolpert , 123 ) , as well as vapniks necessary and sufcient conditions for consistency ( vapnik , 123 , see ) , clearly show that there is no such thing as a completely general learning algorithm .
all practical learning al - gorithms are associated with some sort of explicit or implicit prior that favors some functions over others .
since a quest for a completely general learning method is doomed to failure , one is reduced to searching for learning models that are well suited for a particular type of tasks .
for us , high on the list of useful tasks are those that most animals can per - form effortlessly , such as perception and control , as well as tasks that higher animals and humans can do such as long - term prediction , reasoning , planning , and language understanding .
in short , our aim is to look for learning methods that bring us closer to an articially intelligent agent .
what matters the most in this endeavor is how ef - ciently our model can capture and represent the required knowledge .
the efciency is measured along three main dimensions : the amount of training data required ( espe - cially labeled data ) , the amount of computing resources required to reach a given level of performance , and most importantly , the amount of human effort required to specify the prior knowledge built into the model before training ( explicitly , or implicitly ) this chapter discusses the scaling properties of various learning models , in particular kernel machines , with respect to those three dimensions , in particular the rst two .
kernel machines are non - parametric learning models , which make apparently weak assump - tions on the form of the function f ( ) to be learned .
by non - parametric methods we mean methods which allow the complexity of the solution to increase ( e . g . , by hyper - parameter selection ) when more data are available .
this includes classical k - nearest - neighbor algorithms , modern kernel machines , mixture models , and multi - layer neural networks ( where the number of hidden units can be selected using the data ) .
our ar - guments are centered around two limitations of kernel machines : the rst limitation applies more generally to shallow architectures , which include neural networks with a single hidden layer .
in section 123 we consider different types of function classes , i . e . ,
architectures , including different sub - types of shallow architectures .
we consider the trade - off between the depth of the architecture and its breadth ( number of elements in each layer ) , thus clarifying the representational limitation of shallow architectures .
the second limitation is more specic and concerns kernel machines with a local ker - nel .
this limitation is studied rst informally in section 123 by thought experiments in the use of template matching for visual perception .
section 123 then focusses more formally on local estimators , i . e . , in which the prediction f ( x ) at point x is dominated by the near neighbors of x taken from the training set .
this includes kernel machines in which the kernel is local , like the gaussian kernel .
these algorithms rely on a prior expressed as a distance or similarity function between pairs of examples , and encom - pass classical statistical algorithms as well as modern kernel machines .
this limitation is pervasive , not only in classication , regression , and density estimation , but also in manifold learning and semi - supervised learning , where many modern methods have such locality property , and are often explicitly based on the graph of near neighbors .
using visual pattern recognition as an example , we illustrate how the shallow nature of kernel machines leads to fundamentally inefcient representations .
finally , deep architectures are proposed as a way to escape from the fundamental limitations above .
section 123 concentrates on the advantages and disadvantages of deep architectures , which involve multiple levels of trainable modules between input and output .
they can retain the desired exibility in the learned functions , and increase the efciency of the model along all three dimensions of amount of training data , amount of computational resources , and amount of human prior hand - coding .
although a num - ber of learning algorithms for deep architectures have been available for some time , training such architectures is still largely perceived as a difcult challenge .
we discuss recent approaches to training such deep networks that foreshadows new breakthroughs in this direction .
the trade - off between convexity and non - convexity has , up until recently , favored research into learning algorithms with convex optimization problems .
we have found that non - convex optimization is sometimes more efcient that convex optimization .
non - convex loss functions may be an unavoidable property of learning complex func - tions from weak prior knowledge .
123 learning models towards ai
the no - free - lunch theorem for learning algorithms ( wolpert , 123 ) states that no completely general - purpose learning algorithm can exist , in the sense that for every learning model there is a data distribution on which it will fare poorly ( on both training and test , in the case of nite vc dimension ) .
every learning model must contain im - plicit or explicit restrictions on the class of functions that it can learn .
among the set of all possible functions , we are particularly interested in a subset that contains all the tasks involved in intelligent behavior .
examples of such tasks include visual percep - tion , auditory perception , planning , control , etc .
the set does not just include specic visual perception tasks ( e . g human face detection ) , but the set of all the tasks that an intelligent agent should be able to learn .
in the following , we will call this set of func - tions the ai - set .
because we want to achieve ai , we prioritize those tasks that are in
although we may like to think that the human brain is somewhat general - purpose , it is extremely restricted in its ability to learn high - dimensional functions .
the brains of humans and higher animals , with their learning abilities , can potentially implement the ai - set , and constitute a working proof of the feasibility of ai .
we advance that the ai - set is a tiny subset of the set of all possible functions , but the specication of this tiny subset may be easier than it appears .
to illustrate this point , we will use the example rst proposed by ( lecun and denker , 123 ) .
the connection between the retina and the visual areas in the brain gets wired up relatively late in embryogenesis .
if one makes the apparently reasonable assumption that all possible permutations of the millions of bers in the optic nerve are equiprobable , there is not enough bits in the genome to encode the correct wiring , and no lifetime long enough to learn it .
the at prior assumption must be rejected : some wiring must be simpler to specify ( or more likely ) than others .
in what seems like an incredibly fortunate coincidence , a particularly good ( if not correct ) wiring pattern happens to be one that preserves topology .
coincidentally , this wiring pattern happens to be very simple to describe in almost any language ( for example , the biochemical language used by biology can easily specify topology - preserving wiring patterns through concentration gradients of nerve growth factors ) .
how can we be so fortunate that the correct prior be so simple to describe , yet so informative ? lecun and denker ( 123 ) point out that the brain exists in the very same physical world for which it needs to build internal models .
hence the specication of good priors for modeling the world happen to be simple in that world ( the dimensionality and topology of the world is common to both ) .
because of this , we are allowed to hope that the ai - set , while a tiny subset of all possible functions , may be specied with a relatively small amount of information .
in practice , prior knowledge can be embedded in a learning model by specifying
three essential components :
the representation of the data : pre - processing , feature extractions , etc .
the architecture of the machine : the family of functions that the machine can
implement and its parameterization .
the loss function and regularizer : how different functions in the family are rated , given a set of training samples , and which functions are preferred in the absence of training samples ( prior or regularizer ) .
inspired by ( hinton , to appear .
123 ) , we classify machine learning research strate - gies in the pursuit of ai into three categories .
one is defeatism : since no good pa - rameterization of the ai - set is currently available , lets specify a much smaller set for each specic task through careful hand - design of the pre - processing , the architecture , and the regularizer .
if task - specic designs must be devised by hand for each new task , achieving ai will require an overwhelming amount of human effort .
neverthe - less , this constitutes the most popular approach for applying machine learning to new problems : design a clever pre - processing ( or data representation scheme ) , so that a standard learning model ( such as an svm ) will be able to learn the task .
a somewhat similar approach is to specify the task - specic prior knowledge in the structure of a
graphical model by explicitly representing important intermediate features and con - cepts through latent variables whose functional dependency on observed variables is hard - wired .
much of the research in graphical models ( jordan , 123 ) ( especially of the parametric type ) follows this approach .
both of these approaches , the kernel ap - proach with human - designed kernels or features , and the graphical models approach with human - designed dependency structure and semantics , are very attractive in the short term because they often yield quick results in making progress on a specic task , taking advantage of human ingenuity and implicit or explicit knowledge about the task , and requiring small amounts of labeled data .
the second strategy is denial : even with a generic kernel such as the gaussian kernel , kernel machines can approximate any function , and regularization ( with the bounds ) guarantee generalization .
why would we need anything else ? this belief contradicts the no free lunch theorem .
although kernel machines can represent any labeling of a particular training set , they can efciently represent a very small and very specic subset of functions , which the following sections of this chapter will at - tempt to characterize .
whether this small subset covers a large part of the ai - set is very dubious , as we will show .
in general , what we think of as generic learning algo - rithms can only work well with certain types of data representations and not so well with others .
they can in fact represent certain types of functions efciently , and not others .
while the clever preprocessing / generic learning algorithm approach may be useful for solving specic problems , it brings about little progress on the road to ai .
how can we hope to solve the wide variety of tasks required to achieve ai with this labor - intensive approach ? more importantly , how can we ever hope to integrate each of these separately - built , separately - trained , specialized modules into a coherent ar - ticially intelligent system ? even if we could build those modules , we would need another learning paradigm to be able to integrate them into a coherent system .
the third strategy is optimism : lets look for learning models that can be applied to the largest possible subset of the ai - set , while requiring the smallest possible amount of additional hand - specied knowledge for each specic task within the ai - set .
the question becomes : is there a parameterization of the ai - set that can be efciently im - plemented with computer technology ?
consider for example the problem of object recognition in computer vision : we could be interested in building recognizers for at least several thousand categories of objects .
should we have specialized algorithms for each ? similarly , in natural language processing , the focus of much current research is on devising appropriate features for specic tasks such as recognizing or parsing text of a particular type ( such as spam email , job ads , nancial news , etc ) .
are we going to have to do this labor - intensive work for all the possible types of text ? our system will not be very smart if we have to manually engineer new patches each time new a type of text or new types of object category must be processed .
if there exist more general - purpose learning models , at least general enough to handle most of the tasks that animals and humans can handle , then searching for them may save us a considerable amount of labor in the long run .
as discussed in the next section , a mathematically convenient way to characterize the kind of complex task needed for ai is that they involve learning highly non - linear functions with many variations ( i . e . , whose derivative changes direction often ) .
this is problematic in conjunction with a prior that smooth functions are more likely , i . e . ,
having few or small variations .
we mean f to be smooth when the value of f ( x ) and of its derivative f ( x ) are close to the values of f ( x + ) and f ( x + ) respectively when x and x + are close as dened by a kernel or a distance .
this chapter advances several arguments that the smoothness prior alone is insufcient to learn highly - varying functions .
this is intimately related to the curse of dimensionality , but as we nd throughout our investigation , it is not the number of dimensions so much as the amount of variation that matters .
a one - dimensional function could be difcult to learn , and many high - dimensional functions can be approximated well enough with a smooth function , so that non - parametric methods relying only on the smooth prior can still give good results .
we call strong priors a type of prior knowledge that gives high probability ( or low complexity ) to a very small set of functions ( generally related to a small set of tasks ) , and broad priors a type of prior knowledge that give moderately high probability to a wider set of relevant functions ( which may cover a large subset of tasks within the ai - set ) .
strong priors are task - specic , while broad priors are more related to the general structure of our world .
we could prematurely conjecture that if a function has many local variations ( hence is not very smooth ) , then it is not learnable unless strong prior knowledge is at hand .
fortunately , this is not true .
first , there is no reason to believe that smoothness priors should have a special status over other types of priors .
using smoothness priors when we know that the functions we want to learn are non - smooth would seem counter - productive .
other broad priors are possible .
a simple way to dene a prior is to dene a language ( e . g . , a programming language ) with which we express functions , and favor functions that have a low kolmogorov complexity in that language , i . e .
functions whose program is short .
consider using the c programming language ( along with standard libraries that come with it ) to dene our prior , and learning functions such as g ( x ) = sin ( x ) ( with x a real value ) or g ( x ) = parity ( x ) ( with x a binary vector of xed dimension ) .
these would be relatively easy to learn with a small number of samples because their description is extremely short in c and they are very probable under the corresponding prior , despite the fact that they are highly non - smooth .
we do not advocate the explicit use of kolmogorov complexity in a conventional programming language to design new learning algorithms , but we use this example to illustrate that it is possible to learn apparently complex functions ( in the sense they vary a lot ) using broad priors , by using a non - local learning algorithm , corresponding to priors other than the smoothness prior .
this thought example and the study of toy problems like the parity problem in the rest of the chapter also shows that the main challenge is to design learning algorithms that can discover representations of the data that compactly describe regularities in it .
this is in contrast with the approach of enumerating the variations present in the training data , and hoping to rely on local smoothness to correctly ll in the space between the training samples .
as we mentioned earlier , there may exist broad priors , with seemingly simple de - scription , that greatly reduce the space of accessible functions in appropriate ways .
in visual systems , an example of such a broad prior , which is inspired by natures bias towards retinotopic mappings , is the kind of connectivity used in convolutional net - works for visual pattern recognition ( lecun et al . , 123 , lecun et al . , 123 ) .
this will be examined in detail in section 123
another example of broad prior , which we discuss in section 123 , is that the functions to be learned should be expressible as multi -
ple levels of composition of simpler functions , where different levels of functions can be viewed as different levels of abstraction .
the notion of concept and of abstrac - tion that we talk about is rather broad and simply means a random quantity strongly dependent of the observed data , and useful in building a representation of its distri - bution that generalises well .
functions at lower levels of abstraction should be found useful for capturing some simpler aspects of the data distribution , so that it is possi - ble to rst learn the simpler functions and then compose them to learn more abstract concepts .
animals and humans do learn in this way , with simpler concepts earlier in life , and higher - level abstractions later , expressed in terms of the previously learned concepts .
not all functions can be decomposed in this way , but humans appear to have such a constraint .
if such a hierarchy did not exist , humans would be able to learn new concepts in any order .
hence we can hope that this type of prior may be useful to help cover the ai - set , but yet specic enough to exclude the vast majority of useless
it is a thesis of the present work that learning algorithms that build such deeply layered architectures offer a promising avenue for scaling machine learning towards ai .
another related thesis is that one should not consider the large variety of tasks separately , but as different aspects of a more general problem : that of learning the basic structure of the world , as seen say through the eyes and ears of a growing animal or a young child .
this is an instance of multi - task learning where it is clear that the different tasks share a strong commonality .
this allows us to hope that after training such a system on a large variety of tasks in the ai - set , the system may generalize to a new task from only a few labeled examples .
we hypothesize that many tasks in the ai - set may be built around common representations , which can be understood as a set of interrelated concepts .
if our goal is to build a learning machine for the ai - set , our research should con -
centrate on devising learning models with the following features :
a highly exible way to specify prior knowledge , hence a learning algorithm
that can function with a large repertoire of architectures .
a learning algorithm that can deal with deep architectures , in which a decision involves the manipulation of many intermediate concepts , and multiple levels of
a learning algorithm that can handle large families of functions , parameterized
with millions of individual parameters .
a learning algorithm that can be trained efciently even , when the number of training examples becomes very large .
this excludes learning algorithms requir - ing to store and iterate multiple times over the whole training set , or for which the amount of computations per example increases as more examples are seen .
this strongly suggest the use of on - line learning .
a learning algorithm that can discover concepts that can be shared easily among multiple tasks and multiple modalities ( multi - task learning ) , and that can take advantage of large amounts of unlabeled data ( semi - supervised learning ) .
123 learning architectures , shallow and deep
123 architecture types
in this section , we dene the notions of shallow and deep architectures .
an informal discussion of their relative advantages and disadvantage is presented using examples .
a more formal discussion of the limitations of shallow architectures with local smooth - ness ( which includes most modern kernel methods ) is given in the next section .
following the tradition of the classic book perceptrons ( minsky and papert , 123 ) , it is instructive to categorize different types of learning architectures and to analyze their limitations and advantages .
to x ideas , consider the simple case of classication in which a discrete label is produced by the learning machine y = f ( x , w ) , where x is the input pattern , and w a parameter which indexes the family of functions f that can be implemented by the architecture f = ( f ( , w ) , w w ) .
figure 123 : different types of shallow architectures .
( a ) type - 123 : xed preprocessing and linear predictor; ( b ) type - 123 : template matchers and linear predictor ( kernel machine ) ; ( c ) type - 123 : simple trainable basis functions and linear predictor ( neural net with one hidden layer , rbf network ) .
traditional perceptrons , like many currently popular learning models , are shal - low architectures .
different types of shallow architectures are represented in gure 123
type - 123 architectures have xed preprocessing in the rst layer ( e . g . , perceptrons ) .
type - 123 architectures have template matchers in the rst layer ( e . g . , kernel machines ) .
type - 123 architectures have simple trainable basis functions in the rst layer ( e . g . , neural net with one hidden layer , rbf network ) .
all three have a linear transformation in the
123 . 123 shallow architecture type 123
fixed pre - processing plus linear predictor , gure 123 ( a ) : the simplest shallow archi - tecture is composed of a xed preprocessing layer ( sometimes called features or ba - sis functions ) , followed by a linear predictor .
the type of linear predictor used , and the way it is trained is unspecied ( maximum - margin , logistic regression , perceptron ,
vector : f ( x ) = pk
squared error regression . . . . ) .
the family f is linearly parameterized in the parameter i=123 wii ( x ) .
this type of architecture is widely used in practi - cal applications .
since the pre - processing is xed ( and hand - crafted ) , it is necessarily task - specic in practice .
it is possible to imagine a shallow type - 123 machine that would parameterize the complete ai - set .
for example , we could imagine a machine in which each feature is a member of the ai - set , hence each particular member of the ai - set can be represented with a weight vector containing all zeros , except for a single 123 at the right place .
while there probably exist more compact ways to linearly parame - terize the entire ai - set , the number of necessary features would surely be prohibitive .
more importantly , we do not know explicitly the functions of the ai - set , so this is not
123 . 123 shallow architecture type 123
f ( x ) = b + pn
template matchers plus linear predictor , gure 123 ( b ) : next on the scale of adaptability is the traditional kernel machine architecture .
the preprocessing is a vector of values resulting from the application of a kernel function k ( x , xi ) to each training sample i=123 ik ( x , xi ) , where n is the number of training samples , the pa - rameter w contains all the i and the bias b .
in effect , the rst layer can be seen as a series of template matchers in which the templates are the training samples .
type - 123 architectures can be seen as special forms of type - 123 architectures in which the features are data - dependent , which is to say i ( x ) = k ( x , xi ) .
this is a simple form of unsu - pervised learning , for the rst layer .
through the famous kernel trick ( see ( scholkopf et al . , 123 ) ) , type - 123 architectures can be seen as a compact way of representing type - 123 architectures , including some that may be too large to be practical .
if the kernel function satises the mercer condition it can be expressed as an inner product between feature vectors k ( x , xi ) =< ( x ) , ( xi ) > , giving us a linear relation between the
parameter vectors in both formulations : w for type - 123 architectures ispi i ( xi )
very attractive feature of such architectures is that for several common loss functions ( e . g . , squared error , margin loss ) training them involves a convex optimization program .
while these properties are largely perceived as the magic behind kernel methods , they should not distract us from the fact that the rst layer of a kernel machine is often just a series of template matchers .
in most kernel machines , the kernel is used as a kind of template matchers , but other choices are possible .
using task - specic prior knowledge , one can design a kernel that incorporates the right abstractions for the task .
this comes at the cost of lower efciency in terms of human labor .
when a kernel acts like a template matcher , we call it local : k ( x , xi ) discriminates between values of x that are near xi and those that are not .
some of the mathematical results in this chapter focus on the gaussian kernel , where nearness corresponds to small euclidean distance .
one could say that one of the main issues with kernel machine with local kernels is that they are little more than template matchers .
it is possible to use kernels that are non - local yet not task - specic , such as the linear kernels and polynomial ker - nels .
however , most practitioners have been prefering linear kernels or local kernels .
linear kernels are type - 123 shallow architectures , with their obvious limitations .
local kernels have been popular because they make intuitive sense ( it is easier to insert prior knowledge ) , while polynomial kernels tend to generalize very poorly when extrapo -
lating ( e . g . , grossly overshooting ) .
the smoothness prior implicit in local kernels is quite reasonable for a lot of the applications that have been considered , whereas the prior implied by polynomial kernels is less clear .
learning the kernel would move us to type - 123 shallow architectures or deep architectures described below .
123 . 123 shallow architecture type 123
simple trainable basis functions plus linear predictor , gure 123 ( c ) : in type - 123 shallow architectures , the rst layer consists of simple basis functions that are trainable through supervised learning .
this can improve the efciency of the function representation , by tuning the basis functions to a task .
simple trainable basis functions include linear combinations followed by point - wise non - linearities and gaussian radial - basis func - tions ( rbf ) .
traditional neural networks with one hidden layer , and rbf networks belong to that category .
kernel machines in which the kernel function is learned ( and simple ) also belong to the shallow type - 123 category .
many boosting algorithms belong to this class as well .
unlike with types 123 and 123 , the output is a non - linear function of the parameters to be learned .
hence the loss functions minimized by learning are likely to be non - convex in the parameters .
the denition of type - 123 architectures is somewhat fuzzy , since it relies on the ill - dened concept of simple parameterized
we should immediately emphasize that the boundary between the various cate - gories is somewhat fuzzy .
for example , training the hidden layer of a one - hidden - layer neural net ( a type - 123 shallow architecture ) is a non - convex problem , but one could imag - ine constructing a hidden layer so large that all possible hidden unit functions would be present from the start .
only the output layer would need to be trained .
more specif - ically , when the number of hidden units becomes very large , and an l123 regularizer is used on the output weights , such a neural net becomes a kernel machine , whose kernel has a simple form that can be computed analytically ( bengio et al . , 123b ) .
if we use the margin loss this becomes an svm with a particular kernel .
although convexity is only achieved in the mathematical limit of an innite number of hidden units , we conjecture that optimization of single - hidden - layer neural networks becomes easier as the number of hidden units becomes larger .
if single - hidden - layer neural nets have any advantage over svms , it is that they can , in principle , achieve similar performance with a smaller rst layer ( since the parameters of the rst layer can be optimized for
note also that our mathematical results on local kernel machines are limited in scope , and most are derived for specic kernels such as the gaussian kernel , or for local kernels ( in the sense of k ( u , v ) being near zero when ||u v|| becomes large ) .
however , the arguments presented below concerning the shallowness of kernel ma - chines are more general .
123 . 123 deep architectures
deep architectures are compositions of many layers of adaptive non - linear components , in other words , they are cascades of parameterized non - linear modules that contain trainable parameters at all levels .
deep architectures allow the representation of wide
families of functions in a more compact form than shallow architectures , because they can trade space for time ( or breadth for depth ) while making the time - space product smaller , as discussed below .
the outputs of the intermediate layers are akin to interme - diate results on the way to computing the nal output .
features produced by the lower layers represent lower - level abstractions , that are combined to form high - level features at the next layer , representing higher - level abstractions .
123 the depth - breadth tradeoff
any specic function can be implemented by a suitably designed shallow architec - ture or by a deep architecture .
similarly , when parameterizing a family of functions , we have the choice between shallow or deep architectures .
the important questions are : 123
how large is the corresponding architecture ( with how many parameters , how much computation to produce the output ) ; 123
how much manual labor is involved in specializing the architecture to the task .
using a number of examples , we shall demonstrate that deep architectures are often more efcient ( in terms of number of computational components and parameters ) for representing common functions .
formal analyses of the computational complexity of shallow circuits can be found in hastad ( 123 ) or allender ( 123 ) .
they point in the same direction : shallow circuits are much less expressive than deep ones .
let us rst consider the task of adding two n - bit binary numbers .
the most natural circuit involves adding the bits pair by pair and propagating the carry .
the carry prop - agation takes o ( n ) steps , and also o ( n ) hardware resources .
hence the most natural architecture for binary addition is a deep one , with o ( n ) layers and o ( n ) elements .
a shallow architecture can implement any boolean formula expressed in disjunctive normal form ( dnf ) , by computing the minterms ( and functions ) in the rst layer , and the subsequent or function using a linear classier ( a threshold gate ) with a low threshold .
unfortunately , even for simple boolean operations such as binary addition and multiplication , the number of terms can be extremely large ( up to o ( 123n ) for n - bit inputs in the worst case ) .
the computer industry has in fact devoted a considerable amount of effort to optimize the implementation of exponential boolean functions , but the largest it can put on a single chip has only about 123 input bits ( a 123 - gbit ram chip , as of 123 ) .
this is why practical digital circuits , e . g . , for adding or multiplying two numbers are built with multiple layers of logic gates : their 123 - layer implementation ( akin to a lookup table ) would be prohibitively expensive .
see ( utgoff and stracuzzi , 123 ) for a previous discussion of this question in the context of learning architectures .
another interesting example is the boolean parity function .
the n - bit boolean
parity function can be implemented in at least ve ways :
( 123 ) with n daisy - chained xor gates ( an n - layer architecture or a recurrent circuit
with one xor gate and n time steps ) ;
( 123 ) with n 123 xor gates arranged in a tree ( a log123 n layer architecture ) , for a total
of o ( n log n ) components;
( 123 ) a dnf formula with o ( 123n ) minterms ( two layers ) .
architecture 123 has high depth and low breadth ( small amount of computing elements ) , architecture 123 is a good tradeoff between depth and breadth , and architecture 123 has high breadth and low depth .
if one allows the use of multi - input binary threshold gates ( linear classiers ) in addition to traditional logic gates , two more architectures are possible ( minsky and papert , 123 ) :
( 123 ) a 123 - layer architecture constructed as follows .
the rst layer has n binary thresh - old gates ( linear classiers ) in which unit i adds the input bits and subtracts i , hence computing the predicate xi = ( sum of bits i ) .
the second layer contains ( n 123 ) / 123 and gates that compute ( xian d ( n ot xi+123 ) ) for all i that are odd .
the last layer is a simple or gate .
( 123 ) a 123 - layer architecture in which the rst layer is identical to that of the 123 - layer ar - chitecture above , and the second layer is a linear threshold gate ( linear classier ) where the weight for input xi is equal to ( 123 ) i .
the fourth architecture requires a dynamic range ( accuracy ) on the weight linear in n , while the last one requires a dynamic range exponential in n .
a proof that n - bit parity requires o ( 123n ) gates to be represented by a depth - 123 boolean circuit ( with and , not and or gates ) can be found in ajtai ( 123 ) .
in theorem 123 ( section 123 . 123 ) we state a similar result for learning architectures : an exponential number of terms is required with a gaussian kernel machine in order to represent the parity function .
in many instances , space ( or breadth ) can be traded for time ( or depth ) with considerable
these negative results may seem reminiscent of the classic results in minsky and paperts book perceptrons ( minsky and papert , 123 ) .
this should come as no surprise : shallow architectures ( particularly of type 123 and 123 ) fall into minsky and paperts general denition of a perceptron and are subject to many of its limitations .
another interesting example in which adding layers is benecial is the fast fourier transform algorithm ( fft ) .
since the discrete fourier transform is a linear operation , it can be performed by a matrix multiplication with n 123 complex multiplications , which can all be performed in parallel , followed by o ( n 123 ) additions to collect the sums .
however the fft algorithm can reduce the total cost to 123 123 n log123 n , multiplications , with the tradeoff of requiring log123 n sequential steps involving n 123 multiplications each .
this example shows that , even with linear functions , adding layers allows us to take advantage of the intrinsic regularities in the task .
because each variable can be either absent , present , or negated in a minterm , there are m = 123n different possible minterms when the circuit has n inputs .
the set of all possible dnf formulae with k minterms and n inputs has c ( m , k ) elements ( the number of combinations of k elements from m ) .
clearly that set ( which is associated with the set of functions representable with k minterms ) grows very fast with k .
going from k 123 to k minterms increases the number of combinations by a factor ( m k ) / k .
when k is not close to m , the size of the set of dnf formulae is exponential in the number of inputs n .
these arguments would suggest that only an exponentially ( in n ) small fraction of all boolean functions require a less than exponential number of
we claim that most functions that can be represented compactly by deep architec - tures cannot be represented by a compact shallow architecture .
imagine representing the logical operations over k layers of a logical circuit into a dnf formula .
the op - erations performed by the gates on each of the layers are likely to get combined into a number of minterms that could be exponential in the original number of layers .
to see this , consider a k layer logical circuit where every odd layer has and gates ( with the option of negating arguments ) and every even layer has or gates .
every and - or consecutive layers corresponds to a sum of products in modulo - 123 arithmetic .
the whole circuit is the composition of k / 123 such sums of products , and it is thus a deep factoriza - tion of a formula .
in general , when a factored representation is expanded into a single sum of products , one gets a number of terms that can be exponential in the number of levels .
a similar phenomenon explains why most compact dnf formulae require an exponential number of terms when written as a conjuctive normal form ( cnf ) formula .
a survey of more general results in computational complexity of boolean cir - cuits can be found in allender ( 123 ) .
for example , hastad ( 123 ) show that for all k , there are depth k + 123 circuits of linear size that require exponential size to simulate with depth k circuits .
this implies that most functions representable compactly with a deep architecture would require a very large number of components if represented with a shallow one .
hence restricting ourselves to shallow architectures unduly limits the spectrum of functions that can be represented compactly and learned efciently ( at least in a statistical sense ) .
in particular , highly - variable functions ( in the sense of hav - ing high frequencies in their fourier spectrum ) are difcult to represent with a circuit of depth 123 ( linial et al . , 123 ) .
the results that we present in section 123 yield a similar conclusion : representing highly - variable functions with a gaussian kernel machine is
123 the limits of matching global templates
before diving into the formal analysis of local models , we compare the kernel machines ( type - 123 architectures ) with deep architectures using examples .
one of the fundamental problems in pattern recognition is how to handle intra - class variability .
taking the ex - ample of letter recognition , we can picture the set of all the possible images of the letter e on a 123 123 pixel grid as a set of continuous manifolds in the pixel space ( e . g . , a manifold for lower case and one for cursive ) .
the es on a manifold can be continu - ously morphed into each other by following a path on the manifold .
the dimensionality of the manifold at one location corresponds to the number of independent distortions that can can be applied to an image while preserving its category .
for handwritten let - ter categories , the manifold has a high dimension : letters can be distorted using afne transforms ( 123 parameters ) , distorted using an elastic sheet deformation ( high dimen - sion ) , or modied so as to cover the range of possible writing styles , shapes , and stroke widths .
even for simple character images , the manifold is very non - linear , with high curvature .
to convince ourselves of that , consider the shape of the letter w .
any pixel in the lower half of the image will go from white to black and white again four times as the w is shifted horizontally within the image frame from left to right .
this is the sign of a highly non - linear surface .
moreover , manifolds for other character categories are closely intertwined .
consider the shape of a capital u and an o at the same location .
they have many pixels in common , many more pixels in fact than with a shifted ver - sion of the same u .
hence the distance between the u and o manifolds is smaller than the distance between two us shifted by a few pixels .
another insight about the high curvature of these manifolds can be obtained from the example in gure 123 : the tangent vector of the horizontal translation manifold changes abruptly as we translate the im - age only one pixel to the right , indicating high curvature .
as discussed in section 123 , many kernel algorithms make an implicit assumption of a locally smooth function ( e . g . , locally linear in the case of svms ) around each training example xi .
hence a high cur - vature implies the necessity of a large number of training examples in order to cover all the desired twists and turns with locally constant or locally linear pieces .
this brings us to what we perceive as the main shortcoming of template - based methods : a very large number of templates may be required in order to cover each manifold with enough templates to avoid misclassications .
furthermore , the number of necessary templates can grow exponentially with the intrinsic dimension of a class - invariant manifold .
the only way to circumvent the problem with a type - 123 architec - ture is to design similarity measures for matching templates ( kernel functions ) such that two patterns that are on the same manifold are deemed similar .
unfortunately , devising such similarity measures , even for a problem as basic as digit recognition , has proved difcult , despite almost 123 years of active research .
furthermore , if such a good task - specic kernel were nally designed , it may be inapplicable to other classes
to further illustrate the situation , consider the problem of detecting and identifying a simple motif ( say , of size s = 123 pixels ) that can appear at d different locations in a uniformly white image with n pixels ( say 123 pixels ) .
to solve this problem , a simple kernel - machine architecture would require one template of the motif for each possi - ble location .
this requires n . d elementary operations .
an architecture that allows for spatially local feature detectors would merely require s . d elementary operations .
we should emphasize that this spatial locality ( feature detectors that depend on pixels within a limited radius in the image plane ) is distinct from the locality of kernel func - tions ( feature detectors that produce large values only for input vectors that are within a limited radius in the input vector space ) .
in fact , spatially local feature detectors have non - local response in the space of input vectors , since their output is independent of the input pixels they are not connected to .
a slightly more complicated example is the task of detecting and recognizing a pattern composed of two different motifs .
each motif occupies s pixels , and can appear at d different locations independently of each other .
a kernel machine would need a separate template for each possible occurrence of the two motifs , i . e . , n . d123 computing elements .
by contrast , a properly designed type - 123 architecture would merely require a set of local feature detectors for all the positions of the rst motifs , and a similar set for the second motif .
the total amount of elementary operations is a mere 123s . d .
we do not know of any kernel that would allow to efciently handle compositional structures .
an even more dire situation occurs if the background is not uniformly white , but can contain random clutter .
a kernel machine would probably need many different templates containing the desired motifs on top of many different backgrounds .
by con - trast , the locally - connected deep architecture described in the previous paragraph will handle this situation just ne .
we have veried this type of behavior experimentally
( see examples in section 123 ) .
these thought experiments illustrate the limitations of kernel machines due to the fact that their rst layer is restricted to matching the incoming patterns with global tem - plates .
by contrast , the type - 123 architecture that uses spatially local feature detectors handles the position jitter and the clutter easily and efciently .
both architectures are shallow , but while each kernel function is activated in a small area of the input space , the spatially local feature detectors are activated by a huge ( n s ) - dimensional sub - space of the input space ( since they only look at s pixels ) .
deep architectures with spatially - local feature detectors are even more efcient ( see section 123 ) .
hence the lim - itations of kernel machines are not just due to their shallowness , but also to the local character of their response function ( local in input space , not in the space of image
123 fundamental limitation of local learning
a large fraction of the recent work in statistical machine learning has focused on non - parametric learning algorithms which rely solely , explicitly or implicitly , on a smoothness prior .
a smoothness prior favors functions f such that when x x , f ( x ) f ( x ) .
additional prior knowledge is expressed by choosing the space of the data and the particular notion of similarity between examples ( typically expressed as a kernel function ) .
this class of learning algorithms includes most instances of the kernel machine algorithms ( scholkopf et al . , 123 ) , such as support vector machines ( svms ) ( boser et al . , 123 , cortes and vapnik , 123 ) or gaussian processes ( williams and rasmussen , 123 ) , but also unsupervised learning algorithms that attempt to cap - ture the manifold structure of the data , such as locally linear embedding ( roweis and saul , 123 ) , isomap ( tenenbaum et al . , 123 ) , kernel pca ( scholkopf et al . , 123 ) , laplacian eigenmaps ( belkin and niyogi , 123 ) , manifold charting ( brand , 123 ) , and spectral clustering algorithms ( see weiss ( 123 ) for a review ) .
more recently , there has also been much interest in non - parametric semi - supervised learning algo - rithms , such as zhu et al .
( 123 ) , zhou et al .
( 123 ) , belkin et al .
( 123 ) , delalleau et al .
( 123 ) , which also fall in this category , and share many ideas with manifold
since this is a large class of algorithms and one that continues to attract attention , it is worthwhile to investigate its limitations .
since these methods share many char - acteristics with classical non - parametric statistical learning algorithms such as the k - nearest neighbors and the parzen windows regression and density estimation algo - rithms ( duda and hart , 123 ) which have been shown to suffer from the so - called curse of dimensionality , it is logical to investigate the following question : to what ex - tent do these modern kernel methods suffer from a similar problem ? see ( hardle et al . , 123 ) for a recent and easily accessible exposition of the curse of dimensionality for classical non - parametric methods .
to explore this question , we focus on algorithms in which the learned function is expressed in terms of a linear combination of kernel functions applied on the training
f ( x ) = b +
where we have included an optional bias term b .
the set d = ( z123 , .
, zn ) contains training examples zi = xi for unsupervised learning , zi = ( xi , yi ) for supervised learning .
target value yi can take a special missing value for semi - supervised learning .
the is are scalars chosen by the learning algorithm using d , and kd ( , ) is the ker - nel function , a symmetric function ( sometimes expected to be positive semi - denite ) , which may be chosen by taking into account all the xis .
a typical kernel function is the gaussian kernel ,
k ( u , v ) = e 123
with the width controlling how local the kernel is .
see bengio et al .
( 123 ) to see that lle , isomap , laplacian eigenmaps and other spectral manifold learning algorithms such as spectral clustering can be generalized and written in the form of eq .
123 for a test point x , but with a different kernel ( that is data - dependent , generally performing a kind of normalization of a data - independent kernel ) .
one obtains the consistency of classical non - parametric estimators by appropriately varying the hyper - parameter that controls the locality of the estimator as n increases .
basically , the kernel should be allowed to become more and more local , so that statis - tical bias goes to zero , but the effective number of examples involved in the estimator at x ( equal to k for the k - nearest neighbor estimator ) should increase as n increases , so that statistical variance is also driven to 123
for a wide class of kernel regression estimators , the unconditional variance and squared bias can be shown to be written as follows ( hardle et al . , 123 ) :
expected error =
nd + c123 ,
with c123 and c123 not depending on n nor on the dimension d .
hence an optimal band - 123+d , and the resulting generalization error ( not count - width is chosen proportional to n ing the noise ) converges in n123 / ( 123+d ) , which becomes very slow for large d .
consider for example the increase in number of examples required to get the same level of error , in 123 dimension versus d dimensions .
if n123 is the number of examples required to get a particular level of error , to get the same level of error in d dimensions requires on the order of n ( 123+d ) / 123 examples , i . e . , the required number of examples is exponential in d .
for the k - nearest neighbor classier , a similar result is obtained ( snapp and venkatesh ,
expected error = e +
where e is the asymptotic error , d is the dimension and n the number of examples .
note however that , if the data distribution is concentrated on a lower dimensional manifold , it is the manifold dimension that matters .
for example , when data lies on
a smooth lower - dimensional manifold , the only dimensionality that matters to a k - nearest neighbor classier is the dimensionality of the manifold , since it only uses the euclidean distances between the near neighbors .
many unsupervised and semi - supervised learning algorithms rely on a graph with one node per example , in which nearby examples are connected with an edge weighted by the euclidean distance be - tween them .
if data lie on a low - dimensional manifold then geodesic distances in this graph approach geodesic distances on the manifold ( tenenbaum et al . , 123 ) , as the number of examples increases .
however , convergence can be exponentially slower for
123 minimum number of bases required
in this section we present results showing the number of required bases ( hence of train - ing examples ) of a kernel machine with gaussian kernel may grow linearly with the number of variations of the target function that must be captured in order to achieve a given error level .
123 . 123 result for supervised learning
the following theorem highlights the number of sign changes that a gaussian kernel machine can achieve , when it has k bases ( i . e . , k support vectors , or at least k training
theorem 123 ( theorem 123 of schmitt ( 123 ) ) .
let f : r r computed by a gaussian kernel machine ( eq .
123 ) with k bases ( non - zero is ) .
then f has at most 123k zeros .
we would like to say something about kernel machines in rd , and we can do this simply by considering a straight line in rd and the number of sign changes that the solution function f can achieve along that line .
corollary 123
suppose that the learning problem is such that in order to achieve a given error level for samples from a distribution p with a gaussian kernel machine ( eq .
123 ) , then f must change sign at least 123k times along some straight line ( i . e . , in the case of a classier , the decision surface must be crossed at least 123k times by that straight line ) .
then the kernel machine must have at least k bases ( non - zero is ) .
a proof can be found in bengio et al .
( 123a ) .
example 123
consider the decision surface shown in gure 123 , which is a sinusoidal function .
one may take advantage of the global regularity to learn it with few pa - rameters ( thus requiring few examples ) , but with an afne combination of gaussians , corollary 123 implies one would need at least m 123 = 123 gaussians .
for more complex tasks in higher dimension , the complexity of the decision surface could quickly make learning impractical when using such a local kernel method .
of course , one only seeks to approximate the decision surface s , and does not necessarily need to learn it perfectly : corollary 123 says nothing about the existence of an easier - to - learn decision surface approximating s .
for instance , in the example of
figure 123 : the dotted line crosses the decision surface 123 times : one thus needs at least 123 gaussians to learn it with an afne combination of gaussians with same width .
gure 123 , the dotted line could turn out to be a good enough estimated decision surface if most samples were far from the true decision surface , and this line can be obtained with only two gaussians .
the above theorem tells us that in order to represent a function that locally varies a lot , in the sense that its sign along a straight line changes many times , a gaussian kernel machine requires many training examples and many computational elements .
note that it says nothing about the dimensionality of the input space , but we might expect to have to learn functions that vary more when the data is high - dimensional .
the next theorem conrms this suspicion in the special case of the d - bits parity function :
parity : ( b123 , .
, bd ) ( 123 , 123 ) d 123 ( cid : 123 ) 123 if pd
i=123 bi is even
learning this apparently simple function with gaussians centered on points in ( 123 , 123 ) d is actually difcult , in the sense that it requires a number of gaussians exponential in d ( for a xed gaussian width ) .
note that our corollary 123 does not apply to the d - bits parity function , so it represents another type of local variation ( not along a line ) .
however , it is also possible to prove a very strong result for parity .
theorem 123
let f ( x ) = b +p123d
i=123 ik ( xi , x ) be an afne combination of gaussians with same width centered on points xi xd .
if f solves the parity problem , then there are at least 123d123 non - zero coefcients i .
a proof can be found in bengio et al .
( 123a ) .
the bound in theorem 123 is tight , since it is possible to solve the parity problem with exactly 123d123 gaussians and a bias , for instance by using a negative bias and putting a
positive weight on each example satisfying parity ( xi ) = 123
when trained to learn the parity function , a svm may learn a function that looks like the opposite of the parity on test points ( while still performing optimally on training points ) , but it is an artifact of the specic geometry of the problem , and only occurs when the training set size is appropriate compared to |xd| = 123d ( see bengio et al .
( 123 ) for details ) .
note that if the centers of the gaussians are not restricted anymore to be points in the training set ( i . e . , a type - 123 shallow architecture ) , it is possible to solve the parity problem with only d + 123 gaussians and no bias ( bengio et al . , 123 ) .
one may argue that parity is a simple discrete toy problem of little interest .
but even if we have to restrict the analysis to discrete samples in ( 123 , 123 ) d for mathematical reasons , the parity function can be extended to a smooth function on the ( 123 , 123 ) d hyper - cube depending only on the continuous sum b123 + .
theorem 123 is thus a basis to argue that the number of gaussians needed to learn a function with many variations in a continuous space may scale linearly with the number of these variations , and thus possibly exponentially in the dimension .
123 . 123 results for semi - supervised learning
in this section we focus on algorithms of the type described in recent papers ( zhu et al . , 123 , zhou et al . , 123 , belkin et al . , 123 , delalleau et al . , 123 ) , which are graph - based , non - parametric , semi - supervised learning algorithms .
note that transductive svms ( joachims , 123 ) , which are another class of semi - supervised algorithms , are already subject to the limitations of corollary 123
the graph - based algorithms we con - sider here can be seen as minimizing the following cost function , as shown in delalleau et al .
( 123 ) :
c ( y ) = k yl ylk123 + y l y + k y k123
with y = ( y123 , .
, yn ) the estimated labels on both labeled and unlabeled data , and l the ( un - normalized ) graph laplacian matrix , derived through l = d123 / 123w d123 / 123 from a kernel function k between points such that the gram matrix w , with wij = k ( xi , xj ) , corresponds to the weights of the edges in the graph , and d is a diagonal
matrix containing in - degree : dii = pj wij .
here , yl = ( y123 , .
, yl ) is the vector
of estimated labels on the l labeled examples , whose known labels are given by yl = ( y123 , .
, yl ) , and one may constrain yl = yl as in zhu et al .
( 123 ) by letting 123
we dene a region with constant label as a connected subset of the graph where all nodes xi have the same estimated label ( sign of yi ) , and such that no other node can be added while keeping these properties .
minimization of the cost criterion of eq .
123 can also be seen as a label propagation algorithm , i . e . , labels are spread around labeled examples , with nearness being dened by the structure of the graph , i . e . , by the kernel .
an intuitive view of label propagation suggests that a region of the manifold near a labeled ( e . g . , positive ) example will be entirely labeled positively , as the example spreads its inuence by propagation on the graph representing the underlying manifold .
thus , the number of regions with constant label should be on the same order as ( or less than ) the number of labeled examples .
this is easy to see in the case of a sparse gram matrix w .
we dene a region with constant label as a connected subset of the graph where all nodes xi have the same
estimated label ( sign of yi ) , and such that no other node can be added while keeping these properties .
the following proposition then holds ( note that it is also true , but trivial , when w denes a fully connected graph ) .
proposition 123
after running a label propagation algorithm minimizing the cost of eq .
123 , the number of regions with constant estimated label is less than ( or equal to ) the number of labeled examples .
a proof can be found in bengio et al .
( 123a ) .
the consequence is that we will need at least as many labeled examples as there are variations in the class , as one moves by small steps in the neighborhood graph from one contiguous region of same label to an - other .
again we see the same type of non - parametric learning algorithms with a local kernel , here in the case of semi - supervised learning : we may need about as many la - beled examples as there are variations , even though an arbitrarily large number of these variations could have been characterized more efciently than by their enumeration .
123 smoothness versus locality : curse of dimensionality
consider a gaussian svm and how that estimator changes as one varies , the hyper - parameter of the gaussian kernel .
for large one would expect the estimated function to be very smooth , whereas for small one would expect the estimated function to be very local , in the sense discussed earlier : the near neighbors of x have dominating inuence in the shape of the predictor at x .
the following proposition tells us what happens when is large , or when we con -
sider what a ball whose radius is small compared to .
proposition 123
for the gaussian kernel classier , as increases and becomes large compared with the diameter of the data , within the smallest sphere containing the data
the decision surface becomes linear ifpi i = 123 ( e . g . , for svms ) , or else the normal
vector of the decision surface becomes a linear combination of two sphere surface normal vectors , with each sphere centered on a weighted average of the examples of the corresponding class .
a proof can be found in bengio et al .
( 123a ) .
note that with this proposition we see clearly that when becomes large , a kernel classier becomes non - local ( it approaches a linear classier ) .
however , this non - locality is at the price of constraining the decision surface to be very smooth , making it difcult to model highly varying decision surfaces .
this is the essence of the trade - off between smoothness and locality in many similar non - parametric models ( including the classical ones such as k - nearest - neighbor and parzen windows algorithms ) .
now consider in what senses a gaussian kernel machine is local ( thinking about small ) .
consider a test point x that is near the decision surface .
we claim that the orientation of the decision surface is dominated by the neighbors xi of x in the training set , making the predictor local in its derivative .
if we consider the i xed ( i . e . , ignoring their dependence on the training xis ) , then it is obvious that the prediction f ( x ) is dominated by the near neighbors xi of x , since k ( x , xi ) 123 quickly when ||x xi|| / becomes large .
however , the i can be inuenced by all the xjs .
the following proposition skirts that issue by looking at the rst derivative of f .
figure 123 : for local manifold learning algorithms such as lle , isomap and kernel pca , the manifold tangent plane at x is in the span of the difference vectors between test point x and its neighbors xi in the training set .
this makes these algorithms sensitive to the curse of dimensionality , when the manifold is high - dimensional and not very at .
proposition 123
for the gaussian kernel classier , the normal of the tangent of the decision surface at x is constrained to approximately lie in the span of the vectors ( x xi ) with ||x xi|| not large compared to and xi in the training set .
sketch of the proof
a point x of the decision surface is
the estimator is f ( x ) =pi ik ( x , xi ) .
the normal vector of the tangent plane at
123 k ( x , xi ) .
each term is a vector proportional to the difference vector xix .
this sum is dominated by the terms with ||x xi|| not large compared to .
we are thus left with f ( x ) approximately in the span of the difference vectors x xi with xi a near neighbor of x .
the i being only scalars , they only inuence the weight of each neighbor xi in that linear combination .
hence although f ( x ) can be inuenced by xi far from x , the decision surface near x has a normal vector that is constrained to approximately lie in the span of the vectors x xi with xi near x .
q . e . d .
the constraint of f ( x )
x being in the span of the vectors x xi for neighbors xi of x is not strong if the manifold of interest ( e . g . , the region of the decision surface with high density ) has low dimensionality .
indeed if that dimensionality is smaller or equal to the number of dominating neighbors , then there is no constraint at all .
how - ever , when modeling complex dependencies involving many factors of variation , the region of interest may have very high dimension ( e . g . , consider the effect of variations that have arbitrarily large dimension , such as changes in clutter , background , etc
images ) .
for such a complex highly - varying target function , we also need a very local predictor ( small ) in order to accurately represent all the desired variations .
with a small , the number of dominating neighbors will be small compared to the dimension of the manifold of interest , making this locality in the derivative a strong constraint , and allowing the following curse of dimensionality argument .
this notion of locality in the sense of the derivative allows us to dene a ball around each test point x , containing neighbors that have a dominating inuence on f ( x ) smoothness within that ball constrains the decision surface to be approximately either linear ( case of svms ) or a particular quadratic form ( the decision surface normal vector is a linear combination of two vectors dened by the center of mass of examples of each class ) .
let n be the number of such balls necessary to cover the region where the value of the estimator is desired ( e . g . , near the target decision surface , in the case of classication problems ) .
let k be the smallest number such that one needs at least k examples in each ball to reach error level .
the number of examples thus required is kn .
to see that n can be exponential in some dimension , consider the maximum radius r of all these balls and the radius r of .
if has intrinsic dimension d , then n could be as large as the number of radius - r balls that can tile a d - dimensional manifold
of radius r , which is on the order of ( cid : 123 ) r
in bengio et al .
( 123 ) we present similar results that apply to unsupervised learn - ing algorithms such as non - parametric manifold learning algorithms ( roweis and saul , 123 , tenenbaum et al . , 123 , scholkopf et al . , 123 , belkin and niyogi , 123 ) .
we nd that when the underlying manifold varies a lot in the sense of having high curva - ture in many places , then a large number of examples is required .
note that the tangent plane of the manifold is dened by the derivatives of the kernel machine function f , for such algorithms .
the core result is that the manifold tangent plane at x is dominated by terms associated with the near neighbors of x in the training set ( more precisely it is constrained to be in the span of the vectors x xi , with xi a neighbor of x ) .
this idea is illustrated in gure 123
in the case of graph - based manifold learning algorithms such as lle and isomap , the domination of near examples is perfect ( i . e . , the derivative is strictly in the span of the difference vectors with the neighbors ) , because the kernel im - plicit in these algorithms takes value 123 for the non - neighbors .
with such local manifold learning algorithms , one needs to cover the manifold with small enough linear patches with at least d + 123 examples per patch ( where d is the dimension of the manifold ) .
this argument was previously introduced in bengio and monperrus ( 123 ) to describe the limitations of neighborhood - based manifold learning algorithms .
an example that illustrates that many interesting manifolds can have high curvature is that of translation of high - contrast images , shown in gure 123
the same argument applies to the other geometric invariances of images of objects .
123 deep architectures
the analyzes in the previous sections point to the difculty of learning highly - varying functions .
these are functions with a large number of variations ( twists and turns ) in the domain of interest , e . g . , they would require a large number of pieces to be well - represented by a piecewise - linear approximation .
since the number of pieces can be
figure 123 : the manifold of translations of a high - contrast image has high curvature .
a smooth manifold is obtained by considering that an image is a sample on a discrete grid of an intensity function over a two - dimensional space .
the tangent vector for translation is thus a tangent image , and it has high values only on the edges of the ink .
the tangent plane for an image translated by only one pixel looks similar but changes abruptly since the edges are also shifted by one pixel .
hence the two tangent planes are almost orthogonal , and the manifold has high curvature , which is bad for local learning methods , which must cover the manifold with many small linear patches to correctly capture its shape .
made to grow exponentially with the number of input variables , this problem is directly connected with the well - known curse of dimensionality for classical non - parametric learning algorithms ( for regression , classication and density estimation ) .
if the shapes of all these pieces are unrelated , one needs enough examples for each piece in order to generalize properly .
however , if these shapes are related and can be predicted from each other , non - local learning algorithms have the potential to generalize to pieces not covered by the training set .
such ability would seem necessary for learning in complex domains such as in the ai - set .
one way to represent a highly - varying function compactly ( with few parameters ) is through the composition of many non - linearities .
such multiple composition of non - linearities appear to grant non - local properties to the estimator , in the sense that the value of f ( x ) or f ( x ) can be strongly dependent on training examples far from xi while at the same time allowing to capture a large number of variations .
we have al - ready discussed parity and other examples ( section 123 ) that strongly suggest that the learning of more abstract functions is much more efcient when it is done sequentially , by composing previously learned concepts .
when the representation of a concept re - quires an exponential number of elements , ( e . g . , with a shallow circuit ) , the number of
training examples required to learn the concept may also be impractical .
gaussian processes , svms , log - linear models , graph - based manifold learning and graph - based semi - supervised learning algorithms can all be seen as shallow architec - tures .
although multi - layer neural networks with many layers can represent deep cir - cuits , training deep networks has always been seen as somewhat of a challenge .
until very recently , empirical studies often found that deep networks generally performed no better , and often worse , than neural networks with one or two hidden layers ( tesauro , 123 ) .
a notable exception to this is the convolutional neural network architecture ( le - cun et al . , 123 , lecun et al . , 123 ) discussed in the next section , that has a sparse con - nectivity from layer to layer .
despite its importance , the topic of deep network training has been somewhat neglected by the research community .
however , a promising new method recently proposed by hinton et al .
( 123 ) is causing a resurgence of interest in
a common explanation for the difculty of deep network learning is the presence of local minima or plateaus in the loss function .
gradient - based optimization meth - ods that start from random initial conditions appear to often get trapped in poor local minima or plateaus .
the problem seems particularly dire for narrow networks ( with few hidden units or with a bottleneck ) and for networks with many symmetries ( i . e . , fully - connected networks in which hidden units are exchangeable ) .
the solution re - cently introduced by hinton et al .
( 123 ) for training deep layered networks is based on a greedy , layer - wise unsupervised learning phase .
the unsupervised learning phase provides an initial conguration of the parameters with which a gradient - based super - vised learning phase is initialized .
the main idea of the unsupervised phase is to pair each feed - forward layer with a feed - back layer that attempts to reconstruct the input of the layer from its output .
this reconstruction criterion guarantees that most of the information contained in the input is preserved in the output of the layer .
the resulting architecture is a so - called deep belief networks ( dbn ) .
after the initial unsupervised training of each feed - forward / feed - back pair , the feed - forward half of the network is rened using a gradient - descent based supervised method ( back - propagation ) .
this training strategy holds great promise as a principle to break through the problem of training deep networks .
upper layers of a dbn are supposed to represent more abstract concepts that explain the input observation x , whereas lower layers extract low - level features from x .
lower layers learn simpler concepts rst , and higher layers build on them to learn more abstract concepts .
this strategy has not yet been much exploited in machine learning , but it is at the basis of the greedy layer - wise constructive learning algorithm for dbns .
more precisely , each layer is trained in an unsupervised way so as to capture the main features of the distribution it sees as input .
it produces an internal representation for its input that can be used as input for the next layer .
in a dbn , each layer is trained as a restricted boltzmann machine ( teh and hinton , 123 ) using the contrastive divergence ( hinton , 123 ) approximation of the log - likelihood gradient .
the outputs of each layer ( i . e . , hidden units ) constitute a factored and distributed rep - resentation that estimates causes for the input of the layer .
after the layers have been thus initialized , a nal output layer is added on top of the network ( e . g . , predicting the class probabilities ) , and the whole deep network is ne - tuned by a gradient - based optimization of the prediction error .
the only difference with an ordinary multi - layer neural network resides in the initialization of the parameters , which is not random , but
is performed through unsupervised training of each layer in a sequential fashion .
experiments have been performed on the mnist and other datasets to try to un - derstand why the deep belief networks are doing much better than either shallow networks or deep networks with random initialization .
these results are reported and discussed in ( bengio et al . , 123 ) .
several conclusions can be drawn from these exper - iments , among which the following , of particular interest here :
similar results can be obtained by training each layer as an auto - associator in - stead of a restricted boltzmann machine , suggesting that a rather general prin - ciple has been discovered .
test classication error is signicantly improved with such greedy layer - wise unsupervised initialization over either a shallow network or a deep network with the same architecture but with random initialization .
in all cases many possible hidden layer sizes were tried , and selected based on validation error .
when using a greedy layer - wise strategy that is supervised instead of unsuper - vised , the results are not as good , probably because it is too greedy : unsupervised feature learning extracts more information than strictly necessary for the predic - tion task , whereas greedy supervised feature learning ( greedy because it does not take into account that there will be more layers later ) extracts less information than necessary , which prematurely scuttles efforts to improve by adding layers .
the greedy layer - wise unsupervised strategy helps generalization mostly be -
cause it helps the supervised optimization to get started near a better solution .
123 experiments with visual pattern recognition
one essential question when designing a learning architecture is how to represent in - variance .
while invariance properties are crucial to any learning task , it is particularly apparent in visual pattern recognition .
in this section we consider several experiments in handwriting recognition and object recognition to illustrate the relative advantages and disadvantages of kernel methods , shallow architectures , and deep architectures .
123 representing invariance
the example of gure 123 shows that the manifold containing all translated versions of a character image has high curvature .
because the manifold is highly varying , a classier that is invariant to translations ( i . e . , that produces a constant output when the input moves on the manifold , but changes when the input moves to another class manifold ) needs to compute a highly varying function .
as we showed in the previous section , template - based methods are inefcient at representing highly - varying functions .
the number of such variations may increase exponentially with the dimensionality of the manifolds where the input density concentrates .
that dimensionality is the number of dimensions along which samples within a category can vary .
we will now describe two sets of results with visual pattern recognition .
the rst part is a survey of results obtained with shallow and deep architectures on the mnist
dataset , which contains isolated handwritten digits .
the second part analyzes results of experiments with the norb dataset , which contains objects from ve different generic categories , placed on uniform or cluttered backgrounds .
for visual pattern recognition , type - 123 architectures have trouble handling the wide variability of appearance in pixel images that result from variations in pose , illumi - nation , and clutter , unless an impracticably large number of templates ( e . g . , support vectors ) are used .
ad - hoc preprocessing and feature extraction can , of course , be used to mitigate the problem , but at the expense of human labor .
here , we will concentrate on methods that deal with raw pixel data and that integrate feature extraction as part of the learning process .
123 convolutional networks
convolutional nets are multi - layer architectures in which the successive layers are de - signed to learn progressively higher - level features , until the last layer which represents categories .
all the layers are trained simultaneously to minimize an overall loss func - tion .
unlike with most other models of classication and pattern recognition , there is no distinct feature extractor and classier in a convolutional network .
all the layers are similar in nature and trained from data in an integrated fashion .
the basic module of a convolutional net is composed of a feature detection layer followed by a feature pooling layer .
a typical convolutional net is composed of one , two or three such detection / pooling modules in series , followed by a classication module .
the input state ( and output state ) of each layer can be seen as a series of two - dimensional retinotopic arrays called feature maps .
at layer i , the value cijxy produced by the j - th feature detection layer at position ( x , y ) in the j - th feature map is computed by applying a series of convolution kernels wijk to feature maps in the previous layer ( with index i 123 ) , and passing the result through a hyperbolic tangent
cijxy = tanh bij +xk
where pi and qi are the width and height of the convolution kernel .
the convolution kernel parameters wijkpq and the bias bij are subject to learning .
a feature detection layer can be seen as a bank of convolutional lters followed by a point - wise non - linearity .
each lter detects a particular feature at every location on the input .
hence spatially translating the input of a feature detection layer will translate the output but leave it otherwise unchanged .
translation invariance is normally built - in by constrain - ing wijkpq = wijkp q for all p , p , q , q , i . e . , the same parameters are used at different
a feature pooling layer has the same number of features in the map as the feature detection layer that precedes it .
each value in a subsampling map is the average ( or the max ) of the values in a local neighborhood in the corresponding feature map in the previous layer .
that average or max is added to a trainable bias , multiplied by a trainable coefcient , and the result is passed through a non - linearity ( e . g . , the tanh function ) .
the windows are stepped without overlap .
therefore the maps of a feature
figure 123 : the architecture of the convolutional net used for the norb experiments .
the input is an image pair , the system extracts 123 feature maps of size 123 123 , 123 maps of 123 123 , 123 maps of 123 123 , 123 maps of 123 123 , and 123 dimensional feature vector .
the feature vector is then transformed into a 123 - dimensional vector in the last layer to compute the distance with target vectors .
pooling layer are less than the resolution of the maps in the previous layer .
the role of the pooling layer is build a representation that is invariant to small variations of the positions of features in the input .
alternated layers of feature detection and feature pooling can extract features from increasingly large receptive elds , with increasing robustness to irrelevant variabilities of the inputs .
the last module of a convolutional network is generally a one - or two - layer neural net .
training a convolutional net can be performed with stochastic ( on - line ) gradient descent , computing the gradients with a variant of the back - propagation method .
while convolutional nets are deep ( generally 123 to 123 layers of non - linear functions ) , they do not seem to suffer from the convergence problems that plague deep fully - connected neural nets .
while there is no denitive explanation for this , we suspect that this phenomenon is linked to the heavily constrained parameterization , as well as to the asymmetry of
convolutional nets are being used commercially in several widely - deployed sys - tems for reading bank check ( lecun et al . , 123 ) , recognizing handwriting for tablet - pc , and for detecting faces , people , and objects in videos in real time .
123 the lessons from mnist
mnist is a dataset of handwritten digits with 123 , 123 training samples and 123 , 123 test samples .
digit images have been size - normalized so as to t within a 123 123 pixel window , and centered by center of mass in a 123 123 eld .
with this procedure , the position of the characters vary slightly from one sample to another .
numerous authors have reported results on mnist , allowing precise comparisons among methods .
a small subset of relevant results is listed in table 123
not all good results on mnist are listed in the table .
in particular , results obtained with deslanted images or with
hand - designed feature extractors were left out .
results are reported with three convolutional net architectures : lenet - 123 , lenet - 123 , and the subsampling convolutional net of ( simard et al . , 123 ) .
the input eld is a 123 123 pixel map in which the 123 123 images are centered .
in lenet - 123 ( lecun et al . , 123 ) , the rst feature detection layer produces 123 feature maps of size 123 123 using 123 123 convolution kernels .
the rst feature pooling layer produces 123 123 123 feature maps through a 123 123 subsampling ratio and 123 123 receptive elds .
the second feature detection layer produces 123 feature maps of size 123 123 using 123 123 convolution kernels , and is followed by a pooling layer with 123 123 subsampling .
the next layer produces 123 feature maps of size 123 123 using 123 123 convolution kernels .
the last layer produces 123 feature maps ( one per output category ) .
lenet - 123 has a very similar architecture , but the number of feature maps at each level are much larger : 123 feature maps in the rst layer , 123 in the third layer , and 123 feature maps in the penultimate
the convolutional net in ( simard et al . , 123 ) is somewhat similar to the original one in ( lecun et al . , 123 ) in that there is no separate convolution and subsampling layers .
each layer computes a convolution with a subsampled result ( there is no feature pooling operation ) .
their simple convolutional network has 123 features at the rst layer , with 123 by 123 kernels and 123 by 123 subsampling , 123 features at the second layer , also with 123 by 123 kernels and 123 by 123 subsampling , 123 features at the third layer with 123 by 123 kernels , and 123 output units .
the mnist samples are highly variable because of writing style , but have little variation due to position and scale .
hence , it is a dataset that is particularly favorable for template - based methods .
yet , the error rate yielded by support vector machines with gaussian kernel ( 123% error ) is only marginally better than that of a considerably smaller neural net with a single hidden layer of 123 hidden units ( 123% as reported by ( simard et al . , 123 ) ) , and similar to the results obtained with a 123 - layer neural net as reported in ( hinton et al . , 123 ) ( 123% error ) .
the best results on the original mnist set with a knowledge free method was reported in ( hinton et al . , 123 ) ( 123% error ) , using a deep belief networkby knowledge - free method , we mean a method that has no prior knowledge of the pictorial nature of the signal .
those methods would produce exactly the same result if the input pixels were scrambled with a xed permutation .
convolutional nets use the pictorial nature of the data , and the invariance of cate - gories to small geometric distortions .
it is a broad ( low complexity ) prior , which can be specied compactly ( with a short piece of code ) .
yet it brings about a considerable reduction of the ensemble of functions that can be learned .
the best convolutional net on the unmodied mnist set is lenet - 123 , which yields a record 123% .
as with hintons results , this result was obtained by initializing the lters in the rst layer us - ing an unsupervised algorithm , prior to training with back - propagation ( ranzato et al . , 123 ) .
the same lenet - 123 trained purely supervised from random initialization yields 123% error .
a smaller convolutional net , lenet - 123 yields 123% .
the same network was reported to yield 123% in ( lecun et al . , 123 ) with a smaller number of training
when the training set is augmented with elastically distorted versions of the training samples , the test error rate ( on the original , non - distorted test set ) drops signicantly .
a conventional 123 - layer neural network with 123 hidden units yields 123% error ( simard
123 - layer nn , 123 hid .
units 123 - layer nn , 123+123 units svm , gaussian kernel unsupervised stacked rbm + backprop convolutional network lenet - 123 convolutional network lenet - 123 conv .
lenet - 123 + unsup .
learning training set augmented with afne distortions 123 - layer nn , 123 hid .
units virtual svm , deg .
123 poly training set augmented with elastic distortions 123 - layer nn , 123 hid .
units svm gaussian ker .
+ on - line training shape context features + elastic k - nn conv .
lenet - 123 conv .
lenet - 123 + unsup .
learning
simard et al .
123 hinton et al .
123 cortes et al .
123 hinton et al
ranzato et al .
123 ranzato et al .
123 ranzato et al
simard et al .
123 decoste et al .
123 simard et al
simard et al .
123 this volume , chapter 123 belongie et al .
123 simard et al .
123 ranzato et al .
123 ranzato et al
table 123 : test error rates of various learning models on the mnist dataset .
many results obtained with deslanted images or hand - designed feature extractors were left
et al . , 123 ) .
while svms slightly outperform 123 - layer neural nets on the undistorted set , the advantage all but disappears on the distorted set .
in this volume , loosli et al .
report 123% error with a gaussian svm and a sample selection procedure .
the number of support vectors in the resulting svm is considerably larger than 123
convolutional nets applied to the elastically distorted set achieve between 123% and 123% error , depending on the architecture , the loss function , and the number of training epochs .
simard et al .
( 123 ) reports 123% with a subsampling convolutional net .
ranzato et al .
( 123 ) report 123% using lenet - 123 with random initialization , and 123% using lenet - 123 with unsupervised pre - training of the rst layer .
this is the best error rate ever reported on the original mnist test set .
hence a deep network , with small dose of prior knowledge embedded in the archi - tecture , combined with a learning algorithm that can deal with millions of examples , goes a long way towards improving performance .
not only do deep networks yield lower error rates , they are faster to run and faster to train on large datasets than the best
figure 123 : the 123 testing objects in the normalized - uniform norb set .
the testing objects are unseen by the trained system .
123 the lessons from norb
while mnist is a useful benchmark , its images are simple enough to allow a global template matching scheme to perform well .
natural images of 123d objects with back - ground clutter are considerably more challenging .
norb ( lecun et al . , 123 ) is a publicly available dataset of object images from 123 generic categories .
it contains im - ages of 123 different toys , with 123 toys in each of the 123 generic categories : four - legged animals , human gures , airplanes , trucks , and cars .
the 123 objects are split into a training set with 123 objects , and a test set with the remaining 123 object ( see examples in figure 123 ) .
each object is captured by a stereo camera pair in 123 different views ( 123 elevations , 123 azimuths ) under 123 different illuminations .
two datasets derived from norb are used .
the rst dataset , called the normalized - uniform set , are images of a single object with a normalized size placed at the center of images with uniform background .
the training set has 123 , 123 stereo image pairs of size 123 , and another 123 , 123 for testing ( from different object instances ) .
the second set , the jittered - cluttered set , contains objects with randomly perturbed
positions , scales , in - plane rotation , brightness , and contrast .
the objects are placed on highly cluttered backgrounds and other norb objects placed on the periphery .
a 123 - th category of images is included : background images containing no objects .
some examples images of this set are shown in gure 123
each image in the jittered - cluttered set is randomly perturbed so that the objects are at different positions ( ( - 123 , +123 ) pixels horizontally and vertically ) , scales ( ratio in ( 123 , 123 ) ) , image - plane angles ( ( 123 , 123 ) ) , brightness ( ( - 123 , 123 ) shifts of gray scale ) , and contrasts ( ( 123 , 123 ) gain ) .
the central object could be occluded by the randomly placed distractor .
to generate the training set , each image was perturbed with 123 different congurations of the above parameters , which makes up 123 , 123 image pairs of size 123
the testing set has 123 drawings of perturbations per image , and contains 123 , 123 pairs .
in the norb datasets , the only useful and reliable clue is the shape of the object , while all the other parameters that affect the appearance are subject to variation , or are designed to contain no useful clue .
parameters that are subject to variation are : viewing angles ( pose ) , lighting conditions .
potential clues whose impact was elimi - nated include : color ( all images are grayscale ) , and object texture .
for specic object recognition tasks , the color and texture information may be helpful , but for generic recognition tasks the color and texture information are distractions rather than useful clues .
by preserving natural variabilities and eliminating irrelevant clues and system - atic biases , norb can serve as a benchmark dataset in which no hidden regularity that would unfairly advantage some methods over others can be used .
a six - layer net dubbed lenet - 123 , shown in gure 123 , was used in the experiments with the norb dataset reported here .
the architecture is essentially identical to that of lenet - 123 and lenet - 123 , except of the sizes of the feature maps .
the input is a pair of 123 gray scale images .
the rst feature detection layer uses twelve 123 convolution kernels to generate 123 feature maps of size 123 123
the rst 123 maps take input from the left image , the next two from the right image , and the last 123 from both .
there are 123 trainable parameters in this layer .
the rst feature pooling layer uses a 123 subsampling , to produce 123 feature maps of size 123 123
the second feature detection layer uses 123 convolution kernels of size 123 to output 123 feature maps of size 123 123
each map takes input from 123 monocular maps and 123 binocular maps , each with a different combination , as shown in gure 123
this conguration is used to combine features from the stereo image pairs .
this layer contains 123 , 123 trainable parameters .
the next pooling layer uses a 123 subsampling which outputs 123 feature maps of size 123 123
the next layer has 123 123 convolution kernels to produce 123 feature maps of size 123 123 , and the last layer has 123 units .
in the experiments , we also report results using a hybrid method , which consists in training the convolutional network in the conventional way , chopping off the last layer , and training a gaussian kernel svm on the output of the penultimate layer .
many of the results in this section were previously reported in ( huang and lecun , 123 ) .
123 results on the normalized - uniform set
table 123 shows the results on the smaller norb dataset with uniform background .
this dataset simulates a scenario in which objects can be perfectly segmented from the background , and is therefore rather unrealistic .
fraction of s . v .
123% 123% 123%
step size =
123 - 123
table 123 : testing error rates and training / testing timings on the normalized - uniform dataset of different methods .
the timing is normalized to hypothetical 123ghz single cpu .
the convolutional nets have multiple results with different training passes due to
the svm is composed of ve binary svms that are trained to classify one object category against all other categories .
the convolutional net trained on this set has a smaller penultimate layer with 123 outputs .
the input features to the svm of the hybrid system are accordingly 123 - dimensional vectors .
the timing gures in table 123 represent the cpu time on a ctitious 123ghz cpu .
the results of the convolutional net trained after 123 , 123 , 123 passes are listed in the table .
the network is slightly over - trained with more than 123 passes ( no regularization was used in the experiment ) .
the svm in the hybrid system is trained over the features extracted from the network trained with 123 passes .
the improvement of the combination is marginal over the convolutional net alone .
despite the relative simplicity of the task ( no position variation , uniform back - grounds , only 123 types of illuminations ) , the svm performs rather poorly .
interestingly , it require a very large amount of cpu time for training and testing .
the convolutional net reaches the same error rate as the svm with 123 times less training time .
further training halves the error rate .
it is interesting that despite its deep architecture , its non - convex loss , the total absence of explicit regularization , and a lack of tight gener - alization bounds , the convolutional net is both better and faster than an svm .
123 results on the jittered - cluttered set
the results on this set are shown in table 123
to classify the 123 categories , 123 binary ( one vs .
others ) svm sub - classiers are trained independently , each with the full set of 123 , 123 samples .
the training samples are raw 123 123 pixel image pairs turned into a 123 , 123 - dimensional input vector , with values between 123 to 123
svms have relatively few free parameters to tune prior to learning .
in the case of gaussian kernels , one can choose ( gaussian kernel sizes ) and c ( penalty coefcient ) that yield best results by grid tuning .
a rather disappointing test error rate of 123% is obtained on this set , as shown in the rst column of table 123
the training time depends
123% 123% 123% 123%
step size =
123 - 123
table 123 : testing error rates and training / testing timings on the jittered - cluttered dataset of different methods .
the timing is normalized to hypothetical 123ghz single cpu .
the convolutional nets have multiple results with different training passes due to its iterative
heavily on the value of for gaussian kernel svms .
the experiments are run on a 123 - cpu ( 123ghz ) cluster , and the timing information is normalized into a hypothetical 123ghz single cpu to make the measurement meaningful .
for the convolutional net lenet - 123 , we listed results after different number of passes ( 123 , 123 , 123 ) and their timing information .
the test error rate attens out at 123% after about 123 passes .
no signicant over - training was observed , and no early stopping was performed .
one parameter controlling the training procedure must be heuristically cho - sen : the global step size of the stochastic gradient procedure .
best results are obtained by adopting a schedule in which this step size is progressively decreased .
a full propagation of one data sample through the network requires about 123 mil - lion multiply - add operations .
parallelizing the convolutional net is relatively simple since multiple convolutions can be performed simultaneously , and each convolution can be performed independently on sub - regions of the layers .
the convolutional nets are computationally very efcient .
the training time scales sublinearly with dataset size in practice , and the testing can be done in real - time at a rate of a few frames per
the third column shows the result of a hybrid system in which the last layer of the convolutional net was replaced by a gaussian svm after training .
the training and testing features are extracted with the convolutional net trained after 123 passes .
the penultimate layer of the network has 123 outputs , therefore the features are 123 - dimensional .
the svms applied on features extracted from the convolutional net yield an error rate of 123% , a signicant improvement over either method alone .
by incorpo - rating a learned feature extractor into the kernel function , the svm was indeed able to leverage both the ability to use low - level spatially local features and at the same time keep all the advantages of a large margin classier .
the poor performance of svm with gaussian kernels on raw pixels is not unex - pected .
as we pointed out in previous sections , a gaussian kernel svm merely com - putes matching scores ( based on euclidean distance ) between the incoming pattern and
templates from the training set .
this global template matching is very sensitive to vari - ations in registration , pose , and illumination .
more importantly , most of the pixels in a norb image are actually on the background clutter , rather than on the object to be recognized .
hence the template matching scores are dominated by irrelevant variabili - ties of the background .
this points to a crucial deciency of standard kernel methods : their inability to select relevant input features , and ignore irrelevant ones .
svms have presumed advantages provided by generalization bounds , capacity con - trol through margin maximization , a convex loss function , and universal approximation properties .
by contrast , convolutional nets have no generalization bounds ( beyond the most general vc bounds ) , no explicit regularization , a highly non - convex loss func - tion , and no claim to universality .
yet the experimental results with norb show that convolutional nets are more accurate than gaussian svms by a factor of 123 , faster to train by a large factor ( 123 to 123 ) , and faster to run by a factor of 123
this work was motivated by our requirements for learning algorithms that could ad - dress the challenge of ai , which include statistical scalability , computational scala - bility and human - labor scalability .
because the set of tasks involved in ai is widely diverse , engineering a separate solution for each task seems impractical .
we have explored many limitations of kernel machines and other shallow architectures .
such architectures are inefcient for representing complex , highly - varying functions , which we believe are necessary for ai - related tasks such as invariant perception .
one limitation was based on the well - known depth - breadth tradeoff in circuits de - sign hastad ( 123 ) .
this suggests that many functions can be much more efciently represented with deeper architectures , often with a modest number of levels ( e . g . , log - arithmic in the number of inputs ) .
the second limitation regards mathematical consequences of the curse of dimen - sionality .
it applies to local kernels such as the gaussian kernel , in which k ( x , xi ) can be seen as a template matcher .
it tells us that architectures relying on local kernels can be very inefcient at representing functions that have many variations , i . e . , func - tions that are not globally smooth ( but may still be locally smooth ) .
indeed , it could be argued that kernel machines are little more than souped - up template matchers .
a third limitation pertains to the computational cost of learning .
in theory , the con - vex optimization associated with kernel machine learning yields efcient optimization and reproducible results .
unfortunately , most current algorithms are ( at least ) quadratic in the number of examples .
this essentially precludes their application to very large - scale datasets for which linear - or sublinear - time algorithms are required ( particularly for on - line learning ) .
this problem is somewhat mitigated by recent progress with on - line algorithms for kernel machines ( e . g . , see ( bordes et al . , 123 ) ) , but there remains the question of the increase in the number of support vectors as the number of examples
a fourth and most serious limitation , which follows from the rst ( shallowness ) and second ( locality ) pertains to inefciency in representation .
shallow architectures and local estimators are simply too inefcient ( in terms of required number of examples and
adaptable components ) to represent many abstract functions of interest .
ultimately , this makes them unaffordable if our goal is to learn the ai - set .
we do not mean to suggest that kernel machines have no place in ai .
for example , our results suggest that com - bining a deep architecture with a kernel machine that takes the higher - level learned representation as input can be quite powerful .
learning the transformation from pixels to high - level features before applying an svm is in fact a way to learn the kernel .
we do suggest that machine learning researchers aiming at the ai problem should investi - gate architectures that do not have the representational limitations of kernel machines , and deep architectures are by denition not shallow and usually not local as well .
until recently , many believed that training deep architectures was too difcult an optimization problem .
however , at least two different approaches have worked well in training such architectures : simple gradient descent applied to convolutional net - works ( lecun et al . , 123 , lecun et al . , 123 ) ( for signals and images ) , and more recently , layer - by - layer unsupervised learning followed by gradient descent ( hinton et al . , 123 , bengio et al . , 123 , ranzato et al . , 123 ) .
research on deep architectures is in its infancy , and better learning algorithms for deep architectures remain to be dis - covered .
taking a larger perspective on the objective of discovering learning principles that can lead to ai has been a guiding perspective of this work .
we hope to have helped inspire others to seek a solution to the problem of scaling machine learning towards ai .
we thank geoff hinton for our numerous discussions with him and the neural com - putation and adaptive perception program of the canadian institute of advanced re - search for making them possible .
we wish to thank fu - jie huang for conducting much of the experiments in section 123 , and hans - peter graf and eric cosatto and their collab - orators for letting us use their parallel implementation of svm .
we thank leon bottou for his patience and for helpful comments .
we thank sumit chopra , olivier delalleau , raia hadsell , hugo larochelle , nicolas le roux , marcaurelio ranzato , for helping us to make progress towards the ideas presented here .
this project was supported by nsf grants no .
123 and no .
123 , by nserc , the canada research chairs , and the mitacs nce .
