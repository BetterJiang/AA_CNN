recently , many applications for restricted boltzmann machines ( rbms ) have been de - veloped for a large variety of learning prob - lems .
however , rbms are usually used as feature extractors for another learning al - gorithm or to provide a good initialization for deep feed - forward neural network clas - siers , and are not considered as a stand - alone solution to classication problems .
in this paper , we argue that rbms provide a self - contained framework for deriving com - petitive non - linear classiers .
we present an evaluation of dierent learning algorithms for rbms which aim at introducing a discrimi - native component to rbm training and im - prove their performance as classiers .
this approach is simple in that rbms are used directly to build a classier , rather than as a stepping stone .
finally , we demonstrate how discriminative rbms can also be successfully employed in a semi - supervised setting .
restricted boltzmann machines ( rbms ) ( smolensky , 123 ) are generative models based on latent ( usually binary ) variables to model an input distribution , and have seen their applicability grow to a large variety of problems and settings in the past few years .
from binary inputs , they have been extended to model var - ious types of input distributions ( welling et al . , 123; hinton et al . , 123 ) .
conditional versions of rbms have also been developed for collaborative ltering ( salakhutdinov et al . , 123 ) and to model motion cap - ture data ( taylor et al . , 123 ) and video sequences ( sutskever & hinton , 123 ) .
rbms have been particularly successful in classica - tion problems either as feature extractors for text and
appearing in proceedings of the 123 th international confer - ence on machine learning , helsinki , finland , 123
copy - right 123 by the author ( s ) / owner ( s ) .
image data ( gehler et al . , 123 ) or as a good initial training phase for deep neural network classiers ( hin - ton , 123 ) .
however , in both cases , the rbms are merely the rst step of another learning algorithm , ei - ther providing a preprocessing of the data or an initial - ization for the parameters of a neural network .
when trained in an unsupervised fashion , rbms provide no guarantees that the features implemented by their hid - den layer will ultimately be useful for the supervised task that needs to be solved .
more practically , model selection can also become problematic , as we need to explore jointly the space of hyper - parameters of both the rbm ( size of the hidden layer , learning rate , num - ber of training iterations ) and the supervised learning algorithm that is fed the learned features .
in partic - ular , having two separate learning phases ( feature ex - traction , followed by classier training ) can be prob - lematic in an online learning setting .
in this paper , we argue that rbms can be used suc - cessfully as stand - alone non - linear classiers along - side other standard classiers like neural networks and support vector machines , and not only as fea - ture extractors .
we investigate training objectives for rbms that are more appropriate for training clas - siers than the common generative objective .
we describe discriminative restricted boltzmann ma - chines ( drbms ) , i . e .
rbms that are trained more specically to be good classication models , and hy - brid discriminative restricted boltzmann machines ( hdrbms ) which explore the space between discrim - inative and generative learning and can combine their advantages .
we also demonstrate that rbms can be successfully adapted to the common semi - supervised learning setting ( chapelle et al . , 123 ) for classica - tion problems .
finally , the algorithms investigated in this paper are well suited for online learning on large
restricted boltzmann machines
restricted boltzmann machines are undirected gener - ative models that use a layer of hidden variables to model a distribution over visible variables .
though they are most often trained to only model the inputs
classication using discriminative restricted boltzmann machines
of a classication task , they can also model the joint distribution of the inputs and associated target classes ( e . g .
in the last layer of a deep belief network in hin - ton et al .
( 123 ) ) .
in this section , we will focus on such joint models .
we assume given a training set dtrain = ( ( xi , yi ) ) , comprising for the i - th example an input vector xi and a target class yi ( 123 , .
to train a generative model on such data we consider minimization of the
log p ( yi , xi ) .
an rbm with n hidden units is a parametric model of the joint distribution between a layer of hidden variables ( referred to as neurons or features ) h = ( h123 , .
, hn ) and the observed variables made of x = ( x123 , .
, xd ) and y , that takes the form p ( y , x , h ) ee ( y , x , h )
e ( y , x , h ) = ht wx bt x ct h dt ~ y ht u ~ y
with parameters = ( w , b , c , d , u ) and ~ y = i=123 for c classes .
this model is illustrated in figure 123
for now , we consider for simplicity binary input variables , but the model can be easily gener - alized to non - binary categories , continuous - valued inputs ( welling et al . , 123; hinton et al . , 123 ) .
it is straightforward to show that
p ( x|h ) = y p ( xi = 123|h ) = sigm ( bi +x y edy +p
j ujy hj
where sigm is the logistic sigmoid .
equations 123 and 123 illustrate that the hidden units are meant to capture predictive information about the input vector as well as the target class .
p ( h|y , x ) also has a similar form :
p ( h|y , x ) = y
p ( hj = 123|y , x ) = sigm ( cj + ujy +x
when the number of hidden variables is xed , an rbm can be considered a parametric model , but when it is allowed to vary with the data , it becomes a non - in particular , freund and haus - sler ( 123 ) ; le roux and bengio ( 123 ) showed that
figure 123
restricted boltzmann machine modeling the joint distribution of inputs and target classes
an rbm with enough hidden units can represent any distribution over binary vectors , and that adding hid - den units guarantees that a better likelihood can be achieved , unless the generated distribution already equals the training distribution .
in order to minimize the negative log - likelihood ( eq .
123 ) , we would like an estimator of its gradient with respect to the model parameters .
the exact gradient , for any parameter can be written as follows :
log p ( yi , xi )
e ( yi , xi , h )
e ( y , x , h )
though the rst expectation is tractable , the second one is not .
fortunately , there exists a good stochastic approximation of this gradient , called the contrastive divergence gradient ( hinton , 123 ) .
this approxima - tion replaces the expectation by a sample generated after a limited number of gibbs sampling iterations , with the samplers initial state for the visible variables initialized at the training sample ( yi , xi ) .
even when using only one gibbs sampling iteration , contrastive divergence has been shown to produce only a small bias for a large speed - up in training time ( carreira - perpinan & hinton , 123 ) .
online training of an rbm thus consists in cy - cling through the training examples and updating the rbms parameters according to algorithm 123 , where the learning rate is controlled by .
computing p ( y , x ) is intractable , but it is possible to compute p ( y|x ) , sample from it , or choose the most probable class under this model .
as shown in salakhutdinov et al .
( 123 ) , for reasonable numbers of classes c ( over which we must sum ) , this conditional distribution can be computed exactly and eciently , by writing it as follows :
( cid : 123 ) 123 + ecj +ujy+p ( cid : 123 ) 123 + ecj +ujy +p i wjixi ( cid : 123 ) .
123 123 123 123yxhuwy classication using discriminative restricted boltzmann machines
algorithm 123 training update for rbm over ( y , x ) using contrastive divergence
approach has been used previously for ne - tuning the top rbm of a deep belief network ( hinton , 123 ) .
input : training pair ( yi , xi ) and learning rate % notation : a b means a is set to value b a p means a is sampled from p
% positive phase
y123 yi , x123 xi , bh123 sigm ( c + w x123 + u ~ y123 ) bh123 sigm ( c + w x123 + u ~ y123 )
% negative phase h123 p ( h|y123 , x123 ) , y123 p ( y|h123 ) , x123 p ( x|h123 )
precomputing the terms cj +p them when computing qn
( cid : 123 ) 123 + ecj +ujy +p
i wjixi and reusing for all classes y permits to compute this conditional distribution in time o ( nd + nc ) .
discriminative restricted boltzmann
in a classication setting , one is ultimately only inter - ested in correct classication , not necessarily to have a good p ( x ) .
because our models p ( x ) can be in - appropriate , it can then be advantageous to optimize directly p ( y|x ) instead of p ( y , x ) :
we refer to rbms trained according to ldisc as dis - criminative rbms ( drbms ) .
since rbms ( with enough hidden units ) are universal approximators for binary inputs , it follows also that drbms are uni - versal approximators of conditional distributions with a drbm can be trained by contrastive divergence , as has been done in conditional rbms ( taylor et al . , 123 ) , but since p ( y|x ) can be computed exactly , we can compute the exact gradient :
where oyj ( x ) = cj +p
k wjkxk + ujy .
this gradient can be computed eciently and then used in a stochas - tic gradient descent optimization .
this discriminative
hybrid discriminative restricted
the advantage brought by discriminative training usu - ally depends on the amount of available training data .
smaller training sets tend to favor generative learn - ing and bigger ones favor discriminative learning ( ng & jordan , 123 ) .
however , instead of solely rely - ing on one or the other perspective , one can adopt a hybrid discriminative / generative approach simply by combining the respective training criteria .
though this method cannot be interpreted as a maximum like - lihood approach for a particular generative model as in lasserre et al .
( 123 ) , it proved useful here and elsewhere ( bouchard & triggs , 123 ) .
in this paper , we used the following criterion : lhybrid ( dtrain ) = ldisc ( dtrain ) + lgen ( dtrain )
where the weight of the generative criterion can be optimized ( e . g . , based on the validation set classica - tion error ) .
here , the generative criterion can also be seen as a data - dependent regularizer for a drbm .
we will refer to rbms trained using the criterion of equa - tion 123 as hybrid drbms ( hdrbms ) .
to train an hdrbm , we can use stochastic gradient descent and add for each example the gradient contri - bution due to ldisc with times the stochastic gradi - ent estimator associated with lgen for that example .
semi - supervised learning
a frequent classication setting is where there are few labeled training data but many unlabeled examples of inputs .
semi - supervised learning algorithms ( chapelle et al . , 123 ) address this situation by using the un - labeled data to introduce constraints on the trained model .
for example , for purely discriminative models , these constraints are often imposed on the decision sur - face of the model .
in the rbm framework , a natural constraint is to ask that the model be a good gener - ative model of the unlabeled data , which corresponds to the following objective :
where dunlab = ( ( xi ) ) |dunlab| contains unlabeled ex - amples of inputs .
to train on this objective , we can once again use a contrastive divergence approximation
classication using discriminative restricted boltzmann machines
of the log - likelihood gradient :
e ( yi , xi , h )
e ( y , x , h )
the contrastive divergence approximation is slightly dierent here .
the rst term can be computed in time o ( cn + nd ) , by noticing that it is equal to
e ( yi , xi , h )
one could either average the usual rbm gradient e ( yi , xi , h ) for each class y ( weighted by p ( y|xi ) ) , or
sample a y from p ( y|xi ) and only collect the gradient for that value of y .
in the sampling version , the online training update for this objective can be described by replacing the statement y123 yi with y123 p ( y|xi ) in algorithm 123
we used this version in our experiments .
in order to perform semi - supervised learning , we can weight and combine the objective of equation 123 with those of equations 123 , 123 or 123 lsemisup ( dtrain , dunlab ) = ltype ( dtrain ) ( 123 ) where type ( gen , disc , hybrid ) .
online training according to this objective simply consists in apply - ing the appropriate update for each training example , based on whether it is labeled or not .
related work
as mentioned earlier , rbms ( sometimes also referred to as harmoniums ( welling et al . , 123 ) ) have already been used successfully in the past to extract useful fea - tures for another supervised learning algorithm .
one of the main contributions of this paper lies in the demonstration that rbms can be used on their own without relying on another learning algorithm , and provide a self - contained framework for deriving com - in addition to ensuring that the features learned by the rbms hidden layer are dis - criminative , this approach facilitates model selection since the discriminative power of the hidden layer units ( or features ) can be tracked during learning by observ - ing the progression of classication error on a valida - tion set .
it also makes it easier to tackle online learning problems relatively to approaches where learning fea - tures ( hidden representation ) and learning to classify are done in two separate phases ( hinton et al . , 123; bengio et al . , 123 ) .
gehler et al .
( 123 ) ; xing et al .
( 123 ) have shown that the features learned by an rbm trained by ig - noring the labeled targets can be useful for retriev - ing documents or classifying images of objects .
how - ever , in both these cases , the extracted features were linear in the input , were not trained discriminatively and had to be fed to another learning algorithm which ultimately performed classication .
mccallum et al .
( 123 ) presented multi - conditional learning ( mcl ) 123 for harmoniums in order to introduce a discriminative component to harmoniums training , but the learned features still had to be fed to another learning algo - rbms can also provide a good initialization for the pa - rameters of neural network classiers ( hinton , 123 ) , however model selection issues arise , for instance when considering the appropriate number of learning up - dates and the magnitude of learning rates of each training phase .
it has also been argued that the gen - erative learning aspect of rbm training was a key ele - ment to their success as good starting points for neural network training ( bengio et al . , 123 ) , but the extent to which the nal solution for the parameters of the neural network is inuenced by generative learning is not well controlled .
hdrbms can be seen as a way of addressing this issue .
finally , though semi - supervised learning was never reported for rbms before , druck et al .
troduced semi - supervised learning in hybrid genera - tive / discriminative models using a similar approach to the one presented in section 123
however , they worked with log - linear models , whereas the rbms used here can perform non - linear classication .
log - linear mod - els depend much more on the discriminative quality of the features that are fed as input , whereas an rbm can learn useful features using their hidden variables , at the price of non - convex optimization .
we present experiments on two classication problems : character recognition and text classication .
in all ex - periments , we performed model selection on a valida - tion set before testing .
for the dierent rbm models , model selection123 consisted in nding good values for
123we experimented with a version of mcl for the rbms considered in this paper , however the results did not im - prove on those of hdrbms .
123model selection was done with a grid - like search over ( between 123 and 123 , on a log scale ) , n ( 123 to 123 ) , for hdrbms ( 123 to 123 , on a log scale ) and for semi - supervised learning ( 123 , 123 or 123 ) .
in general , bigger val - ues of n were found to be more appropriate with more generative learning .
if no local minima was apparent , the
classication using discriminative restricted boltzmann machines
the learning rate , the size of the hidden layer n and good weights for the dierent types of learning ( gener - ative and semi - supervised weights ) .
also , the number of iterations over the training set was determined using early stopping according to the validation set classi - cation error , with a look ahead of 123 iterations .
character recognition
we evaluated the dierent rbm models on the prob - lem of classifying images of digits .
the images were taken from the mnist dataset , where we separated the original training set into training and validation sets of 123 and 123 examples and used the stan - dard test set of 123 examples .
the results are given in table 123
the ordinary rbm model is trained generatively ( to model ( x , y ) ) , whereas rbm+nnet is an unsupervised rbm used to initialize a one - hidden layer supervised neural net ( as in ( bengio et al . , 123 ) ) .
we give as a comparison the results of a gaus - sian kernel svm and of a regular neural network ( ran - dom initialization , one hidden layer , hyperbolic tan - gent hidden activation functions ) .
first , we observe that a drbm outperforms a genera - tive rbm .
however , an hdrbm appears able to make the best out of discriminative and generative learning and outperforms the other models .
we also experimented with a sparse version of the hdrbm model , since sparsity is known to be a good characteristic for features of images .
sparse rbms were developed by lee et al .
( 123 ) in the context of deep neural networks .
to introduce sparsity in the hidden layer of an rbm in lee et al .
( 123 ) , after each iteration through the whole training set , the biases c in the hidden layer are set to a value that maintains the average of the conditional expected value of these neurons to an arbitrarily small value .
this procedure tends to make the biases negative and large .
we fol - low a dierent approach by simply subtracting a small constant value , considered as an hyper - parameter123 , from the biases after each update , which is more ap - propriate in an online setting or for large datasets .
this sparse version of hdrbms outperforms all the other rbm models , and yields signicantly lower clas -
grid was extended .
the biases b , c and d were initialized to 123 and the initial values for the elements of the weight matrices u and w were each taken from uniform samples
inm123 , m123 , where m is the maximum between the
number of rows and columns of the matrix .
123to chose , given the selected values for and for the non sparse hdrbm , we performed a second grid - search over ( 123 and 123 , on a log scale ) and the hidden layer size , testing bigger hidden layer sizes then previously
figure 123
filters learned by the hdrbm on the mnist dataset .
the top row shows lters that act as spatially lo - calized stroke detectors , and the bottom shows lters more specic to a particular shape of digit .
table 123
comparison of the classication performances on the mnist dataset .
svm results for mnist were taken from http : / / yann . lecun . com / exdb / mnist / .
on this dataset , dierences of 123% in classication error is statis -
rbm ( = 123 , n = 123 ) drbm ( = 123 , n = 123 ) hdrbm ( = 123 , = 123 , n = 123 ) sparse hdrbm ( idem + n = 123 , = 123 )
sication error then the svm and the standard neural network classiers .
the performance achieved by the sparse hdrbm is particularly impressive when com - pared to reported performances for deep belief net - works ( 123% in hinton et al .
( 123 ) ) or of a deep neural network initialized using rbms ( around 123% in bengio et al .
( 123 ) and hinton ( 123 ) ) for the mnist dataset with 123 training examples .
the discriminative power of the hdrbm can be better understood by looking a the rows of the weight matrix w , which act as lter features .
figure 123 displays some of these learned lters .
some of them are spatially localized stroke detectors which can possibly be active for a wide variety of digit images , and others are much more specic to a particular shape of digit .
document classication
we also evaluated the rbm models on the problem of classifying documents into their corresponding news - group topic .
we used a version of the 123 - newsgroup dataset123 for which the training and test sets contain documents collected at dierent times , a setting that is more reective of a practical application .
the orig - inal training set was divided into a smaller training
123this dataset is available in matlab format here :
classication using discriminative restricted boltzmann machines
set and a validation set , with 123 and 123 examples respectively .
the test set contains 123 examples .
we used the 123 most frequent words for the binary input features .
the results are given in figure 123 ( a ) .
we also provide the results of a gaussian kernel svm123 and of a regular neural network for comparison .
once again , hdrbm outperforms the other rbm models .
however , here the generatively trained rbm performs better then the drbms .
the hdrbm also outperforms the svm and neural network classiers .
in order to get a better understanding of how the hdrbm solves this classication problem , we rst looked at the weights connecting each of the classes to the hidden neurons .
this corresponds to the columns uy of the weight matrix u .
figure 123 ( b ) shows a sim - ilarity matrix m ( u ) for the weights of the dierent newsgroups , where m ( u ) y123y123 = sigm ( uty123uy123 ) .
we see that the hdrbm does not use dierent neurons for dierent newsgroups , but shares some of those neurons for newsgroups that are semantically related .
another interesting visualization of this characteristic is given in figure 123 ( c ) , where the columns of u were projected on their two principal components .
in both cases , we see that the hdrbm tends to share neurons for simi - lar topics , such as computer ( comp . * ) , science ( sci . * ) and politics ( talk . politics . * ) , or secondary topics such as sports ( rec . sports . * ) and other recreational activities ( rec . autos and rec . motorcycles ) .
table 123 also gives the set of words used by the hdrbm to recognize some of the newsgroups .
to obtain this table we proceeded as follows : for each newsgroup y , we looked at the 123 neurons with the largest weight among uy , aggregated ( by summing ) the associated input - to - hidden weight vectors , sorted the words in de - creasing order of their associated aggregated weights and picked the rst words according to that order .
this procedure attempts to approximate the positive contribution of the words to the conditional probabil - ity of each newsgroup .
semi - supervised learning
we evaluated our semi - supervised learning algorithm for the hdrbm on both the digit recognition and doc - ument classication problems .
we also experimented with a version ( noted mnist - bi ) of the mnist dataset proposed by larochelle et al .
background images have been added to mnist digit images .
this version corresponds to a much harder problem , but it will help to illustrate the advantage brought by semi - supervised learning in hdrbms
123we used libsvm v123 to train the svm model
hdrbm trained on this data used truncated exponen - tial input units ( see ( bengio et al . , 123 ) ) .
in this semi - supervised setting , we reduced the size of the labeled training set to 123 examples , and used some of the remaining data to form an unlabeled dataset dunlab .
the validation set was also reduced to 123 labeled examples .
model selection123 covered all the parameters of the hdrbm as well as the unsuper - vised objective weight of equation 123
for compar - ison purposes , we also provide the performance of a standard non - parametric semi - supervised learning al - gorithm based on function induction ( bengio et al . , 123b ) , which includes as a particular case or is very similar to other non - parametric semi - supervised learn - ing algorithms such as zhu et al .
( 123 ) .
we provide results for the use of a gaussian kernel ( np - gauss ) and a data - dependent truncated gaussian kernel ( np - trunc - gauss ) used in bengio et al .
( 123b ) , which es - sentially outputs zero for pairs of inputs that are not near neighbors .
the experiments on the mnist and mnist - bi ( with background images ) datasets used 123 unlabeled examples and the experiment on 123 - newsgroup used 123
the results are given in table 123 , where we observe that semi - supervised learning consis - tently improves the performance of the hdrbm .
the usefulness of non - parametric semi - supervised learning algorithms has been demonstrated many times in the past , but usually so on problems where the dimensionality of the inputs is low or the data lies on a much lower dimensional manifold .
this is reected in the result on mnist for the non - parametric meth - ods .
however , for high dimensional data with many factors of variation , these methods can quickly suer from the curse of dimensionality , as argued by bengio et al .
( 123a ) .
this is also reected in the results for the mnist - bi dataset which contains many factors of variation , and for the 123 - newsgroup dataset where the input is very high dimensional .
finally , it is important to notice that semi - supervised learning in hdrbms proceeds in an online fashion and hence could scale to very large datasets , unlike more standard non - parametric methods .
relationship with feed - forward neural
there are several similarities between discriminative rbms and neural networks .
in particular , the com - putation of p ( y|x ) could be implemented by a single layer neural network with softplus and softmax acti -
123 = 123 for mnist and 123 - newsgroup and = 123
for mnist - bi was found to perform best .
classication using discriminative restricted boltzmann machines
rbm ( = 123 , n = 123 ) drbm ( = 123 , n = 123 ) hdrbm ( = 123 , = 123 , n = 123 )
( a ) classication performances
( b ) similarity matrix
( c ) pca embedding
figure 123
experiment on 123 - newsgroup dataset .
( top left ) classication performance for the dierent models .
the error dierences between hdrbm and other models is statistically signicant .
( bottom left ) similarity matrix of the newsgroup weights vectors uy .
( right ) two dimensional pca embedding of the newsgroup weights .
table 123
most inuential words in the hdrbm for predicting some of the document classes
alt . atheism bible , atheists , benedikt , atheism , religion
sell , condition , oppy , week , am , obo sternlight , bontchev , nsa , escrow , hamburg
table 123
comparison of the classication errors in semi - supervised learning setting .
the errors in bold are statis - tically signicantly better .
semi - sup hdrbm 123%
vation functions in its hidden and output layers re - spectively , with a special structure in the output and hidden weights where the value of the output weights is xed and many of the hidden layer weights are shared .
the advantage of working in the framework of rbms is that it provides a natural way to introduce gener - ative learning , which we used here to derive a semi - supervised learning algorithm .
as mentioned earlier , a form of generative learning can be introduced in stan -
ti , ftp , window , gif , images , pixel cars , ford , autos , sho , toyota , roads rearms , handgun , rearm , gun , rkba
dard neural networks simply by using rbms to ini - tialize the hidden layer weights .
however the extent to which the nal solution for the parameters of the neural network is inuenced by generative learning is not well controlled .
this might explain the superior performance obtained by a hdrbm compared to a single hidden layer neural network initialized with an rbm ( rbm+nnet in the tables ) .
conclusion and future work
we argued that rbms can and should be used as stand - alone non - linear classiers alongside other stan - dard and more popular classiers , instead of merely being considered as simple feature extractors .
we eval - uated dierent training objectives that are more ap - propriate to train an rbm in a classication setting .
these discriminative versions of rbms integrate the process of discovering features of inputs with their use in classication , without relying on a separate classi -
classication using discriminative restricted boltzmann machines
this insures that the learned features are dis - criminative and facilitates model selection .
we also presented a novel but straightforward semi - supervised learning algorithm for rbms and demonstrated its usefulness for complex or high dimensional data .
for future work , we would like to investigate the use of discriminative versions of rbms in more challeng - ing settings such as in multi - task or structured out - put problems .
the analysis of the target weights for the 123 - newsgroup dataset seem to indicate that rbms would be good at capturing the conditional sta - tistical relationship between multiple tasks or in the components in a complex target space .
exact com - putation of the conditional distribution for the tar - get is not tractable anymore , but there exists promis - ing techniques such as mean - eld approximations that could estimate that distribution .
moreover , in the 123 - newsgroup experiment , we only used 123 words in input because generative training using algorithm 123 does not exploit the sparsity of the input , unlike an svm or a drbm ( since in that case the sparsity of the input makes the discriminative gradient sparse too ) .
motivated by this observation , we intend to explore ways to introduce generative learning in rbms and hdrbms which would be less computationally expen - sive when the input vectors are large but sparse .
we thank dumitru erhan for discussions about sparse rbms and anonymous reviewers for helpful comments .
