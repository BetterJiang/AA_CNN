recurrent networks ( crossreference chapter 123 ) can , in principle , use their feedback connections to store representations of recent input events in the form of activations .
the most widely used algorithms for learning what to put in short - term memory , however , take too much time to be feasible or do not work well at all , especially when minimal time lags between inputs and corresponding teacher signals are long .
although theoretically fascinat - ing , they do not provide clear practical advantages over , say , backprop in feedforward networks with limited time windows ( see crossreference chap - ters 123 and 123 ) .
with conventional algorithms based on the computation of the complete gradient , such as back - propagation through time ( bptt , e . g . , ( 123 , 123 , 123 ) ) or real - time recurrent learning ( rtrl , e . g . , ( 123 ) ) error signals owing backwards in time tend to either ( 123 ) blow up or ( 123 ) vanish : the temporal evolution of the backpropagated error exponentially depends on the size of the weights ( 123 , 123 ) .
case ( 123 ) may lead to oscillating weights , while in case ( 123 ) learning to bridge long time lags takes a prohibitive amount of time , or does not work at all .
in what follows , we give a theoretical analysis of this problem by study - ing the asymptotic behavior of error gradients as a function of time lags .
in section 123 , we consider the case of standard rnns and derive the main result using the approach rst proposed in ( 123 ) .
in section 123 , we consider the more general case of adaptive dynamical systems , which include , besides standard rnns , other recurrent architectures based on dierent connectivities and choices of the activation function ( e . g . , rbf or second order connections ) .
using the analysis reported in ( 123 ) we show that one of the following two un - desirable situations necessarily arise : either the system is unable to robustly store past information about its inputs , or gradients vanish exponentially .
finally , in section 123 we shortly review alternative optimization methods and architectures that have been suggested to improve learning in the presence of long - term dependencies .
123 exponential error decay
gradients of the error function
the results we are going to prove hold regardless of the particular kind of cost function used ( as long as its continuous in the output ) and regardless of
the particular algorithm which is employed to compute the gradient .
here we shortly explain how gradients are computed by the standard bptt algorithm ( e . g . , ( 123 ) , see also crossreference chapter 123 for more details ) because its analytical form is better suited to the forthcoming analyses .
the error at time t is denoted by e ( t ) .
considering only the error at
time t , output unit ks error signal is
and some unit js backpropagated error signal at time < t is
j ( ) = f ( cid : 123 )
wij i ( + 123 )
wij aj ( 123 )
neti ( ) =
is unit is current net input ,
ai ( ) = fi ( neti ( ) )
is the activation of a non - input unit i with dierentiable transfer function fi , and wij is the weight on the connection from unit j to i .
the corresponding contribution at time < t to wjls total weight update is j ( ) al ( 123 ) , where is the learning rate , and l stands for an arbitrary unit connected to
error path integral
suppose we have a fully connected net whose non - input unit indices range from 123 to n .
let us focus on local error ow from output unit k to arbitrary unit v ( later we will see that the analysis immediately extends to global error ow ) .
the error occurring at k at time step t is propagated back in time for t s time steps , to an arbitrary unit v at time s < t .
this scales the error by the following factor : v ( netv ( t 123 ) ) wkv
t s = 123 t s > 123
in order to solve the above recurrence , we will expand it by unrolling over time ( as done for example in deriving bptt ) .
in particular , for s < < t let l denote the index of a generic non input unit in the replica of the network at time .
moreover , let ls = v and lt = k .
we obtain :
l ( netl ( ) ) wl l123
( proof by induction ) .
it can be immediately shown that if the local error vanishes , then the
global error vanishes too .
to see this compute
where o denotes the set of output units .
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) > 123
intuitive explanation of equation ( 123 )
l ( netl ( ) ) wl l123
for all then the largest product increases exponentially with t s 123
that is , the error blows up , and conicting error signals ( with opposite signs ) arriving at unit v can lead to oscillating weights and unstable learning due to overshooting ( for error blow - ups or bifurcations see also ( 123 , 123 , 123 ) ) .
on the other hand , if for all , then the largest product decreases exponentially with t s 123
that is , the error vanishes , and nothing can be learned in acceptable time .
if al123 is constant and not equal to zero , then the size of the gradient
if fl is the logistic sigmoid function , then the maximal value of f ( cid : 123 )
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) takes on maximal values where
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) < 123
l ( netl ( ) ) wl l123
l ( netl ) wl l123
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , and it is less than 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) < 123 ( e . g . , if the absolute maximal weight value wmax is smaller
wl l123 =
the size of the derivative goes to zero for
than 123 ) .
hence with conventional logistic sigmoid transfer functions , the error ow tends to vanish as long as the weights have absolute values below 123 , especially in the beginning of the training phase .
in general the use of larger initial weights does not help though as seen above , for the relevant derivative goes to zero faster than the absolute weight can grow ( also , some weights may have to change their signs by crossing zero ) .
likewise , increasing the learning rate does not help either it does not change the ratio of long - range error ow and short - range error ow .
bptt is too sensitive to recent distractions .
note that since the summation terms in equation ( 123 ) may have dierent signs , increasing the number of units n does not necessarily increase error ow .
weak upper bound for scaling factor
the following , slightly extended vanishing error analysis also takes n , the number of units , into account .
for t s > 123 , formula ( 123 ) can be rewritten as
w f ( cid : 123 )
v ( netv ( s ) ) ,
k ) i : = ( w ) ki = wki , and f ( cid : 123 )
where the weight matrix w is dened by ( w ) ij : = wij , vs outgoing weight vector wv is dened by ( wv ) i : = ( w ) iv = wiv , ks incoming weight vector w t is dened by ( w t ( t ) is the diagonal matrix of rst order derivatives dened as : ( f ( cid : 123 ) otherwise .
here t is the transposition operator , ( a ) ij is the element in the i - th column and j - th row of matrix a , and ( x ) i is the i - th component of using a matrix norm ( cid : 123 ) . ( cid : 123 ) a compatible with vector norm ( cid : 123 ) . ( cid : 123 ) x , we dene
( t ) ) ij : = 123 if i ( cid : 123 ) = j , and ( f ( cid : 123 )
( t ) ) ij : = f ( cid : 123 )
for maxi=123 , . . . , n ( |xi| ) ( cid : 123 ) x ( cid : 123 ) x we get
max : = max
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n ( cid : 123 ) x ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) x .
since v ( netv ( s ) ) | ( cid : 123 ) f ( cid : 123 )
we obtain the following inequality :
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n ( f ( cid : 123 )
max ) ts ( cid : 123 ) wv ( cid : 123 ) x ( cid : 123 ) w t
k ( cid : 123 ) x ( cid : 123 ) w ( cid : 123 ) ts123
this inequality results from
( cid : 123 ) wv ( cid : 123 ) x = ( cid : 123 ) w ev ( cid : 123 ) x ( cid : 123 ) w ( cid : 123 ) a ( cid : 123 ) ev ( cid : 123 ) x ( cid : 123 ) w ( cid : 123 ) a
k ( cid : 123 ) x = ( cid : 123 ) ekw ( cid : 123 ) x ( cid : 123 ) w ( cid : 123 ) a ( cid : 123 ) ek ( cid : 123 ) x ( cid : 123 ) w ( cid : 123 ) a ,
where ek is the unit vector whose components are 123 except for the k - th component , which is 123
note that this is a weak , extreme case upper bound ( ) ( cid : 123 ) a take on maximal values , and if the it will be reached only if all ( cid : 123 ) f ( cid : 123 ) contributions of all paths across which error ows back from unit k to unit v have the same sign .
large ( cid : 123 ) w ( cid : 123 ) a , however , typically result in small values of ( cid : 123 ) f ( cid : 123 )
( ) ( cid : 123 ) a , as conrmed by experiments ( see , e . g . , ( 123 ) ) .
for example , with norms
( cid : 123 ) w ( cid : 123 ) a : = maxr
we have f ( cid : 123 )
max = 123 for the logistic sigmoid .
we observe that if
( cid : 123 ) x ( cid : 123 ) x : = maxr
|wij| wmax < 123
then ( cid : 123 ) w ( cid : 123 ) a n wmax < 123 will result in exponential decay; by setting
< 123 , we obtain ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) v ( s )
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n ts .
we refer to hochreiters thesis ( 123 ) for more details .
123 dilemma : avoiding gradient decay prevents
in bengio et al . s paper ( 123 ) , the analysis of the problem of gradient decays is generalized to parameterized dynamical systems ( hence including second order and other recurrent architectures ) .
the main theorem shows that a
sucient condition to obtain gradient decay is also a necessary condition for the system to robustly store discrete state information for the long - term .
in other words , when the weights and the state trajectory are such that the network can latch on information in its hidden units ( i . e . , represent long - term dependencies ) , the problem of gradient decay is obtained .
when the long - term gradients decay exponentially , it is very dicult to learn such long - term dependencies because the total gradient is the sum of long - term and short - term inuences and the short - term inuences then completely dominate
this result is based on a decomposition of the state - space of hidden units in two types of regions : one where gradients decay and one where it is not possible to robustly latch information .
let y ( t ) denote the n - dimensional state vector at time t ( for example , the vector ( net123 ( t ) , .
, netn ( t ) ) when considering a standard rst - order recurrent network ) and let y ( t ) = m ( y ( t 123 ) ) be the map from the state at time t 123 to t for the autonomous ( without inputs ) dynamical system .
the above decomposition is expressed in terms of the condition |m ( cid : 123 ) | > 123 ( no robust latching possible ) or |m ( cid : 123 ) | < 123 ( gradient decay ) , where |m ( cid : 123 ) | is the norm of the jacobian ( matrix of partial derivatives ) of the map m .
the analysis focuses on the basins of attraction of attractors of m in the domain of y ( t ) ( or manifolds within that domain ) .
in particular , the analysis is concerned with so - called hyperbolic attractors , which are locally stable ( but need not be xed points ) and where the eigenvalues of m ( cid : 123 ) less than 123 in absolute value .
if the state ( or a function of it ) remains within a certain region of space ( versus another region ) even in the presence of perturbations ( such as noise in the inputs ) then it is possible to store at least one bit of information for arbitrary durations .
in regions where |m ( cid : 123 ) | > 123 it can be shown that arbitrarily small pertur - bations ( for example due to the inputs ) can eventually kick the state out of a basin of attraction ( 123 ) ( see the sample trajectory on the right of figure 123 ) .
in regions where |m ( cid : 123 ) | < < 123 there is a level of perturbation ( depend - ing on ) below which the state will remain in the basin of attraction ( and will gradually get closer to a certain volume around the attractor see left of figure 123 ) .
for this reason we call this condition information latching , since it allows to store discrete information for arbitrary duration in the state unfortunately , in the regions where |m ( cid : 123 ) | < 123 ( where one can latch in - formation ) one can also show that gradients decay .
the argument is similar to the one developed in the previous section .
the partial derivative of y ( t )
|m ( cid : 123 ) | > 123
|m ( cid : 123 ) | > 123
|m ( cid : 123 ) | < 123
|m ( cid : 123 ) | < 123
figure 123 : robust latching .
for simplicity a xed - point attractor y is shown .
the shadow region is the basin of attraction .
the dark shadow region is the subset where |m ( cid : 123 ) | < 123 and robust latch occurs .
see text for details .
with respect to y ( s ) with s < t is simply the product of the map derivatives between s and t :
y ( 123 ) is equal to the w f ( cid : 123 )
y ( s + 123 )
( ) in previous formulas ( using neural networks compare rst equation of subsection on weak upper bounds . ) when the norm of each of the factors on the right hand side is less than 123 , the left hand side converges exponentially fast to zero as t s increases .
the eect of this decay of gradients can be made explicit as follows : hence for a term of the sum with ( cid : 123 ) t , we have
this term tends to become very small in comparison to terms for which is close to t .
this means that even though there might exist a change in w that would allow y ( ) to jump to another ( better ) basin of attraction , the gradient of the cost with respect to w does not clearly reect that possibility .
the explanation is that the eect of a small change in w would be felt mostly on the near past ( close to t ) .
the above theoretical investigations indicate a basic limitation of gradient descent as a search procedure for nding optimal weights in a rnn .
several proposals have been made to cope with the problem of long - term dependen - cies , some attempting to solve the optimization problem using alternative search algorithms , other trying to devise alternative architectures .
following we give a brief accounts of these proposals .
to deal with long time lags , mozer ( 123 ) uses time constants inuencing changes of unit activations ( devries and principes related approach ( 123 ) may be viewed as a mixture of time - delay neural networks ( tdnn ) ( 123 ) and time constants ) .
for long time lags , however , the time constants need external ne tuning ( 123 ) .
sun et al . s alternative approach ( 123 ) updates the activa - tion of a recurrent unit by adding the old activation and the ( scaled ) current net input .
the net input , however , tends to perturb the stored information , which makes long - term storage impractical .
lin et al .
( 123 ) also propose vari - ants of time - delay networks , called narx networks ( crossreference see also chapter 123 ) .
gradient ow in this architecture can be improved because em - bedded memories eectively introduce shortcuts in the error propagation path through time .
the same idea can be applied to other architectures , by inserting multiple delays in the connections among hidden state units rather than output units ( 123 ) .
however , these architectures cannot solve the general problem since they can only increase by a constant multiplicative factor the duration of the temporal dependencies that can be learned .
finally , el hihi & bengio ( 123 ) looked at hierarchically organized recurrent neural networks with dierent levels of time - constants or time - delays .
ring ( 123 ) also proposes a method for bridging long time lags .
whenever a unit in his network receives conicting error signals , that is , certain error signals suggest to increase the units activity while others suggest otherwise , he adds a higher order unit inuencing appropriate connections .
although his approach can sometimes be extremely fast , to bridge a time lag involving 123 steps may require the addition of 123 units .
also , rings net does not
generalize to unseen lag durations .
searching without gradients
the diculty of learning long - term dependencies is strictly related to the continuous optimization approach that guides the search for a weight solu - tion .
one possibility for avoiding the problem is to resort to other kinds of search in weight space , in which the operators for generating another candi - date weight solution are not based on continuous gradients .
bengio et al .
( 123 ) investigate methods such as simulated annealing , multi - grid random search , and discrete error propagation .
angeline et al .
( 123 ) ( see also crossreference chapter 123 ) propose a genetic approach that also avoids gradient computa -
the simplest kind of search without gradient , however , simply randomly initializes all network weights until the resulting net happens to classify all training sequences correctly .
in fact , as discussed in crossreference chapter 123 of this book , simple weight guessing solves several popular benchmarks de - scribed in previous work faster than the recurrent net algorithms proposed therein ( compare ( 123 ) ) .
this does not mean that weight guessing is a good algorithm .
it just means that the problems are very simple .
more realis - tic tasks require either many free parameters ( e . g . , input weights ) or high weight precision ( e . g . , for continuous - valued parameters ) , such that guessing becomes completely infeasible .
currently it is unclear to which extent more complex gradient - less methods can improve upon guessing in case of more
probabilistic target propagation
bengio and frasconi ( 123 ) propose a probabilistic approach for propagating targets .
with n so - called state networks , at a given time , their system can be in one of only n dierent discrete states .
parameters are adjusted using the expectation - maximization algorithm .
but to solve problems that require a signicant amount of memory to store contextual information , such systems would require an unacceptable number of states ( i . e . , state networks ) .
adaptive sequence chunkers
schmidhubers hierarchical chunker systems ( 123 , 123 ) can in principle bridge arbitrary time lags , but only if there is local predictability across the sub -
sequences causing the time lags ( see also ( 123 ) ) .
for instance , in his post - doctoral thesis ( 123 ) , schmidhuber uses hierarchical recurrent networks with self - organizing time scales to rapidly solve certain grammar learning tasks involving minimal time lags in excess of 123 steps .
the performance of chunker systems , however , deteriorates as the noise level increases and the input sequences become less compressible .
long short - term memory
there is a novel , ecient , gradient - based method called long short - term memory ( lstm ) ( 123 ) .
lstm is designed to get rid of the vanishing error problem .
truncating the gradient where this does not do harm , lstm can learn to bridge minimal time lags in excess of 123 discrete time steps by en - forcing constant error ow through constant error carrousels within special units .
multiplicative gate units learn to open and close access to the constant error ow .
lstm is local in space and time; its computational complexity per time step and weight is o ( 123 ) .
so far , experiments with articial data involved local , distributed , real - valued , and noisy pattern representations .
in comparisons with rtrl , bptt , recurrent cascade - correlation , elman networks , and neural sequence chunking , lstm led to many more success - ful runs , and learned much faster .
lstm also solved complex , articial long time lag tasks that have never been solved by previous recurrent network algorithms .
it will be interesting to examine to which extent lstm is appli - cable to real world problems such as language separation from prosody ( rst results in ( 123 ) ) and speech recognition .
in principle , rnns represent the most general and powerful sequence process - ing method .
for instance , unlike hidden markov models ( hmms , the most successful technique in several applications - see ( 123 ) for a review ) they are not limited to discrete internal states but allow for continuous , distributed se - quence representations .
hence they can solve tasks no other current method can solve ( e . g . , ( 123 ) ) .
the problem of vanishing gradients , however , makes conventional rnns hard to train .
we suspect this is why feedforward neu - ral networks outnumber rnns in terms of successful real - world applications .
some of the remedies outlined in this chapter may lead to more eective
learning systems .
however , long lime lag research still seems to be in an early stage no commercial applications of any of these methods have been reported so far .
long time lags pose problems to any soft computing method , not just rnns .
for instance , when dealing with long sequences ( e . g . , speech or bio - logical data ) , hmms mostly rely on localized representation of time by means of highly constrained non ergodic transition diagrams ( dierent states are de - signed for dierent portions of a sequence ) .
belief propagation over long time lags does not eectively occur , a phenomenon called diusion of credit ( 123 ) , which closely resembles the vanishing gradients problem in rnns .
