we present in this paper a novel approach for training deterministic auto - encoders .
we show that by adding a well chosen penalty term to the classical reconstruction cost func - tion , we can achieve results that equal or sur - pass those attained by other regularized auto - encoders as well as denoising auto - encoders on a range of datasets .
this penalty term corresponds to the frobenius norm of the jacobian matrix of the encoder activations with respect to the input .
we show that this penalty term results in a localized space contraction which in turn yields robust fea - tures on the activation layer .
furthermore , we show how this penalty term is related to both regularized auto - encoders and denoising auto - encoders and how it can be seen as a link between deterministic and non - deterministic auto - encoders .
we nd empirically that this penalty helps to carve a representation that better captures the local directions of varia - tion dictated by the data , corresponding to a lower - dimensional non - linear manifold , while being more invariant to the vast majority of directions orthogonal to the manifold .
fi - nally , we show that by using the learned fea - tures to initialize a mlp , we achieve state of the art classication error on a range of datasets , surpassing other methods of pre -
appearing in proceedings of the 123 th international con - ference on machine learning , bellevue , wa , usa , 123
copyright 123 by the author ( s ) / owner ( s ) .
a recent topic of interest123 in the machine learning community is the development of algorithms for un - supervised learning of a useful representation .
this automatic discovery and extraction of features is often used in building a deep hierarchy of features , within the contexts of supervised , semi - supervised , or unsu - pervised modeling .
see bengio ( 123 ) for a recent review of deep learning algorithms .
most of these methods exploit as basic building block algorithms for learning one level of feature extraction : the represen - tation learned at one level is used as input for learning the next level , etc .
the objective is that these rep - resentations become better as depth is increased , but what denes a good representation ? it is fairly well un - derstood what pca or ica do , but much remains to be done to understand the characteristics and theoret - ical advantages of the representations learned by a re - stricted boltzmann machine ( hinton et al . , 123 ) , an auto - encoder ( bengio et al . , 123 ) , sparse coding ( ol - shausen and field , 123; kavukcuoglu et al . , 123 ) , or semi - supervised embedding ( weston et al . , 123 ) .
all of these produce a non - linear representation which , un - like that of pca or ica , can be stacked ( composed ) to yield deeper levels of representation .
it has also been observed empirically ( lee et al . , 123 ) that the deeper levels often capture more abstract features .
a sim - ple approach ( hinton et al . , 123; bengio et al . , 123 ) , also used here , to empirically verify that the learned representations are useful , is to use them to initialize a classier ( such as a multi - layer neural network ) , and measure classication error .
many experiments show that deeper models can thus yield lower classication error ( bengio et al . , 123; jarrett et al . , 123; vincent
123see nips123 workshop on deep learning and unsu -
pervised feature learning
et al . , 123 ) .
contribution .
what principles should guide the learning of such intermediate representations ? they should capture as much as possible of the information in each given example , when that example is likely under the underlying generating distribution .
that is what auto - encoders ( vincent et al . , 123 ) and sparse coding aim to achieve when minimizing reconstruc - tion error .
we would also like these representations to be useful in characterizing the input distribution , and that is what is achieved by directly optimizing a generative models likelihood ( such as rbms ) .
this paper , we introduce a penalty term that could be added in either of the above contexts , which encour - ages the intermediate representation to be robust to small changes of the input around the training exam - ples .
we show through comparative experiments on many benchmark datasets that this characteristic is useful to learn representations that help training bet - ter classiers .
we hypothesize that whereas the pro - posed penalty term encourages the learned features to be locally invariant without any preference for partic - ular directions , when it is combined with a reconstruc - tion error or likelihood criterion we obtain invariance in the directions that make sense in the context of the given training data , i . e . , the variations that are present in the data should also be captured in the learned rep - resentation , but the other directions may be contracted in the learned representation .
how to extract robust features
to encourage robustness of the representation f ( x ) ob - tained for a training input x we propose to penalize its sensitivity to that input , measured as the frobenius norm of the jacobian jf ( x ) of the non - linear map - ping .
formally , if input x irdx is mapped by encod - ing function f to hidden representation h irdh , this sensitivity penalization term is the sum of squares of all partial derivatives of the extracted features with respect to input dimensions :
f encourages the mapping to the fea - ture space to be contractive in the neighborhood of the training data .
this geometric perspective , which gives its name to our algorithm , will be further elaborated on , in section 123 , based on experimental evidence .
the atness induced by having low valued rst deriva - tives will imply an invariance or robustness of the rep - resentation for small variations of the input .
thus in this study , terms like invariance , ( in - ) sensitivity , ro -
bustness , atness and contraction all point to the same while such a jacobian term alone would encourage mapping to a useless constant representation , counterbalanced in auto - encoder training123 by the need for the learnt representation to allow a good recon - struction of the input123
auto - encoders variants
in its simplest form , an auto - encoder ( ae ) is com - posed of two parts , an encoder and a decoder .
it was introduced in the late 123s ( rumelhart et al . , 123; baldi and hornik , 123 ) as a technique for dimen - sionality reduction , where the output of the encoder represents the reduced representation and where the decoder is tuned to reconstruct the initial input from the encoders representation through the minimization of a cost function .
more specically when the en - coding activation functions are linear and the num - ber of hidden units is inferior to the input dimen - sion ( hence forming a bottleneck ) , it has been shown that the learnt parameters of the encoder are a sub - space of the principal components of the input space ( baldi and hornik , 123 ) .
with the use of non - linear activation functions an ae can however be expected to learn more useful feature - detectors than what can be obtained with a simple pca ( japkowicz et al . , 123 ) .
moreover , contrary to their classical use as dimensionality - reduction techniques , in their modern instantiation auto - encoders are often employed in a so - called over - complete setting to extract a number of fea - tures larger than the input dimension , yielding a rich higher - dimensional representation .
in this setup , using some form of regularization becomes essential to avoid uninteresting solutions where the auto - encoder could perfectly reconstruct the input without needing to ex - tract any useful feature .
this section formally denes the auto - encoder variants considered in this study .
basic auto - encoder ( ae ) .
the encoder is a func - tion f that maps an input x irdx to hidden represen - tation h ( x ) irdh .
it has the form
h = f ( x ) = sf ( w x + bh ) ,
where sf is a nonlinear activation function , typi - 123+ez .
the encoder is cally a logistic sigmoid ( z ) = 123 parametrized by a dh dx weight matrix w , and a 123using also the now common additional constraint of encoder and decoder sharing the same ( transposed ) weights , which precludes a mere global contracting scal - ing in the encoder and expansion in the decoder .
123a likelihood - related criterion would also similarly pre -
vent a collapse of the representation .
bias vector bh irdh .
the decoder function g maps hidden representation h back to a reconstruction y :
y = g ( h ) = sg ( w ( cid : 123 ) h + by ) ,
where sg is the decoders activation function , typically either the identity ( yielding linear reconstruction ) or a sigmoid .
the decoders parameters are a bias vector by irdx , and matrix w ( cid : 123 ) .
in this paper we only explore the tied weights case , in which w ( cid : 123 ) = w t .
auto - encoder training consists in nding parameters = ( w , bh , by ) that minimize the reconstruction error on a training set of examples dn , which corresponds to minimizing the following objective function :
jae ( ) = ( cid : 123 )
where l is the reconstruction error .
typical choices include the squared error l ( x , y ) = ( cid : 123 ) x y ( cid : 123 ) 123 used in cases of linear reconstruction and the cross - entropy loss when sg is the sigmoid ( and inputs are in ( 123 , 123 ) ) :
l ( x , y ) = ( cid : 123 ) dx
i=123 xi log ( yi ) + ( 123 xi ) log ( 123 yi ) .
regularized auto - encoders ( ae+wd ) .
the sim - plest form of regularization is weight - decay which fa - vors small weights by optimizing instead the following
where the hyper - parameter controls the strength of note that rather than having a prior on what the weights should be , it is possible to have a prior on what the hidden unit activations should be .
from this view - point , several techniques have been developed to en - courage sparsity of the representation ( kavukcuoglu et al . , 123; lee et al . , 123 ) .
denoising auto - encoders ( dae ) .
a successful al - ternative form of regularization is obtained through the technique of denoising auto - encoders ( dae ) put forward by vincent et al .
( 123 ) , where one simply corrupts input x before sending it through the auto - encoder , that is trained to reconstruct the clean ver - sion ( i . e .
to denoise ) .
this yields the following objec -
jdae ( ) = ( cid : 123 )
where the expectation is over corrupted versions x of examples x obtained from a corruption process q ( x|x ) .
this objective is optimized by stochastic gradient de - scent ( sampling corrupted examples ) .
typically , we consider corruptions such as additive isotropic gaussian noise : x = x + , n ( 123 , 123i ) and a binary masking noise , where a fraction of input components ( randomly chosen ) have their value set to 123
the degree of the corruption ( or ) controls the degree of regularization .
contractive auto - encoders ( cae )
from the motivation of robustness to small perturba - tions around the training points , as discussed in sec - tion 123 , we propose an alternative regularization that favors mappings that are more strongly contracting at the training samples ( see section 123 for a longer discussion ) .
the contractive auto - encoder ( cae ) is obtained with the regularization term of eq .
123 yielding
jcae ( ) = ( cid : 123 )
( cid : 123 ) l ( x , g ( f ( x ) ) ) + ( cid : 123 ) jf ( x ) ( cid : 123 ) 123
relationship with weight decay .
it is easy to see that the squared frobenius norm of the jacobian cor - responds to a l123 weight decay in the case of a linear encoder ( i . e .
when sf is the identity function ) .
in this special case jcae and jae+wd are identical .
note that in the linear case , keeping weights small is the only way to have a contraction .
but with a sigmoid non - linearity , contraction and robustness can also be achieved by driving the hidden units to their saturated relationship with sparse auto - encoders .
auto - encoder variants that encourage sparse representations aim at having , for each example , a majority of the components of the representation close to zero .
for these features to be close to zero , they must have been computed in the left saturated part of the sigmoid non - linearity , which is almost at , with a tiny rst deriva - tive .
this yields a corresponding small entry in the jacobian jf ( x ) .
thus , sparse auto - encoders that out - put many close - to - zero features , are likely to corre - spond to a highly contractive mapping , even though contraction or robustness are not explicitly encouraged through their learning criterion .
relationship with denoising auto - encoders .
ro - bustness to input perturbations was also one of the motivation of the denoising auto - encoder , as stated in
vincent et al .
( 123 ) .
the cae and the dae dier however in the following ways : caes explicitly encourage robustness of representa - tion f ( x ) , whereas daes encourages robustness of re - construction ( g f ) ( x ) ( which may only partially and indirectly encourage robustness of the representation , as the invariance requirement is shared between the two parts of the auto - encoder ) .
we believe that this property make caes a better choice than daes to learn useful feature extractors .
since we will use only the encoder part for classication , robustness of the extracted features appears more important than ro - bustness of the reconstruction .
daes robustness is obtained stochastically ( eq .
123 ) by having several explicitly corrupted versions of a train - ing point aim for an identical reconstruction .
by con - trast , caes robustness to tiny perturbations is ob - tained analytically by penalizing the magnitude of rst derivatives ( cid : 123 ) jf ( x ) ( cid : 123 ) 123 note that an analytic approximation for daes stochastic robustness criterion can be obtained in the limit of very small additive gaussian noise , by follow - ing bishop ( 123 ) .
this yields , not surprisingly , a term in ( cid : 123 ) jgf ( x ) ( cid : 123 ) 123 f ( jacobian of reconstruction ) rather than the ( cid : 123 ) jf ( x ) ( cid : 123 ) 123 f ( jacobian of representation ) of caes .
in the case of a sigmoid nonlinearity , the penalty on the jacobian norm has the following simple expression :
f at training points only ( eq
( hi ( 123 hi ) ) 123
computing this penalty ( or its gradient ) , is similar to and has about the same cost as computing the re - construction error ( or , respectively , its gradient ) .
the overall computational complexity is o ( dx dh ) .
experiments and results
considered models .
in our experiments , we com - pare the proposed contractive auto encoder ( cae ) against the following models for unsupervised feature
trained by contrastive divergence ,
rbm - binary : restricted boltzmann machine ae : basic auto - encoder , ae+wd : auto - encoder with weight - decay regu - dae - g : denoising auto - encoder with gaussian dae - b : denoising auto - encoder with binary
all auto - encoder variants used tied weights , a sigmoid activation function for both encoder and decoder , and a cross - entropy reconstruction error ( see section 123 ) .
they were trained by optimizing their ( regularized ) objective function on the training set by stochastic gradient descent .
as for rbms , they were trained by these algorithms were applied on the training set without using the labels ( unsupervised ) to extract a rst layer of features .
optionally the procedure was repeated to stack additional feature - extraction layers on top of the rst one .
once thus trained , the learnt parameter values of the resulting feature - extractors ( weight and bias of the encoder ) were used as initiali - sation of a multilayer perceptron ( mlp ) with an extra random - initialised output layer .
the whole network was then ne - tuned by a gradient descent on a super - vised objective appropriate for classication 123 , using the labels in the training set .
datasets used .
we have tested our approach on a benchmark of image classication problems , namely : a gray - scale version of the cifar - 123 image - classication task ( krizhevsky and hinton , 123 ) and mnist : the well - known digit classica - tion problem .
we also used problems from the same benchmark as vincent et al .
( 123 ) which includes ve harder digit recognition problems derived by adding extra factors of variation to mnist digits , each with 123 examples for training , 123 for validation , 123 for test as well as two articial shape classication
classication performance
our rst series of experiments focuses on the mnist and cifar - bw datasets .
we compare the classica - tion performance obtained by a neural network with one hidden layer of 123 units , initialized with each of the unsupervised algorithms under consideration .
for each case , we selected the value of hyperparameters ( such as the strength of regularization ) that yielded , after supervised ne - tuning , the best classication per -
123we used sigmoid+cross - entropy for binary classica -
tion , and log of softmax for multi - class problems
123these datasets are available at http : / / www . iro .
umontreal . ca / ~ lisa / icml123 : basic : smaller subset of mnist; rot : digits with added random rotation; bg - rand : digits with random noise background; bg - img : digits with random image background; bg - img - rot : digits with rota - tion and image background; rect : discriminate between tall and wide rectangles ( white on black ) ; rect - img : discrimi - nate between tall and wide rectangular image on a dierent
table 123
performance comparison of the considered models on mnist ( top half ) and cifar - bw ( bottom half ) .
re - sults are sorted in ascending order of classication error on the test set .
best performer and models whose dierence with the best performer was not statistically signicant are in bold .
notice how the average jacobian norm ( before ne - tuning ) appears correlated with the nal test error .
sat is the average fraction of saturated units per exam - ple .
not surprisingly , the cae yields a higher proportion of saturated units .
formance on the validation set .
final classication er - ror rate was then computed on the test set .
with the parameters obtained after unsupervised pre - training ( before ne - tuning ) , we also computed in each case the average value of the encoders contraction ( cid : 123 ) jf ( x ) ( cid : 123 ) f on the validation set , as well as a measure of the av - erage fraction of saturated units per example123
these results are reported in table 123
we see that the lo - cal contraction measure ( the average ( cid : 123 ) jf ( cid : 123 ) f ) on the pre - trained model strongly correlates with the nal classication error .
the cae , which explicitly tries to minimize this measure while maintaining a good re - construction , is the best - performing model .
datasets .
results given in table 123 compares the performance of stacked caes on the benchmark problems of larochelle et al .
( 123 ) to the three - layer models re - ported in vincent et al .
( 123 ) .
stacking a second layer cae on top of a rst layer appears to signicantly improves performance , thus demonstrating their use - fulness for building deep networks .
moreover on the majority of datasets , 123 - layer cae beat the state - of - the - art 123 - layer model .
123we consider a unit saturated if its activation is below 123 or above 123 .
note that in general the set of saturated units is expected to vary with each example .
closer examination of the contraction
to better understand the feature extractor produced by each algorithm , in terms of their contractive prop - erties , we used the following analytical tools : what happens locally : looking at the singular values of the jacobian .
a high dimensional jaco - bian contains directional information : the amount of contraction is generally not the same in all directions .
this can be examined by performing a singular value decomposition of jf .
we computed the average singu - lar value spectrum of the jacobian over the validation set for the above models .
results are shown in fig - ure 123 and will be discussed in section 123 .
curves .
the frobenius norm of the jacobian at some point x measures the contraction of the mapping lo - cally at that point .
intuitively the contraction induced by the proposed penalty term can be measured beyond the immediate training examples , by the ratio of the distances between two points in their original ( input ) space and their distance once mapped in the feature space .
we call this isotropic measure contraction ra - tio .
in the limit where the variation in the input space is innitesimal , this corresponds to the derivative ( i . e .
jacobian ) of the representation map .
for any encoding function f , we can measure the aver - age contraction ratio for pairs of points , one of which , x123 is picked from the validation set , and the other x123 randomly generated on a sphere of radius r cen - tered on x123 in input space .
how this average ratio evolves as a function of r yields a contraction curve .
we have computed these curves for the models for which we reported classication performance ( the con - traction curves are however computed with their initial parameters prior to ne tuning ) .
results are shown in figure 123 for single - layer mappings and in figure 123 for 123 and 123 layer mappings .
they will be discussed in detail in the next section .
discussion : local space contraction
from a geometrical point of view , the robustness of the features can be seen as a contraction of the input space when projected in the feature space , in particu - lar in the neighborhood of the examples from the data - generating distribution : otherwise ( if the contraction was the same at all distances ) it would not be useful , because it would just be a global scaling .
such a con - traction is happening with the proposed penalty , but much less without it , as illustrated on the contraction curves of figure 123
for all algorithms tested except the proposed cae and the gaussian corruption dae
table 123
comparison of stacked contractive auto - encoders with 123 and 123 layers ( cae - 123 and cae - 123 ) with other 123 - layer stacked models and baseline svm .
test error rate on all considered classication problems is reported together with a 123% condence interval .
best performer is in bold , as well as those for which condence intervals overlap .
clearly caes can be successfully used to build top - performing deep networks .
123 layers of cae often outperformed 123 layers of other
( dae - g ) , the contraction ratio decreases ( i . e . , towards more contraction ) as we move away from the train - ing examples ( this is due to more saturation , and was expected ) , whereas for the cae the contraction ratio initially increases , up to the point where the eect of saturation takes over ( the bump occurs at about the maximum distance between two training examples ) .
think about the case where the training examples con - gregate near a low - dimensional manifold .
the vari - ations present in the data ( e . g .
translation and ro - tations of objects in images ) correspond to local di - mensions along the manifold , while the variations that are small or rare in the data correspond to the direc - tions orthogonal to the manifold ( at a particular point near the manifold , corresponding to a particular ex - ample ) .
the proposed criterion is trying to make the features invariant in all directions around the training examples , but the reconstruction error ( or likelihood ) is making sure that that the representation is faith - ful , i . e . , can be used to reconstruct the input exam - ple .
hence the directions that resist to this contract - ing pressure ( strong invariance to input changes ) are the directions present in the training set .
indeed , if the variations along these directions present in the training set were not preserved , neighboring training examples could not be distinguished and properly reconstructed .
hence the directions where the contraction is strong ( small ratio , small singular values of the jacobian ma - trix ) are also the directions where the model believes that the input density drops quickly , whereas the di - rections where the contraction is weak ( closer to 123 , larger contraction ratio , larger singular values of the jacobian matrix ) correspond to the directions where the model believes that the input density is at ( and large , since we are near a training example ) .
we believe that this contraction penalty thus helps the learner carve a kind of mountain supported by the
training examples , and generalizing to a ridge between them .
what we would like is for these ridges to cor - respond to some directions of variation present in the data , associated with underlying factors of variation .
how far do these ridges extend around each training example and how at are they ? this can be visual - ized comparatively with the analysis of figure 123 , with the contraction ratio for dierent distances from the note that dierent features ( elements of the represen - tation vector ) would be expected to have ridges ( i . e .
directions of invariance ) in dierent directions , and that the dimensionality of these ridges ( we are in a fairly high - dimensional space ) gives a hint as to the local dimensionality of the manifold near which the data examples congregate .
the singular value spec - trum of the jacobian informs us about that geometry .
the number of large singular values should reveal the dimensionality of these ridges , i . e . , of that manifold near which examples concentrate .
this is illustrated in figure 123 , showing the singular values spectrum of the encoders jacobian .
the cae by far does the best job at representing the data variations near a lower - dimensional manifold , and the dae is second best , while ordinary auto - encoders ( regularized or not ) do not succeed at all in this respect .
what happens when we stack a cae on top of an - other one , to build a deeper encoder ? this is illus - trated in figure 123 , which shows the average contrac - tion ratio for dierent distances around each training point , for depth 123 vs depth 123 encoders .
composing two caes yields even more contraction and even more non - linearity , i . e .
a sharper prole , with a atter level of contraction at short and medium distances , and a delayed eect of saturation ( the bump only comes up at farther distances ) .
we would thus expect higher - level features to be more invariant in their feature -
figure 123
average spectrum of the encoders jacobian , for the cifar - bw dataset .
large singular values correspond to the local directions of allowed variation learnt from the dataset .
the cae having fewer large singular values and a sharper decreasing spectrum , it suggests that it does a better job of characterizing a low - dimensional manifold near the training examples .
in this paper , we attempt to answer the following ques - tion : what makes a good representation ? besides be - ing useful for a particular task , which we can measure , or towards which we can train a representation , this paper highlights the advantages for representations to be locally invariant in many directions of change of the raw input .
this idea is implemented by a penalty on the frobenius norm of the jacobian matrix of the encoder mapping , which computes the representation .
the paper also introduces empirical measures of ro - bustness and invariance , based on the contraction ra - tio of the learned mapping , at dierent distances and in dierent directions around the training examples .
we hypothesize that this reveals the manifold struc - ture learned by the model , and we nd ( by look - ing at the singular value spectrum of the mapping ) that the contractive auto - encoder discovers lower - in addition , experiments on many datasets suggest that this penalty always helps an auto - encoder to perform better , and competes or improves upon the representations learned by denois - ing auto - encoders or rbms , in terms of classication
