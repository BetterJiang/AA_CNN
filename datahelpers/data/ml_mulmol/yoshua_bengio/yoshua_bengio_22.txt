several unsupervised learning algorithms based on an eigendecompo - sition provide either an embedding or a clustering only for given train - ing points , with no straightforward extension for out - of - sample examples short of recomputing eigenvectors .
this paper provides a unied frame - work for extending local linear embedding ( lle ) , isomap , laplacian eigenmaps , multi - dimensional scaling ( for dimensionality reduction ) as well as for spectral clustering .
this framework is based on seeing these algorithms as learning eigenfunctions of a data - dependent kernel .
numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data .
many unsupervised learning algorithms have been recently proposed , all using an eigen - decomposition for obtaining a lower - dimensional embedding of data lying on a non - linear manifold : local linear embedding ( lle ) ( roweis and saul , 123 ) , isomap ( tenenbaum , de silva and langford , 123 ) and laplacian eigenmaps ( belkin and niyogi , 123 ) .
there are also many variants of spectral clustering ( weiss , 123; ng , jordan and weiss , 123 ) , in which such an embedding is an intermediate step before obtaining a clustering of the data that can capture at , elongated and even curved clusters .
the two tasks ( manifold learning and clustering ) are linked because the clusters found by spectral clustering can be arbitrary curved manifolds ( as long as there is enough data to locally capture their curvature ) .
123 common framework in this paper we consider ve types of unsupervised learning algorithms that can be cast in the same framework , based on the computation of an embedding for the training points obtained from the principal eigenvectors of a symmetric matrix .
start from a data set d = fx123; : : : ; xng with n points in rd .
construct a n ( cid : 123 ) n neighborhoodor similarity matrix m .
let us denote kd ( ( cid : 123 ) ; ( cid : 123 ) ) ( or k for shorthand ) the data - dependentfunctionwhichproducesm bymij = kd ( xi; xj ) .
optionally transform m , yielding a normalized matrix ~ m .
equivalently , this corre - spondstogenerating ~ m froma ~ kd by ~ mij = ~ kd ( xi; xj ) .
computethem largestpositiveeigenvalues ( cid : 123 ) k andeigenvectorsvk of ~ m .
theembeddingofeachexample xi isthevector yi with yik the i - thelementofthe k - th principaleigenvectorvk of ~ m .
alternatively ( mdsandisomap ) , theembeddingisei , with eik = p ( cid : 123 ) kyik .
iftherst m eigenvaluesarepositive , thenei ( cid : 123 ) ej isthebestapproximation of ~ mij usingonlym coordinates , inthesquarederrorsense .
in the following , we consider the specializations of algorithm 123 for different unsupervised learning algorithms .
let si be the i - th row sum of the afnity matrix m :
we say that two points ( a; b ) are k - nearest - neighbors of each other if a is among the k nearest neighbors of b in d ( fag or vice - versa .
we denote by xij the j - th coordinate of the vector xi .
123 multi - dimensional scaling multi - dimensional scaling ( mds ) starts from a notion of distance or afnity k that is computed between each pair of training examples .
we consider here metric mds ( cox and cox , 123 ) .
for the normalization step 123 in algorithm 123 , these distances are converted to equivalent dot products using the double - centeringformula :
123 mij ( cid : 123 )
~ mij = ( cid : 123 )
the embedding eik of example xi is given by p ( cid : 123 ) kvki .
123 spectral clustering spectral clustering ( weiss , 123 ) can yield impressively good results where traditional clustering looking for roundblobs in the data , such as k - means , would fail miserably .
it is based on two main steps : rst embedding the data points in a space in which clusters are more obvious ( using the eigenvectors of a gram matrix ) , and then applying a classical clustering algorithm such as k - means , e . g .
as in ( ng , jordan and weiss , 123 ) .
the afnity matrix m is formed using a kernel such as the gaussian kernel .
several normalization steps have been proposed .
among the most successful ones , as advocated in ( weiss , 123; ng , jordan and weiss , 123 ) , is the following :
to obtain m clusters , the rst m principal eigenvectors of ~ m are computed and k - means is applied on the unit - norm coordinates , obtained from the embedding yik = vki .
123 laplacian eigenmaps laplacian eigenmaps is a recently proposed dimensionality reduction procedure ( belkin and niyogi , 123 ) that has been proposed for semi - supervised learning .
the authors use an approximation of the laplacian operator such as the gaussian kernel or the matrix whose element ( i; j ) is 123 if xi and xj are k - nearest - neighbors and 123 otherwise .
instead of solving an ordinary eigenproblem , the following generalized eigenproblem is solved :
( s ( cid : 123 ) m ) vj = ( cid : 123 ) jsvj
with eigenvalues ( cid : 123 ) j , eigenvectors vj and s the diagonal matrix with entries given by eq .
the smallest eigenvalue is left out and the eigenvectors corresponding to the other small eigenvalues are used for the embedding .
this is the same embedding that is computed with the spectral clustering algorithm from ( shi and malik , 123 ) .
as noted in ( weiss , 123 ) ( normalization lemma 123 ) , an equivalent result ( up to a componentwise scaling of the embedding ) can be obtained by considering the principal eigenvectors of the normalized matrix dened in eq
isomap ( tenenbaum , de silva and langford , 123 ) generalizes mds to non - linear mani - folds .
it is based on replacing the euclidean distance by an approximation of the geodesic distance on the manifold .
we dene the geodesic distance with respect to a data set d , a distance d ( u; v ) and a neighborhood k as follows :
~ d ( a; b ) = min
where p is a sequence of points of length l ( cid : 123 ) 123 with p123 = a , pl = b , pi 123 d 123i 123 f123; : : : ; l ( cid : 123 ) 123g and ( pi , pi+123 ) are k - nearest - neighbors .
the length l is free in the minimiza - tion .
the isomap algorithm obtains the normalized matrix ~ m from which the embedding is derived by transforming the raw pairwise distances matrix as follows : rst compute the matrix mij = ~ d123 ( xi; xj ) of squared geodesic distances with respect to the data d , then apply to this matrix the distance - to - dot - product transformation ( eq .
( 123 ) ) , as for mds .
as in mds , the embedding is eik = p ( cid : 123 ) kvki rather than yik = vki .
the local linear embedding ( lle ) algorithm ( roweis and saul , 123 ) looks for an em - bedding that preserves the local geometry in the neighborhood of each data point .
first , a
m = ( i ( cid : 123 ) w ) 123 ( i ( cid : 123 ) w )
sparse matrix of local predictive weights wij is computed , such thatpj wij = 123 , wij = 123 if xj is not a k - nearest - neighbor of xi and ( pj wijxj ( cid : 123 ) xi ) 123 is minimized .
then the matrix
is formed .
the embedding is obtained from the lowest eigenvectors of m , except for the smallest eigenvector which is uninteresting because it is ( 123; 123; : : : 123 ) , with eigenvalue 123
note that the lowest eigenvectors of m are the largest eigenvectors of ~ m ( cid : 123 ) = ( cid : 123 ) i ( cid : 123 ) m to t algorithm 123 ( the use of ( cid : 123 ) > 123 will be discussed in section 123 ) .
the embedding is given by yik = vki , and is constant with respect to ( cid : 123 ) .
123 from eigenvectors to eigenfunctions to obtain an embedding for a new data point , we propose to use the nystrom formula ( eq .
123 ) ( baker , 123 ) , which has been used successfully to speed - up kernel methods computations by focussing the heavier computations ( the eigendecomposition ) on a subset of examples .
the use of this formula can be justied by considering the convergence of eigenvectors and eigenvalues , as the number of examples increases ( baker , 123; williams and seeger , 123; koltchinskii and gine , 123; shawe - taylor and williams , 123 ) .
intuitively , the extensions to obtain the embedding for a new example require specifying a new column of the gram matrix ~ m , through a training - set dependent kernel function ~ kd , in which one of the arguments may be required to be in the training set .
if we start from a data set d , obtain an embedding for its elements , and add more and more data , the embedding for the points in d converges ( for eigenvalues that are unique ) .
( shawe - taylor and williams , 123 ) give bounds on the convergence error ( in the case of kernel pca ) .
in the limit , we expect each eigenvector to converge to an eigenfunction for the linear operator dened below , in the sense that the i - th element of the k - th eigenvector converges to the application of the k - th eigenfunction to xi ( up to a normalization factor ) .
consider a hilbert space hp of functions with inner product hf; gip =r f ( x ) g ( x ) p ( x ) dx; with a density function p ( x ) .
associate with kernel k a linear operator kp in hp :
( kpf ) ( x ) =z k ( x; y ) f ( y ) p ( y ) dy :
we dont know the true density p but we can approximate the above inner product and linear operator ( and its eigenfunctions ) using the empirical distribution ^p .
an empirical hilbert space h ^p is thus dened using ^p instead of p .
note that the proposition below can be
applied even if the kernel is not positive semi - denite , although the embedding algorithms we have studied are restricted to using the principal coordinates associated with positive eigenvalues .
for a more rigorous mathematical analysis , see ( bengio et al . , 123 ) .
let ~ k ( a; b ) be a kernel function , not necessarily positive semi - denite , that gives rise to a symmetric matrix ~ m with entries ~ mij = ~ k ( xi; xj ) upon a dataset d = fx123; : : : ; xng .
let ( vk; ( cid : 123 ) k ) be an ( eigenvector , eigenvalue ) pair that solves ~ m vk = ( cid : 123 ) kvk .
let ( fk; ( cid : 123 ) 123k ) be an ( eigenfunction , eigenvalue ) pair that solves ( ~ k ^pfk ) ( x ) = ( cid : 123 ) 123kfk ( x ) for any x , with ^p the empirical distribution over d .
let ek ( x ) = yk ( x ) p ( cid : 123 ) k or yk ( x ) denote the embedding associated with a new point x
fk ( xi ) = pnvki yk ( xi ) = yik;
vki ~ k ( x; xi )
vki ~ k ( x; xi )
ek ( xi ) = eik
see ( bengio et al . , 123 ) for a proof and further justications of the above formulae .
the generalized embedding for isomap and mds is ek ( x ) = p ( cid : 123 ) kyk ( x ) whereas the one for spectral clustering , laplacian eigenmaps and lle is yk ( x ) .
in addition , if the data - dependent kernel ~ kd is positive semi - denite , then
fk ( x ) =r n
where ( cid : 123 ) k ( x ) is the k - th component of the kernel pca projection of x obtained from the kernel ~ kd ( up to centering ) .
this relation with kernel pca ( scholkopf , smola and muller , 123 ) , already pointed out in ( williams and seeger , 123 ) , is further discussed in ( bengio et al . , 123 ) .
123 extending to new points using proposition 123 , one obtains a natural extension of all the unsupervised learning algo - rithms mapped to algorithm 123 , provided we can write down a kernel function ~ k that gives rise to the matrix ~ m on d , and can be used in eq .
( 123 ) to generalize the embedding .
we consider each of them in turn below .
in addition to the convergence properties discussed in section 123 , another justication for using equation ( 123 ) is given by the following proposition : if we dene the fk ( xi ) by eq .
( 123 ) and take a new point x , the value of fk ( x ) that minimizes
xi=123 ~ k ( x; xi ) ( cid : 123 )
is given by eq .
( 123 ) , for m ( cid : 123 ) 123 and any k ( cid : 123 ) m .
the proof is a direct consequence of the orthogonality of the eigenvectors vk .
this proposi - tion links equations ( 123 ) and ( 123 ) .
indeed , we can obtain eq .
( 123 ) when trying to approximate
~ k at the data points by minimizing the cost
xi;j=123 ~ k ( xi; xj ) ( cid : 123 )
for m = 123; 123; : : : when we add a new point x , it is thus natural to use the same cost to approximate the ~ k ( x; xi ) , which yields ( 123 ) .
note that by doing so , we do not seek to approximate ~ k ( x; x ) .
future work should investigate embeddings which minimize the empirical reconstruction error of ~ k but ignore the diagonal contributions .
123 extending mds for mds , a normalized kernel can be dened as follows , using a continuous version of the double - centering eq
~ k ( a; b ) = ( cid : 123 )
( d123 ( a; b ) ( cid : 123 ) ex ( d123 ( x; b ) ) ( cid : 123 ) ex123 ( d123 ( a; x123 ) ) + ex;x123 ( d123 ( x; x123 ) ) )
where d ( a; b ) is the original distance and the expectations are taken over the empirical data d .
an extension of metric mds to new points has already been proposed in ( gower , 123 ) , solving exactly for the embedding of x to be consistent with its distances to training points , which in general requires adding a new dimension .
123 extending spectral clustering and laplacian eigenmaps both the version of spectral clustering and laplacian eigenmaps described above are based on an initial kernel k , such as the gaussian or nearest - neighbor kernel .
an equiva - lent normalized kernel is :
~ k ( a; b ) =
pex ( k ( a; x ) ) ex123 ( k ( b; x123 ) )
where the expectations are taken over the empirical data d .
123 extending isomap to extend isomap , the test point is not used in computing the geodesic distance between training points , otherwise we would have to recompute all the geodesic distances .
a rea - sonable solution is to use the denition of ~ d ( a; b ) in eq .
( 123 ) , which only uses the training points in the intermediate points on the path from a to b .
we obtain a normalized kernel by applying the continuous double - centering of eq .
( 123 ) with d = ~ d .
a formula has already been proposed ( de silva and tenenbaum , 123 ) to approximate isomap using only a subset of the examples ( the landmarkpoints ) to compute the eigen - vectors .
using our notations , this formula is
vki ( ex123 ( ~ d123 ( x123; xi ) ) ( cid : 123 ) ~ d123 ( xi; x ) ) :
where ex123 is an average over the data set .
the formula is applied to obtain an embedding for the non - landmark examples .
the embedding proposed in proposition 123 for isomap ( ek ( x ) ) is equal to formula 123 ( land - mark isomap ) when ~ k ( x; y ) is dened as in eq .
( 123 ) with d = ~ d .
struction .
therefore ( 123; 123; : : : 123 ) is an eigenvector with eigenvalue 123 , and all the other eigen -
proof : the proof relies on a property of the gram matrix for isomap : pi mij = 123 , by con - vectors vk have the property pi vki = 123 because of the orthogonality with ( 123; 123; : : : 123 ) .
writing ( ex123 ( ~ d123 ( x123; xi ) ) ( cid : 123 ) ~ d123 ( x; xi ) ) = 123 ~ k ( x; xi ) +ex123;x123 ( ~ d123 ( x123; x123 ) ) ( cid : 123 ) ex123 ( ~ d123 ( x; x123 ) ) 123p ( cid : 123 ) k pi vki ~ k ( x; xi ) + ( ex123;x123 ( ~ d123 ( x123; x123 ) ) ( cid : 123 ) ex123 ( ~ d123 ( x; x123 ) ) ) pi vki = yields e123k ( x ) = 123 ek ( x ) , since the last sum is 123
123 extending lle the extension of lle is the most challenging one because it does not t as well the frame - work of algorithm 123 : the m matrix for lle does not have a clear interpretation in terms of distance or dot product .
an extension has been proposed in ( saul and roweis , 123 ) , but unfortunately it cannot be cast directly into the framework of proposition 123
their embedding of a new point x is given by
where w ( x; xi ) is the weight of xi in the reconstruction of x by its k - nearest - neighbors in the training set ( if x = xj 123 d , w ( x; xi ) = ( cid : 123 ) ij ) .
this is very close to eq .
( 123 ) , but lacks the normalization by ( cid : 123 ) k .
however , we can see this embedding as a limit case of proposition 123 , as shown below .
we rst need to dene a kernel ~ k ( cid : 123 ) such that
~ k ( cid : 123 ) ( xi; xj ) = ~ m ( cid : 123 ) ;ij = ( ( cid : 123 ) ( cid : 123 ) 123 ) ( cid : 123 ) ij + wij + wji ( cid : 123 ) xk
for xi; xj 123 d .
let us dene a kernel ~ k123 by
~ k123 ( xi; x ) = ~ k123 ( x; xi ) = w ( x; xi )
and ~ k123 ( x; y ) = 123 when neither x nor y is in the training set d .
let ~ k123 be dened by
~ k123 ( xi; xj ) = wij + wji ( cid : 123 ) xk
and ~ k123 ( x; y ) = 123 when either x or y isnt in d .
then , by construction , the kernel ~ k ( cid : 123 ) = ( ( cid : 123 ) ( cid : 123 ) 123 ) ~ k123 + ~ k123 veries eq .
thus , we can apply eq .
( 123 ) to obtain an embedding of a new point x , which yields
yik ( cid : 123 ) ( ( cid : 123 ) ( cid : 123 ) 123 ) ~ k123 ( x; xi ) + ~ k123 ( x; xi ) ( cid : 123 )
( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ^ ( cid : 123 ) k xi
with ( cid : 123 ) k = ( ( cid : 123 ) ( cid : 123 ) ^ ( cid : 123 ) k ) , and ^ ( cid : 123 ) k being the k - th lowest eigenvalue of m .
this rewrites into
yikw ( x; xi ) +
( cid : 123 ) ( cid : 123 ) ^ ( cid : 123 ) k xi
yik ~ k123 ( x; xi ) :
then when ( cid : 123 ) ! 123 , y ( cid : 123 ) ;k ( x ) ! yk ( x ) dened by eq .
since the choice of ( cid : 123 ) is free , we can thus consider eq .
( 123 ) as approximating the use of the kernel ~ k ( cid : 123 ) with a large ( cid : 123 ) in proposition 123
this is what we have done in the experiments described in the next section .
note however that we can nd smoother kernels ~ k ( cid : 123 ) verifying eq .
( 123 ) , giving other extensions of lle from proposition 123
it is out of the scope of this paper to study which kernel is best for generalization , but it seems desirable to use a smooth kernel that would take into account not only the reconstruction of x by its neighbors xi , but also the reconstruction of the xi by their neighbors including the new point x .
we want to evaluate whether the precision of the generalizations suggested in the pre - vious section is comparable to the intrinsic perturbations of the embedding algorithms .
the perturbation analysis will be achieved by considering splits of the data in three sets , d = f ( r123 ( r123 and training either with f ( r123 or f ( r123 , comparing the embeddings on f .
for each algorithm described in section 123 , we apply the following procedure :
x 123 123
x 123 123
x 123 123
figure 123 : training set variability minus out - of - sample error , wrt the proportion of training samples substituted .
top left : mds .
top right : spectral clustering or laplacian eigenmaps .
bottom left : isomap .
bottom right : lle .
error bars are 123% condence intervals .
we choose f ( cid : 123 ) d with m = jfj samples .
the remaining n ( cid : 123 ) m samples in d=f are split into two equal size subsets r123 and r123
we train ( obtain the eigenvectors ) over f ( r123 and f ( r123
when eigenvalues are close , the estimated eigenvectors are unstable and can rotate in the subspace they span .
thus we estimate an afne alignment between the two embeddings using the points in f , and we calculate the euclidean distance between the aligned embeddings obtained for each si 123 f .
for each sample si 123 f , we also train over ff ( r123g=fsig .
we apply the exten - sion to out - of - sample points to nd the predicted embedding of si and calculate the euclidean distance between this embedding and the one obtained when train - ing with f ( r123 , i . e .
with si in the training set .
we calculate the mean difference ( and its standard error , shown in the gure ) between the distance obtained in step 123 and the one obtained in step 123 for each sample si 123 f , and we repeat this experiment for various sizes of f .
the results obtained for mds , isomap , spectral clustering and lle are shown in gure 123 for different values of m .
experiments are done over a database of 123 synthetic face im - ages described by 123 components that is available at http : / / isomap . stanford . edu .
ionosphere ( http : / / www . ics . uci . edu / ~ mlearn / mlsummary . html ) and swissroll ( http : / / www . cs . toronto . edu / ~ roweis / lle / ) .
each algorithm generates a two - dimensional embedding of the images , following the experiments reported for isomap .
the number of neighbors is 123 for isomap and lle , and a gaussian kernel with a standard deviation of 123 is used for spectral clustering / laplacian eigenmaps .
123% condence
results have been obtained over other databases
intervals are drawn beside each mean difference of error on the gure .
