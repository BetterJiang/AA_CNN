we consider the semi - supervised learning problem , where a decision rule is to be learned from labeled and unlabeled data .
in this framework , we motivate minimum entropy regularization , which enables to incorporate unlabeled data in the standard supervised learning .
our approach in - cludes other approaches to the semi - supervised problem as particular or limiting cases .
a series of experiments illustrates that the proposed solu - tion benets from unlabeled data .
the method challenges mixture mod - els when the data are sampled from the distribution class spanned by the generative model .
the performances are denitely in favor of minimum entropy regularization when generative models are misspecied , and the weighting of unlabeled data provides robustness to the violation of the cluster assumption .
finally , we also illustrate that the method can also be far superior to manifold learning in high dimension spaces .
in the classical supervised learning classication framework , a decision rule is to be learned from a learning set ln = ( xi , yi ) n i=123 , where each example is described by a pattern xi x and by the supervisors response yi = ( 123 , .
we consider semi - supervised learning , where the supervisors responses are limited to a subset of ln .
in the terminology used here , semi - supervised learning refers to learning a decision rule on x from labeled and unlabeled data .
however , the related problem of transductive learning , i . e .
of predicting labels on a set of predened patterns , is addressed as a side issue .
semi - supervised problems occur in many applications where labeling is performed by human experts .
they have been receiving much attention during the last few years , but some important issues are unresolved ( 123 ) .
in the probabilistic framework , semi - supervised learning can be modeled as a missing data problem , which can be addressed by generative models such as mixture models thanks to the em algorithm and extensions thereof ( 123 ) . generative models apply to the joint den - sity of patterns and class ( x , y ) .
they have appealing features , but they also have major drawbacks .
their estimation is much more demanding than discriminative models , since the model of p ( x , y ) is exhaustive , hence necessarily more complex than the model of
this work was supported in part by the ist programme of the european community , under the pascal network of excellence ist - 123 - 123
this publication only reects the authors views .
p ( y |x ) .
more parameters are to be estimated , resulting in more uncertainty in the es - timation process .
the generative model being more precise , it is also more likely to be misspecied .
finally , the tness measure is not discriminative , so that better models are not necessarily better predictors of class labels .
these difculties have lead to proposals aiming at processing unlabeled data in the framework of supervised classication ( 123 , 123 , 123 ) .
here , we propose an estimation principle applicable to any probabilistic classier , aiming at making the most of unlabeled data when they are benecial , while providing a control on their contribution to provide robustness to the learning scheme .
123 derivation of the criterion we rst recall how the semi - supervised learning problem ts into standard supervised learning by using the maximum ( conditional ) likelihood estimation principle .
the learning set is denoted ln = ( xi , zi ) n i=123 , where z ( 123 , 123 ) k denotes the dummy variable rep - resenting the actually available labels ( while y represents the precise and complete class information ) : if xi is labeled k , then zik = 123 and zi` = 123 for ` 123= k; if xi is unlabeled , then zi` = 123 for ` = 123 , .
we assume that labeling is missing at random , p ( z|x , k ) = p ( z|x , ` ) , for any ( k , ` ) pair , which implies
that is , for all unlabeled examples ,
p ( k|x , z ) =
`=123 z`p ( `|x )
assuming independent examples , the conditional log - likelihood of ( z|x ) on the observed sample is then
l ( ; ln ) =
zikfk ( xi; ) ! + h ( zi ) ,
where h ( z ) , which does not depend on p ( x , y ) , is only affected by the missingness mech - anism , and fk ( x; ) is the model of p ( k|x ) parameterized by .
this criterion is a concave function of fk ( xi; ) , and for simple models such as the ones provided by logistic regression , it is also concave in , so that the global solution can be obtained by numerical optimization .
maximizing ( 123 ) corresponds to maximizing the complete likelihood if no assumption whatsoever is made on p ( x ) ( 123 ) .
provided fk ( xi; ) sum to one , the likelihood is not affected by unlabeled data : unlabeled data convey no information .
in the maximum a posteriori ( map ) framework , seeger re - marks that unlabeled data are useless regarding discrimination when the priors on p ( x ) and p ( y |x ) factorize ( 123 ) : observing x does not inform about y , unless the modeler assumes so .
benetting from unlabeled data requires assumptions of some sort on the re - lationship between x and y .
in the bayesian framework , this will be encoded by a prior distribution .
as there is no such thing like a universally relevant prior , we should look for an induction bias exploiting unlabeled data when the latter is known to convey information .
123 when are unlabeled examples informative ? theory provides little support to the numerous experimental evidences ( 123 , 123 , 123 ) showing that unlabeled examples can help the learning process .
learning theory is mostly developed at the two extremes of the statistical paradigm : in parametric statistics where examples are known to be generated from a known class of distribution , and in the distribution - free struc - tural risk minimization ( srm ) or probably approximately correct ( pac ) frameworks .
semi - supervised learning , in the terminology used here , does not t the distribution - free frameworks : no positive statement can be made without distributional assumptions , as for
some distributions p ( x , y ) unlabeled data are non - informative while supervised learning is an easy task .
in this regard , generalizing from labeled and unlabeled data may differ from transductive inference .
in parametric statistics , theory has shown the benet of unlabeled examples , either for spe - cic distributions ( 123 ) , or for mixtures of the form p ( x ) = pp ( x|123 ) + ( 123 p ) p ( x|123 ) where the estimation problem is essentially reduced to the one of estimating the mixture parameter p ( 123 ) .
these studies conclude that the ( asymptotic ) information content of un - labeled examples decreases as classes overlap . 123 thus , the assumption that classes are well separated is sensible if we expect to take advantage of unlabeled examples .
the conditional entropy h ( y |x ) is a measure of class overlap , which is invariant to the parameterization of the model .
this measure is related to the usefulness of unlabeled data where labeling is indeed ambiguous .
hence , we will measure the conditional entropy of class labels conditioned on the observed variables
h ( y |x , z ) = ex y z ( log p ( y |x , z ) ) ,
where ex denotes the expectation with respect to x .
in the bayesian framework , assumptions are encoded by means of a prior on the model parameters .
stating that we expect a high conditional entropy does not uniquely dene the form of the prior distribution , but the latter can be derived by resorting to the maximum entropy principle . 123 let ( , ) denote the model parameters of p ( x , y , z ) ; the maximum entropy prior verifying e ( h ( y |x , z ) ) = c , where the constant c quanties how small the entropy should be on average , takes the form
p ( , ) exp ( h ( y |x , z ) ) ) ,
where is the positive lagrange multiplier corresponding to the constant c .
computing h ( y |x , z ) requires a model of p ( x , y , z ) whereas the choice of the diagno - sis paradigm is motivated by the possibility to limit modeling to conditional probabilities .
we circumvent the need of additional modeling by applying the plug - in principle , which consists in replacing the expectation with respect to ( x , z ) by the sample average .
this substitution , which can be interpreted as modeling p ( x , z ) by its empirical distribution ,
hemp ( y |x , z; ln ) =
p ( k|xi , zi ) log p ( k|xi , zi ) .
this empirical functional is plugged in ( 123 ) to dene an empirical prior on parameters , that is , a prior whose form is partly dened from data ( 123 ) .
123 entropy regularization
recalling that fk ( x; ) denotes the model of p ( k|x ) , the model of p ( k|x , z ) ( 123 ) is dened as follows :
gk ( x , z; ) =
`=123 z`f` ( x; )
for labeled data , gk ( x , z; ) = zk , and for unlabeled data , gk ( x , z; ) = fk ( x; ) .
from now on , we drop the reference to parameter in fk and gk to lighten notation
123this statement , given explicitly by ( 123 ) , is also formalized , though not stressed , by ( 123 ) , where the fisher information for unlabeled examples at the estimate p is clearly a measure of the overlap between class conditional densities : iu ( p ) = r
pp ( x|123 ) + ( 123 p ) p ( x|123 ) dx .
( p ( x|123 ) p ( x|123 ) ) 123
123here , maximum entropy refers to the construction principle which enables to derive distributions
from constraints , not to the content of priors regarding entropy .
map estimate is the maximizer of the posterior distribution , that is , the maximizer of
c ( , ; ln ) = l ( ; ln ) hemp ( y |x , z; ln )
gk ( xi , zi ) log gk ( xi , zi ) , ( 123 )
where the constant terms in the log - likelihood ( 123 ) and log - prior ( 123 ) have been dropped .
while l ( ; ln ) is only sensitive to labeled data , hemp ( y |x , z; ln ) is only affected by the value of fk ( x ) on unlabeled data .
note that the approximation hemp ( 123 ) of h ( 123 ) breaks down for wiggly functions fk ( ) with abrupt changes between data points ( where p ( x ) is bounded from below ) .
as a result , it is important to constrain fk ( ) in order to enforce the closeness of the two functionals .
in the following experimental section , we imposed a smoothness constraint on fk ( ) by adding to the criterion c ( 123 ) a penalizer with its corresponding lagrange multiplier .
123 related work self - training self - training ( 123 ) is an iterative process , where a learner imputes the labels of examples which have been classied with condence in the previous step .
amini et al .
( 123 ) analyzed this technique and shown that it is equivalent to a version of the classication em algorithm , which minimizes the likelihood deprived of the entropy of the partition .
in the context of conditional likelihood with labeled and unlabeled examples , the criterion is
gk ( xi ) log gk ( xi ) ,
which is recognized as an instance of the criterion ( 123 ) with = 123
self - condent logistic regression ( 123 ) is another algorithm optimizing the criterion for = 123
using smaller values is expected to have two benets : rst , the inuence of unlabeled examples can be controlled , in the spirit of the em - ( 123 ) , and second , slowly increasing denes a scheme similar to deterministic annealing , which should help the optimization process to avoid poor local minima of the criterion .
minimum entropy methods minimum entropy regularizers have been used in other con - texts to encode learnability priors ( e . g .
in a sense , hemp can be seen as a poors man way to generalize this approach to continuous input spaces .
this empirical functional was also used by zhu et al .
( 123 , section 123 ) as a criterion to learn weight function parameters in the context of transduction on manifolds for learning .
input - dependent regularization our criterion differs from input - dependent regular - ization ( 123 , 123 ) in that it is expressed only in terms of p ( y |x , z ) and does not involve p ( x ) .
however , we stress that for unlabeled data , the regularizer agrees with the complete likelihood provided p ( x ) is small near the decision surface .
indeed , whereas a genera - tive model would maximize log p ( x ) on the unlabeled data , our criterion minimizes the conditional entropy on the same points .
in addition , when the model is regularized ( e . g .
with weight decay ) , the conditional entropy is prevented from being too small close to the decision surface .
this will favor putting the decision surface in a low density area .
123 articial data in this section , we chose a simple experimental setup in order to avoid artifacts stemming from optimization problems .
our goal is to check to what extent supervised learning can be improved by unlabeled examples , and if minimum entropy can compete with generative models which are usually advocated in this framework .
the minimum entropy regularizer is applied to the logistic regression model .
it is compared to logistic regression tted by maximum likelihood ( ignoring unlabeled data ) and logistic regression with all labels known .
the former shows what has been gained by handling unlabeled data , and the latter provides the crystal ball performance obtained by guessing correctly all labels .
all hyper - parameters ( weight - decay for all logistic regression models plus the parameter ( 123 ) for minimum entropy ) are tuned by ten - fold cross - validation .
minimum entropy logistic regression is also compared to the classic em algorithm for gaussian mixture models ( two means and one common covariance matrix estimated by maximum likelihood on labeled and unlabeled examples , see e . g .
bad local maxima of the likelihood function are avoided by initializing em with the parameters of the true distribution when the latter is a gaussian mixture , or with maximum likelihood parameters on the ( fully labeled ) test sample when the distribution departs from the model .
this ini - tialization advantages em , since it is guaranteed to pick , among all local maxima of the likelihood , the one which is in the basin of attraction of the optimal value .
furthermore , this initialization prevents interferences that may result from the pseudo - labels given to unlabeled examples at the rst e - step .
in particular , label switching ( i . e .
badly labeled clusters ) is avoided at this stage .
correct joint density model in the rst series of experiments , we consider two - class problems in an 123 - dimensional input space .
each class is generated with equal probability from a normal distribution .
class 123 is normal with mean ( aa .
a ) and unit covariance matrix .
class 123 is normal with mean ( aa .
a ) and unit covariance matrix .
parameter a tunes the bayes error which varies from 123 % to 123 % ( 123 % , 123 % , 123 % , 123 % , 123 % ) .
the learning sets comprise nl labeled examples , ( nl = 123 , 123 , 123 ) and nu unlabeled examples , ( nu = nl ( 123 , 123 , 123 , 123 , 123 ) ) .
overall , 123 different setups are evaluated , and for each one , 123 different training samples are generated .
generalization performances are estimated on a test set of size 123 123
this benchmark provides a comparison for the algorithms in a situation where unlabeled data are known to convey information .
besides the favorable initialization of the em al - gorithm to the optimal parameters , em benets from the correctness of the model : data were generated according to the model , that is , two gaussian subpopulations with identical covariances .
the logistic regression model is only compatible with the joint distribution , which is a weaker fulllment than correctness .
as there is no modeling bias , differences in error rates are only due to differences in estima - tion efciency .
the overall error rates ( averaged over all settings ) are in favor of minimum entropy logistic regression ( 123 123 % ) .
em ( 123 123 % ) does worse on average than logistic regression ( 123 123 % ) .
for reference , the average bayes error rate is 123 % and logistic regression reaches 123 123 % when all examples are labeled .
figure 123 provides more informative summaries than these raw numbers .
the plots repre - sent the error rates ( averaged over nl ) versus bayes error rate and the nu / nl ratio .
the rst plot shows that , as asymptotic theory suggests ( 123 , 123 ) , unlabeled examples are mostly informative when the bayes error is low .
this observation validates the relevance of the minimum entropy assumption .
this graph also illustrates the consequence of the demand - ing parametrization of generative models .
mixture models are outperformed by the simple logistic regression model when the sample size is low , since their number of parameters grows quadratically ( vs .
linearly ) with the number of input features .
the second plot shows that the minimum entropy model takes quickly advantage of un - labeled data when classes are well separated .
with nu = 123nl , the model considerably improves upon the one discarding unlabeled data .
at this stage , the generative models do not perform well , as the number of available examples is low compared to the number of parameters in the model .
however , for very large sample sizes , with 123 times more unla -
bayes error ( % )
figure 123 : left : test error vs .
bayes error rate for nu / nl = 123; right : test error vs .
nu / nl ratio for 123 % bayes error ( a = 123 ) .
test errors of minimum entropy logistic regression ( ) and mixture models ( + ) .
the errors of logistic regression ( dashed ) , and logistic regression with all labels known ( dash - dotted ) are shown for reference .
beled examples than labeled examples , the generative approach eventually becomes more accurate than the diagnosis approach .
misspecied joint density model in a second series of experiments , the setup is slightly modied by letting the class - conditional densities be corrupted by outliers .
for each class , the examples are generated from a mixture of two gaussians centered on the same mean : a unit variance component gathers 123 % of examples , while the remaining 123 % are gener - ated from a large variance component , where each variable has a standard deviation of 123
the mixture model used by em is slightly misspecied since it is a simple gaussian mix - ture .
the results , displayed in the left - hand - side of figure 123 , should be compared with the right - hand - side of figure 123
the generative model dramatically suffers from the misspec - ication and behaves worse than logistic regression for all sample sizes .
the unlabeled examples have rst a benecial effect on test error , then have a detrimental effect when they overwhelm the number of labeled examples .
on the other hand , the diagnosis models behave smoothly as in the previous case , and the minimum entropy criterion performance
figure 123 : test error vs .
nu / nl ratio for a = 123 .
average test errors for minimum entropy logistic regression ( ) and mixture models ( + ) .
the test error rates of logistic regression ( dotted ) , and logistic regression with all labels known ( dash - dotted ) are shown for refer - ence .
left : experiment with outliers; right : experiment with uninformative unlabeled data .
the last series of experiments illustrate the robustness with respect to the cluster assump - tion , by testing it on distributions where unlabeled examples are not informative , and where a low density p ( x ) does not indicate a boundary region .
the data is drawn from two gaus - sian clusters like in the rst series of experiment , but the label is now independent of the clustering : an example x belongs to class 123 if x123 > x123 and belongs to class 123 otherwise :
the bayes decision boundary is now separates each cluster in its middle .
the mixture model is unchanged .
it is now far from the model used to generate data .
the right - hand - side plot of figure 123 shows that the favorable initialization of em does not prevent the model to be fooled by unlabeled data : its test error steadily increases with the amount of unlabeled data .
on the other hand , the diagnosis models behave well , and the minimum entropy algorithm is not distracted by the two clusters; its performance is nearly identical to the one of train - ing with labeled data only ( cross - validation provides values close to zero ) , which can be regarded as the ultimate performance in this situation .
comparison with manifold transduction although our primary goal is to infer a deci - sion function , we also provide comparisons with a transduction algorithm of the manifold family .
we chose the consistency method of zhou et al .
( 123 ) for its simplicity .
as sug - gested by the authors , we set = 123 and the scale parameter 123 was optimized on test results ( 123 ) .
the results are reported in table 123
the experiments are limited due to the memory requirements of the consistency method in our naive matlab implementation .
table 123 : error rates ( % ) of minimum entropy ( me ) vs .
consistency method ( cm ) , for a = 123 , nl = 123 , and a ) pure gaussian clusters b ) gaussian clusters corrupted by outliers c ) class boundary separating one gaussian cluster
a ) cm 123 123 b ) cm 123 123 c ) cm 123 123
the results are extremely poor for the consistency method , whose error is way above min - imum entropy , and which does not show any sign of improvement as the sample of unla - beled data grows .
furthermore , when classes do not correspond to clusters , the consistency method performs random class assignments .
in fact , our setup , which was designed for the comparison of global classiers , is extremely defavorable to manifold methods , since the data is truly 123 - dimensional .
in this situation , local methods suffer from the curse of dimensionality , and many more unlabeled examples would be required to get sensible results .
hence , these results mainly illustrate that manifold learning is not the best choice in semi - supervised learning for truly high dimensional data .
123 facial expression recognition we now consider an image recognition problem , consisting in recognizing seven ( balanced ) classes corresponding to the universal emotions ( anger , fear , disgust , joy , sadness , surprise and neutral ) .
the patterns are gray level images of frontal faces , with standardized posi - tions .
the data set comprises 123 such pictures made of 123 123 pixels .
we tested kernelized logistic regression ( gaussian kernel ) , its minimum entropy version , nearest neigbor and the consistency method .
we repeatedly ( 123 times ) sampled 123 / 123 of the dataset for providing the labeled part , and the remainder for testing .
although ( , 123 ) were chosen to minimize the test error , the consistency method performed poorly with 123 . 123 % test error ( compared to 123 % error for random assignments ) .
nearest - neighbor get similar results with 123 123 % test error , and kernelized logistic regression ( ignoring unlabeled examples ) improved to reach 123 . 123 % .
minimum entropy kernelized logistic regression regression achieves 123 123 % error ( compared to about 123 % errors for human on this database ) .
the scale parameter chosen for kernelized logistic regression ( by ten - fold cross - validation ) amount to use a global classier .
again , the local methods
this may be explained by the fact that the database contains several pictures of each person , with different facial expressions .
hence , local methods are likely to pick the same identity instead of the same expression , while global methods are able to learn the relevant we propose to tackle the semi - supervised learning problem in the supervised learning framework by using the minimum entropy regularizer .
this regularizer is motivated by the - ory , which shows that unlabeled examples are mostly benecial when classes have small overlap .
the map framework provides a means to control the weight of unlabeled exam - ples , and thus to depart from optimism when unlabeled data tend to harm classication .
our proposal encompasses self - learning as a particular case , as minimizing entropy in - creases the condence of the classier output .
it also approaches the solution of transduc - tive large margin classiers in another limiting case , as minimizing entropy is a means to drive the decision boundary from learning examples .
