in order to compare learning algorithms , experimental results reported in the machine learning literature often use statistical tests of signi ( cid : 123 ) cance to support the claim that a new learning algorithm generalizes better .
such tests should take into account the variability due to the choice of training set and not only that due to the test examples , as is often the case .
this could lead to gross underestimation of the variance of the cross - validation estimator , and to the wrong conclusion that the new algorithm is signi ( cid : 123 ) cantly better when it is not .
we perform a theoretical investigation of the variance of a cross - validation estimator of the generalization error that takes into account the variability due to the randomness of the training set as well as test examples .
our analysis shows that all the variance estimators that are based only on the results of the cross - validation experiment must be biased .
this analysis allows us to propose new estimators of this variance .
we show , via simulations , that tests of hypothesis about the generalization error using those new variance estimators have better properties than tests involving variance estimators currently in use and listed in ( dietterich , 123 ) .
in particular , the new tests have correct size and good power .
that is , the new tests do not reject the null hypothesis too often when the hypothesis is true , but they tend to frequently reject the null hypothesis when the latter is false .
keywords : generalization error , cross - validation , variance estimation , hypothesis tests ,
corresponding author : yoshua bengio , dept .
iro , universit ( cid : 123 ) e de montr ( cid : 123 ) eal , 123 chemin de la tour , montr ( cid : 123 ) eal , qu ( cid : 123 ) ebec , canada h123t 123j123 , suite #123
tel : ( 123 ) 123 - 123 , fax : ( 123 ) 123 - 123 e - mail : yoshua . bengio@umontreal . ca
123 generalization error and its estimation
in order to compare learning algorithms , experimental results reported in the machine learning literature often use statistical tests of signi ( cid : 123 ) cance .
unfortunately , these tests often rely solely on the variability due to the test examples and do not take into account the variability due to the randomness of the training set .
we perform a theoretical investigation of the variance of a cross - validation estimator of the generalization error that takes into account the variability due to the choice of training sets as well as of test examples ( hold - out set ) .
when applying a learning algorithm ( or comparing several algorithms ) , one is typically interested in estimating its generalization error .
its estimation is rather trivial through cross - validation or the bootstrap .
providing a variance estimate of the cross - validation estimator , so that hypothesis testing and / or con ( cid : 123 ) dence intervals are possible , is more di ( cid : 123 ) cult , especially , as pointed out in ( hinton , neal , tibshirani , & delve team members , 123 ) , if one wants to take into account various sources of variability such as the choice of the training set ( breiman , 123 ) or initial conditions of a learning algorithm ( kolen & pollack , 123 ) .
a notable e ( cid : 123 ) ort in that direction is dietterichs work ( dietterich , 123 ) .
see also the review of bounds of the accuracy of various cross - validation estimates in ( devroye , gyro ( cid : 123 ) , & lugosi , 123 ) .
building upon ( dietterich , 123 ) , in this paper we take into account the variability due to the choice of training sets and test examples .
speci ( cid : 123 ) cally , an investigation of the variance to be estimated allows us to provide two new variance estimators , one of which is conservative by construction .
the choice of estimator for the variance of an average test error ( or of the di ( cid : 123 ) erence between the average error made by two algorithms ) is very important : a poor choice of estimator ( especially if it is liberal , i . e .
underestimates the variance ) could lead to a profusion of publications in which method a is incorrectly claimed to be better than a previously proposed method b .
because of the approximately normal behavior of average error , an underestimation of the standard deviation by a factor 123 , say , can yield to about 123 times more \false claims " than would have been expected for a test level of 123% .
if the habit of making rigorous statistical comparisons and in particular avoiding the use of a liberal estimator is not ingrained in the machine learning community , it could be tempting for many researchers to use a liberal estimate of variance when comparing their preferred algorithm against the competition .
for this reason , it is very important that reviewers insist on analyses of results that avoid liberal estimators of variance ( for con ( cid : 123 ) dence intervals or to test the null hypothesis of method a being not better than method b ) .
let us de ( cid : 123 ) ne what we mean by \generalization error " and say how it will be estimated in 123 = fz123; : : : ; zng .
for example , this paper .
we assume that data is available in the form z n in the case of supervised learning , zi = ( xi; yi ) 123 z ( cid : 123 ) rp+q , where p and q denote the dimensions of the xis ( inputs ) and the yis ( outputs ) .
we also assume that the zis are independent with zi ( cid : 123 ) p ( z ) , where the generating distribution p is unknown .
let l ( d; z ) , where d represents a subset of size n123 ( cid : 123 ) n taken from z n 123 , be a function from z n123 ( cid : 123 ) z to r .
for instance , this function could be the loss incurred by the decision that a learning algorithm trained on d makes on a new example z .
123 ; zn+123 ) ) where zn+123 ( cid : 123 ) p ( z ) is independent 123 here ) .
note that the above
we are interested in estimating n ( cid : 123 ) ( cid : 123 ) e ( l ( z n 123 .
the subscript n stands for the size of the training set ( z n
of z n
expectation is taken over z n 123 and zn+123 , meaning that we are interested in the performance of an algorithm rather than the performance of the speci ( cid : 123 ) c decision function it yields on the data at hand .
dietterich ( dietterich , 123 ) has introduced a taxonomy of statistical questions in machine learning , which we briey summarize here .
at the top level is whether the questions refer to single or multiple domains ( type 123 ) .
for single domains , dietterich distinguishes between the analysis of a single ( given or already trained ) predictor ( types 123 through 123 ) and the analysis of a learning algorithm that can yield such predictors given a training set ( types 123 through 123 ) .
for the former , the training set is considered ( cid : 123 ) xed , whereas for the latter the training set is considered to be random .
in this paper , we are concerned with the analysis of the performance of learning algorithms ( types 123 through 123 ) , not of particular trained predictors .
dietterich further splits types 123 through 123 according to whether the sample size is large or not and whether one is interested in the generalization error of a single algorithm or wants to compare the generalization errors of various algorithms .
let us now introduce some notation and de ( cid : 123 ) nitions .
we shall call n ( cid : 123 ) the generalization
error even though it can go beyond that as we now illustrate .
here are two examples .
( cid : 123 ) generalization error
we may take as our basic measurement
l ( d; z ) = l ( d; ( x; y ) ) = q ( f ( d ) ( x ) ; y ) ;
where f represents a learning algorithm that yields a function f = f ( d ) ( f ( d ) : rp ! rq ) , when training the algorithm on d , and q is a loss function measur - ing the inaccuracy of a decision f ( x ) when y is observed .
for instance , for classi ( cid : 123 ) cation problems , we could have
q ( ^y; y ) = i ( ^y 123= y ) ;
) is the indicator function , and in the case of regression ,
q ( ^y; y ) =k ^y y k123;
where k ( cid : 123 ) k is the euclidean norm .
in that case n ( cid : 123 ) = e ( l ( z n tion error of algorithm f on data sampled from p .
123 ; zn+123 ) ) is the generaliza -
( cid : 123 ) comparison of generalization errors
sometimes , what we are interested in is not the performance of algorithms per se , but how two algorithms compare with each other .
in that case we may want to consider
l ( d; z ) = l ( d; ( x; y ) ) = q ( fa ( d ) ( x ) ; y ) q ( fb ( d ) ( x ) ; y ) ;
where fa ( d ) and fb ( d ) are decision functions obtained when training two algorithms ( respectively a and b ) on d , and q is a loss function .
in this case n ( cid : 123 ) would be a di ( cid : 123 ) erence of generalization errors .
the generalization error is often estimated via some form of cross - validation .
since there
are various versions of the latter , we lay out the speci ( cid : 123 ) c form we use in this paper .
j = f123; : : : ; ng n sj denote the complement of sj .
( cid : 123 ) let sj be a random set of n123 distinct integers from f123; : : : ; ng ( n123 < n ) .
here n123 is not random and represents the size of a training set .
we shall let n123 = n n123 be the size of the corresponding test set ( or hold - out set ) .
( cid : 123 ) let s123; : : : sj be such random index sets ( of size n123 ) , sampled independently of each other , and let sc ( cid : 123 ) let zsj = fziji 123 sjg be the training set obtained by subsampling z n 123 according to the = fziji 123 sc random index set sj .
the corresponding test set ( or hold - out set ) is zsc ( cid : 123 ) let l ( j; i ) = l ( zsj ; zi ) .
according to ( 123 ) , this could be the error an algorithm trained on the training set zsj makes on example zi .
according to ( 123 ) , this could be the di ( cid : 123 ) erence of such errors for two di ( cid : 123 ) erent algorithms .
( cid : 123 ) let ^ ( cid : 123 ) j = 123
l ( j; i ) denote the usual \average test error " measured on the test set
then the cross - validation estimate of the generalization error considered in this paper is
n123 ^ ( cid : 123 ) j =
note that this an unbiased estimator of n123 ( cid : 123 ) = e ( l ( z n123
123 ; zn+123 ) ) , which is not quite the same
this paper is about the estimation of the variance of the above cross - validation estimator .
there are many variants of cross - validation , and the above variant is close to the popular k - fold cross - validation estimator , which has been found more reliable than the leave - one - out estimator ( kohavi , 123 ) .
it should be noted that our goal in this paper is not to compare algorithms in order to perform model selection ( i . e .
to choose exactly one among several learning algorithms for a particular task , given a data set on which to train them ) .
the use of cross - validation estimators for model selection has sparked a debate in the last few years ( zhu & rohwer , 123; goutte , 123 ) related to the \no free lunch theorem " ( wolpert & macready , 123 ) , since cross - validation model selection often works well in practice but it is probably not a universally good procedure .
this paper does not address the issue of model selection but rather that of estimating the uncertainty in a cross - validation type of estimator for generalization error , namely n123 for this purpose , this paper studies estimators of the variance of n123 n123 ^ ( cid : 123 ) j ( in the sense that 123 had been sampled di ( cid : 123 ) erent values of n123 from the same unknown underlying distribution p and di ( cid : 123 ) erent random index sets sjs had been generated ) .
the application of the estimators studied in this paper may be for example ( 123 ) to provide con ( cid : 123 ) dence intervals around estimated generalization error , or ( 123 ) to perform a hypothesis test in order to determine whether an algorithms estimated performance is
n123 ^ ( cid : 123 ) j would have been obtained if a di ( cid : 123 ) erent data set z n
signi ( cid : 123 ) cantly above or below the performance of another algorithm .
the latter is very important when researchers introduce a new learning algorithm and they want to show that it brings a signi ( cid : 123 ) cant improvement with respect to previously known algorithms .
we ( cid : 123 ) rst study theoretically the variance of n123
n123 ^ ( cid : 123 ) j in section 123
this will lead us to two new variance estimators we develop in section 123
section 123 shows how to test hypotheses or construct con ( cid : 123 ) dence intervals .
section 123 describes a simulation study we performed to see how the proposed statistics behave compared to statistics already in use .
section 123 concludes the paper .
before proceeding with the rest of the paper , some readers may prefer to read appendix a . 123 that presents some statistical prerequisites relevant to the rest of the paper .
123 analysis of v ar ( n123 in this section , we study v ar ( n123 n123 ^ ( cid : 123 ) j ) and discuss the di ( cid : 123 ) culty of estimating it .
this section is important as it enables us to understand why some inference procedures about n123 ( cid : 123 ) presently in use are inadequate , as we shall underline in section 123
this investigation also enables us to develop estimators of v ar ( n123 n123 ^ ( cid : 123 ) j ) in section 123
before we proceed , we state a lemma that will prove useful in this section , and later ones as well .
lemma 123 let u123; : : : ; uk be random variables with common mean ( cid : 123 ) and the following co -
v ar ( uk ) = ( cid : 123 ) ; 123k
c ov ( uk; uk123 ) = ; 123k 123= k
( cid : 123 ) be the correlation between uk and uk123 ( k 123= k k=123 ( uk ( cid : 123 ) u ) 123 be the sample mean and sample variance respectively
let ( cid : 123 ) u = k
k=123 uk and
let ( cid : 123 ) = u = 123 123
v ar ( ( cid : 123 ) u ) = + ( ( cid : 123 ) )
k = ( cid : 123 )
( cid : 123 ) + 123 ( cid : 123 )
if the stated covariance structure holds for all k ( with and ( cid : 123 ) not depending on k ) ,
( cid : 123 ) ( cid : 123 ) 123 , ( cid : 123 ) limk ! 123v ar ( ( cid : 123 ) u ) = 123 , = 123
u ) = ( cid : 123 ) .
this result is obtained from a standard development of v ar ( ( cid : 123 ) u ) .
if < 123 , then v ar ( ( cid : 123 ) u ) would eventually become negative as k is increased .
we thus conclude that ( cid : 123 ) 123
from item 123 , it is obvious that v ar ( ( cid : 123 ) u ) goes to zero as k goes to in ( cid : 123 ) nity if and only if = 123
again , this only requires careful development of the expectation .
the task is somewhat
easier if one uses the identity
( cid : 123 ) u 123 ) =
although we only need it in section 123 , it is natural to introduce a second lemma here as
it is a continuation of lemma 123
lemma 123 let u123; : : : ; uk ; uk+123 be random variables with mean , variance and covariance as described in lemma 123
in addition , assume that the vector ( u123; : : : ; uk ; uk+123 ) follows the multivariate gaussian distribution .
again , let ( cid : 123 ) u = k u = 123 ( cid : 123 ) u ) 123 be respectively the sample mean and sample variance of u123; : : : ; uk
k=123 uk and s123
123 ( cid : 123 ) uk+123 ( cid : 123 ) p
where ( cid : 123 ) = proof see appendix a . 123
( cid : 123 ) as in lemma 123 , and tk123 refers to students t distribution with ( k 123 ) degrees
to study v ar ( n123
n123 ^ ( cid : 123 ) j ) we need to de ( cid : 123 ) ne the following covariances .
in the following , sj and sj123 are independent random index sets , each consisting of n123 distinct integers from f123; : : : ; ng .
also , expectations are totally unconditional , that is expectations ( as well as variances and covariances ) are taken over z n
123 , sj , sj123 , i and i
123 ; zn123+123 ) ) ) + v arsj;i ( n123 ( cid : 123 ) ) = v ar ( l ( z n123
to establish that ( l ( zsj ; zi ) jsj; i ) ) + ( l ( zsj ; zi ) jsj; i ) ) .
now the distribution of l ( zsj ; zi ) does not depend on 123 ; zn123+123 ) .
thus 123 ; zn123+123 ) ) depends only on
( cid : 123 ) let ( cid : 123 ) 123 = ( cid : 123 ) 123 ( n123 ) = v ar ( l ( j; i ) ) when i is randomly drawn from sc ( cid : 123 ) 123 does not depend on n123 we note that v ar ( l ( j; i ) ) = esj ;i ( v arzn v arsj ;i ( ezn the particular realization of sj and i , it is just the distribution of l ( z n123 ( cid : 123 ) 123 = esj ;i ( v ar ( l ( z n123 n123 , not on n123
( cid : 123 ) let ( cid : 123 ) 123 = ( cid : 123 ) 123 ( n123; n123 ) = v ar ( ^ ( cid : 123 ) j ) .
( cid : 123 ) let ( cid : 123 ) 123 = ( cid : 123 ) 123 ( n123; n123 ) = c ov ( l ( j; i ) ; l ( j dently drawn from sc ( cid : 123 ) let ( cid : 123 ) 123 = ( cid : 123 ) 123 ( n123 ) = c ov ( l ( j; i ) ; l ( j; i sampled without replacement from sc show that ( cid : 123 ) 123 does not depend on n123
123 ) ) for i; i j .
using a similar argument as for ( cid : 123 ) 123 allows one to
123 , that is i and i
123 randomly and indepen -
123 ) ) , with j 123= j
j and i 123= i
123 , i and i
123 123 sc
j and sc
let us look at the mean and variance of ^ ( cid : 123 ) j ( i . e . , over one set ) and n123
concerning expectations , we obviously have e ( ^ ( cid : 123 ) j ) = n123 ( cid : 123 ) and thus e ( n123 lemma 123 , we have
n123 ^ ( cid : 123 ) j ( i . e .
over j sets ) .
n123 ^ ( cid : 123 ) j ) = n123 ( cid : 123 )
( cid : 123 ) 123 = ( cid : 123 ) 123 ( n123; n123 ) = v ar ( ^ ( cid : 123 ) j ) = ( cid : 123 ) 123 + ( cid : 123 ) 123 ( cid : 123 ) 123
( n123 123 ) ( cid : 123 ) 123 + ( cid : 123 ) 123
for j 123= j
123 , we have
c ov ( ^ ( cid : 123 ) j; ^ ( cid : 123 ) j123 ) =
and therefore ( using lemma 123 again )
n123 ^ ( cid : 123 ) j ) = ( cid : 123 ) 123 + ( cid : 123 ) 123 ( cid : 123 ) 123
v ar ( n123
c ov ( l ( j; i ) ; l ( j
) ) = ( cid : 123 ) 123;
= ( cid : 123 ) 123 + ( cid : 123 ) 123 ( cid : 123 ) 123
+ ( cid : 123 ) 123 ( cid : 123 ) 123
where ( cid : 123 ) = ( cid : 123 ) 123 = corr ( ^ ( cid : 123 ) j; ^ ( cid : 123 ) j123 ) : asking how to choose j amounts to asking how large is ( cid : 123 ) .
if it is large , then taking j > 123 ( rather than j = 123 ) does not provide much improvement in the estimation of n123 ( cid : 123 ) .
we provide some guidance on the choice of j in section 123
equation ( 123 ) lends itself to an interesting interpretation .
first we get that ( cid : 123 ) 123 = v ar ( n123
n123 ^ ( cid : 123 ) 123 = lim
n123 ^ ( cid : 123 ) j = lim
where c ( f123; : : : ; ng; n123 ) , as de ( cid : 123 ) ned in appendix a . 123 , is the set of all possible subsets of n123 distinct integers from f123; : : : ; ng .
we justify the last equality as follows .
what happens when n123 di ( cid : 123 ) erent ways to choose a training j goes to in ( cid : 123 ) nity is that all possible errors ( there are n123 ^ ( cid : 123 ) 123 is like set and a test example ) appear with relative frequency possible training sets are chosen exactly once .
briey , sampling n123 ^ ( cid : 123 ) j except that all in ( cid : 123 ) nitely often with replacement is equivalent to sampling exhaustively without replacement n123 ^ ( cid : 123 ) jjz n 123 ) .
a census ) .
we also have n123 thus ( cid : 123 ) 123 = v ar ( e ( n123 we shall often encounter ( cid : 123 ) 123; ( cid : 123 ) 123; ( cid : 123 ) 123 and ( cid : 123 ) 123 in the future , so some knowledge about those
123 ) ) so that we must have e ( v ar ( n123
123 ) 123j and therefore n123
n123 ^ ( cid : 123 ) 123 = esj ( ^ ( cid : 123 ) jjz n
in other words , n123
n123 ^ ( cid : 123 ) 123 = e ( n123
n123 ^ ( cid : 123 ) jjz n
n123 ^ ( cid : 123 ) jjz n
123 ) ) = ( cid : 123 ) 123 ( cid : 123 ) 123
quantities is valuable .
heres what we can say about them .
proposition 123 for given n123 and n123 , we have 123 ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) 123 and 123 ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) 123
proof for j 123= j
123 we have
( cid : 123 ) 123 = c ov ( ^ ( cid : 123 ) j; ^ ( cid : 123 ) j123 ) ( cid : 123 )
v ar ( ^ ( cid : 123 ) j ) v ar ( ^ ( cid : 123 ) j123 ) = ( cid : 123 ) 123 :
since ( cid : 123 ) 123 = v ar ( l ( j; i ) ) ; i 123 sc v ar ( l ( j; i ) ) = ( cid : 123 ) 123
the fact that limj ! 123 v ar ( n123
j and ^ ( cid : 123 ) j is the mean of the l ( j; i ) s , then ( cid : 123 ) 123 = v ar ( ^ ( cid : 123 ) j ) ( cid : 123 )
n123 ^ ( cid : 123 ) j ) = ( cid : 123 ) 123 provides the inequality 123 ( cid : 123 ) ( cid : 123 ) 123
regarding ( cid : 123 ) 123 , we deduce ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) 123 from ( 123 ) while 123 ( cid : 123 ) ( cid : 123 ) 123 is derived from the fact that
limn123 ! 123 v ar ( ^ ( cid : 123 ) j ) = ( cid : 123 ) 123
naturally the inequalities are strict provided l ( j; i ) is not perfectly correlated with l ( j; i ^ ( cid : 123 ) j is not perfectly correlated with ^ ( cid : 123 ) j123 , and the variances used in the proof are positive .
n123 ^ ( cid : 123 ) j is how n123 , n123 and j a ( cid : 123 ) ect its variance .
a natural question about the estimator n123
proposition 123 the variance of n123
n123 ^ ( cid : 123 ) j is non - increasing in j and n123
( cid : 123 ) v ar ( n123
n123 ^ ( cid : 123 ) j ) is non - increasing ( decreasing actually , unless ( cid : 123 ) 123 = ( cid : 123 ) 123 ) in j as obviously seen from ( 123 ) .
this means that averaging over many train / test improves the estimation
( cid : 123 ) from ( 123 ) , we see that to show that v ar ( n123
n123 ^ ( cid : 123 ) j ) is non - increasing in n123 , it is su ( cid : 123 ) cient to show that ( cid : 123 ) 123 and ( cid : 123 ) 123 are non - increasing in n123
for ( cid : 123 ) 123 , this follows from ( 123 ) and proposition 123
regarding ( cid : 123 ) 123 , we show in appendix a . 123 that it is non - increasing in n123
all this to say that for a given n123 , the larger the test set size , the better the estimation
the behavior of v ar ( n123
n123 ^ ( cid : 123 ) j ) with respect to n123 is unclear , but we conjecture as follows .
the variability in n123
n123 ^ ( cid : 123 ) j ) should decrease in n123
conjecture 123 in most situations , v ar ( n123 n123 ^ ( cid : 123 ) j comes from two sources : sampling decision rules ( training process ) and sampling testing examples .
holding n123 and j ( cid : 123 ) xed freezes the second source of variation as it solely depends on those two quantities , not n123
the problem to solve becomes : how does n123 a ( cid : 123 ) ect the ( cid : 123 ) rst source of variation ? it is not unreasonable to expect that the decision function yielded by a \stable " learning algorithm is less variable when the training set is larger .
see ( kearns & ron , 123 ) showing that for a large class of algorithms including those minimizing training error , cross - validation estimators are not much worse than the training error estimator ( which itself improves in o ( v cdim=n123 ) as the size of the training set increases ( vapnik , 123 ) ) .
therefore we conclude that , for many cases of interest , n123 ^ ( cid : 123 ) j ) ) is decreasing in the ( cid : 123 ) rst source of variation , and thus the total variation ( that is v ar ( n123
regarding the estimation of v ar ( n123
asedly ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) , ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) and ( ( cid : 123 ) 123 + ( n123 ( cid : 123 ) ) 123 ) .
n123 ^ ( cid : 123 ) j ) , we show below that we can easily estimate unbi -
( cid : 123 ) from lemma 123 , we obtain readily that the sample variance of the ^ ( cid : 123 ) js ( call it s123 in equation ( 123 ) ) is an unbiased estimate of ( cid : 123 ) 123 ( cid : 123 ) 123 = ( cid : 123 ) 123 ( cid : 123 ) 123 + ( cid : 123 ) 123 ( cid : 123 ) 123 this result .
given z n
let us interpret 123 , the ^ ( cid : 123 ) js are j independent draws ( with replacement ) from a hat possible values of the ^ ( cid : 123 ) js .
the sample variance of those j observations 123 , i . e .
an unbiased
) is therefore an unbiased estimator of the variance of ^ ( cid : 123 ) j , given z n
123here we are not trying to prove the conjecture but to justify our intuition that it is correct .
estimator of v ar ( ^ ( cid : 123 ) jjz n expectation of the sample variance .
indeed , we have
123 ) , not v ar ( ^ ( cid : 123 ) j ) .
this permits an alternative derivation of the
^ ( cid : 123 ) j ) = ezn
123 ) ) = ezn 123 ;sj ( ^ ( cid : 123 ) j ) v arzn
( v arsj ( ^ ( cid : 123 ) jjz n ( esj ( ^ ( cid : 123 ) jjz n
= v arz n
n123 ^ ( cid : 123 ) 123 ) = ( cid : 123 ) 123 ( cid : 123 ) 123; where s : denotes the random index sets ( s123; : : : ; sj ) .
note that e ( ^ ( cid : 123 ) jjz n n123 ^ ( cid : 123 ) 123 and 123 ) = n123 v ar ( n123
n123 ^ ( cid : 123 ) 123 ) = ( cid : 123 ) 123 both come from the discussion after equation ( 123 ) .
123 ) ) = ( cid : 123 ) 123 v arzn
( cid : 123 ) for a given j , the sample variance of the l ( j; i ) s ( i 123 sc j ) is unbiased for ( cid : 123 ) 123 ( cid : 123 ) 123 according to lemma 123 again .
we may average these sample variances over j to obtain a more accurate estimate of ( cid : 123 ) 123 ( cid : 123 ) 123
( cid : 123 ) from equation ( 123 ) we have e ( ^ ( cid : 123 ) j ^ ( cid : 123 ) j123 ) = ( cid : 123 ) 123 + ( n123 ( cid : 123 ) ) 123 for j 123= j the ^ ( cid : 123 ) j ^ ( cid : 123 ) j123 will be unbiased for ( ( cid : 123 ) 123 + ( n123 ( cid : 123 ) ) 123 ) .
123 , so the sample average of
since we can estimate ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) , ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) and ( ( cid : 123 ) 123 + ( n123 ( cid : 123 ) ) 123 ) without bias , we are thus able to estimate unbiasedly any linear combination of ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) , ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) and ( ( cid : 123 ) 123 + ( n123 ( cid : 123 ) ) 123 ) .
this is not su ( cid : 123 ) cient to estimate v ar ( n123 n123 ^ ( cid : 123 ) j ) shown in ( 123 ) unbiasedly .
we now tackle the question of whether or not there exists an unbiased estimator of v ar ( n123 n123 ^ ( cid : 123 ) j ) .
potential estimators may be put in two classes : ( i ) those that are linear and / or quadratic in the l ( j; i ) s , ( ii ) those that are not .
because of the general framework of the paper , it is impossible to say anything about the distribution of the l ( j; i ) s beyond their means and covariances ( to say anything more requires 123 , the learning algorithms and the loss function l ) .
assumptions about the distribution of z n hence we are only able to derive mathematical expectations for estimators within class ( i ) .
we obviously cannot identify an unbiased estimator of v ar ( n123 n123 ^ ( cid : 123 ) j ) in class ( ii ) since we cannot derive expectations in this class .
the following proposition shows that there is no unbiased estimator of v ar ( n123
n123 ^ ( cid : 123 ) j ) in class ( i ) .
proposition 123 there is no general unbiased estimator of v ar ( n123 l ( j; i ) s in a quadratic and / or linear way .
proof let ~ lj be the vector of the l ( j; i ) s involved in ^ ( cid : 123 ) j and ~ l be the vector obtained by concatenating the ~ ljs; ~ l is thus a vector of length n123j .
we know that ~ l has expectation n123 ( cid : 123 ) 123n123j and variance
n123 ^ ( cid : 123 ) j ) that involves the
n123j + ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) ij ( 123n123 n123 ) + ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) in123j ; v ar ( ~ l ) = ( cid : 123 ) 123n123j123
where ik is the identity matrix of order k , 123k is the k ( cid : 123 ) 123 vector ( cid : 123 ) lled with 123s and denotes n123 ^ ( cid : 123 ) j ) of the following form kroneckers product .
we consider estimators of v ar ( n123 a ~ l + b
^v ( n123
using the fact that 123
n123 ^ ( cid : 123 ) j ) = ~ l n123j a123n123j = trace ( a123n123j123
n123j ) , we have
e ( ^v ( n123
n123 ^ ( cid : 123 ) j ) ) = trace ( av ar ( ~ l ) ) + e ( ~ l )
ae ( ~ l ) + b
= ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) trace ( a ( ij ( 123n123 n123 ) ) ) + ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) trace ( a ) n123j a123n123j + n123 ( cid : 123 ) b + ( ( cid : 123 ) 123 + n123 ( cid : 123 ) 123 ) 123
since we wish ^v ( n123 rid of n123 ( cid : 123 ) in the above expectation .
once those restrictions are incorporated into ^v ( n123
n123 ^ ( cid : 123 ) j ) to be unbiased for v ar ( n123
n123 ^ ( cid : 123 ) j ) , we want 123 = b
n123j a123n123j to get
123n123j = 123
n123 ) ) ) + ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) trace ( a ) : n123 ^ ( cid : 123 ) j ) ) = ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) trace ( a ( ij ( 123n123
e ( ^v ( n123 n123 ^ ( cid : 123 ) j ) is not a linear combination of ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) and ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) alone , we conclude
since v ar ( n123 that ^v ( n123
n123 ^ ( cid : 123 ) j ) cannot be unbiased for v ar ( n123
n123 ^ ( cid : 123 ) j ) in general .
123 estimation of v ar ( n123
j = v ar ( n123 n123 ^ ( cid : 123 ) j is as de ( cid : 123 ) ned in ( 123 ) .
we we are interested in estimating n123 provide two new estimators of v ar ( n123 n123 ^ ( cid : 123 ) j ) that shall be compared , in section 123 , to estimators currently in use and presented in section 123
the ( cid : 123 ) rst estimator is simple but may have a positive or negative bias for the actual variance v ar ( n123 n123 ^ ( cid : 123 ) j ) .
the second estimator is meant to lead to conservative inference ( see appendix a . 123 ) , that is , if our conjecture 123 is correct , its expected value exceeds the actual variance v ar ( n123
n123 ^ ( cid : 123 ) j ) where n123
123 first method : approximating ( cid : 123 )
let us recall from ( 123 ) that n123
n123 ^ ( cid : 123 ) j = 123
j=123 ^ ( cid : 123 ) j
be the sample variance of the ^ ( cid : 123 ) js .
according to lemma 123 ,
^ ( cid : 123 ) j ) = ( cid : 123 ) 123 ( 123 ( cid : 123 ) ) =
( cid : 123 ) + 123 ( cid : 123 )
( cid : 123 ) + 123 ( cid : 123 ) j + ( cid : 123 )
v ar ( n123 j + ( cid : 123 )
j + ( cid : 123 )
is an unbiased estimator of v ar ( n123
n123 ^ ( cid : 123 ) j ) .
the only problem is that ( cid : 123 ) = ( cid : 123 ) ( n123; n123 ) = ( cid : 123 ) 123 ( n123;n123 ) ( cid : 123 ) 123 ( n123;n123 ) , the correlation between the ^ ( cid : 123 ) js , is unknown and di ( cid : 123 ) cult to estimate .
indeed , neither ( cid : 123 ) 123 nor ( cid : 123 ) 123 can be written as a linear combination of n123 ( cid : 123 ) , ( ( cid : 123 ) 123 + ( n123 ( cid : 123 ) ) 123 ) , ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) and ( ( cid : 123 ) 123 ( cid : 123 ) 123 ) , the only quantities we know how to estimate unbiasedly ( besides linear combinations of these ) .
we use a very naive surrogate for ( cid : 123 ) as follows .
let us recall l ( zsj ; zi ) .
for the purpose of building our estimator , let us proceed as that ^ ( cid : 123 ) j = 123 if l ( zsj ; zi ) depended only on zi and n123 , i . e .
the loss does not depend on the actual n123 examples ( zsj ) used for training but only on the number of training examples ( n123 ) and on
the testing example ( zi ) .
then it is not hard to show that the correlation between the ^ ( cid : 123 ) js
indeed , when l ( zsj ; zi ) = f ( zi ) , we have
where i123 ( i ) is equal to 123 if zi is a test example for ^ ( cid : 123 ) 123 and is equal to 123 otherwise .
naturally , i123 ( k ) is de ( cid : 123 ) ned similarly .
we obviously have v ar ( ^ ( cid : 123 ) 123 ) = v ar ( ^ ( cid : 123 ) 123 ) with v ar ( ^ ( cid : 123 ) 123 ) = e ( v ar ( ^ ( cid : 123 ) 123ji123 ( : ) ) ) +v ar ( e ( ^ ( cid : 123 ) 123ji123 ( : ) ) ) = e where i123 ( : ) denotes the n ( cid : 123 ) 123 vector made of the i123 ( i ) s .
moreover ,
+v ar ( e ( f ( z123 ) ) ) =
c ov ( ^ ( cid : 123 ) 123; ^ ( cid : 123 ) 123 ) = e ( c ov ( ^ ( cid : 123 ) 123; ^ ( cid : 123 ) 123ji123 ( : ) ; i123 ( : ) ) ) + c ov ( e ( ^ ( cid : 123 ) 123ji123 ( : ) ; i123 ( : ) ) ; e ( ^ ( cid : 123 ) 123ji123 ( : ) ; i123 ( : ) ) )
+ c ov ( e ( f ( z123 ) ) ; e ( f ( z123 ) ) )
n123 + 123 =
therefore our ( cid : 123 ) rst estimator of v ar ( n123
so that the correlation between ^ ( cid : 123 ) 123 and ^ ( cid : 123 ) 123 ( ^ ( cid : 123 ) j and ^ ( cid : 123 ) j123 with j 123= j 123 in general ) is n123
where ( cid : 123 ) o = ( cid : 123 ) o ( n123; n123 ) = , that is by construction , ( cid : 123 ) o will be a good substitute for ( cid : 123 ) when l ( zsj; z ) does not depend much on the training set zsj , that is when the decision function of the underlying algorithm does not change too much when di ( cid : 123 ) erent training sets are chosen .
here are instances where we might suspect this to be true .
this will tend to overestimate or underestimate v ar ( n123
according to whether ( cid : 123 ) o > ( cid : 123 ) or ( cid : 123 ) o < ( cid : 123 ) .
j + ( cid : 123 ) o
n123 ^ ( cid : 123 ) j ) is
j + n123
( cid : 123 ) the capacity ( vc dimension ) of the algorithm is not too large relative to the size of the
training set ( for instance a parametric model that is not too complex ) .
( cid : 123 ) the algorithm is robust relative to perturbations in the training set .
for instance , one could argue that the support vector machine ( burges , 123 ) would tend to fall in this category .
classi ( cid : 123 ) cation and regression trees ( breiman , friedman , olshen , & stone , 123 ) however will typically not have this property as a slight modi ( cid : 123 ) cation in data may lead to substantially di ( cid : 123 ) erent tree growths so that for two di ( cid : 123 ) erent training sets , the corresponding decision functions ( trees ) obtained may di ( cid : 123 ) er substantially on some regions .
k - nearest neighbors techniques will also lead to substantially di ( cid : 123 ) erent decision functions when di ( cid : 123 ) erent training sets are used , especially if k is small .
123 second method : overestimating v ar ( n123
n123 ^ ( cid : 123 ) j )
our second method aims at overestimating v ar ( n123 n123 ^ ( cid : 123 ) j ) .
as explained in appendix a . 123 , this leads to conservative inference , that is tests of hypothesis with actual size less than the nominal size .
this is important because techniques currently in use have the opposite defect , that is they tend to be liberal ( tests with actual size exceeding the nominal size ) , which is normally regarded as less desirable than conservative tests .
123 = b n
j ) = n123
j = v ar ( n123
j = v ar ( n123
we have shown at the end of section 123 that n123
n123 ^ ( cid : 123 ) j ) could not be estimated unbiasedly without some prior knowledge about ( cid : 123 ) 123; ( cid : 123 ) 123; ( cid : 123 ) 123; ( cid : 123 ) 123 ( we showed after ( 123 ) how this is known ) .
however , as we show below , we may estimate unbiasedly can be done when ( cid : 123 ) = ( cid : 123 ) 123 j be the unbiased estimator , developed below , of the above variance .
since n 123 < n123 , we have ( according ^ ( cid : 123 ) j ) ( cid : 123 ) v ar ( n123 to conjecture 123 ) v ar ( n123 is e ( n123
c n123 < n123 ( we assume n123 < b n
j will tend to overestimate n123
n123 ^ ( cid : 123 ) j ) , so that n123
let n123
^ ( cid : 123 ) j ) where n
heres how we may estimate n123
j without bias .
the main idea is that we can get two independent instances of n123 j without bias .
of course variance estimation from only two observations is noisy .
fortunately , the process by which this variance estimate is obtained can be repeated at will , so that we may have many unbiased estimates of n123
averaging these yields a more accurate estimate of n123
^ ( cid : 123 ) j which allows us to estimate n123
123 into two distinct data sets , d123 and dc
obtaining a pair of independent n123
^ ( cid : 123 ) js is simple .
suppose , as before , that our data set z n consists of n = n123 +n123 examples .
for simplicity , assume that n is even123
we have to randomly c each .
let ^ ( cid : 123 ) ( 123 ) be the split our data z n statistic of interest ( n123 ^ ( cid : 123 ) j ) computed on d123
this involves , among other things , drawing j train / test subsets from d123 , respectively of size n ( 123 ) be the statistic computed 123 are independent data sets 123 , so that ( ^ ( cid : 123 ) ( 123 ) ^ ( cid : 123 ) ( 123 ) +^ ( cid : 123 ) c ( 123 ) ) 123 is unbiased for n123 m , with dm ( dc splitting process may be repeated m times .
this yields dm and dc m = z n dm \ dc that is such that
( 123 ) are independent since d123 and dc ) 123 + ( ^ ( cid : 123 ) c m = ; and jdmj = jdc
c for m = 123; : : : ; m .
each split yields a pair ( ^ ( cid : 123 ) ( m ) ; ^ ( cid : 123 ) c
123 and n123
let ^ ( cid : 123 ) c
then ^ ( cid : 123 ) ( 123 ) and ^ ( cid : 123 ) c
123 , of size b n
j = b n
) 123 = 123
v ar ( ^ ( cid : 123 ) ( m ) ^ ( cid : 123 ) c
v ar ( ^ ( cid : 123 ) ( m ) ) + v ar ( ^ ( cid : 123 ) c
123when n is odd , everything is the same except that splitting the data in two will result in a leftover
m are still disjoint subsets of size b n
c from zn
123 , but zn
observation that is ignored .
thus dm and dc is a singleton instead of being the empty set .
123independence holds if the train / test subsets selection process in d123 is independent of the process in dc
otherwise , ^ ( cid : 123 ) 123 and ^ ( cid : 123 ) c
123 may not be independent , but they are uncorrelated , which is all we actually need .
this allows us to use the following unbiased estimator of n123
note that , according to lemma 123 , the variance of the proposed estimator is v ar ( n123 ( m123 ) ) 123 ) for m 123= m 123v ar ( ( ^ ( cid : 123 ) ( m ) ^ ( cid : 123 ) c may deduce from lemma 123 that r > 123 , but simulations yielded r close to 123 , so that v ar ( n123 decreased roughly like 123
we provide some guidance on the choice of m in section 123
with r = corr ( ( ^ ( cid : 123 ) ( m ) ^ ( cid : 123 ) c
( m ) ) 123; ( ^ ( cid : 123 ) ( m123 ) ^ ( cid : 123 ) c
r + 123r
note that the computational e ( cid : 123 ) ort to obtain this variance estimator is proportional to jm .
we could reduce this by a factor j if we use n123 n123 ^ ( cid : 123 ) j ) , but we suspect that overestimation might be too gross as e ( n123 we considered this ultra - conservative estimator of n123 j when we performed the simulations presented in section 123 but , as we suspected , the resulting inference was too conservative .
we do not show the results to avoid overcrowding the paper .
we could also have gone half way by using n123
< j , but we did not pursue this for the same reason as above .
123 to overestimate v ar ( n123
j123 with 123 < j
123 ) = n123
123 inference about n123 ( cid : 123 ) we present seven di ( cid : 123 ) erent techniques to perform inference ( con ( cid : 123 ) dence interval or test ) about n123 ( cid : 123 ) .
the ( cid : 123 ) rst three are methods already in use in the machine - learning community , the others are methods we put forward .
among these new methods , two were shown in the previous section; the other two are the \pseudo - bootstrap " and corrected \pseudo - bootstrap " ( described later ) .
tests 123 of the hypothesis h123 : n123 ( cid : 123 ) = ( cid : 123 ) 123 ( at signi ( cid : 123 ) cance level ( cid : 123 ) ) have the
reject h123 if
j^ ( cid : 123 ) ( cid : 123 ) 123j > c
while con ( cid : 123 ) dence intervals for n123 ( cid : 123 ) ( at con ( cid : 123 ) dence level 123 ( cid : 123 ) ) will look like
n123 ( cid : 123 ) 123 ( ^ ( cid : 123 ) c ^ ( cid : 123 ) 123; ^ ( cid : 123 ) + c
note that in ( 123 ) or ( 123 ) , ^ ( cid : 123 ) will be an average , ^ ( cid : 123 ) 123 is meant to be a variance estimate of ^ ( cid : 123 ) and ( using the central limit theorem to argue that the distribution of ^ ( cid : 123 ) is approximately gaussian ) c will be a percentile from the n ( 123; 123 ) distribution or from students t distribution .
the only di ( cid : 123 ) erence between the seven techniques is in the choice of ^ ( cid : 123 ) , ^ ( cid : 123 ) 123 and c .
in this section we lay out what ^ ( cid : 123 ) , ^ ( cid : 123 ) 123 and c are for the seven techniques considered and comment on whether each technique should be liberal or conservative based on its political ratio .
all this is summarized in table 123
the properties ( size and power of the tests ) of those seven techniques shall be
123at this point , we encourage readers to consult appendix a . 123
investigated in section 123
we are now ready to introduce the statistics we will consider in this paper .
t test statistic
123 be split into a training set zs123 of size n123 and a test set zsc
let the available data z n size n123 = n n123 , with n123 relatively large ( a third or a quarter of n for instance ) .
one may consider ^ ( cid : 123 ) = n123 l is the sample variance l ( 123; i ) 123
inference would be based on the of the l ( 123; i ) s involved in n123 ( incorrect ) belief that
n123 ^ ( cid : 123 ) 123 to estimate n123 ( cid : 123 ) and ^ ( cid : 123 ) 123 = s123
n123 ^ ( cid : 123 ) 123 = n
n123 ^ ( cid : 123 ) 123 n123 ( cid : 123 )
( cid : 123 ) n ( 123; 123 ) :
we use n ( 123; 123 ) here ( instead of tn123 for instance ) as n123 is meant to be fairly large ( greater than 123 , say ) .
lemma 123 tells us that the political ratio here is
( cid : 123 ) = n123 ( cid : 123 ) 123 + ( ( cid : 123 ) 123 ( cid : 123 ) 123 )
v ar ( n123
so this approach leads to liberal inference .
this phenomenon grows worse as n123 increases .
l is a biased estimator of ( cid : 123 ) 123 ( the unconditional variance of l ( 123; i ) = note that s123 l ( zs123; zi ) ; i 123 s123 ) , but is unbiased for the variance of l ( 123; i ) conditional on the train - 123
that is so because , given zs123 , the l ( 123; i ) s are independent variates .
ing set zs123
123we note that this statistic is closely related to the mcnemar statistic ( everitt , 123 ) when the problem at hand is the comparison of two classi ( cid : 123 ) cation algorithms , i . e .
l is of the form ( 123 ) with q of the form ( 123 ) .
indeed , let lab ( 123; i ) = la ( 123; i ) lb ( 123; i ) where la ( 123; i ) indicates whether zi is misclassi ( cid : 123 ) ed ( la ( 123; i ) = 123 ) by algorithm a or not ( la ( 123; i ) = 123 ) ; lb ( 123; i ) is de ( cid : 123 ) ned likewise .
of course , algorithms a and b share the same training set ( s123 ) and testing set ( sc , with njk being the number of times la ( 123; i ) = j and lb ( 123; i ) = k , j = 123; 123 , k = 123; 123
mcnemars statistic is devised for testing h123 : n123 ( cid : 123 ) = 123 ( i . e .
the lab ( 123; i ) s have expectation 123 ) so that one may estimate the variance of the lab ( 123; i ) s with the mean of the ( lab ( 123; i ) 123 ) 123s ( which is n123+n123
n123 ^ ( cid : 123 ) 123 = n123n123
) rather than with s123
we have n123
then ( 123 ) becomes
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n123 n123
n123 + n123
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) > z123 ( cid : 123 ) =123;
reject h123 if
with zp the quantile p of n ( 123; 123 ) , which squared leads to the mcnemars test ( not corrected for continuity ) .
123from this , we can rederive that s123
l ) = e ( e ( s123
l is biased for the unconditional variance as follows : ljzs123 ) ) = e ( v ar ( l ( 123; i ) jzs123 ) )
( cid : 123 ) e ( v ar ( l ( 123; i ) jzs123 ) ) + v ar ( e ( l ( 123; i ) jzs123 ) ) = v ar ( l ( 123; i ) ) :
therefore , although ( 123 ) is wrong , we do have
n123 ^ ( cid : 123 ) 123 e ( n123
( cid : 123 ) n ( 123; 123 )
in so far as n123 is large enough for the central limit theorem to apply .
therefore this n123 ^ ( cid : 123 ) 123jzs123 ) = e ( l ( 123; i ) jzs123 ) = method really allows us to make inference about e ( n123 e ( l ( zs123; zi ) jzs123 ) ; i 123 s123 , that is the generalization error of the speci ( cid : 123 ) c rule obtained by training the algorithm on zs123 , not the generalization error of the algorithm per se .
that is , according to dietterichs taxonomy ( dietterich , 123 ) briey explained in section 123 , it deals with questions of type 123 through 123 rather than questions of type 123 through 123
resampled t test statistic
let us refresh some notation from section 123
particularly , let us recall that n123
n123 ^ ( cid : 123 ) j = j where s123 is the sample variance of the ^ ( cid : 123 ) js ( see ( 123 ) ) .
inference would be based on the ( incorrect )
j=123 ^ ( cid : 123 ) j .
the resampled t test technique123 considers ^ ( cid : 123 ) = n123
n123 ^ ( cid : 123 ) j and ^ ( cid : 123 ) 123 =
n123 ^ ( cid : 123 ) j n123 ( cid : 123 )
combining ( 123 ) and lemma 123 gives us the following political ratio
v ar ( n123
jv ar ( n123
= j ( cid : 123 ) 123 + ( ( cid : 123 ) 123 ( cid : 123 ) 123 )
so this approach leads to liberal inference , a phenomenon that grows worse as j increases .
dietterich ( dietterich , 123 ) observed this empirically through simulations .
as argued in section 123 , s123 conditional on z n
=j actually estimates ( without bias ) the variance of n123
thus while ( 123 ) is wrong , we do have
n123 ^ ( cid : 123 ) j e ( n123
n123 ^ ( cid : 123 ) jjz n
recall from the discussion following ( 123 ) that e ( n123 method really allows us to make inference about n123 we want to make inference about n123 ( cid : 123 ) .
n123 ^ ( cid : 123 ) jjz n n123 ^ ( cid : 123 ) 123
therefore this n123 ^ ( cid : 123 ) 123 , which is not too useful because
123 ) = n123
123when the problem at hand is the comparison of two classi ( cid : 123 ) cation algorithms , i . e .
l is of the form ( 123 ) with q of the form ( 123 ) , this approach is what dietterich ( dietterich , 123 ) calls the \resampled paired t test "
123x123 cv t test
dietterich ( dietterich , 123 ) 123 split z n as in section 123 and let ~ ( cid : 123 ) ( m ) = bn=123c123
123 in half m = 123 times to yield d123; dc
123; : : : ; d123; dc
he then used ^ ( cid : 123 ) = ~ ( cid : 123 ) ( 123 ) , ^ ( cid : 123 ) 123 = ^ ( cid : 123 ) 123 the political ratio is
( m ) ) 123 and c = t123;123 ( cid : 123 ) =123
note that
( m ) = bn=123c123
diet = 123
where ( cid : 123 ) 123 = c ov ( ~ ( cid : 123 ) ( m ) ; ~ ( cid : 123 ) c
distant from n ( cid : 123 ) .
( cid : 123 ) as dietterich noted , this allows inference for ( cid : 123 ) the choice of m = 123 seems arbitrary .
( cid : 123 ) the statistic was developed under the assumption that the ~ ( cid : 123 ) ( m ) s and ~ ( cid : 123 ) c ( m ) s are 123 independent and identically distributed gaussian variates .
even in this ideal case ,
bn=123c ( cid : 123 ) which may be substantially
is not distributed as t123 as assumed in ( dietterich , 123 ) because ~ ( cid : 123 ) ( 123 ) and ( ~ ( cid : 123 ) ( 123 ) ~ ( cid : 123 ) c are not independent .
that is easily ( cid : 123 ) xed in two di ( cid : 123 ) erent ways :
( take the sum from m = 123 to m = 123 and replace 123 by 123 in the denominator of bn=123c ( cid : 123 ) ) which would lead to td ( cid : 123 ) t123
( 123 ) which would result in td ( cid : 123 ) t123 , ( replace the numerator by ( 123 ) and ~ ( cid : 123 ) ( 123 ) ~ ( cid : 123 ) c
( 123 ) are independent .
as ~ ( cid : 123 ) ( 123 ) + ~ ( cid : 123 ) c
in all cases , more degrees of freedom could be exploited; statistics distributed as t123 can be devised by appropriate use of the 123 ( assumed ) independent variates .
conservative z
we estimate n123 ( cid : 123 ) by ^ ( cid : 123 ) = n123 variance estimate .
since n123 expect that its distribution is approximatively normal .
we then proceed as if
j ( equation 123 ) as its conservative n123 ^ ( cid : 123 ) j is the mean of many ( jn123 to be exact ) l ( j; i ) s , we may
n123 ^ ( cid : 123 ) j and use ^ ( cid : 123 ) 123 = n123
n123 ^ ( cid : 123 ) j n123 ( cid : 123 )
123dietterich only considered the comparison of two classi ( cid : 123 ) cation algorithms , that is l of the form ( 123 ) with
q of the form ( 123 ) .
j is designed to overestimate v ar ( n123
was a n ( 123; 123 ) variate ( even though n123 n123 ^ ( cid : 123 ) j ) ) to perform inference , leading us to use c = z123 ( cid : 123 ) =123 in ( 123 ) or ( 123 ) , where z123 ( cid : 123 ) =123 is the percentile 123 ( cid : 123 ) of the n ( 123; 123 ) distribution .
some would perhaps prefer to use percentile from the t distribution , but it is unclear what the degrees of freedom ought to be .
people like to use the t distribution in approximate inference frameworks , such as the one we are dealing with , to yield conservative inference .
this is unnecessary here as inference is already conservative via the variance overestimation .
indeed , the political ratio
v ar ( n123
is smaller than 123 if we believe in conjecture 123
regarding the choice of n123 ( and thus n123 ) , we may take it to be small relatively to n ( the total number of examples available ) .
one may use n123 = n 123 for instance provided j is
n123 ^ ( cid : 123 ) j by a procedure similar to the bootstrap ( efron to estimate the variance of ^ ( cid : 123 ) = n123 & tibshirani , 123 ) , we obtain r other instances of that random variable , by redoing the computation with di ( cid : 123 ) erent splits on the same data z n 123 ; call these ( cid : 123 ) ( cid : 123 ) 123; : : : ; ( cid : 123 ) ( cid : 123 ) r .
thus , in total , ( r + 123 ) j training and testing sets are needed here .
then one could consider ^ ( cid : 123 ) 123 = ( cid : 123 ) ( cid : 123 ) 123 , where ( cid : 123 ) ( cid : 123 ) 123 is the sample variance of ( cid : 123 ) ( cid : 123 ) 123; : : : ; ( cid : 123 ) ( cid : 123 ) r , and take c = tr123;123 ( cid : 123 ) =123 , as ( cid : 123 ) ( cid : 123 ) 123 has r 123 degrees of freedom .
of course n123 n123 ^ ( cid : 123 ) j , ( cid : 123 ) ( cid : 123 ) 123; : : : ; ( cid : 123 ) ( cid : 123 ) r are r + 123 identically distributed random variables .
but they are not independent as we ( cid : 123 ) nd , from ( 123 ) , that the covariance between them is ( cid : 123 ) 123
using lemma 123 , we have = j ( cid : 123 ) 123 + ( ( cid : 123 ) 123 ( cid : 123 ) 123 )
v ar ( n123
note that this political ratio is the same as its counterpart for the resampled t - test because e ( ( cid : 123 ) ( cid : 123 ) 123 ) = e ( j ) .
so the pseudo - bootstrap leads to liberal inference that should worsen with increasing j just like the resampled t test statistic .
in other words , the pseudo - bootstrap only provides a second estimator of ( cid : 123 ) 123 ( cid : 123 ) 123 j which is more complicated and harder to compute than
j which is also unbiased for ( cid : 123 ) 123 ( cid : 123 ) 123
corrected resampled t - test statistic
, where s123
from our discussion in section 123 , we know that an unbiased estimator of j + ( cid : 123 ) is the sample variance of the ^ ( cid : 123 ) js .
unfortunately ( cid : 123 ) , the correlation between the ^ ( cid : 123 ) js , is unknown .
the resampled t - test boldly puts ( cid : 123 ) = 123
we propose here to proceed as if ( cid : 123 ) = ( cid : 123 ) 123 = n123 as our argument in section 123 suggests .
so we use ^ ( cid : 123 ) 123 = .
we must say again that this approximation is gross , but we feel it is better than putting ( cid : 123 ) = 123
furthermore , in the ideal case where the vector
j + n123
of the ^ ( cid : 123 ) js follows the multivariate gaussian distribution and ( cid : 123 ) is actually equal to ( cid : 123 ) 123 , lemma 123 states that finally , let us note that the political ratio
( cid : 123 ) tj123
this is why we use c = tj123;123 ( cid : 123 ) =123
n123 ^ ( cid : 123 ) j n123 ( cid : 123 )
v ar ( n123
j + ( cid : 123 ) j + n123
will be greater than 123 ( liberal inference ) if ( cid : 123 ) > ( cid : 123 ) 123
if ( cid : 123 ) < ( cid : 123 ) 123 , the above ratio is smaller than 123 , so that we must expect the inference to be conservative .
having mentioned earlier that conservative inference is preferable to liberal inference , we therefore hope that the ad hoc ( cid : 123 ) 123 = n123
will tend to be larger than the actual correlation ( cid : 123 ) .
corrected pseudo - bootstrap statistic
naturally , the correction we made in the resampled t test can be applied to the pseudo - ( cid : 123 ) ( cid : 123 ) 123 , where ( cid : 123 ) ( cid : 123 ) 123 is the bootstrap procedure as well .
namely , we note that sample variance of the ( cid : 123 ) ( cid : 123 ) rs , is unbiased for n123 j .
naively replacing ( cid : 123 ) by ( cid : 123 ) 123 leads us to ( cid : 123 ) ( cid : 123 ) 123
furthermore , in the ideal case where ( cid : 123 ) is actually equal to ( cid : 123 ) 123 , use ^ ( cid : 123 ) 123 = and the vector made of n123 n123 ^ ( cid : 123 ) j , ( cid : 123 ) ( cid : 123 ) 123; : : : ( cid : 123 ) ( cid : 123 ) r follows the multivariate gaussian distribution , n123 ^ ( cid : 123 ) j n123 ( cid : 123 ) ( cid : 123 ) tr123
this is why we use c = tr123;123 ( cid : 123 ) =123
finally lemma 123 states that note that , just like in the corrected resampled t - test , the political ratio is
123 + j ( cid : 123 )
123 + jn123
v ar ( n123
j + ( cid : 123 ) j + n123
we conclude this section by providing in table 123 a summary of the seven inference methods
considered in the present section .
123 simulation study
we performed a simulation study to investigate the power and the size of the seven statistics considered in the previous section .
we also want to make recommendations on the value n123 ^ ( cid : 123 ) j .
simulation results will also lead to a of j to use for those methods that involve recommendation on the choice of m when the conservative z is used .
we will soon introduce the three kinds of problems we considered to cover a good range of possible applications .
for a given problem , we shall generate 123 independent sets of data 123 = fz123; : : : zng has been generated , we may of the form fz123; : : : ; zng .
once a data set z n compute con ( cid : 123 ) dence intervals and / or a tests of hypothesis based on the statistics laid out in section 123 and summarized in table 123
a di ( cid : 123 ) culty arises however .
for a given n , those seven methods dont aim at inference for the same generalization error .
for instance , dietterichs method aims at n=123 ( cid : 123 ) ( we take n even for simplicity ) , while the others aim at n123 ( cid : 123 ) where n123 would usually be di ( cid : 123 ) erent for di ( cid : 123 ) erent methods ( e . g .
n123 = 123n
123 for the t - test and n123 = 123n
t - test ( mcnemar ) 123
resampled t 123
dietterichs 123 ( cid : 123 ) 123 cv 123
conservative z
corrected resampled t
corrected pseudo - bootstrap
j + n123 123 + jn123
123 + j ( cid : 123 ) 123 ( cid : 123 ) > 123 123 + j ( cid : 123 ) 123 ( cid : 123 ) > 123
table 123 : summary description of the seven inference methods considered in relation to the rejection criteria shown in ( 123 ) or the con ( cid : 123 ) dence interval shown in ( 123 ) .
zp and tk;p refer to the quantile p of the n ( 123; 123 ) and student tk distributions respectively .
the political ratio , that is v ar ( ^ ( cid : 123 ) ) e ( ^ ( cid : 123 ) 123 ) , indicates if inference according to the corresponding method will tend to be conservative ( ratio less than 123 ) or liberal ( ratio greater than 123 ) .
see section 123 for further
n123 ^ ( cid : 123 ) j ) .
in order to compare the di ( cid : 123 ) erent techniques , for a given n , we shall methods using n123 always aim at n=123 ( cid : 123 ) .
the use of the statistics other than dietterichs 123 ( cid : 123 ) 123 cv shall be modi ( cid : 123 ) ed
( cid : 123 ) t test statistic
we take n123 = n123 = n n123 is one third , say , of n , not one half .
this deviates slightly from the normal usage of the t test where
( cid : 123 ) methods other that the t - test and dietterichs 123 ( cid : 123 ) 123 cv n123 ^ ( cid : 123 ) j where j is a free parameter , that is all methods except for methods involving n123 the t - test and dietterichs 123 ( cid : 123 ) 123 cv , we take n123 = n123 = n 123 .
this deviates substantially from the normal usage where n123 would be 123 to 123 times larger than n123 , say .
for that reason , we also take n123 = n 123 ( assume n is a multiple of 123 for simplicity ) .
this is achieved by throwing away 123% of the data .
note that when we will address the question of the choice of j ( and m for the conservative z ) , we shall use n123 = 123n n123 = n
123 , more in line with the normal usage .
123 and n123 = n
( cid : 123 ) conservative z
for the conservative z , we need to explain how we compute the variance estimate .
indeed , formula ( 123 ) suggests that we have to compute n123 j whenever n123 = n123 = n what we do is that we choose n123 as we would normally do ( 123% of n here ) and do the j = n=123 variance calculation as usual ( n123 j ) .
however , as explained above , we use n=123 ^ ( cid : 123 ) j and n123 n=123 ^ ( cid : 123 ) j instead of n123 nn123 ^ ( cid : 123 ) j as the cross - validation estimators .
recall j was decreasing in n123 and n123
consequently that we have argued in section 123 that n123
n=123 ^ ( cid : 123 ) j = n=123
the variances of n=123 acts as a conservative variance estimate , that is
n=123 ^ ( cid : 123 ) j and n123
n=123^ ( cid : 123 ) j are smaller than n123
j , so that n123
j ) = n123
j = v ar ( n123
^ ( cid : 123 ) j ) ( cid : 123 ) v ar ( n123
n=123 ^ ( cid : 123 ) j ) ( cid : 123 ) v ar ( n=123
thus the variance overestimation will be more severe in the case of n=123
we consider three kinds of problems to cover a good range of possible applications :
prediction in simple normal linear regression
we consider the problem of estimating the generalization error in a simple gaussian x ) and y jx ( cid : 123 ) n ( a + regression problem .
we thus have z = ( x; y ) with x ( cid : 123 ) n ( ( cid : 123 ) x; ( cid : 123 ) 123 y jx is constant ( does not depend on x ) .
the learning algorithms are
y jx ) where ( cid : 123 ) 123 ( a ) sample mean
i123s yi = ( cid : 123 ) ys , that is the mean of the the decision function is fa ( zs ) ( x ) = 123 y s in the training set zs .
note that this decision function does not depend on x .
we use a quadratic loss , so that la ( j; i ) = ( fa ( zsj ) ( xi ) yi ) 123 = ( ( cid : 123 ) ysj
( b ) linear regression
the decision function is fb ( zs ) ( x ) = ^as +^bsx where ^as and ^bs are the intercept and the slope of the ordinary least squares regression of y on x performed on the training set zs .
since we use a quadratic loss , we therefore have lb ( j; i ) = ( fb ( zsj ) ( xi ) yi ) 123 = ( ^asj + ^bsj xi yi ) 123
on top of inference about the generalization errors of algorithm a ( n123 ( cid : 123 ) a ) and algorithm b ( n123 ( cid : 123 ) b ) , we also consider inference about n123 ( cid : 123 ) ab = n123 ( cid : 123 ) a n123 ( cid : 123 ) b , the di ( cid : 123 ) erence of those generalization errors .
this inference is achieved by considering lab ( j; i ) = la ( j; i ) lb ( j; i ) .
table 123 describes the four simulations we performed for the regression problem .
for instance , in simulation 123 , we generated 123 samples of size 123 , with ( cid : 123 ) x = 123 , ( cid : 123 ) 123 x = 123 , it is shown in ( nadeau & bengio , 123 ) that a = 123 , b = 123 and n123 ( cid : 123 ) a = n123+123 y jx .
thus the ( cid : 123 ) rst and third simulation correspond to cases where the two algorithms generalize equally well ( for 123 ) ; in the second and fourth case , the linear regression generalizes better than n123 = n the sample mean 123
the table also provides some summary con ( cid : 123 ) dence quantities of interest , namely n123 ( cid : 123 ) , ( cid : 123 ) ( n123; n123 ) = ( cid : 123 ) 123 ( n123;n123 )
y jx = 123
y jx + b123 ( cid : 123 ) 123 x ) and n123 ( cid : 123 ) b = n123+123
( cid : 123 ) 123 ( n123;n123 ) and r .
classi ( cid : 123 ) cation of two gaussian populations
we consider the problem of estimating the generalization error in a classi ( cid : 123 ) cation problem
123the parameters of the simulations displayed in tables 123 , 123 and 123 were more or less chosen arbitrarily .
however , e ( cid : 123 ) orts were made so that one or two simulations for each problem would correspond to n123 ( cid : 123 ) ab = 123 ( i . e .
n123 ( cid : 123 ) a = n123 ( cid : 123 ) b ) .
123 ; n 123 ; n 123 ; n 123 ; n 123 ; n 123 ; n 123 ; n 123 ; n 123 ; n
y jx as shown in the table .
123% con ( cid : 123 ) dence intervals for n123 ( cid : 123 ) , ( cid : 123 ) ( n123; n123 ) = ( cid : 123 ) 123 ( n123;n123 )
table 123 : description of four simulations for the simple linear regression problem .
in each of the four simulations , 123 independent samples of size n where generated with ( cid : 123 ) x , a , b , ( cid : 123 ) 123 r = corr ( ( ^ ( cid : 123 ) ( m ) ^ ( cid : 123 ) c ( m123 ) ) 123 ) de ( cid : 123 ) ned after ( 123 ) are provided .
the subscripts a , b and ab indicates whether we are working with la , lb or lab .
an asterisk besides ( cid : 123 ) indicates that powers of tests for that ( cid : 123 ) are displayed in a ( cid : 123 ) gure .
( m ) ) 123; ( ^ ( cid : 123 ) ( m123 ) ^ ( cid : 123 ) c
with two classes .
we thus have z = ( x; y ) with p rob ( y = 123 ) = p rob ( y = 123 ) = 123 xjy = 123 ( cid : 123 ) n ( ( cid : 123 ) 123; ( cid : 123 ) 123 ) and xjy = 123 ( cid : 123 ) n ( ( cid : 123 ) 123; ( cid : 123 ) 123 ) .
the learning algorithms are ( a ) regression tree
we train a least square regression tree 123 ( breiman et al . , 123 ) of y against x and the decision function is fa ( zs ) ( x ) = i ( nzs ( x ) > 123 : 123 ) where nzs ( x ) is the leaf value corresponding to x of the tree obtained when training on zs .
thus la ( j; i ) = i ( fa ( zsj ) ( xi ) 123= yi ) is equal to 123 whenever this algorithm misclassi ( cid : 123 ) es example i when the training set is zsj ; otherwise it is 123
( b ) ordinary least squares linear regression
we perform the regression of y against x and the decision function is fb ( zs ) ( x ) = 123 ) where ^ ( cid : 123 ) s is the ordinary least squares regression coe ( cid : 123 ) cient estimates123 obtained by training on the set zs .
thus lb ( j; i ) = i ( fb ( zsj ) ( xi ) 123= yi ) is equal to 123 whenever this algorithm misclassi ( cid : 123 ) es example i when the training set is zsj; otherwise it is 123
on top of inference about the generalization errors n123 ( cid : 123 ) a and n123 ( cid : 123 ) b associated with those two algorithms , we also consider inference about n123 ( cid : 123 ) ab = n123 ( cid : 123 ) a n123 ( cid : 123 ) b = e ( lab ( j; i ) ) where lab ( j; i ) = la ( j; i ) lb ( j; i ) .
table 123 describes the four simulations we performed for the gaussian populations clas - si ( cid : 123 ) cation problem .
again , we considered two simulations with n = 123 and two sim - ulations with n = 123
we also chose the parameters ( cid : 123 ) 123 , ( cid : 123 ) 123 , ( cid : 123 ) 123 and ( cid : 123 ) 123 in such a way that in simulations 123 and 123 , the two algorithms generalize equally well; in simula - tions 123 and 123 , the linear regression generalizes better than the regression tree .
the table also provides some summary con ( cid : 123 ) dence intervals for quantities of interest , namely n123 ( cid : 123 ) , ( cid : 123 ) ( n123; n123 ) = ( cid : 123 ) 123 ( n123;n123 )
( cid : 123 ) 123 ( n123;n123 ) and r .
classi ( cid : 123 ) cation of letters
we consider the problem of estimating generalization errors in the letter recognition classi ( cid : 123 ) cation problem ( blake , keogh , & merz , 123 ) .
the learning algorithms are
( a ) classi ( cid : 123 ) cation tree
we train a classi ( cid : 123 ) cation tree ( breiman et al . , 123 ) 123 to obtain its decision function fa ( zs ) ( x ) .
here the classi ( cid : 123 ) cation loss function la ( j; i ) = i ( fa ( zsj ) ( xi ) 123= yi ) is equal to 123 whenever this algorithm misclassi ( cid : 123 ) es example i when the training set is zsj ; otherwise it is 123
123the function tree in splus 123 for windows with default options and no pruning was used to train the
123 ^ ( cid : 123 ) zs includes an intercept and correspondingly 123 was included in the input vector x .
123we used the function tree in splus version 123 for windows .
the default arguments were used and no pruning was performed .
the function predict with option type=\class " was used to retrieve the decision function of the tree .
123 ; n 123 ; n 123 ; n 123 ; n 123 ; n 123 ; n 123 ; n 123 ; n 123 ; n
( 123 ( cid : 123 ) 123; 123 ( cid : 123 ) 123 )
table 123 : description of four simulations for the classi ( cid : 123 ) cation of two gaussian populations .
in each of the four simulations , 123 independent samples of size n where generated with ( cid : 123 ) 123 , ( cid : 123 ) 123 , ( cid : 123 ) 123 , ( cid : 123 ) 123 as shown in the table .
123% con ( cid : 123 ) dence intervals for n123 ( cid : 123 ) , ( cid : 123 ) ( n123; n123 ) = ( cid : 123 ) 123 ( n123;n123 ) r = corr ( ( ^ ( cid : 123 ) ( m ) ^ ( cid : 123 ) c ( m123 ) ) 123 ) de ( cid : 123 ) ned after ( 123 ) are provided .
the subscripts a , b and ab indicates whether we are working with la , lb or lab .
an asterisk besides ( cid : 123 ) indicates that powers of tests for that ( cid : 123 ) are displayed in a ( cid : 123 ) gure .
( m ) ) 123; ( ^ ( cid : 123 ) ( m123 ) ^ ( cid : 123 ) c
( b ) first nearest neighbor
we apply the ( cid : 123 ) rst nearest neighbor rule with a distorted distance metric to pull down the performance of this algorithm to the level of the classi ( cid : 123 ) cation tree ( as in ( dietterich , 123 ) ) .
speci ( cid : 123 ) cally , the distance between two vectors of inputs x ( 123 ) and x ( 123 ) is
d ( x ( 123 ) ; x ( 123 ) ) =
where c123 = f123; 123; 123; 123g , c123 = f123; 123; 123; 123; 123; 123; 123; 123; 123g and c123 = f123; 123; 123g de - 123 respectively .
table 123 note the sets of components that are weighted by w , 123 and w shows the values of w considered .
we have lb ( j; i ) equal to 123 whenever this algo - rithm misclassi ( cid : 123 ) es example i when the training set is zsj ; otherwise it is 123
in addition to inference about the generalization errors n123 ( cid : 123 ) a and n123 ( cid : 123 ) b associated with those two algorithms , we also consider inference about n123 ( cid : 123 ) ab = n123 ( cid : 123 ) a n123 ( cid : 123 ) b = e ( lab ( j; i ) ) where lab ( j; i ) = la ( j; i ) lb ( j; i ) .
we sample , without replacement , 123 examples from the 123 examples available in the letter recognition data base .
repeating this 123 times , we obtain 123 sets of data of the form fz123; : : : ; z123g .
the table also provides some summary con ( cid : 123 ) dence intervals for quantities of interest , namely n123 ( cid : 123 ) , ( cid : 123 ) ( n123; n123 ) = ( cid : 123 ) 123 ( n123;n123 )
( cid : 123 ) 123 ( n123;n123 ) and r .
n123 ^ ( cid : 123 ) 123 = 123
123 , while the con ( cid : 123 ) dence intervals in the tables use 123 data sets z n
before we comment on tables 123 , 123 and 123 , let us describe how con ( cid : 123 ) dence intervals shown in those tables were obtained .
first , let us point out that con ( cid : 123 ) dence intervals for generalization errors in those tables have nothing to do with the con ( cid : 123 ) dence intervals that we may compute from the statistics shown in section 123
indeed , the latter can be computed on a single data 123 as we now explain .
set z n n123 ^ ( cid : 123 ) 123 , which has expectation n123 ( cid : 123 ) .
recall , from ( 123 ) in for a given data set , we may compute n123 j=123 ^ ( cid : 123 ) j is the average of 123 crude estimates of the generalization section 123 , that n123 error .
also recall from section 123 that those crude estimates have the moment structure ( cid : 123 ) 123 ( n123;n123 ) .
call ~ ( cid : 123 ) = ( ^ ( cid : 123 ) 123; : : : ; ^ ( cid : 123 ) 123 ) 123 the displayed in lemma 123 with ( cid : 123 ) = n123 ( cid : 123 ) and ( cid : 123 ) = ( cid : 123 ) ( n123; n123 ) = ( cid : 123 ) 123 ( n123;n123 ) vector of those crude estimates .
since we generate 123 independent data sets , we have 123 independent instances of such vectors .
as may be seen in the appendix a . 123 , appropriate use of the theory of estimating functions ( white , 123 ) then yields approximate con ( cid : 123 ) dence intervals for n123 ( cid : 123 ) and ( cid : 123 ) ( n123; n123 ) .
con ( cid : 123 ) dence intervals for r = corr ( ( ^ ( cid : 123 ) ( m ) ^ ( cid : 123 ) c de ( cid : 123 ) ned in section 123 , are obtained in the same manner we get con ( cid : 123 ) dence intervals for ( cid : 123 ) ( n123; n123 ) .
namely , we have 123 independent instances of the vector ( ( ^ ( cid : 123 ) ( 123 ) ^ ( cid : 123 ) c 123n=123 ^ ( cid : 123 ) 123s as we advocate the use of j = 123 later in this section .
where the ^ ( cid : 123 ) ( m ) s and ^ ( cid : 123 ) c we see that n123 ( cid : 123 ) may substantially di ( cid : 123 ) er for di ( cid : 123 ) erent n123
this is most evident in table 123 where con ( cid : 123 ) dence intervals for 123 ( cid : 123 ) di ( cid : 123 ) er from con ( cid : 123 ) dence intervals for 123 ( cid : 123 ) in a noticeable manner .
we see that our very naive approximation ( cid : 123 ) 123 ( n123; n123 ) = n123 is not as bad as one could expect .
often the con ( cid : 123 ) dence intervals for the actual ( cid : 123 ) ( n123; n123 ) contains ( cid : 123 ) 123 ( n123; n123 ) 123
( m ) ) 123; ( ^ ( cid : 123 ) ( m123 ) ^ ( cid : 123 ) c ( 123 ) ) 123; : : : ; ( ^ ( cid : 123 ) ( 123 ) ^ ( cid : 123 ) c
( m ) are n=123
123as mentioned before , the corrected pseudo - bootstrap and the corrected resampled t - test are typically used
123 ; n 123 ; n 123 ; n 123 ; n 123 ; n 123 ; n
123 ; n 123 ; n 123 ; n 123 ; n 123 ; n 123 ; n
table 123 : description of six simulations for the letter recognition problem .
in each of the six simulations , 123 independent samples of size n = 123 where generated and algorithms a and b were used with b using the distorted metric factor w shown in the table .
123% con ( cid : 123 ) dence intervals for n123 ( cid : 123 ) , ( cid : 123 ) ( n123; n123 ) = ( cid : 123 ) 123 ( n123;n123 ) after ( 123 ) are provided .
the subscripts a b and ab indicates whether we are working with la , lb or lab .
an asterisk besides ( cid : 123 ) indicates that powers of tests for that ( cid : 123 ) are displayed in a ( cid : 123 ) gure .
see table 123 for the results obtained with algorithm a ( the same for all 123 simulations ) .
( cid : 123 ) 123 ( n123 ) and r = corr ( ( ^ ( cid : 123 ) ( m ) ^ ( cid : 123 ) c
( m ) ) 123; ( ^ ( cid : 123 ) ( m123 ) ^ ( cid : 123 ) c
123 ; n
table 123 : con ( cid : 123 ) dence intervals for the statistics measured with algorithm a for all 123 simulations with the letter recognition problem ( see table 123 ) .
when this is not the case , the approximation ( cid : 123 ) 123 ( n123; n123 ) usually appears to be reasonably close to the actual value of the correlation ( cid : 123 ) ( n123; n123 ) .
furthermore , when we compare two al - gorithms , the approximation ( cid : 123 ) 123 ( n123; n123 ) is not smaller than the actual value of the correlation ( cid : 123 ) ab ( n123; n123 ) , which is good since that indicates that the inference based on the corrected pseudo - bootstrap and on the corrected resampled t - test will not be liberal , as argued in sec - tion 123
we ( cid : 123 ) nally note that the correlation r appears to be fairly small , except when we compare algorithms a and b in the simple linear regression problem .
thus , as we stated at the end of section 123 , we should expect v ar ( n123
j ) to decrease like 123
123 sizes and powers of tests
one of the most important thing to investigate is the size ( probability of rejecting the null hypothesis when it is true ) of the tests based on the statistics shown in section 123 and compare their powers ( probability of rejecting the null hypothesis when it is false ) .
the four panels of figure 123 show the estimated powers of the statistics for the hypothesis h123 : n=123 ( cid : 123 ) a = ( cid : 123 ) 123 for various values of ( cid : 123 ) 123 in the regression problem .
we estimate powers ( probabilities of rejection ) by proportions of rejection observed in the simulation .
we must underline that , despite appearances , these are not \power curves " in the usual sense of the term ( see appendix a . 123 ) .
in a \power curve " , the hypothesized value of n=123 ( cid : 123 ) a is ( cid : 123 ) xed and the actual value of n=123 ( cid : 123 ) a varies .
here , it is the reverse that we see in a given panel : the actual value of n=123 ( cid : 123 ) a is ( cid : 123 ) xed while the hypothesized value of n=123 ( cid : 123 ) a ( i . e .
( cid : 123 ) 123 ) is varied .
we may call this a pseudo - power curve .
we do this because constructing \power curves " would be too computationally expensive .
nevertheless , pseudo - power curves shown in figure 123 convey information similar to conventional \power curves " .
indeed , we can ( cid : 123 ) nd the size of a test by reading its pseudo - power curve at the actual value of n=123 ( cid : 123 ) a ( laying between the two vertical dotted lines ) .
we can also appreciate the progression of the power as the hypothesized value of n=123 ( cid : 123 ) a and the actual value of n=123 ( cid : 123 ) a grow apart .
we shall see in figure 123 that those pseudo - power curves are good surrogate to \power curves " .
through 123 are counterparts of figure 123 for other problems and / or algorithms .
power plots corresponding to tests about n=123 ( cid : 123 ) b in the regression problem and about n=123 ( cid : 123 ) b in the classi ( cid : 123 ) cation of gaussian populations problem are not shown since they convey the same information as figure 123
however , missing ( cid : 123 ) gures are available in ( nadeau & bengio , 123 ) .
note that in order to reduce the number of displayed line types in figure 123 and its coun - terparts appearing later , some curves share the same line type .
so one must take note of the
( cid : 123 ) in a given panel , you will see four solid curves .
they correspond to either the resampled t - 123 .
curves with circled points 123 ( 123% thrown away ) ; curves without circled points correspond to 123 .
telling apart the resampled t - test and the corrected resampled t - test is easy;
test or the corrected resampled t - test with n123 = n correspond to n123 = n n123 = n the two curves that are well above all others correspond to the resampled t - test .
123 or n123 = n
in cases where training sets are 123 or 123 times larger than test sets .
so we must only be concerned with ( cid : 123 ) ( n and ( cid : 123 ) ( 123n
123 ; n
123% thrown away
123% thrown away
123% thrown away
123% thrown away
figure 123 : powers of the tests about h123 : n ( cid : 123 ) a = ( cid : 123 ) 123 at level ( cid : 123 ) = 123 : 123 for varying ( cid : 123 ) 123 for the regression problem .
each panel corresponds to one of the simulations design described in table 123
the dotted vertical lines correspond to the 123% con ( cid : 123 ) dence interval for the actual ( cid : 123 ) a shown in table 123 , therefore that is where the actual size of the tests may be read .
the solid horizontal line displays the nominal size of the tests , i . e .
estimated probabilities of rejection laying above the dotted horizontal line are signi ( cid : 123 ) cantly greater than 123% ( at signi ( cid : 123 ) cance level 123% ) .
where it matters j = 123 , m = 123 and r = 123 were used .
123% thrown away
123% thrown away
123% thrown away
123% thrown away
( cid : 123 ) ab = ( cid : 123 ) 123 at level ( cid : 123 ) = 123 : 123 for varying ( cid : 123 ) 123 for figure 123 : powers of the tests about h123 : the regression problem .
each panel corresponds to one of the simulations design described in table 123
the dotted vertical lines correspond to the 123% con ( cid : 123 ) dence interval for the actual ( cid : 123 ) ab shown in table 123 , therefore that is where the actual size of the tests may be read .
the solid horizontal line displays the nominal size of the tests , i . e .
estimated probabilities of rejection laying above the dotted horizontal line are signi ( cid : 123 ) cantly greater than 123% ( at signi ( cid : 123 ) cance level 123% ) .
where it matters j = 123 , m = 123 and r = 123 were used .
123% thrown away
123% thrown away
123% thrown away
123% thrown away
figure 123 : powers of the tests about h123 : ( cid : 123 ) a = ( cid : 123 ) 123 at level ( cid : 123 ) = 123 : 123 for varying ( cid : 123 ) 123 for the classi ( cid : 123 ) cation of gaussian populations problem .
each panel corresponds to one of the simulations design described in table 123
the dotted vertical lines correspond to the 123% ( cid : 123 ) a shown in table 123 , therefore that is where the actual con ( cid : 123 ) dence interval for the actual size of the tests may be read .
the solid horizontal line displays the nominal size of the tests , i . e .
estimated probabilities of rejection laying above the dotted horizontal line are signi ( cid : 123 ) cantly greater than 123% ( at signi ( cid : 123 ) cance level 123% ) .
where it matters j = 123 , m = 123 and r = 123 were used .
123% thrown away
123% thrown away
123% thrown away
123% thrown away
( cid : 123 ) ab = ( cid : 123 ) 123 at level ( cid : 123 ) = 123 : 123 for varying ( cid : 123 ) 123 for figure 123 : powers of the tests about h123 : the classi ( cid : 123 ) cation of gaussian populations problem .
each panel corresponds to one of the simulations design described in table 123
the dotted vertical lines correspond to the 123% ( cid : 123 ) ab shown in table 123 , therefore that is where the actual con ( cid : 123 ) dence interval for the actual n size of the tests may be read .
the solid horizontal line displays the nominal size of the tests , i . e .
estimated probabilities of rejection laying above the dotted horizontal line are signi ( cid : 123 ) cantly greater than 123% ( at signi ( cid : 123 ) cance level 123% ) .
where it matters j = 123 , m = 123 and r = 123 were used .
123% thrown away
123% thrown away
123% thrown away
123% thrown away
( cid : 123 ) b = ( cid : 123 ) 123 at level ( cid : 123 ) = 123 : 123 for varying ( cid : 123 ) 123 for figure 123 : powers of the tests about h123 : the letter recognition problem .
each panel corresponds to one of the simulations design described in table 123
the dotted vertical lines correspond to the 123% con ( cid : 123 ) dence interval for ( cid : 123 ) b shown in table 123 , therefore that is where the actual size of the tests may be read .
the solid horizontal line displays the nominal size of the tests , i . e .
estimated probabilities of rejection laying above the dotted horizontal line are signi ( cid : 123 ) cantly greater than 123% ( at signi ( cid : 123 ) cance level 123% ) .
where it matters j = 123 , m = 123 and r = 123 were used .
123% thrown away
123% thrown away
123% thrown away
123% thrown away
( cid : 123 ) ab = ( cid : 123 ) 123 at level ( cid : 123 ) = 123 : 123 for varying ( cid : 123 ) 123 for figure 123 : powers of the tests about h123 : the letter recognition problem .
each panel corresponds to one of the simulations design described in table 123
the dotted vertical lines correspond to the 123% con ( cid : 123 ) dence interval for ( cid : 123 ) ab shown in table 123 , therefore that is where the actual size of the tests may the actual n be read .
the solid horizontal line displays the nominal size of the tests , i . e .
estimated probabilities of rejection laying above the dotted horizontal line are signi ( cid : 123 ) cantly greater than 123% ( at signi ( cid : 123 ) cance level 123% ) .
where it matters j = 123 , m = 123 and r = 123 were used .
( cid : 123 ) the dotted curves depict the conservative z test with either n123 = n or n123 = n
123 ( when it is not circled ) .
123 ( when it is circled )
( cid : 123 ) you might have noticed that the pseudo - bootstrap and the corrected pseudo - bootstrap do not appear in figure 123 and all its counterparts ( except figure 123 and figure 123 ) .
we ignored them because , as we anticipated from political ratios shown in table 123 , the pseudo - bootstrap test behaves like the resampled t - test and the corrected pseudo - bootstrap test behaves like the corrected resampled t - test .
if we dont ignore the pseudo - bootstrap , some ( cid : 123 ) gures become too crowded .
we made an exception and plotted curves corresponding to the pseudo - bootstrap in figures 123 and 123
in those two ( cid : 123 ) gures , the pseudo - bootstrap and corrected pseudo - bootstrap curves are depicted with solid curves ( just like the resampled t - test and corrected resampled t - test ) and obey the same logic that applies to resampled t - test and corrected resampled t - test curves .
what you must notice is that these ( cid : 123 ) gures look like the others except that where you would have seen a single solid curve , you now see two solid curves that nearly overlap .
that shows how similar the resampled t - test and the pseudo - bootstrap are .
this similitude is present ( cid : 123 ) ab in the classi ( cid : 123 ) cation for all problems , no just for the inference about of gaussian populations ( figures 123 and 123 ) .
we chose to show the pseudo - bootstrap curves in figures 123 and 123 because this is where the plots looked the least messy when the pseudo - bootstrap curves were added .
heres what we can draw from those ( cid : 123 ) gures .
( cid : 123 ) the most striking feature of those ( cid : 123 ) gures is that the actual size of the resampled t - test and the pseudo - bootstrap procedure are far away from the nominal size 123% .
this is what we expected in section 123
the fact that those two statistics are more liberal when n123 = n 123 ( 123% of the data thrown away ) suggests that ( cid : 123 ) ( n123; n123 ) is increasing in n123
this is in line with what one can see in tables 123 , 123 and 123 , and the simple approximation ( cid : 123 ) 123 ( n123; n123 ) = n123
123 than they are when n123 = n
( cid : 123 ) we see that the sizes of the corrected resampled t - test ( and corrected pseudo - bootstrap ) are in line with what we could have forecasted from tables 123 , 123 and 123
namely the test is liberal when ( cid : 123 ) ( n123; n123 ) > ( cid : 123 ) 123 ( n123; n123 ) , conservative when ( cid : 123 ) ( n123; n123 ) < ( cid : 123 ) 123 ( n123; n123 ) , and pretty much on target when ( cid : 123 ) ( n123; n123 ) does not di ( cid : 123 ) er signi ( cid : 123 ) cantly from ( cid : 123 ) 123 ( n123; n123 ) .
for instance , on figure 123 the sizes of the corrected resampled t - test are close to the nominal 123% .
we see in table 123 that ( cid : 123 ) a ( n123; n123 ) does not di ( cid : 123 ) er signi ( cid : 123 ) cantly from ( cid : 123 ) 123 ( n123; n123 ) .
similarly , in figures 123 and 123 , the corrected resampled t - test appears to be signi ( cid : 123 ) cantly liberal when n123 = n 123 ) is signi ( cid : 123 ) cantly greater 123 ) = 123 than ( cid : 123 ) 123 ( n
123 ( 123% of the data thrown away ) 123
we see that ( cid : 123 ) a ( n
123 ) is signi ( cid : 123 ) cantly greater than ( cid : 123 ) 123 ( n
123 in table 123 , and ( cid : 123 ) b ( n
123 ) = 123
123actually in figure 123 we do see that the corrected resampled t - test with n123 = n
and 123 despite the fact that ( cid : 123 ) ab ( n is barely signi ( cid : 123 ) cantly smaller than 123 v ar ( ^ ( cid : 123 ) ) particular case is that the distribution of n123
123 is liberal in simulations 123 123 ; n 123 ; n 123 in simulation 123
but , as mentioned in appendix a . 123 , the political ratio is not the only thing determining whether inference is liberal or conservative .
what happens in this n123 ^ ( cid : 123 ) 123 did not appear to su ( cid : 123 ) er from this problem .
123 ) do not di ( cid : 123 ) er signi ( cid : 123 ) cantly from 123
123 in simulation 123 and ( cid : 123 ) ab ( n
n123 ^ ( cid : 123 ) 123 is asymmetric; n123
123 ) = 123
123 ) = 123
in table 123
however , in those same ( cid : 123 ) gures , we see that the corrected resampled t - test that do not throw data away is conservative and , indeed , we can see that ( cid : 123 ) a ( n 123 ) is 123 ) is signi ( cid : 123 ) cantly smaller signi ( cid : 123 ) cantly smaller than ( cid : 123 ) 123 ( n than ( cid : 123 ) 123 ( n 123 in table 123
( cid : 123 ) the conservative z with n123 = n = 123 , more in line with normal usage ) , the conservative z has more interesting properties .
it does not quite live up to its name since it is at times liberal , but barely so .
its size is never very far from 123% ( like 123% for instance ) , making it the best inference procedure among those considered in terms of size .
123 is too conservative .
however , when n123 = n
123 in table 123 , and ( cid : 123 ) b ( n
( cid : 123 ) the t - test and dietterichs 123 ( cid : 123 ) 123 cv are usually well behaved in term of size , but they
are sometimes fairly liberal as can be seen in some panels of figures 123 , 123 , 123 and 123
( cid : 123 ) when their sizes are comparable , the powers of the t - test , dietterichs 123 ( cid : 123 ) 123 cv , conser - vative z throwing out 123% of the data and corrected resampled t - test throwing out 123% of the data are fairly similar .
if we have to break the tie , it appears that the t - test is the most powerful , dietterichs 123 ( cid : 123 ) 123 cv is the least powerful procedure and the corrected resampled t - test and the corrected conservative z lay in between .
the fact that the conservative z and the corrected resampled t - test perform well despite throwing 123% of the data indicates that these methods are very powerful compared to dietterichs 123 ( cid : 123 ) 123 cv and the t - test .
this may be seen in figure 123 where the size of the corrected resam - pled t - test with the full data is comparable to the size of other tests .
the power of the corrected resampled t - test is then markedly superior to the powers of other tests with comparable size .
in other ( cid : 123 ) gures , we see the power of the corrected resampled t - test with full data and / or conservative z with full data catch on ( as we move away from the null hypothesis ) the powers of other methods that have larger size .
from table 123 that we have simulated data with
as promised earlier , we now illustrate that pseudo - power curves are good surro - gates to actual real power curves .
for the letter recognition problem , we have the op - portunity to draw real power curves since we have simulated data under six di ( cid : 123 ) erent tively equal to 123 : 123; 123 : 123; 123 : 123; 123 : 123; 123 : 123; 123 : 123 and 123 ( cid : 123 ) ab approximatively equal to 123 : 123; 123 : 123; 123 : 123; 123 : 123; 123 : 123;123 : 123 in simulations 123 through 123 respectively .
the circled lines in figure 123 depict real power curves .
for instance , in the left panel , the power of tests for h123 : 123 ( cid : 123 ) b = 123 : 123 has been obtained in all six simulations , enabling us to draw the circled curves .
the non - circled curves correspond to what we have been plotting so far .
namely , in simulation 123 , we computed the powers of tests for h123 : 123 ( cid : 123 ) b = ( cid : 123 ) 123 with ( cid : 123 ) 123 = 123 : 123; 123 : 123; 123 : 123; 123 : 123; 123 : 123; 123 : 123 , enabling us to draw the non - circled curves
the comparison of algorithm a and b for the regression problem is the only place where this phenomenon was substantial in our simulation .
that is why curves ( other than t - test and dietterichs 123 ( cid : 123 ) 123 cv that are based on n123 ^ ( cid : 123 ) 123 ) are asymmetric and bottom out before the actual value of n=123 ( cid : 123 ) ab ( laying between the vertical dotted lines ) .
we dont observe this in other ( cid : 123 ) gures .
corrected resampled t
corrected resampled t
figure 123 : real power curves ( circle lines ) and pseudo - power curves ( not circled ) in the letter recognition problem .
in the left panel , we see \real " and \pseudo " power curves for the the null hypothesis h123 : 123 ( cid : 123 ) b = 123 : 123
in the right panel , we see \real " and \pseudo " power curves for the the null hypothesis h123 : 123 ( cid : 123 ) ab = 123 : 123
see the end of section 123 for more details on their constructions .
here , the \corrected resampled t " and the \conservative z " statistics are those which do not throw away data .
see that circled and non - circled curves agree relatively well , leading us to believe that our previous plots are good surrogates to real power curves .
123 the choice of j
123 and n123 = n
in section 123 , the statistics involving n123 n123 ^ ( cid : 123 ) j used j = 123
we look at how those statistics behave with varying js , in order to formulate a recommendation on the choice of j .
we 123 , which correspond to a more natural usage for are going to do so with n123 = 123n these statistics .
of the seven statistics displayed in section 123 ( see also table 123 ) , ( cid : 123 ) ve involved n123 ^ ( cid : 123 ) j .
we ignore the pseudo - bootstrap and the corrected pseudo - bootstrap as political ratios provided in section 123 and empirical evidence in section 123 suggest that these statistics are virtually identical to the resampled t - test and the corrected resampled t - test ( but require a lot more computation ) .
we therefore only consider the resampled t - test , the corrected resampled t - test and the conservative z here .
the investigation of the properties of those statistics will again revolve around their sizes and powers .
you will therefore see that ( cid : 123 ) gures in this section ( figures 123 to 123 ) are similar to those of the section 123 .
note that ( cid : 123 ) gures corresponding to 123n=123 ( cid : 123 ) b are not shown as they convey no additional information .
however , missing ( cid : 123 ) gures are available in ( nadeau & in a given plot , we see the powers of the three statistics when j = 123 , j = 123 , j = 123 and j = 123
therefore a total of twelve curves are present in each plot .
heres what we can draw from those ( cid : 123 ) gures .
corrected resampled t
corrected resampled t
corrected resampled t
corrected resampled t
figure 123 : powers of the tests about h123 : 123n=123 ( cid : 123 ) a = ( cid : 123 ) 123 at level ( cid : 123 ) = 123 : 123 for varying ( cid : 123 ) 123 and j for the regression problem .
each panel corresponds to one of the simulations design described in table 123
the dotted vertical lines correspond to the 123% con ( cid : 123 ) dence interval for the actual 123n=123 ( cid : 123 ) a shown in table 123 , therefore that is where the actual size of the tests may be read .
the solid horizontal line displays the nominal size of the tests , i . e .
estimated probabilities of rejection laying above the dotted horizontal line are signi ( cid : 123 ) cantly greater than 123% ( at signi ( cid : 123 ) cance level 123% ) .
for the conservative z , m = 123 was used .
corrected resampled t
corrected resampled t
corrected resampled t
corrected resampled t
123n=123 ( cid : 123 ) ab = ( cid : 123 ) 123 at level ( cid : 123 ) = 123 : 123 for varying ( cid : 123 ) 123 figure 123 : powers of the tests about h123 : and j for the regression problem .
each panel corresponds to one of the simulations design described in table 123
the dotted vertical lines correspond to the 123% con ( cid : 123 ) dence interval for the actual 123n=123 ( cid : 123 ) ab shown in table 123 , therefore that is where the actual size of the tests may be read .
the solid horizontal line displays the nominal size of the tests , i . e .
estimated probabilities of rejection laying above the dotted horizontal line are signi ( cid : 123 ) cantly greater than 123% ( at signi ( cid : 123 ) cance level 123% ) .
for the conservative z , m = 123 was used .
corrected resampled t
corrected resampled t
corrected resampled t
corrected resampled t
( cid : 123 ) a = ( cid : 123 ) 123 at level ( cid : 123 ) = 123 : 123 for varying ( cid : 123 ) 123 and figure 123 : powers of the tests about h123 : j for the classi ( cid : 123 ) cation of gaussian populations problem .
each panel corresponds to one of the simulations design described in table 123
the dotted vertical lines correspond to the ( cid : 123 ) a shown in table 123 , therefore that is where the 123% con ( cid : 123 ) dence interval for the actual actual size of the tests may be read .
the solid horizontal line displays the nominal size of the tests , i . e .
estimated probabilities of rejection laying above the dotted horizontal line are signi ( cid : 123 ) cantly greater than 123% ( at signi ( cid : 123 ) cance level 123% ) .
for the conservative z , m = 123
corrected resampled t
corrected resampled t
corrected resampled t
corrected resampled t
( cid : 123 ) ab = ( cid : 123 ) 123 at level ( cid : 123 ) = 123 : 123 for varying ( cid : 123 ) 123 and figure 123 : powers of the tests about h123 : j for the classi ( cid : 123 ) cation of gaussian populations problem .
each panel corresponds to one of the simulations design described in table 123
the dotted vertical lines correspond to the ( cid : 123 ) ab shown in table 123 , therefore that is where the 123% con ( cid : 123 ) dence interval for the actual actual size of the tests may be read .
the solid horizontal line displays the nominal size of the tests , i . e .
estimated probabilities of rejection laying above the dotted horizontal line are signi ( cid : 123 ) cantly greater than 123% ( at signi ( cid : 123 ) cance level 123% ) .
for the conservative z , m = 123
corrected resampled t
corrected resampled t
corrected resampled t
corrected resampled t
( cid : 123 ) ab = ( cid : 123 ) 123 at level ( cid : 123 ) = 123 : 123 for varying ( cid : 123 ) 123 and j figure 123 : powers of the tests about h123 : for the letter recognition problem .
each panel corresponds to one of the simulations design described in table 123
the dotted vertical lines correspond to the 123% con ( cid : 123 ) dence interval for ( cid : 123 ) ab shown in table 123 , therefore that is where the actual size of the tests may be read .
the solid horizontal line displays the nominal size of the tests , i . e .
estimated probabilities of rejection laying above the dotted horizontal line are signi ( cid : 123 ) cantly greater than 123% ( at signi ( cid : 123 ) cance level 123% ) .
for the conservative z , m = 123 was used .
( cid : 123 ) again , the ( cid : 123 ) rst thing that we see is that the resampled t - test is very liberal .
however , 123 ) is smaller 123 ) .
we also see that the statistic is more liberal when j is large ,
things were even worse in section 123 .
that is due to the fact that ( cid : 123 ) ( 123n than ( cid : 123 ) ( n as it should be according to the theoretical discussion of that statistic in section 123
123 ) and ( cid : 123 ) ( n
123 ; n
( cid : 123 ) the conservative z lives up to its name .
( cid : 123 ) regarding the corrected resampled t - test , the plots again only con ( cid : 123 ) rm what we might have guessed from tables 123 , 123 and 123
namely the resampled t - test is conservative when ( cid : 123 ) ( 123n signi ( cid : 123 ) cantly smaller then 123 , and has size very close to 123 otherwise .
when it is liberal or conservative , things tend to grow worse when j increases; see figure 123 for the liberal case .
that makes sense since the political ratio v ar ( ^ ( cid : 123 ) ) ( see table 123 ) is monotonic in j ( increasing when ( cid : 123 ) > n123
e ( ^ ( cid : 123 ) 123 ) = 123+j ( cid : 123 ) ; decreasing when ( cid : 123 ) < n123
123 ) is signi ( cid : 123 ) cantly greater than ( cid : 123 ) 123 ( 123n
123 ) = 123 : 123 , liberal when ( cid : 123 ) ( 123n
123 ; n
123 ; n
123 ; n
( cid : 123 ) obviously ( from equation ( 123 ) or proposition 123 ) , the greater j is , the greater the power will be .
note that increasing j from 123 to 123 brings about half the improvement in the power obtained by increasing j from 123 to 123
similarly , increasing j from 123 to 123 brings about half the improvement in the power obtained by increasing j from 123 to 123
with that in mind , we feel that one must take j to be at least equal to 123 as j = 123 leads to unsatisfactory power .
going beyond j = 123 gives little additional power and is probably not worth the computational e ( cid : 123 ) ort .
we could tackle this question from a theoretical point of view .
we know from ( 123 ) that v ar ( n123 123 ) ) .
increasing j from 123 to 123 reduces the variance ( cid : 123 ) = 123 : 123 for instance ( that is ( cid : 123 ) 123 ( 123n by 123% .
increasing j from 123 to 123 further halves the variance .
increasing j from 123 to 123 only halves the variance .
we thus see that the bene ( cid : 123 ) t of increasing j quickly becomes
( cid : 123 ) + 123 ( cid : 123 )
n123 ^ ( cid : 123 ) j ) = ( cid : 123 ) 123
123 ; n
( cid : 123 ) since the conservative z is fairly conservative , it rarely has the same size as the corrected resampled t - test , making power comparison somewhat di ( cid : 123 ) cult .
but it appears that the two methods have equivalent powers which makes sense since they are both based on n123 ^ ( cid : 123 ) j .
we can see this in figures 123 and 123 where the two tests have about the same size and similar power .
based on the above observations , we believe that j = 123 is a good choice :
good power with reasonable computational e ( cid : 123 ) ort .
if computational e ( cid : 123 ) ort is not an issue , one may take j > 123 , but must not expect a great gain in power .
another reason in favor of not taking j too large is that the size of the resampled t - test gets worse with increasing j when that method is liberal or conservative .
of course the choice of j is not totally independent of n123 and n123
indeed , if one uses a larger test set ( and thus a smaller train set ) , then we might expect ( cid : 123 ) to be larger and therefore j = 123 might then be su ( cid : 123 ) ciently large .
although it is not related to the choice of j , we may comment on the choice of the inference procedure as ( cid : 123 ) gures in this section are the most informative in that regard .
if one wants an
inference procedure that is not liberal , the obvious choice is the conservative z .
however , if one prefers an inference procedure with size close to the nominal size ( cid : 123 ) and is ready to accept departures in the liberal side as well as in the conservative side , then the corrected resampled t appears to be a good choice .
however , as we shall see shortly , we can make the conservative z more or less conservative by playing with m .
the advantage of the corrected resampled t is that it requires little computing in comparison to the conservative z .
finally , we assessed to what extent the pseudo - power curves shown in figures 123 through 123 are good surrogates to actual real power curves .
the counterpart of figure 123 , not shown here but available in ( nadeau & bengio , 123 ) , shows again that the two types of curves agree
123 the choice of m
123 and n123 = n
when using the conservative z , we have so far always used m = 123
we study the behavior of this statistic for various values of m in order to formulate a recommendation on the choice of m .
again we consider the case where n123 = 123n 123
the investigation will again revolve around the size and power of the statistic .
we see in figure 123 ( ( cid : 123 ) gures for other problems and / or algorithms convey the same information and are therefore not shown but are available in ( nadeau & bengio , 123 ) ) that the conservative z is more conservative when m is large .
we see that there is not a great di ( cid : 123 ) erence in the behavior of the conservative z when m = 123 and when m = 123
for that reason , we recommend using m ( cid : 123 ) 123
the di ( cid : 123 ) erence between m = 123 and m = 123 is more noticeable , m = 123 leads to inference that is less conservative , which is not a bad thing considering that with m = 123 it tends to be a little bit too conservative .
with m = 123 , the conservative z is sometimes liberal , but barely so .
using m < 123 would probably go against the primary goal of the statistic , that is provide inference that is not liberal .
thus 123 ( cid : 123 ) m ( cid : 123 ) 123 appears to be a reasonable choice .
within this range , pick m large if non - liberal inference is important; otherwise take m small if you want the size of the test to be closer to the nominal size ( cid : 123 ) ( you then accept the risk of performing inference that could be slightly liberal ) .
of course , computational e ( cid : 123 ) ort is linear in m so that taking m small has an additional appeal .
we have tackled the problem of estimating the variance of the cross - validation estimator of the generalization error .
in this paper , we paid special attention to the variability introduced by the selection of a particular training set , whereas most empirical applications of machine learning methods concentrate on estimating the variability of the estimate of generalization error due to the ( cid : 123 ) nite test set .
a theoretical investigation of the variance to be estimated shed some valuable insight on reasons why some estimators currently in use underestimate the variance .
we showed that no general unbiased estimator of the variance of the cross - validation estimator could be found .
this analysis allowed us to construct two variance estimates that take into account both the variability due to the choice of the training sets and the choice of the test examples .
one of the
figure 123 : powers of the conservative z ( with j = 123 ) about h123 : 123n=123 ( cid : 123 ) ab = ( cid : 123 ) 123 at level ( cid : 123 ) = 123 : 123 for varying ( cid : 123 ) 123 and m for the regression problem .
each panel corresponds to one of the simulations design described in table 123
the dotted vertical lines correspond to the 123n=123 ( cid : 123 ) ab shown in table 123 , therefore that is where 123% con ( cid : 123 ) dence interval for the actual the actual size of the tests may be read .
the solid horizontal line displays the nominal size of the tests , i . e .
estimated probabilities of rejection laying above the dotted horizontal line are signi ( cid : 123 ) cantly greater than 123% ( at signi ( cid : 123 ) cance level 123% ) .
proposed estimators looks like the 123 ( cid : 123 ) 123 cv method ( dietterich , 123 ) and is speci ( cid : 123 ) cally designed to overestimate the variance to yield conservative inference .
the other may overestimate or underestimate the real variance , but is typically not too far o ( cid : 123 ) the target .
we performed a simulation where the new techniques put forward were compared to test statistics currently used in the machine learning community .
we tackle both the inference for a generalization error of an algorithm and the comparison of the generalization errors of two algorithms .
we considered two kinds of problems : classi ( cid : 123 ) cation and regression .
various algorithms were considered : linear regression , regression trees , classi ( cid : 123 ) cation trees and the nearest neighbor algorithm .
over this wide range of problems and algorithms , we found that the new tests behave better in terms of size and have powers that are unmatched by any known techniques ( with comparable size ) .
the simulation also allowed us to recommend values for the parameters involved in the proposed techniques , namely j around 123 and ( for the conservative z ) m between 123 and 123
if one wants an inference procedure that is not liberal , the natural choice is the conservative z .
however , if one prefers an inference procedure with size close to the nominal size ( cid : 123 ) and is ready to accept small departures in the liberal side as well as in the conservative side , then the corrected resampled t test appears to be a good choice .
the advantage of the latter is that it requires little computing in comparison to the conservative z .
the paper revolved around a speci ( cid : 123 ) c cross - validation estimator; one in which we split the data sets of n examples into a training set ( of size n123 ) and a testing set ( of size n123 = n n123 ) , and repeat this process j times in an independent manner .
so , for instance , the testing sets of two di ( cid : 123 ) erent splits may partially overlap .
this contrasts with the most standard cross - validation estimator for which the testing sets are mutually exclusive .
analyzing the variance of this standard estimator and providing valid estimates of that variance would be valuable
