the exponential increase in the availability of online reviews and recommendations makes sentiment classication an interesting topic in academic and industrial research .
re - views can span so many dierent domains that it is dicult to gather annotated train - ing data for all of them .
hence , this pa - per studies the problem of domain adapta - tion for sentiment classiers , hereby a sys - tem is trained on labeled reviews from one source domain but is meant to be deployed on another .
we propose a deep learning ap - proach which learns to extract a meaningful representation for each review in an unsuper - vised fashion .
sentiment classiers trained with this high - level clearly outperform state - of - the - art methods on a benchmark composed of reviews of 123 types of amazon products .
furthermore , this method scales well and allowed us to success - fully perform domain adaptation on a larger industrial - strength dataset of 123 domains .
with the rise of social media such as blogs and so - cial networks , reviews , ratings and recommendations are rapidly proliferating; being able to automatically lter them is a current key challenge for businesses looking to sell their wares and identify new market opportunities .
this has created a surge of research in sentiment classication ( or sentiment analysis ) , which aims to determine the judgment of a writer with re -
appearing in proceedings of the 123 th international con - ference on machine learning , bellevue , wa , usa , 123
copyright 123 by the author ( s ) / owner ( s ) .
spect to a given topic based on a given textual com - ment .
sentiment analysis is now a mature machine learning research topic , as illustrated with this review ( pang and lee , 123 ) .
applications to many dier - ent domains have been presented , ranging from movie reviews ( pang et al . , 123 ) and congressional oor de - bates ( thomas et al . , 123 ) to product recommenda - tions ( snyder and barzilay , 123; blitzer et al . , 123 ) .
this large variety of data sources makes it dicult and costly to design a robust sentiment classier .
indeed , reviews deal with various kinds of products or services for which vocabularies are dierent .
for instance , con - sider the simple case of training a system analyzing reviews about only two sorts of products : kitchen ap - pliances and dvds .
one set of reviews would con - tain adjectives such as malfunctioning , reliable or sturdy , and the other thrilling , horric or hi - larious , etc .
therefore , data distributions are dier - ent across domains .
one solution could be to learn a dierent system for each domain .
however , this would imply a huge cost to annotate training data for a large number of domains and prevent us from ex - ploiting the information shared across domains .
an alternative strategy , evaluated here , consists in learn - ing a single system from the set of domains for which labeled and unlabeled data are available and then ap - ply it to any target domain ( labeled or unlabeled ) .
this only makes sense if the system is able to discover intermediate abstractions that are shared and mean - ingful across domains .
this problem of training and testing models on dierent distributions is known as domain adaptation ( daume iii and marcu , 123 ) .
in this paper , we propose a deep learning approach for the problem of domain adaptation of sentiment classiers .
the promising new area of deep learn - ing has emerged recently; see ( bengio , 123 ) for a re - view .
deep learning is based on algorithms for dis - covering intermediate representations built in a
domain adaptation for sentiment classication with deep learning
hierarchical manner .
deep learning relies on the dis - covery that unsupervised learning could be used to set each level of a hierarchy of features , one level at a time , based on the features discovered at the previous level .
these features have successfully been used to initialize deep neural networks ( hinton and salakhut - dinov , 123; hinton et al . , 123; bengio et al . , 123 ) .
imagine a probabilistic graphical model in which we in - troduce latent variables which correspond to the true explanatory factors of the observed data .
it is likely that answering questions and learning dependencies in the space of these latent variables would be easier than answering questions about the raw input .
a simple lin - ear classier or non - parametric predictor trained from as few as one or a few examples might be able to do the job .
the key to achieving this is learning better rep - resentations , mostly from unlabeled data : how this is done is what dierentiates deep learning algorithms .
the deep learning system we introduce in section 123 is designed to use unlabeled data to extract high - level features from reviews .
we show in section 123 that senti - ment classiers trained with these learnt features can : ( i ) surpass state - of - the - art performance on a bench - mark of 123 kinds of products and ( ii ) successfully per - form domain adaptation on a large - scale data set of 123 domains , beating all of the baselines we tried .
domain adaptation
domain adaptation considers the setting in which the training and testing data are sampled from dierent distributions .
assume we have two sets of data : a source domain s providing labeled training instances and a target domain t providing instances on which the classier is meant to be deployed .
we do not make the assumption that these are drawn from the same distribution , but rather that s is drawn from a distri - bution ps and t from a distribution pt .
the learning problem consists in nding a function realizing a good transfer from s to t i . e .
it is trained on data drawn from ps and generalizes well on data drawn from pt .
deep learning algorithms learns intermediate con - cepts between raw input and target .
our intuition for using it in this setting is that these intermediate concepts could yield better transfer across domains .
suppose for example that these intermediate concepts indirectly capture things like product quality , product price , customer service , etc .
some of these concepts are general enough to make sense across a wide range of domains ( corresponding to products or services , in the case of sentiment analysis ) .
because the same words or tuples of words may be used across domains to indicate the presence of these higher - level concepts ,
it should be possible to discover them .
furthermore , because deep learning exploits unsupervised learning to discover these concepts , one can exploit the large amounts of unlabeled data across all domains to learn these intermediate representations .
here , as in many other deep learning approaches , we do not engineer what these intermediate concepts should be , but in - stead use generic learning algorithms to discover them .
related work
learning setups relating to domain adaptation have been proposed before and published under dierent names .
daume iii and marcu ( 123 ) formalized the problem and proposed an approach based on a mix - ture model .
a general way to address domain adapta - tion is through instance weighting , in which instance - dependent weights are added to the loss function ( jiang and zhai , 123 ) .
another solution to domain adaptation can be to transform the data representa - tions of the source and target domains so that they present the same joint distribution of observations and labels .
ben - david et al .
( 123 ) formally analyze the eect of representation change for domain adaptation while blitzer et al .
( 123 ) propose the structural cor - respondence learning ( scl ) algorithm that makes use of the unlabeled data from the target domain to nd a low - rank joint representation of the data .
finally , domain adaptation can be simply treated as a standard semi - supervised problem by ignoring the do - main dierence and considering the source instances as labeled data and the target ones as unlabeled data ( dai et al . , 123 ) .
in that case , the framework is very close to that of self - taught learning ( raina et al . , 123 ) , in which one learns from labeled examples of some categories as well as unlabeled examples from a larger set of categories .
the approach of raina et al .
( 123 ) relies crucially on the unsupervised learning of a representation , like the approach proposed here .
applications to sentiment classication
sentiment analysis and domain adaptation are closely related in the literature , and many works have studied domain adaptation exclusively for sentiment analysis .
among those , a large majority propose experiments performed on the benchmark made of reviews of ama - zon products gathered by blitzer et al .
( 123 ) .
amazon data the data set proposes more than 123 , 123 reviews regarding 123 dierent product types123 and for which reviews are labeled as either positive
123the data are available from http : / / www . cs . jhu . edu / ~ mdredze / datasets / sentiment / .
it is actually composed of 123 domains but we removed 123 of them which were very small ( less than 123 instances in total ) .
domain adaptation for sentiment classication with deep learning
table 123
amazon data statistics .
this table depicts the number of training , testing and unlabeled examples for each domain , as well as the portion of negative training examples for both versions of the data set .
train size test size unlab .
size % neg
complete ( large - scale ) data set
or negative .
as detailed in table 123 ( top ) , there is a vast disparity between domains in the total number of instances and in the proportion of negative examples .
since this data set is heterogeneous , heavily unbal - anced and large - scale , a smaller and more controlled version has been released .
the reduced data set con - tains 123 dierent domains : books , dvds , electronics and kitchen appliances .
there are 123 positive and 123 negative instances for each domain , as well as a few thousand unlabeled examples .
the positive and negative examples are also exactly balanced ( see the bottom section of table 123 for details ) .
this latter ver - sion is used as a benchmark in the literature .
to the best of our knowledge , this paper will contain the rst published results on the large amazon dataset .
in the original paper regard - ing the smaller 123 - domain benchmark dataset , blitzer et al .
( 123 ) adapt structural correspondence learn - ing ( scl ) for sentiment analysis .
li and zong ( 123 ) propose the multi - label consensus training ( mct ) approach which combines several base classi - ers trained with scl .
pan et al .
( 123 ) rst use a spectral feature alignment ( sfa ) algorithm to align words from dierent source and target domains to help bridge the gap between them .
these 123 methods serve as comparisons in our empirical evaluation .
deep learning approach
if deep learning algorithms are able to capture , to some extent , the underlying generative factors that explain the variations in the input data , what is re - ally needed to exploit that ability is for the learned representations to help in disentangling the under - lying factors of variation .
the simplest and most useful way this could happen is if some of the features learned ( the individual elements of the learned rep - resentation ) are mostly related to only some of these factors , perhaps only one .
conversely , it would mean that such features would have invariant properties , i . e . , they would be highly specic in their response to a sub - set ( maybe only one ) of these factors of variation and insensitive to the others .
this hypothesis was tested by goodfellow et al .
( 123 ) , for images and geometric invariances associated with movements of the camera .
it is interesting to evaluate deep learning algorithms on sentiment analysis for several reasons .
first , if they can extract features that somewhat disentangle the un - derlying factors of variation , this would likely help to perform transfer across domains , since we expect that there exist generic concepts that characterize product reviews across many domains .
second , for our ama - zon datasets , we know some of these factors ( such as whether or not a review is about a particular prod - uct , or is a positive appraisal for that product ) , so we can use this knowledge to quantitatively check to what extent they are disentangled in the learned rep - resentation : domain adaptation for sentiment analysis becomes a medium for better understanding deep ar - chitectures .
finally , even though deep learning algo - rithms have not yet been evaluated for domain adapta - tion of sentiment classiers , several very interesting re - sults have been reported on other tasks involving tex - tual data , beating the previous state - of - the - art in sev - eral cases ( salakhutdinov and hinton , 123; collobert and weston , 123; ranzato and szummer , 123 ) .
stacked denoising auto - encoders
the basic framework for our models is the stacked denoising auto - encoder ( vincent et al . , 123 ) .
an auto - encoder is comprised of an encoder function h ( ) and a decoder function g ( ) , typically with the dimen - sion of h ( ) smaller than that of its argument .
the reconstruction of input x is given by r ( x ) = g ( h ( x ) ) , and auto - encoders are typically trained to minimize a form of reconstruction error loss ( x , r ( x ) ) .
examples of reconstruction error include the squared error , or like here , when the elements of x or r ( x ) can be consid - ered as probabilities of a discrete event , the kullback -
domain adaptation for sentiment classication with deep learning
liebler divergence between elements of x and elements of r ( x ) .
when the encoder and decoder are linear and the reconstruction error is quadratic , one recovers in h ( x ) the space of the principal components ( pca ) of x .
once an auto - encoder has been trained , one can stack another auto - encoder on top of it , by training a second one which sees the encoded output of the rst one as its training data .
stacked auto - encoders were one of the rst methods for building deep architectures ( bengio et al . , 123 ) , along with restricted boltzmann ma - chines ( rbms ) ( hinton et al . , 123 ) .
once a stack of auto - encoders or rbms has been trained , their pa - rameters describe multiple levels of representation for x and can be used to initialize a supervised deep neu - ral network ( bengio , 123 ) or directly feed a classier , as we do in this paper .
an interesting alternative to the ordinary auto - encoder is the denoising auto - encoder ( vincent et al . , 123 ) or dae , in which the input vector x is stochas - tically corrupted into a vector x , and the model is trained to denoise , i . e . , to minimize a denoising recon - struction error loss ( x , r ( x ) ) .
hence the dae cannot simply copy its input x in its code layer h ( x ) , even if the dimension of h ( x ) is greater than that of x .
the denoising error can be linked in several ways to the likelihood of a generative model of the distribution of the uncorrupted examples x ( vincent , 123 ) .
proposed protocol
in our setting we have access to unlabeled data from various domains , and to the labels for one source do - main only .
we tackle the problem of domain adapta - tion for sentiment classiers with a two - step procedure .
first , a higher - level feature extraction is learnt in an unsupervised fashion from the text reviews of all the available domains using a stacked denoising auto - encoder ( sda ) with rectier units ( i . e .
max ( 123 , x ) ) for the code layer .
rbms with ( soft ) rectier units have been introduced in ( nair and hinton , 123 ) .
we have used such units because they have been shown to outperform other non - linearities on a sentiment anal - ysis task ( glorot et al . , 123 ) .
the sda is learnt in a greedy layer - wise fashion using stochastic gradient descent .
for the rst layer , the non - linearity of the decoder is the logistic sigmoid , the corruption process is a masking noise ( i . e .
each active input has a prob - ability p to be set to 123 ) 123 and the training criterion is the kullback - liebler divergence .
the rectier non - linearity is too hard to be used on output units : reconstruction error gradients would not ow if the
123we also tried to set inactive inputs to 123 with a dierent
probability but we did not observe any improvement .
reconstruction was 123 ( argument of the rectier is neg - ative ) when the target is positive .
for training the daes of upper layers , we use the softplus activation function ( i . e .
log ( 123 + exp ( x ) ) , a smooth version of the rectier ) as non - linearity for the decoder output units .
we also use the squared error as reconstruction er - ror criterion and a gaussian corruption noise , which is added before the rectier non - linearity of the input layer in order to keep the sparsity of the representa - tion .
the code layer activations ( after the rectier ) , at dierent depths , dene the new representations in a second step , a linear classier is trained on the transformed labeled data of the source domain .
sup - port vector machines ( svm ) being known to perform well on sentiment classication ( pang et al . , 123 ) , we use a linear svm with squared hinge loss .
this clas - sier is eventually tested on the target domain ( s ) .
the previous protocol exhibits appealing properties for domain adaptation of sentiment classiers .
existing domain adaptation methods for sentiment analysis focus on the information from the source and target distributions , whereas the sda unsupervised learning can use data from other domains , sharing the representation across all those domains .
this also re - duces the computation required to transfer to several domains because a single round of unsupervised train - ing is required , and allows us to scale well with large amount of data and consider real - world applications .
the code learned by the sda is a non - linear map - ping of the input and can therefore encode complex data variations .
to the best of our knowledge , ex - isting domain adaptation methods for sentiment anal - ysis map inputs into a new or an augmented space using only linear projections .
furthermore , rectier non - linearities have the the nice ability to naturally provide sparse representations ( with exact zeros ) for the code layer , which are well suited to linear classi - ers and are ecient with respect to computational cost and memory use .
empirical evaluation 123 .
experimental setup
for both data sets , the preprocessing corresponds to the setting of ( blitzer et al . , 123 ) : each review text is treated as a bag - of - words and transformed into bi - nary vectors encoding the presence / absence of uni - grams and bigrams .
for computational reasons , only the 123 most frequent terms of the vocabulary of un - igrams and bigrams are kept in the feature set
domain adaptation for sentiment classication with deep learning
figure 123
transfer losses on the amazon benchmark of 123 domains : kitchen ( k ) , electronics ( e ) , dvds ( d ) and books ( b ) .
all methods are trained on the labeled set of one domain and evaluated on the test sets of the others .
sdash outperforms all others on 123 out of 123 cases .
figure 123
left : transfer ratios on the amazon benchmark .
both sda - based systems outperforms the rest even if sdash is better .
right : proxy a - distances between domains of the amazon benchmark for the 123 dierent pairs .
transforming data with sdash increases the proxy a - distance .
figure 123
l123 feature selection on the amazon benchmark .
both graphs depict the number of tasks of domain recognition ( x - axis ) and sentiment analysis ( y - axis ) in which a feature is re - used by l123 - classiers trained on raw features ( left ) or features transformed by sdash .
( right ) .
see section 123 for details .
# domain recognition tasks# sentiment analysis tasksraw data123# domain recognition tasks# sentiment analysis taskssdash 123# features123 domain adaptation for sentiment classication with deep learning
use the train / test splits given in table 123
for all ex - periments , the baseline is a linear svm trained on the raw data whereas our method , denoted sdash , cor - responds to the same kind of svm but trained and tested on data for which features have been trans - formed by the system described in section 123
the hyper - parameters of all svms are chosen by cross - validation on the training set .
explored an extensive for sdash , we a masking noise probability in ( 123 , 123 , 123 , 123 , 123 , 123 , 123 ) , ( its optimal value was usually high : 123 ) ; a gaussian noise standard devia - tion for upper layers in ( 123 , 123 , 123 , 123 , 123 ) ; a size of hidden layers in ( 123 , 123 , 123 ) , ( 123 always gave the best performance ) ; an l123 regularization penalty on the activation values in ( 123 , 123 , 123 , 123 , 123 ) ; a learning rate in ( 123 , 123 , 123 , 123 ) .
all values were selected w . r . t .
the averaged in - domain valida - tion error .
all algorithms were implemented using the theano library ( bergstra et al . , 123 ) .
we denote by e ( s , t ) , the transfer error , dened as the test error obtained by a method trained on the source domain s and tested on the target domain t ( e ( t , t ) is termed the in - domain error ) .
the main point of comparison in domain adaptation is the baseline in - domain error , denoted eb ( t , t ) , which corresponds to the test error obtained by the baseline method , i . e .
a linear svm on raw features trained and tested on the raw features of the target domain .
with these denitions , we can dene the standard do - main adaptation metric : the transfer loss t .
the dierence between the transfer error and the in - domain baseline error i . e .
t ( s , t ) = e ( s , t ) eb ( t , t ) for a source domain s and a target domain t .
unfortunately , when one deals with a large number of heterogeneous domains with dierent diculties ( as with the large amazon data ) , the transfer loss is not satisfactory .
in addition , taking its mean over all pos - sible couples of source - target domains is uninforma - tive .
hence , we also introduce the following metrics :
transfer ratio q : it also characterizes the trans - fer but is dened by replacing the dierence by a quotient in t because this is less sensitive to im - portant variations of in - domain errors , and thus more adapted to averaging .
we report its mean over all source - target couples of the data set : q = 123 eb ( t , t ) ( with n the number of couples ( s , t ) with s ( cid : 123 ) = t ) .
in - domain ratio i : some domain adaptation
methods , like ours , transform the feature repre - sentation of all the domains , including the source .
thus in - domain errors of such methods are dier - ent from those of the baseline .
the in - domain ratio measures this and is dened by : i = eb ( t , t ) ( with m the total number of domains ) .
benchmark experiments
on the benchmark of 123 domains , we compare our do - main adaptation protocol with the 123 methods from the literature introduced in section 123 : scl , sfa and mct .
we report the results from the original papers , which have been obtained using the whole feature vo - cabulary and on dierent splits , but of identical sizes as ours .
from our experience , results are consistent what - ever the train / test splits as long as set sizes are pre - served .
hence , one can check that all baselines achieve similar performances .
we also report results obtained by a transductive svm ( sindhwani and keerthi , 123 ) trained in a standard semi - supervised setup : the train - ing set of the source domain is used as labeled set , and the training set of the other domains as the unlabeled set . 123 on this data set with a relatively small number of training instances , our unsupervised feature extractor is made of a single layer of 123 units .
main results figure 123 depicts the transfer loss for all methods and for all source - target domain pairs .
the best transfer is achieved by the svm trained on our transformed features in 123 out of 123 cases ( scl is only slightly better in kitchen electronics ) and signicantly better for 123 cases .
interestingly , for each target domain , there is one case of negative transfer loss for sdash : an svm trained on a dierent domain can outperform an svm trained on the target domain because of the quality of our features .
figure 123 ( left ) depicts the transfer ratio for the same methods plus the transductive svm ( t - svm ) and a second version of our system denoted sda .
contrary to sdash , the unsupervised training of sda has not been performed on all the available domains but on couples , as does scl : for instance , to transfer from books to dvd , the feature extractor of sda is trained on reviews from these 123 domains only .
the transfer ra - tio for sda being higher than for sdash , we can con - clude that sharing the unsupervised pre - training across all domains ( even on those which are not directly con - cerned ) is benecial , as expected .
figure 123 also shows that the combination of an unsupervised and a super -
123 non - linear models ( mlps ) were also investigated , with a similar protocol as presented in section 123 , but they did not reach the performance in transfer of the base - line .
we believe this is due to the small training set size .
domain adaptation for sentiment classication with deep learning
of features that have been re - used for n sentiment anal - ysis tasks and for m domain recognition tasks .
com - paring graphs obtained using raw data and data trans - formed by the sdash conrms our hypothesis : rel - evant features for domain recognition and sentiment analysis are far less overlapping in the latter case .
in - deed , a complete feature disentangling would lead to a graph for which only the rst column and the bot - tom line would be colored , indicating that each feature is either used for domain recognition or for sentiment classication , but not both .
transforming raw data with sdash brings features closer to that pattern .
large - scale experiments we now present results obtained on the larger version of the data .
these conditions are more realistic and a better representation of the real - world than those of the previous experiments : more domains with dier - ent and larger sizes , dierent ratios between positive and negative examples , etc .
we compare 123 methods in addition to the baseline : our feature extractor with ei - ther one ( sdash123 ) or 123 layers ( sdash123 ) of 123 units , and a multi - layer perceptron ( mlp ) with the follow - ing architecture : a softmax logistic regression on top of one hidden layer with 123 hyperbolic tangent units .
figure 123 presents the transfer ratio of each model ac - cording to their in - domain ratio .
those results corre - spond to the following averaged transfer generalization errors : ( baseline ) - 123% , ( mlp ) - 123% , ( sdash123 ) - 123% and ( sdash123 ) - 123% .
despite the large number of domains and their heterogeneity , there is a signicant improvement for both sda systems .
the performance of the mlp shows that the non - linearity helps but is not sucient to gather all necessary infor - mation from data : one needs an unsupervised phase which can encompass data from all domains .
one can also verify that , on this large - scale problem , a sin - gle layer is not enough to reach optimal performance .
stacking 123 layers yields the best representation from the data .
it is worth noting that the improvement of sdash123 compared to the baseline is higher on the y - axis than on the x - axis : the representation learnt by sdash123 is more benecial for transfer than in - domain and is thus truly tailored to domain adaptation .
this paper has demonstrated that a deep learning system based on stacked denoising auto - encoders with sparse rectier units can perform an unsupervised feature extraction which is highly benecial for the do - main adaptation of sentiment classiers .
indeed , our experiments have shown that linear classiers trained with this higher - level learnt feature representation of
figure 123
transfer ratios according to in - domain ra - tios on the large - scale amazon data .
systems based on sdash are better for both metrics and depth helps .
vised phase performed by sdash ( and sda ) outper - forms the pure semi - supervised t - svm .
in term of ab - solute classication performance , we obtained the fol - lowing averaged transfer generalization errors : base - line - 123% , sfa - 123% , sdash - 123% .
a - distance the a - distance is a measure of similar - ity between two probability distributions .
ben - david et al .
( 123 ) showed that the a - distance between the source and target distributions is a crucial part of an upper generalization bound for domain adaptation .
they hypothesized that it should be dicult to dis - criminate between the source and target domains in order to have a good transfer between them , because this would imply similar feature distributions .
in prac - tice , computing the exact a - distance is impossible and one has to compute a proxy .
hence , we measured the generalization error of a linear svm classier trained to discriminate between two domains .
our proxy for the a - distance is then dened as da = 123 ( 123 123 ) .
figure 123 ( right ) reports the results for each pair of do - mains .
surprisingly , da is increased in the new feature space : domain recognition is improved by the unsu - pervised feature extraction of sdash .
consequently , following ben - david et al .
( 123 ) , the representation of sdash should hurt transfer , but we also observe an improvement ( see figure 123 ) .
an explanation could be that the unsupervised feature extraction disentangles domain specic and sentiment polarity information .
to test this hypothesis , we trained an l123 - regularized svm to select the most relevant features on 123 domain recognition tasks ( one per domain pair ) , and 123 senti - ment analysis tasks ( one per domain plus all domains together ) .
figure 123 shows a histogram of the number of tasks associated with individual features , separated into the number of domain vs sentiment tasks .
the color level at coordinate ( n , m ) indicates the number
domain adaptation for sentiment classication with deep learning
reviews outperform the current state - of - the - art .
fur - thermore , we have been able to successfully perform domain adaptation on an industrial - scale dataset of 123 domains , where we signicantly improve generaliza - tion over the baseline and over a similarly structured but purely supervised alternative .
this work was supported by darpa dl program , crsng , mitacs , rqchp and sharcnet .
