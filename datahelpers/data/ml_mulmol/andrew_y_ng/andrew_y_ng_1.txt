we describe latent dirichlet allocation ( lda ) , a generative probabilistic model for collections of discrete data such as text corpora .
lda is a three - level hierarchical bayesian model , in which each item of a collection is modeled as a nite mixture over an underlying set of topics .
each topic is , in turn , modeled as an innite mixture over an underlying set of topic probabilities .
in the context of text modeling , the topic probabilities provide an explicit representation of a document .
we present efcient approximate inference techniques based on variational methods and an em algorithm for empirical bayes parameter estimation .
we report results in document modeling , text classication , and collaborative ltering , comparing to a mixture of unigrams model and the probabilistic lsi
in this paper we consider the problem of modeling text corpora and other collections of discrete data .
the goal is to nd short descriptions of the members of a collection that enable efcient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classication , novelty detection , summarization , and similarity and relevance
signicant progress has been made on this problem by researchers in the eld of informa - tion retrieval ( ir ) ( baeza - yates and ribeiro - neto , 123 ) .
the basic methodology proposed by ir researchers for text corporaa methodology successfully deployed in modern internet search enginesreduces each document in the corpus to a vector of real numbers , each of which repre - sents ratios of counts .
in the popular tf - idf scheme ( salton and mcgill , 123 ) , a basic vocabulary of words or terms is chosen , and , for each document in the corpus , a count is formed of the number of occurrences of each word .
after suitable normalization , this term frequency count is compared to an inverse document frequency count , which measures the number of occurrences of a
c ( cid : 123 ) 123 david m .
blei , andrew y .
ng and michael i .
jordan .
blei , ng , and jordan
word in the entire corpus ( generally on a log scale , and again suitably normalized ) .
the end result is a term - by - document matrix x whose columns contain the tf - idf values for each of the documents in the corpus .
thus the tf - idf scheme reduces documents of arbitrary length to xed - length lists of
while the tf - idf reduction has some appealing featuresnotably in its basic identication of sets of words that are discriminative for documents in the collectionthe approach also provides a rela - tively small amount of reduction in description length and reveals little in the way of inter - or intra - document statistical structure .
to address these shortcomings , ir researchers have proposed several other dimensionality reduction techniques , most notably latent semantic indexing ( lsi ) ( deerwester et al . , 123 ) .
lsi uses a singular value decomposition of the x matrix to identify a linear subspace in the space of tf - idf features that captures most of the variance in the collection .
this approach can achieve signicant compression in large collections .
furthermore , deerwester et al .
argue that the derived features of lsi , which are linear combinations of the original tf - idf features , can capture some aspects of basic linguistic notions such as synonymy and polysemy .
to substantiate the claims regarding lsi , and to study its relative strengths and weaknesses , it is useful to develop a generative probabilistic model of text corpora and to study the ability of lsi to recover aspects of the generative model from data ( papadimitriou et al . , 123 ) .
given a generative model of text , however , it is not clear why one should adopt the lsi methodologyone can attempt to proceed more directly , tting the model to data using maximum likelihood or bayesian methods .
a signicant step forward in this regard was made by hofmann ( 123 ) , who presented the probabilistic lsi ( plsi ) model , also known as the aspect model , as an alternative to lsi .
the plsi approach , which we describe in detail in section 123 , models each word in a document as a sample from a mixture model , where the mixture components are multinomial random variables that can be viewed as representations of topics .
thus each word is generated from a single topic , and different words in a document may be generated from different topics .
each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a xed set of topics .
this distribution is the reduced description associated with
while hofmanns work is a useful step toward probabilistic modeling of text , it is incomplete in that it provides no probabilistic model at the level of documents .
in plsi , each document is represented as a list of numbers ( the mixing proportions for topics ) , and there is no generative probabilistic model for these numbers .
this leads to several problems : ( 123 ) the number of parame - ters in the model grows linearly with the size of the corpus , which leads to serious problems with overtting , and ( 123 ) it is not clear how to assign probability to a document outside of the training set .
to see how to proceed beyond plsi , let us consider the fundamental probabilistic assumptions underlying the class of dimensionality reduction methods that includes lsi and plsi .
all of these methods are based on the bag - of - words assumptionthat the order of words in a document can be neglected .
in the language of probability theory , this is an assumption of exchangeability for the words in a document ( aldous , 123 ) .
moreover , although less often stated formally , these methods also assume that documents are exchangeable; the specic ordering of the documents in a corpus can also be neglected .
a classic representation theorem due to de finetti ( 123 ) establishes that any collection of ex - changeable random variables has a representation as a mixture distributionin general an innite mixture .
thus , if we wish to consider exchangeable representations for documents and words , we need to consider mixture models that capture the exchangeability of both words and documents .
latent dirichlet allocation
this line of thinking leads to the latent dirichlet allocation ( lda ) model that we present in the
it is important to emphasize that an assumption of exchangeability is not equivalent to an as - sumption that the random variables are independent and identically distributed .
rather , exchange - ability essentially can be interpreted as meaning conditionally independent and identically dis - tributed , where the conditioning is with respect to an underlying latent parameter of a probability distribution .
conditionally , the joint distribution of the random variables is simple and factored while marginally over the latent parameter , the joint distribution can be quite complex .
thus , while an assumption of exchangeability is clearly a major simplifying assumption in the domain of text modeling , and its principal justication is that it leads to methods that are computationally efcient , the exchangeability assumptions do not necessarily lead to methods that are restricted to simple frequency counts or linear operations .
we aim to demonstrate in the current paper that , by taking the de finetti theorem seriously , we can capture signicant intra - document statistical structure via the mixing distribution .
it is also worth noting that there are a large number of generalizations of the basic notion of exchangeability , including various forms of partial exchangeability , and that representation theo - rems are available for these cases as well ( diaconis , 123 ) .
thus , while the work that we discuss in the current paper focuses on simple bag - of - words models , which lead to mixture distributions for single words ( unigrams ) , our methods are also applicable to richer models that involve mixtures for larger structural units such as n - grams or paragraphs .
the paper is organized as follows .
in section 123 we introduce basic notation and terminology .
the lda model is presented in section 123 and is compared to related latent variable models in section 123
we discuss inference and parameter estimation for lda in section 123
an illustrative example of tting lda to data is provided in section 123
empirical results in text modeling , text classication and collaborative ltering are presented in section 123
finally , section 123 presents our
notation and terminology
we use the language of text collections throughout the paper , referring to entities such as words , documents , and corpora .
this is useful in that it helps to guide intuition , particularly when we introduce latent variables which aim to capture abstract notions such as topics .
it is important to note , however , that the lda model is not necessarily tied to text , and has applications to other problems involving collections of data , including data from domains such as collaborative ltering , content - based image retrieval and bioinformatics .
indeed , in section 123 , we present experimental results in the collaborative ltering domain .
formally , we dene the following terms : ( cid : 123 ) a word is the basic unit of discrete data , dened to be an item from a vocabulary indexed by f123 , .
we represent words using unit - basis vectors that have a single component equal to one and all other components equal to zero .
thus , using superscripts to denote components , the vth word in the vocabulary is represented by a v - vector w such that wv = 123 and wu = 123 for u 123= v .
( cid : 123 ) a document is a sequence of n words denoted by w = ( w123 , w123 , .
, wn ) , where wn is the nth word in the sequence .
( cid : 123 ) a corpus is a collection of m documents denoted by d = fw123 , w123 ,
blei , ng , and jordan
we wish to nd a probabilistic model of a corpus that not only assigns high probability to
members of the corpus , but also assigns high probability to other similar documents .
latent dirichlet allocation
latent dirichlet allocation ( lda ) is a generative probabilistic model of a corpus .
the basic idea is that documents are represented as random mixtures over latent topics , where each topic is charac - terized by a distribution over words . 123
lda assumes the following generative process for each document w in a corpus d : 123
choose n ( cid : 123 ) poisson ( x ) .
choose q ( cid : 123 ) dir ( a ) .
for each of the n words wn :
( a ) choose a topic zn ( cid : 123 ) multinomial ( q ) .
( b ) choose a word wn from p ( wnj zn , b ) , a multinomial probability conditioned on the topic
several simplifying assumptions are made in this basic model , some of which we remove in subse - quent sections .
first , the dimensionality k of the dirichlet distribution ( and thus the dimensionality of the topic variable z ) is assumed known and xed .
second , the word probabilities are parameter - i j = p ( w j = 123j zi = 123 ) , which for now we treat as a xed quantity ized by a k ( cid : 123 ) v matrix b where b that is to be estimated .
finally , the poisson assumption is not critical to anything that follows and more realistic document length distributions can be used as needed .
furthermore , note that n is independent of all the other data generating variables ( q and z ) .
it is thus an ancillary variable and we will generally ignore its randomness in the subsequent development .
a k - dimensional dirichlet random variable q can take values in the ( k 123 ) - simplex ( a k - vector lies in the ( k 123 ) - simplex if q i = 123 ) , and has the following probability density on this
i ( cid : 123 ) 123 , ( cid : 123 )
is a k - vector with components a
ja ) =
where the parameter a i > 123 , and where g ( x ) is the gamma function .
the dirichlet is a convenient distribution on the simplex it is in the exponential family , has nite dimensional sufcient statistics , and is conjugate to the multinomial distribution .
in section 123 , these properties will facilitate the development of inference and parameter estimation algorithms for lda .
, a set of n topics z , and
, the joint distribution of a topic mixture q
given the parameters a and b a set of n words w is given by :
, b ) = p ( q
p ( znjq ) p ( wnj zn , b ) ,
we refer to the latent multinomial variables in the lda model as topics , so as to exploit text - oriented intuitions , but we make no epistemological claims regarding these latent variables beyond their utility in representing probability distributions on sets of words .
latent dirichlet allocation
figure 123 : graphical model representation of lda .
the boxes are plates representing replicates .
the outer plate represents documents , while the inner plate represents the repeated choice of topics and words within a document .
where p ( znjq ) is simply q z , we obtain the marginal distribution of a document :
i for the unique i such that zi
integrating over q and summing over
p ( znjq ) p ( wnj zn , b )
, b ) =
, b ) =
p ( q d ja )
finally , taking the product of the marginal probabilities of single documents , we obtain the proba - bility of a corpus :
p ( zdnjq d ) p ( wdnj zdn , b )
the lda model is represented as a probabilistic graphical model in figure 123
as the gure makes clear , there are three levels to the lda representation .
the parameters a and b are corpus - level parameters , assumed to be sampled once in the process of generating a corpus .
the variables q d are document - level variables , sampled once per document .
finally , the variables zdn and wdn are word - level variables and are sampled once for each word in each document .
it is important to distinguish lda from a simple dirichlet - multinomial clustering model .
a classical clustering model would involve a two - level model in which a dirichlet is sampled once for a corpus , a multinomial clustering variable is selected once for each document in the corpus , and a set of words are selected for the document conditional on the cluster variable .
as with many clustering models , such a model restricts a document to being associated with a single topic .
lda , on the other hand , involves three levels , and notably the topic node is sampled repeatedly within the document .
under this model , documents can be associated with multiple topics .
structures similar to that shown in figure 123 are often studied in bayesian statistical modeling , where they are referred to as hierarchical models ( gelman et al . , 123 ) , or more precisely as con - ditionally independent hierarchical models ( kass and steffey , 123 ) .
such models are also often referred to as parametric empirical bayes models , a term that refers not only to a particular model structure , but also to the methods used for estimating parameters in the model ( morris , 123 ) .
in - deed , as we discuss in section 123 , we adopt the empirical bayes approach to estimating parameters such as a and b in simple implementations of lda , but we also consider fuller bayesian approaches
blei , ng , and jordan
123 lda and exchangeability a nite set of random variables fz123 , .
, zng is said to be exchangeable if the joint distribution is invariant to permutation
is a permutation of the integers from 123 to n :
p ( z123 , .
, zn ) = p ( zp ( 123 ) , .
, zp ( n ) ) .
an innite sequence of random variables is innitely exchangeable if every nite subsequence is
de finettis representation theorem states that the joint distribution of an innitely exchangeable sequence of random variables is as if a random parameter were drawn from some distribution and then the random variables in question were independent and identically distributed , conditioned on
in lda , we assume that words are generated by topics ( by xed conditional distributions ) and that those topics are innitely exchangeable within a document .
by de finettis theorem , the prob - ability of a sequence of words and topics must therefore have the form :
p ( znjq ) p ( wnj zn )
is the random parameter of a multinomial over topics .
we obtain the lda distribution on documents in eq .
( 123 ) by marginalizing out the topic variables and endowing q with a dirichlet
123 a continuous mixture of unigrams
the lda model shown in figure 123 is somewhat more elaborate than the two - level models often studied in the classical hierarchical bayesian literature .
by marginalizing over the hidden topic variable z , however , we can understand lda as a two - level model .
in particular , let us form the word distribution p ( wjq , b ) :
p ( wjq , b ) = ( cid : 123 )
p ( wj z , b ) p ( zjq ) .
note that this is a random quantity since it depends on q
we now dene the following generative process for a document w : 123
choose q ( cid : 123 ) dir ( a ) .
for each of the n words wn :
( a ) choose a word wn from p ( wnjq , b ) .
this process denes the marginal distribution of a document as a continuous mixture distribution :
p ( wnjq , b ) ja ) are the mixture weights .
, b ) =
where p ( wnjq , b ) are the mixture components and p ( q figure 123 illustrates this interpretation of lda .
it depicts the distribution on p ( wjq , b ) which is induced from a particular instance of an lda model .
note that this distribution on the ( v 123 ) - simplex is attained with only k + kv parameters yet exhibits a very interesting multimodal structure .
latent dirichlet allocation
figure 123 : an example density on unigram distributions p ( wjq , b ) under lda for three words and four topics .
the triangle embedded in the x - y plane is the 123 - d simplex representing all possible multinomial distributions over three words .
each of the vertices of the trian - gle corresponds to a deterministic distribution that assigns probability one to one of the words; the midpoint of an edge gives probability 123 to two of the words; and the centroid of the triangle is the uniform distribution over all three words .
the four points marked with an x are the locations of the multinomial distributions p ( wj z ) for each of the four topics , and the surface shown on top of the simplex is an example of a density over the ( v 123 ) - simplex ( multinomial distributions of words ) given by lda .
relationship with other latent variable models
in this section we compare lda to simpler latent variable models for textthe unigram model , a mixture of unigrams , and the plsi model .
furthermore , we present a unied geometric interpreta - tion of these models which highlights their key differences and similarities .
123 unigram model
under the unigram model , the words of every document are drawn independently from a single
this is illustrated in the graphical model in figure 123a .
blei , ng , and jordan
( b ) mixture of unigrams
( c ) plsi / aspect model
figure 123 : graphical model representation of different models of discrete data .
123 mixture of unigrams
if we augment the unigram model with a discrete random topic variable z ( figure 123b ) , we obtain a mixture of unigrams model ( nigam et al . , 123 ) .
under this mixture model , each document is gen - erated by rst choosing a topic z and then generating n words independently from the conditional multinomial p ( wj z ) .
the probability of a document is :
p ( w ) = ( cid : 123 )
when estimated from a corpus , the word distributions can be viewed as representations of topics under the assumption that each document exhibits exactly one topic .
as the empirical results in section 123 illustrate , this assumption is often too limiting to effectively model a large collection of in contrast , the lda model allows documents to exhibit multiple topics to different degrees .
this is achieved at a cost of just one additional parameter : there are k 123 parameters associated with p ( z ) in the mixture of unigrams , versus the k parameters associated with p ( q
ja ) in lda .
123 probabilistic latent semantic indexing
probabilistic latent semantic indexing ( plsi ) is another widely used document model ( hofmann , 123 ) .
the plsi model , illustrated in figure 123c , posits that a document label d and a word wn are
latent dirichlet allocation
conditionally independent given an unobserved topic z :
p ( d , wn ) = p ( d ) ( cid : 123 )
p ( wnj z ) p ( zj d ) .
the plsi model attempts to relax the simplifying assumption made in the mixture of unigrams model that each document is generated from only one topic .
in a sense , it does capture the possibility that a document may contain multiple topics since p ( zj d ) serves as the mixture weights of the topics for a particular document d .
however , it is important to note that d is a dummy index into the list of documents in the training set .
thus , d is a multinomial random variable with as many possible values as there are training documents and the model learns the topic mixtures p ( zj d ) only for those documents on which it is trained .
for this reason , plsi is not a well - dened generative model of documents; there is no natural way to use it to assign probability to a previously unseen document .
a further difculty with plsi , which also stems from the use of a distribution indexed by train - ing documents , is that the number of parameters which must be estimated grows linearly with the number of training documents .
the parameters for a k - topic plsi model are k multinomial distri - butions of size v and m mixtures over the k hidden topics .
this gives kv + km parameters and therefore linear growth in m .
the linear growth in parameters suggests that the model is prone to overtting and , empirically , overtting is indeed a serious problem ( see section 123 ) .
in prac - tice , a tempering heuristic is used to smooth the parameters of the model for acceptable predic - tive performance .
it has been shown , however , that overtting can occur even when tempering is used ( popescul et al . , 123 ) .
lda overcomes both of these problems by treating the topic mixture weights as a k - parameter hidden random variable rather than a large set of individual parameters which are explicitly linked to the training set .
as described in section 123 , lda is a well - dened generative model and generalizes easily to new documents .
furthermore , the k + kv parameters in a k - topic lda model do not grow with the size of the training corpus .
we will see in section 123 that lda does not suffer from the same overtting issues as plsi .
123 a geometric interpretation
a good way of illustrating the differences between lda and the other latent topic models is by considering the geometry of the latent space , and seeing how a document is represented in that geometry under each model .
all four of the models described aboveunigram , mixture of unigrams , plsi , and lda operate in the space of distributions over words .
each such distribution can be viewed as a point on the ( v 123 ) - simplex , which we call the word simplex .
the unigram model nds a single point on the word simplex and posits that all words in the corpus come from the corresponding distribution .
the latent variable models consider k points on the word simplex and form a sub - simplex based on those points , which we call the topic simplex .
note that any point on the topic simplex is also a point on the word simplex .
the different latent variable models use the topic simplex in different ways to generate a document .
( cid : 123 ) the mixture of unigrams model posits that for each document , one of the k points on the word simplex ( that is , one of the corners of the topic simplex ) is chosen randomly and all the words of the document are drawn from the distribution corresponding to that point .
blei , ng , and jordan
figure 123 : the topic simplex for three topics embedded in the word simplex for three words .
the corners of the word simplex correspond to the three distributions where each word ( re - spectively ) has probability one .
the three points of the topic simplex correspond to three different distributions over words .
the mixture of unigrams places each document at one of the corners of the topic simplex .
the plsi model induces an empirical distribution on the topic simplex denoted by x .
lda places a smooth distribution on the topic simplex denoted by the contour lines .
( cid : 123 ) the plsi model posits that each word of a training document comes from a randomly chosen topic .
the topics are themselves drawn from a document - specic distribution over topics , i . e . , a point on the topic simplex .
there is one such distribution for each document; the set of training documents thus denes an empirical distribution on the topic simplex .
( cid : 123 ) lda posits that each word of both the observed and unseen documents is generated by a randomly chosen topic which is drawn from a distribution with a randomly chosen parameter .
this parameter is sampled once per document from a smooth distribution on the topic simplex .
these differences are highlighted in figure 123
inference and parameter estimation
we have described the motivation behind lda and illustrated its conceptual advantages over other latent topic models .
in this section , we turn our attention to procedures for inference and parameter estimation under lda .
latent dirichlet allocation
figure 123 : ( left ) graphical model representation of lda .
( right ) graphical model representation
of the variational distribution used to approximate the posterior in lda .
the key inferential problem that we need to solve in order to use lda is that of computing the posterior distribution of the hidden variables given a document :
, b ) = p ( q , z , wja
i g ( a
unfortunately , this distribution is intractable to compute in general .
indeed , to normalize the distri - bution we marginalize over the hidden variables and write eq .
( 123 ) in terms of the model parameters :
, b ) =
a function which is intractable due to the coupling between q and b in the summation over latent topics ( dickey , 123 ) .
dickey shows that this function is an expectation under a particular extension to the dirichlet distribution which can be represented with special hypergeometric functions .
it has been used in a bayesian context for censored discrete data to represent the posterior on q which , in that setting , is a random parameter ( dickey et al . , 123 ) .
i j ) w j
although the posterior distribution is intractable for exact inference , a wide variety of approxi - mate inference algorithms can be considered for lda , including laplace approximation , variational approximation , and markov chain monte carlo ( jordan , 123 ) .
in this section we describe a simple convexity - based variational algorithm for inference in lda , and discuss some of the alternatives in
123 variational inference
the basic idea of convexity - based variational inference is to make use of jensens inequality to ob - tain an adjustable lower bound on the log likelihood ( jordan et al . , 123 ) .
essentially , one considers a family of lower bounds , indexed by a set of variational parameters .
the variational parameters are chosen by an optimization procedure that attempts to nd the tightest possible lower bound .
a simple way to obtain a tractable family of lower bounds is to consider simple modications of the original graphical model in which some of the edges and nodes are removed .
consider in particular the lda model shown in figure 123 ( left ) .
the problematic coupling between q and b
blei , ng , and jordan
arises due to the edges between q , z , and w .
by dropping these edges and the w nodes , and endow - ing the resulting simplied graphical model with free variational parameters , we obtain a family of distributions on the latent variables .
this family is characterized by the following variational
q ( q , zjg , f ) = q ( q
where the dirichlet parameter g and the multinomial parameters ( f 123 , .
, f n ) are the free variational
having specied a simplied family of probability distributions , the next step is to set up an optimization problem that determines the values of the variational parameters g and f .
as we show in appendix a , the desideratum of nding a tight lower bound on the log likelihood translates directly into the following optimization problem :
( g ( cid : 123 ) , f ( cid : 123 ) ) = argmin ( g , f )
d ( q ( q , zjg , f ) k p ( q , zjw , a
thus the optimizing values of the variational parameters are found by minimizing the kullback - leibler ( kl ) divergence between the variational distribution and the true posterior p ( q , zjw , a this minimization can be achieved via an iterative xed - point method .
in particular , we show in appendix a . 123 that by computing the derivatives of the kl divergence and setting them equal to zero , we obtain the following pair of update equations :
i + ( cid : 123 ) n
as we show in appendix a . 123 , the expectation in the multinomial update can be computed as follows :
i ) jg ) = y
is the rst derivative of the logg
function which is computable via taylor approxima -
tions ( abramowitz and stegun , 123 ) .
( 123 ) and ( 123 ) have an appealing intuitive interpretation .
the dirichlet update is a poste - rior dirichlet given expected observations taken under the variational distribution , e ( znjf n ) .
the multinomial update is akin to using bayes theorem , p ( znj wn ) ( cid : 123 ) p ( wnj zn ) p ( zn ) , where p ( zn ) is approximated by the exponential of the expected value of its logarithm under the variational distri -
it is important to note that the variational distribution is actually a conditional distribution , varying as a function of w .
this occurs because the optimization problem in eq .
( 123 ) is conducted for xed w , and thus yields optimizing parameters ( g ( cid : 123 ) , f ( cid : 123 ) ) that are a function of w .
we can write the resulting variational distribution as q ( q , zjg ( cid : 123 ) ( w ) , f ( cid : 123 ) ( w ) ) , where we have made the dependence on w explicit .
thus the variational distribution can be viewed as an approximation to the posterior distribution p ( q , zjw , a in the language of text , the optimizing parameters ( g ( cid : 123 ) ( w ) , f ( cid : 123 ) ( w ) ) are document - specic .
in particular , we view the dirichlet parameters g ( cid : 123 ) ( w ) as providing a representation of a document in the topic simplex .
latent dirichlet allocation
initialize f 123
ni : = 123 / k for all i and n i : = a i + n / k for all i
for n = 123 to n
for i = 123 to k g t+123 : = a + ( cid : 123 ) n
to sum to 123
figure 123 : a variational inference algorithm for lda .
we summarize the variational inference procedure in figure 123 , with appropriate starting points for g and f n .
from the pseudocode it is clear that each iteration of variational inference for lda requires o ( ( n + 123 ) k ) operations .
empirically , we nd that the number of iterations required for a single document is on the order of the number of words in the document .
this yields a total number of operations roughly on the order of n123k .
123 parameter estimation
in this section we present an empirical bayes method for parameter estimation in the lda model ( see section 123 for a fuller bayesian approach ) .
in particular , given a corpus of documents d = fw123 , w123 , .
, wmg , we wish to nd parameters a that maximize the ( marginal ) log likelihood of the data :
, b ) =
log p ( wd ja
as we have described above , the quantity p ( wja
, b ) cannot be computed tractably .
however , variational inference provides us with a tractable lower bound on the log likelihood , a bound which we can maximize with respect to a and b .
we can thus nd approximate empirical bayes estimates for the lda model via an alternating variational em procedure that maximizes a lower bound with respect to the variational parameters g and f , and then , for xed values of the variational parameters , maximizes the lower bound with respect to the model parameters a and b
we provide a detailed derivation of the variational em algorithm for lda in appendix a . 123
the derivation yields the following iterative algorithm :
( e - step ) for each document , nd the optimizing values of the variational parameters fg ( cid : 123 )
d 123 dg .
this is done as described in the previous section .
( m - step ) maximize the resulting lower bound on the log likelihood with respect to the model parameters a and b .
this corresponds to nding maximum likelihood estimates with expected sufcient statistics for each document under the approximate posterior which is computed in
blei , ng , and jordan
figure 123 : graphical model representation of the smoothed lda model .
these two steps are repeated until the lower bound on the log likelihood converges .
in appendix a . 123 , we show that the m - step update for the conditional multinomial parameter b
can be written out analytically :
i j ( cid : 123 )
we further show that the m - step update for dirichlet parameter a efcient newton - raphson method in which the hessian is inverted in linear time .
can be implemented using an
the large vocabulary size that is characteristic of many document corpora creates serious problems of sparsity .
a new document is very likely to contain words that did not appear in any of the documents in a training corpus .
maximum likelihood estimates of the multinomial parameters assign zero probability to such words , and thus zero probability to new documents .
the standard approach to coping with this problem is to smooth the multinomial parameters , assigning positive probability to all vocabulary items whether or not they are observed in the training set ( jelinek , 123 ) .
laplace smoothing is commonly used; this essentially yields the mean of the posterior distribution under a uniform dirichlet prior on the multinomial parameters .
unfortunately , in the mixture model setting , simple laplace smoothing is no longer justied as a maximum a posteriori method ( although it is often implemented in practice; cf .
nigam et al . , 123 ) .
in fact , by placing a dirichlet prior on the multinomial parameter we obtain an intractable posterior in the mixture model setting , for much the same reason that one obtains an intractable posterior in the basic lda model .
our proposed solution to this problem is to simply apply variational inference methods to the extended model that includes dirichlet smoothing on the multinomial parameter .
in the lda setting , we obtain the extended graphical model shown in figure 123
we treat b as a k ( cid : 123 ) v random matrix ( one row for each mixture component ) , where we assume that each row is independently drawn from an exchangeable dirichlet distribution . 123 we now extend our infer - ence procedures to treat the b i as random variables that are endowed with a posterior distribution , 123
an exchangeable dirichlet is simply a dirichlet distribution with a single scalar parameter h
the density is the same
as a dirichlet ( eq .
123 ) where a
for each component .
latent dirichlet allocation
conditioned on the data .
thus we move beyond the empirical bayes procedure of section 123 and consider a fuller bayesian approach to lda .
we consider a variational approach to bayesian inference that places a separable distribution on
the random variables b
, and z ( attias , 123 ) : q ( b 123 : k , z123 : m , q 123 : m jl , f , g ) =
qd ( q d , zd jf d , g d ) ,
where qd ( q , zjf , g ) is the variational distribution dened for lda in eq .
as is easily veried , the resulting variational inference procedure again yields eqs .
( 123 ) and ( 123 ) as the update equations for the variational parameters f and g , respectively , as well as an additional update for the new variational parameter l :
i j = h +
iterating these equations to convergence yields an approximate posterior distribution on b
, and z .
we are now left with the hyperparameter h on the exchangeable dirichlet , as well as the hy - from before .
our approach to setting these hyperparameters is again ( approximate ) empirical bayeswe use variational em to nd maximum likelihood estimates of these parameters based on the marginal likelihood .
these procedures are described in appendix a . 123
in this section , we provide an illustrative example of the use of an lda model on real data .
our data are 123 , 123 documents from a subset of the trec ap corpus ( harman , 123 ) .
after removing a standard list of stop words , we used the em algorithm described in section 123 to nd the dirichlet and conditional multinomial parameters for a 123 - topic lda model .
the top words from some of the resulting multinomial distributions p ( wj z ) are illustrated in figure 123 ( top ) .
as we have hoped , these distributions seem to capture some of the underlying topics in the corpus ( and we have named them according to these topics ) .
as we emphasized in section 123 , one of the advantages of lda over related latent variable mod - els is that it provides well - dened inference procedures for previously unseen documents .
indeed , we can illustrate how lda works by performing inference on a held - out document and examining the resulting variational posterior parameters .
figure 123 ( bottom ) is a document from the trec ap corpus which was not used for parameter estimation .
using the algorithm in section 123 , we computed the variational posterior dirichlet parameters g for the article and variational posterior multinomial parameters f n for each word in the
recall that the ith posterior dirichlet parameter g
i is approximately the ith prior dirichlet pa - i plus the expected number of words which were generated by the ith topic ( see eq .
therefore , the prior dirichlet parameters subtracted from the posterior dirichlet parameters indicate the expected number of words which were allocated to each topic for a particular document .
for the example article in figure 123 ( bottom ) , most of the g i .
four topics , however , are i ( cid : 123 ) 123 ) .
looking at the corresponding distributions over signicantly larger ( by this , we mean g words identies the topics which mixed to form this document ( figure 123 , top ) .
i are close to a
blei , ng , and jordan
further insight comes from examining the f n parameters .
these distributions approximate p ( znjw ) and tend to peak towards one of the k possible topic values .
in the article text in figure 123 , the words are color coded according to these values ( i . e . , the ith color is used if qn ( zi = 123 ) > 123 ) .
with this illustration , one can identify how the different topics mixed in the document text .
while demonstrating the power of lda , the posterior analysis also highlights some of its lim - itations .
in particular , the bag - of - words assumption allows words that should be generated by the same topic ( e . g . , william randolph hearst foundation ) to be allocated to several different top - ics .
overcoming this limitation would require some form of extension of the basic lda model; in particular , we might relax the bag - of - words assumption by assuming partial exchangeability or markovianity of word sequences .
applications and empirical results
in this section , we discuss our empirical evaluation of lda in several problem domainsdocument modeling , document classication , and collaborative ltering .
in all of the mixture models , the expected complete log likelihood of the data has local max - ima at the points where all or some of the mixture components are equal to each other .
to avoid these local maxima , it is important to initialize the em algorithm appropriately .
in our experiments , we initialize em by seeding each conditional multinomial distribution with ve documents , reduc - ing their effective total length to two words , and smoothing across the whole vocabulary .
this is essentially an approximation to the scheme described in heckerman and meila ( 123 ) .
123 document modeling
we trained a number of latent variable models , including lda , on two text corpora to compare the generalization performance of these models .
the documents in the corpora are treated as unlabeled; thus , our goal is density estimationwe wish to achieve high likelihood on a held - out test set .
in particular , we computed the perplexity of a held - out test set to evaluate the models .
the perplexity , used by convention in language modeling , is monotonically decreasing in the likelihood of the test data , and is algebraicly equivalent to the inverse of the geometric mean per - word likelihood .
a lower perplexity score indicates better generalization performance . 123 more formally , for a test set of m documents , the perplexity is :
perplexity ( dtest ) = exp
d=123 log p ( wd )
in our experiments , we used a corpus of scientic abstracts from the c .
elegans community ( av - ery , 123 ) containing 123 , 123 abstracts with 123 , 123 unique terms , and a subset of the trec ap corpus containing 123 , 123 newswire articles with 123 , 123 unique terms .
in both cases , we held out 123% of the data for test purposes and trained the models on the remaining 123% .
in preprocessing the data ,
note that we simply use perplexity as a gure of merit for comparing models .
the models that we compare are all unigram ( bag - of - words ) models , whichas we have discussed in the introductionare of interest in the informa - tion retrieval context .
we are not attempting to do language modeling in this paperan enterprise that would require us to examine trigram or other higher - order models .
we note in passing , however , that extensions of lda could be considered that involve dirichlet - multinomial over trigrams instead of unigrams .
we leave the exploration of such extensions to language modeling to future work .
latent dirichlet allocation
\chide " \ed cai "
actress gveret care
the william randolph hearst foundation will give $123 million to lincoln center , metropoli - tan opera co . , new york philharmonic and juilliard school .
our board felt that we had a real opportunity to make a mark on the future of the performing arts with these grants an act every bit as important as our traditional areas of support in health , medical research , education and the social services , hearst foundation president randolph a .
hearst said monday in announcing the grants .
lincoln centers share will be $123 , 123 for its new building , which will house young artists and provide new public facilities .
the metropolitan opera co .
and new york philharmonic will receive $123 , 123 each .
the juilliard school , where music and the performing arts are taught , will get $123 , 123
the hearst foundation , a leading supporter of the lincoln center consolidated corporate fund , will make its usual annual $123 , 123
figure 123 : an example article from the ap corpus .
each color codes a different factor from which
the word is putatively generated .
blei , ng , and jordan
smoothed mixt .
unigrams fold in plsi
number of topics
smoothed mixt .
unigrams fold in plsi
number of topics
figure 123 : perplexity results on the nematode ( top ) and ap ( bottom ) corpora for lda , the unigram
model , mixture of unigrams , and plsi .
latent dirichlet allocation
topics ( k ) perplexity ( mult .
mixt . ) perplexity ( plsi )
table 123 : overtting in the mixture of unigrams and plsi models for the ap corpus .
similar behav -
ior is observed in the nematode corpus ( not reported ) .
we removed a standard list of 123 stop words from each corpus .
from the ap data , we further removed words that occurred only once .
we compared lda with the unigram , mixture of unigrams , and plsi models described in sec - tion 123
we trained all the hidden variable models using em with exactly the same stopping criteria , that the average change in expected log likelihood is less than 123% .
both the plsi model and the mixture of unigrams suffer from serious overtting issues , though for different reasons .
this phenomenon is illustrated in table 123
in the mixture of unigrams model , overtting is a result of peaked posteriors in the training set; a phenomenon familiar in the super - vised setting , where this model is known as the naive bayes model ( rennie , 123 ) .
this leads to a nearly deterministic clustering of the training documents ( in the e - step ) which is used to determine the word probabilities in each mixture component ( in the m - step ) .
a previously unseen document may best t one of the resulting mixture components , but will probably contain at least one word which did not occur in the training documents that were assigned to that component .
such words will have a very small probability , which causes the perplexity of the new document to explode .
as k increases , the documents of the training corpus are partitioned into ner collections and thus induce more words with small probabilities .
in the mixture of unigrams , we can alleviate overtting through the variational bayesian smooth - ing scheme presented in section 123 .
this ensures that all words will have some probability under every mixture component .
in the plsi case , the hard clustering problem is alleviated by the fact that each document is allowed to exhibit a different proportion of topics .
however , plsi only refers to the training doc - uments and a different overtting problem arises that is due to the dimensionality of the p ( zjd ) parameter .
one reasonable approach to assigning probability to a previously unseen document is by marginalizing over d :
p ( w ) = ( cid : 123 )
p ( wnj z ) p ( zj d ) p ( d ) .
essentially , we are integrating over the empirical distribution on the topic simplex ( see figure 123 ) .
this method of inference , though theoretically sound , causes the model to overt .
the document - specic topic distribution has some components which are close to zero for those topics that do not appear in the document .
thus , certain words will have very small probability in the estimates of
blei , ng , and jordan
each mixture component .
when determining the probability of a new document through marginal - ization , only those training documents which exhibit a similar proportion of topics will contribute to the likelihood .
for a given training documents topic proportions , any word which has small probability in all the constituent topics will cause the perplexity to explode .
as k gets larger , the chance that a training document will exhibit topics that cover all the words in the new document decreases and thus the perplexity grows .
note that plsi does not overt as quickly ( with respect to k ) as the mixture of unigrams .
this overtting problem essentially stems from the restriction that each future document exhibit the same topic proportions as were seen in one or more of the training documents .
given this constraint , we are not free to choose the most likely proportions of topics for the new document .
an alternative approach is the folding - in heuristic suggested by hofmann ( 123 ) , where one ignores the p ( zjd ) parameters and rets p ( zjdnew ) .
note that this gives the plsi model an unfair advantage by allowing it to ret k 123 parameters to the test data .
lda suffers from neither of these problems .
as in plsi , each document can exhibit a different proportion of underlying topics .
however , lda can easily assign probability to a new document; no heuristics are needed for a new document to be endowed with a different set of topic proportions than were associated with documents in the training corpus .
figure 123 presents the perplexity for each model on both corpora for different values of k .
the plsi model and mixture of unigrams are suitably corrected for overtting .
the latent variable models perform better than the simple unigram model .
lda consistently performs better than the
123 document classication
in the text classication problem , we wish to classify a document into two or more mutually ex - clusive classes .
as in any classication problem , we may wish to consider generative approaches or discriminative approaches .
in particular , by using one lda module for each class , we obtain a generative model for classication .
it is also of interest to use lda in the discriminative framework , and this is our focus in this section .
a challenging aspect of the document classication problem is the choice of features .
treating individual words as features yields a rich but very large feature set ( joachims , 123 ) .
one way to reduce this feature set is to use an lda model for dimensionality reduction .
in particular , lda reduces any document to a xed set of real - valued featuresthe posterior dirichlet parameters g ( cid : 123 ) ( w ) associated with the document .
it is of interest to see how much discriminatory information we lose in reducing the document description to these parameters .
we conducted two binary classication experiments using the reuters - 123 dataset
dataset contains 123 documents and 123 , 123 words .
in these experiments , we estimated the parameters of an lda model on all the documents , without reference to their true class label .
we then trained a support vector machine ( svm ) on the low - dimensional representations provided by lda and compared this svm to an svm trained on all the word features .
using the svmlight software package ( joachims , 123 ) , we compared an svm trained on all the word features with those trained on features induced by a 123 - topic lda model .
note that we reduce the feature space by 123 percent in this case .
latent dirichlet allocation
proportion of data used for training
proportion of data used for training
figure 123 : classication results on two binary classication problems from the reuters - 123 dataset for different proportions of training data .
graph ( a ) is earn vs .
not earn .
graph ( b ) is grain vs .
not grain .
fold in plsi smoothed mixt .
unigrams
number of topics
figure 123 : results for collaborative ltering on the eachmovie data .
figure 123 shows our results .
we see that there is little reduction in classication performance in using the lda - based features; indeed , in almost all cases the performance is improved with the lda features .
although these results need further substantiation , they suggest that the topic - based representation provided by lda may be useful as a fast ltering algorithm for feature selection in
blei , ng , and jordan
123 collaborative ltering
our nal experiment uses the eachmovie collaborative ltering data .
in this data set , a collection of users indicates their preferred movie choices .
a user and the movies chosen are analogous to a document and the words in the document ( respectively ) .
the collaborative ltering task is as follows .
we train a model on a fully observed set of users .
then , for each unobserved user , we are shown all but one of the movies preferred by that user and are asked to predict what the held - out movie is .
the different algorithms are evaluated according to the likelihood they assign to the held - out movie .
more precisely , dene the predictive perplexity on m test users to be :
predictive - perplexity ( dtest ) = exp
d=123 log p ( wd , nd
we restricted the eachmovie dataset to users that positively rated at least 123 movies ( a positive rating is at least four out of ve stars ) .
we divided this set of users into 123 training users and 123
under the mixture of unigrams model , the probability of a movie given a set of observed movies
is obtained from the posterior distribution over topics :
p ( wjwobs ) = ( cid : 123 )
in the plsi model , the probability of a held - out movie is given by the same equation except that p ( zjwobs ) is computed by folding in the previously seen movies .
finally , in the lda model , the probability of a held - out movie is given by integrating over the posterior dirichlet :
p ( wjz ) p ( zjq ) p ( q jwobs ) dq ,
where p ( q jwobs ) is given by the variational inference method described in section 123 .
note that this quantity is efcient to compute .
we can interchange the sum and integral sign , and compute a linear combination of k dirichlet expectations .
with a vocabulary of 123 movies , we nd the predictive perplexities illustrated in figure 123
again , the mixture of unigrams model and plsi are corrected for overtting , but the best predictive perplexities are obtained by the lda model .
we have described latent dirichlet allocation , a exible generative probabilistic model for collec - tions of discrete data .
lda is based on a simple exchangeability assumption for the words and topics in a document; it is therefore realized by a straightforward application of de finettis repre - sentation theorem .
we can view lda as a dimensionality reduction technique , in the spirit of lsi , but with proper underlying generative probabilistic semantics that make sense for the type of data that it models .
exact inference is intractable for lda , but any of a large suite of approximate inference algo - rithms can be used for inference and parameter estimation within the lda framework .
we have presented a simple convexity - based variational approach for inference , showing that it yields a fast
latent dirichlet allocation
algorithm resulting in reasonable comparative performance in terms of test set likelihood .
other approaches that might be considered include laplace approximation , higher - order variational tech - niques , and monte carlo methods .
in particular , leisink and kappen ( 123 ) have presented a general methodology for converting low - order variational lower bounds into higher - order varia - tional bounds .
it is also possible to achieve higher accuracy by dispensing with the requirement of maintaining a bound , and indeed minka and lafferty ( 123 ) have shown that improved inferential accuracy can be obtained for the lda model via a higher - order variational technique known as ex - pectation propagation .
finally , grifths and steyvers ( 123 ) have presented a markov chain monte carlo algorithm for lda .
lda is a simple model , and although we view it as a competitor to methods such as lsi and plsi in the setting of dimensionality reduction for document collections and other discrete cor - pora , it is also intended to be illustrative of the way in which probabilistic models can be scaled up to provide useful inferential machinery in domains involving multiple levels of structure .
in - deed , the principal advantages of generative models such as lda include their modularity and their extensibility .
as a probabilistic module , lda can be readily embedded in a more complex model a property that is not possessed by lsi .
in recent work we have used pairs of lda modules to model relationships between images and their corresponding descriptive captions ( blei and jordan , 123 ) .
moreover , there are numerous possible extensions of lda .
for example , lda is readily extended to continuous data or other non - multinomial data .
as is the case for other mixture models , including nite mixture models and hidden markov models , the emission probability p ( wnj zn ) contributes only a likelihood value to the inference procedures for lda , and other likelihoods are readily substituted in its place .
in particular , it is straightforward to develop a continuous variant of lda in which gaussian observables are used in place of multinomials .
another simple extension of lda comes from allowing mixtures of dirichlet distributions in the place of the single dirichlet of lda .
this allows a richer structure in the latent topic space and in particular allows a form of document clustering that is different from the clustering that is achieved via shared topics .
finally , a variety of extensions of lda can be considered in which the distributions on the topic variables are elaborated .
for example , we could arrange the topics in a time series , essentially relaxing the full exchangeability assumption to one of partial exchangeability .
we could also consider partially exchangeable models in which we condition on exogenous variables; thus , for example , the topic distribution could be conditioned on features such as paragraph or sentence , providing a more powerful text model that makes use of information obtained from a parser .
this work was supported by the national science foundation ( nsf grant iis - 123 ) and the multidisciplinary research program of the department of defense ( muri n123 - 123 - 123 - 123 ) .
andrew y .
ng and david m .
blei were additionally supported by fellowships from the microsoft
