previous work on action recognition has focused on adapting hand - designed local features , such as sift or hog , from static images to the video domain .
in this pa - per , we propose using unsupervised feature learning as a way to learn features directly from video data .
more specif - ically , we present an extension of the independent subspace analysis algorithm to learn invariant spatio - temporal fea - tures from unlabeled video data .
we discovered that , despite its simplicity , this method performs surprisingly well when combined with deep learning techniques such as stack - ing and convolution to learn hierarchical representations .
by replacing hand - designed features with our learned fea - tures , we achieve classication results superior to all pre - vious published results on the hollywood123 , ucf , kth and youtube action recognition datasets .
on the challenging hollywood123 and youtube action datasets we obtain 123% and 123% respectively , which are approximately 123% better than the current best published results .
further benets of this method , such as the ease of training and the efciency of training and prediction , will also be discussed .
you can download our code and learned spatio - temporal features
common approaches in visual recognition rely on hand - designed features such as sift ( 123 , 123 ) and hog ( 123 ) .
a weakness of such approaches is that it is difcult and time - consuming to extend these features to other sensor modal - ities , such as laser scans , text or even videos .
there is a growing interest in unsupervised feature learning methods such as sparse coding ( 123 , 123 , 123 ) , deep belief nets ( 123 ) and stacked autoencoders ( 123 ) because they learn features directly from data and consequently are more generalizable .
in this paper , we provide further evidence that unsu - pervised learning not only generalizes to different domains but also achieves impressive performance on many realistic video datasets .
at the heart of our algorithm is the use of in - dependent subspace analysis ( isa ) , an extension of inde -
pendent component analysis ( ica ) , both very well - known in the eld of natural image statistics ( 123 , 123 ) .
experimen - tal studies in this eld have shown that these algorithms can learn receptive elds similar to the v123 area of visual cor - tex when applied to static images and the mt area of visual cortex when applied to sequences of images ( 123 , 123 , 123 ) .
an advantage of isa , compared to the more standard ica algorithm , is that it learns features that are robust to local translation while being selective to frequency , rotation and velocity .
a disadvantage of isa , as well as ica , is that it can be very slow to train when the dimension of the input data is large .
in this paper , we scale up the original isa to larger input data by employing two important ideas from convolutional neural networks ( 123 ) : convolution and in detail , we rst learn features with small in - put patches; the learned features are then convolved with a larger region of the input data .
the outputs of this convo - lution step are inputs to the layer above .
this convolutional stacking idea enables the algorithm to learn a hierarchical representation of the data suitable for recognition ( 123 ) .
we evaluate our method using the experimental pro - tocols described in wang et al .
( 123 ) on four well - known benchmark datasets : kth ( 123 ) , hollywood123 ( 123 ) , ucf ( sport actions ) ( 123 ) and youtube ( 123 ) .
surprisingly , despite its simplicity , our method outperforms all published meth - ods that use either hand - crafted ( 123 , 123 ) or learned features ( 123 ) ( see table 123 ) .
the improvements on hollywood123 and youtube datasets are approximately 123% .
table 123
our results compared to the best results so far on four datasets ( see table 123 , 123 , 123 , 123 for more detailed comparisons ) .
the proposed method is also fast because it requires only matrix vector products and convolution operations .
in our timing experiments , at prediction time , the method is as fast as other hand - engineered features .
previous work
in recent years , low - level hand - designed features have been heavily employed with much success .
typical ex - amples of such successful features for static images are sift ( 123 , 123 ) , hog ( 123 ) , gloh ( 123 ) and surf ( 123 ) .
extending the above features to 123d is the predominant methodology in video action recognition .
these methods usually have two stages : an optional feature detection stage followed by an feature description stage .
well - known fea - ture detection methods ( interest point detectors ) are har - ris123d ( 123 ) , cuboids ( 123 ) and hessian ( 123 ) .
for descrip - tors , popular methods are cuboids ( 123 ) , hog / hof ( 123 ) , hog123d ( 123 ) and extended surf ( 123 ) .
some other inter - esting approaches are proposed in ( 123 , 123 ) .
given the cur - rent trends , challenges and interests in action recognition , this list will probably grow rapidly .
in a very recent work , wang et al .
( 123 ) combine var - ious low - level feature detection , feature description meth - ods and benchmark their performance on kth ( 123 ) , ucf sports action ( 123 ) and hollywood123 ( 123 ) datasets .
to make a fair comparison , they employ the same state - of - the - art pro - cessing pipeline with vector quantization , feature normal - ization and 123 - kernel svms .
the only variable factor in the pipeline is the use of different methods for feature de - tection and feature extraction .
one of their most interesting ndings is that there is no universally best hand - engineered feature for all datasets; their nding suggests that learning features directly from the dataset itself may be more advan -
in our paper , we will follow wang et al .
( 123 ) s experi - mental protocols by using their standard processing pipeline and only replacing the rst stage of feature extraction with our method .
by doing this , we can easily understand the contributions of the learned features .
recently , a novel convolutional grbm method ( 123 ) was proposed for learning spatio - temporal features .
this method can be considered an extension of convolutional rbms ( 123 ) to 123d .
in comparison to our method , their learn - ing procedure is more expensive because the objective func - tion is intractable and thus sampling is required .
as a conse - quence , their method takes 123 - 123 days to train with the holly - wood123 dataset ( 123 ) . 123 this is much slower than our method which only takes 123 - 123 hours to train .
our method is therefore more practical for large scale problems .
furthermore , our experimental procedure is different from one proposed by taylor et al .
specically , in taylor et al .
( 123 ) , the authors create a pipeline with novel pooling mechanisms sparse coding , spatial average pooling and temporal max pooling .
the two new factors , learned features coupled with the new pipeline , make it dif - cult to assess the contributions of each stage .
biologically - inspired sparse learning algorithms such
123personal communications with g .
taylor .
as , sparse coding ( 123 ) , independent component analysis ( ica ) ( 123 ) and independent subspace analysis ( 123 ) have long been studied by researchers in the eld of natural image statistics .
there has been a growing interest in applying these methods to learn visual features .
for example , raina et al .
( 123 ) demonstrate that sparse codes learned from un - labeled and unrelated tasks can be very useful for recogni - tion .
they name this approach self - taught learning .
fur - ther , kanan and cottrell ( 123 ) show that ica can be used as a self - taught learning method to generate saliency maps and features for robust recognition .
they demonstrate that their biologically - inspired method can be very competitive in a number of datasets such as caltech , flowers and faces .
in ( 123 ) , tica , another extension of ica , was proposed for static images that achieves state - of - the - art performance on norb ( 123 ) and cifar - 123 ( 123 ) datasets .
biologically - inspired networks ( 123 , 123 , 123 ) have been applied to action recognition tasks .
however , except for the work of ( 123 ) , these methods have certain weaknesses such as using hand - crafted features or requiring much la - beled data .
for instance , all features in jhuang et al .
( 123 ) are carefully hand - crafted .
similarly , features in the rst layer of ( 123 ) are also heavily hand - tuned; higher layer features are adjusted by backpropagation which requires a large amount of labeled data ( see the conclusion section in ( 123 ) ) .
in con - trast , our features are learned in a purely unsupervised man - ner and thus can leverage the plethora of unlabeled data .
algorithms and invariant properties
in this section , we will rst describe the basic indepen - dent subspace analysis algorithm which is often used to learn features from static images .
next , we will explain how to scale this algorithm to larger images using convolu - tion and stacking and learn hierarchical representations .
also , in this section , we will discuss batch projected gra - dient descent .
finally , we will present a technique to detect interest points in videos .
independent subspace analysis for static images
isa is an unsupervised learning algorithm that learns features from unlabeled image patches .
an isa network ( 123 ) can be described as a two - layered network ( figure 123 ) , with square and square - root nonlinearities in the rst and second layers respectively .
the weights w in the rst layer are learned , and the weights v of the second layer are xed to represent the subspace structure of the neurons in the rst layer .
specically , each of the second layer hidden units pools over a small neighborhood of adjacent rst layer units .
we will call the rst and second layer units simple and pool - ing units , respectively .
tivation of each second layer unit
more precisely , given an input pattern xt ,
is pi ( xt; w , v ) = j ) 123
isa learns parameters w through nding sparse feature representations in the second
figure 123
the neural network architecture of an isa network .
the red bubbles are the pooling units whereas the green bubbles are the simple units .
in this picture , the size of the subspace is 123 : each red pooling unit looks at 123 simple units .
layer , by solving :
figure 123
tuning curves for isa pooling units when trained on static images .
the x - axes are variations in transla - tion / frequency / rotation , the y - axes are the normalized activations of the network .
left : change in translation ( phase ) .
middle : change in frequency .
right : change in rotation .
these three plots show that pooling units in an isa network are robust to translation and selective to frequency and rotation changes .
i=123 pi ( xt; w , v ) ,
subject to w w t = i
t=123 are whitened input examples . 123 here , w rkn is the weights connecting the input data to the simple units , v rmk is the weights connecting the simple units to the pooling units ( v is typically xed ) ; n , k , m are the input dimension , number of simple units and pooling units respectively .
the orthonormal constraint is to ensure the features are diverse .
in figure 123 , we show three pairs of lters learned from natural images .
as can be seen from this gure , the isa algorithm is able to learn gabor lters ( edge detectors ) with many frequencies and orientations .
further , it is also able to group similar features in a group thereby achieving
figure 123
typical lters learned by the isa algorithm when trained on static images .
here , we visualize three groups of bases pro - duced by w ( each group is a subspace and pooled together ) .
one property of the learned isa pooling units is that they are invariant and thus suitable for recognition tasks .
to il - lustrate this , we train the isa algorithm on natural static images and then test its invariance properties using the tun - ing curve test ( 123 ) .
in detail , we nd the optimal stimulus of a particular neuron pi in the network by tting a parametric gabor function to the lter .
we then vary its three degrees of freedom : translation ( phase ) , rotation and frequency and plot the activations of the neurons in the network with re - spect to the variation .
123 figure 123 shows results of the tuning curve test for a randomly selected neuron in the network with respect to spatial variations .
as can be seen from this gure , the neuron is robust to translation ( phase ) while be - ing more sensitive to frequency and rotation .
this combi - nation of robustness and selectivity makes features learned by isa highly invariant ( 123 ) .
in many experiments , we found that this invariant prop - erty makes isa perform much better than other simpler methods such as ica and sparse coding .
stacked convolutional isa
the standard isa training algorithm becomes less ef - cient when input patches are large .
this is because an or - thogonalization method has to be called at every step of pro - jected gradient descent .
the cost of the orthogonalization step grows as a cubic function of the input dimension ( see section 123 ) .
thus , training this algorithm with high dimen - sional data , especially video data , takes days to complete .
in order to scale up the algorithm to large inputs , we de - sign a convolutional neural network architecture that pro - gressively makes use of pca and isa as sub - units for un - supervised learning as shown in figure 123
the key ideas of this approach are as follows .
we rst train the isa algorithm on small input patches .
we then take this learned network and convolve with a larger region of the input image .
the combined responses of the convo - lution step are then given as input to the next layer which is also implemented by another isa algorithm with pca as a prepossessing step .
similar to the rst layer , we use pca to whiten the data and reduce their dimensions such that the next layer of the isa algorithm only works with low dimen -
in our experiments , the stacked model is trained greedily layerwise in the same manner as other algorithms proposed in the deep learning literature ( 123 , 123 , 123 ) .
more specically , we train layer 123 until convergence before training layer 123
using this idea , the training time requirement is reduced to
learning spatio ( cid : 123 ) temporal features
applying the models above to the video domain is rather the inputs to the network are 123d video blocks instead of image patches .
more specically , we take
123i . e . , the input patterns have been linearly transformed to have zero
123in this test , we use image patches of a typical size 123x123
mean and identity covariance .
w to the constraint set by computing ( w w t ) 123 w .
note that the inverse square root of the matrix usually involves solving an eigenvector problem , which requires cubic time .
therefore , this algorithm is expensive when the input di - mension is large .
the convolution and stacking ideas ad - dress this problem by slowly expanding the receptive elds via convolution .
and although we have to resort to pca for whitening and dimension reduction , this step is called only once and hence much less expensive .
training neural networks is difcult and requires much tuning .
our method , however , is very easy to train because batch gradient descent does not need any tweaking with the learning rate and the convergence criterion .
this is in stark contrast with other methods such as deep belief nets ( 123 ) and stacked autoencoders ( 123 ) where tuning the learning rate , weight decay , convergence parameters , etc .
is essential for learning good features .
norm ( cid : 123 ) thresholding interest point detector
in many datasets , an interest point detector is neces - sary for improving recognition and lowering computational costs .
this can be achieved in our framework by discarding features at locations where the norm of the activations is below a certain threshold .
this is based on the observation that the rst layers activations tend to have signicantly higher norms at edge and motion locations than at static and feature - less locations ( c . f .
hence , by threshold - ing the norm , the rst layer of our network can be used as a robust feature detector that lters out features from the
if kp123 ( xt; w , v ) k123 then the features at xt are ignored .
here p123 is the activations of the rst layer of the net - work .
for instance , setting at 123 percentile of the training sets activation norms means that 123% of features from the dataset are discarded .
in our experiments , we only use this detector the kth dataset where an interest point detector has been shown to be useful ( 123 ) .
the value of is chosen via cross validation .
figure 123
stacked convolutional isa network .
the network is built by copying the learned network and pasting it to different places of the input data and then treating the outputs as inputs to a new isa network .
for clarity , the convolution step is shown here non - overlapping , but in the experiments the convolution is done
a sequence of image patches and atten them into a vector .
this vector becomes input features to the network above .
to learn high - level concepts , we can use the convolution and stacking techniques ( see section 123 ) which result in an architecture as shown in figure 123
figure 123
stacked convolutional isa for video data .
in this gure , convolution is done with overlapping; the isa network in the sec - ond layer is trained on the combined activations of the rst layer .
finally , in our experiments , we combine features from both layers and use them as local features for classication ( previously suggested in ( 123 ) ) .
in the experiment section , we will show that this combination works better than using one set of features alone .
feature visualization and analysis
in section 123 , we discussed spatial invariant properties of isa when applied to image patches .
in this section , we extend the analysis for video bases .
learning with batch projected gradient descent
first layer
our method is trained by batch projected gradient de - scent .
compared to other feature learning methods ( e . g . , rbms ( 123 ) ) , the gradient of the objective function in eq
the orthonormal constraint is ensured by projection with symmetric orthogonalization ( 123 ) .
in detail , during opti - mization , projected gradient descent requires us to project
the rst layer of our model learns features that detect a moving edge in time as shown in figure 123
in addition to previously mentioned spatial invariances , these spatio - temporal bases give rise to another property : velocity selec -
we analyze this property by computing the response of isa features while varying the velocity of the moving edge .
figure 123
examples of three isa features learned from holly - wood123 data ( 123x123 spatial size ) .
in this picture , each row consists of two sets of lters .
each set of lters is a lter in 123d ( i . e . , a row in matrix w ) , and two sets grouped together to form an isa
in detail , we t gabor functions to all temporal bases to estimate the velocity of the bases .
we then vary this veloc - ity and plot the response of the features with respect to the changes .
in figure 123 , we visualize this property by plotting the velocity tuning curves of ve randomly - selected units in the rst layer of the network .
figure 123
velocity tuning curves of ve neurons in a isa network trained on hollywood123
most of the tuning curves are unimodal and this means that isa temporal bases can be used as velocity
as can be seen from the gure , the neurons are highly sensitive to changes in the velocity of the stimuli .
this suggests that the features can be used as velocity detec - tors which are valuable for detecting actions in movies .
for example , the running category in hollywood123 has fast motions whereas the eating category in hollywood123 has
informally , we can interpret lters learned with our isa model as features detecting a moving edge through time .
in particular , the pooling units are sensitive to motion how fast the edge moves and also sensitive to orientation but less sensitive to ( translational ) locations of the edge .
we found that the ability to detect accurate velocities is very important for good recognition .
in a control exper - iment , we limit this ability by using a temporal size of 123 frames instead of 123 frames and the recognition rate drops by 123% for the hollywood123 dataset .
not only can the bases detect velocity , they also adapt to the statistics of the dataset .
this ability is shown in fig - ure 123
as can be seen from the gure , for hollywood123 , the algorithm learns that there should be more edge detectors in vertical and horizontal orientations than other orientations .
informally , we can interpret that the bases spend more ef - fort to detect velocity changes in the horizontal and vertical
directions than other directions .
figure 123
a polar plot of edge velocities ( radius ) and orientations ( angle ) to which lters give maximum response .
each red dot in the gure represents a pair of ( velocity , orientation ) for a spatio - temporal lter learned from hollywood123
the outermost circle has velocity of 123 pixels per frame .
higher layers
figure 123
visualization of ve typical optimal stimuli in the second layer learned from hollywood123 data ( for the purpose of better vi - sualization , we use the size of 123x123x123 built on top of 123x123x123 rst layer lters ) .
compare this gure with figure 123
figure 123
comparison of layer 123 lters ( left ) and layer 123 lters ( right ) learned from hollywood123
for ease of visualization , we ignore the temporal dimension and only visualize the middle lter .
visualizing and analyzing higher layer units are usually difcult .
here , we follow ( 123 ) and visualize the optimal stimuli of the higher layer neurons . 123 some typical optimal stimuli for second layer neurons are shown in figure 123 and
123in detail , the method was presented for visualizing optimal stimuli of neurons in a quadratic network for which the corresponding optimization problem has an analytical solution .
as our network is not quadratic , we have to solve an optimization problem subject to a norm bound constraint of the input .
we implement this with minconf ( 123 ) .
figure 123
although the learned features are more difcult to interpret , the visualization suggests they have complex shapes ( e . g . , corners ( 123 ) ) and invariances suitable for de - tecting high - level structures .
in this section we will numerically compare our algo - rithm against the current state - of - the - art action recognition algorithms .
we would like to emphasize that for our method we use an identical pipeline as described in ( 123 ) .
this pipeline extracts local features , then performs vector quan - tization by k - means and classies by 123 kernel .
with our method , the only change is the feature extraction stage : we replaced hand - designed features with the learned fea - tures .
results of control experiments such as speed , ben - ets of the second layer and training features on unrelated data ( 123 ) are also reported .
further results , detailed com - parisons and parameter settings can be seen in the appendix
we evaluate our algorithm on four well - known bench - mark action recognition datasets : kth ( 123 ) , ucf sport actions ( 123 ) , hollywood123 ( 123 ) and youtube action ( 123 ) .
these datasets were obtained from original authors web - sites .
the processing steps , dataset splits and metrics are identical to those described in ( 123 ) or ( 123 ) .
the main pur - pose of using identical protocols is to identify the contribu - tions of the learned features .
details of our model
for our model , the inputs to the rst layer are of size 123x123 ( spatial ) and 123 ( temporal ) .
our rst layer isa net - work learns 123 features ( i . e . , there are 123 red nodes in figure 123 ) .
the inputs to the second layer are of size 123x123 ( spatial ) and 123 ( temporal ) .
our second layer isa network learns 123 features ( i . e . , there are 123 red nodes in the last layer in figure 123 ) .
finally , we train the features on 123 video blocks sampled from the training set of each dataset .
we report the performance of our method on the kth dataset in table 123
in this table , we compare our test set accuracy against best reported results in literature .
more detailed results can be seen in ( 123 ) or ( 123 ) .
we note that for this dataset , an interest point detector can be very useful because the background does not convey any meaningful in - formation ( 123 ) .
therefore , we apply our norm - thresholding interest point detector to this dataset ( see section 123 ) .
us - ing this technique , our method achieves superior perfor - mance compared to all published results in the literature .
there is an increase in performance between our method
( 123% ) and the closest competitive method ( 123% ) . 123
table 123
average accuracy on the kth dataset .
the symbol ( ** ) indicates that the method uses an interest point detector .
our method is the best with the norm - thresholding interest point de -
( ** ) harris123d ( 123 ) + hog / hof ( 123 ) ( from ( 123 ) ) ( ** ) harris123d ( 123 ) + hof ( 123 ) ( from ( 123 ) ) ( ** ) cuboids ( 123 ) + hog123d ( 123 ) ( from ( 123 ) )
dense + hof ( 123 ) ( from ( 123 ) )
( ** ) hessian ( 123 ) + esurf ( 123 ) ( from ( 123 ) )
123d cnn ( 123 )
( ** ) plsa ( 123 )
our method with dense sampling
( ** ) our method with norm - thresholding
table 123
mean ap on the hollywood123 dataset .
harris123d ( 123 ) + hog / hof ( 123 ) ( from ( 123 ) ) cuboids ( 123 ) + hog / hof ( 123 ) ( from ( 123 ) ) hessian ( 123 ) + hog / hof ( 123 ) ( from ( 123 ) ) hessian ( 123 ) + esurf ( 123 ) ( from ( 123 ) ) dense + hog / hof ( 123 ) ( from ( 123 ) ) dense + hog123d ( 123 ) ( from ( 123 ) )
table 123
average accuracy on the ucf sport actions dataset .
harris123d ( 123 ) + hog / hof ( 123 ) ( from ( 123 ) ) cuboids ( 123 ) + hog123d ( 123 ) ( from ( 123 ) ) hessian ( 123 ) + hog / hof ( 123 ) ( from ( 123 ) ) hessian ( 123 ) + esurf ( 123 ) ( from ( 123 ) ) dense + hof ( 123 ) ( from ( 123 ) ) dense + hog123d ( 123 ) ( from ( 123 ) )
table 123
average accuracy on the youtube action dataset .
feature combining and pruning ( 123 ) : - static features :
har + hes + mser ( 123 ) + sift ( 123 )
- motion features :
harris123d ( 123 ) + gradients + pca + heuristics
a comparison of our method against best published re - sults for hollywood123 and ucf sport actions datasets is reported in table 123 and 123
in these experiments , we only consider dense sampling for our algorithm .
as can be seen from the tables , our approach outperforms a wide range of
123our model achieves 123% if we use the interest point detector to l - ter out the background , then run feature extraction more densely than de - scribed in ( 123 ) .
methods .
the performance improvement , in case of the challenging hollywood123 dataset , is signicant : 123% .
finally , in table 123 , we report the performance of our al - gorithm on the youtube actions dataset ( 123 ) .
the results show that our algorithm outperforms a more complicated method ( 123 ) on the dataset by a margin of 123% .
benets of the second layer
in the above experiments , we combine features from layer 123 and layer 123 for classication .
this raises a question : how much does the second layer help ?
to answer this question , we rerun the experiments with the same settings and discard second layers features .
the results are much worse than previous experiments .
more specically , removing the second layer features results in a signicant drop of 123% , 123% , 123% in terms of accu - racy on kth , ucf and hollywood123 datasets respectively .
this conrms that features from the second layer are indeed very useful for recognition .
training and prediction time
unsupervised feature learning is usually computation - ally expensive , especially in the training phase .
for in - stance , the grbm method , proposed by ( 123 ) , takes around 123 - 123 days to train . 123
in contrast , for the training stage , out algorithm takes 123 - 123 hours to learn the parameters on 123 training examples using the setting in section 123 . 123
feature extraction using our method is very efcient and as fast as hand - designed features .
in the following experiment , we compare the speed of our method and hog123d ( 123 ) during feature extraction . 123 this comparison is obtained by extracting features with dense sampling on 123 video clips with a framesize of 123x123 from the holly -
table 123
feature extraction time .
our method with 123 layers on gpu is 123x faster than hog123d .
our method ( 123 layer ) our method ( 123 layers ) our method ( 123 layers , gpu )
the results show that if we use one layer , our method is faster than hog123d .
but if we use two layers , our algo - rithm is slower than hog123d .
however , as our method is dominated by matrix vector products and convolutions , it can be implemented and executed much more efciently on a gpu .
our simple implementation on a gpu ( gtx 123 ) using jacket123 enjoys a speed - up of 123x over hog123d .
details
of the comparison are given in table 123
self ( cid : 123 ) taught learning
in previous experiments , we trained our features on the given training set .
for instance , in hollywood123 , we trained spatio - temporal features on the training split of 123 videos .
the statistics of the data on which features are trained are similar to statistics of the test data .
an interesting question to consider , is how the model performs when the unsupervised learning stage is carried out on unrelated video data , for instance , videos down - loaded from the internet .
this is the self - taught learn - ing paradigm ( 123 ) .
to answer this question , we trained the convolutional isa network on small video blocks ran - domly sampled from ucf and youtube datasets .
using the learned model , we extract features from hollywood123 video clips and run the same evaluation metric .
under this self - taught setting , the model achieves 123% ap on holly - wood123
while this setting performs less well than learning directly from the training set ( 123% ) , it is still better than prior art results reported in wang et .
al ( 123 ) .
the encouraging result
illustrates the ability of our method to learn useful features for classication using widely - available unlabeled video data .
in this paper , we presented a method that learns features from spatio - temporal data using independent subspace anal - ysis .
we scaled up the algorithm to large receptive elds by convolution and stacking and learn hierarchical representa -
experiments were carried out with kth , hollywood123 , ucf sports action and youtube datasets using a very stan - dard processing pipeline ( 123 ) .
using this pipeline , we ob - served that our simple method outperforms many state - of -
this result is interesting , given that our single method , using the same parameters across four datasets , is consis - tently better than a wide variety of combinations of meth - ods .
it also suggests that learning features directly from data is a very important research direction : not only is this ap - proach more generalizable to many domains , it is also very powerful in recognition tasks .
acknowledgments : we thank zhenghao chen , adam coates , pang wei koh , fei - fei li , jiquan ngiam , juan car - los niebles , andrew saxe , graham taylor for comments and suggestions .
this work was supported by the darpa deep learning program under contract number fa123 - 123 -
123personal communications with g .
taylor .
123the timing experiments are done with a machine with 123ghz cpu
