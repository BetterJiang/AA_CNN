abstract .
a critical issue for the application of markov decision processes ( mdps ) to realistic problems is how the complexity of planning scales with the size of the mdp .
in stochastic environments with very large or innite state spaces , traditional planning and reinforcement learning algorithms may be inapplicable , since their running time typically grows linearly with the state space size in the worst case .
in this paper we present a new algorithm that , given only a generative model ( a natural and common type of simulator ) for an arbitrary mdp , performs on - line , near - optimal planning with a per - state running time that has no dependence on the number of states .
the running time is exponential in the horizon time ( which depends only on the discount factor and the desired degree of approximation to the optimal policy ) .
our algorithm thus provides a different complexity trade - off than classical algorithms such as value iterationrather than scaling linearly in both horizon time and state space size , our running time trades an exponential dependence on the former in exchange for no dependence on the latter .
our algorithm is based on the idea of sparse sampling .
we prove that a randomly sampled look - ahead tree that covers only a vanishing fraction of the full look - ahead tree nevertheless sufces to compute near - optimal actions from any state of an mdp .
practical implementations of the algorithm are discussed , and we draw ties to our related recent results on nding a near - best strategy from a given class of strategies in very large partially observable mdps ( kearns , mansour , & ng .
neural information processing systems 123 , to appear ) .
reinforcement learning , markov decision processes , planning
in the past decade , markov decision processes ( mdps ) and reinforcement learning have become a standard framework for planning and learning under uncertainty within the ar - ticial intelligence literature .
the desire to attack problems of increasing complexity with this formalism has recently led researchers to focus particular attention on the case of ( ex - ponentially or even innitely ) large state spaces .
a number of interesting algorithmic and representational suggestions have been made for coping with such large mdps .
function
this research was conducted while the author was at at&t labs .
kearns , y .
mansour , and a . y
approximation ( sutton & barto , 123 ) is a well - studied approach to learning value func - tions in large state spaces , and many authors have recently begun to study the properties of large mdps that enjoy compact representations , such as mdps in which the state transition probabilities factor into a small number of components ( boutilier , dearden , & goldszmidt , 123; meuleau et al . , 123; koller & parr , 123 ) .
in this paper , we are interested in the problem of computing a near - optimal policy in a large or innite mdp that is giventhat is , we are interested in planning .
it should be clear that as we consider very large mdps , the classical planning assumption that the mdp is given explicitly by tables of rewards and transition probabilities becomes infeasible .
one approach to this representational difculty is to assume that the mdp has some special structure that permits compact representation ( such as the factored transition probabili - ties mentioned above ) , and to design special - purpose planning algorithms that exploit this
here we take a slightly different approach .
we consider a setting in which our planning algorithm is given access to a generative model , or simulator , of the mdp .
informally , this is a black box to which we can give any state - action pair ( s , a ) , and receive in return a randomly sampled next state and reward from the distributions associated with ( s , a ) .
generative models have been used in conjunction with some function approximation schemes ( sutton & barto , 123 ) , and are a natural way in which a large mdp might be specied .
moreover , they are more general than most structured representations , in the sense that many structured representations ( such as factored models ( boutilier , dearden , & goldszmidt , 123; meuleau et al . , 123; koller & parr , 123 ) ) usually provide an efcient way of implementing a generative model .
note also that generative models also provide less information than explicit tables of probabilities , but more information than a single continuous trajectory of experience generated according to some exploration policy , and so we view results obtained via generative models as blurring the distinction between what is typically called planning and learning in mdps .
our main result is a new algorithm that accesses the given generative model to perform near - optimal planning in an on - line fashion .
by on - line , we mean that , similar to real - time search methods ( korf , 123; barto , bradtke , & singh , 123; koenig & simmons , 123 ) , our algorithms computation at any time is focused on computing an actions for a single current state , and planning is interleaved with taking actions .
more precisely , given any state s , the algorithm uses the generative model to draw samples for many state - action pairs , and uses these samples to compute a near - optimal action from s , which is then executed .
the amount of time required to compute a near - optimal action from any particular state s has no dependence on the number of states in the mdp , even though the next - state distributions from s may be very diffuse ( that is , have large support ) .
the key to our analysis is in showing that appropriate sparse sampling sufces to construct enough information about the environment near s to compute a near - optimal action .
the analysis relies on a combination of bellman equation calculations , which are standard in reinforcement learning , and uniform convergence arguments , which are standard in supervised learning; this combination of techniques was rst applied in kearns and singh ( 123 ) .
as mentioned , the running time required at each state does have an exponential dependence on the horizon time , which we show to be unavoidable without further assumptions .
however , our results leave open the
a sparse sampling algorithm
possiblity of an algorithm that runs in time polynomial in the accuracy parameter , which remains an important open problem .
note that one can view our planning algorithm as simply implementing a ( stochastic ) policya policy that happens to use a generative model as a subroutine .
in this sense , if we view the generative model as providing a compact representation of the mdp , our algorithm provides a correspondingly compact representation of a near - optimal policy .
we view our result as complementary to work that proposes and exploits particular compact representations of mdps ( meuleau et al . , 123 ) , with both lines of work beginning to demonstrate the potential feasibility of planning and learning in very large environments .
the remainder of this paper is structured as follows : in section 123 , we give the formal denitions needed in this paper .
section 123 then gives our main result , an algorithm for planning in large or innite mdps , whose per - state running time does not depend on the size of the state space .
finally , section 123 describes related results and open problems .
we begin with the denition of a markov decision process on a set of n =|s| states , explicitly allowing the possibility of the number of states being ( countably or uncountably )
denition 123
a markov decision process m on a set of states s and with actions ( a123 , .
, ak ) consists of : transition probabilities : for each state - action pair ( s , a ) , a next - state distribution psa ( s that species the probability of transition to each state s upon execution of action a from reward distributions : for each state - action pair ( s , a ) , a distribution rsa on real - valued rewards for executing action a from state s .
we assume rewards are bounded in absolute value by rmax .
for simplicity , we shall assume in this paper that all rewards are in fact deterministicthat is , the reward distributions have zero variance , and thus the reward received for executing a from s is always exactly rsa .
however , all of our results have easy generalizations for the case of stochastic rewards , with an appropriate and necessary dependence on the variance of the reward distributions .
throughout the paper , we will primarily be interested in mdps with a very large ( or even innite ) number of states , thus precluding approaches that compute directly on the full next - state distributions .
instead , we will assume that our planning algorithms are given m in the form of the ability to sample the behavior of m .
thus , the model given is simulative rather than explicit .
we call this ability to sample the behavior of m a generative model .
denition 123
a generative model for a markov decision process m is a randomized algorithm that , on input of a state - action pair ( s , a ) , outputs rsa and a state s randomly drawn according to the transition probabilities psa ( ) .
, where s
kearns , y .
mansour , and a . y
we think of a generative model as falling somewhere in between being given explicit next - state distributions , and being given only irreversible experience in the mdp ( in which the agent follows a single , continuous trajectory , with no ability to reset to any desired state ) .
on the one hand , a generative model may often be available when explicit next - state distributions are not; on the other , a generative model obviates the important issue of exploration that arises in a setting where we only have irreversible experience .
in this sense , planning results using generative models blur the distinction between what is typically called planning and what is typically called learning .
following standard terminology , we dene a ( stochastic ) policy to be any mapping : s ( cid : 123 ) ( a123 , .
thus ( s ) may be a random variable , but depends only on the current state s .
we will be primarily concerned with discounted mdps , 123 so we assume we are given a number 123 < 123 called the discount factor , with which we then dene the value function v for any policy :
v ( s ) = e
where ri is the reward received on the ith step of executing the policy from state s , and the expectation is over the transition probabilities and any randomization in .
note that for any s and any , |v ( s ) | vmax , where we dene vmax = rmax / ( 123 ) .
we also dene the q - function for a given policy as
q ( s , a ) = rsa + es ( cid : 123 ) psa ( ) ( v ( s ( cid : 123 ) psa ( ) means that s
is drawn according to the distribution psa ( ) ) .
( where the notation s we will later describe an algorithm a that takes as input any state s and ( stochastically ) outputs an action a , and which therefore implements a policy .
when we have such an algorithm , we will also write v to denote the value function and q - function of the policy implemented by a .
finally , we dene the optimal value function and the optimal ( s , a ) = sup q ( s , a ) , and the optimal policy q - function as v
( s ) = sup v ( s ) and q ( s , a ) for all s s .
, ( s ) = arg maxa q
planning in large or innite mdps
usually , one considers the planning problem in mdps to be that of computing a good policy , given as input the transition probabilities psa ( ) and the rewards rsa ( for instance , by solving the mdp for the optimal policy ) .
thus , the input is a complete and exact model , and the output is a total mapping from states to actions .
without additional assumptions about the structure of the mdp , such an approach is clearly infeasible in very large state spaces , where even reading all of the input can take n 123 time , and even specifying a general policy requires space on the order of n .
in such mdps , a more fruitful way of thinking about planning might be an on - line view , in which we examine the per - state complexity of planning .
thus , the input to a planning algorithm would be a single state , and the output would be which
a sparse sampling algorithm
single action to take from that state .
in this on - line view , a planning algorithm is itself simply a policy ( but one that may need to perform some nontrivial computation at each state ) .
our main result is the description and analysis of an algorithm a that , given access to a generative model for an arbitrary mdp m , takes any state of m as input and produces an action as output , and meets the following performance criteria : the policy implemented by a is near - optimal in m; the running time of a ( that is , the time required to compute an action at any state ) has
no dependence on the number of states of m .
this result is obtained under the assumption that there is an o ( 123 ) time and space way to refer to the states , a standard assumption known as the uniform cost model ( aho , hopcroft , & ullman , 123 ) , that is typically adopted to allow analysis of algorithms that operate on real numbers ( such as we require to allow innite state spaces ) .
the uniform cost model essentially posits the availability of innite - precision registers ( and constant - size circuitry for performing the basic arithmetic operations on these registers ) .
if one is unhappy with this model , then algorithm a will suffer a dependence on the number of states only equal to the space required to name the states ( at worst log ( n ) for n states ) .
a sparse sampling planner
here is our main result : theorem 123
there is a randomized algorithm a that , given access to a generative model for any k - action mdp m , takes as input any state s s and any value > 123 , outputs an action , and satises the following two conditions : ( efciency ) the running time of a is o ( ( kc ) h ) , where
h = ( cid : 123 ) log ( / vmax ) ( cid : 123 ) , c = v 123 = ( ( 123 ) 123 ) / 123 , vmax = rmax / ( 123 ) .
in particular , the running time depends only on rmax , , and , and does not depend on n =|s| .
if we view rmax as a constant , the running time bound can also be written
( near - optimality ) the value function of the stochastic policy implemented by a satises
simultaneously for all states s s .
kearns , y .
mansour , and a . y
as we have already suggested , it will be helpful to think of algorithm a in two different ways .
on the one hand , a is an algorithm that takes a state as input and has access to a generative model , and as such we shall be interested in its resource complexityits running time , and the number of calls it needs to make to the generative model ( both per state input ) .
on the other hand , a produces an action as output in response to each state given as input , and thus implements a ( possibly stochastic ) policy .
the proof of theorem 123 is given in appendix a , and detailed pseudo - code for the algorithm is provided in gure 123
we now give some high - level intuition for the algorithm and its analysis .
given as input a state s , the algorithm must use the generative model to nd a near - optimal action to perform from state s .
the basic idea of the algorithm is to sample the generative model from states in the neighborhood of s .
this allows us to construct a small from s is a near - optimal action from s in m . 123 there will be no guarantee that m will contain enough information to compute a good action from any state other than s .
however , in exchange for this limited applicability , the mdp m will have a number of states that does not depend on the number of states
of m such that the optimal action in m
figure 123
algorithm a for planning in large or innite state spaces .
estimatev nds the v text , and estimateq nds analogously dened q
algorithm a implements the policy .
h described in the
a sparse sampling algorithm
the graphical structure of m
will be given by a directed tree in which each node is labeled by a state , and each directed edge to a child is labeled by an action and a reward .
for the sake of simplicity , let us consider only the two - action case here , with actions a123 and a123
each node will have c children in which the edge to the child is labeled a123 , and c children in which the edge to the child is labeled a123
the root node of m
is labeled by the state of interest s , and we generate the 123c children of s in the obvious way : we call the generative model c times on the state - action pair ( s , a123 ) to get the a123 - children , and on c times on ( s , a123 ) to get the a123 - children .
the edges to these children are also labeled by the rewards returned by the generative model , and the child nodes themselves are labeled by the states returned .
we will build this ( 123c ) - ary tree to some depth to be determined .
note that m
is essentially a sparse look - ahead tree .
we can also think of m
as an mdp in which the start state is s , and in which taking an action from a node in the tree causes a transition to a ( uniformly ) random child of that node with the corresponding action label; the childless leaf nodes are considered absorbing states .
under this interpretation , we can compute the optimal action to take from the root s .
figure 123 shows a conceptual picture of this tree for a run of the algorithm from an input state s123 , for c = 123
( c will typically be much larger ) .
from the root s123 , we try action a123 three times and action a123 three times .
from each of the resulting states , we also try each action c times , and so on down to depth h in the tree .
zero values assigned to the leaves then correspond to our estimates of v
their parents , which are in turn backed - up to their parents , and so on , up to the root to nd an estimate of v the central claim we establish about m
123 , which are backed - up to nd estimates of v
is that its size can be independent of the number of states in m , yet still result in our choosing near - optimal actions at the root .
we do this by establishing bounds on the required depth h of the tree and the required degree c .
recall that the optimal policy at s is given by ( s ) = arg maxa q ( s , a ) , and therefore ( s , ) .
estimating the q - values is completely determined by , and easily calculated from , q
figure 123
sparse look - ahead tree of states constructed by the algorithm ( shown with c = 123 , actions a123 , a123 ) .
kearns , y .
mansour , and a . y
is a common way of planning in mdps .
from the standard duality between q - functions and value functions , the task of estimating q - functions is very similar to that of estimating value functions .
so while the algorithm uses the q - function , we will , purely for expository purposes , actually describe here how we estimate v
there are two parts to the approximation we use .
first , rather than estimating v
will actually estimate , for a value of h to be specied later , the h - step expected discounted
( s ) , given by
( s ) = e
where ri is the reward received on the ith time step upon executing the optimal policy
from s .
moreover , we see that the v ( s ) = rsa + es ( cid : 123 ) psa ( ) ( v
( s ) , for h 123 , are recursively given by
( rsa + es ( cid : 123 ) psa ( ) ( v
is the action taken by the optimal policy from state s , and v
( s ) = 123
the quality of the approximation in eq .
( 123 ) becomes better for larger values of h , and is controllably tight for the largest value h = h we eventually choose .
one of the main efforts in the proof is establishing that the error incurred by the recursive application of this approximation can be made controllably small by choosing h sufciently large .
thus , if we are able to obtain an estimate v
( cid : 123 ) ) of v , we can inductively dene an algorithm for nding an estimate v
( s ) by making use of eq .
our ( s ) of v algorithm will approximate the expectation in eq .
( 123 ) by a sample of c random next states from the generative model , where c is a parameter to be determined ( and which , for reasons that will become clear later , we call the width ) .
recursively , given a way of nding the
, we nd our estimate v
( cid : 123 ) ) for any s
( cid : 123 ) ) for any s
( s ) as follows :
( s ) of v
independently sampled states from the next - state distribution psa ( ) .
for each action a , use the generative model to get rsa and to sample a set sa of c 123
use our procedure for nding v
( cid : 123 ) ) for each state s
in any of the
following eq .
( 123 ) , our estimate of v
( s ) is then given by
( s ) = max
h123 to estimate v
to complete the description of the algorithm , all that remains is to choose the depth h and the parameter c , which controls the width of the tree .
bounding the required depth h is the easy and standard part .
it is not hard to see that if we choose depth h = log ( 123 ) / rmax ( the so - called - horizon time ) , then the discounted sum of the rewards that is obtained by considering rewards beyond this horizon is bounded by .
a sparse sampling algorithm
the central claim we establish about c is that it can be chosen independent of the number of states in m , yet still result in choosing near - optimal actions at the root .
the key to the argument is that even though small samples may give very poor approximations to the next - state distribution at each state in the tree , they will , nevertheless , give good estimates of the expectation terms of eq .
( 123 ) , and that is really all we need .
for this we apply a careful combination of uniform convergence methods and inductive arguments on the tree depth .
again , the technical details of the proof are in appendix a .
in general , the resulting tree may represent only a vanishing fraction of all of the h - step paths starting from s123 that have non - zero probability in the mdpthat is , the sparse look - ahead tree covers only a vanishing part of the full look - ahead tree .
in this sense , our algorithm is clearly related to and inspired by classical look - ahead search techniques ( russell & norvig , 123 ) including various real - time search algorithms ( korf , 123; barto , bradtke , & singh , 123; bonet , loerincs , & geffner , 123; koenig & simmons , 123 ) and receding horizon controllers .
most of these classical search algorithms , however , run into difculties in very large or innite mdps with diffuse transitions , since their search trees can have arbitrarily large ( or even innite ) branching factors .
our main contribution is showing that in large stochastic environments , clever random sampling sufces to re - construct nearly all of the information available in the ( exponentially or innitely ) large full look - ahead tree .
note that in the case of deterministic environments , where from each state - action pair we can reach only a single next state , the sparse and full trees coincide ( assuming a memoization trick described below ) , and our algorithm reduces to classical deterministic look - ahead search .
practical issues and lower bounds even though the running time of algorithm a does not depend on the size of the mdp , it still runs in time exponential in the - horizon time h , and therefore exponential in 123 / ( 123 ) .
it would seem that the algorithm would be practical only if is not too close to 123
in a moment , we will give a lower bound showing it is not possible to do much better without further assumptions on the mdp .
nevertheless , there are a couple of simple tricks that may help to reduce the running time in certain cases , and we describe these tricks rst .
the rst idea is to allow different amounts of sampling at each level of the tree .
the intuition is that the further we are from the root , the less inuence our estimates will have on the q - values at the root ( due to the discounting ) .
thus , we can sample more sparsely at deeper levels of the tree without having too adverse an impact on our approximation .
we have analyzed various schemes for letting the amount of sampling at a node depend on its depth .
none of the methods we investigated result in a running time which is polynomial in 123 / .
however , one specic scheme that reduces the running time signicantly is to let the number of samples per action at depth i be ci = 123i c , where the parameter c now controls the amount of sampling done at the root .
the error in the q - values using such a scheme does not increase by much , and the running time is the square root of our original running time .
beyond this and analogous to how classical search trees can often be pruned in ways that signicantly reduce running time , a number of standard tree pruning methods may also be applied to our algorithms trees ( russell & norvig , 123 ) ( see also dearden
kearns , y .
mansour , and a . y
& boutilier , 123 ) , and we anticipate that this may signicantly speed up the algorithm in another way in which signicant savings might be achieved is through the use of memo - ization in our subroutines for calculating the v ( s ) s .
in gure 123 , this means that whenever there are two nodes at the same level of the tree that correspond to the same state , we collapse them into one node ( keeping just one of their subtrees ) .
while it is straightforward to show the correctness of such memoization procedures for deterministic procedures , one must be careful when addressing randomized procedures .
we can show that the important proper - ties of our algorithm are maintained under this optimization .
indeed , this optimization is particularly nice when the domain is actually deterministic : if each action deterministically causes a transition to a xed next - state , then the tree would grow only as k h ( where k is the number of actions ) .
if the domain is nearly deterministic , then we have behavior somewhere in between .
similarly , if there are only some n123 ( cid : 123 ) |s| states reachable from s123 , then the tree would also never grow wider than n123 , giving it a size of o ( n123 h ) .
in implementing the algorithm , one may wish not to specify a targeted accuracy in advance , but rather to try to do as well as is possible with the computational resources available .
in this case , an iterative - deepening approach may be taken .
this would entail simultaneously increasing c and h by decreasing the target .
also , as studied in davies , ng , and moore ( 123 ) , if we have access to an initial estimate of the value function , we can replace our estimates v ( s ) = 123 at the leaves with the estimated value function at those states .
though we shall not do so here , it is again easy to make formal performance guarantees depending on c , h and the supremum error of the value function estimate we are using .
unfortunately , despite these tricks , it is not difcult to prove a lower bound that shows that any planning algorithm with access only to a generative model , and which implements a policy that is - close to optimal in a general mdp , must have running time at least exponential in the - horizon time .
we now describe this lower bound .
theorem 123
let a be any algorithm that is given access only to a generative model for an mdp m , and inputs s ( a state in m ) and .
let the stochastic policy implemented by a
simultaneously for all states s s .
then there exists an mdp m on which a makes at least ( 123h ) = ( ( 123 / ) ( 123 / log ( 123 / ) ) ) calls to the generative model .
proof : let h = log = log ( 123 / ) / log ( 123 / ) .
consider a binary tree t of depth h .
we use t to dene an mdp in the following way .
the states of the mdp are the nodes of the tree .
the actions of the mdp are ( 123 , 123 ) .
when we are in state s and perform an action b we reach ( deterministically ) state sb , where sb is the b - child of s in t .
if s is a leaf of t then we move to an absorbing state .
we choose a random leaf v in the tree .
the reward function for v and any action is rmax , and the reward at any other state and action is zero .
algorithm a is given s123 , the root of t .
for algorithm a to compute a near optimal policy , it has to nd the node v , and therefore has to perform at least ( 123h ) calls to the generative
a sparse sampling algorithm
summary and related work
we have described an algorithm for near - optimal planning from a generative model , that has a per - state running time that does not depend on the size of the state space , but which is still exponential in the - horizon time .
an important open problem is to close the gap between our lower and upper bound .
our lower bound shows that the number of steps has to grow polyno - mially in 123 / while in the upper bound the number of steps grows sub - exponentially in 123 / , more precisely ( 123 / ) o ( log ( 123 / ) ) .
closing this gap , either by giving an algorithm that would be polynomial in 123 / or by proving a better lower bound , is an interesting open problem .
two interesting directions for improvement are to allow partially observable mdps ( pomdps ) , and to nd more efcient algorithms that do not have exponential dependence on the horizon time .
as a rst step towards both of these goals , in a separate paper ( kearns , mansour , & ng , to appear ) we investigate a framework in which the goal is to use a gener - ative model to nd a near - best strategy within a restricted class of strategies for a pomdp .
typical examples of such restricted strategy classes include limited - memory strategies in pomdps , or policies in large mdps that implement a linear mapping from state vectors to actions .
our main result in this framework says that as long as the restricted class of strategies is not too complex ( where this is formalized using appropriate generalizations of standard notions like vc dimension from supervised learning ) , then it is possible to nd a near - best strategy from within the class , in time that again has no dependence on the size of the state space .
if the restricted class of strategies is smoothly parameterized , then this further leads to a number of fast , practical algorithms for doing gradient descent to nd the near - best strategy within the class , where the running time of each gradient descent step now has only linear rather than exponential dependence on the horizon time .
another approach to planning in pomdps that is based on the algorithm presented here is investigated by mcallester and singh ( 123 ) , who show how the approximate belief - state tracking methods of boyen and koller ( 123 ) can be combined with our algorithm .
appendix a : proof sketch of theorem 123
in this appendix , we give the proof of theorem 123
theorem 123
there is a randomized algorithm a that , given access to a generative model for any k - action mdp m , takes as input any state s s and any value > 123 , outputs an action , and satises the following two conditions : ( efciency ) the running time of a is o ( ( kc ) h ) , where
h = ( cid : 123 ) log ( / vmax ) ( cid : 123 ) , c = v 123 = ( ( 123 ) 123 ) / 123 ,
vmax = rmax / ( 123 ) .
kearns , y .
mansour , and a . y
in particular , the running time depends only on rmax , , and , and does not depend on n =|s| .
if we view rmax as a constant , the running time bound can also be written
( near - optimality ) the value function of the stochastic policy implemented by a satises
simultaneously for all states s s .
throughout the analysis we will rely on the pseudo - code provided for algorithm a given in gure 123
the claim on the running time is immediate from the denition of algorithm a .
each call to estimateq generates kc calls to estimatev , c calls for each action .
each recursive call also reduces the depth parameter h by one , so the depth of the recursion is at most h .
therefore the running time is o ( ( kc ) h ) .
the main effort is in showing that the values of estimateq are indeed good estimates for the chosen values of c and h .
there are two sources of inaccuracy in these estimates .
the rst is that we use only a nite sample to approximate an expectationwe draw only c states from the next - state distributions .
the second source of inaccuracy is that ( ) but rather values in computing estimateq , we are not actually using the values of v returned by estimatev , which are themselves only estimates .
the crucial step in the proof is to show that as h increases , the overall inaccuracy decreases .
let us rst dene an intermediate random variable that will capture the inaccuracy due
to the limited sampling .
dene u
( s , a ) as follows :
( s , a ) = rsa +
where the si are drawn according to psa ( ) .
note that u ( s , a ) is averaging values of ( ) , the unknown value function .
since u ( s , a ) is used only for the proof and not in the algorithm , there is no problem in dening it this way .
the next lemma shows that with high ( s , a ) is at most .
probability , the difference between u lemma 123
for any state s and action a , with probability at least 123 e
( s , a ) and q
max we have
( s , a ) u
( s , a ) | =
where the probability is taken over the draw of the si from psa ( ) .
a sparse sampling algorithm
proof : note that q
( s , a ) = rsa + espsa ( ) ( v
the proof is immediate from the
now that we have quantied the error due to nite sampling , we can bound the error ( ) .
we bound this error as from our using values returned by estimatev rather than v ( s , a ) and estimatev .
in order to make our notation simpler , let the difference between u v n ( s ) be the value returned by estimatev ( n , c , , g , s ) , and let qn ( s , a ) be the component in the output of estimateq ( n , c , , g , s ) that corresponds to action a .
using this notation , our algorithm computes
qn ( s , a ) = rsa +
v n123 ( si )
where v n123 ( s ) = maxa ( qn123 ( s , a ) ) , and q123 ( s , a ) = 123 for every state s and action a .
we now dene a parameter n that will eventually bound the difference between q
and qn ( s , a ) .
we dene n recursively :
where 123 = vmax .
solving for h we obtain
n+123 = ( + n )
+ h vmax
+ h vmax .
the next lemma bounds the error in the estimation , at level n , by n .
intuitively , the error due to nite sampling contributes , while the errors in estimation contribute n .
the combined error is + n , but since we are discounting , the effective error is only ( + n ) , which by denition is n+123
lemma 123
with probability at least 123 ( kc ) ne
max we have that
( s , a ) qn ( s , a ) | n .
proof : the proof is by induction on n .
it clearly holds for n = 123
( s , a ) qn ( s , a ) | =
v n123 ( si )
v n123 ( si )
( si ) 123
( + n ) = n+123
kearns , y .
mansour , and a . y
we require that all of the c child estimates be good , for each of the k actions .
this means that the probability of a bad estimate increases by a factor of kc , for each n .
by lemma 123 the probability of a single bad estimate is bounded by e max .
therefore the probability of some bad estimate is bounded by 123 ( kc ) ne
from h h vmax+ / ( 123 ) , we also see that for h = log ( / vmax ) , with probability 123 ( kc ) h e max all the nal estimates q h ( s123 , a ) are within 123 / ( 123 ) from the true q - values .
the next step is to choose c such that = / rmax ( kc ) h e bound the probability of a bad estimate during the entire computation .
specically ,
c = v 123
is sufcient to ensure that with probability 123 all the estimates are accurate .
at this point we have shown that with high probability , algorithm a computes a good ( s123 , a ) for all a , where s123 is the input state .
to complete the proof , we need estimate of q to relate this to the expected value of a stochastic policy .
we give a fairly general result about mdps , which does not depend on our specic algorithm .
( a similar result appears in singh & yee , 123 ) .
lemma 123
assume that is a stochastic policy , so that ( s ) is a random variable .
if for ( s , ( s ) ) < is at least 123 , then the each state s , the probability that q discounted innite horizon return of is at most ( + 123vmax ) / ( 123 ) from the optimal return , i . e . , for any state sv
( s ) v ( s ) ( + 123vmax ) / ( 123 ) .
( s , ( s ) ) q
proof : since we assume that the rewards are bounded by rmax , it implies that the expected return of at each state s is at least ( s , ( s ) ) ) ( 123 ) ( q
( s , ( s ) ) ) vmax
( s , ( s ) ) 123vmax .
( s , ( s ) ) is at most , then v
now we show that if has the property that at each state s the difference between ( s ) v ( s ) / ( 123 ) .
( a similar ( s , ( s ) ) ) and q result was proved by singh and yee ( 123 ) , for the case that each action chosen has ( s , ( s ) ) q ( s , ( s ) ) .
it is easy to extend their proof to handle the case here , and we sketch a proof only for completeness ) .
values immediately implies |e ( r ( s , ( s ) ) ) e ( r ( s , ( s ) ) ) | the assumption on the q .
consider a policy j that executes for the rst j + 123 steps and then executes i .
this we can show by induction on j that for every state s , v i = / ( 123 ) .
implies that v by setting = + 123vmax the lemma follows .
( s ) v ( s ) ( cid : 123 )
( s ) v j ( s ) ( cid : 123 ) j
now we can combine all the lemmas to prove our main theorem .
a sparse sampling algorithm
proof of theorem 123 : as discussed before , the running time is immediate from the algorithm , and the main work is showing that we compute a near - optimal policy .
by lemma 123 we have that the error in the estimation of q is at most h , with probability 123 ( kc ) h e max .
using the values we chose for c and h we have that with probability 123 the error is at most 123 / ( 123 ) .
by lemma 123 this implies that such a policy has the property that from every state s ,
( s ) v ( s )
substituting back the values of = / rmax and = ( 123 ) 123 / 123 that we had chosen , it
( s ) v ( s ) 123
we give warm thanks to satinder singh for many enlightening discussions and numerous insights on the ideas presented here .
henceforth , everything that needs to be measurable is assumed to be measurable .
however , our results can be generalized to the undiscounted nite - horizon case for any xed horizon h
( mcallester & sing , 123 ) .
will not literally be a sub - mdp of m , in the sense of being strictly embedded in m , due to the variations
of random sampling .
but it will be very near such an embedded mdp .
