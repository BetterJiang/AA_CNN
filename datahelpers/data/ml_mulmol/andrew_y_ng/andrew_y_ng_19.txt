andrew y
sparse coding provides a class of algorithms for nding succinct representations of stimuli; given only unlabeled input data , it discovers basis functions that cap - ture higher - level features in the data .
however , nding sparse codes remains a very difcult computational problem .
in this paper , we present efcient sparse coding algorithms that are based on iteratively solving two convex optimization problems : an l123 - regularized least squares problem and an l123 - constrained least squares problem .
we propose novel algorithms to solve both of these optimiza - tion problems .
our algorithms result in a signicant speedup for sparse coding , allowing us to learn larger sparse codes than possible with previously described algorithms .
we apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end - stopping and non - classical receptive eld sur - round suppression and , therefore , may provide a partial explanation for these two phenomena in v123 neurons .
sparse coding provides a class of algorithms for nding succinct representations of stimuli; given only unlabeled input data , it learns basis functions that capture higher - level features in the data .
when a sparse coding algorithm is applied to natural images , the learned bases resemble the recep - tive elds of neurons in the visual cortex ( 123 , 123 ) ; moreover , sparse coding produces localized bases when applied to other natural stimuli such as speech and video ( 123 , 123 ) .
unlike some other unsuper - vised learning techniques such as pca , sparse coding can be applied to learning overcomplete basis sets , in which the number of bases is greater than the input dimension .
sparse coding can also model inhibition between the bases by sparsifying their activations .
similar properties have been observed in biological neurons , thus making sparse coding a plausible model of the visual cortex ( 123 , 123 ) .
despite the rich promise of sparse coding models , we believe that their development has been ham - pered by their expensive computational cost .
in particular , learning large , highly overcomplete representations has been extremely expensive .
in this paper , we develop a class of efcient sparse coding algorithms that are based on alternating optimization over two subsets of the variables .
the optimization problems over each of the subsets of variables are convex; in particular , the optimiza - tion over the rst subset is an l123 - regularized least squares problem; the optimization over the sec - ond subset of variables is an l123 - constrained least squares problem .
we describe each algorithm and empirically analyze their performance .
our method allows us to efciently learn large over - complete bases from natural images .
we demonstrate that the resulting learned bases exhibit ( i ) end - stopping ( 123 ) and ( ii ) modulation by stimuli outside the classical receptive eld ( ncrf surround suppression ) ( 123 ) .
thus , sparse coding may also provide a partial explanation for these phenomena in v123 neurons .
further , in related work ( 123 ) , we show that the learned succinct representation captures higher - level features that can then be applied to supervised classication tasks .
the goal of sparse coding is to represent input vectors approximately as a weighted linear combi - nation of a small number of ( unknown ) basis vectors .
these basis vectors thus capture high - level patterns in the input data .
concretely , each input vector ~ rk is succinctly represented using basis vectors ~ b123 , .
, ~ bn rk and a sparse vector of weights or coefcients ~ s rn such that ~ bjsj .
the basis set can be overcomplete ( n > k ) , and can thus capture a large number of
patterns in the input data .
the standard generative model assumes that the reconstruction error ~ p
sparse coding is a method for discovering good basis vectors automatically using only unlabeled ~ bjsj is distributed as a zero - mean gaussian distribution with covariance 123i .
to favor sparse coefcients , the prior distribution for each coefcient sj is dened as : p ( sj ) exp ( ( sj ) ) , where ( ) is a sparsity function and is a constant .
for example , we can use one of the following :
j + ) 123 log ( 123 + s123
( l123 penalty function ) ( epsilonl123 penalty function ) ( log penalty function ) .
in this paper , we will use the l123 penalty unless otherwise mentioned; l123 regularization is known to produce sparse coefcients and can be robust to irrelevant features ( 123 ) .
consider a training set of m input vectors ~ ( 123 ) , . . . , ~ ( m ) , and their ( unknown ) corresponding coef - cients ~ s ( 123 ) , . . . , ~ s ( m ) .
the maximum a posteriori estimate of the bases and coefcients , assuming a uniform prior on the bases , is the solution to the following optimization problem : 123
j k123 + pm
k ~ bjk123 c , j = 123 , . . . , n .
this problem can be written more concisely in matrix form : let x rkm be the input matrix ( each column is an input vector ) , let b rkn be the basis matrix ( each column is a basis vector ) , and let s rnm be the coefcient matrix ( each column is a coefcient vector ) .
then , the optimization problem above can be written as :
i , j c , j = 123 , . . . , n .
assuming the use of either l123 penalty or epsilonl123 penalty as the sparsity function , the optimization problem is convex in b ( while holding s xed ) and convex in s ( while holding b xed ) , 123 but not convex in both simultaneously .
in this paper , we iteratively optimize the above objective by alternatingly optimizing with respect to b ( bases ) and s ( coefcients ) while holding the other xed .
for learning the bases b , the optimization problem is a least squares problem with quadratic con - straints .
there are several approaches to solving this problem , such as generic convex optimization solvers ( e . g . , qcqp solver ) as well as gradient descent using iterative projections ( 123 ) .
however , generic convex optimization solvers are too slow to be applicable to this problem , and gradient de - scent using iterative projections often shows slow convergence .
in this paper , we derive and solve the lagrange dual , and show that this approach is much more efcient than gradient - based methods .
for learning the coefcients s , the optimization problem is equivalent to a regularized least squares problem .
for many differentiable sparsity functions , we can use gradient - based methods ( e . g . , con - jugate gradient ) .
however , for the l123 sparsity function , the objective is not continuously differen - tiable and the most straightforward gradient - based methods are difcult to apply .
in this case , the following approaches have been used : generic qp solvers ( e . g . , cvx ) , chen et al . s interior point method ( 123 ) , a modication of least angle regression ( lars ) ( 123 ) , or grafting ( 123 ) .
in this paper , we present a new algorithm for solving the l123 - regularized least squares problem and show that it is more efcient for learning sparse coding bases .
123 l123 - regularized least squares : the feature - sign search algorithm consider solving the optimization problem ( 123 ) with an l123 penalty over the coefcients ( s ( i ) keeping the bases xed .
this problem can be solved by optimizing over each ~ s ( i ) individually :
j ) while
notice now that if we know the signs ( positive , zero , or negative ) of the s ( i ) j s at the optimal value , we can replace each of the terms |s ( i ) j > 123 ) , s ( i ) j < 123 ) , or 123 ( if 123we impose a norm constraint for bases : k ~ bjk123 c , j = 123 , . . . , n for some constant c .
norm constraints are necessary because , otherwise , there always exists a linear transformation of ~ bjs and ~ s ( i ) s which keeps j s approach zero .
based on similar motivation , olshausen and field
j unchanged , while making s ( i )
j | with either s ( i )
j k123 + ( 123 ) x
used a scheme which retains the variation of coefcients for every basis at the same level ( 123 , 123 ) .
123a log ( non - convex ) penalty was used in ( 123 ) ; thus , gradient - based methods can get stuck in local optima .
j = 123 ) .
considering only nonzero coefcients , this reduces ( 123 ) to a standard , unconstrained quadratic optimization problem ( qp ) , which can be solved analytically and efciently .
our algo - rithm , therefore , tries to search for , or guess , the signs of the coefcients s ( i ) j ; given any such guess , we can efciently solve the resulting unconstrained qp .
further , the algorithm systematically renes the guess if it turns out to be initially incorrect .
to simplify notation , we present the algorithm for the following equivalent optimization problem :
minimizexf ( x ) ky axk123 + kxk123 ,
where is a constant .
the feature - sign search algorithm is shown in algorithm 123
it maintains an active set of potentially nonzero coefcients and their corresponding signsall other coefcients must be zeroand systematically searches for the optimal active set and coefcient signs .
the algorithm proceeds in a series of feature - sign steps : on each step , it is given a current guess for the active set and the signs , and it computes the analytical solution xnew to the resulting unconstrained qp; it then updates the solution , the active set and the signs using an efcient discrete line search between the current solution and xnew ( details in algorithm 123 ) . 123 we will show that each such step reduces the objective f ( x ) , and that the overall algorithm always converges to the optimal solution .
algorithm 123 feature - sign search algorithm
initialize x : = ~ 123 , : = ~ 123 , and active set : = ( ) , where i ( 123 , 123 , 123 ) denotes sign ( xi ) .
123 from zero coefcients of x , select i = arg maxi
activate xi ( add i to the active set ) only if it locally improves the objective , namely :
123 feature - sign step :
> , then set i : = 123 , active set : = ( i ) active set .
< , then set i : = 123 , active set : = ( i ) active set .
let a be a submatrix of a that contains only the columns corresponding to the active set .
let x and be subvectors of x and corresponding to the active set .
compute the analytical solution to the resulting unconstrained qp ( minimizexky axk123 + > x ) :
xnew : = ( a> a ) 123 ( a>y / 123 ) ,
perform a discrete line search on the closed line segment from x to xnew :
check the objective value at xnew and all points where any coefcient changes sign .
update x ( and the corresponding entries in x ) to the point with the lowest objective value .
123 check the optimality conditions :
remove zero coefcients of x from the active set and update : = sign ( x ) .
( a ) optimality condition for nonzero coefcients : kyaxk123
+ sign ( xj ) = 123 , xj 123= 123
if condition ( a ) is not satised , go to step 123 ( without any new activation ) ; else check condition ( b ) .
( b ) optimality condition for zero coefcients :
if condition ( b ) is not satised , go to step 123; otherwise return x as the solution .
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , xj = 123
to sketch the proof of convergence , let a coefcient vector x be called consistent with a given active set and sign vector if the following two conditions hold for all i : ( i ) if i is in the active set , then sign ( xi ) = i , and , ( ii ) if i is not in the active set , then xi = 123
lemma 123 .
consider optimization problem ( 123 ) augmented with the additional constraint that x is consistent with a given active set and sign vector .
then , if the current coefcients xc are consistent with the active set and sign vector , but are not optimal for the augmented problem at the start of step 123 , the feature - sign step is guaranteed to strictly reduce the objective .
proof sketch .
let xc be the subvector of xc corresponding to coefcients in the given active set .
in step 123 , consider a smooth quadratic function f ( x ) ky axk123 + >x .
since xc is not an optimal point of f , we have f ( xnew ) < f ( xc ) .
now consider the two possible cases : ( i ) if xnew is consistent with the given active set and sign vector , updating x : = xnew strictly decreases the objective; ( ii ) if xnew is not consistent with the given active set and sign vector , let xd be the rst zero - crossing point ( where any coefcient changes its sign ) on a line segment from xc to xnew , then clearly xc 123= xd , 123a technical detail has been omitted from the algorithm for simplicity , as we have never observed it in prac - tice .
in step 123 of the algorithm , in case a> a becomes singular , we can check if q a>y / 123 r ( a> a ) .
if yes , we can replace the inverse with the pseudoinverse to minimize the unconstrained qp; otherwise , we can update x to the rst zero - crossing along any direction z such that z n ( a> a ) , z>q 123= 123
both these steps are still guaranteed to reduce the objective; thus , the proof of convergence is unchanged .
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) > ; this i - th coefcient is activated , and
and f ( xd ) < f ( xc ) by convexity of f , thus we nally have f ( xd ) = f ( xd ) < f ( xc ) = f ( xc ) . 123 therefore , the discrete line search described in step 123 ensures a decrease in the objective value .
lemma 123 .
consider optimization problem ( 123 ) augmented with the additional constraint that x is consistent with a given active set and sign vector .
if the coefcients xc at the start of step 123 are optimal for the augmented problem , but are not optimal for problem ( 123 ) , the feature - sign step is guaranteed to strictly reduce the objective .
proof sketch .
since xc is optimal for the augmented problem , it satises optimality condition ( a ) , but not ( b ) ; thus , in step 123 , there is some i , such that i is added to the active set .
in step 123 , consider the smooth quadratic function f ( x ) ky axk123 + >x .
observe that ( i ) since a taylor expansion of f around x = xc has a rst order term in xi only ( using condition 123 ( a ) for the other coefcients ) , we have that any direction that locally decreases f ( x ) must be consistent with the sign of the activated xi , and , ( ii ) since xc is not an optimal point of f ( x ) , f ( x ) must decrease locally near x = xc along the direction from xc to xnew .
from ( i ) and ( ii ) , the line search direction xc to xnew must be consistent with the sign of the activated xi .
finally , since f ( x ) = f ( x ) when x is consistent with the active set , either xnew is consistent , or the rst zero - crossing from xc to xnew has a lower objective value ( similar argument to lemma 123 ) .
theorem 123 .
the feature - sign search algorithm converges to a global optimum of the optimization problem ( 123 ) in a nite number of steps .
proof sketch .
from the above lemmas , it follows that the feature - sign steps always strictly reduce the objective f ( x ) .
at the start of step 123 , x either satises optimality condition 123 ( a ) or is ~ 123; in either case , x is consistent with the current active set and sign vector , and must be optimal for the augmented problem described in the above lemmas .
since the number of all possible active sets and coefcient signs is nite , and since no pair can be repeated ( because the objective value is strictly decreasing ) , the outer loop of steps 123 ( b ) cannot repeat indenitely .
now , it sufces to show that a nite number of steps is needed to reach step 123 ( b ) from step 123
this is true because the inner loop of steps 123 ( a ) always results in either an exit to step 123 ( b ) or a decrease in the size of the active set .
note that initialization with arbitrary starting points requires a small modication : after initializing and the active set with a given initial solution , we need to start with step 123 instead of step 123 when the initial solution is near the optimal solution , feature - sign search can often obtain the optimal solution more quickly than when starting from ~ 123
123 learning bases using the lagrange dual in this subsection , we present a method for solving optimization problem ( 123 ) over bases b given xed coefcients s .
this reduces to the following problem : i , j c , j = 123 , . . . , n .
this is a least squares problem with quadratic constraints .
in general , this constrained optimization problem can be solved using gradient descent with iterative projection ( 123 ) .
however , it can be much more efciently solved using a lagrange dual .
first , consider the lagrangian :
l ( b , ~ ) = trace ( cid : 123 ) ( x bs ) > ( x bs ) ( cid : 123 ) +
subject to pk
where each j 123 is a dual variable .
minimizing over b analytically , we obtain the lagrange dual :
l ( b , ~ ) = trace ( cid : 123 ) x>x xs> ( ss> + ) 123 ( xs> ) > c ( cid : 123 ) ,
d ( ~ ) = min
where = diag ( ~ ) .
the gradient and hessian of d ( ~ ) are computed as follows :
= kxs> ( ss> + ) 123eik123 c ,
= 123 ( cid : 123 ) ( ss> + ) 123 ( xs> ) >xs> ( ss> + ) 123 ( cid : 123 )
( cid : 123 ) ( ss> + ) 123 ( cid : 123 )
i , j , ( 123 ) 123to simplify notation , we reuse f ( ) even for subvectors such as x; in the case of f ( x ) , we consider only
the coefcients in x as variables , and all coefcients not in the subvector can be assumed constant at zero .
123if the algorithm terminates without reaching step 123 , we are done; otherwise , once the algorithm reaches
step 123 , the same argument in the proof applies .
chen et al . s qp solver ( cvx )
table 123 : the running time in seconds ( and the relative error in parentheses ) for coefcient learning algorithms applied to different natural stimulus datasets .
for each dataset , the input dimension k and the number of bases n are specied as k n .
the relative error for an algorithm was dened as ( fobj f ) / f , where fobj is the nal objective value attained by that algorithm , and f is the best objective value attained among all the algorithms .
where ei rn is the i - th unit vector .
now , we can optimize the lagrange dual ( 123 ) using newtons method or conjugate gradient .
after maximizing d ( ~ ) , we obtain the optimal bases b as follows :
b> = ( ss> + ) 123 ( xs> ) > .
the advantage of solving the dual is that it uses signicantly fewer optimization variables than the primal .
for example , optimizing b r123 , 123 , 123 requires only 123 , 123 dual variables .
note that the dual formulation is independent of the sparsity function ( e . g . , l123 , epsilonl123 , or other sparsity function ) , and can be extended to other similar models such as topographic cells ( 123 ) . 123 123 experimental results 123 the feature - sign search algorithm we evaluated the performance of our algorithms on four natural stimulus datasets : natural images , speech , stereo images , and natural image videos .
all experiments were conducted on a linux ma - chine with amd opteron 123ghz cpu and 123gb ram .
first , we evaluated the feature - sign search algorithm for learning coefcients with the l123 sparsity function .
we compared the running time and accuracy to previous state - of - the - art algorithms : a generic qp solver , 123 a modied version of lars ( 123 ) with early stopping , 123 grafting ( 123 ) , and chen et al . s interior point method ( 123 ) ;123 all the algorithms were implemented in matlab .
for each dataset , we used a test set of 123 input vectors and measured the running time123 and the objective function at convergence .
table 123 shows both the running time and accuracy ( measured by the relative error in the nal objective value ) of different coefcient learning algorithms .
over all datasets , feature - sign search achieved the best objective values as well as the shortest running times .
feature - sign search and modied lars produced more accurate solutions than the other methods . 123 feature - sign search was an order of magnitude faster than both chen et al . s algorithm and the generic qp solver , and it was also signicantly faster than modied lars and grafting .
moreover , feature - sign search has the crucial advantage that it can be initialized with arbitrary starting coefcients ( unlike lars ) ; we will demonstrate that feature - sign search leads to even further speedup over lars when applied to iterative coefcient learning .
123 total time for learning bases the lagrange dual method for one basis learning iteration was much faster than gradient descent with iterative projections , and we omit discussion of those results due to space constraints .
below , we directly present results for the overall time taken by sparse coding for learning bases from natural
123the sparsity penalty for topographic cells can be written asp
function and cell l is a topographic cell ( e . g . , group of neighboring bases in 123 - d torus representation ) .
123we used the cvx package available at http : / / www . stanford . edu / boyd / cvx / .
123lars ( with lasso modication ) provides the entire regularization path with discrete l123 - norm con - straints; we further modied the algorithm so that it stops upon nding the optimal solution of the equation ( 123 ) .
123matlab code is available at http : / / www - stat . stanford . edu / atomizer / .
123for each dataset / algorithm combination , we report the average running time over 123 trials .
123a general - purpose qp package ( such as cvx ) does not explicitly take the sparsity of the solutions into account .
thus , its solution tends to have many very small nonzero coefcients; as a result , the objective values obtained from cvx were always slightly worse than those obtained from feature - sign search or lars .
jcell l s123
123 ) , where ( ) is a sparsity
/ basis learning feature - sign / lagdual feature - sign / graddesc lars / lagdual lars / graddesc grafting / lagdual grafting / graddesc
/ basis learning conjgrad / lagdual conjgrad / graddesc
l123 sparsity function
epsilonl123 sparsity function
table 123 : the running time ( in seconds ) for different algorithm combinations using different sparsity
figure 123 : demonstration of speedup .
left : comparison of convergence between the lagrange dual method and gradient descent for learning bases .
right : the running time per iteration for modied lars and grafting as a multiple of the running time per iteration for feature - sign search .
we evaluated different combinations of coefcient learning and basis learning algorithms : the fastest coefcient learning methods from our experiments ( feature - sign search , modied lars and graft - ing for the l123 sparsity function , and conjugate gradient for the epsilonl123 sparsity function ) and the state - of - the - art basis learning methods ( gradient descent with iterative projection and the lagrange dual formulation ) .
we used a training set of 123 , 123 input vectors for each of the four natural stimulus datasets .
we initialized the bases randomly and ran each algorithm combination ( by alternatingly optimizing the coefcients and the bases ) until convergence . 123 table 123 shows the running times for different algorithm combinations .
first , we observe that the lagrange dual method signicantly outperformed gradient descent with iterative projections for both l123 and epsilonl123 sparsity; a typical convergence pattern is shown in figure 123 ( left ) .
second , we observe that , for l123 sparsity , feature - sign search signicantly outperformed both modied lars and grafting . 123 figure 123 ( right ) shows the running time per iteration for modied lars and grafting as a multiple of that for feature - sign search ( using the same gradient descent algorithm for basis learning ) , demonstrating signicant efciency gains at later iterations; note that feature - sign search ( and grafting ) can be initialized with the coefcients obtained in the previous iteration , whereas modied lars cannot .
this result demonstrates that feature - sign search is particularly efcient for iterative optimization , such as learning sparse coding bases .
123 learning highly overcomplete natural image bases using our efcient algorithms , we were able to learn highly overcomplete bases of natural images as shown in figure 123
for example , we were able to learn a set of 123 , 123 bases ( each 123 pixels ) 123we ran each algorithm combination until the relative change of the objective per iteration became less than 123 ( i . e . , | ( fnew fold ) / fold| < 123 ) .
to compute the running time to convergence , we rst computed the optimal ( minimum ) objective value achieved by any algorithm combination .
then , for each combination , we dened the convergence point as the point at which the objective value reaches within 123% relative error of the observed optimal objective value .
the running time measured is the time taken to reach this convergence point .
we truncated the running time if the optimization did not converge within 123 , 123 seconds .
123we also evaluated a generic conjugate gradient implementation on the l123 sparsity function; however , it did
not converge even after 123 , 123 seconds .
figure 123 : learned overcomplete natural image bases .
left : 123 , 123 bases ( each 123 pixels ) .
right : 123 , 123 bases ( each 123 pixels ) .
figure 123 : left : end - stopping test for 123 sized 123 , 123 bases .
each line in the graph shows the coefcients for a basis for different length bars .
right : sample input image for ncrf effect .
in about 123 hours and a set of 123 , 123 bases ( each 123 pixels ) in about 123 hours . 123 in contrast , the gradient descent method for basis learning did not result in any reasonable bases even after running for 123 hours .
further , summary statistics of our learned bases , obtained by tting the gabor function parameters to each basis , qualitatively agree with previously reported statistics ( 123 ) .
123 replicating complex neuroscience phenomena several complex phenomena of v123 neural responses are not well explained by simple linear models ( in which the response is a linear function of the input ) .
for instance , many visual neurons display end - stopping , in which the neurons response to a bar image of optimal orientation and placement is actually suppressed as the bar length exceeds an optimal length ( 123 ) .
sparse coding can model the interaction ( inhibition ) between the bases ( neurons ) by sparsifying their coefcients ( activations ) , and our algorithms enable these phenomena to be tested with highly overcomplete bases .
first , we evaluated whether end - stopping behavior could be observed in the sparse coding frame - work .
we generated random bars with different orientations and lengths in 123 image patches , and picked the stimulus bar which most strongly activates each basis , considering only the bases which are signicantly activated by one of the test bars .
for each such highly activated basis , and the corresponding optimal bar position and orientation , we vary length of the bar from 123 pixel to the maximal size and run sparse coding to measure the coefcients for the selected basis , relative to their maximum coefcient .
as shown in figure 123 ( left ) , for highly overcomplete bases , we observe many cases in which the coefcient decreases signicantly as the bar length is increased beyond the optimal point .
this result is consistent with the end - stopping behavior of some v123 neurons .
second , using the learned overcomplete bases , we tested for center - surround non - classical receptive eld ( ncrf ) effects ( 123 ) .
we found the optimal bar stimuli for 123 random bases and checked that these bases were among the most strongly activated ones for the optimal stimulus .
for each of these
123we used lagrange dual formulation for learning bases , and both conjugate gradient with epsilonl123 sparsity as well as the feature - sign search with l123 sparsity for learning coefcients .
the bases learned from both methods showed qualitatively similar receptive elds .
the bases shown in the figure 123 were learned using epsilonl123 sparsity function and 123 , 123 input image patches randomly sampled for every iteration .
bases , we measured the response with its optimal bar stimulus with and without the aligned bar stimulus in the surround region ( figure 123 ( right ) ) .
we then compared the basis response in these two cases to measure the suppression or facilitation due to the surround stimulus .
the aligned surround stimuli produced a suppression of basis activation; 123 out of 123 bases showed suppression with aligned surround input images , and 123 bases among them showed more than 123% suppression , in qualitative accordance with observed ncrf surround suppression effects .
123 application to self - taught learning sparse coding is an unsupervised algorithm that learns to represent input data succinctly using only a small number of bases .
for example , using the image edge bases in figure 123 , it represents a new image patch ~ as a linear combination of just a small number of these bases ~ bj .
informally , we think of this as nding a representation of an image patch in terms of the edges in the image; this gives a slightly higher - level / more abstract representation of the image than the pixel intensity values , and is useful for a variety of tasks .
in related work ( 123 ) , we apply this to self - taught learning , a new machine learning formalism in which we are given a supervised learning problem together with additional unlabeled instances that may not have the same class labels as the labeled instances .
for example , one may wish to learn to distinguish between cars and motorcycles given images of each , and additionaland in practice readily availableunlabeled images of various natural scenes .
( this is in contrast to the much more restrictive semi - supervised learning problem , which would require that the unlabeled examples also be of cars or motorcycles only . ) we apply our sparse coding algorithms to the unlabeled data to learn bases , which gives us a higher - level representation for images , thus making the supervised learning task easier .
on a variety of problems including object recognition , audio classication , and text categorization , this approach leads to 123% reductions in test error .
in this paper , we formulated sparse coding as a combination of two convex optimization problems and presented efcient algorithms for each : the feature - sign search for solving the l123 - least squares problem to learn coefcients , and a lagrange dual method for the l123 - constrained least squares problem to learn the bases for any sparsity penalty function .
we test these algorithms on a variety of datasets , and show that they give signicantly better performance compared to previous methods .
our algorithms can be used to learn an overcomplete set of bases , and show that sparse coding could partially explain the phenomena of end - stopping and ncrf surround suppression in v123 neurons .
