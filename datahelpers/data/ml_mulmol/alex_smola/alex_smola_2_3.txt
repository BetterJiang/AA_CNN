this paper addresses the problem of choosing a kernel suitable for estimation with a support vector machine , hence further automating machine learning .
this goal is achieved by dening a reproducing kernel hilbert space on the space of kernels itself .
such a formulation leads to a statistical estimation problem similar to the problem of minimizing a regularized risk functional .
we state the equivalent representer theorem for the choice of kernels and present a semidenite programming formulation of the resulting optimization problem .
several recipes for constructing hyperkernels are provided , as well as the details of common machine learning problems .
experi - mental results for classication , regression and novelty detection on uci data show the feasibility of our approach .
keywords : learning the kernel , capacity control , kernel methods , support vector machines , repre - senter theorem , semidenite programming
kernel methods have been highly successful in solving various problems in machine learning .
the algorithms work by implicitly mapping the inputs into a feature space , and nding a suitable hy - pothesis in this new space .
in the case of the support vector machine ( svm ) , this solution is the hyperplane which maximizes the margin in the feature space .
the feature mapping in question is dened by a kernel function , which allows us to compute dot products in feature space using only objects in the input space .
for an introduction to svms and kernel methods , the reader is referred to numerous tutorials such as burges ( 123 ) and books such as scholkopf and smola ( 123 ) .
choosing a suitable kernel function , and therefore a feature mapping , is imperative to the suc - cess of this inference process .
this paper provides an inference framework for learning the kernel from training data using an approach akin to the regularized quality functional .
this work was done when the author was at the australian national university .
c ( cid : 123 ) 123 cheng soon ong , alexander j .
smola and robert c .
williamson .
ong , smola and williamson
as motivation for the need for methods to learn the kernel , consider figure 123 , which shows the sep - arating hyperplane , the margin and the training data for a synthetic data set .
figure 123 ( a ) shows the classication function for a support vector machine using a gaussian radial basis function ( rbf ) kernel .
the data has been generated using two gaussian distributions with standard deviation 123 in one dimension and 123 in the other .
this difference in scale creates problems for the gaus - sian rbf kernel , since it is unable to nd a kernel width suitable for both directions .
hence , the classication function is dominated by the dimension with large variance .
increasing the value of the regularization parameter , c , and hence decreasing the smoothness of the function results in a hyperplane which is more complex , and equally unsatisfactory ( figure 123 ( b ) ) .
the traditional way to handle such data is to normalize each dimension independently .
instead of normalising the input data , we make the kernel adaptive to allow independent scales for each dimension .
this allows the kernel to handle unnormalised data .
however , the resulting kernel would be difcult to hand - tune as there may be numerous free variables .
in this case , we have a free parameter for each dimension of the input .
we learn this kernel by dening a quantity analogous to the risk functional , called the quality functional , which measures the badness of the kernel function .
the classication function for the above mentioned data is shown in figure 123 ( c ) .
observe that it captures the scale of each dimension independently .
in general , the solution does not consist of only a single kernel but a linear combination of them .
123 123 123
large variance dimension
123 123 123
123 123 123
large variance dimension
large variance dimension
( b ) standard gaussian rbf kernel
( a ) standard gaussian rbf kernel figure 123 : for data with highly non - isotropic variance , choosing one scale for all dimensions leads to unsatisfactory results .
plot of synthetic data , showing the separating hyperplane and the margins given for a uniformly chosen length scale ( left and middle ) and an automatic width selection ( right ) .
( c ) rbf - hyperkernel with adaptive
123 related work
we analyze some recent approaches to learning the kernel by looking at the objective function that is being optimized and the class of kernels being considered .
we will see later ( section 123 ) that this objective function is related to our denition of a quality functional .
cross validation has been used to select the parameters of the kernels and svms ( duan et al . , 123 , meyer et al . , 123 ) , with vary - ing degrees of success .
the objective function is the cross validation risk , and the class of kernels is a nite subset of the possible parameter settings .
duan et al .
( 123 ) and chapelle et al .
( 123 )
test various simple approximations which bound the leave one out error , or some measure of the capacity of the svm .
the notion of kernel target alignment ( cristianini et al . , 123 ) uses the ob - jective function tr ( kyy ) where y are the training labels , and k is from the class of kernels spanned by the eigenvectors of the kernel matrix of the combined training and test data .
the semidenite programming ( sdp ) approach ( lanckriet et al . , 123 ) uses a more general class of kernels , namely a linear combination of positive semidenite matrices .
they minimize the margin of the resulting svm using a sdp for kernel matrices with constant trace .
similar to this , bousquet and herrmann ( 123 ) further restricts the class of kernels to the convex hull of the kernel matrices normalized by their trace .
this restriction , along with minimization of the complexity class of the kernel , allows them to perform gradient descent to nd the optimum kernel .
using the idea of boosting , crammer et al .
( 123 ) optimize ( cid : 123 ) t are the weights used in the boosting algorithm .
the class of base kernels ( kt ) is obtained from the normalized solution of the generalized eigenvector prob - lem .
in principle , one can learn the kernel using bayesian methods by dening a suitable prior , and learning the hyperparameters by optimizing the marginal likelihood ( williams and barber , 123 , williams and rasmussen , 123 ) .
as an example of this , when other information is available , an auxiliary matrix can be used with the em algorithm for learning the kernel ( tsuda et al . , 123 ) .
table 123 summarizes these approaches .
the notation k ( cid : 123 ) 123 means that k is positive semidenite , that is for all a rn , aka > 123
tkt , where b
kernel class ( k )
finite set of kernels
ivivi such that vi are eigenvectors of k ) iki such that ki ( cid : 123 ) 123 , trki = c , b i > 123 )
iki such that ki ( cid : 123 ) 123 , trki = c )
base kernels from generalized eigenvector problem
dependent on prior
linear combination of auxiliary matrix
table 123 : summary of recent approaches to kernel learning .
123 outline of the paper
the contribution of this paper is a theoretical framework for learning the kernel .
using this frame - work , we analyze the regularized risk functional .
motivated by the ideas of cristianini et al .
( 123 ) , we show ( section 123 ) that for most kernel - based learning methods there exists a functional , the qual - ity functional , which plays a similar role to the empirical risk functional .
we introduce a kernel on the space of kernels itself , a hyperkernel ( section 123 ) , and its regularization on the associated hyper reproducing kernel hilbert space ( hyper - rkhs ) .
this leads to a systematic way of parame - terizing kernel classes while managing overtting ( ong et al . , 123 ) .
we give several examples of hyperkernels and recipes to construct others ( section 123 ) .
using this general framework , we consider the specic example of using the regularized risk functional in the rest of the paper .
the positive deniteness of the kernel function is ensured using the positive deniteness of the kernel matrix ( section 123 ) , and the resulting optimization problem is a semidenite program .
the semidenite programming approach follows that of lanckriet et al .
( 123 ) , with a different constraint due to a
ong , smola and williamson
difference in regularization ( ong and smola , 123 ) .
details of the specic optimization problems associated with the c - svm , n - svm , lagrangian svm , n - svr and one class svm are dened in section 123
experimental results for classication , regression and novelty detection ( section 123 ) are shown .
finally some issues and open problems are discussed ( section 123 ) .
kernel quality functionals
we denote by x the space of input data and y the space of labels ( if we have a supervised learning problem ) .
denote by xtrain : = ( x123 , .
, xm ) the training data and with ytrain : = ( y123 , .
, ym ) a set of corresponding labels , jointly drawn independently and identically from some probability distribu - tion pr ( x , y ) on x y .
we shall , by convenient abuse of notation , generally denote ytrain by the vector y , when writing equations in matrix notation .
we denote by k the kernel matrix given by ki j : = k ( xi , x j ) where xi , x j xtrain and k is a positive semidenite kernel function .
we also use trk to mean the trace of the matrix and |k| to mean the determinant .
we begin by introducing a new class of functionals q on data which we will call quality func - tionals .
note that by quality we actually mean badness or lack of quality , as we would like to minimize this quantity .
their purpose is to indicate , given a kernel k and the training data , how suitable the kernel is for explaining the training data , or in other words , the quality of the kernel for the estimation problem at hand .
such quality functionals may be the kernel target alignment , the negative log posterior , the minimum of the regularized risk functional , or any luckiness function for kernel methods .
we will discuss those functionals after a formal denition of the quality functional
123 empirical and expected quality
denition 123 ( empirical quality functional ) given a kernel k , and data x , y , we dene qemp ( k , x , y ) to be an empirical quality functional if it depends on k only via k ( xi , x j ) where xi , x j x for 123 123 i , j 123 m .
by this denition , qemp is a function which tells us how well matched k is to a specic data set x , y .
typically such a quantity is used to adapt k in such a manner that qemp is optimal ( for example , optimal kernel target alignment , greatest luckiness , smallest negative log - posterior ) , based on this one single data set x , y .
provided a sufciently rich class of kernels k it is in general possible to nd a kernel k k that attains the minimum of any such qemp regardless of the data .
however , it is very unlikely that qemp ( k , x , y ) would be similarly small for other x , y , for such a k .
to measure the overall quality of k we therefore introduce the following denition :
denition 123 ( expected quality functional ) denote by qemp ( k , x , y ) an empirical quality func -
is dened to be the expected quality functional .
here the expectation is taken over x , y , where all xi , yi are drawn from pr ( x , y ) .
q ( k ) : = ex , y ( cid : 123 ) qemp ( k , x , y ) ( cid : 123 )
observe the similarity between the empirical quality functional , qemp ( k , x , y ) , and the empirical risk of an estimator , remp ( f , x , y ) = 123 i=123 l ( xi , yi , f ( xi ) ) ( where l is a suitable loss function ) ; in
both cases we compute the value of a functional which depends on some sample x , y drawn from pr ( x , y ) and a function and in both cases we have
q ( k ) = ex , y ( cid : 123 ) qemp ( k , x , y ) ( cid : 123 ) and r ( f ) = ex , y ( cid : 123 ) remp ( f , x , y ) ( cid : 123 ) .
here r ( f ) denotes the expected risk .
however , while in the case of the empirical risk we can interpret remp as the empirical estimate of the expected loss r ( f ) = ex , y ( l ( x , y , f ( x ) ) ) , due to the general form of qemp , no such analogy is available for quality functionals .
finding a general - purpose bound of the expected error in terms of q ( k ) is difcult , since the denition of q depends heavily on the algorithm under consideration .
nonetheless , it provides a general framework within which such bounds can be derived .
to obtain a generalization error bound , it is sufcient that qemp is concentrated around its expected value .
furthermore , one would require the deviation of the empirical risk to be upper bounded by qemp and possibly other terms .
in other words , we assume a ) we have given a concen - tration inequality on quality functionals , such as
and b ) we have a bound on the deviation of the empirical risk in terms of the quality functional
then we can chain both inequalities together to obtain the following bound
pr ( cid : 123 ) |qemp ( k , x , y ) q ( k ) | > e q ( cid : 123 ) < d q , pr ( cid : 123 ) |remp ( f , x , y ) r ( f ) | > e r ( cid : 123 ) < d ( qemp ) .
pr ( cid : 123 ) |remp ( f , x , y ) r ( f ) | > e r ( cid : 123 ) < d q + d ( q + e q ) .
this means that the bound now becomes independent of the particular value of the quality func - tional obtained on the data , rather than the expected value of the quality functional .
bounds of this type have been derived for kernel target alignment ( cristianini et al . , 123 , theorem 123 ) and the algorithmic luckiness framework ( herbrich and williamson , 123 , theorem 123 ) .
123 examples of qemp before we continue with the derivations of a regularized quality functional and introduce a cor - responding reproducing kernel hilbert space , we give some examples of quality functionals and present their exact minimizers , whenever possible .
this demonstrates that given a rich enough fea - ture space , we can arbitrarily minimize the empirical quality functional qemp .
the difference here from traditional kernel methods is the fact that we allow the kernel to change .
this extra degree of freedom allows us to overt the training data .
in many of the examples below , we show that given a feature mapping which can model the labels of the training data precisely , overtting occurs .
that is , if we use the training labels as the kernel matrix , we arbitrarily minimize the quality functional .
the reader who is convinced that one can arbitrarily minimize qemp , by optimizing over a suitably large class of kernels , may skip the following examples .
example 123 ( regularized risk functional ) these are commonly used in svms and related kernel methods ( see wahba ( 123 ) , vapnik ( 123 ) , scholkopf and smola ( 123 ) ) .
they take on the general
rreg ( f , xtrain , ytrain ) : =
l ( xi , yi , f ( xi ) ) +
ong , smola and williamson
where k fk123 h is the rkhs norm of f and l is a loss function such that for f ( xi ) = yi , l ( xi , yi , yi ) = 123
by virtue of the representer theorem ( see section 123 ) we know that the minimizer of ( 123 ) can be written as a kernel expansion .
this leads to the following denition of a quality functional , for a particular loss functional l :
( k , xtrain , ytrain ) : = min
a rm " 123
l ( xi , yi , ( ka
a ka # .
however , we know that qregrisk
the minimizer of ( 123 ) is somewhat difcult to nd , since we have to carry out a double minimization over k and a is bounded from below by 123
hence , it is sufcient if we can nd a ( possibly ) suboptimal pair ( a note that for k = b yy and a = 123 l ( xi , yi , f ( xi ) ) = 123 and therefore qregrisk
, k ) for which qregrisk b kyk123 y we have ka = y and a ka = b 123
this leads to .
for sufciently large b we can
( k , xtrain , ytrain ) arbitrarily close to 123
for any e > 123 :
( k , xtrain , ytrain ) =
even if we disallow setting k arbitrarily close to zero by setting trk = 123 , nding the minimum kzk123 zz , where z rm , and a = z .
then ka = z
of ( 123 ) can be achieved as follows : let k = 123 and we obtain
l ( xi , yi , ( ka
a ka =
l ( xi , yi , zi ) +
choosing each zi = argminz l ( xi , yi , z ( xi ) ) + z 123 , where z are the possible hypothesis functions obtained from the training data , yields the minimum with respect to z .
since ( 123 ) tends to zero and the regularized risk is lower bounded by zero , we can still arbitrarily minimize qregrisk this is not surprising since the set of allowable k is huge .
example 123 ( cross validation ) cross validation is a widely used method for estimating the gener - alization error of a particular learning algorithm .
specically , the leave - one - out cross validation is an almost unbiased estimate of the generalization error ( luntz and brailovsky , 123 ) .
the quality functional for classication using kernel methods is given by :
emp ( k , xtrain , ytrain ) : = min
a rm " 123
which is optimized in duan et al .
( 123 ) , meyer et al .
( 123 ) .
choosing k = yy and a
i and yi are the vectors a and y with the ith element set to zero , we have ka i = yi .
hence we can match the training data perfectly .
for a validation set of larger size , i . e .
k - fold cross validation , the same result can be achieved by dening a corresponding
kyik123 yi , where a
example 123 ( kernel target alignment ) this quality functional was introduced by cristianini et al .
( 123 ) to assess the alignment of a kernel with training labels .
it is dened by
( k , xtrain , ytrain ) : = 123
here kyk123 denotes the 123 norm of the vector of observations and kkkf is the frobenius norm , i . e . , i , j ( ki j ) 123
this quality functional was optimized in lanckriet et al .
( 123 ) .
by decomposing k into its eigensystem one can see that ( 123 ) is minimized , if k = yy , in which case
f : = tr ( kk ) = ( cid : 123 )
( k , xtrain , ytrain ) = 123
= 123 kyk123
we cannot expect that qalignment ( k , x , y ) = 123 for data other than that chosen to determine k , in other words , a restriction of the class of kernels is required .
this was also observed in cristianini et al .
( 123 ) .
the above examples illustrate how existing methods for assessing the quality of a kernel t within the quality functional framework .
we also saw that given a rich enough class of kernels k , optimization of qemp over k would result in a kernel that would be useless for prediction purposes , in the sense that they can be made to look arbitrarily good in terms of qemp but with the result that the generalization performance will be poor .
this is yet another example of the danger of optimizing too much and overtting there is ( still ) no free lunch .
hyper reproducing kernel hilbert spaces
we now propose a conceptually simple method to optimize quality functionals over classes of ker - nels by introducing a reproducing kernel hilbert space on the kernel k itself , so to say , a hyper - rkhs .
we rst review the denition of a rkhs ( aronszajn , 123 ) .
denition 123 ( reproducing kernel hilbert space ) let x be a nonempty set ( the index set ) and denote by h a hilbert space of functions f : x r .
h is called a reproducing kernel hilbert space endowed with the dot product h , i ( and the norm k fk : =ph f , fi ) if there exists a function k : x x r with the following properties .
k has the reproducing property
h f , k ( x , ) i = f ( x ) for all f h , x x ;
in particular , hk ( x , ) , k ( x , ) i = k ( x , x ) for all x , x x .
k spans h , i . e .
h = span ( k ( x , ) |x x ) where x is the completion of the set x .
in the rest of the paper , we use the notation k to represent the kernel function and h to represent the rkhs .
in essence , h is a hilbert space of functions , which has the special property of being generated by the kernel function k .
the advantage of optimization in an rkhs is that under certain conditions the optimal solutions can be found as the linear combination of a nite number of basis functions , regardless of the dimensionality of the space h the optimization is carried out in .
the theorem below formalizes this notion ( see kimeldorf and wahba ( 123 ) , cox and osullivan ( 123 ) ) .
theorem 123 ( representer theorem ) denote by w : ( 123 , ) r a strictly monotonic increasing function , by x a set , and by l : ( x r123 ) m r ( ) an arbitrary loss function .
then each mini - mizer f h of the general regularized risk
l ( ( x123 , y123 , f ( x123 ) ) , .
, ( xm , ym , f ( xm ) ) ) + w
( k fkh )
ong , smola and williamson
admits a representation of the form
i r for all 123 123 i 123 m .
123 regularized quality functional
f ( x ) =
to learn the kernel , we need to dene a function space of kernels , a method to regularize them and a practical optimization procedure .
we will address each of these issues in the following .
we dene an rkhs on kernels k : x x r , simply by introducing the compounded index set , x : = x x and by treating k as a function k : x r : denition 123 ( hyper reproducing kernel hilbert space ) let x be a nonempty set .
and denote by x : = x x the compounded index set .
the hilbert space h of functions k : x r , endowed with a dot product h , i ( and the norm kkk =phk , ki ) is called a hyper reproducing kernel hilbert space if there exists a hyperkernel k : x x r with the following properties : 123
k has the reproducing property hk , k ( x , ) i = k ( x ) for all k h ; in particular , hk ( x , ) , k ( x , ) i =
k spans h , i . e .
h = span ( k ( x , ) |x x ) .
k ( x , y , s , t ) = k ( y , x , s , t ) for all x , y , s , t x .
this is a rkhs with the additional requirement of symmetry in its rst two arguments ( in fact , we can have a recursive denition of an rkhs of an rkhs ad innitum , with suitable restric - tions on the elements ) .
we dene the corresponding notations for elements , kernels , and rkhs by underlining it .
what distinguishes h from a normal rkhs is the particular form of its index set ( x = x 123 ) and the additional condition on k to be symmetric in its rst two arguments , and therefore in its second two arguments as well .
this approach of dening a rkhs on the space of symmetric functions of two variables leads us to a natural regularization method .
by analogy with the denition of the regularized risk functional ( 123 ) , we proceed to dene the regularized quality functional .
denition 123 ( regularized quality functional ) let x , y be the combined training and test set of examples and labels respectively .
for a positive semidenite kernel matrix k on x , the regularized quality functional is dened as
qreg ( k , x , y ) : = qemp ( k , x , y ) +
where l q > 123 is a regularization constant and kkk123
h denotes the rkhs norm in h .
note that although we have possibly non positive kernels in h , we dene the regularized quality functional only on positive semidenite kernel matrices .
this is a slightly weaker condition than requiring a positive semidenite kernel k , since we only require positivity on the data .
since qemp depends on k only via the data , this is sufcient for the above denition .
minimization of qreg is
less prone to overtting than minimizing qemp , since the regularization term controls the complexity of the class of kernels under consideration .
bousquet and herrmann ( 123 ) provide a generalization error bound by estimating the rademacher complexity of the kernel classes in the transduction setting .
regularizers other than kkk123 h are possible , such as p penalties .
in this paper , we restrict ourselves to the 123 norm ( 123 ) .
the advantage of ( 123 ) is that its minimizer satises the representer theorem .
lemma 123 ( representer theorem for hyper - rkhs ) let x be a set , qemp an arbitrary empirical quality functional , and x , y the combined training and test set , then each minimizer k h of the regularized quality functional qreg ( k , x , y ) admits a representation of the form
k ( x , x ) =
i jk ( ( xi , x j ) , ( x , x ) ) for all x , x x ,
i j r , for each 123 123 i , j 123 m .
h is an rkhs regularizer , so the representer theorem applies and ( 123 ) follows .
proof all we need to do is rewrite ( 123 ) so that it satises the conditions of theorem 123
let xi j : = ( xi , x j ) .
then qemp ( k , x , y ) has the properties of a loss function , as it only depends on k via its values at xi j .
note too that the kernel matrix k also only depends on k via its values at xi j .
furthermore , lemma 123 implies that the solution of the regularized quality functional is a linear combination of hyperkernels on the input data .
this shows that even though the optimization takes place over an entire hilbert space of kernels , one can nd the optimal solution by choosing among a nite number .
note that the minimizer ( 123 ) is not necessarily positive semidenite .
in practice , this is not what we want , since we require a positive semidenite kernel but we do not have any guarantees for examples in the test set .
therefore we need to impose additional constraints of the type k ( cid : 123 ) 123 or k is a mercer kernel .
while the latter is almost impossible to enforce directly , the former could be veried directly , hence imposing a constraint only on the values of the kernel matrix k ( xi , x j ) rather than on the kernel function k itself .
this means that the conditions of the representer theorem apply and ( 123 ) applies ( with suitable constraints on the coefcients b
another option is to be somewhat more restrictive and require that all expansion coefcients i , j > 123 and all the functions be positive semidenite kernels .
this latter requirement can be for - mally stated as follows : for any xed x x the hyperkernel k is a kernel in its second argument; that is for any xed x x , the function k ( x , x ) : = k ( x , ( x , x ) ) , with x , x x , is a positive semidenite
proposition 123 given a hyperkernel , k with elements such that for any xed x x , the function k ( xp , xq ) : = k ( x , ( xp , xq ) ) , with xp , xq x , is a positive semidenite kernel , and b i j > 123 for all i , j = 123 , .
, m , then the kernel
k ( xp , xq ) : =
i jk ( xi , x j , xp , xq )
is positive semidenite .
proof the result is obtained by observing that positive combinations of positive semidenite ker - nels are positive semidenite .
ong , smola and williamson
while this may prevent us from obtaining the minimizer of the objective function , it yields a much more amenable optimization problem in practice , in particular if the resulting cone spans a large enough space ( as happens with increasing m ) .
in the subsequent derivations of optimization problems , we choose this restriction as it provides a more tractable problem in practice .
in section 123 , we give examples and recipes for constructing hyperkernels .
before that , we relate our framework dened above to bayesian inference .
123 a bayesian perspective
a generative bayesian approach to inference encodes all knowledge we might have about the prob - lem setting into a prior distribution .
hence , the choice of the prior distribution determines the behaviour of the inference , as once we have the data , we condition on the prior distribution we have chosen to obtain the posterior , and then marginalize to obtain the label that we are interested in .
one popular choice of prior is the normal distribution , resulting in a gaussian process ( gp ) .
all prior knowledge we have about the problem is then encoded in the covariance of the gp .
there exists a gp analog to the support vector machine ( for example opper and winther ( 123 ) , seeger ( 123 ) ) , which is essentially obtained ( ignoring normalizing terms ) by exponentiating the regularized risk functional used in svms .
in this section , we derive the prior and hyperprior implied by our framework of hyperkernels .
this is obtained by exponentiating qreg , again ignoring normalization terms .
given the regularized quality functional ( equation 123 ) , with the qemp set to the svm with squared loss , we obtain the
qreg ( k , x , y ) : =
( yi f ( xi ) ) 123 +
exponentiating the negative of the above equation gives ,
exp ( qreg ( k , x , y ) ) =
( yi f ( xi ) ) 123 ! exp ( cid : 123 )
h ( cid : 123 ) .
we compare equation ( 123 ) to gaussian process estimation .
the general scheme is known in bayesian estimation as hyperpriors ( bishop , 123 , chapter 123 ) , which determine the distribution of the priors ( here the gp with covariance k ) .
figure 123 describes the model of an ordinary gp , where f is drawn from a gaussian distribution with covariance matrix k and y is conditionally independent given f .
for hyperprior estimation , we draw the prior k from a distribution instead of setting it .
k chosen by user
figure 123 : generative model for gaussian process estimation
to determine the distribution from which we draw the prior , we compute the hyperprior explic -
for given data z = ( x , y ) and applying bayes rule , the posterior is given by
p ( f|z , k ) =
p ( z| f , k ) p ( f|k ) p ( k )
we have the directed graphical model shown in figure 123 for a hyperkernel - gp , where we as - sume that the covariance matrix of the gaussian process k is drawn according to a distribution before performing further steps of dependency calculation .
we shall now explicitly compute the terms in the numerator of equation ( 123 ) .
p ( y| f , x )
figure 123 : generative model for gaussian process estimation using hyperpriors on k dened by k .
in the following derivations , we assume that we are dealing with nite dimensional objects , to simplify the calculations of the normalizing constants in the expressions for the distributions
that we have additive gaussian noise , that is e n ( 123 , 123
e i ) , then ,
therefore , for the whole data set ( assumed to be i . i . d . ) ,
p ( y| f , x ) ( cid : 123 )
p ( yi| f , xi ) = ( cid : 123 ) 123p e ( cid : 123 ) m
( y f ( x ) ) 123 ( cid : 123 ) .
( yi f ( xi ) ) 123 ! .
p ( y| f , x ) =
we assume a gaussian prior on the function f , with covariance function k .
the positive semidenite function , k , denes an inner product h , ih k in the rkhs denoted by h k
where f is the dimension of f and g
p ( f|k ) = ( cid : 123 ) 123p
f ( cid : 123 ) f
f is a constant .
123 h f , fih k ( cid : 123 )
we assume a wishart distribution ( lauritzen , 123 , appendix c ) , with p degrees of freedom and covariance k123 , for the prior distribution of the covariance function k , that is k w m ( p , k123 ) .
this is a hyperprior used in the gaussian process literature :
where g m ( p ) denotes the gamma distribution , g m ( p ) = 123 pm tails of the wishart distribution , the reader is referred to lauritzen ( 123 ) .
p ( k|k123 ) = |k|
123 p m ( m123 )
g ( cid : 123 ) pi+123
123 ( cid : 123 ) .
for more de -
observe that tr ( kk123 ) is an inner product between two matrices .
we can dene a general inner
product between two matrices , as the inner product dened in the rkhs denoted by h :
p ( k|k123 , k ) = |k|
ong , smola and williamson
we can interpret the above equation as measuring the similarity between the covariance matrix that we obtain from data and the expected covariance matrix ( given by the user ) .
this similarity is measured by a dot product dened by k .
substituting the expressions for p ( y|x , f ) , p ( f|k ) and p ( k|k123 , k ) into the posterior ( equation 123 ) , we get equation ( 123 ) which is of the same form as the exponentiated negative quality ( equation 123 ) :
( yi f ( xi ) ) 123 ! exp ( cid : 123 )
123 h f , fih k ( cid : 123 ) exp ( cid : 123 )
123hk , k123ih ( cid : 123 ) .
in a nutshell , we assume that the covariance function of the gp k , is distributed according to a wishart distribution .
in other words , we have two nested processes , a gaussian and a wishart pro - cess , to model the data generation scheme .
hence we are studying a mixture of gaussian processes .
note that the maximum likelihood ( ml - ii ) estimator ( mackay , 123 , williams and barber , 123 , williams and rasmussen , 123 ) in bayesian estimation leads to the same optimization problems as those arising from minimizing the regularized quality functional .
having introduced the theoretical basis of the hyper - rkhs , it is natural to ask whether hyperker - nels , k , exist which satisfy the conditions of denition 123
we address this question by giving a set of general recipes for building such kernels .
123 power series construction suppose k is a kernel such that k ( x , x ) 123 for all x , x x , and suppose g : r r is a function with positive taylor expansion coefcients , that is g ( x ) = ( cid : 123 ) i for basis functions x , ci > 123 for all i = 123 ,
, and convergence radius r .
then for pointwise positive k ( x , x ) r ,
k ( x , x ) : = g ( k ( x ) k ( x ) ) =
is a hyperkernel .
for k to be a hyperkernel , we need to check that rst , k is a kernel , and second , for any xed pair of elements of the input data , x , the function k ( x , ( x , x ) ) is a kernel , and third that is satises the symmetry condition .
here , the symmetry condition follows from the symmetry of k .
to see this , observe that for any xed x , k ( x , ( x , x ) ) is a sum of kernel functions , hence it is a kernel itself ( since kp ( x , x ) is a kernel if k is , for p n ) .
to show that k is a kernel , note that k ( x , x ) = hf ( x ) , f ( x ) i , where f ( x ) : = ( c123 , c123k123 ( x ) , c123k123 ( x ) , .
note that we require pointwise positivity , so that the coefcients of the sum in equation ( 123 ) are always positive .
the gaussian rbf kernel satises this condition , but polynomial kernels of odd degree are not always pointwise positive .
in the following example , we use the gaussian kernel to construct a hyperkernel .
example 123 ( harmonic hyperkernel ) suppose k is a kernel with range ( 123 , 123 ) , ( rbf kernels satisfy this property ) , and set ci : = ( 123 l h ) l
k ( x , x ) = ( 123 l h )
h , i n , for some 123 < l h < 123
then we have
123 l hk ( x ) k ( x )
for k ( x , x ) = exp ( s 123kx xk123 ) this construction leads to
k ( ( x , x ) , ( x , x ) ) =
123 l h exp ( s 123 ( kx xk123 +kx xk123 ) )
as one can see , for l h 123 , k converges to d x , x , and thus kkk123 k on x x .
it is straightforward to nd other hyperkernels of this sort , simply by consulting tables on power series of functions .
table 123 contains a short list of suitable expansions .
h converges to the frobenius norm of
ln ( 123 x )
x 123 + 123
x 123 + .
+ 123 x 123 +
x n + .
x ( 123n+123 ) +
x ( 123n ) +
power series expansion 123 + 123 x + 123 x 123 + 123 123 + 123 123 + .
+ 123 +
x 123 + 123 x 123 +
123n+123 +
radius of convergence
table 123 : hyperkernels by power series construction .
however , if we want the kernel to adapt automatically to different widths for each dimension , we need to perform the summation that led to ( 123 ) for each dimension in its arguments sepa - rately .
such a hyperkernel corresponds to ideas developed in automatic relevance determination ( ard ) ( mackay , 123 , neal , 123 ) .
example 123 ( hyperkernel for ard ) let ks ( x , x ) = exp ( ds ( x , x ) ) , where ds ( x , x ) = ( xx ) s ( x x ) , and s j j separately
is a diagonal covariance matrix .
take sums over each diagonal entry s
k ( ( x , x ) , ( x , x ) ) = ( 123 l h )
i=123 ( cid : 123 ) l hks ( x , x ) ks ( x , x ) ( cid : 123 ) i
123 l h exp ( cid : 123 ) s
123 l h j ( ( x j xj ) 123 + ( xj xj ) 123 ) ( cid : 123 )
( 123 ) holds since k ( x ) factorizes into its coordinates .
a similar denition also allows us to use a distance metric d ( x , x ) which is a generalized radial distance as dened by haussler ( 123 ) .
123 hyperkernels invariant to translation
another approach to constructing hyperkernels is via an extension of a result due to smola et al .
( 123 ) concerning the fourier transform of translation invariant kernels .
theorem 123 ( translation invariant hyperkernel ) suppose k ( ( x123x123 ) , ( x123x123 ) ) is a function which depends on its arguments only via x123 x123 and x123 x123
let f 123k ( w , ( x123 x123 ) ) denote the fourier transform with respect to ( x123 x123 ) .
ong , smola and williamson
the function k is a hyperkernel if k ( t , t
) is a kernel in t , t
and f 123k ( w
x ) and w proof from ( smola et al . , 123 ) we know that for k to be a kernel in one of its arguments , its fourier transform has to be nonnegative .
this yields the second condition .
next , we need to show that k is a kernel in its own right .
mercers condition requires that for arbitrary f the following is
, ( xx ) ) 123 for all ( x
r f ( x123 , x123 ) f ( x123 , x123 ) k ( ( x123 x123 ) , ( x123 x123 ) ) dx123dx123dx123dx123
r f ( t 123 + x123 , x123 ) f ( t 123 + x123 , x123 ) dx123 , 123k ( t 123 , t 123 ) dt 123dt 123
r g ( t 123 ) g ( t 123 ) k ( t 123 , t 123 ) dt 123dt 123 ,
where t 123 = x123 x123 and t 123 = x123 x123
here g is obtained by integration over x123 and x123 respectively .
the latter is exactly mercers condition on k , when viewed as a function of two variables only .
this means that we can check whether a radial basis function ( for example gaussian rbf , exponen - tial rbf , damped harmonic oscillator , generalized bn spline ) , can be used to construct a hyperkernel by checking whether its fourier transform is positive .
123 explicit expansion
if we have a nite set of kernels that we want to choose from , we can generate a hyperkernel which is a nite sum of possible kernel functions .
this setting is similar to that of lanckriet et al .
( 123 ) .
suppose ki ( x , x ) is a kernel for each i = 123 , .
, n ( for example the rbf kernel or the polynomial
k ( x , x ) : =
ciki ( x ) ki ( x ) , ki ( x ) > 123 , x
is a hyperkernel , as can be seen by an argument similar to that of section 123 .
k is a kernel since
k ( x , x ) = hf ( x ) , f ( x ) i , where f ( x ) : = ( c123k123 ( x ) , c123k123 ( x ) , .
, cnkn ( x ) ) .
example 123 ( polynomial and rbf combination ) let k123 ( x , x ) = ( hx , xi + b ) 123p for some choice of b r+ and p n , and k123 ( x , x ) = exp ( s 123kx xk123 )
k ( ( x123 , x123 ) , ( x123 , x123 ) ) = c123 ( hx123 , x123i + b ) 123p ( hx123 , x123i + b ) 123p
+c123 exp ( s 123kx123 x123k123 ) exp ( s 123kx123 x123k123 )
is a hyperkernel .
optimization problems for regularized risk based quality functionals
we will now consider the optimization of the quality functionals utilizing hyperkernels .
we choose the regularized risk functional as the empirical quality functional; that is we set qemp ( k , x , y ) : = rreg ( f , x , y ) .
it is possible to utilize other quality functionals , such as the kernel target alignment ( example 123 ) .
we focus our attention on the regularized risk functional , which is commonly used in svms .
furthermore , we will only consider positive semidenite kernels .
for a particular loss function l ( xi , yi , f ( xi ) ) , we obtain the regularized quality functional .
l ( xi , yi , f ( xi ) ) +
by the representer theorem ( theorem 123 and corollary 123 ) we can write the regularizers as
quadratic terms .
using the soft margin loss , we obtain
max ( 123 , 123 yi f ( xi ) ) +
a ka +
subject to b > 123
where a rm are the coefcients of the kernel expansion ( 123 ) , and b rm123 are the coefcients of
the hyperkernel expansion ( 123 ) .
for xed k , the problem can be formulated as a constrained minimization problem in f , and subsequently expressed in terms of the lagrange multipliers a .
however , this minimum depends on k , and for efcient minimization we would like to compute the derivatives with respect to k .
the following lemma tells us how ( it is an extension of a result in chapelle et al .
( 123 ) ) :
lemma 123 let x rm and denote by f ( x , q ) , ci : rm r convex functions , where f is parameter - ized by q .
let r ( q ) be the minimum of the following optimization problem ( and denote by x ( q ) its
f ( x , q ) subject to ci ( x ) 123 for all 123 i n .
q r ( q ) = d j
argument of f .
123 f ( x ( q ) , q ) , where j n and d123 denotes the derivative with respect to the second
proof at optimality we have a saddlepoint in the lagrangian
xl ( x , a ) = x f ( x , q ) +
i xci ( x ) = 123
the kuhn - tucker conditions have to hold , and in particular also ( cid : 123 ) n
i > 123 the condition ci ( x ) = 123 and therefore also
q ci ( x ( q ) ) = 123 has to be satised .
q ci ( x ( q ) ) =
furthermore , for all q 123 , since for all a taking higher order derivatives with respect to q yields q # =
i xci ( x ( q ) )
q " n ( cid : 123 )
q ( cid : 123 ) x f ( x , q )
here the last equality follows from ( 123 ) .
next we use
f ( x , q ) =
q ( cid : 123 ) d123 f ( x , q ) + x f ( x , q )
q d123 f ( x , q ) .
repeated application then proves the claim .
instead of directly minimizing equation ( 123 ) , we derive the dual formulation .
using the ap - proach in lanckriet et al .
( 123 ) , the corresponding optimization problems can be expressed as a sdp .
in general , solving a sdp would be take longer than solving a quadratic program ( a traditional svm is a quadratic program ) .
this reects the added cost incurred for optimizing over a class of
semidenite programming ( vandenberghe and boyd , 123 ) is the optimization of a linear ob -
jective function subject to constraints which are linear matrix inequalities and afne equalities .
ong , smola and williamson
denition 123 ( semidenite program ) a semidenite program ( sdp ) is a problem of the form :
subject to f123 +
xifi ( cid : 123 ) 123 and ax = b
where x rp are the decision variables , a rpq , b rp , c rq , and fi rrr are given .
in general , linear constraints ax+a > 123 can be expressed as a semidenite constraint diag ( ax+a ) ( cid : 123 ) 123 , and a convex quadratic constraint ( ax + b ) ( ax + b ) cx d 123 123 can be written as
ax + b
( ax + b ) cx + d ( cid : 123 ) ( cid : 123 ) 123
when t r , we can write the quadratic constraint aaa 123 t as ka 123 quadratic constraints are simpler and faster to implement in a convex solver .
123 ak 123 t .
in practice , linear and we derive the corresponding sdp for equation ( 123 ) .
the following proposition allows us to derive a sdp from a class of general convex programs .
it follows the approach in lanckriet et al .
( 123 ) , with some care taken with schur complements of positive semidenite matrices ( albert , 123 ) , and its proof is omitted for brevity .
proposition 123 ( quadratic minimax ) let m , n , m n , h : rn rmm , c : rn rm , be linear maps .
let a rmm and a rm .
also , let d : rn r and g ( x ) be a function and the further constraints on x .
then the optimization problem 123xh ( x ) x c ( x ) x + d ( x ) h ( x ) ( cid : 123 ) 123 ax + a > 123 g ( x ) ( cid : 123 ) 123
can be rewritten as
123t + ag + d ( x )
( ag c ( x ) )
( ag c ( x ) )
in the sense that the x which solves ( 123 ) also solves ( 123 ) .
specically , when we have the regularized quality functional , d ( x ) is quadratic , and hence we obtain an optimization problem which has a mix of linear , quadratic and semidenite constraints .
corollary 123 let h , c , a and a be as in proposition 123 , and s ( cid : 123 ) 123
then the solution x to the
123xh ( x ) x c ( x ) x + 123 h ( x ) ( cid : 123 ) 123 ax + a > 123 x > 123
can be found by solving the semidenite programming problem
123t + ag
123t + 123 g > 123 x > 123 proof by applying proposition 123 , and introducing an auxiliary variable t which upper bounds the quadratic term of x , the claim is proved .
123 x k123 123 t ( ag c ( x ) )
( ag c ( x ) )
( cid : 123 ) ( cid : 123 ) 123
as we vary e
comparing the objective function in ( 123 ) with ( 123 ) , we observe that h ( x ) and c ( x ) are linear in x .
let x the constraints are still satised , but the objective function scales with e .
since x is the coefent in the hyperkernel expansion , this implies that we have a set of possible kernels which are just scalar multiples of each other .
to avoid this , we add an additional constraint on x which is 123x = c , where c is a constant .
this breaks the scaling freedom of the kernel matrix .
as a side - effect , the numerical stability of the sdp problems improves considerably .
we chose a linear constraint so that it does not add too much overhead to the optimization problem we make one additional simplication of the optimization problem , which is to replace the upper bound of
the squared norm ( ks
123 x k123 123 t ) with and upper bound on the norm ( ks
123 x k 123 t ) .
in our setting , the regularizer for controlling the complexity of the kernel is taken to be the squared norm of the kernel in the hyper - rkhs .
by looking at the constraints of equation ( 123 ) , this
123 x k 123 t ) .
comparing this result to the sdp obtained in is expressed as a bound on the norm ( ks lanckriet et al .
( 123 , theorem 123 ) , we see that the corresponding regularizer in their setting is tr ( k ) = c , where c is a constant .
hence the main difference between the two sdps is the choice of the regularizer for the kernel .
however , the motivations of the two methods are different .
this paper sets out an induction framework for learning the kernel , and for a particular choice of qemp , namely the regularized risk functional , we obtain an sdp which has similarities to the approach of lanckriet et al .
( 123 ) .
on the other hand , they start out with a transduction problem and derive the optimization problem directly .
it is unclear at this point which is the better approach .
from the general framework above ( corollary 123 , we derive several examples of machine learn - ing problems , specically binary classication , regression , and single class ( also known as novelty detection ) problems .
the following examples illustrate our method for simultaneously optimizing over the class of kernels induced by the hyperkernel , as well as the hypothesis class of the machine learning problem .
we consider machine learning problems based on kernel methods which are de - rived from ( 123 ) .
the derivation is essentially by application of corollary 123 with the two additional
examples of hyperkernel optimization problems in this section , we dene the following notation .
for p , q , r rn , n n let r = p q be dened as element by element multiplication , ri = pi qi ( the hadamard product , or the .
operation in matlab ) .
the pseudo - inverse ( also known as the moore - penrose inverse ) of a matrix k is denoted k .
let ~ k be the m123 by 123 vector formed by concatenating the columns of an m by m matrix .
ong , smola and williamson
we dene the hyperkernel gram matrix k by putting together m123 of these vectors , that is we set p , q=123
other notations include : the kernel matrix k = reshape ( kb ) ( reshaping a m123 by 123 k = ( ~ kpq ) m , to a m by m matrix ) , y = diag ( y ) ( a matrix with y on the diagonal and zero everywhere else ) , g ( b ) = y ky ( the dependence on b is made explicit ) , i the identity matrix , 123 a vector of ones and 123mm a matrix of ones .
let w be the weight vector and boffset the bias term in feature space , that is the hypothesis function in feature space is dened as g ( x ) = wf ( x ) + boffset where f ( ) is the feature mapping dened by the kernel function k .
the number of training examples is assumed to be m , that is xtrain = ( x123 , .
, xm ) and ytrain = y = ( y123 , .
where appropriate , g and c are lagrange multipliers , while h and x are vectors of lagrange multipliers from the derivation of the wolfe dual for the sdp , b are the hyperkernel coefcients , t123 and t123 are the auxiliary variables .
when h rm , we dene h > 123 to mean that each i > 123 for i = 123 , .
we derive the corresponding sdp for the case when qemp is a c - svm ( example 123 ) .
derivations
of the other examples follow the same reasoning , and are omitted .
example 123 ( linear svm ( c - parameterization ) ) a commonly used support vector classier , the c - svm ( bennett and mangasarian , 123 , cortes and vapnik , 123 ) uses an 123 soft margin , l ( xi , yi , f ( xi ) ) = max ( 123 , 123 yi f ( xi ) ) , which allows errors on the training set .
the parameter c is given by the user .
setting the quality functional qemp ( k , x , y ) = min fh
i=123 l ( xi , yi , f ( xi ) ) + 123
subject to yi f ( xi ) > 123 z
recall the dual form of the c - svm ,
jyiy jk ( xi , x j )
iyi = 123
i 123 c m for all i = 123 ,
by considering the optimization problem dependent on f in ( 123 ) , we can use the derivation of h = b kb due to the the dual problem of the standard c - svm .
observe that we can rewrite kkk123 representer theorem for hyperkernels .
substituting the dual c - svm problem into ( 123 ) , we get the following matrix equation ,
a g ( b ) a +
subject to a y = 123 123 123 a 123 c b > 123
, q = b
this is of the quadratic form of corollary 123 where x = a
s = cl qk , the constraints are a = ( cid : 123 ) y y
corollary 123 , we obtain the corresponding sdp .
i i ( cid : 123 ) and a = ( cid : 123 ) 123 123 123
, h ( q ) = g ( b ) , c ( q ) = 123 , m123 ( cid : 123 ) .
applying
the proof of proposition 123 uses the lagrange method .
as an illustration of how this proof
proceeds , we derive it for this special case of the c - svm .
the lagrangian associated with ( 123 ) is
, b , g , h
, x ) = 123a
a g ( b ) a +
b kb + g ya + h a x ( a
where b > 123 , h > 123 , x > 123
the minimum is achieved at
and the corresponding dual optimization problem is
a = g ( b ) ( g y + 123 + h x ) ,
b , g , h
zg ( b ) z +
b kb ,
where z = g y + 123 + h x .
from this point , we replace the quadratic terms with auxiliary variables t123 and t123 , and apply the schur complement lemma ( albert , 123 ) .
the resulting sdp after replacing
123 b k 123 t123 , and introducing the scale breaking constraint 123b = 123 is
123 b k123 123 t123 by kk 123
b , g , h
123t123 + c
subject to h > 123 , x > 123 , b > 123 123 b k 123 t123 , 123b = 123 ( cid : 123 ) g ( b ) t123 ( cid : 123 ) ( cid : 123 ) 123
, which optimizes the corresponding la -
note that the value of the support vector coefcients , a grange function is g ( b ) z , and the classication function , f = sign ( k ( a y ) boffset ) , is given by f = sign ( kg ( b ) ( y z ) g ) .
example 123 ( linear svm ( n - parameterization ) ) an alternative parameterization of the 123 soft margin was introduced by scholkopf et al .
( 123 ) , where the user dened parameter n ( 123 , 123 ) con - trols the fraction of margin errors and support vectors .
using n - svm as qemp , that is , for a given i > 123 for all , qemp ( k , x , y ) = min fh i = 123 , .
, m , the corresponding sdp is given by
subject to yi f ( xi ) > r z
n + x 123
i and z
b , g , h subject to c > 123 , h > 123 , x > 123 , b > 123
123 b k 123 t123 , 123b = 123 ( cid : 123 ) g ( b ) t123 ( cid : 123 ) ( cid : 123 ) 123
the value of a which optimizes the corresponding lagrange function is g ( b ) z , and the classi -
where z = g y + c 123 + h x .
cation function , f = sign ( k ( a y ) boffset ) , is given by f = sign ( kg ( b ) ( y z ) g ) .
ong , smola and williamson
example 123 ( quadratic svm or lagrangian svm ) instead of using an 123 loss class , mangasar - ian and musicant ( 123 ) use an 123 loss class ,
l ( xi , yi , f ( xi ) ) = ( cid : 123 ) 123
( 123 yi f ( xi ) ) 123
if yi f ( xi ) > 123
and regularized the weight vector as well as the bias term .
the empirical quality functional derived from this is qemp ( k , x , y ) = min fh i > 123 for all i = 123 , .
the resulting dual svm problem has fewer constraints , as is evidenced by the smaller number of lagrange multipliers needed in the corresponding sdp below .
offset ) subject to yi f ( xi ) > 123 z
h + b123
subject to h > 123 , b > 123
123 b k 123 t123 , 123b = 123 ( h + 123 ) ( cid : 123 ) h ( b ) ( h + 123 )
( cid : 123 ) ( cid : 123 ) 123
where h ( b ) = y ( k + 123mm + l mi ) y , and z = g 123 + h x .
the value of a which optimizes the corresponding lagrange function is h ( b ) ( h + 123 ) , and the f = sign ( k ( a y ) boffset ) , is given by f = sign ( kh ( b ) ( ( h + 123 ) y ) +
y ( h ( b ) ( h + 123 ) ) ) .
example 123 ( single class svm or novelty detection ) for unsupervised learning , the single class svm computes a function which captures regions in input space where the probability density is in some sense large ( scholkopf et al . , 123 ) .
a suitable quality functional qemp ( k , x , y ) = i > 123 for all i = 123 , .
, m , and r > 123
the corresponding sdp for this problem is
h r subject to f ( xi ) > r z
i , and z
n m g + subject to h > 123 , x > 123 , b > 123
123t123 + x 123
b , g , h
123 b k 123 t123 ( cid : 123 ) k z z t123 ( cid : 123 ) ( cid : 123 ) 123
data to be classied as novel .
where z = g 123 + h x , and n ( 123 , 123 ) is a user selected parameter controlling the proportion of the the score to be used for novelty detection is given by f = ka boffset , which reduces to f =
h x , by substituting a = k ( g 123 + h x ) , boffset = g 123 and k = reshape ( kb ) .
example 123 ( n - regression ) we derive the sdp for n regression ( scholkopf et al . , 123 ) , which insensitive tube for regression .
as in the n - svm case in example 123 , automatically selects the e controls the fraction of errors and support vectors .
using the e - the user dened parameter n insensitive loss , l ( xi , yi , f ( xi ) ) = max ( 123 , |yi f ( xi ) |e ) , and the n - parameterized quality functional , qemp ( k , x , y ) = min fh c ( cid : 123 ) n i , yi f ( xi ) 123 e z i ,
i + z i ) ( cid : 123 ) subject to f ( xi ) yi 123 e z
i > 123 for all i = 123 , .
, m and e > 123 , the corresponding sdp is
b , g , h subject to c > 123 , h > 123 , x > 123 , b > 123
l + x 123
123 b k 123 t123 , 123b = stddev ( ytrain ) ( cid : 123 ) f ( b ) 123 ( cid : 123 ) + h x c ( cid : 123 ) 123
t123 ( cid : 123 ) ( cid : 123 ) 123 123 ( cid : 123 ) and f ( b ) = ( cid : 123 ) k k
y ( cid : 123 ) g ( cid : 123 ) 123
where z = ( cid : 123 ) y the lagrange function is minimized for a = f ( b ) z , and substituting into f = ka boffset , we obtain the regression function f = ( cid : 123 ) k k ( cid : 123 ) f ( b ) z g .
example 123 ( kernel target alignment ) for the kernel target alignment approach ( cristianini et al . , 123 ) , qemp = tr ( kyy ) = yky , we directly minimize the regularized quality functional , obtaining the following optimization problem ( lanckriet et al . , 123 ) ,
subject to b > 123 123 b k 123 t123 , 123b = 123 ( cid : 123 ) k y y t123 ( cid : 123 ) ( cid : 123 ) 123
note that for the case of kernel target alignment , qemp does not provide a direct formulation for the hypothesis function , but instead , it determines a kernel matrix k .
this kernel matrix , k , can be utilized in a traditional svm , to obtain a classication function .
in the following experiments , we use data from the uci repository .
where the data attributes are numerical , we did not perform any preprocessing of the data .
boolean attributes are converted to ( 123 , 123 ) , and categorical attributes are arbitrarily assigned an order , and numbered ( 123 , 123 , .
the optimization problems in section 123 were solved with an approximate hyperkernel matrix as de - scribed in section 123 .
the sdps were solved using sedumi ( sturm , 123 ) , and yalmip ( lofberg , 123 ) was used to convert the equations into standard form .
we used the hyperkernel for automatic relevance determination dened by ( 123 ) for the hyperkernel optimization problems .
the scaling freedom that ( 123 ) provides for each dimension means we do not have to normalize data to some
for the classication and regression experiments , the datasets were split into 123 random permu - tations of 123% training data and 123% test data .
we deliberately did not attempt to tune parameters and instead made the following choices uniformly for all datasets in classication , regression and
the kernel width s i , for each dimension , was set to 123 times the 123% quantile of the value of |xi x j| over the training data .
this ensures sufcient coverage without having too wide a kernel .
this value was estimated from a 123% random sampling of the training data .
ong , smola and williamson
this has commonly been reported to yield good results .
l was adjusted so that l m = 123 ( that is c = 123 in the vapnik - style parameterization of n = 123 for classication and regression .
while this is clearly suboptimal for many datasets , we decided to choose it beforehand to avoid having to change any parameter .
clearly we could use previous reports on generalization performance to set n to this value for better performance .
for novelty detection , n = 123 ( see section 123 for details ) .
l h for the harmonic hyperkernel was chosen to be 123 , giving adequate coverage over various kernel widths in ( 123 ) ( small l h emphasizes wide kernels almost exclusively , l h close to 123 will treat all widths equally ) .
the hyperkernel regularization constant was set to l q = 123
for the scale breaking constraint 123b = c , c was set to 123 for classication as the hypothesis class only involves the sign of the trained function , and therefore is scale free .
however , for regression , c : = stddev ( ytrain ) ( the standard deviation of the training labels ) so that the hyperkernel coefcients are of the same scale as the output ( the constant offset boffset takes care of the mean ) .
in the following experiments , the hypothesis function is computed using the variables of the sdp .
in certain cases , numerical problems in the sdp optimizer or in the pseudo - inverse may pre - vent this hypothesis from optimizing the regularized risk for the particular kernel matrix .
in this case , one can use the kernel matrix k from the sdp and obtain the hypothesis function via a stan -
123 low rank approximation
although the optimization of ( 123 ) has reduced the problem of optimizing over two possibly in - nite dimensional hilbert spaces to a nite problem , it is still formidable in practice as there are m123 coefcients for b .
for an explicit expansion of type ( 123 ) one can optimize in the expansion coef - cients ki ( x ) ki ( x ) directly , which leads to a quality functional with an 123 penalty on the expansion coefcients .
such an approach is appropriate if there are few terms in ( 123 ) .
in the general case ( or if the explicit expansion has many terms ) , one can use a low - rank approx - imation , as described by fine and scheinberg ( 123 ) and zhang ( 123 ) .
this entails picking from
( cid : 123 ) k ( ( xi , x j ) , ) |123 i , j m123 ( cid : 123 ) a small fraction of terms , p ( where m123 p ) , which approximate k on xtrain xtrain sufciently well .
in particular , we choose an m p truncated lower triangular matrix g such that kpkpggkf 123 d , where p is the permutation matrix which sorts the eigenvalues of k into decreasing order , and d is the level of approximation needed .
the norm , kkf is the frobenius norm .
in the following experiments , the hyperkernel matrix was approximated to d = 123 using the incomplete cholesky factorization method ( bach and jordan , 123 ) .
123 classication experiments several binary classication datasets123 from the uci repository were used for the experiments .
a set of synthetic data ( labeled syndata in the results ) sampled from two gaussians was created to illustrate the scaling freedom between dimensions .
the rst dimension had a standard deviation of 123 whereas the second dimension had a standard deviation of 123 ( a sample result is shown in figure 123 ) .
the results of the experiments are shown in table 123
we classied window vs .
non - window for glass data , the other datasets are all binary .
from table 123 , we observe that our method achieves state of the art results for all the datasets , except the heart dataset .
we also achieve results much better than previously reported for the credit dataset .
comparing the results for c - svm and tuned svm , we observe that our method is always equally good , or better than a c - svm tuned using 123 - fold cross validation .
n - svm lag - svm best other cv tuned svm ( c )
table 123 : hyperkernel classication : test error and standard deviation in percent .
the second , third and fourth columns show the results of the hyperkernel optimizations of c - svm ( exam - ple 123 ) , n - svm ( example 123 ) and lagrangian svm ( example 123 ) respectively .
the results in the fth column shows the best results from ( freund and schapire , 123 , ratsch et al . , 123 , meyer et al . , 123 ) .
the rightmost column shows a c - svm tuned in the traditional way .
a gaussian rbf kernel was tuned using 123 - fold cross validation on the training data , with the best value of c shown in brackets .
a grid search was performed on ( c , s ) .
the values of c tested were ( 123 , 123 , .
the values of the kernel width , s were between 123% and 123% quantile of the distance between a pair of sample of points in the data .
these quantiles were estimated by a random sample of 123% of the training data .
123 effect of l q and l h on classication error in order to investigate the effect of varying the hyperkernel regularization constant , l q , and the harmonic hyperkernel parameter , l h , we performed experiments using the c - svm hyperkernel optimization ( example 123 ) .
we performed two sets of experiments with each of our chosen datasets .
the results shown in table 123
from table 123 , we observe that the variation in classication accuracy over the whole range of the hyperkernel regularization constant , l q is less than the standard deviation of the classica - tion accuracies of the various datasets ( compare with table 123 ) .
this demonstrates that our method is quite insensitive to the regularization parameter over the range of values tested for the various
the method shows a higher sensitivity to the harmonic hyperkernel parameter .
since this pa - rameter effectively selects the scale of the problem , by selecting the width of the kernel , it is to be expected that each dataset would have a different ideal value of l h .
it is to be noted that the generalization accuracy at l h = 123 is within one standard deviation ( see table 123 and 123 ) of the best accuracy achieved over the whole range tested .
ong , smola and williamson
table 123 : effect of varying l h and l q on classication error .
in the left experiment , we xed l q = 123 , and l h was varied with the values l h = ( 123 , 123 , .
, 123 , 123 , 123 , 123 , 123 ) . in the right , we set l h = 123 and varied l q = ( 123 , 123 , .
the error columns ( columns
123 and 123 ) report the average error on the test set and the standard deviation of the error over the different parameter settings .
the deviation columns ( columns 123 and 123 ) report the average standard deviation over 123 random 123% / 123% splits .
123 computational time
one of the concerns of an sdp optimization problem is the computational complexity .
of performing worst case analysis of computational complexity , we perform an empirical test to investigate the scaling behaviour of the proposed method .
the total computation time for the rst 123 splits of the data was measured , and the average time taken for each split was computed and plotted on a log scale plot in figure 123
the slope of the graph demonstrates that we have an approximately cubic scaling in computational time .
123 regression experiments
in order to demonstrate that we can solve problems other than binary classication using the same framework , we performed some experiments using regression and novelty detection datasets .
the results of the regression experiments are shown in table 123
we used the same parameter settings as in the previous section .
comparing the second and fourth columns , we observe that the hyperkernel optimization prob - lem performs better than a e - svr tuned using cross validation for all the datasets except the servo dataset .
meyer et al .
( 123 ) used a 123% / 123% split of the data for their experiments , while we used a 123% / 123% split , which may account for the better performance in the cpu and servo datasets .
the reason for the much better rate on the auto imports dataset remains a mystery .
123 novelty detection
we applied the single class support vector machine to detect outliers in the usps data .
the test set of the default split in the usps database was used in the following experiments .
the parameter n was set to 123 for these experiments , hence selecting up to 123% of the data as outliers .
figure 123 : a log scale plot of computational time ( in seconds ) , measured using matlabs cputime , against the number of examples in the respective datasets .
the slope of the least squares t through the points are 123 , 123 and 123 for c - svm ( example 123 ) , n - svm ( exam - ple 123 ) and lag - svm ( example 123 ) respectively , demonstrating that the algorithms have approximately cubic scaling .
n - svr best other cv tuned e - svr
table 123 : hyperkernel regression : mean squared error .
the second column shows the results from the hyperkernel optimization of the n - regression ( example ( 123 ) ) .
the results in the third column shows the best results from ( meyer et al . , 123 ) .
the rightmost column shows a e - svr with a gaussian kernel tuned using 123 - fold cross validation on the training data .
similar to the classication setting , grid search was performed on ( c , s ) .
the values of c tested were ( 123 , 123 , .
the values of the kernel width , s , tested were between the 123% and 123% quantiles of the distance between a pair of sample of points in the data .
these quantiles were estimated by a random 123% sample of the training data .
ong , smola and williamson
figure 123 : top rows : images of digits 123 and 123 , considered novel by algorithm; bottom : typical
images of digits 123 and 123
since there is no quantitative method for measuring the performance of novelty detection , we cannot directly compare our results with the traditional single class svm .
we can only subjectively conclude , by visually inspecting a sample of the digits , that our approach works for novelty detection of usps digits .
figure 123 shows a sample of the digits .
we can see that the algorithm identies novel digits , such as in the top two rows of figure 123
the bottom two rows shows a sample of digits which have been deemed to be common .
summary and outlook
the regularized quality functional allows the systematic solution of problems associated with the choice of a kernel .
quality criteria that can be used include kernel target alignment , regularized risk and the log posterior .
the regularization implicit in our approach allows the control of overt - ting that occurs if one optimizes over a too large a choice of kernels .
we have shown that when the empirical quality functional is the regularized risk functional , the resulting optimization problem is convex , and in fact is a sdp .
this sdp , which learns the best kernel given the data , has a bayesian interpretation in terms of a hierarchical gaussian process .
we dene more general kernels which may have many free parameters , and optimize over them without overtting .
the experimental results on classication demonstrate that it is possible to achieve state of the art performance using our approach with no manual tuning .
furthermore , the same framework and parameter settings work for various data sets as well as regression and novelty detection .
this approach makes support vector based estimation approaches more automated .
parameter adjustment is less critical compared to when the kernel is xed , or hand tuned .
future work will fo - cus on deriving improved statistical guarantees for estimates derived via hyperkernels which match the good empirical performance .
the authors would like to thank stephane canu , laurent el ghaoui , michael jordan , john lloyd , daniela pucci de farias , matthias seeger , grace wahba and the referees for their helpful com - ments and suggestions .
the authors also thank alexandros karatzoglou for his help with svlab .
national ict australia is funded through the australian governments backing australias ability initiative , in part through the australian research council .
