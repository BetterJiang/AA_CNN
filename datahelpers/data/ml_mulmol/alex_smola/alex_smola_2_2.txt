as a fundamental problem in pattern recognition , graph matching has applications in a variety of elds , from com - puter vision to computational biology .
in graph match - ing , patterns are modeled as graphs and pattern recognition amounts to nding a correspondence between the nodes of dierent graphs .
many formulations of this problem can be cast in general as a quadratic assignment problem , where a linear term in the objective function encodes node compati - bility and a quadratic term encodes edge compatibility .
the main research focus in this theme is about designing ecient algorithms for approximately solving the quadratic assign - ment problem , since it is np - hard .
in this paper we turn our attention to a dierent question : how to estimate compati - bility functions such that the solution of the resulting graph matching problem best matches the expected solution that a human would manually provide .
we present a method for learning graph matching : the training examples are pairs of graphs and the labels are matches between them .
our experimental results reveal that learning can substantially improve the performance of standard graph matching algo - rithms .
in particular , we nd that simple linear assignment with such a learning scheme outperforms graduated as - signment with bistochastic normalisation , a state - of - the - art quadratic assignment relaxation algorithm .
graphs are commonly used as abstract representations for complex structures , including dna sequences , documents , text , and images .
in particular they are extensively used in the eld of computer vision , where many problems can be formulated as an attributed graph matching problem .
here the nodes of the graphs correspond to local features of the image and edges correspond to relational aspects between features ( both nodes and edges can be attributed , i . e .
they can encode feature vectors ) .
graph matching then consists of nding a correspondence between nodes of the two graphs such that they look most similar when the vertices are labeled according to such a correspondence .
typically , the problem is mathematically formulated as a quadratic assignment problem , which consists of nding the assignment that maximizes an objective function en - tiberio caetano , julian mcauley , li cheng , and alex smola are with the statistical machine learning program at nicta , and the research school of information sciences and engineering , australian national university .
quoc le is with the department of computer science , stanford university .
coding local compatibilities ( a linear term ) and structural compatibilities ( a quadratic term ) .
the main body of re - search in graph matching has then been focused on devising more accurate and / or faster algorithms to solve the prob - lem approximately ( since it is np - hard ) ; the compatibility functions used in graph matching are typically handcrafted .
an interesting question arises in this context : if we are given two attributed graphs to match , g and g ( cid : 123 ) , should the optimal match be uniquely determined ? for example , assume rst that g and g ( cid : 123 ) come from two images acquired by a surveillance camera in an airports lounge; now , as - sume the same g and g ( cid : 123 ) instead come from two images in a photographers image database; should the optimal match be the same in both situations ? if the algorithm takes into account exclusively the graphs to be matched , the optimal solutions will be the same123 since the graph pair is the same in both cases .
this is the standard way graph matching is
in this paper we address what we believe to be a limitation of this approach .
we argue that if we know the conditions under which a pair of graphs has been extracted , then we should take into account how graphs arising in those con - ditions are typically matched .
however , we do not take the information on the conditions explicitly into account , since this would obviously be impractical .
instead , we approach the problem purely from a statistical inference perspective .
first , we extract graphs from a number of images acquired under the same conditions as those for which we want to solve , whatever the word conditions means ( e . g .
from the surveillance camera or the photographers database ) .
we then manually provide what we understand to be the op - timal matches between the resulting graphs .
this informa - tion is then used in a learning algorithm which learns a map from the space of pairs of graphs to the space of matches .
in terms of the quadratic assignment problem , this learn - ing algorithm amounts to ( in loose language ) adjusting the node and edge compatibility functions such that the ex - pected optimal match in a test pair of graphs agrees with the expected match they would have had , had they been in the training set .
in this formulation , the learning prob - lem consists of a convex , quadratic program which is readily solvable by means of a column generation procedure .
we provide experimental evidence that applying learn - ing to standard graph matching algorithms signicantly im - proves their performance .
in fact , we show that learning improves upon non - learning results so dramatically that lin - ear assignment with learning outperforms graduated as -
123assuming there is a single optimal solution and that the algorithm
signment with bistochastic normalisation , a state - of - the - art quadratic assignment relaxation algorithm .
also , by intro - ducing learning in graduated assignment itself , we obtain results that improve both in accuracy and speed over the best existing quadratic assignment relaxations .
a preliminary version of this paper appeared in ( 123 ) .
123 related literature the graph matching literature is extensive , and many dier - ent types of approaches have been proposed , which mainly focus on approximations and heuristics for the quadratic assignment problem .
an incomplete list includes spec - tral methods ( 123 ) , relaxation labeling and probabilistic approaches ( 123 ) , semidenite relaxations ( 123 ) , replicator equations ( 123 ) , tree search ( 123 ) , and graduated assignment ( 123 ) .
spectral methods consist of studying the similarities between the spectra of the adjacency or laplacian matri - ces of the graphs and using them for matching .
relax - ation and probabilistic methods dene a probability dis - tribution over mappings , and optimize using discrete relax - ation algorithms or variants of belief propagation .
semidef - inite relaxations solve a convex relaxation of the original combinatorial problem .
replicator equations draw an anal - ogy with models from biology where an equilibrium state is sought which solves a system of dierential equations on the nodes of the graphs .
tree - search techniques in general have worst case exponential complexity and work via sequential tests of compatibility of local parts of the graphs .
gradu - ated assignment combines the softassign method ( 123 ) with sinkhorns method ( 123 ) and essentially consists of a series of rst - order approximations to the quadratic assignment objective function .
this method is particularly popular in computer vision since it produces accurate results while scaling reasonably in the size of the graph .
the above literature strictly focuses on trying better algo - rithms for approximating a solution for the graph matching problem , but does not address the issue of how to determine the compatibility functions in a principled way .
in ( 123 ) the authors learn compatibility functions for the relaxation labeling process; this is however a dierent prob - lem than graph matching , and the compatibility functions have a dierent meaning .
nevertheless it does provide an initial motivation for learning in the context of matching tasks .
in terms of methodology , the paper most closely re - lated to ours is possibly ( 123 ) , which uses structured estima - tion tools in a quadratic assignment setting for word align - ment .
a recent paper of interest shows that very signicant improvements on the performance of graph matching can be obtained by an appropriate normalization of the compati - bility functions ( 123 ) ; however , no learning is involved .
123 the graph matching problem
the notation used in this paper is summarized in table 123
in the following we denote a graph by g .
we will often refer to a pair of graphs , and the second graph in the pair will be denoted by g ( cid : 123 ) .
we study the general case of attributed graph matching , and attributes of the vertex i and the edge
table 123 : denitions and notation
g - generic graph ( similarly , g ( cid : 123 ) ) ; gi - attribute of node i in g ( similarly , g ( cid : 123 ) i ( cid : 123 ) for g ( cid : 123 ) ) ; gij - attribute of edge ij in g ( similarly , g ( cid : 123 ) i ( cid : 123 ) j ( cid : 123 ) for g ( cid : 123 ) ) ; g - space of graphs ( g g - space of pairs of graphs ) ; x - generic observation : graph pair ( g , g ( cid : 123 ) ) ; x x , space of y - generic label : matching matrix; y y , space of labels; n - index for training instance; n - number of training in - xn - nth training observation : graph pair ( gn , g ( cid : 123 ) n ) ; yn - nth training label : matching matrix; g - predictor function; yw - optimal prediction for g under w; f - discriminant function; - loss function; - joint feature map; 123 - node feature map; 123 - edge feature map; sn - constraint set for training instance n; y - solution of the quadratic assignment problem; y - most violated constraint in column generation; yii ( cid : 123 ) - ith row and i ( cid : 123 ) th column element of y; cii ( cid : 123 ) - value of compatibility function for map i ( cid : 123 ) i ( cid : 123 ) ; dii ( cid : 123 ) jj ( cid : 123 ) - value of compatibility function for map ij ( cid : 123 ) i ( cid : 123 ) j ( cid : 123 ) ; - tolerance for column generation; w123 - node parameter vector; w123 - edge parameter vector; w : = ( w123 w123 ) - joint parameter vector; w w; n - slack variable for training instance n; - regularization function; - regularization parameter; - convergence monitoring threshold in bistochastic nor -
cii ( cid : 123 ) yii ( cid : 123 ) + ( cid : 123 ) i yii ( cid : 123 ) 123 for all i ( cid : 123 ) , ( cid : 123 )
ij in g are denoted by gi and gij respectively .
standard graphs are obtained if the node attributes are empty and the edge attributes gij ( 123 , 123 ) are binary denoting the absence or presence of an edge , in which case we get the so - called exact graph matching problem .
dene a matching matrix y by yii ( cid : 123 ) ( 123 , 123 ) such that yii ( cid : 123 ) = 123 if node i in the rst graph maps to node i ( cid : 123 ) in the second graph ( i ( cid : 123 ) i ( cid : 123 ) ) and yii ( cid : 123 ) = 123 otherwise .
dene by cii ( cid : 123 ) the value of the compatibility function for the unary assignment i ( cid : 123 ) i ( cid : 123 ) and by dii ( cid : 123 ) jj ( cid : 123 ) the value of the compati - bility function for the pairwise assignment ij ( cid : 123 ) i ( cid : 123 ) j ( cid : 123 ) .
then , a generic formulation of the graph matching problem con - sists of nding the optimal matching matrix y given by the solution of the following ( np - hard ) quadratic assignment
y = argmax
i ( cid : 123 ) ) and ( gij , g ( cid : 123 )
to - one , that is ( cid : 123 ) ( many - to - one , that is ( cid : 123 )
typically subject to either the injectivity constraint ( one - i ( cid : 123 ) yii ( cid : 123 ) 123 for all i ) or simply the constraint that the map should be a function i ( cid : 123 ) yii ( cid : 123 ) = 123 for all i ) .
if dii ( cid : 123 ) jj ( cid : 123 ) = 123 for all ii ( cid : 123 ) jj ( cid : 123 ) then ( 123 ) becomes a linear assignment problem , exactly solvable in worst case cubic time ( 123 ) .
although the compatibility functions c and d obviously depend on the at - tributes ( gi , g ( cid : 123 ) i ( cid : 123 ) j ( cid : 123 ) ) , the functional form of this dependency is typically assumed to be xed in graph matching .
this is precisely the restriction we are going to relax in this paper : both the functions c and d will be parametrized by vectors whose coecients will be learned within a convex optimization framework .
in a way , instead of proposing yet another algorithm for determining how to approximate the solution for ( 123 ) , we are aiming at nding a way to determine what should be maximized in ( 123 ) , since dierent c and d will produce dierent criteria to be maxi -
123 learning graph matching
123 general problem setting
we approach the problem of learning the compatibility func - tions for graph matching as a supervised learning problem ( 123 ) .
the training set comprises n observations x from an input set x , n corresponding labels y from an output set y , and can be represented by ( ( x123; y123 ) , .
, ( xn ; yn ) ) .
critical in our setting is the fact that the observations and labels are structured objects .
in typical supervised learning scenarios , observations are vectors and labels are elements from some discrete set of small cardinality , for example yn ( 123 , 123 ) in the case of binary classication .
however , in our case an observation xn is a pair of graphs , i . e .
xn = ( gn , g ( cid : 123 ) n ) , and the label yn is a match between graphs , represented by a matching matrix as dened in section 123
if x = gg is the space of pairs of graphs , y is the space of matching matrices , and w is the space of parameters of our model , then learning graph matching amounts to estimating
a function g : ggw ( cid : 123 ) y which minimizes the prediction loss on the test set .
since the test set here is assumed not to be available at training time , we use the standard approach of minimizing the empirical risk ( average loss in the training set ) plus a regularization term in order to avoid overtting .
the optimal predictor will then be the one which minimizes an expression of the following type :
( g ( gn , g ( cid : 123 ) n; w ) , yn )
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
where ( g ( gn , g ( cid : 123 ) n; w ) , yn ) is the loss incurred by the pre - dictor g when predicting , for training input ( gn , g ( cid : 123 ) n ) , the output g ( gn , g ( cid : 123 ) n; w ) instead of the training output yn .
the function ( w ) penalizes complex vectors w , and is a pa - rameter that trades o data tting against generalization ability , which is in practice determined using a validation in order to completely specify such an optimization problem , we need to dene the parametrized class of predic - tors g ( g , g ( cid : 123 ) ; w ) whose parameters w we will optimize over , the loss function and the regularization term ( w ) .
in the following we will focus on setting up the optimization problem by addressing each of these points .
123 the model
we start by specifying a w - parametrized class of predictors g ( g , g ( cid : 123 ) ; w ) .
we use the standard approach of discriminant functions , which consists of picking as our optimal estimate the one for which the discriminant function f ( g , g ( cid : 123 ) , y; w ) i . e .
g ( g , g ( cid : 123 ) ; w ) = argmaxy f ( g , g ( cid : 123 ) , y; w ) .
we assume linear discriminant functions f ( g , g ( cid : 123 ) , y; w ) = ( cid : 123 ) w , ( g , g ( cid : 123 ) , y ) ( cid : 123 ) , so that our predictor has the form
g ( g , g ( cid : 123 ) , w ) = argmax
( cid : 123 ) w , ( g , g ( cid : 123 ) , y ) ( cid : 123 ) .
eectively we are solving an inverse optimization problem , as described by ( 123 , 123 ) , that is , we are trying to nd f such that g has desirable properties .
further specication of g ( g , g ( cid : 123 ) ; w ) requires determining the joint feature map ( g , g ( cid : 123 ) , y ) , which has to encode the properties of both graphs as well as the properties of a match y between these graphs .
the key observation here is that we can relate the quadratic assignment formulation of graph matching , given by ( 123 ) , with the predictor given by ( 123 ) , and interpret the solution of the graph matching problem as being the esti - mate of g , i . e .
yw = g ( g , g ( cid : 123 ) ; w ) .
this allows us to interpret the discriminant function in ( 123 ) as the objective function to be maximized in ( 123 ) :
( cid : 123 ) ( g , g ( cid : 123 ) , y ) , w ( cid : 123 ) = ( cid : 123 )
cii ( cid : 123 ) yii ( cid : 123 ) + ( cid : 123 )
this clearly reveals that the graphs and the parameters must be encoded in the compatibility functions .
the last step before obtaining consists of choosing a parametriza - tion for the compatibility functions .
we assume a simple
cii ( cid : 123 ) = ( cid : 123 ) 123 ( gi , g ( cid : 123 )
dii ( cid : 123 ) jj ( cid : 123 ) = ( cid : 123 ) 123 ( gij , g ( cid : 123 )
the compatibility functions are linearly dependent on the parameters , and on new feature maps 123 and 123 that only involve the graphs ( section 123 species the feature maps 123 and 123 ) .
as already dened , gi is the attribute of node i and gij is the attribute of edge ij ( similarly for g ( cid : 123 ) ) .
how - ever , we stress here that these are not necessarily local at - tributes , but are arbitrary features simply indexed by the nodes and edges . 123 for instance , we will see in section 123 an example where gi encodes the graph structure of g as seen from node i , or from the perspective of node i .
note that the traditional way in which graph matching is approached arises as a particular case of equations ( 123 ) : if w123 and w123 are constants , then cii ( cid : 123 ) and dii ( cid : 123 ) jj ( cid : 123 ) depend only on the features of the graphs .
by dening w : = ( w123 w123 ) , we arrive at the nal form for ( g , g ( cid : 123 ) , y ) from ( 123 ) and ( 123 ) :
( g , g ( cid : 123 ) , y ) =
naturally , the nal specication of the predictor g depends on the choices of 123 and 123
since our experiments are con - centrated on the computer vision domain , we use typical computer vision features ( e . g .
shape context ) for construct - ing 123 and a simple edge - match criterion for constructing 123 ( details follow in section 123 ) .
123 the loss next we dene the loss ( y , yn ) incurred by estimating the matching matrix y instead of the correct one , yn .
when both graphs have large sizes , we dene this as the fraction of mismatches between matrices y and yn , i . e .
( y , yn ) = 123 123
( where ( cid : 123 ) ( cid : 123 ) f is the frobenius norm ) .
if one of the graphs has a small size , this measure may be too rough .
in our ex - periments we will encounter such a situation in the context of matching in images .
in this case , we instead use the loss
( g , g ( cid : 123 ) , ) = 123 123
( cid : 123 ) d ( gi , g ( cid : 123 )
here graph nodes correspond to point sets in the images , g corresponds to the smaller , query graph , and g ( cid : 123 ) is the larger , target graph ( in this expression , gi and g ( cid : 123 ) j are par - ticular points in g and g ( cid : 123 ) ; ( i ) is the index of the point in
123as a result in our general setting node compatibilities and edge compatibilities become somewhat misnomers , being more appropri - ately described as unary and binary compatibilities .
we however stick to the standard terminology for simplicity of exposition .
g ( cid : 123 ) to which the ith point in g should be correctly mapped; d is simply the euclidean distance , and is scaled by , which is simply the width of the image in question ) .
hence we are penalising matches based on how distant they are from the correct match; this is commonly referred to as the endpoint finally , we specify a quadratic regularizer ( w ) =
123 the optimization problem here we combine the elements discussed in 123 in order to formally set up a mathematical optimization problem that corresponds to the learning procedure .
the expression that arises from ( 123 ) by incorporating the specics discussed in 123 / 123 still consists of a very dicult ( in particular non - convex ) optimization problem .
although the regulariza - tion term is convex in the parameters w , the empirical risk , i . e .
the rst term in ( 123 ) , is not .
note that there is a nite number of possible matches y , and therefore a nite num - ber of possible values for the loss ; however , the space of parameters w is continuous .
what this means is that there are large equivalence classes of w ( an equivalence class in this case is a given set of ws each of which produces the same loss ) .
therefore , the loss is piecewise constant on w , and as a result certainly not amenable to any type of smooth
one approach to render the problem of minimizing ( 123 ) more tractable is to replace the empirical risk by a convex upper bound on the empirical risk , an idea that has been exploited in machine learning in recent years ( 123 , 123 , 123 ) .
by minimizing this convex upper bound , we hope to decrease the empirical risk as well .
it is easy to show that the convex ( in particular , linear ) function 123 n n is an upper bound n ( g ( gn , g ( cid : 123 ) n; w ) , yn ) for the solution of ( 123 ) with appropriately chosen constraints :
subject to ( cid : 123 ) w , n ( y ) ( cid : 123 ) ( y , yn ) n
for all n and y y .
here we dene n ( y ) : = ( gn , g ( cid : 123 ) n , yn ) ( gn , g ( cid : 123 ) n , y ) .
formally , we have :
lemma 123 for any feasible ( , w ) of ( 123 ) the inequality n ( g ( gn , g ( cid : 123 ) n; w ) , yn ) holds for all n .
lar , for the optimal solution ( , w ) we have 123
n ( g ( gn , g ( cid : 123 ) n; w ) , yn ) .
satises ( cid : 123 ) w , n ( yw ) ( cid : 123 ) 123
consequently n ( yw
proof the constraint ( 123b ) needs to hold for all y , hence in particular for yw = g ( gn , g ( cid : 123 ) n; w ) .
by construction yw
the second part of the claim follows immediately .
f ( gn , g ( cid : 123 ) n , yn; w ) f ( gn , g ( cid : 123 ) n , y; w ) , the gap be - tween the discriminant functions for yn and y should exceed the loss induced by estimating y instead of the
training matching matrix yn .
this is highly intuitive since it reects the fact that we want to safeguard ourselves most against mis - predictions y which incur a large loss ( i . e .
the smaller is the loss , the less we should care about making a mis - prediction , so we can enforce a smaller margin ) .
the presence of n in the constraints and in the objective function means that we allow the hard inequality ( without n ) to be violated , but we penalize violations for a given n by adding to the objective function the cost 123
despite the fact that ( 123 ) has exponentially many con - straints ( every possible matching y is a constraint ) , we will see in what follows that there is an ecient way of nding an - approximation to the optimal solution of ( 123 ) by nding the worst violators of the constrained optimization problem .
123 the algorithm note that the number of constraints in ( 123 ) is given by the number of possible matching matrices |y| times the number of training instances n .
in graph matching the number of possible matches between two graphs grows factorially with their size .
in this case it is infeasible to solve ( 123 ) exactly .
there is however a way out of this problem by using an optimization technique known as column generation ( 123 ) .
instead of solving ( 123 ) directly , one computes the most vi - olated constraint in ( 123 ) iteratively for the current solution and adds this constraint to the optimization problem .
in order to do so , we need to solve
( ( cid : 123 ) w , ( gn , g ( cid : 123 ) n , y ) ( cid : 123 ) + ( y , yn ) ) ,
as this is the term for which the constraint ( 123b ) is tightest ( i . e .
the constraint that maximizes n ) .
the resulting algorithm is given in algorithm 123
we use the bundle methods for regularized risk minimization ( bmrm ) solver of ( 123 ) , which merely requires that for each candidate w , we compute the gradient of 123 123 ( cid : 123 ) w ( cid : 123 ) 123 with respect to w , and the loss ( 123 n ( y , yn ) ) ( y is
the most violated constraint in column generation ) .
see ( 123 ) for further details
( cid : 123 ) ( cid : 123 ) w , ( y ) ( cid : 123 ) +
let us investigate the complexity of solving ( 123 ) .
using the joint feature map as in ( 123 ) and the loss as in ( 123 ) , the argument in ( 123 ) becomes
( cid : 123 ) ( g , g ( cid : 123 ) , y ) , w ( cid : 123 ) + ( y , yn ) =
yii ( cid : 123 ) cii ( cid : 123 ) + ( cid : 123 )
yii ( cid : 123 ) yjj ( cid : 123 ) dii ( cid : 123 ) jj ( cid : 123 ) + constant ,
where cii ( cid : 123 ) = ( cid : 123 ) 123 ( gi , g ( cid : 123 ) dened as in ( 123b ) .
i ( cid : 123 ) ) , w123 ( cid : 123 ) + yn
f and dii ( cid : 123 ) jj ( cid : 123 )
the maximization of ( 123 ) , which needs to be carried out at training time , is a quadratic assignment problem , as is the problem to be solved at test time .
in the particular case where dii ( cid : 123 ) jj ( cid : 123 ) = 123 throughout , both the problems at training and at test time are linear assignment problems , which can be solved eciently in worst case cubic time .
in our experiments , we solve the linear assignment prob - lem with the ecient solver from ( 123 ) ( house sequence ) , and the hungarian algorithm ( video / bikes dataset )
algorithm 123 column generation
n ( y ) : = ( gn , g ( cid : 123 ) n , yn ) ( gn , g ( cid : 123 ) n , y ) h n ( y ) : = ( cid : 123 ) w , ( gn , g ( cid : 123 ) n , y ) ( cid : 123 ) + ( y , yn ) input : training graph pairs ( gn ) , ( g ( cid : 123 ) n ) , training match - ing matrices ( yn ) , sample size n , tolerance initialize sn = for all n , and w = 123
get current w from bmrm for n = 123 to n do
y = argmaxyy h n ( y ) compute gradient of ( cid : 123 ) w , ( gn , g ( cid : 123 ) n , y ) ( cid : 123 ) + w . r . t .
w ( = n ( y ) + w ) compute loss ( y , yn )
( cid : 123 ) n and 123 ( cid : 123 ) n is suciently small
n ( y , yn ) to bmrm
quadratic assignment , we developed a c++ implementation of the well - known graduated assignment algorithm ( 123 ) .
however the learning scheme discussed here is indepen - dent of which algorithm we use for solving either linear or quadratic assignment .
note that the estimator is but a mere approximation in the case of quadratic assignment : since we are unable to nd the most violated constraints of ( 123 ) , we cannot be sure that the duality gap is properly minimized in the constrained optimization problem .
123 features for the compatibility
the joint feature map ( g , g ( cid : 123 ) , y ) has been derived in its full generality ( 123 ) , but in order to have a working model we need to choose a specic form for 123 ( gi , g ( cid : 123 ) as mentioned in section 123
we rst discuss the linear fea - tures 123 and then proceed to the quadratic terms 123
for concreteness , here we only discuss options actually used in
i ( cid : 123 ) ) and 123 ( gij , g ( cid : 123 )
123 node features we construct 123 ( gi , g ( cid : 123 ) i ( cid : 123 ) ) using the squared dierence i ( cid : 123 ) ( r ) |123 , .
this diers i ( cid : 123 ) ) = ( .
, |gi ( r ) g ( cid : 123 ) in which an exponential de - from what is shown in ( 123 ) , cay is used ( i . e .
exp ( |gi ( r ) g ( cid : 123 ) i ( cid : 123 ) ( r ) |123 / ) ) ; we found that using the squared dierence resulted in much better per - formance after learning .
here gi ( r ) and g ( cid : 123 ) the rth coordinates of the corresponding attribute vectors .
note that in standard graph matching without learning we typically have cii ( cid : 123 ) = exp ( ( cid : 123 ) gi g ( cid : 123 ) i ( cid : 123 ) ( cid : 123 ) 123 ) , which can be seen as the particular case of ( 123a ) for both 123 and w123 i ( cid : 123 ) ( cid : 123 ) 123 ) , .
) at , given by 123 ( gi , g ( cid : 123 ) and w123 = ( .
) ( 123 ) .
here instead we have cii ( cid : 123 ) = i ( cid : 123 ) ) , w123 ( cid : 123 ) , where w123 is learned from training data .
in this way , by tuning the rth coordinate of w123 accordingly , the learning process nds the relevance of the rth feature
i ( cid : 123 ) ) = ( .
, exp ( ( cid : 123 ) gi g ( cid : 123 )
in our experiments ( to be described in the next sec - tion ) , we use the well - known 123 - dimensional shape con - text features ( 123 ) .
they encode how each node sees the it is an instance of what we called in sec - tion 123 a feature that captures the node perspective with respect to the graph .
we use 123 angular bins ( for an - 123 , 123 ) ) , and 123 radial bins ( for radii in gles in ( 123 , ( 123 , 123 ) , ( 123 , 123 ) .
( 123 , 123 ) , where the radius is scaled by the average of all distances in the scene ) to obtain our 123 features .
this is similar to the setting described in ( 123 ) .
123 edge features for the edge features gij ( g ( cid : 123 ) i . e .
gij ( g ( cid : 123 ) otherwise .
in this case , we set 123 ( gij , g ( cid : 123 ) that w123 is a scalar ) .
i ( cid : 123 ) j ( cid : 123 ) ) , we use standard graphs , i ( cid : 123 ) j ( cid : 123 ) ) is 123 if there is an edge between i and j and 123
i ( cid : 123 ) j ( cid : 123 ) ) = gijg ( cid : 123 )
123 house sequence
for our rst experiment , we consider the cmu house se - quence a dataset consisting of 123 frames of a toy house ( 123 ) .
each frame in this sequence has been hand - labelled , with the same 123 landmarks identied in each frame ( 123 ) .
we explore the performance of our method as the baseline ( separation between frames ) varies .
for each baseline ( from 123 to 123 , by 123 ) , we identied all pairs of images separated by exactly this many frames .
we then split these pairs into three sets , for training , validation , in order to determine the adjacency matrix for our edge features , we triangulated the set of landmarks using the delaunay triangulation ( see gure 123 ) .
figure 123 ( top ) shows the performance of our method as the baseline increases , for both linear and quadratic assign - ment ( for quadratic assignment we use the graduated as - signment algorithm , as mentioned previously ) .
the values shown report the normalised hamming loss ( i . e .
the propor - tion of points incorrectly matched ) ; the regularization con - stant resulting in the best performance on our validation set is used for testing .
graduated assignment using bistochas - tic normalisation , which to the best of our knowledge is the state - of - the - art relaxation , is shown for comparison ( 123 ) . 123
for both linear and quadratic assignment , gure 123 shows that learning signicantly outperforms non - learning in terms of accuracy .
interestingly , quadratic assignment performs worse than linear assignment before learning is applied this is likely because the relative scale of the linear and quadratic features is badly tuned before learning .
in - deed , this demonstrates exactly why learning is important .
it is also worth noting that linear assignment with learning performs similarly to quadratic assignment with bistochas - tic normalisation ( without learning ) this is an important result , since quadratic assignment via graduated assign - ment is signicantly more computationally intensive .
123exponential decay on the node features was benecial when using the method of ( 123 ) , and has hence been maintained in this case ( see section 123 ) ; a normalisation constant of = 123 was used .
figure 123 : top : performance on the house sequence as the baseline ( separation between frames ) varies ( the nor - malised hamming loss on all testing examples is reported , with error bars indicating the standard error ) .
centre : the weights learned for the quadratic model ( baseline = 123 , = 123 ) .
bottom : a frame from the sequence , together with its landmarks and triangulation; the 123rd and the 123rd frames , matched using linear assignment ( without learning , loss = 123 / 123 ) , and the same match after learning ( = 123 , loss = 123 / 123 ) .
mismatches are shown in red .
figure 123 : running time versus accuracy on the house dataset , for a baseline of 123
standard errors of both run - ning time and performance are shown ( the standard error for the running time is almost zero ) .
note that linear as - signment is around three orders of magnitude faster than
figure 123 ( centre ) shows the weight vector learned us - ing quadratic assignment ( for a baseline of 123 frames , with = 123 ) .
note that the rst 123 points show the weights of the shape context features , whereas the nal point corresponds to the edge features .
the nal point is given a very high score after learning , indicating that the edge features are important in this model . 123 here the rst 123 features corre - spond to the rst radial bin ( as described in section 123 ) etc .
the rst radial bin appears to be more important than the last , for example .
figure 123 ( bottom ) also shows an example match , using the 123rd and the 123rd frames of the sequence for linear assignment , before and after learning .
finally , figure 123 shows the running time of our method compared to its accuracy .
firstly , it should be noted that the use of learning has no eect on running time; since learn - ing outperforms non - learning in all cases , this presents a very strong case for learning .
quadratic assignment with bistochastic normalisation gives the best non - learning per - formance , however , it is still worse than either linear or quadratic assignment with learning and it is signicantly
123 video sequence
for our second experiment , we consider matching features of a human in a video sequence .
we used a video sequence from the sampl dataset ( 123 ) a 123 frame sequence of a human face ( see gure 123 , bottom ) .
to identify landmarks for these scenes , we used the susan corner detector ( 123 , 123 ) .
this detector essentially identies points as corners if their neighbours within a small radius are dissimilar
123this should be interpreted with some caution : the features have dierent scales , meaning that their importances cannot be compared directly .
however , from the point of view of the regularizer , assigning this feature a high weight bares a high cost , implying that it is an
figure 123 : top : performance on the video sequence as the baseline ( separation between frames ) varies ( the endpoint error on all testing examples is reported , with error bars indicating the standard error ) .
centre : the weights learned for the model ( baseline = 123 , = 123 ) .
bottom : the 123th and the 123th frames , matched using linear assignment ( loss = 123 ) , and the same match after learning ( = 123 , loss = 123 ) .
the outline of the points to be matched ( left ) , and the correct match ( right ) are shown in green; the inferred match is outlined in red; the match after learning is much closer to the correct match .
detector was tuned such that no more than 123 landmarks were identied in each scene .
in this setting , we are no longer interested in matching all of the landmarks in both images , but rather those that cor - respond to important parts of the human gure .
we identi - ed the same 123 points in each image ( gure 123 , bottom ) .
it is assumed that these points are known in advance for the template scene ( g ) , and are to be found in the target scene ( g ( cid : 123 ) ) .
clearly , since the correct match corresponds to only a tiny proportion of the scene , using the normalised hamming loss is no longer appropriate we wish to penalise incorrect matches less if they are close to the correct match .
hence we use the loss function ( as introduced in section 123 )
( cid : 123 ) d ( gi , g ( cid : 123 )
( g , g ( cid : 123 ) , ) = 123 123
table 123 : performance on the bikes dataset .
results for the minimiser of the validation loss ( = 123 ) are reported .
standard errors are in parentheses .
here the loss is small if the distance between the chosen match and the correct match is small .
since we are interested in only a few of our landmarks , triangulating the graph is no longer meaningful .
hence we present results only for linear assignment .
figure 123 ( top ) shows the performance of our method as the baseline increases .
in this case , the performance is non - monotonic as the subject moves in and out of view through - out the sequence .
this sequence presents additional dicul - ties over the house dataset , as we are subject to noise in the detected landmarks , and possibly in their labelling also .
nevertheless , learning outperforms non - learning for all base - lines .
the weight vector ( gure 123 , centre ) is heavily peaked about particular angular bins .
for our nal experiment , we used images from the caltech 123 dataset ( 123 ) .
we chose to match images in the touring bike class , which contains 123 images of bicycles .
since the shape context features we are using are robust to only a small amount of rotation ( and not to reection ) , we only included images in this dataset that were taken side - on .
some of these were then reected to ensure that each im - age had a consistent orientation ( in total , 123 images re - mained ) .
again , the susan corner detector was used to identify the landmarks in each scene; 123 points correspond - ing to the frame of the bicycle were identied in each frame ( see gure 123 , bottom ) .
rather than matching all pairs of bicycles , we used a xed template ( g ) , and only varied the target .
this is an easier problem than matching all pairs , but is realistic in many scenarios , such as image retrieval .
table 123 shows the endpoint error of our method , and gives further evidence of the improvement of learning over non - learning .
figure 123 shows a selection of data from our train - ing set , as well as an example matching , with and without
figure 123 : top : some of our training scenes .
bottom : a match from our test set .
the top frame shows the points as matched without learning ( loss = 123 ) , and the bottom frame shows the match with learning ( loss = 123 ) .
the outline of the points to be matched ( left ) , and the correct match ( right ) are outlined in green; the inferred match is outlined in red .
123 conclusions and discussion
we have shown how the compatibility functions for the graph matching problem can be estimated from labeled training examples , where a training input is a pair of graphs and a training output is a matching matrix .
we use large - margin structured estimation techniques with column gen - eration in order to solve the learning problem eciently , despite the huge number of constraints in the optimization problem .
we presented experimental results in three dier - ent settings , each of which revealed that the graph matching problem can be signicantly improved by means of learning .
an interesting nding in this work has been that linear assignment with learning performs similarly to graduated assignment with bistochastic normalisation , a state - of - the - art quadratic assignment relaxation algorithm .
this sug - gests that , in situations where speed is a major issue , lin - ear assignment may be resurrected as a means for graph matching .
in addition to that , if learning is introduced to graduated assignment itself , then the performance of graph matching improves signicantly both on accuracy and speed when compared to the best existing quadratic assignment
there are many other situations in which learning a matching criterion can be useful .
in multi - camera settings for example , when dierent cameras may be of dierent types and have dierent calibrations and viewpoints , it is reasonable to expect that the optimal compatibility func - tions will be dierent depending on which camera pair we consider .
in surveillance applications we should take advan - tage of the fact that much of the context does not change : the camera and the viewpoint are typically the same .
to summarize , by learning a matching criterion from pre - viously labeled data , we are able to substantially improve the accuracy of graph matching algorithms .
