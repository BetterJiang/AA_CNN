in this paper a correspondence is derived between regularization operators used in regularization networks and support vector kernels .
we prove that the greens functions associated with regularization operators are suitable support vector kernels with equivalent regularization properties .
moreover , the paper provides an analysis of currently used support vector kernels in the view of regularization theory and corresponding operators associated with the classes of both polynomial kernels and translation invariant kernels .
the latter are also analyzed on periodical domains .
as a by - product we show that a large number of radial basis functions , namely conditionally positive denite functions , may be used as support vector kernels .
q 123 elsevier science ltd .
all rights reserved .
keywords : support vector machines; mercer kernel; regularization networks; ridge regression; greens functions; conditionally positive denite functions; polynomial kernels; radial basis functions
i lagrange multipliers and expansion
i , b i , b
r set of real numbers c set of complex numbers n set of integers x ( lowercase latin ) scalars x ( boldface ) elements of rn a i , a h . . i dot product in hilbert space k . k norm , induced by a dot product f complex conjugate ( of a function or a scalar ) f fourier transform of f f feature space f , f ( x ) elements and mappings into f d , dij matrices , matrix elements dx123 ( x ) , d ij delta distribution , kronecker delta ( li , f i ) , ( li , w i ) ( eigenvector , eigenvalue ) pairs
i 123 product i 123 convolution i 123 summation
* corresponding author .
tel : ( ++123 ) - 123 - 123 - 123; fax : ( ++123 ) - 123 - 123 - 123; e - mail : smola@rst . gmd . de .
url : http : / / svm . rst . gmd . de .
123 - 123 / 123 / $123 q 123 elsevier science ltd .
all rights reserved .
123 i indicator function on a set i 123 identity map ~ 123 vector with all entries equal to 123
support vector ( sv ) machines for pattern recognition , regression estimation , and operator inversion exploit the idea of mapping data into a high dimensional feature space where they perform a linear algorithm .
instead of evaluating this mapping explicitly , one uses integral opera - tor kernels k ( x , y ) which correspond to dot products of the mapped data in high dimensional space , aizerman et al .
( 123 ) ; boser et al .
( 123 ) , i . e .
k ( x , y ) hf ( x ) f ( y ) i with f : r n ! f denoting the map into feature space f .
mostly , this map and many of its properties are unknown .
even worse , so far no general rule was available which kernel should be used , or why mapping into a very high dimensional space often provides good results , seemingly defying the curse of dimensionality .
in order to clarify this dilemma we show how these kernels k ( x , y ) correspond to regularization operators p , the link being that k is the p denoting the adjoint greens function of operator of p ) .
in other words given a support vector
p p ( with p
smola et al .
/ neural networks 123 ( 123 ) 123
kernel we show how to nd the corresponding regulariza - tion operator and vice versa .
for the sake of simplicity , we shall limit ourselves to the case of regression our con - siderations , however , also hold true for the other cases
this paper123 starts by briey reviewing the concepts of sv machines ( section 123 ) and regularization networks ( section 123 ) .
section 123 contains the main result , the derivation of a correspondence between regularization operators used in regularization networks and sv kernels .
in section 123 appli - cations of this nding to translation invariant kernels for both unbounded and bounded support are presented .
section 123 presents the operators corresponding to polynomial kernels , another frequently used class of sv kernels .
sub - sequently section 123 introduces a new class of possible sv kernels which do not necessarily satisfy mercers condition , namely kernels derived from conditionally positive denite functions .
section 123 concludes the paper with a discussion .
finally appendix a contains a worked through example and appendix b applies the methods presented in this paper to nd a connection between ridge regression and sv machines .
due to its specic setting , however , only a less formal exposition is possible .
support vector machines
the sv algorithm for regression estimation , as described in vapnik ( 123 ) ; vapnik et al .
( 123 ) , exploits the idea of computing a linear function in high dimensional feature space f ( furnished with a dot product ) .
thereby this algo - rithm can compute a nonlinear function in the space of the input data r n .
the functions take the form f ( x ) hwf ( x ) i b with f : r n ! f being the map into feature space and w ( f .
from a given training set ( ( xi , yi ) li 123 , , l , xi ( rn , y i ( r ) , one tries to minimize the regularized risk functional
to estimate f
rreg ( f ) remp ( f ) l
c ( f ( xi ) , yi ) l
the empirical risk functional r emp ( f ) together with a complexity term kwk123 , thereby enforcing atness in feature space .
here c ( f ( x i ) , y i ) is the cost function determining how the distance between f ( x i ) and the target values y i should be penalized , and l ( r is a regularization constant .
the idea of atness is derived from pattern recognition where this corresponds to nding a hyperplane that has maximum dis - tance in f from the classes to be separated boser et al .
( 123 ) ; cortes and vapnik ( 123 ) .
as shown in vapnik
123 portions of this work have been published in smola and scholkopf
( 123 ) for the case of e - insensitive cost functions ,
c ( f ( x ) , y )
lf ( x ) yl e
for lf ( x ) yl $ e
( 123 ) can be minimized by solving a quadratic program - ming problem formulated in terms of dot products in f .
it turns out that the solution w can be expressed in terms of support vectors .
note that the representation can be sparse .
therefore , the points corresponding to nonzero a i , which sufce for describing f , are called support vectors .
therefore , via eq .
( 123 ) f ( x ) hwf ( x ) i b
aik ( xi , x ) b
i , j 123
where k ( x i , x ) is a kernel function computing a dot product in feature space ( a concept introduced by aizerman et al . , 123 ) .
the coefcients a i can be found by solving a quadratic programming problem ( with kij : k ( x i , x j ) , ai bi b i being the solution of the optimiza - tion problem below ) :
i and b i , b
i bi ) yi ( b
i ) 123 , bi; b
i ( 123 , 123
note that eq .
( 123 ) is not the only possible choice of cost functions resulting in a quadratic programming problem ( many convex cost function , in particular quadratic parts and innities are admissible , too ) .
for a detailed discussion see smola and scholkopf ( 123 ) ; smola et al .
( 123 ) .
also note that any continuous symmetric function k ( x , y ) ( l123 # l 123 may be used as an admissible kernel if it satises a weak form of mercers condition ( riesz and nagy , 123 )
k ( x , y ) g ( x ) g ( y ) dxdy $ 123 for all g ( l123 ( rn )
regularization networks
here again we start with minimizing the empirical risk functional r emp ( f ) plus a regularization term k pf k123 dened by a regularization operator p in the sense of tikhonov and arsenin ( 123 ) , i . e .
p is a positive semidenite operator
smola et al .
/ neural networks 123 ( 123 ) 123
mapping from the hilbert space h of functions f under consideration to a dot product space d such that the expres - sion h pf pgi is well dened .
for instance by choosing a suitable operator that penalizes large variations of f one can reduce the well - known overtting effect .
another possible setting also might be an operator p mapping from ( kimeldorf and wahba , 123; girosi , 123 ) .
in appendix a , we provide a worked through example ( mainly taken from girosi et al . , 123 ) for a simple regularization operator to illustrate our reasoning .
into some reproducing kernel hilbert
similar to eq .
( 123 ) , we minimize
rreg ( f ) remp l
k pf k123
c ( f ( xi ) , yi ) l
k pf k123
using an expansion of f in terms of some symmetric func - tion k ( x i , x j ) ( note here , that k need not fulll mercers
aik ( xi , x ) b ,
and the cost function dened in eq .
( 123 ) , this leads to a quadratic programming problem similar to the one for svs .
by computing wolfes dual ( for details of the calcu - lations see smola and scholkopf , 123 ) , and using dij : h ( pk ) ( xi , : ) ( pk ) ( xj , : ) i ( hfgi denotes the dot product of the functions f and ~ a d 123k ( ~ b ~ b
p ) , with bi , b i being the solution of
f ( x ) g ( x ) dx ) , we
j bj ) kd 123k ) ij
i , j 123
i bi ) yi ( b
i ) 123 , bi; b
i ( 123 , 123
unfortunately , this setting of the problem does not preserve sparsity in terms of the coefcients , as a potentially sparse is spoiled by d - 123k , decomposition in terms of bi and b which in general is not diagonal ( eq .
( 123 ) , on the other hand , does typically have many vanishing coefcients , see e . g .
scholkopf et al . , 123; vapnik , 123 ) .
the relation between both methods
comparing eq .
( 123 ) with eq .
( 123 ) leads to the question if and under which condition the two methods might be therefore , also under which conditions , given a suitable cost function , regularization networks
might lead to sparse decompositions ( i . e .
only a few of the expansion coefcients a i in f would differ from zero ) .
a sufcient123 condition is d k ( thus kd123k k ) , i . e .
k ( xi , xj ) h ( pk ) ( xi , : ) ( pk ) ( xj , : ) i ( self consistency ) : this is the main equation of this paper .
our goal now is to solve the following two problems :
given a regularization operator p , nd a kernel k such that a sv machine using k will not only enforce atness in feature space , but also correspond to minimizing a regularized risk functional with p as regularization
given a sv kernel k , nd a regularization operator p such that a sv machine using this kernel can be viewed as a regularization network using p .
these two problems can be solved by employing the con - cept of greens functions as described in girosi et al .
( 123 ) .
these functions had been introduced in the context of solving differential equations .
for our purpose , it is sufcient to know that the greens functions gxi ( x ) is the d - distribution ( not to be confused with the kronecker symbol d ij ) which has the property that hfdxi f ( x i ) .
the relationship between kernels and regularization operators is formalized in the following proposition :
p p satisfy
( x ) of p
proposition 123 ( greens functions and mercer kernels ) .
let p be a regularization operator , and g be the greens p p .
then g is a mercer kernel such that d k .
function of p sv machines using g minimize risk functional eq .
( 123 ) with p as regularization operator . 123
substituting eq .
( 123 ) into gxj
( xi ) h ( pgxi
hence g ( x i , x j ) : gxi ( x j ) is symmetric and satises eq .
thus the sv optimization problem eq .
( 123 ) is equivalent to the regularization network counterpart eq .
further - more , g is an admissible non - negative kernel , as it can be written as a dot product in hilbert space , namely
g ( xi , xj ) hf ( xi ) f ( xj ) i with f : xi ( cid : 123 ) ( pgxi ( : ) :
a similar result can be obtained by exploiting mercers theorem in a more straightforward manner , by using the fact that a mercer kernel k can be expanded into a convergent
123 in the case of k not having of full rank d is only required to be the inverse on the image of k .
the pseudoinverse for instance is such a matrix .
123 this condition is sufcient but not necessary for satisfying eq .
any projection of g onto an invariant subspace of p p would also satisfy this equation .
note that as g ( . , . ) being a function on r n # r n the projection operator has to be applied to it as a function of both the rst and the second
series of its eigensystem ( f l ( x ) , l l ) with ll $ 123 ,
this is particularly useful for the approximation of period - ical functions and will come handy in example 123 as we will have to deal with a discrete eigensystem in this case .
proposition 123 ( a discrete counterpart ) .
given a regular - p p into a discrete ization operator p with an expansion of p eigensystem ( l l , w l ) and a kernel k with
where dl ( ( 123 , 123 ) for all l , and o l ( d l / ll ) convergent .
then k satises eq
evaluating eq .
( 123 ) and using orthonormality of the system ( d l / l l , w l ) , yields : hk ( xi , : ) ( p p pk ) ( xj , : ) i
wl 123 ( xj ) wl 123 ( : ) p pwl 123 ( : ) i
( xj ) k ( xi , xj )
l , l 123
rearranging of the summation coefcients is allowed as the eigenfunctions are orthonormal and the series o l ( d l / ll ) converges .
consequently a large class of kernels can be associated with a given regularization operator ( and vice versa ) thereby restricting ourselves to some subspace of the eigensystem of p
excluding eigenfunctions of p
p p from the kernel expan - sion effectively decreases the expressive power of the set of approximating functions , i . e .
we limit the capacity of the system of functions .
removing low capacity ( i . e .
very at ) eigenfunctions from the expansion will have an adverse effect , though , as the data will have to be approximated by the higher capacity functions .
in the following we will exploit this relationship in both ways : to compute greens functions for a given regulariza - tion operator p and to infer the regularization operator from a given kernel k .
note that a similar reasoning can be applied to connect ridge regression schemes with support vector kernels as shown in appendix b .
123 the intuition of this reasoning is that there exists a one to one corre - spondence between kernels and regularization operators only on the image of h under the integral operator ( of ) ( x ) : k ( x , y ) f ( y ) dy , namely that o o , however , the p p may take on an arbitrary form .
in this case k regularization operator p still will fulll the self consistency condition .
p p are inverse to another .
on the null space of
smola et al .
/ neural networks 123 ( 123 ) 123
translation invariant kernels
let us now more specically consider regularization operators p that may be written as multiplications in fourier h pf pgi
f ( w ) g ( w )
with f ( w ) denoting the fourier transform of f ( x ) , and pw p w real valued , nonnegative and converging to 123 for lwl ! and q supp ( p ( w ) ) .
small values of p ( w ) correspond to a strong attenuation of the correspond - ing frequencies .
hence small values of p ( w ) for large w are desirable since high frequency components of f correspond to rapid changes in f .
p ( w ) describes the lter properties of p p note that no attenuation takes place for p ( w ) 123 as these frequencies have been excluded from the integration
for regularization operators dened in fourier space by eq .
( 123 ) it can be shown by exploiting p ( w ) p ( w )
is a corresponding greens function satisfying translational g ( xi , xj ) g ( xi xj ) and g ( w ) p ( w ) for the proof , one only has to show that g satises eq .
this provides us with an efcient tool for analyzing sv kernels and the types of capacity control they exhibit .
in fact the above is a special case of bochners theorem ( boch - ner , 123 ) stating that the fourier transform of a positive measure constitutes a positive hilbert schmidt kernel .
example 123 ( bq - splines ) .
in vapnik et al .
( 123 ) the use of b q - splines was proposed ( see fig .
123 ) as building blocks for
with x ( r n .
for the sake of simplicity , we consider the case n 123
recalling the denition ( up to scaling factors ) by unser et al .
( 123 ) bq #q 123 123 ( 123 : 123 , 123 : 123 ) we can utilize the above result and the fourierplancherel identity to construct the fourier representation of the corre - sponding regularization operator .
up to a multiplicative constant , it equals p ( w ) k ( w ) sinc
( q 123 ) wi
this answers the question why only b - splines of odd order are admissible although both even and odd order b - splines
smola et al .
/ neural networks 123 ( 123 ) 123
left : b 123 - spline kernel .
right : fourier transform of the kernel .
converge to a gaussian for q ! due to the law of large numbers : the even ones have negative parts in the fourier spectrum ( which would result in an amplication of the corresponding frequency components ) .
the zeros in k that b q has only compact support stem from the fact ( ( q 123 ) / 123 , ( q 123 ) / 123 ) .
by using this kernel we trade reduced computational complexity in calculating f ( we only have to take points into account with kx i x jk # c from some limited neighborhood determined by c ) for a operator as it completely removes frequencies w p with k ( w p ) 123
example 123 ( gaussian kernels ) .
following the exposition of yuille and grzywacz ( 123 ) as described in girosi et al .
( 123 ) , one can see that for
k pf k123
( omf ( x ) ) 123
with o123m dm and o123m 123 d=m , d being the laplacian and = the gradient operator , we get gaussians kernels ( see
p in terms of its fourier properties ,
moreover , we can provide an equivalent representation exp ( ( j 123kwk123 ) / 123 ) up to a multiplicative constant .
training a sv machine with gaussian rbf kernels ( scholkopf et al . , 123 ) corresponds to minimizing the specic cost function with a regularization operator of type eq
recall that eq .
( 123 ) means that all derivatives of f are penalized ( we have a pseudodifferential operator ) to obtain a very smooth estimate .
this also explains the good per - formance of sv machines in this case , as it is by no means obvious that choosing a at function in some high dimen - sional space will correspond to a simple function in low dimensional space , as shown in example 123
gaussian kernels tend to yield good performance under general smoothness assumptions and should be considered especially if no additional knowledge of the data is
left : gaussian kernel with standard deviation 123 .
right : fourier transform of the kernel .
smola et al .
/ neural networks 123 ( 123 ) 123
left : dirichlet kernel of order 123
note that this kernel is periodical .
right : fourier transform of the kernel .
left : regression with a dirichlet kernel of order n 123
one can clearly observe the overtting .
right : regression of the same data with a gaussian kernel of width j 123 123
example 123 ( dirichlet kernels ) .
in vapnik et al .
( 123 ) , a class of kernels generating fourier expansions was intro - duced for interpolating data on r n ,
factor spaces .
without loss of generality assume the period to be 123p consequently one gets translation invariance on
in the following we will show the consequences of this
setting for the operator dened in example 123
( as in example 123 consider x ( r123 to avoid tedious notation . ) i ndi ( w ) .
a regularization operator with these properties , however , may not be desirable as it only damps a nite number of frequencies ( cf .
123 ) and leaves all other frequencies unchanged which can lead to overt - ting ( fig
in some cases it might be useful
periodical functions , e . g .
functions dened on a circle .
this leads to the second possible type of translation invariant123 kernel functions , namely functions dened on
123 obviously dening translation invariant kernels on a bounded interval is not a reasonable concept as the data would hit the bounds of the interval when translated by a large amount .
therefore , only unbounded intervals and factor spaces are possible domains .
example 123 ( periodical gaussian kernels ) .
analogously to eq .
( 123 ) , dene a regularization operator on functions on ( 123 , 123p ) n by
k pf k123 p
( omf ( x ) ) 123
with o as in example 123
for the sake of simplicity assume n 123
a generalization to multidimensional kernels is
it is easy to check that the fourier basis ( 123 / 123 , sin ( lx ) , cos ( lx ) , l ( n ) is an eigensystem of the operator dened above , with eigenvalues exp ( ( l123j 123 ) / 123 ) .
now apply proposi - tion 123 , taking into account all eigenfunctions except l 123
smola et al .
/ neural networks 123 ( 123 ) 123
left : periodical gaussian kernel for several values of j ( normalized to 123 as its maximum and 123 as its minimum value ) .
peaked functions correspond to small j .
right : fourier coefcients of the kernel for j 123 123
this yields the following kernel :
123 ( sin ( l x ) sin ( l x123 ) cos ( l x ) cos ( l x123 ) )
123 cos ( l ( x x123 ) )
for practical purposes one may truncate the expansion after a nite number of terms .
moreover we rescale k to have a range of exactly ( 123 , 123 ) by using the positive l 123 ( 123 ) l 123e ( ( l 123j123 ) =123 ) and the scaling factor
l 123 e ( ( ( 123l 123 ) 123j123 ) =123 )
in the context of periodical functions , the difference between this kernel and the dirichlet kernel of example 123 is that the latter does not distinguish between the different frequency components in w ( ( np , , np ) .
however , it effectively limits the maximum capacity of the system to approximating the data with a fourier expansion up to the
the question that arises now is which kernel to choose .
let us think about two extreme situations .
suppose we already knew the shape of the power spec - trum pow ( w ) of the function we would like to estimate .
in this case we choose k such that k matches the power if we happen to know very little about the given data a general smoothness assumption is a reasonable choice .
hence we might want to choose one of the gaussian kernels in example 123 or 123
if computing time is important one might moreover consider kernels with compact sup - port , e . g .
using the b - spline kernels of example 123
this choice will cause many matrix elements k ij k ( x i x j )
the usual scenario will be in between the two extreme cases and we will have some limited prior knowledge
available .
for more information on using prior knowledge for choosing kernels see scholkopf et al .
( 123 ) .
prior knowledge can also be used to determine the free parameters of the kernel , e . g .
its width ( j ) in the examples 123 and 123
besides that model selection principles like structural risk minimization ( vapnik , 123 ) , cross validation ( bishop , 123; amari et al . , 123; kearns , 123 ) , mdl ( rissanen , 123 ) , bayesian methods ( mackay , 123; bishop , 123 ) , etc .
can be employed .
choosing a small width of the kernels leads to high generalization error as it effectively decouples the separate basis functions of the kernel expansion into very localized functions which is equivalent to memorizing the data , whereas a wide kernel tends to oversmooth .
note that the choice of the width may be more important than the actual functional form of the kernel .
there may be little difference in the relevant part of the lter properties between e . g .
a b - spline and a gaussian kernel ( cf .
the invariance of the kernels presented so far has been exploited only in the context of invariance with respect to the translation symmetry group in r n .
yet they could also be applied to other symmetry transformations corresponding to other canonical coordinate systems such as the rotation and scaling group as proposed by segman et al .
( 123 ) ; ferraro and caelli ( 123 ) , i . e .
to a logpolar parametrization of r n or the parametrization of manifolds .
kernels of dot - product type
there exists a large class of support vector kernels which
are not translation invariant , namely kernels of the type k ( x , x123 ) t ( hxx123i ) for instance , polynomial kernels ( hxx123i c ) p of homo - geneous ( c 123 ) or inhomogeneous type ( c .
123 ) belong to this class .
it follows directly from poggio ( 123 ) that poly - nomial kernels satisfy mercers condition .
now the question
smola et al .
/ neural networks 123 ( 123 ) 123
operator of this type can be obtained for degree 123 homo - geneous polynomials on r123 i . e .
for the kernel k ( x , y ) hxyi123 : corresponding monomials we have
the projectors onto the
h ( x123 , x123 ) ( y123 , y123 ) i123 h ( x123
comparison of regularization properties in the low frequency domain of b 123 - spline kernel and gaussian kernel ( j 123 123 ) .
up to an attenuation factor of 123 123 both types of kernels exhibit qualitatively similar lter characteristics .
arises which regularization operator p these kernels might correspond to , and which functions t might be admissible ones .
obviously p can not be translation invariant , as this is not the case for k .
note that although lacking translation invariance , these kernels still exhibit ( by construction ) the property of rotation invariance orthogonal transforma - tions r are isometries of the euclidean dot product : hxyi skipping tedious calculations , we give an example of an k ( x123 , x123 ) h pk ( x123 , : ) pk ( x123 , : ) i for homogeneous polynomials .
we then use this result to give an analogous expansion for the inhomogeneous case , and present a sufcient condition for t ( hxx123 ) ) to be an admissible mercer kernel .
let m ( m 123 , , mn ) ( nn
123 be a multi index and denote
the multinomial coefcient .
moreover , let
xn f ( x ) lx 123
he me m123 ) d mm123
and e m be an orthonormal basis , observe how for each m123 dm 123 extracts exactly one coef - cient from the monomials of degree m .
now we can dene an operator pp which will act as a regularization operator and satisfy eq .
( 123 ) , namely
an intuitive description of p would be that the data is mapped from r 123 into 123 - dimensional feature space ( f r123 ) by computing monomials of degree 123
subsequently one seeks to compute the attest function in this new space .
note that pp is only well - dened on functions that are p times differentiable .
accordingly , we will have to restrict the space of functions under consideration to c p .
this is not a major restriction as polynomial kernels are in c by
it is interesting that the homogeneous polynomial kernel also satises the self consistency condition eq .
( 123 ) for the
in order to construct an operator for inhomogeneous poly - nomials , we make use of the expansion
( for convenience set c 123 ) .
hence one may decompose the inhomogeneous polynomial kernel into a series of homo - geneous kernels and construct the corresponding operator
exploiting this idea even further allows us to state a suf - cient condition for t ( hxyi ) to be a mercer kernel .
as homo - geneous polynomial kernels satisfy mercers condition so does any positive linear combination of them .
corollary 123 ( functions with non - negative power - series ) .
for every function t ( x ) that can be expanded into a uni - formly convergent power series on r with nonnegative expansion coefcients , i . e .
example 123 ( vapnik , 123 ) .
a simple example of an
aixi with ai $ 123
the kernel k ( x , y ) : t ( hxyi ) is a mercer kernel and a corre - sponding regularization operator is
consequently , functions like e x , cosh ( x ) , sinh ( x ) , etc .
could be used as possible mercer kernels .
moreover , note that the same argument applies for t ( k ( x , y ) ) : if k is any mercer kernel , and t satises the conditions of corollary 123 then t ( k ( x , y ) ) t ( hf ( x ) f ( y ) i ) is a mercer kernel .
( 123 ) provides further means to construct more general kernels , e . g .
sinh ( e
a new class of support vector kernels
we will follow the lines of madych and nelson ( 123 ) as pointed out by girosi et al .
( 123 ) .
the main statement is that conditionally positive denite ( cpd ) functions generate admissible sv kernels .
this is very useful as the property of being cpd often is easier to verify than mercers condition , especially when combined with the results of schoenberg and micchelli on the connection between cpd and comple - tely monotonic functions schoenberg ( 123a ) ; schoenberg ( 123b ) ; micchelli ( 123 ) .
moreover , cpd functions lead to a class of sv kernels that do not necessarily satisfy mercers
denition 123 ( conditionally positive denite functions ) .
a continuous function h , dened on ( 123 , ) , is said to be con - ditionally positive denite ( cpd ) of order m on r n if for any distinct points x 123 , xl ( rn the quadratic form
i , j 123 is non - negative provided that the scalars c 123 , , c l satisfy i 123cip ( xi ) 123 for all polynomials p on r n of degree
lower than m .
denition 123 ( completely monotonic functions ) .
a func - tion h ( x ) is called completely monotonic of order m if ( 123 ) n dn
dxnh ( x ) $ 123 for x ( r
123 and n $ m
it can be shown ( schoenberg , 123a; schoenberg , 123b; micchelli , 123 ) that a function h ( x 123 ) is conditionally posi - tive denite if and only if h ( x ) is completely monotonic of the same order .
this gives a ( sometimes simpler ) criterion for checking whether a function is cpd or not .
proposition 123 ( cpd functions and admissible kernels ) .
m to be the space of polynomials of degree lower than m on r n .
every cpd function h of order m generates an admissible kernel for sv expansions on the
smola et al .
/ neural networks 123 ( 123 ) 123
space of functions f orthogonal to pn h ( kx i x jk123 ) .
m by setting k ( x i , x j ) :
in dyn ( 123 ) ; madych and nelson ( 123 ) it was shown that cpd functions h of order m generate semi - norms k . k h by
dxidxjh ( kxi xjk123 ) f ( xi ) f ( xj )
provided that the projection of f onto pn m is zero .
for these functions , this , however , also denes a dot product in some feature space .
hence they can be used as sv kernels
consequently , one may use kernels like those proposed in the context of regularization networks by girosi et al .
( 123 ) as sv kernels : k ( x , y ) e bkx yk123
gaussian , ( m 123 )
kx yk123 c123
kx yk123 c123
multiquadric , ( m 123 )
inverse multiquadric , ( m 123 )
k ( x , y ) kx yk123ln kx yk thin plate splines , ( m 123 )
here the corresponding regularization operator p is given implicitly by the seminorm ( eq .
( 123 ) ) as k pf k123 : kf k123 however , one has to ensure the orthogonality of our esti - i 123cip ( xi ) 123 mate with respect to pn for all polynomials p on r n of degree lower than m with c i being the expansion coefcients of the estimate , i . e
m , i . e .
ensure that
we proceed with algorithmic details how to actually compute the expansion .
in order not to loose expressive is necessary to take the polynomials separately into account , i . e .
modify eq
in the estimate f
cik ( xi , x ) p ( x ) with p ( x ) ( pn
m for which kf k123
both of these issues can be addressed by splitting f into a term f orthogonal to pn h is well dened and a polynomial term which will not be regularized at all .
( of course one could dene an additional regularization operator for the polynomial part but this would without need render the notation more tedious . ) hence , the regular - ized risk functional ( eq .
( 123 ) ) takes on the following form rreg ( f ) remp ( f ) l with f : ( 123 proj ( pn m ) ) f and proj ( . ) denoting the projection operator .
repeating the calculations that led to eq .
( 123 ) , yields a similar optimization problem with the
smola et al .
/ neural networks 123 ( 123 ) 123
difference being that the equality constraint
i ) 123
has been replaced with
i ) p ( xi ) 123 for all p ( p
note that for the m 123 condition , eq .
( 123 ) reduces to eq .
( 123 ) as pn 123 contains only the constant function .
the resulting optimization problem is positive semidenite , however , only in the feasible region given by the equality constraints .
some of the eigenvalues of the matrix k may be negative in the space of coefcients not satisfying eq .
it can be seen very easily for the multiquadric case ( eq .
( 123 ) ) all entries in k ij are negative .
this can lead to numerical instabilities for quadratic programming codes as they usually assume the quadratic matrix to be positive semi - denite not only in the feasible region of the parameters but on the whole space ( cf .
more and toraldo , 123; vanderbei , 123 ) .
a practical solution to this problem is to remove the space s spanned by all polynomials pn the data ( x 123 , , xl ) from the image of k ij while keeping it symmetric by substituting kij with ( ( 123 p proj ( s ) ) t k ( 123 p proj ( s ) ) ) ij .
here pproj is the projection matrix on r l corre - sponding to proj ( s ) .
the space pn
example 123 ( projecting out pn 123 consists of all polynomials on r n of degree lower than 123 , i . e .
only of the constant function .
hence s , the span of pn 123 on any nonempty set ( x 123 , , xl ) , r n is span ( ~ 123 ) .
consequently , ( 123=l ) ~ 123 ~ 123 projector onto that space and we get123
kij ( cid : 123 ) 123
note that in the standard sv problem this modication of k ij leads to the same solution due to the constraint o i ( a i a
example 123 ( projecting out pn 123 consists of all constant and linear functions on ( x 123 , , xl ) .
here s span ( ( v 123 , , v n ) ) with v123 : ( 123 , , 123 ) vi : ( xi123 , , xil in the case of l # n 123 already a linear model will sufce for reducing r reg ( f ) to 123
in this case the solution of the quadratic optimization problem is just 123 as k ij will have rank 123 after the projection .
for l .
n 123 we will have to transform v 123 , , v n into an orthonormal basis e 123 , , e n of s , e . g .
by applying the gram
) for i ( 123 , , n
123 curiously enough the matrix we obtain by this method is identical to the one that is being diagonalized in kernel pca ( scholkopf et al . , 123 ) .
this is clear as projecting out the span of constant polynomials is equivalent to centering in feature space .
schmidt procedure .
this in turn allows us to construct an orthogonal projector onto s and the corresponding modied matrix from kij .
as one can observe , only cpd functions of order up to 123 are of practical interest for sv methods as the number of additional constraints and projection operations increases in a combinatoric way thereby rendering the calculations com - putationally infeasible for m
a connection between sv kernels and regularization operators has been shown , which may provide one key to understanding why sv machines have been found to exhibit high generalization ability .
in particular for the common choices of kernels , the mapping into feature space is not arbitrary but corresponds to good regularization operators ( see examples 123 , 123 and 123 ) .
for kernels , however , where this is not the case , sv machines may show poor performance ( example 123 ) .
consequently the regularization framework enables us to analyze the regularization properties of kernels used in practice .
capacity control is one of the strengths of sv machines; however , this does not mean that the structure of the learn - ing machine , i . e .
the choice of a suitable kernel for a given task , should be disregarded .
on the contrary , the rather gen - eral class of admissible sv kernels should be seen as another strength , provided that we have a means of choosing the right kernel .
the newly established link to regularization theory can thus be seen as a tool for constructing the structure consisting of sets of functions in which the sv machine ( approximately ) performs structural risk minimization ( e . g .
vapnik , 123 ) .
in other words it allows to choose an appropriate kernel given the data and the problem specic knowledge .
for completeness an explicit construction of the regular - ization operators for polynomial kernels has been given in order to provide corresponding operators not only for trans - lation invariant kernels .
to make things more transparent appendix a contains a worked through example for com - puting a sv kernel for a specic choice of regularization
note that the regularized risk approach can also be dealt with in a reproducing kernel hilbert space ( rkhs ) approach which may lead to sometimes more elegant exposition of the subject , see kimeldorf and wahba ( 123 ) ; micchelli ( 123 ) ; wahba ( 123 ) ; girosi ( 123 ) ; scholkopf ( 123 ) .
finally the regularization framework made it possible to extend the class of admissible kernels to those dened by conditionally positive denite functions a class of kernels that do not necessarily have to satisfy mercers
a simple consequence of the proposed link is a bayesian interpretation of support vector machines .
in this case the choice of a special kernel can be regarded as a prior on the hypothesis space with p ( f ) ~ exp ( l
smola et al .
/ neural networks 123 ( 123 ) 123
future work will be necessary for understanding vapniks capacity bounds ( vapnik , 123 ) from a regulari - zation network point of view .
the authors thank chris burges , federico girosi , leo van hemmen , takashi onoda , john shawe - taylor , vladimir vapnik , grace wahba , and alan yuille for helpful discus - sions and comments .
smola is supported by a grant from the dfg ( ja 123 / 123 ) , and b .
scholkopf is supported by a grant from the studienstiftung des deutschen volkes .
appendix a a worked through example
in this section we will construct a support vector kernel
for the regularization operator k pf k123 h pf pfi hf p p pfi kf k123
this example is taken from girosi et al .
( 123 ) and used to illustrate our reasoning in detail .
for ease of notation assume f : r ! r .
a corresponding representation of p
p p in fourier space ( f
denoting the fourier transform of f ) yields
k pf k123
dwlf ( w ) l123 ( 123 w123 )
or equivalently ( cf .
section 123 , eq .
( 123 ) ) p ( w ) 123 / ( 123 w 123 ) .
in order to satisfy the self consistency condition ( eq .
( 123 ) ) , we have to compute the inverse fourier transform of p ( w ) to p p ( cf .
this leads obtain the greens functions of p to a kernel of the form k ( x , x123 ) e lx x123l
a function expansion in terms of this laplacian kernel ( it has the same shape as a laplacian distribution but should not be confused with the latter at all ) however , may not always be desirable as it is by far not as smooth as regres - sions using a gaussian kernel ( see fig
appendix b ridge regression
another frequently used method for selecting the regular - ization operator is to select d ( see eq .
( 123 ) ) to be the unit - matrix ( d ij d ij ) .
this approach often is called ridge regres - sion and is a very popular , method in the context of shrink - age estimators .
now one may pose a similar question as in section 123 , namely regarding the equivalence of ridge regres - sion and support vectors .
no answer is available for a direct equivalence , however , we will show that one may obtain models generated by the same type of regularization opera - tors .
the requirement for an equivalence of the latter type dij d ( xi , xj ) h ( pk ) ( xi , : ) ( pk ) ( xj , : ) i dij for all possible choices of x i ( r n .
unfortunately this requirement cannot be met for the case of the kronecker d , as eq .
( b123 ) implies the function d ( x 123 , . ) to be nonzero only on a set with ( lebesgue ) measure 123
the solution is to change the nite kronecker d into the more appropriate d - distribution , i . e .
d ( x i x j ) .
by a similar reasoning as in proposition 123 , one can see that this is true for k ( x , y ) being the greens function of p .
p p ) 123=123 is equivalent note that as a regularization operator , ( p to p , as we can always replace the latter by the former without any difference in the regularization properties .
therefore , without loss of generality , we will assume that p is a positive semidenite endomorphism .
formally we
h ( pk ) ( xi , : ) ( pk ) ( xj , : ) i hdxi
( : ) i dxi , xj
left : laplacian kernel .
right : regression with a gaussian ( j 123 ) and a laplacian kernel ( kernel width 123 ) of the data shown in fig
smola et al .
/ neural networks 123 ( 123 ) 123
again , this allows us to connect regularization operators and kernels ( we have to nd the greens function of p to satisfy the equation above ) .
for the special case of translation invariant operators denoted in fourier space we can associ - ate p with p ridge ( w ) , leading to
k pf k123
comparing eq .
( b123 ) with eq .
( 123 ) leads to the conclusion the following relation between kernels for support vector machines and ridge regression has to hold : this also explains the good performance of ridge regression models in a smoothing regularizer context ( the squared norm of the fourier transform of kernel functions describes the regularization properties of the corresponding kernel ) and allows us to transform support vector machines to ridge regression models and vice versa .
note , however , that we are loosing the sparsity properties of support
