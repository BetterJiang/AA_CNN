abstract kernel based algorithms such as support vector machines have achieved considerable success in various problems in the batch setting where all of the training data is available in advance .
support vector machines combine the so - called kernel trick with the large margin idea .
there has been little use of these methods in an online setting suitable for real - time applications .
in this paper we consider online learning in a reproducing kernel hilbert space .
by considering classical stochastic gradient descent within a feature space , and the use of some straight - forward tricks , we develop simple and computationally efcient algorithms for a wide range of problems such as classication , regression , and novelty detection .
in addition to allowing the exploitation of the kernel trick in an online setting , we examine the value of large margins for classication in the online setting with a drifting target .
we derive worst case loss bounds and moreover we show the convergence of the hypothesis to the minimiser of the regularised risk functional .
we present some experimental results that support the theory as well as illustrating the power of the new algorithms for online
index terms reproducing kernel hilbert spaces , stochastic gradient descent , large margin classiers , tracking , novelty detection , condition monitoring , classication , regression .
k ernel methods have proven to be successful in many
batch settings ( support vector machines , gaussian pro - cesses , regularization networks ) ( 123 ) .
whilst one can apply batch algorithms by utilising a sliding buffer ( 123 ) , it would be much better to have a truely online algorithm .
however the extension of kernel methods to online settings where the data arrives sequentially has proven to provide some hitherto
challenges for online kernel algorithms
first , the standard online settings for linear methods are in danger of overtting when applied to an estimator using a hilbert space method because of the high dimensionality of the weight vectors .
this can be handled by use of regularisa - tion ( or exploitation of prior probabilities in function space if the gaussian process view is taken ) .
the functional representation of classical kernel based estimators becomes more complex as the number of observations increases .
the representer theorem ( 123 ) implies that the number of kernel functions can grow up to linearly
manuscript received july 123 , 123; revised july 123 , 123
this work was
supported by the australian research council .
parts of this work were presented at the 123th international conference on algorithmic learning theory , november 123 and the 123th annual conference on neural information processing systems , december 123
the authors are with the research school of information sciences and engineering , the australian national university .
williamson is also with national ict australia .
with the number of observations .
depending on the loss func - tion used ( 123 ) , this will happen in practice in most cases .
thus the complexity of the estimator used in prediction increases linearly over time ( in some restricted situations this can be reduced to logarithmical cost ( 123 ) or constant cost ( 123 ) , yet with linear storage requirements ) .
clearly this is not satisfactory for genuine online applications .
third , the training time of batch and / or incremental update algorithms typically increases superlinearly with the number of observations .
incremental update algorithms ( 123 ) attempt to overcome this problem but cannot guarantee a bound on the number of operations required per iteration .
projection methods ( 123 ) on the other hand , will ensure a limited number of updates per iteration and also keep the complexity of the estimator constant .
however they can be computationally expensive since they require a matrix multiplication at each step .
the size of the matrix is given by the number of kernel functions required at each step and could typically be in the hundreds in the smallest dimension .
in solving the above challenges it is highly desirable to be able to theoretically prove convergence rates and error bounds for any algorithms developed .
one would want to be able to relate the performance of an online algorithm after seeing m observations to the quality that would be achieved in a batch setting .
it is also desirable to be able to provide some theoretical insight in drifting target scenarios when a comparison with a batch algorithm makes little sense .
in this paper we present algorithms which deal effectively with these three challenges as well as satisfying the above
related work
recently several algorithms have been proposed ( 123 ) , ( 123 ) ( 123 ) which perform perceptron - like updates for classication at each step .
some algorithms work only in the noise free case , others not for moving targets , and others assume an upper bound on the complexity of the estimators .
in the present paper we present a simple method which allows the use of kernel estimators for classication , regression , and novelty detection and which copes with a large number of kernel functions
the stochastic gradient descent algorithms we propose ( col - lectively called norma ) differ from the tracking algorithms of warmuth , herbster and auer ( 123 ) , ( 123 ) , ( 123 ) insofar as we do not require that the norm of the hypothesis be bounded beforehand .
more importantly , we explicitly deal with the issues described earlier that arise when applying them to kernel
concerning large margin classication ( which we obtain by performing stochastic gradient descent on the soft margin loss
ieee transactions on signal processing , vol .
123 , no .
123 , october 123
function ) , our algorithm is most similar to gentiles alma ( 123 ) and we obtain similar loss bounds to those obtained for alma .
one of the advantages of a large margin classier is that it allows us to track changing distributions efciently ( 123 ) .
in the context of gaussian processes ( an alternative theoret - ical framework that can be used to develop kernel based algo - rithms ) , related work was presented in ( 123 ) .
the key difference to our algorithm is that csato and opper repeatedly project on to a low - dimensional subspace , which can be computationally costly requiring as it does a matrix multiplication .
mesterharm ( 123 ) has considered tracking arbitrary linear classiers with a variant of winnow ( 123 ) , and bousquet and warmuth ( 123 ) studied tracking of a small set of experts via
finally we note that whilst not originally developed as an online algorithm , the sequential minimal optimization ( smo ) algorithm ( 123 ) is closely related , especially when there is no bias term in which case ( 123 ) it effectively becomes the
outline of the paper
in section ii we develop the idea of stochastic gradient in hilbert space .
this provides the basis of our algorithms .
subsequently we show how the general form of the algorithm can be applied to problems of classication , novelty detection , and regression ( section iii ) .
next we es - tablish mistake bounds with moving targets for linear large margin classication algorithms in section iv .
a proof that the stochastic gradient algorithm converges to the minimum of the regularised risk functional is given in section v , and we conclude with experimental results and a discussion in sections vi and vii .
stochastic gradient descent in hilbert space
moreover we assume that
we consider a problem of function estimation , where the goal is to learn a mapping f : x r based on a sequence s = ( ( x123 , y123 ) , .
, ( xm , ym ) ) of examples ( xt , yt ) x y .
there exists a loss function l : r y r , given by l ( f ( x ) , y ) , which penalises the deviation of estimates f ( x ) from observed labels y .
common loss functions include the soft margin loss function ( 123 ) or the logistic loss for classication and novelty detection ( 123 ) , and the quadratic loss , absolute loss , hubers robust loss ( 123 ) and the - insensitive loss ( 123 ) for regression .
we shall discuss these in section iii .
the reason for allowing the range of f to be r rather than y is that it allows for more renement in evaluation of the learning result .
for example , in classication with y = ( 123 , 123 ) we could interpret sgn ( f ( x ) ) as the prediction given by f for the class of x , and |f ( x ) | as the condence in that classication .
we call the output f of the learning algorithm an hypothesis , and denote the set of all possible hypotheses by h .
we will always assume h is a reproducing kernel hilbert there exists a kernel
space ( rkhs ) ( 123 ) .
this means that k : x x r and a dot product h , ih such that
for x x
123 ) k has the reproducing property hf , k ( x , ) ih = f ( x )
123 ) h is the closure of the span of all k ( x , ) with x x .
in other words , all f h are linear combinations of kernel functions .
the inner product h , ih induces a norm on f h in the usual way : ||f||h : = hf , fih123 / 123
an interesting special case is x = rn with k ( x , y ) = hx , yi ( the normal dot - product in rn ) which corresponds to learning linear functions in rn , but much more varied function classes can be learned by using
risk functionals
in batch learning , it is typically assumed that all the exam - ples are immediately available and are drawn independently from some distribution p over x y .
one natural measure of quality for f in that case is the expected risk r ( f , p ) : = e ( x , y ) p ( l ( f ( x ) , y ) ) .
since p is unknown , given s drawn from p m , a standard approach ( 123 ) is to instead minimise the empirical risk
remp ( f , s ) : =
however , minimising remp ( f ) may lead to overtting ( com - plex functions that t well on the training data but do not generalise to unseen data ) .
one way to avoid this is to penalise complex functions by instead minimising the regularised risk
rreg ( f , s ) : = rreg , ( f , s ) : = remp ( f ) +
where > 123 and ||f||h = hf , fi123 / 123h does indeed measure the complexity of f in a sensible way ( 123 ) .
the constant needs to be chosen appropriately for each problem .
if l has parameters ( for example l see later ) , we write remp , ( f , s ) and rreg , , ( f , s ) .
since we are interested in online algorithms , which deal with one example at a time , we also dene an instantaneous approximation of rreg , , the instantaneous regularised risk on a single example ( x , y ) , by
rinst ( f , x , y ) : = rinst , ( f , x , y ) : = rreg , ( f , ( ( x , y ) ) ) .
online setting
in this paper we are interested in online learning , where the examples become available one by one , and it is desired that the learning algorithm produces a sequence of hypotheses f = ( f123 , .
, fm+123 ) .
here f123 is some arbitrary initial hypothesis and fi for i > 123 is the hypothesis chosen after seeing the ( i 123 ) th example .
thus l ( ft ( xt ) , yt ) is the loss the learning algorithm makes when it tries to predict yt , based on xt and the previous examples ( x123 , y123 ) , .
, ( xt123 , yt123 ) .
this kind of learning framework is appropriate for real - time learning problems and is of course analogous to the usual adaptive signal processing framework ( 123 ) .
we may also use an online algorithm simply as an efcient method of approximately solving a batch problem .
the algorithm we propose below
ieee transactions on signal processing , vol .
123 , no .
123 , october 123
can be effectively run on huge data sets on machines with
a suitable measure of performance for online algorithms in
an online setting is the cumulative loss
given : a sequence s = ( ( xi , yi ) ) in ( x y ) ; a regularisation parameter > 123; a truncation parameter n; a learning rate ( 123 , 123 / ) ; a piecewise differentiable convex loss function l : r y r; and a reproducing kernel hilbert space h with reproducing kernel k , norma ( s , l , k , , ) outputs a sequence of hypotheses f = ( f123 , f123 , .
initialise t : = 123; i : = ( 123 ) i
for i = 123 , .
, ; i=max ( 123 , t ) iti123k ( xi , ) ;
t : = l123 ( ft ( xt ) , yt ) ; t : = t + 123;
norma with constant learning rate , exploiting the truncation
thus , at step t the t - th coefcient may receive a non - zero value .
the coefcients for earlier terms decay by a factor ( which is constant for constant t ) .
notice that the cost for training at each step is not much larger than the prediction cost : once we have computed ft ( xt ) , t is obtained by the value of the derivative of l at ( ft ( xt ) , yt ) .
speedups and truncation
there are several ways of speeding up the algorithm .
instead of updating all old coefcients i , i = 123 , .
, t 123 , one may simply cache the power series 123 , ( 123 ) , ( 123 ) 123 , ( 123 ) 123 , .
and pick suitable terms as needed .
this is particularly useful if the derivatives of the loss function l will only assume discrete values , say ( 123 , 123 , 123 ) as is the case when using the soft - margin type loss functions ( see section iii ) .
alternatively , one can also store t = ( 123 ) tt and i=123 ik ( xi , xt ) , which only requires rescaling once t becomes too large for machine precision this exploits the exponent in the standard oating point number representation .
compute ft ( x ) = ( 123 ) tpt123
a major problem with ( 123 ) and ( 123 ) is that without ad - ditional measures , the kernel expansion at time t contains t terms .
since the amount of computations required for predicting grows linearly in the size of the expansion , this is undesirable .
the regularisation term helps here .
at each iteration the coefcients i with i 123= t are shrunk by ( 123 ) .
thus after iterations the coefcient i will be reduced to ( 123 ) i .
hence one can drop small terms and incur little error as the following proposition shows .
proposition 123 ( truncation error ) suppose l ( z , y ) is a loss function satisfying |zl ( z , y ) | c for all z r , y y and k is a kernel with bounded norm kk ( x , ) k x where k k denotes either k kl or k kh .
let ftrunc : = i=max ( 123 , t ) ik ( xi , ) denote the kernel expansion trun - kf ftrunck tx
( 123 ) ticx < ( 123 ) cx / .
cated to terms .
the truncation error satises
obviously the approximation quality increases exponen -
tially with the number of terms retained .
lcum ( f , s ) =
( again , if l has such as , we write lcum , ( f ) etc . ) notice is tested on the example ( xt , yt ) which was that here ft not available for training ft , so if we can guarantee a low cumulative loss we are already guarding against overtting .
regularisation can still be useful the target we are learning changes over time , regularisation prevents the hypothesis from going too far in one direction , thus hopefully helping recovery when a change occurs .
fur - thermore , if we are interested in large margin algorithms , some kind of complexity control is needed to make the denition of the margin meaningful .
in the online setting :
the general idea of the algorithm
the algorithms we study in this paper are classical stochas - tic gradient descent they perform gradient descent with respect to the instantaneous risk .
the general form of the update rule is
ft+123 : = ft t f rinst , ( f , xt , yt )
where for i n , fi h , f is short - hand for / f ( the gradient with respect to f ) and t > 123 is the learning rate which is often constant t = .
in order to evaluate the gradient , note that the evaluation functional f 123 f ( xi ) is given by ( 123 ) , and therefore
f l ( f ( xt ) , yt ) = l123 ( f ( xt ) , yt ) k ( xt , ) ,
where l123 ( z , y ) : = zl ( z , y ) .
since f||f||123h = 123f , the update
ft+123 : = ( 123 ) ft tl123 ( ft ( xt ) , yt ) k ( xt , ) .
clearly , given > 123 , t needs to satisfy t < 123 / for all t for the algorithm to work .
we also allow loss functions l that are only piecewise differentiable , in which case stands for subgradient .
when the subgradient is not unique , we choose one arbitrarily; the choice does not make any difference either in practice or in theoretical analyses .
all the loss functions we consider are convex in the rst argument .
choose a zero initial hypothesis f123 = 123
for the purposes of practical computations , one can write ft as a kernel expansion
where the coefcients are updated at step t via
t : = tl123 ( ft ( xt ) , yt ) i : = ( 123 t ) i
for i = t for i < t .
ieee transactions on signal processing , vol .
123 , no .
123 , october 123
the regularisation parameter can thus be used to control the storage requirements for the expansion .
in addition , it naturally allows for distributions p ( x , y ) that change over time in which cases it is desirable to forget instances ( xi , yi ) that are much older than the average time scale of the distribution
we call our algorithm norma ( naive online rreg min - imisation algorithm ) and sometimes explicitly write the pa - rameter : norma .
norma is summarised in figure 123
in the applications discussed in the next section it is sometimes necessary to introduce additional parameters that need to be updated .
we nevertheless refer somewhat loosely to the whole family of algorithms as norma .
the general idea of norma can be applied to a wide range of problems .
we utilise the standard ( 123 ) addition of the constant offset b to the function expansion , i . e .
g ( x ) : = f ( x ) + b where f h and b r .
hence we also update b
bt+123 : = bt brinst ( g , xt , yt )
in ( binary ) classication , we have y = ( 123 ) .
the most obvious loss function to use in this context is l ( f ( x ) , y ) = 123 if yf ( x ) 123 and l ( f ( x ) , y ) = 123 otherwise .
thus , no loss is incurred if sgn ( f ( x ) ) is the correct prediction for y; otherwise we say that f makes a mistake at ( x , y ) and charge a unit loss .
however , the mistake loss function has some drawbacks : a ) it fails to take into account the margin yf ( x ) that can be considered a measure of condence in the correct prediction , a non - positive margin meaning an actual mistake; b ) the mistake loss is discontinuous and non - convex and thus is unsuitable for use in gradient based algorithms .
in order to deal with these drawbacks the main loss function
we use here for classication is the soft margin loss
l ( f ( x ) , y ) : = max ( 123 , yf ( x ) )
where 123 is the margin parameter .
the soft margin loss l ( f ( x ) , y ) is positive if f fails to achieve a margin at least on ( x , y ) ; in this case we say that f made a margin error .
if f made an actual mistake , then l ( f ( x ) , y ) .
let t be an indicator of whether ft made a margin error on ( xt , yt ) , i . e . , t = 123 if ytft ( xt ) and zero otherwise .
( ft ( xt ) , yt ) = tyt =
and the update ( 123 ) becomes
ft+123 : = ( 123 ) ft + tytk ( xt , ) bt+123 : =bt + tyt .
suppose now that x > 123 is a bound such that k ( xt , xt )
x 123 holds for all t .
since ||f123||h = 123 and
||ft+123||h ( 123 ) ||ft||h + ||k ( xt , ) ||h = ( 123 ) ||ft||h + k ( xt , xt ) 123 / 123 ,
we obtain ||ft||h x / for all t .
furthermore , |ft ( xt ) | = |hft , k ( xt , ) ih | x 123 / .
hence , when the offset parameter b is omitted ( which we consider particularly in sections iv and v ) , it is reasonable to require x 123 / .
then the loss function becomes effectively bounded , with l ( ft ( xt ) , yt ) 123x 123 / for all t .
the update in terms of i is ( for i = 123 , .
, t 123 ) ( i , t , b ) : = ( ( 123 ) i , tyt , b + tyt ) .
when = 123 and = 123 we recover the kernel perceptron ( 123 ) .
if = 123 and > 123 we have a kernel perceptron with
for classication with the - trick ( 123 ) we also have to take
care of the margin , since there ( recall g ( x ) = f ( x ) + b )
l ( g ( x ) , y ) : = max ( 123 , yg ( x ) ) .
since one can show ( 123 ) that the specic choice of has no inuence on the estimate in - sv classication , we may set = 123 and obtain the update rule ( for i = 123 , .
, t 123 ) ( i , t , b , ) : = ( ( 123 ) i , tyt , b+ tyt , + ( t ) ) .
novelty detection
novelty detection ( 123 ) is like classication without labels .
it is useful for condition monitoring tasks such as network intrusion detection .
the absence of labels yi means the algo - rithm is not precisely a special case of norma as presented earlier , but one can derive a variant in the same spirit .
the - setting is most useful here as it allows one to specify an upper limit on the frequency of alerts f ( x ) < .
the loss function to be utilised is
l ( f ( x ) , x , y ) : = max ( 123 , f ( x ) )
and usually ( 123 ) one uses f h rather than g = f + b where b r in order to avoid trivial solutions .
the update rule is ( for i = 123 , .
, t 123 )
( i , t , ) : =
( ( 123 ) i , , + ( 123 ) ) ( ( 123 ) i , 123 , )
if f ( x ) <
consideration of the update for shows that on average only a fraction of observations will be considered for updates .
thus it is necessary to store only a small fraction of the xis .
we consider the following three settings : squared loss , the - insensitive loss using the - trick , and hubers robust loss function , i . e .
trimmed mean estimators .
for convenience we will only use estimates f h rather than g = f + b where b r .
the extension to the latter case is straightforward .
123 ( y f ( x ) ) 123
con - sequently the update equation is ( for i = 123 , .
, t 123 )
123 ) squared loss : here l ( f ( x ) , y ) : = 123
( i , t ) : = ( ( 123 ) i , ( yt f ( xt ) ) ) .
this means that we have to store every observation we make , or more precisely , the prediction error we made on the
ieee transactions on signal processing , vol .
123 , no .
123 , october 123
123 ) - insensitive loss : the use of
l ( f ( x ) , y ) = max ( 123 , |yf ( x ) | ) introduces a new parameter the width of the insensitivity zone .
by making a variable of the optimisation problem we have
by the algorithm on s .
two key quantities are the number of mistakes , given by
m ( f , s ) : = | ( 123 t m | ytft ( xt ) 123 ) | ,
l ( f ( x ) , y ) : = max ( 123 , |y f ( x ) | ) + .
and the number of margin errors , given by
the update equations now have to be stated in terms of i , t , and which is allowed to change during the optimisation process .
setting t : = yt f ( xt ) the updates are ( for i = 123 , .
, t 123 )
( i , t , ) : =
( ( 123 ) i , sgn t , + ( 123 ) ) ( ( 123 ) i , 123 , )
if |t| >
this means that every time the prediction error exceeds , we increase the insensitive zone by .
if it is smaller than , the insensitive zone is decreased by ( 123 ) .
123 ) hubers robust loss : this loss function was proposed in ( 123 ) for robust maximum likelihood estimation among a family of unknown densities .
it is given by
( |y f ( x ) | 123 123 ( y f ( x ) ) 123
l ( f ( x ) , y ) : =
123 if |y f ( x ) |
setting t : = yt f ( xt ) the updates are ( for i = 123 , .
, t 123 )
( i , t ) : =
( ( 123 ) i , sgn t ) ( ( 123 ) i , 123t )
if |t| >
comparing ( 123 ) with ( 123 ) leads to the question of whether might also be adjusted adaptively .
this is a desirable goal since we may not know the amount of noise present in the data .
while the - setting allowed the formation of adaptive estimators for batch learning with the - insensitive loss , this goal has proven elusive for other estimators in the standard
in the online situation , however , such an extension is quite natural ( see also ( 123 ) ) .
it is merely necessary to make a variable of the optimisation problem and the updates become ( for i = 123 , .
, t 123 ) ( i , t , ) : =
( ( 123 ) i , sgn t , + ( 123 ) ) ( ( 123 ) i , 123t , )
if |t| >
mistake bounds for non - stationary targets in this section we theoretically analyse norma for clas - sication with the soft margin loss with margin .
in the process we establish relative bounds for the soft margin loss .
a detailed comparative analysis between norma and gentiles alma ( 123 ) can be found in ( 123 ) .
m ( f , s ) : = | ( 123 t m | ytft ( xt ) ) | .
notice that margin errors are those examples on which the gradient of the soft margin loss is non - zero , so m ( f , s ) gives the size of the kernel expansion of nal hypothesis fm+123
we use t to denote whether a margin error was made at trial t , i . e . , t = 123 if ytft ( xt ) and t = 123 otherwise .
thus the soft margin loss can be written as l ( ft ( xt ) , yt ) = t ( ytft ( xt ) ) and consequently lcum , ( f , s ) denotes the total soft margin loss of the algorithm .
in our bounds we compare the performance of norma to the performance of function sequences g = ( g123 , .
, gm ) from some comparison class g hm .
notice that we often use a different margin 123= for the comparison sequence , and t always refers to the margin errors of the actual algorithm with respect to its margin .
we always
l ( g ( x ) , y ) yg ( x ) .
we extend the notations m ( g , s ) , m ( g , s ) , l ( gt , yt ) and lcum , ( g , s ) to such comparison sequences in the obvious
a preview
to understand the form of the bounds , consider rst the case of a stationary target , with comparison against a constant sequence g = ( g , .
with = = 123 , our algorithm becomes the kernelised perceptron algorithm .
assuming that some g achieves m ( g , s ) = 123 for some > 123 , the kernelised version of the perceptron convergence theorem ( 123 ) , ( 123 )
m ( f , s ) ||g||123h max
consider now the more general case where the sequence is not linearly separable in the feature space .
then ideally we would wish for bounds of the form
m ( f , s ) min
m ( g , s ) + o ( m ) ,
which would mean that the mistake rate of the algorithm would converge to the mistake rate of the best comparison function .
unfortunately , even approximately minimising the number of mistakes over the training sequence is very difcult , so such strong bounds for simple online algorithms seem unlikely .
instead , we settle for weaker bounds of the form m ( f , s )
lcum , ( g , s ) / + o ( m ) ,
we consider the performance of the algorithm for a xed sequence of observations s : = ( ( x123 , y123 ) , .
, ( xm , ym ) ) and study the sequence of hypotheses f = ( f123 , .
, fm ) , produced
where lcum , ( g , s ) / is an upper bound for m ( g , s ) , and the norm bound b appears as a constant in the o ( m ) term .
for earlier bounds of this form , see ( 123 ) , ( 123 ) .
ieee transactions on signal processing , vol .
123 , no .
123 , october 123
in the non - stationary case , we consider comparison classes
which are allowed to change slowly , that is
g ( b , d123 , d123 )
||gt gt+123||h d123
||gt gt+123||123h d123 and ||gt||h b
the parameter d123 bounds the total distance travelled by the target .
ideally we would wish the target movement to result in an additional o ( d123 ) term in the bounds , meaning there would be a constant cost per unit step of the target .
unfortunately , for technical reasons we also need the d123 parameter which restricts the changes of speed of the target .
the meaning of the d123 parameter will become clearer when we state our bounds and discuss them .
choosing the parameters is an issue in the bounds we have .
the bounds depend on the choice of the learning rate and margin parameters , and the optimal choices depend on quan - tities ( such as ming lcum , ( g , s ) ) that would not be available when the algorithm starts .
in our bounds , we handle this by assuming an upper bound k ming lcum , ( g , s ) that can be used for tuning .
by substituting k = ming lcum , ( g , s ) , we obtain the kind of bound we discussed above; otherwise the estimate k replaces ming lcum , ( g , s ) in the bound .
in a practical application , one would probably be best served to ignore the formal tuning results in the bounds and just tune the parameters by whatever empirical methods are pre - ferred .
recently , online algorithms have been suggested that dynamically tune the parameters to almost optimal values as the algorithm runs ( 123 ) , ( 123 ) .
applying such techniques to our analysis remains an open problem .
relative loss bounds
recall that the update for the case we consider is
ft+123 : = ( 123 ) ft + tytk ( xt , ) .
it will be convenient to give the parameter tunings in terms
of the function
h ( x , k , c ) =
where we assume x , k and c to be positive .
notice that 123 h ( x , k , c ) x holds , and limk123+ h ( x , k , c ) = x / 123
accordingly , we dene h ( x , 123 , c ) = x / 123
we start by analysing margin errors with respect to a given
theorem 123 suppose f is generated by ( 123 ) on a sequence s of length m .
let x > 123 and suppose that k ( xt , xt ) x 123 for all t .
fix k 123 , b > 123 , d123 123 and d123 123
and , given parameters > 123 , , k , c ) / x 123
choose the regularisation parameter
md123 + d123
let 123 = 123h (
b123 + b
123 x 123 ( cid : 123 )
and the learning rate parameter = 123 / ( 123 + 123 ) .
if for some g g ( b , d123 , d123 ) , we have lcum , ( g , s ) k then
m ( f , s ) k
( ) 123 +
the proof can be found in appendix a .
we now consider obtaining mistake bounds from our margin error result .
the obvious method is to set = 123 , turning margin errors directly to mistakes .
interestingly , it turns out that a subtly different choice of parameters allows us to obtain the same mistake bound using a non - zero margin .
theorem 123 suppose f is generated by ( 123 ) on a sequence s of length m .
let x > 123 and suppose that k ( xt , xt ) x 123 for all t .
fix k , b , d123 , d123 and dene c as in ( 123 ) , and given > 123 let 123 = 123r / x 123 where r = h ( , k , c ) .
choose the regularisation parameter as in ( 123 ) , the learning rate = 123 / ( 123 + 123 ) , and set the margin to either = 123 or = r .
then for either of these margin settings , if there exists a comparison sequence g g ( b , d123 , d123 ) such that lcum , ( g , s ) k , we have
m ( f , s ) k
the proof of theorem 123 is also in appendix a .
to gain intuition about theorems 123 and 123 , consider rst the separable case k = 123 with a stationary target ( d123 = d123 = 123 ) .
in this special case , theorem 123 gives the familiar bound from the perceptron convergence theorem .
theorem 123 gives an upper bound of x 123b123 / ( ) 123 margin errors .
the choices given for in theorem 123 for the purpose of minimising the mistake bound are in this case = 123 and = / 123
notice that the latter choice results in a bound of 123x 123b123 / margin errors .
more generally , if we choose = ( 123 ) for some 123 < < 123 and assume to be the largest margin for which separation is possible , we see that the algorithm achieves in o ( 123 ) iterations a margin within a factor 123 of optimal .
this bound is similar to that for alma ( 123 ) , but alma is much more sophisticated in that it automatically tunes its parameters .
removing the separability assumption leads to an additional k / term in the mistake bound , as we expected .
to see the effects of the d123 and d123 terms , assume rst that the target has constant speed : ||gt gt+123||h = for all t where > 123 is a constant .
then d123 = m and d123 = m123 , so md123 = d123
md123 > d123
if the speed is not constant , we always have an extreme case would be ||g123 g123||h = d123 , gt+123 = gt for t > 123
then md123
thus the d123 term increases the bound in case of changing target speed .
convergence of norma
a preview
next we study the performance of norma when it comes to minimising the regularised risk functional rreg ( f , s ) , of which rinst ( f , xt , yt ) is the stochastic approximation at time t .
we show that under some mild assumptions on the loss function ,
ieee transactions on signal processing , vol .
123 , no .
123 , october 123
the average instantaneous risk ( 123 / m ) pm of the average hypothesis ( 123 / m ) pm
t=123 rinst ( ft , xt , yt ) of the hypotheses ft of norma converges towards the mini - mum regularised risk ming rreg ( g , s ) at rate o ( m123 / 123 ) .
this requires no probabilistic assumptions .
if the examples are i . i . d . , then with high probability the expected regularised risk t=123 ft similarly converges towards the minimum expected risk .
convergence can also be guaranteed for the truncated version of the algorithm that keeps its kernel expansion at a sublinear size .
norma with learning rate t = t123 / 123
then for any g h
rinst , ( ft , xt , yt ) mrreg , ( g , s ) + am123 / 123 + b
where a = 123u 123 ( 123 + 123 / ( ) ) , b = u 123 / ( 123 ) and u is as
assumptions and notation
we assume a bound x > 123 such that k ( xt , xt ) x 123 for all t .
then for all g h , |g ( xt ) | = |hg , k ( xt , ) ih | x||g||h .
we assume that the loss function l is convex in its rst argument and also satises for some constant c > 123 the
|l ( z123 , y ) l ( z123 , y ) | c|z123 z123|
for all z123 , z123 r , y y .
fix now > 123
the hypotheses ft produced by ( 123 ) ||ft+123||h = || ( 123 t ) ft tl123 ( f ( xt ) , yt ) k ( xt , ) ||h
( 123 t ) ||ft||h + tcx ,
and since f123 = 123 we have for all t the bound ||ft||h u
u : = cx
since |l123 ( f ( xt ) , yt ) | c , we have ||f l ( f ( xt ) , yt ) ||h cx and ||f rinst ( f , xt , yt ) |||h cx + ||f||h 123cx for any f such that ||f||h u .
fix a sequence s and for 123 < < 123 dene
g : = argmin
g : = ( 123 ) g .
then 123 rreg ( g , s ) rreg ( g , s )
( l ( g ( xt ) , yt ) l ( g ( xt ) , yt ) )
cx||g g||h + = cx||g||h ||g||123h + 123
( ( 123 ) 123 123 ) ||g||123h
considering the limit 123+ shows that ||g||h u where u is as in ( 123 ) .
basic convergence bounds
we start with a simple cumulative risk bound .
to achieve
convergence , we use a decreasing learning rate .
theorem 123 fix > 123 and 123 < < 123 / .
assume that l is convex and satises ( 123 ) .
let the example sequence s = t=123 be such that k ( xt , xt ) x 123 holds for all t , and let ( f123 , .
, fm+123 ) be the hypothesis sequence produced by
the proof , given in appendix b , is based on analysing the progress of ft towards g at update t .
the basic technique is from ( 123 ) , ( 123 ) , and ( 123 ) shows how to adjust the learning rate ( in a much more complicated setting than we have here ) .
note that ( 123 ) holds in particular for g = g , so
rinst , ( ft , xt , yt ) rreg , ( g , s ) + o ( m123 / 123 )
where the constants depend on x , c and the parameters of the algorithm .
however , the bound does not depend on any probabilistic assumptions .
if the example sequence is such that some xed predictor g has a small regularised risk , then the average regularised risk of the on - line algorithm will also be
consider now the implications of theorem 123 to a situation in which we assume that the examples ( xt , yt ) are i . i . d .
according to some xed distribution p .
the bound on the cumulative risk can be transformed into a probabilistic bound by standard methods .
we assume that k ( x , x ) x 123 with probability 123 for ( x , y ) p .
we say that the risk is bounded by l if with probability 123 we have rinst , ( f , xt , yt ) l for all t and f ( g , f123 , .
, fm+123 ) .
as an example , consider the soft margin loss .
by the preceding remarks , we can assume ||f||h x / .
this implies |f ( xt ) | x 123 / so the interesting values of satisfy 123 x 123 / .
hence l ( f ( xt ) , yt ) 123x 123 / , and we can take l = 123x 123 / ( 123 ) .
if we wish to use an offset parameter b , a bound for |b| needs to be obtained and incorporated into l .
similarly , for regression type loss functions we may need a bound for |yt| .
the result of cesa - bianchi et al .
for bounded convex loss functions ( 123 , theorem 123 ) now directly gives the following .
is bounded by l .
let fm = ( 123 / m ) pm123
corollary 123 assume that p is a probability distribution over x y such that k ( x , x ) x 123 holds with probability 123 for ( x , y ) p , and let the example sequence s = ( ( xt , yt ) ) m be drawn i . i . d .
according to p .
fix > 123 and 123 < < 123 / .
assume that l is convex and satises ( 123 ) , and that the risk t=123 ft where ft is the t - th hypothesis produced by norma with learning rate t = t123 / 123
then for any g h and 123 < < 123 , and for a and b as in theorem 123 , we have e ( x , y ) p rinst , ( fm , x , y ) rreg , ( g , s ) +
a + l ( 123 ln ( 123 / ) ) 123 / 123 ( cid : 123 )
with probability at least 123 over random draws of s .
to apply corollary 123 , choose g = g where
g = argmin
e ( x , y ) p rinst , ( f , x , y ) .
ieee transactions on signal processing , vol .
123 , no .
123 , october 123
with high probability , rreg , ( g , s ) will be e ( x , y ) p rinst , ( g , x , y ) , e ( x , y ) p rinst , ( fm , x , y ) will be close to the minimum
effects of truncation
we now consider a version where at time t the hypothesis consist of a kernel expansion of size st , where we allow st to slowly ( sublinearly ) increase as a function of t
t , tk ( xt , x )
where t , t123 is the coefcient of k ( xt , ) in the kernel expansion at time t123
for simplicity , we assume st+123 ( st , st + 123 ) and include in the expansion even the terms where t , t = 123
thus at any update we add a new term to the kernel expansion; if st+123 = st we also drop the oldest previously remaining term .
we can then write
ft+123 = ft tf rinst ( f , xt , yt ) |f =ft t
where t = 123 if st+123 = st + 123 and t = tst , tk ( xtst , ) otherwise .
since t , t123+123 = ( 123 t123 ) t , t123 , we see that the kernel expansion coefcients decay almost geometrically .
however , since we also need to use a decreasing learning rate t = t123 / 123 , the factor 123 t approaches 123
therefore it is somewhat complicated to choose expansion sizes st that are not large but still guarantee that the cumulative effect of the t terms remains under control .
theorem 123 assume that l is convex and satises ( 123 ) .
let the t=123 be such that k ( xt , xt ) example sequence s = ( ( xt , yt ) ) m x 123 holds for all t .
fix > 123 , 123 < < 123 / and 123 < < 123 / 123
then there is a value t123 ( , , ) such that the following holds when we dene st = t for t t123 ( , , ) and st = dt123 / 123+e for t > t123 ( , , ) .
let ( f123 , .
, fm+123 ) be the hypothesis sequence produced by truncated norma with learning rate t = t123 / 123 and expansion sizes st .
then for any g h we have rinst , ( ft , xt , yt ) mrreg , ( g , s ) + am123 / 123 + b
where a = 123u 123 ( 123 + 123 / ( ) ) , b = u 123 / ( 123 ) and u is as
the proof , and the denition of t123 , is given in appendix c .
conversion of the result to a probabilistic setting can be done as previously , although an additional step is needed to estimate how the t terms may affect the maximum norm of ft; we omit the details .
the mistake bounds in section iv are of course only worst - case upper bounds , and the constants may not be very tight .
hence we performed experiments to evaluate the performance of our stochastic gradient descent algorithms in practice .
our bounds suggest that some form of regularisation is useful when the target is moving , and forcing a positive margin may give an additional benet .
this hypothesis was tested using articial data , where we used a mixture of 123 - dimensional gaussians for the positive examples and another for negative ones .
we removed all examples that would be misclassied by the bayes - optimal classier ( which is based on the actual distribution known to us ) or are close to its decision boundary .
this gave us data that were cleanly separable using a gaussian kernel .
in order to test the ability of norma to deal with changing underlying distributions we carried out random changes in the parameters of the gaussians .
we used two movement
in the drifting case , there is a relatively small parameter
change after every ten trials .
in the switching case , there is a very large parameter
change after every 123 trials .
thus , given the form of our bounds , all other things being equal , our mistake bound would be much better in the drifting than in the switching case .
in either case , we ran each algorithm for 123 trials and cumulatively summed up the mistakes made by them .
in our experiments we compared norma , with alma ( 123 ) with p = 123 and the basic perceptron algorithm ( which is the same stochastic gradient descent with the margin in the loss function ( 123 ) and weight decay parameter both set to zero ) .
we also considered variants norma , 123 and alma123 where the margin is xed to zero .
these algorithms are included to see whether regularisation , either by weight decay as in norma or by a norm bound as in alma , helps predicting a moving target even when we are not aiming for a large margin .
we used gaussian kernels to handle the non - linearity of the data .
for these experiments , the parameters of the algorithms were tuned by hand optimally for each example distribution .
figure 123 shows the cumulative mistake counts for the algorithms .
there does not seem to be any decisive differences between the algorithms .
in particular , norma works quite well , also on switching data , even though our bound suggests otherwise ( which is probably due to slack in the bound ) .
in general , it does seem that using a positive margin is better than xing the margin to zero , and regularisation even with zero margin is better than the basic perceptron algorithm .
novelty detection
in our experiments we studied the performance of the novelty detection variant of norma given by ( 123 ) for various kernel parameters and values of .
we performed experiments on the usps database of hand - written digits ( 123 scanned images of handwritten digits at a resolution of 123 123 pixels , out of which 123 were chosen for training and 123 for testing purposes ) .
already after one pass through the database , which took in matlab less than 123s on a 123mhz celeron , the results can be used for weeding out badly written digits ( cf .
the left plot
ieee transactions on signal processing , vol .
123 , no .
123 , october 123
with the goal of understanding the advantage of securing a large margin when tracking a drifting problem .
on the positive side , we have obtained theoretical bounds that give some guidance to the effects of the margin in this case .
on the negative side , the bounds are not that well corroborated by the experiments we performed .
this work was supported by the australian research coun - cil .
thanks to paul wankadia for help with the implementation and to ingo steinwart and ralf herbrich for comments and
proofs of theorems 123 and 123
the following technical lemma , which is proved by a simple differentiation , is used in both proofs for choosing the optimal
lemma 123 given k > 123 , c > 123 and > 123 dene f ( z ) = k / ( z ) + c / ( z ( z ) ) for 123 < z < .
then f ( z ) is maximised for z = h ( , k , c ) where h is as in ( 123 ) , and the maximum value is
f ( h ( , k , c ) ) = k
the main idea in the proofs is to lower bound the progress at update t , which we dene as ||gt ft||123h||gt+123 ft+123||123h .
for notational convenience we introduce gm+123 : = gm .
t+123 = ft + 123tytk ( xt , ) .
proof of theorem 123 : dene f123 we split the progress into three parts : ||gt ft||123h ||gt+123 ft+123||123h = ( ||gt ft||123h ||gt f123
t+123||123h ||gt ft+123||123h )
+ ( ||gt f123 + ( ||gt ft+123||123h ||gt+123 ft+123||123h ) .
by substituting the denition of f123 t+123 , using ( 123 ) and applying tl ( gt ( xt ) , yt ) l ( gt ( xt ) , yt ) , we can estimate the rst part of ( 123 ) as
||gt ft||123h ||gt f123
= 123tyt hk ( xt , ) , gt ftih ||ft f123 = 123tyt ( gt ( xt ) ft ( xt ) ) 123tk ( xt , xt ) 123 ( t l ( gt ( xt ) , yt ) )
123 ( t l ( ft ( xt ) , yt ) ) 123tx 123
for the second part of ( 123 ) , we have
t+123||123h ||gt ft+123||123h
= ||ft+123 f123 t+123 ft+123 = f123
t+123 = ft+123 / ( 123 ) , we have
t+123 ft+123 , ft+123 gt
t+123||123h + 123 ( cid : 123 ) f123
mistakes made by the algorithms on drifting data ( top ) and on switching data ( bottom ) .
of figure 123 ) .
we chose = 123 to allow for a xed fraction of detected outliers .
based on the theoretical analysis of section v we used a decreasing learning rate with t t 123 figure 123 shows how the algorithm improves in its assess - ment of unusual observations ( the rst digits in the left table are still quite regular but degrade rapidly ) .
it could therefore be used as an online data lter .
we have shown how the careful application of classical stochastic gradient descent can lead to novel and practical algorithms for online learning using kernels .
the use of regularisation ( which is essential for capacity control when using the rich hypothesis spaces generated by kernels ) allows for truncation of the basis expansion and thus computationally efcient hypotheses .
we explicitly developed parameterisa - tions of our algorithm for classication , novelty detection and regression .
the algorithm is the rst we are aware of for online novelty detection .
furthermore , its general form is very efcient computationally and allows the easy application of kernel methods to enormous data sets , as well , of course , to real - time online problems .
we also presented a theoretical analysis of the algorithm when applied to classication problems with soft margin
123trialsmistakesperceptronalma123norma123almanorma123trialsmistakesperceptronalma123norma123almanorma ieee transactions on signal processing , vol .
123 , no .
123 , october 123
results of online novelty detection after one pass through the usps database .
the learning problem is to discover ( online ) novel patterns .
we used gaussian rbf kernels with width 123 = 123d = 123 and = 123 .
the learning rate was 123 .
left : the rst 123 patterns which incurred a margin error it can be seen that the algorithm at rst nds even well formed digits novel , but later only nds unusually written ones; middle : the 123 worst patterns according to f ( x ) on the training set they are mostly badly written digits , right : the 123 worst patterns on an unseen test set .
t+123 ft+123 , ft+123 gt
( cid : 123 ) ( ||ft+123||123h hft+123 , gtih
hence , recalling the denition of , we get
t+123||123h ||gt ft+123||123h
= ( cid : 123 ) 123 + 123 ( cid : 123 ) ||ft+123||123h 123hft+123 , gtih . ( 123 )
for the third part of ( 123 ) we have
||gt ft+123||123h ||gt+123 ft+123||123h
= ||gt||123h ||gt+123||123h + 123hgt+123 gt , ft+123ih
substituting ( 123 ) , ( 123 ) and ( 123 ) into ( 123 ) gives us
||gt ft||123h ||gt+123 ft+123||123h
123 ( t l ( gt ( xt ) , yt ) )
123 ( t l ( ft ( xt ) , yt ) ) + ||gt||123h ||gt+123||123h + h ( ft+123 )
h ( f ) = ( cid : 123 ) 123 + 123 ( cid : 123 ) ||f||123h
123hf , gtih + 123hgt+123 gt , fih .
to bound h ( ft+123 ) from below , we write
h ( f ) = a||f||123h 123hr , fih = a||f r / a||123h ||r||123h / a
where a = 123 + 123 and r = ( 123 + 123 ) gt gt+123
hence ,
123 + 123 ( ||gt gt+123||h + 123||gt||h ) 123 123 + 123 + 123||gt gt+123||h||gt||h + 123||gt||123h
since 123 / ( 123 + 123 ) > 123 / 123 , ( 123 ) and ( 123 ) give
||gt ft||123h ||gt+123 ft+123||123h
123 ( t l ( ft ( xt ) , yt ) ) + 123 ( t l ( gt ( xt ) , yt ) ) 123tx 123 + ||gt||123h ||gt+123||123h + 123||gt||h||gt+123 gt||h + 123||gt||123h
by summing ( 123 ) over t = 123 , .
, m and using the assumption that g g ( b , d123 , d123 ) we obtain
||g123 f123||123h ||gm+123 fm+123||123h
123lcum , ( f , s ) 123lcum , ( g , s )
+ 123m ( f , s ) ( cid : 123 ) 123 123 123x 123 ( cid : 123 )
+ ||g123||123h ||gm+123||123h
+ 123bd123 + m123b123
maximised for z = pd123 / ( mb123 ) , we choose as in ( 123 )
now appears only in ( 123 ) as a subexpression q ( 123 ) z zmb123
since the function q ( z ) is where q ( z ) = d123 which gives q ( 123 ) = 123b md123
we assume f123 = 123 , so ||g123 f123||123h ||gm+123 fm+123||123h ||g123||123h .
by moving some terms around and estimating ||gm+123||h b and lcum , ( g , s ) k we get
lcum , ( f , s ) + m ( f , s ) ( cid : 123 ) 123x 123 / 123 ( cid : 123 )
k + b123 + b ( md123 + d123 )
to get a bound for margin errors , notice that the value 123 given in the theorem satises 123x 123 > 123
we make the trivial estimate lcum , ( f , s ) 123 , which gives us
m ( f , s )
+ b123 + b (
md123 + d123 ) 123 ( 123x 123 / 123 ) .
the bound follows by applying lemma 123 with = and z = 123x 123 / 123
ieee transactions on signal processing , vol .
123 , no .
123 , october 123
proof of theorem 123 : the claim for = 123 follows directly from theorem 123
for non - zero , we take ( 123 ) as our starting point .
we choose 123 = 123 ( ) / x 123 , so the term with m ( f , s ) vanishes and we get
lcum , ( f , s ) k + x 123 ( b123 + b (
since lcum , ( f , s ) m ( f , s ) , this implies
md123 + d123 ) )
+ x 123 ( b123 + b (
m ( f , s ) k
the claim follows from lemma 123 with = and z = .
md123 + d123 ) )
we use this to estimate ||t||h .
if st+123 = t + 123 , then clearly t = 123 , so we consider the case t t123 ( , , ) .
let r = tst , so ||t||h x|r , t| .
we have |r , r| rc , and |r , r+ +123| = ( 123r+ ) |r , r+| ( 123t ) |r , r+| for = 123 , .
, st123
|r , t| rc ( 123 t ) st rc
since / t123 / 123 123 , we have
so |r , t| rc exp ( t ) rct123 / 123
finally , since r t / 123 , we have r 123t , so
in particular , we have ||t||h 123tcx , so ||ft+123||h ( 123 t ) ||ft||h
+ t|l123 ( ft ( xt , y ) ) |||k ( xt , ) ||h + ||t||h
( 123 t ) ||ft||h + 123tcx .
since f123 = 123 , we get ||ft||h 123cx / .
again , without loss of generality we can assume g = g and thus in particular ||ft g||h 123cx / .
to estimate the progress at trial t , let ft+123 = ft+123 + t be
the new hypothesis before truncation .
we write
||ft g||123h ||ft+123 g||123h
= ||ft g||123h || ft+123 g||123h
+ || ft+123 g||123h ||ft+123 g||123h .
to estimate ( 123 ) we write || ft+123 g||123h ||ft+123 g||123h
= || ( ft+123 ft+123 ) + ( ft+123 g ) ||123h ||ft+123 g||123h = 123ht , ft+123 gih + ||t||123h
t c123x 123
by combining this with the estimate ( 123 ) for ( 123 ) we get
||ft g||123h ||ft+123 g||123h
t c123x 123 123t ( rinst ( g , xt , yt ) rinst ( ft , xt , yt ) ) ;
notice the similarity to ( 123 ) .
the rest follows as in the proof of theorem 123
proof of theorem 123
without loss of generality we can assume g = g , and in
particular ||g||h u .
first notice that
||ft g||123h ||ft+123 g||123h
= ||ft+123 ft||123h 123hft+123 ft , ft gih
t ||f rinst ( f , xt , yt ) |f =ft||h
+ 123t hf rinst ( f , xt , yt ) |f =ft , ft gih 123t ( rinst ( g , xt , yt ) rinst ( ft , xt , yt ) )
t c123x 123
where we used the lipschitz property of l and the convexity of rinst in its rst argument .
this leads to
||ft g||123h 123 ( cid : 123 ) ||ft g||123h ||ft+123 g||123h ( cid : 123 )
+ 123u 123
123tc123x 123 123rinst ( g , xt , yt ) + 123rinst ( ft , xt , yt )
123 , and noticing that some terms telescope and pm
since ||ft+123 g||h 123u .
by summing over t = 123 ,
123m123 / 123 , we get
123c123x 123m123 / 123 123
rinst ( g , xt , yt )
rinst ( ft , xt , yt ) + 123u 123
( m + 123 ) 123 / 123
the claim now follows by rearranging terms and estimating ||f123g||h u , ||fm+123g||123h 123 and ( m+123 ) 123 / 123 m123 / 123
proof of theorem 123
such that the following hold for all t t123 ( , , ) :
first , let us dene t123 ( , , ) to be the smallest possible exp ( t ) t123 / 123 and
