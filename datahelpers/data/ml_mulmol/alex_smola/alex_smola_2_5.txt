we present a simple sparse greedy technique to approximate the maximum a posteriori estimate of gaussian processes with much improved scaling behaviour in the sample size m .
in particular , computational requirements are o ( n123m ) , storage is o ( nm ) , the cost for prediction is 123 ( n ) and the cost to compute confidence bounds is o ( nm ) , where n : m .
we show how to compute a stopping criterion , give bounds on the approximation error , and show applications to large scale problems .
gaussian processes have become popular because they allow exact bayesian analysis with simple matrix manipulations , yet provide good performance .
they share with support vector machines and regularization networks the concept of regularization via reproducing kernel hilbert spaces ( 123 ) , that is , they allow the direct specification of the smoothness properties of the class of functions under consideration .
however , gaussian processes are not always the method of choice for large datasets , since they involve evaluations of the covariance function at m points ( where m denotes the sample size ) in order to carry out inference at a single additional point .
this may be rather costly to implement - practitioners prefer to use only a small number of basis functions ( le .
covariance function evaluations ) .
furthermore , the maximum a posteriori ( map ) estimate requires computation , storage , and inversion of the full m x m covariance matrix kii = k ( xi , xi ) where xl ! . . .
, xm are training patterns .
while there exist techniques ( 123 , 123 ) to reduce the computational cost of finding an estimate to o ( km123 ) rather than o ( m123 ) when the covariance matrix contains a significant number of small eigenvalues , all these methods still require computation and storage of the full covariance matrix .
none of these methods addresses the problem of speeding up the prediction stage ( except for the rare case when the integral operator corresponding to the kernel can be diagonalized analytically ( 123 ) ) .
we devise a sparse greedy method , similar to those proposed in the context of wavelets ( 123 ) , solutions of linear systems ( 123 ) or matrix approximation ( 123 ) that finds
supported by the dfg ( sm 123 - 123 ) and the australian research council .
an approximation of the map estimate by expanding it in terms of a small subset of kernel functions k ( xi , .
briefly , the technique works as follows : given a set of ( already chosen ) kernel functions , we seek the additional function that increases the posterior probability most .
we add it to the set of basis functions and repeat until the maximum is approximated sufficiently well .
a similar approach for a tight upper bound on the posterior probability gives a stopping criterion .
123 gaussian process regression
consider a finite set x = ( xl . ' " xm ) of inputs .
in gaussian process regression , we assume that for any such set there is a covariance matrix k with elements kij = k ( xi , xj ) .
we assume that for each input x there is a corresponding output y ( x ) , and that these outputs are generated by where t ( x ) and e are both normal random variables , with e rv n ( o , ( 123 ) and t = ( t ( xl ) , . . .
, t ( xm ) ) t rv n ( o , k ) .
we can use bayes theorem to determine the distribution of the output y ( x ) at a ( new ) input x .
conditioned on the data ( x , y ) , the output y ( x ) is normally distributed .
it follows that the mean of this distribution is the maximum a posteriori probability ( map ) estimate of y .
we are interested in estimating this mean , and also the variance .
y ( x ) = t ( x ) + e
it is possible to give an equivalent parametric representation of y that is more con ( cid : 123 ) venient for our purposes .
we may assume that the vector y = ( y ( xl ) " " , y ( xm ) ) t of outputs is generated by where a rv n ( o , k - 123 ) and e rv n ( o , ( 123 123 ) .
consequently the posterior probability p ( aly , x ) over the latent variables a is proportional to
exp ( - 123;123iy - ka123 123 ) exp ( - ! a tka )
and the conditional expectation of y ( x ) for a ( new ) location x is e ( y ( x ) ly , x ) = k t aopt , where k t denotes the vector ( k ( xl .
x ) , . . .
, k ( xm , x ) ) and aopt is the value of a that maximizes ( 123 ) .
thus , it suffices to compute aopt before any predictions are required .
the problem of choosing the map estimate of a is equivalent to the problem of minimizing the negative log - posterior ,
minimize ( - y t ka + ! a t ( a123 k + kt k ) a )
( ignoring constant terms and rescaling by ( 123 ) .
it is easy to show that the mean of the conditional distribution of y ( x ) is k t ( k + ( 123 ) - ly , and its variance is k ( x , x ) + a 123 - k t ( k + ( 123 ) - lk ( see , for example , ( 123 ) ) .
123 approximate minimization of quadratic forms
for gaussian process regression , searching for an approximate solution to ( 123 ) relies on the assumption that a set of variables whose posterior probability is close to that of the mode of the distribution will be a good approximation for the map estimate .
the following theorem suggests a simple approach to estimating the accuracy of an approximate solution to ( 123 ) .
it uses an idea from ( 123 ) in a modified , slightly more
theorem 123 ( approximation bounds for quadratic forms ) denote by k e lrmxm a positive semidefinite matrix , y , a e lrm and define the two quadratic forms
q ( a ) : = - y t ka + _at ( a 123 k + kt k ) a ,
suppose q and q* have minima qmin and q : nn .
then for all a , a* e irffl we have
q* ( a ) : = - y t a + _at ( a123 123 + k ) a .
: : : : : _ ~ iiyi123 - a 123q* ( a* ) ,
q* ( a* ) : : : : : q; ' , . in : : : : : a - 123 ( _ ~ iiyi123_q ( a ) ) , with equalities throughout when q ( a ) = qmin and q* ( a* ) = q; ' , . in .
hence , by minimizing q* in addition to q we can bound q ' s closeness to the optimum and vice versa .
proof the minimum of q ( a ) is obtained for aopt = ( k + a 123 ) - 123y ( which also minimizes q* ) , hence
qmin = - 123 " y k ( k +a 123 ) y and qmin = - 123 " y ( k +a 123 ) y .
this allows us to combine qmin and q; ' , . in to qmin + a 123q; ' , . in = _ ~ llyi123
since by definition q ( a ) : : : : : qmin for all a ( and likewise q* ( a* ) : : : : : q; ' , . in for all a* ) we may solve qmin + a 123q; ' , . in for either q or q* to obtain lower bounds for each of the two quantities .
this proves ( 123 ) and ( 123 ) .
equation ( 123 ) is useful for computing an approximation to the map solution , whereas ( 123 ) can be used to obtain error bars on the estimate .
to see this , note that in calculating the variance , the expensive quantity to compute is - k t ( k +a123 ) - 123k .
however , this can be found by solving
minimize ( - k t a + ~ a t ( a 123 123 + k ) a ) ,
and the expression inside the parentheses is q* ( a ) with y = k ( see ( 123 ) ) .
hence , an approximate minimizer of ( 123 ) gives an upper bound on the error bars , and lower bounds can be obtained from ( 123 ) .
n practice we w123 use t e quantly gap a , a . - - q ( a ) +u123q * ( a* ) + ~ liyli123 , i . e .
t e relative size of the difference between upper and lower bound as stopping criterion .
* ) . - 123 ( q ( a ) +u q* ( a* ) +123liyli )
123 a sparse greedy algorithm
the central idea is that in order to obtain a faster algorithm , one has to reduce the number of free variables .
denote by p e irffl xn with m : : : : : nand m , n e n an extension matrix ( le .
p t is a projection ) with p t p = 123
we will make the ansatz
ap : = p ( 123 where ( 123 e irn
and find solutions ( 123 such that q ( ap ) ( or q* ( ap ) ) is minimized .
the solution is
clearly if pis ofrank m , this will also be the solution of ( 123 ) ( the minimum negative log posterior for all a e irffl ) .
in all other cases , however , it is an approximation .
( 123opt = ( pt ( a123 k + k t k ) p ) - 123 p t k t y .
computational cost of greedy decompositions
for a given p e irffl xn let us analyze the computational cost involved in the esti ( cid : 123 ) mation procedures .
to compute ( 123 ) we need to evaluate pt ky which is o ( nm ) , ( kp ) t ( kp ) which is o ( n123m ) and invert an n x n matrix , which is o ( n123 ) .
hence the total cost is o ( n123m ) .
predictions then cost only k t a which is o ( n ) .
using p also to minimize q* ( p ( 123* ) costs no more than o ( n123 ) , which is needed to upper - bound the log posterior .
for error bars , we have to approximately minimize ( 123 ) which can done for a = p ( 123 at o ( n123 ) cost .
if we compute ( pkpt ) - l beforehand , this can be done by at o ( n123 ) and likewise for upper bounds .
we have to minimize - k t k p ( 123 + ! ( 123t pt ( ( 123 k + kt k ) p ( 123 which costs o ( n123m ) ( once the inverse matrices have been computed , one may , however , use them to compute error bars at different locations , too , thus costing only o ( n123 ) ) .
the lower bounds on the error bars may not be so crucial , since a bad estimate will only lead to overly conservative confidence intervals and not have any other negative effect .
finally note that all we ever have to compute and store is k p , i . e .
the m x n submatrix of k rather than k itself .
table 123 summarizes the scaling behaviour of several optimization algorithms .
solution gradient ( 123 ) decomposition
o ( n123m ) or o ( n123 ) o ( k . n123m ) or o ( n123 )
table 123 : computational cost of optimization methods .
note that n <t : : m and also note that the n used in conjugate gradient , sparse decomposition , and sparse greedy approximation methods will differ , with neg : : ; nsd : : ; nsga since the search spaces are more restricted .
= 123 gives near - optimal results .
sparse greedy approxhnation
several choices for p are possible , including choosing the principal components of k ( 123 ) , using conjugate gradient descent to minimize q ( 123 ) , symmetric cholesky factorization ( 123 ) , or using a sparse greedy approximation of k ( 123 ) .
yet these methods have the disadvantage that they either do not take the specific form of y into account ( 123 , 123 ) or lead to expansions that cost o ( m ) for prediction and require computation and storage of the full matrix ( 123 , 123 ) .
if we require a sparse expansion of y ( x ) in terms of k ( xi , x ) ( i . e .
many ai in y = k t a will be 123 ) we must consider matrices p that are a collection of unit vectors ei ( here ( ei ) j = oij ) .
we use a greedy approach to find a good approximation .
first , for n = 123 , we choose p = ei such that q ( p ( 123 ) is minimal .
in this case we could permit ourselves to consider all possible indices i e ( i , . . .
m ) and find the best one by trying out all of them .
next assume that we have found a good solution p ( 123 where p contains n columns .
in order to improve this solution , we may expand p into the matrix pnew : = ( pold , ei ) e lrmx ( n+123 ) and seek the best ei such that pnew minimizes min , 123 q ( pnew ( 123 ) .
( performing a full search over all possible n + 123 out of m indices would be too costly . ) this greedy approach to finding a sparse approximate solution is described in algorithm 123
the algorithm also maintains an approximate minimum of q* , and exploits the bounds of theorem 123 to determine when the approximation is sufficiently accurate .
( n ote that we leave unspecified how the subsets m ~ i , m* ~ i* are chosen .
assume for now that we choose m = i , m* = i* , the full set of indices that have not yet been selected . ) this method is very similar to matching pursuit ( 123 ) or iterative reduced set support vector algorithms ( 123 ) , with the difference that the target to be approximated ( the full solution a ) is only given implicitly via q ( a ) .
natarajan ( 123 ) studies the following sparse linear approximation problem : given a e lrmxn , b e lrm , e > 123 , find x e lrn with minimal number of nonzero entries such that iiax - bl123 123 : : ; e .
if we define a : = ( a123k + ktk ) ~ and b : = a - 123ky , then we may write q ( o ) = ! llb - aol123 123 + c where c is a constant independent of o .
thus the problem of sparse approximate minimization of q ( o ) is a special case of natarajan ' s problem ( where the matrix a is square , symmetric , and positive definite ) .
in addition , the algorithm considered by natarajan in ( 123j involves sequentially choosing columns of a to maximally decrease iiax - bll .
this is clearly equivalent to the sparse greedy algorithm described above .
hence , it is straightforward to obtain the following result from theorem 123 in ( 123j .
theorem 123 ( approximation rate ) algorithm 123 achieves q ( o ) : : ; q ( oopt ) + e when a has
non - zero components , where n* ( e / 123 ) is the minimal number of nonzero components in vectors a for which q ( o ) : : ; q ( oopt ) + e / 123 , a = ( a123k + ktk ) 123 / 123 , and ) . l is the minimum of the magnitudes of the singular values of a , the matrix obtained by normalizing the columns of a .
randomized algorithms for subset selection
unfortunately , the approximation algorithm considered above is still too expensive for large m since each search operation involves o ( m ) indices .
yet , if we are satisfied with finding a relatively good index rather than the best , we may resort to selecting a random subset of size k : m .
in algorithm 123 , this corresponds to choosing m ~ i , m* ~ 123* as random subsets of size k .
in fact , a constant value of k will typically suffice .
to see why , we recall a simple lemma from ( 123j : the cumulative distribution function of the maximum of m i . i . d .
random variables 123 , . . .
, em is fo m , where f ( ) is the cdf of ei .
thus , in order to find a column to add to p that is with probability 123 among the best 123 of all such columns , a random subsample of size ilogo . 123 / log123 = 123 will suffice .
algorithm 123 sparse greedy quadratic minimization .
require : training data x = ( xl , . . .
, xm ) , targets y , noise a 123 , precision e
initialize index sets i , 123* = ( i , . . .
, m ) j s , s* = 123
choose m ~ i , m* ~ i* .
find arg milliem q ( ( p , eij , bopt ) , argmilli " em " q* op* , ei " j , b ~ pt ) move i from i to s , i* from 123* to s* .
set p : = ( p , eij , p* : = ( p* , ei " j .
until q ( p , bopt ) + a123q* ( p , b ~ pt ) + ! llyl123 : : ; hiq ( p , bopt ) i + la123q* ( p , b ~ pt ) + ! iiyl123
output : set of indices s , , bopt , ( ptkp ) - t , and ( pt ( ktk +a123k ) p ) - 123
the crucial part is to obtain the values of q ( p , bopt ) cheaply ( with p = ( pold , eij ) , provided we solved the problem for pold .
from ( 123 ) one can see that all that needs to be done is a rank - i update on the inverse .
in the following we will show that this can be obtained in o ( mn ) operations , provided the inverse of the smaller subsystem is known .
expressing the relevant terms using pold and ki we obtain
pt ( ktk +a123k ) p
( pold , eijtkt y = ( poidkt y , kj y ) poid ( kt k + a 123 k ) pold pjd ( kt + a123 ) ~ 123
kjki + a123kii
thus computation of the terms costs only o ( nm ) , given the values for pold ' fur ( cid : 123 ) thermore , it is easy to verify that we can write the inverse of a symmetric positive semidefinite matrix as
where ' y : = ( c + bt a - 123 b ) - 123
hence , inversion of pt ( kt k + a 123 k ) p costs only o ( n123 ) .
thus , to find p of size m x n takes o ( ltn123m ) time .
for the error bars , ( p t kp ) - 123 will generally be a good starting value for the minimization of ( 123 ) , so the typical cost for ( 123 ) will be o ( tmn ) for some t < n , rather than o ( mn123 ) .
finally , for added numerical stability one may want to use an incremental cholesky factorization in ( 123 ) instead of the inverse of a matrix .
123 experiments and discussion
we used the abalone dataset from the vci repository to investigate the properties of the algorithm .
the dataset is of size 123 , split into 123 training and 123 testing split to analyze the numerical performance , and a ( 123 , 123 ) split to assess the generalization error ( the latter was needed in order to be able to invert ( and keep in memory ) the full matrix k + a 123 123 for a comparison ) .
the data was rescaled to zero mean and unit variance coordinate - wise .
finally , the gender encoding in abalone ( male / female / infant ) was mapped into ( ( i , 123 , 123 ) , ( 123 , 123 , 123 ) , ( 123 , 123 , i ) ) .
in all our experiments we used gaussian kernels k ( x , x ' ) = exp ( - ~ ) as co - variance kernels .
figure 123 analyzes the speed of convergence for different it .
figure 123 : speed of convergence .
we plot the size of the gap be ( cid : 123 ) tween upper and lower bound of the log posterior ( gap ( a , a* ) ) for the first 123 samples from the abalone dataset ( a 123 = 123 and 123w 123 = 123 ) .
from top to bottom : subsets of size 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123
the results were averaged over 123 runs .
the relative variance of the gap size was less than 123% .
one can see that that subsets of size 123o - ' o ' - - - - - - - - , l - - - - , l - - - - - - , o ' - - - - - - - - - " c - - - , l - - - - - - ' - , - - - - - - - , - l - - - , l - - - - - - ' - , - - - - - - , - ! 123 and above ensure rapid conver -
number of ilerahons
for the optimal parameters ( 123a 123 = 123 and 123w123 = 123 , chosen after ( 123 ) ) the average test error of the sparse greedy approximation trained until gap ( a , a* ) < 123 on a ( 123 , 123 ) split ( the results were averaged over ten independent choices of training sets . ) was 123 123 , slightly worse than for the gp estimate ( 123 123 ) .
the log posterior was - 123 . 123 ( 123 123 ) , the optimal value - 123 .
123 ( 123 123 ) .
hence for all practical purposes full inversion of the covariance matrix and the sparse greedy approximation have statistically indistinguishable generalization
in a third experiment ( table 123 ) we analyzed the number of basis functions needed to minimize the log posterior to gap ( a , a* ) < 123 , depending on different choices of the kernel width a .
in all cases , less than 123% of the kernel functions suffice to
find a good minimizer of the log posterior , for the error bars , even less than 123% are sufficient .
this is a dramatic improvement over previous techniques .
kernel width 123w : & kernels for log - posterior 123 kernels for error bars
123 123 123 123 123 123
table 123 : number of basis functions needed to minimize the log posterior on the abalone dataset ( 123 training samples ) , depending on the width of the kernel w .
also , number of basis functions required to approximate k t ( k + 123 - 123 ) - l k which is needed to compute the error bars .
we averaged over the remaining 123 test samples .
to ensure that our results were not dataset specific and that the algorithm scales well we tested it on a larger synthetic dataset of size 123 in 123 dimensions distributed according to n ( o , 123 ) .
the data was generated by adding normal noise with variance 123 - 123 = 123 to a function consisting of 123 randomly chosen gaussians of width 123w 123 = 123 and normally distributed coefficients and centers .
we purposely chose an inadequate gaussian process prior ( but correct noise level ) of gaussians with width 123w 123 = 123 in order to avoid trivial sparse expansions .
after 123 iterations ( i . e .
after using 123% of all basis functions ) the size of the gap ( cr , cr ) was less than 123 ( note that this problem is too large to be solved exactly ) .
we believe that sparse greedy approximation methods are a key technique to scale up gaussian process regression to sample sizes of 123 and beyond .
the tech ( cid : 123 ) niques presented in the paper , however , are by no means limited to regression .
work on the solutions of dense quadratic programs and classification problems is in progress .
the authors thank bob williamson and bernhard sch123lkopf .
