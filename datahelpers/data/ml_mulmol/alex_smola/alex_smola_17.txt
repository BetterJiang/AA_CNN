learning from structured data is becom - ing increasingly important .
however , most prior work on kernel methods has focused on learning from attribute - value data .
only recently , research started investigating ker - nels for structured data .
this paper consid - ers kernels for multi - instance problems - a class of concepts on individuals represented by sets .
the main result of this paper is a ker - nel on multi - instance data that can be shown to separate positive and negative sets under natural assumptions .
this kernel compares favorably with state of the art multi - instance learning algorithms in an empirical study .
fi - nally , we give some concluding remarks and propose future work that might further im - prove the results .
support vector machines ( svm ) and other kernel methods ( boser et al . , 123; scholkopf & smola , 123 ) have successfully been applied to various tasks in attribute - value learning .
most real - world data , how - ever , has no natural representation as a tuple of con - stants .
dening kernels on individuals that can not easily be described by a single feature vector means crossing the boundary between attribute - value and re - lational learning .
it allows kernel methods to be ap - plied more easily to complex representation spaces .
multi - instance ( mi ) learning problems ( dietterich et al . , 123 ) occur whenever example objects , indi -
viduals , can not be described by a single characteristic feature vector , but by a bag of vectors .
any of these vectors could be responsible for the classication of the bag .
the inherent diculty of mi problems is to iden - tify the characteristic element of each bag .
extending kernel methods to mi problems is one step towards crossing the boundary between attribute - value and re - the main result of this paper is a kernel on mi data that can be shown to separate positive and negative sets under natural assumptions .
empirical studies compare this kernel to other mi learning algorithms .
on one hand , an svm using this kernel is compared to state of the art mi algorithms .
on the other hand , it is compared to other methods that can be used to apply svms to mi data .
in both studies the kernel proposed in this paper compares favorably .
section 123 introduces the mi setting under various as - pects .
section 123 summarizes prior work on kernels for discrete spaces , in particular the work of haus - sler ( 123 ) and gartner ( 123 ) .
section 123 gives an account of the separability of mi problems in kernel feature space and devises a suitable kernel .
section 123 discusses an alternative way to apply attribute - value learning algorithms to mi problems .
after that we empirically evaluate our kernel on the musk - dataset , and conclude with some nal remarks .
the multi - instance setting
mi problems have been introduced under this name by dietterich et al .
( 123 ) .
however , similar problems and algorithms have been considered earlier , for exam -
ple in pattern recognition ( keeler et al . , 123 ) .
within the last couple of years , several approaches have been made to upgrade attribute - value learning algorithms to tackle mi problems .
other approaches focused on new algorithms specically designed for mi learning .
formally , concepts are functions i : x , where x is often referred to as the instance space or prob - lem domain ( examples are elements of this domain ) and = ( ( cid : 123 ) , ) are the labels ( in what follows al - ways means ( ( cid : 123 ) , ) ) .
there are 123|x| concepts on the instance space x .
a function f : x r is said to separate the concept if f ( x ) > 123 i ( x ) .
if examples are represented by subsets of some domain x concepts are functions set : 123x .
there are dierent concepts on sets ( in other words : set 123c , c x ) .
such concepts are sometimes referred to as multi - part concepts .
mi concepts are a specic kind of these concepts .
denition 123 an mi concept is a function mi 123x .
it is dened as :
mi ( x ) x x : i ( x )
where i is a concept over an instance space ( referred to as the underlying concept ) , and x x is a set .
there are 123|x| dierent mi concepts .
the diculty in this task is not just to generalize beyond examples , but also identifying the characteristic element of each bag .
any learning algorithm that sees a positive bag ( a bag with label ( cid : 123 ) ) cannot infer much about the elements of the bag , except that one of its elements is positive in the underlying concept .
with large bag sizes this information is of limited use .
a popular real - world example of an mi problem is the prediction of drug activity , described by dietterich et al .
( 123 ) .
a drug is active if it binds well to en - zymes or cell - surface receptors .
the binding strength is determined by the shape of the drug molecule .
how - ever , most molecules can change their shape by rotat - ing some of their internal bonds .
the possible shapes of a molecule , i . e . , a combination of the angles of the rotatable bonds of the molecule , are known as confor - mations .
a drug binds well to enzymes or cell - surface receptors if one of its conformations binds well .
thus the drug activity prediction problem is an mi problem .
apart from approaches that ignore the mi setting dur - ing training , two major categories of approaches can be distinguished : upgrading existing attribute - value learning algorithms , and designing a new learning al - gorithm specically for mi problems .
algorithms that have specically been designed for mi problems are :
the axis - parallel rectangles ( apr ) algorithm and vari - ants ( dietterich et al . , 123 ) , an algorithm based on simple statistics of the bags ( auer , 123 ) , and algo - rithms based on the diverse density approach ( maron & lozano - perez , 123; zhang & goldman , 123 ) .
al - gorithms that have been upgraded until now are : the lazy learning algorithms bayesian - knn and citation - knn ( wang & zucker , 123 ) , the neural network mi - nn ( ramon & de raedt , 123 ) , the decision tree learner relic ( ruo , 123 ) , and the rule learner ( naive ) rippermi ( chevaleyre & zucker , 123 ) .
ductive logic programming algorithms have also been used , for instance , the rst - order decision tree learner tilde ( blockeel & de raedt , 123 ) .
of the aforementioned approaches , the apr algorithm is the classical approach for mi learning .
it assumes that the features of the conformations are independent and thus orthogonal .
we then seek an axis - aligned ( hyper - ) rectangle covering at least one element of each positive bag and none of a negative bag .
kernels on discrete spaces
in svms and other kernel methods the only informa - tion needed about instances is an inner product in some feature space - a positive denite mercer ker - nel k ( . , . ) = ( cid : 123 ) ( . ) , ( . ) ( cid : 123 ) , where ( cid : 123 ) . , . ( cid : 123 ) denotes the inner product and is a feature transformation .
below we summarize prior work on kernels for discrete spaces that is most relevant in our context .
for brevity , be - low a kernel always means a mercer kernel .
convolution kernels the best known kernel for representation spaces that are not mere attribute value tuples , is the convolution kernel proposed by haussler ( 123 ) .
it is dened as :
kconv ( x , y ) =
where r is a relation between instances z and their 123 decomposes an instance into a set of parts ( cid : 123 ) z , i . e . , r d - tuples .
zd denotes the d - th component of ( cid : 123 ) z , and kd is a valid kernel function on this component .
the term convolution kernel refers to the class of ker - nels given by the above denition .
the advantage of convolution kernels is that they are very general and can be applied in many dierent problems .
however , because of that generality they require a signicant amount of work to adapt them to a specic problem , which makes choosing r a non - trivial task .
more specic kernels on discrete spaces can be found in gartner ( 123 ) .
there , kernels for elementary sym -
bols ( k - the matching kernel ) , sets and multi - sets of elementary symbols ( kset and kmultiset ) , and boolean domains ( k - and a polynomial variant ) are discussed along with concept classes that can be separated by linear classiers using one of these kernels .
the spe - cic kernels are described in more detail below .
123 set kernels
a kernel on sets can easily be derived form the de - nition of convolution kernels by letting r be the set - membership function , i . e . , with ( cid : 123 ) z r z , d = 123 and for notational convenience z123 = z , k123 =
( cid : 123 ) ) : = ( cid : 123 ) where is a kernel on x , and x , x lar , haussler proved the following theorem :
kx ( x , x
( cid : 123 ) x .
in particu -
proposition 123 a function k on sets dened by ( 123 ) is a kernel if and only if itself is a kernel .
for instance , if x , y x are elementary symbols ( con - stants - no internal structure ) a natural choice for a kernel is the matching kernel , dened as :
k ( x , y ) = x , y
since this kernel induces a hilbert space on ( cid : 123 ) 123 ( x ) , assuming x is countable , all possible concepts ( x ) x c , c x on this representation space can be separated using the matching kernel .
clearly for
kset ( x , y ) = ( cid : 123 )
k ( x , y ) = |x y | .
next we extend the summation operation over sets to more general structures on x by using the notion of cosets .
assume that there exists a set y ( the coset ) as - sociated with x .
then we can represent sets and their generalizations as mappings x y .
the following examples explain the use of this denition : sets : here y = ( 123 , 123 ) and the map x : x y indi - cates whether x x ( in which case y = 123 ) or not .
multisets : setting y = n ( 123 ) allows us to extend the notion of sets to multisets , i . e . , sets x which may contain elements several times .
here y = x ( x ) de - notes the number of elements x contained in x , and , as before , y = 123 indicates that x ( cid : 123 ) x .
measures : we may extend the notion of cosets to general measures on x by letting y = ( 123 , )
a denition would also allow us to cater for situations where class membership is described in more vague terms , as common in fuzzy sets ( here y = ( 123 , 123 ) ) .
now that we extended our denition of sets , we need to dene a kernel on it .
in general , any kernel on the space of functions x : x y will satisfy the formal requirements .
for practical purposes , however , we consider a class of kernels given by
kx ( x , x
( cid : 123 ) ) = ( cid : 123 )
( cid : 123 ) ) ) = x ( x ) x
it is easy to check that ( 123 ) reduces to the kernel dened in ( 123 ) in the case of sets and in the case of multisets to a kernel where the summation ranges over all elements of the
it is useful to note that ( 123 ) corresponds to an averag - ing process which is carried out in feature space , or in other words , given a kernel on all elements xij xi on all sets xi , the kernel matrix on xi is obtained by summing up corresponding rows and columns of a such a procedure indicates that if the cardinalities of xi vary considerably , sets with a large cardinality will dominate the solution of the estimation problem ( as can be observed in the experiments on the musk dataset ) .
this is not always desirable , which leads us to the issue of normalization .
a natural denition in this regard is
where fnorm ( x ) is a suitable normalization function ( cid : 123 ) ) ( cid : 123 ) = 123
be - which is nonnegative for any kset ( x , x low we see that various choices of fnorm ( x ) will lead to feature - space normalization , sample normalization , variance rescaling , etc .
feature - space normalization : we simply set
fnorm ( x ) : = ( cid : 123 ) kset ( x , x ) .
thus we recover the normalization proposed by ( her - brich , 123 ) , which proves to be very useful in mi averaging : in this case we need to compute the gen - eralized cardinality of the set x , which can be achieved
fnorm ( x ) : = ( cid : 123 )
for simple sets this just means that we average the instances mapped into feature space .
a minor mod -
ication is to use ( cid : 123 ) fnorm ( x ) instead , which will be
more useful if most of the averages are close to 123
separating mi problems
in this section we dene a kernel function on sets of instances that separates mi problems under natural assumptions .
using this kernel function , svms and other kernel methods can easily be applied to mi prob - lems .
we begin by introducing the notion of separa - separability a concept i : x is called lin - early separable in input space if
x x : ( cid : 123 ) x , c ( cid : 123 ) i ( x )
holds for some constant c and threshold .
likewise , a concept is called linearly separable with respect to a feature map if
x x : ( cid : 123 ) ( x ) , c ( cid : 123 ) i ( x )
holds for some c span ( ( x ) |x x ) .
in what fol - lows we often use the term separable to refer to con - cepts that are linearly separable with respect to a fea - ture transformation given implicitly by a kernel .
for our purposes we further need the notion of a con - cept that is separable with non - zero margin and some
denition 123 we call a concept separable with mar - gin > 123 with respect to the feature map , if there exist c , b such that for every x x :
i ( x ) 123 ( cid : 123 ) ( x ) , c ( cid : 123 ) + b 123 .
( cid : 123 ) ( x ) , c ( cid : 123 ) + b 123
note that we can , without loss of generality , assume that b = 123 , since for given , c , b the map x ( ( x ) , 123 ) together with ( c , b ) will satisfy the above condition without a constant oset .
furthermore , any separable concept on a nite domain ( or example set ) is separable with non - zero margin and some lower bound on this domain ( or example set ) .
we will now dene a kernel kmi that separates mi con - cepts .
this kernel is a variant of the set kernel , as de - ned in ( 123 ) .
it will be useful in proving necessary and sucient conditions for separability of mi problems .
kmi ( x , y ) = ( cid : 123 )
i ( x , y )
where p n is a constant .
since products of kernels are kernels , also kp i ( x , y ) is a kernel , and consequently also kmi ( x , y ) is a kernel .
in order to show that mi concepts are separable if and only if the underlying concept is separable , we need the following two lemmas .
lemma 123 an mi concept mi is separable with the kernel kmi , as dened in ( 123 ) , for suciently large p , if the underlying concept i is separable with the kernel ki and some , according to ( 123 ) .
proof : consider rst the case that i = 123x , i . e . , each instance satises the concept .
then it follows trivially from the denition of mi concepts that mi = 123x .
for the remainder of the proof we can thus assume that at least one instance does not satisfy the concept .
let now p > 123 if = 123 , and p > log m here m is a bound on the cardinality of the bags x .
since the concept is separable , there exists a c satis - fying ( 123 ) .
now consider the function
one can see that f ( x ) < 123 if and only if no x x satises i : in this case , we have
f ( x ) = ( cid : 123 )
f ( x ) m ( 123 ) p < m ( 123 )
log ( 123 ) = 123
the nal step consists of showing that f can be writ - ten as a dot product in the feature space induced by kmi .
clearly f ( ( x ) ) is a function in the space of dot ( cid : 123 ) ) p .
next note that kmi is products induced by k ( x , x the space of linear combinations of such functions on x , and clearly f , being a monomial , is one of such example 123 let x = 123 = ( ( cid : 123 ) , ) 123 , x x , x = ( x123 , x123 , x123 , x123 ) , i ( x ) x123 x123 , and ki ( x , y ) = k ( x , y ) = ( cid : 123 )
then m = 123 , as maxi |xi| |x| = 123 = 123 , and = 123 / 123
here we have p = 123 > log 123 / log123
the following example illustrates that the simple set - kernel separates mi concepts on discrete sets .
example 123 let x = ( a , b , c , d ) , c = ( a , c ) , i ( x ) x c , and ki ( x , y ) = k ( x , y ) .
then = 123 , m = 123 as maxi |xi| |x| = 123 ) , and p = 123 > 123
it kmi ( x , y ) = ( cid : 123 )
k ( x , y ) = |x y | = kset ( x , y )
it follows directly from the lemma above and from the denition of convolution kernels ( haussler , 123 ) that ( for nite example sets ) mi concepts can be separated with convolved gaussian rbf kernels if the underlying concept can be separated with gaussian rbf kernels , since in this case kp is a gaussian rbf kernel itself , albeit with width 123 / p instead of 123
in this case the mi kernel does not require an additional parameter to be chosen .
also note that for rbf kernels ( 123 ) always holds with b = 123
we further need
lemma 123 if an mi concept mi is separable then the underlying concept i is separable .
proof : say fmi ( z ) is a function that separates posi - tive and negative bags ( fmi ( z ) > mi ( z ) ) .
then fi ( z ) = fmi ( ( z ) ) is a function that separates the un - derlying concept ( with fi ( z ) > i ( z ) ) .
it is now easy to show :
theorem 123 an mi concept mi is separable by kmi with a non - zero margin if and only if the underlying concept i is separable by the kernel ki with a non - zero
proof : in lemma 123 the margin of the mi problem is mi = 123 m ( 123 ) p > 123
furthermore , assuming a margin on fmi in lemma 123 , a margin of can be found on fi .
the lower bounds are maintained note that the mi kernel can also be used in distance - based algorithms such as k - nearest neighbors .
the distance metric is then dened on the kernel in the
standard manner d ( x , y ) = ( cid : 123 ) ( cid : 123 ) x , x ( cid : 123 ) 123 ( cid : 123 ) x , y ( cid : 123 ) + ( cid : 123 ) y , y ( cid : 123 ) .
example 123 consider the kernel kmi ( x , y ) = |x y | for solving mi problems on discrete sets based on the matching kernel ( see example 123 ) .
in such a case , the corresponding distance metric can be derived easily as the symmetric dierence d ( x , y ) = |x ( cid : 123 ) y | .
learning ray concepts
having shown in the previous section that mi prob - lems can be separated with our mi kernel , we will now describe a simple approach that can be used to apply any propositional learning algorithm to mi problems .
the motivation is based on some observations in the drug activity prediction domain .
the advantage of this approach is the eciency - in real world drug activity prediction problems bags can be huge which renders the computation of kmi too expensive .
in the empir - ical evaluation of this method ( see section 123 ) we will show that - in spite of the simplicity of this approach -
an svm using this kernel can outperform several other mi learning algorithms .
if we can make further assumptions on the properties of x , such as being generated by a nor - mal distribution , by a mixture thereof , or other prop - erties that can be summarized in a compact fashion ( cid : 123 ) x can be useful then computing statistics on x , x in dening kernels on sets .
denition 123 denote by s : x s ( x ) a map com - puting statistics on x x .
then we call ( cid : 123 ) ) : = k ( s ( x ) , s ( x
the statistic kernel .
here s ( x ) is a collection of properties of the set , say the mean , median , maximum , minimum , etc .
typi - cally , s ( x ) will be a vector of real numbers .
a similar approach has been used in the context of in - ductive logic programming ( krogel & wrobel , 123 ) , where relational aggregations are used to compute statistics over a database .
it is not uncommon in drug activity prediction to rep - resent a molecule by a bag of descriptions of its dier - ent conformations .
each conformation is in turn de - scribed by a feature vector such that each component of the vector corresponds to one ray emanating from the origin and measuring the distance to the molecule surface ( dietterich et al . , 123 ) .
it is often believed that the concept space of drug activity prediction can be described by putting upper and lower bounds along consider mi rays , i . e . , concepts on sets of real numbers such that rm i ( x ) x x : x .
these con - cepts are the complement ( negation ) of upper bounds .
motivated by the observation that rm i can be learned using only the maximal element of the set , we nd the following statistics kernel particularly in - teresting for drug activity prediction :
example 123 ( minimax kernel ) dene s to be the vector of the coordinate - wise maxima and minima of x , i . e . ,
s ( x ) = ( min
in our experiments we used s ( x ) combined with poly - ( cid : 123 ) ) ( cid : 123 ) + 123 ) p .
nomial kernels , i . e . , k ( x , x the minimax kernel is related to the mi kernel as kmi can be seen as a soft max function , while minimax cor - responds to a component - wise min and max function .
( cid : 123 ) ) = ( ( cid : 123 ) s ( x ) , s ( x
drug activity prediction
often drug activity prediction problems are used to asses mi learning algorithms , most prominently the musk data set ( dietterich et al . , 123 ) .
the problem consists of predicting the strength of synthetic musk molecules .
the class labels have been found by human domain experts .
two overlapping data sets are avail - able .
musk123 contains 123 molecules labeled as musk ( if the molecule is known to smell musky ) and 123 la - beled as non - musk .
the 123 molecules are altogether described by 123 conformations .
musk123 contains 123 musk molecules and 123 non - musk molecules , de - scribed by 123 conformations altogether .
several empirical results on these two data sets have been achieved and reported in literature .
the results in table 123 are in alphabetical order .
they have either been obtained by multiple tenfold cross - validation runs ( 123cv ) or by leave - one - out estimation ( loo ) .
the best classication results from each section are marked in boldface .
the table is organized as follows : the rst section contains algorithms specically designed for mi learning .
the second one contains algorithms that are designed as general purpose learning algorithms , but have been adapted to learn mi problems .
the third section contains algorithms that have been run on the musk data using the minimax feature space described above , and the forth section contains results achieved by ignoring the mi setting while learning but obeying it when testing .
for the svm , an rbf kernel was used with = 123
boosted nb ( dt ) refers to a boosted naive bayes ( decision tree ) classier .
the fth section of the table contains results from us - ing svms with a polynomial version of the minimax kernel .
the sixth section contains results for svms using mi kernels .
due to the extremely small sam - ple size ( musk123 contains only 123 bags and musk123 only 123 ) which is furthermore not divisible by 123 , 123 - fold cross - validation is a very noisy and unreliable process .
to address this problem , we opted to com - pute an error estimate by averaging over 123 trials of randomly leaving out 123 instances .
the advantage of this approach is that in addition to the error estimates we also obtain condence intervals , which allows us to compare our results with the ones obtained in the lit - erature .
finally , for the sake of comparability , also the leave - one - out error was computed .
we chose a gaussian rbf kernel for the dot prod - uct on the elements of the bag , using the rule of thumb that should be in the order of magnitude 123d123 123 or lower , where d is the dimensionality
table 123
classication errors ( in % ) on musk123 and musk123
musk123 musk123 eval .
gfs kde apr
loo 123 - out loo 123 - out
minimax feature space ignoring the mi setting while training
polynomial minimax kernel p = 123 ) = exp ( ( cid : 123 ) x x mi kernel ( ki ( x , x
of the data123
this led to an almost optimal choice of parameters ( the optimum lay within an order of mag - nitude of the initial estimate ) .
as for preprocessing , the data was rescaled to zero mean and unit variance on a per coordinate basis .
in the ray - kernel case , we simply used a polynomial kernel on s ( x ) .
to avoid adjusting too many parameters , we chose the - parameterization ( scholkopf et al . , 123 ) , with set to 123 .
the latter corresponds to an error level com - parable to the ones in the published literature .
normalization proved to be critical : the un - normalized
sets , and the sets normalized by ( cid : 123 ) fnorm ( x ) performed
worst , whereas there was no signicant dierence in performance between the kernels which employed nor - malization in feature space and those with averaging in feature space .
this may be due to the fact that the av - erage length of the sum of features of equal length may 123note that the parameter p of mi kernels is chosen im - plicitly when choosing ; as for gaussian rbf kernels , kp is a gaussian rbf kernel itself .
be more strongly concentrated , which makes both ap - proaches qualitatively equivalent .
we only report re - sults on the kernel with normalization in feature space .
to assess the true performance of the estimation pro - cedure , one must not , however , x a set of parameters and only then use a cross - validation step to assess the error for the now xed set of parameters .
we therefore adjusted the parameters , such as kernel width and the value of and for each leave - 123 - out sample sepa - rately and computed the cv error for this procedure .
123 random leave - 123 - out samples were drawn .
the cor - responding results are reported in the fth block of table 123 , denoted by loo and loo norm ( the latter refers to kernels with normalization in feature space ) .
it is rather obvious that the method degrades dramat - ically due to the small sample size eects ( only 123 ob - servations ) .
also , the choice of suitable normalization in feature space yields only diminishing returns , given the high variance of the model selection procedure .
it is well known ( cherkassky & mulier , 123 ) that the choice of model selection rules has a signicant inu - ence on the performance of the overall estimator , quite often more than the choice of the estimator itself .
the fact that polynomial minimax kernels outper - formed the mi kernel ( gaussian rbf ) on musk123 can be explained by the fact that musk123 contains much fewer conformations per bag than musk123 , hence the min and max statistic s ( x ) is a quite adequate descrip - tion of each bag .
a small bag size is a major shortcom - ing also of other ( synthetically generated ) datasets , making them very unrealistic - as to our understand - ing real - world drug activity prediction problems usu - ally involve big bags .
while em - dd ( zhang & goldman , 123 ) is superior on both datasets , among the algorithms that are based on general purpose learning methods , svms with mi kernels outperform all other methods .
we conjecture that replacing the averaging process over the data in feature space by a weighted average taking an estimate of the class of each element in the bag into account would signicantly improve generalization .
we consider now another drug activity prediction problem - predicting the mutagenicity of molecules .
the original dataset ( srinivasan et al . , 123 ) , de - scribed by a set of prolog predicates , has widely been used in the inductive logic programming com - munity .
the only time this dataset has been used with mi learning algorithms is described in ( cheva - leyre & zucker , 123 ) .
while the basic dataset is de - scribed by two relations , the atoms of the molecule and the bonds between the atoms , other representa - tions include global molecular descriptors .
two sets of
instances are frequently used , the so called friendly dataset containing 123 instances , and the unfriendly dataset containing 123 instances .
we consider only ex - periments without global molecular features and rep - resent each molecule by set of bonds together with the two adjacent atoms .
this setup is similar to the one described in ( chevaleyre & zucker , 123 ) where error rates between 123% ( rippermi ) and 123% ( foil ) are reported on the friendly dataset .
so far , no re - sults on the unfriendly dataset have been reported using mi algorithms .
using our mi kernel and the data description without any global molecular descrip - tors we are able to achieve error rates of 123% on the friendly dataset and 123% on the unfriendly dataset .
as before , we converted the data to zero mean and unit variance .
symbolic values were converted to orthonor - mal vectors .
model selection ( and ) was carried out by loo cross - validation , and the results are reported for an average over 123 random leave - 123 - out subsets .
conclusions and future work
in this paper we demonstrated a successful approach to extend the applicability of svms to data other than mere attribute - value tuples .
it has been mentioned in literature that mi problems capture most of the com - plexity of relational learning problems .
therefore , mi kernels are an important step in crossing the boundary between successful attribute - value learning and rela - we dened a general kernel for mi problems , proved that it separates mi concepts under natural assump - tions , and showed its performance on benchmark data .
favorable for our approaches are the high accuracy and the simplicity with which other kernel methods can now be extended to mi problems .
for example , by simply plugging our kernel into svm regression , the only very recently formulated problem of mi regres - sion can be tackled ( amar et al . , 123; ray & page , 123 ) .
clustering and feature extraction tasks have to the best of our knowledge not yet been investigated for mi data .
support vector clustering and kernel princi - pal component analysis algorithms can now easily be applied to mi data by using our kernel function .
other distance - based algorithms can be applied to mi prob - lems by dening a distance on the kernel in the stan - dard manner .
these are promising topics for future finally , the idea of labeling the elements of the bags , as done in em - dd indicates further space for improve - ment , e . g . , by modifying the averaging process , that is
currently carried out within the bags ( ( cid : 123 ) xx ( x ) ) into
a weighted average , where preference is given to ele -
ments which are close to elements of other bags with the same class label .
the kernel function would be a simple extension of the coset kernel and our mi kernel .
research supported in part by the esprit v project ( ist - 123 - 123 ) data mining and decision support for business competitiveness : solomon virtual en - terprise and a grant of the arc .
the authors thank tamas horvath , thorsten joachims , john lloyd , bob williamson , and stefan wrobel for valuable discus - sions and / or comments .
