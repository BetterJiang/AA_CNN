kernlab is an extensible package for kernel - based machine learning methods in r123
it takes advantage of rs new s123 object model and provides a framework for creating and using kernel - based algorithms .
the package contains dot product primitives ( kernels ) , implementations of support vector machines and the relevance vector machine , gaussian processes , a ranking algorithm , kernel pca , kernel cca , and a spectral clustering algorithm .
moreover it provides a general purpose quadratic programming solver , and an incomplete cholesky decomposition
keywords : kernel methods , support vector machines , quadratic programming , ranking , cluster - ing , s123 , r .
machine learning is all about extracting structure from data , but it is often dicult to solve prob - lems like classication , regression and clustering in the space in which the underlying observations have been made .
kernel - based learning methods use an implicit mapping of the input data into a high dimensional feature space dened by a kernel function , i . e . , a function returning the inner product h ( x ) , ( y ) i between the images of two data points x , y in the feature space .
the learning then takes place in the feature space , provided the learning algorithm can be entirely rewritten so that the data points only appear inside dot products with other points .
this is often referred to as the kernel trick ( scholkopf and smola , 123 ) .
more precisely , if a projection : x h is used , the dot product h ( x ) , ( y ) i can be represented by a kernel function k
k ( x , y ) = h ( x ) , ( y ) i ,
which is computationally simpler than explicitly projecting x and y into the feature space h .
one interesting property of kernel - based systems is that , once a valid kernel function has been selected , one can practically work in spaces of any dimension without paying any computational cost , since feature mapping is never eectively performed .
in fact , one does not even need to know which features are being used .
another advantage is the that one can design and use a kernel for a particular problem that could be applied directly to the data without the need for a feature extraction process .
this is particularly important in problems where a lot of structure of the data is lost by the feature extraction process ( e . g . , text processing ) .
the inherent modularity of kernel - based learning methods allows one to use any valid kernel on a kernel - based algorithm .
institut fur statistik & wahrscheinlichkeitstheorie , technische universitat wien , austria australian national university , department of engineering and rsise , australia institut fur statistik und mathematik , wirtschaftsuniversitat wien , austria institut fur statistik und mathematik , wirtschaftsuniversitat wien , austria
123 software review
the most prominent kernel based learning algorithm is without doubt the support vector machine ( svm ) , so the existence of many support vector machine packages comes as little surprise .
most of the existing svm software is written in c or c++ , e . g .
the award winning libsvm123 ( chang and lin , 123 ) , svmlight123 ( joachims , 123 ) , svmtorch123 , royal holloway support vector machines123 , mysvm123 , and m - svm123 with many packages providing interfaces to matlab ( such as libsvm ) , and even some native matlab toolboxes123 123 123
putting svm specic software aside and considering the abundance of other kernel - based algo - rithms published nowadays , there is little software available implementing a wider range of kernel methods with some exceptions like the spider123 software which provides a matlab interface to vari - ous c / c++ svm libraries and matlab implementations of various kernel - based algorithms , torch123 which also includes more traditional machine learning algorithms , and the occasional matlab or c program found on a personal web page where an author includes code from a published paper .
123 r software
the e123 r package oers an interface to the award winning libsvm ( chang and lin , 123 ) , a very ecient svm implementation .
libsvm provides a robust and fast svm implementation and produces state of the art results on most classication and regression problems ( meyer et al . , 123 ) .
the r interface provided in e123 adds all standard r functionality like object orientation and formula interfaces to libsvm .
another svm related r package which was made recently available is klar ( roever et al . , 123 ) which includes an interface to svmlight , a popular svm implementation along with other classication tools like regularized discriminant analysis .
however , most of the libsvm and klar svm code is in c++ .
therefore , if one would like to extend or enhance the code with e . g .
new kernels or dierent optimizers , one would have to modify the core c++ code .
kernlab aims to provide the r user with basic kernel functionality ( e . g . , like computing a kernel matrix using a particular kernel ) , along with some utility functions commonly used in kernel - based methods like a quadratic programming solver , and modern kernel - based algorithms based on the functionality that the package provides .
taking advantage of the inherent modularity of kernel - based methods , kernlab aims to allow the user to switch between kernels on an existing algorithm and even create and use own kernel functions for the kernel methods provided in the package .
123 s123 objects
kernlab uses rs new object model described in programming with data ( chambers , 123 ) which is known as the s123 class system and is implemented in the methods package .
in contrast with the older s123 model for objects in r , classes , slots , and methods relationships must be declared explicitly when using the s123 system .
the number and types of slots in an instance
of a class have to be established at the time the class is dened .
the objects from the class are validated against this denition and have to comply to it at any time .
s123 also requires formal declarations of methods , unlike the informal system of using function names to identify a certain method in s123
an s123 method is declared by a call to setmethod along with the name and a signature of the arguments .
the signature is used to identify the classes of one or more arguments of the method .
generic functions can be declared using the setgeneric function .
although such formal declarations require package authors to be more disciplined then when using the informal s123 classes , they provide assurance that each object in a class has the required slots and that the names and classes of data in the slots are consistent .
an example of a class used in kernlab is shown below .
typically , in a return object we want to include information on the result of the method along with additional information and parameters .
usually kernlabs classes include slots for the kernel function used and the results and additional
representation ( " vector " , # the vector containing the cluster
withinss = " vector " ) , # within cluster sum of squares
# the cluster centers # size of each cluster # kernel function used
prototype = structure ( . data = vector ( ) ,
centers = matrix ( ) , size = matrix ( ) , kernelf = ls , withinss = vector ( ) ) )
accessor and assignment function are dened and used to access the content of each slot which can be also accessed with the @ operator .
namespaces were introduced in r 123 and provide a means for packages to control the way global variables and methods are being made available .
due to the number of assignment and accessor function involved , a namespace is used to control the methods which are being made visible outside the package .
since s123 methods are being used , the kernlab namespace also imports methods and variables form the methods package .
the kernlab package also includes data set which will be used to illustrate the methods included in the package .
the spam data set ( hastie et al . , 123 ) set collected at hewlett - packard labs classies 123 e - mails as spam or non - spam .
the 123 variables of each data vector indicate the frequency of certain words and characters in the e - mail .
the data set contains 123 and 123 e - mails classied as non - spam and spam , respectively .
another data set included in kernlab the income data set ( hastie et al . , 123 ) is taken by a marketing survey in the san francisco bay concerning the income of shopping mall customers .
it consists of 123 demographic attributes ( nominal and ordinal variables ) including the income and the ticdata data set ( van der putten et al . , 123 ) was used in the 123 coil challenge and contains information on customers of an insurance company .
the data consists of 123 variables and includes product usage data and socio - demographic data derived from zip area codes .
the data was collected to answer the following question : can you predict who would be interested in buying a caravan insurance policy and give an explanation why ?
the spirals data set was created by the mlbench . spirals function in the mlbench package ( leisch and dimitriadou , 123 ) .
this two - dimensional data set with 123 data points consists of two spirals where gaussian noise is added to each data point .
a kernel function k calculates the inner product of two vectors x , x123 in a given feature mapping : x h .
the notion of a kernel is obviously central in the making of any kernel - based algorithm and consequently also in any software package containing kernel - based methods .
kernels in kernlab are s123 objects of class kernel extending the function class with one additional slot containing a list with the kernel hyper - parameters .
package kernlab includes 123 dierent kernel classes which all contain the class kernel and are used to implement the existing kernels .
these classes are used in the function dispatch mechanism of the kernel utility functions described below .
existing kernel functions are initialized by creator functions .
all kernel functions take two feature vectors as parameters and return the scalar dot product of the vectors .
an example of the functionality of a kernel in kernlab :
r> rbf < - rbfdot ( sigma = 123 )
gaussian radial basis kernel function .
hyperparameter : sigma = 123
r> x < - rnorm ( 123 ) r> y < - rnorm ( 123 ) r> rbf ( x , y )
the package includes implementations of the following kernels :
the linear vanilladot kernel implements the simplest of all kernel functions
k ( x , x123 ) = hx , x123i
which is useful specially when dealing with large sparse data vectors x as is usually the case in text categorization .
the gaussian radial basis function rbfdot
k ( x , x123 ) = exp ( kx x123k123 )
which is a general purpose kernel and is typically used when no further prior knowledge is available about the data .
the polynomial kernel polydot
k ( x , x123 ) = ( scale hx , x123i + oset ) degree .
which is used in classication of images .
the hyperbolic tangent kernel tanhdot
k ( x , x123 ) = tanh ( scale hx , x123i + oset )
which is mainly used as a proxy for neural networks .
the bessel function of the rst kind kernel besseldot
k ( x , x123 ) =
is a general purpose kernel and is typically used when no further prior knowledge is available and mainly popular in the gaussian process community .
the laplace radial basis kernel laplacedot
k ( x , x123 ) = exp ( kx x123k )
which is a general purpose kernel and is typically used when no further prior knowledge is
the anova radial basis kernel anovadot
k ( x , x123 ) =
which performs well in multidimensional regression problems .
123 kernel utility methods
the package also includes methods for computing commonly used kernel expressions ( e . g . , the gram matrix ) .
these methods are written in such a way that they take functions ( i . e . , kernels ) and matrices ( i . e . , vectors of patterns ) as arguments .
these can be either the kernel functions already included in kernlab or any other function implementing a valid dot product ( taking two vector arguments and returning a scalar ) .
in case one of the already implemented kernels is used , the function calls a vectorized implementation of the corresponding function .
moreover , in the case of symmetric matrices ( e . g . , the dot product matrix of a support vector machine ) they only require one argument rather than having to pass the same matrix twice ( for rows and columns ) .
the computations for the kernels already available in the package are vectorized whenever possible which guarantees good performance and acceptable memory requirements .
users can dene their own kernel by creating a function which takes two vectors as arguments ( the data points ) and returns a scalar ( the dot product ) .
this function can then be based as an argument to the kernel utility methods .
for a user dened kernel the dispatch mechanism calls a generic method implementation which calculates the expression by passing the kernel function through a pair of for loops .
the kernel methods included are : kernelmatrix this is the most commonly used function .
it computes k ( x , x123 ) , i . e . , it computes
the matrix k where kij = k ( xi , xj ) and x is a row vector .
in particular ,
k < - kernelmatrix ( kernel , x )
computes the matrix kij = k ( xi , xj ) where the xi are the columns of x and
k < - kernelmatrix ( kernel , x123 , x123 )
computes the matrix kij = k ( x123i , x123j ) .
kernelmult is a convenient way of computing kernel expansions .
it returns the vector f =
( f ( x123 ) , .
, f ( xm ) ) where
k ( xi , xj ) j , hence f = k .
the need for such a function arises from the fact that k may sometimes be larger than the memory available .
therefore , it is convenient to compute k only in stripes and discard the latter after the corresponding part of k has been computed .
the parameter blocksize determines the number of rows in the stripes .
in particular ,
f < - kernelmult ( kernel , x , alpha )
computes fi =pm computes fi =pm
j=123 k ( xi , xj ) j and
j=123 k ( x123i , x123j ) j .
f < - kernelmult ( kernel , x123 , x123 , alpha )
kernelpol is a method very similar to kernelmatrix with the only dierence that rather than
computing kij = k ( xi , xj ) it computes kij = yiyjk ( xi , xj ) .
this means that
k < - kernelpol ( kernel , x , y )
computes the matrix kij = yiyjk ( xi , xj ) where the xi are the columns of x and yi are elements of the vector y .
moreover ,
k < - kernelpol ( kernel , x123 , x123 , y123 , y123 )
computes the matrix kij = y123iy123jk ( x123i , x123j ) .
both x123 and x123 may be matrices and y123 and
an example using these functions :
r> poly < - polydot ( degree = 123 ) r> x < - matrix ( rnorm ( 123 ) , 123 , 123 ) r> y < - matrix ( rnorm ( 123 ) , 123 , 123 ) r> kx < - kernelmatrix ( poly , x ) r> kxy < - kernelmatrix ( poly , x , y )
123 kernel methods
providing a solid base for creating kernel - based methods is part of what we are trying to achieve with this package , the other being to provide a wider range of kernel - based methods in r .
in the rest of the paper we present the kernel - based methods available in kernlab .
all the methods in kernlab can be used with any of the kernels included in the package as well as with any valid user - dened kernel .
user dened kernel functions can be passed to existing kernel - methods in the
123 support vector machine
support vector machines ( vapnik , 123 ) have gained prominence in the eld of machine learning and pattern classication and regression .
the solutions to classication and regression problems sought by kernel - based algorithms such as the svm are linear functions in the feature space :
for some weight vector w f .
the kernel trick can be exploited in this whenever the weight
vector w can be expressed as a linear combination of the training points , w = pn
f ( x ) = w> ( x )
implying that f can be written as
a very important issue that arises is that of choosing a kernel k for a given learning task .
intuitively , we wish to choose a kernel that induces the right metric in the space .
support vector machines choose a function f that is linear in the feature space by optimizing some criterion over the sample .
in the case of the 123 - norm soft margin classication the optimization problem takes the form :
( i = 123 ,
kwk123 + c t ( w , ) = yi ( hxi , wi + b ) 123 i ( i = 123 ,
based on similar methodology , svms deal with the problem of novelty detection ( or one class classication ) and regression .
kernlabs implementation of support vector machines , ksvm , is based on the optimizers found in bsvm123 ( hsu and lin , 123b ) and libsvm ( chang and lin , 123 ) which includes an very ecient version of the sequential minimization optimization ( smo ) .
smo decomposes the svm quadratic problem ( qp ) without using any numerical qp optimization steps .
instead , it chooses to solve the smallest possible optimization problem involving two elements of i because the must obey one linear equality constraint .
at every step , smo chooses two i to jointly optimize and nds the optimal values for these i analytically , thus avoiding numerical qp optimization , and updates the svm to reect the new optimal values .
the svm implementations available in ksvm include the c - svm classication algorithm along with the - svm classication formulation which is equivalent to the former but has a more natural ( ) model parameter taking values in ( 123 , 123 ) and is proportional to the fraction of support vectors found in the data set and the training error .
for classication problems which include more then two classes ( multi - class ) a one - against - one or pairwise classication method ( knerr et al . , 123; kreel , 123 ) is used .
this method constructs
( cid : 123 ) classiers where each one is trained on data from two classes .
prediction is done by voting
where each classier gives a prediction and the class which is predicted more often wins ( max wins ) .
this method has been shown to produce robust results when used with svms ( hsu and lin , 123a ) .
furthermore the ksvm implementation provides the ability to produce class probabilities as output instead of class labels .
this is done by an improved implementation ( lin et al . , 123 ) of platts posteriori probabilities ( platt , 123 ) where a sigmoid function
p ( y = 123 | f ) =
123 + eaf +b
is tted on the decision values f of the binary svm classiers , a and b are estimated by minimizing the negative log - likelihood function .
to extend the class probabilities to the multi - class case , each binary classiers class probability output is combined by the couple method which implements methods for combing class probabilities proposed in ( wu et al . , 123 ) .
another approach for multi - class classication supported by the ksvm function is the one proposed in ( crammer and singer , 123 ) .
this algorithm works by solving a single optimization problem including the data from all classes :
kwnk123 + c
t ( wn , ) = hxi , wyii hxi , wni bn i = 123 yi , n
where the decision function is
( i = 123 ,
this optimization problem is solved by a decomposition method proposed in ( hsu and lin , 123b ) where optimal working sets are found ( that is , sets of i values which have a high probability of being non - zero ) .
the qp sub - problems are then solved by a modied version of the tron123 ( lin and more , 123 ) optimization software .
one - class classication or novelty detection ( scholkopf et al . , 123; tax and duin , 123 ) , where essentially an svm detects outliers in a data set , is another algorithm supported by ksvm .
svm novelty detection works by creating a spherical decision boundary around a set of data points by a set of support vectors describing the spheres boundary .
the parameter is used to control the volume of the sphere and consequently the number of outliers found .
again , the value of represents the fraction of outliers found .
furthermore , - svm ( vapnik , 123 ) and - svm ( scholkopf et al . , 123 ) regression are also available .
the problem of model selection is partially addressed by an empirical observation for the popular gaussian rbf kernel ( caputo et al . , 123 ) , where the optimal values of the hyper - parameter of sigma are shown to lie in between the 123 and 123 quantile of the kx x123k statistics .
the sigest function uses a sample of the training set to estimate the quantiles and returns a vector containing the values of the quantiles .
pretty much any value within this interval leads to good performance .
an example for the ksvm function is shown below .
r> index < - sample ( 123 : dim ( spam ) ( 123 ) ) r> spamtrain < - spam ( index ( 123 : floor ( 123 * dim ( spam ) ( 123 ) / 123 ) ) , r> spamtest < - spam ( index ( ( ( 123 * ceiling ( dim ( spam ) ( 123 ) / 123 ) ) + r> filter < - ksvm ( type ~ . , data = spamtrain , kernel = " rbfdot " ,
kpar = " automatic " , c = 123 , cross = 123 , prob . model = true )
using automatic sigma estimation ( sigest ) for rbf kernel
support vector machine object of class " ksvm "
sv type : c - classification
parameter : cost c = 123
gaussian radial basis kernel function .
hyperparameter : sigma = 123
number of support vectors : 123 training error : 123 cross validation error : 123
r> predict ( filter , spamtest ( 123 , ) )
levels : nonspam spam
r> predict ( filter , spamtest , type = " probabilities " ) ( 123 ,
123 relevance vector machine
the relevance vector machine ( tipping , 123 ) is a probabilistic sparse kernel model identical in functional form to the svm making predictions based on a function of the form
nk ( x , xn ) + a123
where n are the model weights and k ( , ) is a kernel function .
it adopts a bayesian approach to learning , by introducing a prior over the weights
p ( , ) =
n ( i | 123 , a123
) gamma ( i | , )
governed by a set of hyper - parameters , one associated with each weight , whose most probable values are iteratively estimated for the data .
sparsity is achieved because in practice the posterior distribution in many of the weights is sharply peaked around zero .
furthermore , unlike the svm classier , the non - zero weights in the rvm are not associated with examples close to the decision boundary , but rather appear to represent prototypical examples .
these examples are termed kernlab currently has an implementation of the rvm based on a type ii maximum likelihood method for which can be used for regression .
the functions returns an s123 object containing the model parameters along with indexes for the relevance vectors and the kernel function and
r> rvmm < - rvm ( x , y )
relevance vector machine object of class " rvm " problem type : regression
gaussian radial basis kernel function .
hyperparameter : sigma = 123
number of relevance vectors : 123 variance : 123 training error : 123
r> ytest < - predict ( rvmm , x )
123 gaussian processes
gaussian processes ( williams and rasmussen , 123 ) are based on the prior assumption that ad - jacent observations should convey information about each other .
in particular , it is assumed that the observed variables are normal , and that the coupling between them takes place by means of the covariance matrix of a normal distribution .
using the kernel matrix as the covariance matrix is a convenient way of extending bayesian modeling of linear estimators to nonlinear situations .
fur - thermore it represents the counterpart of the kernel trick in methods minimizing the regularized for regression estimation we assume that rather then observing t ( xi ) we observe yi = t ( xi ) + i where i is assumed to be independed gaussian distributed noise with zero mean .
the posterior distribution is given by
p ( y | t ) =
figure 123 : relevance vector regression on data points created by the sinc ( x ) function , relevance vectors are shown circled .
and after substituting t = k and taking logarithms
ln p ( | y ) = 123
123ky kk123 123
123 t k + c
and maximizing ln p ( | y ) for to obtain the maximum a posteriori approximation yields
= ( k + 123 ) 123y
knowing allows for prediction of y at a new location x through y = k ( x , xi ) .
in similar fashion gaussian processes can be used for classication .
gausspr is the function in kernlab implementing gaussian processes for classication and regres -
the success of google has vividly demonstrated the value of a good ranking algorithm in real world problems .
kernlab includes a ranking algorithm based on work published in ( zhou et al . , 123 ) .
this algorithm exploits the geometric structure of the data in contrast to the more naive approach which uses the euclidean distances or inner products of the data .
since real world data are usually highly structured , this algorithm should perform better than a simpler approach based on a euclidean distance measure .
first , a weighted network is dened on the data and an authoritative score is assigned to every point .
the query points act as source nodes that continually pump their scores to the remaining points via the weighted network , and the remaining points further spread the score to their neigh - bors .
the spreading process is repeated until convergence and the point are ranked according to the scores they received .
suppose we are given a set of data points x = x123 , .
, xs , xs+123 , .
, xm in rn where the rst s points are the query points and the rest are the points to be ranked .
the algorithm works by connecting the two nearest points iteratively until a connected graph g = ( x , e ) is obtained where e is the set of edges .
the anity matrix k dened e . g .
by kij = exp ( kxi xjk123 ) if there is an edge e ( i , j ) e and 123 for the rest and diagonal elements .
the matrix is normalized as
l = d123 / 123kd123 / 123 where dii =pm
j=123 kij , and f ( t + 123 ) = lf ( t ) + ( 123 ) y
lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll123 . 123 . 123 . 123xylllllllllllllllllllllllll edgegraph = true )
r> ran < - spirals ( rowsums ( abs ( spirals ) < 123 ) == 123 , ) r> ranked < - ranking ( ran , 123 , kernel = " rbfdot " , kpar = list ( sigma = 123 ) , r> ranked ( 123 , 123 ) < - max ( ranked ( - 123 , 123 ) ) r> c < - 123 : 123 r> op < - par ( mfrow = c ( 123 , 123 ) , pty = " s " ) r> plot ( ran , cex = c ( ranked ( , 123 ) ) / 123 )
figure 123 : the points on the left are ranked according to their similarity to the upper most left point .
points with a higher rank appear bigger .
instead of ranking the points on simple euclidean distance the structure of the data is recognized and all points on the upper structure are given a higher rank although further away in distance then points in the lower structure .
is iterated until convergence , where is a parameter in ( 123 , 123 ) .
the points are then ranked according to their nal scores fi ( tf ) .
kernlab includes an s123 method implementing the ranking algorithm .
the algorithm can be used both with an edge - graph where the structure of the data is taken into account , and without which is equivalent to ranking the data by their distance in the projected space .
123 spectral clustering
spectral clustering ( ng et al . , 123 ) is a recently emerged promising alternative to common clus - in this method one uses the top eigenvectors of a matrix created by some similarity measure to cluster the data .
similarly to the ranking algorithm , an anity matrix is created out from the data as
kij = exp ( kxi xjk123 )
and normalized as l = d123 / 123kd123 / 123 where dii =pm
j=123 kij .
then the top k eigenvectors ( where k is the number of clusters to be found ) of the anity matrix are used to form an n k matrix y where each column is normalized again to unit length .
treating each row of this matrix as a data point , kmeans is nally used to cluster the points .
kernlab includes an s123 method called specc implementing this algorithm which can be used through an formula interface or a matrix interface .
the s123 object returned by the method extends the class vector and contains the assigned cluster for each point along with information on the
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll123 . 123 . 123 . 123 . 123 . 123ran ( , 123 ) ran ( , 123 ) lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll123 . 123 . 123 . 123 . 123 . 123ran ( , 123 ) ran ( , 123 ) r> data ( spirals ) r> sc < - specc ( spirals , centers = 123 ) r> plot ( spirals , col = sc )
figure 123 : clustering the two spirals data set with specc
centers size and within - cluster sum of squares for each cluster .
in case a gaussian rbf kernel is being used a model selection process can be used to determine the optimal value of the hyper - parameter .
for a good value of the values of y tend to cluster tightly and it turns out that the within cluster sum of squares is a good indicator for the quality of the sigma parameter found .
we then iterate through the sigma values to nd an optimal value for .
123 kernel principal components analysis
principal component analysis ( pca ) is a powerful technique for extracting structure from possibly high - dimensional datasets .
pca is an orthogonal transformation of the coordinate system in which we describe the data .
the new coordinates by which we represent the data are called principal components .
kernel pca ( scholkopf et al . , 123 ) performs a nonlinear transformation of the coordinate system by nding principal components which are nonlinearly related to the input variables .
given a set of centered observations xk , k = 123 , .
, m , xk rn , pca diagonalizes the covariance matrix c = 123 j by solving the eigenvalue problem v = cv .
the same computation can be done in a dot product space f which is related to the input space by a possibly nonlinear map : rn f , x 123 x .
assuming that we deal with centered data and use
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll123 . 123 . 123 . 123 . 123 . 123spirals ( , 123 ) spirals ( , 123 ) the covariance matrix in f ,
the kernel principal components are then computed by taking the eigenvectors of the centered kernel matrix kij = h ( xj ) , ( xj ) i .
kpca , the the function implementing kpca in kernlab , can be used both with a formula and a matrix interface , and returns an s123 object of class kpca containing the principal components the corresponding eigenvalues along with the projection of the training data on the new coordinate system .
furthermore , the predict function can be used to embed new data points into the new
123 kernel canonical correlation analysis
canonical correlation analysis ( cca ) is concerned with describing the linear relations between if we have two data sets x123 and x123 , then the classical cca attempts to nd linear combination of the variables which give the maximum correlation between the combinations
y123 = w123x123 =x y123 = w123x123 =x
one wishes to nd those values of w123 and w123 which maximize the correlation between y123 and y123
similar to the kpca algorithm , cca can be extended and used in a dot product space f which is related to the input space by a possibly nonlinear map : rn f , x 123 x as
y123 = w123 ( x123 ) =x y123 = w123 ( x123 ) =x
following ( kuss and graepel , 123 ) , the kernlab implementation of a kcca projects the data vectors on a new coordinate system using kpca and uses linear cca to retrieve the correla - tion coecients .
the kcca method in kernlab returns an s123 object containing the correlation coecients for each data set and the corresponding correlation along with the kernel used .
123 interior point code quadratic optimizer
in many kernel based algorithms , learning implies the minimization of some risk function .
typi - cally we have to deal with quadratic or general convex problems for support vector machines of
f and ci are convex functions and n n .
kernlab provides the s123 method ipop implementing an optimizer of the interior point family ( vanderbei , 123 ) which solves the quadratic programming
ci ( x ) 123 for all i ( n ) .
c>x + 123 b ax b + r l x u
this optimizer can be used in regression , classication , and novelty detection in svms .
r> train < - sample ( 123 : dim ( spam ) ( 123 ) , 123 ) r> kpc < - kpca ( ~ . , data = spam ( train , - 123 ) , kernel = " rbfdot " , r> kpcv < - pcv ( kpc ) r> plot ( rotated ( kpc ) , col = as . integer ( spam ( train , 123 ) ) ,
kpar = list ( sigma = 123 ) , features = 123 )
xlab = " 123st principal component " , ylab = " 123nd principal component " )
figure 123 : projection of the spam data on two kernel principal components using an rbf kernel
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll123st principal component123nd principal component 123 incomplete cholesky decomposition
when dealing with kernel based algorithms , calculating a full kernel matrix should be avoided since it is already a o ( n 123 ) operation .
fortunately , the fact that kernel matrices are positive semidenite is a strong constraint and good approximations can be found with small computational cost .
the cholesky decomposition factorizes a positive semidenite n n matrix k as k = zz t , where z is an upper triangular n n matrix .
exploiting the fact that kernel matrices are usually of low rank , an incomplete cholesky decomposition ( wright , 123 ) nds a matrix z of size n m where m ( cid : 123 ) n such that the norm of k z z t is smaller than a given tolerance .
the main dierence of incomplete cholesky decomposition to the standard cholesky decomposition is that pivots which are below a certain threshold are simply skipped .
if l is the number of skipped pivots , we obtain a z with only m = n l columns .
the algorithm works by picking a column from k to be added by maximizing a lower bound on the reduction of the error of the approximation .
kernlab has an implementation of an incomplete cholesky factorization called inc . chol which computes the decomposed matrix z from the original data for any given kernel without the need to compute a full kernel matrix beforehand .
this has the advantage that no full kernel matrix has to be stored
in this paper we described kernlab , a exible and extensible kernel methods package for r with existing modern kernel algorithms along with tools for constructing new kernel based algorithms .
it provides a unied framework for using and creating kernel - based algorithms in r while using all of rs modern facilities , like s123 classes and namespaces .
our aim for the future is to extend the package and add more kernel - based methods as well as kernel relevant tools .
sources and binaries for the latest version of kernlab are available at cran123 under the gnu public license .
