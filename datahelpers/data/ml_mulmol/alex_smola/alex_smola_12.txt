support vector ( sv ) machines comprise a new class of learning algorithms , motivated by results of statistical learning theory ( vapnik , 123 ) .
originally developed for pattern recognition ( vapnik & chervonenkis , 123; boser , guyon , & vapnik , 123 ) , they represent the decision boundary in terms of a typically small subset ( scholkopf , burges , & vapnik , 123 ) of all training examples , called the support vectors .
in order for this sparseness property to carry over to the case of sv regression , vapnik devised the so - called " - insensitive loss function ,
jy f . x / j " d maxf123;jy f . x / j " g;
which does not penalize errors below some " > 123 , chosen a priori .
his algorithm , which we will henceforth call " - svr , seeks to estimate functions ,
f . x / d . w x / c b; w; x 123 rn; b 123 r;
present address : microsoft research , 123 guildhall street , cambridge , u . k .
neural computation 123 , 123 ( 123 )
c ( cid : 123 ) 123 massachusetts institute of technology
scholkopf , a .
smola , r .
williamson , and p .
bartlett
based on independent and identically distributed ( i . i . d . ) data ,
. x123; y123 / ; : : : ; . x; y / 123 rn r :
here , rn is the space in which the input patterns live but most of the fol - lowing also applies for inputs from a set x .
the goal of the learning process is to nd a function f with a small risk ( or test error ) ,
f; x; y / dp . x; y / ;
r ( f ) d
where p is the probability measure , which is assumed to be responsible for the generation of the observations ( see equation 123 ) and l is a loss func - tion , for example , l .
f; x; y / d .
f . x / y / 123 , or many other choices ( smola & scholkopf , 123 ) .
the particular loss function for which we would like to minimize equation 123 depends on the specic regression estimation prob - lem at hand .
this does not necessarily have to coincide with the loss function used in our learning algorithm .
first , there might be additional constraints that we would like our regression estimation to satisfy , for instance , that it have a sparse representation in terms of the training data .
in the sv case , this is achieved through the insensitive zone in equation 123 .
second , we cannot minimize equation 123 directly in the rst place , since we do not know p .
instead , we are given the sample , equation 123 , and we try to obtain a small risk by minimizing the regularized risk functional ,
here , kwk123 is a term that characterizes the model complexity ,
emp ( f ) :
kwk123 c c r "
emp ( f ) : d 123
jyi f . xi / j " ;
measures the " - insensitive training error , and c is a constant determining the trade - off .
in short , minimizing equation 123 captures the main insight of statistical learning theory , stating that in order to obtain a small risk , one needs to control both training error and model complexitythat is , explain the data with a simple model .
the minimization of equation 123 is equivalent to the following con -
strained optimization problem ( see figure 123 ) :
. w; . / / d 123
kwk123 c c 123
new support vector algorithms
figure 123 : in sv regression , a desired accuracy " is specied a priori .
it is then attempted to t a tube with radius " to the data .
the trade - off between model complexity and points lying outside the tube ( with positive slack variables ) is determined by minimizing the expression 123 .
subject to . . w xi / c b / yi " c i yi . . w xi / c b / " c
here and below , it is understood that i d 123; : : : ; , and that boldface greek letters denote - dimensional vectors of the corresponding variables; . / is a shorthand implying both the variables with and without asterisks .
by using lagrange multiplier techniques , one can show ( vapnik , 123 )
that this leads to the following dual optimization problem .
maximize
c i / c x
the resulting regression estimates are linear; however , the setting can be generalized to a nonlinear one by using the kernel method .
as we will use precisely this method in the next section , we shall omit its exposition at this
scholkopf , a .
smola , r .
williamson , and p .
bartlett
to motivate the new algorithm that we shall propose , note that the pa - rameter " can be useful if the desired accuracy of the approximation can be specied beforehand .
in some cases , however , we want the estimate to be as accurate as possible without having to commit ourselves to a specic level of accuracy a priori .
in this work , we rst describe a modication of the " - svr algorithm , called - svr , which automatically minimizes " .
following this , we present two theoretical results on - svr concerning the connection to robust estimators ( section 123 ) and the asymptotically optimal choice of the parameter ( section 123 ) .
next , we extend the algorithm to handle parametric insensitivity models that allow taking into account prior knowledge about heteroscedasticity of the noise .
as a bridge connecting this rst theoreti - cal part of the article to the second one , we then present a denition of a margin that both sv classication and sv regression algorithms maximize ( section 123 ) .
in view of this close connection between both algorithms , it is not surprising that it is possible to formulate also a - sv classication al - gorithm .
this is done , including some theoretical analysis , in section 123
we conclude with experiments and a discussion .
123 - sv regression
to estimate functions ( see equation 123 ) from empirical data ( see equa - tion 123 ) we proceed as follows ( scholkopf , bartlett , smola , & williamson , 123 ) .
at each point xi , we allow an error of " .
everything above " is cap - tured in slack variables . / , which are penalized in the objective function via a regularization constant c , chosen a priori ( vapnik , 123 ) .
the size of " is traded off against model complexity and slack variables via a constant
; 123 , and obtain the
. w; . / ; " / d 123
kwk123 c c
subject to . . w xi / c b / yi " c i yi . . w xi / c b / " c
123; " 123 :
for the constraints , we introduce multipliers . /
l . w; b; . / ; ; . / ; " ; . / / kwk123 c c " c c
/ " x
new support vector algorithms
i . i c yi . w xi / b c " /
c . w xi / c b yi c " / :
; ; . /
to minimize the expression 123 , we have to nd the saddle point of lthat is , minimize over the primal variables w; " ; b; . / and maximize over the dual variables . / .
setting the derivatives with respect to the primal variables equal to zero yields four equations :
/ d 123
in the sv expansion , equation 123 , only those . / i will be nonzero that cor - respond to a constraint , equations 123 or 123 , which is precisely met; the corresponding patterns are called support vectors .
this is due to the karush - kuhn - tucker ( kkt ) conditions that apply to convex constrained optimiza - tion problems ( bertsekas , 123 ) .
if we write the constraints as g . xi; yi / 123 , with corresponding lagrange multipliers i , then the solution satises i g . xi; yi / d 123 for all i .
substituting the above four conditions into l leads to another optimiza - tion problem , called the wolfe dual .
before stating it explicitly , we carry out one further modication .
following boser et al .
( 123 ) , we substitute a kernel k for the dot product , corresponding to a dot product in some feature space related to input space via a nonlinear map 123 ,
k . x; y / d . 123x / 123y / / :
by using k , we implicitly carry out all computations in the feature space that 123 maps into , which can have a very high dimensionality .
the feature space has the structure of a reproducing kernel hilbert space ( wahba , 123; girosi , 123; scholkopf , 123 ) and hence minimization ofkwk123 can be understood in the context of regularization operators ( smola , scholkopf , & m uller , 123 ) .
the method is applicable whenever an algorithm can be cast in terms of dot products ( aizerman , braverman , & rozonoer , 123; boser et al . , 123; scholkopf , smola , & m uller , 123 ) .
the choice of k is a research topic in its
/ c :
f . x / d x
i / k . xi; x / c b;
scholkopf , a .
smola , r .
williamson , and p .
bartlett
own right that we shall not touch here ( williamson , smola , & scholkopf , 123; scholkopf , shawe - taylor , smola , & williamson , 123 ) ; typical choices include gaussian kernels , k . x; y / d exp . kx yk123= . 123 ( cid : 123 ) 123 / / and polynomial kernels , k . x; y / d . x y / d . ( cid : 123 ) > 123; d 123 n / .
i 123 do not appear in the dual , we arrive at the - svr optimization problem : for 123; c > 123 ,
rewriting the constraints , noting that ; . /
maximize w . . / / d x
the regression estimate then takes the form ( cf .
equations 123 , 123 , and 123 ) ,
where b ( and " ) can be computed by taking into account that equations 123 j / k . xj; x / for . w x / is understood; cf .
and 123 ( substitution of equations 123 and 123 ) become equalities with . / i d 123 for points with 123 < . /
i < c= , respectively , due to the kkt conditions .
before we give theoretical results explaining the signicance of the pa - rameter , the following observation concerning " is helpful .
if > 123 , then " d 123 , since it does not pay to increase " ( cf .
equation 123 ) .
if 123 , it can still happen that " d 123for example , if the data are noise free and can be perfectly interpolated with a low - capacity model .
the case " d 123 , however , is not what we are interested in; it corresponds to plain l123 - loss regression .
we will use the term errors to refer to training points lying outside the tube123 and the term fraction of errors or svs to denote the relative numbers of
123 for n > 123 , the tube should actually be called a slabthe region between two
new support vector algorithms
errors or svs ( i . e . , divided by ) .
in this proposition , we dene the modulus j f . bi / of absolute continuity of a function f as the function . / d sup f . ai / j , where the supremum is taken over all disjoint intervals . ai; bi / with . bi ai / < .
loosely speaking , the condition on the ai < bi satisfying conditional density of y given x asks that it be absolutely continuous on
proposition 123
suppose - svr is applied to some data set , and the resulting " is nonzero .
the following statements hold :
is an upper bound on the fraction of errors .
is a lower bound on the fraction of svs .
suppose the data ( see equation 123 ) were generated i . i . d .
from a distribution p . x; y / d p . x / p . yjx / with p . yjx / continuous and the expectation of the modulus of absolute continuity of its density satisfying lim ! 123 e . / d 123
with probability 123 , asymptotically , equals both the fraction of svs and the fraction of errors .
i d c= .
all examples with . / i d c= ( if not , . /
ad ( i ) .
the constraints , equations 123 and 123 , imply that at most a fraction of all examples can have . / i > 123 ( i . e . , those outside the tube ) certainly satisfy . / grow further to reduce . / ad ( ii ) .
by the kkt conditions , " > 123 implies d 123
hence , equation 123 becomes an equality ( cf .
equation 123 ) . 123 since svs are those examples for which 123 < . / d 123 for all i; vapnik ,
i c= , the result follows ( using i
ad ( iii ) .
the strategy of proof is to show that asymptotically , the proba - bility of a point is lying on the edge of the tube vanishes .
the condition on p . yjx / means that
j f . x / c t yj < ( cid : 123 )
< . ( cid : 123 ) /
for some function . ( cid : 123 ) / that approaches zero as ( cid : 123 ) ! 123
since the class of sv regression estimates f has well - behaved covering numbers , we have ( anthony & bartlett , 123 , chap .
123 ) that for all t ,
op . j f . x / ct yj < ( cid : 123 ) =123 / <p . j f . x / ctyj < ( cid : 123 ) /
123 in practice , one can alternatively work with equation 123 as an equality constraint .
scholkopf , a .
smola , r .
williamson , and p .
bartlett where op is the sample - based estimate of p ( that is , the proportion of points that satisfyj f . x / yctj < ( cid : 123 ) ) , and c123; c123 may depend on ( cid : 123 ) and .
discretizing the values of t , taking the union bound , and applying equation 123 shows that the supremum over f and t of op .
f . x / y c t d 123 / converges to zero in probability .
thus , the fraction of points on the edge of the tube almost surely converges to 123
hence the fraction of svs equals that of errors .
combining statements i and ii then shows that both fractions converge almost surely
hence , 123 123 can be used to control the number of errors ( note that for d 123 for all i ( vapnik , 123 ) ) .
123 , equation 123 implies 123 , since i moreover , since the constraint , equation 123 , implies that equation 123 is i c=123 , we conclude that proposition 123 actually holds for the upper and the lower edges of the tube separately , with =123 each .
( note that by the same argument , the number of svs at the two edges of the standard " - svr tube asymptotically agree . )
d 123 , hence d . @=@ " / r "
a more intuitive , albeit somewhat informal , explanation can be given in terms of the primal objective function ( see equation 123 ) .
at the point of the solution , note that if " > 123 , we must have . @=@ " / . w; " / d 123 , that is , c . @=@ " / r " emp .
this is greater than or equal to the fraction of errors , since the points outside the tube certainly do contribute to a change in remp when " is changed .
points at the edge of the tube possibly might also contribute .
this is where the inequality comes case , " d 123 , since it does not pay to increase " ( cf .
equation 123 ) .
note that this does not contradict our freedom to choose > 123
in that
let us briey discuss how - svr relates to " - svr ( see section 123 ) .
both al - gorithms use the " - insensitive loss function , but - svr automatically com - putes " .
in a bayesian perspective , this automatic adaptation of the loss function could be interpreted as adapting the error model , controlled by the hyperparameter .
comparing equation 123 ( substitution of a kernel for the dot product is understood ) and equation 123 , we note that " - svr requires c i / , which , for xed " > 123 , encourages an additional term , " that some of the . / i will turn out to be 123
accordingly , the constraint ( see equation 123 ) , which appears in - svr , is not needed .
the primal problems , equations 123 and 123 , differ in the term " .
if d 123 , then the optimization can grow " arbitrarily large; hence zero empirical risk can be obtained even when all s are zero .
case , using kernels , nw is a vector in feature space .
in the following sense , - svr includes " - svr .
note that in the general
priori to n " , and the same value of c , has the solution nw; nb .
if - svr leads to the solution n " ; nw; nb , then " - svr with " set a
new support vector algorithms
remaining variables .
the solution does not change .
if we minimize equation 123 , then x " and minimize only over the
123 the connection to robust estimators
using the " - insensitive loss function , only the patterns outside the " - tube enter the empirical risk term , whereas the patterns closest to the actual regression have zero loss .
this , however , does not mean that it is only the outliers that determine the regression .
in fact , the contrary is the case .
proposition 123 ( resistance of sv regression ) .
using support vector regression with the " - insensitive loss function ( see equation 123 ) , local movements of target values of points outside the tube do not inuence the regression .
shifting yi locally does not change the status of . xi; yi / as being a point outside the tube .
then the dual solution . / is still feasible; it satises the constraints ( the point still has . / i d c= ) .
moreover , the primal solution , with i transformed according to the movement of yi , is also feasible .
finally , the kkt conditions are still satised , as still . / i d c= .
thus ( bertsekas , 123 ) , . / is still the optimal solution .
the proof relies on the fact that everywhere outside the tube , the upper bound on the . / is the same .
this is precisely the case if the loss func - tion increases linearly ouside the " - tube ( cf .
huber , 123 , for requirements on robust cost functions ) .
inside , we could use various functions , with a derivative smaller than the one of the linear part .
for the case of the " - insensitive loss , proposition 123 implies that essentially , the regression is a generalization of an estimator for the mean of a random variable that does the following :
throws away the largest and smallest examples ( a fraction =123 of either category; in section 123 , it is shown that the sum constraint , equation 123 , implies that proposition 123 can be applied separately for the two sides , estimates the mean by taking the average of the two extremal ones of
the remaining examples .
this resistance concerning outliers is close in spirit to robust estima - tors like the trimmed mean .
in fact , we could get closer to the idea of the trimmed mean , which rst throws away the largest and smallest points and then computes the mean of the remaining points , by using a quadratic loss inside the " - tube .
this would leave us with hubers robust loss func -
scholkopf , a .
smola , r .
williamson , and p .
bartlett
note , moreover , that the parameter is related to the breakdown point of the corresponding robust estimator ( huber , 123 ) .
because it species the fraction of points that may be arbitrarily bad outliers , is related to the fraction of some arbitrary distribution that may be added to a known noise model without the estimator failing .
finally , we add that by a simple modication of the loss function ( white , 123 ) weighting the slack variables . / above and below the tube in the tar - get function , equation 123 , by 123 and 123 / , with 123 ( 123; 123 ) respectively one can estimate generalized quantiles .
the argument proceeds as follows .
asymptotically , all patterns have multipliers at bound ( cf .
proposition 123 ) .
the , however , changes the upper bounds in the box constraints applying to the two different types of slack variables to 123c= and 123c . 123 / = , re - spectively .
the equality constraint , equation 123 , then implies that . 123 / and give the fractions of points ( out of those which are outside the tube ) that lie on the two sides of the tube , respectively .
123 asymptotically optimal choice of
using an analysis employing tools of information geometry ( murata , yoshizawa , & amari , 123; smola , murata , scholkopf , & m uller , 123 ) , we can derive the asymptotically optimal for a given class of noise models in the sense of maximizing the statistical efciency . 123
remark .
denote p a density with unit variance , 123 and p a family of noise models generated from p by p : d fpjp d 123 / ; ( cid : 123 ) > 123g .
moreover assume that the data were generated i . i . d .
from a distribution p . x; y / d p . x / p . y f . x / / with p . y f . x / / continuous .
under the assumption that sv regression produces an estimate of converging to the underlying functional dependency f , the asymptotically optimal , for the estimation - of - location - parameter model of sv regression described in smola , murata , scholkopf , & m uller ( 123 ) , is
( cid : 123 ) p
" : d argmin
p . t / dt where
123 this section assumes familiarity with some concepts of information geometry .
a more complete explanation of the model underlying the argument is given in smola , murata , scholkopf , & m uller ( 123 ) and can be downloaded from http : / / svm . rst . gmd . de .
123 p is a prototype generating the class of densities p .
normalization assumptions are
made for ease of notation .
new support vector algorithms
to see this , note that under the assumptions stated above , the probability
of a deviation larger than " , prfjy of . x / j > " g , will converge to
' jy f . x / j > "
xfrn ( " ; " ) g p . x / p .
/ dx d
this is also the fraction of samples that will ( asymptotically ) become svs ( proposition 123 , iii ) .
therefore an algorithm generating a fraction d 123
r " " p .
/ d svs will correspond to an algorithm with a tube of size "
consequence is that given a noise model p .
/ , one can compute the optimal " for it , and then , by using equation 123 , compute the corresponding optimal
to this end , one exploits the linear scaling behavior between the standard deviation ( cid : 123 ) of a distribution p and the optimal " .
this result , established in smola , murata , scholkopf , & m uller ( 123 ) and smola ( 123 ) , cannot be proved here; instead , we shall merely try to give a avor of the argument .
the basic idea is to consider the estimation of a location parameter using the " - insensitive loss , with the goal of maximizing the statistical efciency .
us - ing the cramer - rao bound and a result of murata et al .
( 123 ) , the efciency is found to be
here , i is the fisher information , while q and g are information geometrical quantities computed from the loss function and the noise model .
this means that one only has to consider distributions of unit variance , say , p , to compute an optimal value of that holds for the whole class of distributions p .
using equation 123 , one arrives at
. p . " / c p . " / / 123
the minimum of equation 123 yields the optimal choice of " , which allows computation of the corresponding and thus leads to equation 123 .
consider now an example : arbitrary polynomial noise models ( / e
with unit variance can be written as
where 123 denotes the gamma function .
table 123 shows the optimal value of for different polynomial degrees .
observe that the more lighter - tailed
scholkopf , a .
smola , r .
williamson , and p .
bartlett
table 123 : optimal for various degrees of polynomial additive noise .
polynomial degree p
the distribution becomes , the smaller are optimalthat is , the tube width increases .
this is reasonable as only for very long tails of the distribution ( data with many outliers ) it appears reasonable to use an early cutoff on the inuence of the data ( by basically giving all data equal inuence via i d c= ) .
the extreme case of laplacian noise ( d 123 ) leads to a tube width of 123 , that is , to l123 regression .
we conclude this section with three caveats : rst , we have only made an asymptotic statement; second , for nonzero " , the sv regression need not necessarily converge to the target f : measured using j : j " , many other functions are just as good as f itself; third , the proportionality between " and ( cid : 123 ) has only been established in the estimation - of - location - parameter context , which is not quite sv regression .
123 parametric insensitivity models
we now return to the algorithm described in section 123
we generalized " - svr by estimating the width of the tube rather than taking it as given a priori .
what we have so far retained is the assumption that the " - insensitive zone has a tube ( or slab ) shape .
we now go one step further and use parametric models of arbitrary shape .
this can be useful in situations where the noise is heteroscedastic , that is , where it depends on x .
g ( here and below , q d 123; : : : ; p is understood ) be a set of 123p positive functions on the input space x .
consider the following quadratic program : for given . /
p 123 , minimize
let f . /
; : : : ; . /
. w; . / ; " . / / d kwk123=123
subject to . . w xi / c b / yi yi . . w xi / c b /
" qq . xi / c i
new support vector algorithms
a calculation analogous to that in section 123 shows that the wolfe dual con - sists of maximizing the expression 123 subject to the constraints 123 and 123 , and , instead of 123 , the modied constraints , still linear in . / ,
. xi / c . /
in the experiments in section 123 , we use a simplied version of this opti - mization problem , where we drop the term q from the objective function , equation 123 , and use " q and q in equation 123 .
by this , we render the prob - lem symmetric with respect to the two edges of the tube .
in addition , we use p d 123
this leads to the same wolfe dual , except for the last constraint , which becomes ( cf .
equation 123 ) ,
/ . xi / c :
note that the optimization problem of section 123 can be recovered by using the constant function 123
, evaluated on the xi with 123 < . /
the advantage of this setting is that since the same is used for both sides of the tube , the computation of " ; b is straightforward : for instance , by solving a linear system , using two conditions as those described following equation 123 .
otherwise , general statements are harder to make; the linear system can have a zero determinant , depending on whether the functions i < c= , are linearly dependent .
the latter occurs , for instance , if we use constant functions . / 123
in this case , it is pointless to use two different values ; ; for the constraint ( see i will be bounded by equation 123 ) then implies that both sums c minf; g .
we conclude this section by giving , without proof , a gener - alization of proposition 123 to the optimization problem with constraint ( see
proposition 123
suppose we run the above algorithm on a data set with the result that " > 123
. xi / is an upper bound on the fraction of errors .
. xi / is an upper bound on the fraction of svs .
123 observe the similarity to semiparametric sv models ( smola , frie , & scholkopf , 123 ) where a modication of the expansion of f led to similar additional constraints .
the important difference in the present setting is that the lagrange multipliers i and treated equally and not with different signs , as in semiparametric modeling .
scholkopf , a .
smola , r .
williamson , and p .
bartlett
suppose the data in equation 123 were generated i . i . d .
from a distribution p . x; y / d p . x / p . yjx / with p . yjx / continuous and the expectation of its modulus of continuity satisfying lim ! 123 e . / d 123
with probability 123 , . x / d qp . x / / 123 , asymptotically , the fractions of svs and errors equal .
where qp is the asymptotic distribution of svs over x .
123 margins in regression and classication
the sv algorithm was rst proposed for the case of pattern recognition ( boser et al . , 123 ) , and then generalized to regression ( vapnik , 123 ) .
con - ceptually , however , one can take the view that the latter case is actually the simpler one , providing a posterior justication as to why we started this article with the regression case .
to explain this , we will introduce a suitable denition of a margin that is maximized in both cases .
at rst glance , the two variants of the algorithm seem conceptually dif - ferent .
in the case of pattern recognition , a margin of separation between two pattern classes is maximized , and the svs are those examples that lie closest to this margin .
in the simplest case , where the training error is xed to 123 , this is done by minimizing kwk123 subject to yi . . w xi / c b / 123 ( note that in pattern recognition , the targets yi are in f123g ) .
in regression estimation , on the other hand , a tube of radius " is tted to the data , in the space of the target values , with the property that it corre - sponds to the attest function in feature space .
here , the svs lie at the edge of the tube .
the parameter " does not occur in the pattern recognition case .
we will show how these seemingly different problems are identical ( cf .
also vapnik , 123; pontil , rifkin , & evgeniou , 123 ) , how this naturally leads to the concept of canonical hyperplanes ( vapnik , 123 ) , and how it suggests different generalizations to the estimation of vector - valued functions .
denition 123 ( " - margin ) .
let . e;k : ke / , . f;k : kf / be normed spaces , andx e .
we dene the " - margin of a function f : x ! f as
f / : d inffkx yke : x; y 123 x ;k f . x / f . y / kf 123 " g :
f / can be zero , even for continuous functions , an example being f . x / d 123=x on x d r
there , m " .
f / d 123 for all " > 123
note that the " - margin is related ( albeit not identical ) to the traditional modulus of continuity of a function : given > 123 , the latter measures the largest difference in function values that can be obtained using points within a distance in e .
the following observations characterize the functions for which the mar -
gin is strictly positive .
lemma 123 ( uniformly continuous functions ) .
with the m " .
f / is positive for all " > 123 if and only if f is uniformly continuous .
new support vector algorithms
by denition of m " , we have
k f . x / f . y / kf 123 " h ) kx yke m " .
f / ( ) kx yke < m " .
f / h ) k f . x / f . y / kf < 123 "
that is , if m " .
f / > 123 , then f is uniformly continuous .
similarly , if f is uni - formly continuous , then for each " > 123 , we can nd a > 123 such that k f . x / f . y / kf 123 " implies kx yke .
since the latter holds uniformly , we can take the inmum to get m " .
f / > 123
we next specialize to a particular set of uniformly continuous functions .
lemma 123 ( lipschitz - continuous functions ) .
that for all x; y 123 x , k f . x / f . y / kf l kx yke , then m " 123 " proof .
take the inmum over kx yke k f . x / f . y / kf
if there exists some l > 123 such
example 123 ( sv regression estimation ) .
suppose that e is endowed with a dot product . : : / ( generating the norm k : ke ) .
for linear functions ( see equation 123 ) the margin takes the form m " .
f / d 123 " kwk : to see this , note that since j f . x / f . y / j d j . w . x y / / j , the distance kx yk will be smallest given j . w . x y / / j d 123 " , when x y is parallel to w ( due to cauchy - schwartz ) , i . e .
if x y d 123 " w=kwk123
in that case , kx yk d 123 " =kwk .
for xed " > 123 , maximizing the margin hence amounts to minimizing kwk , as done in sv regression : in the simplest form ( cf .
equation 123 without slack variables i ) the training on data ( equation 123 ) consists of minimizing kwk123 subject to
j f . xi / yij " :
example 123 ( sv pattern recognition; see figure 123 ) .
we specialize the setting of example 123 to the case where x d fx123; : : : ; xg .
then m123
f / d 123kwk is equal to the margin dened for vapniks canonical hyperplane ( vapnik , 123 ) .
the latter is a way in which , given the data set x , an oriented hyperplane in e can be uniquely expressed by a linear function ( see equation 123 ) requiring that
minfj f . x / j : x 123 xg d 123 :
vapnik gives a bound on the vc - dimension of canonical hyperplanes in terms of kwk .
an optimal margin sv machine for pattern recognition can be constructed
. x123; y123 / ; : : : ; . x; y / 123 x f123g
scholkopf , a .
smola , r .
williamson , and p .
bartlett
figure 123 : 123d toy problem .
separate x from o .
the sv classication algorithm constructs a linear function f . x / d w x c b satisfying equation 123 ( " d 123 ) .
to maximize the margin m " .
f / , one has to minimize jwj .
as follows ( boser et al . , 123 ) :
minimize kwk123 subject to yi f . xi / 123 :
the decision function used for classication takes the form
. x / d sgn . . w x / c b / :
the parameter " is superuous in pattern recognition , as the resulting decision
. x / d sgn . . w x / c b / ;
will not change if we minimize kwk123 subject to
yi f . xi / " :
finally , to understand why the constraint ( see equation 123 ) looks different from equation 123 ( e . g . , one is multiplicative , the other one additive ) , note that in re - gression , the points . xi; yi / are required to lie within a tube of radius " , whereas in pattern recognition , they are required to lie outside the tube ( see figure 123 ) , and on the correct side .
for the points on the tube , we have 123 d yi f . xi / d 123j f . xi / yij .
so far , we have interpreted known algorithms only in terms of maxi - mizing m " .
next , we consider whether we can use the latter as a guide for constructing more general algorithms .
example 123 ( sv regression for vector - valued functions ) .
assume e d rn .
for linear functions f . x / d wxc b; with w being an n n matrix , and b 123 rn ,
new support vector algorithms
we have , as a consequence of lemma 123 ,
f / 123 "
where kwk is any matrix norm of w that is compatible ( horn & johnson , 123 ) with k : ke .
if the matrix norm is the one induced by k : ke , that is , there exists a unit vector z 123 e such that kwzke d kwk , then equality holds in 123 .
to see the latter , we use the same argument as in example 123 , setting x y d ij , which is compatible with the vector normk : k123 , the problem of minimizingkwk subject to separate constraints for each output dimension separates into n regression problems .
for the hilbert - schmidt norm kwk123 d
in smola , williamson , mika , & scholkopf ( 123 ) , it is shown that one can specify invariance requirements , which imply that the regularizers act on the output dimensions separately and identically ( i . e . , in a scalar fashion ) .
in particular , it turns out that under the assumption of quadratic homogeneity and permutation symmetry , the hilbert - schmidt norm is the only admissible one .
123 - sv classication
we saw that - svr differs from " - svr in that it uses the parameters and c instead of " and c .
in many cases , this is a useful reparameterization of the original algorithm , and thus it is worthwhile to ask whether a similar change could be incorporated in the original sv classication algorithm ( for brevity , we call it c - svc ) .
there , the primal optimization problem is to minimize ( cortes & vapnik , 123 )
. w; / d 123
kwk123 c c
yi . . xi w / c b / 123 i;
( see equation 123 ) the goal of the learning process is to estimate a function f such that the probability of misclassication on an independent test set , the risk r ( f
) , is small . 123
here , the only parameter that we can dispose of is the regularization constant c .
to substitute it by a parameter similar to the used in the regression case , we proceed as follows .
as a primal problem for - svc , we
123 implicitly we make use of thef123; 123g loss function; hence the risk equals the probability
scholkopf , a .
smola , r .
williamson , and p .
bartlett
consider the minimization of
. w; ; / d 123
kwk123 c 123
subject to ( cf .
equation 123 )
yi . . xi w / c b / i;
for reasons we shall expain , no constant c appears in this formulation .
to understand the role of , note that for d 123 , the constraint ( see 123 ) simply states that the two classes are separated by the margin 123=kwk ( cf .
to derive the dual , we consider the lagrangian
kwk123 c 123 l . w; ; b; ; ; ; / d 123 . i . yi . . xi w / c b / c i / c ii /
using multipliers i; i; 123
this function has to be mimimized with re - spect to the primal variables w; ; b; and maximized with respect to the dual variables ; ; .
to eliminate the former , we compute the correspond - ing partial derivatives and set them to 123 , obtaining the following conditions :
i d :
i c i d 123=;
w . / d 123
in the sv expansion ( see equation 123 ) , only those i can be nonzero that correspond to a constraint ( see 123 ) that is precisely met ( kkt conditions; cf .
vapnik , 123 ) .
substituting equations 123 and 123 into l , using i; i; 123 , and incor - porating kernels for dot products leaves us with the following quadratic optimization problem : maximize
new support vector algorithms
123 i 123=
the resulting decision function can be shown to take the form
iyik . x; xi / c b
. x / d sgn
compared to the original dual ( boser et al . , 123; vapnik , 123 ) , there are two differences .
first , there is an additional constraint , 123 , similar to the i of boser et al .
( 123 ) no regression case , 123 .
second , the linear term longer appears in the objective function 123 .
this has an interesting conse - quence : 123 is now quadratically homogeneous in .
it is straightforward to verify that one obtains exactly the same objective function if one starts with the primal function . w; ; / d kwk123=123 c c .
c . 123= / if one does use c ) , the only difference being that the constraints , 123 and 123 would have an extra factor c on the right - hand side .
in that case , due to the homogeneity , the solution of the dual would be scaled by c; however , it is straightforward to see that the corresponding decision function will not change .
hence we may set c d 123
to compute b and , we consider two sets s , of identical size s > 123 , containing svs xi with 123 < i < 123 and yi d 123 , respectively .
then , due to the kkt conditions , 123 becomes an equality with i d 123
hence , in terms of
b d 123
as in the regression case , the parameter has a more natural interpreta - tion than the one we removed , c .
to formulate it , let us rst dene the term margin error .
by this , we denote points with i > 123that is , that are either errors or lie within the margin .
formally , the fraction of margin errors is
emp ( f ) : d 123
jfi : yi f . xi / < gj :
scholkopf , a .
smola , r .
williamson , and p .
bartlett
here , f is used to denote the argument of the sgn in the decision function , equation 123 , that is , f
d sgn f .
we are now in a position to modify proposition 123 for the case of pattern
proposition 123
suppose k is a real analytic kernel function , and we run - svc with k on some data with the result that > 123
is an upper bound on the fraction of margin errors .
is a lower bound on the fraction of svs .
suppose the data ( see equation 123 ) were generated i . i . d .
from a distribution p . x; y / d p . x / p . yjx / such that neither p . x; y d 123 / nor p . x; y d 123 / contains any discrete component .
suppose , moreover , that the kernel is an - alytic and non - constant .
with probability 123 , asymptotically , equals both the fraction of svs and the fraction of errors .
ad ( i ) .
by the kkt conditions , > 123 implies d 123
hence , inequal - ity 123 becomes an equality ( cf .
equations 123 ) .
thus , at most a fraction of all examples can have i d 123= .
all examples with i > 123 do satisfy i d 123= ( if not , i could grow further to reduce i ) .
ad ( ii ) .
svs can contribute at most 123= to the left - hand side of 123; hence
there must be at least of them .
ad ( iii ) .
it follows from the condition on p . x; y / that apart from some set of measure zero ( arising from possible singular components ) , the two class distributions are absolutely continuous and can be written as inte - grals over distribution functions .
because the kernel is analytic and non - constant , it cannot be constant in any open set; otherwise it would be con - stant everywhere .
therefore , functions f constituting the argument of the sgn in the sv decision function ( see equation 123 ) essentially functions in the class of sv regression functions ) transform the distribution over x into distributions such that for all f , and all t 123 r , lim ( cid : 123 ) ! 123 p . j f . x / c tj < ( cid : 123 ) / d 123
at the same time , we know that the class of these func - tions has well - behaved covering numbers; hence we get uniform conver - jp . j f . x / c tj < ( cid : 123 ) / op . j f . x / c tj < ( cid : 123 ) / j con - gence : for all ( cid : 123 ) > 123 , supf verges to zero in probability , where op is the sample - based estimate of p ( that is , the proportion of points that satisfy j f . x / c tj < ( cid : 123 ) ) .
but then for op . j f . x / c tj < ( cid : 123 ) / > / d 123
hence , all > 123 , lim ( cid : 123 ) ! 123 lim ! 123 p . supf op . j f . x / c tj d 123 / converges to zero in probability .
using t d thus shows that almost surely the fraction of points exactly on the mar - gin tends to zero; hence the fraction of svs equals that of margin errors .
new support vector algorithms
combining ( i ) and ( ii ) shows that both fractions converge almost surely
moreover , since equation 123 means that the sums over the coefcients of positive and negative svs respectively are equal , we conclude that proposi - tion 123 actually holds for both classes separately , with =123
( note that by the same argument , the number of svs at the two sides of the margin asymp -
a connection to standard sv classication , and a somewhat surprising interpretation of the regularization parameter c , is described by the follow -
with c set a priori to 123= , leads to the same decision function .
if - sv classication leads to > 123 , then c - sv classication ,
if one minimizes the function 123 and then xes to minimize only over the remaining variables , nothing will change .
hence the obtained solution w123; b123; 123 minimizes the function 123 for c d 123 , subject to the con - straint 123 .
to recover the constraint 123 , we rescale to the set of variables 123 d = .
this leaves us , up to a constant scaling factor 123 d w=; b 123 , with the objective function 123 using c d 123= .
123 d b=;
as in the case of regression estimation ( see proposition 123 ) , linearity of the target function in the slack variables . / leads to outlier resistance of the estimator in pattern recognition .
the exact statement , however , differs from the one in regression in two respects .
first , the perturbation of the point is carried out in feature space .
what it precisely corresponds to in input space therefore depends on the specic kernel chosen .
second , instead of referring to points outside the " - tube , it refers to margin error pointspoints that are misclassied or fall into the margin .
below , we use the shorthand zi for
proposition 123 ( resistance of sv classication ) .
suppose w can be expressed in terms of the svs that are not at bound , that is ,
123d 123 only if i 123 . 123; 123= / ( where the i are the coefcients of the dual solution ) .
then local movements of any margin error zm parallel to w do not change the hyperplane .
since the slack variable of zm satises m > 123 , the kkt conditions ( e . g . , bertsekas , 123 ) imply m d 123= .
if is sufciently small , then trans - m : d zm c w results in a slack that is still nonzero , forming the point into z
scholkopf , a .
smola , r .
williamson , and p .
bartlett d 123= d m .
updating the m and keeping that is , 123 all other primal variables unchanged , we obtain a modied set of primal variables that is still feasible .
> 123; hence we have 123
we next show how to obtain a corresponding set of feasible dual vari -
to keep w unchanged , we need to satisfy
iyizi c mymz
condition for this to hold is that for all i 123d m ,
d zm c w and equation 123 , we note that a sufcient
d i ( cid : 123 ) iyimym :
since by assumption ( cid : 123 ) i is nonzero only if i 123 . 123; 123= / , 123 i will be in . 123; 123= / if i is , provided is sufciently small , and it will equal 123= if i does .
in both , and the kkt conditions are still cases , we end up with a feasible solution satised .
thus ( bertsekas , 123 ) , . w; b / are still the hyperplane parameters of the solution .
the sv expansion of the solution , w dp
note that the assumption ( 123 ) is not as restrictive as it may seem .
although iyizi , often contains many mul - tipliers i that are at bound , it is nevertheless conceivable that , especially when discarding the requirement that the coefcients be bounded , we can obtain an expansion ( see equation 123 ) in terms of a subset of the original vectors .
for instance , if we have a 123d problem that we solve directly in in - put space , with k . x; y / d . x y / , then it already sufces to have two linearly independent svs that are not at bound in order to express w .
this holds for any overlap of the two classeseven if there are many svs at the upper
for the selection of c , several methods have been proposed that could probably be adapted for ( scholkopf , 123; shawe - taylor & cristianini , 123 ) .
in practice , most researchers have so far used cross validation .
clearly , this could be done also for - svc .
nevertheless , we shall propose a method that takes into account specic properties of - svc .
the parameter lets us control the number of margin errors , the crucial quantity in a class of bounds on the generalization error of classiers using covering numbers to measure the classier capacity .
we can use this con - nection to give a generalization error bound for - svc in terms of .
there are a number of complications in doing this the best possible way , and so here we will indicate the simplest one .
it is based on the following result :
proposition 123 ( bartlett , 123 ) .
suppose > 123; 123 < < 123 123 , p is a probability distribution on x f123; 123g from which the training set , equation 123 , is drawn .
then with probability at least 123 for every f in some function class f , the
new support vector algorithms
probability of error of the classication function f test set is bounded according to
lnn . f ; l123; =123 / c ln . 123= /
d sgn f on an independent
emp ( f ) c
where n . f ; l123; / d supxdx123; : : : ;x n . fjx; l123; / , fjx d f .
f . x123 / ; : : : ; f . x / / : f 123 fg , n . fx; l123; / is the - covering number of fx with respect to l123 , the usual l123 metric on a set of vectors .
to obtain the generalization bound for - svc , we simply substitute the emp ( f ) ( proposition 123 , i ) and some estimate of the covering numbers in terms of the margin .
the best available bounds are stated in terms of the functional inverse of n , hence the slightly complicated expres - sions in the following .
proposition 123 ( williamson et al . , 123 ) .
denote br the ball of radius r around the origin in some hilbert space f .
then the covering number n of the class of
f d fx 123 ! . w x / : kwk 123; x 123 brg
at scale satises
n . f ; l123; / inf
where c < 123 is a constant .
this is a consequence of a fundamental theorem due to maurey .
for 123 one thus obtains
n . f ; l123; / c123r123
to apply these results to - svc , we rescale w to length 123 , thus obtaining a margin =kwk ( cf .
equation 123 ) .
moreover , we have to combine propo - sitions 123 and 123
using =123 instead of in the latter yields the following proposition 123
suppose - svc is used with a kernel of the form k . x; y / d k . kx yk / with k . 123 / d 123
then all the data points 123xi / in feature space live in a ball of radius 123 centered at the origin .
consequently with probability at least 123 d sgn f , over the training set ( see equation 123 ) , the - svc decision function f
scholkopf , a .
smola , r .
williamson , and p .
bartlett
iyik . x; xi / ( cf .
equation 123 ) , has a probability of test error
with f . x / d p
bounded according to
emp ( f ) c
. 123 / 123 c ln . 123= /
. 123 / 123 c ln . 123= / notice that in general , kwk is a vector in feature space .
note that the set of functions in the proposition differs from support vector decision functions ( see equation 123 ) in that it comes without the cb term .
this leads to a minor modication ( for details , see williamson et al . ,
better bounds can be obtained by estimating the radius or even opti - mizing the choice of the center of the ball ( cf .
the procedure described by scholkopf et al .
123; burges , 123 ) .
however , in order to get a theorem of the above form in that case , a more complex argument is necessary ( see shawe - taylor , bartlet , williamson , & anthony , 123 , sec .
vi for an indica -
we conclude this section by noting that a straightforward extension of the - svc algorithm is to include parametric models k . x / for the margin , and qq . xi / instead of in the constraint ( see equation 123 ) in thus to use complete analogy to the regression case discussed in section 123
123 regression estimation .
in the experiments , we used the optimizer loqo . 123 this has the serendipitous advantage that the primal variables b and " can be recovered as the dual variables of the wolfe dual ( see equa - tion 123 ) ( i . e . , the double dual variables ) fed into the optimizer .
123 . 123 toy examples .
the rst task was to estimate a noisy sinc function , given examples . xi; yi / , with xi drawn uniformly from ( 123; 123 ) , and yi d sin . xi / = . xi / c ( cid : 123 ) i , where the ( cid : 123 ) i were drawn from a gaussian with zero mean and variance ( cid : 123 ) 123
unless stated otherwise , we used the radial basis 123j123 / , d 123; c d 123; d 123 : 123 , function ( rbf ) kernel k . x; x and ( cid : 123 ) d 123 : 123
whenever standard deviation error bars are given , the results were obtained from 123 trials .
finally , the risk ( or test error ) of a regression estimate f was computed with respect to the sinc function without noise , j f . x / sin . x / = . x / j dx .
results are given in table 123 and figures 123
123 / d exp . jx x
123 available online at http : / / www . princeton . edu / rvdb / .
new support vector algorithms
figure 123 : - sv regression with d 123 : 123 ( top ) and d 123 : 123 ( bottom ) .
the larger allows more points to lie outside the tube ( see section 123 ) .
the algorithm auto - matically adjusts " to 123 ( top ) and 123 ( bottom ) .
shown are the sinc function ( dotted ) , the regression f , and the tube f " .
scholkopf , a .
smola , r .
williamson , and p .
bartlett
figure 123 : - sv regression on data with noise ( cid : 123 ) d 123 ( top ) and ( cid : 123 ) d 123; ( bottom ) .
in both cases , d 123 : 123
the tube width automatically adjusts to the noise ( top : " d 123; bottom : " d 123 : 123 ) .
new support vector algorithms
figure 123 : " - sv regression ( vapnik , 123 ) on data with noise ( cid : 123 ) d 123 ( top ) and ( cid : 123 ) d 123 ( bottom ) .
in both cases , " d 123 : 123
this choice , which has to be specied a priori , is ideal for neither case .
in the upper gure , the regression estimate is biased; in the lower gure , " does not match the external noise ( smola , murata , scholkopf , & m uller , 123 ) .
scholkopf , a .
smola , r .
williamson , and p .
bartlett
figure 123 : - svr for different values of the error constant .
notice how " de - creases when more errors are allowed ( large ) , and that over a large range of , the test error ( risk ) is insensitive toward changes in .
new support vector algorithms
figure 123 : - svr for different values of the noise ( cid : 123 ) .
the tube radius " increases linearly with ( cid : 123 ) ( largely due to the fact that both " and the . / enter the cost function linearly ) .
due to the automatic adaptation of " , the number of svs and points outside the tube ( errors ) are , except for the noise - free case ( cid : 123 ) d 123 , largely independent of ( cid : 123 ) .
scholkopf , a .
smola , r .
williamson , and p .
bartlett
figure 123 : - svr for different values of the constant c .
( top ) " decreases when the regularization is decreased ( large c ) .
only very little , if any , overtting occurs .
( bottom ) upper bounds the fraction of errors , and lower bounds the fraction of svs ( cf .
proposition 123 ) .
the bound gets looser as c increases; this corresponds to a smaller number of examples relative to c ( cf .
table 123 ) .
new support vector algorithms
table 123 : asymptotic behavior of the fraction of errors and svs .
fraction of errors
fraction of svs
notes : the " found by - sv regression is largely independent of the sample size .
the frac - tion of svs and the fraction of errors approach d 123 : 123 from above and below , respectively , as the number of training examples increases ( cf .
proposition 123 ) .
figure 123 gives an illustration of how one can make use of parametric insensitivity models as proposed in section 123
using the proper model , the estimate gets much better .
in the parametric case , we used d 123 : 123 and . x / dp . x / d 123=123 , corresponds to our . x / d sin123 . 123=123 / x / , which , due to standard choice d 123 : 123 in - svr ( cf .
proposition 123 ) .
although this relies on the assumption that the svs are uniformly distributed , the experimental ndings are consistent with the asymptotics predicted theoretically : for d 123 , we got 123 : 123 and 123 : 123 for the fraction of svs and errors , respectively .
123 . 123 boston housing benchmark .
empirical studies using " - svr have reported excellent performance on the widely used boston housing regres - sion benchmark set ( stitson et al . , 123 ) .
due to proposition 123 , the only difference between - svr and standard " - svr lies in the fact that different parameters , " versus , have to be specied a priori .
accordingly , the goal of the following experiment was not to show that - svr is better than " - svr , but that is a useful parameter to select .
consequently , we are interested only in and " , and hence kept the remaining parameters xed .
we adjusted c and the width 123 ( cid : 123 ) 123 in k . x; y / d exp . kx yk123= . 123 ( cid : 123 ) 123 / / as in scholkopf et al .
( 123 ) .
we used 123 ( cid : 123 ) 123 d 123 : 123 n , where n d 123 is the input dimensionality , and c= d 123 123 ( i . e . , the original value of 123 was corrected since in the present case , the maximal y - value is 123 rather than 123 ) .
we performed 123 runs , where each time the overall set of 123 examples was randomly split into a training set of d 123 examples and a test set of 123 examples ( cf .
stitson et al . , 123 ) .
table 123 shows that over a wide range of ( note that only 123 123 makes sense ) , we obtained performances that are close to the best performances that can be achieved by selecting " a priori by looking at the test set .
finally , although we did not use validation techniques to select the optimal values for c and 123 ( cid : 123 ) 123 , the performances are state of the art ( stitson et al . , 123 , report an mse of 123 : 123 for " - svr using anova kernels , and 123 : 123 for bagging regression trees ) .
table 123 , moreover , shows that in this real - world application , can be used to control the fraction of svs / errors .
scholkopf , a .
smola , r .
williamson , and p .
bartlett
table 123 : results for the boston housing benchmark ( top : - svr; bottom : " - svr ) .
note : mse : mean squared errors; std : standard deviations thereof ( 123 trials ) ; errors : fraction of training points outside the tube; svs : fraction of training points that are svs .
123 classication .
as in the regression case , the difference between c - svc and - svc lies in the fact that we have to select a different parameter a priori .
if we are able to do this well , we obtain identical performances .
in other words , - svc could be used to reproduce the excellent results ob - tained on various data sets using c - svc ( for an overview; see scholkopf , burges , & smola , 123 ) .
this would certainly be a worthwhile project; how - ever , we restrict ourselves here to showing some toy examples illustrating the inuence of ( see figure 123 ) .
the corresponding fractions of svs and margin errors are listed in table 123
we have presented a new class of sv algorithms , which are parameterized by a quantity that lets one control the number of svs and errors .
we de - scribed - svr , a new regression algorithm that has been shown to be rather
123 / d exp . jx x
figure 123 : facing page .
- svr for different values of the gaussian kernel width 123s123 , 123j123= . 123s123 / / .
using a kernel that is too wide results in using k . x; x undertting; moreover , since the tube becomes too rigid as 123s123 gets larger than 123 , the " needed to accomodate a fraction . 123 / of the points , increases signif - icantly .
in the bottom gure , it can again be seen that the speed of the uniform convergence responsible for the asymptotic statement given in proposition 123 depends on the capacity of the underlying model .
increasing the kernel width leads to smaller covering numbers ( williamson et al . , 123 ) and therefore faster
new support vector algorithms
useful in practice .
we gave theoretical results concerning the meaning and the choice of the parameter .
moreover , we have applied the idea under - lying - sv regression to develop a - sv classication algorithm .
just like its regression counterpart , the algorithm is interesting from both a prac -
scholkopf , a .
smola , r .
williamson , and p .
bartlett
figure 123 : toy example , using prior knowledge about an x - dependence of the noise .
additive noise ( ( cid : 123 ) d 123 ) was multiplied by the function sin123 . 123=123 / x / .
( top ) the same function was used as as a parametric insensitivity tube ( sec - tion 123 ) .
( bottom ) - svr with standard tube .
new support vector algorithms
table 123 : fractions of errors and svs , along with the margins of class separation , for the toy example depicted in figure 123
fraction of errors fraction of svs note : upper bounds the fraction of errors and lower bounds the fraction of svs , and that increasing , i . e .
allowing more errors , increases the margin .
tical and a theoretical point of view .
controlling the number of svs has consequences for ( 123 ) run - time complexity , since the evaluation time of the estimated function scales linearly with the number of svs ( burges , 123 ) ; ( 123 ) training time , e . g . , when using a chunking algorithm ( vapnik , 123 ) whose complexity increases with the number of svs; ( 123 ) possible data com - pression applications characterizes the compression ratio : it sufces to train the algorithm only on the svs , leading to the same solution ( scholkopf et al . , 123 ) ; and ( 123 ) generalization error bounds : the algorithm directly op - timizes a quantity using which one can give generalization bounds .
these , in turn , could be used to perform structural risk minimization over .
more - over , asymptotically , directly controls the number of support vectors , and the latter can be used to give a leave - one - out generalization bound ( vapnik ,
figure 123 : toy problem ( task : separate circles from disks ) solved using - sv classication , using parameter values ranging from d 123 : 123 ( top left ) to d 123 : 123 ( bottom right ) .
the larger we select , the more points are allowed to lie inside the margin ( depicted by dotted lines ) .
as a kernel , we used the gaussian k . x; y / d exp . kx yk123 / .
scholkopf , a .
smola , r .
williamson , and p .
bartlett
in both the regression and the pattern recognition case , the introduction of has enabled us to dispose of another parameter .
in the regression case , this was the accuracy parameter " ; in pattern recognition , it was the reg - ularization constant c .
whether we could have as well abolished c in the regression case is an open problem .
note that the algorithms are not fundamentally different from previous sv algorithms; in fact , we showed that for certain parameter settings , the results coincide .
nevertheless , we believe there are practical applications where it is more convenient to specify a fraction of points that is allowed to become errors , rather than quantities that are either hard to adjust a priori ( such as the accuracy " ) or do not have an intuitive interpretation ( such as c ) .
on the other hand , desirable properties of previous sv algorithms , including the formulation as a denite quadratic program , and the sparse sv representation of the solution , are retained .
we are optimistic that in many applications , the new algorithms will prove to be quite robust .
among these should be the reduced set algorithm of osuna and girosi ( 123 ) , which approximates the sv pattern recognition decision surface by " - svr .
here , - svr should give a direct handle on the desired speed - up .
future work includes the experimental test of the asymptotic predictions of section 123 and an experimental evaluation of - sv classication on real - world problems .
moreover , the formulation of efcient chunking algorithms for the - sv case should be studied ( cf .
platt , 123 ) .
finally , the additional freedom to use parametric error models has not been exploited yet .
we expect that this new capability of the algorithms could be very useful in situations where the noise is heteroscedastic , such as in many problems of nancial data analysis , and general time - series analysis applications ( m uller et al . , 123; mattera & haykin , 123 ) .
if a priori knowledge about the noise is available , it can be incorporated into an error model ; if not , we can try to estimate the model directly from the data , for example , by using a variance estimator ( e . g . , seifert , gasser , & wolf , 123 ) or quantile estimator ( section 123 ) .
this work was supported in part by grants of the australian research coun - cil and the dfg ( ja 123 / 123 - 123 and ja 123 / 123 - 123 ) .
thanks to s .
ben - david , a .
elis - seeff , t .
jaakkola , k .
m uller , j .
platt , r .
von sachs , and v .
vapnik for discus - sions and to l .
almeida for pointing us to whites work .
jason weston has independently performed experiments using a sum inequality constraint on the lagrange multipliers , but declined an offer of coauthorship .
