abstract .
we introduce a family of kernels on graphs based on the notion of regularization operators .
this generalizes in a natural way the notion of regularization and greens functions , as commonly used for real valued functions , to graphs .
it turns out that diusion kernels can be found as a special case of our reasoning .
we show that the class of positive , monotonically decreasing functions on the unit interval leads to kernels and corresponding regularization operators .
there has recently been a surge of interest in learning algorithms that operate on input spaces x other than rn , specically , discrete input spaces , such as strings , graphs , trees , automata etc . .
since kernel - based algorithms , such as support vector machines , gaussian processes , kernel pca , etc .
capture the structure of x via the kernel k : x x 123 r , as long as we can dene an appropriate kernel on our discrete input space , these algorithms can be imported wholesale , together with their error analysis , theoretical guarantees and empirical success .
one of the most general representations of discrete metric spaces are graphs .
even if all we know about our input space are local pairwise similarities between points xi , xj x , distances ( e . g shortest path length ) on the graph induced by these similarities can give a useful , more global , sense of similarity between objects .
in their work on diusion kernels , kondor and laerty ( 123 ) gave a specic construction for a kernel capturing this structure .
belkin and niyogi ( 123 ) proposed an essentially equivalent construction in the context of approx - imating data lying on surfaces in a high dimensional embedding space , and in the context of leveraging information from unlabeled data .
in this paper we put these earlier results into the more principled framework of regularization theory .
we propose a family of regularization operators ( equiv - alently , kernels ) on graphs that include diusion kernels as a special case , and show that this family encompasses all possible regularization operators invariant under permutations of the vertices in a particular sense .
alexander smola and risi kondor
outline of the paper : section 123 introduces the concept of the graph laplacian and relates it to the laplace operator on real valued functions .
next we dene an extended class of regularization operators and show why they have to be es - sentially a function of the laplacian .
an analogy to real valued greens functions is established in section 123 , and ecient methods for computing such functions are presented in section 123
we conclude with a discussion .
123 laplace operators
123 w d 123
let d be an n n diagonal matrix with dii =p
the laplacian of g 123 .
the following two theorems are well known results from spectral
an undirected unweighted graph g consists of a set of vertices v numbered 123 to n , and a set of edges e ( i . e . , pairs ( i , j ) where i , j v and ( i , j ) e ( j , i ) e ) .
we will sometimes write i j to denote that i and j are neighbors , i . e .
( i , j ) e .
the adjacency matrix of g is an n n real matrix w , with wij =123 if i j , and 123 otherwise ( by construction , w is symmetric and its diagonal entries are zero ) .
these denitions and most of the following theory can trivially be extended to weighted graphs by allowing wij ( 123 , ) .
is dened as l : = dw and the normalized laplacian is l : = d 123 graph theory ( chung - graham , 123 ) : theorem 123 ( spectrum of l ) .
l is a symmetric , positive semidenite matrix , and its eigenvalues 123 , 123 , .
, n satisfy 123 i 123
furthermore , the number of eigenvalues equal to zero equals to the number of disjoint components in g .
the bound on the spectrum follows directly from gerschgorins theorem .
theorem 123 ( l and l for regular graphs ) .
now let g be a regular graph of degree d , that is , a graph in which every vertex has exactly d neighbors .
then l = d iw and l = i 123 d l .
finally , w , l , l share the same eigenvectors ( vi ) , where vi = 123 l and l can be regarded as linear operators on functions f : v 123 r , or , equiv - alently , on vectors f = ( f123 , f123 , .
, fn ) > .
we could equally well have dened l
i w vi = ( d i ) 123lvi = ( 123 d123i ) 123 lvi for all i .
d w = 123
123 ld 123
( fi fj ) 123 for all f rn ,
hf , lfi = f
lf = 123
which readily generalizes to graphs with a countably innite number of vertices .
the laplacian derives its name from its analogy with the familiar laplacian operator = 123 on continuous spaces .
regarding ( 123 ) as inducing a semi - norm k f kl = hf , lfi on rn , the analogous expression for dened on a compact space is k f k = hf , fi =
( f ) ( f ) d .
f ( f ) d =
both ( 123 ) and ( 123 ) quantify how much f and f vary locally , or how smooth they are over their respective domains .
dierence discretization of on a regular lattice :
f ( x + 123
f ( x + ei ) + f ( x ei ) 123f ( x )
more explicitly , when = rm , up to a constant , l is exactly the nite
kernels and regularization on graphs
( fx123 , . . . , xi+123 , . . . , xm + fx123 , . . . , xi123 , . . . , xm 123fx123 , . . . , xm ) = 123
123 ( lf ) x123 , . . . , xm ,
where e123 , e123 , .
, em is an orthogonal basis for rm normalized to k ei k = , the vertices of the lattice are at x = x123e123 + .
+ xmem with integer valued coordinates xin , and f x123 , x123 , . . . , xm = f ( x ) .
moreover , both the continuous and the dis - crete laplacians are canonical operators on their respective domains , in the sense that they are invariant under certain natural transformations of the underlying space , and in this they are essentially unique .
regular grid in two dimensions
the laplace operator is the unique self - adjoint linear second order dier - ential operator invariant under transformations of the coordinate system under the action of the special orthogonal group som , i . e .
invariant under rotations .
this well known result can be seen by using schurs lemma and the fact that som is irreducible on rm .
we now show a similar result for l .
here the permutation group plays a similar role to som .
we need some additional denitions : denote by sn the group of permutations on ( 123 , 123 , .
, n ) with sn being a specic permutation taking i ( 123 , 123 , .
n ) to ( i ) .
the so - called dening representation of sn consists of n n matrices , such that ( ) i , ( i ) =123 and all other entries of are zero .
theorem 123 ( permutation invariant linear functions on graphs ) .
let l be an n n symmetric real matrix , linearly related to the n n adjacency matrix w , i . e .
l = t ( w ) for some linear operator l in a way invariant to permutations of vertices in the sense that
t ( w ) = t ( cid : 123 ) >
for any sn .
then l is related to w by a linear combination of the follow - ing three operations : identity; row / column sums; overall sum; row / column sum restricted to the diagonal of l; overall sum restricted to the diagonal of w .
li123i123 = t ( w ) i123i123 : =
t i123i123i123i123 wi123i123
with t rn123
( 123 ) then implies t ( i123 ) ( i123 ) ( i123 ) ( i123 ) = ti123i123i123i123 for any sn .
alexander smola and risi kondor
the indices of t can be partitioned by the equality relation on their values , e . g .
( 123 , 123 , 123 , 123 ) is of the partition type ( 123 123| 123| 123 ) , since i123 = i123 , but i123= i123 , i123= i123 and i123 123= i123
the key observation is that under the action of the permutation group , elements of t with a given index partition structure are taken to elements with the same index partition structure , e . g .
if i123 = i123 then ( i123 ) = ( i123 ) and if i123 123= i123 , then ( i123 ) 123= ( i123 ) .
furthermore , an element with a given index index partition structure can be mapped to any other element of t with the same index partition structure by a suitable choice of .
hence , a necessary and sucient condition for ( 123 ) is that all elements of t of a given index partition structure be equal .
therefore , t must be a linear combination of the following tensors ( i . e .
multilinear forms ) :
ai123i123i123i123 = 123 i123i123i123i123 = i123i123 i123i123i123i123 = i123i123 i123i123i123i123 = i123i123i123i123 i123i123i123i123 = i123i123i123i123 i123i123i123i123 = i123i123i123i123 i123i123i123i123 = i123i123i123i123i123i123 .
i123i123i123i123 = i123i123 i123i123i123i123 = i123i123
i123i123i123i123 = i123i123 i123i123i123i123 = i123i123 i123i123i123i123 = i123i123i123i123 i123i123i123i123 = i123i123i123i123 i123i123i123i123 = i123i123i123i123 d ( 123 , 123 ) ( 123 , 123 )
i123i123i123i123 = i123i123i123i123
the tensor a puts the overall sum in each element of l , while b ( 123 , 123 ) returns the the same restricted to the diagonal of l .
since w has vanishing diagonal , b ( 123 , 123 ) , c ( 123 , 123 , 123 ) , c ( 123 , 123 , 123 ) , d ( 123 , 123 ) ( 123 , 123 ) and e ( 123 , 123 , 123 , 123 )
produce zero .
without loss of generality we can therefore ignore them .
by symmetry of w , the pairs ( b ( 123 , 123 ) , b ( 123 , 123 ) ) , ( b ( 123 , 123 ) , b ( 123 , 123 ) ) , ( c ( 123 , 123 , 123 ) , c ( 123 , 123 , 123 ) ) have the same eect on w , hence we can set the coecient of the second member of each to zero .
furthermore , to enforce symmetry on l , the coecient of b ( 123 , 123 ) and b ( 123 , 123 ) must be the same ( without loss of generality 123 ) and this will give the
row / column sum matrix ( p give the row / column sum restricted to the diagonal : ij ( ( p
similarly , c ( 123 , 123 , 123 ) and c ( 123 , 123 , 123 ) must have the same coecient and this will finally , by symmetry of w , d ( 123 , 123 ) ( 123 , 123 ) and d ( 123 , 123 ) ( 123 , 123 ) are both equivalent to
k wik ) + ( p
k wik ) + ( p
the identity map .
the various row / column sum and overall sum operations are uninteresting from a graph theory point of view , since they do not heed to the topology of the graph .
imposing the conditions that each row and column in l must sum to zero , we recover the graph laplacian .
hence , up to a constant factor and trivial additive components , the graph laplacian ( or the normalized graph laplacian if we wish to rescale by the number of edges per vertex ) is the only invariant dierential operator for given w ( or its normalized counterpart w ) .
unless stated otherwise , all results below hold for both l and l ( albeit with a dierent spectrum ) and we will , in the following , focus on l due to the fact that its spectrum is contained in ( 123 , 123 ) .
kernels and regularization on graphs
the fact that l induces a semi - norm on f which penalizes the changes between adjacent vertices , as described in ( 123 ) , indicates that it may serve as a tool to design regularization operators .
123 regularization via the laplace operator we begin with a brief overview of translation invariant regularization operators on continuous spaces and show how they can be interpreted as powers of .
this will allow us to repeat the development almost verbatim with l ( or l ) instead .
some of the most successful regularization functionals on rn , leading to
kernels such as the gaussian rbf , can be written as ( smola et al . , 123 )
hf , p fi : =
| f ( ) |123 r ( kk123 ) d = hf , r ( ) fi .
here f l123 ( rn ) , f ( ) denotes the fourier transform of f , r ( kk123 ) is a function penalizing frequency components | f ( ) | of f , typically increasing in kk123 , and nally , r ( ) is the extension of r to operators simply by applying r to the spectrum of ( dunford and schwartz , 123 )
hf , r ( ) f123i =x
hf , ii r ( i ) hi , f123i
where ( ( i , i ) ) is the eigensystem of .
the last equality in ( 123 ) holds because applications of become multiplications by kk123 in fourier space .
kernels are obtained by solving the self - consistency condition ( smola et al . , 123 )
hk ( x , ) , p k ( x123 , ) i = k ( x , x123 ) .
one can show that k ( x , x123 ) = ( x x123 ) , where is equal to the inverse fourier transform of r123 ( kk123 ) .
several r functions have been known to yield good results .
the two most popular are given below :
gaussian rbf exp
laplacian rbf 123 + 123kk123 in summary , regularization according to ( 123 ) is carried out by penalizing f ( ) by a function of the laplace operator .
for many results in regularization theory one requires r ( kk123 ) for kk123 .
123 regularization via the graph laplacian in complete analogy to ( 123 ) , we dene a class of regularization functionals on
hf , p fi : = hf , r ( l ) fi .
alexander smola and risi kondor
regularization function r ( ) .
from left to right : regularized laplacian ( 123 = 123 ) , diusion process ( 123 = 123 ) , one - step random walk ( a = 123 ) , 123 - step random walk ( a = 123 ) ,
here r ( l ) is understood as applying the scalar valued function r ( ) to the eigen - values of l , that is ,
where ( ( i , vi ) ) constitute the eigensystem of l .
the normalized graph lapla - cian l is preferable to l , since ls spectrum is contained in ( 123 , 123 ) .
the obvious goal is to gain insight into what functions are appropriate choices for r .
from ( 123 ) we infer that vi with large i correspond to rather uneven functions on the graph g .
consequently , they should be penalized more strongly than vi with small i .
hence r ( ) should be monotonically increasing in .
requiring that r ( l ) ( cid : 123 ) 123 imposes the constraint r ( ) 123 for all ( 123 , 123 ) .
finally , we can limit ourselves to r ( ) expressible as power series , since the
latter are dense in the space of c123 functions on bounded domains .
in section 123 we will present additional motivation for the choice of r ( ) in the context of spectral graph theory and segmentation .
as we shall see , the following functions are of particular interest :
r ( ) = 123 + 123
r ( ) = exp ( cid : 123 ) 123 / 123 ( cid : 123 )
r ( ) = ( ai ) 123 with a 123 r ( ) = ( ai ) p with a 123 r ( ) = ( cos / 123 ) 123
( one - step random walk ) ( p - step random walk )
figure 123 shows the regularization behavior for the functions ( 123 ) - ( 123 ) .
the introduction of a regularization matrix p = r ( l ) allows us to dene a hilbert space h on rm via hf , fih : = hf , p fi .
we now show that h is a reproducing kernel hilbert space .
kernels and regularization on graphs
k ( i , j ) = ( cid : 123 ) p 123 ( cid : 123 )
theorem 123
denote by p rmm a ( positive semidenite ) regularization ma - trix and denote by h the image of rm under p .
then h with dot product hf , fih : = hf , p fi is a reproducing kernel hilbert space and its kernel is ij , where p 123 denotes the pseudo - inverse if p is not invertible .
proof since p is a positive semidenite matrix , we clearly have a hilbert space on p rm .
to show the reproducing property we need to prove that
note that k ( i , j ) can take on at most m123 dierent values ( since i , j ( 123 : m ) ) .
in matrix notation ( 123 ) means that for all f h
f ( i ) = hf , k ( i , ) ih .
f ( i ) = f>p ki , : for all i f> = f>p k .
the latter holds if k = p 123 and f p rm , which proves the claim .
in other words , k is the greens function of p , just as in the continuous case .
the notion of greens functions on graphs was only recently introduced by chung - graham and yau ( 123 ) for l .
the above theorem extended this idea to arbitrary regularization operators r ( l ) .
corollary 123
denote by p = r ( l ) a regularization matrix , then the correspond - ing kernel is given by k = r123 ( l ) , where we take the pseudo - inverse wherever necessary .
more specically , if ( ( vi , i ) ) constitute the eigensystem of l , we have
i where we dene 123 123
123 examples of kernels
by virtue of corollary 123 we only need to take ( 123 ) - ( 123 ) and plug the denition of r ( ) into ( 123 ) to obtain formulae for computing k .
this yields the following
k = ( i + 123 l ) 123 k = exp ( 123 / 123l ) k = ( ai l ) p with a 123 k = cos l / 123
( p - step random walk )
equation ( 123 ) corresponds to the diusion kernel proposed by kondor and laf - ferty ( 123 ) , for which k ( x , x123 ) can be visualized as the quantity of some sub - stance that would accumulate at vertex x123 after a given amount of time if we injected the substance at vertex x and let it diuse through the graph along the edges .
note that this involves matrix exponentiation dened via the limit k = exp ( b ) = limn ( i+b / n ) n as opposed to component - wise exponentiation ki , j = exp ( bi , j ) .
alexander smola and risi kondor
the rst 123 eigenvectors of the normalized graph laplacian corresponding to the graph drawn above .
each line attached to a vertex is proportional to the value of the corresponding eigenvector at the vertex .
positive values ( red ) point up and negative values ( blue ) point down .
note that the assignment of values becomes less and less uniform with increasing eigenvalue ( i . e .
from left to right ) .
for ( 123 ) it is typically more ecient to deal with the inverse of k , as it avoids the costly inversion of the sparse matrix l .
such situations arise , e . g . , in gaussian process estimation , where k is the covariance matrix of a stochastic process ( williams , 123 ) .
regarding ( 123 ) , recall that ( ai l ) p = ( ( a123 ) i + w ) p is up to scaling terms equiv - alent to a p - step random walk on the graph with random restarts ( see section a for de - tails ) .
in this sense it is similar to the dif - fusion kernel .
however , the fact that k in - volves only a nite number of products of matrices makes it much more attractive for practical purposes .
in particular , entries in kij can be computed cheaply using the fact that l is a sparse matrix .
a nearest neighbor graph .
finally , the inverse cosine kernel treats lower complexity functions almost equally , with a signicant reduction in the upper end of the spectrum .
figure 123 shows the leading eigenvectors of the graph drawn above and figure 123 provide examples of some of the kernels discussed above .
123 clustering and spectral graph theory we could also have derived r ( l ) directly from spectral graph theory : the eigen - vectors of the graph laplacian correspond to functions partitioning the graph into clusters , see e . g . , ( chung - graham , 123 , shi and malik , 123 ) and the ref - erences therein .
in general , small eigenvalues have associated eigenvectors which vary little between adjacent vertices .
finding the smallest eigenvectors of l can be seen as a real - valued relaxation of the min - cut problem . 123
for instance , the smallest eigenvalue of l is 123 , its corresponding eigenvector 123 123n with 123n : = ( 123 , .
, 123 ) rn .
the second smallest eigenvalue / eigenvector pair , also often referred to as the fiedler - vector , can be used to split the graph
123 only recently , algorithms based on the celebrated semidenite relaxation of the min - cut problem by goemans and williamson ( 123 ) have seen wider use ( torr , 123 ) in segmentation and clustering by use of spectral bundle methods .
kernels and regularization on graphs
top : regularized graph laplacian; middle : diusion kernel with = 123 , bottom : 123 - step random walk kernel .
each gure displays kij for xed i .
the value kij at vertex i is denoted by a bold line .
note that only adjacent vertices to i bear signicant value .
into two distinct parts ( weiss , 123 , shi and malik , 123 ) , and further eigenvec - tors with larger eigenvalues have been used for more nely - grained partitions of the graph .
see figure 123 for an example .
such a decomposition into functions of increasing complexity has very de - sirable properties : if we want to perform estimation on the graph , we will wish to bias the estimate towards functions which vary little over large homogeneous portions 123
consequently , we have the following interpretation of hf , fih .
as - i ivi , where ( ( vi , i ) ) is the eigensystem of l .
then we can rewrite hf , fih to yield
sume that f =p
hf , r ( l ) fi =
this means that the components of f which vary a lot over coherent clusters in the graph are penalized more strongly , whereas the portions of f , which are essentially constant over clusters , are preferred .
this is exactly what we want .
123 approximate computation
often it is not necessary to know all values of the kernel ( e . g . , if we only observe instances from a subset of all positions on the graph ) .
there it would be wasteful to compute the full matrix r ( l ) 123 explicitly , since such operations typically scale with o ( n123 ) .
furthermore , for large n it is not desirable to compute k via ( 123 ) , that is , by computing the eigensystem of l and assembling k directly .
123 if we cannot assume a connection between the structure of the graph and the values of the function to be estimated on it , the entire concept of designing kernels on graphs obviously becomes meaningless .
alexander smola and risi kondor
instead , we would like to take advantage of the fact that l is sparse , and con - sequently any operation l has cost at most linear in the number of nonzero ele - ments of l , hence the cost is bounded by o ( |e|+ n ) .
moreover , if d is the largest i=123 ( min ( d+123 , n ) ) i operations : at each step the number of non - zeros in the rhs decreases by at most a factor of d + 123
this means that as long as we can approximate k = r123 ( l ) by i=123 i li , signicant savings are possible .
note that we need not necessarily require a uniformly good approximation and put the main emphasis on the approximation for small .
however , we need to ensure that ( l ) is positive semidenite .
degree of the graph , then computing lpei costs at most | e |pp123 a low order polynomial , say ( l ) : =pn diusion kernel : the fact that the series r123 ( x ) = exp ( x ) =p
has alternating signs shows that the approximation error at r123 ( x ) is bounded ( n +123 ) ! , if we use n terms in the expansion ( from theorem 123 we know that klk 123 ) .
for instance , for = 123 , 123 terms are sucient to obtain an error of the order of 123
variational approximation : in general , if we want to approximate r123 ( ) on ( 123 , 123 ) , we need to solve the l ( ( 123 , 123 ) ) approximation problem
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( 123 , 123 )
clearly , ( 123 ) is equivalent to minimizing sup l k ( l ) r123 ( l ) k , since the matrix norm is determined by the largest eigenvalues , and we can nd l such that the discrepancy between ( ) and r123 ( ) is attained .
variational problems of this form have been studied in the literature , and their solution may provide much better approximations to r123 ( ) than a truncated power series expansion .
123 products of graphs
as we have already pointed out , it is very expensive to compute k for arbitrary r and l .
for special types of graphs and regularization , however , signicant computational savings can be made .
123 factor graphs
the work of this section is a direct extension of results by ellis ( 123 ) and chung - graham and yau ( 123 ) , who study factor graphs to compute inverses of the graph laplacian .
denition 123 ( factor graphs ) .
denote by ( v , e ) and ( v 123 , e123 ) the vertices v and edges e of two graphs , then the factor graph ( vf , ef ) : = ( v , e ) ( v 123 , e123 ) is dened as the graph where ( i , i123 ) vf if i v and i123 v 123; and ( ( i , i123 ) , ( j , j123 ) ) ef if and only if either ( i , j ) e and i123 = j123 or ( i123 , j123 ) e123 and i= j .
kernels and regularization on graphs
for instance , the factor graph of two rings is a torus .
the nice property of factor graphs is that we can compute the eigenvalues of the laplacian on products very easily ( see e . g . , chung - graham and yau ( 123 ) ) : theorem 123 ( eigenvalues of factor graphs ) .
the eigenvalues and eigen - vectors of the normalized laplacian for the factor graph between a regular graph of degree d with eigenvalues ( j ) and a regular graph of degree d123 with eigenvalues
l ) are of the form :
j , l = d ( i , i123 ) = ej
d + d123 j + d123
d + d123 123
i123 , where ej is an eigenvector of l and
and the eigenvectors satisfy ej , l e123l is an eigenvector of l123
this allows us to apply corollary 123 to obtain an expansion of k as
k = ( r ( l ) ) 123 =x
while providing an explicit recipe for the computation of kij without the need to compute the full matrix k , this still requires o ( n123 ) operations per entry , which may be more costly than what we want ( here n is the number of vertices of the factor graph ) .
two methods for computing ( 123 ) become evident at this point : if r has a special structure , we may exploit this to decompose k into the products and sums of terms depending on one of the two graphs alone and pre - compute these expressions beforehand .
secondly , if one of the two terms in the expansion can be computed for a rather general class of values of r ( x ) , we can pre - compute this expansion and only carry out the remainder corresponding to ( 123 ) explicitly .
123 product decomposition of r ( x ) central to our reasoning is the observation that for certain r ( x ) , the term 123 can be expressed in terms of a product and sum of terms depending on a and b only .
we assume that
r ( a + b )
in the following we will show that in such situations the kernels on factor graphs can be computed as an analogous combination of products and sums of kernel functions on the terms constituting the ingredients of the factor graph .
before we do so , we briey check that many r ( x ) indeed satisfy this property .
exp ( ( a + b ) ) = exp ( a ) exp ( b )
( a ( a + b ) ) =
( a ( a + b ) ) p =
( a + b )
= cos a
alexander smola and risi kondor
in a nutshell , we will exploit the fact that for products of graphs the eigenvalues of the joint graph laplacian can be written as the sum of the eigenvalues of the laplacians of the constituent graphs .
this way we can perform computations on n and n separately without the need to take the other part of the the product of graphs into account
km ( i , j ) : =x
d + d123
j and km ( i123 , j123 ) : =x
d + d123
then we have the following composition theorem : theorem 123
denote by ( v , e ) and ( v 123 , e123 ) connected regular graphs of degrees d with m vertices ( and d123 , m123 respectively ) and normalized graph laplacians l , l123
furthermore denote by r ( x ) a rational function with matrix - valued exten - sion r ( x ) .
in this case the kernel k corresponding to the regularization operator r ( l ) on the product graph of ( v , e ) and ( v 123 , e123 ) is given by
k ( ( i , i123 ) , ( j , j123 ) ) =
km ( i , j ) km ( i123 , j123 )
proof plug the expansion of
r ( a+b ) as given by ( 123 ) into ( 123 ) and collect terms .
from ( 123 ) we immediately obtain the corollary ( see kondor and laerty ( 123 ) ) that for diusion processes on factor graphs the kernel on the factor graph is given by the product of kernels on the constituents , that is k ( ( i , i123 ) , ( j , j123 ) ) = k ( i , j ) k123 ( i123 , j123 ) .
the kernels km and km can be computed either by using an analytic solution of the underlying factors of the graph or alternatively they can be computed numerically .
if the total number of kernels kn is small in comparison to the number of possible coordinates this is still computationally benecial .
123 composition theorems if no expansion as in ( 123 ) can be found , we may still be able to compute ker - nels by extending a reasoning from ( ellis , 123 ) .
more specically , the following composition theorem allows us to accelerate the computation in many cases , whenever we can parameterize ( r ( l + i ) ) 123 in an ecient way .
for this pur - pose we introduce two auxiliary functions
d + d123 l + d123 ( i , j ) : = ( l123 + i ) 123 =x d + d123 i
k ( i , j ) : =
( cid : 123 ) dl + d123
d + d123
in some cases k ( i , j ) may be computed in closed form , thus obviating the need to perform expensive matrix inversion , e . g . , in the case where the underlying graph is a chain ( ellis , 123 ) and k = g .
kernels and regularization on graphs
theorem 123
under the assumptions of theorem 123 we have
( j123 , l123 ) d =x
k ( ( j , j123 ) , ( l , l123 ) ) =
where c c is a contour of the c containing the poles of ( v 123 , e123 ) including 123
for practical purposes , the third term of ( 123 ) is more amenable to computation .
proof from ( 123 ) we have
k ( ( j , j123 ) , ( l , l123 ) ) =x
( cid : 123 ) du + d123v
( cid : 123 ) du + d123
d + d123
d + d123
a pole p yields r
here the second equality follows from the fact that the contour integral over p d = 123if ( p ) , and the claim is veried by checking the .
the last equality can be seen from ( 123 ) by splitting
denitions of k and g123 up the summation over u and v .
we have shown that the canonical family of kernels on graphs are of the form of power series in the graph laplacian .
equivalently , such kernels can be char - acterized by a real valued function of the eigenvalues of the laplacian .
special cases include diusion kernels , the regularized laplacian kernel and p - step ran - dom walk kernels .
we have developed the regularization theory of learning on graphs using such kernels and explored methods for eciently computing and approximating the kernel matrix .
acknowledgments this work was supported by a grant of the arc .
the authors thank eleazar eskin , patrick haner , andrew ng , bob williamson and s . v . n .
vishwanathan for helpful comments and suggestions .
a link analysis
rather surprisingly , our approach to regularizing functions on graphs bears re - semblance to algorithms for scoring web pages such as pagerank ( page et al . , 123 ) , hits ( kleinberg , 123 ) , and randomized hits ( zheng et al . , 123 ) .
more specically , the random walks on graphs used in all three algorithms and the stationary distributions arising from them are closely connected with the eigen - system of l and l respectively .
we begin with an analysis of pagerank .
given a set of web pages and links between them we construct a directed graph in such a way that pages correspond
alexander smola and risi kondor
to vertices and edges correspond to links , resulting in the ( nonsymmetric ) matrix w .
next we consider the random walk arising from following each of the links with equal probability in addition to a random restart at an arbitrary vertex with probability .
this means that the probability distribution over states follows the discrete time evolution equation
where d is a diagonal matrix with dii =p ( cid : 123 ) i + ( 123 ) w d123 ( cid : 123 ) will determine the stationary distribution p ( ) , and the
j wij and p is the vector of proba - bilities of being on a certain page .
the pagerank is then determined from the stationary distribution of p .
clearly the largest eigenvalue / eigenvector pair of
p ( t + 123 ) = ( cid : 123 ) i + ( 123 ) w d123 ( cid : 123 ) p ( t )
contribution of the other eigenvectors decays geometrically ( one may conjecture that in practice only few iterations are needed ) .
now consider the same formalism in the context of a 123 - step random walk ( 123 ) : here one computes ai l = ( a 123 ) i + d 123 setting = 123a a yields a matrix with the same spectrum as the linear dierence equation ( 123 ) .
furthermore , for all eigenvectors vi of i + ( 123 ) w d123 we can nd eigenvectors of ai l of the form d 123
rescaling by 123
123 w d 123
the main dierence , however , is that while graphs arising from web pages are directed ( following the direction of the link ) , which leads to asymmetric w , the graphs we studied in this paper are all undirected , leading to symmetric w and l , l .
we can now view the assignment of a certain pagerank to a page , as achieved via the stationary distribution of the random walk , as a means of nding a simple function on the graph of web pages .
in hits ( kleinberg , 123 ) one uses the concept of hubs and authorities to obtain a ranking between web pages .
given the graph g , as represented by w , one seeks to nd the largest eigenvalue of the matrix m : = can be shown to be equivalent to nding singular value decomposition of w ( zheng et al . , 123 ) ( the latter is also used if we wish to perform latent semantic indexing on the matrix w ) .
more specically , with ( vi , i ) being the eigensystem of w w > ( we assume that the eigenvalues are sorted in increasing order ) , one
mj as the weight of page j .
to small perturbations .
more specically , they usepm
this setting was modied by zheng et al .
( 123 ) to accommodate for a larger subspace ( subspace hits ) , which renders the system more robust with respect ij for some mono - tonically increasing function g ( ) to assess the relevance of page j .
the latter , however , is identical to the diagonal entry of g ( w ) .
note the similarity to 123 , where we used an essentially rescaled version of w to determine the complex - ity of the functions under consideration .
more specically , if for regular graphs r ( 123 / d ) we can see that the hits rank assigned to of order d we set g ( ) = pages j is simply the length of the corresponding page in feature space as given by kii .
in other words , pages with a high hits rank correspond to unit vectors which are considered simple with respect to the regularizer induced by the underlying graph .
( cid : 123 ) 123 w
