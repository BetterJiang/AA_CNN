suppose you are given some dataset drawn from an underlying probabil - ity distribution p and you want to estimate a simple subset s of input space such that the probability that a test point drawn from p lies outside of s equals some a priori specied ( cid : 123 ) between and .
we propose a method to approach this problem by trying to estimate a function f which is positive on s and negative on the complement .
the functional form of f is given by a kernel expansion in terms of a poten - tially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space .
we provide a theoretical analysis of the statistical performance of our algorithm .
the algorithm is a natural extension of the support vector algorithm to the case of unlabelled data .
during recent years , a new set of kernel techniques for supervised learning has been devel - oped ( 123 ) .
specically , support vector ( sv ) algorithms for pattern recognition , regression estimation and solution of inverse problems have received considerable attention .
there have been a few attempts to transfer the idea of using kernels to compute inner products in feature spaces to the domain of unsupervised learning .
the problems in that domain are , however , less precisely specied .
generally , they can be characterized as estimating functions of the data which tell you something interesting about the underlying distribu - tions .
for instance , kernel pca can be characterized as computing functions which on the training data produce unit variance outputs while having minimum norm in feature space ( 123 ) .
another kernel - based unsupervised learning technique , regularized principal mani - folds ( 123 ) , computes functions which give a mapping onto a lower - dimensional manifold minimizing a regularized quantization error .
clustering algorithms are further examples of unsupervised learning techniques which can be kernelized ( 123 ) .
an extreme point of view is that unsupervised learning is about estimating densities .
clearly , knowledge of the density of p would then allow us to solve whatever problem can be solved on the basis of the data .
the present work addresses an easier problem : it
proposes an algorithm which computes a binary function which is supposed to capture re - gions in input space where the probability density lives ( its support ) , i . e .
a function such that most of the data will live in the region where the function is nonzero ( 123 ) .
in doing so , it is in line with vapniks principle never to solve a problem which is more general than the one we actually need to solve .
moreover , it is applicable also in cases where the density of the datas distribution is not even well - dened , e . g .
if there are singular components .
part of the motivation for the present work was the paper ( 123 ) .
it turns out that there is a considerable amount of prior work in the statistical literature; for a discussion , cf .
the full version of the present paper ( 123 ) .
we rst introduce terminology and notation conventions .
we consider training data x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) x ( cid : 123 ) where ( cid : 123 ) n is the number of observations , and x is some set .
for simplicity , we think of it as a compact subset of r n .
let ( cid : 123 ) be a feature map x ( cid : 123 ) f , i . e .
a map into a dot product space f such that the dot product in the image of ( cid : 123 ) can be computed by evaluating some simple kernel ( 123 )
k ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
such as the gaussian kernel
indices i and j are understood to range over ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( in compact notation : i ( cid : 123 ) j ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ) .
bold face greek letters denote ( cid : 123 ) - dimensional vectors whose components are labelled using normal face typeset .
k ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) e ( cid : 123 ) kx ( cid : 123 ) yk ( cid : 123 ) c ( cid : 123 )
in the remainder of this section , we shall develop an algorithm which returns a function f that takes the value ( cid : 123 ) in a small region capturing most of the data points , and ( cid : 123 ) elsewhere .
our strategy is to map the data into the feature space corresponding to the kernel , and to separate them from the origin with maximum margin .
for a new point x , the value f ( cid : 123 ) x ( cid : 123 ) is determined by evaluating which side of the hyperplane it falls on , in feature space .
via the freedom to utilize different types of kernel functions , this simple geometric picture corresponds to a variety of nonlinear estimators in input space .
to separate the data set from the origin , we solve the following quadratic program :
( cid : 123 ) ( cid : 123 ) pi ( cid : 123 ) i ( cid : 123 ) ( cid : 123 )
subject to ( cid : 123 ) w ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) ( cid : 123 )
here , ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) is a parameter whose meaning will become clear later .
since nonzero slack variables ( cid : 123 ) i are penalized in the objective function , we can expect that if w and ( cid : 123 ) solve this problem , then the decision function f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) sgn ( cid : 123 ) ( cid : 123 ) w ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) will be positive for most examples xi contained in the training set , while the sv type regularization term kwk will still be small .
the actual trade - off between these two goals is controlled by ( cid : 123 ) .
deriving the dual problem , and using ( 123 ) , the solution can be shown to have an sv expansion
f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) sgn ( cid : 123 ) xi
( cid : 123 ) ik ( cid : 123 ) xi ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
( patterns xi with nonzero ( cid : 123 ) i are called svs ) , where the coefcients are found as the solu - tion of the dual problem :
( cid : 123 ) i ( cid : 123 ) jk ( cid : 123 ) xi ( cid : 123 ) xj ( cid : 123 ) subject to ( cid : 123 ) ( cid : 123 ) i ( cid : 123 )
( cid : 123 ) i ( cid : 123 ) ( cid : 123 )
this problem can be solved with standard qp routines .
it does , however , possess features that sets it apart from generic qps , most notably the simplicity of the constraints .
this can be exploited by applying a variant of smo developed for this purpose ( 123 ) .
the offset ( cid : 123 ) can be recovered by exploiting that for any ( cid : 123 ) i which is not at the upper or
lower bound , the corresponding pattern x i satises ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) w ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) pj ( cid : 123 ) jk ( cid : 123 ) xj ( cid : 123 ) xi ( cid : 123 ) .
note that if ( cid : 123 ) approaches , the upper boundaries on the lagrange multipliers tend to inn - ity , i . e .
the second inequality constraint in ( 123 ) becomes void .
the problem then resembles the corresponding hard margin algorithm , since the penalization of errors becomes innite , as can be seen from the primal objective function ( 123 ) .
it can be shown that if the data set is separable from the origin , then this algorithm will nd the unique supporting hyperplane with the properties that it separates all data from the origin , and its distance to the origin is maximal among all such hyperplanes ( 123 ) .
if , on the other hand , ( cid : 123 ) approaches 123 , then the constraints alone only allow one solution , that where all ( cid : 123 ) i are at the upper bound ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) .
in this case , for kernels with integral , such as normalized versions of ( 123 ) , the decision function corresponds to a thresholded parzen windows estimator .
to conclude this section , we note that one can also use balls to describe the data in feature space , close in spirit to the algorithms of ( 123 ) , with hard boundaries , and ( 123 ) , with soft margins .
for certain classes of kernels , such as gaussian rbf ones , the corresponding algorithm can be shown to be equivalent to the above one ( 123 ) .
in this section , we show that the parameter ( cid : 123 ) characterizes the fractions of svs and outliers ( proposition 123 ) .
following that , we state a robustness result for the soft margin ( proposition 123 ) and error bounds ( theorem 123 ) .
further results and proofs are reported in the full version of the present paper ( 123 ) .
we will use italic letters to denote the feature space images of the corresponding patterns in input space , i . e .
x i ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) proposition 123 assume the solution of ( 123 ) satises ( cid : 123 ) ( cid : 123 ) .
the following statements hold : ( i ) ( cid : 123 ) is an upper bound on the fraction of outliers .
( ii ) ( cid : 123 ) is a lower bound on the fraction of svs .
( iii ) suppose the data were generated independently from a distribution p ( cid : 123 ) x ( cid : 123 ) which does not contain discrete components .
suppose , moreover , that the kernel is analytic and non - constant .
with probability 123 , asymptotically , ( cid : 123 ) equals both the fraction of svs and the fraction of outliers .
the proof is based on the constraints of the dual problem , using the fact that outliers must have lagrange multipliers at the upper bound .
proposition 123 local movements of outliers parallel to w do not change the hyperplane .
we now move on to the subject of generalization .
our goal is to bound the probability that a novel point drawn from the same underlying distribution lies outside of the estimated region by a certain margin .
we start by introducing a common tool for measuring the capacity of a class f of functions that map x to r .
denition 123 let ( cid : 123 ) x ( cid : 123 ) d ( cid : 123 ) be a pseudo - metric space , 123 let a be a subset of x and ( cid : 123 ) ( cid : 123 ) .
a set b ( cid : 123 ) x is an ( cid : 123 ) - cover for a if , for every a a , there exists b b such that d ( cid : 123 ) a ( cid : 123 ) b ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) .
the ( cid : 123 ) - covering number of a , n d ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) a ( cid : 123 ) , is the minimal cardinality of an ( cid : 123 ) - cover for a ( if there is no such nite cover then it is dened to be ) .
with a distance function that differs from a metric in that it is only semidenite
the idea is that b should be nite but approximate all of a with respect to the pseudometric d .
we will use the l distance over a nite sample x ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) for the pseudo - metric in the space of functions , dx ( cid : 123 ) f ( cid : 123 ) g ( cid : 123 ) ( cid : 123 ) maxi ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) jf ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) g ( cid : 123 ) xi ( cid : 123 ) j ( cid : 123 ) let n ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) f ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) supxx ( cid : 123 ) ndx ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) f ( cid : 123 ) .
below , logarithms are to base 123
theorem 123 consider any distribution p on x and any ( cid : 123 ) r .
suppose x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) are from p .
then with probability ( cid : 123 ) ( cid : 123 ) over such an ( cid : 123 ) - sample , if we nd f f such that f ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) for all i ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ,
pfx ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) g ( cid : 123 )
( cid : 123 ) ( cid : 123 ) k ( cid : 123 ) log ( cid : 123 )
where k ( cid : 123 ) dlog n ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) f ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) e .
we now consider the possibility that for a small number of points f ( cid : 123 ) x i ( cid : 123 ) fails to exceed ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) .
this corresponds to having a non - zero slack variable ( cid : 123 ) i in the algorithm , where we take ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) kwk and use the class of linear functions in feature space in the application of the theorem .
there are well - known bounds for the log covering numbers of this class .
let f be a real valued function on a space x .
fix ( cid : 123 ) r .
for x x , dene
d ( cid : 123 ) x ( cid : 123 ) f ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) maxf ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) g ( cid : 123 )
similarly for a training sequence x , we dene d ( cid : 123 ) x ( cid : 123 ) f ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) pxx d ( cid : 123 ) x ( cid : 123 ) f ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) theorem 123 fix ( cid : 123 ) r .
consider a xed but unknown probability distribution p on the input space x and a class of real valued functions f with range ( cid : 123 ) a ( cid : 123 ) b ( cid : 123 ) .
then with probability ( cid : 123 ) ( cid : 123 ) over randomly drawn training sequences x of size ( cid : 123 ) , for all ( cid : 123 ) ( cid : 123 ) and any f f ,
( cid : 123 ) ( cid : 123 ) k ( cid : 123 ) log ( cid : 123 )
p fx ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) and x xg ( cid : 123 )
d ( cid : 123 ) x ( cid : 123 ) f ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) log ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) b ( cid : 123 ) a ( cid : 123 )
where k ( cid : 123 ) llog n ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) f ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) b ( cid : 123 ) a ( cid : 123 ) d ( cid : 123 ) x ( cid : 123 ) f ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) the theorem bounds the probability of a new point falling in the region for which f ( cid : 123 ) x ( cid : 123 ) has value less than ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , this being the complement of the estimate for the support of the distribution .
the choice of ( cid : 123 ) gives a trade - off between the size of the region over which the bound holds ( increasing ( cid : 123 ) increases the size of the region ) and the size of the probability with which it holds ( increasing ( cid : 123 ) decreases the size of the log covering numbers ) .
the result shows that we can bound the probability of points falling outside the region of estimated support by a quantity involving the ratio of the log covering numbers ( which can be bounded by the fat shattering dimension at scale proportional to ( cid : 123 ) ) and the number of training examples , plus a factor involving the 123 - norm of the slack variables .
it is stronger than related results given by ( 123 ) , since their bound involves the square root of the ratio of the pollard dimension ( the fat shattering dimension when ( cid : 123 ) tends to 123 ) and the number of
the output of the algorithm described in sec .
123 is a function f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) pi ( cid : 123 ) ik ( cid : 123 ) xi ( cid : 123 ) x ( cid : 123 ) which is greater than or equal to ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) i on example xi .
though non - linear in the input space , this function is in fact linear in the feature space dened by the kernel k .
at the same time the 123 - norm of the weight vector is given by b ( cid : 123 ) p ( cid : 123 ) t k ( cid : 123 ) , and so we can apply the theorem with the function class f being those linear functions in the feature space with 123 - norm bounded by b .
if we assume that ( cid : 123 ) is xed , then ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , hence the support of the distribution is the set fx ( cid : 123 ) f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) g , and the bound gives the probability of a randomly generated point falling outside this set , in terms of the log covering numbers of the function class f and the sum of the slack variables ( cid : 123 ) i .
since the log covering numbers
at scale ( cid : 123 ) ( cid : 123 ) of the class f can be bounded by o ( cid : 123 ) rb of the 123 - norm of the weight vector .
log ( cid : 123 ) ( cid : 123 ) this gives a bound in terms
ideally , one would like to allow ( cid : 123 ) to be chosen after the value of ( cid : 123 ) has been determined , perhaps as a xed fraction of that value .
this could be obtained by another level of struc - tural risk minimisation over the possible values of ( cid : 123 ) or at least a mesh of some possible values .
this result is beyond the scope of the current preliminary paper , but the form of the result would be similar to theorem 123 , with larger constants and log factors .
whilst it is premature to give specic theoretical recommendations for practical use yet , one thing is clear from the above bound .
to generalize to novel data , the decision function to be used should employ a threshold ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , where ( cid : 123 ) ( cid : 123 ) ( this corresponds to a nonzero ( cid : 123 ) ) .
we apply the method to articial and real - world data .
figure 123 displays 123 - d toy examples , and shows how the parameter settings inuence the solution .
next , we describe an experiment on the usps dataset of handwritten digits .
the database contains digit images of size ( cid : 123 ) ( cid : 123 ) ; the last constitute the test set .
we trained the algorithm , using a gaussian kernel ( 123 ) of width c ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( a common value for svm classiers on that data set , cf .
( 123 ) ) , on the test set and used it to identify outliers it is folklore in the community that the usps test set contains a number of patterns which are hard or impossible to classify , due to segmentation errors or mislabelling .
in the experiment , we augmented the input patterns by ten extra dimensions corresponding to the class labels of the digits .
the rationale for this is that if we disregarded the labels , there would be no hope to identify mislabelled patterns as outliers .
123 shows the 123 worst outliers for the usps test set .
note that the algorithm indeed extracts patterns which are very hard to assign to their respective classes .
in the experiment , which took seconds on a pentium ii running at mhz , we used a ( cid : 123 ) value of ( cid : 123 ) .
( cid : 123 ) , width c
figure 123 : first two pictures : a single - class svm applied to two toy problems; ( cid : 123 ) ( cid : 123 ) c ( cid : 123 ) ( cid : 123 ) , domain : ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) .
note how in both cases , at least a fraction of ( cid : 123 ) of all examples is in the estimated region ( cf .
table ) .
the large value of ( cid : 123 ) causes the additional data points in the upper left corner to have almost no inuence on the decision function .
for smaller values of ( cid : 123 ) , such as ( cid : 123 ) ( third picture ) , the points cannot be ignored anymore .
alternatively , one can force the algorithm to take these outliers into account by changing the kernel width ( 123 ) : in the fourth picture , using c ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , the data is effectively analyzed on a different length scale which leads the algorithm to consider the outliers as meaningful points .
figure 123 : outliers identied by the proposed algorithm , ranked by the negative output of the svm ( the argument of the sgn in the decision function ) .
the outputs ( for convenience in units of ( cid : 123 ) ) are written underneath each image in italics , the ( alleged ) class labels are given in bold face .
note that most of the examples are difcult in that they are either atypical or even mislabelled .
one could view the present work as an attempt to provide an algorithm which is in line with vapniks principle never to solve a problem which is more general than the one that one is actually interested in .
e . g . , in situations where one is only interested in detecting novelty , it is not always necessary to estimate a full density model of the data .
indeed , density estimation is more difcult than what we are doing , in several respects .
mathematically speaking , a density will only exist if the underlying probability measure possesses an absolutely continuous distribution function .
the general problem of estimat - ing the measure for a large class of sets , say the sets measureable in borels sense , is not solvable ( for a discussion , see e . g .
therefore we need to restrict ourselves to making a statement about the measure of some sets .
given a small class of sets , the simplest esti - mator accomplishing this task is the empirical measure , which simply looks at how many training points fall into the region of interest .
our algorithm does the opposite .
it starts with the number of training points that are supposed to fall into the region , and then esti - mates a region with the desired property .
often , there will be many such regions the solution becomes unique only by applying a regularizer , which in our case enforces that the region be small in a feature space associated to the kernel .
this , of course , implies , that the measure of smallness in this sense depends on the kernel used , in a way that is no dif - ferent to any other method that regularizes in a feature space .
a similar problem , however , appears in density estimation already when done in input space .
let p denote a density on x .
if we perform a ( nonlinear ) coordinate transformation in the input domain x , then the density values will change; loosely speaking , what remains constant is p ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) dx , while dx is transformed , too .
when directly estimating the probability measure of regions , we are not faced with this problem , as the regions automatically change accordingly .
an attractive property of the measure of smallness that we chose to use is that it can also be placed in the context of regularization theory , leading to an interpretation of the solution as maximally smooth in a sense which depends on the specic kernel used ( 123 ) .
the main inspiration for our approach stems from the earliest work of vapnik and collab - orators .
they proposed an algorithm for characterizing a set of unlabelled data points by separating it from the origin using a hyperplane ( 123 ) .
however , they quickly moved on to two - class classication problems , both in terms of algorithms and in the theoretical devel - opment of statistical learning theory which originated in those days .
from an algorithmic point of view , we can identify two shortcomings of the original approach which may have caused research in this direction to stop for more than three decades .
firstly , the original
algorithm in was limited to linear decision rules in input space , secondly , there was no way of dealing with outliers .
in conjunction , these restrictions are indeed severe a generic dataset need not be separable from the origin by a hyperplane in input space .
the two mod - ications that we have incorporated dispose of these shortcomings .
firstly , the kernel trick allows for a much larger class of functions by nonlinearly mapping into a high - dimensional feature space , and thereby increases the chances of separability from the origin .
in partic - ular , using a gaussian kernel ( 123 ) , such a separation exists for any data set x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) : to see this , note that k ( cid : 123 ) xi ( cid : 123 ) xj ( cid : 123 ) ( cid : 123 ) for all i ( cid : 123 ) j , thus all dot products are positive , implying that all mapped patterns lie inside the same orthant .
moreover , since k ( cid : 123 ) x i ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) for all i , they have unit length .
hence they are separable from the origin .
the second modication allows for the possibility of outliers .
we have incorporated this softness of the decision rule using the ( cid : 123 ) - trick and thus obtained a direct handle on the fraction of outliers .
we believe that our approach , proposing a concrete algorithm with well - behaved compu - tational complexity ( convex quadratic programming ) for a problem that so far has mainly been studied from a theoretical point of view has abundant practical applications .
to turn the algorithm into an easy - to - use black - box method for practicioners , questions like the selection of kernel parameters ( such as the width of a gaussian kernel ) have to be tackled .
it is our expectation that the theoretical results which we have briey outlined in this paper will provide a foundation for this formidable task .
acknowledgement .
part of this work was supported by the arc and the dfg ( # ja 123 / 123 - 123 ) , and done while bs was at the australian national university and gmd first .
as is supported by a grant of the deutsche forschungsgemeinschaft ( sm 123 / 123 - 123 ) .
thanks to s .
ben - david , c .
bishop , c .
schnorr , and m .
tipping for helpful discussions .
