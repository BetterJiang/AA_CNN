the support vector ( sv ) method was recently proposed for es ( cid : 123 ) timating regressions , constructing multidimensional splines , and solving linear operator equations ( vapnik , 123 ) .
in this presenta ( cid : 123 ) tion we report results of applying the sv method to these problems .
the support vector method is a universal tool for solving multidimensional function estimation problems .
initially it was designed to solve pattern recognition problems , where in order to find a decision rule with good generalization ability one selects some ( small ) subset of the training data , called the support vectors ( svs ) .
optimal separation of the sv s is equivalent to optimal separation the entire data .
this led to a new method of representing decision functions where the decision functions are a linear expansion on a basis whose elements are nonlinear functions parameterized by the svs ( we need one sv for each element of the basis ) .
this type of function representation is especially useful for high dimensional input space : the number of free parameters in this representation is equal to the number of svs but does not depend on the dimensionality of the space .
later the sv method was extended to real - valued functions .
this allows us to expand high - dimensional functions using a small basis constructed from svs
vapnik , s .
golowich and a .
smola
novel type of function representation opens new opportunities for solving various problems of function approximation and estimation .
in this paper we demonstrate that using the sv technique one can solve problems that in classical techniques would require estimating a large number of free param ( cid : 123 ) eters .
in particular we construct one and two dimensional splines with an arbitrary number of grid points .
using linear splines we approximate non - linear functions .
we show that by reducing requirements on the accuracy of approximation , one de ( cid : 123 ) creases the number of svs which leads to data compression .
we also show that the sv technique is a useful tool for regression estimation .
lastly we demonstrate that using the sv function representation for solving inverse ill - posed problems provides an additional opportunity for regularization .
123 sv method for estimation of real functions
let x e rn and y e rl .
consider the following set of real functions : a vector x is mapped into some a priori chosen hilbert space , where we define functions that are linear in their parameters
y = i ( x , w ) = l wi<pi ( x ) , w = ( wi , . . .
, wn , . .
in ( vapnik , 123 ) the following method for estimating functions in the set ( 123 ) based on training data ( xl , yd , . .
, ( xl , yl ) was suggested : find the function that minimizes the following functional :
r ( w ) = l
iyi - i ( xi , w ) lt : + i ( w , w ) ,
iy - i ( x , w ) lt : =
iy - i ( x , w ) l - otherwise ,
if iy - i ( x , w ) 123 < ,
( w , w ) is the inner product of two vectors , and i is some constant .
it was shown that the function minimizing this functional has a form :
i ( x , a , a* ) = l ( a; - ai ) ( <i> ( xi ) , <i> ( x ) ) + b
where ai , ai 123 : : 123 with aiai = 123 and ( <i> ( xi ) , <i> ( x is the inner product of two elements of hilbert space .
to find the coefficients a; and ai one has to solve the following quadratic optimiza ( cid : 123 ) tion problem : maximize the functional
w ( a* , a ) = - l ( a; +ai ) + ly ( a; - ai ) - ~ l
( a; - ai ) ( aj - aj ) ( <i> ( xi ) , <i> ( xj ) ) ,
subject to constraints
l ( ai - ai ) =o , o ~ ai , a; ~ c , i=l , . .
sv method for function approximation and regression estimation
the important feature of the solution ( 123 ) of this optimization problem is that only some of the coefficients ( a; - ai ) differ from zero .
the corresponding vectors xi are called support vectors ( svs ) .
therefore ( 123 ) describes an expansion on svs .
it was shown in ( vapnik , 123 ) that to evaluate the inner products ( <123> ( xi ) ' <123> ( x ) ) both in expansion ( 123 ) and in the objective function ( 123 ) one can use the general form of the inner product in hilbert space .
according to hilbert space theory , to guarantee that a symmetric function k ( u , v ) has an expansion
k ( u , v ) = l ak123fjk ( u ) tpk ( v )
with positive coefficients ak > 123 , i . e .
to guarantee that k ( u , v ) is an inner product in some feature space <123> , it is necessary and sufficient that the conditions
j k ( u , v ) g ( u ) g ( v ) du dv > 123
be valid for any non - zero function 123 on the hilbert space ( mercer ' s theorem ) .
therefore , in the sv method , one can replace ( 123 ) with
i ( x , a , a* ) = l ( a; - ai ) k ( x , xi ) + b
where the inner product ( <123> ( xi ) , <123> ( x coefficients ai and ai one has to maximize the function
is defined through a kernel k ( xi , x ) .
to find
w ( a* , a ) = - ( l ( a; +ai ) + ly ( a; - ai ) - ~ l
( a; - ai ) ( aj - aj ) k ( xi , xj ) ( 123 )
subject to constraints ( 123 ) .
123 constructing kernels for inner products
to define a set of approximating functions one has to define a kernel k ( xi , x ) that generates the inner product in some feature space and solve the corresponding quadratic optimization problem .
123 kernels generating splines
we start with the spline functions .
according to their definition , splines are piece ( cid : 123 ) wise polynomial functions , which we will consider on the set ( 123 , 123 ) .
splines of order n have the following representation
in ( x ) = l arxr + l wj ( x - t ~ r ~ .
where ( x - t ) + = max ( ( x - t ) , o ) , tl , . . .
, tn e ( 123 , 123 ) are the nodes , and ar , wj are real values .
one can consider the spline function ( 123 ) as a linear function in the n + n + 123 dimensional feature space spanned by
123 , x , . . .
, xn , ( x - tdf . , . . .
, ( x - tn ) f . .
vapnik , s .
golowich and a .
smola
therefore the inner product that generates splines of order n in one dimension is
ixi , xj ) = lx;xj + l ( xi - t123 ) ~ ( xj - t123 ) ~ '
two dimensional splines are linear functions in the ( n + n + 123 ) 123 dimensional space
123 , x , . . .
, xn , y , . . .
, yn , . . .
, ( x - td ~ ( y - t ~ ) ~ , . . .
, ( x - tn ) ~ ( y - tn ) ~ .
let us denote by ui = ( xi , yi ) , uj = ( xi , yj ) two two - dimensional vectors .
then the generating kernel for two dimensional spline functions of order n is
it is easy to check that the generating kernel for the m - dimensional splines is the product of m one - dimensional generating kernels .
in applications of the sv method the number of nodes does not play an impor ( cid : 123 ) tant role .
therefore , we introduce splines of order d with an infinite number of nodes s ~ oo ) .
to do this in the r123 case , we map any real value xi to the element 123 , xi , . . .
, xi , ( xi - t ) + of the hilbert space .
the inner product becomes
ixi , xj ) = lx;xj+ 123 ( xi - t ) ~ ( xj - t ) ~ dt
for linear splines s ~ oo ) we therefore have the following generating kernel :
in many applications expansions in bn - splines ( unser & aldroubi , 123 ) are used ,
bn ( x ) = e ( - ~ y ( n + 123 ) ( x + n + 123 _ r ) n .
one may use bn - splines to perform a construction similar to the above , yielding
123 kernels generating fourier expansions lastly , fourier expansion can be considered as a hyperplane in following 123n + 123 dimensional feature space
v123 ' cos x , sln x , . . .
, cos x , sln x .
n ' n
the inner product in this space is defined by the dirichlet formula :
sv method for function approximation and regression estimation
123 function estimation and data compression
in this section we approximate functions on the basis of observations at f points
we demonstrate that to construct an approximation within an accuracy of c at the data points , one can use only the subsequence of the data containing the svs .
we consider approximating the one and two dimensional functions
f ( x ) = smclxl = - i - xl -
on the basis of a sequence of measurements ( without noise ) on the uniform lattice ( 123 for the one dimensional case and 123 , 123 for the two - dimensional case ) .
for different c we approximate this function by linear splines from si 123 ) .
figure 123 : approximations with different levels of accuracy require different numbers ofsv : 123 sv for c = 123 ( left ) and 123 sv for c = 123 .
large dots indicate svs .
- - + : . " ' . 123\ , ~ .
" . . . . .
+ : : .
, + : : ,
figure 123 : approximation of f ( x , y ) splines with accuracy c = 123 ( left ) required 123 sv ( right )
sinc vi x 123 + y123 by two dimensional linear
figure 123 : sincx function corrupted by different levels of noise ( 123 = 123 left , 123 right ) and its regression .
black dots indicate sv , circles non - sv data .
vapnik , s .
golowich and a
123 solution of the linear operator equations
in this section we consider the problem of solving linear equations in the set of functions defined by svs .
consider the problem of solving a linear operator equation
af ( t ) = f ( x ) ,
f ( t ) e 123 , f ( x ) e w ,
where we are given measurements of the right hand side
consider the set of functions f ( t , w ) e 123 linear in some feature space ( <i> ( t ) = ( >o ( t ) , . . .
, >n ( t ) , . .
( xl , fi ) , . . .
, ( xl , fl ) .
f ( t , w ) = l wr>r ( t ) = ( w , <i> ( t .
the operator a maps this set of functions into
f ( x , w ) = af ( t , w ) = l wra>r ( t ) = l wrtpr ( x ) = ( w , w ( x
where tpr ( x ) = a>r ( t ) , w ( x ) = ( tpl ( x ) , . . .
, tpn ( x ) , . . .
let us define the generating kernel in image space
k ( xi , xj ) = l
tpr ( xi ) tpr ( xj ) = ( w ( xi ) ' w ( xj
and the corresponding cross - kernel function
k , ( xi ' t ) = l
tpr ( xd>r ( t ) = ( w ( xi ) , <i> ( t .
the problem of solving ( 123 ) in the set of functions f ( t , w ) e 123 ( finding the vector w ) is equivalent to the problem of regression estimation ( 123 ) using data ( 123 ) .
to estimate the regression on the basis of the kernel k ( xi , xj ) one can use the methods described in section 123
the obtained parameters ( a; - ai , i = 123 , . . .
f ) define the approximation to the solution of equation ( 123 ) based on data ( 123 ) :
f ( t , a ) = l ( ai - ai ) k , ( xi , t ) .
we have applied this method to solution of the radon equation
f ( m cos tt + u sin tt , m sin tt - u cos tt ) du = p ( m , tt ) ,
- 123 ~ m ~ 123 , 123 < tt < 123 " ,
using noisy observations ( ml ' ttl , pd , . . .
, ( ml ' ttl , pi ) ' where pi = p ( mi , tti ) + ~ i and ( ed are independent with eei = 123 , eel < 123
a ( m ) = - / 123 - m 123
sv method for function approximation and regression estimation
for two - dimensional linear splines s ~ 123 ) we obtained analytical expressions for the ' kernel ( 123 ) and cross - kernel ( 123 ) .
we have used these kernels for solving the corre ( cid : 123 ) sponding regression problem and reconstructing images based on data that is similar to what one might get from a positron emission tomography scan ( shepp , vardi & kaufman , 123 ) .
a remarkable feature of this solution is that it avoids a pixel representation of the function which would require the estimation of 123 , 123 to 123 , 123 parameters .
the spline approximation shown here required only 123 svs .
figure 123 : original image ( dashed line ) and its reconstruction ( solid line ) from 123 , 123 observations ( left ) .
123 svs ( support lines ) were used in the reconstruction ( right ) .
in this article we present a new method of function estimation that is especially useful for solving multi - dimensional problems .
the complexity of the solution of the function estimation problem using the sv representation depends on the com ( cid : 123 ) plexity of the desired solution ( i . e .
on the required number of svs for a reasonable approximation of the desired function ) rather than on the dimensionality of the space .
using the sv method one can solve various problems of function estimation both in statistics and in applied mathematics .
we would like to thank chris burges ( lucent technologies ) and bernhard scholkopf ( mpik tiibingen ) for help with the code and useful discussions .
this work was supported in part by nsf grant phy 123 - 123 ( steven golowich ) and by arpa grant n123 - 123 - c - 123 and the german national scholarship foun ( cid : 123 ) dation ( alex smola ) .
