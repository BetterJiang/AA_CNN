we propose two statistical tests to determine if two samples are from different dis - tributions .
our test statistic is in both cases the distance between the means of the two samples mapped into a reproducing kernel hilbert space ( rkhs ) .
the rst test is based on a large deviation bound for the test statistic , while the second is based on the asymptotic distribution of this statistic .
the test statistic can be com - puted in o ( m123 ) time .
we apply our approach to a variety of problems , including attribute matching for databases using the hungarian marriage method , where our test performs strongly .
we also demonstrate excellent performance when compar - ing distributions over graphs , for which no alternative tests currently exist .
we address the problem of comparing samples from two probability distributions , by proposing a statistical test of the hypothesis that these distributions are different ( this is called the two - sample or homogeneity problem ) .
this test has application in a variety of areas .
in bioinformatics , it is of interest to compare microarray data from different tissue types , either to determine whether two subtypes of cancer may be treated as statistically indistinguishable from a diagnosis perspective , or to detect differences in healthy and cancerous tissue .
in database attribute matching , it is desir - able to merge databases containing multiple elds , where it is not known in advance which elds correspond : the elds are matched by maximising the similarity in the distributions of their entries .
in this study , we propose to test whether distributions p and q are different on the basis of samples drawn from each of them , by nding a smooth function which is large on the points drawn from p , and small ( as negative as possible ) on the points from q .
we use as our test statistic the difference between the mean function values on the two samples; when this is large , the samples are likely from different distributions .
we call this statistic the maximum mean discrepancy ( mmd ) .
clearly the quality of mmd as a statistic depends heavily on the class f of smooth functions that dene it .
on one hand , f must be rich enough so that the population mmd vanishes if and only if p = q .
on the other hand , for the test to be consistent , f needs to be restrictive enough for the empirical estimate of mmd to converge quickly to its expectation as the sample size increases .
we shall use the unit balls in universal reproducing kernel hilbert spaces ( 123 ) as our function class , since these will be shown to satisfy both of the foregoing properties .
on a more practical note , mmd is cheap to compute : given m points sampled from p and n from q , the cost is o ( m + n ) 123 time .
we dene two non - parametric statistical tests based on mmd .
the rst , which uses distribution - independent uniform convergence bounds , provides nite sample guarantees of test performance , at the expense of being conservative in detecting differences between p and q .
the second test is
based on the asymptotic distribution of mmd , and is in practice more sensitive to differences in distribution at small sample sizes .
these results build on our earlier work in ( 123 ) on mmd for the two sample problem , which addresses only the second kind of test .
in addition , the present approach employs a more accurate approximation to the asymptotic distribution of the test statistic .
we begin our presentation in section 123 with a formal denition of the mmd , and a proof that the population mmd is zero if and only if p = q when f is the unit ball of a universal rkhs .
we also give an overview of hypothesis testing as it applies to the two - sample problem , and review previous approaches .
in section 123 , we provide a bound on the deviation between the population and empirical mmd , as a function of the rademacher averages of f with respect to p and q .
this leads to a rst hypothesis test .
we take a different approach in section 123 , where we use the asymptotic distribution of an unbiased estimate of the squared mmd as the basis for a second test .
finally , in section 123 , we demonstrate the performance of our method on problems from neuroscience , bioinformatics , and attribute matching using the hungarian marriage approach .
our approach performs well on high dimensional data with low sample size; in addition , we are able to successfully apply our test to graph data , for which no alternative tests exist .
proofs and further details are provided in ( 123 ) , and software may be downloaded from http : / / www . kyb . mpg . de / bs / people / arthur / mmd . htm
123 the two - sample - problem our goal is to formulate a statistical test that answers the following question :
problem 123 let p and q be distributions dened on a domain x .
given observations x : = ( x123 , .
, xm ) and y : = ( y123 , .
, yn ) , drawn independently and identically distributed ( i . i . d . ) from p and q respectively , is p 123= q ? to start with , we wish to determine a criterion that , in the population setting , takes on a unique and distinctive value only when p = q .
it will be dened based on ( 123 , lemma 123 . 123 ) .
lemma 123 let ( x , d ) be a separable metric space , and let p , q be two borel probability measures dened on x .
then p = q if and only if ep ( f ( x ) ) = eq ( f ( x ) ) for all f c ( x ) , where c ( x ) is the space of continuous bounded functions on x .
although c ( x ) in principle allows us to identify p = q uniquely , such a rich function class is not practical in the nite sample setting .
we thus dene a more general class of statistic , for as yet unspecied function classes f , to measure the discrepancy between p and q , as proposed in ( 123 ) .
denition 123 let f be a class of functions f : x r and let p , q , x , y be dened as above .
then we dene the maximum mean discrepancy ( mmd ) and its empirical estimate as
mmd ( f , p , q ) : = sup
mmd ( f , x , y ) : = sup
( exp ( f ( x ) ) eyq ( f ( y ) ) ) , f ( yi ) ! .
f f 123
we must now identify a function class that is rich enough to uniquely establish whether p = q , yet restrictive enough to provide useful nite sample estimates ( the latter property will be established in subsequent sections ) .
to this end , we select f to be the unit ball in a universal rkhs h ( 123 ) ; we will henceforth use f only to denote this function class .
with the additional restriction that x be compact , a universal rkhs is dense in c ( x ) with respect to the l norm .
it is shown in ( 123 ) that gaussian and laplace kernels are universal .
theorem 123 let f be a unit ball in a universal rkhs h , dened on the compact metric space x , with associated kernel k ( , ) .
then mmd ( f , p , q ) = 123 if and only if p = q .
this theorem is proved in ( 123 ) .
we next express the mmd in a more easily computable form .
this is simplied by the fact that in an rkhs , function evaluations can be written f ( x ) = h ( x ) , fi , where ( x ) = k ( x , . ) .
denote by ( p ) : = exp ( x ) ( ( x ) ) the expectation of ( x ) ( assuming that it exists
a sufcient condition for this is k ( p ) k123 h < , which is rearranged as ep ( k ( x , x ) ) < , where x and x are independent random variables drawn according to p ) .
since ep ( f ( x ) ) = h ( p ) , fi , we
mmd ( f , p , q ) = sup
kf kh123h ( p ) ( q ) , fi = k ( p ) ( q ) kh .
using ( x ) : = 123
i=123 ( xi ) and k ( x , x ) = h ( x ) , ( x ) i , an empirical estimate of mmd is
mmd ( f , x , y ) =
k ( xi , yj ) +
( 123 ) provides us with a test statistic for p 123= q .
we shall see in section 123 that this estimate is biased , although it is straightforward to upper bound the bias ( we give an unbiased estimate , and an associated test , in section 123 ) .
intuitively we expect mmd ( f , x , y ) to be small if p = q , and the quantity to be large if the distributions are far apart .
computing ( 123 ) costs o ( ( m + n ) 123 ) time .
overview of statistical hypothesis testing , and of previous approaches having dened our test statistic , we briey describe the framework of statistical hypothesis testing as it applies in the present context , following ( 123 , chapter 123 ) .
given i . i . d .
samples x p of size m and y q of size n , the statistical test , t ( x , y ) : xm xn 123 ( 123 , 123 ) is used to distinguish between the null hypothesis h123 : p = q and the alternative hypothesis h123 : p 123= q .
this is achieved by comparing the test statistic mmd ( f , x , y ) with a particular threshold : if the threshold is exceeded , then the test rejects the null hypothesis ( bearing in mind that a zero population mmd indicates p = q ) .
the acceptance region of the test is thus dened as any real number below the threshold .
since the test is based on nite samples , it is possible that an incorrect answer will be returned : we dene the type i error as the probability of rejecting p = q based on the observed sample , despite the null hypothesis being true .
conversely , the type ii error is the probability of accepting p = q despite the underlying distributions being different .
the level of a test is an upper bound on the type i error : this is a design parameter of the test , and is used to set the threshold to which we compare the test statistic ( nding the test threshold for a given is the topic of sections 123 and 123 ) .
a consistent test achieves a level , and a type ii error of zero , in the large sample limit .
we will see that both of the tests proposed in this paper are consistent .
we next give a brief overview of previous approaches to the two sample problem for multivariate data .
since our later experimental comparison is with respect to certain of these methods , we give abbreviated algorithm names in italics where appropriate : these should be used as a key to the tables in section 123
we provide further details in ( 123 ) .
a generalisation of the wald - wolfowitz runs test to the multivariate domain was proposed and analysed in ( 123 , 123 ) ( wolf ) , which involves counting the number of edges in the minimum spanning tree over the aggregated data that connect points in x to points in y .
the computational cost of this method using kruskals algorithm is o ( ( m + n ) 123 log ( m + n ) ) , although more modern methods improve on the log ( m + n ) term .
two possible generalisations of the kolmogorov - smirnov test to the multivariate case were studied in ( 123 , 123 ) .
the approach of friedman and rafsky ( smir ) in this case again requires a minimal spanning tree , and has a similar cost to their multivariate runs test .
a more recent multivariate test was proposed in ( 123 ) , which is based on the minimum distance non - bipartite matching over the aggregate data , at cost o ( ( m + n ) 123 ) .
another recent test was proposed in ( 123 ) ( hall ) : for each point from p , it requires computing the closest points in the aggregated data , and counting how many of these are from q ( the procedure is repeated for each point from q with respect to points from p ) .
the test statistic is costly to compute; ( 123 ) consider only tens of points in their experiments .
yet another approach is to use some distance ( e . g .
l123 or l123 ) between parzen window estimates of the densities as a test statistic ( 123 , 123 ) , based on the asymptotic distribution of this distance given p = q .
when the l123 norm is used , the test statistic is related to those we present here , although it is arrived at from a different perspective ( see ( 123 ) : the test in ( 123 ) is obtained in a more restricted setting where the rkhs kernel is an inner product between parzen windows .
since we are not doing density estimation , however , we need not decrease the kernel width as the sample grows .
in fact , decreasing the kernel width reduces the convergence rate of the associated two - sample test , compared with the ( m + n ) 123 / 123 rate for xed kernels ) .
the l123 approach of ( 123 ) ( biau ) requires the space to be partitioned into a grid of bins , which becomes difcult or impossible for high dimensional problems .
hence we use this test only for low - dimensional problems in our experiments .
123 a test based on uniform convergence bounds
in this section , we establish two properties of the mmd .
first , we show that regardless of whether or not p = q , the empirical mmd converges in probability at rate 123 / m + n to its population value .
this establishes the consistency of statistical tests based on mmd .
second , we give probabilistic bounds for large deviations of the empirical mmd in the case p = q .
these bounds lead directly to a threshold for our rst hypothesis test .
we begin our discussion of the convergence of mmd ( f , x , y ) to mmd ( f , p , q ) .
theorem 123 let p , q , x , y be dened as in problem 123 , and assume |k ( x , y ) | k .
then prn|mmd ( f , x , y ) mmd ( f , p , q ) | > 123 ( cid : 123 ) ( k / m )
123 ( cid : 123 ) + o 123 exp ( cid : 123 ) 123mn
123 + ( k / n )
our next goal is to rene this result in a way that allows us to dene a test threshold under the null hypothesis p = q .
under this circumstance , the constants in the exponent are slightly improved .
theorem 123 under the conditions of theorem 123 where additionally p = q and m = n ,
mmd ( f , x , y ) > m 123
+ > 123 ( k / m ) 123 / 123
123q123ep ( k ( x , x ) k ( x , x ) )
both with probability less than exp ( cid : 123 ) 123m
123k ( cid : 123 ) ( see ( 123 ) for the proof ) .
in this theorem , we illustrate two possible bounds b123 ( f , p ) and b123 ( f , p ) on the bias in the empirical estimate ( 123 ) .
the rst inequality is interesting inasmuch as it provides a link between the bias bound b123 ( f , p ) and kernel size ( for instance , if we were to use a gaussian kernel with large , then k ( x , x ) and k ( x , x ) would likely be close , and the bias small ) .
in the context of testing , however , we would need to provide an additional bound to show convergence of an empirical estimate of b123 ( f , p ) to its population equivalent .
thus , in the following test for p = q based on theorem 123 , we use b123 ( f , p ) to bound the bias .
lemma 123 a hypothesis test of level for the null hypothesis p = q ( equivalently mmd ( f , p , q ) =
123 ) has the acceptance region mmd ( f , x , y ) < 123pk / m ( cid : 123 ) 123 +plog 123 ( cid : 123 ) .
we emphasise that theorem 123 guarantees the consistency of the test , and that the type ii error probability decreases to zero at rate 123 / m ( assuming m = n ) .
to put this convergence rate in perspective , consider a test of whether two normal distributions have equal means , given they have unknown but equal variance ( 123 , exercise 123 ) .
in this case , the test statistic has a student - t distri - bution with n + m 123 degrees of freedom , and its error probability converges at the same rate as
123 an unbiased test based on the asymptotic distribution of the u - statistic
we now propose a second test , which is based on the asymptotic distribution of an unbiased estimate of mmd123
we begin by dening this test statistic .
lemma 123 given x and x independent random variables with distribution p , and y and y indepen - dent random variables with distribution q , the population mmd123 is
mmd123 ( f , p , q ) = ex , xp ( k ( x , x ) ) 123exp , yq ( k ( x , y ) ) + ey , y q ( k ( y , y ) )
( see ( 123 ) for details ) .
let z : = ( z123 , .
, zm ) be m i . i . d .
random variables , where zi : = ( xi , yi ) ( i . e .
we assume m = n ) .
an unbiased empirical estimate of mmd123 is
u ( f , x , y ) =
which is a one - sample u - statistic with h ( zi , zj ) : = k ( xi , xj ) + k ( yi , yj ) k ( xi , yj ) k ( xj , yi ) .
the empirical statistic is an unbiased estimate of mmd123 , although it does not have minimum vari - ance ( the minimum variance estimate is almost identical : see ( 123 , section 123 . 123 ) ) .
we remark that these quantities can easily be linked with a simple kernel between probability measures : ( 123 ) is a special case of the hilbertian metric ( 123 , eq .
( 123 ) ) with the associated kernel k ( p , q ) = ep , qk ( x , y ) ( 123 , theorem 123 ) .
the asymptotic distribution of this test statistic under h123 is given by ( 123 , section 123 . 123 ) , and the distribution under h123 is computed based on ( 123 , section 123 . 123 ) and ( 123 , appendix ) ; see ( 123 ) for details .
theorem 123 we assume e ( cid : 123 ) h123 ( cid : 123 ) < .
under h123 , mmd123
( 123 , section 123 ) ) to a gaussian according to
u converges in distribution ( dened e . g
u mmd123 ( f , p , q ) ( cid : 123 ) d n ( cid : 123 ) 123 , 123
u = 123 ( cid : 123 ) ez ( cid : 123 ) ( ez h ( z , z ) ) 123 ( cid : 123 ) ( ez , z ( h ( z , z ) ) ) 123 ( cid : 123 ) , uniformly at rate 123 / m ( 123 , theorem
under h123 , the u - statistic is degenerate , meaning ez h ( z , z ) = 123
in this case , mmd123 converges in distribution according to
where zl n ( 123 , 123 ) i . i . d . , i are the solutions to the eigenvalue equation
l 123 ( cid : 123 ) ,
k ( x , x ) i ( x ) dp ( x ) = ii ( x ) ,
and k ( xi , xj ) : = k ( xi , xj ) exk ( xi , x ) exk ( x , xj ) + ex , xk ( x , x ) is the centred rkhs kernel .
our goal is to determine whether the empirical test statistic mmd123 u is so large as to be outside the 123 quantile of the null distribution in ( 123 ) ( consistency of the resulting test is guaranteed by the form of the distribution under h123 ) .
one way to estimate this quantile is using the bootstrap ( 123 ) on the aggregated data .
alternatively , we may approximate the null distribution by tting pearson curves to its rst four moments ( 123 , section 123 ) .
taking advantage of the degeneracy of the u - statistic , we obtain ( see ( 123 ) )
ez , z ( h ( z , z ) ez ( h ( z , z ) h ( z , z ) ) ) + o ( m123 ) .
and expensive to calculate ( o ( m123 ) ) .
u ( cid : 123 ) 123 ( cid : 123 ) is not computed , since it is both very small ( o ( m123 ) ) the fourth moment e ( cid : 123 ) ( cid : 123 ) mmd123
instead , we replace the kurtosis with its lower bound
we conducted distribution comparisons using our mmd - based tests on datasets from three real - world domains : database applications , bioinformatics , and neurobiology .
we investigated the uniform convergence approach ( mmd ) , the asymptotic approach with bootstrap ( mmd123 and the asymptotic approach with moment matching to pearson curves ( mmd123 u m ) .
we also compared against several alternatives from the literature ( where applicable ) : the multivariate t - test , the friedman - rafsky kolmogorov - smirnov generalisation ( smir ) , the friedman - rafsky wald - wolfowitz generalisation ( wolf ) , the biau - gyor test ( biau ) , and the hall - tajvidi test ( hall ) .
note that we do not apply the biau - gyor test to high - dimensional problems ( see end of section 123 ) , and that mmd is the only method applicable to structured data such as graphs .
an important issue in the practical application of the mmd - based tests is the selection of the kernel parameters .
we illustrate this with a gaussian rbf kernel , where we must choose the kernel width
( we use this kernel for univariate and multivariate data , but not for graphs ) .
the empirical mmd is zero both for kernel size = 123 ( where the aggregate gram matrix over x and y is a unit matrix ) , and also approaches zero as ( where the aggregate gram matrix becomes uniformly constant ) .
we set to be the median distance between points in the aggregate sample , as a compromise between these two extremes : this remains a heuristic , however , and the optimum choice of kernel size is an ongoing area of research .
data integration as a rst application of mmd , we performed distribution testing for data inte - gration : the objective is to aggregate two datasets into a single sample , with the understanding that both original samples are generated from the same distribution .
clearly , it is important to check this last condition before proceeding , or an analysis could detect patterns in the new dataset that are caused by combining the two different source distributions , and not by real - world phenomena .
we chose several real - world settings to perform this task : we compared microarray data from normal and tumor tissues ( health status ) , microarray data from different subtypes of cancer ( subtype ) , and local eld potential ( lfp ) electrode recordings from the macaque primary visual cortex ( v123 ) with and without spike events ( neural data i and ii ) .
in all cases , the two data sets have different statistical properties , but the detection of these differences is made difcult by the high data dimensionality .
we applied our tests to these datasets in the following fashion .
given two datasets a and b , we either chose one sample from a and the other from b ( attributes = different ) ; or both samples from either a or b ( attributes = same ) .
we then repeated this process up to 123 times .
results are reported in table 123
our asymptotic tests perform better than all competitors besides wolf : in the latter case , we have greater type ii error for one neural dataset , lower type ii error on the health status data ( which has very high dimension and low sample size ) , and identical ( error - free ) performance on the remaining examples .
we note that the type i error of the bootstrap test on the subtype dataset is far from its design value of 123 , indicating that the pearson curves provide a better threshold estimate for these low sample sizes .
for the remaining datasets , the type i errors of the pearson and bootstrap approximations are close .
thus , for larger datasets , the bootstrap is to be preferred , since it costs o ( m123 ) , compared with a cost of o ( m123 ) for pearson ( due to the cost of computing ( 123 ) ) .
finally , the uniform convergence - based test is too conservative , nding differences in distribution only for the data with largest sample size .
neural data i
neural data ii
u b mmd123
u m t - test wolf
table 123 : distribution testing for data integration on multivariate data .
numbers indicate the per - centage of repetitions for which the null hypothesis ( p=q ) was accepted , given = 123 .
sample size ( dimension; repetitions of experiment ) : neural i 123 ( 123; 123 ) ; neural ii 123 ( 123; 123 ) ; health status 123 ( 123 , 123; 123 ) ; subtype 123 ( 123 , 123; 123 ) .
attribute matching our second series of experiments addresses automatic attribute matching .
given two databases , we want to detect corresponding attributes in the schemas of these databases , based on their data - content ( as a simple example , two databases might have respective elds wage and salary , which are assumed to be observed via a subsampling of a particular population , and we wish to automatically determine that both wage and salary denote to the same underlying attribute ) .
we use a two - sample test on pairs of attributes from two databases to nd corresponding pairs . 123 this procedure is also called table matching for tables from different databases .
we performed attribute matching as follows : rst , the dataset d was split into two halves a and b .
each of the n attributes
123note that corresponding attributes may have different distributions in real - world databases .
hence , schema matching cannot solely rely on distribution testing .
advanced approaches to schema matching using mmd as one key statistical test are a topic of current research .
in a ( and b , resp . ) was then represented by its instances in a ( resp .
we then tested all pairs of attributes from a and from b against each other , to nd the optimal assignment of attributes a123 , .
, an from a to attributes b123 , .
, bn from b .
we assumed that a and b contain the same number of attributes .
as a naive approach , one could assume that any possible pair of attributes might correspond , and thus that every attribute of a needs to be tested against all the attributes of b to nd the optimal match .
we report results for this naive approach , aggregated over all pairs of possible attribute matches , in table 123
we used three datasets : the census income dataset from the uci kdd archive ( cnum ) , the protein homology dataset from the 123 kdd cup ( bio ) ( 123 ) , and the forest dataset from the uci ml archive ( 123 ) .
for the nal dataset , we performed univariate matching of attributes ( forest ) and multivariate matching of tables ( forest123d ) from two different databases , where each table represents one type of forest .
both our asymptotic mmd123 u - based tests perform as well as or better than the alternatives , notably for cnum , where the advantage of mmd123 u is large .
unlike in table 123 , the next best alternatives are not consistently the same across all data : e . g .
in bio they are wolf or hall , whereas in forest they are smir , biau , or the t - test .
thus , mmd123 u appears to perform more consistently across the multiple datasets .
the friedman - rafsky tests do not always return a type i error close to the design parameter : for instance , wolf has a type i error of 123% on the bio dataset ( on these data , mmd123 u has the joint best type ii error without compromising the designed type i performance ) .
finally , our uniform convergence approach performs much better than in table 123 , although surprisingly it fails to detect differences in forest123d .
a more principled approach to attribute matching is also possible .
assume that ( a ) = ( 123 ( a123 ) , 123 ( a123 ) , . . . , n ( an ) ) : in other words , the kernel decomposes into kernels on the individual attributes of a ( and also decomposes this way on the attributes of b ) .
in this case , m m d123 can be i=123 ki ( ai ) i ( bi ) k123 , where we sum over the mmd terms on each of the attributes .
our goal of optimally assigning attributes from b to attributes of a via mmd is equivalent to nd - ing the optimal permutation of attributes of b that minimizespn i=123 ki ( ai ) i ( b ( i ) ) k123
if we dene cij = ki ( ai ) i ( bj ) k123 , then this is the same as minimizing the sum over ci , ( i ) .
this is the linear assignment problem , which costs o ( n123 ) time using the hungarian method ( 123 ) .
u b mmd123
u m t - test wolf
table 123 : naive attribute matching on univariate ( bio , forest , cnum ) and multivariate data ( forest123d ) .
numbers indicate the percentage of accepted null hypothesis ( p=q ) pooled over attributes .
= 123 .
sample size ( dimension; attributes; repetitions of experiment ) : bio 123 ( 123; 123; 123 ) ; forest 123 ( 123; 123; 123 ) ; cnum 123 ( 123; 123; 123 ) ; forest123d 123 ( 123; 123; 123 ) .
we tested this hungarian approach to attribute matching via mmd123 u b on three univariate datasets ( bio , cnum , forest ) and for table matching on a fourth ( forest123d ) .
to study mmd123 on structured data , we obtained two datasets of protein graphs ( proteins and enzymes ) and used the graph kernel for proteins from ( 123 ) for table matching via the hungarian method ( the other tests were not applicable to this graph data ) .
the challenge here is to match tables representing one functional class of proteins ( or enzymes ) from dataset a to the corresponding tables ( functional classes ) in b .
results are shown in table 123
besides on the bio dataset , mmd123 u b made no errors .
123 summary and discussion
we have established two simple multivariate tests for comparing two distributions p and q .
the test statistics are based on the maximum deviation of the expectation of a function evaluated on each of the random variables , taken over a sufciently rich function class .
we do not require density
sample size repetitions % correct matches
table 123 : hungarian method for attribute matching via mmd123 u b on univariate ( bio , cnum , for - est ) , multivariate ( forest123d ) , and structured data ( enzymes , proteins ) ( = 123; % correct matches is the percentage of the correct attribute matches detected over all repetitions ) .
