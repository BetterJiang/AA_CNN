of the distribution ,
i . e . , given a specific as
however , that there are certain inde
this allows a natural and compact
and supports effective inference
eases knowledge ac
that we cannot capture qualitatively
bayesian networks provide a language for qualitatively of a distribution .
it is well - known , the bayesian network structure : hold only in certain signment of values to certain variables . .
per , we propose a formal notion of context - specific given network .
we then focus on a particular for capturing csi .
we suggest ways in which this rep prove the performance
can be used to support effective inference of the resulting
network which can im
based on cutset conditioning .
in the condi tables ( cpts ) at a node .
we present
to ( and based on ) d - separation ,
( csi ) , based on regularities
when such independence
we present a structural
in this pa
holds in a
in bns in much the same way indepen
we formally chatacterize
a number of the
a random variable
can be exploited dence among variables a bn is a directed dence .
more precisely , p ( z ( x ) for all values z .
a bn encodes about each random variable : in the network ( 123 ) .
for example , can be read from the network using a graph - theoretic
given a set of variables
graph where each node rep
and edges represent
we say that variables x if p ( z i : c , y ) = x , y and z of variables
z andy are x , y and
given the state of its patents
from these local statements
in the network
i , z is of u , v andy given x and w .
further
shown in figure
bn also represents by a set each node x has of conditional probability tion of x given different ents .
using the independencies the joint distribution
for its par can be computed
cpt that describes
in the structure
lies in the efficient
among random variables .
in the rep
ares;_ to provide of a distribution ,
ease of knowledge
the power of bayesian and domain modeling , this power by refining gate how independence
the bn representation
of this paper is to increase
in the in
using a tabular
of a conditional
that all of
in which each assignment
in its most naive form , a cpt is encoded of x requires over x .
thus , for example , u , v , w and x in figure ify eight such distributions of this representation figure 123 , for example , when u holds ( i . e . ,
fails to capture cer in the cpt of in the node distribution .
123 are binary , ( or eight parameters ) .
in the number of par
we need to spec
p ( x i u , v , w ) is equal to some of the values
taken by v and w :
when u = t ) we need not consider
refers to the computation
of a posterior
pi v / " - . .
p123 p123
independence and arc
where each variable
a finite set u = ( x 123 , . . .
, x n )
a finite domain .
we use capital names and lowercase
x , y , z , for variable all values of x is denoted noted by boldface of values to the variables
x; e u may take on x , y , z to values taken by those variables .
the set of val ( x ) .
sets of variables x , y , z , and as signmen ts in these sets will be denoted by a : , y , z ( we use val ( x ) in the ob
definition 123 : let p be a joint probability in u , and let x , y , z be subsets over the variables x and y are conditionally given z , denoted i ( x; y i z ) , iffor all a : e vai ( x ) , y e vai ( y ) , z e
p ( x i z , y ) = p ( x i z ) whenever p ( y , z ) > 123
in that they involve
( for all values of a : , y , z )
this last statement
network is a directed
to the random variables
graph b whose x 123 , . . .
, x n , and of b encodes the set of inde
of its non - descendants given its par in b .
other i ( ) statements , in arbitrary sets of variables , follow from these local these can be read from the structure of b us
by p ( x i z , y ) = p ( x i z ) .
whose edges represent the graph structure node x; is independent ents ilx123 these statements only a node and its parents ing a graph - theoretic that can be tested in polynomial time .
a bn b represents to be an / - map for the distribution quired to be a minimal of any edge in the network destroys network with respect b for p permits tion : we need only specify , u , , ) for each possible ( see ( 123 ) for details . ) ables in z .
however ,
thus , we require
encoded in b hold for p .
more precisely ,
in b holds in p , a bn is re
a compact representation
ofthe form i ( x; y i z ) , that is , indepen
x; , a condi in ( x; , ilx , ) .
that hold for any assignment
we are often interested
of the bn can only capture
table ( cpt ) encoding
to the distribution
value of the variables
that hold only in certain
for each variable
of values to the vari
that the indepen
b is said
about a par
123 - map , in the sense that the deletion
of the distribu
p if every independence
definition 123 : let x , y , z , c be pairwise disjoint
x and y are contextually
figure 123 : context - specific
of eight .
such reg
the values of v and w .
clearly , we need to specify over x instead most five distributions occur often enough tool and knowledge
that at least two well
that allow the user to more easily specify
in their knowledge
for such reg
by using the notion of context - specific
in our example , the regularities
a formal foundation
in this paper , we provide of w and v given cpt of x ensure that x is independent u ( u = t ) , but is dependent on w , v in the con text u ( u = f ) .
this is an assertion dependence ( csi ) , which is more restricted than the state ments of variable
namely , ease of
that are encoded by the as we show in this paper , such
can be used to extend the advantages
in order to capture
not the first to suggest
the use of asymmetric
we are certainly the related multinets trees ) have been used to encode cpts ( 123 , 123 ) .
the intent this work is to formalize pose methods for utilizing
making ( 123 , 123 ) and poole ' s ( 123 ) use of horn rules to encode dependencies
csi , to study its rep and to pro
even the representation
as part of a more general
the notion of
so that csi statements
123 by defining context - specific
and how this representation
we begin in section tion for a bn based on arc deletion can be readily determined cusses in detail how trees can be used to represent may reduce clique size for clustering strategy - in cutset conditioning .
and future research
that use csi - and the associated
123 offers sug
of csi .
we present
can be exploited
with a dis
network transformations that
in bayesian networks 123
e val ( c ) , denoted ic ( x; y i z , c ) , if z and the contexte p ( x i z , c , y ) = p ( x i z , c ) whenever p ( y , z , c ) > 123
theorem 123 : let b be a network
w ith band i; , such that ( b , i; ) is a perfect
then there exists
i; be a set of
and exhaustive generalized set itx .
a generalized proposition is
c to c .
of x and y
that the independence
is similar to that in equation ( 123 ) , taking cuz
hold only for the particular it is easy to see that certain local ic statements the form ic ( x; y i c ) for y , c c;;; iix - can be veri fied by direct examination of the cpt for x .
in figure 123 , for example , we can verify ic ( x; v i u ) by checking in the cpt for x whether , for each value w ofw , p ( x i v , w , u ) it is the same for all values v of does not depend on v ( i . e . , v ) .
the next section explores different representations the cpts that will allow us to check these local to the principle method for deciding it turns out that this problem can be solved by a simple re duction to a problem of validating variable in a simpler network .
the latter problem can be solved using d - separation .
of non - local ic statements .
now is to establish
on the resulting
and can be determined
from y given z u c in b ( c ) .
definition 123 : an edge from y into x will be called vac uous in b , given a context c , if ic ( x; y i c n iix ) .
given bn b and a context c , we define b ( c ) as thebn that results vacuous edges in b given c .
we say that x from y given z in context c in b if x is note that the statement ic ( x; y i c n iix ) is a local ic the cpt for x .
thus , we can decide csi - separation into b ( c ) using these local ic statements to delete vacuous edges , and then usingd - separation we now show that this notion of csi - separation and ( in a strong sense ) complete given these local indepen let b be a network structure and i : be a set of local ic statements over b .
we say that ( b , z; ) is a cs / - map inp , i . e . , ic ( x;y i z , c ) holdsin p whenever x is cs / - separated from y given z in con text c in ( b , i; ) .
we say that ( b , i; ) is a peifect if the implied independencies are the only ones that hold in p , i . e . , if ic ( x; y i z , c ) ifandonlyif x is csj - separated from y given z in context c in ( b , in theorem 123 : let b be a network band i; .
then ( b , i; ) is a csi - mapof
i be a set of and p a distribution
p if the independencies
of a distribution
of this procedure .
the theorem establishes the procedure also complete ? as for any such procedure , there may be independencies only local independencies provides the best results that we can hope to derive based solely on the structural
theorem shows that , in a sense , this procedure
that we cannot detect using
of the distribution .
and network structure .
however ,
and can be
we can represent
we discuss possible
of specific variable
using a set
the fact that distinct
in much the
that capture this regularity
cpts by simply partitioning
over x .
while this representation
with the same distribution .
therefore ,
into regions mapping to the same distribution .
for reasons of space ,
over x .
a compact represen of this function elements of val ( iix )
local csi statements
within cpts .
in this section , same way that a bn structure we focus primarily in general , we can view a cpt as a function that maps tation of cpts is simply a representation of mutually exclusive tions over the variable simply a truth functional so that if y , z e iix , we may have a par by the generalized y ) v - . ( z = z ) .
each such proposition is fully gen eral , it does not easily support either probabilistic ing .
for example , one could use a canonical such as minimal cnf .
classification the machine learning other popular function state space induced by the labeling ing the fact that vacuous edges can be detected , cpts produced in linear time ( in the size of the cpt repre pact cnf or tree representation for the purposes of this paper , we focus on cpt - trees sults for cnf representations ( of the form discussed paper .
a major advantage of tree structures is to " rule " structure man expert .
as we show in subsequent as we discuss also amenable to well - studied
there is a tradeoff : the most com of a cpt might be much larger in the worst case ) than the min in terms of generalized propositions .
of branches in the tree .
have a number of advantages ,
to speed up bn inference al
about csi .
fortunately ,
community as decision
can also be utilized
trees ( also known in
with branch labels corresponding
( see figure i ) .
this intuition
by ( 123 ) ) to a longer version of this
in some sense
from a hu
for this type of partition
to elicit probabilities
we can often use other ,
in the conclusion ,
trees ) are an
123\ / ' - . . . .
pl p123 pj c ( ) p123 ' b p123 p123 p123 " p123 ' " pl p123
network tree for x ( 123 )
figure 123 : cpt - tree representation
this using the criterion
ever , the test above is complete in the sense that no other edge is vacuous given the tree structure .
in the theorem
theorem 123 : lett be a cpt - tree let c e c be some context that is consistent to the leaves
with c , then there exists
oft such that y - + x is not vac
for x , lety e ilx and on a path
( y ( / : .
ify occurs
this is similar in spirit to d - separation :
above is , in fact , the best
of the tree and not the ac
this shows that the test described test that uses only the structure it detects all conditional cies that are hidden in the quantification soundness in order to exploit csi in inference .
in belief networks ,
of the links .
as for we need only
of the network , but it cannot detect independen
the cpt conditioned
to produce a reduced cpt - tree rep it is also straightforward on context c .
assume c an certain parents of x and t is the cpt - tree of x , with root r and immediate sub trees t123 , tk .
the reduced t ( c ) is defined re ables c , then t ( c ) consists labelofrissomey e c , thent ( c ) = 123j ( c ) , wheretj the subtree pointed to by the t - are
if the label of r is not among the vari labeled with value y e c .
reduced tree t ( c ) can be produced with one tree in o ( iti ) time .
of r with subtrees tj ( c ) ;
and only if y c and y occurs
in t ( c ) if on a path in t that is
only if y - + x
this implies that y appears in t ( c ) if and is not deemed vacuous by the test described the reduced tree , determining that can be deleted requires thus , reducing the tree gives us an efficient and sound test parents of x .
the list of arcs pointing a simple tree traversal
csi in probabilistic
that are exploited
by well - known algorithms
of a bn lays bare variable
how best that information
when deciding what information a similar fashion , trees make csi relationships explicit .
scribe how csi might be exploited ing and cutset conditioning .
also emphasize that these in which bn inference
we provide only the basic intuitions
to ( say ) a given can be summarized .
in of cpts such as in this section ,
uses in cluster
are by no means the only ways
in various bn inference
can employ csi .
methods ( 123 ) .
in this section ,
we show that they admit fast
this operation
cpt when we condition
x is vacuous; the second is to determine
we wish to perform in general , there are two operations given a context c : the first is to determine whether arc into a variable carried out whenever we set evidence and should reflect the changes to x ' s parents that are implied by context - specific given c .
we examine how to perform both types of operations to avoid confusion , use t - node and t - are to denote nodes and arcs in the tree ( as opposed to nodes and arcs in the bn ) .
to illustrate x in figure 123
( left t - ares are labeled true and right t - ares
the cpt - tree for the variable
easy to tell which
it is relatively
of x given context c
given this representation , parents are rendered independent sume that tree l represents clearly d remains relevant dependent of x .
given a 123\ b , both c and d are rendered this is so because the distri bution on x does not depend on c and d once we know c = a 123\ b : every path from the root to leaf which is consis tent with c fails to mention cor d .
the cpt for x .
in context a , while c and b are rendered in
intuitively ,
definition 123 : a path in the cpt - tree is the set of t - ares lying between the root and a leaf .
the labeling of a path is induced by the labels on the t ares of the path .
a variable tent with a context c iff the labeling with the assignment
the path tests the value ofy .
a path is consis
on a path if one of the
of the path
of values in c .
theorem 123 : lett be a cpt - tree for x and let y be one of its parents .
( y ( / : .
if y does not lie on any path consistent y - + x is vacuous
let c e c be some context
with c , then the edge
this provides us with a sound test for context - specific dependence ( only valid independencies however , the test is not complete , tures that cannot be represented stance , suppose that pl = p123 and p123 = p123 in the example above .
given context bac , we can tell that a is irrelevant
but , the choice of variable
since there are cpt struc
by a tree .
for in
in bayesian networks 123
a x " - ' x123 - t j t t j j 123 j f t 123 f f j 123
figure 123 : ( a ) a simple decomposition of x , utilizing
of the node x; ( b ) the cpt for the new node x; ( c ) a more effective
123 network transformations
and then , when the value of a is revealed , value for x is chosen .
for cpts is not a novel
of x make independent
the use of idea .
for instance , tions ( 123 ) ) allow compact representation to the value of x .
these distributions fall into the gen dence ( 123 , 123 ) .
for such distributions , we can perform a in a new network tially , the transformation introduces auxiliary from causal independence ,
where many of these independencies
ideas can be applied : can be used to capture
while csi is quite distinct
of csi directly within
within the network structure .
them via a cascading
on our original
can be very useful when one uses based on clustering ( 123 ) .
roughly
over the set val ( x ) of the nodes x in the cluster .
or clique , encodes
is carried out on the join tree , and its
a join tree , whose in the orig
nodes denote ( overlapping ) inal bn .
each cluster , that each fam ily in the bn - a node and its parents - be a subset of at least one clique in the join tree .
therefore , a large set clique and thereby
val ( ( x; ) u itx , ) will lead to a large to poor performance
this is where the structural
by the size of the largest
in a family can offer considerable computa
we define a random variable
x a=t , with a condi
only on b 123 , . .
the variable x
we can similarly define is equal to x a=t if a = t and is equal to xa=f if a = f .
x a=t andx a=j both have the same note that the variables set of values as x .
this perspective allows us to replace node x in any network figure 123 ( a ) .
the node call a multiplexer the value of xa=t or of xa - : = f , depending on the value of a ) .
its cpt
x is a deterministic
node ( since x takes either
with the subnetwork
is not particularly
( with its many tightly
cpt for x;
for one thing ,
if x exhibits
the total size of the two new cpts
does not admit a more effective
node x , this decomposition
the same as the size of the original
for a generic amount of csi , this type of transformation more compact representation .
assume that x depends and only on b123 and b123 when a is false .
then and xa=f will have only two parents , two cpts with four entries cpt with 123 entries .
tree with much smaller
of the re network may well allow the construction of a join
node with 123 ( predetermined )
in a far let k = 123 , and
only on b123 and b123 when a is true ,
as in figure 123 ( c )
each of xa=t
of x had a single
the new representation each , plus a single
a family with
of clustering algo
we first consider
and let b123 , . .
let a be one node x in a bayesian network .
in order to understand of x ' s parents , ents .
assume , for simplicity , value that x would take if a were true , and the value that x would take if a were false .
ply this decomposition x is first decomposed according at the root of its cpt tree .
each of the conditional ( x a=t and xa=f in the binary case ) has , as its cpt , one of x as the outcome of two conditional variables : xa=f , b=j .
the node xa=j , b=f can then be decomposed
that x and a are both binary we can view the value of the random
nodes can be decomposed in figure 123 , for example ,
lly , each node to the parent a which is
a in the cpt for x .
the result
bk be the remaining par
the node corre
to xa=j can be decomposed
where these two variables
uses the structure
into xa=j , b=t and
we can conduct
of a cpt - tree
of the t - node
friedman , goldszmidt , and koller
figure 123 : a decomposition cording to tree ( 123 ) .
of the network in figure 123 , ac
if it is observed to be true .
123 in both cases , these csi rela tions are captured by the deterministic relationships pendent if the node is set to false .
in a multiplexer the value depends only on one parent once the value of the
the parents are inde
parent ( the original
in an " or " node ,
123 cutset conditioning
to take advantage of independencies
or tree representations ,
the use of static precompilation makes it diffi
even using noisy - or gorithm can only take advantage of fixed structural cult for the algorithm that only occur in certain circumstances , more dynamic algorithms , cies more effectively .
gorithms can be modified to exploit csi using our decision
( 123 ) , can exploit context - specific
such as cutset
below how cutset al
e . g . , as new ev
since they have no parents .
into xa=j , b=j , c=t and xa=f , b=j , c=f .
the nodes xa=j , b=t and xa=j , b=j , c=t cannot be de sible , this is not beneficial , that this procedure in the cpt of a node .
thus , in general , that this includes
w hile further of nodes xa=t and xa=f , b=f , c=f since the cpts for these nodes ( a complete tree of depth 123 ) .
it is clear
we want to stop the when the cpt of a node is a full tree
leaves a special case . )
only if there is a structure
that , once in
render the network singly connected .
algorithm works roughly as fol i . e . , a set of variables
the cutset conditioning lows .
we select a cutset , is then carried out using reasoning case is a possible assignment c .
each such assignment call to the polytree ence on the resulting are combined to give the final answer .
the running time is by the number of calls to the poly tree al gorithm ( i . e . , ivai (
algorithm ( 123 ) , which performs infer network .
the results of these calls
by cases , where each
to the variables
as evidence in a
in the cutset
but each generally
can allow clustering
for example , figure 123 describes
for noisy - or nodes of as in the structural ( 123 ) , our decomposition form smaller cliques .
after the transformation , many more nodes in the network ( on the order of the size of all cpt tree representations ) , mation ofthe cpt of tree ( 123 ) of figure 123
in this transfor mation we have eliminated ing on implementing ness in practice .
be further exploited
function of their parents .
such nodes can
a family with four parents and we are currently
we also note that a large fraction
several smaller families .
these ideas , and testing
nodes we introduce
in the clustering
in clique size ( and the result depend heavily on the structure
we note that the reduction of the decision trees .
a similar phenomenon occurs in the the order in which we choose to cascade the different par ents of the node .
of ( 123 ) , where the effectiveness
bn cannot capture all independencies none of the csi relations
be read from
as in the case of noisy - or , plicit in the cpts .
in particular , induced by particular is our inability
in the noisy - or
case , the analogue that a node ' s parents
loops without the need for instantiating
csi offers a rather obvious advantage to inference of loop cutsets .
by in rithms based on the conditioning to a certain value in order to vacuous , perhaps cut cut a loop , csi may render other arcs suppose the network in fig ure 123 is to be solved using the cutset ( u , v , w ) ( this might be the optimal strategy we solve the reduced singly - connected ues to u , v , w .
however , by recognizing u , we need not instantiate this replaces jval ( v ) a single evaluation .
ation of v , w can no longer be ignored ( the edges are not
v and w when we assign u = t .
w ) i network evaluations
i i val ( however , when u = f , the instanti
between x and ( v , w ) are vacuous in context
if i val ( x ) i is very large ) .
typically ,
i times , oncefor each assignment
the fact that the
to capture this phenomenon , tion of a cutset by considering sets .
these reflect the need to instantiate a tree with interior
but not in others , in order to render the net
the standard no
nodes labeled by variables
and edges ia -
zthis last fact is heavily 123we believe similar ideas can be applied to other compact cpt
networks ( mostly bn123 networks ) .
utilized by algorithms
ically at noisy - or
if the node is observed to be false , but not
such as noisy - or .
jtj a y ' z . .
jtj a figure 123 : valid conditional
value on each edge .
the tree is a conditional
to the set of assignments
each branch through the induced by fixing
beled by ( sets of ) variable cutset if : ( a ) each branch through the tree represents text that renders the network singly - connected; and ( b ) the set of such assignments cutsets for the bn in figure 123 tive .
examples of conditional in figure 123 : ( a ) is theobviouscompact ( b ) is the tree representation fails to exploit the structure uation for each instantiation
is mutually exclusive
of the cpt , requiring of u , v , w .
cutset in hand , the extension is fairly obvious .
we con
of values to variables
once we have a conditional sider each assignment branches through the tree , instantiate run the poly tree algorithm work , and combine the results plexity of this algorithm tinct paths through cial to find good heuristic algorithms of vacuous arcs maximally .
this algorithm
is a function
in a fashion similar to stan to the problem ( 123 , 123 ) .
we dis near the end of
as usual . 123 clearly ,
of the number of dis cutset .
it is therefore
the network with this on the resulting net
csi and the existence
selects nodes for the cutset according to the heuristic
" greedy " approach to cutset construction
of x in the net
where the weight w ( x ) of variable x is and d ( x ) is the out - degree
work graph ( 123 , 123 ) .
123 the weight measures the work needed x in a cutset , while the degree of a vertex gives an idea of its arc - cutting outgoing edges mean a larger chance to extend this heuristic the extent to which arcs are cut due to csi .
the obvious namely adding to d ( x ) ally rendered vacuous by x ( averaging
the number of arcs actu over values of x ) , is
to deal with csi , we must estimate
to cut loops .
in order
123we explain the need for set - valued 123 as in the standard cutset algorithm , the weights required
arc labels below .
combine the answers from the different cases can be obtained the polytree computations ( 123 ) .
123we assume that the network has been preprocessed
so that legitimate
cutsets can be selected
easily .
see ( 123 )
we focus on a " computationally inten
tiated is given by
number of arc deletions
from b if x is instan
independence in bayesian networks 123
we should consider the expected
it ignores the potential
for example , consider
value ( other things being
using b , c and d will be very small .
for arcs to be the family in fig the cpt for x .
adding a or x to be cut , so
myopic .
in particular , ure 123 , with tree 123 reflecting b to a cutset causes no additional they will have the same heuristic equal ) .
however , clearly a is the more desirable cause , given either value of a , the conditional rather than using the actual num ber of arcs cut by select ing a node for the cutset , number of arcs that will be cut .
we do this by consider ing , for each of the children v of x , how many distinct are found in the structured of the cpt for that child for each instantia tion x = x; ( i . e . , cpt ) .
the log of this value is the expected number of parents required the child v after x erage this number for each of the values x may take , and sum the expected number of cut arcs for each of x ' s chil dren .
this measure then plays the role of d ( x ) in the cutset let t ( v ) be the size of the cpt and let t ( v , xi ) be the size of the reduced cpt given con text x = xi ( we assume x is a parent of v ) .
we define the
( i . e . , number of entries )
for v in a fixed network;
= xi is known , with fewer parents
the size of the reduced
number of parents
we can then av
ep ( v , x; ) = l . . , , e arents , , - , . , _
of v given xi to be r123t\ v logl , i ( ) i t ( v , x = x; ) w . . .
thus , ; , gives an reasonably
accurate picture of the cutset in a network b .
node x to a branch of the a new network for of x that reflects csi; and 123 )
value of adding x to a conditional our cutset construction 123 ) adding a heuristically each value x; e val ( x ) ; each of these instantiations the node that looks best in the new network corresponding to that branch .
we can very roughly sketch it as follows .
cutset; 123 ) adding t - ares to the cutset - tree
each of these new arcs recursively
begins with the original
remove singly - connected if no nodes remain , return the
nodes from b , leaving br .
choose node x in br s . t .
w ( x ) jd ' ( x ) 123
for each x; e val ( x ) ,
construct bx , by removing
vacuous arcs from br and replacing reduced cpts using x = x; .
all cpts by the
return the tree t ' where : a ) x labels the root oft ' ; b ) one t - are for each x; emanates from the root; and c )
the context already
value of x is determined
is standard ( 123 , 123 ) .
instep 123 , it is im
step 123 of the algorithm portant to realize that the heuristic with respect to the current network and established in the existing required to ensure that the selection flects the fact that nally , step 123 emphasizes the selection : different variables may be selected next given different values of x .
steps 123 - 123 capture the key features of our approach and have certain computational to which we now turn our
branch of the cutset .
step 123 is of the next variable re
x = xi is part of the current context .
nature of variable
can be run on
we can add an optional
csi to a great degree , but requires
effort greater than that for normal cutset con : a tree rep
stored : as variables are to particular values for conditioning , the selec can be made .
conceptually , of the tree , with only
struction .
first , the cutset itself is structured resentation of a standard cutset is potential larger ( a full tree ) .
however , the algorithm line , and the tree never completely tion of the next amounts to a depth - first one ( partial or complete ) branch ever being stored .
in ad step before step 123 that de effect on the arcs in b and the representation of reduced cpts , then we need not ( in cutset construction ) .
rather , in step 123 , we one new t - are in the cutset - tree labeled with the set ( xi , xj ) ( as in figure 123 ) .
this reduces the number of graphs that need to be constructed ( and concomitant com putations discussed below ) .
in tings , the representation size similar to a normal cutset ,
of a conditional cutset would be of
of x to x; and xi have the same structural
the networks bx , .
if , say , the
as in figure 123 ( b ) .
in order to enhance this algorithm .
less ideal but more tractable methods of con
there are several other directions that we are currently ditional cutset constructi on .
for example , we might select a cutset by standard means , and use the considerations within this cutset .
another direction these ideas with standard cutset algorithms .
to order ( on - line )
ideas of ( 123 ) for
the variable instantiations
123 concluding remarks
adding to the regularities
in bns .
our results provide
the notion of context - specific
we have defined dence as a way of capturing the independencies by specific variable mined using local computation in a bn and how specific mechanisms ( in particular , trees ) allow compact representa tion of cpts and enable
particular , we have shown how csi can
inference in both clustering and cutset - style
efficient detection of csi .
further
cpts can be used to speed up
and its role in infer
the bn representa
work on extending
as we mentioned in the introduction , tion to cap ture additional independencies .
our notion of csi is re lated to what heckerman calls subset independence in his work on similarity networks ( 123 ) .
yet , our approach is sig the cpts within a single and multinets ( 123 , 123 ) rely on a family of networks .
in fact the approach we described in spirit to that of poole ' s rule - based works ( 123 123 ) .
network , while similarity networks
ons of net
by providing a structured
based on decision
different in that we try to capture the additional
goldszmidt , and koller
the t - node attached to the end ofthe x; t - are is the tree produced by recursively
calling the algorithm
that gave rise to the current
apart from the amount of information in a conditiona effort is needed to decide which variables component d ' ( x ) is more in to a branch , since the heuristic volved than vertex degree .
unfortunately , the is not fixed ( in which case it would involve a single set of prior computations ) ; it must be recomputed in step 123 tore flect the variable network .
part of the re - evaluation cpts also be updated ( step 123 ) .
fortunately , cpts that have small : only the children to have cpts updated .
this can be done cient .
these updates then affect the heuristic " spouses " v of x need to only their parents; i . e . , only the have their value d ' ( v ) recomputed .
thus , the amount of work required is not too large , so that the reduction in the number of network evaluations will the extra work .
we in the process of imple menting this algorithm to test
of d ' ( x ) requires that the number of to be updated for assignment x = x; is of x ( in the current graph ) need
its performance in practice .
which are very effi
using the cpt
of the network trans
the arc - cutting technique and network transformation in troduced in section 123 is reminiscent formations introduced by pearl in his of action ( 123 123 ) .
indeed the semantics of in that paper can be viewed as an instance of csi .
this is not a mere coincidence , representing plans and influence diagrams usually some variables , and are vacuous or trivial work on adding additional by smith et al .
( 123 ) , fung and shachter ( 123 ) , and the work by in the context of markov decision processes .
amount of csi .
the effects of actions ( or de when these in are not realized .
testimony to this fact is the
as it is easy to see that networks
et al ( 123 ) on using decision
to influence diagrams
trees to represent
usually only take place for specific instantiation
the ideas presented
there are a number of future research directions needed to elaborate the role that csi and compact cpt representati probabilistic reasoning .
we are currently exploring of different cpt representations , such as decision and the potential
ons play in interaction between csi and causal
here , and to expand
in bayesian networks 123
l cutset construct
of section 123 ( and its variants ) .
( as in the noisy - or
figure 123 and suppose
no csi ) cannot be
are being conducted
a cpt exhibiting
model ) .
a deeper examina
ar , to determine the extent of the overhead
inference .
in many cases , we may wish to trade
we are currently
in many cases , is weaker in some circumstances
to determine the for the con
es under which the reductions
tion of the network ing both of csi can also play a significant role in approximate to speed up inference , allow a certain amount of accuracy more compact representation or ease knowledge tion .
for instance , may require a full tree .
however , than in oth ers .
consider tree 123 in that none of p123 ' , p123 " , p123 ' " are very different , reflecting the fact the influ weak in the case where a is true and d is false .
in this case , we may assume that these three entries ing the true cpt using a decision this saving ( both in representation racy .
in ongoing error and the computational quired directly from the user , to simplify the cpt - tree in or der to allow for faster inference .
and inference , using the of this paper ) comes at the expense of accu
error of a local approximation of
the cpts , thereby that trade off the from the simpli
can be used to from a full conditional
b and c ' s on x is relatively
( 123 123 ) can then be used on this tree , or on one ac
work , we show how to estimate the ( cross
in this regard .
in particular ,
tree with the structure
of the network .
from the machine
tions turn out to be
cial in learning bayesian complexity of the interactions
of cpts have also proven benefi from data ( 123 ) .
due to the
on , learning procedures that better emulate the true
in the data .
of the representati
ents a starting
point for a rigorous
of bayesian network
this paper repres a deep and far - ranging impact on the theory and practice of many aspects ion , inference algorithms , ing .
we consider the exploration ideas to be a promising avenue for future
approximation and learn
as we have seen , csi
acknowledgements : we would like to thank dan geiger , adam grove , daishi harada , and zohar yakhini for useful some of this work was performed while nir were at rockwell center , and daphne koller was at u . c .
berke -
and moises goldszmidt
( 123 123 ) j .
quinlan .
c123 : programs for machince learning
gan kaufmann , 123 123
by a university
this work tract f123 - 123 - c - 123 ( goldszmidt ) ,
and nsf grant iri - 123 - 123
grant ogp123 ( boutilier ) .
123 ( friedman ) , and
an ibm graduate
