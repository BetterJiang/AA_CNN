a large portion of real - world data is stored in com - mercial relational database systems .
most statistical learning methods work only with at data representations .
thus , to apply these methods , we are forced to convert our data into a at form , thereby losing much of the relational structure present in our database .
this paper builds on the recent work on probabilistic relational mod - els ( prms ) , and describes how to learn them from databases .
prms allow the properties of an object to depend probabilistically both on other proper - ties of that object and on properties of related ob - jects .
although prms are signicantly more ex - pressive than standard models , such as bayesian networks , we show how to extend well - known sta - tistical methods for learning bayesian networks to learn these models .
we describe both parameter estimation and structure learning the automatic induction of the dependency structure in a model .
moreover , we show how the learning procedure can exploit standard database retrieval techniques for efcient learning from large datasets .
we present experimental results on both real and synthetic re -
relational models are the most common representation of structured data .
enterprise business information , marketing and sales data , medical records , and scientic datasets are all stored in relational databases .
indeed , relational databases are a multi - billion dollar industry .
recently , there has been grow - ing interest in making more sophisticated use of these huge amounts of data , in particular mining these databases for cer - tain patterns and regularities .
by explicitly modeling these regularities , we can gain a deeper understanding of our do - main and may discover useful relationships .
we can also use our model to ll in unknown but important information
the institute of computer science , hebrew university ,
jerusalem 123 , israel
computer science department , stanford university , gates
building 123a , stanford ca 123 - 123
example , we may be interested in predicting whether a person is a potential money - launderer based on their bank deposits , international travel , business connections and arrest records of known associates ( jensen , 123 ) .
in another case , we may be interested in classifying web pages as belonging to a stu - dent , a faculty member , a project , etc . , using attributes of the web page and of related pages ( craven et al . , 123 ) .
unfortunately , few inductive learning algorithms are capa - ble of handling data in its relational form .
most are restricted to dealing with a at set of instances , each with its own sepa - rate attributes .
to use these methods , one typically attens the relational data , removing its richer structure .
this pro - cess , however , loses information which might be crucial in understanding the data .
consider , for example , the problem of predicting the value of an attribute of a certain entity , e . g . , whether a person is a money - launderer .
this attribute will be correlated with other attributes of this entity , as well as with attributes of related entities , e . g . , of nancial transac - tions conducted by this person , of other people involved in these transactions , of other transactions conducted by these people , etc .
in order to atten this problem , we would need to decide in advance on a xed set of attributes that the learn - ing algorithm can use in this task .
thus , we want a learning algorithm that can deal with multiple entities and their prop - erties , and can reach conclusions about an entitys character - istics based on the properties of the entities to which it is re - lated .
until now , inductive logic programming ( ilp ) ( lavrac and dzeroski , 123 ) has been the primary learning frame - work with this capability .
ilp algorithms learn logical horn rules for determining when some rst - order predicate holds .
while ilp is an excellent solution in many settings , it may be inappropriate in others .
the main limitation is the determin - istic nature of the rules discovered .
in many domains , such as the examples above , we encounter interesting correlations that are far from being deterministic .
our goal in this paper is to learn more rened probabilis - tic models , that represent statistical correlations both between the properties of an entity and between the properties of re - lated entities .
such a model can then be used for reasoning about an entity using the entire rich structure of knowledge encoded by the relational representation .
the starting point for our work is the structured representa - tion of probabilistic models , as exemplied in bayesian net - works ( bns ) .
a bn allows us to provide a compact rep -
resentation of a complex probability distribution over some xed set of attributes or random variables .
the representa - tion exploits the locality of inuence that is present in many domains .
we build on two recent developments in the eld of bayesian networks .
the rst is the deep understanding of the statistical learning problem in such models ( hecker - man , 123; heckerman et al . , 123 ) and the role of struc - ture in providing an appropriate bias for the learning task .
the second is the recent development of representations that extend the attribute - based bn representation to incorporate a much richer relational structure ( koller and pfeffer , 123; ngo and haddawy , 123; poole , 123 ) .
in this paper , we combine these two advances .
indeed , one of our key contributions is to show that many of the tech - niques of bayesian network learning can be extended to the task of learning these more complex models .
this contribu - tion generalizes ( koller and pfeffer , 123 ) s preliminary work on this topic .
we start by describing the semantics of proba - bilistic relational models .
we then examine the problems of parameter estimation and structure selection for this class of models .
we deal with some crucial technical issues that dis - tinguish the problem of learning relational probabilistic mod - els from that of learning bayesian networks .
we provide a formulation of the likelihood function appropriate to this set - ting , and show how it interacts with the standard assumptions of bn learning .
the search over coherent dependency struc - tures is signicantly more complex than in the case of learn - ing bn structure and we introduce the necessary tools and concepts to do this effectively .
we then describe experimen - tal results on synthetic and real - world datasets , and nally discuss possible extensions and applications .
123 underlying framework 123 relational model we describe our relational model in generic terms , closely re - lated to the language of entity - relationship models .
this gen - erality allows our framework to be mapped into a variety of specic relational systems , including the probabilistic logic programs of ( ngo and haddawy , 123; poole , 123 ) , and the probabilistic frame systems of ( koller and pfeffer , 123 ) .
our learning results apply to all of these frameworks .
each relation
and a set of relations
the vocabulary of a relational model consists of a set of takes on values in some xed domain is typed .
this vocabulary
entity type is associated with a set of attributes denes a schema for our relational model .
consider a simple genetic model of the inheritance of a single gene that determines a persons blood type .
each per - son has two copies of the chromosome containing this gene , one inherited from her mother , and one inherited from her fa - ther .
there is also a possibly contaminated test that attempts to recognize the persons blood type .
our schema contains two classes person and blood - test , and three relations father , mother , and test - of .
attributes of person are name , gender , p - chromosome ( the chromosome inherited from the father ) , m - chromosome ( inherited from the mother ) .
the attributes
for each entity type
for each entity
of a schema denes a set of entities , the instance has an associated at - .
for each relation
; its value in
we are interested in describing a probability model over instances of a relational schema .
however , some attributes , such as a name or social security number , are fully deter - mined .
we label such attributes as xed .
we assume that they are known in any instantiation of the schema .
the other attributes are called probabilistic .
a skeleton structure a relational schema is a partial specication of an instance of the schema .
it species the set of objects class , the values of the xed attributes of these objects , and the relations that hold between the objects .
however , it leaves the values of probabilistic attributes unspecied .
a comple - extends the skeleton by also specifying the values of the probabilistic attributes .
of the skeleton structure
one nal denition which will turn out to be useful is the is any relation , we notion of a slot chain .
if - th arguments to obtain a , which we can then view as a slot , we let .
for any holds .
( in relational algebra notation .
we can concatenate slots to form longer slot chains , dened by composition of binary relations .
s in the chain must be appropriately typed . ) ( each of the
. ) objects in this set are called
denote all the elements
of blood - test are serial - number , date , contaminated , and
123 probabilistic relational models we now proceed to the denition of probabilistic relational models ( prms ) .
the basic goal here is to model our uncer - tainty about the values of the non - xed , or probabilistic , at - tributes of the objects in our domain of discourse .
in other words , given a skeleton structure , we want to dene a proba - bility distribution over all completions of the skeleton .
our probabilistic model consists of two components : the , and the parameters as - qualitative dependency structure , .
the dependency structure is dened by sociated with it , associating with each attribute these correspond to formal parents; they will be instantiated in different ways for different objects in .
intuitively , the parents are attributes that are direct inuences on
a set of parents pa
we distinguish between two types of formal parents
the attribute
will depend probabilistically on
can depend on another probabilistic attribute
pendency for individual objects : for any object can also depend on attributes of related objects dependence for an individual object
this formal dependence induces a corresponding de - is a slot chain .
to understand the semantics of this formal , recall that .
except in cases where the slot chain is guaranteed to be single - valued , we must specify the probabilistic dependence of database theory gives us precisely the right tool to address
sents the set of objects that are w - relatives of
the notion of aggregation from
figure 123 : the prm structure for a simple genetics domain .
fixed attributes are shown in regular font and probabilistic attributes are shown in italic .
dotted lines indicate relations between entities and solid arrows indicate probabilistic de -
this issue; i . e . , will depend probabilistically on some ag - gregate property of this multiset .
there are many natural and useful notions of aggregation : the mode of the set ( most fre - quently occurring value ) ; mean value of the set ( if values are numerical ) ; median , maximum , or minimum ( if values are ordered ) ; cardinality of the set; etc .
more formally , our language allows a notion of an aggre - takes a multiset of values of some ground type , and returns a summary of it .
the type of the aggregate can be the same as that of its arguments .
however , we allow other types as well , e . g . , an aggregate that reports the size of the multiset .
; the semantics is that for any
returning to our genetics example , consider the attribute result .
since the result of a blood test depends on whether it was contaminated , it has blood - test the result also depends on the ge - as a parent .
netic material of the person tested .
single - valued , we add blood - test p - chromosome as parents .
figure 123 shows the structure of a simple prm for this domain .
to have as a parent
will depend on the value of in the obvious way .
we associate
more precisely , let
, we can dene given a set of parents pa a local probability model for a conditional probability distribution ( cpd ) that species be the set of par -
a simple attribute in the same relation or an aggregate of a set in some ground relatives has a set of values
, the cpd species type .
for each tuple of values .
the parameters in
all of these cpds comprise
recall that each of these parents
given a skeleton structure for our schema , we want to use these local probability models to dene a probability distri - bution over completions of the skeleton .
first , note that the skeleton determines the set of objects in our model .
we asso - ciate a random variable with each probabilistic attribute .
the skeleton also determines the relations
of each object
between objects , and thereby the set of w - relatives associated with every object for each relationship chain w
also note that by assuming that the relations between objects are al - ways specied by , we are disallowing uncertainty over the relational structure of the model .
to dene a coherent probabilistic model over this skele - ton , we must ensure that our probabilistic dependencies are acyclic , so that a random variable does not depend , directly or indirectly , on its own value .
consider the parents of an , we dene an is a parent of
we say that a , we dene an edge the directed graph dened by acyclic .
in this case , we can dene a coherent probabilistic model over complete instantiations
is acyclic relative to a skeleton over the variables
is a parent of
" ci zs (
proposition 123 : if a distribution over completions
is acyclic relative to
, then ( 123 ) denes
we briey sketch a proof of this proposition , by showing how to construct a bn over the probabilistic attributes of a .
this construction is reminiscent of the knowledge - based model construction approach ( wellman et al . , 123 ) .
here , however , the construction is merely a thought - experiment; our learning algorithm never constructs this network .
in this network there is a node for each vari - and for aggregate quantities required by parents .
the parents of these aggregate random variables are all of the at - tributes that participate in the aggregation , according to the .
the cpds of random variables that relations specied by correspond to probabilistic attributes are simply the cpds de - , and the cpds of random variables that corre - spond to aggregate nodes capture the deterministic function of the particular aggregate operator .
it is easy to verify that if the probabilistic dependencies are acyclic , then so is the induced bayesian network .
this construction also suggests one way of answering queries about a relational model .
we can compile the corresponding bayesian network and use standard tools for answering queries about it .
although for each skeleton , we can compile a prm into a bayesian network , a prm expresses much more information than the resulting bn .
a bn denes a probability distribution over a xed set of attributes .
a prm species a distribution over any skeleton; in different skeletons , the set ( and num - ber ) of entities in the domain will vary , as will the relations between the entities .
in a way , prms are to bns as a set of rules in rst - order logic is to a set of rules in propositional logic : a rule such as
induces a potentially innite set of ground
123 parameter estimation we now move to the task of learning prms .
we begin with learning the parameters for a prm where the dependency
structure is known .
in other words , we are given the structure that determines the set of parents for each attribute , and our task is to learn the parameters that dene the cpds for this structure .
our learning is based on a particular training set , which we will take to be a complete instance this task is relatively straightforward , it is of interest in and of itself .
in addition , it is a crucial component in the structure learning algorithm described in the next section .
the key ingredient in parameter estimation is the likelihood function , the probability of the data given the model .
this function captures the response of the probability distribution to changes in the parameters .
as usual , the likelihood of a parameter set is dened to be the probability of the data given as usual , we
typically work with the log of this function :
" ci zs (
the key insight is that this equation is very similar to the log - likelihood of data given a bayesian network ( heck - in fact , it is the likelihood function of the bayesian network induced by the structure given the skele - ton .
the main difference from standard bayesian network parameter learning is that parameters for different nodes in the network are forced to be identical .
thus , we can use the well - understood theory of learning from bayesian networks .
consider the task of performing maximum likelihood param - eter estimation .
here , our goal is to nd the parameter set - .
this estimation is simplied by the de - composition of log - likelihood function into a summation of terms corresponding to the various attributes of the different classes .
each of the terms in the square brackets in ( 123 ) can be maximized independently of the rest .
hence , maximal likeli - hood estimation reduces to independent maximization prob - lems , one for each cpd .
that maximizes the likelihoods (
for multinomial cpds , maximum likelihood estimation can be done via sufcient statistics which in this case are just that the at - the counts c
and its parents can jointly take .
proposition 123 : assuming multinomial cpds , the maximum
likelihood parameter setting
& of the different values
as a consequence of this proposition , parameter learning in prms is reduced to counting sufcient statistics .
we need to count one vector of sufcient statistics for each cpd .
such counting can be done in a straightforward manner using stan - dard databases queries .
note that this proposition shows that learning parameters in prms is very similar to learning parameters in bayesian net - works .
in fact , we might view this as learning parameters for the bn that the prm induces given the skeleton .
however , as
discussed above , the learned parameters can then be used for reasoning about other skeletons , which induce a completely
in many cases , maximum likelihood parameter estimation is not robust , as it overts the training data .
the bayesian ap - proach uses a prior distribution over the parameters to smooth the irregularities in the training data , and is therefore sig - nicantly more robust .
as we will see in section 123 , the bayesian framework also gives us a good metric for evaluat - ing the quality of different candidate structures .
due to space limitations , we only briey describe this alternative approach .
roughly speaking , the bayesian approach introduces a prior over the unknown parameters , and performs bayesian conditioning , using the data as evidence , to compute a poste - rior distribution over these parameters .
to apply this idea in our setting , recall that the prm parameters of a set of individual probability distribution conditional distribution of the form .
following the work on bayesian approaches for learn - ing bayesian networks ( heckerman , 123 ) , we make two as - sumptions .
first , we assume parameter independence : the priors over the parameters are independent .
second , we assume that the prior over is a dirichlet distribution .
briey , a dirichlet prior for is specied by a set
a multinomial distribution of a variable ! is dirichlet if& ( ' k ( )
for a parameter prior satisfying these two assumptions , the posterior also has this form .
that is , it is a product of inde - pendent dirichlet distributions over the parameters which can be computed easily .
proposition 123 : if is a complete assignment , and the prior satises parameter independence and dirichlet with hyper -
the parameters of ( for more details see ( degroot , 123 ) . )
a distribution on
for the different
is a product of dirichlet distributions with hyperparameters
& , then the posterior
once we have updated the posterior , how do we evaluate the probability of new data ? in the case of bn learning , we assume that instances are iid , which implies that they are in - dependent given the value of the parameters .
thus , to evalu - ate a new instance , we only need the posterior over the param - eters .
the probability of the new instance is then the proba - bility given every possible parameter value , weighted by the posterior probability over these values .
in the case of bns , this term can be rewritten simply as the instance probabil - ity according to the expected value of the parameters ( i . e . , the mean of the posterior dirichlet for each parameter ) .
this sug - gests that we might use the expected parameters for evaluat - ing new data .
indeed , the formula for the expected parameters is analogous to the one for bns : proposition 123 : assuming multinomial cpds , prior in - dependence , and dirichlet priors , with hyperparameters
" , we have that :
unfortunately , the expected parameters are not the proper bayesian solution for computing probability of new data .
there are two possible complications .
already in the database .
in this case , the introduction
of some per - also changes our probability about the .
we therefore cannot simply use our old pos -
the rst problem is that , in our setting , the assumption of iid data is often violated .
specically , a new instance might not be conditionally independent of old ones given the param - eters .
consider the genetics domain , and assume that our new data involves information about the mother of the new object terior about the parameters to reason about the new instance .
this problem does not occur if the new data is not related to the training data , that is , when the new data is essentially a disjoint database with the same scheme .
more interestingly , the problem also disappears when attributes of new objects are not parents of any attribute in the training set .
in the ge - netics example , this means that we can insert new people into our database , as long as they are not ancestors of people al - ready in the database .
the second problem involves the formal justication for using expected parameters values .
this argument depends on the fact that the probability of a new instance is linear in the value of each parameter .
that is , each parameter is used at most once .
this assumption is violated when we consider the probability of a complex database involving multiple in - stances from the same class .
in this case , our integral of the probability of the new data given the parameters can no longer be reduced to computing the probability relative to the ex - pected parameter value .
the correct expression is called the marginal likelihood of the ( new ) data; we use it in section 123 for scoring structures .
for now , we note that if the posterior is sharply peaked ( i . e . , we have seen many training instances ) , we can approximate this term by using the expected parame - ters of proposition 123 , as we could for a single instance .
in practice , we will often use these expected parameters as our
123 structure selection we now move to the more challenging problem of learning a dependency structure automatically , as opposed to having it given by the user .
there are three important issues that need to be addressed .
we must determine which dependency struc - tures are legal; we need to evaluate the goodness of differ - ent candidate structures; and we need to dene an effective search procedure that nds a good structure .
123 legal structures when we consider different dependency structures , it is im - portant to be sure that the dependency structure results in coherent probability models .
to guarantee this property , we see from proposition 123 that the skeleton .
of course , we can easily verify for be acyclic relative to a given candidate structure that it is acyclic relative to the of our training database .
however , we also want to guarantee that it will be acyclic relative to other databases that we may encounter in our domain .
how do we guarantee acyclicity for an arbitrary database ? a simple approach is to
if either ( a )
is a parent of
ensure that dependencies among attributes respect some order ( i . e . , are stratied ) .
more precisely , we say that is a parent of , or ( b ) and the w - relatives .
we then require that are of class pends only on attributes that precede it in the order .
while this simple approach clearly ensures acyclicity , it is too limited to cover many important cases .
con - sider again our genetic model .
of a person depends on the genotype of her parents; thus , we have person p - chromosome depending directly on p - chromosome , which clearly violates the require - ments of our simple approach .
in this model , the appar - ent cyclicity at the attribute level is resolved at the level of individual objects , as a person cannot be his / her own an - cestor .
that is , the resolution of acyclicity relies on some prior knowledge that we have about the domain .
to allow our learning algorithm to deal with dependency models such as this we must allow the user to give our algorithm prior knowledge .
we allow the user to assert that certain slots
guaranteed that there is a partial ordering say that w
- relative for some is guaranteed acyclic if each of its components
i are guaranteed acyclic; i . e . , we are
such that if
is guaranteed acyclic .
is a parent of
which is green if w
we use this prior knowledge determine the legality of cer - tain dependency models .
we start by building a graph that describes the direct dependencies between the attributes .
in this graph , we have a yellow edge , we have an is guaranteed acyclic and red otherwise .
( note that there might be several edges , of different colors , between two attributes ) .
the intuition is that dependency along green edges relates objects that are or - dered by an acyclic order .
thus these edges by themselves or combined with intra - object dependencies ( yellow edges ) can - not cause a cyclic dependency .
we must take care with other dependencies , for which we do not have prior knowledge , as these might form a cycle .
this intuition suggests the follow - ing denition : a ( colored ) dependency graph is stratied if every cycle in the graph contains at least one green edge and no red edges .
is stratied , then for any skeleton
proposition 123 : if the colored dependency graph of
distribution over assignments to
for which the slots denes a coherent probability
are jointly acyclic ,
this notion of stratication generalizes the two special cases we considered above .
when we do not have any guaran - teed acyclic relations , all the edges in the dependency graph are colored either yellow or red .
thus , the graph is strati - ed if and only if it is acyclic .
in the genetics example , all the relations would be in .
thus , it sufces to check that dependencies within objects ( yellow edges ) are acyclic .
proposition 123 : stratication of a colored graph can be de - termined in time linear in the number of edges in the graph .
we omit the details of the algorithm for lack of space , but it relies on standard graph algorithms .
finally , we note that it
is easy to expand this denition of stratication for situations where our prior knowledge involves several sets of guaran - teed acyclic relations , each set with its own order ( e . g . , ob - jects on a grid with a north - south ordering and an east - west ordering ) .
we simply color the graph with several colors , and check that each cycle contains edges with exactly one color other than yellow , except for red .
123 evaluating different structures now that we know which structures are legal , we need to de - cide how to evaluate different structures in order to pick one that ts the data well .
we adapt bayesian model selection methods to our framework .
formally , we want to compute the posterior probability of a structure given an instantia - .
using bayes rule we have that .
this score is composed of two main parts : the prior probability of the structure , and the probability of the data assuming that structure .
the rst component is
, which denes a prior over structures .
we assume that the choice of structure is in - dependent of the skeleton , and thus .
in the context of bayesian networks , we often use a simple uniform prior over possible dependency structures .
unfortunately , this assumption does not work in our setting .
the problem is that there may be innitely many possible structures .
in our ge - netics example , a persons genotype can depend on the geno - type of his parents , or of his grandparents , or of his great - grandparents , etc .
a simple and natural solution penalizes
the second component is the marginal likelihood :
long indirect slot chains , by having
the sum of the lengths of the chains w appearing in
if we use a parameter independent dirichlet prior ( as above , this integral decomposes into a product of integrals each of which has a simple closed form solution .
( this is a sim - ple generalization of the ideas used in the bayesian score for proposition 123 : if
is a complete assignment , and
, is equal to
satises parameter independence and is dirichlet with hy - , the marginal
the marginal likelihood is a product of simple terms , each of which corresponds to a distribution .
moreover , the term for depends only on the hyperparameters " sufcient statistics c the marginal likelihood term is the dominant term in the probability of a structure .
it balances the complexity of the
" and the
" , then ,
is the gamma function .
structure with its t to the data .
this balance can be made explicitly via the asymptotic relation of the marginal likeli - hood to explicit penalization , such as the mdl score ( see , e . g . , ( heckerman , 123 ) ) .
finally , we note that the bayesian score requires that we assign a prior over parameter values for each possible struc - ture .
since there are many ( perhaps innitely many ) alter - native structures , this is a formidable task .
in the case of bayesian networks , there is a class of priors that can be de - scribed by a single network ( heckerman et al . , 123 ) .
these priors have the additional property of being structure equiva - lent , that is , they guarantee that the marginal likelihood is the same for structures that are , in some strong sense , equivalent .
these notions have not yet been dened for our richer struc - tures , so we defer the issue to future work .
instead , we simply assume that some simple dirichlet prior ( e . g . , a uniform one ) has been dened for each attribute and parent set .
123 structure search now that we have a test for determining whether a structure is legal , and a scoring function that allows us to evaluate dif - ferent structures , we need only provide a procedure for nd - ing legal high - scoring structures .
for bayesian networks , we know that this task is np - hard ( chickering , 123 ) .
as prm learning is at least as hard as bn learning ( a bn is simply a prm with one class and no relations ) , we cannot hope to nd an efcient procedure that always nds the highest scoring structure .
thus , we must resort to heuristic search .
the sim - plest such algorithm is greedy hill - climbing search , using our score as a metric .
we maintain our current candidate structure and iteratively improve it .
at each iteration , we consider a set of simple local transformations to that structure , score all of them , and pick the one with highest score .
we deal with local maxima using random restarts .
as in bayesian networks , the decomposability property of the score has signicant impact on the computational ef - ciency of the search algorithm .
first , we decompose the score into a sum of local scores corresponding to individual attributes and their parents .
now , if our search algorithm con - siders a modication to our current structure where the parent set of a single attribute is different , only the component will change .
thus , we need of the score associated with only reevaluate this particular component , leaving the others unchanged; this results in major computational savings .
there are two problems with this simple approach .
first , as discussed in the previous section , we have innitely many possible structures .
second , even the atomic steps of the search are expensive; the process of computing sufcient statistics requires expensive database operations .
even if we restrict the set of candidate structures at each step of the search , we cannot afford to do all the database operations nec - essary to evaluate all of them .
we propose a heuristic search algorithm that addresses both these issues .
at a high level , the algorithm proceeds , we have a set of potential parents .
we then do a standard structure search restricted to the space of structures in which .
the advantage of the parents of each this approach is that we can precompute the view correspond -
in phases .
at each phase
are in pot
for each attribute
; most of the expensive computations the joins and the aggregation required in the denition of the parents are precomputed in these views .
the suf - cient statistics for any subset of potential parents can easily be derived from this view .
the above construction , together with the decomposability of the score , allows the steps of the search ( say , greedy hill - climbing ) to done very efciently .
the success of this approach depends on the choice of the potential parents .
clearly , a wrong initial choice can result to poor structures .
following ( friedman et al . , 123 ) , which ex - amines a similar approach in the context of learning bayesian networks , we propose an iterative approach that starts with some structure ( possibly one where each attribute does not have any parents ) , and select the sets pot this structure .
we then apply the search procedure and get a new , higher scoring , structure .
we choose new potential par - ents based on this new structure and reiterate , stopping when no further improvement is made .
it remains only to discuss the choice of pot
different phases .
perhaps the simplest approach is to begin by .
in succes -
sive phases , pot would consist of all of pa as well as all attributes that are related to via slot chains .
of course , these new attributes would require
aggregation; we sidestep the issue by predening possible ag - gregates for each attribute .
to be the set of attributes in
this scheme expands the set of potential parents at each iteration .
however , it usually results in large set of poten - tial parents .
thus , we actually use a more rened algorithm if they seem to add that only adds parents to pot .
there are several reasonable ways value beyond pa of evaluating the additional value provided by new parents .
some of these are discussed in ( friedman et al . , 123 ) in the context of learning bayesian networks .
their results suggest that we should evaluate a new potential parent by measur - if we add the ing the change of score for the family of to its current parents .
we then choose the highest scoring of these , as well as the current parents , to be the new set of potential parents .
this approach allows us to signi - cantly reduce the size of the potential parent set , and thereby of the resulting view , while being unlikely to cause signicant degradation in the quality of the learned model .
123 implementation and experimental results we implemented our learning algorithm on top of the post - gres object - relational database management system .
all re - quired counts were obtained simply through database selec - tion queries , and cached to avoid performing the same query twice .
during the search process , we created temporary ma - terialized views corresponding to joins between different re - lations , and these views were then used for computing the
we tested our proposed learning algorithm on two do - mains , one real and one synthetic .
the two domains have very different characteristics .
the rst is a movie database that contains three relations : movie , actor and appears , which relates actors to movies in which they played
obtained from http : / / www - db . stanford . edu / pub / movies / doc . html
database contains about 123 movies and 123 actors .
while this database has a simple structure , it presents the kind of problems one often encounters when dealing with real data : missing values , large domains for attributes , and incon - sistent use of values .
the fact that our algorithm was able to deal with this kind of real - world problem is quite promis - ing .
our algorithm learned the model shown in figure 123 ( a ) .
this model is reasonable , and close to one that we would con - sider to be correct .
it learned that the genre of a movie depended on its decade and its lm process ( color , black & white , technicolor etc . ) and that the decade depended on its lm process .
it also learned an interesting dependency com - bining all three relations : the role - type played by an actor in a movie depends on the gender of the actor and the genre of
the second database , an articial genetic database similar to the example in this paper , presented quite different chal - lenges .
for one thing , the recursive nature of this domain allows arbitrarily complex joins to be dened .
in addition , the probabilistic model in this domain is fairly subtle .
each person has three relevant attributes p - chromosome , m - chromosome , and bloodtype all with the same domain and all related somehow to the same attributes of the persons mother and father .
the gold standard is the model used to generate the data; the structure of that model was shown ear - lier in figure 123
we trained our algorithm on datasets of var - ious sizes ranging up to 123
a data set of size of a family tree containing people , with an average of 123 blood tests per person .
we evaluated our algorithm on a test set of size 123 , 123
figure 123 ( b ) shows the log - likelihood of the test set for the learned models .
in most cases , our algorithm learned a model with the correct structure , and scored well .
however , in a small minority of cases , the algorithm got stuck in local maxima , learning a model with incorrect structure that scored quite poorly .
this can be seen in the scatter plots of figure 123 ( b ) which show that the median log - likelihood of the learned models is quite reasonable , but there are a few outliers .
standard techniques such as random restarts can be used to deal with local maxima .
123 discussion and conclusions in this paper , we dened a new statistical learning task : learn - ing probabilistic relational models from data .
we have shown that many of the ideas from bayesian network learning carry over to this new task .
however , we have also shown that it also raises many new challenges .
scaling these ideas to large databases is an important issue .
we believe that this can be achieved by a closer integration with the technology of database systems , including indices and query optimization .
furthermore , there has been a lot of recent work on extracting information from massive data sets , including work on nding frequently occurring combinations of values for attributes .
we believe that these ideas will help signicantly in the computation of sufcient statistics .
there are also several important possible extensions to this work .
perhaps the most obvious one is the treatment of miss - ing data and hidden variables .
we can extend standard tech - niques ( such as expectation maximization for missing data )
figure 123 : ( a ) the prm learned for the movie domain , a real - world database containing about 123 movies and 123 actors .
( b ) learning curve showing the generalization performance of prms learned in the genetic domain .
the - axis shows the databases size; the - axis shows log - likelihood of a test set of size 123 , 123
for each sample size , we show 123 independent learning experiments .
the curve shows median log - likelihood of the models as a function of the sample size .
to this task ( see ( koller and pfeffer , 123 ) for some prelim - inary work on related models . ) however , the complexity of inference on large databases with many missing values make the cost of a naive application of such algorithms prohibitive .
clearly , this domain calls both for new inference algorithms and for new learning algorithms that avoid repeated calls to inference over these very large problems .
even more interest - ing is the issue of automated discovery of hidden variables .
there are some preliminary answers to this question in the context of bayesian networks ( friedman , 123 ) , in the con - text of ilp ( lavrac and dzeroski , 123 ) , and very recently in the context of simple binary relations ( hofmann et al . , 123 ) .
combining these ideas and extending them to this more com - plex framework is a signicant and interesting challenge .
another direction extends the class of models we consider .
here , we assumed that the relational structure is specied be - fore the probabilistic attribute values are determined .
a richer class of prms ( e . g . , that of ( koller and pfeffer , 123 ) ) would allow probabilities over the structure of the model; for ex - ample : uncertainty over the set of objects in the model , e . g . , the number of children a couple has , or over the relations be - tween objects , e . g . , whose is the blood that was found on a crime scene .
ultimately , we would want these techniques to help us automatically discover interesting entities and rela - tionships that hold in the world .
nir friedman was supported by a grant from the michael sacher trust .
lise getoor , daphne koller , and avi pfef - fer were supported by onr contract n123 - 123 - c - 123 un - der darpas hpkb program , by onr grant n123 - 123 - 123 - 123 , by the aro under the muri program integrated ap - proach to intelligent systems , and by the generosity of the sloan foundation and of the powell foundation .
