in typical classication tasks , we seek a function which assigns a label to a sin - gle object .
kernel - based approaches , such as support vector machines ( svms ) , which maximize the margin of condence of the classier , are the method of choice for many such tasks .
their popularity stems both from the ability to use high - dimensional feature spaces , and from their strong theoretical guaran - tees .
however , many real - world tasks involve sequential , spatial , or structured data , where multiple labels must be assigned .
existing kernel - based methods ig - nore structure in the problem , assigning labels independently to each object , los - ing much useful information .
conversely , probabilistic graphical models , such as markov networks , can represent correlations between labels , by exploiting problem structure , but cannot handle high - dimensional feature spaces , and lack strong theoretical generalization guarantees .
in this paper , we present a new framework that combines the advantages of both approaches : maximum mar - gin markov ( m123 ) networks incorporate both kernels , which efciently deal with high - dimensional features , and the ability to capture correlations in structured data .
we present an efcient algorithm for learning m123 networks based on a compact quadratic program formulation .
we provide a new theoretical bound for generalization in structured domains .
experiments on the task of handwrit - ten character recognition and collective hypertext classication demonstrate very signicant gains over previous approaches .
in supervised classication , our goal is to classify instances into some set of discrete cat - egories .
recently , support vector machines ( svms ) have demonstrated impressive suc - cesses on a broad range of tasks , including document categorization , character recognition , image classication , and many more .
svms owe a great part of their success to their ability to use kernels , allowing the classier to exploit a very high - dimensional ( possibly even innite - dimensional ) feature space .
in addition to their empirical success , svms are also appealing due to the existence of strong generalization guarantees , derived from the margin - maximizing properties of the learning algorithm .
however , many supervised learning tasks exhibit much richer structure than a simple cat - egorization of instances into one of a small number of classes .
in some cases , we might need to label a set of inter - related instances .
for example : optical character recognition ( ocr ) or part - of - speech tagging both involve labeling an entire sequence of elements into some number of classes; image segmentation involves labeling all of the pixels in an im - age; and collective webpage classication involves labeling an entire set of interlinked webpages .
in other cases , we might want to label an instance ( e . g . , a news article ) with multiple non - exclusive labels .
in both of these cases , we need to assign multiple labels si - multaneously , leading to a classication problem that has an exponentially large set of joint
labels .
a common solution is to treat such problems as a set of independent classication tasks , dealing with each instance in isolation .
however , it is well - known that this approach fails to exploit signicant amounts of correlation information ( 123 ) .
an alternative approach is offered by the probabilistic framework , and specically by probabilistic graphical models .
in this case , we can dene and learn a joint probabilistic model over the set of label variables .
for example , we can learn a hidden markov model , or a conditional random eld ( crf ) ( 123 ) over the labels and features of a sequence , and then use a probabilistic inference algorithm ( such as the viterbi algorithm ) to classify these instances collectively , nding the most likely joint assignment to all of the labels simultane - ously .
this approach has the advantage of exploiting the correlations between the different labels , often resulting in signicant improvements in accuracy over approaches that classify instances independently ( 123 , 123 ) .
the use of graphical models also allows problem structure to be exploited very effectively .
unfortunately , even probabilistic graphical models that are trained discriminatively do not usually achieve the same level of generalization accuracy as svms , especially when kernel features are used .
moreover , they are not ( yet ) associated with generalization bounds comparable to those of margin - based classiers .
clearly , the frameworks of kernel - based and probabilistic classiers offer complemen - tary strengths and weaknesses .
in this paper , we present maximum margin markov ( m123 ) networks , which unify the two frameworks , and combine the advantages of both .
our ap - proach denes a log - linear markov network over a set of label variables ( e . g . , the labels of the letters in an ocr problem ) ; this network allows us to represent the correlations be - tween these label variables .
we then dene a margin - based optimization problem for the parameters of this model .
for markov networks that can be triangulated tractably , the re - sulting quadratic program ( qp ) has an equivalent polynomial - size formulation ( e . g . , linear for sequences ) that allows a very effective solution .
by contrast , previous margin - based formulations for sequence labeling ( 123 , 123 ) require an exponential number of constraints .
for non - triangulated networks , we provide an approximate reformulation based on the relax - ation used by belief propagation algorithms ( 123 , 123 ) .
importantly , the resulting qp supports the same kernel trick as do svms , allowing probabilistic graphical models to inherit the important benets of kernels .
we also show a generalization bound for such margin - based classiers .
unlike previous results ( 123 ) , our bound grows logarithmically rather than lin - early with the number of label variables .
our experimental results on character recognition and on hypertext classication , demonstrate dramatic improvements in accuracy over both kernel - based instance - by - instance classication and probabilistic models .
123 structure in classication problems in supervised classication , the task is to learn a function h : x 123 ! y from a set of m i . i . d .
instances s = f ( x ( i ) ; y ( i ) = t ( x ( i ) ) ) gm i=123 , drawn from a x ed distribution dx ( cid : 123 ) y .
the classication function h is typically selected from some parametric family h .
a common choice is the linear family : given n real - valued basis functions fj : x ( cid : 123 ) y 123 ! ir , a hypothesis hw 123 h is dened by a set of n coefcients wj such that :
hw ( x ) = arg max
wjfj ( x; y ) = arg max
w>f ( x; y ) ;
where the f ( x; y ) are features or basis functions .
the most common classication setting single - label classication takes y = fy123; : : : ; ykg .
in this paper , we consider the much more general setting of multi - label classication , where y = y123 ( cid : 123 ) : : : ( cid : 123 ) yl with yi = fy123; : : : ; ykg .
in an ocr task , for example , each yi is a character , while y is a full word .
in a webpage collective classica - tion task ( 123 ) , each yi is a webpage label , whereas y is a joint label for an entire website .
in these cases , the number of possible assignments to y is exponential in the number of labels l .
thus , both representing the basis functions fj ( x; y ) in ( 123 ) and computing the maximization arg maxy are infeasible .
an alternative approach is based on the framework of probabilistic graphical models .
in this case , the model denes ( directly or indirectly ) a conditional distribution p ( y j x ) .
we can then select the label arg maxy p ( y j x ) .
the advantage of the probabilistic framework is that it can exploit sparseness in the correlations between labels yi .
for example , in the ocr task , we might use a markov model , where yi is conditionally independent of the rest of the labels given yi ( cid : 123 ) 123; yi+123
we can encode this structure using a markov network .
in this paper , purely for sim - plicity of presentation , we focus on the case of pairwise interactions between labels .
we emphasize that our results extend easily to the general case .
a pairwise markov network is dened as a graph g = ( y; e ) , where each edge ( i; j ) is associated with a potential function ij ( x; yi; yj ) .
the network encodes a joint conditional probability distribution as p ( y j x ) / q ( i;j ) 123e ij ( x; yi; yj ) .
these networks exploit the interaction structure to parameterize a classier very compactly .
in many cases ( e . g . , tree - structured networks ) , we can use effective dynamic programming algorithms ( such as the viterbi algorithm ) to nd the highest probability label y; in others , we can use approximate inference algorithms that also exploit the structure ( 123 ) .
the markov network distribution is simply a log - linear model , with the pairwise potential ij ( x; yi; yj ) representing ( in log - space ) a sum of basis functions over x; yi; yj .
we can therefore parameterize such a model using a set of pairwise basis functions f ( x; yi; yj ) for ( i; j ) 123 e .
we assume for simplicity of notation that all edges in the graph denote the same type of interaction , so that we can dene a set of features
fk ( x; y ) = x
fk ( x; yi; yj ) :
k=123 wkfk ( x; yi; yj ) ) =
the network potentials are then ij ( x; yi; yj ) = exp ( pn exp ( cid : 123 ) w>f ( x; yi; yj ) ( cid : 123 ) : the parameters w in a log - linear model can be trained to t the data , typically by maxi - mizing the likelihood or conditional likelihood ( e . g . , ( 123 , 123 ) ) .
this paper presents an algo - rithm for selecting w that maximize the margin , gaining all of the advantages of svms .
123 margin - based structured classication for a single - label binary classication problem , support vector machines ( svms ) ( 123 ) pro - vide an effective method of learning a maximum - margin decision boundary .
for single - label multi - class classication , crammer and singer ( 123 ) provide a natural extension of this framework by maximizing the margin ( cid : 123 ) subject to constraints :
maximize ( cid : 123 ) s : t :
jjwjj ( cid : 123 ) 123; w> ( cid : 123 ) fx ( y ) ( cid : 123 ) ( cid : 123 ) ; 123 x 123 s; 123y 123= t ( x ) ;
where ( cid : 123 ) fx ( y ) = f ( x; t ( x ) ) ( cid : 123 ) f ( x; y ) .
the constraints in this formulation ensure that arg maxy w>f ( x; y ) = t ( x ) .
maximizing ( cid : 123 ) magnies the difference between the value of the true label and the best runner - up , increasing the condence of the classication .
in structured problems , where we are predicting multiple labels , the loss function is usu - ally not simple 123 - 123 loss i ( arg maxy w>fx ( y ) = t ( x ) ) , but per - label loss , such as the proportion of incorrect labels predicted .
in order to extend the margin - based framework to the multi - label setting , we must generalize the notion of margin to take into account the number of labels in y that are misclassied .
in particular , we would like the margin between t ( x ) and y to scale linearly with the number of wrong labels in y , ( cid : 123 ) tx ( y ) : jjwjj ( cid : 123 ) 123; w> ( cid : 123 ) fx ( y ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) tx ( y ) ; 123x 123 s; 123 y;
maximize ( cid : 123 ) s : t :
where ( cid : 123 ) tx ( y ) = p l transformation to eliminate ( cid : 123 ) , we get a quadratic program ( qp ) :
i=123 ( cid : 123 ) tx ( yi ) and ( cid : 123 ) tx ( yi ) ( cid : 123 ) i ( yi 123= ( t ( x ) ) i ) .
now , using a standard
s : t : w> ( cid : 123 ) fx ( y ) ( cid : 123 ) ( cid : 123 ) tx ( y ) ; 123x 123 s; 123 y :
unfortunately , the data is often not separable by a hyperplane dened over the space of the given set of features .
in such cases , we need to introduce slack variables ( cid : 123 ) x to allow
some constraints to be violated .
we can now present the complete form of our optimization problem , as well as the equivalent dual problem ( 123 ) :
primal formulation ( 123 )
jjwjj123 + cxx
s : t : w> ( cid : 123 ) fx ( y ) ( cid : 123 ) ( cid : 123 ) tx ( y ) ( cid : 123 ) ( cid : 123 ) x; 123x; y :
( cid : 123 ) x ( y ) = c; 123x; ( cid : 123 ) x ( y ) ( cid : 123 ) 123 ; 123x; y :
( note : for each x , we add an extra dual variable ( cid : 123 ) x ( t ( x ) ) , with no effect on the solution . )
123 exploiting structure in m123 networks unfortunately , both the number of constraints in the primal qp in ( 123 ) , and the number of variables in the dual qp in ( 123 ) are exponential in the number of labels l .
in this section , we present an equivalent , polynomially - sized , formulation .
our main insight is that the variables ( cid : 123 ) x ( y ) in the dual formulation ( 123 ) can be interpreted as a density function over y conditional on x , as py ( cid : 123 ) x ( y ) = c and ( cid : 123 ) x ( y ) ( cid : 123 ) 123
the dual objective is a function of expectations of ( cid : 123 ) tx ( y ) and ( cid : 123 ) fx ( y ) with respect to ( cid : 123 ) x ( y ) .
since both ( cid : 123 ) tx ( y ) = pi ( cid : 123 ) tx ( yi ) and ( cid : 123 ) fx ( y ) = p ( i;j ) ( cid : 123 ) fx ( yi; yj ) are sums of functions over nodes and edges , we only need node and edge marginals of the measure ( cid : 123 ) x ( y ) to compute their expectations .
we dene the marginal dual variables as follows :
( cid : 123 ) x ( yi; yj ) = py ( cid : 123 ) ( yi;yj ) ( cid : 123 ) x ( y ) ; 123 ( i; j ) 123 e; 123yi; yj; 123 x;
123 i; 123yi; 123 x;
= py ( cid : 123 ) ( yi ) ( cid : 123 ) x ( y ) ;
where y ( cid : 123 ) ( yi; yj ) denotes a full assignment y consistent with partial assignment yi; yj .
now we can reformulate our entire qp ( 123 ) in terms of these dual variables .
consider , for
example , the rst term in the objective function :
( cid : 123 ) x ( y ) ( cid : 123 ) tx ( y ) =xy xi
the decomposition of the second term in the objective uses edge marginals ( cid : 123 ) x ( yi; yj ) .
in order to produce an equivalent qp , however , we must also ensure that the dual variables ( cid : 123 ) x ( yi; yj ) ; ( cid : 123 ) x ( yi ) are the marginals resulting from a legal density ( cid : 123 ) ( y ) ; that is , that they belong to the marginal polytope ( 123 ) .
in particular , we must enforce consistency between the pairwise and singleton marginals ( and hence between overlapping pairwise marginals ) :
( cid : 123 ) x ( yi; yj ) = ( cid : 123 ) x ( yj ) ; 123yj; 123 ( i; j ) 123 e; 123x :
if the markov network for our basis functions is a forest ( singly connected ) , these con - straints are equivalent to the requirement that the ( cid : 123 ) variables arise from a density .
there - fore , the following factored dual qp is equivalent to the original dual qp :
( cid : 123 ) x ( yi; yj ) ( cid : 123 ) ^x ( yr; ys ) fx ( yi; yj ) >f^x ( yr; ys ) ;
max xx xi;yi
( cid : 123 ) x ( yi; yj ) = ( cid : 123 ) x ( yj ) ; xyi
123 xx;^x x ( i;j )
similarly , the original primal can be factored as follows :
( cid : 123 ) x ( yi ) = c; ( cid : 123 ) x ( yi; yj ) ( cid : 123 ) 123 :
jjwjj123 + cxx xi
( cid : 123 ) x;i + cxx x ( i;j )
s : t : w> ( cid : 123 ) fx ( yi; yj ) + x ( i123;j ) : i123=i
mx;i123 ( yj ) + x ( j 123;i ) : j 123=j
mx;j 123 ( yi ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x;ij;
mx;j ( yi ) ( cid : 123 ) ( cid : 123 ) tx ( yi ) ( cid : 123 ) ( cid : 123 ) x;i;
( cid : 123 ) x;ij ( cid : 123 ) 123; ( cid : 123 ) x;i ( cid : 123 ) 123 :
the solution to the factored dual gives us : w = px p ( i;j ) pyi;yj theorem 123 if for each x the edges e form a forest , then a set of weights w will be optimal for the qp in ( 123 ) if and only if it is optimal for the factored qp in ( 123 ) .
( cid : 123 ) x ( yi; yj ) ( cid : 123 ) fx ( yi; yj ) .
if the underlying markov net is not a forest , then the constraints in ( 123 ) are not sufcient to enforce the fact that the ( cid : 123 ) s are in the marginal polytope .
we can address this problem by triangulating the graph , and introducing new ( cid : 123 ) lp variables that now span larger subsets of yis .
for example , if our graph is a 123 - cycle y123y123y123y123y123 , we might triangulate the graph by adding an arc y123y123 , and introducing ( cid : 123 ) variables over joint instantiations of the cliques y123; y123; y123 and y123; y123; y123
these new ( cid : 123 ) variables are used in linear equalities that constrain the original ( cid : 123 ) variables to be consistent with a density .
the ( cid : 123 ) variables appear only in the constraints; they do not add any new basis functions nor change the objective function .
the number of constraints introduced is exponential in the number of variables in the new cliques .
nevertheless , in many classication problems , such as sequences and other graphs with low tree - width ( 123 ) , the extended qp can be solved efciently .
unfortunately , triangulation is not feasible in highly connected problems .
however , we can still solve the qp in ( 123 ) dened by an untriangulated graph with loops .
such a proce - dure , which enforces only local consistency of marginals , optimizes our objective only over a relaxation of the marginal polytope .
in this way , our approximation is analogous to the approximate belief propagation ( bp ) algorithm for inference in graphical models ( 123 ) .
in fact , bp makes an additional approximation , using not only the relaxed marginal polytope but also an approximate objective ( bethe free - energy ) ( 123 ) .
although the approximate qp does not offer the theoretical guarantee in theorem 123 , the solutions are often very accu - rate in practice , as we demonstrate below .
as with svms ( 123 ) , the factored dual formulation in ( 123 ) uses only dot products between basis functions .
this allows us to use a kernel to dene very large ( and even innite ) set of features .
in particular , we dene our basis functions by fx ( yi; yj ) = ( cid : 123 ) ( yi; yj ) ( cid : 123 ) ij ( x ) , i . e . , the product of a selector function ( cid : 123 ) ( yi; yj ) with a possibly innite feature vector ( cid : 123 ) ij ( x ) .
for example , in the ocr task , ( cid : 123 ) ( yi; yj ) could be an indicator function over the class of two adjacent characters i and j , and ( cid : 123 ) ij ( x ) could be an rbf kernel on the images of these two characters .
the operation fx ( yi; yj ) >f^x ( yr; ys ) used in the objective function of the factored dual qp is now ( cid : 123 ) ( yi; yj ) ( cid : 123 ) ( yr; ys ) k ( cid : 123 ) ( x; i; j; ^x; r; s ) , where k ( cid : 123 ) ( x; i; j; ^x; r; s ) = ( cid : 123 ) ij ( x ) ( cid : 123 ) ( cid : 123 ) rs ( x ) is the kernel function for the feature ( cid : 123 ) .
even for some very complex functions ( cid : 123 ) , the dot - product required to compute k ( cid : 123 ) can be executed efciently ( 123 ) .
123 smo learning of m123 networks although the number of variables and constraints in the factored dual in ( 123 ) is polynomial in the size of the data , the number of coefcients in the quadratic term ( kernel matrix ) in the objective is quadratic in the number of examples and edges in the network .
unfortunately , this matrix is often too large for standard qp solvers .
instead , we use a coordinate descent method analogous to the sequential minimal optimization ( smo ) used for svms ( 123 ) .
let us begin by considering the original dual problem ( 123 ) .
the smo approach solves this qp by analytically optimizing two - variable subproblems .
recall that py ( cid : 123 ) x ( y ) = c .
we can therefore take any two variables ( cid : 123 ) x ( y123 ) , ( cid : 123 ) x ( y123 ) and move weight from one to the other , keeping the values of all other variables x ed .
more precisely , we optimize for
clearly , however , we cannot perform this optimization in terms of the original dual , which is exponentially large .
fortunately , we can perform precisely the same optimization in terms of the marginal dual variables .
let ( cid : 123 ) = ( cid : 123 ) 123 consider a dual variable ( cid : 123 ) x ( yi; yj ) .
it is easy to see that a change from ( cid : 123 ) x ( y123 ) ; ( cid : 123 ) x ( y123 ) to
x ( y123 ) ( cid : 123 ) ( cid : 123 ) x ( y123 ) = ( cid : 123 ) x ( y123 ) ( cid : 123 ) ( cid : 123 ) 123
x ( y123 ) = ( cid : 123 ) x ( y123 ) + ( cid : 123 ) x ( y123 ) .
x ( y123 ) such that ( cid : 123 ) 123
x ( y123 ) + ( cid : 123 ) 123
x ( y123 ) has the following effect on ( cid : 123 ) x ( yi; yj ) : i ; yj = y123
x ( yi; yj ) = ( cid : 123 ) x ( yi; yj ) + ( cid : 123 ) i ( yi = y123
j ) ( cid : 123 ) ( cid : 123 ) i ( yi = y123
i ; yj = y123
we can solve the one - variable quadratic subproblem in ( cid : 123 ) analytically and update the ap - propriate ( cid : 123 ) variables .
we use inference in the network to test for optimality of the current solution ( the kkt conditions ( 123 ) ) and use violations from optimality as a heuristic to select the next pair y123; y123
we omit details for lack of space .
123 generalization bound in this section , we show a generalization bound for the task of multi - label classication that allows us to relate the error rate on the training set to the generalization error .
as we shall see , this bound is signicantly stronger than previous bounds for this problem .
our goal in multi - label classication is to maximize the number of correctly classi - ed labels .
thus an appropriate error function is the average per - label loss l ( w; x ) = l ( cid : 123 ) tx ( arg maxy w>fx ( y ) ) .
as in other generalization bounds for margin - based classi - ers , we relate the generalization error to the margin of the classier .
in sec .
123 , we dene the notion of per - label margin , which grows with the number of mistakes between the cor - rect assignment and the best runner - up .
we can now dene a ( cid : 123 ) - margin per - label loss :
l ( cid : 123 ) ( w; x ) = supz : jz ( y ) ( cid : 123 ) w>fx ( y ) j ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) tx ( y ) ; 123y
l ( cid : 123 ) tx ( arg maxy z ( y ) ) :
this loss function measures the worst per - label loss on x made by any classier z which is perturbed from w>fx by at most a ( cid : 123 ) - margin per - label .
we can now prove that the gen - eralization accuracy of any classier is bounded by its expected ( cid : 123 ) - margin per - label loss on the training data , plus a term that grows inversely with the margin . intuitively , the rst term corresponds to the bias , as margin ( cid : 123 ) decreases the complexity of our hypothesis class by considering a ( cid : 123 ) - per - label margin ball around w>fx and selecting one ( the worst ) classier within this ball .
as ( cid : 123 ) shrinks , our hypothesis class becomes more complex , and the rst term becomes smaller , but at the cost of increasing the second term , which intuitively cor - responds to the variance .
thus , the result provides a bound to the generalization error that trades off the effective complexity of the hypothesis space with the training error .
theorem 123 if the edge features have bounded 123 - norm , max ( i;j ) ;yi;yj kfx ( yi; yj ) k123 ( cid : 123 ) redge , then for a family of hyperplanes parameterized by w , and any ( cid : 123 ) > 123 , there exists a constant k such that for any ( cid : 123 ) > 123 per - label margin , and m > 123 samples , the per - label loss is bounded by :
( ln m + ln l + ln q + ln k ) + ln
exl ( w; x ) ( cid : 123 ) esl ( cid : 123 ) ( w; x ) +vuut
m " r123
with probability at least 123 ( cid : 123 ) ( cid : 123 ) , where q = maxi jf ( i; j ) 123 egj is the maximum edge degree in the network , k is the number of classes in a label , and l is the number of labels .
unfortunately , we omit the proof due to lack of space .
( see a longer version of the paper at http : / / cs . stanford . edu / btaskar / . ) the proof uses a covering number argument analogous to previous results in svms ( 123 ) .
however we propose a novel method for covering structured problems by constructing a cover to the loss function from a cover of the individual edge basis function differences ( cid : 123 ) fx ( yi; yj ) .
this new type of cover is polynomial in the number of edges , yielding signicant improvements in the bound .
specically , our bound has a logarithmic dependence on the number of labels ( ln l ) and depends only on the 123 - norm of the basis functions per - edge ( redge ) .
this is a signicant gain over the previous result of collins ( 123 ) which has linear dependence on the number of labels ( l ) , and depends on the joint 123 - norm of all of the features ( which is ( cid : 123 ) lredge , unless each sequence is normalized separately , which is often ineffective in practice ) .
finally , note m = o ( 123 ) ( for example , in ocr , if the number of instances is at least a constant that if l times the length of a word ) , then our bound is independent of the number of labels l .
such a result was , until now , an open problem for margin - based sequence classication ( 123 ) .
we evaluate our approach on two very different tasks : a sequence model for handwriting recognition and an arbitrary topology markov network for hypertext classication .
figure 123 : ( a ) 123 example words from the ocr data set; ( b ) ocr : average per - character test error for logistic regression , crfs , multiclass svms , and m123ns , using linear , quadratic , and cubic kernels; ( c ) hypertext : test error for multiclass svms , rmns and m123ns , by school and average .
handwriting recognition .
we selected a subset of ( cid : 123 ) 123 handwritten words , with av - erage length of ( cid : 123 ) 123 characters , from 123 human subjects , from the data set collected by kassel ( 123 ) .
each word was divided into characters , each character was rasterized into an image of 123 by 123 binary pixels .
( see fig .
123 ( a ) . ) in our framework , the image for each word corresponds to x , a label of an individual character to yi , and a labeling for a complete word to y .
each label yi takes values from one of 123 classes fa; : : : ; zg .
the data set is divided into 123 folds of ( cid : 123 ) 123 training and ( cid : 123 ) 123 testing examples .
the accuracy results , summarized in fig .
123 ( b ) , are averages over the 123 folds .
we im - plemented a selection of state - of - the - art classication algorithms : independent label ap - proaches , which do not consider the correlation between neighboring characters logistic regression , multi - class svms as described in ( 123 ) , and one - against - all svms ( whose perfor - mance was slightly lower than multi - class svms ) ; and sequence approaches crfs , and our proposed m123 networks .
logistic regression and crfs are both trained by maximiz - ing the conditional likelihood of the labels given the features , using a zero - mean diagonal gaussian prior over the parameters , with a standard deviation between 123 and 123
the other methods are trained by margin maximization .
our features for each label yi are the corre - sponding image of ith character .
for the sequence approaches ( crfs and m123 ) , we used an indicator basis function to represent the correlation between yi and yi+123
for margin - based methods ( svms and m123 ) , we were able to use kernels ( both quadratic and cubic were eval - uated ) to increase the dimensionality of the feature space .
using these high - dimensional feature spaces in crfs is not feasible because of the enormous number of parameters .
123 ( b ) shows two types of gains in accuracy : first , by using kernels , margin - based methods achieve a very signicant gain over the respective likelihood maximizing methods .
second , by using sequences , we obtain another signicant gain in accuracy .
interestingly , the error rate of our method using linear features is 123% lower than that of crfs , and about the same as multi - class svms with cubic kernels .
once we use cubic kernels our error rate is 123% lower than crfs and about 123% lower than the best previous approach .
for comparison , the previously published results , although using a different setup ( e . g . , a larger training set ) , are about comparable to those of multiclass svms .
hypertext .
we also tested our approach on collective hypertext classication , using the data set in ( 123 ) , which contains web pages from four different computer science depart - ments .
each page is labeled as one of course , faculty , student , project , other .
in all of our experiments , we learn a model from three schools , and test on the remaining school .
the text content of the web page and anchor text of incoming links is represented using a set of binary attributes that indicate the presence of different words .
the baseline model is a simple linear multi - class svm that uses only words to predict the category of the page .
the second model is a relational markov network ( rmn ) of taskar et al .
( 123 ) , which in addi - tion to word - label dependence , has an edge with a potential over the labels of two pages that are hyper - linked to each other .
this model denes a markov network over each web site that was trained to maximize the conditional probability of the labels given the words
and the links .
the third model is a m123 net with the same features but trained by maximizing the margin using the relaxed dual formulation and loopy bp for inference .
123 ( c ) shows a gain in accuracy from svms to rmns by using the correlations between labels of linked web pages , and a very signicant additional gain by using maximum margin training .
the error rate of m123ns is 123% lower than that of rmns , and 123% lower than we present a discriminative framework for labeling and segmentation of structured data such as sequences , images , etc .
our approach seamlessly integrates state - of - the - art kernel methods developed for classication of independent instances with the rich language of graphical models that can exploit the structure of complex data .
in our experiments with the ocr task , for example , our sequence model signicantly outperforms other approaches by incorporating high - dimensional decision boundaries of polynomial kernels over charac - ter images while capturing correlations between consecutive characters .
we construct our models by solving a convex quadratic program that maximizes the per - label margin .
al - though the number of variables and constraints of our qp formulation is polynomial in the example size ( e . g . , sequence length ) , we also address its quadratic growth using an effec - tive optimization procedure inspired by smo .
we provide theoretical guarantees on the average per - label generalization error of our models in terms of the training set margin .
our generalization bound signicantly tightens previous results of collins ( 123 ) and suggests possibilities for analyzing per - label generalization properties of graphical models .
