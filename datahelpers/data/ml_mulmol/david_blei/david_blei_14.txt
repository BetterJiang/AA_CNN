probabilistic topic models are a popular tool for the unsupervised analysis of text , providing both a predictive model of future text and a latent topic representation of the corpus .
practitioners typically assume that the latent space is semantically meaningful .
it is used to check models , summarize the corpus , and guide explo - ration of its contents .
however , whether the latent space is interpretable is in need of quantitative evaluation .
in this paper , we present new quantitative methods for measuring semantic meaning in inferred topics .
we back these measures with large - scale user studies , showing that they capture aspects of the model that are undetected by previous measures of model quality based on held - out likelihood .
surprisingly , topic models which perform better on held - out likelihood may infer less semantically meaningful topics .
probabilistic topic models have become popular tools for the unsupervised analysis of large document collections ( 123 ) .
these models posit a set of latent topics , multinomial distributions over words , and assume that each document can be described as a mixture of these topics .
with algorithms for fast approxiate posterior inference , we can use topic models to discover both the topics and an assignment of topics to documents from a collection of documents .
( see figure 123 ) these modeling assumptions are useful in the sense that , empirically , they lead to good models of documents .
they also anecdotally lead to semantically meaningful decompositions of them : topics tend to place high probability on words that represent concepts , and documents are represented as expressions of those concepts .
perusing the inferred topics is effective for model verication and for ensuring that the model is capturing the practitioners intuitions about the documents .
moreover , producing a human - interpretable decomposition of the texts can be a goal in itself , as when browsing or summarizing a large collection of documents .
in this spirit , much of the literature comparing different topic models presents examples of topics and examples of document - topic assignments to help understand a models mechanics .
topics also can help users discover new content via corpus exploration ( 123 ) .
the presentation of these topics serves , either explicitly or implicitly , as a qualitative evaluation of the latent space , but there is no explicit quantitative evaluation of them .
instead , researchers employ a variety of metrics of model t , such as perplexity or held - out likelihood .
such measures are useful for evaluating the predictive model , but do not address the more explatory goals of topic modeling .
work done while at princeton university .
( b ) document assignments to topics
figure 123 : the latent space of a topic model consists of topics , which are distributions over words , and a distribution over these topics for each document .
on the left are three topics from a fty topic lda model trained on articles from the new york times .
on the right is a simplex depicting the distribution over topics associated with seven documents .
the line from each documents title shows the documents position in the
in this paper , we present a method for measuring the interpretatability of a topic model .
we devise two human evaluation tasks to explicitly evaluate both the quality of the topics inferred by the model and how well the model assigns topics to documents .
the rst , word intrusion , measures how semantically cohesive the topics inferred by a model are and tests whether topics correspond to natural groupings for humans .
the second , topic intrusion , measures how well a topic models decomposition of a document as a mixture of topics agrees with human associations of topics with a document .
we report the results of a large - scale human study of these tasks , varying both modeling assumptions and number of topics .
we show that these tasks capture aspects of topic models not measured by existing metrics andsurprisinglymodels which achieve better predictive perplexity often have less interpretable latent spaces .
123 topic models and their evaluations
topic models posit that each document is expressed as a mixture of topics .
these topic proportions are drawn once per document , and the topics are shared across the corpus .
in this paper we will consider topic models that make different assumptions about the topic proportions .
probabilistic latent semantic indexing ( plsi ) ( 123 ) makes no assumptions about the document topic distribution , treating it as a distinct parameter for each document .
latent dirichlet allocation ( lda ) ( 123 ) and the correlated topic model ( ctm ) ( 123 ) treat each documents topic assignment as a multinomial random variable drawn from a symmetric dirichlet and logistic normal prior , respectively .
while the models make different assumptions , inference algorithms for all of these topic models build the same type of latent space : a collection of topics for the corpus and a collection of topic proportions for each of its documents .
while this common latent space has explored for over two decades , its interpretability remains unmeasured .
pay no attention to the latent space behind the model
although we focus on probabilistic topic models , the eld began in earnest with latent semantic analysis ( lsa ) ( 123 ) .
lsa , the basis of plsis probabilistic formulation , uses linear algebra to decom - pose a corpus into its constituent themes .
because lsa originated in the psychology community , early evaluations focused on replicating human performance or judgments using lsa : matching performance on standardized tests , comparing sense distinctions , and matching intuitions about synonymy ( these results are reviewed in ( 123 ) ) .
in information retrieval , where lsa is known as latent semantic indexing ( lsi ) ( 123 ) , it is able to match queries to documents , match experts to areas of expertise , and even generalize across languages given a parallel corpus ( 123 ) .
computer , technology , system , service , site , phone , internet , machineplay , lm , movie , theater , production , star , director , stagesell , sale , store , product , business , advertising , market , consumertopic 123topic 123topic 123forget the bootleg , just download the movie legallymultiplex heralded as linchpin to growththe shape of cinema , transformed at the click of a mousea peaceful crew puts muppets where its mouth isstock trades : a better deal for investors isn ' t simplethe three big internet portals begin to distinguish among themselves as shopping mallsred light , green light : a 123 - tone l . e . d .
to simplify screenstopic 123topic 123topic 123 the reticence to look under the hood of these models has persisted even as models have moved from psychology into computer science with the development of plsi and lda .
models either use measures based on held - out likelihood ( 123 , 123 ) or an external task that is independent of the topic space such as sentiment detection ( 123 ) or information retrieval ( 123 ) .
this is true even for models engineered to have semantically coherent topics ( 123 ) .
for models that use held - out likelihood , wallach et al .
( 123 ) provide a summary of evaluation techniques .
these metrics borrow tools from the language modeling community to measure how well the information learned from a corpus applies to unseen documents .
these metrics generalize easily and allow for likelihood - based comparisons of different models or selection of model parameters such as the number of topics .
however , this adaptability comes at a cost : these methods only measure the probability of observations; the internal representation of the models is ignored .
grifths et al .
( 123 ) is an important exception to the trend of using external tasks or held - out likelihood .
they showed that the number of topics a word appears in correlates with how many distinct senses it has and reproduced many of the metrics used in the psychological community based on human performance .
however , this is still not a deep analysis of the structure of the latent space , as it does not examine the structure of the topics themselves .
we emphasize that not measuring the internal representation of topic models is at odds with their presentation and development .
most topic modeling papers display qualitative assessments of the inferred topics or simply assert that topics are semantically meaningful , and practitioners use topics for model checking during the development process .
hall et al .
( 123 ) , for example , used latent topics deemed historically relevant to explore themes in the scientic literature .
even in production environments , topics are presented as themes : rexa ( http : / / rexa . info ) , a scholarly publication search engine , displays the topics associated with documents .
this implicit notion that topics have semantic meaning for users has even motivated work that attempts to automatically label topics ( 123 ) .
our goal is to measure the success of interpreting topic models across number of topics and modeling
123 using human judgments to examine the topics
although there appears to be a longstanding assumption that the latent space discovered by topic models is meaningful and useful , evaluating such assumptions is difcult because discovering topics is an unsupervised process .
there is no gold - standard list of topics to compare against for every corpus .
thus , evaluating the latent space of topic models requires us to gather exogenous data .
in this section we propose two tasks that create a formal setting where humans can evaluate the two components of the latent space of a topic model .
the rst component is the makeup of the topics .
we develop a task to evaluate whether a topic has human - identiable semantic coherence .
this task is called word intrusion , as subjects must identify a spurious word inserted into a topic .
the second task tests whether the association between a document and a topic makes sense .
we call this task topic intrusion , as the subject must identify a topic that was not associated with the document by the
123 word intrusion
to measure the coherence of these topics , we develop the word intrusion task; this task involves evaluating the latent space presented in figure 123 ( a ) .
in the word intrusion task , the subject is presented with six randomly ordered words .
the task of the user is to nd the word which is out of place or does not belong with the others , i . e . , the intruder .
figure 123 shows how this task is presented to users .
when the set of words minus the intruder makes sense together , then the subject should easily identify the intruder .
for example , most people readily identify apple as the intruding word in the set ( dog , cat , horse , apple , pig , cow ) because the remaining words , ( dog , cat , horse , pig , cow ) make sense together they are all animals .
for the set ( car , teacher , platypus , agile , blue , zaire ) , which lacks such coherence , identifying the intruder is difcult .
people will typically choose an intruder at random , implying a topic with poor coherence .
in order to construct a set to present to the subject , we rst select at random a topic from the model .
we then select the ve most probable words from that topic .
in addition to these words , an intruder
figure 123 : screenshots of our two human tasks .
in the word intrusion task ( left ) , subjects are presented with a set of words and asked to select the word which does not belong with the others .
in the topic intrusion task ( right ) , users are given a documents title and the rst few sentences of the document .
the users must select which of the four groups of words does not belong .
word is selected at random from a pool of words with low probability in the current topic ( to reduce the possibility that the intruder comes from the same semantic group ) but high probability in some other topic ( to ensure that the intruder is not rejected outright due solely to rarity ) .
all six words are then shufed and presented to the subject .
123 topic intrusion
the topic intrusion task tests whether a topic models decomposition of documents into a mixture of topics agrees with human judgments of the documents content .
this allows for evaluation of the latent space depicted by figure 123 ( b ) .
in this task , subjects are shown the title and a snippet from a document .
along with the document they are presented with four topics ( each topic is represented by the eight highest - probability words within that topic ) .
three of those topics are the highest probability topics assigned to that document .
the remaining intruder topic is chosen randomly from the other low - probability topics in the model .
the subject is instructed to choose the topic which does not belong with the document .
as before , if the topic assignment to documents were relevant and intuitive , we would expect that subjects would select the topic we randomly added as the topic that did not belong .
the formulation of this task provides a natural way to analyze the quality of document - topic assignments found by the topic models .
each of the three models we t explicitly assigns topic weights to each document; this task determines whether humans make the same association .
due to time constraints , subjects do not see the entire document; they only see the title and rst few sentences .
while this is less information than is available to the algorithm , humans are good at extrapolating from limited data , and our corpora ( encyclopedia and newspaper ) are structured to provide an overview of the article in the rst few sentences .
the setup of this task is also meaningful in situations where one might be tempted to use topics for corpus exploration .
if topics are used to nd relevant documents , for example , users will likely be provided with similar views of the documents ( e . g .
title and abstract , as in rexa ) .
for both the word intrusion and topic intrusion tasks , subjects were instructed to focus on the meanings of words , not their syntactic usage or orthography .
we also presented subjects with the option of viewing the correct answer after they submitted their own response , to make the tasks more engaging .
here the correct answer was determined by the model which generated the data , presented as if it were the response of another user .
at the same time , subjects were encouraged to base their responses on their own opinions , not to try to match other subjects ( the models ) selections .
in small experiments , we have found that this extra information did not bias subjects responses .
123 experimental results
to prepare data for human subjects to review , we t three different topic models on two corpora .
in this section , we describe how we prepared the corpora , t the models , and created the tasks described in section 123
we then present the results of these human trials and compare them to metrics traditionally used to evaluate topic models .
word intrusiontopic intrusion 123 models and corpora
in this work we study three topic models : probabilistic latent semantic indexing ( plsi ) ( 123 ) , latent dirichlet allocation ( lda ) ( 123 ) , and the correlated topic model ( ctm ) ( 123 ) , which are all mixed membership models ( 123 ) .
the number of latent topics , k , is a free parameter in each of the models; here we explore this with k = 123 , 123 and 123
the remaining parameters k , the topic multinomial distribution for topic k; and d , the topic mixture proportions for document d are inferred from data .
the three models differ in how these latent parameters are inferred .
plsi in plsi , the topic mixture proportions d are a parameter for each document .
thus , plsi is not a fully generative model , and the number of parameters grows linearly with the number of documents .
we t plsi using the em algorithm ( 123 ) but regularize plsis estimates of d using pseudo - count smoothing , = 123
lda lda is a fully generative model of documents where the mixture proportions d are treated as a random variable drawn from a dirichlet prior distribution .
because the direct computation of the posterior is intractable , we employ variational inference ( 123 ) and set the symmetric dirichlet prior parameter , , to 123
ctm in lda , the components of d are nearly independent ( i . e . , d is statistically neutral ) .
ctm allows for a richer covariance structure between topic proportions by using a logistic normal prior over the topic mixture proportions d .
for each topic , k , a real is drawn from a normal distribution and exponentiated .
this set of k non - negative numbers are then normalized to yield d .
here , we train the ctm using variational inference ( 123 ) .
we train each model on two corpora .
for each corpus , we apply a part of speech tagger ( 123 ) and remove all tokens tagged as proper nouns ( this was for the benet of the human subjects; success in early experiments required too much encyclopedic knowledge ) .
stop words ( 123 ) and terms occurring in fewer than ve documents are also removed .
the two corpora we use are 123 ) a collection of 123 articles from the new york times from the years 123 to 123 with a vocabulary size of 123 unique types and around one million tokens and 123 ) a sample of 123 articles from wikipedia ( http : / / www . wikipedia . org ) with a vocabulary size of 123 unique types and three million tokens .
123 evaluation using conventional objective measures
there are several metrics commonly used to evaluate topic models in the literature ( 123 ) .
many of these metrics are predictive metrics; that is , they capture the models ability to predict a test set of unseen documents after having learned its parameters from a training set .
in this work , we set aside 123% of the documents in each corpus as a test set and train on the remaining 123% of documents .
we then compute predictive rank and predictive log likelihood .
to ensure consistency of evaluation across different models , we follow teh et al . s ( 123 ) approximation of the predictive likelihood p ( wd|dtrain ) using p ( wd|dtrain ) p ( wd|d ) , where d is a point estimate of the posterior topic proportions for document d .
for plsi d is the map estimate; for lda and ctm d is the mean of the variational posterior .
with this information , we can ask what words the model believes will be in the document and compare it with the documents actual composition .
given document wd , we rst estimate d and then for every word in the vocabulary , we compute z p ( w|z ) p ( z|d ) .
then we compute the average rank for the terms that actually
p ( w|d ) = ( cid : 123 )
appeared in document wd ( we follow the convention that lower rank is better ) .
the average word likelihood and average rank across all documents in our test set are shown in table 123
these results are consistent with the values reported in the literature ( 123 , 123 ) ; in most cases ctm performs best , followed by lda .
123 analyzing human evaluations
the tasks described in section 123 were offered on amazon mechanical turk ( http : / / www . mturk . com ) , which allows workers ( our pool of prospective subjects ) to perform small jobs for a fee through a web interface .
no specialized training or knowledge is typically expected of the workers .
amazon mechanical turk has been successfully used in the past to develop gold - standard data for natural language processing ( 123 ) and to label images ( 123 ) .
for both the word intrusion and topic intrusion
table 123 : two predictive metrics : predictive log likelihood / predictive rank .
consistent with values reported in the literature , ctm generally performs the best , followed by lda , then plsi .
the bold numbers indicate the best performance in each row .
new york times
- 123 / 123 - 123 / 123 - 123 / 123 - 123 / 123 - 123 / 123 - 123 / 123
- 123 / 123 - 123 / 123 - 123 / 123 - 123 / 123 - 123 / 123 - 123 / 123
- 123 / 123 - 123 / 123 - 123 / 123 - 123 / 123 - 123 / 123 - 123 / 123
figure 123 : the model precision ( equation 123 ) for the three models on two corpora .
higher is better .
surprisingly , although ctm generally achieves a better predictive likelihood than the other models ( table 123 ) , the topics it infers fare worst when evaluated against human judgments .
tasks , we presented each worker with jobs containing ten of the tasks described in section 123
each job was performed by 123 separate workers , and workers were paid between $123 $123 per job .
word intrusion as described in section 123 , the word intrusion task measures how well the inferred topics match human concepts ( using model precision , i . e . , how well the intruders detected by the subjects correspond to those injected into ones found by the topic model ) .
k be the index of the intruding word among the words generated from the kth topic inferred by model m .
further let im k , s be the intruder selected by subject s on the set of words generated from the kth topic inferred by model m and let s denote the number of subjects .
we dene model precision by the fraction of subjects agreeing with the model ,
k , s = m
figure 123 shows boxplots of the precision for the three models on the two corpora .
in most cases lda performs best .
although ctm gives better predictive results on held - out likelihood , it does not perform as well on human evaluations .
this may be because ctm nds correlations between topics and correlations within topics are confounding factors; the intruder for one topic might be selected from another highly correlated topic .
the performance of plsi degrades with larger numbers of topics , suggesting that overtting ( 123 ) might affect interpretability as well as predictive power .
figure 123 ( left ) shows examples of topics with high and low model precisions from the ny times data t with lda using 123 topics .
in the example with high precision , the topic words all coherently express a painting theme .
for the low precision example , taxis did not t in with the other political words in the topic , as 123% of subjects chose taxis as the intruder .
the relationship between model precision , mpm k , and the models estimate of the likelihood of the intruding word in figure 123 ( top row ) is surprising .
the highest probability did not have the best interpretability; in fact , the trend was the opposite .
this suggests that as topics become more ne - grained in models with larger number of topics , they are less useful for humans .
the downward
model precision123 . 123 . 123 . 123 . 123 . 123 . 123 topicslllctmldaplsi123 topicslllllllllctmldaplsi123 topicslllllllllllctmldaplsinew york timeswikipedia figure 123 : a histogram of the model precisions on the new york times corpus ( left ) and topic log odds on the wikipedia corpus ( right ) evaluated for the fty topic lda model .
on the left , example topics are shown for several bins; the topics in bins with higher model precision evince a more coherent theme .
on the right , example document titles are shown for several bins; documents with higher topic log odds can be more easily decomposed as a mixture of topics .
figure 123 : a scatter plot of model precision ( top row ) and topic log odds ( bottom row ) vs .
predictive log likelihood .
each point is colored by model and sized according to the number of topics used to t the model .
each model is accompanied by a regression line .
increasing likelihood does not increase the agreement between human subjects and the model for either task ( as shown by the downward - sloping regression lines ) .
sloping trend lines in figure 123 implying that the models are often trading improved likelihood for the model precision showed a negative correlation ( spearmans = 123 averaged across all models , corpora , and topics ) with the number of senses in wordnet of the words displayed to the subjects ( 123 ) and a slight positive correlation ( = 123 ) with the average pairwise jiang - conrath similarity of words123 ( 123 ) .
topic intrusion in section 123 , we introduced the topic intrusion task to measure how well a topic model assigns topics to documents .
we dene the topic log odds as a quantitative measure of the agreement between the model and human judgments on this task .
let m d denote model ms point estimate of the topic proportions vector associated with document d ( as described in section 123 ) .
d , s ( 123 .
k ) be the intruding topic selected by subject s for document d on model further , let jm m and let jm d , denote the true intruder , i . e . , the one generated by the model .
we dene the topic log odds as the log ratio of the probability mass assigned to the true intruder to the probability mass
123words without entries in wordnet were ignored; polysemy was handled by taking the maximum over all senses of words .
to handle words in the same synset ( e . g .
ght and battle ) , the similarity function was capped at 123 .
model precisionnumber of topics123 . 123 . 123 . 123 . 123committeelegislationproposalrepublicantaxisreplacegaragehousekitchenlistamericansjapanesejewishstatesterroristartistexhibitiongallerymuseumpaintingtopic log oddsnumber of documents ! 123 ! 123 ! 123 ! 123 ! 123 ! 123 ! 123 . 123bookjohn quincy adamsmicrosoft wordlindy hoppredictive log likelihood123 . 123 . 123 . 123 . 123new york timesllllllllllllllllll123 . 123 . 123wikipediallllllllllllllllll123 . 123 . 123 . 123model precisiontopic log oddsmodellctmlldalplsinumber of topicsl123l123l123 figure 123 : the topic log odds ( equation 123 ) for the three models on two corpora .
higher is better .
although ctm generally achieves a better predictive likelihood than the other models ( table 123 ) , the topics it infers fare worst when evaluated against human judgments .
d = ( ( cid : 123 )
s log m
d , log m
assigned to the intruder selected by the subject ,
d , the greater the correspondence between the judgments of the model d is 123
this is achieved when the subjects choose
the higher the value of tlom and the subjects .
the upper bound on tlom intruders with a mixture proportion no higher than the true intruders .
figure 123 shows boxplots of the topic log odds for the three models .
as with model precision , lda and plsi generally outperform ctm .
again , this trend runs counter to ctms superior performance on predictive likelihood .
a histogram of the tlo of individual wikipedia documents is given in figure 123 ( right ) for the fty - topic lda model .
documents about very specic , unambiguous concepts , such as lindy hop , have high tlo because it is easy for both humans and the model to assign the document to a particular topic .
when documents express multiple disparate topics , human judgments diverge from those of the model .
at the low end of the scale is the article book which touches on diverse areas such as history , science , and commerce .
it is difcult for lda to pin down specic themes in this article which match human perceptions .
figure 123 ( bottom row ) shows that , as with model precision , increasing predictive likelihood does not imply improved topic log odds scores .
while the topic log odds are nearly constant across all numbers of topics for lda and plsi , for ctm topic log odds and predictive likelihood are negatively correlated , yielding the surprising conclusion that higher predictive likelihoods do not lead to improved model interpretability .
we presented the rst validation of the assumed coherence and relevance of topic models using human experiments .
for three topic models , we demonstrated that traditional metrics do not capture whether topics are coherent or not .
traditional metrics are , indeed , negatively correlated with the measures of topic quality developed in this paper .
our measures enable new forms of model selection and suggest that practitioners developing topic models should thus focus on evaluations that depend on real - world task performance rather than optimizing likelihood - based measures .
in a more qualitative vein , this work validates the use of topics for corpus exploration and information retrieval .
humans appreciate the semantic coherence of topics and can associate the same documents with a topic that a topic model does .
an intriguing possibility is the development of models that explicitly seek to optimize the measures we develop here either by incorporating human judgments into the model - learning framework or creating a computational proxy that simulates human judgments .
david m .
blei is supported by onr 123 - 123 , nsf career 123 and grants from google and microsoft .
we would also like to thank dan osherson for his helpful comments .
