in this paper , we develop the continuous time dynamic topic model ( cdtm ) .
the cdtm is a dynamic topic model that uses brownian motion to model the latent topics through a sequential collection of documents , where a topic is a pattern of word use that we expect to evolve over the course of the col - lection .
we derive an ecient variational approximate inference algorithm that takes advantage of the sparsity of observations in text , a property that lets us easily han - dle many time points .
in contrast to the cdtm , the original discrete - time dynamic topic model ( ddtm ) requires that time be discretized .
moreover , the complexity of vari - ational inference for the ddtm grows quickly as time granularity increases , a drawback which limits ne - grained discretization .
we demonstrate the cdtm on two news corpora , reporting both predictive perplexity and the novel task of time stamp prediction .
tools for analyzing and managing large collections of electronic documents are becoming increasingly im - portant .
in recent years , topic models , which are hi - erarchical bayesian models of discrete data , have be - come a widely used approach for exploratory and pre - dictive analysis of text .
topic models , such as latent dirichlet allocation ( lda ) and the more general dis - crete component analysis ( 123 , 123 ) , posit that a small number of distributions over words , called topics , can be used to explain the observed collection .
lda is a probabilistic extension of latent semantic indexing ( lsi ) ( 123 ) and probabilistic latent semantic indexing ( plsi ) ( 123 ) .
owing to its formal generative semantics , lda has been extended and applied to authorship ( 123 ) ,
email ( 123 ) , computer vision ( 123 ) , bioinformatics ( 123 ) , and information retrieval ( 123 ) .
for a good review , see ( 123 ) .
most topic models assume the documents are ex - changeable in the collection , i . e . , that their probability is invariant to permutation .
many document collec - tions , such as news or scientic journals , evolve over time .
in this paper , we develop the continuous time dynamic topic model ( cdtm ) , which is an extension of the discrete dynamic topic model ( ddtm ) ( 123 ) .
given a sequence of documents , we infer the latent topics and how they change through the course of the collection .
the ddtm uses a state space model on the natural pa - rameters of the multinomial distributions that repre - sent the topics .
this requires that time be discretized into several periods , and within each period lda is used to model its documents .
in ( 123 ) , the authors an - alyze the journal science from 123 - 123 , assuming that articles are exchangeable within each year .
while the ddtm is a powerful model , the choice of discretiza - tion aects the memory requirements and computa - tional complexity of posterior inference .
this largely determines the resolution at which to t the model .
to resolve the problem of discretization , we consider time to be continuous .
the continuous time dynamic topic model ( cdtm ) proposed here replaces the dis - crete state space model of the ddtm with its continu - ous generalization , brownian motion ( 123 ) .
the cdtm generalizes the ddtm in that the only discretization it models is the resolution at which the time stamps of the documents are measured .
the cdtm model will , generally , introduce many more latent variables than the ddtm .
however , this seem - ingly more complicated model is simpler and more e - cient to t .
as we will see below , from this formulation the variational posterior inference procedure can take advantage of the natural sparsity of text , the fact that not all vocabulary words are used at each measured time step .
in fact , as the resolution gets ner , fewer and fewer words are used .
this provides an inferential speed - up that makes it possible to t models at varying granularities .
as ex - amples , journal articles might be exchangeable within an issue , an assumption which is more realistic than one where they are exchangeable by year .
other data , such as news , might experience periods of time without any observation .
while the ddtm requires represent - ing all topics for the discrete ticks within these periods , the cdtm can analyze such data without a sacrice of memory or speed .
with the cdtm , the granularity can be chosen to maximize model tness rather than to limit computational complexity .
we note that the cdtm and ddtm are not the only topic models to take time into consideration .
topics over time models ( tot ) ( 123 ) and dynamic mixture models ( dmm ) ( 123 ) also include timestamps in the analysis of documents .
the tot model treats the time stamps as observations of the latent topics , while dmm assumes that the topic mixture proportions of each document is dependent on previous topic mix - ture proportions .
in both tot and dmm , the topics themselves are constant , and the time information is used to better discover them .
in the setting here , we are interested in inferring evolving topics .
the rest of the paper is organized as follows .
in sec - tion 123 we describe the ddtm and develop the cdtm in detail .
section 123 presents an ecient posterior in - ference algorithm for the cdtm based on sparse varia - tional methods .
in section 123 , we present experimental results on two news corpora .
123 continuous time dynamic topic
in a time stamped document collection , we would like to model its latent topics as changing through the course of the collection .
in news data , for example , a single topic will change as the stories associated with it develop .
the discrete - time dynamic topic model ( ddtm ) builds on the exchangeable topic model to provide such machinery ( 123 ) .
in the ddtm , documents are divided into sequential groups , and the topics of each slice evolve from the topics of the previous slice .
documents in a group are assumed exchangeable .
more specically , a topic is represented as a distribu - tion over the xed vocabulary of the collection .
the ddtm assumes that a discrete - time state space model governs the evolution of the natural parameters of the multinomial distributions that represent the topics .
( recall that the natural parameters of the multino - mial are the logs of the probabilities of each item . ) this is a time - series extension to the logistic normal
figure 123 : graphical model representation of the cdtm .
the evolution of the topic parameters t is governed by brownian motion .
the variable st is the observed time stamp of document dt .
a drawback of the ddtm is that time is discretized .
if the resolution is chosen to be too coarse , then the assumption that documents within a time step are ex - changeable will not be true .
if the resolution is too ne , then the number of variational parameters will ex - plode as more time points are added .
choosing the dis - cretization should be a decision based on assumptions about the data .
however , the computational concerns might prevent analysis at the appropriate time scale .
thus , we develop the continuous time dynamic topic model ( cdtm ) for modeling sequential time - series data with arbitrary granularity .
the cdtm can be seen as a natural limit of the ddtm at its nest pos - sible resolution , the resolution at which the document time stamps are measured .
in the cdtm , we still represent topics in their natural parameterization , but we use brownian motion ( 123 ) to model their evolution through time .
let i , j ( j > i > 123 ) be two arbitrary time indexes , si and sj be the time stamps , and sj , si be the elapsed time between them .
in a k - topic cdtm model , the distribution of the kth ( 123 k k ) topics parameter at term w is :
j , k , w|i , k , w , s n ( cid : 123 ) i , k , w , vsj , si
123 , k , w n ( m , v123 )
where the variance increases linearly with the lag .
this construction is used as a component in the full generative process .
( note : if j = i + 123 , we write sj , si as sj for short . ) 123
for each topic k , 123 k k , ( a ) draw 123 , k n ( m , v123i ) .
for document dt at time st ( t > 123 ) :
( a ) for each topic k , 123 k k ,
from the brownian motion model , draw
t , k|t123 , k , s n ( t123 , k , vsti ) .
( b ) draw t dir ( ) .
( c ) for each word ,
draw zt , n mult ( t ) .
draw wt , n mult ( ( t , zt , n ) ) .
figure 123 : documents are available only at time s and s ( cid : 123 ) , and no documents between them .
when 123 123 , the ddtm becomes a cdtm , and we no longer need to represent the steps between i and j .
the function maps the multinomial natural parame - ters , which are unconstrained , to its mean parameters , which are on the simplex ,
w exp ( t , k , w ) .
the cdtm is illustrated in figure 123
the cdtm can be seen as a generalization of the ddtm .
both models assume that the log probability of a term exhibits variance over an interval of time between observations .
in the ddtm , this interval is evenly divided into discrete ticks .
a parameter con - trols the variance at each tick , and the variance across the whole interval is that parameter multiplied by the number of ticks .
as a consequence of this represen - tation , the topic , i . e . , the full distribution over terms , is explicitly represented at each tick .
for ne - grained time series , this leads to high memory requirements for posterior inference , even if the observations are sparsely distributed throughout the timeline .
in the cdtm , however , the variance is a function of the lag between observations , and the probabilities at discrete steps between those observations need not be inference , as we will see below , can be handled sparsely .
thus , choosing the right granularity becomes a modeling issue rather than one governed by computational concerns .
a ddtm is obtained with a cdtm by measuring the time stamps of the documents at the desired granularity .
akin to brownian motion as the limiting process of a discrete - time gaussian random walk ( 123 ) , the cdtm is the limiting process of the ddtm .
denote the per - tick variance in the ddtm by 123 , and note that it is a function of the tick granularity ( to make mod - els comparable ) .
the cdtm is the limiting model in this setting as 123 approaches zero .
we emphasize that with the cdtm , we need not represent the log proba - bilities at the ticks between observed documents .
this perspective is illustrated in figure 123
123 sparse variational inference
the central problem in topic modeling is posterior in - ference , i . e . , determining the distribution of the la -
tent topic structure conditioned on the observed doc - in sequential topic models , this structure comprises the per - document topic proportions d , per - word topic assignments zd , n , and the k sequences of topic distributions t , k .
the true posterior is not tractable ( 123 ) .
we must appeal to an approximation .
several approximate inference methods have been de - veloped for topic models .
the most widely used are variational inference ( 123 , 123 ) and collapsed gibbs sam - in the sequential setting collapsed gibbs sampling is not an option because the distribution of words for each topic is not conjugate to the word prob - abilities .
thus , we employed variational methods .
the main idea behind variational methods is to posit a simple family of distributions over the latent vari - indexed by free variational parameters , and to nd the member of that family which is closest in kullback - leibler divergence to the true posterior .
good overviews of this methodology can be found in ( 123 ) and ( 123 ) .
for continuous time processes , varia - tional inference has been applied in markov jump pro - cesses ( 123 ) and diusion processes ( 123 ) , where the vari - ational distributions are also random processes .
for the cdtm described above , we adapt variational kalman ltering ( 123 ) to the continuous time setting .
for simplicity , assume that one document occurs at each time point .
in their algorithm , the variational distribution over the latent variables is : q ( 123 : t , z123 : t , 123 : n , 123 : t | , , ) =
q ( 123 , k , .
, t , k| 123 , k , .
, t , k )
the variational parameters are a dirichlet t for the per - document topic proportions , multinomials for each words topic assignment , and variables , which are observations to a variational kalman lter .
these variables are t such that the approximate pos - terior is close to the true posterior .
from the varia - tional kalman lter , the k , t , 123 t t retain their chained structure in the variational distribution
ational inference proceeds by coordinate ascent , up - dating each of these parameters to minimize the kl between the true posterior and variational posterior .
for simplicity , now we consider a model with only one topic .
these calculations are simpler versions of those we need for the more general latent variable model but exhibit the essential features of the algorithm .
for the cdtm , we assume a similar variational distri - bution , with the same variational dirichlet and vari - ational multinomials for the per - document variables .
the cdtm updates for these parameters are identical to those in ( 123 ) , and we do not replicate them here .
in principle , we can directly use the variational kalman ltering algorithm for the cdtm by replac - ing the state space model with brownian motion .
let v be the size of the vocabulary .
while conceptually straightforward , this will yield vt variational param - eters in the vectors 123 : t .
when t and v are large , as in a ne - graned model , posterior inference will require massive amounts of time and memory .
thus , we de - velop a sparse variational inference procedure , which signicantly improves its complexity without sacric - the main idea behind the sparse variational kalman ltering algorithm is that if certain t , w do not de - scribe any term emissions , i . e . , there are no observa - tions of w at t , then the true posterior of t , w is only determined by the observations of the other words at that time .
therefore , we dont need to explicitly rep - resent t , w for those w that are not observed .
figure 123 illustrates the idea behind sparse variational inference for the cdtm .
in figure 123 , the variational posterior of the log probability of a word t , w is deter - mined by the variational observations of the observed words .
from the belief propagation point of view , the belief propagated from t , w to node t+123 , w is not re - vised by term w , and this property is retained in the sparse variational inference algorithm .
the probabil - ity of variational observation t , w given t , w is a gaus -
t , w|t , w n ( t , w , vt ) .
we next describe the forward - backward algorithm for the sparse variational kalman lter , which is needed to compute the expectations for updating the varia - tional parameters .
for a certain term w , the varia - tional forward distribution p ( t , w| i , it , w ) is a gaus - sian ( 123 ) and can be characterized as follows .
t , w| i , it , w n ( mt , w , vt , w ) mt , w = e ( t , w| i , it , w ) vt , w = e ( ( t , w mt , w ) 123| i , it , w )
figure 123 : a simplied graphical model shows how inference works with only single topic .
note this generation process needs normaliza - tion to t according to equation 123 , but this will not aect the sparse solution .
for term w , there are no ob - servations at time index t+123 ( or time st+123 ) , the corre - sponding variational observations dont appear at time index t + 123
for term w ( cid : 123 ) , there are no observations at time index t+123 ( or time st+123 ) , the corresponding vari - ational observations dont appear at time index t + 123
if w is not observed at time step t then
mt , w = mt123 , w vt , w = pt , w , pt , w = vt123 , w + vst ,
which means that the forward mean remains the same as the previous step .
otherwise ,
vt , w = vt
t , wpt , w + vtmt123 , w
pt , w + vt
pt , w + vt
t , w| i , it123 , w n ( mt123 , w , pt , w + vt ) .
the variational backward distribution
p ( t , w| i , it , w ) is also a gaussian :
t , w| i , it , w n ( ( cid : 123 ) mt , w , ( cid : 123 ) vt , w ) ( cid : 123 ) mt , w = e ( t , w| i , it , w ) ( cid : 123 ) vt , w = e ( ( t , w ( cid : 123 ) mt , w ) 123| i , it , w ) .
( cid : 123 ) mt123 , w = mt123 , w
( cid : 123 ) vt123 , w = vt123 , w +
hour day week month
with this forward - backward computation in hand , we turn to optimizing the variational observations w , k in the sparse setting .
equivalent to minimizing kl is tightening the bound on the likelihood of the observa - tions given by jensens inequality ( 123 ) .
l ( ) t ( cid : 123 )
eq ( ( log p ( wt|t ) + log p ( t|t123 ) ) + h ( q ) ,
where h ( q ) is the entropy .
this is simplied to
l ( ) t ( cid : 123 )
log p ( wt|t ) log q ( t|t )
log q ( t| i , it123 ) ,
table 123 : sparsity for two data sets where available .
higher numbers indicate a sparser data set and more eciency for the cdtm over the ddtm .
evenly spaced over the time line .
in the ddtm , these documents were separated by years .
to analyze them at a ner scale , e . g . , issue by issue , one needs to con - sider 123 time points .
with a vocabulary size of 123 , for a 123 - topic setting , the cdtm requires 123g mem - ory while the ddtm requires 123g memory , nearly 123 times larger .
the sparsity of science is 123 .
this means that a term only appears in about a third of the total time points .
we use t , w = 123 or 123 to represent whether t , w is in the variational observations or not .
then the terms
t , weq log q ( t , w|t , w ) t , w log q ( t , w| i , it123 , w ) .
eq log q ( wt|t ) ( cid : 123 ) eq log p ( t|t ) = ( cid : 123 ) log q ( t| i , it123 ) = ( cid : 123 )
the count of w in document dt is nt , w and nt =
thus , to optimize the variational observations , we need only to compute the derivative l / t , w for those t , w = 123
the general memory requirement w t , w ) the sum of the number of unique terms at each time pointwhich is usually much smaller than o ( vt ) , the memory requirement for the densely represented algorithm .
formally , we can de - ne the sparsity of the data set to be
sparsity = 123 ( ( cid : 123 )
w t , w ) / ( vt ) ,
which we will compute for several data sets in the next section .
finally , we note that we use the conjugate gradient algorithm ( 123 ) to optimize the variational ob - servations from these partial derivatives .
as an example of the speed - up oered by sparse vari - ational inference , consider the science corpus from 123 - 123 , analyzed by ( 123 ) , which contains 123 is - sues of the magazine .
note that these issues are not
in this section , we demonstrate the cdtm model on two news corpora .
we report predictive perplexity and a results on the novel task of time stamp prediction .
123 news corpora
we used two news corpora .
first , ap is a subset from the trec ap corpus ( 123 ) containing the news from 123 / 123 / 123 to 123 / 123 / 123
we extracted the documents about the presidential election in 123 re - sulting in 123 , 123 documents .
these documents are time stamped by hour .
second , the election 123 data are summaries of the top articles from digg123 classied as being part of the 123 presidential election .
we used articles from 123 - 123 - 123 to 123 - 123 - 123
this data set has 123 , 123 summaries .
time is measured in days .
table 123 shows the sparsity information for these data in terms of the resolution at which we can analyze them .
this illustrates the gain in eciency of the cdtm .
for example , in the day setting of the elec - tion 123 data , the sparsity is 123 .
the ddtm model will need at least 123 times more parameters than the cdtm to analyze the data at this resolution .
123 per - word predictive perplexity
let dt be the set of documents at time index t .
we performed approximate posterior inference on these data with the cdtm at dierent levels of granular - ity .
to make models comparable , we set the variance across the entire period to be the same ( see equation 123 ) .
we evaluated the models with perplexity .
speci -
cally , we computed the per - word predictive perplexity of the documents at time t based on the data of the previous t 123 time indices ,
perplexitypw ( t ) = exp
note that lower numbers are better .
since each document is predicted exactly once in all models at dierent granularities , we also compute the averaged per - word perplexity over the time line , which is dened as
perplexitypw = exp
dd log p ( wd )
in the ap data , we made predictions from 123 / 123 / 123 to 123 / 123 / 123
in the election 123 data , we made pre - dictions from 123 / 123 / 123 to 123 / 123 / 123
figure 123 shows the results of the per - word predictive perplexity over the time line on both data sets for the 123 topic model .
figure 123 shows the results of average per - word perplexity for 123 , 123 , 123 and 123 topics .
from the computational perspective , we note that the sparse inference algorithm lets us t models of dier - ent granularities eciently .
for the ap data , the day model and week are almost comparable .
models with 123 and 123 topics perform better .
in the election 123 data , the 123 - topic model performs best .
we suspect that this is because the summaries are very short .
more complex models , i . e . , those with more topics , are not appropriate .
the models perform dierently at dierent levels of granularity because the amount of data supported at each time point depends on the chosen level .
it is not necessarily the case that a ner grained model will contain enough data to pro - vide a better predictive distribution .
123 time stamp prediction
we can further use the cdtm for time stamp predic - tion , dating a document based on its content .
to assess this task , we split each data set into 123% training and 123% testing sets .
we predict the time stamp of each test document by nding its most likely location over the time line .
we measure the error in terms of the same granularity at which the data are measured .
we investigated two approaches .
the rst is the at approach .
each model of dierent granularity predicts as best it can .
the second is the hierarchical approach .
we use models of increasing granularity to zoom in on the prediction .
for example , to predict the day , we rst nd the best month , then the best week within the month , and then the best day within the week .
we compute the average absolute error over the test data set .
figure 123 illustrates the results .
the hierarchical approach always performs better than or as well as the at approach .
the hour model in the ap data and day model in election 123 perform worse .
with the small data sets , a larger granularity is better .
the reason may also lie in the parameter v .
currently it is shared among all models .
in the future , wed like to infer it from the data .
123 example topics
we provide some example topics by using the week model in the election 123 data .
we sample the topics every two months .
figure 123 shows one of the topics .
at the beginning the election ( year 123 ) , general issues were discussed more , such as healthcare .
as the competition went up ( year 123 ) , the topics were more about candidates themselves and changing faster .
in this paper , we have developed the cdtm , using brownian motion to model continuous - time topic evo - lution .
the main advantage of the cdtm is that we can employ sparse variational inference for fast model comparison .
we demonstrated the use of cdtm by measuring the predictive likelihood and time stamp prediction accuracy on two real - world data sets .
in fu - ture work , we plan to explore the ornstein - uhlenbeck ( ou ) model ( 123 ) , a generalization of brownian model , that allows bounded variance .
acknowledgments .
we thank anonymous reviewers for their valuable comments .
we would also like to thank jordan boyd - graber and jonathan chang for many insightful discussions .
david m .
blei is sup - ported by onr 123 - 123 , nsf career 123 , and grants from google and microsoft .
