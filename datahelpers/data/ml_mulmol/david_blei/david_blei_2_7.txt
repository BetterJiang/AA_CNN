the hierarchical dirichlet process ( hdp ) is a bayesian nonparametric model that can be used to model mixed - membership data with a poten - tially innite number of components .
it has been applied widely in probabilistic topic modeling , where the data are documents and the compo - nents are distributions of terms that reect recur - ring patterns ( or topics ) in the collection .
given a document collection , posterior inference is used to determine the number of topics needed and to characterize their distributions .
one limitation of hdp analysis is that existing posterior infer - ence algorithms require multiple passes through all the datathese algorithms are intractable for very large scale applications .
we propose an on - line variational inference algorithm for the hdp , an algorithm that is easily applicable to massive and streaming data .
our algorithm is signicantly faster than traditional inference algorithms for the hdp , and lets us analyze much larger data sets .
we illustrate the approach on two large collections of text , showing improved performance over on - line lda , the nite counterpart to the hdp topic
the hierarchical dirichlet process ( hdp ) ( 123 ) is a powerful mixed - membership model for the unsupervised analysis of grouped data .
applied to document collections , the hdp provides a nonparametric topic model where documents are viewed as groups of observed words , mixture components ( called topics ) are distributions over terms , and each docu - ment exhibits the topics with different proportions .
given a collection of documents , the hdp topic model nds a low - dimensional latent structure that can be used for tasks
like classication , exploration , and summarization .
unlike its nite counterpart , latent dirichlet allocation ( 123 ) , the hdp topic model infers the number of topics from the data .
posterior inference for the hdp is intractable , and much research is dedicated to developing approximate inference algorithms ( 123 , 123 , 123 ) .
these methods are limited for massive scale applications , however , because they require multiple passes through the data and are not easily applicable to streaming data . 123 in this paper , we develop a new approx - imate inference algorithm for the hdp .
our algorithm is designed to analyze much larger data sets than the existing state - of - the - art allows and , further , can be used to analyze streams of data .
this is particularly apt to the hdp topic model .
topic models promise to help summarize and orga - nize large archives of texts that cannot be easily analyzed by hand and , further , could be better exploited if available on streams of texts such as web apis or news feeds .
our methodonline variational bayes for the hdp was inspired by the recent online variational bayes algorithm for lda ( 123 ) .
online lda allows lda models to be t to massive and streaming data , and enjoys signicant improve - ments in computation time without sacricing model quality .
our motivation for extending this algorithm to the hdp is that lda requires choosing the number of topics in advance .
in a traditional setting , where tting multiple models might be viable , the number of topics can be determined with cross validation or held - out likelihood .
however , these techniques become impractical when the data set size is large , and they become impossible when the data are streaming .
online hdp provides the speed of online variational bayes with the modeling exibility of the hdp .
the idea behind online variational bayes in general is to opti - mize the variational objective function of ( 123 ) with stochastic optimization ( 123 ) .
optimization proceeds by iteratively tak - ing a random subset of the data , and updating the variational parameters with respect to the subset .
online variational bayes is particularly efcient when using the natural gradi - ent ( 123 ) on models in which traditional variational bayes
appearing in proceedings of the 123th international conference on articial intelligence and statistics ( aistats ) 123 , fort laud - erdale , fl , usa .
volume 123 of jmlr : w&cp 123
copyright 123 by the authors .
123one exception that may come to mind is the particle lter ( 123 , 123 ) .
however , this algorithm still requires periodically resampling a variable for every data point .
data cannot be thrown away as in a true streaming algorithm .
123 online variational inference for the hierarchical dirichlet process
can be performed by simple coordinate ascent ( 123 ) .
( this is the property that allowed ( 123 ) to derive an efcient online variational bayes algorithm for lda . ) in this setting , on - line variational bayes is signicantly faster than traditional variational bayes ( 123 ) , which must make multiple passes through the data .
the challenge we face is that the existing coordinate as - cent variational bayes algorithms for the hdp require com - plicated approximation methods or numerical optimiza - tion ( 123 , 123 , 123 ) .
we will begin by reviewing sethuramans stick - breaking construction of the hdp ( 123 ) .
we show that this construction allows for coordinate - ascent variational bayes without numerical approximation , which is a new and simpler variational inference algorithm for the hdp .
we will then use this approach in an online variational bayes algorithm , allowing the hdp to be applied to massive and streaming data .
finally , on two large archives of scientic articles , we will show that the online hdp topic model pro - vides a signicantly better t than online lda .
online vari - ational bayes lets us apply bayesian nonparametric models at much larger scales .
123 a stick breaking construction
of the hdp
we describe the stick - breaking construction of the hdp ( 123 ) using the sethuramans construction for the dp ( 123 ) .
this is amenable to simple coordinate - ascent variational inference , and we will use it to develop online variational inference for a two - level hierarchical dirichlet process ( hdp ) ( 123 ) ( the focus of this paper ) is a collection of dirichlet processes ( dp ) ( 123 ) that share a base distribution g123 , which is also drawn from a dp .
mathematically ,
gj dp ( 123g123 ) , for each j ,
where j is an index for each group of data .
a notable feature of the hdp is that all dps gj share the same set of atoms and only the atom weights differ .
this is a result of the almost sure discreteness of the top - level dp .
in the hdp topic modelwhich is the focus of this paper we model groups of words organized into documents .
the variable wjn is the nth word in the jth document; the base distribution h is a symmetric dirichlet over the vocabulary simplex; and the atoms of g123 , which are independent draws from h , are called topics .
the hdp topic model contains two additional steps to gener - ate the data .
first we generate the topic associated with the nth word in the jth document; then we generate the word from that topic ,
jn gj , wjn mult ( jn ) .
the discreteness of the corpus - level draw g123 ensures that all documents share the same set of topics .
the document - level draw gj inherits the topics from g123 , but weights them according to document - specic topic proportions .
tehs stick - breaking construction .
the denition of the hdp in eq .
123 is implicit .
( 123 ) propose a more construc - tive representation of the hdp using two stick - breaking representations of a dirichlet distribution ( 123 ) .
for the corpus - level dp draw , this representation is
k beta ( 123 , ) , l=123 ( 123 ( cid : 123 ) k = ( cid : 123 )
k=123 kk .
jk = ( cid : 123 )
k=123 with weights = ( k )
thus , g123 is discrete and has support at the atoms = k=123
the distribution for is also written as gem ( ) ( 123 ) .
the construction of each document - level gj is
l=123 ( 123 ( cid : 123 )
k=123 jkk , k=123 are the same atoms as g123 in eq
where = ( k ) this construction is difcult to use in an online variational inference algorithm .
online variational inference is partic - ularly efcient when the model is also amenable to coordi - nate ascent variational inference , and where each update is available in closed form .
in the construction above , the stick - breaking weights are tightly coupled between the bottom and top - level dps .
as a consequence , it is not amendable to closed form variational updates ( 123 , 123 ) .
sethuramans stick - breaking construction .
to ad - dress this issue , we describe an alternative stick - breaking construction for the hdp that allows for closed - form coordinate - ascent variational inference due to its full conju - gacy .
( this construction was also briey described in ( 123 ) . ) the construction is formed by twice applying sethuramans stick - breaking construction of the dp .
we again construct the corpus - level base distribution g123 as in eq .
the differ - ence is in the document - level draws .
we use sethuramans construction for each gj , jt beta ( 123 , 123 ) , l=123 ( 123 ( cid : 123 ) jt = ( cid : 123 )
notice that each document - level atom ( i . e . , topic ) jt maps to a corpus - level atom k in g123 according to the distribution
123 chong wang
david m
123 online variational inference
for the hdp
with sethuramans construction of the hdp in hand , we now turn to our original aimapproximate posterior inference in the hdp for massive and streaming data .
given a large collection of documents , our goal is to approximate the posterior distribution of its latent topic structure .
we will use online variational inference ( 123 ) .
traditional variational inference approximates the posterior over the hidden variables by positing a simpler distribution which is optimized to be close in kullback - leibler ( kl ) divergence to the true posterior ( 123 ) .
this problem is ( approximately ) solved by optimizing a function equal up to a constant to the kl of interest .
in online variational inference , we optimize that function with stochastic approximation .
online variational inference enjoys a close relationship with coordinate - ascent variational inference .
consider a model with latent variables and observations for which the poste - rior is intractable to compute .
one strategy for variational inference is the mean - eld approach : posit a distribution where each latent variable is independent and governed by its own parameter , and optimize the variational parameters with coordinate ascent .
now , suppose that those coordinate ascent updates are avail - able in closed form and consider updating them in parallel .
( note this is no longer coordinate ascent . ) it turns out that the vector of parallel coordinate updates is exactly the nat - ural gradient of the variational objective function under conjugate priors ( 123 ) .
this insight makes stochastic opti - mization of the variational objective , based on a subset of the data under analysis , a simple and efcient alternative to let us now return to the hdp topic model .
we will rst show that sethuramans representation of the hdp above allows for closed - form coordinate - ascent updates for vari - ational inference .
then , we will derive the corresponding online algorithm , which provides a scalable method for hdp
123 a new coordinate - ascent variational inference
when applied to bayesian nonparametric models , vari - ational methods are usually based on stick - breaking representationsthese representations provide a concrete set of hidden variables on which to place an approximate posterior ( 123 , 123 , 123 ) .
furthermore , the approximate pos - terior is usually truncated .
the user rst sets a truncation on the number of topics to allow , and then relies on vari - ational inference to infer a smaller number that are used in the data .
( two exceptions are found in ( 123 , 123 ) , who developed methods that allow the truncation to grow . ) note that setting a truncation level is different from asserting a number of components in a model .
when set large , the
figure 123 : illustration of the sethuramans stick - breaking construction of the two - level hdp .
in the rst level , k h and gem ( ) ; in the second level , j gem ( 123 ) , cjt mult ( ) and jt = cjt .
dened by g123
further note there will be multiple document - level atoms jt which map to the same corpus - level atom k , but we can verify that gj contains all of the atoms in g123 almost surely .
a second way to represent the document - level atoms j = t=123 is to introduce a series of indicator variables , cj = t=123 , which are drawn i . i . d . ,
where gem ( ) ( as mentioned above ) .
then let
jt = cjt ,
thus , we do not need to explicitly represent the document atoms j .
this further simplies online inference .
the property that multiple document - level atoms jt can map to the same corpus - level atom k in this representa - tion is similar in spirit to the chinese restaurant franchise ( crf ) ( 123 ) , where each restaurant can have multiple tables serving the same dish k .
in the crf representation , a hierarchical chinese restaurant process allocates dishes to tables .
here , we use a series of random indicator variables cj to represent this structure .
figure 123 illustrates the concept .
given the representation in eq .
123 , the generative process for the observed words in jth document , wjn , is as follows ,
jn = jzjn = cjzjn
the indicator zjn selects topic parameter jt , which maps to one topic k through the indicators cj .
this also provides the mapping from topic jn to k , which we need in eq
123 online variational inference for the hierarchical dirichlet process
hdp assumptions encourage the approximate posterior to use fewer components .
we use a fully factorized variational distribution and per - form mean - eld variational inference .
the hidden vari - ables that we are interested in are the top - level stick propor - tions ( cid : 123 ) = ( ( cid : 123 ) k=123 , bottom - level stick proportions ( cid : 123 ) t=123 for each gj .
we also infer atom / topic distributions = ( k ) topic index zjn for each word wjn .
thus our variational distribution has the following form ,
t=123 and the vector of indicators cj = ( cjt )
q ( ( cid : 123 ) , ( cid : 123 ) , c , z , ) = q ( ( cid : 123 ) ) q ( ( cid : 123 ) ) q ( c ) q ( z ) q ( ) .
this further factorizes into
where the variational parameters are jt ( multinomial ) , jn ( multinomial ) and k ( dirichlet ) .
the factorized forms of q ( ( cid : 123 ) ) and q ( ( cid : 123 ) ) are
k = 123 ) = 123 and q ( ( cid : 123 )
where ( uk , bk ) and ( ajt , bjt ) are parameters of beta distri - butions .
we set the truncations for the corpus and document levels to k and t .
here , t can be set much smaller than k , because in practice each document gj requires far fewer topics than those needed for the entire corpus ( i . e . , the atoms of g123 ) .
with this truncation , our variational distribution has using standard variational theory ( 123 ) , we lower bound the marginal log likelihood of the observed data d = ( wj ) d using jensens inequality , log p ( d| , 123 , ) eq ( log p ( d , ( cid : 123 ) , ( cid : 123 ) , c , z , ) ) + h ( q )
( cid : 123 ) log ( cid : 123 ) p ( wj|cj , zj , ) p ( cj| ( cid : 123 ) ) p ( zj| ( cid : 123 )
+ h ( q ( cj ) ) + h ( q ( zj ) ) + h ( q ( ( cid : 123 )
jt = 123 ) = 123 , for all j .
+ eq ( log p ( ( cid : 123 ) ) p ( ) ) + h ( q ( ( cid : 123 ) ) ) + h ( q ( ) )
where h ( ) is the entropy term for the variational distribu - tion .
this is the variational objective function , which up to a constant is equivalent to the kl to the true posterior .
taking derivatives of this lower bound with respect to each vari - ational parameter , we can derive the following coordinate document - level updates : at the document level we up - date the parameters to the per - document stick , the parame - ters to the per word topic indicators , and the parameters to
the per document topic indices ,
ajt = 123 + ( cid : 123 ) bjt = 123 + ( cid : 123 ) jtk exp ( ( cid : 123 )
uk = 123 + ( cid : 123 ) vk = + ( cid : 123 ) kw = + ( cid : 123 )
n jnteq ( log p ( wjn|k ) ) + eq ( log k ) ) , k=123 jtkeq ( log p ( wjn|k ) ) + eq ( log jt ) corpus - level updates : at the corpus level , we update the parameters to top - level sticks and the topics ,
t=123 jtk ( ( cid : 123 ) ( cid : 123 ) = ( ajt ) ( ajt + bjt ) , jt ) ( cid : 123 ) = ( bjt ) ( ajt + bjt ) , eq ( log p ( wjn = w|k ) ) = ( kw ) ( ( cid : 123 )
the expectations involved above are taken under the varia - tional distribution q , and are eq ( log k ) = eq ( log ( cid : 123 ) eq ( log ( cid : 123 ) eq ( log ( 123 ( cid : 123 ) eq ( log jt ) = eq
k ) ) = ( vk ) ( uk + vk ) ,
k ) = ( uk ) ( uk + vk ) ,
eq ( log ( 123 ( cid : 123 )
n jnti ( wjn = w ) ) ,
where ( ) is the digamma function .
unlike previous variational inference methods for the hdp ( 123 , 123 ) , this method only contains simple closed - form updates due to the full conjugacy of the stick - breaking con - struction .
( we note that , even in the batch setting , this is a new posterior inference algorithm for the hdp . )
123 online variational inference
we now develop online variational inference for an hdp in online variational inference , we apply stochastic optimization to the variational objective .
we subsample the data ( in this case , documents ) , compute an approximation of the gradient based on the subsample , and follow that gradient with a decreasing step - size .
the key in - sight behind efcient online variational inference is that co - ordinate ascent updates applied in parallel precisely form the natural gradient of the variational objective function ( 123 , 123 ) .
our approach is similar to that described in ( 123 ) .
let d be the total number of documents in the corpus , and dene the variational lower bound for document j as lj = eq
( cid : 123 ) log ( cid : 123 ) p ( wj|cj , zj , ) p ( cj| ( cid : 123 ) ) p ( zj| ( cid : 123 )
+ h ( q ( cj ) ) + h ( q ( zj ) ) + h ( q ( ( cid : 123 )
d ( eq ( log p ( ( cid : 123 ) ) p ( ) ) + h ( q ( ( cid : 123 ) ) ) + h ( q ( ) ) ) .
123 chong wang
david m
we have taken the corpus - wide terms and multiplied them by 123 / d .
with this expression , we can see that the lower bound l in eq .
123 can be written as
j lj = ej ( dlj ) ,
where the expectation is taken over the empirical distribution of the data set .
the expression dlj is the variational lower bound evaluated with d duplicate copies of document j .
with the objective construed as an expectation over our data , online hdp proceeds as follows .
given the exist - ing corpus - level parameters , we rst sample a document j and compute its optimal document - level variational pa - rameters ( aj , bj , j , j ) by coordinate ascent ( see eq .
123 to 123 ) .
then , take the gradient of the corpus - level param - eters ( , u , v ) of dlj , which is a noisy estimate of the gradient of the expectation above .
we follow that gradient according to a decreasing learning rate , and repeat .
natural gradients .
the gradient of the variational objec - tive contains , as a component , the covariance matrix of the variational distribution .
this is a computational problem in topic modeling because each set of topic parameters in - volves a v v covariance matrix , where v is the size of the vocabulary ( e . g . , 123 , 123 ) .
the natural gradient ( 123 ) which is the inverse of the riemannian metric multiplied by the gradienthas a simple form in the variational setting ( 123 ) that allows for fast online inference .
multiplying the gradient by the inverse of riemannian met - ric cancels the covariance matrix of the variational distri - bution , leaving a natural gradient which is much easier to work with .
specically , the natural gradient is structurally equivalent to the coordinate updates of eq 123 to 123 taken in parallel .
( and , in stochastic optimization , we treat the sampled document j as though it is the whole corpus . ) let ( j ) , u ( j ) and v ( j ) be the natural gradients for dlj .
using the analysis in ( 123 , 123 ) , the components of the natural
kw ( j ) = kw + + d ( cid : 123 ) t uk ( j ) = uk + 123 + d ( cid : 123 ) t vk ( j ) = vk + + d ( cid : 123 ) t
t=123 jtk ( ( cid : 123 )
in online inference , an appropriate learning rate to is needed to ensure the parameters to converge to a stationary point ( 123 , 123 ) .
then the updates of , u and v become
u u + tou ( j ) v v + tov ( j ) , where the learning rate to should satisfy
to=123 to = , ( cid : 123 )
n jnti ( wjn = w ) ) ,
123 : initialize = ( k ) k
k=123 , u = ( uk ) k123
k=123 and v =
k=123 randomly .
set to = 123
123 : while stopping criterion is not met do
fetch a random document j from the corpus .
compute aj , bj , j and j using variational infer - ence using document - level updates , eq .
123 to 123
compute the natural gradients , ( j ) , u ( j ) and v ( j ) using eq .
123 to 123
set to = ( 123 + to ) , to to + 123
update , u and v using eq .
123 to 123
123 : end while
figure 123 : online variational inference for the hdp
which ensures convergence ( 123 ) .
in our experiments , we use to = ( 123 + to ) , where ( 123 , 123 ) and 123 > 123
note that the natural gradient is essential to the efciency of the algorithm .
the online variational inference algorithm for the hdp topic model is illustrated in figure 123
mini - batches .
to improve stability of the online learning algorithm , practitioners typically use multiple samples to compute gradients at a timea small set of documents in our case .
let s be a small set of documents and s = |s| be its size .
in this case , rather than computing the natural js lj .
the update
gradients using dlj , we use ( d / s ) ( cid : 123 )
equations can then be similarly derived .
123 experimental results
in this section , we evaluate the performance of online varia - tional hdp compared with batch variational hdp and online
123 data and metric
data sets .
our experiments are based on two datasets : nature : this dataset contains 123 , 123 documents , with about 123 million tokens and a vocabulary size of 123 , 123
these articles are from the years 123 to 123
pnas : the proceedings of the national academy of sciences ( pnas ) dataset contains 123 , 123 documents , with about 123 million tokens and a vocabulary size of 123 , 123
these articles are from the years 123 to 123
standard stop words and those words that appear too fre - quently or too rarely are removed .
evaluation metric .
we use the following evaluation met - ric to compare performance .
for each dataset , we held out 123 documents as a test set dtest , with the remainder as training data dtrain .
for testing , we split document wj in dtest into two parts , wj = ( wj123 , wj123 ) , and compute
123 online variational inference for the hierarchical dirichlet process
the predictive likelihood of the second part wj123 ( 123% of the words ) conditioned on the rst part wj123 ( 123% of the words ) and on the training data .
this is similar to the met - rics used in ( 123 , 123 ) , which tries to avoid comparing different hyperparameters .
the metric is
where |wj123| is the number of tokens in wj123 and pw means per - word .
exact computation is intractable , and so we use the following approximation .
for all algorithms , let be the variational expectation of given dtrain .
for lda , let j be the variational expectation given wj123 and be its dirichlet hyperparameter for topic proportions .
the predictive marginal probability of wj123 is approximated by
to use this approximation for the hdp , we set the dirichlet , where is the variational hyperparameter to = 123 expectation of , obtained from the variational expectation
experimental settings .
for the hdp , we set = 123 = 123 , although using priors is also an option .
we set the top - level truncation k = 123 and the second level truncation t = 123
here t ( cid : 123 ) k , since docu - ments usually dont have many topics .
for online vari - ational lda , we set its dirichlet hyperparameter = ( 123 / k , .
, 123 / k ) , where k is the number of topics; we set k = ( 123 , 123 , 123 , 123 , 123 , 123 ) . 123 we set 123 = 123 based on the suggestions in ( 123 ) , and vary = ( 123 , 123 , 123 ) and the batch size s = ( 123 , 123 , 123 , 123 , 123 ) .
we collected experimental results during runs of 123 hours each . 123
in figure 123 , we plot the per - word log likelihood as a function of computation time for online hdp , online lda , and batch hdp .
( for the online algorithms , we set = 123 and the batch size was s = 123 ) this gure shows that online hdp performs better than online lda .
the hdp uses about 123 topics out of its potential 123
in contrast , online lda uses almost all the topics and exhibits overtting at 123 topics .
note that batch hdp is only trained on a subset of 123 , 123 documentsotherwise it is too slowand its performance suffers .
in figure 123 , we plot the per - word likelihood after 123 hours of computation , exploring the effect of batch size and values of .
we see that , overall , online hdp performs better than online lda .
( this matches the reported results in ( 123 ) , which compares batch variational inference for the hdp and
123this is different from the top level truncation k in the hdp .
123the python package will be available at rst authors home -
figure 123 : experimental results on nature with = 123 and batch size s = 123 ( for the online algorithms ) .
points are sub - sampled for better view .
the label olda - 123 indicates online lda with 123 topics .
( not all numbers of topics are shown; see figure 123 for more details . ) online hdp performs better than online lda and batch hdp .
lda . ) further , we found that small favors larger batch sizes .
( this matches the results seen for online lda in ( 123 ) . ) we also ran online hdp on the full nature dataset using only one pass ( with = 123 and a batch size s = 123 ) by sequentially processing the articles from the year 123 to 123
table 123 tracks the most probable ten words from two topics as we encounter more articles in the collection .
note that the hdp here is not a dynamic topic model ( 123 , 123 ) ; we show these results to demonstrate the online inference these results show that online inference for streaming data nds different topics at different speeds , since the relevant information for each topic does not come at the same time .
in this sequential setting , some topics are rarely used until there are documents that can provide enough information to update them ( see the top topic in table 123 ) .
other topics are updated throughout the stream because relevant documents occur throughout the whole collection ( see the bottom topic in table 123 ) .
pnas corpus we ran the same experiments on the pnas corpus .
since pnas is smaller than nature , we were able to run batch hdp on the whole data set .
figure 123 shows the result with = 123 and batch size s = 123
online hdp performs better than online lda .
here batch hdp performs a little better than online hdp , but online hdp is much faster .
figure 123 plots the comparison between online hdp and online lda across different batch sizes and values
we developed an online variational inference algorithm for the hierarchical dirichlet process topic model .
our algo -
123naturetime ( in seconds , log scale ) perword log likelihood123 . 123 . 123 . 123lllllllllllllllllllllllllllllllllllll123 . 123algorithmllllllohdpbhdpolda123olda123olda123olda123 chong wang
david m
figure 123 : comparisons of online lda and online hdp on the nature corpus under various settings of batch size s and parameter ( kappa ) , run for 123 hours each .
( some lines for online hdp and points for online lda do not appear due to gure limits . ) the best result among all is achieved by online hdp .
figure 123 : comparisons of online lda and online ohdp on the pnas corpus under various settings of batch size s and parameter ( kappa ) , run for 123 hours each .
( some lines for online hdp and points for online lda do not appear due to gure limits . ) the best result among all is achieved by online hdp .
123naturetopiclikelihood123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123s=123lllllllllllllll123s=123lllllllllllllllll123s=123llllllllllllllllll123s=123llllllllllllllllll123s=123llllllllllllllllll123kappa=123kappa=123kappa=123algorithmloldalohdppnastopiclikelihood123 . 123 . 123 . 123 . 123 . 123 . 123 . 123s=123lllllllllllll123s=123llllllllllllllllll123s=123llllllllllllllllll123s=123llllllllllllllllll123s=123lllllllllllllllll123kappa=123kappa=123kappa=123algorithmloldalohdp online variational inference for the hierarchical dirichlet process
table 123 : the top ten words from two topics , displayed after different numbers of documents have been processed for inference .
the two topics are separated by the dashed line .
the rst line of the table indicates the number of articles seen so far ( beginning from the year 123 ) .
the topic on the top ( which could be labeled neuroscience research on rats ) does not have a clear meaning until we have analyzed 123 , 123 documents .
this topic is rarely used in the earlier part of the corpus and few documents provide useful information about it .
in contrast , the topic on the bottom ( which could be labeled astronomy research ) has a clearer meaning from the beginning .
this subject is discussed earlier in nature history .
rithm is based on a stick - breaking construction of the hdp that allows for closed - form coordinate ascent variational inference , which is a key factor in developing the online al - gorithm .
our experimental results show that for large - scale applications , the online variational inference for the hdp can address the model selection problem for lda and avoid the application of natural gradient learning to online vari - ational inference may be generalized to other bayesian nonparametric models , as long as we can construct varia - tional inference algorithms with closed form updates un - der conjugacy .
for example , the indian buffet process ( ibp ) ( 123 , 123 , 123 ) might be another model that can use an efcient online variational inference algorithm for large and streaming data sets .
acknowledgement .
chong wang is supported by google phd fellowship .
david m .
blei is supported by onr 123 - 123 , nsf career 123 , afosr 123nl123 , the al - fred p .
sloan foundation , and a grant from google .
