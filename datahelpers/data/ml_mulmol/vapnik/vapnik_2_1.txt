we present a method for discovering informative patterns from data .
with this method , large databases can be reduced to only a few representative data en - tries .
our framework encompasses also methods for cleaning databases containing corrupted data .
both on - line and off - line algorithms are proposed and experimen - tally checked on databases of handwritten images .
the generality of the framework makes it an attractive candidate for new applications in knowledge discovery .
keywords - knowledge discovery , machine learning , cleaning , information gain .
informative patterns , data
databases often contain redundant data .
it would be convenient if large databases could be replaced by only a subset of informative patterns .
a difficult , yet important problem , is to define what informative patterns are .
we use the learning theoretic definition ( 123 , 123 , 123 ) : given a model trained on a sequence of patterns , to predict by a model trained on previously seen data .
with that definition , we derive on - line and batch algorithms for discovering informative patterns .
the techniques were developed for classification regression and density estimation problems .
problems , but are also applicable
a new pattern
informative patterns are often intermixed with other " bad " outlyers which corre - spond to errors introduced non - intentionally in the database .
for databases containing errors , our algorithms can be used to do computer - aided data cleaning , with or without supervision .
we review several results of experiments in handwriting recognition ( 123 , 123 , 123 ) which demonstrate the usefulness of our data cleaning techniques .
aaai - 123 workshop on knowledge discovery
label one w ~ level -
figure 123 : small example database containing " zeros " and " ones " .
( a ) a data entry .
( b ) a sequence of data entries during a training session showing the variation of the surprise level .
the patterns which are most surprising are most informative .
in this section , we assume that the data is perfectly clean .
first , we give an intuition of what informative patterns ought to be .
then , we show that this intuition coincides with the information theoretic definition .
finally , we derive algorithms to discover
123 informative patterns are most surprising
in figure i , we constructed a small example database containing only handwritten zeros and . ones .
most patterns of a given category look similar .
a typical zero is a circle and a typical one is a vertical bar .
however , there exist other shapes of zeros and ones .
if we wanted to keep only a few data representatives , we would probably keep at least one example of each basic shape .
to choose the best data representatives , we run an imaginary experiment .
imagine
that we found 123 people who did not know at all what the shape of a zero and that of a one are .
we teach those people to recognize zeros and ones by letting them examine the patterns of our database in sequence .
every time we show them a new image , we first hide the label and let them make a guess .
we show in figure 123 the average value of the guesses for a particular sequence of data .
since we assumed that our subjects had never seen a zero nor a one before the experiment , about 123% guessed " zero " and 123% guessed " one " when they were shown the first pattern .
but , for the second example of the same shape , the majority made the correct guess .
as learning goes on , familiar shapes are guessed more and more accurately arid the percentage of wrong guesses raises only occasionally when a new
we represent with the size of a question mark the average amount of surprise that was generated among our subjects when the true label was uncovered to them .
people who guessed the correct label where not surprised while people who made the wrong guess were surprised .
we see on figure 123 that a large average surprise
aaai - 123 workshop on knowledge discovery
with the apparition of a new shape .
therefore , the level of surprise is a good indication of how informative a pattern is .
more formally , the level of surprise varies in the opposite direction as the probability
of guessing the correct label pk ( ~ k = yk ) = p ( ~ k = yk ( z ~ ; ( x123 , y123 ) , ( zl , yl ) , . . . ( xk - 123 , yk - 123 ) ) l .
this probability of making the correct guess is precisely what is involved in shannons
where sk is an image , yk e ( 123 , 123 ) is its associated label , k - 123 is the number of data entries seen thus far and ~ k is the label predicted for pattern xk .
the log dependency ensures additivity of information quantities .
in the information theoretic sense , the data entries that are most informative are those that are most surprising .
123 machine learning techniques
it is somewhat unrealistic let us now replace people with machines .
to hire 123 ignorant people to estimate the information gain .
in the machine learning framework , patterns drawn from a database are presented to the learning machine which makes predictions .
the prediction error is evaluated and used to improve the accuracy of further predictions by adjusting the learning machine
that we trained 123 different
its own value of ~ k .
this is referred
learning machines ( substituting our 123 people ) , each one predicting to as a " bayesian " approach ( 123 ) .
formula 123 can be readily used to determine the information gain .
al - is a perfectly valid method , we propose here a more economical one : we train a single learning machine to give an estimate pk ( yk = 123 ) of the probability the correct label is " one " ( figure 123 ) .
our prediction ~ k will me the most likely category according to pk ( yk - " 123 ) .
in formula 123 , we substitute
l= ~ k ( yk - " 123 ) to pk ( yk " - 123 ) .
many machine learning techniques can be applied to discover informative patterns .
for example , the learning machine can be a simple k - nearest - neighbor classifier patterns presented to the classifier are stored .
jbk ( yk " - 123 ) is given by the fraction of the k training patterns that are nearest to xk which have label " one " .
another example is a neural network trained with a " cross - entropy " cost function for which the information gain is the cost function itself .
the mean square error cost function ( yk - - pk ( yk - - 123 ) ) 123 provides an information criterion which ranks patterns in the same order as the cross - entropy cost function and can therefore be used as well .
in the following , we use the size of the symbol " question mark " , in the figures , and to represent the information criteria used to measure the
in the text ,
the notation i ( k ) ,
123to be perfectly correct , the probability should be also conditioned on the model class .
aaai - 123 workshop on knowledge discovery
the information gain ( or surprise level ) .
figure 123 : a learning machine used to predict in the classical machine learning framework , the goal is to train the learning machine , in our framework the goal is to discover the informative patterns of the database ( dashed line ) .
to provide a model of the data or to make predictions .
123 on - line algorithms
and batch algorithms
in an on - line algorithm , patterns are presented in sequence and the learning machine adjusts its parameters at each presentation of a new pattern .
this is a situation similar to that of our example of section 123 .
in that case , we will say that a pattern informative if the information gain exceeds a pre - defined threshold .
the disadvantage of this method is that informative patterns depend on the sequence in which patterns are presented .
however , there may be practical situations where the data is only available
in a batch algorithm , conversely , all data entries are available at once and the infor - if there are p data in the database , we need to train p machines , each one on all the data but one this is feasible only if
mation gain is independent on pattern ordering .
this implies that , pattern , and then try to predict that last pattern .
in practice , training is unexpensive , as for the k - nearest - neighbor algorithm .
for other batch algorithms , we rather train the learning machine only once on all the data .
we then approximate the information gain of each pattern with an estimate of how much the cumulative information gain would decrease if we removed that pattern from the training set .
for batch algorithms all the patterns of the database are uniquely ranked according to their information gain .
the m most informative patterns can be selected to represent the entire database .
123 minimax algorithms
minimax algorithms are batch algorithms that are particularly well suited to discover informative patterns . - most algorithms train the learning machine to minimize the average loss ( e . g .
the mean - square - error ) .
minimax algorithms minimize the maximum
aaai - 123 workshop on knowledge discovery in databases
where w represents the parameters of the learning machine and k runs over all data entries .
minimax algorithms are extreme cases of some " active learning " methods which emphasize the patterns with large information gain ( 123 ) .
the solution of a minimax algorithm is a function of only a small subset of the training patterns , precisely called
these are the patterns
that have maximum loss .
in reference ( 123 ) , we propose and study a minimax algorithm for classification
lems : the optimum margin classifier .
the algorithm maximize the minimum distance of the training patterns to the decision boundary .
it is shown that the solution w* is a linear combination of basis functions of the informative patterns :
ak > 123 ,
where yk ~ ( 123 , 123 ) indicates are all zeros , except for the informative patterns .
we use the value of the cost function at the solution as cumulative information criterion :
the class membership and the ak coefficients
from which we derive and estimate of the information informative pattern k :
loss incurred by removing the
one important question is : what is the rate of growth of the number of informative patterns with the size of the database .
the results of experiments carried out on a database of handwritten digits using the optimal margin classifier suggest a logarithmic growth , in the regime when the number of patterns is small compared to the total number of distinct patterns ( figure 123 ) .
other minimax algorithms have been proposed for classification
and regression ( 123 , 123 ) .
123 data cleaning
in this section we tackle the problem of real world databases which may contain cor - rupted data entries .
we propose data cleaning algorithms and analyze experimental
123 garbage patterns are also surprising
the information theoretic definition of infor , , , ative pattern does not always coincide with the common sense definition .
in figure 123 , we show examples of patterns drawn from our database of " zeros " and " ones " , which have a large information gain .
we see two kinds of patterns :
patterns that are actually garbage patterns : meaningless or mislabeled patterns .
informative : atypical shapes or ambiguous shapes .
aaai - 123 workshop on knowledge discovery in databases
123ooo123ooo123o123 123obo . izeof
figure 123 : variation of the number of informative patterns as a function of the number in the database .
experiments were carried out on a database of handwrit - ten digits , encoded with 123 real - valued global features derived from the pen trajectory information .
a polynomial classifier of degree 123 was trained with a minimax algo - rithm ( optimal margin classifier ) .
the informative patterns are the patterns for which i ( zk ) = ak #
meaningless " " " mislabeled
figure 123 : ( a ) informative patterns versus ( b ) garbage patterns .
are intermixed with garbage patterns which also generate a lot of surprise ( i . e .
have large information gain ) .
aaai - 123 workshop on knowledge discovery
figure 123 : flow diagram of on - line cleaning .
in the process of learning , patterns which information gain exceeds threshold 123 are examined by a human operator .
good patterns are kept in the database and sent to the recognizer for adaptation .
bad patterns are removed from the database .
truly informative patterns should be kept in the database while garbage patterns
should be eliminated .
purely automatic cleaning could be performed by eliminating systematically all pat - terns with suspiciously large information gain .
however , this is dangerous since valuable informative patterns may also be eliminated .
purely manual cleaning , by examining all in the database , is tedious and impractical for very large databases .
we pro - pose a computer - aided cleaning method where a human operator must check only those
information gain and are therefore most suspicious .
that have largest
123 on - line algorithms
and batch algorithms
in figure 123 we present our on - line version of data cleaning .
it combines cleaning and trained with a few clean training in one single session .
the learning machine is initially examples .
at step k of the cleaning process , a new pattern xk is presented learning machine .
the prediction of the learning machine and the desired value yk are used to compute the information criterion i ( k ) .
if i ( k ) is below a given threshold 123 , the pattern is directly sent to the learning machine for adaptation .
otherwise , the pattern is sent to the human operator for checking .
depending on the decision of the operator , the pattern is either
trashed or sent to the learning machine for adaptation .
the data has been processed , both training and cleaning are completed , since the learning machine was trained only on clean data .
this is an advantage if further use is made of the learning machine .
but on - line cleaning has several disadvantages :
one has to find find an appropriate threshold 123
aaai - 123 workshop on knowledge discovery
gain ( surprise level )
i ~ ! manually remove i
i ~ | the bad data entriesi
with the ~
figure 123 : block diagram of batch cleaning .
the learning macl ~ ne is trained on unclean data .
the information information would be lost the learning machine needs to be for cleaning by the human operator .
after cleaning ,
that pattern would be removed .
the database
to obtain a good model and / or make good predictions .
only the top ranked patterns
to the information
for each pattern
learning machine may be slower in wasting the time of the operator .
the patterns which
the method depends on the order of presentation of the patterns;
is not possible
to revert a decision on whether a pattern
is good or bad .
batch methods are preferred
is computed as explained
performed on all patterns , " good " patterns exceeds a given threshold .
may be necessary
order of information
in sections 123 and 123 .
the data entries
the patterns
gain of the are examined by the the number of consecutive
if the database contains correlated
from the top ( most suspicious ) ,
this procedure several
the combination of batch cleaning
in that case , only informative patterns
be examined .
in figure 123 we show the first discriminate between the digit " two " and all ak ( our estimate of the information gain from equation 123 ) is a garbage pattern .
( the optimum margin classifier
and minimax algorithms
times to remove all " bad " patterns .
information gain ) need ( 123 ) ) .
the classifier
obtained with a
the other digits .
the patterns with largest
of optimal cleaning
we may wonder how reliable
that should be candidates
did we remove too many or too few patterns ?
techniques are : did we examine all
for cleaning ? among the patterns
that were candidates
aaai - 123 workshop on knowledge discovery
obtained by a minimax algorithm
figure 123 : the informative patterns margin algorithm ) , for the separation of handwritten digit " two " against all other ( 123 ) .
patterns are represented by a 123x123 grey - level pixel - map .
the informative patterns are shown in order of decreasing information gain . ( a ) other classes : several ambiguous or atypical shapes are among the patterns with largest
for class 123 : a garbage pattern comes first .
( b ) informative patterns
training error ( % )
validation error ( % )
figure 123 : data cleaning results showing a point of optimal cleaning ( 123 ) .
a neural network was trained with a mean - square - error cost function to recognize handwritten lowercase letters .
the representation is 123 local features of the pen trajectory .
a training data set of 123 characters and a validation data set of 123 characters from disjoint set of writers was used for clea ~ ning .
starting with the " uncleaned " database , several levels of cleaning were applied by the human operator; each stage was more strict , lowered the tolerance for marginal - quality characters .
to test the power of the cleaning technique , we also corrupted the initial , uncleaned database , by artificially mislabeling 123% of the characters in the training set ( left most column ) .
aaai - 123 workshop on knowledge discovery
we can again use our learning machine to provide us with an answer .
in the exper - iments of figure 123 , we varied the amount of cleaning .
the data was split into a training set and a validation set .
the learning machine was trained on data from the training set with various amounts of cleaning .
it was tested with the unclean data of the validation set .
we observe that , as more patterns are removed through the cleaning process , error rate on the training set decreases .
this is understandable since the learning task to the learning machine .
the validation error however goes becomes easier and easier through a minimum .
this is because we remove first only really bad patterns , removing ~ aluable informative patterns that we mistook for garbage patterns .
the minimum of the validation error is the point of optimal cleaning .
if our validation error does not go through a minimum , more patterns should be examined and considered for
it is not always possible nor desirable
the database into a training and a validation set .
another way of obtaining the point of optimum cleaning is to use the predictions of the vapnik - chervonenkis ( vc ) theory ( 123
according to the vc - theory , the point of optimum cleaning is the minimum of the so - called " guaranteed risk " , which is a bound on the validation set frequency of errors :
bd ( in123 - - ~ ~ + 123 ) + m ( ln ~ + 123 )
where / ~ is a constant which was experimentally determined with the method de - is the frequency training errors when m
scribed in reference ( 123 ) ( / ~ - 123 ) , et , . , , , , ( m ) suspicious patterns have been removed from the training set , p is the number of train - ing patterns before cleaning ( p - - 123 ) and d is the vc - dimension .
these predictions can be made only if the capacity ( or vc - dimension ) of the learning machine is known and if the training algorithm does not learn the training set with in our experiments on data cleaning wi ~ h neural networks ( figure 123 ) ( 123 ) we " obtained good agreement between the prediction of the vc - theory and that of the validation set , even with a very rough estimate of the vc - dimension .
in fact , we checked the robustness of the vc - prediction of optimum cleaning with respect to a change in the estimate of the vc - dimension .
we found no significant change in the position of the optimum in the range 123 < d < 123
we presented a computer - aided method for detecting informative patterns and cleaning data .
we used this method on various databases of handwritten characters .
the results the number of informative are summarized in table 123
it is important to stress patterns varies sub - linearly with the number of data entries and that cleaning time varies sub - linearly with the num__ber of data entries .
the point of optimum cleaning can be predictedby between a training set and a validation set .
our cleaning method clearly quality of the data , as measured by the improvement in classification a test set independent from the training to other problems than classification problems ( regression or density estimation ) which makes it attractive
our framework is general and applies
the vc - theory which does not require splitting
in knowledge discovery .
for new applications
aaai - 123 workshop on knowledge discovery
: f ( f . ~ = . , p , m , d )
i123 123 123 123 123 123 123
% removed pattems
a neural network was trained
figure 123 : detection of the point of optimal cleaning with neural network was estimated free .
parameters ) .
g has been rescaled
to d=123 ( approximately to fit on the figure .
the same as in figure 123
the vc - dimension of the of the number of
the maximum squared
