support vector learning machines ( cid : 123 ) svm ( cid : 123 ) are ( cid : 123 ) nding application in pattern recognition ( cid : 123 ) regression estimation ( cid : 123 ) and operator inver ( cid : 123 ) sion for ill ( cid : 123 ) posed problems ( cid : 123 ) against this very general backdrop ( cid : 123 ) any methods for improving the generalization performance ( cid : 123 ) or for improving the speed in test phase ( cid : 123 ) of svms are of increasing in ( cid : 123 ) terest ( cid : 123 ) in this paper we combine two such techniques on a pattern recognition problem ( cid : 123 ) the method for improving generalization per ( cid : 123 ) formance ( cid : 123 ) the ( cid : 123 ) virtual support vector ( cid : 123 ) method ( cid : 123 ) does so by incor ( cid : 123 ) porating known invariances of the problem ( cid : 123 ) this method achieves a drop in the error rate on ( cid : 123 ) nist test digit images of ( cid : 123 ) ( cid : 123 ) to ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) the method for improving the speed ( cid : 123 ) the ( cid : 123 ) reduced set ( cid : 123 ) method ( cid : 123 ) does so by approximating the support vector decision sur ( cid : 123 ) face ( cid : 123 ) we apply this method to achieve a factor of ( cid : 123 ) fty speedup in test phase over the virtual support vector machine ( cid : 123 ) the combined approach yields a machine which is both times faster than the original machine ( cid : 123 ) and which has better generalization performance ( cid : 123 ) achieving ( cid : 123 ) ( cid : 123 ) error ( cid : 123 ) the virtual support vector method is appli ( cid : 123 ) cable to any svm problem with known invariances ( cid : 123 ) the reduced set method is applicable to any support vector machine ( cid : 123 )
support vector machines are known to give good results on pattern recognition problems despite the fact that they do not incorporate problem domain knowledge ( cid : 123 )
( cid : 123 ) part of this work was done while b ( cid : 123 ) s ( cid : 123 ) was with at ( cid : 123 ) t research ( cid : 123 ) holmdel ( cid : 123 ) nj ( cid : 123 )
however ( cid : 123 ) they exhibit classi ( cid : 123 ) cation speeds which are substantially slower than those of neural networks ( cid : 123 ) lecun et al ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
the present study is motivated by the above two observations ( cid : 123 ) first ( cid : 123 ) we shall improve accuracy by incorporating knowledge about invariances of the problem at hand ( cid : 123 ) second ( cid : 123 ) we shall increase classi ( cid : 123 ) cation speed by reducing the complexity of the decision function representation ( cid : 123 ) this paper thus brings together two threads explored by us during the last year ( cid : 123 ) sch ( cid : 123 ) olkopf ( cid : 123 ) burges ( cid : 123 ) vapnik ( cid : 123 ) ( cid : 123 ) burges ( cid : 123 )
the method for incorporating invariances is applicable to any problem for which the data is expected to have known symmetries ( cid : 123 ) the method for improving the speed is applicable to any support vector machine ( cid : 123 ) thus we expect these methods to be widely applicable to problems beyond pattern recognition ( cid : 123 ) for example ( cid : 123 ) to the regression estimation problem ( cid : 123 ) vapnik ( cid : 123 ) golowich ( cid : 123 ) smola ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
after a brief overview of support vector machines in section ( cid : 123 ) we describe how problem domain knowledge was used to improve generalization performance in sec ( cid : 123 ) tion ( cid : 123 ) section contains an overview of a general method for improving the classi ( cid : 123 ) cation speed of support vector machines ( cid : 123 ) results are collected in section ( cid : 123 ) we conclude with a discussion ( cid : 123 )
support vector learning machines
this section summarizes those properties of support vector machines ( cid : 123 ) svm ( cid : 123 ) which are relevant to the discussion below ( cid : 123 ) for details on the basic svm approach ( cid : 123 ) the reader is referred to ( cid : 123 ) boser ( cid : 123 ) guyon ( cid : 123 ) vapnik ( cid : 123 ) ( cid : 123 ) cortes ( cid : 123 ) vapnik ( cid : 123 ) ( cid : 123 ) vapnik ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) we end by noting a physical analogy ( cid : 123 )
let the training data be elements xi l ( cid : 123 ) l ( cid : 123 ) rd ( cid : 123 ) i ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) with corresponding class labels yi f ( cid : 123 ) g ( cid : 123 ) an svm performs a mapping ( cid : 123 ) ( cid : 123 ) l ( cid : 123 ) h ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) x into a high ( cid : 123 ) possibly in ( cid : 123 ) nite ( cid : 123 ) dimensional hilbert space h ( cid : 123 ) in the following ( cid : 123 ) vectors in h will be denoted with a bar ( cid : 123 ) in h ( cid : 123 ) the svm decision rule is simply a separating hyperplane ( cid : 123 ) the algorithm constructs a decision surface with normal ( cid : 123 ) ( cid : 123 ) h which separates the xi into two classes ( cid : 123 )
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) b ( cid : 123 ) k ( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) yi ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) b ( cid : 123 ) k ( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) yi ( cid : 123 ) ( cid : 123 )
where the ( cid : 123 ) i are positive slack variables ( cid : 123 ) introduced to handle the non ( cid : 123 ) separable case ( cid : 123 ) cortes ( cid : 123 ) vapnik ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) and where k and k are typically de ( cid : 123 ) ned to be ( cid : 123 ) and ( cid : 123 ) ( cid : 123 ) respectively ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) is computed by minimizing the objective function
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
subject to ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) where c is a constant ( cid : 123 ) and we choose p ( cid : 123 ) ( cid : 123 ) in the separable case ( cid : 123 ) the svm algorithm constructs that separating hyperplane for which the margin between the positive and negative examples in h is maximized ( cid : 123 ) a test vector x l is then assigned a class label f ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) g depending on whether ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) b is greater or less than ( cid : 123 ) k ( cid : 123 ) k ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) support vectors sj l are de ( cid : 123 ) ned as training samples for which one of equations ( cid : 123 ) ( cid : 123 ) or ( cid : 123 ) ( cid : 123 ) is an equality ( cid : 123 ) ( cid : 123 ) we name the support vectors s to distinguish them from the rest of the training data ( cid : 123 ) ( cid : 123 ) the solution ( cid : 123 ) ( cid : 123 ) may be
where ( cid : 123 ) j ( cid : 123 ) are the positive weights ( cid : 123 ) determined during training ( cid : 123 ) yj f ( cid : 123 ) g the class labels of the sj ( cid : 123 ) and ns the number of support vectors ( cid : 123 ) thus in order to classify a test point x one must compute
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 )
( cid : 123 ) jyj ( cid : 123 ) sj ( cid : 123 ) ( cid : 123 ) x ( cid : 123 )
( cid : 123 ) jyj ( cid : 123 ) ( cid : 123 ) sj ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 )
one of the key properties of support vector machines is the use of the kernel k to compute dot products in h without having to explicitly compute the mapping ( cid : 123 ) ( cid : 123 )
it is interesting to note that the solution has a simple physical interpretation in the high dimensional space h ( cid : 123 ) if we assume that each support vector ( cid : 123 ) sj exerts a perpendicular force of size ( cid : 123 ) j and sign yj on a solid plane sheet lying along the hyperplane ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) b ( cid : 123 ) ( cid : 123 ) k ( cid : 123 ) k ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) then the solution satis ( cid : 123 ) es the requirements of mechanical stability ( cid : 123 ) at the solution ( cid : 123 ) the ( cid : 123 ) j can be shown to satisfy pns j ( cid : 123 ) ( cid : 123 ) jyj ( cid : 123 ) ( cid : 123 ) which translates into the forces on the sheet summing to zero ( cid : 123 ) and equation ( cid : 123 ) ( cid : 123 ) implies that the torques also sum to zero ( cid : 123 )
this section follows the reasoning of ( cid : 123 ) sch ( cid : 123 ) olkopf ( cid : 123 ) burges ( cid : 123 ) ( cid : 123 ) vapnik ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) problem domain knowledge can be incorporated in two di ( cid : 123 ) erent ways ( cid : 123 ) the knowledge can be directly built into the algorithm ( cid : 123 ) or it can be used to generate arti ( cid : 123 ) cial training examples ( cid : 123 ) ( cid : 123 ) virtual examples ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) the latter signi ( cid : 123 ) cantly slows down training times ( cid : 123 ) due to both correlations in the arti ( cid : 123 ) cial data and to the increased training set size ( cid : 123 ) simard et al ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) however it has the advantage of being readily implemented for any learning machine and for any invariances ( cid : 123 ) for instance ( cid : 123 ) if instead of lie groups of symmetry transformations one is dealing with discrete symmetries ( cid : 123 ) such as the bilateral symmetries of vetter ( cid : 123 ) poggio ( cid : 123 ) ( cid : 123 ) b ( cid : 123 ) ultho ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) then derivative ( cid : 123 ) based methods ( cid : 123 ) e ( cid : 123 ) g ( cid : 123 ) simard et al ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) are not applicable ( cid : 123 )
for support vector machines ( cid : 123 ) an intermediate method which combines the advan ( cid : 123 ) tages of both approaches is possible ( cid : 123 ) the support vectors characterize the solution to the problem in the following sense ( cid : 123 ) if all the other training data were removed ( cid : 123 ) and the system retrained ( cid : 123 ) then the solution would be unchanged ( cid : 123 ) furthermore ( cid : 123 ) those support vectors ( cid : 123 ) si which are not errors are close to the decision boundary in h ( cid : 123 ) in the sense that they either lie exactly on the margin ( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) ( cid : 123 ) or close to it ( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) finally ( cid : 123 ) di ( cid : 123 ) erent types of svm ( cid : 123 ) built using di ( cid : 123 ) erent kernels ( cid : 123 ) tend to produce the same set of support vectors ( cid : 123 ) sch ( cid : 123 ) olkopf ( cid : 123 ) burges ( cid : 123 ) ( cid : 123 ) vapnik ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) this suggests the following algorithm ( cid : 123 ) ( cid : 123 ) rst ( cid : 123 ) train an svm to generate a set of support vectors fs ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) sns g ( cid : 123 ) then ( cid : 123 ) generate the arti ( cid : 123 ) cial examples ( cid : 123 ) virtual support vec ( cid : 123 ) tors ( cid : 123 ) by applying the desired invariance transformations to fs ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) snsg ( cid : 123 ) ( cid : 123 ) nally ( cid : 123 ) train another svm on the new set ( cid : 123 ) to build a ten ( cid : 123 ) class classi ( cid : 123 ) er ( cid : 123 ) this procedure is carried out separately for ten binary classi ( cid : 123 ) ers ( cid : 123 )
apart from the increase in overall training time ( cid : 123 ) by a factor of two ( cid : 123 ) in our ex ( cid : 123 ) periments ( cid : 123 ) ( cid : 123 ) this technique has the disadvantage that many of the virtual support vectors become support vectors for the second machine ( cid : 123 ) increasing the number of summands in equation ( cid : 123 ) ( cid : 123 ) and hence decreasing classi ( cid : 123 ) cation speed ( cid : 123 ) however ( cid : 123 ) the latter problem can be solved with the reduced set method ( cid : 123 ) which we describe next ( cid : 123 )
improving classification speed
the discussion in this section follows that of ( cid : 123 ) burges ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) consider a set of vectors zk l ( cid : 123 ) k ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) nz and corresponding weights ( cid : 123 ) k r for which
minimizes ( cid : 123 ) for ( cid : 123 ) xed nz ( cid : 123 ) the euclidean distance to the original solution ( cid : 123 )
( cid : 123 ) ( cid : 123 ) k ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) k ( cid : 123 )
note that ( cid : 123 ) ( cid : 123 ) expressed here in terms of vectors in h ( cid : 123 ) can be expressed entirely in terms of functions ( cid : 123 ) using the kernel k ( cid : 123 ) of vectors in the input space l ( cid : 123 ) the f ( cid : 123 ) ( cid : 123 ) k ( cid : 123 ) zk ( cid : 123 ) j k ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) nzg is called the reduced set ( cid : 123 ) to classify a test point x ( cid : 123 ) the expansion in equation ( cid : 123 ) ( cid : 123 ) is replaced by the approximation
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 )
( cid : 123 ) k ( cid : 123 ) zk ( cid : 123 ) ( cid : 123 ) x ( cid : 123 )
the goal is then to choose the smallest nz ( cid : 123 ) ns ( cid : 123 ) and corresponding reduced set ( cid : 123 ) such that any resulting loss in generalization performance remains acceptable ( cid : 123 ) clearly ( cid : 123 ) by allowing nz ( cid : 123 ) ns ( cid : 123 ) ( cid : 123 ) can be made zero ( cid : 123 ) interestingly ( cid : 123 ) there are non ( cid : 123 ) trivial cases where nz ( cid : 123 ) ns and ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) in which case the reduced set leads to an increase in classi ( cid : 123 ) cation speed with no loss in generalization performance ( cid : 123 ) note that reduced set vectors are not support vectors ( cid : 123 ) in that they do not necessarily lie on the separating margin and ( cid : 123 ) unlike support vectors ( cid : 123 ) are not training samples ( cid : 123 )
while the reduced set can be found exactly in some cases ( cid : 123 ) in general an uncon ( cid : 123 ) strained conjugate gradient method is used to ( cid : 123 ) nd the zk ( cid : 123 ) while the corresponding optimal ( cid : 123 ) k can be found exactly ( cid : 123 ) for all k ( cid : 123 ) ( cid : 123 ) the method for ( cid : 123 ) nding the reduced set is computationally very expensive ( cid : 123 ) the ( cid : 123 ) nal phase constitutes a conjugate gradient descent in a space of ( cid : 123 ) d ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) nz variables ( cid : 123 ) which in our case is typically of order
in this section ( cid : 123 ) by ( cid : 123 ) accuracy ( cid : 123 ) we mean generalization performance ( cid : 123 ) and by ( cid : 123 ) speed ( cid : 123 ) we mean classi ( cid : 123 ) cation speed ( cid : 123 ) in our experiments ( cid : 123 ) we used the mnist database of ( cid : 123 ) handwritten digits ( cid : 123 ) which was used in the comparison investigation of lecun et al ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) in that study ( cid : 123 ) the error rate record of ( cid : 123 ) ( cid : 123 ) is held by a boosted convolutional neural network ( cid : 123 ) ( cid : 123 ) lenet ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
we start by summarizing the results of the virtual support vector method ( cid : 123 ) we trained ten binary classi ( cid : 123 ) ers using c ( cid : 123 ) in equation ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) we used a polynomial kernel k ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) combining classi ( cid : 123 ) ers then gave ( cid : 123 ) ( cid : 123 ) error on the ( cid : 123 ) test set ( cid : 123 ) this system is referred to as orig below ( cid : 123 ) we then generated new train ( cid : 123 ) ing data by translating the resulting support vectors by one pixel in each of four directions ( cid : 123 ) and trained a new machine ( cid : 123 ) using the same parameters ( cid : 123 ) ( cid : 123 ) this machine ( cid : 123 ) which is referred to as vsv below ( cid : 123 ) achieved ( cid : 123 ) ( cid : 123 ) error on the test set ( cid : 123 ) the results for each digit are given in table ( cid : 123 )
note that the improvement in accuracy comes at a cost in speed of approximately a factor of ( cid : 123 ) furthermore ( cid : 123 ) the speed of orig was comparatively slow to start with ( cid : 123 ) lecun et al ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) requiring approximately million multiply adds for one
table ( cid : 123 ) generalization performance improvement by incorporating invariances ( cid : 123 ) ne and nsv are the number of errors and number of support vectors respec ( cid : 123 ) tively ( cid : 123 ) ( cid : 123 ) orig ( cid : 123 ) refers to the original support vector machine ( cid : 123 ) ( cid : 123 ) vsv ( cid : 123 ) to the machine trained on virtual support vectors ( cid : 123 )
digit ne orig ne vsv nsv orig nsv vsv
table ( cid : 123 ) dependence of performance of reduced set system on threshold ( cid : 123 ) the numbers in parentheses give the corresponding number of errors on the test set ( cid : 123 ) note that thrsh test gives a lower bound for these numbers ( cid : 123 )
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
classi ( cid : 123 ) cation ( cid : 123 ) this can be reduced by caching results of repeated support vectors in order to become competitive with systems with comparable accuracy ( cid : 123 ) we will need approximately a factor of ( cid : 123 ) fty improvement in speed ( cid : 123 ) we therefore approximated vsv with a reduced set system rs with a factor of ( cid : 123 ) fty fewer vectors than the number of support vectors in vsv ( cid : 123 )
since the reduced set method computes an approximation to the decision surface in the high dimensional space ( cid : 123 ) it is likely that the accuracy of rs could be improved by choosing a di ( cid : 123 ) erent threshold b in equations ( cid : 123 ) ( cid : 123 ) and ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) we computed that threshold which gave the empirical bayes error for the rs system ( cid : 123 ) measured on the training set ( cid : 123 ) this can be done easily by ( cid : 123 ) nding the maximum of the di ( cid : 123 ) erence between the two un ( cid : 123 ) normalized cumulative distributions of the values of the dot products ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) where the xi are the original training data ( cid : 123 ) note that the e ( cid : 123 ) ects of bias are reduced by the fact that vsv ( cid : 123 ) and hence rs ( cid : 123 ) was trained only on shifted data ( cid : 123 ) and not on any of the original data ( cid : 123 ) thus ( cid : 123 ) in the absence of a validation set ( cid : 123 ) the original training data provides a reasonable means of estimating the bayes threshold ( cid : 123 ) this is a serendipitous bonus of the vsv approach ( cid : 123 ) table compares results obtained using the threshold generated by the training procedure for the vsv system ( cid : 123 ) the estimated bayes threshold for the rs system ( cid : 123 ) and ( cid : 123 ) for comparison
table ( cid : 123 ) speed improvement using the reduced set method ( cid : 123 ) the second through fourth columns give numbers of errors on the test set for the original system ( cid : 123 ) the virtual support vector system ( cid : 123 ) and the reduced set system ( cid : 123 ) the last three columns give ( cid : 123 ) for each system ( cid : 123 ) the number of vectors whose dot product must be computed in test phase ( cid : 123 )
digit orig err vsv err rs err orig sv vsv sv rsv
purposes only ( cid : 123 ) to see the maximum possible e ( cid : 123 ) ect of varying the threshold ( cid : 123 ) ( cid : 123 ) the bayes error computed on the test set ( cid : 123 )
table compares results on the test set for the three systems ( cid : 123 ) where the bayes threshold ( cid : 123 ) computed with the training set ( cid : 123 ) was used for rs ( cid : 123 ) the results for all ten digits combined are ( cid : 123 ) ( cid : 123 ) error for orig ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) for vsv ( cid : 123 ) with roughly twice as many multiply adds ( cid : 123 ) and ( cid : 123 ) ( cid : 123 ) for rs ( cid : 123 ) with a factor of fewer multiply adds than
the reduced set conjugate gradient algorithm does not reduce the objective function ( cid : 123 ) ( cid : 123 ) equation ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) to zero ( cid : 123 ) for example ( cid : 123 ) for the ( cid : 123 ) rst digits ( cid : 123 ) ( cid : 123 ) is only reduced on average by a factor of ( cid : 123 ) ( cid : 123 ) the algorithm is stopped when progress becomes too slow ( cid : 123 ) ( cid : 123 ) it is striking that nevertheless ( cid : 123 ) good results are achieved ( cid : 123 )
the only systems in lecun et al ( cid : 123 ) ( cid : 123 ) with better than ( cid : 123 ) ( cid : 123 ) error are lenet ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) error ( cid : 123 ) with approximately k multiply ( cid : 123 ) adds ( cid : 123 ) and boosted lenet ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) error ( cid : 123 ) approximately k multiply ( cid : 123 ) adds ( cid : 123 ) ( cid : 123 ) clearly svms are not in this league yet ( cid : 123 ) the rs system described here requires approximately k multiply ( cid : 123 ) adds ( cid : 123 ) ( cid : 123 )
however ( cid : 123 ) svms present clear opportunities for further improvement ( cid : 123 ) ( cid : 123 ) in fact ( cid : 123 ) we have since trained a vsv system with ( cid : 123 ) ( cid : 123 ) error ( cid : 123 ) by choosing a di ( cid : 123 ) erent kernel ( cid : 123 ) ( cid : 123 ) more invariances ( cid : 123 ) for example ( cid : 123 ) for the pattern recognition case ( cid : 123 ) small rotations ( cid : 123 ) or varying ink thickness ( cid : 123 ) could be added to the virtual support vector approach ( cid : 123 ) further ( cid : 123 ) one might use only those virtual support vectors which provide new infor ( cid : 123 ) mation about the decision boundary ( cid : 123 ) or use a measure of such information to keep only the most important vectors ( cid : 123 ) known invariances could also be built directly into the svm objective function ( cid : 123 )
viewed as an approach to function approximation ( cid : 123 ) the reduced set method is cur ( cid : 123 ) rently restricted in that it assumes a decision function with the same functional form as the original svm ( cid : 123 ) in the case of quadratic kernels ( cid : 123 ) the reduced set can be computed both analytically and e ! ciently ( cid : 123 ) burges ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) however ( cid : 123 ) the conjugate gradient descent computation for the general kernel is very ine ! cient ( cid : 123 ) perhaps re ( cid : 123 )
laxing the above restriction could lead to analytical methods which would apply to more complex kernels also ( cid : 123 )
we wish to thank v ( cid : 123 ) vapnik ( cid : 123 ) a ( cid : 123 ) smola and h ( cid : 123 ) drucker for discussions ( cid : 123 ) c ( cid : 123 ) burges was supported by arpa contract n ( cid : 123 ) ( cid : 123 ) c ( cid : 123 ) ( cid : 123 ) b ( cid : 123 ) sch ( cid : 123 ) olkopf was supported by the studienstiftung des deutschen volkes ( cid : 123 )
