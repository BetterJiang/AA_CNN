we explore methods for incorporating prior knowledge about a problem at hand in support vector learning machines .
we show that both invari - ances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions .
when we are trying to extract regularities from data , we often have additional knowledge about functions that we estimate .
for instance , in image classication tasks , there exist transformations which leave class membership invariant ( e . g .
local translations ) ; moreover , it is usually the case that images have a local structure in that not all correlations between image regions carry equal amounts of information .
the present study investigates the question how to make use of these two sources of know - ledge by designing appropriate support vector ( sv ) kernel functions .
to this end , we start by giving a brief introduction to sv machines , following vapnik & chervonenkis ( 123 ) and vapnik ( 123 ) ( sec .
regarding prior knowledge about invariances , we develop an idea of vapnik ( 123 ) and present a method to design kernel functions which lead to invari - ant classication hyperplanes ( sec .
the method is applicable to invariances under the action of differentiable local 123parameter groups of local transformations , e . g .
translational invariance in pattern recognition .
in sec .
123 , we describe kernels which take into account image locality by using localized receptive elds .
123 presents experimental results on both types of kernels , followed by a discussion ( sec
123 optimal margin hyperplanes
for linear hyperplane decision functions f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) sgn ( cid : 123 ) ( cid : 123 ) w ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) b ( cid : 123 ) , the vcdimension can be controlled by controlling the norm of the weight vector w .
given training data ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xi rn ( cid : 123 ) yi f ( cid : 123 ) g ( cid : 123 ) a separating hyperplane which generalizes
well can be found by minimizing
kwk subject to yi ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) w ( cid : 123 ) ( cid : 123 ) b ( cid : 123 ) ( cid : 123 ) for i ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
the latter being the conditions for separating the training data with a margin .
nonseparable cases are dealt with by introducing slack variables ( cortes & vapnik 123 ) , but we shall omit this modication to simplify the exposition .
all of the following also applies for the
to solve the above convex optimization problem , one introduces a lagrangian with multi - pliers ( cid : 123 ) i and derives the dual form of the optimization problem : maximize
( cid : 123 ) iyi ( cid : 123 ) kyk ( cid : 123 ) xi ( cid : 123 ) xk ( cid : 123 ) subject to ( cid : 123 ) i ( cid : 123 ) ( cid : 123 )
( cid : 123 ) iyi ( cid : 123 ) ( cid : 123 )
it turns out that the solution vector has an expansion in terms of training examples , w ( cid : 123 ) i ( cid : 123 ) ( cid : 123 ) iyixi , where only those ( cid : 123 ) i corresponding to constraints ( 123 ) which are met can become nonzero; the respective examples xi are called support vectors .
substituting this expansion for w yields the decision function
f ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) sgn ( cid : 123 ) ( cid : 123 )
( cid : 123 ) iyi ( cid : 123 ) x ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) b ( cid : 123 ) ( cid : 123 )
it can be shown that minimizing ( 123 ) corresponds to minimizing an upper bound on the vc dimension of separating hyperplanes , or , equivalently , to maximizing the separation margin between the two classes .
in the next section , we shall depart from this and modify the dot product used such that the minimization of ( 123 ) corresponds to enforcing transformation invariance , while at the same time the constraints ( 123 ) still hold .
123 invariant hyperplanes
invariance by a selfconsistency argument .
we face the following problem : to express the condition of invariance of the decision function , we already need to know its coef - cients which are found only during the optimization , which in turn should already take into account the desired invariances .
as a way out of this circle , we use the following ansatz : consider decision functions f ( cid : 123 ) ( cid : 123 ) sgn ( cid : 123 ) g ( cid : 123 ) , where g is dened as
( cid : 123 ) iyi ( cid : 123 ) bxj ( cid : 123 ) bxi ( cid : 123 ) ( cid : 123 ) b ( cid : 123 )
with a matrix b to be determined below .
this follows vapnik ( 123 ) , who suggested to incorporate invariances by modifying the dot product used .
any nonsingular b denes a dot product , which can equivalently be written as ( cid : 123 ) xj ( cid : 123 ) axi ( cid : 123 ) , with a positive denite matrix a ( cid : 123 ) b ( cid : 123 ) b .
clearly , invariance of g under local transformations of all xj is a sufcient condition for the local invariance of f , which is what we are aiming for .
strictly speaking , however , invariance of g is not necessary at points which are not support vectors , since these lie in a region where ( cid : 123 ) sgn ( cid : 123 ) g ( cid : 123 ) is constant however , before training , it is hard to predict which examples will turn out to become svs .
in the virtual sv method ( scholkopf , burges , & vapnik , 123 ) , a rst run of the standard sv algorithm is carried out to obtain an initial sv set; similar heuristics could be applied in the present case .
local invariance of g for each pattern xj under transformations of a differentiable local 123parameter group of local transformations lt ,
g ( cid : 123 ) ltxj ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
can be approximately enforced by minimizing the regularizer
note that the sum may run over labelled as well as unlabelled data , so in principle one could also require the decision function to be invariant with respect to transformations of elements of a test set .
moreover , we could use different transformations for different patterns .
for ( 123 ) , the local invariance term ( 123 ) becomes
( cid : 123 ) iyi ( cid : 123 ) bltxj ( cid : 123 ) bxi ( cid : 123 ) ( cid : 123 ) b ( cid : 123 ) ( cid : 123 )
( cid : 123 ) iyi ( cid : 123 ) ( cid : 123 ) blxj ( cid : 123 ) bxi ( cid : 123 ) ( cid : 123 ) b
ltxj ( cid : 123 ) ( 123 )
using the chain rule .
here , ( cid : 123 ) ( cid : 123 ) blxj ( cid : 123 ) bxi ( cid : 123 ) denotes the gradient of ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) with respect to x , evaluated at the point ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) blxj ( cid : 123 ) bxi ( cid : 123 ) .
substituting ( 123 ) into ( 123 ) , using the facts that l ( cid : 123 ) i and ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) y ( cid : 123 ) , yields the regularizer
q ( cid : 123 ) ( cid : 123 ) b
( cid : 123 ) iyi ( cid : 123 ) kyk ( cid : 123 ) bxi ( cid : 123 ) qbxk ( cid : 123 )
we now choose q such that ( 123 ) reduces to the quadratic term in the standard sv optimiza - tion problem ( 123 ) utilizing the dot product chosen in ( 123 ) , i . e .
such that ( cid : 123 ) bx i ( cid : 123 ) qbxk ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) bxi ( cid : 123 ) bxk ( cid : 123 ) ( cid : 123 ) assuming that the xi span the whole space , this condition becomes b ( cid : 123 ) qb ( cid : 123 ) b ( cid : 123 ) b ( cid : 123 ) or , by requiring b to be nonsingular , i . e .
that no information get lost during the preprocessing , q ( cid : 123 ) i .
this can be satised by a preprocessing matrix
the nonnegative square root of the inverse of the nonnegative matrix
b ( cid : 123 ) c ( cid : 123 )
in practice , we use a matrix
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) instead of c .
as c is nonnegative , c ( cid : 123 ) is invertible .
for ( cid : 123 ) ( cid : 123 ) , we recover the standard sv optimal hyperplane algorithm , other values of ( cid : 123 ) determine the tradeoff be - tween invariance and model complexity control .
it can be shown that using c ( cid : 123 ) corresponds
c ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) c ( cid : 123 ) ( cid : 123 ) i ( cid : 123 )
( cid : 123 ) t jt ( cid : 123 ) ltxi ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) kwk .
to using an objective function ( cid : 123 ) ( cid : 123 ) w ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) pi ( cid : 123 ) w ( cid : 123 ) ( cid : 123 )
by choosing the preprocessing matrix b according to ( 123 ) , we have obtained a formulation of the problem where the standard sv quadratic optimization technique does in effect min - imize the tangent regularizer ( 123 ) : the maximum of ( 123 ) , using the modied dot product as in ( 123 ) , coincides with the minimum of ( 123 ) subject to the separation conditions y i ( cid : 123 ) g ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) , where g is dened as in ( 123 ) .
note that preprocessing with b does not affect classication speed : since ( cid : 123 ) bxj ( cid : 123 ) bxi ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xj ( cid : 123 ) b ( cid : 123 ) bxi ( cid : 123 ) , we can precompute b ( cid : 123 ) bxi for all svs xi and thus obtain a machine ( with modied svs ) which is as fast as a standard sv machine ( cf
the nonlinear case .
to construct nonlinear sv machines , kernel functions k ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) are substituted for every occurence of a dot product ( in ( 123 ) and ( 123 ) ) , corresponding to a dot product in some feature space which is nonlinearly related to input space ( boser , guyon , & vapnik , 123 ) .
in that case , the above analysis leads to the regularizer
( cid : 123 ) iyi ( cid : 123 ) k ( cid : 123 ) bxj ( cid : 123 ) bxi ( cid : 123 ) ( cid : 123 ) b
the derivative of k must be evaluated for specic kernels , e . g .
for k ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) d , ( cid : 123 ) k ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) d ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) d ( cid : 123 ) ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) to obtain a kernelspecic constraint on the matrix b , one has to equate the result with the quadratic term in the nonlinear objective function ,
relationship to principal component analysis ( pca ) .
let us presently return to the linear case , and provide some interpretation of ( 123 ) and ( 123 ) .
if we sum over derivatives ( cid : 123 ) t jt ( cid : 123 ) ltxj have zero mean and c is a sample estimate in both directions , the vectors ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) t jt ( cid : 123 ) ltx .
being positive , c can be of the covariance matrix of the random vector ( cid : 123 ) ( cid : 123 ) diagonalized , c ( cid : 123 ) sds ( cid : 123 ) , with an orthogonal matrix s consisting of cs eigenvectors and a diagonal matrix d containing the eigenvalues .
then we can compute b ( cid : 123 ) c ( cid : 123 ) being diagonal and nonnegative .
since the dot product is invariant under orthogonal transformations , we may drop the leading s and ( 123 ) becomes
s ( cid : 123 ) , with d ( cid : 123 )
s ( cid : 123 ) xj ( cid : 123 ) d ( cid : 123 )
s ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) b ( cid : 123 )
a given pattern x is thus rst transformed by projecting it onto the eigenvectors of the tangent covariance matrix c , which are the rows of s ( cid : 123 ) .
the resulting feature vector is then rescaled by dividing by the square roots of cs eigenvalues . 123 in other words , the directions ( cid : 123 ) t jt ( cid : 123 ) ltx are scaled back , thus more emphasis is of main variance of the random vector ( cid : 123 ) put on features which are less variant under lt .
for example , in image analysis , if lt represent the group of translations , more emphasis is put on the relative proportions of ink in the image rather than the positions of lines .
the pca interpretation of our preprocessing matrix suggests the possibility to regularize and reduce dimensionality by discarding part of the features , as it is common usage when doing pca .
going nonlinear by using nonlinear pca .
we are now in a position to describe the way how to generalize to the nonlinear case .
to this end , we use kernel principal component analysis ( scholkopf , smola , & muller , 123 ) .
this technique allows us to compute prin - cipal components in a space f nonlinearly related to input space , just as nonlinear sv machines construct decision rules in f .
the kernel function k plays the role of the dot product in f , i . e .
k ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) .
kernel pca consists in diagonalizing the ma - trix ( cid : 123 ) k ( cid : 123 ) xi ( cid : 123 ) xj ( cid : 123 ) ( cid : 123 ) ij in order to obtain the eigenvalues of the covariance matrix of the images of the data in f and an implicit expression of its eigenvectors : since they live in f , the eigenvectors cannot in general be given explicitely ( f may be very high or even innite dimensional ) .
however , kernel pca nds coefcients ( cid : 123 ) k
i such that
i k ( cid : 123 ) xi ( cid : 123 ) x ( cid : 123 )
123as an aside , note that our goal to build invariant sv machines has thus serendipitously provided us with an approach for an open problem in sv learning , namely the one of scaling : in sv machines , there has so far been no way of automatically assigning different weight to different directions in input space : in a trained sv machine , the weights of the rst layer form a subset of the training set .
choosing these support vectors from the training set only gives rather limited possibilities for appropriately dealing with different scales in different directions of input space .
is the projection of x onto the kth eigenvector of the covariance matrix in f .
to generalize ( 123 ) to the nonlinear case , we thus need to compute the tangent covariance matrix c ( eq .
123 ) in f and its projection onto the subspace of f given by the linear span of the images of our data .
here the considerations of the linear case apply .
without going into further detail on this issue , we state that the whole procedure reduces to computing dot products in f , which can be done using k , without explicitly mapping into f .
123 kernels using local correlations
by using a kernel k ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) d , one implicitly constructs a decision boundary in the space of all possible products of d pixels .
this may not be desirable , since in natural images , correlations over short distances are much more reliable features than longrange correlations .
to take this into account , we dene a kernel k d ( cid : 123 ) d
compute a third image z , dened as the pixelwise product of x and y 123
sample z with a pyramidal receptive eld of diameter p , centered at all locations
( cid : 123 ) i ( cid : 123 ) j ( cid : 123 ) , to obtain the values zij
raise each zij to the power d , to take into account local correlations within the
range of the pyramid
sum zd
ij over the whole image , and raise the result to the power d to allow for some longerange correlations between the outputs of the pyramidal receptive
the resulting kernel will be of order dd , however , it will not contain all possible correla - tions of dd pixels .
123 experimental results
in the experiments , we used a subset of the mnist data base of handwritten characters ( bottou at al . , 123 ) , consisting of 123 training examples and 123 test examples at a resolution of 123x123 pixels , with entries in ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) .
using a linear sv machine ( i . e .
a sep - arating hyperplane ) , we obtain a test error rate of ( cid : 123 ) ( cid : 123 ) ( training 123 binary classiers , and using the maximum value of g ( eq .
123 ) for 123class classication ) ; by using a polynomial kernel of degree 123 , this drops to ( cid : 123 ) ( cid : 123 ) .
in all of the following experiments , we used degree 123 kernels of various types .
the number 123 was chosen as it can be written as a product of two integers , thus we could compare results to a kernel kp with d ( cid : 123 ) d ( cid : 123 ) .
for the considered classication task , results for higher polynomial degrees are very similar .
in a series of experiments with a homogeneous polynomial kernel k ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) , using preprocessing with gaussian smoothing kernels of standard deviation ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , we obtained error rates which gradually increased from ( cid : 123 ) ( cid : 123 ) to ( cid : 123 ) ( cid : 123 ) ; thus no improvement of this performance was possible by a simple smoothing operation .
applying the virtual sv method ( retraining the sv machine on translated svs; scholkopf , burges , & vapnik , 123 ) to this problem results in an improved error rate of ( cid : 123 ) ( cid : 123 ) .
invariant hyperplanes .
table 123 reports results obtained by preprocessing all patterns with b ( cf .
( 123 ) ) , choosing different values of ( cid : 123 ) ( cf .
in the experiments , the patterns were rst rescaled to have entries in ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , then b was computed , using horizontal and vertical translations , and preprocessing was carried out; nally , the resulting patterns were scaled back again .
this was done to ensure that patterns and derivatives lie in comparable regions of rn ( note that if the pattern background level is a constant ( cid : 123 ) , then its derivative is ) .
the results show that even though ( 123 ) was derived for the linear case , it can lead to improvements in the nonlinear case ( here , for a degree 123 polynomial ) , too .
table 123 : classication error rates for modifying the kernel k ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) y ( cid : 123 ) with the ( cid : 123 ) ; cf .
( 123 ) ( 123 ) .
enforcing invariant hyperplane preprocessing matrix b ( cid : 123 ) ( cid : 123 ) c invariance with ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) leads to improvements over the original performance ( ( cid : 123 ) ( cid : 123 ) ) .
error rate in % 123
dimensionality reduction .
the above ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) scaling operation is afne rather than linear , hence the argument leading to eq .
123 does not hold for this case .
we thus only report results on dimensionality reduction for the case where the data is kept in ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) scaling from the very beginning on .
dropping principal components which are less important leads to substantial improvements ( table 123 ) ; cf .
the explanation following eq .
( 123 ) ) .
table 123 : dropping directions corresponding to small eigenvalues of c ( cf .
( 123 ) ) leads to substantial improvements .
all results given are for the case ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cf .
table 123 ) ; degree 123 homogeneous polynomial kernel .
principal components discarded
error rate in %
kernels using local correlations .
to exploit locality in images , we used pyramidal re - ceptive eld kernel kd ( cid : 123 ) d with diameter p ( cid : 123 ) ( cf .
for d ( cid : 123 ) d ( cid : 123 ) , we ob - tained an improved error rate of ( cid : 123 ) ( cid : 123 ) , another degree 123 kernel with only local correlations ( d ( cid : 123 ) ( cid : 123 ) d ( cid : 123 ) ) led to ( cid : 123 ) ( cid : 123 ) .
albeit signicantly better than the ( cid : 123 ) ( cid : 123 ) for the degree 123 homogeneous polynomial ( the error rates on the 123 element test set have an accuracy of about ( cid : 123 ) ( cid : 123 ) , cf .
bottou at al . , 123 ) , this is still worse than the virtual sv result of ( cid : 123 ) ( cid : 123 ) .
as the two methods , however , exploit different types of prior knowledge , it could be ex - pected that combining them leads to still better performance; and indeed , this yielded the best performance of all ( ( cid : 123 ) ( cid : 123 ) ) .
for the purpose of benchmarking , we also ran our system on the us postal service database of 123 ( cid : 123 ) 123 handwritten digits at a resolution of ( cid : 123 ) .
in that case , we obtained the following test error rates : sv with degree 123 polynomial kernel ( cid : 123 ) ( cid : 123 ) , virtual sv ( same ker - nel ) ( cid : 123 ) ( cid : 123 ) , sv with k ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) .
the latter compares favourably to almost all known results on that data base , and is second only to a memorybased tangentdistance nearest neighbour classier at ( cid : 123 ) ( cid : 123 ) ( simard , lecun , & denker , 123 ) .
123% , virtual sv with k ( cid : 123 )
with its rather general class of admissible kernel functions , the sv algorithm provides am - ple possibilities for constructing taskspecic kernels .
we have considered an image classi - cation task and used two forms of domain knowledge : rst , pattern classes were required to be locally translationally invariant , and second , local correlations in the images were assumed to be more reliable than longrange correlations .
the second requirement can be seen as a more general form of prior knowledge it can be thought of as arising partially from the fact that patterns possess a whole variety of transformations; in object recognition , for instance , we have object rotations and deformations .
typically , these transformations are continuous , which implies that local relationships in an image are fairly stable , whereas global relationships are less reliable .
we have incorporated both types of domain knowledge into the sv algorithm by construct -
ing appropriate kernel functions , leading to substantial improvements on the considered pattern recognition tasks .
our method for constructing kernels for transformation invari - ant sv machines , put forward to deal with the rst type of domain knowledge , so far has only been applied in the linear case , which partially explains why it only led to moder - ate improvements ( also , we so far only used translational invariance ) .
it is applicable for differentiable transformations other types , e . g .
for mirror symmetry , have to be dealt with using other techniques ( e . g .
scholkopf , burges , & vapnik , 123 ) .
its main advantages compared to the latter technique is that it does not slow down testing speed , and that using more invariances leaves training time almost unchanged .
the proposed kernels respecting locality in images led to large improvements; they are applicable not only in image classi - cation but in all cases where the relative importance of subsets of products features can be specied appropriately .
they do , however , slow down both training and testing by a constant factor which depends on the specic kernel used .
both described techniques should be directly applicable to other kernelbased methods as sv regression ( vapnik , 123 ) and kernel pca ( scholkopf , smola , & muller , 123 ) .
future work will include the nonlinear case ( cf .
our remarks in sec .
123 ) , the incorporation of invariances other than translation , and the construction of kernels incorporating local feature extractors ( e . g .
edge detectors ) different from the pyramids described in sec
acknowledgements .
we thank chris burges and leon bottou for parts of the code and for
