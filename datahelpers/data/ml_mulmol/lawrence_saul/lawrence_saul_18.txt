an introduction to nonlinear dimensionality reduction
by maximum variance unfolding
kilian q .
weinberger and lawrence k
department of computer and information science , university of pennsylvania
levine hall , 123 walnut street , philadelphia , pa 123 - 123
many problems in ai are simplied by clever representations of sensory or symbolic input .
how to discover such rep - resentations automatically , from large amounts of unlabeled data , remains a fundamental challenge .
the goal of statis - tical methods for dimensionality reduction is to detect and discover low dimensional structure in high dimensional data .
in this paper , we review a recently proposed algorithm maximum variance unfoldingfor learning faithful low di - mensional representations of high dimensional data .
the algorithm relies on modern tools in convex optimization that are proving increasingly useful in many areas of ma -
a fundamental challenge of ai is to develop useful internal representations of the external world .
the human brain ex - cels at extracting small numbers of relevant features from large amounts of sensory data .
consider , for example , how we perceive a familiar face .
a friendly smile or a menac - ing glare can be discerned in an instant and described by a few well chosen words .
on the other hand , the digital representations of these images may consist of hundreds or thousands of pixels .
clearly , there are much more compact representations of images , sounds , and text than their native digital formats .
with such representations in mind , we have spent the last few years studying the problem of dimension - ality reductionhow to detect and discover low dimensional structure in high dimensional data .
for higher - level decision - making in ai , the right repre - sentation makes all the difference .
we mean this quite lit - erally , in the sense that proper judgments of similiarity and difference depend crucially on our internal representations of the external world .
consider , for example , the images of teapots in fig .
each image shows the same teapot from a different angle .
compared on a pixel - by - pixel basis , the query image and image a are the most similar pair of im - ages; that is , their pixel intensities have the smallest mean - squared - difference .
the viewing angle in image b , however , is much closer to the viewing angle in the query image evidence that distances in pixel space do not support crucial copyright c ( cid : 123 ) 123 , american association for articial intelli - gence ( www . aaai . org ) .
all rights reserved .
figure 123 : images of teapots : pixel distances versus percep - tual distances .
as measured by the mean - squared - difference of pixel intensities , image a is closer to the query image than image b , despite the fact that the view in image a involves a full 123 degrees of rotation .
judgments of similarity and difference .
( consider the em - barrassment when your robotic butler grabs the teapot by its spout rather than its handle , not to mention the liability when it subsequently attempts to rell your guests cup . ) a more useful representation of these images would index them by the teapots angle of rotation , thus locating image b closer to the query image than image a .
objects may be similar or different in many ways .
in the teapot example of fig .
123 , there is only one degree of free - dom : the angle of rotation .
more generally , there may be many criteria that are relevant to judgments of similarity and difference , each associated with its own degree of freedom .
these degrees of freedom are manifested over time by vari - abilities in appearance or presentation .
the most important modes of variability can often be dis - tilled by automatic procedures that have access to large num - bers of observations .
in essence , this is the goal of statis - tical methods for dimensionality reduction ( burges 123; saul et al .
the observations , initially represented as high dimensional vectors , are mapped into a lower dimen - sional space .
if this mapping is done faithfully , then the axes of the lower dimensional space relate to the datas intrinsic degrees of freedom .
the linear method of principal components analysis ( pca ) performs this mapping by projecting high dimen - sional data into low dimensional subspaces .
the principal subspaces of pca have the property that they maximize the variance of the projected data .
pca works well if the most important modes of variability are approximately linear .
in this case , the high dimensional observations can be very well
figure 123 : results of pca applied to a data set of face im - ages .
the gure shows a grayscale face image ( right ) and its linear reconstructions from different numbers of principal components .
the number of principal components required for accurate reconstruction greatly exceeds the small num - ber of characteristic poses and expressions in the data set .
reconstructed from their low dimensional linear projections .
pca works poorly if the most important modes of vari - ability are nonlinear .
to illustrate the effects of nonlinearity , we applied pca to a data set of 123 123 grayscale images .
each image in the data set depicted a different pose or ex - pression of the same persons face .
the variability of faces is not expressed linearly in the pixel space of grayscale images .
123 shows the linear reconstructions of a particular im - age from different numbers of principal components ( that is , from principal subspaces of different dimensionality ) .
the reconstructions are not accurate even when the number of principal components greatly exceeds the small number of characteristic poses and expressions in this data set .
in this paper , we review a recently proposed algorithm for nonlinear dimensionality reduction .
the algorithm , known as maximum variance unfolding ( sun et al .
123; saul et al .
123 ) , discovers faithful low dimensional repre - sentations of high dimensional data , such as images , sounds , and text .
it also illustrates many ideas in convex optimiza - tion that are proving increasingly useful in the broader eld of machine learning .
our work builds on earlier frameworks for analyzing high dimensional data that lies on or near a low dimen - sional manifold ( tenenbaum , de silva , & langford 123; roweis & saul 123 ) .
manifolds are spaces that are locally linear , but unlike euclidean subspaces , they can be globally nonlinear .
curves and surfaces are familiar examples of one and two dimensional manifolds .
compared to earlier frame - works for manifold learning , maximum variance unfolding has many interesting properties , which we describe in the
maximum variance unfolding
algorithms for nonlinear dimensionality reduction map i=123 to low dimensional out - high dimensional inputs ( ( cid : 123 ) xi ) n i=123 , where ( cid : 123 ) xi ( cid : 123 ) d , ( cid : 123 ) yi ( cid : 123 ) r , and r ( cid : 123 ) d .
the re - duced dimensionality r is chosen to be as small as possible , yet sufciently large to guarantee that the outputs ( cid : 123 ) yi ( cid : 123 ) r provide a faithful representation of the inputs ( cid : 123 ) xi ( cid : 123 ) d .
what constitutes a faithful representation ? suppose that the high dimensional inputs lie on a low dimensional manifold .
for a faithful representation , we ask that the dis - tances between nearby inputs match the distances between
nearby outputs .
such locally distance - preserving represen - tations are exactly the kind constructed by maximum vari -
the algorithm for maximum variance unfolding is based on a simple intuition .
imagine the inputs ( cid : 123 ) xi as connected to their k nearest neighbors by rigid rods .
( the value of k is the algorithms one free parameter . ) the algorithm at - tempts to pull the inputs apart , maximizing the sum total of their pairwise distances without breaking ( or stretching ) the rigid rods that connect nearest neighbors .
the outputs are obtained from the nal state of this transformation .
the effect of this transformation is easy to visualize for inputs that lie on low dimensional manifolds , such as curves or surfaces .
for example , imagine the inputs as beads on a necklace that is coiled up in three dimensions .
by pulling the necklace taut , the beads are arranged in a line , a nonlin - ear dimensionality reduction from ( cid : 123 ) 123 to ( cid : 123 ) 123
alternatively , imagine the inputs as the lattice of sites in a crumpled sh - ing net .
by pulling on the ends of the net , the inputs are arranged in a plane , a nonlinear dimensionality reduction from ( cid : 123 ) 123 to ( cid : 123 ) 123
as we shall see , this intuition for maximum variance unfolding also generalizes to higher dimensions .
the unfolding transformation described above can be formulated as a quadratic program .
let ij ( 123 , 123 ) de - note whether inputs ( cid : 123 ) xi and ( cid : 123 ) xj are k - nearest neighbors .
the outputs ( cid : 123 ) yi from maximum variance unfolding , as described above , are those that solve the following optimization :
maximize ( cid : 123 ) ij ( cid : 123 ) ( cid : 123 ) yi ( cid : 123 ) yj ( cid : 123 ) 123 subject to : ( 123 ) ( cid : 123 ) ( cid : 123 ) yi ( cid : 123 ) yj ( cid : 123 ) 123 = ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) xj ( cid : 123 ) 123 for all ( i , j ) with ij =123
( 123 ) ( cid : 123 ) i ( cid : 123 ) yi = 123 here , the rst constraint enforces that distances between nearby inputs match distances between nearby outputs , while the second constraint yields a unique solution ( up to rotation ) by centering the outputs on the origin .
the apparent intractability of this quadratic program can be nessed by a simple change of variables .
note that as written above , the optimization over the outputs ( cid : 123 ) yi is not convex , meaning that it potentially suffers from spu - rious local minima .
dening the inner product matrix kij = ( cid : 123 ) yi ( cid : 123 ) yj , we can reformulate the optimization as a semidenite program ( sdp ) ( vandenberghe & boyd 123 ) over the matrix k .
the resulting optimization is simply a linear program over the matrix elements kij , with the additional constraint that the matrix k has only nonnega - tive eigenvalues , a property that holds for all inner prod - in earlier work ( weinberger & saul 123; weinberger , sha , & saul 123 ) , we showed that the sdp over k can be written as : maximize trace ( k ) subject to : ( 123 ) kii123kij +kjj = ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) xj ( cid : 123 ) 123 for all ( i , j ) with ij =123
( 123 ) ijkij = 123
( 123 ) k ( cid : 123 ) 123
the last ( additional ) constraint k ( cid : 123 ) 123 requires the ma - trix k to be positive semidenite .
unlike the original quadratic program for maximum variance unfolding , this sdp is convex .
in particular , it can be solved efciently with
123 polynomial - time guarantees , and many off - the - shelf solvers are available in the public domain .
from the solution of the sdp in the matrix k , we can derive outputs ( cid : 123 ) yi ( cid : 123 ) n satisfying kij = ( cid : 123 ) yi ( cid : 123 ) yj by singu - lar value decomposition .
an r - dimensional representation that approximately satises kij ( cid : 123 ) yi ( cid : 123 ) yj can be obtained from the top r eigenvalues and eigenvectors of k .
roughly speaking , the number of dominant eigenvalues of k indi - cates the number of dimensions needed to preserve local distances while maximizing variance .
in particular , if the top r eigenvalues of k account for ( say ) 123% of its trace , this indicates that an r - dimensional representation can cap - ture 123% of the unfolded datas variance .
we have used maximum variance unfolding ( mvu ) to ana - lyze many high dimensional data sets of interest .
here we show some solutions ( weinberger & saul 123; blitzer et al .
123 ) that are particularly easy to visualize .
123 shows a two dimensional representation of teapot images discovered by mvu .
the data set consisted of n=123 high resolution color images showing a porcelain teapot viewed from different angles in the plane .
the teapot was viewed under a full 123 degrees of rotation .
each image contained 123 123 rgb pixels , so that the pixel space had dimensionality d = 123
the two dimensional represen - tation discovered by mvu is easily visualized by superim - posing represenative images on top of their corresponding outputs in the plane .
the outputs are arranged in a circle , reecting the cyclic degree of freedom in the data set .
note also how this representation supports judgments of similar - ity and difference that are not evident in the original pixel space , as discussed in fig
123 shows a three dimensional representation of face images discovered by mvu .
the data set consisted of
figure 123 : three dimensional representation from mvu of n = 123 grayscale images of faces .
the superimposed im - ages reveal a small number of characteristic actions as the underlying degrees of freedom .
n=123 grayscale images of the same persons face in dif - ferent poses and expressions .
each image had 123 pixels , so that the pixel space had dimensionality d = 123
in con - trast to the results from pca in fig .
123 , the solution obtained by mvu reveals a small number of characteristic actions ( e . g . , left and right tilt , smile , pucker ) as the underlying de - grees of freedom in the data set .
123 shows a two dimensional representation of words discovered by mvu .
the inputs to mvu were derived from the co - occurrence statistics of the n = 123 most frequently occuring words in a large corpus of text .
each word was rep - resented by a sparse d = 123 dimensional vector of nor - malized counts , as typically collected for bigram language modeling .
the gure shows that many semantic relation - ships between words are preserved despite the drastic reduc - tion in dimensionality from d = 123 to two dimensions ( for visualization in the plane ) .
table 123 compares the estimated dimensionalities of the data sets in figs .
123 from the results of linear versus non - linear dimensionality reduction .
the estimates from pca were computed from the minimum dimensionality subspace required to contain 123% of the original datas variance .
the estimates from mvu were computed from the minimum di - mensionality subspace required to contain 123% of the un - folded datas variance .
for all these data sets , mvu dis - covers much more compact representations than pca .
figure 123 : two dimensional representation from mvu of n = 123 images of a teapot , viewed from different angles in the plane .
the circular arrangement reects the under - lying rotational degree of freedom .
in this representation , image b is closer to the query image than image a , unlike in fig
in this paper we have described the use of maximum variance unfolding for nonlinear dimensionality reduction .
large - scale applications of maximum variance unfolding re - quire one additional insight .
as originally formulated , the size of the sdp scales linearly with the number of exam - ples , n .
in previous work ( weinberger , packer , & saul 123; sha & saul 123 ) , we showed that the sdp can be tremen -
123 may , would , could , should ,
might , must , can , cannot ,
couldn ' t , won ' t , will
one , two , three ,
four , five , six ,
seven , eight , nine ,
figure 123 : two dimensional representation from mvu of the 123 most frequently occuring words in the nab corpus .
the representation preserves clusters of words with similar meanings .
table 123 : dimensionalities of different data sets , as estimated from the results of linear versus nonlinear dimensionality re - duction .
the top row shows the dimensionality of the datas
dously simplied by factoring the n n target matrix as k qlq ( cid : 123 ) , where l rmm and q rnm with m ( cid : 123 ) n .
the matrix q in this factorization can be pre - computed from the results of faster but less robust methods for nonlinear dimensionality reduction .
the factorization transforms the original sdp over the matrix k into a much smaller sdp over the matrix l .
this approach works well in practice , enabling maximum variance unfolding to analyze much larger data sets than we originally imagined .
one advantage of maximum variance unfolding is its ex - ibility to be adapted to particular applications .
for exam - ple , the distance - preserving constraints in the sdp can be relaxed to handle noisy data or to yield more aggressive re - sults in dimensionality reduction ( sha & saul 123 ) .
alter - natively , additional constraints can be enforced to incorpo - rate prior knowledge .
along these lines , a rather novel ex - tension of maximum variance unfolding has been developed for visual robot navigation and mapping ( bowling , ghodsi , & wilkinson 123 ) .
the authors use a semidenite program to construct a map of a simulated robots virtual environ - ment .
they adapt our framework to learn from the actions of the robot as well as the images of its environment .
the algorithm has also been applied to statistical language mod - eling ( blitzer et al .
123 ) , where low dimensional represen - tations of words were derived from bigram counts and used to improve on traditional models .
we are hopeful that appli - cations will continue to emerge in many areas of ai .
this work was supported by nsf award 123
