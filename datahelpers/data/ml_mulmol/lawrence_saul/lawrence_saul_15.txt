we show how to learn a mahanalobis distance metric for k - nearest neigh - bor ( knn ) classication by semidenite programming .
the metric is trained with the goal that the k - nearest neighbors always belong to the same class while examples from different classes are separated by a large margin .
on seven data sets of varying size and difculty , we nd that metrics trained in this way lead to signicant improvements in knn classicationfor example , achieving a test error rate of 123% on the mnist handwritten digits .
as in support vector machines ( svms ) , the learning problem reduces to a convex optimization based on the hinge loss .
unlike learning in svms , however , our framework requires no modication or extension for problems in multiway ( as opposed to bi -
the k - nearest neighbors ( knn ) rule ( 123 ) is one of the oldest and simplest methods for pattern classication .
nevertheless , it often yields competitive results , and in certain domains , when cleverly combined with prior knowledge , it has signicantly advanced the state - of - the - art ( 123 , 123 ) .
the knn rule classies each unlabeled example by the majority label among its k - nearest neighbors in the training set .
its performance thus depends crucially on the distance metric used to identify nearest neighbors .
in the absence of prior knowledge , most knn classiers use simple euclidean distances to measure the dissimilarities between examples represented as vector inputs .
euclidean distance metrics , however , do not capitalize on any statistical regularities in the data that might be estimated from a large training set of labeled examples .
ideally , the distance metric for knn classication should be adapted to the particular problem being solved .
it can hardly be optimal , for example , to use the same dis - tance metric for face recognition as for gender identication , even if in both tasks , dis - tances are computed between the same xed - size images .
in fact , as shown by many re - searchers ( 123 , 123 , 123 , 123 , 123 , 123 ) , knn classication can be signicantly improved by learning a distance metric from labeled examples .
even a simple ( global ) linear transformation of input features has been shown to yield much better knn classiers ( 123 , 123 ) .
our work builds in a novel direction on the success of these previous approaches .
in this paper , we show how to learn a mahanalobis distance metric for knn classication .
the metric is optimized with the goal that k - nearest neighbors always belong to the same class while examples from different classes are separated by a large margin .
our goal for metric learning differs in a crucial way from those of previous approaches that minimize the pairwise distances between all similarly labeled examples ( 123 , 123 , 123 ) .
this latter objective is far more difcult to achieve and does not leverage the full power of knn classication , whose accuracy does not require that all similarly labeled inputs be tightly clustered .
our approach is largely inspired by recent work on neighborhood component analysis ( 123 ) and metric learning by energy - based models ( 123 ) .
though based on the same goals , however , our methods are quite different .
in particular , we are able to cast our optimization as an instance of semidenite programming .
thus the optimization we propose is convex , and its global minimum can be efciently computed .
our approach has several parallels to learning in support vector machines ( svms ) most notably , the goal of margin maximization and a convex objective function based on the hinge loss .
in light of these parallels , we describe our approach as large margin nearest neighbor ( lmnn ) classication .
our framework can be viewed as the logical counterpart to svms in which knn classication replaces linear classication .
our framework contrasts with classication by svms , however , in one intriguing respect : it requires no modication for problems in multiway ( as opposed to binary ) classica - tion .
extensions of svms to multiclass problems typically involve combining the results of many binary classiers , or they require additional machinery that is elegant but non - trivial ( 123 ) .
in both cases the training time scales at least linearly in the number of classes .
by contrast , our learning problem has no explicit dependence on the number of classes .
let ( ( ~ xi , yi ) ) n i=123 denote a training set of n labeled examples with inputs ~ xi rd and dis - crete ( but not necessarily binary ) class labels yi .
we use the binary matrix yij ( 123 , 123 ) to indicate whether or not the labels yi and yj match .
our goal is to learn a linear transforma - tion l : rdrd , which we will use to compute squared distances as :
d ( ~ xi , ~ xj ) = kl ( ~ xi ~ xj ) k123
specically , we want to learn the linear transformation that optimizes knn classication when distances are measured in this way .
we begin by developing some useful terminology .
in addition to the class label yi , for each input ~ xi we also specify k target neighbors that is , k other inputs with the same label yi that we wish to have minimal distance to ~ xi , as computed by eq .
in the absence of prior knowledge , the target neighbors can simply be identied as the k nearest neighbors , determined by euclidean distance , that share the same label yi .
( this was done for all the experiments in this paper . ) we use ij ( 123 , 123 ) to indicate whether input ~ xj is a target neighbor of input ~ xi .
like the binary matrix yij , the matrix ij is xed and does not change during learning .
our cost function over the distance metrics parameterized by eq .
( 123 ) has two competing terms .
the rst term penalizes large distances between each input and its target neighbors , while the second term penalizes small distances between each input and all other inputs that do not share the same label .
specically , the cost function is given by :
ij ( 123yil ) ( cid : 123 ) 123 + kl ( ~ xi ~ xj ) k123kl ( ~ xi ~ xl ) k123 ( cid : 123 )
ijkl ( ~ xi ~ xj ) k123 + c
where in the second term ( z ) + = max ( z , 123 ) denotes the standard hinge loss and c > 123 is some positive constant ( typically set by cross validation ) .
note that the rst term only penalizes large distances between inputs and target neighbors , not between all similarly the second term in the cost function in - corporates the idea of a margin .
in par - ticular , for each input ~ xi , the hinge loss is incurred by differently labeled inputs whose distances do not exceed , by one absolute unit of distance , the distance from input ~ xi to any of its target neigh - bors .
the cost function thereby favors distance metrics in which differently la - beled inputs maintain a large margin of distance and do not threaten to invade each others neighborhoods .
the learn - ing dynamics induced by this cost func - tion are illustrated in fig .
123 for an input with k =123 target neighbors .
parallels with svms the competing terms in eq .
( 123 ) are anal - ogous to those in the cost function for svms ( 123 ) .
in both cost functions , one term penalizes the norm of the parame - ter vector ( i . e . , the weight vector of the maximum margin hyperplane , or the linear trans - formation in the distance metric ) , while the other incurs the hinge loss for examples that violate the condition of unit margin .
finally , just as the hinge loss in svms is only trig - gered by examples near the decision boundary , the hinge loss in eq .
( 123 ) is only triggered by differently labeled examples that invade each others neighborhoods .
we can reformulate the optimization of eq .
( 123 ) as an instance of semidenite program - ming ( 123 ) .
a semidenite program ( sdp ) is a linear program with the additional constraint that a matrix whose elements are linear in the unknown variables is required to be posi - tive semidenite .
sdps are convex; thus , with this reformulation , the global minimum of eq .
( 123 ) can be efciently computed .
to obtain the equivalent sdp , we rewrite eq .
( 123 ) as :
figure 123 : schematic illustration of one inputs neighborhood ~ xi before training ( left ) versus after training ( right ) .
the distance metric is op - timized so that : ( i ) its k =123 target neighbors lie within a smaller radius after training; ( ii ) differ - ently labeled inputs lie outside this smaller ra - dius , with a margin of at least one unit distance .
arrows indicate the gradients on distances aris - ing from the optimization of the cost function .
d ( ~ xi , ~ xj ) = ( ~ xi ~ xj ) >m ( ~ xi ~ xj ) ,
where the matrix m = l>l , parameterizes the mahalanobis distance metric induced by the linear transformation l .
rewriting eq .
( 123 ) as an sdp in terms of m is straightforward , since the rst term is already linear in m = l>l and the hinge loss can be mimicked by introducing slack variables ij for all pairs of differently labeled inputs ( i . e . , for all hi , ji such that yij = 123 ) .
the resulting sdp is given by :
ij ij ( 123 yil ) ijl subject to :
ij ij ( ~ xi ~ xj ) >m ( ~ xi ~ xj ) + cp
( 123 ) ( ~ xi ~ xl ) >m ( ~ xi ~ xl ) ( ~ xi ~ xj ) >m ( ~ xi ~ xj ) 123 ijl ( 123 ) ijl 123 ( 123 ) m ( cid : 123 ) 123
the last constraint m ( cid : 123 ) 123 indicates that the matrix m is required to be positive semidef - inite .
while this sdp can be solved by standard online packages , general - purpose solvers
! xi ! ximarginlocal neighborhood ! xi ! ximarginbeforeaftersimilarly labeleddifferently labeleddifferently labeledtarget neighbor tend to scale poorly in the number of constraints .
thus , for our work , we implemented our own special - purpose solver , exploiting the fact that most of the slack variables ( ij ) never attain positive values123
the slack variables ( ij ) are sparse because most labeled inputs are well separated; thus , their resulting pairwise distances do not incur the hinge loss , and we obtain very few active constraints .
our solver was based on a combination of sub - gradient descent in both the matrices l and m , the latter used mainly to verify that we had reached the global minimum .
we projected updates in m back onto the positive semidenite cone after each step .
alternating projection algorithms provably converge ( 123 ) , and in this case our implementation worked much faster than generic solvers123
we evaluated the algorithm in the previous section on seven data sets of varying size and difculty .
table 123 compares the different data sets .
principal components analysis ( pca ) was used to reduce the dimensionality of image , speech , and text data , both to speed up training and avoid overtting .
except for isolet and mnist , all of the experimental results are averaged over several runs of randomly generated 123 / 123 splits of the data .
isolet and mnist have pre - dened training / test splits .
for the other data sets , we randomly gener - ated 123 / 123 splits for each run .
both the number of target neighbors ( k ) and the weighting parameter ( c ) in eq .
( 123 ) were set by cross validation .
( for the purpose of cross - validation , the training sets were further partitioned into training and validation sets . ) we begin by reporting overall trends , then discussing the individual data sets in more detail .
we rst compare knn classication error rates using mahalanobis versus euclidean dis - tances .
to break ties among different classes , we repeatedly reduced the neighborhood size , ultimately classifying ( if necessary ) by just the k = 123 nearest neighbor .
123 sum - marizes the main results .
except on the smallest data set ( where over - training appears to be an issue ) , the mahalanobis distance metrics learned by semidenite programming led to signicant improvements in knn classication , both in training and testing .
the training error rates reported in fig .
123 are leave - one - out estimates .
we also computed test error rates using a variant of knn classication , inspired by previous work on energy - based models ( 123 ) .
energy - based classication of a test example ~ xt was done by nding the label that minimizes the cost function in eq .
in particular , for a hypothetical label yt , we accumulated the squared distances to the k nearest neighbors of ~ xt that share the same label in the training set ( corresponding to the rst term in the cost function ) ; we also accumulated the hinge loss over all pairs of differently labeled examples that result from labeling ~ xt by yt ( corresponding to the second term in the cost function ) .
finally , the test example was classied by the hypothetical label that minimized the combination of these two terms :
ij ( 123yil ) ( cid : 123 ) 123 + kl ( ~ xi ~ xj ) k123kl ( ~ xi ~ xl ) k123 ( cid : 123 )
as shown in fig .
123 , energy - based classication with this assignment rule generally led to even further reductions in test error rates .
finally , we compared our results to those of multiclass svms ( 123 ) .
on each data set ( except mnist ) , we trained multiclass svms using linear and rbf kernels; fig .
123 reports the results of the better classier .
on mnist , we used a non - homogeneous polynomial kernel of degree four , which gave us our best results .
( see also ( 123 ) . )
conditions , then using the resulting solution as a starting point for the actual sdp of interest .
123a great speedup can be achieved by solving an sdp that only monitors a fraction of the margin 123a matlab implementation is currently available at http : / / www . seas . upenn . edu / kilianw / lmnn .
features after pca cpu time ( per run )
table 123 : properties of data sets and experimental parameters for lmnn classication .
figure 123 : training and test error rates for knn classication using euclidean versus ma - halanobis distances .
the latter yields lower test error rates on all but the smallest data set ( presumably due to over - training ) .
energy - based classication ( see text ) generally leads to further improvement .
the results approach those of state - of - the - art multiclass svms .
small data sets with few classes the wine , iris , and balance data sets are small data sets , with less than 123 training exam - ples and just three classes , taken from the uci machine learning repository123
on data sets of this size , a distance metric can be learned in a matter of seconds .
the results in fig .
123 were averaged over 123 experiments with different random 123 / 123 splits of each data set .
our results on these data sets are roughly comparable ( i . e . , better in some cases , worse in others ) to those of neighborhood component analysis ( nca ) and relevant component analysis ( rca ) , as reported in previous work ( 123 ) .
the at&t face recognition data set123 contains 123 grayscale images of 123 individuals in 123 different poses .
we downsampled the images from to 123 123 pixels and used pca to obtain 123 - dimensional eigenfaces ( 123 ) .
training and test sets were created by randomly sampling 123 images of each person for training and 123 images for testing .
the task involved 123 - way classicationessentially , recognizing a face from an unseen pose .
123 shows the improvements due to lmnn classication .
123 illustrates the improvements more graphically by showing how the k = 123 nearest neighbors change as a result of learning a mahalanobis metric .
( though the algorithm operated on low dimensional eigenfaces , for clarity the gure shows the rescaled images . )
123available at http : / / www . ics . uci . edu / mlearn / mlrepository . html .
123available at http : / / www . uk . research . att . com / facedatabase . html
mnistnewsisoletbalfacesiriswine123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123energy basedclassificationknn mahalanobisdistanceknn euclideandistancemulticlasssvmtesting error rate ( % ) training error rate ( % ) figure 123 : images from the at&t face recognition data base .
top row : an image correctly recognized by knn classication ( k = 123 ) with mahalanobis distances , but not with eu - clidean distances .
middle row : correct match among the k =123 nearest neighbors according to mahalanobis distance , but not euclidean distance .
bottom row : incorrect match among the k =123 nearest neighbors according to euclidean distance , but not mahalanobis distance .
spoken letter recognition the isolet data set from uci machine learning repository has 123 examples and 123 classes corresponding to letters of the alphabet .
we reduced the input dimensionality ( orig - inally at 123 ) by projecting the data onto its leading 123 principal componentsenough to account for 123% of its total variance .
on this data set , dietterich and bakiri report test error rates of 123% using nonlinear backpropagation networks with 123 output units ( one per class ) and 123% using nonlinear backpropagation networks with a 123 - bit error correcting code ( 123 ) .
lmnn with energy - based classication obtains a test error rate of 123% .
the 123 - newsgroups data set consists of posted articles from 123 newsgroups , with roughly 123 articles per newsgroup .
we used the 123 - version of the data set123 which has cross - postings removed and some headers stripped out .
we tokenized the newsgroups using the rainbow package ( 123 ) .
each article was initially represented by the weighted word - counts of the 123 , 123 most common words .
we then reduced the dimensionality by projecting the data onto its leading 123 principal components .
the results in fig .
123 were obtained by av - eraging over 123 runs with 123 / 123 splits for training and test data .
our best result for lmmn on this data set at 123% test error rate improved signicantly on knn classication using euclidean distances .
lmnn also performed comparably to our best multiclass svm ( 123 ) , which obtained a 123% test error rate using a linear kernel and 123 dimensional inputs .
handwritten digit recognition the mnist data set of handwritten digits123 has been extensively benchmarked ( 123 ) .
we deskewed the original 123 grayscale images , then reduced their dimensionality by re - taining only the rst 123 principal components ( enough to capture 123% of the datas overall variance ) .
energy - based lmnn classication yielded a test error rate at 123% , cutting the baseline knn error rate by over one - third .
other comparable benchmarks ( 123 ) ( not exploit - ing additional prior knowledge ) include multilayer neural nets at 123% and svms at 123% .
123 shows some digits whose nearest neighbor changed as a result of learning , from a mismatch using euclidean distance to a match using mahanalobis distance .
123 related work
many researchers have attempted to learn distance metrics from labeled examples .
we briey review some recent methods , pointing out similarities and differences with our work .
123available at http : / / people . csail . mit . edu / jrennie / 123newsgroups / 123available at http : / / yann . lecun . com / exdb / mnist /
among 123 nearest neighbors before but not after training : test image : among 123 nearest neighbors after but not before training : figure 123 : top row : examples of mnist images whose nearest neighbor changes dur - ing training .
middle row : nearest neighbor after training , using the mahalanobis distance metric .
bottom row : nearest neighbor before training , using the euclidean distance metric .
xing et al ( 123 ) used semidenite programming to learn a mahalanobis distance metric for clustering .
their algorithm aims to minimize the sum of squared distances between similarly labeled inputs , while maintaining a lower bound on the sum of distances between differently labeled inputs .
our work has a similar basis in semidenite programming , but differs in its focus on local neighborhoods for knn classication .
shalev - shwartz et al ( 123 ) proposed an online learning algorithm for learning a mahalanobis distance metric .
the metric is trained with the goal that all similarly labeled inputs have small pairwise distances ( bounded from above ) , while all differently labeled inputs have large pairwise distances ( bounded from below ) .
a margin is dened by the difference of these thresholds and induced by a hinge loss function .
our work has a similar basis in its appeal to margins and hinge loss functions , but again differs in its focus on local neigh - borhoods for knn classication .
in particular , we do not seek to minimize the distance between all similarly labeled inputs , only those that are specied as neighbors .
goldberger et al ( 123 ) proposed neighborhood component analysis ( nca ) , a distance metric learning algorithm especially designed to improve knn classication .
the algorithm min - imizes the probability of error under stochastic neighborhood assignments using gradient descent .
our work shares essentially the same goals as nca , but differs in its construction of a convex objective function .
chopra et al ( 123 ) recently proposed a framework for similarity metric learning in which the metrics are parameterized by pairs of identical convolutional neural nets .
their cost function penalizes large distances between similarly labeled inputs and small distances between differently labeled inputs , with penalties that incorporate the idea of a margin .
our work is based on a similar cost function , but our metric is parameterized by a linear transformation instead of a convolutional neural net .
in this way , we obtain an instance of relevant component analysis ( rca ) constructs a mahalanobis distance metric from a weighted sum of in - class covariance matrices ( 123 ) .
it is similar to pca and linear discrim - inant analysis ( but different from our approach ) in its reliance on second - order statistics .
hastie and tibshirani ( ? ) and domeniconi et al ( 123 ) consider schemes for locally adaptive distance metrics that vary throughout the input space .
the latter work appeals to the goal of margin maximization but otherwise differs substantially from our approach .
in partic - ular , domeniconi et al ( 123 ) suggest to use the decision boundaries of svms to induce a locally adaptive distance metric for knn classication .
by contrast , our approach ( though similarly named ) does not involve the training of svms .
in this paper , we have shown how to learn mahalanobis distance metrics for knn clas - sication by semidenite programming .
our framework makes no assumptions about the structure or distribution of the data and scales naturally to large number of classes .
ongoing
test image : nearest neighbor before training : nearest neighbor after training : work is focused in three directions .
first , we are working to apply lmnn classication to problems with hundreds or thousands of classes , where its advantages are most apparent .
second , we are investigating the kernel trick to perform lmnn classication in nonlin - ear feature spaces .
as lmmn already yields highly nonlinear decision boundaries in the original input space , however , it is not obvious that kernelizing the algorithm will lead to signicant further improvement .
finally , we are extending our framework to learn locally adaptive distance metrics ( 123 , 123 ) that vary across the input space .
such metrics should lead to even more exible and powerful large margin classiers .
