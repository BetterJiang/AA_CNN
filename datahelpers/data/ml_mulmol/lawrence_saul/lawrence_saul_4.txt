we investigate how to learn a kernel matrix for high dimensional data that lies on or near a low dimensional manifold .
noting that the kernel matrix implicitly maps the data into a nonlinear feature space , we show how to discover a mapping that unfolds the underlying manifold from which the data was sampled .
the kernel matrix is constructed by maximizing the variance in feature space subject to local constraints that preserve the angles and distances between nearest neighbors .
the main optimization involves an instance of semidefinite programming - - - a fundamentally different computation than previous algorithms for manifold learning , such as isomap and locally linear embedding .
the optimized kernels perform better than polynomial and gaussian kernels for problems in manifold learning , but worse for problems in large margin classification .
we explain these results in terms of the geometric properties of different kernels and comment on various interpretations of other manifold learning algorithms as kernel methods .
kernels , machine learning
proceedings of the twenty first international conference on machine learning ( icml 123 ) , held 123 - 123 july 123 , banff , alberta , canada .
this conference paper is available at scholarlycommons : http : / / repository . upenn . edu / cis_papers / 123
learning a kernel matrix for nonlinear dimensionality reduction
kilian q .
weinberger lawrence k .
saul department of computer and information science , university of pennsylvania , philadelphia , pa 123 , usa
we investigate how to learn a kernel matrix for high dimensional data that lies on or near a low dimensional manifold .
noting that the kernel matrix implicitly maps the data into a nonlinear feature space , we show how to discover a mapping that unfolds the un - derlying manifold from which the data was sampled .
the kernel matrix is constructed by maximizing the variance in feature space subject to local constraints that preserve the angles and distances between nearest neigh - bors .
the main optimization involves an in - stance of semidenite programminga fun - damentally dierent computation than pre - vious algorithms for manifold learning , such as isomap and locally linear embedding .
the optimized kernels perform better than poly - nomial and gaussian kernels for problems in manifold learning , but worse for problems in large margin classication .
we explain these results in terms of the geometric properties of dierent kernels and comment on various interpretations of other manifold learning al - gorithms as kernel methods .
kernel methods ( scholkopf & smola , 123 ) have proven to be extremely powerful in many areas of ma - chine learning .
the so - called kernel trick is by now widely appreciated : a canonical algorithm ( e . g . , the linear perceptron , principal component analysis ) is re - formulated in terms of gram matrices , then general - ized to nonlinear problems by substituting a kernel function for the inner product .
well beyond this fa - miliar recipe , however , the eld continues to develop
appearing in proceedings of the 123 st international confer - ence on machine learning , ban , canada , 123
copyright 123 by the authors .
as researchers devise novel types of kernels , exploit - ing prior knowledge in particular domains and insights from computational learning theory and convex op - indeed , much work revolves around the simple question : how to choose the kernel ? the an - swers are diverse , reecting the tremendous variety of problems to which kernel methods have been applied .
kernels based on string matching ( lodhi et al . , 123 ) and weighted transducers ( cortes et al . , 123 ) have been proposed for problems in bioinformatics , text , and speech processing .
other specialized kernels have been constructed for problems in pattern recognition involving symmetries and invariances ( burges , 123 ) .
most recently , kernel matrices have been learned by semidenite programming for large margin classica - tion ( graepel , 123; lanckriet et al . , 123 ) .
in this paper , we revisit the problem of nonlinear dimensionality reduction and its solution by kernel principal component analysis ( pca ) ( scholkopf et al . , 123 ) .
our specic interest lies in the application of kernel pca to high dimensional data whose basic modes of variability are described by a low dimensional manifold .
the goal of nonlinear dimensionality reduc - tion in these applications is to discover the underly - ing manifold ( tenenbaum et al . , 123; roweis & saul , 123 ) .
for problems of this nature , we show how to learn a kernel matrix whose implicit mapping into fea - ture space unfolds the manifold from which the data was sampled .
the main optimization of our algorithm involves an instance of semidenite programming , but unlike earlier work in learning kernel matrices ( grae - pel , 123; lanckriet et al . , 123 ) , the setting here is the problem of manifold learning has recently at - tracted a great deal of attention ( tenenbaum et al . , 123; roweis & saul , 123; belkin & niyogi , 123; saul & roweis , 123 ) , and a number of authors ( ben - gio et al . , 123; ham et al . , 123 ) have developed connections between manifold learning algorithms and kernel pca .
in contrast to previous work , however ,
our paper does not serve to reinterpret pre - existing algorithms such as isomap and locally linear embed - ding as instances of kernel pca .
instead , we propose a novel optimization ( based on semidenite program - ming ) that bridges the literature on kernel methods and manifold learning in a rather dierent way .
the algorithm we describe can be viewed from several com - plementary perspectives .
this paper focuses mainly on its interpretation as a kernel method , while a com - panion paper ( weinberger & saul , 123 ) focuses on its application to the unsupervised learning of image
kernel pca
scholkopf , smola , and muller ( 123 ) introduced ker - nel pca as a nonlinear generalization of pca ( jol - lie , 123 ) .
the generalization is obtained by mapping the original inputs into a higher ( and possibly in - nite ) dimensional feature space f before extracting the principal components .
in particular , consider inputs x123 , .
, xn rd and features ( x123 ) , .
, ( xn ) f computed by some mapping : rd f .
kernel pca is based on the insight that the principal com - ponents in f can be computed for mappings ( x ) that are only implicitly dened by specifying the inner product in feature spacethat is , the kernel function k ( x , y ) = ( x ) ( y ) .
kernel pca can be used to obtain low dimensional representations of high dimensional inputs .
for this , it suces to compute the dominant eigenvectors of the kernel matrix kij = ( xi ) ( xj ) .
the kernel matrix can be expressed in terms of its eigenvalues and .
assuming the eigenvalues are sorted from largest to smallest , the d - dimensional embedding that best preserves inner prod -
ucts in feature space is obtained by mapping the input xird to the vector yi = ( the main freedom in kernel pca lies in choosing the kernel function k ( x , y ) or otherwise specifying the kernel matrix kij .
some widely used kernels are the linear , polynomial and gaussian kernels , given by :
eigenvectors v as k =p
123v123i ,
k ( x , y ) = x y , k ( x , y ) = ( 123 + x y ) p , k ( x , y ) = e
the linear kernel simply identies the feature space with the input space .
implicitly , the polynomial kernel maps the inputs into a feature space of dimensional - ity o ( dp ) , while the gaussian kernel maps the inputs onto the surface of an innite - dimensional sphere .
the dominant eigenvalues of the kernel matrix kij
measure the variance along the principal components in feature space , provided that the features are cen - tered on the origin .
the features can always be cen - tered by subtracting out their meannamely , by the transformation ( xi ) ( xi ) 123 j ( xj ) .
when the mapping ( x ) is only implicitly specied by the kernel function , the centering transformation can be applied directly to the kernel matrix .
in particular , re - computing the inner products kij = ( xi ) ( xj ) from the centered features gives :
kij kij 123
for a centered kernel matrix , the relative weight of the leading d eigenvalues , obtained by dividing their sum by the trace , measures the relative variance cap - tured by the leading d eigenvectors .
when this ratio is nearly unity , the data can be viewed as inhabiting a d - dimensional subspace of the feature space , or equiv - alently , a d - dimensional manifold of the input space .
learning the kernel matrix
the choice of the kernel plays an important role in ker - nel pca , in that dierent kernels are bound to reveal ( or conceal ) dierent types of low dimensional struc - ture .
in this section , we show how to learn a kernel matrix that reveals when high dimensional inputs lie on or near a low dimensional manifold .
as in earlier work on support vector machines ( svms ) ( graepel , 123; lanckriet et al . , 123 ) , we will cast the prob - lem of learning the kernel matrix as an instance of semidenite programming .
the similarity ends there , however , as the optimization criteria for nonlinear di - mensionality reduction dier substantially from the criteria for large margin classication .
we describe the constraints on the optimization in section 123 , the objective function in section 123 , and the optimization itself in section 123 .
the kernel matrix k is constrained by three criteria .
the rst is semipositive deniteness , a condition re - quired to interpret the kernel matrix as storing the inner products of vectors in a hilbert space .
we thus constrain the optimization over k to the cone of sym - metric matrices with nonnegative eigenvalues .
though not a linear constraint , the cone of semipositive de - nite matrices denes a convex domain for the overall the second constraint is that the kernel matrix stores
the inner products of features that are centered on the
( xi ) = 123
as described in section 123 , this condition enables us to interpret the eigenvalues of the kernel matrix as measures of variance along principal components in feature space .
( 123 ) can be expressed in terms of the kernel matrix as :
( xi ) ( xj ) = x
note that this is a linear constraint on the elements of the kernel matrix , thus preserving the convexity of the domain of optimization .
the nal constraints on the kernel matrix reect our goals for nonlinear dimensionality reduction .
in partic - ular , we are interested in the setting where the inputs lie on or near a low dimensional manifold , and the goals of kernel pca are to detect the dimensionality of this underlying manifold and discover its modes of variabil - ity .
we imagine that this manifold is isometric to an open connected subset of euclidean space ( tenenbaum et al . , 123; donoho & grimes , 123 ) , and the prob - lem of learning the kernel matrix is to discover how the inner products between inputs transform under this mapping .
an isometry123 is a smooth invertible map - ping that looks locally like a rotation plus translation , thus preserving local ( and hence geodesic ) distances .
thus , in our application of kernel pca to manifold learning , the nal constraints we impose on the kernel matrix are to restrict the ( implicitly dened ) mappings between inputs and features from fully general nonlin - ear transformations to the special class of isometries .
how is this done ? we begin by extending the no - tion of isometry to discretely sampled manifolds , in inputs ( x123 , .
, xn ) and fea - particular to sets of tures ( ( x123 ) , .
, ( xn ) ) in one - to - one correspon - dence .
let the nn binary matrix indicate a neigh - borhood relation on both sets , such that we regard xj as a neighbor of xi if and only if ij =123 ( and similarly , for ( xj ) and ( xi ) ) .
we will say that the inputs xi and features ( xi ) are locally isometric under the neighborhood relation if for every point xi , there ex - ists a rotation and translation that maps xi and its neighbors precisely onto ( xi ) and its neighbors .
the above denition translates naturally into various
123formally , two riemannian manifolds are said to be iso - metric if there is a dieomorphism such that the metric on one pulls back to the metric on the other .
sets of linear constraints on the elements of the ker - nel matrix kij .
note that the local isometry between neighborhoods will exist if and only if the distances and angles between points and their neighbors are pre - served .
thus , whenever both xj and xk are neighbors of xi ( that is , ijik = 123 ) , for local isometry we must ( ( xi ) ( xj ) ) ( ( xi ) ( xk ) ) = ( xixj ) ( xixk ) .
( 123 ) is sucient for local isometry because the tri - angle formed by any point and its neighbors is deter - mined up to rotation and translation by specifying the lengths of two sides and the angle between them .
in fact , such a triangle is similarly determined by speci - fying the lengths of all its sides .
thus , we can also say that the inputs and features are locally isometric un - der if whenever xi and xj are themselves neighbors ( that is , ij = 123 ) or are common neighbors of another input ( that is , ( t ) ij >123 ) , we have :
| ( xi ) ( xj ) |123 = |xixj|123 .
this is an equivalent characterization of local isome - try as eq .
( 123 ) , but expressed only in terms of pairwise distances .
finally , we can express these constraints purely in terms of dot products .
let gij = xixj de - note the gram matrix of the inputs , and recall that the kernel matrix kij = ( xi ) ( xj ) represents the gram matrix of the features .
then eq .
( 123 ) can be written as :
kii+kjjkijkji = gii+gjjgijgji .
( 123 ) constrains how the inner products between nearby inputs are allowed to transform under a lo - cally isometric mapping .
we impose these constraints to ensure that the mapping dened ( implicitly ) by the kernel matrix is an isometry .
note that these con - straints are also linear in the elements of the kernel matrix , thus preserving the convexity of the domain the simplest choice for neighborhoods is to let ij in - dicate whether input xj is one of the k nearest neigh - bors of input xi computed using euclidean distance .
in this case , eq .
( 123 ) species o ( n k123 ) constraints that x the distances between each point and its nearest neighbors , as well as the pairwise distances between nearest neighbors .
provided that k ( cid : 123 ) n , however , the kernel matrix is unlikely to be fully specied by these constraints since it contains o ( n 123 ) elements .
objective function
in the previous section , we showed how to restrict the kernel matrices so that the features ( xi ) could be re - garded as images of a locally isometric mapping
goal of nonlinear dimensionality reduction is to dis - cover the particular isometric mapping that unfolds the underlying manifold of inputs into a euclidean space of its intrinsic dimensionality .
this intrinsic dimensionality may be much lower than the extrin - sic dimensionality of the input space .
to unfold the manifold , we need to construct an objective function over locally isometric kernel matrices that favors this type of dimensionality reduction .
to this end , imagine each input xi as a steel ball con - nected to its k nearest neighbors by rigid rods .
( for simplicity , we assume that the graph formed this way is fully connected; if not , then each connected com - ponent of the graph should be analyzed separately . ) the eect of the rigid rods is to lock the neighbor - hoods in place , xing the distances and angles between nearest neighbors .
the lattice of steel balls formed in this way can be viewed as a discretized version of the underlying manifold .
now imagine that we pull the steel balls as far apart as possible , recording their - nal positions by ( xi ) .
the discretized manifold will remain connecteddue to the constraints imposed by the rigid rodsbut it will also atten , increasing the variance captured by its leading principal components .
( for a continuous analogy , imagine pulling on the ends of a string; a string with any slack in it occupies at least two dimensions , whereas a taut , fully extended string occupies just one . ) we can formalize this intuition as an optimization over semipositive denite matrices .
the constraints im - posed by the rigid rods are , in fact , precisely the con - straints imposed by eq .
an objective function that measures the pairwise distances between steel balls is
it is easy to see that this function is bounded above due to the constraints on distances between neighbors imposed by the rigid rods .
suppose the distance be - tween any two neighbors is bounded by some maximal distance .
providing the graph is connected , then for any two points , there exists a path along the graph of distance at most n , which ( by the triangle in - equality ) provides an upper bound on the euclidean distance between the points that appears in eq .
this results in an upper bound on the objective func - tion of order o ( n 123 123 ) .
( 123 ) can be expressed in terms of the elements of the kernel matrix by expanding the right hand side :
( kii + kjj 123kij ) = tr ( k ) .
the last step in eq .
( 123 ) follows from the centering con - straint in eq .
thus the objective function for the optimization is simply the trace of the kernel matrix .
maximizing the trace also corresponds to maximizing the variance in feature space .
semidenite embedding ( sde )
the constraints and objective function from the pre - vious sections dene an instance of semidenite pro - gramming ( sdp ) ( vandenberghe & boyd , 123 ) .
specically , the goal is to optimize a linear function of the elements in a semipositive denite matrix sub - ject to linear equality constraints .
collecting the con - straints and objective function , we have the following
maximize : tr ( k ) subject to : 123
k ( cid : 123 ) 123
for all i , j such that ij =123 or ( cid : 123 ) t ( cid : 123 )
kii+kjjkijkji = gii+gjjgijgji
ij kij = 123
the optimization is convex and does not suer from local optima .
there are several general - purpose tool - boxes and polynomial - time solvers available for prob - lems in semidenite programming .
the results in this paper were obtained using the sedumi tool - box ( sturm , 123 ) in matlab .
once the kernel ma - trix is computed , a nonlinear embedding can be ob - tained from its leading eigenvectors , as described in section 123
because the kernel matrices in this approach are optimized by semidenite programming , we will re - fer to this particular form of kernel pca as semide - nite embedding ( sde ) .
experimental results
experiments were performed on several data sets to evaluate the learning algorithm described in section 123
though the sde kernels were expressly optimized for problems in manifold learning , we also evaluated their performance for large margin classication .
nonlinear dimensionality reduction
we performed kernel pca with linear , polynomial , gaussian , and sde kernels on data sets where we knew or suspected that the high dimensional inputs were sampled from a low dimensional manifold .
where nec - essary , kernel matrices were centered before computing principal components , as in eq
figure 123
top : results of sde applied to n = 123 in - puts sampled from a swiss roll ( top left ) .
the inputs had d = 123 dimensions , of which 123 were lled with small amounts of noise ( not shown ) .
the two dimensional plot shows the embedding from kernel pca with the sde ker - nel .
bottom : eigenvalues of dierent kernel matrices , nor - malized by their trace .
only the eigenvalues from sde indicate the correct intrinsic dimensionality ( d = 123 ) of the
in the rst experiment , the inputs were sampled from a three dimensional swiss roll , a data set com - monly used to evaluate algorithms in manifold learn - ing ( tenenbaum et al . , 123 ) .
123 shows the orig - inal inputs ( top left ) , the embedding discovered by sde with k =123 nearest neighbors ( top right ) , and the eigenvalue spectra from several dierent kernel matri - ces ( bottom ) .
the color coding of the embedding re - veals that the swiss roll has been successfully unrav - eled .
note that the kernel matrix learned by sde has two dominant eigenvalues , indicating the correct un - derlying dimensionality of the swiss roll , whereas the eigenspectra of other kernel matrices fail to reveal this structure .
in particular , the linear kernel matrix has three dominant eigenvalues , reecting the extrinsic di - mensionality of the swiss roll , while the eigenspectra of the polynomial ( p = 123 ) and gaussian ( = 123 ) kernel matrices123 indicate that the variances of their features ( xi ) are spread across a far greater number of dimensions than the original inputs xi .
the second experiment was performed on a data set consisting of n = 123 color images of a teapot viewed from dierent angles in the plane .
with a resolution of
123for all the data sets in this section , we set the width parameter of the gaussian kernel to the estimated stan - dard deviation within neighborhoods of size k = 123 , thus reecting the same length scale used in sde .
figure 123
results of sde applied to n = 123 images of a teapot viewed from dierent angles in the plane , under a full 123 degrees of rotation .
the images were represented by inputs in a d = 123 dimensional vector space .
sde faithfully represents the 123 degrees of rotation by a circle .
the eigenvalues of dierent kernel matrices are also shown , normalized by their trace .
123 and three bytes of color information per pixel , the images were represented as points in a d = 123 dimensional vector space .
though very high dimen - sional , the images in this data set are eectively pa - rameterized by a single degree of freedomnamely , the angle of rotation .
the low dimensional embedding of these images by sde and the eigenvalue spectra of dif - ferent kernel matrices are shown in fig .
the kernel matrix learned by sde ( with k =123 nearest neighbors ) concentrates the variance of the feature space in two dimensions and maps the images to a circle , a highly intuitive representation of the full 123 degrees of rota - tion .
by contrast , the linear , polynomial ( p = 123 ) , and gaussian ( = 123 ) kernel matrices have eigenvalue spectra that do not reect the low intrinsic dimen - sionality of the data set .
why does kernel pca with the gaussian kernel per - form so dierently on these data sets when its width parameter reects the same length scale as neighbor - hoods in sde ? note that the gaussian kernel com - putes a nearly zero inner product ( kij 123 ) in fea - ture space for inputs xi and xj that do not belong to
inputseigenvalues ( normalized by trace ) sde kernel ! ! " " ! " # " ! $ " " $ " 123 . 123 . 123 . 123gaussianpolynomiallinearsde123 . 123 . 123 . 123gaussianpolynomiallinearsdeeigenvalues ( normalized by trace ) sde kernel figure 123
embeddings from kernel pca with the gaussian kernel on the swiss roll and teapot data sets in figs .
123 and 123
the rst three principal components are shown .
in both cases , dierent patches of the manifolds are mapped to orthogonal parts of feature space .
the same or closely overlapping neighborhoods .
it fol - lows from these inner products that the feature vectors ( xi ) and ( xj ) must be nearly orthogonal .
as a re - sult , the dierent patches of the manifold are mapped into orthogonal regions of the feature space : see fig .
thus , rather than unfolding the manifold , the gaus - sian kernel leads to an embedding whose dimensional - ity is equal to the number of non - overlapping patches of length scale .
this explains the generally poor per - formance of the gaussian kernel for manifold learning ( as well as its generally good performance for large margin classication , discussed in section 123 ) .
another experiment on the data set of teapot images was performed by restricting the angle of rotation to 123 degrees .
the results are shown in fig .
interest - ingly , in this case , the eigenspectra of the linear , poly - nomial , and gaussian kernel matrices are not qualita - tively dierent .
by contrast , the sde kernel matrix now has only one dominant eigenvalue , indicating that the variability for this subset of images is controlled by a single ( non - cyclic ) degree of freedom .
as a nal experiment , we compared the performance of the dierent kernels on a real - world data set described by an underlying manifold .
the data set ( hull , 123 ) consisted of n =123 grayscale images at 123 resolu - tion of handwritten twos and threes ( in roughly equal proportion ) .
in this data set , it is possible to nd a rel - atively smooth morph between any pair of images , and a relatively small number of degrees of freedom describe the possible modes of variability ( e . g .
writ - ing styles for handwritten digits ) .
fig 123 shows the results .
note that the kernel matrix learned by sde concentrates the variance in a signicantly fewer num - ber of dimensions , suggesting it has constructed a more
figure 123
results of kernel pca applied to n = 123 images of a teapot viewed from dierent angles in the plane , under 123 degrees of rotation .
the eigenvalues of dierent kernel matrices are shown , normalized by their trace .
the one dimensional embedding from sde is also shown .
appropriate feature map for nonlinear dimensionality
large margin classication
we also evaluated the use of sde kernel matrices for large margin classication by svms .
several training and test sets for problems in binary classication were created from the usps data set of handwritten dig - its ( hull , 123 ) .
each training and test set had 123 and 123 examples , respectively .
for each experiment , the sde kernel matrices were learned ( using k = 123 nearest neighbors ) on the combined training and test data sets , ignoring the target labels .
the results were compared to those obtained from linear , polynomial ( p = 123 ) , and gaussian ( = 123 ) kernels .
table 123 shows that the sde kernels performed quite poorly in this capacity , even worse than the linear kernels .
123 oers an explanation of this poor performance .
the sde kernel can only be expected to perform well for large margin classication if the decision bound - ary on the unfolded manifold is approximately linear .
there is no a priori reason , however , to expect this type of linear separability .
the example in fig .
123 shows a particular binary labeling of inputs on the swiss roll for which the decision boundary is much simpler in the input space than on the unfolded mani - fold .
a similar eect seems to be occurring in the large margin classication of handwritten digits .
thus , the strength of sde for nonlinear dimensionality reduction is generally a weakness for large margin classication .
by contrast , the polynomial and gaussian kernels lead to more powerful classiers precisely because they map inputs to higher dimensional regions of feature space .
related and ongoing work
sde can be viewed as an unsupervised counterpart to the work of graepel ( 123 ) and lanckriet et al ( 123 ) , who proposed learning kernel matrices by semidenite
bdacbcadbcdabcda figure 123
results of kernel pca applied to n = 123 images of handwritten digits .
the eigenvalues of dierent kernel matrices are shown , normalized by their trace .
digits linear polynomial gaussian 123 vs 123 123 vs 123 123 vs 123 123 vs 123
table 123
percent error rates for svm classication using dierent kernels on test sets of handwritten digits .
each line represents the average of 123 experiments with dierent 123 / 123 splits of training and testing data .
here , the sde kernel performs much worse than the other kernels .
figure 123
the twenty eigenvalues of from isomap and sde on the data sets of teapot images and handwritten digits .
note that the similarity matrices constructed by isomap have negative eigenvalues .
programming for large margin classication .
the ker - nel matrices learned by sde dier from those usually employed in svms , in that they aim to map inputs into an ( eectively ) lower dimensional feature space .
this explains both our positive results in nonlinear dimen - sionality reduction ( section 123 ) , as well as our negative results in large margin classication ( section 123 ) .
sde can also be viewed as an alternative to mani - fold learning algorithms such as isomap ( tenenbaum et al . , 123 ) , locally linear embedding ( lle ) ( roweis & saul , 123; saul & roweis , 123 ) , hessian lle ( hlle ) ( donoho & grimes , 123 ) , and laplacian eigenmaps ( belkin & niyogi , 123 ) .
all these al - gorithms share a similar structure , creating a graph based on nearest neighbors , computing an n n ma - trix from geometric properties of the inputs , and con - structing an embedding from the eigenvectors with the largest or smallest nonnegative eigenvalues .
a more detailed discussion of dierences between these algo -
linearly separable inputs ( in black versus figure 123
left : white ) sampled from a a swiss roll .
right : unfolding the manifold leads to a more complicated decision boundary .
rithms is given in a companion paper ( weinberger & saul , 123 ) .
here , we comment mainly on their various interpretations as kernel methods ( ham et al . , 123 ) .
in general , these other methods give rise to matrices whose geometric properties as kernels are less robust or not as well understood .
for example , unlike sde , the similarity matrix constructed by isomap from nite data sets can have negative eigenvalues .
in some cases , moreover , these negative eigenvalues can be apprecia - ble in magnitude to the dominant positive ones : see fig .
unlike both sde and isomap , lle and hlle construct matrices whose bottom eigenvectors yield low dimensional embeddings; to interpret these matrices as kernels , their eigenvalues must be ipped , either by inverting the matrix itself or by subtracting it from a large multiple of the identity matrix .
moreover , it does not appear that these eigenvalues can be used to estimate the intrinsic dimensionality of underlying manifolds ( saul & roweis , 123 ) .
a kernel can be de - rived from the discrete laplacian by noting its role in the heat diusion equation , but the intuition gained from this analogy , in terms of diusion times through a network , does not relate directly to the geometric properties of the kernel matrix .
sde stands apart from these methods in its explicit construction of a semipositive denite kernel matrix that preserves the geometric properties of the inputs up to local isometry and whose eigenvalues indicate the dimensionality of the underlying manifold .
we are pursuing many directions in ongoing work .
the rst is to develop faster and potentially dis - tributed ( biswas & ye , 123 ) methods for solving the
123 ! 123 . 123 . 123 ! 123 . 123 . 123isomap123 . 123 . 123sde instance of semidenite programming in sde and for out - of - sample extensions ( bengio et al . , 123 ) .
thus far we have been using generic solvers with a relatively high time complexity , relying on toolboxes that do not exploit any special structure in our problem .
we are also investigating many variations on the objective function and constraints in sdefor example , to al - low some slack in the preservation of local distances and angles , or to learn embeddings onto spheres .
fi - nally , we are performing more extensive comparisons with other methods in nonlinear dimensionality reduc - tion .
not surprisingly , perhaps , all these directions re - ect a more general trend toward the convergence of research in kernel methods and manifold learning .
the authors are grateful to s .
boyd and y .
ye ( stan - ford ) for useful discussions of semidenite program - ming and to the anonymous reviewers for many helpful
