abstract .
we study markov models whose state spaces arise from the cartesian product of two or more discrete random variables .
we show how to parameterize the transition matrices of these models as a convex combinationor mixtureof simpler dynamical models .
the parameters in these models admit a simple probabilistic interpretation and can be tted iteratively by an expectation - maximization ( em ) procedure .
we derive a set of generalized baum - welch updates for factorial hidden markov models that make use of this parameterization .
we also describe a simple iterative procedure for approximately computing the statistics of the hidden states .
throughout , we give examples where mixed memory models provide a useful representation of complex stochastic processes .
keywords : markov models , mixture models , discrete time series
the modeling of discrete time series is a fundamental problem in machine learning , with widespread applications .
these include speech recognition ( rabiner , 123 ) , natural lan - guage processing ( nadas , 123 ) , protein modeling ( haussler , krogh , mian , & sjolander , 123 ) , musical analysis / synthesis ( dirst & weigend , 123 ) , and numerous others .
probabilistic models of discrete time series typically start from some form of markov assumptionnamely , that the future is independent of the past given the present .
for the purpose of statistical estimation , problems arise if either : ( i ) the system possesses a large number of degrees of freedom , or ( ii ) the window of present knowledge required to predict the future extends over several time steps .
in these cases , the number of parameters to specify the markov model can overwhelm the amount of available data .
in particular , for a system with n possible states and memory length k , the number of free parameters scales exponentially as nk+123
the difculties are compounded for latent variable models in which the markov assump - tion applies to the hidden state space .
in this case , it may be computationally intractable to infer values for the hidden states .
for example , in rst - order hidden markov models ( hmms ) , computing the likelihood of a sequence of observations scales as n123 , where n is the number of hidden states ( rabiner , 123 ) .
in practice , exact probabilistic inference is therefore limited to hmms with relatively small ( or tightly constrained ) state spaces .
saul and jordan
in this technical note , we propose a principled way to investigate markov models with large state spaces .
this is done by representing the transition matrix as a convex combinationormixtureofsimpler dynamical models .
we refer to the resulting models as mixed memory markov models .
while the use of mixture distributions to parameterize higher - order markov models is well known ( raftery , 123; ney , essen , & kneser , 123; macdonald & zucchini , 123 ) , here we apply this methodology more broadly to factorial modelsmodelsin which large state spaces are represented via the cartesian product of
our note builds on earlier work describing factorial hmms ( ghahramani & jordan , 123 ) and dynamic probabilistic networks ( binder , koller , russell , & kanazawa , 123 ) .
these papers show that complex stochastic processes can be graphically represented by sets of markov chains connected ( via directed links ) to a common set of observable nodes .
such models arise naturally in the study of coupled time series , where the observations have an a priori decomposition as the cartesian product of two or more random variables .
factorial hmms aim to combine the power of latent , distributed representations with the richness of probabilistic semantics ( williams & hinton , 123 ) .
capturing this type of probabilistic reasoning is a fundamental problem in articial intelligence ( dean & kanazawa , 123 ) .
we believe that mixed memory markov models have several advantages for representing complex stochastic processes and learning from examples .
the parameters in these models admit a simple probabilistic interpretation and can be tted iteratively by an expectation - maximization ( em ) procedure ( dempster , laird , & rubin , 123 ) .
the em algorithm has several desirable properties , including monotone convergence in log - likelihood , lack of step size parameters , and naturalness at handling probabilistic constraints .
in many situations , it provides a compelling alternative to gradient - based learning methods ( baldi & chauvin , 123; binder et al . , 123 ) .
mixed memory models can also express a rich set of probabilistic dependencies , making them appropriate for modeling complex stochastic processes .
applied to factorial hmms , they generalize the work by ghahramani & jordan ( 123 ) in two important directions : by introducing coupled dynamics , and by considering non - gaussian observations .
they also give rise to a simple iterative procedure for making inferences about the hidden states .
we describe this procedure not only for its practical value , but also because it very cleanly illustrates the idea of exploiting tractable substructures in intractable probabilistic networks ( saul & jordan , 123 ) .
the main signicance of this work lies in its application to factorial hmms and the modeling of coupled time series .
in principle , though , mixed memory markov models can be applied wherever large state spaces arise as the cartesian product of two or more random variables .
we will take advantage of this generality to present mixed memory models in a number of different settings .
in doing this , our goal is to build upina gradual waythe somewhat involved notation needed to describe factorial hmms .
the organization of this note is therefore as follows .
in order of increasing complexity , we consider : ( 123 ) higher - order markov models , where large state spaces arise as the cartesian product of several time slices; ( 123 ) factorial markov models , where the dynamics are rst order but the observations have a componential structure; and ( 123 ) factorial hmms , where the markov dynamics apply to hidden states , as opposed to the observations themselves .
mixed memory markov models
we conclude that mixed memory models provide a valuable tool for understanding complex
higher order markov models
let it 123 f123; 123; : : : ; ng denote a discrete random variable that can take on n possible values .
a kth order markov model is specied by the transition matrix p ( itjit ( cid : 123 ) 123; it ( cid : 123 ) 123; : : : ; it ( cid : 123 ) k ) .
to avoid having to specify the o ( nk+123 ) elements of this matrix , we consider parameterizing the model by the convex combination ( raftery , 123; ney et al . , 123 ) :
p ( itjit ( cid : 123 ) 123; it ( cid : 123 ) 123; : : : ; it ( cid : 123 ) k ) =
where ( ( cid : 123 ) ) ( cid : 123 ) 123 , p ( cid : 123 ) ( ( cid : 123 ) ) = 123 , and a ( cid : 123 ) ( i123ji ) are k elementary n ( cid : 123 ) n transition matrices .
the model in eq .
( 123 ) is specied by o ( kn123 ) parameters , as opposed to o ( nk+123 ) for the full memory model .
note how ( ( cid : 123 ) ) is used to weight the inuence of past observations on the distribution over it .
this type of weighted sum is the dening characteristic of mixed
the mixture model in eq .
( 123 ) is to be distinguished from models that approximate higher - order markov models by n - gram smoothing; that is , by employing a linear combination of nth order transition matrices ( chen & goodman , 123 ) .
our model is not an n - gram smoother; rather it approximates a higher - order markov model by taking a linear combination of non - adjacent bigram models .
the model in eq .
( 123 ) also differs from mixture - of - experts models as applied to continuous time series ( zeevi , meir , & adler , 123 ) , in which the predictions of different nth order regressors are combined by the weights of a softmax gating function .
for the purpose of parameter estimation , it is convenient to interpret the index ( cid : 123 ) in eq .
( 123 ) as the value of a latent variable .
we denote this latent variable ( at each time step ) by xt and consider the joint probability distribution :
p ( it; xt = ( cid : 123 ) jit ( cid : 123 ) 123; : : : ; it ( cid : 123 ) k ) = ( ( cid : 123 ) ) a ( cid : 123 ) ( itjit ( cid : 123 ) ( cid : 123 ) ) :
note that marginalizing out xt ( i . e . , summing over ( cid : 123 ) ) recovers the previous model for the transition matrix , eq .
thus we have expressed the dynamics as a mixture model , in which the parameters ( ( cid : 123 ) ) are the prior probabilities , p ( xt = ( cid : 123 ) ) .
likewise , we can view the parameters a ( cid : 123 ) ( i123ji ) as the conditional probabilities , p ( it = i123jit ( cid : 123 ) 123; : : : ; it ( cid : 123 ) k; xt = ( cid : 123 ) ) .
let i = fi123; i123; : : : ; ilg denote an observed time series of length l .
the sufcient statistics for a full memory markov model are the transition frequencies .
to t the mixed memory markov model we avail ourselves of the em procedure ( dempster et al . , 123 ) .
in general terms the em algorithm calculates expected sufcient statistics and sets them equal to the observed sufcient statistics .
the procedure iterates and is guaranteed to increase the likelihood at each step .
for the model in eq .
( 123 ) , the em updates are ( ney et al , 123 ) :
( ( cid : 123 ) ) pt p ( xt = ( cid : 123 ) ji ) pt; ( cid : 123 ) p ( xt = ( cid : 123 ) ji )
a ( cid : 123 ) ( i123ji ) pt p ( xt = ( cid : 123 ) ; it ( cid : 123 ) ( cid : 123 ) = i; it = i123ji )
pt p ( xt = ( cid : 123 ) ; it ( cid : 123 ) ( cid : 123 ) = iji )
saul and jordan
table 123
entropy per character , computed from various markov models .
in the case where multiple time series are available as training data , the sums over t should be interpreted as sums over series as well .
the em updates for this model are easy to understand; at each iteration , the model parameters are adjusted so that the statistics of the joint distribution match the statistics of the posterior distribution .
the expectations in eqs .
( 123 ) and ( 123 ) may be straightforwardly computed from bayes rule :
p ( xt = ( cid : 123 ) ji ) =
p ( cid : 123 ) ( ( cid : 123 ) ) a ( cid : 123 ) ( itjit ( cid : 123 ) ( cid : 123 ) )
note that this algorithm requires no ne - tuning of step sizes , as does gradient descent .
in terms of representational power , the model of eq .
( 123 ) lies somewhere in between a rst order markov model and a kth order markov model .
to demonstrate this point , we tted various markov models to word spellings in english , italian , and finnish .
the state space for these models was the alphabet ( e . g . , a to z for english ) , and the training data came from very long lists of words with four or more letters .
the matrices a ( cid : 123 ) ( i123ji ) were initialized by count - based bigram models predicting each letter by the ( cid : 123 ) th preceding one .
( this type of initialization , in which the component sub - models are rst trained independently of one another , is useful to avoid poor local maxima in the learning procedure . ) in table 123 , we give the results measured in entropy per character .
the results show that the mixed memory model does noticeably better than the rst - order model .
of course , it cannot capture all the structure of the full second - order model , which has over ten times as many parameters .
the mixture model should accordingly be viewed as an intermediate step between rst and
we envision two situations in which the model of eq .
( 123 ) may be gainfully applied .
the rst is when the dynamics of the process generating the data are faithfully described by a mixture model .
in this case , one would expect the mixture model to perform as well as the ( full ) higher - order model while requiring substantially less data for its parameter estimation .
a real - world example might be the modeling of web sites visited during a session on the world wide web .
the modeling of these sequences has applications to web page prefetching and resource management on the internet ( bestavros & cunha , 123; cunha , bestavros , & crovella , 123 ) .
typically , the choice of the next web page is conditioned on a previous site , but not necessarily the last one that has been visited .
( recall how often it is necessary to retrace ones steps , using the back option . ) the model in eq .
( 123 ) captures this type of conditioning explicitly .
here , the states of the markov model would correspond to web pages; the matrices a ( cid : 123 ) ( i123ji ) , to links from web page i to web page j; and the index ( cid : 123 ) , to the number of backward or retraced steps taken before activating a new link .
the second situation in which this model may be appropriate is when the amount of in this case ,
training data is extremely sparse relative to the size of the state space .
mixed memory markov models
the parameterization in eq .
( 123 ) , though a poor approximation to the true model , may be desirable to avoid overtting .
ney et al ( 123 ) have investigated models of this form for large vocabulary language modeling .
the ability to discern likely sequences of words from unlikely sequences is an important component of automated speech recognition .
for large vocabulariesinthe tens of thousands of wordsthereis never sufcient data to estimate ( robustly ) the statistics of second or higher order markov models .
in practice , therefore , these models are smoothed or interpolated ( chen & goodman , 123 ) with lower order models .
the interpolation with lower order models is forced on practitioners by the enormous size of the state space ( e . g . , 123 words ) and the small ( in relative terms ) amount of training data ( e . g . , 123 words ) .
recently , one of us applied a more sophisticated version of eq .
( 123 ) to large vocabulary language modeling ( saul & pereira , 123 ) .
in only a few cpu hours , it was possible to t over ten million parameters to the statistics of an eighty million word corpus .
moreover , the smoothed combination of mixed and full memory markov models led to signicantly lower entropies on out - of - sample predictions .
factorial markov models
in the last section , we saw how large state spaces arose as the result of higher order in this section , we consider another source of large state spacesnamely , factorial representations .
many time series have a natural componential structure .
consider for example the four voicessoprano ( s ) , alto ( a ) , tenor ( t ) , and bass ( b ) ofa bach fugue ( dirst & weigend , 123 ) .
we can model each voice by a separate markov model , but this will not capture the correlations due to harmony .
the most straightforward way to model the coupling between voices is to write down a markov model whose dynamical state is the cartesian product of the four voices .
but the combinatorial structure of this state space leads to an explosion in the number of free parameters; thus it is imperative to provide a compact representation of the transition matrix .
mixed memory models are especially geared to these sorts of situations .
let it denote the tth element of a vector time series , and i ( cid : 123 ) t the ( cid : 123 ) th component of it .
if each vector has k components , and each component can take on n values , then the overall state space has size nk .
to model the coupling between these components in a compact way , we make two simplifying assumptions : ( i ) that the components i ( cid : 123 ) t at time t are conditionally independent given the vector it ( cid : 123 ) 123 , or
p ( itjit ( cid : 123 ) 123 ) =
and ( ii ) that the conditional probabilities p ( i ( cid : 123 ) of cross - transition matrices :
t jit ( cid : 123 ) 123 ) can be expressed as a weighted sum
t jit ( cid : 123 ) 123 ) =
( cid : 123 ) ( ( cid : 123 ) ) a ( cid : 123 ) ( cid : 123 ) ( i ( cid : 123 )
here again , the parameters a ( cid : 123 ) ( cid : 123 ) ( i123ji ) are k123 elementary n ( cid : 123 ) n transition matrices , while the parameters ( cid : 123 ) ( ( cid : 123 ) ) are positive numbers that satisfy p ( cid : 123 ) ( cid : 123 ) ( ( cid : 123 ) ) = 123
the number of
saul and jordan
table 123
portion of the four - component time series generated by bachs last fugue .
free parameters in eq .
( 123 ) is therefore o ( k 123n123 ) , as opposed to o ( n123k ) for the full memory model .
( by allowing non - square transition matrices , this model can also be generalized to the case where the different components take on different numbers of values . )
the parameters ( cid : 123 ) ( ( cid : 123 ) ) measure the amount of correlation between the different compo - nents of the time series .
in particular , if there is no correlation , then ( cid : 123 ) ( ( cid : 123 ) ) is the identity matrix , and the ( cid : 123 ) th component is independent of all the rest .
on the other hand , for non - zero ( cid : 123 ) ( ( cid : 123 ) ) , all the components at one time step inuence the ( cid : 123 ) th component at the next .
the matrices a ( cid : 123 ) ( cid : 123 ) ( i123ji ) provide a compact way to parameterize these inuences .
as in the previous section , it is convenient to introduce latent variables x ( cid : 123 )
t and view
t is to select which component of it ( cid : 123 ) 123 determines the transition matrix t .
as before , we can derive an em algorithm to t the parameters of this model
t = ( cid : 123 ) ; i ( cid : 123 )
t ( cid : 123 ) 123 = i; i ( cid : 123 )
t = i123ji )
pt p ( x ( cid : 123 )
t = ( cid : 123 ) ; i ( cid : 123 )
t ( cid : 123 ) 123 = iji )
where i stands for the observed time series .
naturally , the structure of these updates is quite similar to the model of the previous section .
to test this algorithm , we learned a model of the four - component time series generated by bachs last fugue .
this fugue has a rich history ( dirst & weigend , 123 ) .
the time series ( 123 beats long ) was made public following the santa fe competition on time series prediction .
table 123 shows a portion of this time series : here , each element represents a sixteenth note , while the numerical value codes the pitch .
to help avoid poor local maxima in the learning procedure , the transition matrices a ( cid : 123 ) ( cid : 123 ) ( i123ji ) were initialized by count - based bigram models predicting the ( cid : 123 ) th voice at time t from the ( cid : 123 ) th voice at the previous time
by examining the parameters of the tted model , we can see to what extent each voice enables one to make predictions about the others .
in general , we observed that the mixture coefcients ( cid : 123 ) ( ( cid : 123 ) ) were very close to zero or one .
the reason for this is that the voices do not typically change pitch with every sixteenth note .
hence , for each voice the note at the previous beat is a very good predictor of the note at the current one .
( 123 ) as a mixture model .
thus we may write :
t ; x ( cid : 123 ) p ( it; xtjit ( cid : 123 ) 123 ) = y
t = ( cid : 123 ) jit ( cid : 123 ) 123 ) = ( cid : 123 ) ( ( cid : 123 ) ) a ( cid : 123 ) ( cid : 123 ) ( i ( cid : 123 ) t ; x ( cid : 123 )
here , the role of x ( cid : 123 ) this case , the em updates are : ( cid : 123 ) ( ( cid : 123 ) ) pt p ( x ( cid : 123 ) pt; ( cid : 123 ) 123 p ( x ( cid : 123 )
a ( cid : 123 ) ( cid : 123 ) ( i123ji ) pt p ( x ( cid : 123 )
t = ( cid : 123 ) ji ) t = ( cid : 123 ) 123ji )
mixed memory markov models
figure 123
plot of soprano - tenor correlations versus time , as measured by the posterior probabilities of a mixed memory markov model .
when the voices do make a transition ( i . e . , move up or down in pitch ) , however , the coupling between voices becomes evident .
to see this , we can look at the posterior probabilities of the latent variables , x ( cid : 123 ) t , which reveal the extent to which the voices interact at t = t ji ) , specic moments in time .
figure 123 shows a plot of the posterior probabilities , p ( xs versus time calculated from the tted model .
within the framework of the mixture model , these probabilities measure the relative degree to which the sopranos note at time t can be predicted from the tenors note at the previous time step .
the moments at which this probability acquires a non - zero value indicate times when the tenor and soprano are tightly coupled .
not surprisingly , these pulses of coupling ( when viewed as a time series ) have a discernible local rhythm and regularity of their own .
factorial hmms
building on the results of the last section , we now consider the generalization to factorial hidden markov models ( hmms ) .
these are hmms whose states and observations have an internal , combinatorial structure ( ghahramani & jordan , 123; binder et al . , 123 ) .
how might such structure arise ? suppose we are trying to model the processes that give rise to a speech signal .
a number of unobserved variables interact to generate the signal that we ultimately observe .
in an articulatory model of speech production , these variables might encode the positions of various organs , such as the lip , tongue , and jaw .
in a recognizer , these variables might encode the current phonemic context , the speaker accent and gender , and the presence of background noise .
in either case , the hidden state for these models is naturally decomposed as the cartesian product of several random variables .
another motivation for factorial representations is that in many applications , the ob - servations have an a priori componential structure .
this is the case , for example , in audiovisual speech recognition ( bregler & omohundro , 123 ) , where information from different modalities is being combined and presented to the recognizer .
it is also the case in
saul and jordan
frequency subband - based speech recognition ( bourlard & dupont , 123 ) , where different recognizers are trained on sub - bands of the speech signal and then combined to make a global decision .
simple ways to integrate these different components are : ( a ) collapsing the data into a single time series or ( b ) reweighting and combining the likelihood scores of independent hmms .
one might hope for a more sophisticated integration , however , by building a joint model that looks for correlations on the actual time scale of the observations .
whatever the manner in which they arise , factorial hmms pose two concrete problems .
the rst is representation .
in most applications , there is not sufcient data to estimate the elements of the full transition and emission matrices formed by taking the cartesian product of the individual factors .
how should one parameterize these matrices without making restrictive or inelegant assumptions ? ideally , the representation should not make unjustied assumptions of conditional independence , nor should it force us to give up desirable properties of the em algorithm , such as monotone convergence in log - likelihood .
the second problem in factorial hmms is one of computational complexity .
the baum - welch algorithm for parameter estimation scales as o ( n 123 ) , where n is the number of hidden states .
if the hidden state is a cartesian product of k random variables , each of degree n , then the effective number of hidden states is n = nk .
even for small k , this may be prohibitively large to calculate the statistics in the e - step of the em algorithm .
hence , one is naturally led to consider approximations to these statistics .
let us now return to our development of factorial hmms with these issues in mind .
we will see that mixture models provide a good compromise to the problem of represen - tation , and that efcient deterministic approximations exist for the problem of parameter
for concreteness , suppose that we have trained k simple hmms on separate time series of length l .
now we wish to combine these hmms into a single model in order to capture ( what may be ) useful correlations between the different time series .
if each individual hmm had n hidden states and m types of observations , then the hidden state space of the combined model has size nk; likewise , the observation space of the combined model has size mk .
at each time step , we denote these spaces by the cartesian products :
it = i123 jt = j 123
t ( cid : 123 ) i123 t ( cid : 123 ) j 123
t ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ik t ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) jk
in an hmm , it is the hidden states ( as opposed to the observations ) that have a markov dynamics .
accordingly , in this setting , we use eq .
( 123 ) to model the hidden state transition matrix .
by analogy to eqs .
( 123 ) , we parameterize the emission probabilities by :
p ( jtjit ) = y
where b ( cid : 123 ) ( cid : 123 ) ( jji ) are k123 elementary n ( cid : 123 ) m emission matrices .
note that this model can capture correlations between the hidden states of the ( cid : 123 ) th markov chain and the observations in the ( cid : 123 ) th time series .
for the purposes of parameter estimation , it is again convenient to introduce latent variables that encode the mixture components in eq .
by analogy to eqs .
( 123 ) and ( 123 ) ,
t ; y ( cid : 123 )
t = ( cid : 123 ) jit ) = ( cid : 123 ) ( cid : 123 ) ( ( cid : 123 ) ) b ( cid : 123 ) ( cid : 123 ) ( i ( cid : 123 )
t ; j ( cid : 123 )
mixed memory markov models
p ( jt; ytjit ) = y
t ; y ( cid : 123 )
having encoded the mixture components as hidden variables , we can now apply an em algorithm to estimate the model parameters .
in this case , the updates have the form :
( cid : 123 ) ( ( cid : 123 ) ) pt p ( x ( cid : 123 ) pt; ( cid : 123 ) 123 p ( x ( cid : 123 )
t = ( cid : 123 ) jj ) t = ( cid : 123 ) 123jj )
a ( cid : 123 ) ( cid : 123 ) ( i123ji ) pt p ( x ( cid : 123 )
t = ( cid : 123 ) ; i ( cid : 123 )
t ( cid : 123 ) 123 = i; i ( cid : 123 )
t = i123jj )
pt p ( x ( cid : 123 )
t = ( cid : 123 ) ; i ( cid : 123 )
t ( cid : 123 ) 123 = ijj )
( cid : 123 ) ( cid : 123 ) ( ( cid : 123 ) ) pt p ( y ( cid : 123 ) pt; ( cid : 123 ) 123 p ( y ( cid : 123 )
t = ( cid : 123 ) jj ) t = ( cid : 123 ) 123jj )
b ( cid : 123 ) ( cid : 123 ) ( jji ) pt p ( y ( cid : 123 )
t = ( cid : 123 ) ; i ( cid : 123 )
t = i; j ( cid : 123 )
t = jjj )
pt p ( y ( cid : 123 )
t = ( cid : 123 ) ; i ( cid : 123 )
t = ijj )
where j denotes the observed time series .
a viterbi approximation is obtained by con - ditioning not only on j , but also on the most probable sequence of hidden states , i ( cid : 123 ) ,
i ( cid : 123 ) = arg max
p ( itjit ( cid : 123 ) 123 ) p ( jtjit ) :
note that computing the posterior probabilities in these updates requires o ( ln123k ) opera - tions; the same is true for computing the viterbi path .
to avoid this computational burden , we have used an approximation for estimating the statistics in factorial hmms , rst outlined in saul & jordan ( 123 ) .
the basic idea behind our approach is simple : the structure of the factorial hmm , though intractable as a whole , gives rise to efcient approximations that exploit the tractability of its underlying components .
in this note , we discuss how these approximations can be used to estimate the viterbi path .
in general , these ideas may be extended to approximate the full statistics of the posterior distribution , as for example in ghahramani & jordan ( 123 ) .
in the factorial hmm , dynamic programming procedures to compute the viterbi path algorithm require o ( ln123k ) steps .
as a practical alternative , we consider an iterative procedure that returns a ( possibly sub - optimal ) path in polynomial time .
our iteration is based on a subroutine that nds the optimal path of hidden states through the ( cid : 123 ) th chain given xed values for the hidden states of the others .
note that when we instantiate the hidden variables in all but one of the chains , the effective size of the hidden state space collapses from nk to n , and we can perform the optimization with respect to the remaining hidden states in o ( ln123 ) steps .
a factor of k123 is picked up when converting the right hand side of eq .
( 123 ) into a form for which the standard viterbi algorithm can be applied; thus this elementary chainwise viterbi operation requires o ( lk 123n123 ) steps .
the algorithm for approximately computing the full viterbi path of the factorial hmm is obtained by piecing these subroutines together in the obvious way .
first , an initial guess is made for the viterbi path of each component hmm .
( typically , this is done by ignoring the intercomponent correlations and computing a separate viterbi path for each chain . ) then , the chainwise viterbi algorithm is applied , in turn , to each of the component hmms
saul and jordan
figure 123
plot of soprano - tenor correlations versus time , as measured by the posterior probabilities of a mixed
the viterbi algorithm has been applied k times , or once to each chain , the cycle repeats; each iteration of this process therefore involves o ( lk 123n123 ) steps .
note that each iteration results in a sequence of hidden states that is more probable than the preceding one; hence , this process is guaranteed to converge to a nal ( though possibly in practice , we have found that this process typically converges to a stable path in three or four iterations .
the chainwise viterbi algorithm is not guaranteed to nd the truly optimal sequence of hidden states for the factorial hmm .
the success of the algorithm depends on the quality of the initial guess and , as always , the good judgment of the modeler .
the approximation is premised on the assumption that the model describes a set of weakly coupled time series in particular , that the auto - correlations within each time series are as strong or stronger than the cross - correlations between them .
we view the approximation as a computationally cheap way of integrating hmms that have been trained on parallel data streams .
its main virtue is that it exploits the modelers prior knowledge that these separate hmms should be weakly coupled .
when this assumption holds , the approximation is quite accurate .
to test these ideas , we tted a mixed memory hmm to the bach fugue from section 123
one hopes in this model that the hidden states will reect musical structure over longer time scales than a single note .
in our experiments , each voice had a component hmm with six hidden states; thus , in our previous notation , n = 123 and k = 123
we employed a viterbi approximation to the full em algorithm , meaning that the posterior probabilities in eqs .
( 123 ) ( 123 ) were conditioned not only on the observations j , but also on the viterbi path , i ( cid : 123 ) .
the most probable sequence of hidden states i ( cid : 123 ) was estimated by the iterative procedure described above .
again it was interesting to see how this model discovered correlations between the different voices of the fugue .
figure 123 shows a plot of the posterior probabilities p ( xs t = t ji ( cid : 123 ) ; j ) versus time , calculated from the factorial hmm ( after training ) .
the frequent pulses indicate ( within the framework of this model ) moments of strong coupling between the soprano and tenor themes of the fugue .
mixed memory markov models
many parameterizations have been proposed for probabilistic models of time series .
the mixed memory models in this note have three distinguishing features .
first , they can express a rich set of probabilistic dependencies , including coupled dynamics in factorial models .
second , they can be tted by em algorithms , thus avoiding potential drawbacks of gradient descent .
third , they are compact and easy to interpret; notably , as in ordinary markov models , every parameter denes a simple conditional probability .
all these features should enable researchers to build more sophisticated models of dynamical systems .
we thank marney smyth for retrieving the word lists , tommi jaakkola for helping us with finnish , and fernando pereira for pointing out the application to web page prefetching .
we also acknowledge useful discussions with zoubin ghahramani and yoram singer .
this work was initiated while the authors were afliated with the center for biological and computational learning at mit .
during that time , it was supported by nsf grant cda - 123 and onr grant n123 - 123 - 123 - 123
