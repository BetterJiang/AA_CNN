we develop a framework for large margin classication by gaussian mixture models ( gmms ) .
large margin gmms have many parallels to support vector machines ( svms ) , but with classes modeled by ellipsoids instead of half - spaces .
model parameters are trained discriminatively to maximize the margin of correct classication , as measured in terms of mahalanobis distances .
the required optimization is convex over the models parameter space of positive semidenite ma - trices and can be performed efciently .
large margin gmms are naturally suited to large problems in multiway classica - tion; we apply them to phonetic classication and recogni - tion on the timit database .
on both tasks , we obtain signi - cant improvement over baseline systems trained by maximum likelihood estimation .
for the problem of phonetic classi - cation , our results are competitive with other state - of - the - art classiers , such as hidden conditional random elds .
much of the acoustic - phonetic modeling in automatic speech recognition ( asr ) is handled by gaussian mixture models ( gmms ) ( 123 ) .
it is widely recognized that maximum likeli - hood ( ml ) estimation of gmms does not directly optimize the performance of these models as classiers .
it is therefore of interest to develop alternative learning paradigms that op - timize discriminative measures of performance ( 123 , 123 , 123 ) .
support vector machines ( svms ) currently provide state - of - the - art performance for many problems in pattern recog - nition ( 123 ) .
the simplest setting for svms is binary classi - cation .
if the positively and negatively labeled examples are linearly separable , svms compute the linear decision bound - ary that maximizes the margin of correct classication that is , the distance of the closest example ( s ) to the separating hyperplane .
if the labeled examples are not linearly separa - ble , the kernel trick can be used to map the examples into a nonlinear feature space and to compute the maximum margin hyperplane in this space .
alternately , or in conjunction with the kernel trick , the optimization for svms can be relaxed to
permit margin violations ( i . e . , incorrectly labeled examples ) in the training data .
for various reasons , it can be challenging to apply svms to large problems in multiway classication .
first , to ap - ply the kernel trick ( which is required for nonlinear decision boundaries ) , one must construct a large kernel matrix with as many rows and columns as training examples .
second , the training complexity increases with the number of classes , depending to some extent on the way that binary svms are generalized to multiway classication .
in this paper , we develop a framework for large margin classication by gmms .
as in svms , our approach is based on the idea of margin maximization .
intuitively , we show how to train large margin gmms that maximize the mahanalo - bis distance of labeled examples from the decision boundaries that dene competing classes .
as in svms , the parameters of large margin gmms are trained by a convex optimization that focuses on examples near the decision boundaries .
after developing the basic approach in section 123 , we discuss exten - sions for segmental training and outlier handling in section 123 and report experimental results on phonetic classication and recognition in section 123
our approach has certain advantages over svms for large problems in multiway classication .
for example , the classes in large margin gmms are modeled by ellipsoidswhich in - duce nonlinear decision boundaries in the input spaceas op - posed to the half - spaces and hyperplanes in svms .
because the kernel trick is not necessary to induce nonlinear decision boundaries , large margin gmms are more readily trained on very large and difcult data sets , as arise in asr .
large margin mixture models
we begin by describing large margin gmms in the simple setting where each class is modeled by a single ellipsoid .
we then extend this framework to the case where each class is modeled by one or more ellipsoids .
finally , we relate our framework to other discriminative paradigms that have been proposed for training gmms .
c cc + c
large margin classication
the simplest large margin gmm represents each class of la - beled examples by a single ellipsoid .
each ellipsoid is pa - rameterized by a vector centroid <d and a positive semidenite orientation matrix <dd .
these param - eters are analogous to the means and inverse covariance ma - trices of multivariate gaussians , but they are not estimated in the same way .
in addition , a nonnegative scalar offset 123 for each class is used in the scoring procedure .
let ( c , c , c ) denote the centroid , orientation matrix , and scalar offset representing examples in class c .
we label an example x <d by whichever ellipsoid has the smallest mahanalobis distance ( plus offset ) to its centroid :
( cid : 123 ) ( xc ) tc ( xc ) + c
y = argmin
the goal of learning is to estimate the parameters ( c , c , c ) for each class of labeled examples that optimize the perfor - mance of this decision rule .
it is useful to collect the ellipsoid parameters of each class
in a single enlarged positive semidenite matrix :
( cid : 123 ) ztc z ( cid : 123 ) where
we can then rewrite the decision rule in eq .
( 123 ) as simply :
y = argmin
here , z <d+123 is the vector created by appending a unit element to x <d .
in this transformed representation , the goal of learning is simply to estimate the single matrix c < ( d+123 ) ( d+123 ) for each class of labeled examples .
we now consider the problem of learning in more detail .
let ( ( xn , yn ) ) n n=123 denote a set of n labeled examples drawn from c classes , where xn <d and yn ( 123 , 123 , .
in large margin gmms , we seek matrices c such that all the examples in the training set are correctly classied by a large margini . e . , situated far from the decision boundaries that dene competing classes .
for the nth example with class label yn , this condition can be written as : n czn 123 + zt
( 123 ) states that for each competing class c 123= yn , the maha - lanobis distance ( plus offset ) to the cth centroid exceeds the mahalanobis distance ( plus offset ) to the target centroid by a margin of at least one unit .
c 123= yn ,
we adopt a convex loss function for training large margin gmms .
analogous to svms , the loss function has two terms , one that penalizes margin violations of eq .
( 123 ) and one that regularizes the matrices c .
letting ( f ) + =max ( 123 , f ) denote the so - called hinge function , we can write the loss function for large margin gmms as :
n ( yn c ) zn
the second term regularizes the orientation matrices c which appear in the d d upper left blocks of c .
in real - izable settings , where all the examples can be correctly clas - sied , the second term favors the minimum trace maha - lanobis metrics consistent with the unit margin constraints in eq .
the relative weight of the two terms is controlled by a hyperparameter >123 set by cross - validation .
the loss function in eq .
( 123 ) is a piecewise linear , convex function of the matrices c , which are further constrained to be positive semidenite .
its optimization can thus be formu - lated as a problem in semidenite programming ( 123 ) .
such problems can be generically solved by interior point algo - rithms with polynomial time guarantees ( though we imple - mented a special - purpose solver using gradient - based meth - ods for the results in this paper ) .
most importantly , eq .
( 123 ) has the desirable property that its optimization is not plagued by spurious local minima .
mixture models
we now extend the previous model to represent each class by multiple ellipsoids .
this is analogous to modeling each class by its own gmm , as opposed to a single gaussian .
let cm denote the matrix for the mth ellipsoid in class c .
the most straightforward extension is to imagine that each example xn has not only a class label yn , but also a mixture component label mn .
the latter labels are not provided a priori , but we can generate proxy labels by tting a gmm to the exam - ples in each class by ml estimation , then for each example , computing the mixture component with the highest posterior probability under this gmm .
given joint labels ( yn , mn ) , we rewrite the large margin criterion in eq .
( 123 ) as : n cmzn 123 + zt
c 123= yn , logx follows from the fact that logp n ynmnzn + logp
( 123 ) states that for each competing class c 123= yn , the match to any centroid in class c is worse than the match to the target centroid by a margin of at least one unit .
this interpretation
m eam minm am .
classes and mixture components : p
the loss function for mixture models is a simple exten - sion of eq .
we replace the hinge loss in the rst term by ( 123 + zt n cmzn ) + , which penal - izes violations of the margin inequalities in eq .
the regu - larizer in the second term changes only to sum over different cm trace ( cm ) .
due to the softmin operation over mixture components , the result - ing loss function is no longer piecewise linear in the matri - ces cm; however , it is easy to verify that it remains convex .
thus , even the optimization of this more general loss function for large margin gmms is quite tractable .
relation to previous work
our framework differs in important aspects from previous frameworks for discriminative training of gmms ( 123 )
pose the class - conditional densities p ( x|y ) are modeled by gmms .
one common approach to discriminative training es - timates the means , covariance matrices , and mixture weights of these models that maximize the conditional log - likelihood n log p ( yn|xn ) .
such models generally outperform gmms that are estimated by maximizing the joint log - likelihood n log p ( xn , yn ) .
in contrast to our framework , however , the optimization of gmm parameters in this way is not convex .
moreover , as a loss function , the conditional log - likelihood does not focus on examples near the decision boundaries nor incorporate the idea of a large margin .
the main difference between large margin gmms and svms is that classes are modeled by ellipsoids , rather than half - spaces , and that the kernel trick ( which involves main - taining a large kernel matrix ) is not required for nonlinear de - cision boundaries .
of course , one can generate quadratic de - cision boundaries in svms by choosing a polynomial kernel of degree two , or by expanding the vectors xn to include pairwise products of their original elements .
large margin gmms differ from such svms by restricting their quadratic forms to be positive semidenite and by imagining the sup - port of each class as some bounded region in input space .
in addition , such svms cannot represent the large margin gmms in section 123 , with multiple ellipsoids per class .
two further extensions of large margin gmms are impor - tant for problems in asr : handling of outliers , and segmental training .
we describe each extension in isolation , assuming for simplicity that each class is modeled by a single ellip - soid , as in section 123 .
the generalization to the large margin gmms described in section 123 is straightforward , as is the handling of outliers in combination with segmental training .
handling of outliers
many discriminative learning algorithms are sensitive to out - liers .
margin - based loss functions , in particular , do not closely track the classication error rate when the training data has many outliers .
we adopt a simple strategy to detect outliers and reduce their malicious effect on learning .
outliers are detected using ml estimates of the mean and covariance matrix of each class .
these estimates are used to initialize matrices ml of the form in eq .
then , for each example xn , we compute the accumulated hinge loss incurred by violations of the large margin constraints in eq
n 123 measures the decrease in the loss func - note that hml tion when an initially misclassied example xn is corrected during the course of learning .
we associate outliers with large values of hml
outliers distort the learning process by diverting its fo - cus away from misclassied examples that could otherwise be easily corrected .
in particular , correcting one badly misclas - sied outlier decreases the cost function proposed in eq .
( 123 ) more than correcting multiple examples that lie just barely on the wrong side of a decision boundary .
to x this sit - uation , we reweight the hinge loss terms in eq .
( 123 ) involv - ing example xn by a multiplicative factor of min ( 123 , 123 / hml this reweighting equalizes the losses incurred by all initially misclassied examples , thus reducing the malicious effect of outliers .
we compute the weighting factors once from the ml estimates and hold them xed during discriminative training .
in practice , this scheme appears to work very satisfactorily .
segmental training
the margin constraints in eq .
( 123 ) apply to individually labeled examples .
we can also relax them to apply , collectively , to multiple examples known to share the same class label .
this is useful for asr , where we can train on variable - length seg - ments , consisting of multiple consecutive analysis frames , all of which belong to the same phoneme .
specically , let p index the frames in the nth phonetic segment ( xnp ) segmental training , we rewrite the constraints in eq .
( 123 ) as : c 123= yn ,
npcznp 123 +
where the scores on both sides have been normalized by the segment length .
the segment - based constraint in eq .
( 123 ) is especially well motivated if a segment - based decision rule is used for classication ( e . g . , y = argminc p czp ) as opposed to the frame - based rule in eq
experimental results
we applied large margin gmms to well - benchmarked prob - lems in phonetic classication and recognition on the timit database ( 123 , 123 , 123 , 123 ) .
we used the standard partition of train - ing and test data and the same development set as in earlier work ( 123 , 123 ) .
all sa sentences were excluded .
we mapped the 123 phonetic labels in timit to 123 classes and trained ml and large margin gmms for each classes .
results were eval - uated by mapping these 123 classes to 123 phones to remove further confusions , as in previous benchmarks .
our front end computed mel - frequency cepstral coefcients ( mfccs ) with 123 ms windows at a 123 ms frame rate .
we retained the rst 123 mfcc coefcients of each frame , along with their rst and second time derivatives .
gmms modeled these 123 - dimensional feature vectors after they were whitened by pca .
phonetic classication
phonetic classication is an articial but instructive prob - lem in asr .
one assumes in this case that the speech has
# of mixture
123% 123% 123% 123%
table 123
error rates for phonetic classication and recognition on the timit database .
large margin gmms are compared to baseline gmms trained by ml estimation .
see text for details .
been correctly segmented into phonetic units , but that the phonetic class label of each segment is unknown .
the in - put to the classier is the segment of consecutive analysis frames that spans precisely one phoneme .
we trained large margin gmms using the segment - based margin criteria in section 123 and compared them to baseline ( full covariance ) gmms trained by ml estimation .
the baseline gmms were also used to determine the proxy labels for mixture com - ponents in eq .
( 123 ) and to detect and reweight outliers , using eq .
we used the development data set to choose the hyper - parameter > 123 in eq .
( 123 ) , to tune a unigram language model , and to perform early stopping of the optimization procedure .
the training time on 123m frames ( roughly , 123k segments ) ranged from 123 - 123 hours depending on the model size .
table 123 shows the percentage of incorrectly classied pho - netic segments on the timit test set .
large margin gmms consistently and signicantly outperform baseline gmms with equal numbers of mixture components .
the best large margin gmm also yields a slightly lower classication error rate than state - of - the - art results ( 123% ) obtained by hidden conditional random elds ( 123 ) .
phonetic recognition
the same baseline and large margin gmms were used to build phonetic recognizers .
the recognizers were rst - order hidden markov models ( hmms ) with one context - independent state per phonetic class .
baseline or large mar - gin gmms were used in these hmms to compute the log probabilities ( or scores ) of observed frames .
in all hmms , we used the development set to optimize the weighting of log transition probabilities .
table 123 compares the phone er - ror rates of these hmms , obtained by aligning the results of viterbi decoding with the ground - truth phonetic transcrip - tions ( 123 ) .
again , the large margin gmms lead to consis - tently lower error rates , here computed as the sum of sub - stitution , deletion , and insertion error rates .
we also ob - serve , as in previous work ( 123 ) , that discriminatively - trained context - independent phone models achieve lower error rates than context - dependent models trained by ml estimation .
we have shown how to learn gmms for multiway classi - cation based on similar principles as large margin classica - tion in svms .
classes are represented by ellipsoids whose location , shape , and size are discriminatively trained to maxi - mize the margin of correct classication , as measured in terms of mahalanobis distances .
the required optimization is con - vex over the models parameter space of positive semide - nite matrices .
applied to problems in phonetic classication and recognition , we showed that large margin gmms led to signicant improvement over baseline gmms .
work , we are investigating phonetic recognizers with context - dependent phone models , which are known to reduce phone error rates ( 123 , 123 ) .
we are also studying schemes for integrat - ing the large margin training of gmms with sequence models such as hmms and / or conditional random elds ( 123 ) .
this work was supported by nsf award 123
we thank a .
gunawardana ( microsoft research ) and k .
crammer ( uni - versity of pennsylvania ) for many useful discussions and
