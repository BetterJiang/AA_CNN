we consider the use of language models whose size and accuracy are intermedi - ate between dierent order n - gram models .
two types of models are studied in partic - ular .
aggregate markov models are class - based bigram models in which the map - ping from words to classes is probabilis - tic .
mixed - order markov models combine bigram models whose predictions are con - ditioned on dierent words .
both types of models are trained by expectation - maximization ( em ) algorithms for maxi - mum likelihood estimation .
we examine smoothing procedures in which these mod - els are interposed between dierent order n - grams .
this is found to signicantly re - duce the perplexity of unseen word combi -
the purpose of a statistical language model is to as - sign high probabilities to likely word sequences and low probabilities to unlikely ones .
the challenge here arises from the combinatorially large number of possibilities , only a fraction of which can ever be in general , language models must learn to recognize word sequences that are functionally similar but lexically distinct .
the learning problem , one of generalizing from sparse data , is particularly acute for large - sized vocabularies ( jelinek , mercer , and roukos , 123 ) .
the simplest models of natural language are n - gram markov models .
in these models , the prob - ability of each word depends on the n 123 words that precede it .
the problems in estimating ro - bust models of this form are well - documented .
the number of parametersor transition probabilities scales as v n , where v is the vocabulary size
typical models ( e . g . , n = 123 , v = 123 ) , this num - ber exceeds by many orders of magnitude the total number of words in any feasible training corpus .
the transition probabilities in n - gram models are estimated from the counts of word combinations in the training corpus .
maximum likelihood ( ml ) esti - mation leads to zero - valued probabilities for unseen n - grams .
in practice , one adjusts or smoothes ( chen and goodman , 123 ) the ml estimates so that the language model can generalize to new phrases .
smoothing can be done in many waysfor example , by introducing articial counts , backing o to lower - order models ( katz , 123 ) , or combining models by interpolation ( jelinek and mercer , 123 ) .
often a great deal of information is lost in the smoothing procedure .
this is due to the great dis - crepancy between n - gram models of dierent order .
the goal of this paper is to investigate models that are intermediate , in both size and accuracy , between dierent order n - gram models .
we show that such models can intervene between dierent order n - grams in the smoothing procedure .
experimentally , we nd that this signicantly reduces the perplexity of unseen word combinations .
the language models in this paper were evalu - ated on the arpa north american business news ( nab ) corpus .
all our experiments used a vo - cabulary of sixty - thousand words , including tokens for punctuation , sentence boundaries , and an un - known word token standing for all out - of - vocabulary words .
the training data consisted of approxi - mately 123 million words ( three million sentences ) ; the test data , 123 million words ( one - half million sentences ) .
all sentences were drawn randomly without replacement from the nab corpus .
all perplexity gures given in the paper are com - puted by combining sentence probabilities; the prob - ability of sentence w123w123 wnwn+123 is given by i=123 p ( wi|w123 wi123 ) , where w123 and wn+123 are the start - and end - of - sentence markers , respectively .
though not reported below , we also conrmed that the results did not vary signicantly for dierent ran - domly drawn test sets of the same size .
the organization of this paper is as follows .
in section 123 , we examine aggregate markov mod - els , or class - based bigram models ( brown et al . , 123 ) in which the mapping from words to classes is probabilistic .
we describe an iterative algo - rithm for discovering soft word classes , based on the expectation - maximization ( em ) procedure for maximum likelihood estimation ( dempster , laird , and rubin , 123 ) .
several features make this algo - rithm attractive for large - vocabulary language mod - eling : it has no tuning parameters , converges mono - tonically in the log - likelihood , and handles proba - bilistic constraints in a natural way .
the number of classes , c , can be small or large depending on the constraints of the modeler .
varying the number of classes leads to models that are intermediate be - tween unigram ( c = 123 ) and bigram ( c = v ) models .
in section 123 , we examine another sort of inter - mediate model , one that arises from combinations of non - adjacent words .
language models using such combinations have been proposed by huang et al .
( 123 ) , ney , essen , and kneser ( 123 ) , and rosen - feld ( 123 ) , among others .
we consider specically the skip - k transition matrices , m ( wtk , wt ) , whose predictions are conditioned on the kth previous word in the sentence .
( the value of k determines how many words one skips back to make the predic - tion . ) these predictions , conditioned on only a sin - gle previous word in the sentence , are inherently weaker than those conditioned on all k previous words .
nevertheless , by combining several predic - tions of this form ( for dierent values of k ) , we can create a model that is intermediate in size and ac - curacy between bigram and trigram models .
mixed - order markov models express the predic - tions p ( wt|wt123 , wt123 , .
, wtm ) as a convex com - bination of skip - k transition matrices , m ( wtk , wt ) .
we derive an em algorithm to learn the mixing co - ecients , as well as the elements of the transition matrices .
the number of transition probabilities in these models scales as mv 123 , as opposed to v m+123
mixed - order models are not as powerful as trigram models , but they can make much stronger predic - tions than bigram models .
the reason is that quite often the immediately preceding word has less pre - dictive value than earlier words in the same sentence .
in section 123 , we use aggregate and mixed - order models to improve the probability estimates from n - grams .
this is done by interposing these models between dierent order n - grams in the smoothing procedure .
we compare our results to a baseline tri -
gram model that backs o to bigram and unigram models .
the use of intermediate models is found to reduce the perplexity of unseen word combina - tions by over 123% .
in section 123 , we discuss some extensions to these models and some open problems for future research .
we conclude that aggregate and mixed - order models provide a compelling alternative to language models based exclusively on n - grams .
123 aggregate markov models
in this section we consider how to construct class - based bigram models ( brown et al . , 123 ) .
the problem is naturally formulated as one of hidden variable density estimation .
let p ( c|w123 ) denote the probability that word w123 is mapped into class c .
likewise , let p ( w123|c ) denote the probability that words in class c are followed by the word w123
the class - based bigram model predicts that word w123 is followed by word w123 with probability
p ( w123|w123 ) =
p ( w123|c ) p ( c|w123 ) ,
where c is the total number of classes .
the hidden variable in this problem is the class label c , which is unknown for each word w123
note that eq .
( 123 ) represents the v 123 elements of the transition matrix p ( w123|w123 ) in terms of the 123cv elements of p ( w123|c ) and p ( c|w123 ) .
the expectation - maximization ( em ) algorithm ( dempster , laird , and rubin , 123 ) is an iterative procedure for estimating the parameters of hidden variable models .
each iteration consists of two steps : an e - step which computes statistics over the hidden variables , and an m - step which updates the param - eters to reect these statistics .
the em algorithm for aggregate markov models is particularly simple .
the e - step is to compute , for each bigram w123w123 in the training set , the posterior
p ( c|w123 , w123 ) =
p ( w123|c ) p ( c|w123 )
pc p ( w123|c ) p ( c|w123 )
( 123 ) gives the probability that word w123 was as - signed to class c , based on the observation that it was followed by word w123
the m - step uses these posterior probabilities to re - estimate the model pa - rameters .
the updates for aggregate markov models
p ( c|w123 ) pw n ( w123 , w ) p ( c|w123 , w ) pwc n ( w123 , w ) p ( c|w123 , w ) p ( w123|c ) pw n ( w , w123 ) p ( c|w , w123 ) pww n ( w , w ) p ( c|w , w )
table 123 : perplexities of aggregate markov models on the training and test sets; c is the number of classes .
the case c = 123 corresponds to a ml unigram model; c = v , to a ml bigram model .
winning assignment probability
figure 123 : histogram of the winning assignment probabilities , maxc p ( c|w ) , for the three hundred most commonly occurring words .
where n ( w123 , w123 ) denotes the number of counts of w123w123 in the training set .
these updates are guar - anteed to increase the overall log - likelihood ,
n ( w123 , w123 ) ln p ( w123|w123 ) ,
at each iteration .
in general , they converge to a local ( though not global ) maximum of the log - likelihood .
the perplexity v is related to the log - likelihood by v = e / n , where n is the total number of words
though several algorithms ( brown et al . , 123; pereira , tishby , and lee , 123 ) have been proposed for performing the decomposition in eq .
( 123 ) , it is worth noting that only the em algorithm directly optimizes the log - likelihood in eq .
this has ob - vious advantages if the goal of nding word classes is to improve the perplexity of a language model .
the em algorithm also handles probabilistic constraints in a natural way , allowing words to belong to more than one class if this increases the overall likelihood .
our approach diers in important ways from the use of hidden markov models ( hmms ) for class - based language modeling ( jelinek et al . , 123 ) .
while hmms also use hidden variables to represent word classes , the dynamics are fundamentally dif - ferent .
in hmms , the hidden state at time t + 123 is predicted ( via the state transition matrix ) from the hidden state at time t .
on the other hand , in aggre - gate markov models , the hidden state at time t + 123 is predicted ( via the matrix p ( ct+123|wt ) ) from the word at time t .
the state - to - state versus word - to - state dynamics lead to dierent learning algorithms .
for example , the baum - welch algorithm for hmms requires forward and backward passes through each training sentence , while the em algorithm we use
we trained aggregate markov models with 123 , 123 , 123 , 123 , and 123 classes .
figure 123 shows typical plots of the training and test set perplexities versus the number of iterations of the em algorithm .
clearly , the two curves are very close , and the monotonic decrease in test set perplexity strongly suggests lit - tle if any overtting , at least when the number of classes is small compared to the number of words in the vocabulary .
table 123 shows the nal perplexities ( after thirty - two iterations of em ) for various ag - gregate markov models .
these results conrm that aggregate markov models are intermediate in accu - racy between unigram ( c = 123 ) and bigram ( c = v )
the aggregate markov models were also observed to discover meaningful word classes .
table 123 shows , for the aggregate model with c = 123 classes , the most probable class assignments of the three hun - dred most commonly occurring words .
to be precise , for each class c , we have listed the words for which c = arg maxc p ( c|w ) .
figure 123 shows a histogram of the winning assignment probabilities , maxc p ( c|w ) , for these words .
note that the winning assignment probabilities are distributed broadly over the inter - val ( 123 c , 123 ) .
this demonstrates the utility of allowing soft membership classes : for most words , the max - imum likelihood estimates of p ( c|w ) do not corre - spond to a winner - take - all assignment , and therefore any method that assigns each word to a single class ( hard clustering ) , such as those used by brown et al .
( 123 ) or ney , essen , and kneser ( 123 ) , would
we conclude this section with some nal com - ments on overtting .
our models were trained by thirty - two iterations of em , allowing for nearly com - plete convergence in the log - likelihood .
moreover , we did not implement any ooring constraints123 on
123it is worth noting , in this regard , that individual zeros in the matrices p ( w123|c ) and p ( c|w123 ) do not nec - essarily give rise to zeros in the matrix p ( w123|w123 ) , as computed from eq
iteration of em
iteration of em
figure 123 : plots of ( a ) training and ( b ) test perplexity versus number of iterations of the em algorithm , for the aggregate markov model with c = 123 classes .
123 as cents made make take
ago day earlier friday monday month quarter reported said thursday trading tuesday wednesday h
123 even get to
based days down home months up work years
123 those h , i hi 123 h . i h ? i
eighty fty forty ninety seventy sixty thirty twenty h ( i hi
123 can could may should to will would 123 about at just only or than h&i h;i 123 economic high interest much no such tax united
123 because do how if most say so then think very
what when where
123 according back expected going him plan used way 123 dont i people they we you
bush company court department more ocials police retort spokesman
123 former the
american big city federal general house military national party political state union york
123 billion hundred million nineteen 123 did hi hi 123 but called san h : i hstart - of - sentencei
bank board chairman end group members number oce out part percent price prices rate sales shares use a an another any dollar each rst good her his its my old our their this
123 long mr
business california case companies corporation dollars incorporated industry law money thousand time today war week h ) i hunknowni
123 also government he it market she that there
123 both foreign international major many new oil
other some soviet stock these west world after all among and before between by during for from in including into like of o on over since through told under until while with eight fteen ve four half last next nine oh one second seven several six ten third three twelve two zero h - i
123 are be been being had has have is its not still
123 chief exchange news public service trade
table 123 : most probable assignments for the 123 most frequent words in an aggregate markov model with c = 123 classes .
class 123 is absent because it is not the most probable class for any of the selected words . )
the probabilities p ( c|w123 ) or p ( w123|c ) .
nevertheless , in all our experiments , the ml aggregate markov models assigned non - zero probability to all the bi - grams in the test set .
this suggests that for large vocabularies there is a useful regime 123 c v in which aggregate models do not suer much from overtting .
in this regime , aggregate models can be relied upon to compute the probabilities of unseen word combinations .
we will return to this point in
section 123 , when we consider how to smooth n - gram
123 mixed - order markov models
one of the drawbacks of n - gram models is that their size grows rapidly with their order .
in this section , we consider how to make predictions based on a con - vex combination of pairwise correlations .
this leads
to language models whose size grows linearly in the number of words used for each prediction .
for each k > 123 , the skip - k transition matrix m ( wtk , wt ) predicts the current word from the kth previous word in the sentence .
a mixed - order markov model combines the information in these matrices for dierent values of k .
let m denote the number of bigram models being combined .
the probability distribution for these models has the
p ( wt|wt123 , .
, wtm ) =
k ( wtk ) mk ( wtk , wt )
( 123 j ( wtj ) ) .
the terms in this equation have a simple interpreta - tion .
the v v matrices mk ( w , w ) in eq .
( 123 ) de - ne the skip - k stochastic dependency of w at some position t on w at position t k; the parameters k ( w ) are mixing coecients that weight the predic - tions from these dierent dependencies .
the value of k ( w ) can be interpreted as the probability that the model , upon seeing the word wtk , looks no further back to make its prediction ( singer , 123 ) .
thus the model predicts from wt123 with probability 123 ( wt123 ) , from wt123 with probability ( 123 123 ( wt123 ) ) 123 ( wt123 ) , and so on .
though included in eq .
( 123 ) for cosmetic reasons , the parameters m ( w ) are actually xed to unity so that the model never looks further than m
we can view eq .
( 123 ) as a hidden variable model .
imagine that we adopt the following strategy to pre - dict the word at time t .
starting with the previous word , we toss a coin ( with bias 123 ( wt123 ) ) to see if this word has high predictive value .
if the answer is yes , then we predict from the skip - 123 transition matrix , m123 ( wt123 , wt ) .
otherwise , we shift our at - tention one word to the left and repeat the process .
if after m 123 tosses we have not settled on a pre - diction , then as a last resort , we make a prediction using mm ( wtm , wt ) .
the hidden variables in this process are the outcomes of the coin tosses , which are unknown for each word wtk .
viewing the model in this way , we can derive an em algorithm to learn the mixing coecients k ( w ) and the transition matrices123 mk ( w , w ) .
the e - step of the algorithm is to compute , for each word in the
123note that the ml estimates of mk ( w , w ) do not depend only on the raw counts of k - separated bigrams; they are also coupled to the values of the mixing coef - cients , k ( w ) .
in particular , the em algorithm adapts the matrix elements to the weighting of word combina - tions in eq .
the raw counts of k - separated bigrams , however , do give good initial estimates .
iteration of em
figure 123 : plot of ( training set ) perplexity versus number of iterations of the em algorithm .
the re - sults are for the m = 123 mixed - order markov model .
training set , the posterior probability that it was generated by mk ( wtk , wt ) .
denoting these poste - rior probabilities by k ( t ) , we have :
k ( wtk ) mk ( wtk , wt ) qk123
j=123 ( 123j ( wtj ) )
p ( wt|wt123 , wt123 , .
, wtm )
where the denominator is given by eq .
the m - step of the algorithm is to update the parame - ters k ( w ) and mk ( w , w ) to reect the statistics in eq .
the updates for mixed - order markov models are given by :
k ( w ) pt ( w , wtk ) k ( t )
j=k ( w , wtk ) j ( t )
mk ( w , w ) pt ( w , wtk ) ( w , wt ) k ( t )
pt ( w , wtk ) k ( t )
where the sums are over all the sentences in the training set , and ( w , w ) = 123 i w = w .
we trained mixed - order markov models with 123 m 123
figure 123 shows a typical plot of the train - ing set perplexity as a function of the number of iterations of the em algorithm .
table 123 shows the nal perplexities on the training set ( after four iter - ations of em ) .
mixed - order models cannot be used directly on the test set because they predict zero probability for unseen word combinations .
unlike standard n - gram models , however , the number of unseen word combinations actually decreases with the order of the model .
the reason for this is that mixed - order models assign nite probability to all n - grams w123w123 .
wn for which any of the k - separated bigrams wkwn are observed in the training set .
to illustrate this point , table 123 shows the fraction of words in the test set that were assigned zero proba - bility by the mixed - order model .
as expected , this fraction decreases monotonically with the number of bigrams that are mixed into each prediction .
m train missing
table 123 : results for ml mixed - order models; m de - notes the number of bigrams that were mixed into each prediction .
the rst column shows the per - plexities on the training set .
the second shows the fraction of words in the test set that were assigned zero probability .
the case m = 123 corresponds to a ml bigram model .
clearly , the success of mixed - order models de - pends on the ability to gauge the predictive value of each word , relative to earlier words in the same sentence .
let us see how this plays out for the second - order ( m = 123 ) model in table 123
model , a small value for 123 ( w ) indicates that the word w typically carries less information that the word that precedes it .
on the other hand , a large value for 123 ( w ) indicates that the word w is highly predictive .
the ability to learn these relationships is conrmed by the results in table 123
of the three - hundred most common words , table 123 shows the fty with the lowest and highest values of 123 ( w ) .
note how low values of 123 ( w ) are associated with prepositions , mid - sentence punctuation marks , and conjunctions , while high values are associated with contentful words and end - of - sentence markers .
( a particularly interesting dichotomy arises for the two forms a and an of the indenite article; the lat - ter , because it always precedes a word that begins with a vowel , is inherently more predictive . ) these results underscore the importance of allowing the coecients 123 ( w ) to depend on the context w , as opposed to being context - independent ( ney , essen , and kneser , 123 ) .
smoothing plays an essential role in language models where ml predictions are unreliable for rare events .
in n - gram modeling , it is common to adopt a re - cursive strategy , smoothing bigrams by unigrams , trigrams by bigrams , and so on .
here we adopt a similar strategy , using the ( m 123 ) th mixed - order model to smooth the mth one .
at the root of our smoothing procedure , however , lies not a uni - gram model , but an aggregate markov model with c > 123 classes .
as shown in section 123 , these models assign nite probability to all word combinations ,
123 < 123 ( w ) < 123
h - i and of hi or h;i to h , i h&i by with s .
from nine were for that eight low seven the h ( i h : i six are not against was four between a their two three its hunknowni b .
on as is hi ve h ) i into c .
her him over than a .
123 < 123 ( w ) 123
ocials prices which go way he last they earlier an tuesday there foreign quarter she former federal dont days friday next wednesday h%i thursday i monday mr .
we half based part united its years going nineteen thousand months hi million very cents san ago u .
percent billion h ? i according h . i
table 123 : words with low and high values of 123 ( w ) in an m = 123 mixed order model .
table 123 : perplexities of bigram models smoothed by aggregate markov models with dierent numbers of
even those that are not observed in the training set .
hence , they can legitimately replace unigrams as the base model in the smoothing procedure .
let us rst examine the impact of replacing uni - gram models by aggregate models at the root of the smoothing procedure .
to this end , a held - out inter - polation algorithm ( jelinek and mercer , 123 ) was used to smooth an ml bigram model with the aggre - gate markov models from section 123
the smoothing parameters , one for each row of the bigram transi - tion matrix , were estimated from a validation set the same size as the test set .
table 123 gives the nal per - plexities on the validation set , the test set , and the unseen bigrams in the test set .
note that smooth - ing with the c = 123 aggregate markov model has nearly halved the perplexity of unseen bigrams , as compared to smoothing with the unigram model .
let us now examine the recursive use of mixed - order models to obtain smoothed probability esti - mates .
again , a held - out interpolation algorithm was used to smooth the mixed - order markov models from section 123
the mth mixed - order model had mv smoothing parameters k ( w ) , corresponding to the v rows in each skip - k transition matrix
table 123 : perplexities of smoothed mixed - order mod - els on the validation and test sets .
mth mixed - order model was smoothed by discount - ing the weight of each skip - k prediction , then ll - ing in the leftover probability mass by a lower - order model .
in particular , the discounted weight of the skip - k prediction was given by
( 123 j ( wtj ) )
leaving a total mass of
( 123 j ( wtj ) )
for the ( m 123 ) th mixed - order model .
the m = 123 mixed - order model corresponds to a ml
table 123 shows the perplexities of the smoothed mixed - order models on the validation and test sets .
an aggregate markov model with c = 123 classes was used as the base model in the smoothing proce - dure .
the rst row corresponds to a bigram model smoothed by a aggregate markov model; the second row corresponds to an m = 123 mixed - order model , smoothed by a ml bigram model , smoothed by an aggregate markov model; the third row corresponds to an m = 123 mixed - order model , smoothed by a m = 123 mixed - order model , smoothed by a ml bi - gram model , etc .
a signicant decrease in perplex - ity occurs in moving to the smoothed m = 123 mixed - order model .
on the other hand , the dierence in perplexity for higher values of m is not very dra -
our last experiment looked at the smoothing of a trigram model .
our baseline was a ml trigram model that backed o 123 to bigrams ( and when nec - essary , unigrams ) using the katz backo procedure ( katz , 123 ) .
in this procedure , the predictions of the ml trigram model are discounted by an amount determined by the good - turing coecients; the left - over probability mass is then lled in by the backo
123we used a backo procedure ( instead of interpo - lation ) to avoid the estimation of trigram smoothing
table 123 : perplexities of two smoothed trigram mod - els on the test set and the subset of unseen word combinations .
the baseline model backed o to bi - grams and unigrams; the other backed o to the m = 123 model in table 123
we compared this to a trigram model that backed o to the m = 123 model in table 123
this was handled by a slight variant of the katz procedure ( dagan , pereira , and lee , 123 ) in which the mixed - order model substituted for the backo model .
one advantage of this smoothing procedure is that it is straightforward to assess the performance of dif - ferent backo models .
because the backo models are only consulted for unseen word combinations , the perplexity on these word combinations serves as a reasonable gure - of - merit .
table 123 shows those perplexities for the two smoothed trigram models ( baseline and backo ) .
the mixed - order smoothing was found to reduce the perplexity of unseen word combinations by 123% .
also shown in the table are the perplexities on the entire test set .
the overall perplexity decreased by 123%a signicant amount considering that only 123% of the predictions involved unseen word com - binations and required backing o from the trigram
the models in table 123 were constructed from all n - grams ( 123 n 123 ) observed in the training data .
because many n - grams occur very infrequently , a natural question is whether truncated models , which omit low - frequency n - grams from the training set , can perform as well as untruncated ones .
the ad - vantage of truncated models is that they do not need to store nearly as many non - zero parameters as un - truncated models .
the results in table 123 were ob - tained by dropping trigrams that occurred less than t times in the training corpus .
the t = 123 row cor - responds to the models in table 123
the most in - teresting observation from the table is that omitting very low - frequency trigrams does not decrease the quality of the mixed - order model , and may in fact slightly improve it .
this contrasts with the standard backo model , in which truncation causes signicant increases in perplexity .
table 123 : eect of truncating trigrams that occur less than t times .
the table shows the baseline and mixed - order perplexities on the test set , the num - ber of distinct trigrams with t or more counts , and the fraction of trigrams in the test set that required
our results demonstrate the utility of language mod - els that are intermediate in size and accuracy be - tween dierent order n - gram models .
the two models considered in this paper were hidden vari - able markov models trained by em algorithms for maximum likelihood estimation .
combinations of intermediate - order models were also investigated by rosenfeld ( 123 ) .
his experiments used the 123 , 123 - word vocabulary wall street journal corpus , a pre - decessor of the nab corpus .
he trained a maximum - entropy model consisting of unigrams , bigrams , tri - grams , skip - 123 bigrams and trigrams; after selecting long - distance bigrams ( word triggers ) on 123 million words , the model was tested on a held - out 123 thou - sand word sample .
rosenfeld reported a test - set perplexity of 123 , a 123% reduction from the 123 per - plexity of a baseline trigram backo model .
in our experiments , the perplexity gain of the mixed - order model ranged from 123% to 123% , depending on the amount of truncation in the trigram model .
while rosenfelds results and ours are not di - rectly comparable , both demonstrate the utility of it is worth discussing , how - ever , the dierent approaches to combining infor - mation from non - adjacent words .
unlike the max - imum entropy approach , which allows one to com - bine many non - independent features , ours calls for a careful markovian decomposition .
rosenfeld ar - gues at length against nave linear combinations in favor of maximum entropy methods .
his arguments do not apply to our work for several reasons .
first , we use a large number of context - dependent mixing parameters to optimize the overall likelihood of the combined model .
thus , the weighting in eq .
( 123 ) en - sures that the skip - k predictions are only invoked when the context is appropriate .
second , we adjust the predictions of the skip - k transition matrices ( by
em ) so that they match the contexts in which they are invoked .
hence , the count - based models are in - terpolated in a way that is consistent with their
training eciency is another issue in evaluating language models .
the maximum entropy method requires very long training times : e . g . , 123 cpu - days in rosenfelds experiments .
our methods re - quire signicantly less; for example , we trained the smoothed m = 123 mixed - order model , from start to in less than 123 cpu - hours ( while using a larger training corpus ) .
even accounting for dier - ences in processor speed , this amounts to a signi - cant mismatch in overall training time .
in conclusion , let us mention some open problems for further research .
aggregate markov models can be viewed as approximating the full bigram tran - sition matrix by a matrix of lower rank .
( 123 ) , it should be clear that the rank of the class - based transition matrix is bounded by the num - ber of classes , c . ) as such , there are interesting parallels between expectation - maximization ( em ) , which minimizes the approximation error as mea - sured by the kl divergence , and singular value de - composition ( svd ) , which minimizes the approxi - mation error as measured by the l123 norm ( press et al . , 123; schutze , 123 ) .
whereas svd nds a global minimum in its error measure , however , em only nds a local one .
it would clearly be desirable to improve our understanding of this fundamental
in this paper we have focused on bigram models , but the ideas and algorithms generalize in a straight - forward way to higher - order n - grams .
aggregate models based on higher - order n - grams ( brown et al . , 123 ) might be able to capture multi - word struc - tures such as noun phrases .
likewise , trigram - based mixed - order models would be useful complements to 123 - gram and 123 - gram models , which are not uncom - mon in large - vocabulary language modeling .
issue that needs to be addressed is scalingthat is , how the performance of these mod - els depends on the vocabulary size and amount of training data .
generally , one expects that the sparser the data , the more helpful are models that can intervene between dierent order n - grams .
nev - ertheless , it would be interesting to see exactly how this relationship plays out for aggregate and mixed - order markov models .
we thank michael kearns and yoram singer for use - ful discussions , the anonymous reviewers for ques - tions and suggestions that helped improve the paper ,
and don hindle for help with his language modeling tools , which we used to build the baseline models considered in the paper .
rosenfeld .
a maximum entropy approach to adaptive statistical language modeling .
com - puter speech and language , 123 : 123
schutze .
dimensions of meaning .
in pro - ceedings of supercomputing , 123
minneapolis
singer .
adaptive mixtures of probabilistic in d .
touretzky , m .
mozer , and m .
hasselmo ( eds ) .
advances in neural information processing systems 123 : 123
mit press : cam -
