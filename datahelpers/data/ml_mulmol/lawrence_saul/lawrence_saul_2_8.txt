in this paper we compare three frameworks for discriminative train - ing of continuous - density hidden markov models ( cd - hmms ) .
spe - cically , we compare two popular frameworks , based on conditional maximum likelihood ( cml ) and minimum classication error ( mce ) , to a new framework based on margin maximization .
unlike cml and mce , our formulation of large margin training explicitly penal - izes incorrect decodings by an amount proportional to the number of mislabeled hidden states .
it also leads to a convex optimization over the parameter space of cd - hmms , thus avoiding the problem of spurious local minima .
we used discriminatively trained cd - hmms from all three frameworks to build phonetic recognizers on the timit speech corpus .
the different recognizers employed ex - actly the same acoustic front end and hidden state space , thus en - abling us to isolate the effect of different cost functions , parameter - izations , and numerical optimizations .
experimentally , we nd that our framework for large margin training yields signicantly lower error rates than both cml and mce training .
index terms speech recognition , discriminative training , mmi ,
mce , large margin , phoneme recognition
most modern speech recognizers are built from continuous - density hidden markov models ( cd - hmms ) .
the hidden states in these cd - hmms model different phonemes or sub - phonetic elements , while the observations model cepstral feature vectors .
distributions of cep - stral feature vectors are most often represented by gaussian mixture models ( gmms ) .
the accuracy of the recognizer depends critically on the careful estimation of gmm parameters .
in this paper , we present a systematic comparison of several leading frameworks for parameter estimation in cd - hmms .
these frameworks include a recently proposed scheme based on the goal of margin maximization ( 123 , 123 ) , an idea that has been widely applied in the eld of machine learning .
we compare the objective func - tion and learning algorithm in this framework for large margin train - ing to those of other traditional approaches for parameter estimation in cd - hmms .
the most basic of these traditional approaches in - volves maximum likelihood ( ml ) estimation .
mainly , however , we focus on competing discriminative methods in which parameters are
part of the work was performed at university of pennsylvania .
this work is supported by the national science foundation under grant number 123 and the ucsd fwgrid project , nsf research infrastruc - ture grant number nsf eia - 123
estimated directly to maximize the conditional likelihood ( 123 , 123 ) or minimize the classication error rate ( 123 ) .
though not as straightfor - ward to implement as ml estimation , discriminative methods yield much lower error rates on most tasks in automatic speech recogni -
we investigate salient differences between cml , mce , and large margin training through carefully designed experiments on the timit speech corpus ( 123 ) .
though much smaller than typical corpora used for large vocabulary asr , the timit corpus provides an apt bench - mark for evaluating the intrinsic merits of different frameworks for discriminative training .
we compare the phonetic error rates on the timit corpus from multiple systems trained with different param - eterizations , initial conditions , and learning algorithms .
all other aspects of these systems , however , were held xed .
in particular , the different systems employed exactly the same acoustic front end and model architectures ( e . g . , monophone cd - hmms with full gaussian covariance matrices ) .
from the results of these experiments , we are able to tease apart the signicant factors that differentiate competing methods for discriminative training .
the paper is organized as follows .
in section 123 , we review cd - hmms as well as several different methods for parameter estimation , including our own recent formulation of large margin training ( 123 , 123 ) .
in section 123 , we compare the performance of phonetic recognizers trained in all these different frameworks .
finally , in section 123 , we conclude with a brief discussion of future directions for research .
parameter estimation in hmms
cd - hmms dene a joint probability distribution over a hidden state sequence s = ( s123 , s123 , .
, st ) and an observed output sequence x = ( x123 , x123 , .
, xt ) , given by
log p ( x , s ) =
( log p ( st|st123 ) + log p ( xt|st ) ) .
for asr , the hidden states st and observed outputs xt denote pho - netic labels and acoustic feature vectors , respectively , and the distri - butions p ( xt|st ) are typically modeled by multivariate gmms :
p ( xt|st = j ) =
jmn ( xt; jm , jm ) .
( 123 ) , we have used n ( x; , ) to denote the gaussian dis - tribution with mean vector and covariance matrix , while the constant m denotes the number of mixture components per gmm .
the mixture weights jm in eq .
( 123 ) are constrained to be nonnega -
tive and normalized : p
m jm = 123 for all states j .
let denote all the model parameters including transition prob - abilities , mixture weights , mean vectors , and covariance matrices .
the goal of parameter estimation in cd - hmms is to compute the optimal ( with respect to a particular measure of optimality ) , given n pairs of observation and target label sequences ( xn , yn ) n
in what follows , we review the optimizations for well - known frameworks based on maximum likelihood ( ml ) , conditional max - imum likelihood ( cml ) , and minimum classication error ( mce ) .
we also review our most recently proposed framework for large mar - gin training ( 123 ) .
maximum likelihood estimation the simplest approach to parameter estimation in cd - hmms maxi - mizes the joint likelihood of output and label sequences .
the corre - sponding estimator is given by
ml = arg max
log p ( xn , yn )
for transition probabilities , ml estimates in this setting are obtained from simple counts ( assuming the training corpus provides phonetic label sequences ) .
for gmm parameters , the em algorithm provides iterative update rules that converge monotonically to local stationary points of the likelihood .
the main attraction of the em algorithm is that no free parameters need to be tuned for its convergence .
conditional maximum likelihood cd - hmms provide transcriptions of unlabeled speech by inferring the hidden label sequence y with the highest posterior probability : y = arg maxs p ( s|x ) .
the cml estimator in cd - hmms directly attempts to maximize the probability that this inference returns the correct transcription .
thus , it optimizes the conditional likelihood :
cml = arg max
in cml training , the parameters must be adjusted to increase the likelihood gap between correct labelings yn and incorrect labelings s .
this can be seen more explicitly by rewriting eq .
( 123 ) as :
between the correct labeling and all competing labelings .
unlike cml training , however , the size of the gap in eq .
( 123 ) does not matter , as long as it is nite .
the nondifferentiability of the sign and max functions in eq .
( 123 ) makes it difcult to minimize the misclassication error directly .
thus , mce training ( 123 ) adopts the surrogate cost function :
123b@ log p ( xn , yn ) + log
e log p ( xn , s )
in this approximation , a sigmoid function ( z ) = ( 123 + ez ) 123 re - places the sign function sign ( z ) , and a softmax function ( parameter - ized by ) replaces the original max .
the parameters and in this approximation must be set by heuristics .
the sum in the second term is taken over the top c competing label sequences .
large margin training recently , we proposed a new framework for discriminative train - ing of cd - hmms based on the idea of margin maximization ( 123 , 123 ) .
our framework has two salient features : ( i ) it attempts to separate the likelihoods of correct versus incorrect label sequences by mar - gins proportional to the number of mislabeled states ( 123 ) ; ( ii ) the required optimization is convex , thus avoiding the pitfall of spurious local minima .
these features also distinguish our approach to large margin training of cd - hmms from other recent formulations ( 123 ) .
we start by reviewing the discriminant functions in large margin cd - hmms ( 123 , 123 ) .
these parameterized functions of observations x and states s take a form analogous to the log - probability in eq .
in particular , we dene d ( x , s ) =
( ( st123 , st ) + ( xt , st ) )
in terms of state - state transition scores ( st123 , st ) and state - output emission scores ( xt , st ) .
unlike eq .
( 123 ) , however , eq .
( 123 ) does not assume that the transition scores ( st123 , st ) are derived from the logarithm of normalized probabilities .
likewise , the emission scores ( xt , st ) in eq .
( 123 ) are parameterized by sums of unnormal - ized gaussian distributions :
cml = arg max
log p ( xn , yn ) log
( xt , st = j ) = log
the cml estimator in eq .
( 123 ) is closely related to the maximum mutual information ( mmi ) estimator ( 123 , 123 ) , given by :
mmi = arg max
note that eqs .
( 123 ) and ( 123 ) yield identical estimators in the setting where the ( language model ) probabilities p ( yn ) are held xed .
minimum classication error mce training is based on minimizing the number of sequence mis - classications .
the number of such misclassications is given by :
sign ( log p ( xn , yn ) + max
log p ( xn , s ) )
where the nonnegative scalar parameter jm 123 is entirely inde - pendent of jm ( as opposed to being related to its log - determinant ) .
to obtain a convex optimization for large margin training , we further reparameterize the emission score in eq .
in particular , we express each mixture components parameters ( jm , jm , jm ) as elements of the following matrix :
jmjm + jm
our framework for large margin training optimizes the matrices jm , as opposed to the conventional gmm parameters ( jm , jm , jm ) .
since the matrix jm is positive denite and the scalar jm is non - negative , we also require the matrix jm to be positive semidenite ( as denoted by the constraint jm ( cid : 123 ) 123 ) .
with this reparameteriza - tion , the emission score in eq .
( 123 ) can be written as :
where sign ( z ) = 123 for z > 123 and sign ( z ) = 123 for z 123
to minimize eq .
( 123 ) , the parameters must be adjusted to maintain a likelihood gap
( xt , st = j ) = log
t jmzt where zt =
note that this score is convex in the elements of the matrices jm .
for large margin training of cd - hmms , we seek parameters that separate the discriminant functions for correct and incorrect la - bel sequences .
specically , for each joint observation - label sequence ( xn , yn ) in the training set , we seek parameters such that : d ( xn , yn ) d ( xn , s ) h ( yn , s ) , s 123= yn
where h ( yn , s ) denotes the hamming distance between the two label sequences ( 123 ) .
note how this constraint requires the log - likelihood gap between the target sequence yn and each incorrect decoding s to scale in proportion to the number of mislabeled states .
( 123 ) actually species an exponentially large number of con - straints , one for each alternative label sequence s .
we can fold all these constraints into a single constraint by writing :
d ( xn , yn ) + max
( h ( yn , s ) + d ( xn , s ) ) 123
in the same spirit as the mce derivation for eq .
( 123 ) , we obtain a more tractable ( i . e . , differentiable ) expression by replacing the max function in eq .
( 123 ) with a softmax upper bound :
d ( xn , yn ) + log
the exponential terms in eq .
( 123 ) can be summed efciently using a modication of the standard forward - backward procedure .
while we would like to nd parameters that satisfy the large margin constraint in eq .
( 123 ) for all training sequences ( xn , yn ) n in general this is not possible .
for such infeasible scenarios , we instead compute the parameters that minimize the total amount by which these constraints are violated :
123d ( xn , yn ) + log
the + subscript in eq .
( 123 ) denotes the hinge function : ( z ) + = z if z > 123 and ( z ) + = 123 if z 123
the optimization of eq .
( 123 ) is per - formed subject to the positive semidenite constraints jm ( cid : 123 ) 123
we can further simplify the optimization by assuming that each emission score ( xt , yt ) in the rst term is dominated by the contribution from a single ( pre - specied ) gaussian mixture component .
in this case , the overall optimization is convex; see ( 123 ) for further details .
empirical comparison of methods
we experimented with the discriminative frameworks in section 123 ( as well as several variants of these frameworks ) to explore the effects of different parameterizations , initializations , and cost functions .
cd - hmms were evaluated on the task of phonetic recognition ( 123 ) namely , mapping speech utterances to sequences of phonemes , as opposed to higher - level units , such as words .
phonetic label se - quences of test utterances were inferred using viterbi decoding .
note that for the viterbi decoding of test utterances , we did not make any use of manually time - aligned phonetic transcriptions; in particular , we did not assume that the boundaries between phonetic segments were correctly located prior to decoding .
this distinguishes the task of phonetic recognition , considered in this paper , from the simpler task of phonetic classication ( 123 ) considered in our earlier work ( 123 ) .
cml mce margin 123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123%
table 123
phonetic error rates from differently trained cd - hmms , with m mixture components per gmm .
see text for details .
for each test utterance , viterbi decoding yielded a frame - by - frame phonetic transcription with one label for each analysis window of speech .
we matched the label sequences from viterbi decoding against ground truth phonetic transcriptions ( obtained manually ) and used dynamic programming to compute the minimum string edit dis - tance for each utterance .
we report error rates as the number of in - sertion , deletion , and substitution errors over the entire corpus , nor - malized by the total string length .
all cd - hmms were trained and tested on utterances from the timit speech corpus ( 123 ) .
we followed standard practices in prepar - ing the training , development , and test sets ( 123 , 123 ) .
our recogniz - ers employed standard front ends and model architectures .
acous - tic feature vectors were derived from 123 - dimensional mel - frequency cepstral coefcients ( mfccs ) and their rst and second derivatives .
mfccs were computed on 123 ms analysis windows with 123 ms of overlap between consecutive windows .
each cd - hmm recognizer had 123 states , one for each of 123 broad phonetic categories and tran - scription markers ( e . g . , silence ) .
for evaluation , these 123 labels were further simplied to 123 phonetic categories , following the conven - tion in ( 123 ) .
to vary the model size , we experimented with differ - ent numbers of mixture components per state , ranging from 123 to 123 in each gmm .
for each mixture component , we estimated a full covari - ance matrix from the training corpus; there was no parameter - tying across different states or mixture components .
we used gradient - based numerical optimizations for cml , mce , and large margin training .
for cml training , we used conjugate gra - dient descent; for mce training , we used steepest gradient descent ( which worked better ) ; for margin maximization , we used a combi - nation of conjugate gradient and projected subgradient descent , as described in previous work ( 123 , 123 ) .
for cml training , we obtained competitive results from conjugate gradient descent and did not ex - periment with the extended baum - welch algorithm ( 123 ) .
experimental results table 123 shows the error rates of different cd - hmms trained by ml , cml , mce , and margin maximization .
here , m denotes the number of mixture components per state ( in each gmm ) .
as ex - pected , all the discriminatively trained cd - hmms yield signicant improvements over the baseline cd - hmms trained by ml .
on this particular task , the results show that mce does slightly better than cml , while the largest relative improvements are obtained by large margin training ( by a factor of two or more ) .
using mmi on this task , kapadia et al ( 123 ) reported larger relative reductions in error rates than we have observed for cml ( though not better performance in absolute terms ) .
it is difcult to compare our ndings directly to theirs , however , since their ml and mmi recognizers used different front ends and numerical optimizations than those in our work .
several possible factors might explain the better performance of cd - hmms trained by margin maximization .
these include : ( i ) the relaxation of gaussian normalization constraints by the parameteri - zation in eq .
( 123 ) , yielding more exible models , ( ii ) the convexity of our margin - based cost function eq .
( 123 ) , which ensures that its
unnormalized reinitialized reweighted
table 123
phonetic error rates from cd - hmms trained by cml and three variants of cml .
see text for details .
optimization ( unlike those for cml and mce ) does not suffer from spurious local minima , and ( iii ) the closer tracking of phonetic error rates by the margin - based cost function , which penalizes incorrect decodings in direct proportion to their hamming distance from the target label sequence .
we conducted several experiments with vari - ants of cml and mce training in an attempt to determine which of these factors ( if any ) played a signicant role .
some preliminary results are reported in table 123 , for cd - hmms trained by three variants of cml .
in the rst variant , we relaxed the normalization constraints on the mixture weights and log - determinant prefactors of the gmms .
in the second variant , we initialized the gmms from a different starting point; in particular , instead of base - line gmms trained by ml estimation , we used large margin gmms that had been trained for segment - based phonetic classication ( 123 ) .
finally , in the third variant , we maximized a reweighted version of the conditional likelihood :
log p ( xn , yn ) log
the reweighting in eq .
( 123 ) penalizes incorrect decodings in pro - portion to their hamming distance from the target label sequence , analogous to the cost function of eq .
( 123 ) for large margin training .
the experimental results on these variants of cml training re - veal several interesting ndings .
first , the unnormalized gmms per - formed slightly worse than the normalized gmms , possibly due to it seems that in the absence of margin - based criteria , the extra degrees of freedom in unnormalized gmms help to maxi - mize the conditional likelihood in ways that are not correlated with the phonetic error rate .
second , with better initializations , the cd - hmms trained by cml approached the performance of cd - hmms trained by margin maximization .
this result highlights a signicant drawback of optimizations , such as cml and mce , that are not con - vex and depend on initial conditions .
third , we observed that the reweighted conditional likelihood in eq .
( 123 ) led to improved per - formance for smaller models .
this positive effect diminished for larger models , however , perhaps due to the increased difculty of non - convex global optimization in larger parameter spaces .
finally , though not reported here , we also experimented with corresponding variants of mce training , obtaining similar results .
in this paper we have compared large margin training in cd - hmms to two other leading frameworks for discriminative training .
on the task of phonetic recognition , we observed that our formulation of large margin training achieved signicantly better performance than either cml or mce training .
follow - up experiments suggested two possible reasons for this better performance : the convexity of the op - timization for large margin training ( versus those for cml and mce training ) , and the penalizing of incorrect decodings in direct propor - tion to the number of mislabeled states .
in future research , we are interested in applying large margin training to large vocabulary asr ,
where both cml and mce training have already demonstrated sig - nicant reductions in word error rates ( 123 , 123 , 123 ) .
