this paper explores online learning approaches for detecting malicious web sites ( those involved in criminal scams ) using lexical and host - based features of the associated urls .
we show that this application is particularly appropriate for on - line algorithms as the size of the training data is larger than can be efciently processed in batch and because the distribution of features that typify malicious urls is changing contin - uously .
using a real - time system we developed for gathering url features , combined with a real - time source of labeled urls from a large web mail provider , we demonstrate that recently - developed online algorithms can be as accurate as batch techniques , achieving classication ac - curacies up to 123% over a balanced data set .
as new communications technologies drive new opportu - nities for commerce , they inevitably create new opportuni - ties for criminal actors as well .
the world wide web is no exception to this pattern , and today millions of rogue web sites advance a wide variety of scams including mar - keting counterfeit goods ( e . g . , pharmaceuticals or luxury watches ) , perpetrating nancial fraud ( e . g . , phishing ) and propagating malware ( e . g . , via drive - by exploits or social engineering ) .
what all of these activities have in common is the use of the uniform resource locator ( url ) as a vec - tor to bring internet users into their inuence .
thus , each time a user decides whether to click on an unfamiliar url they must implicitly evaluate the associated risk .
url safe to click on , or will it expose the user to poten - tial exploitation ? not surprisingly , this can be a difcult judgment for individual users to make .
as a result , security researchers have developed various
appearing in proceedings of the 123 th international conference on machine learning , montreal , canada , 123
copyright 123 by the author ( s ) / owner ( s ) .
systems to protect users from their uninformed choices .
by far the most common technique , deployed in browser toolbars , web ltering appliances and search engines , is blacklisting .
using this approach , a third - party service compiles the names of known bad web sites ( labeled by combinations of user feedback , web crawling and heuristic analysis of site content ) and distributes the list to its sub - scribers .
while such systems have minimal query overhead ( just searching for a url within the list ) they can only of - fer modest accuracy since it is impractical for any blacklist to be comprehensive and up - to - date ( sinha et al . , 123 ) .
thus , a user may click on a malicious url before it ap - pears on a blacklist ( if it ever does ) .
alternatively , some systems also intercept and analyze full web site content as it is downloaded .
this approach can offer higher accuracy , but requires far more run - time overhead , and may inadver - tently expose users to the very threats they seek to avoid .
in this paper , we focus on a complementary technique lightweight real - time classication of the url itself to pre - dict whether the associated site is malicious .
we use vari - ous lexical and host - based features of the url for classi - cation , but exclude web page content .
we are motivated by prior studies noting distinguishing commonalities in such features for malicious urls ( chou et al . , 123; mcgrath & gupta , 123 ) .
if properly automated , this technique can afford the low classication overhead of blacklisting while offering far greater accuracy ( and may also be used as a low - cost pre - lter for more expensive techniques ) .
to that end , our papers primary contribution is the success - ful application of online learning algorithms to the problem of predicting such malicious urls .
our earlier work ( ma et al . , 123 ) is among several previous systems for url classication that have relied on batch learning algorithms .
however , we argue that online methods are far better suited to the practical nature of this problem for two reasons : ( 123 ) online methods can process large numbers of examples far more efciently than batch methods .
( 123 ) we need to adapt to changes in malicious urls and their features over time .
to demonstrate this approach , we have built a url classi - cation system that uses a live feed of labeled urls from a large web mail provider , and that collects features for the
identifying suspicious urls : an application of large - scale online learning
table123feature breakdown on day 123 of the experiments .
last path token
123 , 123 whois info
urls in real time ( see figure 123 ) .
using this data , we show that online algorithms can be more accurate than batch al - gorithms in practice because the amount of data batch algo - rithms can train on is resource - limited .
we compare clas - sical and modern online learning algorithms and nd the condence - weighted algorithm achieves accuracies up to 123% over a balanced data set .
finally , we show that contin - uous retraining over newly - encountered features is critical for adapting the classier to detect new , malicious urls .
we begin the rest of the paper by providing more back - ground on the application of detecting malicious urls , then describing the online algorithms we use for classi - cation .
next , we describe our data collection methodology and evaluate the models over our data set of labeled urls .
finally , we conclude with an overall discussion .
our goal is to detect malicious web sites from the lexical and host - based features of their urls .
this section pro - vides background for our application with respect to the features we use for url classication , as well as placing our work in context with related work .
like our previous study ( ma et al . , 123 ) , we analyze lex - ical and host - based features because they contain informa - tion about the url and host that is straightforward to col - lect using automated crawling tools .
thus , the list of fea - tures is extensive , but not necessarily exhaustive .
we do not include the content of the web page or the con - text of the url ( e . g . , the page or email containing the url ) as features for several reasons .
( 123 ) avoiding page content downloads is strictly safer .
( 123 ) classifying a url with a trained model is a lightweight operation compared to rst downloading the page contents and then analyzing them .
( 123 ) we want to apply our methods on urls regard - less of the context in which they appear ( pages , email , chat , calendars , games , etc . ) , so we are not tied to a particular application setting .
and ( 123 ) reliably obtaining the content of a page can become an issue due to content cloaking , whereby a malicious site may serve benign versions of a page to a honeypot ip address run by a security practitioner , but serve malicious versions to other users .
nevertheless , the evaluations in section 123 show that classication with just lexical and host - based features of urls , without any context , can still be highly accurate .
table 123 lists the lexical and host - based feature types we consider and the number contributed by each type .
overall , lexical types account for 123% of features and host - based types account for 123% .
we next describe the feature types and the motivation behind including them for classication .
lexical features : these features allow us to capture the property that malicious urls tend to look dif - ferent from benign urls .
for example , ance of the token . com in the url www . ebay . com is not unusual .
however , the appearance of . com in www . ebay . com . phishy . biz or phish . biz / www . ebay . com / index . php could indicate an attempt by criminals to spoof the domain name of a legitimate commercial web site .
similarly , we would like to capture the fact that there are certain red ag keywords that tend to appear in malicious urls e . g . , ebayisapi would appear frequently in the path of urls attempting to spoof an ebay page .
to implement these features , we use a bag - of - words repre - sentation of tokens in the url , where / , ? , . , = , - , and are delimiters .
we distinguish tokens that appear in the hostname , path , the top - level domain ( tld ) , primary domain name ( the domain name given to a registrar ) , and last token of the path ( to capture le extensions ) .
thus , com in the tld position of a url would be a different token from com in other parts of the url .
we also use the lengths of the hostname and the url as features .
host - based features : these features describe properties of the web site host as identied by the hostname portion of the url .
they allow us to approximate where mali - cious sites are hosted , who own them , and how they are managed .
we examine the following sets of properties to construct host - based features :
whois information this includes domain name regis - tration dates , registrars , and registrants .
so if a set of ma - licious domains are registered by the same individual , we would like to treat such ownership as a malicious feature .
location this refers to the hosts geography , ip address prex , and autonomous system ( as ) number .
so if mali - cious urls tend to be hosted in a specic ip prex of an internet service provider ( isp ) , then we want to account for that disreputable isp when classifying urls .
connection speed if some malicious sites tend to reside on compromised residential machines ( connected via cable or dsl ) , then we want to record the host connection speed .
membership in blacklists over our experiments , 123% of malicious urls were present in blacklists .
thus , although this feature is useful , it is still not comprehensive .
identifying suspicious urls : an application of large - scale online learning
new ip prefix
hosts 123 spam sites
from same direct
new registrant token a direct marketing company that registered 123 spam sites
new domain " cq . bz " : a prolific phishing site
figure123cumulative number of features observed over time for our live url feeds .
we highlight a few examples of new features at the time they were introduced by new malicious urls .
other dns - related properties these include time - to - live ( ttl ) , spam - related domain name heuristics ( rudd , 123 ) , and whether the dns records share the same isp .
figure 123 shows the cumulative number of features for each day of the evaluations .
each days total includes new fea - tures introduced that day and all old features from previous days ( see section 123 on our methodology for new features ) .
the dimensionality grows quickly because we assign a bi - nary feature for every token we encounter among the url lexical tokens , as well as whois and location properties .
as we will show in section 123 , accounting for new fea - tures like the ones in figure 123 is benecial for detecting new malicious urls .
related work
the most direct comparison to our work comes from gar - era et al .
( 123 ) , who classify phishing urls using logistic regression over 123 hand - selected features .
the features in - clude red - ag keywords , google page rank and web qual - ity guidelines scores .
their classier achieves 123% ac - curacy over a set of 123 , 123 urls .
although similar in mo - tivation and methodology , our study differs in scope ( with evaluations focused on detecting spamming and phishing sites ) , scale ( orders of magnitude more features and urls ) , and algorithmic approach ( online vs .
batch learning ) .
provos et al .
( 123 ) study drive - by exploit urls , and use a patented machine learning algorithm along with features derived from web page content .
fette et al .
( 123 ) and bergholz et al .
( 123 ) examine select properties of urls contained within an email to aid the machine learning clas - sication of phishing emails ( not the urls themselves ) .
several projects have also explored operating systems - level techniques whereby the client visits the web site using an instrumented virtual machine ( vm ) ( moshchuk et al . , 123; wang et al . , 123; provos et al . , 123 ) .
the vm can emulate any client - side exploits that occur as a result of visiting a malicious site , and the instrumentation can de -
tect whether an infection has occurred .
in this way , the vm serves as a protective buffer for the user .
finally , many commercial efforts exist to protect users from visiting malicious urls such as mcafees sitead - visor , ironport web reputation , websense threatseeker network , wot web of trust , and google toolbar .
these approaches are based on blacklist construction , user feed - back , and proprietary feature analysis .
online algorithms
this section briey describes the online learning algo - rithms we use for our evaluations .
formally , the algorithms are trying to solve an online classication problem over a sequence of pairs ( ( x123 , y123 ) , ( x123 , y123 ) , . . . , ( xt , yt ) ) , where each xt is an examples feature vector and yt ( 123 , +123 ) is its label .
at each time step t during training , the al - gorithm makes a label prediction ht ( xt ) , which for linear classiers is ht ( x ) = sign ( wt x ) .
after making a prediction , the algorithm receives the actual label yt .
( if ht ( xt ) 123= yt , we record an error for time t . ) then , the algorithm constructs the hypothesis for the next time step ht+123 using ht , xt and yt .
as practitioners , we have no vested interest in any partic - ular strategy for online learning .
we simply want to de - termine the approach that scales well to problems of our size and yields the best performance .
to that end , the on - line methods we evaluate are a mix of classical and recent algorithms .
we present the models in order of increasing sophistication with respect to the objective functions and the treatment of classication margin ( which we can also interpret as classication condence ) .
perceptron : this classical algorithm is a linear classier that makes the following update to the weight vector when - ever it makes a mistake ( rosenblatt , 123 ) :
wt+123 wt + ytxt
the advantage of the perceptron is its simple update rule .
however , because the update rate is xed , the perceptron cannot account for the severity of the misclassication .
as a result , the algorithm can overcompensate for mistakes in some cases and undercompensate for mistakes in others .
logistic regression with stochastic gradient descent : many batch algorithms use gradient descent to optimize an objective function that is expressed as a sum of the exam - ples individual objective functions .
stochastic gradient de - scent ( sgd ) provides an online means for approximating the gradient of the original objective , whereby the model parameters are updated incrementally by the gradients of in this paper we evaluate sgd as applied to logistic regression .
identifying suspicious urls : an application of large - scale online learning
let p ( yt = +123|xt ) = ( w xt ) be the likelihood that ex - ample ts label is +123 , where the sigmoid function is ( z ) = ( 123 + ez ) 123
moreover , let lt ( w ) = log ( yt ( w xt ) ) be the log - likelihood for example t .
then the update for each example in logistic regression with sgd is as follows :
wt+123 wt +
= wt + txt
where t = yt+123 123 ( wt xt ) and is a constant training rate .
we do not decrease over time so that the parameters can continually adapt to new urls .
the update resembles a perceptron , except with a learning rate that is proportional to t , the difference between the actual and predicted like - lihood that the label is +123
this multiplier allows the model to be updated ( perhaps by a small factor ) even when there is no prediction mistake .
sgd has received renewed attention because of recent re - sults on the convergence of sgd algorithms and the cast - ing of classic algorithms as sgd approximations ( bottou , 123; bottou & lecun , 123 ) .
for example , the percep - tron can be viewed as an sgd minimization of the hinge - loss function loss ( w ) = pt max ( 123 , yt ( w xt ) ) .
passive - aggressive ( pa ) algorithm : the goal of the passive - aggressive algorithm is to change the model as little as possible to correct for any mistakes and low - condence predictions it encounters ( crammer et al . , 123 ) .
specically , with each example pa solves the fol -
123 kwt wk123 yi ( w xt ) 123
updates occur when the inner product does not exceed a xed condence margin i . e . , yt ( wt xt ) < 123
the closed - form update for all examples is as follows :
wt+123 wt + tytxt
where t = max ( 123yt ( wtxt ) ( the details of the derivation are in crammer et al . , 123 ) the pa algorithm has been successful in practice because the updates explic - itly incorporate the notion of classication condence .
condence - weighted ( cw ) algorithm : the idea behind condence - weighted classication is to maintain a differ - ent condence measure for each feature so that less con - dent weights are updated more aggressively than more con - dent weights .
the stdev update rule for cw is similar in spirit to pa .
however , instead of describing each feature with a single coefcient , cw describes per - feature con - dence by modeling uncertainty in weight wi with a gaus - sian distribution n ( i , i ) ( dredze et al . , 123; crammer et al . , 123 ) .
let us denote as the vector of feature means ,
and as the diagonal covariance matrix ( i . e . , the con - dence ) of the features .
then the decision rule becomes ht ( x ) = sign ( t x ) which is the result of comput - ing the average signed margin wt x , where wt is drawn from n ( t , t ) , and then taking the sign .
the cw update rule adjusts the model as little as possi - ble so that xt can be correctly classied with probability .
specically , cw minimizes the kl divergence between gaussians subject to a condence constraint at time t :
( t+123 , t+123 ) argmin
dkl ( n ( , ) kn ( t , t ) )
yi ( xt ) 123 ( ) px
where is the cumulative distribution function of the stan - dard normal distribution .
this optimization yields the fol - lowing closed - form update :
t+123 t + tyttxt
t + tu
where t , ut and are dened in crammer et al .
( 123 ) .
however , we can see that if the variance of a feature is large , the update to the feature mean will be more aggres - sive .
as for performance , the run time of the update is linear in the number of non - zero features in x .
overall , because the cw algorithm makes a ne - grain dis - tinction between each features weight condence , cw can be especially well - suited to detecting malicious urls since our data feed continually introduces a dynamic mix of new and recurring features .
related algorithms : we experimented with nonlinear classication using online kernel - based algorithms such as the forgetron ( dekel et al . , 123 ) and the projec - tron ( orabona et al . , 123 ) .
to make computation tractable , these algorithms budget ( or at least try to reduce ) the size of the support set used for kernel calculations .
our pre - liminary evaluations revealed no improvement over linear classiers .
however , we are continuing to evaluate budget algorithms for long term use .
data collection
this section describes our live sources of labeled urls and the system we deploy to collect features in real time .
figure 123 illustrates our data collection architecture , which starts with two feeds of malicious and benign urls .
we obtain examples of malicious urls from a large web mail provider , whose live , real - time feed supplies 123 , 123 - 123 , 123 examples of spam and phishing urls per day .
the malicious urls are extracted from email messages that users manually label as spam , run through pre - lters to ex - tract easily - detected false positives , and then veried man - ually as malicious .
identifying suspicious urls : an application of large - scale online learning
by training regimen , we refer to ( 123 ) when the classier is allowed to retrain itself after attempting to predict the label of an incoming url , and ( 123 ) how many features the classier uses during training .
for ( 123 ) , we compare continuous vs .
interval - based training .
under the continuous training regimen , the classier may retrain its model after each incoming ex - ample ( the typical operating mode of online algorithms ) .
in the interval - based training regimen , the classier may only retrain after a specied time interval has passed .
in our experiments , we set the interval to be one day .
batch algorithms are restricted to interval - based training , since continuous retraining would be computationally impracti - cal .
unless otherwise specied , we use continuous retrain - ing for all experiments with online algorithms ( and then evaluate the benet of doing so in section 123 ) .
for ( 123 ) , we compare training using a variable vs .
xed number of features .
under the xed - feature regimen , we train using a pre - determined set of features for all evalu - ation days .
for example , if we x the features to those encountered up to day 123 , then we use those 123 , 123 fea - tures for the whole experiment ( see figure 123 ) .
under the variable - feature regimen , we allow the dimensionality of our models to grow with the number of new features en - countered; on day 123 , for instance , we classify with up to 123 , 123 features .
implicitly , examples that were intro - duced before a feature i was rst encountered will have value 123 for feature i .
unless otherwise specied , we use the variable - feature training regimen for all algorithms ( and then evaluate the benet of doing so in section 123 ) .
as for the sizes of the training sets , online algorithms im - plicitly train on a cumulative data set , since they can incre - mentally update models from the previous day .
for batch algorithms , we vary the training set size to include day - long and multi - day sets ( details in section 123 ) .
advantages of online learning
we start by evaluating the benet of using online over batch algorithms for our application in terms of classication accuracy in particular , whether the benet of efcient computation in online learning comes at the expense of ac - curacy .
specically , we compare the online condence - weighted ( cw ) algorithm against four different training set congurations of a support vector machine .
we use the liblinear implementation of an svm with a linear - kernel as our canonical batch algorithm ( fan et al . , 123 ) .
evaluations with other batch algorithms such as logistic re - gression yielded similar results .
figure 123 shows the classication rates for cw and for svm using four types of training sets .
we tuned all classier parameters over one day of holdout data , setting c = 123
figure123overview of real - time url feed , feature collection , and
we randomly draw our examples of benign urls from yahoos directory listing .
a random sample from this directory can be generated by visiting the link
combined , we collect a total of 123 , 123 urls per day from the two url feeds , and the average ratio of benign - to - malicious urls is 123 - to - 123
we ran our experiments for 123 days , collecting nearly 123 million urls ( there were feed outages during days 123 ) .
however , the feeds only pro - vide urls , not the accompanying features .
thus , we deploy a system to gather features in real - time .
the real - time aspect is important because we want the val - ues to reect the features a url had when it was rst in - troduced to the feed ( which ideally reects values for when it was introduced to the wild ) .
for every incoming url , our feature collector immediately queries dns , whois , blacklist and geographic information servers , as well as processing ip address - related and lexical - related features .
our live feed is notably different from data sets such as the webspam set from the pascal large scale learning challenge ( sonnenburg et al . , 123 ) .
our application uses urls as a starting point , and it is our responsibility to fetch lexical and host - based features in real - time to construct the data set on an ongoing basis .
by contrast , the webspam set is a static representation of web pages ( using strings ) , not urls , and provides no notion of the passage of time .
finally , our live feed provides a real - time snapshot of ma - licious urls that reect the evolving strategies of internet criminals .
the freshness of this data suggests that good classication results over the set will be a strong indicator of future success in real - world deployments .
in this section , we evaluate the effectiveness of online learning over the live url feed .
to demonstrate this ef - fectiveness , we address the following questions : do on - line algorithms provide any benet over batch algorithms ? which online algorithms are most appropriate for our ap - plication ? and is there a particular training regimen that fully realizes the potential of these online classiers ?
identifying suspicious urls : an application of large - scale online learning
fortunately , online algorithms do not have that limitation .
moreover , they have the added benet that they can incre - mentally adapt to new data .
as we see in figure 123 , the accuracy for cw beats svm - multi .
since the online algo - rithm is making a single pass over a cumulative training set , it does not incur the overhead of loading the entire data set in memory .
because its training is incremental , it is capa - ble of adapting to new examples in real time , whereas batch algorithms are restricted to retraining at the next available interval ( more on interval - based training in section 123 ) .
these advantages allow the online classier to have the best accuracy in our experiments .
comparison of online algorithms
given the demonstrated benets of online learning over batch learning , we next evaluate which of the online al - gorithms from section 123 are best suited to malicious url detection .
the main issue that these experiments address is whether recent developments in online algorithms , which include optimizing different objective functions , adjusting for classication condence , and treating features differ - ently , can benet the classiers in our application .
figure 123 ( a ) shows the cumulative error rates for the online algorithms .
all algorithms in this experiment adopt the continuous training regimen .
we also note that the error rates improve steadily over time for all classiers , reafrm - ing that training on cumulative data is benecial .
the perceptron is the simplest of the algorithms , but it also has the highest error rates across all of the days at around 123 123% .
this result suggests that because the perceptron treats mistakes equally ( and ignores all correct classications ) , its updates are too coarse to accurately keep up with new ex - amples .
there needs to be a more ne - grain distinction be - tween misclassied and correctly - classied examples with respect to their impact on model updates .
both logistic regression with stochastic gradient de - scent ( lrsgd ) and the passive - aggressive ( pa ) algorithm achieve a cumulative error approaching 123% , improving over the perceptron results .
( here we tuned the lrsgd learning rate to = 123 over one day of holdout data . ) presumably , this improvement occurs because lrsgd and pa account for classication condence .
lrsgd updates are proportional to t , and pa updates are proportional to the normalized classication margin t .
these results are comparable to svm - multi .
the cw results suggest that the nal leap comes from treating features differently both in terms of how they af - fect classication condence , and how quickly they should be updated .
with an error approaching 123% , cw clearly outperforms the other algorithms .
most of the gap between cw and the other online methods comes from cws lower
figure123cumulative error rates for cw and for batch algorithms under different training sets .
note the y - axis starts at 123% .
for svm , and = 123 for cw .
the x - axis shows the number of days in the experiment , and the y - axis shows the cumulative error rate : the percentage of misclassied examples for all urls encountered up to that date .
the svm - once curve represents training once on day 123s data and using that model for testing on all other days .
the cumulative error steadily worsens to 123% , and the per - day false negative rate gets as high as 123% .
these high er - ror rates suggests that , to achieve better accuracy , the model must train on fresh data to account for new features of ma - licious and benign urls encountered over time .
svm - daily retrains only on data collected the previous day e . g . , day 123 results reect training on the urls collected on day 123 , and testing on day 123 urls .
the only exception is that we do not retrain during the feed outages on days 123
as a result , the cumulative error is just under 123% , most of which is due to high per - day false negatives ( 123 123% on some days ) , whereas per - day false positives are around 123% .
although fresh data eventually helps svm - daily improve over svm - once , one days training data is
we use multi - day training sets to address this issue by train - ing on as much data as our evaluation machine with 123 gb ram can handle ( which is 123 days worth , or 123 , 123 123 , 123 examples ) .
svm - multi - once is the multi - day ana - logue to svm - once .
here , svm - multi - once trains on data from days 123 to 123 , and from day 123 on it uses that xed model for testing on subsequent days .
the improvement over svm - once shows the benet of more training data , but the steadily worsening error again demonstrates the nonsta - tionarity of the url data set .
svm - multi is the multi - day analogue of svm - daily .
here , svm - multi trains on the previous 123 days worth of data ( depending on what can t in memory ) .
the resulting cu - mulative error reaches 123% .
svm - multis improvement over svm - multi - once suggests the url feature distribu - tion evolves over time , thus requiring us to use as much fresh data as possible to succeed .
overall , these results sug - gest that more training data yields better accuracy .
how - ever , this accuracy is fundamentally limited by the amount of computing resources available .
identifying suspicious urls : an application of large - scale online learning
percep .
( int . ) x
cw ( int . ) x
percep .
( fixed ) x
cw ( fixed ) x
( a ) error rates for online algorithms .
all use continuous / variable - feature training .
( b ) benets of using continuous training over interval - based training .
( c ) benets of using variable - feature sets over xed - feature sets .
figure123comparing the effectiveness of various online algorithms , their use of continuous vs .
interval training , and their use of xed vs .
variable feature sets .
false negatives cw has 123% false negatives per day , whereas others have 123% .
we hypothesize the gap oc - curs because cw can update select portions of its model very aggressively to account for new malicious features , all without perturbing more established features .
overall , we nd that the more recent online algorithms out - perform the simpler ones .
because the live combined url feed contains a dynamic mix of new and recurring fea - tures , cws per - feature condence weighting can exploit that structure to achieve the best accuracy .
training regimen
in this section , we show that there is a signicant advan - tage to continuous training vs .
interval - based training .
we also demonstrate that there is signicant benet to adding newly - encountered features as opposed to using a xed fea - ture set .
the aforementioned training regimens can help online algorithms stay abreast of changing trends in url features .
thus , choosing the right training regimen can be just as important as choosing the right algorithm .
figure 123 ( b ) shows the value of using continuous training over interval training with the cw and perceptron algo - rithms .
the higher error rates for interval training show that there is enough variation between days that a model can become stale if it is not retrained soon enough .
in particu - lar , the higher number of false negatives for interval - trained cw is responsible for the persistent gap with continuously - trained cw .
notwithstanding the aforementioned feed out - ages on days 123 , the 123% error difference between con - tinuous and interval - based perceptron is due to spikes in the false positive / negative rates for the interval - trained per - ceptron .
thus , continuous retraining yields as much im - provement for the simpler perceptron as it does for cw .
in addition to continuous retraining , accounting for new features is critical to an algorithms success .
figure 123 ( c ) shows the value of using variable - feature training over xed - feature training .
in this graph , xed features means that we restrict the model to using the features encountered on day 123 only ( 123 , 123 features total ) .
we see that the
performance for xed - feature cw degrades to a point that it is no better than a perceptron .
feature perceptron only achieves a marginal improvement over xed - feature perceptron .
one explanation is that , even though variable - feature perceptron can occasionally bene - t from adding new features , it does not update the new feature weights aggressively enough to correct for future errors .
by contrast , the cw algorithm updates new fea - tures aggressively by design , and hence can reap the full benets of variable - feature training .
overall , continuous retraining with a variable feature set allows a model to successfully adapt to new data and new features on a sub - day granularity .
and this adaptiveness is critical to the full benets of online algorithms .
malicious web sites are a prominent and undesirable in - ternet scourge .
to protect end users from visiting those sites , the identication of suspicious urls using lexical and host - based features is an important part of a suite of de - fenses .
however , url classication is a challenging task because new features are introduced daily as such , the distribution of features that characterize malicious urls evolves continually .
with an eye toward ultimately con - structing a real - time malicious url detection system , we evaluated batch and online learning algorithms for our ap - plication to study their benets and tradeoffs .
experiments over a live url feed revealed the limitations of batch algorithms in this setting , where we were forced to make a tradeoff between accuracy and coping with resource limitations ( e . g . , running out of memory ) .
we demon - strated that recently - developed online algorithms such as cw can be highly accurate classiers , capable of achiev - ing classication accuracies up to 123% .
furthermore , we showed that retraining algorithms continuously with new features is crucial for adapting successfully to the ever - evolving stream of urls and their features .
identifying suspicious urls : an application of large - scale online learning
we thank koby crammer , mark dredze , fernando pereira and boris babenko for many useful discussions .
we also thank the anonymous reviewers for their valuable feed - back .
this work was supported by national science foundation grants nsf - 123 , nsf - 123 and nsf - 123 and by generous research , operational and in - kind support from cisco , google , microsoft , yahoo and the ucsd center for networked systems .
