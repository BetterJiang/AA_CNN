many unsupervised algorithms for nonlinear di - mensionality reduction , such as locally linear embedding ( lle ) and laplacian eigenmaps , are derived from the spectral decompositions of sparse matrices .
while these algorithms aim to preserve certain proximity relations on average , their embeddings are not explicitly designed to preserve local features such as distances or an - gles .
in this paper , we show how to construct a low dimensional embedding that maximally pre - serves angles between nearby data points .
the embedding is derived from the bottom eigenvec - tors of lle and / or laplacian eigenmaps by solv - ing an additional ( but small ) problem in semidef - inite programming , whose size is independent of the number of data points .
the solution obtained by semidenite programming also yields an esti - mate of the datas intrinsic dimensionality .
ex - perimental results on several data sets demon - strate the merits of our approach .
the problem of discovering low dimensional structure in high dimensional data arises in many areas of information processing ( burges , 123 ) .
much recent work has focused on the setting in which such data is assumed to have been sampled from a low dimensional submanifold .
many al - gorithms , based on variety of geometric intuitions , have been proposed to compute low dimensional embeddings in this setting ( roweis & saul , 123; tenenbaum et al . , 123; belkin & niyogi , 123; donoho & grimes , 123; wein - berger & saul , 123 ) .
in contrast to linear methods such as principal component analysis ( pca ) , these manifold learning algorithms are capable of discovering highly non - linear structure .
nevertheless , their main optimizations are
appearing in proceedings of the 123 nd international conference on machine learning , bonn , germany , 123
copyright by the
quite tractableinvolving ( for example ) nearest neighbor searches , least squares ts , dynamic programming , eigen - value problems , and semidenite programming .
one large family of algorithms for manifold learning con - sists of approaches based on the spectral decomposition of sparse matrices ( chung , 123 ) .
algorithms in this family include locally linear embedding ( lle ) ( roweis & saul , 123 ) and laplacian eigenmaps ( belkin & niyogi , 123 ) .
the matrices in these algorithms are derived from sparse weighted graphs whose nodes represent high dimensional inputs and whose edges indicate neighborhood relations .
low dimensional embeddings are computed from the bot - tom eigenvectors of these matrices .
this general approach to manifold learning is attractive for computational reasons because it reduces the main problem to solving a sparse eigensystem .
in addition , the resulting embeddings tend to preserve proximity relations without imposing the poten - tially rigid constraints of isometric ( distance - preserving ) embeddings ( tenenbaum et al . , 123; weinberger & saul , 123 ) .
on the other hand , this general approach also has several shortcomings : ( i ) the solutions do not yield an es - timate of the underlying manifolds dimensionality; ( ii ) the geometric properties preserved by these embedding are dif - cult to characterize; ( iii ) the resulting embeddings some - times exhibit an unpredictable dependence on data sam - pling rates and boundary conditions .
in the rst part of this paper , we review lle and lapla - cian eigenmaps and provide an extended analysis of these shortcomings .
as part of this analysis , we derive a theoret - ical result relating the distribution of smallest eigenvalues in these algorithms to a data sets intrinsic dimensionality .
in the second part of the paper , we propose a framework to remedy the key deciencies of lle and laplacian eigen - in particular , we show how to construct a more robust , angle - preserving embedding from the spectral de - compositions of these algorithms ( one of which must be run as a rst step ) .
the key aspects of our framework are the following : ( i ) a d - dimensional embedding is com - puted from the m bottom eigenvectors of lle or lapla - cian eigenmaps with m > d , thus incorporating informa -
analysis and extension of spectral methods for nonlinear dimensionality reduction
tion that the original algorithm would have discarded for a similar result; ( ii ) the new embeddings explicitly optimize the degree of neighborhood similaritythat is , equivalence up to rotation , translation , and scalingwith the aim of discovering conformal ( angle - preserving ) mappings; ( iii ) the required optimization is performed by solving an addi - tional ( but small ) semidenite program ( vandenberghe & boyd , 123 ) , whose size is independent of the number of data points; ( iv ) the solution of the semidenite program yields an estimate of the underlying manifolds dimension - ality .
finally , we present experimental results on several data sets , including comparisons with other algorithms .
analysis of existing methods
the problem of manifold learning is simply stated .
assume that high dimensional inputs have been sampled from a low dimensional submanifold .
denoting the inputs by ( ( cid : 123 ) x i ) n where ( cid : 123 ) xirp , the goal is to compute outputs ( ( cid : 123 ) y i ) n provide a faithful embedding in d ( cid : 123 ) p dimensions .
lle and laplacian eigenmaps adopt the same general framework for solving this problem .
in their simplest forms , both algorithms consist of three steps : ( i ) construct a graph whose nodes represents inputs and whose edges in - dicate k - nearest neighbors; ( ii ) assign weights to the edges in the graph and use them to construct a sparse positive semidenite matrix; ( iii ) output a low dimensional embed - ding from the bottom eigenvectors of this matrix .
the main practical difference between the algorithms lies in the sec - ond step of choosing weights and constructing a cost func - tion .
we briey review each algorithm below , then provide an analysis of their particular shortcomings .
locally linear embedding
lle appeals to the intuition that each high dimensional in - put and its k - nearest neighbors can be viewed as samples from a small linear patch on a low dimensional subman - ifold .
weights wij are computed by reconstructing each input ( cid : 123 ) xi from its k - nearest neighbors .
specically , they are chosen to minimize the reconstruction error :
e ( w ) =
the minimization is performed subject to two constraints : ( i ) wij =123 if ( cid : 123 ) xj is not among the k - nearest neighbors of ( cid : 123 ) x i; j wij = 123 for all i .
the weights thus constitute a sparse matrix w that encodes local geometric properties of the data set by specifying the relation of each input ( cid : 123 ) x i to its k - nearest neighbors .
lle constructs a low dimensional embedding by comput - ing outputs ( cid : 123 ) yi rd that respect these same relations to their k - nearest neighbors .
specically , the outputs are cho -
sen to minimize the cost function :
( y ) =
the minimization is performed subject to two constraints that prevent degenerate solutions : ( i ) the outputs are cen - i ( cid : 123 ) yi = 123 , and ( ii ) the outputs have unit covari - ance matrix .
the d - dimensional embedding that mini - mizes eq .
( 123 ) subject to these constraints is obtained by computing the bottom d + 123 eigenvectors of the matrix = ( iw ) t ( iw ) .
the bottom ( constant ) eigenvec - tor is discarded , and the remaining d eigenvectors ( each of size n ) yield the embedding ( cid : 123 ) yird for i ( 123 , 123 ,
laplacian eigenmaps
laplacian eigenmaps also appeal to a simple geometric in - tuition : namely , that nearby high dimensional inputs should be mapped to nearby low dimensional outputs .
to this end , a positive weight wij is associated with inputs ( cid : 123 ) xi and ( cid : 123 ) xj if either input is among the others k - nearest neighbors .
typically , the values of the weights are either chosen to be constant , say wij = 123 / k , or exponentially decaying , as wij = exp ( ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) xj ( cid : 123 ) 123 / 123 ) where 123 is a scale pa - rameter .
let d denote the diagonal matrix with elements j wij .
the outputs ( cid : 123 ) yi can be chosen to minimize the cost function :
( y ) =
wij ( cid : 123 ) ( cid : 123 ) yi ( cid : 123 ) yj ( cid : 123 ) 123
as in lle , the minimization is performed subject to con - straints that the outputs are centered and have unit covari - ance .
the embedding is computed from the bottom eigen - vectors of the matrix = id 123 123 .
the matrix is a symmetrized , normalized form of the graph laplacian , given by dw .
again , the optimization is a sparse eigen - value problem that scales well to large data sets .
123 wd 123
shortcomings for manifold learning
both lle and laplacian eigenmaps can be viewed as spec - tral decompositions of weighted graphs ( belkin & niyogi , 123; chung , 123 ) .
the complete set of eigenvectors of the matrix ( in lle ) and ( in laplacian eigenmaps ) yields an orthonormal basis for functions dened on the graph whose nodes represent data points .
the eigenvec - tors of lle are ordered by the degree to which they reect the local linear reconstructions of nearby inputs; those of laplacian eigenmaps are ordered by the degree of smooth - ness , as measured by the discrete graph laplacian .
the bottom eigenvectors from these algorithms often produce reasonable embeddings .
the orderings of these eigenvec - tors , however , do not map precisely onto notions of local
analysis and extension of spectral methods for nonlinear dimensionality reduction
figure123top .
data set of n = 123 inputs randomly sampled from a swiss roll .
bottom .
two dimensional embedding and ten smallest nonzero eigenvalues computed by lle .
distance or angle preservation , nor do their smallest eigen - values have a telltale gap that yields an estimate of the un - derlying manifolds dimensionality .
evidence and implica - tions of these shortcomings are addressed in the next sec -
empirical results fig .
123 shows the results of lle applied to n = 123 inputs sampled from a swiss roll ( using k = 123 nearest neigh - bors ) .
the ten smallest nonzero eigenvalues of the ma - trix are also plotted on a log scale .
while lle does unravel this data set , the aspect ratio and general shape of its solution do not reect the underlying manifold .
there is also no apparent structure in its bottom eigenvalues ( such as a prominent gap ) to suggest that the inputs were sampled from a two dimensional surface .
such results are fairly typ - ical : while the algorithm often yields embeddings that pre - serve proximity relations on average , it is difcult to char - acterize their geometric properties more precisely .
this behavior is also manifested by a somewhat unpredictable dependence on the data sampling rate and boundary condi - tions .
finally , in practice , the number of nearly zero eigen - values has not been observed to provide a robust estimate of the underlying manifolds dimensionality ( saul & roweis ,
theoretical analysis
how do the smallest eigenvalues of lle and laplacian eigenmaps reect the underlying manifolds dimensional - ity , if at all ? in this section , we analyze a simple setting in which the distribution of smallest eigenvalues can be precisely characterized .
our analysis does in fact reveal a mathematical relationship between these methods eigen - spectra and the intrinsic dimensionality of the data set
suspect , however , that this relationship is not likely to be of much practical use for estimating dimensionality .
consider inputs ( cid : 123 ) xi rd that lie on the sites of an innite d - dimensional hypercubic lattice .
each input has 123d neigh - bors separated by precisely one lattice spacing .
the two dimensional case is illustrated in the left panel of fig .
choosing k = 123d nearest neighbors to construct a sparse graph and assigning constant weights to the edges , we ob - tain an ( innite ) weight matrix w for laplacian eigenmaps
if ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) xj ( cid : 123 ) = 123
a simple calculation shows that for this example , lapla - cian eigenmaps are based on the spectral decomposition of the matrix = iw .
note that lle would use the same weight matrix w for these inputs; it would thus perform a spectral decomposition on the matrix = 123
in what follows , we analyze the distribution of smallest eigenval - ues of ; the corresponding result for lle follows from a simple change of variables .
the matrix is diagonalized by a fourier basis : namely , for each ( cid : 123 ) q ( , ) d , form ( ei ( cid : 123 ) q ( cid : 123 ) xi ) with eigenvalue : ( ( cid : 123 ) q ) = 123 123
it has an eigenvector of
thus , has a continuous eigenspectrum; in particular , it has no gaps .
we can compute the distribution of its eigen - values from the integral :
where = ( , ) d and ( ) denotes the dirac delta function .
for d = 123 , the integral gives the simple result : ( 123 ) .
for d > 123 , the integral cannot be evaluated analytically , but we can compute its asymp - totic behavior as 123 , which characterizes the distribution of smallest eigenvalues .
( this is the regime of interest for understanding lle and laplacian eigenmaps . ) the asymp - totic behavior may be derived by approximating eq .
( 123 ) by its second - order taylor expansion ( ( cid : 123 ) q ) ( cid : 123 ) ( cid : 123 ) q ( cid : 123 ) 123 / ( 123d ) , since ( cid : 123 ) 123 implies ( cid : 123 ) ( cid : 123 ) q ( cid : 123 ) ( cid : 123 ) 123
with this substitution , eq .
( 123 ) reduces to an integral over a hypersphere of radius
123d , yielding the asymptotic result :
d / 123 as 123 ,
where ( ) is the gamma function .
our result thus relates the dimensionality of the input lattice to the power law that characterizes the distribution of smallest eigenvalues .
analysis and extension of spectral methods for nonlinear dimensionality reduction
figure123left : inputs from an innite square lattice , for which one can calculate the distribution of smallest eigenvalues from lle and laplacian eigenmaps .
right : inputs from a regularly sampled submanifold which give rise to the same results .
the power laws in eq .
( 123 ) were calculated from the weight matrix in eq .
( 123 ) ; thus they are valid for any inputs that give rise to matrices ( or equivalently , graphs ) of this form .
the right panel of fig .
123 shows how inputs that were reg - ularly sampled from a two dimensional submanifold could give rise to the same graph as inputs from a square lattice .
could these power laws be used to estimate the intrinsic dimensionality of a data set ? while in principle such an approach seems possible , in practice it seems unlikely to succeed .
fitting a power law to the distribution of small - est eigenvalues would require computing many more than the d smallest ones .
the t would also be confounded by the effects of nite sample sizes , boundary conditions , and random ( irregular ) sampling .
a more robust way to esti - mate dimensionality from the results of lle and laplacian eigenmaps is therefore needed .
conformal eigenmaps
in this section , we describe a framework to remedy the previously mentioned shortcomings of lle and laplacian eigenmaps .
recall that we can view the eigenvectors from these algorithms as an orthonormal basis for functions de - ned on the n points of the data set .
the embeddings from these algorithms are derived simply from the d bottom ( non - constant ) eigenvectors; as such , they are not explicitly designed to preserve local features such as distances or an - gles .
our approach is motivated by the following question : from the m bottom eigenvectors of these algorithms , where m > d but m ( cid : 123 ) n , can we construct a more faithful embed - ding of dimensionality d ?
to proceed , we must describe more precisely what we mean by faithful .
a conformal mapping f between two manifoldsm and m ( cid : 123 ) is a smooth , one - to - one mapping that looks locally like a rotation , translation , and scaling , thus preserving local angles ( though not local distances ) .
this property is shown schematically in fig .
let c 123 and c123 denote two curves intersecting at a point xm , and let c 123 denote another curve that intersects c 123 and c123 in the neigh -
figure123schematic illustration of conformal mapping f between two manifolds , which preserves the angles ( but not areas ) of in - nitessimally small triangles .
borhood of x .
let be the innitessimally small triangle dened by these curves in the limit that c 123 approaches x , and let ( cid : 123 ) be its image under the mapping f .
for a confor - mal mapping , these triangles will be similar ( having equal angles ) though not necessarily congruent ( with equal sides and areas ) .
all isometric ( distance - preserving ) mappings are conformal mappings , but not vice versa .
we can now more precisely state the motivation behind our approach .
assume that the inputs ( cid : 123 ) xi rp were sampled from a submanifold that can be conformally mapped to d - dimensional euclidean space .
can we ( approximately ) construct the images of ( cid : 123 ) xi under such a mapping from the bottom m eigenvectors of lle or laplacian eigenmaps , where m > d but m ( cid : 123 ) n ? we will answer this question in three steps : rst , by dening a measure of local dissim - ilarity between discrete point sets; second , by minimizing this measure over an m - dimensional function space derived from the spectral decompositions of lle and laplacian eigenmaps; third , by casting the required optimization as a problem in semidenite programming and showing that its solution also provides an estimate of the datas intrinsic
cost function let ( cid : 123 ) zi = f ( ( cid : 123 ) xi ) dene a mapping between two discrete point sets for i ( 123 , 123 , .
suppose that the points ( cid : 123 ) xi were densely sampled from a low dimensional submanifold : to what extent does this mapping look locally like a rotation , translation , and scaling ? consider any triangle ( ( cid : 123 ) x i , ( cid : 123 ) xj , ( cid : 123 ) xj ( cid : 123 ) ) where j and j ( cid : 123 ) are among the k - nearest neighbors of ( cid : 123 ) x i , as well as the image of this triangle ( ( cid : 123 ) zi , ( cid : 123 ) zj , ( cid : 123 ) zj ( cid : 123 ) ) .
these triangles are similar if and only if :
the constant of proportionality in this equation indicates the change in scale of the similarity transformation ( i . e . , the ratio of the areas of these triangles ) .
let ij = 123 if ( cid : 123 ) xj is one of the k - nearest neighbors of ( cid : 123 ) x i or if i = j .
also , let si denote the hypothesized change of scale induced by
analysis and extension of spectral methods for nonlinear dimensionality reduction
bottom m eigenvectors
lle or laplacian
m ( cid : 123 ) n
( cid : 123 ) yi = 123
yiyi = n
high dimensional inputs
i ( 123 , 123 ,
low dimensional embedding
( cid : 123 ) zi = l ( cid : 123 ) yi rm
sparse singular values of l may suggest dimensionality d < m
figure123steps of the algorithm for computing maximally angle - preserving embeddings from the bottom eigenvectors of lle or lapla - cian eigenmaps , as described in section 123
the mapping at ( cid : 123 ) xi .
with this notation , we can measure the degree to which the mapping f takes triangles at ( cid : 123 ) x i into similar triangles at ( cid : 123 ) zi by the cost function :
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) zj ( cid : 123 ) zj ( cid : 123 ) ( cid : 123 ) 123 si ( cid : 123 ) ( cid : 123 ) xj ( cid : 123 ) xj ( cid : 123 ) ( cid : 123 ) 123
a global measure of local dissimilarity is obtained by sum - ming over all points :
i=123 and ( ( cid : 123 ) zi ) n
note that given xed point sets ( ( cid : 123 ) xi ) n i=123 , it is straightforward to compute the scale parameters that mini - mize this cost function , since each si in eq .
( 123 ) can be op - timized by a least squares t .
is it possible , however , that given only ( ( cid : 123 ) xi ) n i=123 , one could also minimize this cost func - tion with respect to ( ( cid : 123 ) zi ) n i=123 , yielding a nontrivial solution where the latter lives in a space of much lower dimension - ality than the former ? such a solution ( assuming it had low cost ) would be suggestive of a conformal map for nonlinear
eigenvector expansion
we obtain a well - posed optimization for the above prob - lem by constraining the points ( cid : 123 ) z i to be spanned by the bot - tom m eigenvectors returned by lle or laplacian eigen - in particular , denoting the outputs of these algo - rithms by ( cid : 123 ) yirm , we look for solutions of the form :
( cid : 123 ) zi = l ( cid : 123 ) yi ,
where l rmm is a general linear transformation and i ( 123 , 123 , .
thus our goal is to compute the scale pa - rameters si and the linear transformation l that minimize the dissimilarity cost function in eqs .
( 123 ) with the sub - stitution ( cid : 123 ) zi = l ( cid : 123 ) yi .
we also impose the further constraint
trace ( lt l ) = 123
in order to rule out the trivial solution l = 123 , which zeroes the cost function by placing all ( cid : 123 ) z i at the origin .
before showing how to optimize eqs .
( 123 ) subject to the constraints in eq .
( 123 ) , we make two observations .
first , by equating l to a multiple of the identity matrix , we recover the original m - dimensional embedding ( up to a global change of scale ) obtained from lle or laplacian eigenmaps .
in general , however , we shall see that this set - ting of l does not lead to maximally angle - preserving em - beddings .
second , if we are allowed to express ( cid : 123 ) z i in terms of the complete basis of eigenvectors ( taking m = n ) , then we can reconstruct the original inputs ( cid : 123 ) x i up to a global ro - tation and change of scale , which does not yield any form of dimensionality reduction .
by choosing m ( cid : 123 ) min ( n , p ) where ( cid : 123 ) xi rp , however , we can force a solution that achieves some form of dimensionality reduction .
semidenite programming
minimizing the cost function in eqs .
( 123 ) in terms of the matrix l and the scale parameters si can be cast as a prob - lem in semidenite programming ( sdp ) ( vandenberghe & boyd , 123 ) .
details are omitted due to lack of space , but the derivation is easy to understand at a high level .
the optimal scaling parameters can be computed in closed form as a function of the matrix l and eliminated from eqs .
the resulting form of the cost function only depends on the matrix l through the positive semidenite matrix p = lt l .
let p = vec ( p ) denote the vector ob - tained by concatenating the columns of p .
then , using schur complements , the optimization can be written as :
p ( cid : 123 ) 123 , trace ( p ) = 123 ,
where eq .
( 123 ) indicates that the matrix p is constrained to be positive semidenite and eq .
( 123 ) enforces the earlier
analysis and extension of spectral methods for nonlinear dimensionality reduction
constraint from eq .
( 123 ) , i and s are m 123m123 matrices; i denotes the identity matrix , while s depends on ( ( cid : 123 ) xi , ( cid : 123 ) yi ) n i=123 but is independent of the optimization vari - ables p and t .
the optimization is an sdp over the m 123 unknown elements of p ( or equivalently p ) .
thus its size is independent of the number of inputs , n , as well as their extrinsic dimensionality , p .
for small problems with ( say ) m = 123 , these sdps can be solved in under a few minutes on a typical desktop machine .
after solving the sdp , the linear map l is recovered via l=p123 / 123 , and the maximally angle - preserving embedding is computed from eq .
the square root operation is well dened since p is positive semidenite .
the solution computed from ( cid : 123 ) zi =l ( cid : 123 ) yi denes an m - dimensional embed - ding .
note , however , that if the matrix l has one or more small singular values , then the variance of this embedding will be concentrated in fewer than m dimensions .
thus , by examining the singular values of l ( or equivalently , the eigenvalues of p = lt l ) , we can obtain an estimate of the datas intrinsic dimensionality , d .
we can also output a d - dimensional embedding by simply projecting the points ( cid : 123 ) zi rm onto the rst d principal components of their co - variance matrix .
the overall algorithm , which we call con - formal eigenmaps , is summarized in fig
experimental results
we experimented with the algorithm in fig .
123 to com - pute maximally angle - preserving embeddings of several data sets .
we used the algorithm to visualize the low di - mensional structure of each data set , as well as to esti - mate their intrinsic dimensionalities .
we also compared the eigenvalue spectra from this algorithm to those from pca , isomap ( tenenbaum et al . , 123 ) , and semidenite embedding ( sde ) ( weinberger & saul , 123 ) .
swiss roll
the top panel of fig .
123 shows the angle - preserving embed - ding computed by conformal eigenmaps on the swiss roll data set from section 123 .
the embedding was constructed from the bottom m=123 eigenvectors of lle .
compared to the result in fig .
123 , the angle - preserving embedding more faithfully preserves the shape of the underlying manifolds boundary .
the eigenvalues of the matrix p = l t l , nor - malized by their sum , are shown in the middle panel of fig .
123 , along with similarly normalized eigenvalues from pca , isomap , and sde .
the relative magnitudes of indi - vidual eigenvalues are indicated by the widths of differently colored regions in each bar plot .
the two leading eigenvalues in these graphs reveal the ex - tent to which each algorithms embedding is conned to two dimensions .
all the manifold learning algorithms ( but
eigenvalues ( normalized by trace )
top four rows of transformation matrix l
figure123top : two dimensional embedding of swiss roll inputs in fig .
123 by conformal eigenmaps ( ce ) .
middle : comparison of eigenspectra from pca , isomap , sde , and ce .
the relative mag - nitudes of individual eigenvalues are indicated by the widths of differently colored regions in each bar plot .
bottom : top four rows of the transformation matrix l in eq
not pca ) correctly identify the underlying dimensionality of the swiss roll as d = 123
finally , the bottom panel in fig .
123 shows the top four rows of the transformation ma - trix l from eq .
only the rst two rows have matrix elements of appreciable magnitude , reecting the underly - ing two dimensional structure of this data set .
note , how - ever , that sizable matrix elements appear in all the columns of l .
this shows that the maximally angle - preserving em - bedding exploits structure in the bottom m = 123 eigenvec - tors of lle , not just in the bottom d=123 eigenvectors .
images of edges fig .
123 shows examples from a synthetic data set of n=123 grayscale images of edges .
each image has 123 resolu - tion .
the edges were generated by sampling pairs of points at regularly spaced intervals along the image boundary .
the images lie on a two dimensional submanifold , but this sub - manifold has a periodic structure that cannot be unraveled in the same way as the swiss roll from the previous section .
we synthesized this data set with the aim of understanding how various manifold learning algorithms might eventually perform when applied to patches of natural images .
the top panel of fig .
123 shows the rst two dimensions of the maximally angle - preserving embedding of this data set .
the embedding was constructed from the bottom m = 123 eigenvectors of laplacian eigenmaps using k = 123 near - est neighbors .
the embedding has four distinct quadrants in which edges with similar orientations are mapped to nearby points in the plane .
the middle panel in fig .
123 com - pares the eigenspectrum from the angle - preserving embed -
analysis and extension of spectral methods for nonlinear dimensionality reduction
figure123examples from a synthetic data set of n = 123 gray - scale images of edges .
each image has 123 resolution .
ding to those from pca , isomap , and sde .
isomap does not detect the low dimensional structure of this data set , possibly foiled by the cyclic structure of the submanifold .
the distance - preserving embedding of sde has variance in more dimensions than the angle - preserving embedding , suggesting that the latter has exploited the extra exibility of conformal versus isometric maps .
the bottom panel in fig .
123 displays the matrix l from eq .
the result shows that the semidenite program in eq .
( 123 ) mixes all of the bottom m = 123 eigenvectors from laplacian eigenmaps to obtain the maximally angle - preserving embedding .
eigenvalues ( normalized by trace )
top four rows of transformation matrix l
three dimensional embedding of n = 123 face images by conformal eigenmaps ( ce ) .
middle : comparison of eigenspectra from pca , isomap , sde and ce .
bottom : top four rows of the transformation matrix l in eq
eigenvalues ( normalized by trace )
top four rows of transformation matrix l
figure123top : two dimensional embedding of n = 123 edge images by conformal eigenmaps ( ce ) .
middle : comparison of eigenspectra from pca , isomap , sde , and ce .
bottom : top four rows of the transformation matrix l in eq
images of faces fig .
123 shows results on a data set of n=123 images of faces .
the faces were initially processed by lle using k =123 nearest neighbors .
the maximally angle - preserving embedding was computed from the bottom m=123 eigen - vectors of lle .
though the intrinsic dimensionality of this data set is not known a priori , several methods have re - ported similar ndings ( brand , 123; weinberger & saul , 123 ) .
as shown in the middle panel , the embedding from conformal eigenmaps concentrates most of its variance in three dimensions , yielding a somewhat lower estimate of the dimensionality than isomap or sde .
the top panel of fig .
123 visualizes the three dimensional embedding from conformal eigenmaps; the faces are arranged in an intuitive manner according to their expression and pose .
analysis and extension of spectral methods for nonlinear dimensionality reduction
in this work , we have appealed to conformal transforma - tions as a basis for nonlinear dimensionality reduction .
our approach casts a new light on older algorithms , such as lle and laplacian eigenmaps .
while the bottom eigen - vectors from these algorithms have been used to derive low dimensional embeddings , these solutions do not generally preserve local features such as distances or angles .
view - ing these bottom eigenvectors as a partial basis for func - tions on the data set , we have shown how to compute a maximally angle - preserving embedding by solving an ad - ditional ( but small ) problem in semidenite programming .
at little extra computational cost , this framework signi - cantly extends the utility of lle and laplacian eigenmaps , yielding more faithful embeddings as well as a global esti - mate of the datas intrinsic dimensionality .
previous studies in dimensionality reduction have shared similar motivations as this work .
an extension of isomap was proposed to learn conformal transformations ( de silva & tenenbaum , 123 ) ; like isomap , however , it relies on the estimation of geodesic distances , which can lead to spuri - ous results when the underlying manifold is not isomorphic to a convex region of euclidean space ( donoho & grimes , 123 ) .
hessian lle is a variant of lle that learns isome - tries , or distance - preserving embeddings , with theoretical guarantees of asymptotic convergence ( donoho & grimes , 123 ) .
like lle , however , it does not yield an estimate of the underlying manifolds dimensionality .
finally , there has been work on angle - preserving linear projections ( ma - gen , 123 ) .
this work , however , focuses on random linear projections ( johnson & lindenstrauss , 123 ) that are not suitable for manifold learning .
our approach in this paper builds on lle and laplacian eigenmaps , thus inheriting their strengths as well as their weaknesses .
obviously , when these algorithms return spu - rious results ( due to , say , outliers or noise ) , the angle - preserving embeddings computed from their spectral de - compositions are also likely to be of poor quality .
in gen - eral , though , we have found that our approach comple - ments and extends these earlier approaches to nonlinear di - mensionality reduction at very modest extra cost .
this work was supported by the national science founda - tion under award number 123
