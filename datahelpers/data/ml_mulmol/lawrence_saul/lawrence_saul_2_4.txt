in this paper , we propose a model for representing and predicting distances in large - scale networks by matrix factorization .
the model is useful for network distance sensitive applications , such as content distribution networks , topology - aware overlays , and server selections .
our approach overcomes several limitations of previous coordinates - based mechanisms , which cannot model sub - optimal routing or asymmetric routing policies .
we describe two algorithms - - singular value decomposition ( svd ) and nonnegative matrix factorization ( nmf ) - - for representing a matrix of network distances as the product of two smaller matrices .
with such a representation , we build a scalable system - - internet distance estimation service ( ides ) - - that predicts large numbers of network distances from limited numbers of measurements .
extensive simulations on real - world data sets show that ides leads to more accurate , efficient and robust predictions of latencies in large - scale networks than previous approaches .
postprint version .
copyright acm , 123
this is the author ' s version of the work .
it is posted here by permission of acm for your personal use .
not for redistribution .
the definitive version was published in proceedings of the 123th acm sigcomm conference on internet measurement ( imc 123 ) , pages 123 - 123
publisher url : http : / / doi . acm . org / 123 / 123
this conference paper is available at scholarlycommons : http : / / repository . upenn . edu / cis_papers / 123
modeling distances in large - scale networks
by matrix factorization
yun mao and lawrence k
department of computer and information science
university of pennsylvania
in this paper , we propose a model for representing and predicting distances in large - scale networks by matrix factorization .
the model is useful for net - work distance sensitive applications , such as con - tent distribution networks , topology - aware overlays , and server selections .
our approach overcomes sev - eral limitations of previous coordinates - based mech - anisms , which cannot model sub - optimal routing or asymmetric routing policies .
we describe two algo - rithms | singular value decomposition ( svd ) and nonnegative matrix factorization ( nmf ) |for repre - senting a matrix of network distances as the product of two smaller matrices .
with such a representation , we build a scalable system|internet distance esti - mation service ( ides ) |that predicts large numbers of network distances from limited numbers of mea - surements .
extensive simulations on real - world data sets show that ides leads to more accurate , e ( cid : 123 ) cient and robust predictions of latencies in large - scale net - works than previous approaches .
wide - area distributed applications have evolved con - siderably beyond the traditional client - server model , in which a client only communicates with a single server .
in content distribution networks ( cdn ) , peer - to - peer distributed hash tables ( dht ) ( 123 , 123 , 123 , 123 ) , and overlay routing ( 123 ) , nodes often have the ( cid : 123 ) exibil - ity to choose their communication peers .
this ( cid : 123 ) exi - bility can greatly improve performance if relevant net - work distances are known .
for example , in a cdn , an optimized client can download web objects from the particular mirror site to which it has the highest bandwidth .
likewise , in dht construction , a peer can route lookup requests to the peer ( among those
that are closer to the target in the virtual overlay network ) with the lowest latency in the ip underlay
unfortunately , knowledge of network distances is not available without cost .
on - demand network mea - surements are expensive and time - consuming , espe - cially when the number of possible communication peers is large .
thus , a highly promising approach is to construct a model that can predict unknown net - work distances from a set of partially observed mea - surements ( 123 , 123 , 123 , 123 , 123 , 123 ) .
many previously proposed models are based on the embedding of host positions in a low dimensional space , with network distances estimated by euclidean distances .
such models , however , share certain lim - in particular , they cannot represent net - works with complex routing policies , such as sub - optimal routing123 or asymmetric routing , since eu - clidean distances satisfy the triangle inequality and are inherently symmetric .
on the internet , routing schemes of this nature are quite common ( 123 , 123 , 123 ) , and models that do not take them into account yield inaccurate predictions of network distances .
in this paper , we propose a model based on ma - trix factorization for representing and predicting dis - tances in large - scale networks .
the essential idea is to approximate a large matrix whose elements repre - sent pairwise distances by the product of two smaller matrices .
such a model can be viewed as a form of dimensionality reduction .
models based on ma - trix factorization do not su ( cid : 123 ) er from the limitations of previous work : in particular , they can represent distances that violate the triangle inequality , as well as asymmetric distances .
two algorithms|singular
123with sub - optimal routing policies , the network distance between two end hosts does not necessarily represent the short - est path in the network .
such routing policies exist widely in the internet for various technical , political and economic rea -
value decomposition ( svd ) and nonnegative matrix factorization ( nmf ) ( are presented for learning mod - els of this form .
we evaluate the advantages and dis - advantages of each algorithm for learning compact models of network distances .
the rest of the paper is organized as follows .
sec - tion 123 reviews previous work based on the low di - mensional embedding of host positions in euclidean space .
section 123 presents the model for matrix factor - ization of network distances .
the svd and nmf al - gorithms for learning these models from network mea - surements are presented and evaluated in section 123
section 123 proposes an architecture to estimate dis - tances required by an arbitrary host from low dimen - sional reconstructions .
the architecture is evaluated in section 123
finally , section 123 summarizes the paper .
123 network embeddings
one way to predict network distance between arbi - trary internet end hosts is to assign each host a \po - sition " in a ( cid : 123 ) nite - dimensional vector space .
this can be done at the cost of a limited number of network measurements to a set of well - positioned infrastruc - ture nodes123 , or other peer nodes .
in such a model , a pair of hosts can estimate the network distance between them by applying a distance function to their positions , without direct network measurement .
most previous work on these models has represented the host positions by coordinates in euclidean space and adopted euclidean distance as the distance func -
we de ( cid : 123 ) ne the problem formally as follows .
sup - pose there are n hosts h = fh123;h123; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ;hng in the network .
the pairwise network distance matrix is a n ( cid : 123 ) n matrix d , such that dij ( cid : 123 ) 123 is the network distance from hi to hj .
a network embedding is a mapping h : h ! rd
dij ( cid : 123 ) ^dij = kh ( hi ) ( cid : 123 ) h ( hj ) k;123i; j = 123; : : : ; n ( 123 ) where ^dij is the estimated network distance from hi to hj and h ( hi ) is the position coordinate of hi as a d - dimensional real vector .
we simplify the coordinate notation from h ( hi ) to ~ hi = ( hi123; hi123; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; hid ) .
the network distance between two hosts hi and hj is estimated by the euclidean distance of their coor -
123referred as landmark nodes in this paper .
they are also
called beacon nodes .
^dij = k ~ hi ( cid : 123 ) ~ hjk = d
( hik ( cid : 123 ) hjk ) 123 !
the main problem in constructing a network em - bedding is to compute the position vectors ~ hi for all hosts hi from a partially observed distance matrix d .
a number of learning algorithms have been proposed to solve this problem , which we describe in the next
123 previous work
the ( cid : 123 ) rst work in the network embedding area was done by ng and zhang ( 123 ) , whose global network positioning ( gnp ) system embedded network hosts in a low - dimensional euclidean space .
many algo - rithms were subsequently proposed to calculate the coordinates of network hosts .
gnp uses a simplex downhill method to minimize the sum of relative er -
total err =xi xj
jdij ( cid : 123 ) ^dijj
the drawback of gnp is that the simplex downhill method converges slowly , and the ( cid : 123 ) nal results depend on the initial values of the search .
pic ( 123 ) applies the same algorithm to the sum of squared relative errors and studies security - related issues .
cox , dabek et .
proposed the vivaldi algo - rithm ( 123 , 123 ) based on an analogy to a network of in this approach , the problem of minimizing the sum of errors is related to the prob - lem of minimizing the potential energy of a spring system .
vivaldi has two main advantages : it is a dis - tributed algorithm , and it does not require landmark
lim et .
( 123 ) and tang et .
( 123 ) independently proposed models based on lipschitz embeddings and principal component analysis ( pca ) .
these models begin by embedding the hosts in an n - dimensional space , where the coordinates of the host hi are given by its distances ( di123; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; din ) to n landmark nodes .
this so - called lipschitz embedding has the property that hosts with similar distances to other hosts are located nearby in the n - dimensional space .
to re - duce the dimensionality , the host positions in this n - dimensional space are then projected into the d - dimensional subspace of maximum variance by pca .
a linear normalization is used to further calibrate the results , yielding the ( cid : 123 ) nal host positions ~ hi 123 rd .
one possible 123 ( cid : 123 ) - ( cid : 123 ) d embedding ( cid : 123 )
figure 123 : four hosts h123 ( cid : 123 ) h123 in a simple network
euclidean distances are inherently symmetric; they also satisfy the triangle inequality .
thus , in any net -
^dij = ^dji
^dij + ^djk ( cid : 123 ) ^dik 123i; j; k
these two properties are inconsistent with observed network distances .
on the internet , studies indi - cate that as many as 123% of node pairs of real - world data sets have a shorter path through an alternate node ( 123 , 123 ) .
another study shows that asymmetric routing is quite common ( 123 ) ; even for the same link , the upstream and downstream capacities may be very
in addition to these limitations , low - dimensional embeddings of host positions cannot always model distances in networks where there are pairs of nodes that do not have a direct path between them , even if the distances are symmetric and satisfy triangle in - equality .
figure 123 illustrates a simple network topol - ogy in which four hosts in di ( cid : 123 ) erent autonomous sys - tems are connected with unit distance to their neigh - bors .
an intuitive two - dimensional embedding is also shown .
in the given embedding , the estimated dis - tances are ^d123 = ^d123 = p123 , but the real distances are d123 = d123 = 123
it is provable that there exists no euclidean space embedding ( of any dimensional - ity ) that can exactly reconstruct the distances in this network .
similar cases arise in networks with tree -
123 distance matrix factoriza -
the limitations of previous models lead us to consider a di ( cid : 123 ) erent framework for compactly representing net - work distances .
suppose that two nearby hosts have similar distances to all the other hosts in the network .
in this case , their corresponding rows in the distance matrix will be nearly identical .
more generally , there may be many rows in the distance matrix that are equal or nearly equal to linear combinations of other rows .
recall from linear algebra that an n ( cid : 123 ) n ma - trix whose rows are not linearly independent has rank strictly less than n and can be expressed as the prod - uct of two smaller matrices .
with this in mind , we seek an approximate factorization of the distance ma - trix , given by :
d ( cid : 123 ) xy t ;
where x and y are n ( cid : 123 ) d matrices with d ( cid : 123 ) n .
from such a model , we can estimate the network distance from hi to hj by ^dij = ~ xi ( cid : 123 ) ~ yj , where ~ xi is the ith row vector of the matrix x and ~ yj is the jth row vector of the matrix y .
more formally , for a network with distance matrix dij , we de ( cid : 123 ) ne a distance matrix factorization as two
x : h ! rd; : h ! rd;
and an approximate distance function computed by
^dij = x ( hi ) ( cid : 123 ) y ( hj ) :
as shorthand , we denote x ( hi ) as ~ xi and y ( hi ) as ~ yi , so that we can write the above distance compu -
^dij = ~ xi ( cid : 123 ) ~ yj =
note that in contrast to the model in section 123 , which maps each host to one position vector , our model associates two vectors with each host .
we call ~ xi the outgoing vector and ~ yi the incoming vector for hi .
the estimated distance from hi to hj is simply the dot product between the outgoing vector of hi and the incoming vector of hj .
applying this model of network distances in dis - tributed applications is straightforward .
for exam - ple , consider the problem of mirror selection
locate the closest server among several mirror can - didates , a client can retrieve the outgoing vectors of the mirrors from a directory server , calculate the dot product of these outgoing vectors with its own in - coming vector , and choose the mirror that yields the smallest estimate of network distance ( i . e . , the small - est dot product ) .
our model for representing network distances by matrix factorization overcomes certain limitations of models based on low dimensional embeddings .
particular , it does not require that network distances are symmetric because in general ^dij = ~ xi ( cid : 123 ) ~ yj 123= ~ xj ( cid : 123 ) ~ yi = ^dji .
distances computed in this way also are not constrained to satisfy the triangle inequal - ity .
the main assumption of our model is that many rows in the distance matrix are linearly dependent , or nearly so .
this is likely to occur whenever there are clusters of nearby nodes in the network which have similar distances to distant nodes .
in this case , the distance matrix d will be well approximated by the product of two smaller matrices .
123 distance reconstruction
in this section we investigate how to estimate outgo - ing and incoming vectors ~ xi and ~ yi for each host hi from the distance matrix d .
we also examine the ac - curacy of models that approximate the true distance matrix by the product of two smaller matrices in this
the distance matrix d can be viewed123 as storing n row - vectors in n - dimensional space .
factoring this matrix d ( cid : 123 ) xy t is essentially a problem in linear dimensionality reduction , where y stores d ba - sis vectors and x stores the linear coe ( cid : 123 ) cients that best reconstruct each row vector of d .
we present two algorithms for matrix factorization that solve this problem in linear dimensionality reduction .
123 singular value decomposition an n ( cid : 123 ) n distance matrix d can be factored into three matrices by its singular value decomposition ( svd ) , of the form :
d = u sv t ;
123note that d does not have to be a square matrix of pair - wise distances .
it can be the distance matrix from one set of n hosts h to another set of n 123 hosts h123 , which may or may not overlap with each other .
in this case , x 123 rn ( cid : 123 ) d contains the outgoing vectors for h and y 123 rd ( cid : 123 ) n 123 contains the incoming vectors for h123
for simplicity , though , we consider the case n = n123 in what follows .
where u and v are n ( cid : 123 ) n orthogonal matrices and s is an n ( cid : 123 ) n diagonal matrix with nonnegative ele - ments ( arranged in decreasing order ) .
let a = u s ii = psii .
it is easy to see and b = s that abt = u s 123 v t = d .
thus 123 ) t = u s svd yields an exact factorization d = ab t , where the matrices a and b are the same size as d .
123 v , where s
123 ( v s
we can also use svd , however , to obtain an ap - proximate factorization of the distance matrix into two smaller matrices .
in particular , suppose that only a few of the diagonal elements of the matrix s are appreciable in magnitude .
de ( cid : 123 ) ne the n ( cid : 123 ) d
xij = uijpsjj ; yij = vijpsjj ;
where i = 123 : : : n and j = 123 : : : d .
the product xy t is a low - rank approximation to the distance matrix d; if the distance matrix is itself of rank d or less , as in - dicated by sjj = 123 for j > d , then the approximation will in fact be exact .
the low - rank approximation obtained from svd can be viewed as minimizing the squared error function
( dij ( cid : 123 ) ~ xi ( cid : 123 ) ~ yj ) 123
with respect to xi 123 rd and yj 123 rd .
and ( 123 ) compute the global minimum of this error
matrix factorization by svd is related to princi - pal component analysis ( pca ) ( 123 ) on the row vectors .
principal components of the row vectors are obtained from the orthogonal eigenvectors of their correlation matrix; each row vector can be expressed as a linear combination of these eigenvectors .
the diagonal val - ues of s measure the signi ( cid : 123 ) cance of the contribution from each principal component .
in previous work on embedding of host positions by pca , such as ics ( 123 ) and virtual landmark ( 123 ) , the ( cid : 123 ) rst d rows of the ma - trix u were used as coordinates for the hosts , while discarding the information in the matrices s and v .
by contrast , our approach uses u , s and v to com - pute outgoing and incoming vectors for each host .
we use the topology in figure 123 as an example to show how the algorithm works .
the distance matrix
we obtain the svd result as
note that s123 = 123
therefore , an exact d = 123 fac - torization exists with :
; y =123
one can verify in this case that the reconstructed distance matrix xy t is equal to the original distance
123 non - negative matrix factorization
non - negative matrix factorization ( nmf ) ( 123 ) is an - other form of linear dimensionality reduction that can be applied to the distance matrix dij .
the goal of nmf is to minimize the same error function as in eq .
( 123 ) , but subject to the constraint that x and y are non - negative matrices .
in contrast to svd , nmf guarantees that the approximately reconstructed dis - tances are nonnegative : ^dij ( cid : 123 ) 123
the error func - tion for nmf can be minimized by an iterative algo - rithm .
compared to gradient descent and the sim - plex downhill method , however , the algorithm for nmf converges much faster and does not involve any heuristics , such as choosing a step size .
the only constraint on the algorithm is that the true network distances must themselves be nonnegative , dij ( cid : 123 ) 123; this is generally true and holds for all the examples we consider .
the algorithm takes as input initial ( ran - dom ) matrices x and y and updates them in an al - ternating fashion .
the update rules for each iteration
( xy t y ) ia ( x t d ) aj
( x t xy t ) aj
our experience shows that two hundred it - erations su ( cid : 123 ) ce to converge to a local minimum .
one major advantage of nmf over svd is that it is straightforward to modify nmf to handle missing entries in the distance matrix d .
for various reasons , a small number of elements in d may be unavailable .
svd can proceed with missing values if we eliminate the rows and columns in d that contain them , but doing so will leave the corresponding host positions
nmf can cope with missing values if we slightly change the update rules .
suppose m is a binary matrix where mij = 123 indicates dij is known and mij = 123 indicates dij is missing .
the modi ( cid : 123 ) ed up - date rules are :
xia xia pk dikmikyka pk ( xy t ) ikmikyka yja yja pk ( x t ) akdkjmkj pk ( x t ) ak ( xy t ) kj mkj error function , pij mijjdij ( cid : 123 ) ~ xi ( cid : 123 ) ~ yjj123
these update rules converge to local minima of the
we evaluated the accuracy of network distance ma - trices modeled by svd and nmf and compared the results to those of pca from the lipschitz embed - dings used by virtual landmark ( 123 ) and ics ( 123 ) .
we did not evaluate the simplex downhill algorithm used in gnp because while its accuracy is not ob - viously better than lipschitz embedding , it is much more expensive , requiring hours of computation on large data sets ( 123 ) .
accuracies were evaluated by the modi ( cid : 123 ) ed relative error ,
relative error = jdij ( cid : 123 ) ^dijj min ( dij ; ^dij )
where the min - operation in the denominator serves to increase the penalty for underestimated network
123 . 123 data sets
we used the following ( cid : 123 ) ve real - world data sets in sim - ulation .
parts of the data sets were ( cid : 123 ) ltered out to eliminate missing elements in the distance matrices ( since none of the algorithms except nmf can cope with missing data ) .
it is known that these update rules converge mono - tonically to stationary points of the error function ,
the network distances in the data sets are round - trip time ( rtt ) between pairs of internet hosts
is symmetric between two end hosts , but it does vio - late the triangle inequality and also give rise to other e ( cid : 123 ) ects ( described in section 123 ) that are poorly mod - eled by network embeddings in euclidean space .
( cid : 123 ) nlanr : the nlanr active measurement project ( 123 ) collects a variety of measurements between all pairs of participating nodes .
the nodes are mainly at nsf supported hpc sites , with about 123% outside the us .
the data set we used was collected on january 123 , 123 , consist - ing of measurements of a 123 ( cid : 123 ) 123 clique .
each host was pinged once per minute , and network distance was taken as the minimum of the ping times over the day .
( cid : 123 ) gnp and agnp : the gnp project measured minimum round trip time between 123 active sites in may 123
about half of the hosts are in north america; the rest are distributed globally .
we used gnp to construct a symmetric 123 ( cid : 123 ) 123 data set and agnp to construct an asymmetric 123 ( cid : 123 ) 123 dataset .
the p123psim project
sured a distance matrix of rtts among about 123 internet dns servers based on the king method ( 123 ) .
the dns servers were obtained from an internet - scale gnutella network trace .
( cid : 123 ) pl - rtt : obtained from planetlab pairwise ping project ( 123 ) .
we chose the minimum rtt measured at 123 / 123 / 123 123 : 123 est .
a 123 ( cid : 123 ) 123 full distance matrix was obtained by ( cid : 123 ) ltering out
123 . 123 simulated results
figure 123 illustrates the cumulative density function ( cdf ) of relative errors of rtt reconstructed by svd when d = 123 , on 123 rtt data sets .
the best re - sult is over gnp data set : more than 123% distances are reconstructed within 123% relative error .
this is not too surprising because the gnp data set only contains 123 nodes .
however , svd also works well over nlanr , which has more than 123 nodes : about 123% fraction of distances are reconstructed within 123% relative error .
over p123psim and pl - rtt data sets , svd achieves similar accuracy results : 123 per - centile relative error is 123% .
we ran the same tests on nmf and observed similar results .
therefore , we chose nlanr and p123psim as two representative data sets for the remaining simulations .
figure 123 : cumulative distribution of relative error by svd over various data sets , d = 123
figure 123 compares the reconstruction accuracy of three algorithms : matrix factorization by svd and nmf , and pca applied to the lipschitz embedding .
the algorithms were simulated over nlanr and p123psim data sets .
it is shown that nmf has al - most exactly the same median relative errors as svd on both data sets when the dimension d < 123
both nmf and svd yield much more accurate results than lipschitz : the median relative error of svd and nmf is more than 123 times smaller than lipschitz when d = 123
svd is slightly better than nmf when d is large .
the reason for this may be that the algorithm for nmf is only guaranteed to converge to local min - ima .
considering that the hosts in the data sets come from all over the internet , the results show that ma - trix factorization is a scalable approach to modeling distances in large - scale networks .
in terms of main - taining a low - dimensional representation , d ( cid : 123 ) 123 ap - pears to be a good tradeo ( cid : 123 ) between complexity and accuracy for both svd and nmf .
123 distance prediction
the simulation results from the previous section demonstrate that pairwise distances in large - scale networks are well modeled by matrix factorization .
in this section we present the internet distance esti - mation service ( ides ) | a scalable and robust ser - vice based on matrix factorization to estimate net - work distances between arbitrary internet hosts .
( a ) comparison over nlanr data set
( b ) comparison over p123psim data set
figure 123 : reconstruction error comparison of svd , nmf and lipschitz over nlanr and p123psim data set
123 basic architecture
we classify internet hosts into two categories : land - mark nodes and ordinary hosts .
landmark nodes are a set of well - positioned distributed hosts .
the network distances between each of them is available to the information server of ides .
we assume that landmarks can measure network distances to others and report the results to the information server .
the information server can also measure the pairwise dis - tances via indirect methods without landmark sup - port , e . g .
by the king method ( 123 ) if the metric is rtt .
an ordinary host is an arbitrary end node in the internet , which is identi ( cid : 123 ) ed by a valid ip address .
suppose there are m landmark nodes .
the ( cid : 123 ) rst step of ides is to gather the m ( cid : 123 ) m pairwise distance matrix d on the information server .
then , we can apply either svd or nmf algorithm over d to obtain landmark outgoing and incoming vectors ~ xi and ~ yi in d dimensions , d < m , for each host hi .
as before , we use x and y to denote the d ( cid : 123 ) m matrices with ~ xi and ~ yi as row vectors .
note that nmf can be used even when d contains missing elements .
now suppose an ordinary host hnew wants to gather distance information over the network .
the ( cid : 123 ) rst step is to calculate its outgoing vector ~ xnew and incoming vector ~ ynew .
to this end , it measures the network distances to and from the landmark nodes .
we denote dout as the distance to landmark i , and i as the distance from landmark i to the host .
ide - ally , we would like the outgoing and incoming vectors
to satisfy dout solution with the least squares error is given by :
i = ~ xnew ( cid : 123 ) ~ yi and din
i = ~ xi ( cid : 123 ) ~ ynew
~ xnew = arg min
~ ynew = arg min
i ( cid : 123 ) ~ u ( cid : 123 ) ~ yi ) 123
i ( cid : 123 ) ~ xi ( cid : 123 ) ~ u ) 123
the global minima of these error functions , computed by simple matrix operations , have the closed form :
~ xnew = ( douty ) ( y t y ) ( cid : 123 ) 123 ~ ynew = ( dinx ) ( x t x ) ( cid : 123 ) 123
( 123 ( 123 ) assume that the optimizations are un - constrained .
alternatively , one can impose nonnega - tivity constraints on ~ xnew and ~ ynew; this will guaran - tee that the predicted distances are themselves non - negative ( assuming that the landmark distance ma - trix was also modeled by nmf ) .
the least squared error problems in eqs .
( 123 ( 123 ) can be solved with nonnegativity constraints , but the solution is some - what more complicated .
our simulation results did not reveal any signi ( cid : 123 ) cant di ( cid : 123 ) erence between the pre - diction accuracies of least squares solutions with and without non - negativity constraints; thus , in what fol - lows , we focus on the simpler unconstrained solutions in eqs .
( 123 ( 123 ) .
we give a simple example of this procedure in fig - ure 123
the network is an enlarged version of the net - work in figure 123 , with the four original nodes serv - ing as landmarks and two new nodes introduced as
figure 123 : four landmark nodes l123 ( cid : 123 ) l123 and two or - dinary hosts h123 , h123 interconnected by a simple net -
ordinary hosts .
the ( cid : 123 ) rst step is to measure inter - landmark distances and calculate landmark incom - ing and outgoing vectors .
we used svd to factor the landmark distance matrix in this example .
the result is the same as the example in section 123 :
; y =123
note that svd can be substituted by nmf and the following steps are identical .
second , we measure the distance vectors for the ordinary hosts : dout = din = ( 123 : 123 123 : 123 123 : 123 123 : 123 ) for ordinary host h123
according to eqs .
( 123 ( 123 ) , ~ xh123 = ( ( cid : 123 ) 123 : 123 123 123 ) , ~ yh123 = ( ( cid : 123 ) 123 : 123 123 ( cid : 123 ) 123 ) .
similarly , we obtain the distance vector of h123 as ( 123 : 123 123 : 123 123 : 123 123 : 123 ) , and calculate its outgoing and incoming vectors : ~ xh123 = ( ( cid : 123 ) 123 : 123 123 ( cid : 123 ) 123 ) , ~ yh123 = ( ( cid : 123 ) 123 : 123 123 123 ) .
one can verify that distances between ordinary hosts and landmarks are exactly preserved .
the distance between two or - dinary hosts is not measured , but can be estimated as ~ xh123 ( cid : 123 ) ~ yh123 = ~ xh123 ( cid : 123 ) ~ yh123 = 123 : 123 , while the real network distance is 123
the basic architecture requires an ordinary host to measure network distances to all landmarks , which limits the scalability of ides .
furthermore , if some of the landmark nodes experience transient failures or a network partition , an ordinary host may not be able to retrieve the measurements it needs to solve
to improve the scalability and robustness of ides , we propose a relaxation to the basic architecture : an ordinary host hnew only has to measure distances to a set of k nodes with pre - computed outgoing and in - coming vectors .
the k nodes can be landmark nodes ,
or other ordinary hosts that have already computed their vectors .
suppose the outgoing vectors of those k nodes are ~ x123; ~ x123; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; ~ xk and the incoming vectors are ~ y123; ~ y123; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; ~ yk .
we measure dout i as the distance from and to the ith node , for all i = 123; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ; k .
calculating the new vectors ~ xnew and ~ ynew for hnew is done by solving the least squares problems :
~ xnew = arg min
~ ynew = arg min
i ( cid : 123 ) ~ u ( cid : 123 ) ~ yi ) 123
i ( cid : 123 ) ~ xi ( cid : 123 ) ~ u ) 123
the solution is exactly the same form as described in eq .
( 123 ) and eq .
the constraint k ( cid : 123 ) d is necessary ( and usually su ( cid : 123 ) cient ) to ensure that the problem is not singular .
in general , larger values of k lead to better prediction results , as they incorporate more measurements of network distances involving hnew into the calculation of the vectors ~ xnew and we use the topology in figure 123 again to demon - strate how the system works .
as in the basic archi - tecture , the ( cid : 123 ) rst step is to measure inter - landmark distances and calculate landmark outgoing and in - coming vectors .
secondly , the ordinary host h123 mea - sures the distances to l123 , l123 and l123 as ( 123 123 123 ) .
( 123 ) and eq .
( 123 ) , the vectors are ~ xh123 = ( - 123 123 123 ) , ~ yh123 = ( - 123 123 - 123 ) .
note that we did not measure the distance between h123 and l123 , but it can be esti - mated as ~ xh123 ( cid : 123 ) ~ yl123 = ( - 123 123 123 ) ( cid : 123 ) ( - 123 123 123 ) = 123 : 123 , which is in fact the true distance .
finally , the ordinary host h123 measures the distances to l123 , l123 and h123 as ( 123 123 123 ) .
because all of them already have pre - computed vectors , h123 can compute its own vectors by eq .
( 123 ) and eq .
the results are ~ xh123 = ( - 123 123 - 123 ) , ~ yh123 = ( - 123 - 123 123 ) .
the distances between ordinary host h123 and l123 / l123 are not measured directly , but can be estimated as ~ xh123 ( cid : 123 ) ~ yl123 = ( - 123 123 - 123 ) ( cid : 123 ) ( - 123 123 - 123 ) = 123 : 123 and ~ xh123 ( cid : 123 ) ~ yl123 = ( - 123 123 - 123 ) ( cid : 123 ) ( - 123 - 123 123 ) = 123 : 123
this example illustrates that even without mea - surement to all landmarks , the estimated distances can still be accurate .
in this example , most of the pairwise distances are exactly preserved; the maxi - mum relative error is 123% when predicting the dis - tance between h123 and l123
in the example , the load is well distributed among landmarks .
as shown in fig - ure 123 , distances to l123 are only measured twice dur - ing this estimation procedure .
such a scheme allows ides to scale to a large number of ordinary hosts and landmarks .
it is also robust against partial landmark
figure 123 : learning outgoing and incoming vectors for two ordinary hosts .
solid lines indicate that real network measurement is conducted .
each edge is an - notated with ( real network distance / estimated dis -
in this section we evaluate ides , using svd and nmf algorithms to learn models of network dis - tances , and compare them to the gnp ( 123 ) and ics ( 123 ) systems .
the experiments were performed on a dell dimen - sion 123 with pentium 123 123ghz cpu , 123gb ram .
the gnp implementation was obtained from the of - ( cid : 123 ) cial gnp software release written in c .
we imple - mented ides and ics in matlab 123 .
we identify four evaluation criteria :
we measure e ( cid : 123 ) ciency by the total running time required by a system to build its model of net - work distances between all landmark nodes and
the prediction error between dij and ^dij should be small .
we use the modi ( cid : 123 ) ed relative error function in eq .
( 123 ) to evaluate accuracy , which is also used in gnp and vivaldi .
note that pre - dicted distances are computed between ordinary
hosts that have not conducted any network mea - surements of their distance .
predicted distance errors are di ( cid : 123 ) erent than reconstructed distance errors ( where actual network measurements are
the storage requirements are o ( d ) for models based on network embeddings ( with one position vector for each host ) and matrix factorizations ( with one incoming and outgoing vector for each in large - scale networks , the number of hosts n is very large .
the condition d ( cid : 123 ) n allows the model to scale , assuming that rea - sonable accuracy of predicted distances is main - tained .
also , to support multiple hosts concur - it is desirable to distribute the load| for instance , by only requiring distance measure - ments to partial sets of landmarks .
a robust system should be resilient against host failures and temporary network partitioning .
in should not prevent the system from building models of network distances .
123 e ( cid : 123 ) ciency and accuracy
we use three data sets for evaluating accuracy and
( cid : 123 ) gnp : 123 out of 123 nodes in the symmetric data set were selected as landmarks .
the rest of the 123 nodes and the 123 nodes in the agnp data set were selected as ordinary hosts .
prediction accuracy was evaluated on 123 ( cid : 123 ) 123 pairs of hosts .
( cid : 123 ) nlanr : 123 out of 123 nodes were selected ran - domly as landmarks .
the remaining 123 nodes were treated as ordinary hosts .
the prediction accuracy was evaluated on 123 ( cid : 123 ) 123 pairs of hosts .
( cid : 123 ) p123psim : 123 out of 123 nodes were selected ran - domly as landmarks .
the remaining 123 nodes were treated as ordinary hosts .
the prediction accuracy was evaluated on 123 ( cid : 123 ) 123 pairs of
although deliberate placement of landmarks may yield more accurate results , we chose the landmarks randomly since in general they may be placed any - where on the internet .
a previous study also shows that random landmark selection is fairly e ( cid : 123 ) ective if
table 123 : e ( cid : 123 ) ciency comparison on ides , ics and gnp over four data sets
more than 123 landmarks are employed ( 123 ) .
to ensure fair comparisons , we used the same set of landmarks for all four algorithms .
we also repeated the simu - lation several times , and no signi ( cid : 123 ) cant di ( cid : 123 ) erences in results were observed from one run to the next .
table 123 illustrates the running time comparison between ides , ics and gnp .
gnp is much more ine ( cid : 123 ) cient than the ides and ics .
this is because gnp uses simplex downhill method , which con - verges slowly to local minima .
both ides and ics have running time less than 123 second , even when the data sets contain thousands of nodes .
it is possible to reduce the running time of gnp by sacrifying the accuracy , but the parameters are hard to tune , which is another drawback of simplex downhill method .
figure 123 plots the cdf of prediction errors for ides using svd , ides using nmf , ics and gnp over the three data sets respectively .
in figure 123 ( a ) , the gnp system is the most accurate system for the gnp data set .
ides using svd and nmf are as accurate as gnp for 123% of the predicted distances .
the gnp data set is somewhat atypical , however , in that the predicted distance matrix has many more columns ( 123 ) than rows ( 123 ) .
figure 123 ( b ) and 123 ( c ) depict the cdf of prediction errors over nlanr and p123psim data sets , which are more typical .
in both cases , ides has the best prediction accuracy .
on the nlanr data set , ides yields better results than gnp and ics : the median relative error of ides us - ing svd is only 123 .
its 123 percentile relative error is about 123 .
the accuracy is worse for all three sys - tems in p123psim data set than in nlanr data set .
however , ides ( with either svd or nmf ) is still the most accurate system among the three .
the better prediction results on the nlanr data set may be due to the fact that 123% of the hosts in nlanr are in north america and the network distances , computed from minimum rtt over a day , are not a ( cid : 123 ) ected much by queueing delays and route congestion .
these prop - erties make the data set more uniform , and therefore , more easily modeled by a low dimensional represen -
( a ) cdf of relative error over gnp data set ,
( b ) cdf of relative error over nlanr data set , 123 landmarks
( c ) cdf of relative error over p123psim data set , 123 landmarks
figure 123 : accuracy comparison on ides using svd and nmf , ics , and gnp , d = 123
123 landmarks , d=123 123 landmarks , d=123
123 landmarks , d=123 123 landmarks , d=123
fraction of unobserved landmarks
fraction of unobserved landmarks
( a ) over nlanr data set
( b ) over p123psim data set
figure 123 : the correlation between accuracy and landmark failures on ides using svd algorithm .
123 scalability and robustness
in the previous subsection , we showed that ides can accurately model the network distances in low dimen - sions d ( cid : 123 ) 123 , which is fundamental to make the sys - tem scale to large - scale networks .
in this subsection , we study the impact of partially observed landmarks on the accuracy of ides .
measuring the distances to only a subset of landmark nodes reduces the overall load and allows the system to support more ordinary hosts concurrently .
it also makes the system robust to partial landmark failures .
we simulated partially observed landmark scenar - ios in ides using svd to model partial distance ma - trices from the nlanr and p123psim data sets .
for each data set , we experimented with two settings : 123 random landmarks and 123 random landmarks .
the simulation results are shown in figure 123
the x - axis indicates the fraction of unobserved landmarks .
the unobserved landmarks for each ordinary host were in - dependently generated at random .
when the number of landmarks is less than twice the model dimension - ality d , the accuracy appears sensitive to the fraction of unobserved landmarks .
however , as the number of landmarks increases , the system tolerates more fail - ure : for example , not observing 123% of the landmarks has little impact on the system accuracy when 123 landmarks are used in the test .
between arbitrary internet hosts .
our model imposes fewer constraints on network distances than models based on low dimensional embeddings; lar , it can represent distances that violate the tri - angle inequality , as well as asymmetric network dis - tances .
such a model is more suitable for modeling the topology and complex routing policies on the in - ternet .
based on this model , we proposed the ides system and two learning algorithms , svd and nmf , for factoring matrices of network distances between arbitrary internet hosts .
simulations on real world data sets have shown that ides is computationally e ( cid : 123 ) cient , scalable to large - scale networks , more accu - rate than previous models , and resilient to temporary
we are grateful to jonathan m .
smith ( upenn ) for helpful comments on the manuscript , and frank dabek ( mit ) for sharing the p123psim data set .
this material is based upon work supported by the na - tional science foundation under grant no .
123 and darpa under contract f123 - 123 - 123 - 123
