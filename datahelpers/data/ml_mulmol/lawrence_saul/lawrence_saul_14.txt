in this paper we study how to improve near - est neighbor classication by learning a ma - halanobis distance metric .
we build on a re - cently proposed framework for distance met - ric learning known as large margin nearest neighbor ( lmnn ) classication .
our paper makes three contributions .
first , we describe a highly ecient solver for the particular instance of semidenite programming that arises in lmnn classication; our solver can handle problems with billions of large margin constraints in a few hours .
second , we show how to reduce both training and testing times using metric ball trees; the speedups from ball trees are further magnied by learning low dimensional representations of the input space .
third , we show how to learn dier - ent mahalanobis distance metrics in dierent parts of the input space .
for large data sets , the use of locally adaptive distance metrics leads to even lower error rates .
many algorithms for pattern classication and ma - chine learning depend on computing distances in a multidimensional input space .
often , these distances are computed using a euclidean distance metrica choice which has both the advantages of simplicity and generality .
notwithstanding these advantages , though , the euclidean distance metric is not very well adapted to most problems in pattern classication .
viewing the euclidean distance metric as overly sim -
preliminary work .
under review by the international con - ference on machine learning ( icml ) .
do not distribute .
plistic , many researchers have begun to ask how to learn or adapt the distance metric itself in order to achieve better results ( xing et al . , 123; chopra et al . , 123; frome et al . , 123 ) .
distance metric learning is an emerging area of statistical learning in which the goal is to induce a more powerful distance met - ric from labeled examples .
the simplest instance of this problem arises in the context of k - nearest neigh - bor ( knn ) classication using mahalanobis distances .
mahalanobis distances are computed by linearly trans - forming the input space , then computing euclidean distances in the transformed space .
a well - chosen lin - ear transformation can improve knn classication by decorrelating and reweighting elements of the feature in fact , signicant improvements have been observed within several dierent frameworks for this problem , including neighborhood components analy - sis ( goldberger et al . , 123 ) , large margin knn clas - sication ( weinberger et al . , 123 ) , and information - theoretic metric learning ( davis et al . , 123 ) .
these studies have established the general utility of distance metric learning for knn classication .
how - ever , further work is required to explore its promise in more dicult regimes .
in particular , larger data sets raise new and important challenges in scalability .
they also present the opportunity to learn more adap - tive and sophisticated distance metrics .
in this paper , we study these issues as they arise in the recently proposed framework of large margin near - est neighbor ( lmnn ) classication ( weinberger et al . , 123 ) .
in this framework , a mahalanobis distance met - ric is trained with the goal that the k - nearest neigh - bors of each example belong to the same class while examples from dierent classes are separated by a large margin .
simple in concept , useful in practice , the ideas behind lmnn classication have also inspired other related work in machine learning and computer vision ( torresani & lee , 123; frome et al . , 123 ) .
fast solvers and ecient implementations for distance metric learning
the role of the margin in lmnn classication is in - spired by its role in support vector machines ( svms ) .
not surprisingly , given these roots , lmnn classica - tion also inherits various strengths and weaknesses of svms ( scholkopf & smola , 123 ) .
for example , as in svms , the training procedure in lmnn classication reduces to a convex optimization based on the hinge loss .
however , as described in section 123 , nave imple - mentations of this optimization do not scale well to larger data sets .
addressing the challenges and opportunities raised by larger data sets , this paper makes three contributions .
first , we describe how to optimize the training pro - cedure for lmnn classication so that it can readily handle data sets with tens of thousands of training in order to scale to this regime , we have implemented a special - purpose solver for the particu - lar instance of semidenite programming that arises in lmnn classication .
in section 123 , we describe the details of this solver , which we have used to tackle problems involving billions of large margin constraints .
to our knowledge , problems of this size have yet to be tackled by other recently proposed methods ( gold - berger et al . , 123; davis et al . , 123 ) for learning mahalanobis distance metrics .
as the second contribution of this paper , we explore the use of metric ball trees ( liu et al . , 123 ) for lmnn classication .
these data structures have been widely used to accelerate nearest neighbor search .
tion 123 , we show how similar data structures can be used for faster training and testing in lmnn classi - cation .
ball trees are known to work best in input spaces of low to moderate dimensionality .
mindful of this regime , we also show how to modify the optimiza - tion in lmnn so that it learns a low - rank mahalanobis distance metric .
with this modication , the metric can be viewed as projecting the original inputs into a lower dimensional space , yielding further speedups .
as the third contribution of this paper , we describe an important extension to the original framework for lmnn classication .
specically , in section 123 , we show how to learn dierent mahalanobis distance met - rics for dierent parts of the input space .
the novelty of our approach lies in learning a collection of dierent local metrics to maximize the margin of correct knn classication .
the promise of this approach is sug - gested by recent , related work in computer vision that has achieved state - of - the - art results on image classi - cation ( frome et al . , 123 ) .
our particular approach begins by partitioning the training data into disjoint clusters using class labels or unsupervised methods .
we then learn a mahalanobis distance metric for each
cluster .
while the training procedure couples the dis - tance metrics in dierent clusters , the optimization re - mains a convex problem in semidenite programming .
the globally coupled training of these metrics also distinguishes our approach from earlier work in adap - tive distance metrics for knn classication ( hastie & tibshirani , 123 ) .
to our knowledge , our approach yields the best knn test error rate on the extensively benchmarked mnist data set of handwritten digits that does not incorporate domain - specic prior knowl - edge ( lecun et al . , 123; simard et al . , 123 ) .
thus , our results show that we can exploit large data sets to learn more powerful and adaptive distance metrics for
of the many settings for distance metric learning , the simplest instance of the problem arises in the con - text of knn classication using mahalanobis distances .
a mahalanobis distance metric computes the squared distances between two points ( cid : 123 ) xi and ( cid : 123 ) xj as : m ( ( cid : 123 ) xi , ( cid : 123 ) xj ) = ( ( cid : 123 ) xi ( cid : 123 ) xj ) ( cid : 123 ) m ( ( cid : 123 ) xi ( cid : 123 ) xj ) ,
where m ( cid : 123 ) 123 is a positive semidenite matrix .
when m is equal to the identity matrix , eq .
( 123 ) reduces to the euclidean distance metric .
in distance metric learning , the goal is to discover a matrix m that leads to lower knn error rates than the euclidean distance metric .
here we briey review how mahalanobis distance met - rics are learned for lmnn classication ( weinberger et al . , 123 ) .
let the training data consist of n la - beled examples ( ( ( cid : 123 ) xi , yi ) ) n i=123 where ( cid : 123 ) xi rd and yi ( 123 , .
, c ) , where c is the number of classes .
for lmnn classication , the training procedure has two steps .
the rst step identies a set of k similarly labeled target neighbors for each input ( cid : 123 ) xi .
target neighbors are selected by using prior knowledge ( if available ) or by simply computing the k nearest ( similarly labeled ) neighbors using euclidean distance .
we use the nota - tion j ( cid : 123 ) i to indicate that ( cid : 123 ) xj is a target neighbor of ( cid : 123 ) xi .
the second step adapts the mahalanobis distance met - ric so that these target neighbors are closer to ( cid : 123 ) xi than all other dierently labeled inputs .
the mahalanobis distance metric is estimated by solving a problem in semidenite programming .
distance metrics obtained in this way were observed to yield consistent and sig - nicant improvements in knn error rates .
the semidenite program in lmnn classication arises from an objective function which balances two terms .
the rst term penalizes large distances be - tween inputs and their target neighbors .
the second term penalizes small distances between dierently la -
fast solvers and ecient implementations for distance metric learning
beled inputs; specically , a penalty is incurred if these distances do not exceed ( by a nite margin ) the dis - tances to the target neighbors of these inputs .
the terms in the objective function can be made precise with further notation .
let yij ( 123 , 123 ) indicate whether the inputs ( cid : 123 ) xi and ( cid : 123 ) xj have the same class label .
also , let ijl 123 denote the amount by which a dierently labeled input ( cid : 123 ) xl invades the perimeter around input ( cid : 123 ) xi dened by its target neighbor ( cid : 123 ) xj .
the mahalanobis distance metric m is obtained by solving the following
m ( ( cid : 123 ) xi , ( cid : 123 ) xl ) d123
( b ) ijl 123 ( c ) m ( cid : 123 ) 123
m ( ( cid : 123 ) xi , ( cid : 123 ) xj ) + ( cid : 123 )
m ( ( cid : 123 ) xi , ( cid : 123 ) xj ) 123 ijl
the constant denes the trade - o between the two terms in the objective function; in our experiments , we set = 123
the constraints of type ( a ) encourage in - puts ( ( cid : 123 ) xi ) to be at least one unit closer to their k target neighbors ( ( cid : 123 ) xj ) than to any other dierently labeled in - put ( ( cid : 123 ) xl ) .
when dierently labeled inputs ( cid : 123 ) xl invade the local neighborhood of ( cid : 123 ) xi , we refer to them as impos - tors .
impostors generate positive slack variables ijl which are penalized in the second term of the objective function .
the constraints of type ( b ) enforce nonneg - ativity of the slack variables , and the constraint ( c ) enforces that m is positive semidenite , thus dening a valid ( pseudo ) metric .
noting that the squared ma - halanobis distances d123 m ( ( cid : 123 ) xi , ( cid : 123 ) xj ) are linear in the matrix m , the above optimization is easily recognized as an
the semidenite program in the previous section grows in complexity with the number of training examples ( n ) , the number of target neighbors ( k ) , and the di - mensionality of the input space ( d ) .
the objective function is optimized with respect to o ( kn123 ) large margin constraints of type ( a ) and ( b ) , while the mahalanobis distance metric m itself is a d d matrix .
thus , for even moderately large and / or high dimensional data sets , the required optimization ( though convex ) cannot be solved by standard o - the - shelf packages ( borchers , 123 ) .
in order to tackle larger problems in lmnn classica - tion , we implemented our own special - purpose solver .
our solver was designed to exploit the particular struc - ture of the semidenite program in the previous sec - tion .
the solver iteratively re - estimates the maha -
lanobis distance metric m to minimize the objective function for lmnn classication .
the amount of com - putation is minimized by careful book - keeping from one iteration to the next .
the speed - ups from these optimizations enabled us to work comfortably on data sets with up to n=123 , 123 training examples .
our solver works by eliminating the slack variables ijl from the semidenite program for lmnn classica - tion , then minimizing the resulting objective function by sub - gradient methods .
the slack variables are elim - inated by folding the constraints ( a ) and ( b ) into the objective function as a sum of hinge losses .
the hinge function is dened as ( z ) + = z if z > 123 and ( z ) + = 123 if z < 123
in terms of this hinge function , we can express ijl as a function of the matrix m :
ijl ( m ) = ( cid : 123 ) 123 + d123
m ( ( cid : 123 ) xi , ( cid : 123 ) xj ) d123
finally , writing the objective function only in terms of the matrix m , we obtain :
m ( ( cid : 123 ) xi , ( cid : 123 ) xj ) +
this objective function is not dierentiable due to the hinge losses that appear in eq .
nevertheless , be - cause it is convex , we can compute its sub - gradient and use standard descent algorithms to nd its minimum .
at each iteration of our solver , the optimization takes a step along the sub - gradient to reduce the objective function , then projects the matrix m back onto the cone of positive semidenite matrices .
iterative meth - ods of this form are known to converge to the correct solution , provided that the gradient step - size is su - ciently small ( boyd & vandenberghe , 123 ) .
the gradient computation can be done most eciently by careful book - keeping from one iteration to the next .
as simplifying notation , let cij = ( ( cid : 123 ) xi ( cid : 123 ) xj ) ( ( cid : 123 ) xi ( cid : 123 ) xj ) ( cid : 123 ) .
in terms of this notation , we can express the squared mahalanobis distances in eq .
( 123 ) as :
dm ( ( cid : 123 ) xi , ( cid : 123 ) xj ) = tr ( cijm ) .
to evaluate the gradient , we denote the matrix m at the tth iteration as mt .
at each iteration , we also dene a set n t of triplet indices such that ( i , j , l ) n t if and only if the triplets corresponding slack variable exceeds zero : ijl ( mt ) > 123
with this notation , we can write the gradient gt =
( cid : 123 ) ( cid : 123 ) mt at the tth iteration as :
gt = ( cid : 123 )
( cij cil ) .
computing the gradient requires computing the outer products in cij; it thus scales quadratically in the
fast solvers and ecient implementations for distance metric learning
input dimensionality .
as the set n t is potentially large , a nave computation of the gradient would be extremely expensive .
however , we can exploit the fact that the gradient contribution from each active triplet ( i , j , l ) does not depend on the degree of its margin violation .
thus , the changes in the gradient from one iteration to the next are determined entirely by the dierences between the sets n t and n t+123
using this fact , we can derive an extremely ecient update that relates the gradient at one iteration to the gradient at the previous one .
the update subtracts the contribu - tions from triples that are no longer active and adds the contributions from those that just became active :
gt+123 = gt
( cij cil ) +
( i , j , l ) n tn t+123
( i , j , l ) n t+123n t
for small gradient step sizes , the set n t changes very little from one iteration to the next .
in this case , the right hand side of eq .
( 123 ) can be computed very fast .
to further accelerate the solver , we adopt an active set method .
this method is used to monitor the large margin constraints that are actually violated .
note that computing the set n t at each iteration requires checking every triplet ( i , j , l ) with j ( cid : 123 ) i for a po - tential margin violation .
this computation scales as o ( nd123 + kn123d ) , making it impractical for large data sets .
to avoid this computational burden , we observe that the great majority of triples do not incur mar - in particular , for each training exam - ple , only a very small fraction of dierently labeled examples typically lie nearby in the input space .
con - sequently , a useful approximation is to check only a subset of likely triples for margin violations per gra - dient computation and only occasionally perform the full computation .
we set this active subset to the list of all triples that have ever violated the margin , ie i=123 n i .
when the optimization converges , we verify that the working set n t does contain all active triples that incur margin violations .
this nal check is needed to ensure convergence to the correct minimum .
if the check is not satised , the optimization continues with the newly expanded active set .
table 123 shows how quickly the solver works on prob - lems of dierent sizes .
the results in this table were generated by learning a mahalanobis distance metric on the mnist data set of 123 grayscale handwrit - ten digits ( lecun et al . , 123 ) .
the digits were pre - processed by principal component analysis ( pca ) to reduce their dimensionality from d = 123 to d = 123
we experimented by learning a distance metric from dierent subsets of the training examples .
the experi - ments were performed on a standard desktop machine
table 123
statistics of the solver on subsets of the data set of mnist handwritten digits .
see text for details .
with a 123 ghz dual core 123 processor .
for each ex - periment , the table shows the number of training ex - amples , the cpu time to converge , the number of ac - tive constraints , the total number of constraints , and the knn test error ( with k = 123 ) .
note that for the full mnist training set , the semidenite program has over three billion large margin constraints .
neverthe - less , the active set method converges in less than four hoursfrom a euclidean distance metric with 123% test error to a mahalanobis distance metric with 123%
tree - based search
nearest neighbor search can be accelerated by storing training examples in hierarchical data structures ( liu et al . , 123 ) .
these data structures can also be used to reduce the training and test times for lmnn classi - cation .
in this section , we describe how these speedups are obtained using metric ball trees .
ball trees
we begin by reviewing the use of ball trees ( liu et al . , 123 ) for fast knn search .
ball trees recursively par - tition the training inputs by projecting them onto di - rections of large variance , then splitting the data on the mean or median of these projected values .
each subset of data obtained in this way denes a hyper - sphere ( or ball ) in the multidimensional input space that encloses its training inputs .
the distance to such a hypersphere can be easily computed for any test in - put; moreover , this distance provides a lower bound on the test inputs distance to any of the enclosed train - ing inputs .
this bound is illustrated in fig .
let s be the set of training inputs inside a specic ball with radius r .
the distance from a test input ( cid : 123 ) xt to any training input ( cid : 123 ) xi s is bounded from below by : ( cid : 123 ) xi s ( cid : 123 ) ( cid : 123 ) xt ( cid : 123 ) xi ( cid : 123 ) max ( ( cid : 123 ) ( cid : 123 ) xt ( cid : 123 ) c ( cid : 123 ) 123 r , 123 ) .
these bounds can be exploited in a tree - based search for nearest neighbors .
in particular , if the distance to the currently kth closest input ( cid : 123 ) xj is smaller than the bound from eq .
( 123 ) , then all inputs inside the ball s can be pruned away .
this pruning of unexplored subtrees can signicantly accelerate knn search .
the same basic strategy can also be applied to knn search under a mahalanobis distance metric .
ntime|active set||total set|train errortest error123s123k123%123%123s123k123%123%123m123m123%123%123h123m123b123%123% fast solvers and ecient implementations for distance metric learning
dimensionality reduction
across all our experiments , we observed that the gains from ball trees diminished rapidly with the dimen - sionality of the input space .
this observation is con - sistent with previous studies of ball trees and nn search .
when the data is high dimensional , nn search is plagued by the so - called curse of dimensionality .
in particular , distances in high dimensions tend to be more uniform , thereby reducing the opportunities for pruning large subtrees .
the curse of dimensionality is often addressed in ball trees by projecting the stored training inputs into a lower dimensional space .
the most commonly used methods for dimensionality reduction are random pro - jections and pca .
despite their widespread use , how - ever , neither of these methods is especially geared to preserve ( or improve ) the accuracy of knn classica - we experimented with two methods for dimensionality reduction in the particular context of lmnn classica - tion .
both methods were based on learning a low - rank mahalanobis distance metric .
such a metric can be viewed as projecting the original inputs into a lower di - mensional space .
in our rst approach , we performed a singular value decomposition ( svd ) on the full rank solution to the semidenite program in section 123
the full rank solution for the distance metric was then re - placed by a low rank approximation based on its lead - ing eigenvectors .
we call this approach lmnn - svd .
in our second approach , we followed a suggestion from previous work on lmnn classication ( torresani & in this approach , we explicitly parame - terized the mahalanobis distance metric as a low - rank matrix , writing m = l ( cid : 123 ) l , where l is a rectangular matrix .
to obtain the distance metric , we optimized the same objective function as before , but now in terms of the explicitly low - rank linear transformation l .
the optimization over l is not convex unlike the original optimization over m , but a ( possibly local ) minimum can be computed by standard gradient - based methods .
we call this approach lmnn - rect .
123 shows the results of knn classication from both these methods on the mnist data set of handwritten digits .
for these experiments , the raw mnist im - ages ( of size 123 123 ) were rst projected onto their 123 leading principal components .
the training pro - cedure for lmnn - svd optimized a full - rank distance metric in this 123 dimensional space , then extracted a low - rank distance metric from its leading eigenvectors .
the training procedures for lmnn - rect optimized a low - rank rectangular matrix of size r 123 , where r varied from 123 to 123
also shown in the gure are
figure 123
how ball trees work : for any input ( cid : 123 ) xi s , the distance ( cid : 123 ) ( cid : 123 ) xt ( cid : 123 ) xi ( cid : 123 ) is bounded from below by eq .
if a training example ( cid : 123 ) xj is known to be closer to ( cid : 123 ) xt , then the inputs inside the ball can be ruled out as nearest neighbors .
we rst experimented with ball trees to reduce the test times for lmnn classication .
in our experiments , we observed a factor of 123x speed - up for 123 - dimensional data and a factor of 123x speedup for 123 - dimensional data .
note that these speedups were measured rel - ative to a highly optimized baseline implementation of knn search .
in particular , our baseline implemen - tation rotated the input space to align its coordinate axes with the principal components of the data; the coordinate axes were also sorted in decreasing order of variance .
in this rotated space , distance computations were terminated as soon as any partially accumulated results ( from leading principal components ) exceeded the currently smallest k distances from the knn search we also experimented with ball trees to reduce the training times for lmnn classication .
to reduce training times , we integrated ball trees into our special - purpose solver .
specically , ball trees were used to accelerate the search for so - called impostors .
recall that for each training example ( cid : 123 ) xi and for each of its similarly labeled target neighbors ( cid : 123 ) xj , the im - postors consist of all dierently labeled examples ( cid : 123 ) xl with dm ( ( cid : 123 ) xi , ( cid : 123 ) xl ) 123 dm ( ( cid : 123 ) xi , ( cid : 123 ) xj ) 123 +123
the search for im - postors dominates the computation time in the train - ing procedure for lmnn classication .
to reduce the amount of computation , the solver described in sec - tion 123 maintains an active list of previous margin viola - tions .
nevertheless , the overall computation still scales as o ( n123d ) , which can be quite expensive .
note that we only need to search for impostors among training examples with dierent class labels .
to speed up train - ing , we built one ball tree for the training examples in each class and used them to search for impostors ( as the ball - tree creation time is negligible in comparison with the impostor search , we re - built it in every iter - ation ) .
we observed the ball trees to yield speedups ranging from a factor of 123x with 123 - dimensional data to a factor of 123x with 123 dimensional data .
! xt ! cr ! ! xt ! c ! r ! ! xt ! xi ! ! xi ! xj ! ! xt ! xj ! s fast solvers and ecient implementations for distance metric learning
we compute its squared distance to a training input ( cid : 123 ) xi in partition i as :
( ( cid : 123 ) xt , ( cid : 123 ) xi ) = ( ( cid : 123 ) xt ( cid : 123 ) xi ) ( cid : 123 ) mi ( ( cid : 123 ) xt ( cid : 123 ) xi ) .
these distances are then sorted as usual to determine nearest neighbors and label the test input .
note , how - ever , how dierent distance metrics are used for train - ing inputs in dierent partitions .
we can also use these metrics to compute distances between training inputs , with one important caveat .
note that for inputs belonging to dierent partitions , the distance between them will depend on the par - ticular metric used to compute it .
this asymmetry does not present any inherent diculty since , in fact , the dissimilarity measure in knn classication is not required to be symmetric .
thus , even on the train - ing set , we can use multiple metrics to measure dis - tances and compute meaningful leave - one - out knn er -
learning algorithm
in this section we describe how to learn multiple ma - halanobis distance metrics for lmnn classication .
each of these metrics is associated with a particular cluster of training examples .
to derive these clusters , we experimented with both unsupervised methods , such as the k - means algorithm , and fully supervised methods , in which each cluster contains the training examples belonging to a particular class .
before providing details of the learning algorithm , we make the following important observation .
multiple mahalanobis distance metrics for lmnn classication cannot be learned in a decoupled fashionthat is , by solving a collection of simpler , independent problems of the type already considered ( e . g . , one within each partition of training examples ) .
rather , the metrics must be learned in a coordinated fashion so that the distances from dierent metrics can be meaningfully compared for knn classication .
in our framework , such comparisons arise whenever an unlabeled test ex - ample has potential nearest neighbors in two or more dierent clusters of training examples .
our learning algorithm for multiple local distance met - =123 generalizes the semidenite program for ordinary lmnn classication in section 123
first , we modify the objective function so that the distances to target neighbors ( cid : 123 ) xj are measured under the met - ric mj .
next , we modify the large margin constraints in ( a ) so that the distances to potential impostors ( cid : 123 ) xl are measured under the metric ml .
finally , we re - place the single positive semidenite constraint in ( c ) by multiple such constraints , one for each local met -
figure 123
graph of knn error rate ( with k = 123 ) on dierent low dimensional representations of the mnist data set .
the results from further dimensionality reduction us - ing pca , as well as the baseline knn error rate in the original ( high dimensional ) space of raw images .
the speedup from ball trees is shown at the top of the graph .
the amount of speedup depends signicantly on the amount of dimensionality reduction , but very little on the particular method of dimensionality re - duction .
of the three methods compared in the gure , lmnn - rect is the most eective , improving signif - icantly over baseline knn classication while operat - ing in a much lower dimensional space .
overall , these results show that aggressive dimensionality reduction can be combined with distance metric learning .
multiple metrics
the originally proposed framework for lmnn clas - sication has one clear limitation : the same maha - lanobis distance metric is used to compute distances everywhere in the input space .
writing the metric as m = l ( cid : 123 ) l , we see that mahalanobis distances are equivalent to euclidean distances after a global lin - ear transformation ( cid : 123 ) x l ( cid : 123 ) x of the input space .
such a transformation cannot adapt to nonlinear variabilities in the training data .
in this section , we describe how to learn dierent ma - halanobis distance metrics in dierent parts of the in - put space .
we begin by simply describing how such a collection of local distance metrics is used at test time .
assume that the data set is divided into p disjoint par - =123 , such that p p = ( ) for any ( cid : 123 ) = i=123
also assume that each parti - tion p has its own mahalanobis distance metric m for use in knn classication .
given a test vector ( cid : 123 ) xt ,
p = ( ( cid : 123 ) xi ) n
123 . 123 . 123 . 123 . 123pcaldalmnn - svdlmnn - rectbaselineinput dimensionalityclassification error in %123x123x123x123x123x ( 123 ) 123 - nn classification after dimensionality reductionball tree speedup123x fast solvers and ecient implementations for distance metric learning
figure 123
visualization of multiple local distance metrics for mnist handwritten digits .
see text for details .
taken together , these steps lead to the new
( b ) ijl 123 ( c ) m ( cid : 123 ) 123
( ( cid : 123 ) xi , ( cid : 123 ) xl ) d123
( ( cid : 123 ) xi , ( cid : 123 ) xj ) + ( cid : 123 )
( ( cid : 123 ) xi , ( cid : 123 ) xj ) 123 ijl
note how the new constraints in ( a ) couple the dif - ferent mahalanobis distance metrics .
by jointly op - timizing these metrics to minimize a single objective function , we ensure that the distances they compute can be meaningfully compared for knn classication .
we evaluated the performance of this approach on ve publicly available data sets : the mnist data set123 of handwritten digits ( n = 123 , c = 123 ) , the 123 - newsgroups data set123 of text messages ( n = 123 , c = 123 ) , the letters data set123 of distorted computer fonts ( n=123 , c=123 ) , the isolet data set123 of spoken letters ( n = 123 , c = 123 ) , and the yalefaces123 data set of face images ( n = 123 , c = 123 ) .
the data sets were preprocessed by pca to reduce their dimensionality .
the amount of dimensionality reduction varied with each experiment , as discussed below .
to start , we sought to visualize the multiple metrics learned in a simple experiment on mnist handwritten digits of zeros , ones , twos , and fours .
for ease of visu -
figure 123
test knn error rates on the isolet and mnist data sets as a function of the number of distance metrics .
alization , we worked with only the leading two princi - pal components of the mnist data set .
123 shows these two dimensional inputs , color - coded by class la - bel .
with these easily visualized inputs , we minimized the objective function in section 123 to learn a special - ized distance metric for each type of handwritten digit .
the ellipsoids in the plot reveal the directions ampli - ed by the local distance metric of each digit class .
notably , each distance metric learns to amplify the di - rection perpendicular to the decision boundary for the nearest , competing class of digits .
our next experiments examined the performance of lmnn classication as a function of the number of dis - tance metrics .
in these experiments , we used pca to reduce the input dimensionality to d=123; we also only worked with a subset of n = 123 training examples of mnist handwritten digits .
to avoid overtting , we used an early stopping approach while monitor - ing the knn error rates on a held - out validation set consisting of 123% of the training data .
123 shows the test knn error rates on the isolet and mnist data sets as a function of the number of in these experiments , we explored both unsupervised and supervised methods for parti - tioning the training inputs as a precursor to learning local distance metrics .
in the unsupervised setting , the training examples were partitioned by k - means cluster - ing , with the number of clusters ranging from 123 to 123 ( just 123 cluster is identical to single - matrix lmnn ) .
as k - means clustering is prone to local minima , we aver - aged these results over 123 runs .
the gure shows the average test error rates in red , as well as their stan - dard deviations ( via error bars ) .
in the supervised set - ting , the training examples were partitioned by their class labels , resulting in the same number of clusters as classes .
the test error rates in these experiments are shown as blue crosses .
in both the unsupervised and supervised settings , the test error rates decreased with the use of multiple metrics .
however , the im - provements were far greater in the supervised setting .
123 . 123 . 123 . 123 . 123% classication errornumber of clustersnumber of clusters123isolet 123 . 123 . 123mnistmeanstdstdstd123 metric / class123 global metric123isolet 123 . 123 . 123mnistmeanstdstdstd123 metric / class123 global metric123isolet 123 . 123 . 123mnistmeanst .
deviationstdstd123 metric / class123 global metric123 . 123isolet123 . 123 . 123 . 123mnistmnistisolet fast solvers and ecient implementations for distance metric learning
figure 123
the classication train - and testerror rates with one metric ( lmnn ) and multiple metrics .
the value of k was set by cross validation .
finally , our last experiments explored the improve - ment in knn error rates when one distance metric was learned for the training examples in each class .
in these experiments , we used the full number of train - ing examples for each data set .
in addition , we used pca to project the training inputs into a lower di - mensional subspace accounting for at least 123% of the datas total variance .
123 shows generally consis - tent improvement in training and test knn error rates , though overtting is an issue , especially on the 123 - newsgroups and yalefaces data sets .
this overtting is to be expected from the relatively large number of classes and high input dimensionality of these data sets : the number of model parameters in these exper - iments grows linearly in the former and quadratically in the latter .
on these data sets , only the use of a validation set prevents the training error from vanish - ing completely while the test error skyrockets .
on the other hand , a signicant improvement in the test er - ror rate is observed on the largest data set , that of mnist handwritten digits .
on this data set , multiple distance metrics yield a 123% test error ratea highly competitive result for a method that does not take into account prior domain knowledge ( lecun et al . , 123 ) .
in this paper , we have extended the original framework for lmnn classication in several important ways : by describing a solver that scales well to larger data sets , by integrating metric ball trees into the training and testing procedures , by exploring the use of dimension - ality reduction for further speedups , and by showing how to train dierent mahalanobis distance metrics in dierent parts of the input space .
these exten - sions should prove useful in many applications of knn classication .
more generally , we also hope they spur further work on problems in distance metric learning and large - scale semidenite programming , both areas of great interest in the larger eld of machine learning .
this research is based upon work supported by the na - tional science foundation under grant no .
123
