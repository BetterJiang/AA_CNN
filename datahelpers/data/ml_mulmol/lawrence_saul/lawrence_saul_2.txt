many problems in neural computation and statistical learning involve optimizations with nonnegativity constraints .
in this article , we study convex problems in quadratic programming where the optimization is confined to an axis - aligned region in the nonnegative orthant .
for these problems , we derive multiplicative updates that improve the value of the objective function at each iteration and converge monotonically to the global minimum .
the updates have a simple closed form and do not involve any heuristics or free parameters that must be tuned to ensure convergence .
despite their simplicity , they differ strikingly in form from other multiplicative updates used in machine learning . we provide complete proofs of convergence for these updates and describe their application to problems in signal processing and pattern recognition .
medicine and health sciences
sha , f . , lin , y . , saul , l . k .
and lee , d . d .
( 123 ) .
multiplicative updates for nonnegative quadratic programming .
neural computation .
123 , 123 - 123
123 mit press
this journal article is available at scholarlycommons : http : / / repository . upenn . edu / ese_papers / 123
communicated by sebastian seung
multiplicative updates for nonnegative quadratic
computer science division , university of california , berkeley , berkeley , ca 123 , u . s . a .
department of electrical and systems engineering , university of pennsylvania , philadelphia , pa 123 , u . s . a .
lawrence k .
saul department of computer science and engineering , university of california , san diego , la jolla , ca 123 , u . s . a .
daniel d .
lee department of electrical and systems engineering , university of pennsylvania , philadelphia , pa 123 , u . s . a .
many problems in neural computation and statistical learning involve optimizations with nonnegativity constraints .
in this article , we study convex problems in quadratic programming where the optimization is conned to an axis - aligned region in the nonnegative orthant .
for these problems , we derive multiplicative updates that improve the value of the objective function at each iteration and converge monotonically to the global minimum .
the updates have a simple closed form and do not involve any heuristics or free parameters that must be tuned to en - sure convergence .
despite their simplicity , they differ strikingly in form from other multiplicative updates used in machine learning .
we provide complete proofs of convergence for these updates and describe their ap - plication to problems in signal processing and pattern recognition .
many problems in neural computation and statistical learning involve op - timizations with nonnegativity constraints .
examples include large margin classication by support vector machines ( vapnik , 123 ) , density estimation
neural computation 123 , 123 ( 123 )
c ( cid : 123 ) 123 massachusetts institute of technology
multiplicative updates for nonnegative quadratic programming
in bayesian networks ( bauer , koller , & singer , 123 ) , dimensionality reduc - tion by nonnegative matrix factorization ( lee & seung , 123 ) , and acoustic echo cancellation ( lin , lee , & saul , 123 ) .
the optimizations for these prob - lems cannot be solved in closed form; thus , iterative learning rules are required that converge in the limit to actual solutions .
the simplest such learning rule is gradient descent .
minimizing an ob -
jective function f ( v ) by gradient descent involves the additive update ,
vi vi ( f / vi ) ,
where >123 is a positive learning rate and all the elements of the parameter vector v = ( v123 , v123 , .
, vn ) are updated in parallel .
gradient descent is not particularly well suited to constrained optimizations , however , because the additive update in equation 123 can lead to violations of the constraints .
a simple extension enforces the nonnegativity constraints :
vi max ( vi ( f / vi ) , 123 ) .
the update rule in equation 123 is a special instance of gradient projection methods ( bertsekas , 123; serani , zanghirati , & zanni , 123 ) .
the nonneg - ativity constraints are enforced by projecting the gradient - based updates in equation 123 onto the convex feasible setnamely , the nonnegative or - thant vi 123
the projected gradient updates also depend on a learning rate
for optimizations with nonnegativity constraints , an equally simple but more appropriate learning rule involves the so - called exponentiated gradi - ent ( eg ) ( kivinen & warmuth , 123 ) :
vi vi e
( f / vi ) .
equation 123 is an example of a multiplicative update .
because the elements of the exponentiated gradient are always positive , this update naturally enforces the nonnegativity constraints on vi .
by taking the logarithm of both sides of equation 123 , we can view the eg update as an additive update123 in the log domain :
log vi log vi ( f / vi ) .
multiplicative updates such as eg typically lead to faster convergence than additive updates ( kivinen & warmuth , 123 ) if the solution v
123 this update differs slightly from gradient descent in the variable ui =log vi , which would involve the partial derivative f / ui = vi ( f / vi ) as opposed to what appears in
sha , y .
lin , l .
saul , and d
optimization problem is sparse , containing a large number of zero ele - ments .
note , moreover , that sparse solutions are more likely to arise in problems with nonnegativity constraints because in these problems , min - =123 without the precise vanishing of the partial deriva - ima can emerge at v tive ( f / vi ) |v ( as would be required in an unconstrained optimization ) .
the eg update in equation 123 , like gradient descent in equation 123 and projected gradient descent in equation 123 , depends on the explicit introduction of a learning rate >123
the size of the learning rate must be chosen to avoid divergent oscillations ( if is too large ) and unacceptably slow convergence ( if is too small ) .
the necessity of choosing a learning rate can be viewed as a consequence of the generality of these learning rules; they do not assume or exploit any structure in the objective function f ( v ) beyond the fact that it is differentiable .
not surprisingly , many objective functions in machine learning have structure that can be exploited in their optimizationsand in particular , by multiplicative updates .
such updates need not involve learning rates , and they may also involve intuitions rather different from the connection between eg and gradient descent in equations 123 and 123 .
for example , the expectation - maximization ( em ) algorithm for latent variable models ( dempster , laird , & rubin , 123 ) and the generalized iterative scaling ( gis ) algorithm for logistic regression ( darroch & ratcliff , 123 ) can be viewed as multiplicative updates ( saul , sha , & lee , 123 ) , but unlike the eg update , they cannot be cast as simple variants of gradient descent in the log domain .
in this article , we derive multiplicative updates for convex problems in quadratic programming where the optimization is conned to an axis - aligned region in the nonnegative orthant .
our multiplicative updates have the property that they improve the value of the objective function at each iteration and converge monotonically to the global minimum .
despite their simplicity , they differ strikingly in form from other multiplicative updates used in statistical learning , including eg , em , and gis .
this article provides a complete derivation and proof of convergence for the multiplicative up - dates , originally described in previous work ( sha , saul , & lee , 123a , 123b ) .
the proof techniques should be of general interest to researchers in neural computation and statistical learning faced with problems in constrained
the basic problem that we study in this article is quadratic programming
with nonnegativity constraints :
123 vtav + btv
f ( v ) = 123
the constraint indicates that the variable v is conned to the nonnegative orthant .
we assume that the matrix a is symmetric and strictly positive denite , so that the objective function f ( v ) in equation 123 is bounded below ,
multiplicative updates for nonnegative quadratic programming
and its optimization is convex .
in particular , it has one global minimum and no local minima .
monotonically convergent multiplicative updates for minimizing equa - tion 123 were previously developed for the special case of nonnegative matrix factorization ( nmf ) ( lee & seung , 123 ) .
in this setting , the matrix elements of a are nonnegative , and the vector elements of b are negative .
the updates for nmf are derived from an auxiliary function similar to the one used in em algorithms .
they take the simple , elementwise multiplica -
which is guaranteed to preserve the nonnegativity constraints on v .
the validity of these updates for nmf hinges on the assumption that the matrix elements of a are nonnegative : otherwise , the denominator in equation 123 could become negative , leading to a violation of the nonnegativity con - straints on v .
in this article , we generalize the multiplicative updates in equation 123 to a wider range of problems in nonnegative quadratic programming ( nqp ) .
our updates assume only that the matrix a is positive semidenite : in par - ticular , it may have negative elements off the diagonal , and the vector b may have both positive and negative elements .
despite the greater gener - ality of our updates , they retain a simple , elementwise multiplicative form .
the multiplicative factors in the updates involve only two matrix - vector multiplications and reduce to equation 123 for the special case of nmf .
the updates can also be extended in a straightforward way to the more general problem of nqp with upper - bound constraints on the variable v ( cid : 123 ) .
under these additional constraints , the variable v is restricted to an axis - aligned box in the nonnegative orthant with opposing vertices at the origin and the nonnegative vector ( cid : 123 ) .
we prove that our multiplicative updates converge monotonically to the global minimum of the objective function for nqp .
the proof relies on constructing an auxiliary function , as in earlier proofs for em and nmf al - gorithms ( dempster et al . , 123; lee & seung , 123 ) .
in general , monotonic improvement in an auxiliary function sufces only to establish convergence to a local stationary point , not necessarily a global minimum .
for our up - dates , however , we are able to prove global convergence by exploiting the particular structure of their xed points as well as the convexity of the
the rest of this article is organized as follows .
in section 123 , we present the multiplicative updates and develop some simple intuitions behind their form .
the updates are then derived more formally and their convergence properties established in section 123 , which completes the proofs sketched in earlier work ( sha et al . , 123a , 123b ) .
in section 123 , we briey describe
sha , y .
lin , l .
saul , and d
some applications to problems in signal processing ( lin et al . , 123 ) and pattern recognition ( cristianini & shawe - taylor , 123 ) .
finally , in section 123 , we conclude by summarizing the main advantages of our approach .
we begin by presenting the multiplicative updates for the basic problem of nqp in equation 123 .
some simple intuitions behind the updates are developed by analyzing the karush - kuhn - tucker ( kkt ) conditions for this problem .
we then extend the multiplicative updates to handle the more general problem of nqp with additional upper - bound constraints v ( cid : 123 ) .
123 updates for nqp .
the multiplicative updates for nqp are ex - pressed in terms of the positive and negative components of the matrix a .
in particular , let a denote the nonnegative matrices with elements :
|aij| if aij < 123 ,
aij if aij > 123 ,
it follows that a = a .
in terms of these nonnegative matrices , the objective function in equation 123 can be decomposed as the combination of three terms , which we write as f ( v ) = fa ( v ) + fb ( v ) fc ( v )
for reasons that will become clear shortly .
we use the rst and third terms in equation 123 to split the quadratic piece of f ( v ) and the second term to capture the linear piece :
fa ( v ) = 123 fb ( v ) = btv , fc ( v ) = 123
the decomposition ( see equation 123 ) follows trivially from the denitions in equations 123 and 123 .
the gradient of f ( v ) can be similarly decomposed in terms of contributions from these three pieces .
we have chosen our notation in equation 123 so that bi = fb / vi; for the quadratic terms in the objective function , we dene the corresponding derivatives :
ai = fa
multiplicative updates for nonnegative quadratic programming
ci = fc
note that the partial derivatives in equations 123 and 123 are guaranteed to be nonnegative when evaluated at vectors v in the nonnegative orthant .
the multiplicative updates are expressed in terms of these partial derivatives as
+ 123ai ci
note that these updates reduce to the special case of equation 123 for nmf when the matrix a has no negative elements .
the updates in equation 123 are meant to be applied in parallel to all the elements of v .
they are remarkably simple to implement and notable for their absence of free parameters or heuristic criteria that must be tuned to ensure convergence .
since ai 123 and ci 123 , it follows that the multiplicative prefactor in equation 123 is always nonnegative; thus , the optimization remains conned to the feasible region for nqp .
as we show in section 123 , moreover , these updates are guaranteed to decrease the value of f ( v ) at
there is a close link between the sign of the partial derivative f / vi and the effect of the multiplicative update on vi .
in particular , using the fact that f / vi = ai +bi ci , it is easy to show that the update decreases vi if f / vi >123 and increases vi if f / vi <123
thus , the multiplicative update in equation 123 moves each element vi in an opposite direction to its partial
123 fixed points .
further intuition for the updates in equation 123 can be gained by examining their xed points .
let mi denote the mul - tiplicative prefactor inside the brackets on the right - hand side of equa - tion 123 .
fixed points of the updates occur when either ( 123 ) vi =123 or ( 123 ) mi =123
what does the latter condition imply ? note that the expression for mi is simply the quadratic formula for the larger root of the polynomial p ( m ) = ai m123+bi mci .
thus , mi =123 implies that ai +bi ci = 123
from the def - initions in equations 123 to 123 , moreover , it follows that f / vi = 123
thus , the two criteria for xed points can be restated as ( 123 ) vi =123 or ( 123 ) f / vi =123
these are consistent with the karush - kuhn - tucker ( kkt ) conditions for the nqp problem in equation 123 , as we now show .
let i denote the lagrange multiplier used to enforce the nonnegativity
constraint on vi .
the kkt conditions are given by v = 123 ,
av + b = ,
sha , y .
lin , l .
saul , and d .
lee in which stands for elementwise vector multiplication .
a necessary and sufcient condition for v to solve equation 123 is that there exists a vec - tor such that v and satisfy this system .
it follows from equation 123 that the gradient of f ( v ) at its minimum is nonnegative : f = av + b 123
moreover , for inactive constraints ( corresponding to elements of the mini - mizer that are strictly positive ) , the corresponding partial derivatives of the objective function must vanish : f / vi = 123 if vi > 123
thus , the kkt condi - tions imply that ( 123 ) vi = 123 or ( 123 ) f / vi = 123 , and any solution satisfying the kkt conditions corresponds to a xed point of the multiplicative updates , though not vice versa .
123 upper - bound constraints .
the multiplicative updates in equation 123 can also be extended to incorporate upper - bound constraints of the form v ( cid : 123 ) .
a simple way of enforcing such constraints is to clip the output of the updates in equation 123 :
+ 123ai ci
as we show in the next section , this clipped update is also guaranteed to decrease the objective function f ( v ) in equation 123 if it results in a change
123 convergence analysis
in this section , we prove that the multiplicative updates in equation 123 con - verge monotonically to the global minimum of the objective function f ( v ) .
our proof is based on the derivation of an auxiliary function that provides an upper bound on the objective function .
similar techniques have been used to establish the convergence of many algorithms in statistical learn - ing ( e . g . , the em algorithm , dempster et al . , 123 , for maximum likelihood estimation ) and nonnegative matrix factorization ( lee & seung , 123 ) .
the proof is composed of two parts .
we rst show that the multiplicative up - dates monotonically decrease the objective function f ( v ) .
then we show that the updates converge to the global minimum .
we assume throughout the article that the matrix a is positive denite such that the objective func - tion is convex .
( though theorem 123 does not depend on this assumption , convexity is used to establish the stronger convergence results that follow . )
123 monotonic convergence .
an auxiliary function g ( u , v ) for the ob - jective function in equation 123 has two crucial properties : ( 123 ) f ( u ) g ( u , v ) and ( 123 ) f ( v ) = g ( v , v ) for all positive vectors u and v .
from such an auxiliary
multiplicative updates for nonnegative quadratic programming
figure 123 : using an auxiliary function g ( u , v ) to minimize an objective function f ( v ) .
the auxiliary function is constructed around the current estimate of the minimizer; the next estimate is found by minimizing the auxiliary function , which provides an upper bound on the objective function .
the procedure is iterated until it converges to a stationary point ( generally a local minimum ) of the objective function .
function , we can derive the update rule v increases ( and generally decreases ) the objective function f ( v ) :
( cid : 123 ) = arg minug ( u , v ) , which never
( cid : 123 ) , v ) g ( v , v ) = f ( v ) .
by iterating this update , we obtain a series of values of v that improve the objective function .
figure 123 graphically illustrates how the auxiliary function g ( u , v ) is used to compute a minimum of the objective function f ( v ) at v = v
to derive an auxiliary function for nqp , we rst decompose the objective function f ( v ) in equation 123 into three terms as in equations 123 and 123 and then derive the upper bounds for each of them separately .
the following two lemmas establish the bounds relevant to the quadratic terms fa ( u )
lemma 123
let a denote the matrix composed of the positive elements of the matrix a , as dened in equation 123 .
then for all positive vectors u and v , the quadratic form fa ( u ) = 123
u satises the following inequality :
123 ut a
fa ( u ) 123
sha , y .
lin , l .
saul , and d
let ij denote the kronecker delta function , and let k be the diagonal matrix with elements
kij = ij
since fa ( u ) = 123 statement that the matrix ( k a matrix m whose elements
u , the inequality in equation 123 is equivalent to the ) is positive semidenite .
consider the
mij = vi ( kij a ij ) v j
) is positive semidenite if m is positive semidenite .
we note that
are obtained by rescaling componentwise the elements of ( k a for all vectors u ,
ui vi ( kij a ij ) v j u j
vi v j ui u j
v ) i ui u j v j
vi v j u123
vi v j ui u j
vi v j ( ui u j ) 123 123
thus , ( k a an alternative proof that ( k a by appealing to the frobenius - perron theorem ( lee & seung , 123 ) .
) is positive semidenite , proving the bound in equation 123 .
) is semidenite positive can also be made
for the terms related to the negative elements in the matrix a , we have
lemma 123
let a denote the matrix composed of the negative elements of the matrix a , as dened in equation 123 .
then for all positive vectors u and v , the quadratic form fc ( u ) = 123
u satises the following inequality :
123 ut a
vi v j
123 + log
vi v j
multiplicative updates for nonnegative quadratic programming proof .
to prove this bound , we use the simple inequality : z 123 + log z .
substituting z = ui u j / ( vi v j ) into this inequality gives
ui u j vi v j
123 + log
ui u j vi v j
substituting the above inequality into fc ( u ) = 123
ij ui u j and noting the negative sign , we arrive at the bound in equation 123 .
combining lemmas 123 and 123 and noting that fb ( u ) = ( cid : 123 )
i bi ui , we have
proved the following theorem :
theorem 123
dene a function g ( u , v ) on positive vectors u and v by
g ( u , v ) = 123
vi v j
123 + log
ui u j vi v j
bi ui .
then g ( u , v ) is an auxiliary function for the function f ( v ) = 123 satisfying f ( u ) g ( u , v ) and f ( v ) = g ( v , v ) .
123 vtav + btv ,
as explained previously , a new estimate that improves the objective function f ( v ) at its current estimate v is obtained by minimizing the aux - iliary function g ( u , v ) with respect to its rst argument u , as shown by
theorem 123
given a positive vector v and a mapping v ( cid : 123 ) = arg minu g ( u , v ) , we have
( cid : 123 ) = m ( v ) such that
) f ( v ) .
( cid : 123 ) ( cid : 123 ) = v , then the inequality holds strictly .
therefore , the objective moreover , if v function is strictly decreased unless at the xed point of the mapping m ( v ) , where v = m ( v ) .
the mapping m ( v ) takes the form of equation 123 if v is constrained only to be nonnegative and takes the form of equation 123 if v is box - constrained .
the inequality in equation 123 is a direct result from the deni - tion of the auxiliary function and its relation to the objective function .
the derivation in equation 123 is reproduced here for easy reference :
( cid : 123 ) , v ) g ( v , v ) = f ( v ) .
sha , y .
lin , l .
saul , and d
to show that the objective function is strictly decreased if the new esti - is not the same as the old estimate v , we must also show that the ( cid : 123 ) , v ) < g ( v , v ) .
auxiliary function is strictly decreased : if v this can be proved by further examining the properties of the auxiliary
( cid : 123 ) ( cid : 123 ) = v , then g ( v
we begin by showing that g ( u , v ) is the sum of strictly convex functions of u .
for a strictly convex function , the minimizer is unique , and the min - imum is strictly less than any other values of the function .
we reorganize the expression of the auxiliary function g ( u , v ) given by equation 123 such that there are no interaction terms among the variables ui :
bi ui 123
vi v j .
g ( u , v ) = 123
we identify the auxiliary function with g ( u , v ) = ( cid : 123 )
v ) i vi log
where gi ( ui ) is a single - variable function of ui :
gi ( ui ) = 123
v ) i vi log
+ bi ui .
i gi ( ui ) 123
note that the minimizer of g ( u , v ) can be easily found by minimizing each = arg minui gi ( ui ) .
moreover , we will show that gi ( ui ) gi ( ui ) separately : v ( cid : 123 ) is strictly convex in ui .
to see this , we examine its second derivative with respect to ui :
i ( ui ) = ( a
v ) i and ( a
for a positive vector v , ( a v ) i cannot be simultaneously equal to zero .
otherwise , the ith row of a is all - zero , contradicting our assumption that a is strictly convex .
this implies that g i ( ui ) is strictly positive and gi ( ui ) is strictly convex in ui .
theorem 123 follows directly from the above observation .
in particular , if vi is not a minimizer of gi ( ui ) , then v ( cid : 123 ) i ) < gi ( ui ) .
since the auxiliary function g ( u , v ) is the sum of all the individual terms gi ( ui ) plus ( cid : 123 ) , v ) is strictly less than a term independent of u , we have shown that g ( v g ( v , v ) if v
( cid : 123 ) ( cid : 123 ) = v .
this leads to f ( v
( cid : 123 ) = vi and gi ( v ( cid : 123 )
) < f ( v ) .
as explained previously , the minimizer v
can be computed by nding the minimizer of each individual term gi ( ui ) .
computing the derivative of gi ( ui ) with respect to ui , setting it to zero , and solving for ui lead to the multiplicative updates in equation 123 .
minimizing gi ( ui ) subject to box constraints ui ( 123 , ( cid : 123 ) i ) leads to the clipped multiplicative updates in
multiplicative updates for nonnegative quadratic programming
123 global convergence .
the multiplicative updates dene a mapping m from the current estimate v of the minimizer to a new estimate v .
by it - eration , the updates generate a sequence of estimates ( v123 , v123 , .
. ) , satisfying vk+123=m ( vk ) .
the sequence monotonically improves the objective function f ( v ) .
since the sequence ( f ( v123 ) , f ( v123 ) , .
, ) is monotonically decreasing and is bounded below by the global minimum value of f ( v ) , the sequence converges to some value when k is taken to the limit of innity .
while establishing monotonic convergence of the sequence , however , the above observation does not rule out the possibility that the sequence converges to spurious xed points of the iterative procedure vk+123=m ( vk ) that are not the global minimizer of the objective function .
in this section , we prove that the multiplicative updates do indeed converge to the global minimizer and attain the global minimum of the objective function .
( the technical details of this section are not necessary for understanding how to derive or implement the multiplicative updates . )
123 . 123 outline of the proof .
our proof relies on a detailed investigation of the xed points of the mapping m dened by the multiplicative updates .
in what follows , we distinguish between the spurious xed points of m that violate the kkt conditions versus the unique xed point of m that satises the kkt conditions and attains the global minimum value of f ( v ) .
the basic idea of the proof is to rule out both the possibility that the multiplicative updates converge to a spurious xed point , as well as the possibility that they lead to oscillations among two or more xed points .
our proof consists of three stages .
first , we show that any accumulation point of the sequence ( v123 , v123 , .
. ) must be a xed point of the multiplica - tive updateseither a spurious xed point or the global minimizer .
such a result is considerably weaker than global convergence to the minimizer .
second , we show that there do not exist convergent subsequences s of the mapping m with spurious xed points as accumulation points .
in par - ticular , we show that if such a sequence s converges to a spurious xed point , then it must have a subsequence converging to a different xed point , yielding a contradiction .
therefore , the accumulation point of any conver - gent subsequence must be the global minimizer .
third , we strengthen the result on subsequence convergence and show that the sequence ( v123 , v123 , .
. ) converges to the global minimizer .
our proof starts from zangwills convergence theorem ( zangwill , 123 ) , a well - known convergence result for general iterative methods , but our nal result does not follow simply from this general framework .
we re - view zangwills convergence theorem in appendix a .
the application of this theorem in our setting yields the weaker result in the rst step of our proof : convergence to a xed point of the multiplicative updates .
as ex - plained in the appendix , however , zangwills convergence theorem does not exclude the possibility of convergence to spurious xed points .
we de - rive our stronger result of global convergence by exploiting the particular
sha , y .
lin , l .
saul , and d
structure of the objective function and the multiplicative update rules for nqp .
a key step ( see lemma 123 ) in our proof is to analyze the mapping m on sequences that are in the vicinity of spurious xed points .
our analysis appeals repeatedly to the specic properties of the objective function and the mapping induced by the multiplicative updates .
the following notation and preliminary observations will be useful .
we 123 to denote the corresponding sequence ( f ( v123 ) , f ( v123 ) , .
, f ( vk ) , .
we assume that the matrix a in equation 123 is strictly positive denite so that the objective function has a unique global minimum .
from this , it also follows that the in equation 123 has strictly positive elements along the diagonal .
123 to denote the sequence ( v123 , v123 , .
, vk , .
. ) and ( f ( vk ) )
123 . 123 positivity .
our proof will repeatedly invoke the observation that for a strictly positive vector v , the multiplicative updates in equation 123 ( cid : 123 ) = m ( v ) .
there is one exception to this yield a strictly positive vector v rule , which we address here .
starting from a strictly positive vector v , the = 123 directly to zero in the case multiplicative updates will set an element v ( cid : 123 ) that bi 123 and the ith row of the matrix a has no negative elements .
it is easy to verify in this case , however , that the global minimizer v = 123
once an element
objective function does have a zero element at v in zeroed by the multiplicative updates , it remains zero under successive updates .
in effect , when this happens , the original problem in nqp reduces to a smaller problemof dimensionality equal to the number of nontrivial modes in the original system .
without loss of generality , therefore , we will assume in what follows that any trivial degrees of freedom have already been removed from the problem .
more specically , we will assume that the ith row of the matrix a has one or more negative elements whenever bi 123 and that consequently , a strictly positive vector v is always mapped to a strictly positive vector v
( cid : 123 ) = m ( v ) .
123 . 123 accumulation points of ( vk )
the following lemma is a direct result of zangwills convergence theorem , as reviewed in appendix a .
it estab - lishes the link between the accumulation points of ( vk ) 123 and the xed points
lemma 123
given a point v123 , suppose the update rule in equation 123 generates a 123 .
then either the algorithm terminates at a xed point of m or the accumulation point of any convergent subsequence in ( vk ) 123 is a xed point of m .
if there is a k 123 such that vk is a xed point of m , then the update rule terminates .
therefore , we consider the case that an innite sequence is generated and show how to apply zangwills convergence theorem .
let m be the update procedure in zangwills convergence theorem .
we rst verify that the sequence ( vk ) 123 generated by m is in a compact set .
multiplicative updates for nonnegative quadratic programming because ( f ( vk ) ) for all k ,
is a monotonically decreasing sequence , it follows that
vk = ( v|f ( v ) f ( v123 ) ) .
note that the set is compact because it denes an ellipsoid conned to the positive orthant .
we dene the desired set s to be the collection of all the xed points of m .
if v / s , then from theorem 123 , we have that f ( m ( v ) ) < f ( v ) .
on the other hand , if v s , then we have that f ( m ( v ) ) = f ( v ) .
this shows that the mapping m maintains strict monotonicity of the objective function outside the desired set .
the last condition to verify is that m is closed at v if v is not in the desired set .
note that m is continuous if v ( cid : 123 ) = 123
therefore , if the origin v = 123 is a xed point of m , then m is closed outside the desired set .
if the origin is not a xed point of m , then it cannot be the global minimizer .
moreover , we can choose the initial estimate v123 such that f ( v123 ) < f ( 123 ) .
with this choice , it follows from the monotonicity of m that the origin is not contained in and that m is continuous on .
either way , we have shown that m is closed on a proper domain .
there - fore , we can apply zangwills convergence theorem to the mapping m restricted on : the limit of any convergent subsequence in ( vk ) 123 is in the desired set or equivalently , a xed point of m .
remark .
it is easy to check whether the global minimizer occurs at the origin with value f ( 123 ) =123
in particular , if all the elements of b are nonneg - ative , then the origin is the global minimizer .
on the other hand , if there is a nonnegative element of b , then we can choose the initial estimate v123 such that f ( v123 ) < f ( 123 ) .
for example , suppose bk < 123
then we can choose v123 such that its kth element is and all other elements are .
a positive and can be found such that f ( v123 ) < 123 by noting
aij 123 +
|aij| 123 +
f ( v123 ) = 123
bi + 123
akk 123 + bk
akk + bk
note that if we choose a positive < 123bk / akk , we can always nd a positive such that f ( v123 ) < 123 because the right - most term of the inequality in equation 123 is negative and the left and middle terms vanish as 123
sha , y .
lin , l .
saul , and d
figure 123 : fixed points of the multiplicative updates : the global minimizer v , indicated by squares .
indicated by a star and spurious xed points v and v contour lines of the objective function are shown as ellipses .
a hypothetical 123 with a subsequence converging to the spurious xed point v is represented by solid lines connecting small black circles .
the - ball around the spurious xed point v does not intersect the ( cid : 123 ) - ball around the other spurious xed point v
lemma 123 states that any convergent subsequence in ( vk )
123 . 123 properties of the fixed points .
as stated in section 123 , the minimizer of f ( v ) satises the kkt conditions and corresponds to a xed point of the mapping m dened by the multiplicative update rule in equation 123 .
the mapping m , however , also has xed points that do not satisfy the kkt conditions .
we refer to these as spurious xed points .
123 must have a xed point of m as its accumulation point .
to prove that the multiplicative updates converge to the global minimizer , we will show that spurious xed points cannot be accumulation points .
our strategy is to demonstrate that any subsequence s converging to a spurious xed point must itself have a subsequence running away from the xed point .
the idea of the proof is shown schematically in figure 123
the star in the center of the gure denotes the global minimizer v .
black squares denote spurious xed points v and v the gure also shows a hypothetical subsequence that converges to the spurious xed point v .
multiplicative updates for nonnegative quadratic programming
at a high level , the proof ( by contradiction ) is as follows .
suppose that there exists a convergent subsequence as shown in figure 123
then we can draw a very small - ball around the spurious xed point v containing an innite number of elements of the subsequence .
we will show that under the mapping m , the subsequence must have an innite number of successors that are outside the - ball yet inside a - ball where > .
this bounded successor sequence must have a subsequence converging to an accumulation point , which by lemma 123 must also be a xed point .
however , we can choose the - ball to be sufciently small such that the annulus between the - ball and - ball contains no other xed points .
this yields a contradiction .
more formally , we begin by proving the following lemma :
lemma 123
let v123 denote a positive initial vector satisfying f ( v123 ) < f ( 123 ) .
sup - pose that the sequence vk+123 = m ( vk ) generated by the iterative update in equation 123 has a subsequence that converges to the spurious xed point v .
then there exists an > 123 and a > 123 such that for every v ( cid : 123 ) = v such that ( cid : 123 ) v v ( cid : 123 ) < , there exists an integer p 123 such that < ( cid : 123 ) mp ( v ) v ( cid : 123 ) < , where mp ( v ) is p times composition of m applied to v : m m m
( cid : 123 ) = 123 or vi = 123 for any i .
if the former proof .
if v is a xed point , then either vi is true , as shown in section 123 , it follows that ( f / vi ) |v=123
when vi = 123 , then either ( f / vi ) |v123 or ( f / vi ) |v <123
if v is a spurious xed point that violates the kkt conditions , then there exists at least one i such that
vi = 123 and
let v be a small ball centered at v with radius of : v = ( v| ( cid : 123 ) v v ( cid : 123 ) < ) .
by continuity , there exists an such that ( f / vi ) <123 for all v v .
let be the image of v under the mapping m .
since m is a continuous mapping , we can nd a minimum ball v = ( v| ( cid : 123 ) v v ( cid : 123 ) < ) to encircle .
we claim that the and satisfy the lemma .
as observed in section 123 , the multiplicative update increases vi if f / vi is negative .
consider the sequence ( m ( v ) , m m ( v ) , .
, mk ( v ) , .
the ith component of the sequence is monotonically increasing until the con - dition ( f / vi ) becomes nonnegative .
this happens only if the element of the sequence is outside the v ball .
thus , for every v v , the update will push vi to larger and larger values until it escapes from the ball .
let p be the smallest integer such that
mp123 ( v ) v and mp ( v ) / v
sha , y .
lin , l .
saul , and d .
lee by construction of the v ball , mp is inside the ball since is the image of the v under the mapping m and mp123 ( v ) v .
therefore , mp ( v ) v .
the size of the - ball depends on the spurious xed point v and .
can v contain another spurious xed point v ? the following lemma shows that we can choose sufciently small such that the ball v contains no other
if v and v
be the radii for v
are two different spurious xed points , let and be the .
there exists an ) 123 and v v ( cid : 123 ) is empty .
radii for v such that lemma 123 holds and ( cid : 123 ) 123 > 123 such that max ( , ( cid : 123 ) proof .
it sufces to show that m ( v ) becomes arbitrarily close to v as v approaches v .
since m is a bounded and continuous mapping , the image of v under m becomes arbitrarily small as 123
note that v is a xed point of m , and we have v .
therefore , the - ball centered at v can be made arbitrarily small as v approaches v .
and choose ( cid : 123 )
let us choose sufciently small such that is less than the half of the likewise .
then the intersection of
distance between v and v v and v ( cid : 123 ) is empty .
this is illustrated in figure 123
because the spurious xed points are separated by their - balls , we will show that the existence of a subsequence converging to a spurious xed point leads to a contradiction .
this observation leads to the following
subsequence of ( vk ) as an accumulation point .
if the matrix a is strictly positive denite , then no convergent 123 can have a spurious xed point of the multiplicative updates
suppose there is a convergent subsequence ( vk ) ( vk )
the number of spurious xed points is nite and bounded above by the number of ways of choosing zero elements of v .
let > 123 be the minimum pairwise distance between these xed points .
by lemma 123 , we can choose a small such that the radius of the - ball for any spurious xed point is less than / 123
with this choice , the - balls at different spurious xed points are nonoverlapping .
123 such that vk v with k k , where k is an index subset and v is a spurious xed point .
without loss of generality , we assume the whole subsequence is contained in the - ball of v .
for each element vk of the subsequence , by lemma 123 , there exists an integer pk such that mpk ( vk ) is outside the - ball yet inside the - ball .
con - sider the successor sequence mpk ( vk ) with k k , schematically shown in figure 123 as the black circles between the - ball and the - ball .
the innite successor sequence is bounded between the - ball and - ball and therefore
multiplicative updates for nonnegative quadratic programming
must have a convergent subsequence .
by lemma 123 , the accumulation point of this subsequence must be a xed point of m .
however , this leads to contradiction .
on one hand , the subsequence is outside the - ball of v so v is not the accumulation point .
on the other hand , the subsequence is inside the - ball of v : therefore , it cannot have any other spurious xed point v its accumulation point , because we have shown that all pairs of xed points are separated by their respective - balls .
therefore , the accumulation point of the subsequence cannot be a xed point .
thus , we arrive at a contradic - tion , showing that spurious xed points cannot be accumulation points of any convergent subsequence .
123 . 123 convergence to the global minimizer .
we have shown that the only possible accumulation point of ( vk ) 123 is the global minimizer : the xed point of m that satises the kkt conditions .
we now show that the sequence
123 itself does indeed converge to the global minimizer .
= f ( v
, and ( f ( vk ) )
theorem 123
suppose that the origin is not the global minimizer and that we choose a positive initial vector v123 such that f ( v123 ) < 123
then the sequence ( vk ) converges to the global minimizer v converges to the optimal proof .
as shown in equation 123 , the innite sequence ( vk ) 123 is a bounded set; therefore , it must have an accumulation point .
by the preceding theo - rem , the accumulation point of any convergent subsequence of ( vk ) be a spurious xed point; thus , any convergent subsequence must converge to the xed point that satises the kkt conditions : the global minimizer .
by monotonicity , it immediately follows that ( f ( vk ) ) 123 converges to the optimal of the objective function .
< f ( v ) for all spurious xed points v , we can nd an > 123
such that the set
= ( v|f ( v ) f
contains no spurious xed points ofm .
moreover , since ( f ( vk ) ) tonically decreasing sequence converging to f
we now prove the theorem by contradiction .
suppose ( vk )
for all k k123
123 is a mono - , there exists a k123 such that
123 does not .
then there exists an > 123 such that the set
converge to the minimizer v ( cid : 123 ) > )
= ( vk : ( cid : 123 ) vk v
has an innite number of elements .
in other words , there must be a sub - sequence of ( vk ) in which every element has distance at least from the minimizer .
moreover , the intersection of and must have an innite number of elements .
note that by construction , contains no xed points
sha , y .
lin , l .
saul , and d
figure 123 : estimating time delay from signals and their echos in reverberant
other than the global minimizer , and does not contain the global min - imizer .
thus , there are no xed points in .
the innite set however , is bounded , and therefore must have an accumulation point; by lemma 123 , this accumulation point must be a xed point .
this yields a con - tradiction .
hence , the set cannot have an innite number of elements , and the sequence ( vk )
123 must converge to the global minimizer .
in this section , we sketch two real - world applications of the multiplicative updates to problems in signal processing and pattern recognition .
recently , the updates have also been applied by astrophysicists to estimate the mass distribution of a gravitational lens and the positions of the sources from combined strong and weak lensing data ( diego , tegmark , protopapas , &
123 acoustic time delay estimation .
in a reverberant acoustic envi - ronment , microphone recordings capture echos reected from objects such as walls and furniture in addition to the signals that propagate directly from a sound source .
as illustrated in figure 123 , the signal at the microphone x ( t ) can be modeled as a linear combination of the source signal s ( t ) at different delay times : s ( t t ) , s ( t t ) .
given a received signal x ( t )
) and s ( t t
multiplicative updates for nonnegative quadratic programming
and its source signal s ( t ) , how can we identify in x ( t ) all the time - delayed components of s ( t ) ? to this end , we consider the model ( lin et al . , 123 )
i s ( t ti ) with i 123 ,
x ( t ) = n ( cid : 123 )
in which ( ti ) n i=123 are all possible time delays ( discretized to some nite resolution ) and ( i ) are the relative amplitudes ( or attenuations ) of the time - delayed components .
the nonnegativity constraints in equation 123 incorporate the assumption that only the amplitudes of acoustic waves are affected by reections while the phases are retained ( allen & berkley , 123 ) .
within this model , the time - delayed components of s ( t ) can be identied by computing the amplitudes i that best reconstruct x ( t ) .
the reconstruction with least - squares error is obtained from the nonnegative deconvolution :
i s ( t ti ) |123
= arg min
in the least - squares reconstruction are interpreted as indicating time - delayed components in the received signal with delays ti .
it is convenient to rewrite this optimization in the frequency domain .
let x ( f ) and s ( f ) denote the fourier transforms of x ( t ) and s ( t ) , respec - tively .
also , dene the positive semidenite matrix kij and the real - valued coefcients ci by
|s ( f ) |123e j123 f ( tjti ) ,
( f ) x ( f ) e j123 f ti ,
where the sums are over positive and negative frequencies .
in terms of the matrix kij and coefcients ci , the optimization in equation 123 can be
subject to i 123
i kij j ( cid : 123 )
i ci i
this has the same form as the nqp problem in equation 123 and can be solved by the multiplicative updates in equation 123 .
note that kij denes a toeplitz matrix if the possible time delays ti are linearly spaced .
using fast fourier transforms , the toeplitz structure of kij can be exploited for much faster matrix - vector operations per multiplicative update .
sha , y .
lin , l .
saul , and d
time delay ( t
time delay ( t
time delay ( t
figure 123 : convergence of the multiplicative updates for acoustic time delay
figure 123 shows the convergence of the multiplicative updates for a prob - lem in acoustic time delay estimation .
the source signal s ( t ) in this example was a 123 ms window of speech , and the received signal x ( t ) was given by
x ( t ) = s ( t ts ) + 123 s ( t 123ts ) ,
where ts was the sampling period .
the vertical axes measure the estimated amplitudes i after different numbers of iterations of the multiplicative updates; the horizontal axes measure the time delays ti in the units of ts .
the vertical dashed lines indicate the delays at ts and 123ts .
the gure shows that as the number of iterations is increased , the time delays are accurately predicted by the peaks in the estimated amplitudes i .
123 large margin classication .
large margin classiers have been applied successfully to many problems in machine learning and statistical pattern recognition ( cristianini & shawe - taylor , 123; vapnik , 123 ) .
these classiers use hyperplanes as decision boundaries to separate positively and negatively labeled examples represented by multidimensional vectors .
generally the hyperplanes are chosen to maximize the minimum distance ( known as the margin ) from any labeled example to the decision boundary ( see figure 123 ) .
let ( ( xi , yi ) ) n i=123 denote a data set of n labeled training examples with binary class labels yi = 123
the simplest case , shown in figure 123 ( left ) ,
multiplicative updates for nonnegative quadratic programming
figure 123 : large margin classiers .
positively and negatively labeled examples are indicated by lled and hollowed circles , respectively .
( left ) a linearly sep - arable data set .
the support vectors for the maximum margin hyperplane h parallel to the decision boundary .
large lie on two hyperplanes h margin classiers maximize the distance between these hyperplanes .
( right ) a linearly inseparable data set .
the support vectors in this case also include examples lying between h
that cannot be classied correctly .
is that the two classes are linearly separable by a hyperplane that passes through the origin .
let w denote the hyperplanes normal vector; the clas - sication rule , given by y = sgn ( w tx ) , labels examples based on whether they lie above or below the hyperplane .
the maximum margin hyperplane is computed by solving the constrained optimization :
yi wtxi 123 , i = 123 , 123 ,
the constraints in this optimization ensure that all the training examples are correctly labeled by the classiers decision rule .
while a potentially innite number of hyperplanes satisfy these constraints , the classier with mini - mal||w|| ( and thus maximal margin ) has provably small error rates ( vapnik , 123 ) on unseen examples .
the optimization problem in equation 123 is a convex quadratic programming problem in the vector w .
its dual formula -
i 123 , i = 123 , 123 ,
i x j ( cid : 123 )
i j yi yj xt
let a denote the positive semidenite matrix with elements yi yj xt i x j , and let e denote the column vector of all ones .
the objective function for the dual in equation 123 can then be written as the nqp problem :
l ( ) = 123
123 123 123 123 123 123
sha , y .
lin , l .
saul , and d
123 123 123 123 123 123
figure 123 : convergence of the multiplicative updates in equation 123 for a large margin classier distinguishing handwritten digits ( 123s versus 123s ) .
the coef - cients corresponding to nonsupport vectors are quickly attenuated to zero .
and the multiplicative updates in equation 123 can be used to nd the .
labeled examples that correspond to active constraints in equa - tion 123 are called support vectors .
the normal vector w determined by support vectors since the solution to the primal problem , equation 123 , is given by
i xi .
for nonsupport vectors , the inequalities are strictly satised in equation 123 , and their corresponding lagrange multipliers vanish ( that is ,
figure 123 illustrates the convergence of the multiplicative updates for large margin classication of handwritten digits ( scholkopf et al . , 123 ) .
the plots show the estimated support vector coefcients i after different numbers of iterations of the multiplicative updates .
the horizontal axes in these plots index the coefcients i of the n = 123 training examples , and the vertical axes show their values .
for ease of visualization , the training examples were ordered so that support vectors appear to the left and non - support vectors to the right .
the coefcients were uniformly initialized as i = 123
note that the nonsupport vector coefcients are quickly attenuated
multiplicative updates can also be used to train large margin classiers when the labeled examples are not linearly separable , as shown in figure 123 ( right ) .
in this case , the constraints in equation 123 cannot all be simultane - ously satised , and some of them must be relaxed .
one simple relaxation
multiplicative updates for nonnegative quadratic programming
is to permit some slack in the constraints but to penalize the degree of slack as measured by the ( cid : 123 ) 123 - norm : minimize wtw + c
yi wtxi 123 i
the parameter c balances the slackness penalty versus the large margin criterion; the resulting classiers are known as soft margin classiers .
the dual of this optimization is an nqp problem with the same quadratic form as the linearly separable case but with box constraints :
subject to 123 i c , i = 123 , 123 ,
the clipped multiplicative updates in equation 123 can be used to perform this optimization for soft margin classiers .
another way of handling linear inseparability is to embed the data in a high - dimensional nonlinear feature space and then to construct the maxi - mum margin hyperplane in feature space .
the nonlinear mapping is per - formed implicitly by specifying a kernel function that computes the inner product in feature space .
the optimization for the maximum margin hy - perplane in feature space has the same form as equation 123 except that the original gram matrix with elements xt i x j is replaced by the kernel ma - trix of inner products in feature space ( cristianini & shawe - taylor , 123; vapnik , 123 ) .
the use of multiplicative updates for large margin classi - cation of linearly inseparable data sets is discussed further in ( sha et al . ,
a large number of algorithms have been investigated for nonnegative quadratic programming in svms .
among them , there are many criteria that could be compared , such as speed of convergence , memory requirements , and ease of implementation .
the main utility of the multiplicative updates appears to lie in their ease of implementation .
the updates are very well suited for applications involving small to moderately sized data sets , where computation time is not a primary concern and where the simple , parallel form of the updates makes them easy to implement in high - level languages such as matlab .
the most popular methods for training svmsso - called subset methodstake a fundamentally different approach to nqp .
in contrast to the parallel form of the multiplicative updates , subset methods split the variables at each iteration into two sets : a xed set in which the variables are held constant and a working set in which the variables are optimized
sha , y .
lin , l .
saul , and d
by an internal subroutine .
at the end of each iteration , a heuristic is used to transfer variables between the two sets and improve the objective function .
two subset methods have been widely used for training svms .
the rst is the method of sequential minimal optimization ( smo ) ( platt , 123 ) , which updates only two coefcients of the weight vector per iteration .
in this case , there exists an analytical solution for the updates , so that one avoids the expense of an iterative optimization within each iteration of the main loop .
smo enforces the sum and box constraints for soft margin classiers .
if the sum constraint is lifted , then it is possible to update the coefcients of the weight vector sequentially , one at a time , with an adaptive learning rate that ensures monotonic convergence .
this coordinate descent approach is also known as the kernel adatron ( friess , cristianini , & campbell , 123 ) .
smo and kernel adatron are among the most viable methods for training svms on large data sets , and experiments have shown that they converge much faster than the multiplicative updates ( sha et al . , 123b ) .
nevertheless , for simplicity and ease of implementation , we believe that the multiplicative updates provide an attractive starting point for experimenting with large
123 summary and discussion
in this article , we have described multiplicative updates for solving convex problems in nqp .
the updates are distinguished by their simplicity in both form and computation .
we showed that the updates lead to monotonic improvement in the objective function for nqp and converge to global minima .
the updates can be viewed as generalizations of the iterative rules previously developed for nonnegative matrix factorization ( lee & seung , 123 ) .
they have a strikingly different form from other additive and multi - plicative updates used in statistical learning .
we can also compare the multiplicative updates to interior point methods ( wright , 123 ) that have been studied for nqp .
these methods start from an interior point of the feasible region and then iteratively update the current estimate of the minimizer along particular search directions .
there are many ways to determine the search directionsfor example , using newtons method to solve the equations characterizing primal and dual optimality , or approximating the original optimization problem by a simpler subproblem inside the trust region of the current estimate .
the resulting updates take an elementwise additive form , stepping along a particular search direction in the nonnegative orthant .
the step size is chosen to ensure that the search remains in the feasible region while making progress toward the minimizer .
if the search direction corresponds to the negative gradient of the objective function f ( v ) , then the updates reduce to steepest descent .
most of the computational effort in interior point methods is devoted to deriving search directions and maintaining the feasibility of the updated estimates .
multiplicative updates for nonnegative quadratic programming
the multiplicative updates are similar to trust region methods in spirit .
instead of constructing an ellipsoidal trust region centered at the current estimate of the minimizer , however , we have derived the updates from a nonlinear yet analytically tractable auxiliary function .
optimizing the auxiliary function guarantees the improvement of the objective function in the nonnegative orthant , which can be viewed as the trust region for each update .
the search direction derived from the auxiliary function is very simple to compute , as opposed to that of many interior point methods .
it remains an open question to quantify more precisely the rate of convergence of the multiplicative updates .
though not as well theoretically characterized as traditional methods for nqp , the multiplicative updates have nevertheless proven extremely useful in practice .
in this article , we have described our own use of the updates for acoustic echo cancellation and large margin classication .
meanwhile , others have applied the updates to nqp problems that arise in the analysis of astrophysical data ( diego et al . , 123 ) .
we are hopeful that more appli - cations will continue to emerge in other areas of neural computation and
appendix : zangwills convergence theorem
zangwills convergence theorem enumerates the conditions for global con - vergence of general iterative procedures .
the theorem is presented in its most general form in zangwill ( 123 ) .
for our purposes here , we speci - cally state the theorem in the context of optimization .
let f ( v ) denote the objective function to be minimized by an iterative update rule vk+123 = m ( vk ) , where the domain and range of the mapping m : v v lie in the feasible set .
suppose that we apply the mapping m to generate a sequence of pa - k=123
our goal is to examine whether this sequence converges to a point in a desired set v v .
assume that the objective function f ( v ) and the mapping m are continuous and that the following conditions are
all points vk are in a compact set that is a subset of v .
if v / v 123
if v v
, then the update leads to a strict reduction in the objective , then either m ( v ) = v or f ( m ( v ) ) f ( v ) .
function .
that is , f ( m ( v ) ) < f ( v ) .
zangwills convergence theorem states that under these conditions , either the sequence ( vk ) , or all accumulation points of the sequence are in the set v
k=123 stops at a point in the set v
the theorem can be used to analyze the convergence of iterative up - date rules by verifying that these three conditions hold for particular de - sired sets .
for the multiplicative updates in equation 123 , the theorem implies convergence to a xed point v = m ( v ) .
it does not , however , imply
sha , y .
lin , l .
saul , and d
convergence to the unique xed point that is the global minimizer of the objective function .
in particular , if we constrain v to contain only the global minimizer , then condition ( 123 ) , which stipulates that f ( v ) strictly decreases under the mapping m for all v / v , is clearly violated due to the existence of spurious xed points .
the proof of global convergence thus requires the extra machinery of section 123 .
this work was supported by nsf award 123
