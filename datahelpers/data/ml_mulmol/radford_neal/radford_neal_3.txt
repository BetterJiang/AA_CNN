abstract .
simulated annealing moving from a tractable distribution to a distribu - tion of interest via a sequence of intermediate distributions has traditionally been used as an inexact method of handling isolated modes in markov chain samplers .
here , it is shown how one can use the markov chain transitions for such an annealing sequence to dene an importance sampler .
the markov chain aspect allows this method to per - form acceptably even for high - dimensional problems , where nding good importance sampling distributions would otherwise be very dicult , while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases .
this annealed importance sampling procedure resembles the second half of the previously - studied tempered transitions , and can be seen as a generalization of a recently - proposed variant of sequential importance sampling .
it is also related to thermodynamic integration methods for estimating ratios of normalizing constants .
annealed importance sampling is most attractive when isolated modes are present , or when estimates of normalizing constants are required , but it may also be more generally useful , since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in markov chain samplers .
in bayesian statistics and statistical physics , expectations of various quantities with respect to complex distributions must often be computed .
for simple distributions , we can estimate expectations by sample averages based on points drawn independently from the distribution of interest .
this simple monte carlo approach cannot be used when the distribution is too complex to allow easy generation of independent points .
we might instead generate independent points from some simpler approximating distribution , and then use an importance sampling estimate , in which the points are weighted to compen - sate for use of the wrong distribution .
alternatively , we could use a sample of dependent points obtained by simulating a markov chain that converges to the correct distribution .
i show in this paper how these two approaches can be combined , by using an importance sampling distribution dened by a series of markov chains .
this method is inspired by the idea of annealing as a way of coping with isolated modes , which leads me to call it annealed importance sampling .
the method is especially suitable when multimodality may be a problem , but may be attractive even when it is not , since it allows one to bypass some of the problems of convergence assessment .
annealed importance sampling also supplies an estimate for the normalizing constant of the distribution sampled from .
in statistical physics , minus the log of the normalizing constant for a canonical distribution is known as the free energy , and its estimation is a long - standing problem .
in independent work , jarzynski ( 123a , b ) has described a method primarily aimed at free energy estimation that is essentially the same as the annealed importance sampling method described here .
i will focus instead on statistical applications , and will discuss use of the method for estimating expectations of functions of state , as well as the normalizing constant .
importance sampling works as follows ( see , for example , geweke 123 ) .
suppose that we are interested in a distribution for some quantity , x , with probabilities or probability densities that are proportional to the function f ( x ) .
suppose also that computing f ( x ) for any x is feasible , but that we are not able to directly sample from the distribution it denes .
however , we are able to sample from some other distribution that approximates the one dened by f ( x ) , whose probabilities or probability densities are proportional to the function g ( x ) , which we are also able to evaluate .
we base our estimates on a sample of n independent points , x ( 123 ) , .
, x ( n ) , generated from the distribution dened by g ( x ) .
for each x ( i ) , we compute an importance weight
w ( i ) = f ( x ( i ) ) / g ( x ( i ) )
we can then estimate the expectation of a ( x ) with respect to the distribution dened by f ( x ) by
provided g ( x ) 123= 123 whenever f ( x ) 123= 123 , it is easy to see that n 123p w ( i ) will converge as n to zf / zg , where zf = r f ( x ) dx and zg = r g ( x ) dx are the normalizing
constants for f ( x ) and g ( x ) .
one can also see that a will converge to the expectation of a ( x ) with respect to the distribution dened by f ( x ) .
the accuracy of a depends on the variability of the importance weights .
when these weights vary widely , the estimate will eectively be based on only the few points with the largest weights .
for importance sampling to work well , the distribution dened by g ( x ) must therefore be a fairly good approximation to that dened by f ( x ) , so that the ratio f ( x ) / g ( x ) does not vary wildly .
when x is high - dimensional , and f ( x ) is complex , and perhaps multimodal , nding a good importance sampling distribution can be very dicult , limiting the applicability of the method .
an alternative is to obtain a sample of dependent points by simulating a markov chain that converges to the distribution of interest , as in the metropolis - hastings algorithm ( metropolis , et al 123; hastings 123 ) .
such markov chain methods have long been used
in statistical physics , and are now widely applied to statistical problems , as illustrated by the papers in the book edited by gilks , richardson , and spiegelhalter ( 123 ) .
markov chains used to sample from complex distributions must usually proceed by making only small changes to the state variables .
this causes problems when the dis - tribution contains several widely - separated modes , which are nearly isolated from each other with respect to these transitions .
because such a chain will move between modes only rarely , it will take a long time to reach equilibrium , and will exhibit high autocor - relations for functions of the state variables out to long time lags .
the method of simulated annealing was introduced by kirkpatrick , gelatt , and vecchi ( 123 ) as a way of handling multiple modes in an optimization context .
it employs a sequence of distributions , with probabilities or probability densities given by p123 ( x ) to pn ( x ) , in which each pj diers only slightly from pj+123
the distribution p123 is the one of interest .
the distribution pn is designed so that the markov chain used to sample from it allows movement between all regions of the state space .
a traditional scheme is to set pj ( x ) p123 ( x ) j , for 123 = 123 > 123 > > n .
an annealing run is started at some initial state , from which we rst simulate a markov chain designed to converge to pn , for some number of iterations , which are not necessarily enough to actually approach equilibrium .
we next simulate some number of iterations of a markov chain designed to converge to pn123 , starting from the nal state of the previous simulation .
we continue in this fashion , using the nal state of the simulation for pj as the initial state of the simulation for pj123 , until we nally simulate the chain designed to converge to p123
we hope that the distribution of the nal state produced by this process is close to p123
note that if p123 contains isolated modes , simply simulating the markov chain designed to converge to p123 starting from some arbitrary point could give very poor results , as it might become stuck in whatever mode is closest to the starting point , even if that mode has little of the total probability mass .
the annealing process is a heuristic for avoiding this , by taking advantage of the freer movement possible under the other distributions , while gradually approaching the desired p123
unfortunately , there is no reason to think that annealing will give the precisely correct result , in which each mode of p123 is found with exactly the right probability .
this is of little consequence in an optimization context , where the nal distribution is degenerate ( at the maximum ) , but it is a serious aw for the many applications in statistics and statistical physics that require a sample from a
the annealed importance sampling method i present in this paper is essentially a way of assigning weights to the states found by multiple simulated annealing runs , so as to produce estimates that converge to the correct value as the number of runs increases .
this is done by viewing the annealing process as dening an importance sampling dis - tribution , as explained below in section 123
after discussing the accuracy of importance sampling in general in section 123 , i analyse the eciency of annealed importance sampling in section 123 , and nd that good results can be obtained by using a sucient number of interpolating distributions , provided that these vary smoothly .
demonstrations on simple distributions in section 123 and on a statistical problem in section 123 conrm this .
annealed importance sampling is related to tempered transitions ( neal 123 ) , which are another way of modifying the annealing procedure so as to produce correct results .
as discussed in section 123 , annealed importance sampling will sometimes be preferable to using tempered transitions .
when tempered transitions are still used , the relationship to annealed importance sampling allows one to nd estimates for ratios of normalizing constants that were previously unavailable .
section 123 shows how one can also view a form of sequential importance sampling due to maceachern , clyde , and liu ( 123 ) as an instance of annealed importance sampling .
finally , in section 123 , i discuss the general utility of annealed importance sampling , as a way of handling multimodal distributions , as a way of calculating normalizing constants , and as a way of combining the adaptivity of markov chains with the advantages of independent sampling .
123 the annealed importance sampling procedure
suppose that we wish to nd the expectation of some function of x with respect to a distribution with probabilities or probability densities given by p123 ( x ) .
we have available a sequence of other distributions , given by p123 ( x ) up to pn ( x ) , which we hope will assist us in sampling from p123 , and which satisfy pj ( x ) 123= 123 wherever pj123 ( x ) 123= 123
for each distribution , we must be able to compute some function fj ( x ) that is proportional to pj ( x ) .
we must also have some method for sampling from pn , preferably one that produces independent points .
finally , for each i from 123 to n 123 , we must be able to simulate some markov chain transition , tj , that leaves pj invariant .
the sequence of distributions used can be specially constructed to suit the problem , but the following scheme may be generally useful .
we x f123 to give the distribution of interest , and x fn to give the simple distribution we can sample from , and then let
fj ( x ) = f123 ( x ) j fn ( x ) 123j
where 123 = 123 > 123 > .
> n = 123
note that the traditional simulated annealing scheme with fj ( x ) = f123 ( x ) j would usually be less suitable , since it usually leads to a pn for which independent sampling is not easy .
for applications in bayesian statistics , fn would be the prior density , which is often easy to sample from , and f123 would be the unnormalized posterior distribution ( the product of fn and the likelihood ) .
when only posterior expectations are of interest , neither the prior nor the likelihood need be normalized .
when the normalizing constant for the posterior ( the marginal likelihood ) is of interest , the likelihood must be properly normalized , but the prior need not be , as discussed below .
the markov chain transitions are represented by functions tj ( x , x ) giving the proba - bility or probability density of moving to x when the current state is x .
it will not be necessary to actually compute tj ( x , x ) , only to generate an x from a given x using tj .
these transitions may be constructed in any of the usual ways ( eg , metropolis or gibbs sampling updates ) , and may involve several scans or other iterations .
for the annealed importance sampling scheme to be valid , each tj must leave the corresponding pj invari - ant , but it is not essential that each tj produce an ergodic markov chain ( though this
would usually be desirable ) .
annealed importance sampling produces a sample of points , x ( 123 ) , .
, x ( n ) , and cor - responding weights , w ( 123 ) , .
, w ( n ) .
an estimate for the expectation of some function , a ( x ) , can then be found as in equation ( 123 ) .
to generate each point , x ( i ) , and associated weight , w ( i ) , we rst generate a sequence of points , xn123 , .
, x123 , as follows :
generate xn123 from pn .
generate xn123 from xn123 using tn123
generate x123 from x123 using t123
generate x123 from x123 using t123
we then let x ( i ) = x123 , and set
to avoid overow problems , it may be best to do the computations in terms of log ( w ( i ) ) .
to see that annealed importance sampling is valid , we can consider an extended state space , with points ( x123 , .
, xn123 ) .
we identify x123 with the original state , so that any function of the original state can be considered a function of the extended state , by just looking at only this component .
we dene the distribution for ( x123 , .
, xn123 ) by the following function proportional to the joint probability or probability density :
here , etj is the reversal of the transition dened by tj .
that is ,
f ( x123 , .
, xn123 ) = f123 ( x123 ) et123 ( x123 , x123 ) et123 ( x123 , x123 ) etn123 ( xn123 , xn123 ) etj ( x , x ) = tj ( x , x ) pj ( x ) / pj ( x ) = tj ( x , x ) fj ( x ) / fj ( x )
the invariance of pj with respect to tj ensures that these are valid transition probabili -
ties , for whichr etj ( x , x ) dx = 123
this in turn guarantees that the marginal distribution
for x123 in ( 123 ) is the same as the original distribution of interest ( since the joint probability there is the product of this marginal probability for x123 and the conditional probabilities for each of the later components given the earlier components ) .
for use below , we apply equation ( 123 ) to rewrite the function f as follows :
f ( x123 , .
, xn123 ) = f123 ( x123 )
f123 ( x123 ) et123 ( x123 , x123 )
fn123 ( xn123 ) etn123 ( xn123 , xn123 )
tn123 ( xn123 , xn123 ) fn123 ( xn123 ) ( 123 )
we now look at the joint distribution for ( x123 , .
, xn123 ) dened by the annealed im -
portance sampling procedure ( 123 ) .
it is proportional to the following function :
g ( x123 , .
, xn123 ) = fn ( xn123 ) tn123 ( xn123 , xn123 ) t123 ( x123 , x123 ) t123 ( x123 , x123 )
we regard this as an importance sampler for the distribution ( 123 ) on the extended state space .
the appropriate importance weights are found using equations ( 123 ) , ( 123 ) , and ( 123 ) .
dropping the superscript ( i ) on the right side to simplify notation , they are :
f ( x123 , .
, xn123 ) g ( x123 , .
, xn123 )
these weights are the same as those of equation ( 123 ) , showing that the annealed impor - tance sampling procedure is valid .
the above procedure produces a sample of single independent points x ( i ) for use in estimating expectations as in equation ( 123 ) .
in practice , better estimates will often be obtained if we use each such point as the initial state for a markov chain that leaves p123 invariant , which we simulate for some pre - determined number of iterations .
we can then estimate the expectation of a ( x ) by the weighted average ( using the w ( i ) ) of the simple average of a over the states of this markov chain .
this is valid because the expectation of a ( x ) with respect to p123 ( x ) is the same as the expectation with respect to p123 ( x ) of the average value of a along a markov chain that leaves p123 invariant and which is started in state x ( since if the start state has distribution p123 , all later states will also be from p123 ) .
annealed importance sampling also provides an estimate of the ratio of the normalizing constants for f123 and fn .
such normalizing constants are important in statistical physics and for statistical problems such as bayesian model comparison .
the normalizing con - stant for f , as dened by equation ( 123 ) , is the same as that for f123 , and the normalizing constant for g in equation ( 123 ) is the same as that for fn .
the average of the importance
weights , p w ( i ) / n , converges to the ratio of these normalizing constants , z123 / zn , where z123 =r f123 ( x ) dx and zn =r fn ( x ) dx .
in a bayesian application where fn is proportional to the prior and f123 is the product of fn and the likelihood , the ratio z123 / zn will be the marginal likelihood of the model that is , the prior probability or probability density of the observed data .
note that the prior need not be normalized , since any constant factors there will cancel in this ratio , but the likelihood must include all constant factors for this estimate of the marginal likelihood to be correct .
the data collected during annealed importance sampling runs from pn down to p123 can also be used to estimate expectations with respect to any of the intermediate distribu - tions , pj for 123 < j < n .
one simply uses the states , xj , found after application of tj123 in ( 123 ) , with weights found by omitting the factors in equation ( 123 ) that pertain to later states .
similarly , one can estimate the ratio of the normalizing constants for fj and fn by averaging these weights .
finally , although we would usually prefer to start annealing runs with a distribution pn from which we can generate independent points , annealed importance sampling is still valid even if the points xn123 generated at the start of each run are not independent .
in particular , these points could be generated using a markov chain that samples from pn .
the annealed importance sampling estimates will still converge to the correct values , provided the markov chain used to sample from pn is ergodic .
123 accuracy of importance sampling estimates
before discussing annealed importance sampling further , it is necessary to consider the accuracy of importance sampling estimates in general .
these results will also be needed for the demonstrations in sections 123 and 123
for reference , here again is the importance sampling estimate , a , for ef ( a ) , based on
points x ( i ) drawn independently from the density proportional to g ( x ) :
w ( i ) = n 123
w ( i ) a ( x ( i ) )
where w ( i ) = f ( x ( i ) ) / g ( x ( i ) ) are the importance weights .
the accuracy of this importance sampling estimator is discussed by geweke ( 123 ) .
an estimator of the same form is also used with regenerative markov chain methods ( mykland , tierney , and yu 123; ripley 123 ) , where the weights are the lengths of tours between regeneration points .
in determining the accuracy of this estimator , we can assume without loss of generality that the normalizing constant for g is such that eg ( w ( i ) ) = 123 , since multiplying all the w ( i ) by a constant has no eect on a .
we can also assume that ef ( a ) = eg ( w ( i ) a ( x ( i ) ) ) = 123 , since adding a constant to a ( x ) simply shifts a by that amount , without changing its variance .
for large n , the numerator and denominator on the right side of equation ( 123 ) will converge to their expectations , which on these assumptions gives
a = ( cid : 123 ) e ( w ( i ) a ( x ( i ) ) ) + e123 ( cid : 123 ) . ( cid : 123 ) e ( w ( i ) ) + e123 ( cid : 123 ) =
123 + e123
= e123 e123e123 +
where e123 and e123 are the dierences of the averages from their expectations .
when n is large , we can discard all but the rst term , e123
we can judge the accuracy of a by its variance ( assuming this is nite ) , which we can approximate as
varg ( a ) varg ( e123 ) = n 123egh ( cid : 123 ) w ( i ) a ( x ( i ) ( cid : 123 ) 123i
we now return to an actual situation , in which eg ( w ( i ) ) may not be one , and ef ( a )
may not be zero , by modifying equation ( 123 ) suitably :
var ( a ) n 123 egh ( cid : 123 ) w ( i ) ( a ( x ( i ) ) ef ( a ) ) ( cid : 123 ) 123i .
egh w ( i ) i123
geweke ( 123 ) estimates this from the same data used to compute a , as follows :
nxi=123 ( cid : 123 ) w ( i ) ( a ( x ( i ) ) a ) ( cid : 123 ) 123 . h nxi=123
this is equivalent to the estimate discussed by ripley ( 123 , section 123 ) in the context of regenerative simulation .
when n is small , ripley recommends using a jacknife estimate
when w ( i ) and a ( x ( i ) ) are independent under g , equation ( 123 ) simplies to
varg ( a ) n 123 egh ( w ( i ) ) 123i egh ( a ( x ( i ) ) ef ( a ) ) 123i .
egh w ( i ) i123
= n 123h 123 + varghw ( i ) / eg ( w ( i ) ) ii varfha ( x ( i ) ) i
the last step above uses the following :
varfha ( x ( i ) ) i = efh ( a ( x ( i ) ) ef ( a ) ) 123i = eghw ( i ) ( a ( x ( i ) ef ( a ) ) 123i .
eghw ( i ) i ( 123 )
= egh ( a ( x ( i ) ) ef ( a ) ) 123i
equation ( 123 ) shows that when w ( i ) and a ( x ( i ) ) are independent , the cost of using points drawn from g ( x ) rather than f ( x ) is given by one plus the variance of the normalized importance weights .
we can estimate this using the sample variance of sample size is eectively reduced , without reference to any particular function whose ex - pectation is to be estimated .
note that in many applications the expectations of several functions will be estimated from the same sample of x ( i ) .
= w ( i ) / n 123p w ( i ) .
this gives us a rough indication of the factor by which the
the variance of the w ( i )
is also intuitively attractive as an indicator of how accurate our estimates will be , since when it is large , the few points with the largest importance weights will dominate the estimates .
it would be imprudent to trust an estimate when the adjusted sample size , n / ( 123 + var ( w ( i ) ) ) , is very small , even if equation ( 123 ) gives a small estimate for the variance of the estimator .
one should note , however , that it is possible for the sample variance of the w ( i ) to be small even when the estimates are wildly inaccurate , since this sample variance could be a very bad estimate of the true variance of the normalized importance weights .
this could happen , for example , if an important mode of f is almost never seen when sampling from g .
earlier , it was suggested that ef ( a ) might be estimated by the weighted average of the values of a over the states of a markov chain that is started at each of the x ( i ) .
the accuracy of such an estimate should be estimated by treating these average values for a as single data points .
treating the dependent states from along the chain as if they were independently drawn from g could lead to overestimation of the eective sample size .
finally , if the x ( i ) are not independently drawn from g , but are instead generated by a markov chain sampler , assessing the accuracy of the estimates will be more dicult , as it will depend both on the variance of the normalized importance weights and on the autocorrelations produced by the markov chain used .
this is one reason for preferring a pn from which we can generate points independently at the start of each annealed importance sampling run .
123 eciency of annealed importance sampling
the eciency of annealed importance sampling depends on the normalized importance weights , w ( i ) / eg ( w ( i ) ) , not having too large a variance .
there are several sources of variability in the importance weights .
first , dierent annealing runs may end up in
dierent modes , which will be assigned dierent weights .
the variation in weights due to this will be large if some important modes are found only rarely .
there is no general guarantee that this will not happen , and if it does , one can only hope to nd a more eective scheme for dening the annealing distributions , or use a radically dierent markov chain that eliminates the isolated modes altogether .
high variability in the importance weights can also result from using transitions for each of these distributions that do not bring the distribution close to equilibrium .
the extreme case of this is when all the tj do nothing , in which case annealed importance sampling reduces to simple importance sampling based on pn , which will be very inef - cient if pn is not close to p123
variability from this source can reduced by increasing the number of iterations of the basic markov chain update used .
for example , if each tj consists of k metropolis updates , the variance of the importance weights might be reduced by increasing k , so that tj brings the state closer to its equilibrium distribution , pj ( at least within a local mode ) .
variability in the importance weights can also come from using a nite number of distributions to interpolate between p123 and pn , we can analyse how this aects the variance of the w ( i ) when the sequence of distributions used comes from a smoothly - varying one - parameter family , as in equation ( 123 ) .
for this analysis , we will assume that each tj produces a state drawn from pj , independent of the previous state .
this assumption is of course unrealistic , especially when there are isolated modes , but the purpose here is to understand eects unrelated to markov chain convergence .
it is convenient to look at log ( w ( i ) ) rather than w ( i ) itself .
as discussed in section 123 , we can measure the ineciency of estimation by one plus the variance of the normalized importance weights .
using the fact that e ( y q ) = exp ( q + q123 / 123 ) when y = exp ( x ) and x is gaussian with mean and variance 123 , we see that if the log ( w ( i ) ) are gaussian with mean and variance 123 , the sample size will be eectively reduced by the factor
123 + varg " w ( i )
exp ( 123 + 123 / 123 ) ( exp ( + 123 / 123 ) ) 123 = exp ( 123 )
from equation ( 123 ) ,
nxj=123h log ( fj123 ( xj123 ) ) log ( fj ( xj123 ) ) i
if the distributions used are as dened by equation ( 123 ) ,
( j123 j ) h log ( f123 ( xj123 ) ) log ( fn ( xj123 ) ) i
if we further assume that the j are equally spaced ( between 123 and 123 ) , we have
nxj=123 h log ( f123 ( xj123 ) ) log ( fn ( xj123 ) ) i
under the assumption that tj produces a state drawn independently from pj , and pro - vided that log ( f123 ( xj123 ) ) log ( fn ( xj123 ) ) has nite variance ( when xj123 is drawn from
pj ) , the central limit theorem can be applied to conclude that log ( w ( i ) ) will have an approximately gaussian distribution for large n ( keeping f123 and fn xed as n increases ) .
123 / n , for some constant 123 the variance of log ( w ( i ) ) will asymptotically have the form 123 and one plus the variance of the normalized weights will have the form exp ( 123 we assume that each transition , tj , takes a xed amount of time ( regardless of n ) , the time required to produce an estimate of a given degree of accuracy will be proportional to n exp ( 123 123 , at which point the variance of the logs of the importance weights will be one and the variance of the normalized importance weights will be e123
123 / n ) , which is minimized when n = 123
the same behaviour will occur when the j are not equally spaced , as long as they are chosen by a scheme that leads to j123 j going down approximately in inverse proportion to n .
over a range of values for which pj is close to gaussian , and p123 ( x ) is approximately constant in regions of high density under pj , an argument similar to that used for tempered transitions ( neal 123 , section 123 ) shows that the best scheme uses a uniform spacing for log ( j ) ( ie , a geometric spacing of the j themselves ) .
the results above also hold more generally for annealing schemes that are based on families of distributions for which the density at a given x varies smoothly with a parameter analogous to .
we can get some idea of how the eciency of annealed importance sampling will be aected by the dimensionality of the problem by supposing that under each pj , the k components of x are independent and identically distributed .
assuming as above that each tj produces an independent state drawn from pj , the quantities log ( f123 ( xj123 ) ) log ( fn ( xj123 ) ) will be composed of k identically distributed independent terms .
the variance of each such quantity will increase in proportion to k , as will the variance of log ( w ( i ) ) , which will asymptotically have the form k123 123 / n .
the optimal choice of n will be k123 123 , which makes the variance of the normalized importance weights e123 , as above .
assuming that behaviour is similar for more interesting distributions , where the components are not independent , this analysis shows that increasing the dimensionality of the problem will slow down annealed importance sampling .
however , this linear slowdown is much less severe than that for simple importance sampling , whose eciency goes down exponentially with k .
the above analysis assumes that each tj generates a state nearly independent of the previous state , which would presumably require many metropolis or gibbs sampling iterations .
it is probably better in practice , however , to use transitions that do not come close to producing an independent state , and hence take much less time , while increasing the number of interpolating distributions to produce the same total computation time .
the states generated would still come from close to their equilibrium distributions , since these distributions will change less from one annealing step to the next , and the increased number of distributions may help to reduce the variance of the importance weights , though perhaps not as much as in the above analysis , since the terms in equation ( 123 ) will no longer be independent .
we therefore see that the variance of the importance weights can be reduced as needed by increasing the number of distributions used in the annealing scheme , provided that
the transitions for each distribution are good enough at establishing equilibrium .
when there are isolated modes , the latter provision will not be true in a global sense , but transitions that sample well within a local mode can be used .
whether the performance of annealed importance sampling is adequate will then depend on whether the annealing heuristic is in fact capable of nding all the modes of the distribution .
in the absence of any theoretical information pointing to where the modes are located , reliance on some such heuristic is inevitable .
123 demonstrations on simple distributions
to illustrate the behaviour of annealed importance sampling , i will show how it works on a simple distribution with a single mode , using markov chain transitions that sample well for all intermediate distributions , and on a distribution with two modes , which are isolated with respect to the markov chain transitions for the distribution of interest .
both distributions are over r123
in the unimodal distribution , the six components of the state , x123 to x123 , are independent under p123 , with the distribution for each being gaussian with mean 123 and standard
deviation 123 .
this distribution was dened by f123 ( x ) = ( 123 / 123 ) pi ( xi 123 ) 123 / 123 , whose
normalizing constant is ( 123 ) 123 / 123 = 123 .
a sequence of annealing distributions was dened according to the scheme of equation ( 123 ) .
under the distribution chosen for pn , the components were independent , each being gaussian with mean zero and standard deviation 123
the function fn used to dene this distribution was chosen to be the corresponding gaussian probability density , which was normalized .
we can therefore estimate the normalizing constant for f123 by the average of the importance weights .
to use annealed importance sampling , we must choose a sequence of j that dene the intermediate distributions .
both the number and the spacing of the j must be appropriate for the problem .
as mentioned in the previous section , for a gaussian p123 , and a diuse pn , we expect that a geometric spacing will be appropriate for the j that are not too far from one .
i spaced the j near zero arithmetically .
in detail , for the rst test , i used 123 j spaced uniformly from 123 to 123 , followed by 123 j spaced geometrically from 123 to 123 , for a total of 123 distributions .
in later tests , annealing sequences with twice as many and half as many distributions were also used , spaced according to the same scheme .
we must also dene markov chain transitions , tj , for each of these distributions .
in general , one might use dierent schemes for dierent distributions , but in these tests , i used metropolis updates with the same proposal distributions for all tj ( the transi - tion probabilities themselves were of course dierent for each tj , since the metropolis acceptance criterion changes ) .
in detail , i used sequences of three metropolis updates , with gaussian proposal distributions centred on the current state having covariances of 123i , 123i , and 123i .
used together , these three proposal distributions lead to adequate mixing for all of the intermediate distributions .
for the rst test , this sequence of three updates was repeated 123 times to give each tj; in one later test , it was repeated only 123 times .
for each test , 123 annealing runs were done .
in the rst test , 123 states were produced in each run , as a result of applying each tj in succession , starting from a point generated independently from p123
i saved only every twentieth state , however , after applying t123 , t123 , etc .
down to t123
note that t123 was applied at the end of each run in these tests , even though this is not required ( this occurs naturally with the program used ) .
only the state after applying t123 was used for the estimates , even though it is valid to use the state after t123 as well .
figure 123 shows the results of this rst test .
the upper graphs show how the variance of the log of the importance weights increases during the course of a run .
( importance weights before the run is over are dened as in equation ( 123 ) , but with the factors for the later distributions omitted . ) when , as here , the transitions for all distributions are expected to mix well , the best strategy for minimizing the variance of the nal weights is to space the j so that the variance of the log weights increases by an equal amount in each annealing step .
the plot in the upper right shows that the spacing chosen for this test is close to optimal in this respect .
furthermore , according to the analysis of section 123 , the number of intermediate distributions used here is close to optimal , since the variance of the logs of the weights at the end of the annealing run is close to one .
the lower two graphs in figure 123 show the distribution of the value of the rst com - ponent of the state ( x123 ) in this test .
as seen in the lower left , this distribution narrows to the distribution under p123 as approaches one .
the plot in the lower right shows the values of the rst component and of the importance weights for the states at the ends of the runs .
in this case , the values and the weights appear to be independent .
the estimate for the expectation of the rst component of the state in this rst test is 123 , with standard error 123 , as estimated using equation ( 123 ) .
this is compatible with the true value of one .
in this case , the error estimate from equation ( 123 ) is close what one would arrive at from the estimated standard deviation of 123 and the adjusted sample size of n / ( 123 + var ( w ) ) = 123 / ( 123 + 123 ) = 123 , as expected when the values and the weights are independent .
the average of the importance weights for this test was 123 , with standard error 123 ( estimated simply from the sample variance of the weights divided by n ) ; this is compatible with the true normalizing constant of 123 .
two tests were done in which each run used half as much computer time as in the rst test .
in one of these , the annealing sequence was identical to the rst test , but the number of repetitions of the three metropolis updates in each tj was reduced from 123 to 123
this increased the variance of the normalized importance weights to 123 , with a corresponding increase in the standard errors of the estimates .
in the other test , the number of distributions in the annealing sequence was cut in half ( spaced according to the same scheme as before ) , while the number of metropolis repetitions was kept at 123
this increased the variance of the normalized importance weights to 123 .
as expected , spreading a given number of updates over many intermediate distributions appears to be better than using many updates to try to produce nearly independent points at each of fewer stages .
the nal test on this unimodal distribution used twice as many intermediate distri -
butions , spaced according to the same scheme as before .
this reduced the variance of the normalized importance weights to 123 , with a corresponding reduction in standard errors , but the benet in this case was not worth the factor of two increase in computer time .
however , this test does conrm that when each tj mixes well , the variance of the importance weights can be reduced as desired by spacing the j more closely .
tests were also done on a distribution with two modes , which was a mixture of two gaussians , under each of which the six components were independent , with the same means and standard deviations .
one of these gaussians , with mixing proportion 123 / 123 , had means of 123 and standard deviations of 123 , the same as the distribution used in the unimodal tests .
the other gaussian , with mixing proportion 123 / 123 , had means of 123 and standard deviations of 123 .
this mixture distribution was dened by the following f123 :
f123 ( x ) = exp "
# + 123 exp "
( xi + 123 ) 123
the normalizing constant for this f123 is 123 ( 123 ) 123 / 123 = 123 .
the means of the components with respect to this p123 are 123 / 123
the same fn as before was used for these tests ( independent standard gaussian dis - tributions for each component , normalized ) .
the same transitions based on metropolis updates were used as well , along with the same scheme for spacing the j .
for the rst test , the number of distributions used was 123 , as in the rst test on the unimodal
the results are shown in figure 123
as seen in the lower left of the gure , the distri - butions for near zero cover both modes , but as is increased , the two modes become separated .
the metropolis updates are not able to move between these modes when is near one , even when using the larger proposals with standard deviation 123 , since the probability of proposing a movement to the other mode simultaneously for all six components is very small .
both modes are seen when annealing , but the mode at 123 is seen only rarely 123 times in the 123 runs despite the fact that it has twice the probability of the other mode under the nal distribution at = 123
an unweighted average over the nal states of the annealing runs would therefore give very inaccurate
the plot in the lower right of the gure shows how the importance weights compensate for this unrepresentative sampling .
the runs that ended in the rarely - sampled mode received much higher weights than those ending in the well - sampled mode .
the estimate for the expectation of the rst component from these runs was 123 , with an estimated standard error of 123 ( from equation ( 123 ) ) , which is compatible with the true value of 123 / 123
this standard error estimate is less than one might expect from the estimated standard deviation of 123 and the adjusted sample size of n / ( 123 + var ( w ) ) , which was 123 .
the dierence arises because the values and the importance weights are not independent in this case .
the average of the importance weights in these runs was 123 , with an estimated standard error of 123 , which is compatible with the true value of 123 for the normalizing constant of f123
index of distribution
first component of state
figure 123 : results of the rst test on the unimodal distribution .
upper left : the logs of the importance weights at ten values of , for each of the 123 runs .
upper right : the variance of the log weights as a function of the index of .
lower left : the distribution of the rst component of the state at ten values .
lower right : the joint distribution of the rst component and the importance weight at the ends of the runs .
random jitter was added to the values in the plots on the left to improve the presentation .
index of distribution
. . . . . . . . .
first component of state
figure 123 : results of the rst test on the distribution with two modes .
the four plots here correspond to those in figure 123
we therefore see that annealed importance sampling produces valid estimates for this example .
however , the procedure is less ecient than we might hope , because so few runs end in the mode at 123
another symptom of the problem is that the variance of the normalized importance weights in this test was 123 quite high compared to the variance of 123 seen in the similar test on the unimodal distribution .
we can see how this comes about from the upper plots in figure 123
for small values of , these plots are quite similar to those in figure 123 , presumably because the mode at 123 has almost no inuence for these distributions .
however , this mode becomes important as approaches one , producing a high variance for the weights at the end .
one might hope to reduce the variance of the importance weights by increasing the number of intermediate distributions ( ie , by spacing the j more closely ) .
i ran tests with twice as many distributions , and with four times as many distributions , in both cases using the same number of metropolis updates for each distribution as before .
the results diered little from those in the rst test .
the variance of the importance weights for runs ending within each mode was reduced , but the dierence in importance weights between modes was not reduced , and the number of runs ending in the mode at 123 did not increase .
there was therefore little dierence in the standard errors for the estimates .
for this example , the annealing heuristic used was only marginally adequate .
one could expect to obtain better results only by nding a better initial distribution , pn , or a better scheme for interpolating from pn to p123 than that of equation ( 123 ) .
this example also illustrates the dangers of uncritical reliance on empirical estimates of accuracy .
if only 123 runs had been done , the probability that none of the runs would have found the mode at 123 would have been around 123 .
this result can be simulated using the rst 123 runs that ended in the mode at +123 from the 123 runs of the actual test .
based on these 123 runs , the estimate for the expectation of the rst component is 123 , with an estimated standard error 123 , and the estimate for the normalizing constant of f123 is 123 , with an estimated standard error of 123 .
both estimates dier from the true values by many times the estimated standard error .
such unrecognized inaccuracies are of course also possible with any other importance sampling or markov chain method , whenever theoretically - derived guarantees of accuracy are not available .
123 demonstration on a linear regression problem
to illustrate the use of annealed importance sampling for statistical problems , i will briey describe its application to two bayesian models for a linear regression problem , based on gaussian and cauchy priors .
this example , and that of the previous section , are implemented using my software for exible bayesian modeling ( version of 123 - 123 - 123 ) .
the data and command les used are included with that software , which is available from my web page .
the data consists of 123 independent cases , each having 123 real - valued predictor vari -
ables , x123 , .
, x123 and a real - valued response variable , y , which is modeled by
k xi +
the residual , , is modeled as gaussian with mean zero and unknown variance 123
the 123 cases were synthetically generated from this model with 123 = 123 and with 123 = 123 , 123 = 123 , 123 = 123 , and k = 123 for 123 i 123
the predictor variables were generated from a multivariate gaussian with the variance of each xi being one and with correlations of 123 between each pair of xi .
two bayesian models were tried .
in both , the prior for the reciprocal of the residual variance ( 123 / 123 ) was gamma with mean 123 / 123 and shape parameter 123 .
both models also had a hyperparameter , 123 , controlling the width of the distribution of the k .
its reciprocal was given a gamma prior with mean 123 / 123 and shape parameter 123 .
for the model with gaussian priors , 123 was the variance of the k , which had mean zero , and were independent conditional on 123
the model based on cauchy priors was similar , except that was the width parameter of the cauchy distribution ( ie , the density for k conditional on was ( 123 / ) ( 123+123 k / 123 ) 123 ) .
one might suspect that the cauchy prior will prove more appropriate for the actual data , since this prior gives substantial probability to situations where many of the k are close to zero , but a few k are much bigger .
it seems quite possible that the posterior using the cauchy prior could be multimodal .
since the xi are highly correlated , one k can to some extent substitute for another .
the cauchy prior favours situations where only a few k are large .
this could produce several posterior modes that correspond to dierent sets of k being regarded as signicant .
i sampled for both models using a combination of gibbs sampling for 123 and the hybrid monte carlo method for the k ( see neal 123 ) .
there was no sign of any problems with isolated modes , but it is dicult to be sure on this basis that no such modes exist .
annealed importance sampling was applied in order to either nd any isolated modes or provide further evidence of their absence , and also to compare the two models by calculating their marginal likelihoods .
an annealing schedule based on equation ( 123 ) was used .
after some experimentation , adequate results were obtained using such a schedule with 123 distributions : 123 dis - tributions geometrically spaced from = 123 to = 123 , then 123 distributions geometrically spaced from = 123 to = 123 , and nally 123 distributions geometri - cally spaced from = 123 to = 123
hybrid monte carlo updates were used for each distribution .
a single annealing run took approximately 123 seconds on our 123 mhz sgi machine .
i did 123 such runs for each model .
because a few of the annealing runs resulted in much smaller weights than others , the variance of the logs of the weights was very large , and hence was not useful in judging whether the annealing schedule was good .
instead , i looked at w = log ( 123 + var ( w ( i ) the log of one plus the variance of the normalized importance weights .
if the distribution of the logs of the weights were gaussian , w would be equal to the variance of the logs of the weights .
when this distribution is not gaussian , w is less aected by a few extremely small weights .
plots of w show that for both models it increases approximately linearly with the index of the distribution , reaching a nal value around 123 , only a bit less than the optimal value of one .
for both models , the estimates of the posterior means of the k found using annealed
importance did not dier signicantly from those found using hybrid monte carlo with - out annealing .
it therefore appears that isolated modes were not present in this problem .
the annealed importance sampling runs yielded estimates for the log of the marginal likelihood for the model with gaussian priors of - 123 and for the model with cauchy priors of - 123 , with a standard error of 123 for both estimates .
the dierence of 123 corresponds to a bayes factor of 123 in favour of the model with cauchy priors .
123 relationship to tempered transitions
several ways of modifying the simulated annealing procedure in order to produce asymp - totically correct estimates have been developed in the past , including simulated tem - pering ( marinari and parisi 123; geyer and thompson 123 ) and metropolis coupled markov chains ( geyer 123 ) .
the method of tempered transitions ( neal 123 ) is closely related to the annealed importance sampling method of this paper .
the tempered transition method samples from a distribution of interest , p123 , using a markov chain whose transitions are dened in terms of an elaborate proposal procedure , involving a sequence of other distributions , p123 to pn .
the proposed state is found by simulating a sequence of base transitions , t123 to tn , which leave invariant the distributions p123 to pn , followed by a second sequence of base transitions , tn to t123 , which leave pn to p123 invariant , and which are the reversals of the corresponding tj with respect to the pj .
the decision whether to accept or reject the nal state is based on a product of ratios of probabilities under the various distributions; if the proposed state is rejected , the new state is the same as the old state .
in detail , such a tempered transition operates as follows , starting from state x123 :
generate x123 from x123 using t123
generate x123 from x123 using t123
generate xn from xn123 using tn .
generate xn123 from xn using tn .
generate x123 from x123 using t123
generate x123 from x123 using t123
the state x123 is then accepted as the next state of the markov chain with probability
the second half of the tempered transition procedure ( 123 ) is identical to the annealed importance sampling procedure ( 123 ) , provided that tn in fact generates a point from pn that is independent of xn .
we can also recognize that the annealed importance sampling weight given by equation ( 123 ) is essentially the same as the second half of the product dening the tempered transition acceptance probability ( 123 ) .
due to these similarities ,
the characteristics of annealed importance sampling will be quite similar to those of the corresponding tempered transitions .
in particular , the comparison by neal ( 123 ) of tempered transitions with simulated tempering is relevant to annealed importance sampling as well .
the major dierence between annealed importance sampling and tempered transitions is that each tempered transition requires twice as much computation as the corresponding annealing run , since a tempered transition involves an upward sequence of transitions , from p123 to pn , as well as the downward sequence , from pn to p123 , that is present in both methods .
this is a reason to prefer annealed importance sampling when it is easy to generate independent points from the distribution pn .
when this is not easy , tempered transitions might be preferred , though annealed importance sampling could still be used in conjunction with a markov chain sampler that produces dependent points from pn .
with tempered transitions , there is also the possibility of using more than one sequence of annealing distributions ( with the sequence chosen randomly for each tempered transition , or in some xed order ) .
potentially , this could lead to good sampling even when neither annealing sequence would be adequate by itself .
there appears to be no way of employing multiple annealing sequences with annealed importance sampling without adding an equivalent of the upward sequence present in tempered transitions .
when tempered transitions are used , the idea behind annealed importance sampling can be applied in order to estimate ratios of normalizing constants , which were previously unavailable when using tempered transitions .
to see how to do this , note that the rst half of a tempered transition ( up to the generation of xn123 from xn123 using tn123 ) is the same as an annealed importance sampling run , but with the sequence of distributions reversed ( p123 and pn exchange roles , the rst state of the run is the current state , x123 , which comes from p123 , and in general , xj of ( 123 ) corresponds to xn123j of ( 123 ) ) .
the importance weights for this backwards annealed importance sampling are
the average of these weights for all tempered transitions ( both accepted and rejected )
will converge tor fn ( x ) dx / r f123 ( x ) dx , the ratio of normalizing constants for fn and f123
a similar estimate can be found by imagining the reversal of the markov chain dened by the tempered transitions .
in this chain , the states are visited in the reverse order , the accepted transitions of the original chain become accepted transitions in the reversed chain ( but with the reversed sequence of states ) , and the rejected transitions of the original chain remain unchanged .
an importance sampling estimate for the ratio of normalizing constants for fn and f123 can be obtained using this reversed chain , in the same manner as above .
the importance weights for the accepted transitions are as follows , in terms of the original chain :
the importance weights for the rejected transitions are the same as in equation ( 123 ) .
these two estimates can be averaged , producing an estimate that uses the states at both
the beginning and the end of the accepted transitions , plus the states at the beginning of the rejected transitions , with double weight .
an estimate for the ratio of the normalizing constant for fj to that for f123 can be found in similar fashion for any of the intermediate distributions , by simply averaging the weights obtained by truncating the products in equations ( 123 ) and ( 123 ) at the appropriate point .
these weights can also be used to estimate expectations of functions with respect to these intermediate distributions .
note that error assessment for all these importance sampling estimates will have to take into account both the variance of the importance weights and the autocorrelations produced by the markov chain based on the tempered
a cautionary note regarding these estimates comes from considering the situation when only two distributions are used , which are the prior and the posterior for a bayesian model .
the estimate for the reciprocal of the marginal likelihood based on equation ( 123 ) will then be the average over points drawn from the posterior of the reciprocal of the likelihood .
this estimator will often have innite variance , and will be very bad for any problem where there is enough data that the posterior is not much aected by the prior ( since the marginal likelihood is aected by the prior ) .
compare this to the annealed importance sampling estimate for the marginal likelihood using just these two distributions , which will be the average of the likelihood over points drawn from the prior .
this is not very good when the posterior is much more concentrated than the prior , but it is not as bad as averaging the reciprocal of the likelihood .
even when many intermediate distributions are used , it seems possible the annealed importance sampling estimates may be better than the corresponding backwards estimates using tempered transitions ( assuming that pn is more diuse than p123 ) .
123 relationship to sequential importance sampling
a variant of sequential importance sampling recently developed by maceachern , clyde , and liu ( 123 ) can be viewed as an instance of annealed importance sampling , in which the sequence of distributions is obtained by looking at successively more data points .
this method ( which maceachern , et al call sequential importance sampler s123 ) ap - plies to a model for the joint distribution of observable variables x123 , .
, xn along with associated latent variables s123 , .
, sn ( which have a nite range ) .
we are able to compute these joint probabilities , as well as the marginal probabilities for the xk together with the sk over any subset of the indexes .
we wish to estimate expectations with respect to the conditional distribution of s123 , .
, sn given known values for x123 , .
we could apply gibbs sampling to this problem , but it is possible that it will be slow to converge , due to isolated modes .
the method of maceachern , et al can be viewed as annealed importance sampling with a sequence of distributions , p123 to pn , in which pj is related to the distribution conditional on nj of the observed variables; p123 is then the distribution of interest , conditional on in detail , these distributions have probabilities proportional to the all of x123 ,
fj ( s123 ,
= p ( s123 , .
, snj , x123 , .
, xnj )
p ( sk | x123 , .
, xk , s123 , .
, sk123 )
we can apply annealed importance sampling with this sequence of distributions , using transitions dened as follows .
tj begins with some number of gibbs sampling updates for s123 to snj , based only on p ( s123 , .
, snj | x123 , .
, xnj ) .
we can ignore snj+123 to sn here because we can generate values for them afterward from their conditional distribution ( under fj ) given s123 to snj , independently of their previous values .
this is done by forward simulation based on their conditional probabilities .
( actually , there is no need to generate values for sk with k > n j + 123 , since these values have no eect on the subsequent computations anyway . ) this is easily seen to be equivalent to the sampling done in procedure s123 of maceachern , et al .
the importance weights of equation ( 123 ) are products of factors of the following form :
fj123 ( s123 , .
, sn ) fj ( s123 ,
p ( s123 , .
, snj+123 , x123 , .
, xnj+123 )
p ( s123 , .
, snj , x123 , .
, xnj ) p ( snj+123 | x123 , .
, xnj+123 , s123 , .
, snj )
p ( snj+123 , xnj+123 | x123 , .
, xnj , s123 , .
, snj )
p ( snj+123 | x123 , .
, xnj+123 , s123 , .
, snj )
= p ( xnj+123 | x123 , .
, xnj , s123 , .
, snj )
the product of these factors produces the same weights as used by maceachern , et al .
sequential importance sampler s123 of maceachern , et al is thus equivalent to annealed importance sampling with the annealing distributions dened by equation ( 123 ) .
unlike the family of distributions given by equation ( 123 ) , these distributions form a xed , dis - crete family .
consequently , the variance of the importance weights cannot be decreased by increasing the number of distributions .
this could sometimes make the method too inecient for practical use .
however , it is possible that the sequence of distributions dened by equation ( 123 ) could be extended to a continuous family by partially condi - tioning on the xk in some way ( eg , by adjusting the variance in a gaussian likelihood ) .
other forms of annealed importance sampling ( eg , based on the family of equation ( 123 ) ) could also be applied to this problem .
annealed importance sampling is potentially useful as a way of dealing with isolated modes , as a means of calculating ratios of normalizing constants , and as a general monte carlo method that combines independent sampling with the adaptivity of markov chain
handling isolated modes was the original motivation for annealing , and has been the primary motivation for developing methods related to annealing that produce asymp - totically correct results .
annealed importance sampling is another such method , whose characteristics are similar to those of tempered transitions .
as i have discussed ( neal 123 ) , which of these methods is best may depend on whether the sequence of annealing distributions is deceptive in certain ways .
it is therefore not possible to say that an - nealed importance sampling will always be better than other methods such as simulated tempering , but it is probably the most easily implemented of these methods .
annealing methods are closely related to methods for estimating ratios of normalizing constants based on simulations from many distributions , many of which are discussed by gelman and meng ( 123 ) .
it is therefore not surprising that the methods of simu - lated tempering ( marinari and parisi 123; geyer and thompson 123 ) and metropolis coupled markov chains ( geyer 123 ) easily yield estimates for ratios of normalizing con - stants as a byproduct .
tempered transitions were previously seen as being decient in this respect ( neal 123 ) , but we now see that such estimates can in fact be obtained by us - ing annealed importance sampling estimators in conjunction with tempered transitions .
one can also estimate expectations with respect to all the intermediate distributions in this way ( as is also possible with simulated tempering and metropolis coupled markov
ratios of normalizing constants can also be obtained when using annealed importance sampling itself , which from this perspective can be seen as a form of thermodynamic integration ( see gelman and meng 123 ) .
one might expect a thermodynamic integra - tion estimate based on a nite number of points to suer from systematic error , but the results of this paper show that the annealed importance sampling estimate for the ratio of normalizing constants is in fact unbiased , and will converge to the correct value as the number of annealing runs increases .
( note that in this procedure one averages the estimates from multiple runs for the ratio of normalizing constants , not for the log of this ratio , as might perhaps seem more natural . )
unlike simulated tempering and the related method of umbrella sampling ( torrie and valleau 123 ) , no preliminary estimates for ratios of normalizing constants are required when using annealed importance sampling .
metropolis coupled markov chains share this advantage , but have the disadvantage that they require storage for states from all the intermediate distributions .
annealed importance sampling may therefore be the most convenient general method for estimating normalizing constants .
in addition to these particular uses , annealed importance sampling may sometimes be attractive because it combines independent sampling with the ability of a markov chain sampler to adapt to the characteristics of the distribution .
evans ( 123 ) has also devised an adaptive importance sampling method that makes use of a sequence of intermediate distributions , similar to that used for annealing .
his method requires that a class of tractable importance sampling densities be dened that contains a density appropriate for each of the distributions in this sequence .
annealed importance sampling instead uses a sampling distribution that is implicitly dened by the operation of the markov chain transitions , whose density is generally not tractable to compute , making its use for
simple importance sampling infeasible .
from this perspective , the idea behind annealed importance sampling is that one can nevertheless nd appropriate importance weights for use with this sampling distribution by looking at ratios of densities along the sequence of intermediate distributions .
one annoyance with markov chain monte carlo is the need to estimate autocorrelations in order to assess the accuracy of the estimates obtained .
provided the points from pn used to start the annealing runs are generated independently , there is no need to do this with annealed importance sampling .
instead , one must estimate the variance of the normalized importance weights .
this may perhaps be easier , though nightmare scenarios in which drastically wrong results are obtained without there there being any indication of a problem are possible when using methods of either sort .
for annealed importance sampling , this can occur when the distribution of the importance weights has a heavy upward tail that is not apparent from the data collected .
another annoyance with markov chain monte carlo is the need to decide how much of a run to discard as burn - in ie , as not coming from close to the equilibrium distribution .
if only one , long run is simulated , the exact amount discarded as burn - in may not be crucial , but if several shorter runs are done instead , as is desirable in order to diagnose possible non - convergence , the decision may be harder .
discarding too little will lead to biased estimates; discarding too much will waste data .
with annealed importance sampling , one must make an analogous decision of how much computation time to spend on the annealing runs themselves , which determine the importance weights , and how much to spend on simulating a chain that samples from p123 starting from the nal state from the annealing run ( as is usually desirable , see section 123 ) .
however , this decision aects only the variance of the estimates the results are asymptotically correct regardless of how far the annealing process is from reaching equilibrium .
regenerative methods ( mykland , tierney , and yu 123 ) also eliminate the problems of dealing with sequential dependence ( and also replace them with possible problems due to heavy - tailed distributions ) .
to use regenerative methods , an appropriate splitting scheme must be devised for the markov chain sampler .
for high - dimensional problems , this may be harder than dening an appropriate sequence of intermediate distributions for use with annealed importance sampling .
as discussed in section 123 , the time required for annealed importance sampling can be expected to increase in direct proportion to the dimensionality of the problem ( in addition to any increase due to the markov chain samplers used being slower in higher dimensions ) .
one must also consider the human and computer time required to select an appropriate sequence of intermediate distributions , along with appropriate markov chain transitions for each .
for these reasons , annealed importance sampling will probably be most useful when it allows one to nd needed ratios of normalizing constants , or serves to avoid problems with isolated modes .
one should note , however , that the potential for problems with multiple modes exists whenever there is no theoretical guarantee that the distribution is unimodal .
i thank david mackay for helpful comments .
this research was supported by the natural sciences and engineering research council of canada .
