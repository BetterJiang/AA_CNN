during its long gestation in the 123s and early 123s , arithmetic coding ( rissanen 123; rissanen and langdon 123; rubin 123; rissanen and langdon 123; langdon 123 ) was widely regarded more as a curiosity than a practical coding technique .
this was particularly true for applica - tions where the alphabet has many symbols , as huffman coding is usually reasonably effective in such cases ( manstetten 123 ) .
one factor that helped arithmetic coding gain the popularity it enjoys today was the publication of source code for a multisymbol arithmetic coder by witten et al .
( 123 ) in communications of the acm , which we refer to as the cacm implementation .
one decade later , our understanding of arithmetic coding has further matured , and it is timely to review the components of that implementation and summarize the improvements that have emerged .
we also describe a novel , previously unpublished , method for performing the underlying calculation needed for arithmetic coding .
software is available that implements the revised method .
the major improvements discussed in this article and implemented in
the software are as follows :
enhanced models that allow higher - performance compression .
a more modular division into modeling , estimation , and coding sub -
data structures that support arithmetic coding on large alphabets .
changes to the coding procedure that reduce the number of multiplica - tions and divisions and which permit most of them to be done with
support for larger alphabet sizes and for more accurate representations
a reformulation of the decoding procedure that greatly simplifies the
decoding loop and improves decoding speed .
an extension providing efficient coding for binary alphabets .
to motivate these changes , we examine in detail the needs of a word - based model for text compression .
while not the best - performing model for text ( see , for example , the compression results listed by witten et al .
( 123 ) ) , word - based modeling combines several attributes that test the capabilities and limitations of an arithmetic coding system .
the new implementation of arithmetic coding is both more versatile and more efficient than the cacm implementation .
when combined with the same character - based model as the cacm implementation , the changes that we advocate result in up to two - fold speed improvements , with only a small loss of compression .
this faster coding will also be of benefit in any other compression system that makes use of arithmetic coding ( such as the block - sorting method of burrows and wheeler ( 123 ) ) , though the percent -
acm transactions on information systems , vol .
123 , no .
123 , july 123
moffat et al .
age of overall improvement will of course vary depending on the time used in other operations and on the exact nature of the hardware being used .
the new implementation is written in c , and is publicly available through the internet by anonymous ftp , at munnari . oz . au , directory / pub / arith_coder , file arith_coder . tar . z or arith_coder . tar . gz .
the original cacm package ( witten et al .
123 ) is at ftp . cpsc . ucalgary .
ca in file / pub / projects / ar . cod / cacm - 123shar .
software that imple - ments the new method for performing the arithmetic coding calculations , but is otherwise similar to the cacm version , can be found at ftp . cs .
toronto . edu in the directory / pub / radford , file lowp_ac . shar .
in the remainder of this introduction we give a brief review of arithmetic coding , describe modeling in general , and word - based models in particular , and discuss the attributes that the arithmetic coder must embody if it is to be usefully coupled with a word - based model .
section 123 examines the interface between the model and the coder , and explains how it can be designed to maximize their independence .
section 123 shows how accurate probability estimates can be maintained efficiently in an adaptive compres - sion system , and describes an elegant structure due to fenwick ( 123 ) .
in section 123 the cacm arithmetic coder is reexamined , and our improvements are described .
section 123 analyzes the cost in compression effectiveness of using low precision for arithmetic operations .
low - precision operations may be desirable because they permit a shift / add implementation , details of which are discussed in section 123
section 123 describes the restricted coder for binary alphabets , and examines a simple binary model for text compres - sion .
finally , section 123 reviews the results and examines the various situations in which arithmetic coding should and should not be used .
123 the idea of arithmetic coding we now give a brief overview of arithmetic coding .
for additional back - ground the reader is referred to the work of langdon ( 123 ) , witten et al .
( 123; 123 ) , bell et al .
( 123 ) , and howard and vitter ( 123; 123 ) .
suppose we have a message composed of symbols over some finite alphabet .
suppose also that we know the probability of appearance of each of the distinct symbols , and seek to represent the message using the smallest possible number of bits .
the well - known algorithm of huffman ( 123 ) takes a set of probabilities and calculates , for each symbol , a code word that unambiguously represents that symbol .
huffmans method is known to give the best possible representation when all of the symbols must be assigned discrete code words , each an integral number of bits long .
the latter constraint in effect means that all symbol probabilities are approximated by negative powers of two .
in an arithmetic coder the exact symbol probabilities are preserved , and so compression effectiveness is better , sometimes markedly so .
on the other hand , use of exact probabili - ties means that it is not possible to maintain a discrete code word for each symbol; instead an overall code for the whole message must be calculated .
acm transactions on information systems , vol .
123 , no .
123 , july 123
arithmetic coding revisited
the mechanism that achieves this operates as follows .
suppose that p i is the probability of the ith symbol in the alphabet , and that variables l and r are initialized to 123 and 123 respectively .
value l represents the smallest binary value consistent with a code representing the symbols processed so far , and r represents the product of the probabilities of those symbols .
to encode the next symbol , which ( say ) is the jth of the alphabet , both l and r must be refined : l is replaced by l 123 ro j123p i and r is replaced by r z p j , preserving the relationship between l , r , and the symbols so far processed .
at the end of the message , any binary value between l and l 123 r will unambiguously specify the input message .
we transmit the shortest such binary string , c .
because c must have at least 123 log123 r and at most 123 log123 r 123 123 bits of precision , the procedure is such that a symbol with probability p j is effectively coded in approximately 123log123 p j bits , thereby meeting the entropy - based lower bound of shannon ( 123 ) .
this simple description has ignored a number of important problems .
specifically , the process described above requires extremely high precision arithmetic , since l and r must potentially be maintained to a million bits or more of precision .
we may also wonder how best to calculate the cumulative probability distribution , and how best to perform the arith - metic .
solving these problems has been a major focus of past research , and of the work reported here .
123 the role of the model the cacm implementation ( witten et al .
123 ) included two driver pro - grams that coupled the coder with a static zero - order character - based model , and with a corresponding adaptive model .
these were supplied solely to complete a compression program , and were certainly not intended to represent excellent models for compression .
nevertheless , several people typed in the code from the printed page and compiled and executed it , onlymuch to our chagrinto express disappointment that the new method was inferior to widely available benchmarks such as compress ( hamaker 123; witten et al .
123 ) .
in fact , all that the cacm article professed to supply was a state - of - the art coder with two simple , illustrative , but mediocre models .
one can think of the model as the intelligence of a compression scheme , which is responsible for deducing or interpolating the structure of the input , whereas the coder is the engine room of the compression system , which converts a probability distribution and a single symbol drawn from that distribution into a code ( bell et al .
123; rissanen and langdon 123 ) .
in particular , the arithmetic coding engine is independent of any particular model .
the example models in this article are meant purely to illustrate the demands placed upon the coder , and to allow different coders to be compared in a uniform test harness .
any improvements to the coder will
acm transactions on information systems , vol .
123 , no .
123 , july 123
moffat et al .
primarily yield better compression efficiency , that is , a reduction in time or space usage .
improvements to the model will yield improved compression effectiveness , that is , a decrease in the size of the encoded data .
in this article we are primarily interested in compression efficiency , although we will also show that the approximations inherent in the revised coder do not result in any substantial loss of compression effectiveness .
the revised implementation does , however ,
include a more effective word - based model ( bentley et al .
123; horspool and cormack 123; moffat 123 ) , which represents the stream as a sequence of words and nonwords rather than characters , with facilities for spelling out new words as they are encountered using a subsidiary character mode .
since the entropy of words in typical english text is around 123 123 bits each , and that of nonwords is around 123 bits , between 123 and 123 bits are required to encode a typical five - character word and the following one - character nonword .
large texts are therefore compressed to around 123% of their input size ( 123 bits per character ) a significant improvement over the 123% 123% ( 123 123 bits per character ) achieved by zero - order character - based models of en - glish .
witten et al .
( 123 ) give results comparing character - based models with word - based models .
a word - based compressor can also be faster than a character - based one .
once a good vocabulary has been established , most words are coded as single symbols rather than as character sequences , reducing the number of time - consuming coding operations required .
what is more relevant , for the purposes of this article , is that word - based models illustrate several issues that do not arise with character - based
an efficient data structure is needed to accumulate frequency counts for
a large alphabet .
multiple coding contexts are necessary ,
for tokens , characters , and lengths , for both words and nonwords .
here , a coding context is a conditioning class on which the probability distribution for the next symbol is based .
an escape mechanism is required to switch from one coding context to
data structures must be resizable because there is no a priori bound on
all of these issues are addressed in this article .
arithmetic coding is most useful for adaptive compression , especially with large alphabets .
this is the application envisioned in this article , and in the design of the new implementation .
for static and semistatic coding , in which the probabilities used for encoding are fixed , huffman coding is usually preferable to arithmetic coding ( bookstein and klein 123; moffat and turpin 123; moffat et al .
123 ) .
acm transactions on information systems , vol .
123 , no .
123 , july 123
arithmetic coding revisited
modeling , statistics , and coder modules .
cooperating modules it is useful to divide the process of data compression into three logically disjoint activities : modeling , statistics - gathering , and coding .
this separa - tion was first articulated by rissanen and langdon ( 123 ) , although the cacm implementation of witten et al .
( 123 ) combined statistics gathering with modeling to give a two - way split .
this section describes the three - way partition , which is reflected in our implementation by three cooperating modules .
examples are given that show how the interfaces are used .
123 modeling , statistics , and coding of the three modules , the most visible is that which performs the modeling .
least visible is the coder .
between these two is the statistics module , which manipulates a data structure to record symbol frequency counts ( or some other estimate of symbol probabilities ) .
in detail , a statistics module used with an arithmetic coder must be able to report the cumulative frequency of all symbols earlier in the alphabet than a given symbol , and to record that this symbol has occurred one more time .
both the model and the coder are oblivious to the exact mechanism used to accomplish this : the model is unaware of the probability attached to each symbol; and the coder is unaware of symbol identifiers and the size of the alphabet .
this organiza - tion is shown in figure 123
the cacm implementation ( witten et al .
123 ) has just one interface level , reflecting the two - way modeling / coding division of activity .
an array cumfreq containing cumulative frequencies and an actual symbol identifier s are passed from model to coder to achieve the transmission of each
acm transactions on information systems , vol .
123 , no .
123 , july 123
moffat et al .
table i .
module interface functions
c 123 create_context ( )
c 123 create_context ( ) s 123 decode ~ c !
arithmetic_encode ~ l , h , t !
target 123 decode_target ~ t ! arithmetic_decode ~ l , h , t !
symbol .
this forces both modules to use an array to maintain their informationan unnecessarily restrictive requirement .
by divorcing the statistics module from both model and coder , any suitable data structure can be used to maintain the statistics .
section 123 below considers some
the main routines required to interface the modules are listed in table i .
( the implementation includes several other routines , mainly for initializing and terminating compression . ) the routine install_symbol ( ) in both encoder and decoder has the same functionality as encode ( ) except that no output bits are transmitted or consumed : its purpose is to allow contexts to be primed , as if text had preceded the beginning of the transmission .
the routine purge_context removes all records for that context , leaving it as if it had just been created .
this allows synchronization points to be inserted in the compressed output using a finish_encode and start_encode pair , from which points adaptive decoding can commence without needing to process the entire compressed message .
purging model frequencies and inserting synchronization points does , of course , reduce the compression
a zero - order character - based model requires just one context and rela - tively simple control structures , as shown in the psuedocode of algorithm zero - order character - based ( figure 123 ) , which closely corresponds to the adaptive model described by witten et al .
( 123 ) .
a context c is created , install_symbol ( ) is used to make each valid character available , and en - code ( ) is called once for each character in the input .
the compressed message is terminated by an end_of_message symbol which has also been previously installed in c .
the method of algorithm zero - order character - based can easily be extended to a first - order character - based model using an array of contexts , one for each possible conditioning character .
this would require considerably more memory , but would improve the compres - sion effectiveness without impacting execution speed .
many other modifica - tions are possible .
complex models require the use of multiple contexts .
the word - based model described in section 123 uses six contexts : a zero - order context for words , a zero - order context for nonwords ( sequences of spaces and punctu -
acm transactions on information systems , vol .
123 , no .
123 , july 123
arithmetic coding revisited
algorithm zero - order character - based .
ation ) , a zero - order character context for spelling out new words , a zero - order character context for spelling out new nonwords , and contexts for specifying the lengths of words and of nonwords .
the encoder for that model is sketched as algorithm zero - order word - based ( figure 123 ) , except that for brevity the input is treated as a sequence of words rather than alternating word , nonword pairs and so only three contexts , denoted w , c , and l , are used .
to cater for nonwords as well requires additional contexts w123 , c123 , and l123 , along with an outer loop that alternates between words and nonwords by using each set of contexts in turn .
note that algorithm zero - order word - based assumes that the length of each word is bounded , so that context l can be initialized .
in our implementation the actual definition of a word was a string of at most 123 alphanumeric characters; long symbols are handled by breaking them into shorter ones with zero - length opposite symbols between .
acm transactions on information systems , vol .
123 , no .
123 , july 123
moffat et al .
algorithm zero - order word - based .
the decoder , omitted in figure 123 , is the natural combination of the ideas presented in algorithms zero - order character - based ( figure 123 ) and zero - order word - based ( figure 123 ) .
123 coding novel symbols the character - based model of algorithm zero - order character - based ( fig - ure 123 ) codes at most 123 different symbols ( 123 different eight - bit charac - ters plus the end_of_message symbol ) , all of which are made available in that context by explicit install_symbol ( ) calls .
in contrast to this , in the word - based model there is no limit to the number of possible symbolsthe number of distinct word tokens in an input stream might be hundreds , thousands , or even millions .
to cater for situations such as this in which the alphabet size is indeterminate , the function call encode ~ c , s ! returns a flag escape_transmitted if the symbol s is not known in context c , or if , for some other reason , s has zero probability .
in this event , the word is
acm transactions on information systems , vol .
123 , no .
123 , july 123
arithmetic coding revisited
encoded using the length and character models , and is then installed into the lexicon .
as well as returning a flag to indicate a zero - probability symbol , the encoder must also explicitly transmit an escape code to the decoder so that the corresponding call decode ~ c ! can similarly return an
this raises the vexatious question as to what probability should be assigned to this escape codethe so - called zero - frequency problem .
of the methods described in the literature ( summarized by moffat et al .
( 123 ) ) , we chose a modified version of method xc ( witten and bell 123 ) which we call method ax , for approximate x .
method xc approximates the number of symbols of frequency zero by the number of symbols of frequency one .
if t123 symbols have appeared exactly once , symbol s i has appeared c i times , and t 123 oic i is the total number of symbols that have been coded to date , then the escape code probability p escape is given by t123 / t and the probability of symbol s i is estimated as p i 123 ~ 123 123 t123 / t ! z ~ c i / t ! .
the drawback of method xc is the need for a two - stage coding process when the symbol is not novel one step to transmit the information not novel ( probability 123 123 t123 / t ) , and a second to indicate which nonnovel symbol it is ( probability c i / t ) .
it is not feasible to calculate the exact probability for use in a single coding step , since the need to represent the product p i restricts t to a very small range of values if overflow is to be avoided ( see also the discussion in section 123 below ) .
instead , for method ax
pescape 123 ~ t123 123 123 ! / ~ t 123 t123 123 123 !
pi 123 ci / ~ t 123 t123 123 123 ! .
the 123 allows novel events to be represented even when there are no frequency one ( in method xc this exception is handled by reverting to another method called method c in the literature ) ; and t123 is incorporated additively in the denominator by taking a first - order binomial approximation to ~ 123 123 t123 / t ! 123
with this method a single coding step suffices , as t 123 t123 123 123 can be represented in roughly half the number of bits as the denominator t123 required by method xc .
the difference is crucial , since for flexibility we desire t to be similar in magnitude to the largest integer that can be represented in a machine word .
the change distorts the probabilities slightly , but all zero - frequency methods are heuristics any - way , and the effect is small .
once the escape code has been transmitted , the new token can be spelled out and added to the w context .
both encoder and decoder assign it the next available symbol number , so there is no need to specify an identifier .
123 storage management an industrial - strength compression system must provide some mechanism to bound the amount of memory used during encoding and decoding
acm transactions on information systems , vol .
123 , no .
123 , july 123
moffat et al .
example , some compressors reclaim list items using a least - recently - used policy , so that the model structure continues to evolve when memory is exhausted .
others purge the model when memory is full , but retain a sliding window buffer of recent symbols so that a smaller replacement model can be rebuilt ( using install_symbol ) immediately .
the exact mecha - nism used in any application will depend upon the needs of that applica - tion , in particular , on the amount of memory consumed by the structural model ( figure 123 ) .
because of this dependence , the only facility we provide in our implementation is the routine purge_context ( ) , which removes all records for that context , as if it had just been created .
one rationalization for this abrupt trash and start again approach is that memory is typically so plentiful that trashing should be rare enough to cause little deteriora - tion in the average compression rate .
furthermore , in the particular case of the word - based model the impact is softened by the fact that the underlying character - based models do not need to be purged , so transmission of novel words while the lexicon is rebuilt is less expensive than it was originally .
in practice , purging of the lexicon in the word - based compressor is rare .
a memory limit of one megabyte is ample to process texts with a vocabulary of about 123 , 123 distinct words , such as the complete works of shakespeare .
