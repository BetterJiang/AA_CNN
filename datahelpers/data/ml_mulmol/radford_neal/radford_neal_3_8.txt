we propose a new scheme for selecting pool states for the embedded hidden markov model ( hmm ) markov chain monte carlo ( mcmc ) method .
this new scheme allows the embedded hmm method to be used for ecient sampling in state space models where the state can be high - dimensional .
previously , embedded hmm methods were only applied to models with a one - dimensional state space .
we demonstrate that using our proposed pool state selection scheme , an embedded hmm sampler can have similar performance to a well - tuned sampler that uses a combination of particle gibbs with backward sampling ( pgbs ) and metropolis updates .
the scaling to higher dimensions is made possible by selecting pool states locally near the current value of the state sequence .
the proposed pool state selection scheme also allows each iteration of the embedded hmm sampler to take time linear in the number of the pool states , as opposed to quadratic as in the original embedded hmm sampler .
we also consider a model with a multimodal posterior , and show how a technique we term mirroring can be used to eciently move between the modes .
consider a non - linear , non - gaussian state space model for an observed sequence y = ( y123 , .
this model , with parameters , assumes that the yi are drawn from an observation density p ( yi|xi , ) , where xi is an unobserved markov process with initial density p ( x123| ) and transition density p ( xi|xi123 , ) .
here , the xi might be either continuous or discrete .
we may be interested in inferring both the realized values of the markov process x = ( x123 , .
, xn ) and the model parameters .
in a bayesian approach to this problem , this can be done by drawing a sample of values for x and using a markov chain that alternately samples from the conditional posterior distributions p ( x| , y ) and p ( |x , y ) .
in this paper , we will only consider inference for x by sampling from
p ( x| , y ) , taking the parameters to be known .
as a result , we will omit in model densities for the rest of the paper .
except for linear gaussian models and models with a nite state space , this sampling problem has no exact solution and hence approximate methods such as mcmc must
one method for sampling state sequences in non - linear , non - gaussian state space models is the embedded hmm method ( neal , 123; neal , beal and roweis , 123 ) .
an embedded hmm update proceeds as follows .
first , at each time i , a set of l pool states in the latent space is constructed .
in this set , l 123 of the pool states are drawn from a chosen pool state density and one is the current value of xi .
this step can be thought of as temporarily reducing the state space model to an hmm with a nite set of l states , hence the name of the method .
then , using ecient forward - backward computations , which take time proportional to l123n , a new sequence x ( cid : 123 ) is selected from the ensemble of ln sequences passing through the set of pool states , with the probability of choosing each sequence proportional to its posterior density divided by the probability of the sequence under the pool state density .
at the next iteration of the sampler , a new set of pool states is constructed , so that the chain can sample all possible xi , even when the set of possible values is innite .
another method is the particle gibbs with backward sampling ( pgbs ) method .
the particle gibbs ( pg ) method was rst introduced in andrieu , doucet and holenstein ( 123 ) ; whiteley suggested the backward sampling modication in the discussion following this paper .
lindsten and schon ( 123 ) implemented backward sampling and showed that it improves the eciency of pg .
starting with a current sequence x , pgbs rst uses conditional sequential monte carlo ( smc ) to construct a set of candidate sequences and then uses backward sampling to select a new sequence from the set of candidate ones .
here , conditional smc works in the same way as ordinary smc when generating a set of particles , except that one of the particles at time i is always set to the current xi , similar to what is done in the embedded hmm method , which allows the sampler to remain at xi if xi lies in a high - density region .
while this method works well for problems with low - dimensional state spaces , the reliance of the smc procedure on choosing an appropriate importance density can make it challenging to make the method work in high dimensions .
an important advantage of particle gibbs , however , is that each iteration takes time that is only linear in the number of particles .
both the pgbs and embedded hmm methods can facilitate sampling of a latent state se - quence , x , when there are strong temporal dependencies amongst the xi .
in this case , using a method that samples xi conditional on xed values of xi123 and xi+123 can be an inecient way of producing a sample from p ( x|y , ) , because the conditional density of xi given xi123 and xi+123 can be highly concentrated relative to the marginal density of xi .
in contrast , with the embedded hmm and pgbs methods it is possible to make changes to blocks of xis at once .
this allows larger changes to the state in each iteration of the sampler , making updates more ecient .
however , good performance of the embedded hmm and pgbs methods relies on appropriately choosing the set of pool states or particles at each time i .
in this paper , our focus will be on techniques for choosing pool states for the embedded hmm method .
when the latent state space is one - dimensional , embedded hmms work well when choosing pool states in a variety of ways .
for example , in shestopalo and neal ( 123 ) , we
choose pool states at each time i by constructing a pseudo - posterior for each latent variable by taking the product of a pseudo - prior and the observation density , the latter treated as a pseudo - likelihood for the latent variable .
in shestopalo and neal ( 123 ) , we choose pool states at each time i by sampling from the marginal prior density of the latent process .
ways of choosing pool states that work well in one dimension begin to exhibit problems when applied to models with higher - dimensional state spaces .
this is true even for dimensions as small as three .
since these schemes are global , designed to produce sets of pool states without reference to the current point , as the dimension of the latent space grows , a higher proportion of the sequences in the ensemble ends up having low posterior density .
ensuring that performance doesnt degrade in higher dimensions thus requires a signicant increase in the number of pool states .
as a result , computation time may grow so large that any advantage that comes from using embedded hmms is eliminated .
one advantage of the embedded hmm method over pgbs is that the embedded hmm construction allows placing pool states locally near the current value of xi , potentially allowing the method to scale better with the dimensionality of the state space .
switching to such a local scheme xes the problem to some extent .
however , local pool state schemes come with their own problems , such as making it dicult to handle models with multiple posterior modes that are well - separated the pool states might end up being placed near only some of the modes .
in this paper , we propose an embedded hmm sampler suitable for models where the state space is high dimensional .
this sampler uses a sequential approximation to the density p ( xi|y123 , .
yi ) or to the density p ( xi|yi+123 , .
, yn ) as the pool state density .
we show that by using this pool state density , together with an ecient mcmc scheme for sampling from it , we can reduce the cost per iteration of the embedded hmm sampler to be proportional to nl , as with pgbs .
at the same time , we retain the ability to generate pool states locally , allowing better scaling for high - dimensional state spaces .
our proposed scheme can thus be thought of as combining the best features of the pgbs and the embedded hmm methods , while overcoming the deciencies of both .
we use two sample state space models as examples .
both have gaussian latent processes and poisson observations , with one model having a unimodal posterior and the second a multimodal one .
for the multimodal example , we introduce a mirroring technique that allows ecient movement between the dierent posterior modes .
for these models , we show how our proposed embedded hmm method compares to a simple metropolis sampler , a pgbs sampler , as well as a sampler that combines pgbs and simple metropolis updates .
further details on ensemble methods are available in the phd thesis of shestopalo ( 123 ) .
123 embedded hmm mcmc
we review the embedded hmm method ( neal , 123; neal , beal and roweis , 123 ) here .
we take the model parameters , , to be xed , so we do not write them explicitly .
let p ( x ) be the density from which the state at time 123 is drawn , let p ( xi|xi123 ) be the transition density between states at times i and i 123 , and let p ( yi|xi ) be the density of the observation yi given xi .
suppose our current sequence is x = ( x123 , .
the embedded hmm sampler updates x to
x ( cid : 123 ) as follows .
first , at each time i = 123 , .
, n , we generate a set of l pool states , denoted by pi = i ) .
the pool states are sampled independently across the dierent times i .
, x ( l ) choose li ( 123 , .
, l ) uniformly at random and set x ( li ) to xi .
we sample the remaining l 123 pool states x ( 123 ) n using a markov chain that leaves a pool density i invari - ant , as follows .
let ri ( x ( cid : 123 ) |x ) be the transitions of this markov chain with ri ( x|x ( cid : 123 ) ) the transitions for this markov chain reversed ( i . e .
ri ( x|x ( cid : 123 ) ) = ri ( x ( cid : 123 ) |x ) i ( x ) / i ( x ) ) , so that
, x ( li123 )
, x ( l )
i ( x ) ri ( x ( cid : 123 ) |x ) = i ( x ( cid : 123 ) ) ri ( x|x ( cid : 123 ) )
for all x and x ( cid : 123 ) .
then , starting at j = li 123 , use reverse transitions ri ( x ( j ) , .
, x ( l )
i and starting at j = li + 123 use forward transitions ri ( x ( j )
, x ( 123 )
) to gener - ) to generate
at each i = 123 , .
, n , we then compute the forward probabilities i ( x ) , with x taking values
at time i = 123 , we have
and at times i = 123 , .
, n , we have
finally , we sample a new state sequence x ( cid : 123 ) using a stochastic backwards pass .
this is done n amongst the set , pn , of pool states at time n , with probabilities proportional to i123 from the set pi123 , with probabilities proportional i|x ) .
note that only the relative values of the i ( x ) will be required , so the i may
by selecting x ( cid : 123 ) n ( x ) , and then going backwards , sampling x ( cid : 123 ) be computed up to some constant factor .
alternatively , given a set of pool states , embedded hmm updates can be done by rst comput - ing the backward probabilities .
we will see later on that the backward probability formulation of the embedded hmm method allows us to introduce a variation of our proposed pool state selection scheme .
setting n ( x ) = 123 for all x pn , we compute for i < n
123 to one of the x in the pool p123 with probabilities proportional to 123 ( x ) p ( x ) p ( y123|x ) and then choosing subsequent
a new state sequence is then sampled using a stochastic forward pass , setting x ( cid : 123 )
i from the pools pi with probabilities proportional to i ( x ) p ( x|x ( cid : 123 )
computing the i or i at each time i > 123 takes time proportional to l123 , since for each of the l pool states it takes time proportional to l to compute the sums in ( 123 ) or ( 123 ) .
hence each iteration of the embedded hmm sampler takes time proportional to l123n .
123 particle gibbs with backward sampling mcmc
we review the particle gibbs with backward sampling ( pgbs ) sampler here .
for full details , see the articles by andrieu , doucet and holenstein ( 123 ) and lindsten and schon ( 123 ) .
let q123 ( x|y123 ) be the importance density from which we sample particles at time 123 , and let qi ( x|yi , xi123 ) be the importance density for sampling particles at times i > 123
these may depend on the current value of the parameters , , which we suppressed in this notation .
suppose we start with a current sequence x .
we set the rst particle x ( 123 ) to the current state x123
we then sample l 123 particles x ( 123 )
from q123 and compute and normalize the weights of the particles :
, x ( l )
for l = 123 ,
for i > 123 , we proceed sequentially .
we rst set x ( 123 ) ancestor indices for particles at time i , dened by a ( l ) probabilities proportional to w ( l ) sample each of the l123 particles , x ( l ) and normalize the weights at time i
i = xi .
we then sample a set of l 123 i123 ( 123 , .
, l ) , for l = 123 , .
, l , with i123
the ancestor index for the rst state , a ( 123 ) i123 , is 123
we then i , at time i , for l = 123 , .
, l , from qi ( x|yi , x ) and compute
i |yi , x
a new sequence taking values in the set of particles at each time is then selected using a back - wards sampling pass .
this is done by rst selecting x ( cid : 123 ) n from the set of particles at time n with probabilities w ( l ) n and then selecting the rest of the sequence going backward in time to time 123 , i to x ( l ) i with probability
a common choice for q is the models transition density , which is what is compared to in this
note that each iteration of the pgbs sampler takes time proportional to ln , since it takes time proportional to l to create the set of particles at each time i , and to do one step of backward
123 an embedded hmm sampler for high dimensions
we propose two new ways , denoted f and b , of generating pool states for the embedded hmm sampler .
unlike previously - used pool state selection schemes , where pool states are selected independently at each time , our new schemes select pool states sequentially , with pool states at time i selected conditional on pool states at time i 123 , or alternatively at time i + 123
123 pool state distributions
the rst way to generate pool states is to use a forward pool state selection scheme , with a sequential approximation to p ( xi|y123 , .
, yi ) as the pool state density .
in particular , at time 123 , we set the pool state distribution of our proposed embedded hmm sampler to
as a result of equation ( 123 ) , 123 ( x ) is constant .
at time i > 123 , we set the pool state distribution to
123 ( x ) p ( x ) p ( y123|x )
i ( x|pi123 ) p ( yi|x )
which makes i ( x ) constant for i > 123 as well ( see equation ( 123 ) ) .
we then draw a sequence composed of these pool states with the forward probability imple -
mentation of the embedded hmm method , with the i ( x ) s all set to 123
the second way is to instead use a backward pool state selection scheme , with a sequential approximation of p ( xi|yi+123 , .
, yn ) as the pool state density .
we begin by creating the pool pn , consisting of the current state xn and the remaining l 123 pool states sampled from pn ( x ) , the marginal density at time n , which is the same as p ( x ) if the latent process is stationary .
the backward probabilities n ( x ) , for x in pn , are then set to 123
at time i < n we set the pool state
i ( x|pi+123 ) l ( cid : 123 )
so that i ( x ) is constant for all i = 123 , .
, n ( see equation 123 ) .
we then draw a sequence composed of these pool states as in the backward probability im -
plementation of the embedded hmm method , with the i ( x ) s all set to 123
if the latent process is gaussian , and the latent state at time 123 is sampled from the stationary distribution of the latent process , it is possible to update the latent variables by applying the forward scheme to the reversed sequence ( yn , .
, y123 ) by making use of time reversibility , since xn is also sampled from the stationary distribution , and the latent process evolves backward in time according to the same transition density as it would going forward .
we then use the forward pool state selection scheme along with a stochastic backward pass to sample a sequence ( xn , .
, x123 ) , starting with x123 and going to xn .
it can sometimes be advantageous to alternate between using forward and backward ( or , alternatively , forward applied to the reversed sequence ) embedded hmm updates , since this can improve sampling of certain xi .
the sequential pool state selection schemes use only part of the observed sequence in generating the pool states .
by alternating update directions , the pool states can depend on dierent parts of the observed data , potentially allowing us to better cover the region where xi has high posterior density .
for example , at time 123 , the pool state density may disperse the pool states too widely , leading to poor sampling for x123 , but sampling x123 using a backwards scheme can be much better , since we are now using all of the data in the sequence when sampling pool states at time 123
123 sampling pool states
to sample from f i , we can use any markov transitions ri that leave this distribution invariant .
the validity of the method does not depend on the markov transitions for sampling
i reaching equilibrium or even on them being ergodic .
i or b
i or b
directly using these pool state densities in an mcmc routine leads to a computational cost per iteration that is proportional to l123n , like in the original embedded hmm method , since at times i > 123 we need at least l updates to produce l pool states , and the cost of computing an acceptance probability is proportional to l .
however , it is possible to reduce the cost per iteration of the embedded hmm method to be proportional to nl when we use f i as the pool state densities .
to do this , we start by thinking of the pool state densities at each time i > 123 as marginal densities summing over the variable ( cid : 123 ) = 123 , .
, l that indexes a pool state at the previous time .
specically , f i can be viewed as a marginal of the density
i or b
i ( x , ( cid : 123 ) ) p ( yi|x ) p ( x|x ( ( cid : 123 ) )
i is a marginal of the density
i ( x , ( cid : 123 ) ) p ( yi+123|x ( ( cid : 123 ) )
both of these densities are dened given a pool pi123 at time i 123 or pool pi+123 at time i + 123
this technique is reminiscent of the auxiliary particle lter of pitt and shephard ( 123 ) .
we then use markov transitions , ri , to sample a set of values of x and ( cid : 123 ) , with probabilities proportional to i for the forward scheme , or probabilities proportional to i for the backward scheme .
the chain is started at x set to the current xi , and the initial value of ( cid : 123 ) is chosen randomly with probabilities proportional to p ( xi|x ( ( cid : 123 ) ) the backward scheme .
this stochastic initialization of ( cid : 123 ) is needed to make the algorithm valid when we use i or i to generate the pool states .
i123 ) for the forward scheme or p ( yi+123|x ( ( cid : 123 ) )
sampling values of x and ( cid : 123 ) from i or i can be done by updating each of x and ( cid : 123 ) separately , alternately sampling values of x conditional on ( cid : 123 ) , and values of ( cid : 123 ) conditional on x , or by updating x and ( cid : 123 ) jointly , or by a combination of these .
updating x given ( cid : 123 ) can be done with any appropriate sampler , such as metropolis , or for a gaussian latent process we can use autoregressive updates , which we describe below .
to update ( cid : 123 ) given x , we can also use metropolis updates , proposing ( cid : 123 ) ( cid : 123 ) = ( cid : 123 ) +k , with k drawn from some proposal distribution on ( k , .
, 123 , 123 , .
alternatively , we can simply propose ( cid : 123 ) ( cid : 123 ) uniformly at random from ( 123 ,
to jointly update x and ( cid : 123 ) , we propose two novel updates , a shift update and a ip update .
since these are also metropolis updates , using them together with metropolis or autoregressive updates , for each of x and ( cid : 123 ) separately , allows embedded hmm updates to be performed in time proportional to nl .
123 autoregressive updates
for sampling pool states in our embedded hmm mcmc schemes , as well as for comparison mcmc schemes , we will make use of neals ( 123 ) autoregressive metropolis - hastings update , which we review here .
this update is designed to draw samples from a distribution of the form p ( w ) p ( y|w ) where p ( w ) is multivariate gaussian with mean and covariance and p ( y|w ) is typically a density for some observed data .
this autoregressive update proceeds as follows .
let l be the lower triangular cholesky de - composition of , so = llt , and n be a vector of i . i . d .
normal random variables with zero mean and identity covariance .
let ( 123 , 123 ) be a tuning parameter that determines the scale of the proposal .
starting at w , we propose
w ( cid : 123 ) = +
123 123 ( w ) + ln
because these autoregressive proposals are reversible with respect to p ( w ) , the proposal density and p ( w ) cancel in the metropolis - hastings acceptance ratio .
this update is therefore accepted
note that for this update , the same value of is used for scaling along every dimension .
it would be of independent interest to develop a version of this update where can be dierent for each dimension of w .
123 shift updates
we can simultaneously update ( cid : 123 ) and x at time i > 123 by proposing to update ( x , ( cid : 123 ) ) to ( x ( cid : 123 ) , ( cid : 123 ) ( cid : 123 ) ) where ( cid : 123 ) ( cid : 123 ) is proposed in any valid way while x ( cid : 123 ) is chosen in a way such that x ( cid : 123 ) and x ( ( cid : 123 ) ( cid : 123 ) ) i123 are linked in the same way as x and x ( ( cid : 123 ) ) i123
the shift update makes it easier to generate a set of pool states at time i with dierent predecessor states at time i 123 , helping to ensure that the pool states are well - dispersed .
this update is accepted with the usual metropolis probability .
for a concrete example we use later , suppose that the latent process is an autoregressive in this case ,
gaussian process of order 123 , with the model being that xi|xi123 n ( xi123 , ) .
given ( cid : 123 ) ( cid : 123 ) , we propose x ( cid : 123 )
this update is accepted with probability
i = xi + ( x ( ( cid : 123 ) ( cid : 123 ) )
as a result of the transition densities in the acceptance ratio cancelling out , since
i123 = xi + ( x ( ( cid : 123 ) ( cid : 123 ) ) = xi x ( ( cid : 123 ) )
to be useful , shift updates normally need to be combined with other updates for generating pool states .
when combining shift updates with other updates , tuning of acceptance rates for both updates needs to be done carefully in order to ensure that the shift updates actually improve sampling performance .
in particular , if the pool states at time i 123 are spread out too widely , then the shift updates may have a low acceptance rate and not be very useful .
therefore , jointly optimizing proposals for x and for x and ( cid : 123 ) may lead to a relatively high acceptance rate on updates of x , in order to ensure that the acceptance rate for the shift updates isnt low .
123 flip updates
generating pool states locally can be helpful when applying embedded hmms to models with high - dimensional state spaces but it also makes sampling dicult if the posterior is multimodal .
consider the case when the observation probability depends on |xi| instead of xi , so that many modes with dierent signs for some xi exist .
we propose to handle this problem by adding an additional ip update that creates a mirror set of pool states , in which xi will be in the pool if xi is .
by having a mirror set of pool states , we are able to ip large segments of the sequence in a single update , allowing ecient exploration of dierent posterior modes .
to generate a mirror set of pool states , we must correctly use the ip updates when sampling the pool states .
since we want each pool state to have a negative counterpart , we choose the number of pool states l to be even .
the chain used to sample pool states then alternates two types of updates , a usual update to generate a pool state and a ip update to generate its negated version .
the usual update can be a combination of any updates , such as those we consider above .
so that each state will have a ipped version , we start with a ip transition between x ( 123 ) and x ( 123 ) , a usual transition between x ( 123 ) and x ( 123 ) , and so on up to a ip transition between x ( l123 ) to x ( l ) .
at time 123 , we start with the current state x123 and randomly assign it to some index l123 in the chain used to generate pool states .
then , starting at x123 we generate pool states by reversing the markov chain transitions back to 123 and going forward up to l .
each ip update is then a metropolis update proposing to generate a pool state x123 given that the chain is at some pool state x123
note that if the observation probability depends on x123 only through |x123| and p ( x ) is symmetric around zero then this update is always accepted .
at time i > 123 , a ip update proposes to update a pool state ( x , ( cid : 123 ) ) to ( x , ( cid : 123 ) ( cid : 123 ) ) such that i123 = x ( ( cid : 123 ) ) i123
here , since the pool states at each time are generated by alternating ip and , the proposal to move from ( cid : 123 ) to ( cid : 123 ) ( cid : 123 ) can be viewed usual updates , starting with a ip update to x ( 123 ) as follows .
suppose that instead of labelling our pool states from 123 to l we instead label them 123 to l 123
the pool states at times 123 and 123 , then 123 and 123 , and so on will then be ipped pairs , and the proposal to change ( cid : 123 ) to ( cid : 123 ) ( cid : 123 ) can be seen as proposing to ip the lower order bit in a binary representation of ( cid : 123 ) ( cid : 123 ) .
for example , a proposal to move from ( cid : 123 ) = 123 to ( cid : 123 ) = 123 can be seen as proposing to change ( cid : 123 ) from 123 to 123 ( in binary ) .
such a proposal will always be accepted assuming a transition density for which p ( xi|xi123 ) = p ( xi|xi123 ) and an observation probability which depends on xi only via |xi| .
123 relation to pgbs
the forward pool state selection scheme can be used to construct a sampler with properties similar to pgbs .
this is done by using independence metropolis to sample values of x and ( cid : 123 ) from i .
at time 123 , we propose our pool states from p ( x ) .
at times i > 123 , we propose ( cid : 123 ) ( cid : 123 ) by selecting
it uniformly at random from ( 123 , .
, l ) and we propose x ( cid : 123 ) by sampling from p ( x|x ( ( cid : 123 ) ( cid : 123 ) ) proposals at all times i are accepted with probability
this sampler has computational cost proportional to ln per iteration , like pgbs .
it is analogous to a pgbs sampler with importance densities
q123 ( x|y123 ) = p ( x )
qi ( x|xi123 , yi ) = p ( x|xi123 ) ,
with the key dierence between these two samplers being that pgbs uses importance weights p ( yi|xi ) on each particle , instead of an independence metropolis accept - reject step .
123 proof of correctness
we modify the original proof of neal ( 123 ) , which assumes that the sets of pool states p123 , .
, pn are selected independently at each time , to show the validity of our new sequential pool state selection scheme .
another change in the proof is to account for generating the pool states by sampling them from i or i instead of f
i or b
this proof shows that the probability of starting at x and moving to x ( cid : 123 ) with given sets of pool i of x ( cid : 123 )
states pi ( consisting of values of x at each time i ) , pool indices li of xi , and pool indices l ( cid : 123 )
is the same as the probability of starting at x ( cid : 123 ) and moving to x with the same set of pool states pi , pool indices l ( cid : 123 ) i , and pool indices li of xi .
this in turn implies , by summing / integrating over pi and li , that the embedded hmm method with the sequential pool state scheme satises detailed balance with respect to p ( x|y ) , and hence leaves p ( x|y ) invariant .
i of x ( cid : 123 )
suppose we use the sequential forward scheme .
the probability of starting at x and moving to x ( cid : 123 ) decomposes into the product of the probability of starting at x , which is p ( x|y ) , the probability of choosing a set of pool state indices li , which is 123 ln , the probability of selecting the initial values of ( cid : 123 ) i for the stochastic initialization step , the probability of selecting the sets of pool states pi , p ( p123 , .
, pn ) , and nally the probability of choosing x ( cid : 123 ) .
the probability of selecting given initial values for the links to previous states ( cid : 123 ) 123 , .
, ( cid : 123 ) n is
at time 123 , we use a markov chain with invariant density 123 to select pool states in p123
therefore
p ( p123 , .
, pn ) = p ( p123 )
the probability of choosing a given set of pool states is
p ( p123 ) =
for times i > 123 we use a markov chain with invariant density i to sample a set of pool states , given pi123
the chain is started at x ( li )
i = xi and ( cid : 123 ) ( li )
i = ( cid : 123 ) i .
therefore
p ( pi|pi123 ) =
i , ( cid : 123 ) ( j )
i , ( cid : 123 ) ( j )
i , ( cid : 123 ) ( j )
i , ( cid : 123 ) ( j )
i , ( cid : 123 ) ( j )
i , ( cid : 123 ) ( j )
i , ( cid : 123 ) ( j )
i , ( cid : 123 ) ( j )
i , ( cid : 123 ) ( j )
finally , we choose a new sequence x ( cid : 123 ) amongst the collection of sequences consisting of the pool n uniformly at random
states with a backward pass .
this is done by rst choosing a pool state x ( cid : 123 ) from pn .
we then select the remaining states x ( l ( cid : 123 ) i by selecting l ( cid : 123 )
, l ( cid : 123 )
n123 with probability
thus , the probability of starting at x and going to x ( cid : 123 ) , with given p123 , .
, pn , l123 , .
, ln and
, l ( cid : 123 )
i , ( cid : 123 ) ( j )
also 123 ( x123 ) = p ( x123 ) p ( y123|x123 ) / ( cid : 123 ) i123 = x ( cid : 123 )
123 ) n ( cid : 123 )
i=123 i ( xi , ( cid : 123 ) i )
i ( xi , ( cid : 123 ) i ) =
i , ( cid : 123 ) ( j )
here , we have x
therefore ( 123 ) can be simplied to
123 ) n ( cid : 123 )
the last factor in the product only depends on the selected set of pool states .
by exchanging x and x ( cid : 123 ) we see that the probability of starting at x ( cid : 123 ) and then going to x , with given sets of pool states pi , pool indices li of xi and pool indices l ( cid : 123 )
i is the same .
i of x ( cid : 123 )
i , ( cid : 123 ) ( j )
123 test models
to demonstrate the performance of our new pool state scheme , we use two dierent state space models .
the latent process for both models is a vector autoregressive process , with
x123 n ( 123 , init )
xi|xi123 n ( xi123 , ) ,
i = 123 ,
where xi = ( xi , 123 , .
, xi , p ) ( cid : 123 ) and
note that init is the covariance of the stationary distribution for this process .
for model 123 , the observations are given by
yi , j|xi , j poisson ( exp ( cj + jxi , j ) ) ,
i = 123 ,
j = 123 ,
for model 123 , the observations are given by
i = 123 ,
j = 123 ,
for model 123 , we use a 123 - dimensional latent state and a sequence length of n = 123 , setting
parameter values to = 123 , and cj = 123 , j = 123 , j = 123 for j = 123 , .
, p , with p = 123
for model 123 , we increase the dimensionality of the latent space to 123 and the sequence length
to 123
we set = 123 and j = 123 , j = 123 for j = 123 , .
, p , with p = 123
we generated one random sequence from each model to test our samplers on .
these observa - tions from model 123 and model 123 are shown in figure 123
note that we are testing only sampling of the latent variables , with the parameters set to their true values .
the code for all experiments in this paper is available at http : / / arxiv . org / abs / 123 .
( a ) model 123
( b ) model 123
figure 123 : observations from model 123 and model 123 along dimension j = 123
123 single - state metropolis sampler
a simple scheme for sampling the latent state sequence is to use metropolis - hastings updates that sample each xi in sequence , conditional on xi = ( x123 , .
, xi123 , xi+123 , .
, xi ) and the data , starting at time 123 and going to time n .
we sample all dimensions of xi at once using autoregressive updates ( see section 123 . 123 ) .
the conditional densities of the xi are p ( x123|x123 , y ) p ( x123|x123 ) p ( y123|x123 ) p ( x123 ) p ( x123|x123 ) p ( y123|x123 ) p ( xi|xi , y ) p ( xi|xi123 , xi+123 ) p ( yi|xi ) p ( xi|xi123 ) p ( xi+123|xi ) p ( yi|xi ) , p ( xn|xn , y ) p ( xn|xn123 ) p ( yn|xn )
123 i n 123
the densities p ( x123|x123 ) , p ( xi|xi123 , xi+123 ) , and p ( xn|xn123 ) are all gaussian .
the means and co - variances for these densities can be derived by viewing p ( x123 ) or p ( xi|xi123 ) as a gaussian prior for xi and p ( xi+123|xi ) as a gaussian likelihood for xi .
in particular , we have
x123|x123 n ( 123 , 123 ) xi|xi , xi+123 n ( i , i ) xn|xn123 n ( n , n )
123 = ( ( 123 ) 123 + 123 = ( ( 123 ) 123 + 123 = ( 123 + 123 123 = ( ( 123 ) 123 + 123
= ( ( 123 ) + 123
i = ( ( 123 ) 123 + 123 ) 123 ( 123xi123 + ( 123 ) 123xi+123 )
= ( 123 ) 123 + 123 ) 123 ( 123 ( ( xi123 + xi+123 ) ) ) = ( 123 + i ) 123 ( xi123 + xi+123 ) i = ( ( 123 ) 123 + 123 ) 123
= ( ( 123 ) + 123 ) 123
n = xn123
to speed up the metropolis updates , we precompute and store the matrices ( 123 + 123 ( 123 + i ) 123 as well as the cholesky decompositions of the posterior covariances .
in both of our test models , the posterior standard deviation of the latent variables xi , j varies depending on the value of the observed yi , j .
to address this , we alternately use a larger or a smaller proposal scaling , , in the autoregressive update when performing an iteration of the
123 particle gibbs with backward sampling with metropolis
we implement the pgbs method as described in section 123 , using the initial density p ( x ) and the transition densities p ( xi|xi123 ) as importance densities to generate particles .
we combine pgbs updates with single - state metropolis updates from section 123 . 123
this way , we combine the strengths of the two samplers in targeting dierent parts of the posterior distribution .
in particular , we expect the metropolis updates to do better for the xi with highly informative yi , and the pgbs updates to do better for the xi where yi is not as informative .
123 tuning the baseline samplers
for model 123 , we compared the embedded hmm sampler to the simple single - state metropolis sampler , to the pgbs sampler , and to the combination of pgbs with metropolis .
for model 123 , we compared the embedded hmm sampler to the pgbs with metropolis sampler .
for both models and all samplers , we ran the sampler ve times using ve dierent random number generator seeds .
we implemented the samplers in matlab on a linux system with a 123 ghz intel
123 . 123 model 123
for the single - state metropolis sampler , we initialized all xi , j to 123
every iteration alternately used a scaling factor , , of either 123 or 123 , which resulted in an average acceptance rate of between 123% and 123% for the dierent xi over the sampler run .
we ran the sampler for 123 iterations , and prior to analysis , the resulting sample was thinned by a factor of 123 , to 123
the thinning was done due to the diculty of working with all samples at once , and after thinning the samples still possessed autocorrelation times signicantly greater than 123
each of the 123 samples took about 123 seconds to draw .
for the pgbs sampler and the sampler combining pgbs and metropolis updates , we also initialized all xi , j to 123
we used 123 particles for the pgbs updates .
for the metropolis updates , we alternated between scaling factors of 123 and 123 , which also gave acceptance rates between 123% and 123% .
for the standalone pgbs sampler , we performed a total of 123 iterations .
each iteration produced two samples for a total of 123 samples and consisted of a pgbs update using the forward sequence and a pgbs update using the reversed sequence .
each sample took about 123 seconds to draw .
for the pgbs with metropolis sampler , we performed a total of 123 iterations of the sampler .
each iteration was used to produce four samples , for a total of 123 samples , and consisted of a pgbs update using the forward sequence , ten metropolis updates ( of which only the value after the tenth update was retained ) , a pgbs update using the reversed sequence , and another ten metropolis updates , again only keeping the value after the tenth update .
the average time to draw each of the 123 samples was about 123 seconds .
123 . 123 model 123
for model 123 , we were unable to make the single - state metropolis sampler converge to anything resembling the actual posterior in a reasonable amount of time .
in particular , we found that for xi , j suciently far from 123 , the metropolis sampler tended to be stuck in a single mode , never visiting values with the opposite sign .
for the pgbs with metropolis sampler , we set the initial values of xi , j to 123
we set the number of particles for pgbs to 123 , which was nearly the maximum possible for the memory capacity of the computer we used .
for the metropolis sampler , we alternated between scaling factors of 123 and 123 , which resulted in acceptance rates ranging between 123% and 123% .
we performed a total of 123 iterations of the sampler .
as for model 123 , each iteration produced four samples , for a total of 123 samples , and consisted of a pgbs update with the forward sequence , fty metropolis updates ( of which we only keep the value after the last one ) , a pgbs update using the reversed sequence , and another fty metropolis updates ( again only keeping the last value ) .
it took about 123 seconds to draw each sample .
123 embedded hmm sampling
for both model 123 and model 123 , we implemented the proposed embedded hmm method using the forward pool state selection scheme , alternating between updates that use the original and the
reversed sequence .
as for the baseline samplers , we ran the embedded hmm samplers ve times for both models , using ve dierent random number generator seeds .
we generate pool states at time 123 using autoregressive updates to sample from f
at times i 123 , we sample each pool state from i ( x , l ) by combining an autoregressive and shift update .
the autoregressive update proposes to only change x , keeping the current l xed .
the shift update samples both x and l , with a new l proposed from a uniform ( 123 , .
, l ) distribution .
for model 123 , we also add a ip update to generate a negated version of each pool state .
note that the chain used to produce the pool states now uses a sequence of updates .
therefore , if our forward transition rst does an autoregressive update and then a shift update , the reverse transitions must rst do a shift update and then an autoregressive update .
as for the single - state metropolis updates , it is benecial to use a dierent proposal scaling , , when generating each pool state at each time i .
this allows generation of sets of pool states which are more concentrated when yi is informative and more dispersed when yi holds little information .
123 . 123 model 123
for model 123 , we initialized all xi , j to 123
we used 123 pool states for the embedded hmm updates .
for each metropolis update to sample a pool state , we used a dierent scaling , chosen at random from a uniform ( 123 , 123 ) distribution .
the acceptance rates ranged between 123% and 123% for the metropolis updates and between 123% and 123% for the shift updates .
we performed a total of 123 iterations of the sampler , with each iteration consisting of an embedded hmm update using the forward sequence and an embedded hmm update using the reversed sequence , for a total of 123 samples .
each sample took about 123 seconds to draw .
123 . 123 model 123
for model 123 , we initialized the xi , j to 123
we used a total of 123 pool states for the embedded hmm sampler ( i . e .
123 positive - negative pairs due to ip updates ) .
each metropolis update used to sample a pool state used a scaling , , randomly drawn from the uniform ( 123 , 123 ) distribution .
the acceptance rates ranged between 123% and 123% for the metropolis updates and between 123% and 123% for the shift updates .
we performed a total of 123 iterations of the sampler , producing two samples per iteration with an embedded hmm update using the forward sequence and an embedded hmm update using the reversed sequence .
each of the 123 samples took about 123 seconds to draw .
as a way of comparing the performance of the two methods , we use an estimate of autocorrelation time123 for each of the latent variables xi , j .
autocorrelation time is a measure of how many draws need to be made using the sampling chain to produce the equivalent of one independent sample .
i=123 k , where k is the autocorrelation at lag
the autocorrelation time is dened as = 123 + 123 ( cid : 123 )
it is commonly estimated as
= 123 + 123
where k are estimates of lag - k autocorrelations and the cuto point k is chosen so that k is negligibly dierent from 123 for k > k
where k is an estimate of the lag - k autocovariance
( xl x ) ( xk+l x )
when estimating autocorrelation time , we remove the rst 123% of the sample as burn - in .
then , to estimate k , we rst estimate autocovariances for each of the ve runs , taking x to be the overall mean over the ve runs .
we then average these ve autocovariance estimates to produce k .
to speed up autocovariance computations , we use the fast fourier transform .
the autocorrelation estimates are then adjusted for computation time , by multiplying the estimated autocorrelation time by the time it takes to draw a sample , to ensure that the samplers are compared fairly .
the computation time - adjusted autocorrelation estimates for model 123 , for all the latent vari - ables , plotted over time , are presented in figure 123
we found that the combination of single - state metropolis and pgbs works best for the unimodal model .
the other samplers work reasonably well too .
we note that the spike in autocorrelation time for the pgbs and to a lesser extent for the pgbs with metropolis sampler occurs at the point where the data is very informative .
this in turn makes the use of the diuse transition distribution the particles are drawn from ine - cient and much of the sampling in that region is due to the metropolis updates .
here , we also note that the computation time adjustment is sensitive to the particularities of the implementa - tion , in this case done in matlab , where performance depends a lot on how well vectorization
123technically , when we alternate updates with the forward and reversed sequence or mix pgbs and single - state metropolis updates , we cannot use autocorrelation times to measure how well the chain explores the space .
while the sampling scheme leaves the correct target distribution invariant , the ipping of the sequence makes the sampling chain for a given variable non - homogeneous .
however , suppose that instead of deterministically ipping the sequence at every step , we add an auxiliary indicator variable that determines ( given the current state ) whether the forward or the reversed sequence is used , and that the probability of ipping this indicator variable is nearly one .
with this auxiliary variable the sampling chain becomes homogeneous , with its behaviour nearly identical to that of our proposed scheme .
using autocorrelation time estimates to evaluate the performance of our sampler is therefore valid , for all practical purposes .
( a ) metropolis ( 123 seconds / sample )
( b ) pgbs ( 123 seconds / sample )
( c ) pgbs+metropolis ( 123 seconds / sample )
( d ) embedded hmm ( 123 seconds / sample )
figure 123 : estimated autocorrelation times for each latent variable for model 123 , adjusted for
can be exploited .
implementing the samplers in a dierent language might change the relative
we now look at how the samplers perform on the more challenging model 123
we rst did a preliminary check of whether the samplers do indeed explore the dierent modes of the distribution by looking at variables far apart in the sequence , where we expect to see four modes ( with all possible combinations of signs ) .
this is indeed the case for both the pgbs with metropolis and embedded hmm samplers .
next , we look at how eciently the latent variables are sampled .
of particular interest are the latent variables with well - separated modes , since sampling performance for such variables is illustrative of how well the samplers explore the dierent posterior modes .
consider the variable x123 , 123 , which has true value 123 .
figure 123 shows how the dierent samplers explore the two modes for this variable , with equal computation times used to produced the samples for the trace plots .
we can see that the embedded hmm sampler with ip updates performs signicantly better for sampling a variable with well - separated modes .
experiments showed that the performance of the embedded hmm sampler on model 123 without ip updates is
123 ( a ) embedded hmm
( b ) particle gibbs with metropolis
figure 123 : comparison of samplers for model 123 , x123 , 123
( a ) embedded hmm
( b ) particle gibbs with metropolis
figure 123 : comparison of samplers for model 123 , x123 , 123x123 , 123
we can also look at the product of the two variables x123 , 123x123 , 123 , with true value 123 .
the trace plot is given in figure 123
in this case , we can see that the pgbs with metropolis sampler performs better .
since the ip updates change the signs of all dimensions of xi at once , we do not expect them to be as useful for improving sampling of this function of state .
the vastly greater number of particles used by pgbs , 123 , versus 123 for the embedded hmm method , works to the advantage of pgbs , and explains the performance dierence .
looking at these results , we might expect that we can get a good sampler for both x123 , 123 and x123 , 123x123 , 123 by alternating embedded hmm and pgbs with metropolis updates .
this is indeed the case , which can be seen in figure 123
for producing these plots , we used an embedded hmm sampler with the same settings as in the experiment for model 123 and a pgbs with metropolis sampler with 123 particles and metropolis updates using the same settings as in the experiment
123 ( a ) trace plot for x123 , 123
( b ) trace plot for x123 , 123x123 , 123
figure 123 : combination of embedded hmm and pgbs with metropolis samplers
for model 123
this example of model 123 demonstrates another advantage of the embedded hmm viewpoint , which is that it allows us to design updates for sampling pool states to handle certain properties of the density .
this is arguably easier than designing importance densities in high dimensions .
we have demonstrated that it is possible to use embedded hmms to eciently sample state sequences in models with higher dimensional state spaces .
we have also shown how embedded hmms can improve sampling eciency in an example model with a multimodal posterior , by introducing a new pool state selection scheme .
there are several directions in which this research can be further developed .
the most obvious extension is to treat the model parameters as unknown and add a step to sample parameters given a value of the latent state sequence .
in the unknown parameter context , it would also be interesting to see how the proposed sequential pool state selection schemes can be used together with ensemble mcmc updates of shestopalo and neal ( 123 ) .
for example , one approach is to have the pool state distribution depend on the average of the current and proposed parameter values in an ensemble metropolis update , as in shestopalo and neal ( 123 ) .
one might also wonder whether it is possible to use the entire current state of x in constructing the pool state density at a given time .
it is not obvious how ( or if it is possible ) to overcome this limitation .
for example , for the forward scheme , using the current value of the state sequence at some time k > i to construct pool states at time i means that the pool states at time k will end up depending on the current value of xk , which would lead to an invalid sampler .
at each time i < n , the pool state generation procedure does not depend on the data after time i , which may cause some diculties in scaling this method further .
on one hand , this
123 allows for greater dispersion in the pool states than if we were to impose a constraint from the other direction as with the single - state metropolis method , potentially allowing us to make larger moves .
on the other hand , the removal of this constraint also means that the pool states can become too dispersed .
in higher dimensions , one way in which this can be controlled is by using a markov chain that samples pool states close to the current xi that is , a markov chain that is deliberately slowed down in order not to overdisperse the pool states , which could lead to a collection of sequences with low posterior density .
we thank arnaud doucet for helpful comments .
this research was supported by the natural sciences and engineering research council of canada .
is in part funded by an nserc postgraduate scholarship .
holds a canada research chair in statistics and machine learn -
