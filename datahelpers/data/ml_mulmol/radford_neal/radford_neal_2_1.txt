we show how the hamiltonian monte carlo algorithm can sometimes be speeded up by splitting the hamiltonian in a way that allows much of the movement around the state space to be done at low computational cost .
one context where this is possible is when the log density of the distribution of interest ( the potential energy function ) can be written as the log of a gaussian density , which is a quadratic function , plus a slowly varying function .
hamiltonian dynamics for quadratic energy functions can be analytically solved .
with the splitting technique , only the slowly - varying part of the energy needs to be handled numer - ically , and this can be done with a larger stepsize ( and hence fewer steps ) than would be necessary with a direct simulation of the dynamics .
another context where splitting helps is when the most important terms of the potential energy function and its gradient can be eval - uated quickly , with only a slowly - varying part requiring costly computations .
with splitting , the quick portion can be handled with a small stepsize , while the costly portion uses a larger stepsize .
we show that both of these splitting approaches can reduce the computational cost of sampling from the posterior distribution for a logistic regression model , using either a gaussian approximation centered on the posterior mode , or a hamiltonian split into a term that depends on only a small number of critical cases , and another term that involves the larger number of cases whose inuence on the posterior distribution is small .
supplemental materials for this paper are available online .
keywords : markov chain monte carlo , hamiltonian dynamics , bayesian analysis
the simple metropolis algorithm ( metropolis et al . , 123 ) is often eective at exploring low -
dimensional distributions , but it can be very inecient for complex , high - dimensional distributions
successive states may exhibit high autocorrelation , due to the random walk nature of the
movement .
faster exploration can be obtained using hamiltonian monte carlo , which was rst
department of statistics , university of california , irvine , usa .
department of statistics and department of computer science , university of toronto , canada .
introduced by duane et al .
( 123 ) , who called it hybrid monte carlo , and which has been recently
reviewed by neal ( 123 ) .
hamiltonian monte carlo ( hmc ) reduces the random walk behavior
of metropolis by proposing states that are distant from the current state , but nevertheless have
a high probability of acceptance .
these distant proposals are found by numerically simulating
hamiltonian dynamics for some specied amount of ctitious time .
for this simulation to be reasonably accurate ( as required for a high acceptance probability ) ,
the stepsize used must be suitably small .
this stepsize determines the number of steps needed to
produce the proposed new state .
since each step of this simulation requires a costly evaluation of
the gradient of the log density , the stepsize is the main determinant of computational cost .
in this paper , we show how the technique of splitting the hamiltonian ( leimkuhler and
reich , 123; neal , 123 ) can be used to reduce the computational cost of producing proposals for
hamiltonian monte carlo .
in our approach , splitting separates the hamiltonian , and consequently
the simulation of the dynamics , into two parts .
we discuss two contexts in which one of these
parts can capture most of the rapid variation in the energy function , but is computationally cheap .
simulating the other , slowly - varying , part requires costly steps , but can use a large stepsize
result is that fewer costly gradient evaluations are needed to produce a distant proposal
illustrate these splitting methods using logistic regression models .
computer programs for our
methods are publicly available from http : / / www . ics . uci . edu / ~ babaks / site / codes . html .
before discussing the splitting technique , we provide a brief overview of hmc .
( see neal ,
123 , for an extended review of hmc . ) to begin , we briey discuss a physical interpretation of
hamiltonian dynamics .
consider a frictionless puck that slides on a surface of varying height .
the state space of this dynamical system consists of its position , denoted by the vector q , and its
momentum ( mass , m , times velocity , v ) , denoted by a vector p .
based on q and p , we dene the
potential energy , u ( q ) , and the kinetic energy , k ( p ) , of the puck .
u ( q ) is proportional to the height of the surface at position q .
the kinetic energy is m|v|123 / 123 , so k ( p ) = |p|123 / ( 123m ) .
as the puck moves on an upward slope , its potential energy increases while its kinetic energy decreases , until
it becomes zero .
at that point , the puck slides back down , with its potential energy decreasing
and its kinetic energy increasing .
the above dynamic system can be represented by a function of q and p known as the hamil -
tonian , which for hmc is usually dened as the sum of a potential energy , u , depending only on
the position and a kinetic energy , k , depending only on the momentum :
h ( q , p ) = u ( q ) + k ( p )
the partial derivatives of h ( q , p ) determine how q and p change over time , according to hamiltons
these equations dene a mapping ts from the state at some time t to the state at time t + s .
we can use hamiltonian dynamics to sample from some distribution of interest by dening
the potential energy function to be minus the log of the density function of this distribution
( plus any constant ) .
the position variables , q , then correspond to the variables of interest
also introduce ctitious momentum variables , p , of the same dimension as q , which will have a
distribution dened by the kinetic energy function .
the joint density of q and p is dened by the
hamiltonian function as
p ( q , p ) =
exp ( h ( q , p ) )
when h ( q , p ) = u ( q ) + k ( p ) , as we assume in this paper , we have
p ( q , p ) =
exp ( u ( q ) ) exp ( k ( p ) )
matrix with elements m123 , .
, md , so that k ( p ) = ( cid : 123 )
so q and p are independent .
typically , k ( p ) = pt m123p / 123 , with m usually being a diagonal i / 123mi .
the pj are then independent and
gaussian with mean zero , with pj having variance mj .
in applications to bayesian statistics , q consists of the model parameters ( and perhaps latent
variables ) , and our objective is to sample from the posterior distribution for q given the observed
data d .
to this end , we set
u ( q ) = log ( p ( q ) l ( q|d ) )
where p ( q ) is our prior and l ( q|d ) is the likelihood function given data d .
having dened a hamiltonian function corresponding to the distribution of interest ( e . g . , a
posterior distribution of model parameters ) , we could in theory use hamiltons equations , ap -
plied for some specied time period , to propose a new state in the metropolis algorithm
hamiltonian dynamics leaves invariant the value of h ( and hence the probability density ) , and
preserves volume , this proposal would always be accepted .
( for a more detailed explanation , see
in practice , however , solving hamiltonians equations exactly is too hard , so we need to ap -
proximate these equations by discretizing time , using some small step size .
for this purpose ,
the leapfrog method is commonly used .
it consists of iterating the following steps :
pj ( t + / 123 ) = pj ( t ) ( / 123 )
qj ( t + ) = qj ( t ) +
( p ( t + / 123 ) )
in a typical case when k ( p ) = ( cid : 123 )
pj ( t + ) = pj ( t + / 123 ) ( / 123 )
( q ( t + ) )
i / 123mi , the time derivative of qj is k / pj = pj / mj
computational cost of a leapfrog step will then usually be dominated by evaluation of u / qj .
we can use some number , l , of these leapfrog steps , with some stepsize , , to propose a new
state in the metropolis algorithm .
we apply these steps starting at the current state ( q , p ) , with ctitious time set to t = 123
the nal state , at time t = l , is taken as the proposal , ( q , p ) .
( to make the proposal symmetric , we would need to negate the momentum at the end of the trajectory , but this can be omitted when , as here , k ( p ) = k ( p ) and p will be replaced ( see below ) before the next update . ) this proposal is then either accepted or rejected ( with the state
remaining unchanged ) , with the acceptance probability being
min ( 123 , exp ( h ( q , p ) + h ( q , p ) ) ) = min ( 123 , exp ( u ( q ) + u ( q ) k ( p ) + k ( p ) ) )
these metropolis updates will leave h approximately constant , and therefore do not explore
the whole joint distribution of q and p .
the hmc method therefore alternates these metropolis
updates with updates in which the momentum is sampled from its distribution ( which is inde -
pendent of q when h has the form in eq .
when k ( p ) = ( cid : 123 )
i / 123mi , each pj is sampled
independently from the gaussian distribution with mean zero and variance mj .
figure 123 : comparison of hamiltonian monte carlo ( hmc ) and random walk metropolis ( rwm )
when applied to a bivariate normal distribution .
left plot : the rst 123 iterations of hmc with
123 leapfrog steps .
right plot : the rst 123 iterations of rwm with 123 updates per iterations .
as an illustration , consider sampling from the following bivariate normal distribution
q n ( , ) , with =
for hmc , we set l = 123 and = 123 .
the left plot in figure 123 shows the rst 123 states from an
hmc run started with q = ( 123 , 123 ) .
the density contours of the bivariate normal distribution are
shown as gray ellipses .
the right plot shows every 123th state from the rst 123 iterations of a run
of a simple random walk metropolis ( rwm ) algorithm .
( this takes time comparable to that for
the hmc run . ) the proposal distribution for rwm is a bivariate normal with the current state
as the mean , and 123i123 as the covariance matrix .
( the standard deviation of this proposal is the
same as the stepsize of hmc . ) figure 123 shows that hmc explores the distribution more eciently ,
with successive samples being further from each other , and autocorrelations being smaller
an extended review of hmc , its properties , and its advantages over the simple random walk
metropolis algorithm , see neal ( 123 ) .
in this example , we have assumed that one leapfrog step for hmc ( which requires evaluating
q123q123q123q123 the gradient of the log density ) takes approximately the same computation time as one metropolis
update ( which requires evaluating the log density ) , and that both move approximately the same
distance .
the benet of hmc comes from this movement being systematic , rather than in a
random walk . 123 we now propose a new approach called split hamiltonian monte carlo ( split
hmc ) , which further improves the performance of hmc by modifying how steps are done , with
the eect of reducing the time for one step or increasing the distance that one step moves .
123 splitting the hamiltonian
as discussed by neal ( 123 ) , variations on hmc can be obtained by using discretizations of
hamiltonian dynamics derived by splitting the hamiltonian , h , into several terms :
h ( q , p ) = h123 ( q , p ) + h123 ( q , p ) + + hk ( q , p )
we use ti , t , for i = 123 , .
, k to denote the mapping dened by hi for time t .
assuming that we can implement hamiltonian dynamics hk exactly , the composition t123 , t123 , .
tk , is a valid discretization of hamiltonian dynamics based on h if the hi are twice dierentiable ( leimkuhler
and reich , 123 ) .
this discretization is symplectic and hence preserves volume .
it will also be
reversible if the sequence of hi are symmetric : hi ( q , p ) = hki+123 ( q , p ) .
indeed , the leapfrog method ( 123 ) can be regarded as a symmetric splitting of the hamiltonian
h ( q , p ) = u ( q ) + k ( p ) as
h ( q , p ) = u ( q ) / 123 + k ( p ) + u ( q ) / 123
in this case , h123 ( q , p ) = h123 ( q , p ) = u ( q ) / 123 and h123 ( q , p ) = k ( p ) .
hamiltonian dynamics for h123 is
123indeed , in this two - dimensional example , it is better to use metropolis with a large proposal standard deviation ,
even though this leads to a low acceptance probability , because this also avoids a random walk .
however , in higher -
dimensional problems with more than one highly - conning direction , a large proposal standard deviation leads to
such a low acceptance probability that this strategy is not viable .
which for a duration of gives the rst part of a leapfrog step .
for h123 , the dynamics is
for time , this gives the second part of the leapfrog step .
hamiltonian dynamics for h123 is the
same as that for h123 since h123 = h123 , giving the the third part of the leapfrog step .
123 splitting the hamiltonian when a partial analytic solution
suppose the potential energy u ( q ) can be written as u123 ( q ) + u123 ( q ) .
then , we can split h as
h ( q , p ) = u123 ( q ) / 123 + ( u123 ( q ) + k ( p ) ) + u123 ( q ) / 123
here , h123 ( q , p ) = h123 ( q , p ) = u123 ( q ) / 123 and h123 ( q , p ) = u123 ( p ) + k ( p ) .
the rst and the last terms in
this splitting are similar to eq .
( 123 ) , except that u123 ( q ) replaces u ( q ) , so the rst and the last part
of a leapfrog step remain as before , except that we use u123 ( q ) rather than u ( q ) to update p
suppose that the middle part of the leapfrog , which is based on the hamiltonian u123 ( q ) + k ( p ) ,
can be handled analytically that is , we can compute the exact dynamics for any duration of
we hope that since this part of the simulation introduces no error , we will be able to use
a larger step size , and hence take fewer steps , reducing the computation time for the dynamical
we are mainly interested in situations where u123 ( q ) provides a reasonable approximation to
u ( q ) , and in particular on bayesian applications , where we approximate u by focusing on the
the posterior mode , q , and the second derivatives of u at that point .
we can obtain q using fast
methods such as newton - raphson iteration when analytical solutions are not available .
we then approximate u ( q ) with u123 ( q ) , the energy function for n ( q , j 123 ( q ) ) , where j ( q ) is the hessian matrix of u at q .
finally , we set u123 ( q ) = u ( q ) u123 ( q ) , the error in this approximation .
beskos et al .
( 123 ) have recently proposed a similar splitting strategy for hmc , in which a
gaussian component is handled analytically , in the context of high - dimensional approximations
to a distribution on an innite - dimensional hilbert space .
in such applications , the gaussian
distribution will typically be derived from the problem specication , rather than being found as a
numerical approximation , as we do here .
using a normal approximation in which u123 ( q ) = 123
( the energy for the standard normal distribution ) , h123 ( q , p ) = u123 ( q ) + k ( p ) in eq .
( ( 123 ) ) will be
123 ( q q ) tj ( q ) ( q q ) , and letting k ( p ) = 123
quadratic , and hamiltons equations will be a system of rst - order linear dierential equations that can be handled analytically ( polyanin et al . , 123 ) .
specically , setting q = q q , the dynamical equations can be written as follows :
j ( q ) 123
where i is the identity matrix .
dening x = ( q , p ) , this can be written as
j ( q ) 123
x ( t ) = ax ( t ) ,
the solution of this system is x ( t ) = eat x123 , where x123 is the initial value at time t = 123 , and eat = i + ( at ) + ( at ) 123 / 123 ! + is a matrix exponential .
this can be simplied by diagonalizing the coecient matrix a as
a = d123
where is invertible and d is a diagonal matrix .
the system of equations can then be written as
now , let y ( t ) = 123x ( t )
x ( t ) = d123x ( t )
y ( t ) = dy ( t )
the solution for the above equation is y ( t ) = edt y123 , where y123 = 123x123
therefore ,
x ( t ) = edt 123 x123
and edt can be easily computed by simply exponentiating the diagonal elements of d times t .
algorithm 123 : leapfrog for split hamiltonian
algorithm 123 : nested leapfrog for split hamil -
monte carlo with a partial analytic solution .
tonian monte carlo with splitting of data .
sample initial values for p from n ( 123 , i )
sample initial values for p from n ( 123 , i )
for ( cid : 123 ) = 123 to l do
for ( cid : 123 ) = 123 to l do
p p ( / 123 ) q q q x123 ( q , p ) ( q , p ) rx123 q q + q p p ( / 123 )
p p ( / 123 ) for m = 123 to m do p p ( / 123m ) q q + ( / m ) p p p ( / 123m )
p p ( / 123 )
the above analytical solution is of course for the middle part ( denoted as h123 ) of eq .
( 123 ) only .
we still need to approximate the overall hamiltonian dynamics based on h , using the leapfrog
method .
algorithm 123 shows the corresponding leapfrog steps after an initial step of size / 123
based on u123 ( q ) , we obtain the exact solution for a time step of based on h123 ( q , p ) = u123 ( q ) + k ( p ) ,
and nish by taking another step of size / 123 based on u123 ( q ) .
123 splitting the hamiltonian by splitting the data
the method discussed in the previous section requires that we be able to handle the hamiltonian
h123 ( q , p ) = u123 ( q ) + k ( p ) analytically .
if this is not so , splitting the hamiltonian in this way may
still be benecial if the computational cost for u123 ( q ) is substantially lower than for u ( q ) .
in these
situations , we can use the following split :
h ( q , p ) = u123 ( q ) / 123 +
( u123 ( q ) / 123m + k ( p ) / m + u123 ( q ) / 123m ) + u123 ( q ) / 123
for some m > 123
the above discretization can be considered as a nested leapfrog , where the outer
part takes half steps to update p based on u123 alone , and the inner part involves m leapfrog steps
of size / m based on u123
algorithm 123 implements this nested leapfrog method .
for example , suppose our statistical analysis involves a large data set with many observations ,
but we believe that a small subset of data is sucient to build a model that performs reasonably
well ( i . e . , compared to the model that uses all the observations ) .
in this case , we can construct
u123 ( q ) based on a small part of the observed data , and use the remaining observations to construct
if this strategy is successful , we will able to use a large stepsize for steps based on u123 ,
reducing the cost of a trajectory computation .
in detail , we divide the observed data , y , into two subsets : r123 , which is used to construct
u123 ( q ) , and r123 , which is used to construct u123 :
u ( ) = u123 ( ) + u123 ( )
u123 ( ) = log ( p ( ) ) ( cid : 123 ) u123 ( ) = ( cid : 123 )
note that the prior appears in u123 ( ) only .
neal ( 123 ) discusses a related strategy for splitting the hamiltonian by splitting the observed
data into multiple subsets .
however , instead of randomly splitting data , as proposed there , here
we split data by building an initial model based on the maximum a posterior ( map ) estimate , q ,
and use this model to identify a small subset of data that captures most of the information in the
full data set .
we next illustrate this approach for logistic regression models .
123 application of split hmc to logistic regression models
we now look at how split hmc can be applied to bayesian logistic regression models for binary
classication problems .
we will illustrate this method using the simulated data set with n = 123
data points and p = 123 covariates that is shown in figure 123
the logistic regression model assigns probabilities to the two possible classes ( denoted by 123
and 123 ) in case i ( for i = 123 , .
, n ) as follows :
p ( yi = 123| xi , , ) =
exp ( + xt
123 + exp ( + xt
here , xi is the vector of length p with the observed values of the covariates in case i , is the
figure 123 : an illustrative binary classication problem with n = 123 data points and two covariates ,
x123 and x123 , with the two classes represented by white circles and black squares .
intercept , and is the vector of p regression coecients .
we use to denote the vector of all p + 123
unknown parameters , ( , ) .
proportional to p ( ) ( cid : 123 ) n
let p ( ) be the prior distribution for .
the posterior distribution of given x and y is
i=123 p ( yi|xi , ) .
the corresponding potential energy function is
u ( ) = log ( p ( ) ) n ( cid : 123 )
log ( p ( yi|xi , ) )
we assume the following ( independent ) priors for the model parameters :
n ( 123 , 123 j n ( 123 , 123
j = 123 ,
where and are known constants .
the potential energy function for the above logistic regression model is therefore as follows :
u ( ) =
( yi ( + xt
i ) log ( 123 + exp ( + xt
- 123 - 123 - 123 - 123x123x123 the partial derivatives of the energy function with respect to and the j are
( yi exp ( + xt
123 + exp ( + xt
xij ( yi exp ( + xt
123 + exp ( + xt
123 split hmc with a partial analytical solution for a logistic model
to apply algorithm 123 for split hmc to this problem , we approximate the potential energy function
u ( ) for the logistic regression model with the potential energy function u123 ( ) of the normal distribution n ( , j 123 ( ) ) , where is the map estimate of model parameters .
u123 ( ) usually provides a reasonable approximation to u ( ) , as illustrated in figure 123
in the plot on the left ,
the solid curve shows the value of the potential energy , u , as 123 varies , with 123 and xed to their
map values , while the dashed curve shows u123 for the approximating normal distribution
right plot of figure 123 compares the partial derivatives of u and u123 with respect to 123 , showing
that u123 / j provides a reasonable linear approximation to u / j .
since there is no error when solving hamiltonian dynamics based on u123 ( ) , we would expect
that the total discretization error of the steps taken by algorithm 123 will be less that for the
standard leapfrog method , for a given stepsize , and that we will therefore be able to use a larger
stepsize and hence need fewer steps for a given trajectory length while still maintaining a
good acceptance rate .
the stepsize will still be limited to the region of stability imposed by the discretization error from u123 = u u123 , but this limit will tend to be larger than for the standard
123 split hmc with splitting of data for a logistic model
to apply algorithm 123 to this logistic regression model , we split the hamiltonian by splitting the
data into two subsets .
consider the illustrative example discussed above .
in the left plot of figure 123 , the thick line represents the classication boundary using the map estimate , .
for the points
that fall on this boundary line , the estimated probabilities for the two groups are equal , both
being 123 / 123
the probabilities of the two classes become less equal as the distance of the covariates
from this line increases .
we will dene u123 using the points within the region , r123 , within some
distance of this line , and dene u123 using the points in the region , r123 , at a greater distance from
figure 123 : left plot : the potential energy , u , for the logistic regression model ( solid curve ) and
its normal approximation , u123 ( dashed curve ) , as 123 varies , with other parameters at their map
values .
right plot : the partial derivatives of u and u123 with respect to 123
this line .
equivalently , r123 contains those points for which the probability that y = 123 ( based on
the map estimates ) is closest to 123 / 123
the shaded area in figure 123 shows the region , r123 , containing the 123% of the observations
closest to the map line , or equivalently the 123% of observations for which the probability of
class 123 is closest ( in either direction ) to 123 / 123
the unshaded region containing the remaining data
points is denoted as r123
using these two subsets , we can split the energy function u ( ) into two
terms : u123 ( ) based on the data points that fall within r123 , and u123 based on the data points that
fall within r123 ( see eq .
then , we use eq .
( 123 ) to split the hamiltonian dynamics .
note that u123 is not used to approximate the potential energy function , u , for the acceptance
test at the end of the trajectory the exact value of u is used for this test , ensuring that the
equilibrium distribution is exactly correct .
rather , u123 / j is used to approximate u / j , which
is the costly computation when we simulate hamiltonian dynamics .
to see that it is appropriate to split the data according to how close the probability of class 123
is to 123 / 123 , note rst that the leapfrog step of eq .
( 123 ) will have no error when the derivatives u / qj
123energy functionuu123 - 123 - 123 - 123 - 123partial derivative of the energy function w . r . t .
123u / 123u123 / 123 figure 123 : left plot : a split of the data into two parts based on the map model , represented by
the solid line; the energy function u is then divided into u123 , based on the data points in r123 , and
u123 , based on the data points in r123
right plot : the partial derivatives of u and u123 with respect
to 123 , with other parameters at their map values .
do not depend on q that is , when the second derivatives of u are zero .
recall that for the
from which we get
yi exp ( + xt 123 + exp ( + xt
exp ( + xt
123 + exp ( + xt
( cid : 123 ) exp ( + xt
123 + exp ( + xt
xijxij ( cid : 123 ) p ( yi = 123|xi , , ) ( 123 p ( yi = 123|xi , , ) )
the product p ( yi = 123|xi , , ) ( 123 p ( yi = 123|xi , , ) ) is symmetrical around its maximum where p ( yi = 123|xi , , ) is 123 / 123 , justifying our criterion for selecting points in r123
the right plot of figure 123 shows the approximation of u / 123 by u123 / 123 with 123 and xed to their map values .
- 123 - 123 - 123 - 123x123x123r123r123r123 - 123 - 123 - 123 - 123partial derivative of the energy function w . r . t .
123u / 123u123 / 123 123 experiments
in this section , we use simulated and real data to compare our proposed methods to standard
for each problem , we set the number of leapfrog steps to l = 123 for standard hmc , and
nd such that the acceptance probability ( ap ) is close to 123 ( neal , 123 ) .
we set l and for
the split hmc methods such that the trajectory length , l , remains the same , but with a larger
stepsize and hence a smaller number of steps .
note that this trajectory length is not necessarily
optimal for these problems , but this should not aect our comparisons , in which the length is kept
we try to choose for the split hmc methods such that the acceptance probability is equal to
that of standard hmc .
however , increasing the stepsize beyond a certain point leads to instability
of trajectories , in which the error of the hamiltonian grows rapidly with l ( neal , 123 ) , so that
proposals are rejected with very high probability .
this sometimes limits the stepsize of split hmc
to values at which the acceptance probability is greater than the 123 aimed at for standard hmc .
additionally , to avoid near periodic hamiltonian dynamics ( neal , 123 ) , we randomly vary the
stepsize over a small range .
specically , at each iteration of mcmc , we sample the stepsize from
the uniform ( 123 , ) distribution , where is the reported stepsize for each experiment .
to measure the eciency of each sampling method , we use the autocorrelation time ( act )
by dividing the n posterior samples into batches of size b , and estimating act as follows ( neal ,
123; geyer , 123 ) :
here , s123 is the sample variance and s123
b is the sample variance of batch means .
following thomp - son ( 123 ) , we divide the posterior samples into n 123 / 123 batches of size b = n 123 / 123
throughout
this section , we set the number of markov chain monte carlo ( mcmc ) iterations for simulating
posterior samples to n = 123
the autocorrelation time can be roughly interpreted as the number of mcmc transitions
required to produce samples that can be considered as independent .
for the logistic regression
problems discussed in this section , we could nd the autocorrelation time separately for each pa -
rameter and summarize the autocorrelation times using their maximum value ( i . e . , for the slowest
moving parameter ) to compare dierent methods .
however , since one common goal is to use
logistic regression models for prediction , we look at the autocorrelation time for the log likelihood ,
normal appr .
data splitting
table 123 : split hmc ( with normal approximation and data splitting ) compared to standard hmc
using simulated data , on a data set with n = 123 observations and p = 123 covariates .
i=123 log ( p ( yi|xi , ) ) using the posterior samples of .
we also look at the autocorrelation time j ( j ) 123 ( denoting it ) , since this may be more relevant when the goal is interpretation of
we adjust ( and similarly ) to account for the varying computation time needed by the dierent methods in two ways .
one is to compare dierent methods using s , where s is the cpu time per iteration , using an implementation written in r .
this measures the cpu time required
to produce samples that can be regarded as independent samples .
we also compare in terms of g , where g is the number of gradient computations on the number of cases in the full data set required for each trajectory simulated by hmc .
this will be equal to the number of leapfrog steps ,
l , for standard hmc or split hmc using a normal approximation .
when using data splitting with a fraction f of data in r123 and m inner leapfrog steps , g will be ( f m + ( 123f ) ) l .
in general , we expect that computation time will be dominated by the gradient computations counted by g , so that g will provide a measure of performance independent of any particular implementation .
in our experiments , s was close to being proportional to g , except for slightly larger than expected
times for split hmc with data splitting .
123 . 123 simulated data
we rst tested the methods on a simulated data set with 123 covariates and 123 observations .
the covariates were sampled as xij n ( 123 , 123 j ) , for i = 123 , .
, 123 and j = 123 , .
, 123 , with j set to 123 for the rst ve variables , to 123 for the next ve variables , and to 123 for the remaining 123
variables .
we sampled true parameter values , and j , independently from n ( 123 , 123 ) distributions .
finally , we sampled the class labels according to the model , as yi bernoulli ( pi ) with logit ( pi ) =
for the bayesian logistic regression model , we assumed normal priors with mean zero and
standard deviation 123 for and j , where j = 123 , .
we ran standard hmc , split hmc
with normal approximation , and split hmc with data splitting for n = 123 iterations .
for the standard hmc , we set l = 123 and = 123 , so the trajectory length was 123 123 = 123 .
for split hmc with normal approximation and split hmc with data splitting , we reduce the number
of leapfrog steps to 123 and 123 respectively , while increasing the stepsizes so that the trajectory
length remained 123 .
for the data splitting method , we use 123% of the data points for u123 and set
m = 123 , which makes g equal 123l .
for this example , we set l = 123 so g = 123 , which is smaller
than g = l = 123 used for the standard hmc algorithm
table 123 shows the results for the three methods .
the cpu times ( in seconds ) per iteration , s , and s for the split hmc methods are substantially lower than for standard hmc .
the comparison is similar looking at g .
based on s and g , however , the improvement in eciency is more substantial for the data splitting method compared to the normal approximation
method mainly because of the dierence in their corresponding values of .
123 . 123 results on real data sets
in this section , we evaluate our proposed method using three real binary classication prob -
the data for these three problems are available from the uci machine learning repository
( http : / / archive . ics . uci . edu / ml / index . html ) .
for all data sets , we standardized the numeri -
cal variables to have mean zero and standard deviation 123
further , we assumed normal priors with
mean zero and standard deviation 123 for the regression parameters .
we used the setup described
at the beginning of section 123 , running each markov chain for n = 123 iterations .
table 123
n = 123 , p = 123
n = 123 , p = 123
n = 123 , p = 123
normal appr .
data splitting
table 123 : hmc and split hmc ( normal approximation and data splitting ) on three real data sets .
summarizes the results using the three sampling methods .
the rst problem , statlog , involves using multi - spectral values of pixels in a satellite image in
order to classify the associated area into soil or cotton crop .
( in the original data , dierent types
of soil are identied . ) the sample size for this data set is n = 123 , and the number of features is
p = 123
for the standard hmc , we set l = 123 and = 123 .
for the two split hmc methods with
normal approximation and data splitting , we reduce l to 123 and 123 respectively while increasing so l remains the same as that of the standard hmc .
for the data splitting methods , we use 123% of data points for u123 and set m = 123
as seen in the table , the split hmc methods
improve eciency with the data splitting method performing substantially better than the normal
the second problem , ctg , involves analyzing 123 fetal cardiotocograms along with their
respective diagnostic features ( de campos et al . , 123 ) .
the objective is to determine whether the
fetal state class is pathologic or not .
the data include 123 observations and 123 features
the standard hmc , we set l = 123 and = 123 .
we reduced the number of leapfrog steps to 123
and 123 for split hmc with normal approximation and data splitting respectively .
for the latter ,
we use 123% of data points for u123 and set m = 123
both splitting methods improved performance
signicantly .
as before , the data splitting method outperforms the normal approximation method .
the objective of the last problem , chess , is to predict chess endgame outcomes either white
can win or white cannot win .
this data set includes n = 123 instances , where each instance
is a board - descriptions for the chess endgame .
there are p = 123 attributes describing the board .
for the standard hmc , we set l = 123 and = 123 .
for the two split hmc methods with normal
approximation and data splitting , we reduced l to 123 and 123 respectively .
for the data splitting
method , we use 123% of the data points for u123 and set m = 123
using the split hmc methods ,
the computational eciency is improved substantially compared to standard hmc .
this time
however , the normal approximation approach performs better than the data splitting method in terms of g , s , and s , while the latter performs better in terms of g .
we have proposed two new methods for improving the eciency of hmc , both based on splitting
the hamiltonian in a way that allows much of the movement around the state space to be performed
at low computational cost .
while we demonstrated our methods on binary logistic regression models , they can be extended
to multinomial logistic ( mnl ) models for multiple classes .
for mnl models , the regression
parameters for p covariates and j classes form a matrix of ( p + 123 ) rows and j columns , which we can vectorize so that the model parameters , , become a vector of ( p + 123 ) j elements .
for split hmc with normal approximation , we can dene u123 ( ) using an approximate multivariate normal n ( , j 123 ( ) ) as before .
for split hmc with data splitting , we can still construct u123 ( ) using a small subset of data , based on the class probabilities for each data item found using the
map estimates for the parameters ( the best way of doing this is a subject for future research ) .
the data splitting method could be further extended to any model for which it is feasible to nd
a map estimate , and then divide the data into two parts based on residuals of some form .
another area for future research is to look for tractable approximations to the posterior dis -
tribution other than normal distributions .
one could also investigate other methods for splitting
the hamiltonian dynamics by splitting the data for example , tting a support vector machine
( svm ) to binary classication data , and using the support vectors for constructing u123
while the results on simulated data and real problems presented in this paper have demon -
strated the advantages of splitting the hamiltonian dynamics in terms of improving the sampling
eciency , our proposed methods do require preliminary analysis of data , mainly , nding the map
estimate .
the performance of our approach obviously depends on how well the corresponding
normal distribution based on map estimates approximates the posterior distribution , or how well
a small subset of data found using this map estimate captures the overall patterns in the whole
data set .
this preliminary analysis involves some computational overhead , but the computational
cost associated with nding the map estimate is often negligible compared to the potential im -
provement in sampling eciency for the full bayesian model .
for the data splitting method , one
could also consider splitting based on the class probabilities produced by a model that is simpler
than the one being tted using hmc .
another approach to improving hmc has recently been proposed by girolami and calderhead
( 123 ) .
their method , riemannian manifold hmc ( rmhmc ) , can also substantially improve
performance .
rmhmc utilizes the geometric properties of the parameter space to explore the
best direction , typically at higher computational cost , to produce distant proposals with high
probability of acceptance .
in contrast , our method attempts to nd a simple approximation to
the hamiltonian to reduce the computational time required for reaching distant states .
possible that these approaches could be combined , to produce a method that performs better
than either method alone .
the recent proposals of homan and gelman ( 123 ) for automatic
tuning of hmc could also be combined with our split hmc methods .
