multiple realizations of continuous - valued time series from a stochastic process often contain systematic variations in rate and amplitude .
to leverage the information contained in such noisy replicate sets , we need to align them in an appropriate way ( for example , to allow the data to be properly combined by adaptive averaging ) .
we present the continuous prole model ( cpm ) , a generative model in which each observed time series is a non - uniformly subsampled version of a single latent trace , to which local rescaling and additive noise are applied .
after unsupervised training , the learned trace represents a canonical , high resolution fusion of all the replicates .
as well , an alignment in time and scale of each observation to this trace can be found by inference in the model .
we apply cpm to successfully align speech signals from multiple speakers and sets of liquid chromatography - mass spectrometry proteomic data .
123 a prole model for continuous data
when observing multiple time series generated by a noisy , stochastic process , large sys - tematic sources of variability are often present .
for example , within a set of nominally replicate time series , the time axes can be variously shifted , compressed and expanded , in complex , non - linear ways .
additionally , in some circumstances , the scale of the mea - sured data can vary systematically from one replicate to the next , and even within a given
we propose a continuous prole model ( cpm ) for simultaneously analyzing a set of such time series .
in this model , each time series is generated as a noisy transformation of a single latent trace .
the latent trace is an underlying , noiseless representation of the set of replicated , observable time series .
output time series are generated from this model by moving through a sequence of hidden states in a markovian manner and emitting an observable value at each step , as in an hmm .
each hidden state corresponds to a particular location in the latent trace , and the emitted value from the state depends on the value of the latent trace at that position .
to account for changes in the amplitude of the signals across and within replicates , the latent time states are augmented by a set of scale states , which control how the emission signal will be scaled relative to the value of the latent trace .
during training , the latent trace is learned , as well as the transition probabilities controlling the markovian evolution of the scale and time states and the overall noise level of the
observed data .
after training , the latent trace learned by the model represents a higher resolution fusion of the experimental replicates .
figure 123 illustrate the model in action .
unaligned , linear warp alignment and cpm alignment
figure 123 : a ) top : ten replicated speech energy signals as described in section 123 ) , middle : same signals , aligned using a linear warp with an offset , bottom : aligned with cpm ( the learned latent trace is also shown in cyan ) .
b ) speech waveforms corresponding to energy signals in a ) , top : unaligned originals , bottom : aligned using cpm .
123 dening the continuous prole model ( cpm )
123 ; xk
123 ; : : : ; xk
the the cpm is generative model for a set of k time series , ~ xk = ( xk temporal sampling rate within each ~ xk need not be uniform , nor must it be the same across the different ~ xk .
constraints on the variability of the sampling rate are discussed at the end of this section .
for notational convenience , we henceforth assume n k = n for all k , but this is not a requirement of the model .
the cpm is set up as follows : we assume that there is a latent trace , ~ z = ( z123; z123; : : : ; zm ) , a canonical representation of the set of noisy input replicate time series .
any given observed time series in the set is modeled as a non - uniformly subsampled version of the latent trace to which local scale transformations have been applied .
ideally , m would be innite , or at least very large relative to n so that any experimental data could be mapped precisely to the correct underlying trace point .
aside from the computational impracticalities this would pose , great care to avoid overtting would have to be taken .
thus in practice , we have used m = ( 123 + ( cid : 123 ) ) n ( double the resolution , plus some slack on each end ) in our experiments and found this to be sufcient with ( cid : 123 ) < 123 : 123
because the resolution of the latent trace is higher than that of the observed time series , experimental time can be made effectively to speed up or slow down by advancing along the latent trace in larger or smaller jumps .
the subsampling and local scaling used during the generation of each observed time se - ries are determined by a sequence of hidden state variables .
let the state sequence for observation k be ~ ( cid : 123 ) k .
each state in the state sequence maps to a time state / scale state pair : i g .
time states belong to the integer set ( 123 : : m ) ; scale states belong to an i ! f ( cid : 123 ) k ( in our experiments we have used q=123 , evenly spaced scales in ordered set ( ( cid : 123 ) 123 : : ( cid : 123 ) q ) .
i , are related by the emission logarithmic space ) .
states , ( cid : 123 ) k probability distribution : a ( cid : 123 ) k i uk; ( cid : 123 ) ) , where ( cid : 123 )
i , and observation values , xk
i ; ~ z; ( cid : 123 ) ; uk ) ( cid : 123 ) n ( xk
i ; ( cid : 123 ) k
i j ~ z ) ( cid : 123 ) p ( xk
i ; z ( cid : 123 ) k
is the noise level of the observed data , n ( a; b; c ) denotes a gaussian probability density for a with mean b and standard deviation c .
the uk are real - valued scale parameters , one per observed time series , that correct for any overall scale difference between time series k and the latent trace .
to fully specify our model we also need to dene the state transition probabilities .
we dene the transitions between time states and between scale states separately , so that ( cid : 123 ) i ( cid : 123 ) 123; ( cid : 123 ) i ( cid : 123 ) p ( ( cid : 123 ) ij ( cid : 123 ) i ( cid : 123 ) 123 ) = p ( ( cid : 123 ) ij ( cid : 123 ) i ( cid : 123 ) 123 ) pk ( ( cid : 123 ) ij ( cid : 123 ) i ( cid : 123 ) 123 ) .
the constraint that time must move forward , cannot stand still , and that it can jump ahead no more than j ( cid : 123 ) time states is en - forced .
( in our experiments we used j ( cid : 123 ) = 123 ) as well , we only allow scale state transitions between neighbouring scale states so that the local scale cannot jump arbitrarily .
these constraints keep the number of legal transitions to a tractable computational size and work well in practice .
each observed time series has its own time transition probability dis - tribution to account for experiment - specic patterns .
both the time and scale transition probability distributions are given by multinomials :
pk ( ( cid : 123 ) i = aj ( cid : 123 ) i ( cid : 123 ) 123 = b ) =
p ( ( cid : 123 ) i = aj ( cid : 123 ) i ( cid : 123 ) 123 = b ) =123>><
if a ( cid : 123 ) b = 123 if a ( cid : 123 ) b = 123
if a ( cid : 123 ) b = j ( cid : 123 )
if d ( a; b ) = 123 if d ( a; b ) = 123 if d ( a; b ) = ( cid : 123 ) 123
where d ( a; b ) = 123 means that a is one scale state larger than b , and d ( a; b ) = ( cid : 123 ) 123 means that a is one scale state smaller than b , and d ( a; b ) = 123 means that a = b .
the distributions
are constrained by : pj ( cid : 123 )
i = 123 and 123s123 + s123 = 123
j ( cid : 123 ) determines the maximum allowable instantaneous speedup of one portion of a time series relative to another portion , within the same series or across different series .
however , the length of time for which any series can move so rapidly is constrained by the length of the latent trace; thus the maximum overall ratio in speeds achievable by the model between any two entire time series is given by min ( j ( cid : 123 ) ; m after training , one may examine either the latent trace or the alignment of each observable time series to the latent trace .
such alignments can be achieved by several methods , in - cluding use of the viterbi algorithm to nd the highest likelihood path through the hidden states ( 123 ) , or sampling from the posterior over hidden state sequences .
we found viterbi alignments to work well in the experiments below; samples from the posterior looked quite
123 training with the expectation - maximization ( em ) algorithm
as with hmms , training with the em algorithm ( often referred to as baum - welch in the context of hmms ( 123 ) ) , is a natural choice .
in our model the e - step is computed exactly using the forward - backward algorithm ( 123 ) , which provides the posterior probability over s ( i ) ( cid : 123 ) p ( ( cid : 123 ) i = sj ~ x ) and also states for each time point of every observed time series , ( cid : 123 ) k the pairwise state posteriors , ( cid : 123 ) s;t ( i ) ( cid : 123 ) p ( ( cid : 123 ) i ( cid : 123 ) 123 = s; ( cid : 123 ) i = tj ~ xk ) .
the algorithm is modied
only in that the emission probabilities depend on the latent trace as described in section 123
the m - step consists of a series of analytical updates to the various parameters as detailed
given the latent trace ( and the emission and state transition probabilities ) , the complete log likelihood of k observed time series , ~ xk , is given by lp ( cid : 123 ) l + p .
l is the likelihood term arising in a ( conditional ) hmm model , and can be obtained from the forward - backward algorithm .
it is composed of the emission and state transition terms .
p is the log prior ( or penalty term ) , regularizing various aspects of the model parameters as explained below .
these two terms are :
log a ( cid : 123 ) i ( xk
i j ~ z ) +
xk=123 log p ( ( cid : 123 ) 123 ) +
log t k
p ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
( zj+123 ( cid : 123 ) zj ) 123 +
v g ) + log d ( svjf ( cid : 123 ) 123
where p ( ( cid : 123 ) 123 ) are priors over the initial states .
the rst term in equation 123 is a smoothing penalty on the latent trace , with ( cid : 123 ) controlling the amount of smoothing .
( cid : 123 ) k dirichlet hyperprior parameters for the time and scale state transition probability distribu - tions respectively .
these ensure that all non - zero transition probabilities remain non - zero .
v corresponds to the pseudo - count data for for the time state transitions , v 123 f123; j ( cid : 123 ) g and ( cid : 123 ) k the parameters d123 , d123 .
dj ( cid : 123 ) .
for the scale state transitions , v 123 f123; 123g and ( cid : 123 ) k to the pseudo - count data for the parameters s123 and s123
letting s be the total number of possible states , that is , the number of elements in the cross - product of possible time states and possible scale states , the expected complete log
v and ( cid : 123 ) 123
s ( 123 ) log t k
s ( i ) log as ( xk
i j ~ z ) + : : :
: : : +
s;s123 ( i ) log t k
123;s ( cid : 123 ) p ( ( cid : 123 ) 123 = s ) , and where ( cid : 123 ) k
using the notation t k s;s123 ( i ) are the posteriors over states as dened above .
taking derivatives of this quantity with respect to each of the parameters and nding the critical points provides us with the m - step update equations .
in updating the latent trace ~ z we obtain a system of m simultaneous equations , for j = 123 : : m :
s ( i ) and ( cid : 123 ) k
s ( i ) ( cid : 123 ) suk ( xk
i ( cid : 123 ) zjuk ( cid : 123 ) s )
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( 123zj ( cid : 123 ) 123zj ( cid : 123 ) 123 ( cid : 123 ) 123zj+123 )
for the cases j = 123; n , the terms 123zj ( cid : 123 ) 123 and 123zj+123 , respectively , drop out .
considering all such equations we obtain a system of m equations in m unknowns .
each equation depends only linearly on three variables from the latent trace .
thus the solution is easily obtained numerically by solving a tridiagonal linear system .
analytic updates for ( cid : 123 ) 123 and uk are given by :
( cid : 123 ) 123 = ps
i ( cid : 123 ) z ( cid : 123 ) s uk ( cid : 123 ) s ) 123
s=123 z ( cid : 123 ) s ( cid : 123 ) spn uk = ps
lastly , updates for the scale and state transition probability distributions are given by :
where h ( s; j ) ( cid : 123 ) fs123js123is exactly j scale states away from sg .
not normalize the dirichlets , and omit v and d ( svjf ( cid : 123 ) 123
note that we do the traditional minus one in the exponent :
the m - step updates uk , ( cid : 123 ) , and ~ z are coupled .
thus we arbitrarily pick an order to update them and as one is updated , its new values are used in the updates for the coupled parameter updates that follow it .
in our experiments we updated in the following order : ( cid : 123 ) , ~ z , uk .
the other two parameters , dk
v and sv , are completely decoupled .
v g ) =qj ( cid : 123 )
123 experiments with laboratory and speech data
we have applied the cpm model to analyze several liquid chromatography - mass spec - trometry ( lc - ms ) data sets from an experimental biology laboratory .
mass spectrometry technology is currently being developed to advance the eld of proteomics ( 123 , 123 ) .
a mass spectrometer takes a sample as input , for example , human blood serum , and produces a measure of the abundance of molecules that have particular mass / charge ratios .
in pro - teomics the molecules in question are small protein fragments .
from the pattern of abun - dance values one can hope to infer which proteins are present and in what quantity .
for protein mixtures that are very complex , such as blood serum , a sample preparation step is used to physically separate parts of the sample on the basis of some property of the molecules , for example , hydrophobicity .
this separation spreads out the parts over time so that at each unique time point a less complex mixture is fed into the mass spectrometer .
the result is a two - dimensional time series spectrum with mass / charge on one axis and time of input to the mass spectrometer on the other .
in our experiments we collapsed the data at each time point to one dimension by summing together abundance values over all mass / charge values .
this one - dimensional data is referred to as the total ion count ( tic ) .
we discuss alternatives to this in the last section .
after alignment of the tics , we assessed the alignment of the lc - ms data by looking at both the tic alignments , and also the cor - responding two - dimensional alignments of the non - collapsed data , which is where the true
the rst data set was a set of 123 replicates , each using protein extracted from lysed e .
coli cells .
proteins were digested and subjected to capillary - scale lc - ms coupled on - line to an ion trap mass spectrometer .
first we trained the model with no smoothing ( i . e . , ( cid : 123 ) = 123 ) on the 123 replicates .
this provided nice alignments when viewed in both the tic space and the full two - dimensional space .
next we used leave - one - out cross - validation on six of the v are time series replicates in order to choose a suitable value for ( cid : 123 ) .
because the uk and dk specic , we ran a restricted em on the hold - out case to learn these parameters , holding the other parameters xed at the values found from learning on the training set .
sixteen values of ( cid : 123 ) over ve orders of magnitude , and also zero , were used .
note that we did not include the regularization likelihood term in the calculations of hold - out likelihood .
one of the non - zero values was found to be optimal ( statistically signicant at a p=123 level using a paired sample t - test to compare it to no smoothing ) .
visually , there did not appear to be a difference between no regularization and the optimal value of ( cid : 123 ) , in either the tic space
or the full two - dimensional space .
figure 123 shows the alignments applied to the tics and also the two - dimensional data , using the optimal value of ( cid : 123 ) .
unaligned and aligned time series
original time series
aligned experimental time series
123 time jump from previous state
figure 123 : figure 123 : a ) top : 123 replicate pre - processed tics as described in section 123 ) , bottom : same as top , but aligned with cpm ( the learned latent trace is also shown ) .
b ) the fth tic replicate aligned to the learned latent trace ( inset shows the original , unaligned ) .
below are three strips showing , from top - to - bottom , i ) the error residual , ii ) the number of time states moved between every two states in the viterbi alignment , and iii ) the local scaling applied at each point in the alignment .
c ) a portion of the two - dimensional lc - ms data from replicates two ( in red ) and four ( in green ) .
d ) same as c ) , but after alignment ( the same one dimensional alignment was applied to every mass / charge value ) .
marker lines labeled a to f show how time in c ) was mapped to latent time using the viterbi alignment .
we also trained our model on ve different sets of lc - ms data , each consisting of human blood serum .
we used no smoothing and found the results visually similar in quality to the rst data set .
to ensure convergence to a good local optimum and to speed up training , we pre - processed the lc - ms data set by coarsely aligning and scaling each time series as follows : we 123 ) translated each time series so that the center of mass of each time series was aligned to the median center of mass over all time series , 123 ) scaled the abundance values such that the sum of abundance values in each time series was equal to the median sum of abundance values over all time series .
we also used our model to align 123 speech signals , each an utterance of the same sentence
spoken by a different speaker .
the short - time energy ( using a 123ms hanning window ) was computed every 123ms for each utterance and the resulting vectors were used as the input to cpm for alignment .
the smoothing parameter ( cid : 123 ) was set to zero .
for comparison , we also performed a linear warping of time with an offset .
each signal was translated so as to start at the same time , and the length of each signal was stretched or compressed so as to each occupy the same amount of time ) .
figure 123 shows the successful alignment of the speech signals by cpm and also the ( unsuccessful ) linear warp .
audio for this exam - ple can be heard at http : / / www . cs . toronto . edu / jenn / alignmentstudy , which also contains some supplemental gures for the paper .
initialization for em training was performed as follows : ( cid : 123 ) was set to 123% of the difference between the maximum and minimum values of the rst time series .
the latent trace was initialized to be the rst observed time series , with gaussian , zero - mean noise added , with standard deviation equal to ( cid : 123 ) .
this was then upsampled by a factor of two by repeating every value twice in a row .
the additional slack at either end of the latent trace was set to be the minimum value seen in the given time series .
the uk were each set to one and the multinomial scale and state transition probabilities were set to be uniform .
123 related algorithms and models
our proposed cpm has many similarities to input / output hmms ( iohmms ) , also called conditional hmms ( 123 ) .
iohmms extend standard hmms ( 123 ) by conditioning the emission and transition probabilities on an observed input sequence .
each component of the output sequence corresponds to a particular component of the input .
training of an iohmm is supervised a mapping from an observed input sequence to output target sequence is learned .
our cpm also requires input and thus is also a type of conditional hmm .
however , the input is unobserved ( but crucially it is shared between all replicates ) and hence learning is unsupervised in the cpm model .
one could also take the alternative view that the cpm is simply an hmm with an extra set of parameters , the latent trace , that affect the emission probabilities and which are learned by the model .
the cpm is similar in spirit to prole hmms which have been used with great success for discrete , multiple sequence alignment , modeling of protein families and their con - served structures , gene nding ( 123 ) , among others .
prole hmm are hmms augmented by constrained - transition delete and insert states , with the former emitting no observa - tions .
multiple sequences are provided to the prole hmm during training and a summary of their shared statistical properties is contained in the resulting model .
the development of prole hmms has provided a robust , statistical framework for reasoning about sets of related discrete sequence data .
we put forth the cpm as a continuous data , conditional
many algorithms currently used for aligning continuous time series data are variations of dynamic time warping ( dtw ) ( 123 ) , a dynamic programming based approach which origi - nated in the speech recognition community as a robust distance measure between two time series .
dtw works on pairs of time series , aligning one time series to a specied reference time series .
dtw does not take in to account systematic variations in the amplitude of the signal .
our cpm can be viewed as a rich and robust extension of dtw that can be applied to many time series in parallel and which automatically uncovers the underlying template of the data .
123 discussion and conclusion
we have introduced a generative model for sets of continuous , time series data .
by training this model one can leverage information contained in noisy , replicated experimental data ,
and obtain a single , superior resolution fusion of the data .
we demonstrated successful use of this model on real data , but note that it could be applied to a wide range of problems involving time signals , for example , alignment of gene expression time proles , alignment of temporal physiological signals , alignment of motion capture data , to name but a few .
certain assumptions of the model presented here may be violated under different ex - perimental conditions .
for example , the gaussian emission probabilities treat errors in large amplitudes in the same absolute terms as in smaller amplitudes , whereas in real - ity , it may be that the error scales with signal amplitude .
similarly , the penalty term j=123 ( zj+123 ( cid : 123 ) zj ) 123 does not scale with the amplitude; this might result in the model arbitrarily preferring a lower amplitude latent trace .
( however , in practice , we did not nd this to be a problem . )
one immediate and straight - forward extension to the model would be to allow the data at each time point to be a multi - dimensional feature vector rather than a scalar value .
this could easily be realized by allowing the emission probabilities to be multi - dimensional .
in this way a richer set of information could be used : either the raw , multi - dimensional feature vector , or some transformation of the feature vectors , for example , principal components analysis .
the rest of the model would be unchanged and each feature vector would move as a coherent piece .
however , it might also be useful to allow different dimensions of the feature vector to be aligned differently .
for example , with the lc - ms data , this might mean allowing different mass / charge peptides to be aligned differently at each time point .
however , in its full generality , such a task would be extremely computational intense .
a perhaps more interesting extension is to allow the model to work with non - replicate data .
for example , suppose one had a set of lc - ms experiments from a set of cancer patients , and also a set from normal persons .
it would be desirable to align the whole set of time series and also to have the model tease out the differences between them .
one approach is to consider the model to be semi - supervised - the model is told the class membership of each training example .
then each class is assigned its own latent trace , and a penalty is introduced for any disagreements between the latent traces .
care needs to be taken to ensure that the penalty plateaus after a certain amount of disagreement between latent trace points , so that parts of the latent trace which are truly different are able to whole - heartedly disagree .
assuming that the time resolution in the observed time series is sufciently high , one might also want to encourage the amount of disagreement over time to be markovian .
that is , if the previous time point disagreed with the other latent traces , then the current point should be more likely to disagree .
