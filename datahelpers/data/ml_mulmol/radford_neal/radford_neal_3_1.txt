in this paper , we introduce ecient ensemble markov chain monte carlo ( mcmc ) sampling methods for bayesian computations in the univariate stochastic volatility model .
we compare the performance of our ensemble mcmc methods with an improved version of a recent sampler of kastner and fruwirth - schnatter ( 123 ) .
we show that ensemble samplers are more ecient than this state of the art sampler by a factor of about 123 , on a data set simulated from the stochastic volatility model .
this performance gain is achieved without the ensemble mcmc sampler relying on the assumption that the latent process is linear and gaussian , unlike the sampler of kastner and fruwirth - schnatter .
the stochastic volatility model is a widely - used example of a state space model with non - linear or non - gaussian transition or observation distributions .
it models observed log - returns y = ( y123 , .
, yn ) of a nancial time series with time - varying volatility , as follows :
yi|xi n ( 123 , exp ( c + xi ) ) , x123 n ( 123 , 123 / ( 123 123 ) )
xi|xi123 n ( xi123 , 123 )
i = 123 ,
here , the latent process xi determines the unobserved log - volatility of yi .
because the relation of the observations to the latent state is not linear and gaussian , this model cannot be directly handled by ecient methods based on the kalman lter .
in a bayesian approach to this problem , we estimate the unknown parameters = ( c , , ) by sampling from their marginal posterior distribution p ( |y ) .
this distribution cannot be written down in closed form .
we can , however , write down the joint posterior of and the log - volatilities x = ( x123 , .
, xn ) , p ( , x|y ) and draw samples of ( , x ) from it .
discarding the x coordinates in each draw will give us a sample from the marginal posterior distribution of .
to sample from the posterior distribution of the stochastic volatility model , we develop two new mcmc samplers within the framework of ensemble mcmc , introduced by neal ( 123 ) .
the key idea underlying ensemble mcmc is to simultaneously look at a collection of points ( an ensemble ) in the space we are sampling from , with the k ensemble elements chosen in such a way that the density of interest can be simultaneously evaluated at all of the ensemble elements in less time than it would take to evaluate the density at all k points separately .
previously , shestopalo and neal ( 123 ) developed an ensemble mcmc sampler for non - linear , non - gaussian state space models , with ensembles over latent state sequences , using the embedded hmm ( hidden markov model ) technique of neal ( 123 ) , neal et al .
( 123 ) .
this ensemble mcmc sampler was used for bayesian inference in a population dynamics model and shown to be more ecient than methods which only look at a single sequence at a time .
in this paper we consider ensemble mcmc samplers that look not only at ensembles over latent state sequences as in shestopalo and neal ( 123 ) but also over a subset of the parameters .
we see how well both of these methods work for the widely - used stochastic volatility model .
123 bayesian inference for the stochastic volatility model
bayesian inference for the stochastic volatility model has been extensively studied .
in this paper , we focus on comparisons with the method of kastner and fruwirth - schnatter ( 123 ) .
this state - of - the - art method combines the method of kim et al .
( 123 ) with the asis ( ancillary suciency interweaving strategy ) technique of yu and meng ( 123 ) .
kastner and fruwirth - schnatters method consists of two parts .
the rst is an update of the latent variables x and the second is a joint update of and the latent variables x .
we improve this method here by saving and re - using sucient statistics to do multiple parameter updates at little additional computational cost .
123 a linear gaussian approximation for sampling latent sequences
the non - linear relationship between the latent and the observation process prohibits the direct use of kalman lters for sampling the latent variables xi .
kim et al .
( 123 ) introduced an approximation to the stochastic volatility model that allows using kalman lters to draw samples of xi which can be later reweighed to give samples from their exact posterior distribution .
this approximation proceeds as follows .
first , the observation process for the stochastic volatility model is written in the form
i ) = c + xi + i
where i has a log ( 123
next , the distribution of i is approximated by a ten - component mixture of gaussians with mixture weights k , means mk and variances 123 k , k = 123 , .
the values of these mixture weights , means and variances can be found in omori ( 123 ) .
at each step of the sampler , at each time i , a single component of the mixture is chosen to approximate the distribution of i by
drawing a mixture component indicator ri ( 123 , .
, 123 ) with probabilities proportional to
p ( ri = k|yi , xi , c , ) k
i ) c xi ) 123 / 123
conditional on ri , the observation process is now linear and gaussian , as is the latent process :
i ) |xi , ri , c , n ( mri + c + xi , 123
x123| n ( 123 , 123 / ( 123 123 ) )
xi|xi123 n ( xi123 , 123 )
i = 123 ,
kalman ltering followed by a backward sampling pass can now be used to sample a latent
sequence x .
for a description of this sampling procedure , see petris et al .
( 123 ) .
the mis - specication of the model due to the approximation can be corrected using importance
i=123 f ( yi|x ( l )
, c ( l ) , ( l ) )
refers to a draw .
posterior expectations of functions of can then be computed as ( cid : 123 ) w ( l ) f ( ( l ) ) ,
where f is the n ( 123 , exp ( c + xi ) ) density , g is the n ( mk + c + xi , k ) density and the index l
, c ( l ) , ( l ) , mk , k ) )
with ( l ) the draws .
we note that , when doing any other updates aecting x in combination with this approxi - mate scheme , we need to continue to use the same mixture of gaussians approximation to the observation process .
if an update drawing from an approximate distribution is combined with an update drawing from an exact distribution , neither update will draw samples from their target distribution , since neither update has a chance to reach equilibrium before the other update dis - turbs things .
we would then be unable to compute the correct importance weights to estimate posterior expectations of functions of .
123 asis updates
asis methods ( yu and meng ( 123 ) ) are based on the idea of interweaving two parametrizations .
for the stochastic volatility model , these are the so - called non - centered ( nc ) and centered ( c ) parametrizations .
the nc parametrization is the one in which the stochastic volatility model was originally presented above .
the c parametrization for the stochastic volatility model is
yi|x123 n ( 123 , exp ( xi ) ) , x123 n ( c , 123 / ( 123 123 ) )
xi|xi123 n ( c + ( xi123 c ) , 123 )
i = 123 ,
the mixture of gaussians approximation for c is the same as for nc .
kastner and fruwirth - schnatter ( 123 ) propose two new sampling schemes , gis - c and gis - nc , in which they interweave these two parametrizations , using either the nc or c parameteri - zation as the baseline .
the authors report a negiligible performance dierence between using nc or c as the baseline .
for the purposes of our comparisons , we use the method with nc as the baseline , gis - nc , which proceeds as follows .
draw x given , r , y using the linear gaussian approximation update ( nc )
draw given x , r , y using a metropolis update ( nc )
move to c by setting x = c + x
draw given x , r , y using a metropolis update ( c ) 123
move back to nc by setting x = xc
redraw the mixture component indicators r given , x , y .
theorem 123 of yu and meng ( 123 ) establishes a link between asis and the px - da ( param - eter expansion - data augmentation ) method of liu and wu ( 123 ) .
in the case of the stochastic volatility model , this means that we can view the asis scheme for updating x and as a combi - nation of two updates , both done in the nc parametrization .
the rst of these draws new values for conditional on x .
the second draws new values for both x and , such that when we propose to update c to c and to , we also propose to update the sequence x to x = ( ( c+x ) c ) / .
for this second update , the metropolis acceptance probability needs to be multiplied by a jaco - bian factor ( / ) n to account for scaling .
a joint translation update for c and x has been previously considered by liu and sabatti ( 123 ) and successfully applied to to stochastic volatility model .
scale updates are considered by liu and sabatti ( 123 ) as well , though they do not apply them to the stochastic volatility model .
the view of asis updates as joint updates to and x makes it easier to see why asis updates improve eciency .
at rst glance , they look like they only update the parameters , but they actually end up proposing to change both and x in a way that preserves dependence between them .
this means that moves proposed in asis updates are more likely to end up in a region of high posterior density , and so be accepted .
kastner and fruwirth - schnatter ( 123 ) do a single metropolis update of the parameters for every update of the latent sequence .
however , we note that given values for the mixture indices r , y and x , low - dimensional sucient statistics exist for all parameters in the centered parametriza - tion .
in the non - centered parametrization , given r , y and x , low - dimensional sucient statistics exist for .
we propose doing multiple metropolis updates given saved values of these sucient statistics ( for all parameters in the case of c and for in the case of nc ) .
this allows us to reach equilibrium given a xed latent sequence at little computational cost since additional up - dates have small cost , not dependent on n .
also , this eliminates the need to construct complex proposal schemes , since with these repeated samples the algorithm becomes less sensitive to the particular choice of proposal density .
the sucient statistics in the case of nc are
t123 = x123
123 + x123
with the log - likelihood of as a function of the sucient statistics being log ( l ( |t ) ) = ( 123 / 123 ) log ( 123 123 ) ( 123 / 123 ) ( 123 ( t123 t123 ) 123t123 + t123 )
in the case of c the sucient statistics are
t123 = x123 + xn
with the log - likelihood as a function of the sucient statistics being
log ( l ( c , , 123|t ) ) = ( n / 123 ) log ( 123 ) + ( 123 / 123 ) log ( 123 123 )
( 123 / 123 ) ( t123 + 123t123 123t123 123c123t123 123c ( t123 + t123 ) +123ct123 + 123ct123 + ( n 123 ) ( c ( 123 ) ) 123 + c123 ( 123 123 ) ) / 123
the details of the derivations are given in the appendix .
123 ensemble mcmc methods for
stochastic volatility models
the general framework underlying ensemble mcmc methods was introduced by neal ( 123 ) .
an ensemble mcmc method using embedded hmms for parameter inference in non - linear , non - gaussian state space models was introduced by shestopalo and neal ( 123 ) .
we briey review ensemble methods for non - linear , non - gaussian state space models here .
ensemble mcmc builds on the idea of mcmc using a temporary mapping .
suppose we are interested in sampling from a distribution with density ( z ) on z .
we can do this by constructing a markov chain with transition kernel t ( z ( cid : 123 ) |z ) with invariant distribution .
the temporary mapping strategy takes t to be a composition of three stochastic mappings .
the rst mapping , t , takes z to an element w of some other space w .
the second , t , updates w to w ( cid : 123 ) .
the last , t , takes w ( cid : 123 ) back to some z ( cid : 123 ) z .
the idea behind this strategy is that doing updates in an intermediate space w may allow us to make larger changes to z , as opposed to doing updates directly in z .
in the ensemble method , the space w is taken to be the k - fold cartesian product of z .
first , z mapped to an ensemble w = ( z ( 123 ) , .
, z ( k ) ) , with the current value z assigned to z ( k ) , with k ( 123 , .
, k ) chosen uniformly at random .
the remaining elements z ( j ) for j ( cid : 123 ) = k are chosen
from their conditional distribution under an ensemble base measure , given that z ( k ) = z .
the marginal density of an ensemble element z ( k ) in the ensemble base measure is denoted by ( z ( k ) ) .
next , w is updated to w ( cid : 123 ) using any update that leaves invariant the ensemble density
( w ) = ( ( z ( 123 ) , .
, z ( k ) ) ) = ( ( z ( 123 ) , .
, z ( k ) ) )
finally , a new value z ( cid : 123 ) is chosen by selecting an element z ( k ) from the ensemble with probabilities proportional to ( z ( k ) ) / k ( z ( k ) ) .
the benet of doing , say metropolis , updates in the space of ensembles is that a proposed move is more likely to be accepted , since for the ensemble density to be large it is enough that the proposed ensemble contains at least some elements with high density under .
in shestopalo and neal ( 123 ) , we consider an ensemble over latent state sequences x .
specif - ically , the current state , ( x , ) , consisting of the latent states x and the parameters is mapped to an ensemble y = ( ( x ( 123 ) , ) , .
, ( x ( k ) , ) ) where the ensemble contains all distinct sequences x ( k ) passing through a collection of pool states chosen at each time i .
the ensemble is then updated to y ( cid : 123 ) = ( ( x ( 123 ) , ( cid : 123 ) ) , .
, ( x ( k ) , ( cid : 123 ) ) using a metropolis update that leaves invariant .
at this step , only is changed .
we then map back to a new x ( cid : 123 ) = ( x ( cid : 123 ) , ( cid : 123 ) ) , where x ( cid : 123 ) is now potentially dierent from the original x .
we show that this method considerably improves sampling eciency in the ricker model of population dynamics .
as in the original neal ( 123 ) paper , we emphasize here that applications of ensemble methods are worth investigating when the density at each of the k elements of an ensemble can be computed in less time than it takes to do k separate density evaluations .
for the stochastic volatility model , this is possible for ensembles over latent state sequences , and over the parameters c and 123
in this paper , we will only consider joint ensembles over x and over .
since we will use = log ( 123 ) in the mcmc state , we will refer to ensembles over below .
we propose two ensemble mcmc sampling schemes for the stochastic volatility model .
the rst , ens123 , updates the latent sequence , x , and by mapping to an ensemble composed of latent sequences x and values of , then immediately mapping back to new values of x and .
the second , ens123 , maps to an ensemble of latent state sequences x and values of , like ens123 , then updates using an ensemble density summing over x and , and nally maps back to new values of x and .
for both ens123 and ens123 , we rst create a pool of values with l elements , and at each time , i , a pool of values for the latent state xi , with lx elements .
the current value of is assigned to the pool element ( 123 ) and for each time i , the current xi is assigned to the pool .
( since the pool states are drawn independently , we dont need to randomly assign an index to the current and the current xis in their pools . ) the remaining pool elements are drawn independently from some distribution having positive probability for all possible values of xi and , say i for xi and for .
the total number of ensemble elements that we can construct using the pools over xi and over x .
naively evaluating the ensemble density presents an enormous computational burden x .
by using the forward algorithm , together with
for lx > 123 , taking time on the order of lln
a caching technique , we can evaluate the ensemble density much more eciently , in time on the order of ll123 xn .
the forward algorithm is used to eciently evaluate the densities for the ensemble over the xi .
the caching technique is used to eciently evaluate the densities for the ensemble over , which gives us a substantial constant factor speed - up in terms of computation
in detail , we do the following .
let p ( x123 ) be the initial state distribution , p ( xi|xi123 ) the transition density for the latent process and p ( yi|xi , ) the observation probabilities .
we begin by computing and storing the initial latent state probabilities which do not depend on for each pool state x ( k )
123 at time 123
p123 = ( p ( x ( 123 )
, p ( x ( lx )
for each ( l ) in the pool and each pool state x ( k )
123 we then compute and store the initial forward
123 | ( l ) ) = p ( x ( k )
123 , ( l ) )
then , for i > 123 , we similarly compute and store the matrix of transition probabilities
p ( x ( lx ) .
p ( x ( lx )
for k123 , k123 ( 123 , .
we then are transition probabilities between pool states x ( k123 ) use the stored values of the transition probabilities pi to eciently compute the vector of forward probabilities for all values of ( l ) in the pool
i123 and x ( k123 )
i ( xi| ( l ) ) =
with xi ( x ( 123 )
, x ( lx )
at each time i , we divide the forward probabilities ( l ) i values and using the normalized ( l )
i ( xi| ( l ) ) , storing i s in the next step of the recursion .
this is needed to prevent underow and for ensemble density computations .
in the case of all the forward probabilities summing to 123 , we set the forward probabilities at all subsequent times to 123
note that we wont get underows for all values of ( l ) , since we are guaranteed to have a log - likelihood that is not for the current value of in the mcmc state .
i ( xi ) by c ( l )
i = 123 ,
for each ( l ) , the ensemble density can then be computed as
to avoid overow or underow , we work with the logarithm of ( l ) .
even with caching , computing the forward probabilities for each ( l ) in the pool is still an x operation since we multiply the vector of forward probabilities from the previous step by the transition matrix .
however , if we do not cache and re - use the transition probabilities pi when computing the forward probabilities for each value of ( l ) in the pool , the computation of the ensemble densities ( l ) , for all l = 123 , .
, l , would be about 123 times slower .
this is because computing forward probabilities for a value of given saved transition probabilities only involves multiplications and additions , and not exponentiations , which are comparatively more expensive .
in ens123 , after mapping to the ensemble , we immediately sample new values of and x from the ensemble .
we rst sample a ( l ) from the marginal ensemble distribution , with probabilities proportional to ( l ) .
after we have sampled an ( l ) , we sample a latent sequence x conditional on ( l ) , using a stochastic backwards recursion .
the stochastic backwards recursion rst samples a n ( xn| ( l ) ) .
then , given the state xn from the pool at time n with probabilities proportional to ( l ) sampled value of xi , we sample xi123 from the pool at time i 123 with probabilities proportional
i123 ( xi123| ( l ) ) , going back to time 123
in the terminology of shestopalo and neal ( 123 ) this is a single sequence update combined with an ensemble update for ( which is a fast variable in the terminology of neal ( 123 ) since recomputation of the likelihood function after changes to this variable is fast given the saved
in ens123 , before mapping back to a new and a new x as in ens123 , we perform a metropolis update for using the ensemble density summing over all ( l ) and all latent sequences in the l=123 ( l ) .
this approximates updating using the posterior density of with x and integrated out , when the number of pool states is large .
the update nevertheless leaves the correct distribution exactly invariant , even if the number of pool states is not large .
123 choosing the pool distribution
for a pool distribution for xi , a good candidate is the stationary distribution of xi in the
a good choice for the pool distribution is crucial for the ecient performance of the ensemble
ar ( 123 ) latent process , which is n ( 123 , 123 / ( cid : 123 ) 123 123 ) .
the question here is how to choose .
for call it cur and draw pool states from n ( 123 , c / ( cid : 123 ) 123 123
ens123 , which does not change , we can simply use the current value of from the mcmc state , cur ) for some scaling factor c .
typically , we would choose c > 123 in order to ensure that for dierent values of , we produce pool states that cover the region where xi has high probability density .
we cannot use this pool selection scheme for ens123 because the reverse transition after a change
for example , we can propose a value , and draw the pool states from n ( 123 , c / ( cid : 123 ) 123 123
in would use dierent pool states , undermining the proof via reversibility that the ensemble transitions leave the posterior distribution invariant .
however , we can choose pool states that depend on both the current and the proposed values of , say and , in a symmetric fashion .
where avg is the average of and .
the validity of this scheme can be seen by considering to be an additional variable in the model; proposing to update to can then be viewed as proposing to swap and within the mcmc state .
we choose pool states for by sampling them from the model prior .
alternative schemes are possible , but we do not consider them here .
for example , it is possible to draw local pool states for which stay close to the current value of by running a markov chain with some desired stationary distribution j steps forwards and l j 123 steps backwards , starting at the current value of .
for details , see neal ( 123 ) .
in our earlier work ( shestopalo and neal ( 123 ) ) , one recommendation we made was to consider pool states that depend on the observed data yi at a given point , constructing a pseudo - posterior for xi using data observed at time i or in a small neighbourhood around i .
for the ensemble updates ens123 and ens123 presented here , we cannot use this approach , as we would then need to make the pool states also depend on the current values of c and , the latter of which is aected by the update .
we could switch to the centered parametrization to avoid this problem , but that would prevent us from making a fast variable .
the goal of our computational experiments is to determine how well the introduced variants of the ensemble method compare to our improved version of the kastner and fruwirth - schnatter ( 123 ) method .
we are also interested in understanding when using a full ensemble update is helpful or not .
we use a series simulated from the stochastic volatility model with parameters c = 123 , = 123 , 123 = 123 with n = 123
a plot of the data is presented in figure 123
we use the following priors for the model parameters .
c n ( 123 , 123 ) 123 inverse - gamma ( 123 , 123 )
we use the parametrization in which the inverse - gamma ( , ) has probability density
f ( x ) =
figure 123 : data set used for testing .
for = 123 , = 123 the 123% and 123% quantiles of this distribution are approximately
in the mcmc state , we transform and 123 to
= log ( ( 123 + ) / ( 123 ) )
with the priors transformed correspondingly .
123 sampling schemes and tuning
we compare three sampling schemes the kastner and fruwirth - schnatter ( kf ) method , and our two ensemble schemes , ens123 , in which we map to an ensemble of and x values and imme - diately map back , and ens123 , in which we additionally update with an ensemble update before
we combine the ensemble scheme with the computationally cheap asis metropolis updates .
it is sensible to add cheap updates to a sampling scheme if they are available .
note that the asis ( or translation and scale ) updates we use in this paper are generally applicable to location - scale models and are not restricted by the linear and gaussian assumption .
pilot runs showed that 123 updates appears to be the point at which we start to get diminishing returns from using more metropolis updates ( given the sucient statistics ) in the kf scheme .
this is the number of metropolis updates we use with the ensemble schemes as well .
the kf scheme updates the state as follows :
update x , using the kalman lter - based update , using the current mixture indicators r .
123 123
update the parameters using the mixture approximation to the observation density .
this step consists of 123 metropolis updates to given the sucient statistics for nc , followed by one joint update of c and .
change to the c parametrization .
update all three parameters simultaneously using 123 metropolis updates , given the sucient statistics for c .
note that this update does not depend on the observation density and is
update the mixture indicators r .
the ens123 scheme proceeds as follows :
map to an ensemble of and x .
map back to a new value of and x .
do steps 123 ) - 123 ) as for kf , but with the exact observation density .
the ens123 scheme proceeds as follows :
map to an ensemble of and x .
update using an ensemble metropolis update .
map back to a new value of and x .
do steps 123 ) - 123 ) as for kf , but with the exact observation density .
the metropolis updates use a normal proposal density centered at the current parameter values .
proposal standard deviations for the metropolis updates in nc were set to estimated marginal posterior standard deviations , and to half of that in c .
this is because in c , we update all three parameters at once , whereas in nc we update c and jointly and separately .
the marginal posterior standard deviations were estimated using a pilot run of the ens123 method .
the tuning settings for the metropolis updates are presented in table 123
for ensemble updates of , we also use a normal proposal density centered at the current value of , with a proposal standard deviation of 123 , which is double the estimated marginal posterior standard deviation of .
the pool states over xi are selected from the stationary distribution cur for the ens123 scheme and
of the ar ( 123 ) latent process , with standard deviation 123 / ( cid : 123 ) 123 123
avg for the ens123 scheme .
we used the prior density of to select pool states for .
for each method , we started the samplers from 123 randomly chosen points .
parameters were initalized to their prior means ( which were 123 for c , 123 for and 123 for ) , and each xi , i = 123 , .
, n , was initialized independently to a value randomly drawn from the stationary distribution of the ar ( 123 ) latent process , given set to the prior mean .
for the kf updates , the mixture indicators r where all initialized to 123s , this corresponds to the mixture component whose median matches the median of the log ( 123 123 ) distribution most closely .
all methods were run for approximately the same amount of computational time .
before comparing the performance of the methods , we veried that the methods give the same answer up to expected variation by looking at the 123% condence intervals each produced for the posterior means of the parameters .
these condence intervals were obtained from the standard error of the average posterior mean estimate over the ve runs .
the kf estimates were ad - justed using the importance weights that compensate for the use of the approximate observation distribution .
no signicant disagreement between the answers from the dierent methods was apparent .
we then evaluated the performance of each method using estimates of autocorrela - tion time , which measures how many mcmc draws are needed to obtain the equivalent of one
to estimate autocorrelation time , we rst estimated autocovariances for each of the ve runs , discarding the rst 123% of the run as burn - in , and plugging in the overall mean of the ve runs into the autocovariance estimates .
( this allows us to detect if the dierent runs for each method are exploring dierent regions of the parameter / latent variable space ) .
we then averaged the resulting autocovariance estimates and used this average to get autocorrelation estimates k .
k=123 i , with k chosen to be the point beyond which the k become approximately 123
all autocovariances were estimated using the fast fourier transform for computational eciency .
finally , autocorrelation time was estimated as 123 + 123 ( cid : 123 ) k
the results are presented in tables 123 and 123
the timings for each sampler represent an average over 123 iteratons ( each iteration consisting of the entire sequence of updates ) , with the samplers started from a point taken after the sampler converged to equilibrium .
the program was written in matlab and run on a linux system with an intel xeon x123 123 ghz cpu .
for a fair comparison , we multiply estimated autocorrelation times by the time it takes to do one iteration and compare these estimates .
we ran the kf method for 123 , 123 iterations , with estimated autocorrelation times using the original ( unweighed ) sequence for ( c , , ) of ( 123 , 123 , 123 ) , which after adjusting by computation time of 123 seconds per iteration are ( 123 , 123 , 123 ) .
it follows that the ens123 method with lx set to 123 and l set to 123 is better than the kf method by a factor of about 123 for the parameter .
for ens123 , the same settings lx = 123 and l = 123 appears to give the best results , with ens123 worse by a factor of about 123 than ens123 for sampling .
we also see that the ens123 and ens123 methods arent too sensitive to the particular tuning parameters , so long at there is a sucient number of ensemble elements both for xi and for .
( nc ) acc
for ( c , ) ( nc )
for ( c , , )
table 123 : metropolis proposal standard deviations with associated acceptance rates .
iterations time / iter ( s )
table 123 : performance of method ens123
lx l acc .
rate for
iterations time / iter ( s )
table 123 : performance of method ens123
123 ( 123 ) 123 ( 123 ) 123 ( 123 )
123 ( 123 ) 123 ( 123 ) 123 ( 123 )
- 123 ( 123 ) - 123 ( 123 ) - 123 ( 123 )
table 123 : estimates of posterior means , with standard errors of posterior means shown in brackets .
the results show that using a small ensemble ( 123 or so pool states ) over is particularly helpful .
one reason for this improvement is the ability to use the caching technique to make these updates computationally cheap .
a more basic reason is that updates of consider the entire collection of latent sequences , which allows us to make large changes to , compared to the
even though the ens123 method in this case is outperformed by the ens123 method , we have only applied it to one data set and there is much room for further tuning and improvement of the methods .
a possible explanation for the lack of substantial performance gain with the ensemble method is that conditional on a single sequence , the distribution of has standard deviation comparable to its marginal standard deviation , which means that we cant move too much further with an ensemble update than we do with our metropolis updates .
an indication of this comes from the acceptance rate for ensemble updates of in ens123 , which we can see isnt improved by much as more pool states are added .
parameter estimates for the best performing kf , ens123 and ens123 settings are presented in table 123
these estimates were obtained by averaging samples from all 123 runs with 123% of the sample discarded as burn - in .
we see that the dierences between the standard errors are in approximate agreement with the dierences in autocorrelation times for the dierent methods .
we found that noticeable performance gains can be obtained by using ensemble mcmc based sampling methods for the stochastic volatility model .
it may be possible to obtain even larger gains on dierent data sets , and with even better tuning .
in particular , it is possible that the method of updating with an ensemble , or some variation of it , actually performs better than a single sequence method in some other instance .
the method of kastner and fruwirth - schnatter ( 123 ) relies on the assumption that the state process is linear and gaussian , which enables ecient state sequence sampling using kalman lters .
the method would not be applicable if this was not the case .
however , the ensemble method could still be applied to this case as well .
it would be of interest to investigate the performance of ensemble methods for stochastic volatility models with dierent noise structures for the latent process .
it would also be interesting to compare the performance of the ensemble mcmc method with the pmcmc - based methods of andrieu et .
al ( 123 ) and also to see whether techniques used to improve pmcmc methods can be used to improve ensemble methods and vice
multivariate versions of stochastic volatility models , for example those considered in scharth and kohn ( 123 ) are another class of models for which inference is dicult , and that it would be interesting to apply the ensemble mcmc method to .
we have done preliminary experiments applying ensemble methods to multivariate stochastic volatility models , with promising results .
for these models , even though the latent process is linear and gaussian , due to a non - constant covariance matrix the observation process does not have a simple and precise mixture of gaussians
this research was supported by the natural sciences and engineering research council of canada .
is in part funded by an nserc postgraduate scholarship .
holds a canada research chair in statistics and machine learning .
