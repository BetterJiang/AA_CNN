the probability of accepting a candidate move in the hybrid monte carlo algorithm can be increased by considering a transition to be between windows of several states at the beginning and end of the trajectory , with a state within the selected win - dow being chosen according to the boltzmann probabilities .
the detailed balance condition used to justify the algorithm still holds with this procedure , provided the start state is randomly positioned within its window .
the new procedure is shown empirically to signicantly improve performance for a test system of uncoupled os -
( figures 123 , 123 , and 123 are not present in this version . )
the hybrid monte carlo algorithm of duane , kennedy , pendleton , and roweth ( 123 ) is a method of sampling from complex distributions , such as those encountered in statistical physics , that combines the advantages of dynamical methods ( 123 ) with those of the metropolis monte carlo method ( 123 ) .
these and related methods are reviewed by toussaint ( 123 ) .
the problem is to simulate a system parameterized by a vector , q , of dimension n , with a dierentiable potential energy function , e ( q ) .
this energy function induces a boltzmann distribution over q , for which the probability density is
p ( q ) = z 123
where ze =rrn exp ( e ( q ) ) dq .
( a temperature of one is assumed throughout , for
the aim of the simulation is to estimate the expectation of some function , h ( q ) :
hhi = zrn
h ( q ) p ( q ) dq
where q123 , q123 , .
, qn123 are obtained from the simulation , and are each distributed according to the boltzmann distribution for q ( but are not , in general , independent ) .
i will rst describe the dynamical approach to solving this problem , which suers from systematic error in the sampling , and then describe the hybrid monte carlo method , which eliminates this error by accepting only some of the dynamical moves .
next , i present and justify a generalization of this algorithm in which moves are made between windows of states at the beginning and end of a trajectory , rather than between single states .
situations in which this algorithm will have a higher acceptance probability than the standard algorithm are then illustrated , and an empirical comparison is given for systems of uncoupled oscillators .
finally , i discuss two variations on the algorithm that may be useful in some circumstances .
123 the dynamical method
in the dynamical simulation method , we introduce a momentum vector , p , which , like q , has dimension n , and a hamiltonian function , h ( q , p ) , that incorporates both potential and kinetic energy :
h ( q , p ) = e ( q ) + 123
we then seek to sample from the boltzmann distribution that h induces on the
phase space , ( q , p ) , for which the density is
p ( q , p ) = z 123
h exp ( h ( q , p ) ) = p ( q ) p ( p )
where zh =rrnrrn exp ( h ( q , p ) ) dq dp , and
p ( p ) = ( 123 ) n / 123 exp ( 123
this sample is generated by simulating an ergodic markov chain that has the boltzmann distribution for ( q , p ) as its stationary distribution .
values of q from successive states of this markov chain , whose marginal distribution is that of equa - tion ( 123 ) , are used to estimate hhi using equation ( 123 ) .
this markov chain operates by alternating dynamical transitions with stochastic
the dynamical transitions consist of simulating the system for some predened
period in a ctitious time , , using hamiltons equations :
these equations leave h invariant .
furthermore , the volume of a region of phase space remains constant as it evolves according to this dynamics ( liouvilles the - in consequence , the dynamical transitions sample regions of constant h
the stochastic transitions allow regions with dierent values of h to be explored .
they consist of replacing p with a value picked from its boltzmann distribution ( equation ( 123 ) ) .
such transitions clearly leave the boltzmann distribution of ( q , p ) with respect to h invariant .
the presence of these stochastic transitions will also usually be enough to ensure that the markov chain is ergodic i . e .
that the system can move to any point in phase space .
it is generally desirable that the trajectories simulated in the dynamical tran - sitions be long enough that they reach congurations almost independent of their starting congurations .
this avoids the slow exploration , at the rate of a random walk , that would result if the direction of motion were frequently randomized by
in practice , the dynamics must be simulated with some nite step size .
the leapfrog method is generally used , with the following steps being iterated some predened number of times , l , with some specied step size , :
123 ) = p ( )
123 e ( q ( ) )
q ( + ) = q ( ) + p ( +
p ( + ) = p ( +
123 e ( q ( + ) )
this discretized mapping still preserves phase space volume exactly .
however , with a nite , it does not leave h exactly constant .
this will introduce some systematic error into the sampling .
123 the hybrid monte carlo algorithm
the systematic error of the dynamical method is eliminated in the hybrid monte carlo algorithm ( 123 ) by considering the end - point of the trajectory found with the leapfrog method to be merely a candidate for the next state of the markov chain , to be accepted or rejected as in the metropolis monte carlo algorithm ( 123 ) .
acceptance or rejection of the candidate state is based on the amount , h , by which h for the candidate state exceeds h for the current state .
the probability of acceptance , a ( h ) , is given by
a ( h ) = min ( 123 , exp ( h ) )
thus candidate states with lower h are always accepted , while those with higher h are accepted with probability exp ( h ) .
if the candidate state is rejected , the new state is the same as the current state ( and is counted again in the average of
the validity of this procedure for producing a sample from the boltzmann distri - bution is more easily seen if we imagine that the trajectory for a dynamical transition is computed using a value for whose sign is chosen at random , with positive and negative values being equally likely .
the leapfrog method ( equations ( 123 ) to ( 123 ) ) is time - reversible , so that a forward trajectory , with a positive , and a backward trajectory , with the corresponding negative , are inverses of each other , a fact cru - cial to the justication of the algorithm .
in fact , usual practice is to always use a positive , since an eect equivalent to randomly choosing its sign is produced in any case by the randomization of the direction of p in the stochastic transitions .
to show that the boltzmann distribution ( equation ( 123 ) ) is invariant under such dynamical transitions , it suces to show that these transitions satisfy the condition known as detailed balance that the probability of a transition from a to b occurring is the same as that for a transition from b to a , given that the start state is boltzmann distributed .
to see that detailed balance holds , consider a small region of volume v around the point a = ( qa , pa ) .
suppose that a forward trajectory from a leads to the point b = ( qb , pb ) .
the other points in the region around a will lead to a region around b , which will also have volume v , since the leapfrog method preserves phase space volume .
due to time reversibility , a backward trajectory from b will lead to a .
the detailed balance condition with respect to the regions around a and b can now be written as
p ( qa , pa ) v 123
123 a ( h ( qb , pb ) h ( qa , pa ) )
= p ( qb , pb ) v 123
123 a ( h ( qa , pa ) h ( qb , pb ) )
the left side of the above equation is the probability of moving from the region around a to the region around b .
the rst factor is the boltzmann probability for
being in the region around a at the start , the second factor ( 123 123 ) is the probability of selecting a forward direction for the trajectory , and the third factor is the probability that this trajectory will be accepted .
the right side expresses the probability of moving from the region around b to the region around a in analogous fashion .
the equality of these two probabilities is seen by substituting from equations ( 123 ) and ( 123 ) .
the detailed balance condition can be similarly veried for the case where a backward trajectory from a is chosen .
the boltzmann distribution is also invariant with respect to the stochastic tran - sitions , which simply replace p with a value chosen from its boltzmann distribution .
thus , if ( as we expect ) the markov chain is ergodic , it will have the boltzmann dis - tribution as its unique stationary distribution .
123 an acceptance procedure using windows
the standard hybrid monte carlo algorithm can be generalized to consider a window of states at the end of the trajectory as candidate destinations for a dynamical transition , rather than just a single end state .
in order to maintain detailed balance , a possible move to this window must be considered in relation to an equal - sized window around the current state , with the location of the current state within that window being determined randomly .
due to this later requirement , the trajectory may have to be computed for some number of steps in the reverse of its primary direction .
acceptance or rejection of a move between windows is based on the total probability of the states they contain .
whichever window is selected , a particular state within that window is then picked according to the boltzmann probabilities .
this procedure can signicantly increase the probability of accepting a move , as will be discussed in section 123
first , though , i will dene the algorithm in more
to begin , values for the total number of steps in the trajectory , l , the base step size , 123 , and the window size , w , are selected from some xed distribution , with 123 w l + 123
a forward or backward direction , , for the trajectory is then chosen , with equal probabilities for = +123 and = 123 , and an oset , k , for the current state within the start window is selected , with k ( 123 , .
, w 123 ) , each possible value being equally likely .
a reverse portion of the trajectory is then computed by applying the leapfrog method with a step size of = 123 , beginning with the start state , x ( 123 ) .
this is done for k iterations , producing states labeled x ( 123 ) , .
, x ( k ) .
the start state is then restored , and the leapfrog method is applied with a step size of = +123 for l k iterations , producing states x ( 123 ) , .
, x ( l k ) .
the reject window at the front of the trajectory , around the current state , is dened as the set r = ( x ( k ) , .
, x ( k + w 123 ) ) .
the accept window at the far end of the trajectory is dened as a = ( x ( l k w + 123 ) , .
, x ( l k ) ) .
these windows may overlap .
the free energy for a window is dened as follows :
f ( w ) = log xxw
the free energies are used to decide whether the next state will come from the accept window or the reject window .
using the acceptance function of equation ( 123 ) , the accept window is selected with probability a ( f ) where f = f ( a ) f ( r ) ; otherwise we remain in the reject window ( which contains the current state ) .
having decided on the window w , a particular state within that window is then selected according to the probabilities
p ( x ) = exp ( h ( x ) + f ( w ) )
the state selected becomes the next state in the markov chain .
when implementing the generalized algorithm , it is not necessary to save all the states in the accept and reject windows .
one need only save the start state , so it can be restored after the reversed portion of the trajectory has been calculated , along with a single state from the accept window and a single state from the reject window , one or the other of which will become the next state of the markov chain .
a be the free energy for the rst i states that have been visited in the accept window , and let c i a be a state chosen according to the boltzmann probabilities from among these rst i states .
these variables can be calculated incrementally as new states in the accept window are visited .
to start , f 123 and c 123 a is undened .
when we visit the i - th state in the accept window , ( qi , pi ) , we can calculate
in detail , let f i
a = log ( cid : 123 ) exp ( h ( qi , pi ) ) + exp ( f i123 a = ( c i123
with probability exp ( f i123
( qi , pi ) with probability exp ( h ( qi , pi ) + f i
a + f i
once all states have been visited , we will have f ( a ) = f w a will be a state picked from a according to the boltzmann probabilities .
analogous variables , f i and c i r are maintained for the reject window .
once all states have been seen , a decision as to whether to use the accept window or the reject window can easily be made , and , in either case , a state selected from the chosen window is available .
a , and c w
when w = 123 , the generalized algorithm is equivalent to the standard hybrid monte carlo algorithm .
when w = l + 123 the procedure reduces to simply picking a state from those anywhere along the trajectory in accordance with their boltzmann probabilities .
some simplication in the implementation is then possible .
123 validity of the generalized algorithm
to demonstrate the validity of the generalized algorithm , we need to show that the detailed balance condition holds for the dynamical transitions .
as l , w , and 123 are chosen from a xed distribution , independently of the current state , we may choose to regard them as xed , since if detailed balance holds for transitions with any values of these parameters , it will hold for a mixture of such transitions .
it will prove necessary to average over the values selected for and k , however .
detailed balance will be proved separately for transitions to a state in the accept window and for those to a state in the reject window .
if the two windows overlap , as they will if w > ( l + 123 ) / 123 , there will for some pairs of states be the possibility of transitions of either type .
if detailed balance holds for the two types of transitions individually , however , it will also hold for the combination .
to prove detailed balance for transitions within the reject window , note rst that the set of possible trajectories ( for the various values of and k ) that start at state a and that include state b in the reject window is the same as the set of trajectories that start at b and include a in the reject window .
here , a trajectory is dened as the set of states visited , along with the subsets of states that make up the accept and reject windows .
in detail , if for the trajectory starting at a = xa ( 123 ) , with direction a and window oset ka , we have b = xa ( j ) within the reject window , then the identical trajectory will be produced by starting at b = xb ( 123 ) , with direction b = a and window oset kb = ka + j , and a = xb ( j ) will be in the reject
we can thus prove detailed balance separately for each such trajectory .
the probability of being in a small region of volume v around a , of then picking values for and k that generate a particular trajectory for which a state in the region of b is in the reject window , of then chosing to pick a state from the reject window , and of nally chosing the state in the region of b as the next state , is as follows :
p ( qa , pa ) v
( 123 a ( f ( a ) f ( r ) ) ) exp ( h ( qb , pb ) + f ( r ) )
the probability of generating the same trajectory starting from the region of b , and of then ending up in the region of a , is
p ( qb , pb ) v
( 123 a ( f ( a ) f ( r ) ) ) exp ( h ( qa , pa ) + f ( r ) )
these are readily seen to be equal upon substituting from equation ( 123 ) .
detailed balance for transitions to a state in the accept window follows similarly , using the fact that the set of possible trajectories that start at state a and that include state b in the accept window is the same as the set of trajectories that start at b and include a in the accept window , except that in the later trajectories the accept and reject windows are exchanged .
in detail , if for the trajectory starting at a = xa ( 123 ) , with window oset ka and direction a , we have b = xa ( j ) within
the accept window , aa , while a is in the reject window , ra , then the trajectory produced by starting at b = xb ( 123 ) , with window oset kb = l ka j and b = a , will lead to a = xb ( j ) being in the accept window , ab , while b is in the reject window , rb .
furthermore , we will have aa = rb and ra = ab .
the probability of being in a small region of volume v around a , of then picking values for and k that generate a particular trajectory for which a state in the region of b is in the accept window , of then chosing to pick a state from the accept window , and of nally chosing the state in the region of b as the next state , is as
p ( qa , pa ) v
a ( f ( aa ) f ( ra ) ) exp ( h ( qb , pb ) + f ( aa ) )
for the probability of generating the same trajectory , but with accept and reject windows exchanged , starting from the region of b , and of then picking a state in the region of a , we have
p ( qb , pb ) v
a ( f ( ab ) f ( rb ) ) exp ( h ( qa , pa ) + f ( ab ) )
again , these are seen to be equal upon substituting from equations ( 123 ) and ( 123 ) , remembering that aa = rb and ra = ab .
123 acceptance probability for the generalized algorithm
two situations where the use of windows will increase the acceptance probability of the hybrid monte carlo algorithm are illustrated in figure 123
in the trajectory of figure 123 ( a ) , the energies of the states in the reject window , at the start of the trajectory , are all approximately equal to the energy of the current state , h123
most of the states in the accept window , at the end of the trajectory , have energies in the vicinity of h+ , much greater than that of the current state .
however , one state has a much lower energy , h .
if the standard hybrid monte carlo algorithm is applied in this situation , with the end - point of the trajectory randomly picked from the last w states shown , the probability of acceptance will be approximately 123 / w , since moves to states with energy h+ would almost certainly be rejected , while a move to the one state with energy h would be accepted .
in contrast , if the generalized algorithm is applied with a window of size w , the
move will always be accepted .
the free energy of the reject window will be
f ( r ) log ( w exp ( h123 ) ) h123 log ( w )
assuming h+ h , the free energy of the accept window will be
f ( a ) log ( ( w 123 ) exp ( h+ ) + exp ( h ) ) h
thus , if h123 h > log ( w ) , the move will be accepted , and the particular state chosen within the accept window will almost certainly be the one with energy h .
note that it is essential to this example that a state in the accept window have lower energy than those in the reject window .
if this is not the case , the acceptance probability is the same as for the standard algorithm .
the trajectory of figure 123 ( b ) illustrates a dierent , perhaps more typical , sit - uation where the use of windows also increases the acceptance probability .
here , both windows contain approximately equal numbers of low and high energy states .
note that the current state is likely to be one of the low energy ones , since it is
with the standard algorithm , the end state might equally well be one of high energy or one of low energy .
in the former case , the move would likely be rejected , while in the later , it would have a good probability of being accepted .
the total acceptance probability will thus be around 123 / 123
if the generalized algorithm is used with a window size large compared to the time scale of energy uctuations , however , the free energy of both the accept and reject windows will be approximately equal , and the acceptance probability will be near one .
the particular state chosen within the accept window will likely be one with low energy .
these examples show that the use of windows can be benecial , but they do not indicate the magnitude of the benet , nor whether there is any improvement in the asymptotic form of the time requirements as system size increases .
indeed , for xed values of l , 123 , and w , the acceptance probability declines exponentially with increasing system size , as for the standard algorithm .
this scaling behaviour is due to the free energies of the windows , and hence their dierence , being extensive quantities that increase in proportion to system size .
123 performance for a system of uncoupled oscillators
to gain some insight into the degree of benet from using the generalized algorithm , and into its scaling behaviour , i have tested it empirically on systems of uncoupled oscillators .
these simple systems are meant to model more complex systems in which the components are not completely independent , but interact only weakly .
the behaviour of the standard hybrid monte carlo algorithm for systems of uncoupled oscillators has been analysed in detail by kennedy and pendleton ( 123 ) .
the potential energy function for such a system is
in the boltzmann distribution with respect to e , each qi is independent , and dis - tributed as a gaussian with zero mean and standard deviation 123 / i .
since the
operation of the hybrid monte carlo algorithm is invariant with respect to transla - tions and rotations of the coordinates , these systems in fact model the behaviour of the algorithm as applied to any multivariate gaussian distribution .
note that for the leapfrog method to be stable , with the error in h remaining bounded even for long trajectories , it is necessary for the step size to be less than 123 / max , where max is the largest of the i .
for ecient exploration of the other coordinates , the average length of a trajectory in ctitious time , tt , should be in the vicinity of 123 / min .
in using these systems as a test bed , we must be clear concerning which aspects of the system we expect to carry over to realistic problems , and which will not .
i assume here that the approximate magnitudes of min and max and the general distribution of the i are known , and that these may be used to select appropriate values for the step size and trajectory length .
i assume that their exact values would not be known in a realistic system indeed , they will not be precisely dened , since the components of the real system will not be completely uncoupled .
to model this , the step size ( and hence the trajectory length as well ) was varied slightly at random , in order to avoid results that depend on precise tuning of these parameters .
i will also assume that we are interested in systems for which the ratio max / min is large .
this ratio is a measure of the inherent diculty of the problem , being ( roughly ) the number of leapfrog steps required to generate an independent cong - uration .
in the experiments , the length in ctitious time of a trajectory was chosen to be large enough for the acceptance probability to have reached equilibrium .
in a real system , however , the appropriate trajectory length ( tt 123 / min ) might well be much longer than this .
since chosing any particular value for tt would be arbitrary , i have for the most part evaluated the algorithms on the assumption that tt is very large .
the cost of a particular combination of window size and average step size was accordingly taken to be
c = 123 / ( ( 123 ) )
where is the probability of a move being rejected , and is the average step size ( recall that the actual step sizes used will be slight perturbations around this aver - age ) .
this cost measure is proportional to the number of energy gradient evaluations needed to generate a given number of accepted moves of average length tt , provided tt is much greater than tw , the length in ctitious time of the windows .
note that this cost measure places no value on transitions within the reject window , though these presumably improve sampling at least somewhat .
if tw is comparable to tt , then the above measure would not be appropriate , since it ignores the eort expended in computing those portions of the trajectory that lie before the current state and after the new state .
an appropriate cost in this case would be ( 123 + tw / tt ) / ( ( 123 ) ) .
i assume that for a given window length , tw , and a given system size , n , the
value of that minimizes c can feasibly be found , and that this minimal value of c is thus the appropriate measure of the cost of the algorithm for a particular window length .
if the rejection probability has the functional form
= f ( g ( n ) k )
then one can show by straightforward means that this minimum is reached at a value of that is independent of n .
the scaling properties of the algorithm can then be found by determining how must decrease as n increases in order to keep the rejection probability constant .
note that we are measuring cost as the number of evaluations of the energy gradient , not the time required for these evaluations .
computation time per evaluation will vary with system size in a manner which depends on the application , but which will generally add an additional factor of at least n to the total computation time as a function of system size .
i have tested the standard and the generalized algorithms on systems for which n = 123 , 123 , 123 , 123 , 123 , and 123
in each case , the i were selected randomly from the range 123 to 123 , with a uniform distribution for log ( ) .
though i assume that components with much smaller would also be present in a real system , such were not actually included in the simulation , in order to save time .
their inclusion would have had a negligible eect on the acceptance probability , since the accuracy of the leapfrog method is very high when 123 / is much greater than the step size .
an average trajectory length of tt = 123 123 ( 123 / max ) was used .
several of the simulations were done with tt = 123 as well , and the results conrmed that the acceptance probability had indeed reached equilibrium at tt = 123
for a given average step size , , and for a given window size , w , the number of steps in the entire trajectory , l , was set so that tt = ( lw +123 ) .
( this calculation accounts for the average number of steps before and after the current and new states . ) the trajectory was then computed with these values of l and w and with a value of 123 randomly selected from the region within 123% of the given .
runs were done of the standard algorithm , for which w = 123 , and of the gen - eralized algorithm with a window length of tw = 123 , for which the number of states in the window was w = tw / .
some runs with tw = 123 and tw = 123 were done as well , with results that were similar to but not quite as good as those for tw = 123 .
values for of .
, 123 , 123 , 123 , 123 , .
. , in geometric steps of 123 / 123 were used .
in each run , 123 trajectories were generated starting from position and momentum coordinates picked from the boltzmann dis - tribution , independently for each trajectory .
the proportion of rejected moves , , was taken to be an estimate of the rejection probability , .
the standard error for this estimate is 123 at = 123 , declining to 123 at = 123 or = 123
estimate , bc , for the cost follows from equation ( 123 ) .
the results are shown in figure 123 , which plots the estimated rejection probability and consequent cost for each value of n and for various values of , for both the standard algorithm and the generalized algorithm with tw = 123 .
when the best
value of is used in each case , the cost of the generalized algorithm is roughly half that of the standard algorithm , with some indication that the advantage of the generalized algorithm may be greater for the larger n .
interestingly , the rejection probability with the optimal is signicantly lower for the generalized algorithm than for standard algorithm .
in analysing this data further , we can rst compare with the analytic results of ( 123 ) .
they derive the following expression for the rejection probability of the standard algorithm applied to a system of uncoupled oscillators ( adapted from their equation
i sin123 ( itt ) / 123
assuming that the small random variation in , and hence tt , is enough to randomize the phases in this sum , and using the fact that 123 , we get that , for the experiment described here , the rejection
where = n 123pi 123
sin123 ( x ) dx = 123 probability should be
where = n 123pi 123
i 123 123
the rejection probability thus has the form of equation ( 123 ) .
as system size increases , should be scaled as n 123 / 123 in order to keep the rejection probability constant , and the cost will grow as n 123 / 123
in figure 123 , is plotted against n 123 for all runs of the standard algorithm .
as expected , appears to be a function of n 123 , as the spread in the data points in comparable to the standard error .
( note that here , and in figure 123 , the standard error is apparently larger for smaller values of , due to the logarithmic scale . ) comparison with the exact predictions of equation ( 123 ) ( not shown in the gure ) shows a good t , except for a slight departure for large values of the rejection rate .
we can hypothesize that the rejection probability of the generalized algorithm for a given window length will also be a function of n 123
if this is the case , the cost for the generalized algorithm will also scale as n 123 / 123 , though there could , as seen above , be an improvement in the constant factor over the standard algorithm .
to test this hypothesis , figure 123 also shows plotted against n 123 for all runs of the generalized algorithm with tw = 123 .
a tendency of the curves for the larger n to lie below those for the smaller n is apparent , leading one to reject this hypothesis .
the data for the generalized algorithm is better t by the alternative hypothesis that the rejection probability is a function of n 123 / 123 123
this is seen in figure 123
for comparison , the data for the standard algorithm is also plotted under this assumption , and it is clear that in this case the t is bad .
if this hypothesis is true , then the step size for the generalized algorithm should be scaled as n 123 / 123 to maintain a constant rejection rate , and the cost will con - sequently grow with system size as n 123 / 123 , an improvement over the n 123 / 123 scaling
of the standard algorithm .
this result must be regarded as tentative , however .
it seems possible that for very large n the window length , tw , might have to increase at some rate in order to maintain good scaling behaviour .
this would ultimately aect the cost , once tw became comparable to tt .
behaviour could also conceivably depend on the exact distribution of the i .
a theoretical analysis is thus needed to gain a better understanding of the performance of the generalized algorithm .
it is clear , however , that at the very least , it can improve performance by a signicant
123 variations on the algorithm
two variations on the generalized algorithm described here are worth mentioning .
first , when a move to the accept window is rejected , it is valid to simply re - main at the current state , rather than selecting a new state from the reject window according to the boltzmann probabilities .
this follows from the fact that detailed balance was shown above to hold independently for the accept and reject transitions .
detailed balance thus continues to hold if all reject transitions are eliminated .
with this variation , the overhead of saving states is somewhat reduced .
typi - cally , however , this overhead is small compared to the cost of evaluating the gradient of the energy , and its elimination may not be worth giving up the additional explo - ration provided by the reject transitions .
note that it is in any case necessary to visit all the states in the reject window , in order to compute its free energy .
a second variation may be useful when the ideal step size is not known a priori .
in such cases , we may have to use step sizes selected at random from a fairly broad distribution .
trajectories computed with a step size that is too large will result in large changes in h and are unlikely to be accepted .
to save computation , we can terminate such trajectories early , stopping when - ever a single leapfrog step changes h by an amount , positive or negative , whose magnitude is greater than some threshold .
the state reached after this large change in h is not included .
such truncated trajectories will have smaller than normal accept and / or reject windows , but examination of the proof of validity in section 123 shows that they may validly be treated the same way as normal trajectories .
the accept window for a truncated trajectory may , in fact , be null , in which case the move is rejected .
( the reject window will always contain at least the current state . ) this variation is applicable to the standard hybrid monte carlo algorithm , where the window size is one .
in this case , all truncated trajectories are rejected , with the current state remaining unchanged .
i thank david mackay for helpful comments .
this work was supported by the natural sciences and engineering research council of canada , and by the ontario
information technology research centre .
