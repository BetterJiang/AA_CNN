abstract .
gaussian processes are a natural way of dening prior distributions over func - tions of one or more input variables .
in a simple nonparametric regression problem , where such a function gives the mean of a gaussian distribution for an observed response , a gaussian process model can easily be implemented using matrix computations that are feasible for datasets of up to about a thousand cases .
hyperparameters that dene the covariance function of the gaussian process can be sampled using markov chain methods .
regression models where the noise has a t distribution and logistic or probit models for classication applications can be implemented by sampling as well for latent values un - derlying the observations .
software is now available that implements these methods using covariance functions with hierarchical parameterizations .
models dened in this way can discover high - level properties of the data , such as which inputs are relevant to predicting
a nonparametric bayesian regression model must be based on a prior distribution over the innite - dimensional space of possible regression functions .
it has been known for many years that such priors over functions can be dened using gaussian processes ( ohagan 123 ) , and essentially the same model has long been used in spatial statistics under the name of kriging .
gaussian processes seem to have been largely ignored as general - purpose re - gression models , however , apart from the special case of smoothing splines ( wahba 123 ) , and some applications to modeling noise - free data from computer experiments ( eg , sack , welch , mitchell , and wynn 123 ) .
recently , i have shown that many bayesian regression models based on neural networks converge to gaussian processes in the limit of an innite network ( neal 123 ) .
this has motivated examination of gaussian process models for the high - dimensional applications to which neural networks are typically applied ( williams and rasmussen 123 ) .
the empirical work of rasmussen ( 123 ) has demonstrated that gaus - sian process models have better predictive performance than several other nonparametric
regression methods over a range of tasks with varying characteristics .
the conceptual sim - plicity , exibility , and good performance of gaussian process models should make them very attractive for a wide range of problems .
one reason for the previous neglect of gaussian process regression may be that in a straightforward implementation it involves matrix operations whose time requirements grow as the cube of the number of cases , and whose space requirements grow as the square of the number of cases .
twenty years ago , this may have limited use of such models to datasets with less than about a hundred cases , but with modern computers , it is feasible to apply gaussian process models to datasets with a thousand or more cases .
it may also be possible to reduce the time requirements using more sophisticated algorithms ( gibbs and mackay
the characteristics of a gaussian process model can easily be controlled by writing the covariance function in terms of hyperparameters .
one approach to adapting these hyper - parameters to the observed data is to estimate them by maximum likelihood ( or maximum penalized likelihood ) , as has long been done in the context of spatial statistics ( eg , mardia and marshall 123 ) .
in a fully bayesian approach , the hyperparameters are given prior distributions .
predictions are then made by averaging over the posterior distribution for the hyperparameters , which can be done using markov chain monte carlo methods .
these two approaches often give similar results ( williams and rasmussen 123 , rasmussen 123 ) , but the fully bayesian approach may be more robust when the models are elaborate .
applying gaussian process models to classication problems presents new computational problems , since the joint distribution of all quantities is no longer gaussian .
approximate methods of bayesian inference for such models have been proposed by barber and williams ( 123 ) and by gibbs and mackay ( 123b ) .
a general approach to exactly handling classi - cation and other generalized models ( eg , for a poisson response ) is to use a markov chain monte carlo scheme in which unobserved latent values associated with each case are explicitly represented .
this paper applies this approach to classication using logistic or probit models , and to regression models in which the noise follows a t distribution .
i have written software in c for unix systems that implements gaussian process methods for regression and classication , within the same framework as is used by my bayesian neural network software .
this software is is freely available for research and educational use . 123 the covariance functions supported may consist of several parts , and may be specied in terms of hyperparameters , as described in detail in section 123
these covariance functions provide functionality similar to that of the neural network models .
the software implements full bayesian inference for these hierarchical models using matrix computations and markov chain sampling methods , as described in sections 123 and 123
in sections 123 and 123 , i demonstrate the use of the software on a three - way classication problem , using a model that can identify which of the inputs are relevant to predicting the class , and on a regression problem with i conclude by discussing some areas for future research .
first , however , i will
123follow the links from my home page , at http : / / www . cs . utoronto . ca / radford / .
the version described
here is that of 123 - 123 - 123
introduce in more detail the idea of bayesian modeling using gaussian processes .
123 regression and classication using gaussian processes
, x ( i )
assume we have observed data for n cases , ( x ( 123 ) , t ( 123 ) ) , ( x ( 123 ) , t ( 123 ) ) , .
, ( x ( n ) , t ( n ) ) , in which x ( i ) = x ( i ) is the vector of p inputs ( predictors ) for case i and t ( i ) is the associated target ( response ) .
our primary purpose is to predict the target , t ( n+123 ) , for a new case where we have observed only the inputs , x ( n+123 ) .
( we might sometimes be interested in interpretation as well , but there is no point in interpreting a model that has failed to capture the regularities that would support good predictive performance . ) for a regression problem , the targets will be real - valued; for a classication problem , the targets will be from some nite set of class labels , which we will take to be ( 123 , .
, k 123 ) .
it will sometimes be convenient to represent the distributions of the targets , t ( i ) , in terms of unobserved latent values , y ( i ) , associated with each case .
bayesian regression and classication models are usually formulated in terms of a prior distribution for a set of unknown model parameters , from which a posterior distribution for the parameters is derived , and generally exhibited explicitly .
if our focus is on prediction for a future case , however , the nal result is a predictive distribution for a new target value , t ( n+123 ) , that is obtained by integrating over the unknown parameters .
this predictive distribution can therefore be expressed directly in terms of the inputs for the new case , x ( n+123 ) , and the inputs and targets for the n observed cases , without any mention of the model parameters .
what is more , rather than expressing our prior knowledge in terms of a prior for the parameters , we can instead integrate over the parameters to obtain a prior distribution for the targets in any set of cases .
a predictive distribution for an unknown target can then be obtained by conditioning on the known targets .
these operations are most easily carried out if all the distributions are gaussian .
fortunately , gaussian processes are exible enough to represent a wide variety of interesting regression models , many of which would have an innite number of parameters if formulated in more conventional
before discussing such nonparametric models , however , it may help to see how the scheme
works for a simple linear regression model , which can be written as
t ( i ) = +
u u + ( i )
where ( i ) is the gaussian noise for case i , assumed to be independent from case to case , and to have mean zero and variance 123 but that and the u are unknown .
for the moment , we will assume that 123
let us give and the u independent gaussian priors with means of zero and variances u .
for any set of cases with xed inputs , x ( 123 ) , x ( 123 ) , .
. , this prior distribution for parameters implies a prior distribution for the associated target values , t ( 123 ) , t ( 123 ) , .
. , which
will be multivariate gaussian , with mean zero , and with covariances given by
covht ( i ) , t ( j ) i = e " +
u u + ( i ) ! +
u + ij123
u u + j ! #
where ij is one if i = j and zero otherwise .
this mean and covariance function are sucient to dene a gaussian process giving a distribution over possible relationships between the inputs and the target .
( strictly speaking , one might wish to conne the term gaussian process to distributions over functions from the inputs to the target .
the relationship above is not functional , since ( due to noise ) t ( i ) may dier from t ( j ) even if x ( i ) is identical to x ( j ) .
the looser usage is convenient here , however . )
suppose now that we know the inputs , x ( 123 ) , .
, x ( n ) , for n observed cases , as well as x ( n+123 ) , the inputs in a case for which we wish to predict the target .
we can use equation ( 123 ) to compute the n+123 by n+123 covariance matrix of the associated targets , t ( 123 ) , .
, t ( n ) , t ( n+123 ) .
together with the assumption that the means are zero , these covariances dene a gaussian joint distribution for the targets in the observed and unobserved cases .
we can condition on the known targets to obtain the predictive distribution for t ( n+123 ) given t ( 123 ) , .
, t ( n ) .
well - known results ( eg , von mises 123 , section 123 ) show that this predictive distribution is gaussian , with mean and variance given by
t ( 123 ) , .
, t ( n ) i = kt c123 t t ( 123 ) , .
, t ( n ) i = v kt c123 k
where c is the n by n covariance matrix for the targets in the observed cases ( from equa - tion ( 123 ) ) , t = ( t ( 123 ) t ( n ) ) t is the vector of known target values in these cases , k is the vector of covariances between t ( n+123 ) and the n known targets , and v is the prior variance of t ( n+123 ) ( ie , cov ( t ( n+123 ) , t ( n+123 ) ) from equation ( 123 ) ) .
in practice , our prior knowledge will usually not be sucient to x appropriate values for the hyperparameters that dene the covariance ( , , and the u for the simple model of equation ( 123 ) ) .
we will therefore give prior distributions to the hyperparameters , and base predictions on a sample of values from their posterior distribution .
sampling from the posterior distribution requires computation of the log likelihood based on the n observed cases , which is
log det c
tt c123 t
the derivatives of l can also be computed , and used when sampling , as described in sec - tions 123 and 123
this procedure is unnecessarily expensive for the simple regression model just discussed , which is better handled by more standard computational procedures .
however , the gaussian
process procedure can handle more interesting models by simply using a dierent covariance function than that of equation ( 123 ) .
for example , a regression model based on arbitrary smooth functions can be obtained using the covariance function
covht ( i ) , t ( j ) i = 123 exp
u ) 123 ! + ij123
here , and the u are hyperparameters , which would usually be given some prior distribu - tion rather than being xed .
other possibilities for the covariance function are discussed in
regression models with non - gaussian noise and models for classication problems , where the targets are from the set ( 123 , .
, k 123 ) , can be dened in terms of a gaussian process model for latent values associated with each case .
these latent values are used to dene a distribution for the target in a case .
for example , a logistic model for binary targets can be dened in terms of latent values
y ( i ) by letting the distribution for the target in case i be given by
the latent values are given some gaussian process prior , such as
p ( t ( i ) = 123 ) = h123 + exp ( cid : 123 ) y ( i ) ( cid : 123 ) i123
covhy ( i ) , y ( j ) i = 123 exp
this covariance function gives a model in which the probability of the target being 123 varies smoothly as a function of the inputs .
when there are three or more classes , an analogous model can be dened using k latent
k 123 , which dene class probabilities as follows :
values for each case , y ( i )
, y ( i )
p ( t ( i ) = k ) = exp ( cid : 123 ) y ( i )
the k latent values can be given independent gaussian process priors .
( this representation is redundant , but removing the redundancy by forcing one of these latent values to always be zero would introduce an arbitrary asymmetry into the prior . )
for computational reasons , the covariance function of equation ( 123 ) must usually be mod -
ied by the addition of at least a small amount of jitter , as follows :
covhy ( i ) , y ( j ) i = 123 exp
u ) 123 ! + ijj 123
here , j gives the amount of jitter , which is similar to the noise in a regression model .
including a small amount of jitter ( eg , j = 123 ) makes the matrix computations better
conditioned , and improves the eciency of sampling , while having only a small eect on
the eect of a probit model can be produced by using a larger amount of jitter .
a probit model for binary targets could be dened directly in terms of latent values , z ( i ) , having a covariance function without jitter , as follows :
p ( t ( i ) = 123 ) = ( cid : 123 ) z ( i ) ( cid : 123 )
where is the standard gaussian cumulative distribution function .
this formulation of the probit model can be mimicked using latent values , y ( i ) , whose covariance function includes a jitter term ( as in equation ( 123 ) ) .
when j = 123 , the y ( i ) can be regarded as sums of jitter - free latent variables , z ( i ) , and independent jitter of variance one .
a probit model can then be
p ( t ( i ) = 123 | y ( i ) ) = ( cid : 123 ) y ( i ) ( cid : 123 )
where ( y ) = ( 123 if y < 123; 123 if y >= 123 ) .
integrating over the jitter in y ( i ) gives the eect of equation ( 123 ) .
finally , scaling up the magnitude of both the jitter and non - jitter parts of the covariance ( eg , so that j = 123 ) will leave the eect of equation ( 123 ) unchanged , at which point the threshold function can be replaced by the logistic function of equation ( 123 ) , since the magnitude of y ( i ) will usually be large enough that the value of the logistic function will be close to zero or one . 123
if the covariance function used allows the latent values to be any function of the inputs ( plus jitter ) , the same class probabilities will be representable using either a logistic or a probit model .
the two sorts of models would dier only in the exact prior over class proba - bility functions that they embody .
it is not yet clear which of the two models will be better in typical situations .
it is also possible to make the amount of jitter be a hyperparameter , allowing the data to determine which of the two models is more appropriate , or to select an intermediate model .
latent values can also be used to dene regression models with non - gaussian noise , with the latent value being the noise - free value of the regression function .
( in practice , it is usually necessary to include a small amount of jitter in the covariance function for the latent values , which has the eect of introducing some minimum amount of gaussian noise . ) a t distribution for the noise is particularly convenient , since it can be expressed in terms of a gaussian noise model in which the noise variances for the cases are independently drawn from an inverse gamma distribution .
in the implementation of this model , these case - by - case noise variances are explicitly represented , and sampled .
the latent values are needed to sample for the noise variances , but can be discarded once used for this purpose .
123it would be possible for the software to allow the option of using the or functions instead of the logistic , thereby allowing a probit model to be implemented exactly , but this is not done at the moment .
it might also be possible to allow the logistic to be replaced by another function that produces the exact logistic model when some nite amount of jitter is used , but this has not been investigated in detail either .
123 covariance functions and their hyperparameters
a wide variety of covariance functions can be used in the gaussian process framework , sub - ject to the requirement that a valid covariance function must result in a positive semidenite covariance matrix for the targets in a set of any number of cases , in which the inputs take on any possible values .
in a bayesian model , the covariance function will usually depend on various hyperparameters , which are themselves given prior distributions .
such hyper - parameters can control the amount of noise in a regression model , the scale of variation in the regression function , the degree to which various input variables are relevant , and the magnitudes of dierent additive components of a model .
the posterior distribution of these hyperparameters will be concentrated on values that are appropriate for the data that was
characterizing the set of valid covariance functions is not trivial , as seen by the exten - sive discussions in the book by yaglom ( 123 ) .
one way to construct a variety of covari - ance functions is by adding and multiplying together other covariance functions , since the element - by - element sum or product of any two symmetric , positive semidenite matrices is also symmetric and positive semidenite ( horn and johnson 123 , 123 . 123 and 123 . 123 ) .
sums of covariance functions are useful in dening models with an additive structure , since the covariance function for a sum of independent gaussian processes is simply the sum of their separate covariance functions .
products of covariance functions are useful in dening a covariance function for cases with multidimensional inputs in terms of covariance functions for single inputs .
the current software supports covariance functions that are the sum of one or more terms
of the following types :
123 ) a constant part , which is the same for any pair of cases , regardless of the inputs in those cases .
this adds a constant component to the regression function ( or to the latent values for a classication model ) , with the prior for the value of this constant component having the variance given by this constant term in the covariance function .
123 ) a linear part , which for the covariance between cases i and j has the form
this produces a linear function of the inputs , as seen in section 123 , or adds a linear component to the function , if there are other terms in the covariance as well .
123 ) a jitter part , which is zero for dierent cases , and a constant for the covariance of a case with itself .
jitter is used to improve the conditioning of the matrix computations , or to produce the eect of a probit classication model .
the noise in a regression model is similar , but is treated separately in this implementation ( jitter aects the latent values , and through them the targets; noise aects only the targets ) .
123 ) any number of exponential parts , each of which , for the covariance between cases i
and j , has the form
if there are several exponential parts , they may use dierent values of r , , and the u .
for the covariance function to be positive denite , r must be in the range 123 to 123
the default value of r = 123 produces a function ( or additive component of a function ) that is innitely dierentiable , but not constrained to be of any particular form .
the parameters of these terms in the covariance function may be xed , or they may be treated as hyperparameters , with given prior distributions , except that the power , r , for an exponential part must currently be xed .
some of the possible distributions over functions that can be obtained using covariance functions of this form are illustrated in figure 123
( these are functions of a single input , so the index u is dropped ) .
the top left and top right each show functions drawn randomly from a gaussian process with a covariance function consisting of a single exponential part .
the distance over which the function varies by an amount comparable to its full range , given by 123 / , is smaller for the top - right than the top - left .
the bottom left shows functions generated using a covariance function that is the sum of constant , linear , and exponential parts .
the magnitude of the exponential part , given by , is rather small , so the functions depart only slightly from straight lines .
the bottom right shows functions drawn from a prior whose covariance function is the sum of two exponential parts , that produce variation at dierent scales , and with dierent magnitudes .
the software can produce such plots of randomly drawn functions in one and two dimensions , using the cholesky decomposition of the covariance matrix for the targets over a grid of input points , as described in section 123
for problems with more than one input variable , the u and u parameters control the degree to which each input is relevant to predicting the target .
if u is close to zero , input u will have little eect on the degree of covariance between cases ( or at least , little eect on the portion of the covariance due to the exponential part in which the u hyperparameter occurs ) .
two cases could therefore have high covariance even if they have greatly dierent values for input u ie , input u is eectively ignored .
in typical applications , the constant part and jitter part ( if any ) of the covariance would be given xed values , but the available prior information would not be sucient to x the other hyperparameters , which specify the magnitudes of the linear and exponential parts , and the scales of variation and relevances of the various inputs .
the standard deviation of the noise for a regression model would also typically be unknown .
these hyperparameters should therefore usually be given fairly vague prior distributions .
these distributions should be proper , however , as using an improper prior will often produce an improper posterior .
the priors for hyperparameters supported by the software all take the same form .
if is a hyperparameter ( all of which take on only positive values ) , the value of = 123 can be given a gamma prior with density
covht ( i ) , t ( j ) i = exp ( cid : 123 ) ( cid : 123 ) x ( i ) x ( j ) ( cid : 123 ) 123 ( cid : 123 )
covht ( i ) , t ( j ) i = exp ( cid : 123 ) 123 ( cid : 123 ) x ( i ) x ( j ) ( cid : 123 ) 123 ( cid : 123 )
covht ( i ) , t ( j ) i = 123 + x ( i ) x ( j )
+ 123 exp ( cid : 123 ) 123 ( cid : 123 ) x ( i ) x ( j ) ( cid : 123 ) 123 ( cid : 123 )
covht ( i ) , t ( j ) i = exp ( cid : 123 ) ( cid : 123 ) x ( i ) x ( j ) ( cid : 123 ) 123 ( cid : 123 ) + 123 exp ( cid : 123 ) 123 ( cid : 123 ) x ( i ) x ( j ) ( cid : 123 ) 123 ( cid : 123 )
figure 123 : functions drawn from gaussian processes with various covariance functions .
each of the graphs shows two functions that were independently drawn from the gaussian process with mean zero and with the covariance function given below the graph .
here , is a positive shape parameter , and is the mean of .
the software accepts prior specications in terms of and 123 ( whose units correspond to those of the original hyperparameter , ) .
large values of produce priors for concentrated near 123 , whereas small values of produce vague priors . 123
single hyperparameters , such as in an exponential part , or the noise standard deviation for a simple regression model , may either be given a prior as described above , or be given a xed value ( equivalent to letting = ) .
hyperparameters that come in groups , such as the u in an exponential part , or the u in a linear part , can be given hierarchical priors , expressed in terms of a higher - level hyperparameter associated with the group , which has no direct eect on the covariance function , but which determines the mean for the lower - level hyperparameters .
for example , the u hyperparameters for an exponential part might be accompanied by a higher - level hyperparameter , .
at the top level , = 123 given a gamma prior of the form
for a given value of , the u hyperparameters associated with particular inputs are inde - pendent , with u = 123
u having a gamma prior with mean , as follows :
p ( u | ) =
note that the shape parameters for the two levels , 123 and 123 , can be dierent ( 123 is the same for all inputs , u , however ) .
the top level of the hierarchy can be omitted ( eectively , 123 = ) , in which case the u are independent .
the lower level of the hierarchy can also be omitted ( eectively , 123 = ) , in which case the u are all equal to .
finally both levels can be omitted ( 123 = 123 = ) , in which case the u have xed values .
these hierarchical priors can also link together the u parameters in the linear part of the covariance , as well as the noise standard deviations for a regression model with more than one target .
however , at present there is no way of linking hyperparameters of dierent types , nor of linking hyperparameters pertaining to dierent parts of the covariance function ( eg , the u for dierent exponential parts ) .
when there is more than one target or latent value for each case , the same hyperparameters are currently used for the independent gaussian processes that model the relationship of each value to the inputs .
the only exception to this is that dierent noise standard deviations are possible for regression models with more than one target .
in contrast to the elaborate provisions for dierent covariance functions , the software currently assumes that the mean function for the gaussian process is always zero .
this is appropriate for problems where prior knowledge is vague .
note that using a zero mean
123there is an arbitrary aspect to this form of prior specication , since controls not only how diuse the prior is , but also its shape .
this could be xed by letting the gamma prior be for = r , with r being any specied power .
the present scheme is analogous to the priors used for neural network models , where r = 123 results in a conjugate prior with some computational advantages .
gaussian process does not mean that we expect the actual regression function to take on positive and negative values over equal parts of its range .
if the covariance function has a large constant term , we would not be surprised if the actual function were always positive , or always negative ( at least over the range of interest ) .
using a mean function of zero simply reects our lack of prior knowledge as to what the sign will turn out to be .
in practice , it will usually be desirable to transform the targets so that their mean is approximately zero , in order to eliminate any need for a large constant term in the covariance .
including a large constant term is undesirable because it increases the round - o error in the matrix
123 matrix computations
inferences regarding a gaussian process model given particular values for its hyperparam - eters can be performed using computations involving the covariance matrix for the targets or latent values associated with the observed cases .
if appropriate hyperparameter values are known a priori , these matrix computations are all that are needed to make predictions for the targets in new cases .
in the more common situation where the hyperparameters are unknown , such matrix computations are used to support the markov chain sampling methods described in section 123 , and then to make predictions using the resulting sample of hyperparameter values ( and latent values , if required ) .
the central object in these computations is the n by n covariance matrix of the latent values underlying the observed cases , or of the target values themselves , for a regression model .
this covariance matrix , which we will denote by c , depends on the observed inputs for these cases , and on the particular values of the hyperparameters , both of which are considered xed here .
the diculty of computations involving c is determined by its condition number the ratio of its largest eigenvalue to its smallest eigenvalue .
condition number is large , round - o error in the matrix computations may cause them to fail or to be highly inaccurate .
this potential problem can be controlled by using covariance functions that include jitter terms ( see section 123 ) , since the jitter contributes additively to every eigenvalue of the matrix , reducing the condition number .
when c is the covariance matrix for the targets in a regression model , the noise variance has an equivalent eect .
for most problems , it appears that the addition of a small amount of jitter to the covariance will not seriously aect the statistical properties of the model , and may even be desirable .
accordingly , the software does not attempt to handle covariance matrices that are very
the present implementation is based on nding the cholesky decomposition of c that is , the lower - triangular matrix , l , for which c = llt .
the cholesky decomposition can be found by a simple algorithm ( see , for example , thisted 123 , section 123 ) , which runs in time proportional to n123
once the cholesky decomposition has been found , the determinant of c can easily be computed as the square of the product of the diagonal elements of l .
( in practice , the log of the determinant is found from the sum of the logs of the diagonal
elements . ) another use of the cholesky decomposition is in generating latent or target values from the prior .
standard methods can be used to randomly generate a vector , n , composed of n independent gaussian variates with mean zero and variance one .
one can then compute the vector ln , which will have mean zero and covariance matrix llt = c .
this procedure was used to produce the plots in figure 123 , using the covariance matrix for the targets over a grid of input values , with the addition of an unnoticeable amount of jitter .
the primary use of the cholesky decomposition is in computing the inverse of c , which arises both in the predictive distribution for a new case ( equations ( 123 ) and ( 123 ) ) and in the log likelihood ( equation ( 123 ) ) .
these computations could be performed without explicitly nding the inverse of c , since c123 b can be found using the cholesky decomposition by rst solving lv = b for v using forward substitution , and then solving lt u = v for u using backward substitution .
however , it is more convenient to explicitly compute the inverse , since it will often be needed anyway in order to compute derivatives of the log likelihood .
computation of c123 is done by applying the procedure just described to compute c123 b for the n vectors b that are all zero except for one element with the value one .
this takes time proportional to n123
once c123 has been computed , we can prepare to make predictions from a regression model by computing b = c123 t , where t is the vector of targets in the training cases .
the mean of the predictive distribution for the target in a test case can then be found in time proportional to n .
we rst compute the vector , k , of covariances between the targets in the test case and in the n training cases .
we then compute the predictive mean of equation ( 123 ) as kt b .
this is the method used in the present implementation .
an alternative is to solve lu = k for u and to solve lv = t for v , and then compute the predictive mean as ut v .
this is less ecient , taking time proportional to n123 for each test case , but gibbs and mackay ( 123a ) report that it is more accurate when c is poorly conditioned .
if we require the predictive variance as well as the mean , we must compute kt c123 k , for use in equation ( 123 ) , which will take time proportional to n123
predictions for classication models involve similar operations , but focused on the latent value associated with a test case .
a vector of latent values , y , associated with the training cases must be available .
the vector of covariances , k , between these latent values and the latent value in a test case can be computed , and a predictive mean and variance for the latent value in the test case can then be found as above .
a sample of values from this gaussian predictive distribution can easily be obtained , from which monte carlo estimates for the class probabilities in the test case can be computed by simply averaging the probabilities that are obtained by substituting the latent values in this sample into equation ( 123 ) or ( 123 ) .
one may also wish to sample from the joint posterior distribution of the targets or la - tent values in a set of cases , either as part of some other computation , or in order to plot regression functions drawn from the posterior distribution .
conditional on values for the hyperparameters , for the latent variables associated with training cases ( for a classication model ) , and for the case - by - case noise variances ( for a regression model with t - distributed noise ) , these distributions will be gaussian , with means and covariances given by general -
izations of equations ( 123 ) and ( 123 ) .
in a regression model , for example , if y is the vector of latent values in a set of m test cases , and t is the vector of target values in n training cases ,
e ( y | t ) = kt c123 t
cov ( y | t ) = w kt c123 k
where c is the n by n covariance matrix for the targets in training cases , w is the m by m covariance matrix for latent values in the test cases , and k is the n by m matrix of covariances between targets in training cases and latent values in test cases .
once these means and covariances have been computed , a value for y can be generated using the cholesky decomposition of its covariance matrix , as described above in regard to sampling from the prior .
the markov chain methods used to sample from the posterior distribution of the hyperpa - rameters in a regression model require computation of the log likelihood , l , of equation ( 123 ) .
as seen above , this is easily done using the cholesky decomposition of c .
for some of the markov chain sampling methods , the derivatives of l with respect to the various hy - perparameters are also required .
the derivative of the log likelihood with respect to a hyperparameter can be written as follows ( mardia and marshall 123 ) :
tt c123 c
the trace of the product in the rst term can be computed in time proportional to n123 , assuming that c123 has already been computed .
the second term can also be computed in time proportional to n123 , by rst computing b = c123 t ( which is probably needed anyway , to compute l itself ) , multiplying this on the left by the matrix of derivatives , and nally multiplying the result by bt .
apart from the computation of b , this procedure must be repeated for each hyperparameter , and for each regression target , if there is more than one .
the markov chain methods used for classication models require a similar computation ,
but with the vector of targets , t , replaced by the vector of current latent values , y .
for large data sets , the time required for these computations is dominated by that required to form the cholesky decomposition of c , and to then compute c123 , for which the number of operations required grows in proportion to n123
indeed , on machines with memory caches , the time required for these computations may grow at a rate even faster than n123 , since larger matrices will not t in the fast cache .
the software attempts to reduce such cache eects by whenever possible scanning matrices along rows rather than down columns , but for large matrices the slowdown on our sgi machine can still be substantial .
for small data sets ( eg , 123 cases ) , the time required to compute the derivatives of the log likelihood with respect to the hyperparameters can dominate , even though this time grows only in proportion to n123
this may occur , for example , when there are many hyperparameters controlling the relevance of many input variables , so that computing the
matrix of derivatives of the covariances takes a lot of time .
these computations can be sped up if the individual values for the exponential parts of the covariances have been saved ( as these appear in the expressions for the derivatives ) .
the software does this when n is small enough that the memory required to do so is not too large; when n is larger , the other operations dominate anyway .
123 markov chain sampling
the covariance functions for most gaussian process models will contain unknown hyper - parameters , which must be integrated over in a fully bayesian treatment .
the number of hyperparameters will vary from around three or four for a very simple regression model up to several dozen or more for a model with many inputs , whose relevances are individually controlled using hyperparameters such as the u of equation ( 123 ) .
markov chain monte carlo methods ( see ( neal 123 ) for a review ) seem to be the only feasible approach to per - forming these integrations , at least for the more complex models .
for classication models , latent values for each training case must also be integrated over , and for regression models in which the noise has a t distribution , we must integrate over the case - by - case noise vari - ances .
these latent values and variances can be included in the state of the markov chain and sampled along with the hyperparameters .
sampling from the posterior distribution of the hyperparameters is facilitated by repre - senting them in logarithmic form , as this makes the sampling methods independent of the scale of the data .
the widely - used method of gibbs sampling cannot easily be applied to this problem , since it seems dicult to sample from the conditional distributions for one hyperparameter given values for the others ( and the latent values , if any ) .
the metropolis algorithm could be used with some simple proposal distribution , such as a gaussian with diagonal covariance matrix .
the software supports this option , along with a variety of other markov chain sampling methods .
however , simple methods such as this explore the region of high probability by an inecient random walk .
it is probably better for most models to use a method that can suppress these random walks ( neal 123 , 123 ) .
the most appropriate way to suppress random walks for this problem seems to be to use the hybrid monte carlo method of duane , kennedy , pendleton , and roweth ( 123 ) , or the variant of this method due to horowitz ( 123 ) .
i have employed the hybrid monte carlo method to do bayesian inference for neural network models ( neal 123 ) , and rasmussen ( 123 ) has used it for gaussian process regression .
several variants of the hybrid monte carlo method are supported by the markov chain modules that i use for both the neural network and the gaussian process software .
i will give only a brief , informal description of the method here .
more details can be found elsewhere ( neal 123 , 123; rasmussen 123 ) .
the hybrid monte carlo method suppresses random walks by introducing momentum variables that are associated with the position variables that are the focus of interest .
for the gaussian process application , the position variables are the hyperparameters dening the covariance function .
the state of the simulation evolves in the same way as the position
and momentum of a physical particle travelling through a region of variable potential energy .
the momentum causes the particle to continue in a consistent direction until such time as a region of high energy ( low probability ) is encountered .
this motion must be randomized a bit in order to ensure that the correct distribution is sampled from , but not so much that undesirable random walk behaviour results .
in practice , the dierential equations that describe how the position and momentum change through time are discretized , and the bias due to discretization error is eliminated by accepting or rejecting the new state in the metropolis style .
the leapfrog discretization is usually used .
in order to perform a leapfrog update , the derivatives of the log of the posterior probability with respect to the hyperparameters must be computed .
to decide whether to accept an update ( or sequence of updates ) , the log of the posterior probability must be found ( except for its normalizing constant ) .
the log posterior probability is computed from the log of the prior probabilities for the hyperparameters , which have the easily computed gamma form , and the log likelihood , from equation ( 123 ) .
the derivatives are found by adding the derivative of the log prior , which is easily computed , to the derivative of the log likelihood , which is computed using equation ( 123 ) .
in the original hybrid monte carlo method of duane , et al .
( 123 ) , several leapfrog updates are done , after which a decision whether to accept the result is made .
the momentum is also randomized at this time .
a variation using windows of states ( neal 123 ) can be used to increase the acceptance probability .
in the variation due to horowitz ( 123 ) , an acceptance decision is made after each leapfrog update , after which the momentum is only partially randomized .
i refer to this as hybrid monte carlo with persistence of the momentum .
for hybrid monte carlo to work well , appropriate stepsizes for the leapfrog updates must be selected if too large a stepsize is used , the acceptance rate will be very low , but if the stepsize is too small , progress will be needlessly slow .
dierent stepsizes can be used for dierent hyperparameters; this is equivalent to rescaling the hyperparameters ( in their logarithmic form ) using dierent scale factors .
the software includes a heuristic procedure that automatically selects a stepsize for each hyperparameter .
these selections are based on estimates of the second derivatives of the log posterior density with respect to the hyperparameters , which indicate how large a change can be made to a hyperparameter without getting into a region of low probability .
these automatically selected stepsizes can be ( and usually are ) manually adjusted by multiplying them all by some factor , which is chosen on the basis of preliminary runs .
accordingly , the real role of the heuristics is to set the relative stepsizes for dierent hyperparameters .
the heuristics used at present are rather simple .
the stepsizes for high - level hyperpa - rameters are scaled down by the square root of the number of low - level hyperparameters that they control .
this is in accord with how one would expect the width of their posterior distribution to scale .
similarly , the stepsize for the noise variance in a regression model is scaled down by the square root of the number of training cases .
however , the stepsizes for the other hyperparameters ( eg , the and u hyperparameters in an exponential part of the covariance ) are not scaled on the basis of the number of training cases .
whether this is the
right thing to do depends on whether the posterior distribution for these hyperparameters becomes more tightly concentrated as the number of training cases increases .
i conjecture that these posterior distributions are typically more concentrated than the prior , but that they do not become more and more concentrated as the number of training cases increases , except perhaps for the u parameters in an exponential part with r < 123 , for which the functions produced are fractal .
mardia and marshall ( 123 ) consider this problem in a spatial statistics context , under the assumption that the range of the input variables in - creases with the number of training cases , which i presume is not the typical situation for regression and classication problems .
if additional training cases instead provide denser sampling within a xed region , it seems that they can provide only a limited amount of information about the hyperparameters , unless the function modeled has a fractal nature , in which information is repeated at all scales .
for a classication model , these hybrid monte carlo updates of the hyperparameters use the likelihood based on the current latent values associated with training cases , not on the targets directly .
these hyperparameter updates must be interleaved with updates of the latent values themselves , for which gibbs sampling is presently used .
new latent values are chosen for each case in a sequential scan . 123 these values are drawn from the conditional distribution for such a latent value given the observed target for that training case , and given the current values of the hyperparameters and all the other latent values .
the density for this conditional distribution is proportional to the product of the likelihood given the target , from equation ( 123 ) or ( 123 ) , and the gaussian conditional density given the other latent values .
the conditional density for y ( i ) given the other latent values , all of which are collected in y , is proportional to exp ( 123 123 yt c123y ) , and can be found in time proportional to n if c123 has already been computed .
the nal conditional density is log - concave , and hence can eciently be sampled from using the adaptive rejection method of gilks and wild ( 123 ) .
once c123 has been computed , taking time proportional to n123 , a complete gibbs sampling scan takes time proportional only to n123
it therefore makes sense to perform quite a few gibbs sampling scans between each update of the hyperparameters , as this adds little to the time requirements , and probably makes the markov chain mix faster .
the software also supports regression models with t - distributed noise , expressed as gaus - sian noise with case - by - case variances drawn from an inverse gamma distribution .
the markov chain must then sample somehow for the case - by - case noise variances , which are needed to compute the covariances of the targets .
in one approach , case - by - case latent values are maintained , and updated using gibbs sampling , in a manner analogous to that used for classication models .
gibbs sampling can then easily be done for the case - by - case noise variances as well , based only on the hyperparameters controlling the noise level , the latent values , and the targets .
the software also supports a second approach , however , in which latent values are not kept around permanently .
instead , latent values are temporarily
123when there are several latent values for each case , it makes no dierence whether the inner loop of the
scan is over cases or over the several values for one case .
figure 123 : the 123 training cases used for the three - way classication problem .
each case is plotted ac - cording to its values for x123 and x123 , with the plot symbol indicating the class , as follows :
class 123 = lled square class 123 = plus sign class 123 = open triangle
generated just before the noise variances are updated , using equations ( 123 ) and ( 123 ) , and then discarded after being used to generate new values for the noise variances .
123 example : a three - way classication problem
123 , x ( i )
123 , x ( i )
123 , and x ( i )
to demonstrate the use of the software for classication , i applied it to a synthetic three - way classication problem .
pairs of data items , ( x ( i ) , t ( i ) ) were generated by rst randomly drawing quantities x ( i ) independently from the uniform distribution over the interval ( 123 , 123 ) .
the class of the item , t ( i ) , encoded as 123 , 123 , or 123 , was then selected as follows : if the two - dimensional euclidean distance of ( x ( i ) 123 ) from the point ( 123 , 123 ) was less than 123 , the class was set to 123; otherwise , if 123 x ( i ) 123 was less than 123 , the class was set to 123; and if neither of these conditions held , the class was set to 123
note that 123 , x ( i ) 123 and x ( i ) 123 , available for prediction of the target were the values of x ( i ) 123 plus independent gaussian noise of standard deviation 123 .
i generated 123 cases in this way , of which 123 were used for training the model , and 123 for testing the resulting predictive performance .
the 123 training case are shown in figure 123
123 have no eect on the class .
the inputs , x ( i )
123 , x ( i ) 123 + 123 x ( i )
123 , x ( i ) 123 , and x ( i )
123 , and x ( i )
123 , x ( i )
123 , x ( i )
this data was modeled using a gaussian process for the latent values , y ( i ) , whose co - variance function consisted of three terms a constant part ( xed at 123 ) , an exponential part in which the magnitude , , and the scales for the four inputs , u , were variable hyper - parameters , and a jitter part , xed at j = 123
the fairly large amount of jitter produces an eect close to a probit model , as discussed in section 123
since each of the u can vary separately ( under the control of a common higher - level hyperparameter ) , the model is ca -
pable of discovering that some of the inputs are in fact irrelevant to the task of predicting the target .
we hope that the posterior distribution of u for these irrelevant inputs will be concentrated near zero , so that they will not degrade predictive performance .
the persistent form of hybrid monte carlo was used in the sampling , as this allows the latent values to be resampled between each leapfrog update of the hyperparameters .
a fairly low persistence was used for the rst few leapfrog updates , in order to allow energy to be dissipated rapidly at rst ( through replacement of the momentum , and consequent elimination of kinetic energy ) .
a larger persistence was used thereafter , in order to suppress random walk behaviour .
before every update of the hyperparameters , the latent values associated with training cases were updated using 123 gibbs sampling scans .
a sequence of ve of these combined gibbs sampling and leapfrog updates were done in each sampling iteration , after which the hyperparameters and latent values were saved for possible later use .
sampling was continued for 123 such iterations ( 123 leapfrog updates ) , which took about 123 minutes on our sgi machine .
complete details regarding the model and the sampling procedure used may be found in
the software documentation , where this problem is also used as an example .
the convergence of the markov chain simulation can be assessed by plotting how the values of the hyperparameters change over the course of the simulation .
figure 123 shows the progress of the u hyperparameters in the exponential part of the covariance .
as hoped , we see that by about iteration 123 , an apparent equilibrium has been reached in which the hyperparameters 123 and 123 , associated with the irrelevant inputs , have values that are much smaller than those for 123 and 123 , which are associated with the inputs that provide information about the target class .
the markov chain simulation also updates the three latent values associated with each training case , which dene the class probabilities by equation ( 123 ) .
these latent values for a particular training case are plotted over the course of the simulation in figure 123
the gibbs sampling scans appear to be eective in moving these values about their equilibrium distribution fairly rapidly .
to make predictions for test cases , we can average together the predictive probabilities based on iterations after equilibrium was apparently reached .
to reduce computation time , only every fth iteration was used , starting at iteration 123 ( for a total of ten iterations ) .
for each such iteration , the covariance matrix for the latent values in training cases was inverted , after which the predictive mean and variance for the latent values in each of the 123 test cases was found , using the latent values for training cases saved for that iteration .
a sample of 123 points from this predictive distribution was used to produce a monte carlo estimate of the predictive probabilities for each of the three classes .
the nal predictive probabilities were found by averaging the predictions found in this way for each of the iterations used .
the guess for the class in a test case was the one with the largest predictive
figure 123 : progress of the four relevance hyperparameters during the course of the markov chain simulation .
the values are plotted on a log scale , with 123 = solid , 123 = long dash , 123 = short dash , and 123 = dotted .
figure 123 : the latent values associated with one training case , for which x123 = 123 , x123 = 123 , and t = 123 , over the course of the markov chain simulation .
the three latent values are shown as class 123 = solid , class 123 = dashed , and class 123 = dotted .
this procedure took about 123 minutes on our sgi machine .
the classication error rate on the 123 test cases was 123% .
this performance is close to that of an analogous neural network model .
a proper comparison of predictive performance with that of other classi - cation methods is beyond the scope of this paper .
( rasmussen ( 123 ) has done extensive comparisons of gaussian process models with other methods for regression problems . )
as expected , the time required for this problem varies considerably with the number of training cases .
with only 123 training cases , the time for the markov chain simulation was about 123 minutes and the time required to make predictions for the 123 test cases was less than a minute .
the classication error rate using only the rst 123 training cases was 123% .
123 example : a regression problem with outliers
to demonstrate how the software can be used to handle regression problems with outliers , i applied a gaussian process model with non - gaussian noise to a simple synthetic problem with a single input variable .
cases were generated in which the input variable , x , was drawn from a standard gaussian distribution , and the corresponding target value came from a distribution with mean of
123 + 123 x + 123 sin ( 123 x ) + 123 / ( 123 + x123 )
for most cases , the distribution of the target about this mean was gaussian with standard deviation 123 .
however , with probability 123 , a case was made an outlier , for which the standard deviation was 123 instead .
this data was modeled using a gaussian process for the expected value of the target , with the noise assumed to come from a t distribution with 123 degrees of freedom .
this is not particularly close to the actual noise distribution , as described above , but the heavy tails of the t distribution may nevertheless allow this data to be modeled without the outliers having an undue eect .
for comparison , the data was also modeled under the assumption of gaussian noise .
the data and the predictions from these two models are shown in figure 123
for these models , the covariance function used contained a constant part ( xed at 123 ) and an exponential part ( with variable hyperparameters ) .
the model with non - gaussian noise also included a small amount of jitter ( j = 123 ) , in order to improve the conditioning of the matrix computations used to sample for the latent values .
this jitter is equivalent to a small amount of additional noise in the model; since the amount of other noise is a variable hyperparameter , the only real eect is to constrain the total noise to be no less than the
markov chain sampling for the model with t - distributed noise was done by alternating hy - brid monte carlo updates for the hyperparameters ( each consisting of 123 leapfrog updates ) with updates for the case - by - case noise variances .
latent values were generated in order to allow gibbs sampling updates for the noise variances , using equations ( 123 ) and ( 123 ) , but were discarded thereafter .
the markov chain was simulated for 123 such iterations , and
figure 123 : the regression problem with outliers .
the 123 training cases are shown as dots , with the input on the horizontal axis , and the target on the vertical axis .
the solid line gives the mean of the predictive distribution using a model in which the noise was assumed to come from a t distribution with 123 degrees of freedom .
the dotted line gives the mean of the predictive distribution using a model in which the noise was assumed to be gaussian .
predictions were then made based on every fth iteration after iteration 123
the time required for the simulation was about six minutes on our sgi machine .
further details of the model and the markov chain method can be obtained from the
description of this example in the software documentation .
as can be seen in figure 123 , the model with t - distributed noise produces predictions that seem more reasonable than those produced by the model with gaussian noise , based just on looking at the scatterplot of the data .
the predictions using t - distributed noise are also closer to the true function .
the software described in this paper extends the scope of gaussian process models to classication problems and to regression problems with non - gaussian noise , by using a markov chain monte carlo method in which latent values for each case are represented .
models can be based on a variety of covariance functions , which can be dened in terms of hyperparameters with hierarchical priors .
the implementation also allows a wide variety of markov chain sampling methods to be used .
with these facilities , the usefulness of gaussian process models for a variety of problems can be explored .
the examples in this paper show that gaussian process models can be practically applied to classication problems of moderate size , and to regression problems
with non - gaussian noise; other examples of regression and classication models are included in the software documentation .
one major focus for future work is to explore the uses of elaborate covariance functions in real problems .
the fairly simple model of section 123 illus - trates how the hyperparameters dening the covariance function can adaptively determine how relevant the various inputs are for predicting the target .
the range of covariance functions implemented permits hierarchical models that are more elaborate than this .
for example , by including several exponential parts in the covariance function , each with a separate set of relevance hyperparameters , it is possible to dene a prior distribution that puts considerable prior weight on models that are of a nearly additive form , in which the function is decomposed into the sum of several functions , each of which depends on only a small subset of the inputs .
such a model can automatically determine an appropriate additive decomposition , if an additive model is in fact appropriate .
this mirrors a similar idea for neural network models ( neal 123 , section 123 ) .
the implementation described here is rather straightforward .
most operations are per - formed in the simplest way that gives acceptable results .
a number of modications can be contemplated .
faster convergence could probably be obtained by updating the latent variables using hybrid monte carlo rather than gibbs sampling .
computation time for matrix operations might be reduced by using the conjugate gradient approach of gibbs and mackay ( 123a ) .
in another direction , one might look for ways of reducing or eliminating the need for jitter in the covariance function , since although this appears to usually be an acceptable solution to the problem of poorly conditioned matrices , there may be some circumstances where it is undesirable , such as when using a gaussian process to model noise - free data from computer experiments ( eg , sack , welch , mitchell , and wynn 123 ) .
even without further algorithmic improvements , gaussian process models are now feasible for datasets of up to about a thousand cases , using fairly run - of - the - mill computers , provided one is willing to wait up to several hours for results on the larger datasets .
using these models is therefore a feasible option for many regression and classication problems .
despite the fairly unfavourable n123 growth in memory requirements and n123 growth in computation time of the present gaussian process algorithms , improvements in computer technology over the next few years will likely allow these models to be applied to most problems encountered in practice .
because of the ease with which exible hierarchical models can be dened using gaussian processes , i believe that they will prove to be among the most useful techniques for nonparametric regression and classication .
i thank carl rasmussen for many helpful discussions , and for the opportunity to learn from the implementation of gaussian process regression that he used for his thesis .
i also thank david mackay and chris williams for their comments on the manuscript .
this research was supported by the natural sciences and engineering research council of canada .
