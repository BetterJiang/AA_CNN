we consider the sparse inverse covariance regularization problem or graphical lasso with regular - ization parameter l .
suppose the sample covariance graph formed by thresholding the entries of the sample covariance matrix at l is decomposed into connected components .
we show that the vertex - partition induced by the connected components of the thresholded sample covariance graph ( at l ) is exactly equal to that induced by the connected components of the estimated concentration graph , obtained by solving the graphical lasso problem for the same l .
this characterizes a very interesting property of a path of graphical lasso solutions .
furthermore , this simple rule , when used as a wrapper around existing algorithms for the graphical lasso , leads to enormous performance gains .
for a range of values of l , our proposal splits a large graphical lasso problem into smaller tractable problems , making it possible to solve an otherwise infeasible large - scale problem .
we illustrate the graceful scalability of our proposal via synthetic and real - life microarray examples .
keywords : sparse inverse covariance selection , sparsity , graphical lasso , gaussian graphical mod - els , graph connected components , concentration graph , large scale covariance estimation
consider a data matrix xnp comprising of n sample realizations from a p dimensional gaus - sian distribution with zero mean and positive denite covariance matrix s pp ( unknown ) , that is , mvn ( 123 , s ) , i = 123 , .
the task is to estimate the unknown s based on the n samples .
123 regularized sparse inverse covariance selection also known as graphical lasso ( friedman et al . , 123; banerjee et al . , 123; yuan and lin , 123 ) estimates the covariance matrix s , under the as - sumption that the inverse covariance matrix , that is , s 123 is sparse .
this is achieved by minimizing the regularized negative log - likelihood function :
logdet ( q ) + tr ( sq ) + l
i , j |q
where s is the sample covariance matrix .
problem ( 123 ) is a convex optimization problem in the ( l ) denote the solution to ( 123 ) .
we note that ( 123 ) can also be used in a more non - parametric fashion for any positive semi - denite input matrix s , not necessarily a sample covariance matrix of a mvn sample as described above .
( boyd and vandenberghe , 123 ) .
let bq
also in the department of health , research and policy
c ( cid : 123 ) 123 rahul mazumder and trevor hastie .
mazumder and hastie
a related criterion to ( 123 ) is one where the diagonals are not penalizedby substituting s s + l ipp in the unpenalized problem we get ( 123 ) .
in this paper we concentrate on problem ( 123 ) .
developing efcient large - scale algorithms for ( 123 ) is an active area of research across the elds of convex optimization , machine learning and statistics .
many algorithms have been proposed for this task ( friedman et al . , 123; banerjee et al . , 123; lu , 123 , 123; scheinberg et al . , 123; yuan , 123 , for example ) .
however , it appears that certain special properties of the solution to ( 123 ) have been largely ignored .
this paper is about one such ( surprising ) propertynamely estab - lishing an equivalence between the vertex - partition induced by the connected components of the ( l ) and the thresholded sample covariance matrix s .
this paper is not about a specic algorithm for the problem ( 123 ) it focuses on the aforementioned observation that leads to a novel thresholding / screening procedure based on s .
this provides interesting insight into the ) l 123 obtained by solving ( 123 ) , over a path of l values .
the behavior of ) l 123 can be completely understood by simple screening rules on s .
this can be done without even attempting to solve ( 123 ) arguably a very challenging convex optimization problem .
furthermore , this thresholding rule can be used as a wrapper to enormously boost the performance of existing algorithms , as seen in our experiments .
this strategy becomes extremely effective in solving large problems over a range of values of l sufciently restricted to ensure sparsity and the separation into connected components .
of course , for sufciently small values of l there will be no separation into components , and hence no computational savings .
non - zero pattern of bq path of solutions ( bq the connected - components obtained from the non - zero patterns of ( bq
at this point we introduce some notation and terminology , which we will use throughout the
123 notations and preliminaries
for a matrix z , its ( i , j ) th entry is denoted by zi j .
we also introduce some graph theory notations and denitions ( bollobas , 123 ) sufcient for this exposition .
a nite undirected graph g on p vertices is given by the ordered tuple g = ( v , e ) , where v is the set of nodes and e the collection of ( undirected ) edges .
the edge - set is equiv - alently represented via a ( symmetric ) 123 - 123 matrix123 ( also known as the adjacency matrix ) with p rows / columns .
we use the convention that a node is not connected to itself , so the diagonals of the adjacency matrix are all zeros .
let |v | and |e| denote the number of nodes and edges respectively .
we say two nodes u , v v are connected if there is a path between them .
a maximal connected subgraph123 is a connected component of the graph g .
connectedness is an equivalence relation that decomposes a graph g into its connected components ( ( v , e ) ) 123kwith g = k =123 ( v , e ) , where k denotes the number of connected components .
this decomposition partitions the vertices v of g into ( v ) 123k .
note that the labeling of the components is unique upto permutations on ( 123 , .
throughout this paper we will often refer to this partition as the vertex - partition if the size of a component is one , that is , |v| = induced by the components of the graph g .
123 , we say that the node is isolated .
suppose a graph bg dened on the set of vertices v admits =123 ( bv , be ) .
we say the vertex - the following decomposition into connected components : bg = bk
123 denotes absence of an edge and 123 denotes its presence .
g = ( v , e ) is a subgraph of g if v v and e e .
e ( l )
i j = ( 123 ifbq
123= 123 , i 123= j;
exact thresholding for graphical lasso
partitions induced by the connected components of g and bg are equal if bk = k and there is a permutation p on ( 123 , .
, k ) such that bvp ( ) = v for all ( 123 ,
the paper is organized as follows .
section 123 describes the covariance graph thresholding idea along with theoretical justication and related work , followed by complexity analysis of the algo - rithmic framework in section 123
numerical experiments appear in section 123 , concluding remarks in section 123 and the proofs are gathered in the appendix a .
methodology : exact thresholding of the covariance graph
( l ) to ( 123 ) gives rise to the symmetric edge matrix / skeleton
the sparsity pattern of the solution bq
( 123 , 123 ) pp dened by :
the above denes a symmetric graph g ( l ) = ( v , e ( l ) ) , namely the estimated concentration graph ( cox and wermuth , 123; lauritzen , 123 ) dened on the nodes v = ( 123 , .
, p ) with edges e ( l ) .
suppose the graph g ( l ) admits a decomposition into k ( l ) connected components :
g ( l ) =
k ( l ) =123 g ( l )
, e ( l )
= ( bv ( l )
) are the components of the graph g ( l ) .
note that k ( l ) ( 123 , .
, p ) , with
where g ( l ) k ( l ) = p ( large l ) implying that all nodes are isolated and for small enough values of l only one component , that is , k ( l ) = 123
on the entries of the sample covariance matrix s and obtain a graph edge skeleton e ( l ) ( 123 , 123 ) pp
we now describe the simple screening / thresholding rule .
given l
, we perform a thresholding
, there is
i j = ( cid : 123 ) 123 if |si j| > l
, i 123= j;
the symmetric matrix e ( l ) denes a symmetric graph on the nodes v = ( 123 , .
, p ) given by g ( l ) = ( v , e ( l ) ) .
we refer to this as the thresholded sample covariance graph .
similar to the decomposi - tion in ( 123 ) , the graph g ( l ) also admits a decomposition into connected components :
g ( l ) =
where g ( l )
= ( v ( l )
, e ( l )
) are the components of the graph g ( l ) .
note that the components of g ( l ) require knowledge ofbq
( l ) the solution to ( 123 ) .
construction of g ( l ) and its components require operating on san operation that can be performed completely independent of the optimization problem ( 123 ) , which is arguably more expensive ( see section 123 ) .
the surprising message we describe in this paper is that the vertex - partition of the connected components of ( 123 ) is exactly equal to that of ( 123 ) .
this observation has the following consequences :
we obtain a very interesting property of the path of solutions ( bq
) l 123the behavior of the connected components of the estimated concentration graph can be completely understood by simple screening rules on s .
mazumder and hastie
the cost of computing the connected components of the thresholded sample covariance graph ( 123 ) is orders of magnitude smaller than the cost of tting graphical models ( 123 ) .
furthermore , the computations pertaining to the covariance graph can be done off - line and are amenable to parallel computation ( see section 123 ) .
the optimization problem ( 123 ) completely separates into k ( l ) separate optimization sub - problems of the form ( 123 ) .
the sub - problems have size equal to the number of nodes in each component pi : = |vi| , i = 123 , .
, k ( l ) .
hence for certain values of l , solving prob - lem ( 123 ) becomes feasible although it may be impossible to operate on the p p dimensional ( global ) variable q on a single machine .
suppose that for l 123 , there are k ( l 123 ) components and the graphical model computations are distributed . 123 since the vertex - partitions induced via ( 123 ) and ( 123 ) are nested with increasing l ( see theorem 123 ) , it sufces to operate independently on these separate machines to obtain the
path of solutions ( bq
for all l l 123
consider a distributed computing architecture , where every machine allows operating on a graphical lasso problem ( 123 ) of maximal size pmax .
then with relatively small effort we can nd the smallest value of l = l pmax , such that there are no connected components of size larger than pmax .
problem ( 123 ) thus splits up independently into manageable problems across the different machines .
when this structure is not exploited the global problem ( 123 ) remains
the following theorem establishes the main technical contribution of this paperthe equivalence of the vertex - partitions induced by the connected components of the thresholded sample covariance graph and the estimated concentration graph .
theorem 123 for any l > 123 , the components of the estimated concentration graph g ( l ) , as dened in ( 123 ) and ( 123 ) induce exactly the same vertex - partition as that of the thresholded sample covari - ance graph g ( l ) , dened in ( 123 ) and ( 123 ) .
that is k ( l ) = k ( l ) and there exists a permutation p on ( 123 , .
, k ( l ) ) such that :
proof the proof of the theorem appears in appendix a . 123
i = v ( l )
p ( i ) , i = 123 , .
, k ( l ) .
bv ( l )
since the decomposition of a symmetric graph into its connected components depends upon the ordering / labeling of the components , the permutation p appears in theorem 123
remark 123 note that the edge - structures within each block need not be preserved .
under a match - ing reordering of the labels of the components of g ( l ) and g ( l ) :
= v ( l )
the edge - sets e ( l )
and e ( l )
are not necessarily equal .
for every xed such that bv ( l )
distributing these operations depend upon the number of processors available , their capacities , communication lag , the number of components and the maximal size of the blocks across all machines .
these of - course depend upon the computing environment .
in the context of the present problem , it is often desirable to club smaller components into a single machine .
exact thresholding for graphical lasso
theorem 123 leads to a special property of the path - of - solutions to ( 123 ) , that is , the vertex - partition .
this is the content of
induced by the connected components of g ( l ) are nested with increasing l the following theorem .
theorem 123 consider two values of the regularization parameter such that l > l > 123 , with corre - sponding concentration graphs g ( l ) and g ( l ) as in ( 123 ) and connected components ( 123 ) .
then the vertex - partition induced by the components of g ( l ) are nested within the partition induced by the components of g ( l ) 123k ( l ) forms a ner
) and the vertex - partition ( bv ( l )
proof the proof of this theorem appears in the appendix a . 123
formally , k ( l ) k ( l
resolution of ( bv ( l
remark 123 it is worth noting that theorem 123 addresses the nesting of the edges across connected components and not within a component .
in general , the edge - set e ( l ) of the estimated concentra - tion graph need not be nested as a function of l : for l > l
, in general , e ( l ) 123 e ( l
see friedman et al .
( 123 , figure 123 ) , for numerical examples demonstrating the non - monotonicity of the edge - set across l
, as described in remark 123
if l max j123=i|si j| , then the a simple consequence of theorem 123 is that of node - thresholding .
ith node will be isolated from the other nodes , the off - diagonal entries of the ith row / column are all zero , that is , max j123=i|bq i j | = 123
furthermore , the ith diagonal entries of the estimated covari - ance and precision matrices are given by ( sii + l ) and , respectively .
hence , as soon as l maxi=123 , . . . , p ( max j123=i|si j| ) , the estimated covariance and precision matrices obtained from ( 123 )
are both diagonal .
123 related work
witten et al .
( 123 ) independently discovered block screening as described in this paper .
at the time of our writing , an earlier version of their paper was available ( witten and friedman , 123 ) ; it proposed a scheme to detect isolated nodes for problem ( 123 ) via a simple screening of the entries of s , but no block screening .
earlier , banerjee et al .
( 123 , theorem 123 ) made the same observation about isolated nodes .
the revised manuscript ( witten et al . , 123 ) that includes block screening became available shortly after our paper was submitted for publication .
zhou et al .
( 123 ) use a thresholding strategy followed by re - tting for estimating gaussian graphical models .
their approach is based on the node - wise lasso - regression procedure of mein - shausen and buhlmann ( 123 ) .
a hard thresholding is performed on the 123 - penalized regression coefcient estimates at every node to obtain the graph structure .
a restricted mle for the concen - tration matrix is obtained for the graph .
the proposal in our paper differs since we are interested in solving the glasso problem ( 123 ) .
mazumder and hastie
computational complexity
the overall complexity of our proposal depends upon ( a ) the graph partition stage and ( b ) solving ( sub ) problems of the form ( 123 ) .
in addition to these , there is an unavoidable complexity associated with handling and / or forming s .
the cost of computing the connected components of the thresholded covariance graph is fairly negligible when compared to solving a similar sized graphical lasso problem ( 123 ) see also our sim - ulation studies in section 123
in case we observe samples xi p , i = 123 , .
, n the cost for creating the sample covariance matrix s is o ( n p123 ) .
thresholding the sample covariance matrix costs o ( p123 ) .
obtaining the connected components of the thresholded covariance graph costs o ( |e ( l ) | + p ) ( tar - jan , 123 ) .
since we are interested in a region where the thresholded covariance graph is sparse enough to be broken into smaller connected components|e ( l ) | p123
note that all computations pertaining to the construction of the connected components and the task of computing s can be computed off - line .
furthermore the computations are parallelizable .
gazit ( 123 , for example ) describes parallel algorithms for computing connected components of a graphthey have a time
complexity o ( log ( p ) ) and require o ( ( |e ( l ) | + p ) / log ( p ) ) processors with space o ( p +|e ( l ) | ) .
there are a wide variety of algorithms for the task of solving ( 123 ) .
while an exhaustive review of the computational complexities of the different algorithms is beyond the scope of this paper , we provide a brief summary for a few algorithms below .
banerjee et al .
( 123 ) proposed a smooth accelerated gradient based method ( nesterov , 123 ) ) to obtain an e accurate solutionthe per iteration cost being o ( p123 )
with complexity o ( p123 also proposed a block coordinate method which has a complexity of o ( p123 ) .
the complexity of the glasso algorithm ( friedman et al . , 123 ) which uses a row - by - row block coordinate method is roughly o ( p123 ) for reasonably sparse - problems with p nodes .
for denser problems the cost can be as large as o ( p123 ) .
the algorithm smacs proposed in lu ( 123 ) has a per iteration complexity of o ( p123 ) and an
overall complexity of o ( p123
e ) to obtain an e > 123 accurate solution .
it appears that most existing algorithms for ( 123 ) , have a complexity of at least o ( p123 ) to o ( p123 ) or possibly larger , depending upon the algorithm used and the desired accuracy of the solution making computations for ( 123 ) almost impractical for values of p much larger than 123
it is quite clear that the role played by covariance thresholding is indeed crucial in this context .
assume that we use a solver of complexity o ( pj ) with j ( 123 , 123 ) , along with our screening proce - dure .
suppose for a given l , the thresholded sample covariance graph has k ( l ) componentsthe total cost of solving these smaller problems is then ( cid : 123 ) |j ) o ( pj ) , with j ( 123 , 123 ) .
this difference in practice can be enormoussee section 123 for numerical examples .
this is what makes large scale graphical lasso problems solvable !
i=123 o ( |v ( l )
numerical examples
in this section we show via numerical experiments that the screening property helps in obtaining many fold speed - ups when compared to an algorithm that does not exploit it .
section 123 considers synthetic examples and section 123 discusses real - life microarray data - examples .
exact thresholding for graphical lasso
123 synthetic examples
experiments are performed with two publicly available algorithm implementations for the problem
glasso : the algorithm of friedman et al .
( 123 ) .
we used the matlab wrapper available at http : / / www - stat . stanford . edu / tibs / glasso / index . html to the fortran code .
the specic criterion for convergence ( lack of progress of the diagonal entries ) was set to 123 and the maximal number of iterations was set to 123
smacs : denotes the algorithm of lu ( 123 ) .
we used the matlab implementation smooth_covsel available at http : / / people . math . sfu . ca / zhaosong / codes / smooth_ covsel / .
the criterion for convergence ( based on duality gap ) was set to 123 and the max - imal number of iterations was set to 123
we will like to note that the convergence criteria of the two algorithms glasso and smacs are not the same .
for obtaining the connected components of a symmetric adjacency matrix we used the matlab function graphconncomp .
all of our computations are done in matlab 123 . 123 on a 123 ghz intel xeon processor .
the simulation examples are created as follows .
we generated a block diagonal matrix given by s = blkdiag ( s123 , .
, sk ) , where each block s = 123ppa matrix of all ones and ( cid : 123 ) p = p .
in the examples we took all ps to be equal to p123 ( say ) .
noise of the form s uu ( u is a p p matrix with i . i . d .
standard gaussian entries ) is added to s such that 123 times the largest ( in absolute value ) off block - diagonal ( as in the block structure of s ) entry of s uu equals the smallest absolute non - zero entry in s , that is , one .
the sample covariance matrix is s = s + s we consider a number of examples for varying k and p123 values , as shown in table 123
sizes were chosen such that it is at - least conceivable to solve ( 123 ) on the full dimensional problem , without screening .
in all the examples shown in table 123 , we set l i : = ( l max + l min ) / 123 , where for all values in the interval ( l min , l max ) the thresh - holded version of the sample covariance matrix has exactly k connected components .
we also took a larger value of l ii : = l max , which gave sparser estimates of the precision matrix but the number of connected components were the same .
, that is , l
the computations across different connected blocks could be distributed into as many machines .
this would lead to almost a k fold improvement in timings , however in table 123 we report the timings by operating serially across the blocks .
the serial loop across the different blocks are implemented
table 123 shows the rather remarkable improvements obtained by using our proposed covariance thresholding strategy as compared to operating on the whole matrix .
timing comparisons between glasso and smacs are not fair , since glasso is written in fortran and smacs in matlab .
however , we note that our experiments are meant to demonstrate how the thresholding helps in improving the overall computational time over the baseline method of not exploiting screening .
clearly our proposed strategy makes solving larger problems ( 123 ) , not only feasible but with quite attractive computational time .
the time taken by the graph - partitioning step in splitting the thresh - olded covariance graph into its connected components is negligible as compared to the timings for the optimization problem .
mazumder and hastie
algorithm timings ( sec )
p123 / p
123 / 123
table 123 : table showing ( a ) the times in seconds with screening , ( b ) without screening , that is , on the whole matrix and ( c ) the ratio ( b ) / ( a ) speedup factor for algorithms glasso and smacs .
algorithms with screening are operated seriallythe times reect the total time summed across all blocks .
the column graph partition lists the time for computing the connected components of the thresholded sample covariance graph .
since l former gives sparser models .
* denotes the algorithm did not converge within 123 iterations .
- refers to cases where the respective algorithms failed to converge within 123
ii > l
123 micro - array data examples
the graphical lasso is often used in learning connectivity networks in gene - microarray data ( fried - man et al . , 123 , see for example ) .
since in most real examples the number of genes p is around tens of thousands , obtaining an inverse covariance matrix by solving ( 123 ) is computationally imprac - tical .
the covariance thresholding method we propose easily applies to these problemsand as we
exact thresholding for graphical lasso
see gracefully delivers solutions over a large range of the parameter l .
we study three different micro - array examples and observe that as one varies l from large to small values , the thresholded covariance graph splits into a number of non - trivial connected components of varying sizes .
we continue till a small / moderate value of l when the maximal size of a connected component gets larger than a predened machine - capacity or the computational budget for a single graphical lasso problem .
note that in relevant micro - array applications , since p n ( n , the number of samples is at most a few hundred ) heavy regularization is required to control the variance of the covariance estimatesso it does seem reasonable to restrict to solutions of ( 123 ) for large values of l
following are the data - sets we used for our experiments :
( a ) this data - set appears in alon et al .
( 123 ) and has been analyzed by rothman et al .
( 123 , for example ) .
in this experiment , tissue samples were analyzed using an affymetrix oligonu - cleotide array .
the data were processed , ltered and reduced to a subset of p = 123 gene expression values .
the number of colon adenocarcinoma tissue samples is n = 123
( b ) this is an early example of an expression array , obtained from the patrick brown laboratory at stanford university .
there are n = 123 patient samples of tissue from various regions of the body ( some from tumors , some not ) , with gene - expression measurements for p = 123
( c ) the third example is the by now famous nki data set that produced the 123 - gene prognostic signature for breast cancer ( van - de - vijver et al . , 123 ) .
here there are n = 123 samples and p = 123 genes .
among the above , both ( b ) and ( c ) have few missing valueswhich we imputed by the respective global means of the observed expression values .
for each of the three data - sets , we took s to be the corresponding sample correlation matrix .
the exact thresholding methodolgy could have also been applied to the sample covariance matrix .
since it is a common practice to standardize the genes , we operate on the sample correlation matrix .
figure 123 shows how the component sizes of the thresholded covariance graph change across l
we describe the strategy we used to arrive at the gure .
note that the connected components change only at the absolute values of the entries of s .
from the sorted absolute values of the off - diagonal entries of s , we obtained the smallest value of l , say l min , for which the size of the maximal connected component was 123
for a grid of values of l min , we computed the connected components of the thresholded sample - covariance matrix and obtained the size - distribution of the various connected components .
figure 123 shows how these components change over a range of values for the three examples ( a ) , ( b ) and ( c ) .
the number of connected components of a particular size is denoted by a color - scheme , described by the color - bar in the gures .
with increasing l : the larger connected components gradually disappear as they decompose into smaller components; the sizes of the connected components decrease and the frequency of the smaller components increase .
since these are all correlation matrices , for l 123 all the nodes in the graph become isolated .
the range of l values for which the maximal size of the components is smaller than 123 differ across the three examples .
for ( c ) there is a greater variety in the sizes of the components as compared to ( a ) and ( b ) .
note that by theorem 123 , the pattern of the components appearing in figure 123 are exactly the same as the components appearing in the solution of ( 123 ) for that l
mazumder and hastie
log123 ( size of components )
log123 ( size of components )
log123 ( size of components )
figure 123 : figure showing the size distribution ( in the log - scale ) of connected components arising from the thresholded sample covariance graph for examples ( a ) - ( c ) .
for every value of l ( vertical axis ) , the horizontal slice denotes the sizes of the different components appearing in the thresholded covariance graph .
the colors represent the number of components in the graph having that specic size .
for every gure , the range of l values is chosen such that the maximal size of the connected components do not exceed 123
for examples ( b ) and ( c ) we found that the full problem sizes are beyond the scope of glasso and smacsthe screening rule is apparently the only way to obtain solutions for a reasonable range of l - values as shown in figure 123
in this paper we present a novel property characterizing the family of solutions to the graphical lasso problem ( 123 ) , as a function of the regularization parameter l .
the property is fairly surprisingthe vertex partition induced by the connected components of the non - zero pattern of the estimated concentration matrix ( at l ) and the thresholded sample covariance matrix s ( at l ) are exactly equal .
this property seems to have been unobserved in the literature .
our observation not only provides interesting insights into the properties of the graphical lasso solution - path but also opens the door to solving large - scale graphical lasso problems , which are otherwise intractable .
this simple rule when used as a wrapper around existing algorithms leads to enormous performance boostson occasions by a factor of thousands !
this work is supported by nsf - dms grant 123
exact thresholding for graphical lasso
appendix a .
proofs
here we provide proofs of theorems 123 and 123
a . 123 proof of theorem 123
( we suppress the superscript l
for notational convenience ) solves problem ( 123 ) ,
then standard kkt conditions of optimality ( boyd and vandenberghe , 123 ) give :
proof suppose bq
|si j cwi j| l cwi j = si j + l
i j = 123;
i j > 123; cwi j = si j l bq
i j < 123;
wherecw = ( bq ) 123
the diagonal entries satisfycwii = sii + l using ( 123 ) and ( 123 ) , there exists an ordering of the vertices ( 123 , .
, p ) of the graph such that e ( l ) is
block - diagonal .
for notational convenience , we will assume that the matrix is already in that order .
under this ordering of the vertices , the edge - matrix of the thresholded covariance graph is of the
, for i = 123 ,
e ( l ) =
, = 123 , .
, k ( l ) .
where the different components represent blocks of indices given by : v ( l )
we will construct a matrixcw having the same structure as ( 123 ) which is a solution to ( 123 ) .
note that ifcw is block diagonal then so is its inverse .
letcw and its inversebq be given by :
, bq = dene the block diagonal matricescw or equivalentlybq
via the following sub - problems
i j | ( q
) i j | )
123 cwk ( l )
) + tr ( sq
for = 123 , .
, k ( l ) , where s is a sub - block of s , with row / column indices from v ( l ) same notation is used for q
satises the kkt conditions ( 123 ) and ( 123 ) .
note that by construction of the thresholded sample covariance graph ,
denote the inverses of the block - precision matrices by ( bq and j v ( l )
with 123= , then |si j| l
we will show that the abovebq and j v ( l ) if i v ( l ) hence , for i v ( l )
v ( l ) i j =cwi j = 123 satises the kkt
with 123= ; the choice bq
for all the off - diagonal entries in the block - matrix ( 123 ) .
|si j cwi j| l
mazumder and hastie
satises the kkt conditions
the above argument shows that the connected components obtained from the estimated preci -
by construction ( 123 ) it is easy to see that for every , the matrixbq ( 123 ) and ( 123 ) corresponding to the th block of the p p dimensional problem .
hence bq sion graph g ( l ) leads to a partition of the vertices ( bv ( l ) there is a ( 123 , .
, k ( l ) ) such that bv ( l ) v ( l ) conversely , if bq for i bv ( l ) with 123= ; |si j cwi j| l proves that the connected components of g ( l ) leads to a partition of the vertices , which is ner than the vertex - partition induced by the components of g ( l ) .
in particular this implies that k ( l ) k ( l ) .
combining the above two we conclude k ( l ) = k ( l ) and also the equality ( 123 ) .
the permutation in the theorem appears since the labeling of the connected components is not unique .
) 123k ( l ) such that for every ( 123 , .
, k ( l ) ) , .
in particular k ( l ) k ( l ) .
sincecwi j = 123 , we have |si j| l
admits the decomposition as in the statement of the theorem , then it follows
from ( 123 ) that :
and j bv ( l )
a . 123 proof of theorem 123
proof this proof is a direct consequence of theorem 123 , which establishes that the vertex - partitions induced by the the connected components of the estimated precision graph and the thresholded sample covariance graph are equal .
observe that , by construction , the connected components of the thresholded sample covariance graph , that is , g ( l ) are nested within the connected components of g ( l ) .
in particular , the vertex - partition induced by the components of the thresholded sample covariance graph at l , is contained inside the vertex - partition induced by the components of the thresholded sample covariance graph .
now , using theorem 123 we conclude that the vertex - partition induced by the components of the estimated precision graph at l ) 123k ( l ) is contained inside the vertex - partition induced by the components of the estimated precision graph at l proof is thus complete .
, given by ( bv ( l )
, given by ( bv ( l
