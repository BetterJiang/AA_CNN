principal component analysis ( pca ) is widely used in data processing and dimensionality
reduction .
however , pca suers from the fact that each principal component is a linear combi -
nation of all the original variables , thus it is often dicult to interpret the results .
we introduce
a new method called sparse principal component analysis ( spca ) using the lasso ( elastic net )
to produce modied principal components with sparse loadings .
we show that pca can be
formulated as a regression - type optimization problem , then sparse loadings are obtained by im -
posing the lasso ( elastic net ) constraint on the regression coecients .
ecient algorithms are
proposed to realize spca for both regular multivariate data and gene expression arrays
also give a new formula to compute the total variance of modied principal components
illustrations , spca is applied to real and simulated data , and the results are encouraging .
keywords : multivariate analysis , gene expression arrays , elastic net , lasso , singular value de -
hui zou is a ph . d student in the department of statistics at stanford university , stanford , ca 123
email :
ytrevor hastie is professor , department of statistics and department of health research & policy , stanford
university , stanford , ca 123
email : hastie@stat . stanford . edu .
zrobert tibshirani is professor , department of health research & policy and department of statistics , stanford
university , stanford , ca 123
email : tibs@stat . stanford . edu .
principal component analysis ( pca ) ( jollie 123 ) is a popular data processing and dimension
reduction technique .
as an un - supervised learning method , pca has numerous applications such
as handwritten zip code classication ( hastie et al .
123 ) and human face recognition ( hancock
recently pca has been used in gene expression data analysis ( misra et al .
123 ) .
hastie et al .
( 123 ) propose the so - called gene shaving techniques using pca to cluster high
variable and coherent genes in microarray data .
pca seeks the linear combinations of the original variables such that the derived variables
capture maximal variance .
pca can be done via the singular value decomposition ( svd ) of the
in detail , let the data x be a n p matrix , where n and p are the number of observations and the number of variables , respectively .
without loss of generality , assume the
column means of x are all 123
suppose we have the svd of x as
x = udvt
where t means transpose .
u are the principal components ( pcs ) of unit length , and the columns
of v are the corresponding loadings of the principal components .
the variance of the ith pc is
in gene expression data the pcs u are called the eigen - arrays and v are the eigen - genes
( alter et al .
usually the rst q ( q p ) pcs are chosen to represent the data , thus a great dimensionality reduction is achieved .
the success of pca is due to the following two important optimal properties :
principal components sequentially capture the maximum variability among x , thus guaran -
teeing minimal information loss;
principal components are uncorrelated , so we can talk about one principal component without
referring to others .
however , pca also has an obvious drawback , i . e . , each pc is a linear combination of all p variables
and the loadings are typically nonzero .
this makes it often dicult to interpret the derived pcs .
rotation techniques are commonly used to help practitioners to interpret principal components
( jollie 123 ) .
vines ( 123 ) considered simple principal components by restricting the loadings to
take values from a small set of allowable integers such as 123 , 123 and - 123
we feel it is desirable not only to achieve the dimensionality reduction but also to reduce the
size of explicitly used variables .
an ad hoc way is to articially set the loadings with absolute
values smaller than a threshold to zero .
this informal thresholding approach is frequently used in
practice but can be potentially misleading in various respects ( cadima & jollie 123 ) .
mccabe
( 123 ) presented an alternative to pca which found a subset of principal variables .
jollie & uddin
( 123 ) introduced scotlass to get modied principal components with possible zero loadings .
recall the same interpretation issue arising in multiple linear regression , where the response is
predicted by a linear combination of the predictors .
interpretable models are obtained via variable
selection .
the lasso ( tibshirani 123 ) is a promising variable selection technique , simultaneously
producing accurate and sparse models .
zou & hastie ( 123 ) propose the elastic net , a generalization
of the lasso , to further improve upon the lasso .
in this paper we introduce a new approach to get
modied pcs with sparse loadings , which we call sparse principal component analysis ( spca ) .
spca is built on the fact that pca can be written as a regression - type optimization problem , thus
the lasso ( elastic net ) can be directly integrated into the regression criterion such that the resulting
modied pca produces sparse loadings .
in the next section we brie ( cid : 123 ) y review the lasso and the elastic net .
the method details of spca
are presented in section 123
we rst discuss a direct sparse approximation approach via the elas -
tic net , which is a useful exploratory tool .
we then show that nding the loadings of principal
components can be reformulated as estimating coecients in a regression - type optimization prob -
thus by imposing the lasso ( elastic net ) constraint on the coecients , we derive the modied
principal components with sparse loadings .
an ecient algorithm is proposed to realize spca .
we also give a new formula , which justies the correlation eects , to calculate the total variance
of modied principal components .
in section 123 we consider a special case of the spca algorithm
to eciently handle gene expression arrays .
the proposed methodology is illustrated by using real
data and simulation examples in section 123
discussions are in section 123
the paper ends up with
an appendix summarizing technical details .
123 the lasso and the elastic net
consider the linear regression model .
suppose the data set has n observations with p predictors .
let y = ( y123; : : : ; yn ) t be the response and xj = ( x123j; : : : ; xnj ) t ; i = 123; : : : ; p are the predictors .
after a location transformation we can assume all xj and y are centered .
the lasso is a penalized least squares method , imposing a constraint on the l123 norm of the
regression coecients .
thus the lasso estimates ^lasso are obtained by minimizing the lasso criterion
^lasso = arg min
where is a non - negative value .
the lasso was originally solved by quadratic programming
( tibshirani 123 ) .
efron et al .
( 123 ) proved that the lasso estimates as a function of are piece -
wise linear , and proposed an algorithm called lars to eciently solve the whole lasso solution
path in the same order of computations as a single least squares t .
the lasso continuously shrinks the coecients toward zero , thus gaining its prediction accuracy
via the bias variance trade - o .
moreover , due to the nature of the l123 penalty , some coecients
will be shrunk to exact zero if 123 is large enough .
therefore the lasso simultaneously produces an
accurate and sparse model , which makes it a favorable variable selection method .
however , the
lasso has several limitations as pointed out in zou & hastie ( 123 ) .
the most relevant one to this
work is that the number of selected variables by the lasso is limited by the number of observations .
for example , if applied to the microarray data where there are thousands of predictors ( genes )
( p > 123 ) with less than 123 samples ( n < 123 ) , the lasso can only select at most n genes , which
is clearly unsatisfactory .
the elastic net ( zou & hastie 123 ) generalizes the lasso to overcome its drawbacks , while
enjoying the similar optimal properties .
for any non - negative 123 and 123 , the elastic net estimates
^en are given as follows
^en = ( 123 + 123 ) arg min
jjj123 + 123
hence the elastic net penalty is a convex combination of ridge penalty and the lasso penalty .
obviously , the lasso is a special case of the elastic net with 123 = 123
given a xed 123 , the lars -
en algorithm ( zou & hastie 123 ) eciently solves the elastic net problem for all 123 with the
computation cost as a single least squares t .
when p > n , we choose some 123 > 123
then the
elastic net can potentially include all variables in the tted model , so the limitation of the lasso is
removed .
an additional benet oered by the elastic net is its grouping eect , that is , the elastic
net tends to select a group of highly correlated variables once one variable among them is selected .
in contrast , the lasso tends to select only one out of the grouped variables and does not care which
one is in the nal model .
zou & hastie ( 123 ) compare the elastic net with the lasso and discuss
the application of the elastic net as a gene selection method in microarray analysis .
123 motivation and method details
in both lasso and elastic net , the sparse coecients are a direct consequence of the l123 penalty ,
not depending on the squared error loss function .
jollie & uddin ( 123 ) proposed scotlass
by directly putting the l123 constraint in pca to get sparse loadings .
scotlass successively
maximizes the variance
k ( xt x ) ak
k ak = 123
and ( for k 123 ) at
h ak = 123;
h < k;
and the extra constraints
for some tuning parameter t .
although suciently small t yields some exact zero loadings , scot -
lass seems to lack of a guidance to choose an appropriate t value .
one might try several t values ,
but the high computational cost of scotlass makes it an impractical solution .
the high compu -
tational cost is due to the fact that scotlass is not a convex optimization problem .
moreover ,
the examples in jollie & uddin ( 123 ) show that the obtained loadings by scotlass are not
sparse enough when requiring a high percentage of explained variance .
we consider a dierent approach to modify pca , which can more directly make good use of
the lasso .
in light of the success of the lasso ( elastic net ) in regression , we state our strategy
we seek a regression optimization framework in which pca is done exactly .
in addition ,
the regression framework should allow a direct modication by using the lasso ( elastic
net ) penalty such that the derived loadings are sparse .
123 direct sparse approximations
we rst discuss a simple regression approach to pca .
observe that each pc is a linear combination
of the p variables , thus its loadings can be recovered by regressing the pc on the p variables .
theorem 123 123i , denote yi = uidi .
yi is the i - th principal component .
123 > 123 , suppose ^ridge is the ridge estimates given by
^ridge = arg min
jyi xj123 + jj123 :
let ^v =
, then ^v = vi :
the theme of this simple theorem is to show the connection between pca and a regression
method is possible .
regressing pcs on variables was discussed in cadima & jollie ( 123 ) , where
they focused on approximating pcs by a subset of k variables .
we extend it to a more general
ridge regression in order to handle all kinds of data , especially the gene expression data .
obviously
when n > p and x is a full rank matrix , the theorem does not require a positive .
note that if
p > n and = 123 , ordinary multiple regression has no unique solution that is exactly vi .
the same
story happens when n > p and x is not a full rank matrix .
however , pca always gives a unique
solution in all situations .
as shown in theorem 123 , this discrepancy is eliminated by the positive
ridge penalty ( jj123 ) .
note that after normalization the coecients are independent of , therefore the ridge penalty is not used to penalize the regression coecients but to ensure the reconstruction
of principal components .
hence we keep the ridge penalty term throughout this paper .
now let us add the l123 penalty to ( 123 ) and consider the following optimization problem
^ = arg min
jyi xj123 + jj123 + 123 jj123 :
we call ^vi =
an approximation to vi , and x ^vi the ith approximated principal component
is called naive elastic net ( zou & hastie 123 ) which diers from the elastic net by a scaling factor
( 123 + ) .
since we are using the normalized tted coecients , the scaling factor does not aect ^vi .
clearly , large enough 123 gives a sparse ^ , hence a sparse ^vi .
given a xed , ( 123 ) is eciently solved
for all 123 by using the lars - en algorithm ( zou & hastie 123 ) .
thus we can ( cid : 123 ) exibly choose a
sparse approximation to the ith principal component .
123 sparse principal components based on spca criterion
theorem 123 depends on the results of pca , so it is not a genuine alternative .
however , it can be
used in a two - stage exploratory analysis : rst perform pca , then use ( 123 ) to nd suitable sparse
we now present a \self - contained " regression - type criterion to derive pcs .
we rst consider
the leading principal component .
theorem 123 let xi denote the ith row vector of the matrix x .
for any > 123 , let
( ^; ^ ) = arg min
nxi=123xi t xi123
jj123 = 123 :
then ^ / v123 :
the next theorem extends theorem 123 to derive the whole sequence of pcs .
theorem 123 suppose we are considering the rst k principal components .
let and be p k matrices .
xi denote the i - th row vector of the matrix x .
for any > 123 , let
( ^; ^ ) = arg min
nxi=123xi t xi123
subject to t = ik :
then ^i / vi for i = 123; 123; : : : ; k .
theorem 123 eectively transforms the pca problem to a regression - type problem .
the critical el -
ement is the object functionpn i=123xi t xi123
i=123xi t xi123
if we restrict = , thenpn
i=123xi t xi123
, whose minimizer under the orthonormal constraint on is exactly the rst k
loading vectors of ordinary pca .
this is actually an alternative derivation of pca other than the
maximizing variance approach , e . g .
hastie et al .
( 123 ) .
theorem 123 shows that we can still have
exact pca while relaxing the restriction = and adding the ridge penalty term .
as can be seen
later , these generalizations enable us to ( cid : 123 ) exibly modify pca .
to obtain sparse loadings , we add the lasso penalty into the criterion ( 123 ) and consider the
following optimization problem
( ^; ^ ) = arg min
nxi=123xi t xi123
subject to t = ik :
whereas the same is used for all k components , dierent 123;js are allowed for penalizing the
loadings of dierent principal components .
again , if p > n , a positive is required in order to get
exact pca when the sparsity constraint ( the lasso penalty ) vanishes ( 123;j = 123 ) .
( 123 ) is called the
spca criterion hereafter .
123 numerical solution
we propose an alternatively minimization algorithm to minimize the spca criterion .
from the
proof of theorem 123 ( see appendix for details ) we get
i=123xi t xi123
= trxt x +pk
j=123 jjj123 +pk
j ( xt x + ) j 123t
j=123 123;j jjj123
j xt xj + 123;j jjj123 :
hence if given , it amounts to solve k independent elastic net problems to get ^j for j = 123; 123; : : : ; k .
on the other hand , we also have ( details in appendix )
i=123xi t xi123
j=123 jjj123 +pk = trxt x 123trt xt x + trt ( xt x + ) +pk
j=123 123;j jjj123
j=123 123;j jjj123 :
thus if is xed , we should maximize trt ( xt x ) subject to t = ik , whose solution is given
by the following theorem .
theorem 123 let and be m k matrices and has rank k .
consider the constrained maxi -
^ = arg max
subject to t = ik :
suppose the svd of is = u dv t , then ^ = u v t .
here are the steps of our numerical algorithm to derive the rst k sparse pcs .
general spca algorithm
let start at v ( ; 123 : k ) , the loadings of rst k ordinary principal components .
given xed , solve the following naive elastic net problem for j = 123; 123; : : : ; k
j = arg min
t ( xt x + ) 123t
j xt x + 123;j jj123 :
for each xed , do the svd of xt x = u dv t , then update = u v t .
repeat steps 123 - 123 , until converges .
normalization : ^vj = j
, j = 123; : : : ; k .
empirical evidence indicates that the outputs of the above algorithm vary slowly as changes .
for n > p data , the default choice of can be zero .
practically is a small positive number
to overcome potential collinearity problems of x .
section 123 discusses the default choice of
for the data with thousands of variables , such as gene expression arrays .
in principle , we can try several combinations of f123;jg to gure out a good choice of the tunning parameters , since the above algorithm converges quite fast .
there is a shortcut
provided by the direct sparse approximation ( 123 ) .
the lars - en algorithm eciently deliver
a whole sequence of sparse approximations for each pc and the corresponding values of 123;j .
hence we can pick a 123;j which gives a good compromise between variance and sparsity
this selection , variance has a higher priority than sparsity , thus we tend to be conservative in
both pca and spca depend on x only through xt x .
note that xt x
is actually the
sample covariance matrix of variables ( xi ) .
therefore if , the covariance matrix of ( xi ) , is
known , we can replace xt x with and have a population version of pca or spca .
if x is
standardized beforehand , then pca or spca uses the ( sample ) correlation matrix , which is
preferred when the scales of the variables are dierent .
123 adjusted total variance
the ordinary principal components are uncorrelated and their loadings are orthogonal .
let ^ =
xt x , then vt v = ik and vt ^v is diagonal .
it is easy to check that only the loadings of
ordinary principal components can satisfy both conditions .
in jollie & uddin ( 123 ) the loadings
were forced to be orthogonal , so the uncorrelated property was sacriced .
spca does not explicitly
impose the uncorrelated components condition too .
let ^u be the modied pcs .
usually the total variance explained by ^u is calculated by
trace ( ^ut ^u ) .
this is unquestionable when ^u are uncorrelated .
however , if they are correlated , the
computed total variance is too optimistic .
here we propose a new formula to compute the total
variance explained by ^u , which takes into account the correlations among ^u .
suppose ( ^ui; i = 123; 123; : : : ; k ) are the rst k modied pcs by any method .
denote ^uj123; : : : ;j123 the
reminder of ^uj after adjusting the eects of ^u123; : : : ; ^uj123 , that is
^uj123; : : : ;j123 = ^uj h123; : : : ;j123 ^uj;
where h123; : : : ;j123 is the projection matrix on ^ui i = 123; 123; : : : ; j 123
then the adjusted variance .
when the
, and the total explained variance is given by pk
of ^uj is ^uj123; : : : ;j123
modied pcs ^u are uncorrelated , then the new formula agrees with trace ( ^ut ^u ) .
note that the
above computations depend on the order of ^ui .
however , since we have a natural order in pca ,
ordering is not an issue here .
using the qr decomposition , we can easily compute the adjusted variance .
suppose ^u = qr ,
where q is orthonormal and r is upper triangular .
then it is straightforward to see that
hence the explained total variance is equal topk
123 computation complexity
pca is computationally ecient for both n > p or p ( cid : 123 ) n data .
we separately discuss the computational cost of the general spca algorithm for n > p and p ( cid : 123 ) n .
traditional multivariate data t in this category .
note that although the spca
criterion is dened using x , it only depends on x via xt x .
a trick is to rst compute the
p p matrix ^ = xt x once for all , which requires np123 operations .
then the same ^ is used at each step within the loop .
computing xt x costs p123k and the svd of xt x is of
order o ( pk123 ) .
each elastic net solution requires at most o ( p123 ) operations .
since k p , the total computation cost is at most np123 + mo ( p123 ) , where m is the number of iterations before
convergence .
therefore the spca algorithm is able to eciently handle data with huge n , as
long as p is small ( say p < 123 ) .
p ( cid : 123 ) n .
gene expression arrays are typical examples of this p ( cid : 123 ) n category .
the trick of ^ is no longer applicable , because ^ is a huge matrix ( p p ) in this case .
the most consuming step is solving each elastic net , whose cost is of order o ( pj 123 ) for a positive nite , where j is
the number of nonzero coecients .
generally speaking the total cost is of order mo ( pj 123k ) ,
which is expensive for a large j .
fortunately , as shown in section 123 , there exits a special
spca algorithm for eciently dealing with p ( cid : 123 ) n data .
123 spca for p ( cid : 123 ) n and gene expression arrays
gene expression arrays are a new type of data where the number of variables ( genes ) are much
bigger than the number of samples .
our general spca algorithm still ts this situation using a
positive .
however the computation cost is expensive when requiring a large number of nonzero
loadings .
it is desirable to simplify the general spca algorithm to boost the computation .
observe that theorem 123 is valid for all > 123 , so in principle we can use any positive .
it turns
out that a thrifty solution emerges if ! 123
precisely , we have the following theorem .
theorem 123 let ^vi ( ) =
be the loadings derived from criterion ( 123 ) .
dene ( ^; ^ ) as the
solution of the optimization problem
( ^; ^ ) = arg min
; 123trt xt x +
subject to t = ik :
when ! 123 , ^vi ( ) !
j ^i j
by the same statements in section 123 , criterion ( 123 ) is solved by the following algorithm , which
is a special case of the general spca algorithm with = 123 :
gene expression arrays spca algorithm
replacing step 123 in the general spca algorithm with
step 123 : given xed , for j = 123; 123; : : : ; k
j xt x
j xt x ) :
the operation in ( 123 ) is called soft - thresholding .
figure 123 gives an illustration of how the
soft - thresholding rule operates .
recently soft - thresholding has become increasingly popular in
figure 123 : an illstration of soft - thresholding rule y = ( jxj ) +sign ( x ) with = 123
the literature .
for example , nearest shrunken centroids ( tibshirani et al .
123 ) adopts the soft -
thresholding rule to simultaneously classify samples and select important genes in microarrays .
123 pitprops data
the pitprops data rst introduced in jeers ( 123 ) has 123 observations and 123 measured variables .
it is the classic example showing the diculty of interpreting principal components .
jeers ( 123 )
tried to interpret the rst 123 pcs .
jollie & uddin ( 123 ) used their scotlass to nd the modied
table 123 presents the results of pca , while table 123 presents the modied pcs loadings by
scotlass and the adjusted variance computed using ( 123 ) .
as a demonstration , we also considered the rst 123 principal components .
since this is a usual
n ( cid : 123 ) p data set , we set = 123
123 = ( 123 : 123; 123 : 123; 123 : 123; 123 : 123; 123 : 123; 123 : 123 ) were chosen according to figure 123 such that each sparse approximation explained almost the same amount of variance as the ordinary
pc did .
table 123 shows the obtained sparse loadings and the corresponding adjusted variance .
compared with the modied pcs by scotlass , pcs by spca account for nearly the same
amount of variance ( 123% vs .
123% ) but with a much sparser loading structure .
the important
variables associated with the 123 pcs do not overlap , which further makes the interpretations easier
and clearer .
it is interesting to note that in table 123 even though the variance does not strictly
monotonously decrease , the adjusted variance follows the right order .
however , table 123 shows this
is not true in scotlass .
it is also worthy to mention that the whole computation of spca was
done in seconds in r , while the implementation of scotlass for each t was expensive ( jollie &
uddin 123 ) .
optimizing scotlass over several values of t is even a more dicult computational
although the informal thresholding method , which is referred to as simple thresholding hence -
forth , has various drawbacks , it may serve as the benchmark for testing sparse pcs methods
variant of simple thresholding is soft - thresholding .
we found that used in pca , soft - thresholding
performs very similarly to simple thresholding .
thus we omitted the results of soft - thresholding
in this paper .
both scotlass and spca were compared with simple thresholding .
table 123
presents the loadings and the corresponding explained variance by simple thresholding .
to make
fair comparisons , we let the numbers of nonzero loadings by simple thresholding match the results
of scotlass and spca .
in terms of variance , it seems that simple thresholding is better than
scotlass and worse than spca .
moreover , the variables with non - zero loadings by spca are
very dierent to that chosen by simple thresholding for the rst three pcs; while scotlass seems
to create a similar sparseness pattern as simple thresholding does , especially in the leading pc .
figure 123 : pitprops data : the sequences of sparse approximations to the rst 123 principal components .
plots show the percentage of explained variance ( pev ) as a function of 123
table 123 : pitprops data : loadings of the rst 123 principal components
cumulative variance ( % )
table 123 : pitprops data : loadings of the rst 123 modied pcs by scotlass
t = 123 : 123 number of nonzero loadings adjusted variance ( % ) cumulative adjusted variance ( % )
table 123 : pitprops data : loadings of the rst 123 sparse pcs by spca
pc123 pc123 pc123
number of nonzero loadings adjusted variance ( % ) cumulative adjusted variance ( % )
123 a simulation example
we rst created three hidden factors
v123 n ( 123; 123 ) ;
v123 n ( 123; 123 )
v123 = 123 : 123v123 + 123 : 123v123 + ;
n ( 123; 123 )
v123; v123 and
then 123 observed variables were generated as the follows
xi = v123 + 123
xi = v123 + 123
xi = v123 + 123
i n ( 123; 123 ) ;
i n ( 123; 123 ) ;
i n ( 123; 123 ) ;
i = 123; 123; 123; 123;
i = 123; 123; 123; 123;
i = 123; 123;
ig are independent;
j = 123; 123; 123
i = 123; ; 123 :
table 123 : pitprops data : loadings of the rst 123 modied pcs by simple thresholding
number of nonzero loadings adjusted variance ( % ) cumulative adjusted variance ( % ) number of nonzero loadings adjusted variance ( % ) cumulative adjusted variance ( % )
to avoid the simulation randomness , we used the exact covariance matrix of ( x123; : : : ; x123 ) to
perform pca , spca and simple thresholding .
in other words , we compared their performances
using an innity amount of data generated from the above model .
the variance of the three underlying factors is 123 , 123 and 123 , respectively .
the numbers of
variables associated with the three factors are 123 , 123 and 123
therefore v123 and v123 are almost equally
important , and they are much more important than v123
the rst two pcs together explain 123 : 123%
of the total variance .
these facts suggest that we only need to consider two derived variables with
right sparse representations .
ideally , the rst derived variable should recover the factor v123 only
using ( x123; x123; x123; x123 ) , and the second derived variable should recover the factor v123 only using
( x123; x123; x123; x123 ) .
in fact , if we sequentially maximize the variance of the rst two derived variables
under the orthonormal constraint , while restricting the numbers of nonzero loadings to four , then
the rst derived variable uniformly assigns nonzero loadings on ( x123; x123; x123; x123 ) ; and the second
derived variable uniformly assigns nonzero loadings on ( x123; x123; x123; x123 ) .
both spca ( = 123 ) and simple thresholding were carried out by using the oracle information
that the ideal sparse representations use only four variables .
table 123 summarizes the comparison
results .
clearly , spca correctly identies the sets of important variables .
as a matter of fact ,
spca delivers the ideal sparse representations of the rst two principal components .
mathemat -
ically , it is easy to show that if t = 123 is used , scotlass is also able to nd the same sparse
solution .
in this example , both spca and scotlass produce the ideal sparse pcs , which may
be explained by the fact that both methods explicitly use the lasso penalty .
in contrast , simple thresholding wrongly includes x123; x123 in the most important variables
explained variance by simple thresholding is also lower than that by spca , although the relative
dierence is small ( less than 123% ) .
due to the high correlation between v123 and v123 , variables x123; x123
gain loadings which are even higher than that of the true important varaibles ( x123; x123; x123; x123 )
table 123 : results of the simulation example : loadings and variance
spca ( = 123 ) simple thresholding
the truth is disguised by the high correlation .
on the other hand , simple thresholding correctly
discovers the second factor , because v123 has a low correlation with v123
123 ramaswamy data
ramaswamy data ( ramaswamy et al .
123 ) has 123 ( p = 123 ) genes and 123 ( n = 123 ) samples .
its rst principal component explains 123% of the total variance .
in a typical microarray data like
this , it appears that scotlass cannot be practically useful .
we applied spca ( = 123 ) to nd the sparse leading pc .
a sequence of 123 were used such that the number of nonzero loadings varied
in a rather wide range .
as displayed in figure 123 , the percentage of explained variance decreases at
a slow rate , as the sparsity increase .
as few as 123% of these 123 genes can suciently construct
the leading principal component with little loss of explained variance ( from 123% to 123% ) .
simple
thresholding was also applied to this data .
it seems that when using the same number of genes ,
simple thresholding always explains slightly higher variance than spca does .
among the same
number of selected genes by spca and simple thresholding , there are about 123% dierent genes ,
and this dierence rate is quite consistent .
number of nonzero loadings
figure 123 : the sparse leading principal component : percentage of explained variance versus sparsity .
simple thresholding and spca have similar performances .
however , there still exists consistent dierence in the selected genes ( the ones with nonzero loadings ) .
it has been a long standing interest to have a formal approach to derive principal components with
sparse loadings .
from a practical point of view , a good method to achieve the sparseness goal
should ( at least ) possess the following properties .
without any sparsity constraint , the method should reduce to pca .
it should be computationally ecient for both small p and big p data .
it should avoid mis - identifying the important variables .
the frequently used simple thresholding is not criterion based .
however , this informal ad hoc
method seems to have the rst two of the good properties listed above .
if the explained variance
and sparsity are the only concerns , simple thresholding is not such a bad choice , and it is extremely
convenient .
we have shown that simple thresholding can work pretty well in gene expression
arrays .
the serious problem with simple thresholding is that it can mis - identify the real important
variables .
nevertheless , simple thresholding is regarded as a benchmark for any potentially better
using the lasso constraint in pca , scotlass successfully derives sparse loadings .
however ,
scotlass is not computationally ecient , and it lacks a good rule to pick its tunning parameter .
in addition , it is not feasible to apply scotlass to gene expression arrays , while in which pca
is a quite popular tool .
in this work we have developed spca using the spca criterion .
the new spca criterion
gives exact pca results when its sparsity ( lasso ) penalty term vanishes .
spca allows a quite
( cid : 123 ) exible control on the sparse structure of the resulting loadings .
unied ecient algorithms have
been proposed to realize spca for both regular multivariate data and gene expression arrays .
as a principled procedure , spca enjoys advantages in several aspects , including computational
eciency , high explained variance and ability of identifying important variables .
123 appendix : proofs
theorem 123 proof : using xt x = vd123vt and vt v = i , we have
^ridge = xt x + i123
= v ( cid : 123 ) d123
d123 + i vt vi
theorem 123 proof : note that
nxi=123xi t xi123
i ( i t ) ( i t ) xi i=123 tr ( i t ) ( i t ) xixt tr ( i t ) ( i t ) ( pn
= tr ( i t t + t t ) xt x = trxt x + trt xt x 123trt xt x :
since t xt x and t xt x are both scalars , we get
i=123xi t xi123
= trxt x 123t xt x + t ( xt x + ) :
for a xed , the above quantity is minimized at
=xt x + 123
substituting ( 123 ) into ( 123 ) gives
i=123xi t xi123
= trxt x 123t xt x ( xt x + ) 123xt x :
^ = arg max
t xt x ( xt x + ) 123xt x
subject to t = 123 :
and ^ =xt x + 123
by x = udvt , we have
xt x ( xt x + ) 123xt x = v
hence ^ = sv123 with s=123 or - 123
then ^ = s d123
theorem 123 proof : by the same steps in the proof of theorem 123 we derive ( 123 ) as long as t = ik .
hence we have
i=123xi t xi123 trxt x 123trt xt x + trt ( xt x + ) j xt xj = trxt x +pk
j ( xt x + ) j 123t
thus given a xed , the above quantity is minimized at j = xt x + 123
j = 123; 123; : : : ; k; or equivalently
xt xj for
=xt x + 123
^ = arg max
trt xt x ( xt x + ) 123xt x
subject to t = ik :
this is an eigen - analysis problem whose solution is ^j = sjvj with sj=123 or - 123 for j = 123; 123; : : : ; k ,
because the eigenvectors of xt x ( xt x + ) 123xt x are v .
hence ( 123 ) gives ^j = sj
j = 123; 123; : : : ; k .
j + vj for
theorem 123 proof : by assumption = u dv t with u t u = ik and v v t = v t v = ik
constraint t = ik is equivalent to k ( k+123 )
i i = 123;
i = 123; 123 : : : ; k
i j = 123;
j > i :
using lagrangian multipliers method , we dene
i i 123 ) +
= 123 gives i = i;i ^i + i;j ^j; or in a matrix form = ^ , where i;j = j;i
and are full rank , so is invertible and = 123
we have
tr^t = tr123t = tr123;t v d123v t ;
ik = ^t ^ = 123;t t 123 = 123;t v d123v t 123 :
let a = v t 123v , observe
tr123v d123v t = trv t 123v d123 = trat d123 =
at d123a = ik :
the \= " is taken if only if a is diagonal and ajj = d123
therefore 123 = v av t = v d123v t ,
and ^ = = u dv t v d123v t = u v t :
theorem 123 proof : let ^ = ( 123 + ) ^ , then we observe ^vi ( ) =
: on the other hand , ^ =
j ^i j
( ^; ^ ) = arg min
subject to t = ik :
then by ( 123 ) , we have
j=123 123;j j j xt xj + 123;j jjj123 123+ j 123t 123+ j + 123;j jjj123 123trt xt x :
= trxt x + 123
= trxt x + 123
( ^; ^ ) = arg min
; 123trt xt x +
subject to t = ik :
xt x +
as ! 123 , ( 123 ) approaches ( 123 ) .
thus the conclusion follows .
