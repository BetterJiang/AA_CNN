sentiment classication is the task of labeling a re - view document according to the polarity of its pre - vailing opinion ( favorable or unfavorable ) .
proaching this problem , a model builder often has three sources of information available : a small col - lection of labeled documents , a large collection of unlabeled documents , and human understanding of language .
ideally , a learning method will utilize all three sources .
to accomplish this goal , we general - ize an existing procedure that uses the latter two .
we extend this procedure by re - interpreting it as a naive bayes model for document sentiment .
viewed as such , it can also be seen to extract a pair of derived features that are linearly combined to predict sentiment .
this perspective allows us to improve upon previous methods , primarily through two strategies : incorporating additional derived fea - tures into the model and , where possible , using la - beled data to estimate their relative inuence .
text documents are available in ever - increasing numbers , making automated techniques for infor - mation extraction increasingly useful .
traditionally , most research effort has been directed towards ob - jective information , such as classication accord - ing to topic; however , interest is growing in produc - ing information about the opinions that a document contains; for instance , morinaga et al .
( 123 ) .
in march , 123 , the american association for arti - cial intelligence held a symposium in this area , en - titled exploring affect and attitude in text .
one task in opinion extraction is to label a re - view document d according to its prevailing senti - ment s 123 f ( cid : 123 ) 123; 123g ( unfavorable or favorable ) .
sev - eral previous papers have addressed this problem by building models that rely exclusively upon la - beled documents , e . g .
pang et al .
( 123 ) .
by learning models from labeled data , one can apply familiar , powerful techniques directly; however , in practice it may be difcult to
obtain enough labeled reviews to learn model pa -
a contrasting approach ( turney , 123 ) relies only upon documents whose labels are unknown .
this makes it possible to use a large underlying corpus in this case , the entire internet as seen through the altavista search engine .
as a result , estimates for model parameters are subject to a relatively small amount of random variation .
the corresponding drawback to such an approach is that its predictions are not validated on actual documents .
in machine learning ,
it has often been effec - tive to use labeled and unlabeled examples in tan - dem , e . g .
nigam et al .
( 123 ) .
turneys model introduces the further consideration of incorporat - ing human - provided knowledge about language .
in this paper we build models that utilize all three sources : labeled documents , unlabeled documents , and human - provided information .
the basic concept behind turneys model is quite the sentiment orientation ( hatzivas - siloglou and mckeown , 123 ) of a pair of words is taken to be known .
these words serve as an - chors for positive and negative sentiment .
words that co - occur more frequently with one anchor than the other are themselves taken to be predictive of sentiment .
as a result , information about a pair of words is generalized to many words , and then to
in the following section , we relate this model with naive bayes classication , showing that tur - neys classier is a pseudo - supervised approach : it effectively generates a new corpus of labeled doc - uments , upon which it ts a naive bayes classier .
this insight allows the procedure to be represented as a probability model that is linear on the logistic scale , which in turn suggests generalizations that are developed in subsequent sections .
123 a logistic model for sentiment 123 turneys sentiment classier in turneys model , the sentiment orientation ( cid : 123 ) of word w is estimated as follows .
123 naive bayes classication bayes theorem provides a convenient framework for predicting a binary response s 123 f ( cid : 123 ) 123; 123g from a feature vector x :
^ ( cid : 123 ) ( w ) = log
pr ( s = 123jx ) =
here , na is the total number of sites on the internet that contain an occurrence of a a feature that can be a word type or a phrase .
n ( w;a ) is the number of sites in which features w and a appear near each other , i . e .
in the same passage of text , within a span of ten words .
both numbers are obtained from the hit count that results from a query of the altavista search engine .
the rationale for this estimate is that words that express similar sentiment often co - occur , while words that express conicting sentiment co - occur more rarely .
thus , a word that co - occurs more frequently with excellent than poor is estimated to have a positive sentiment orientation .
to extrapolate from words to documents , the esti - mated sentiment ^s 123 f ( cid : 123 ) 123; 123g of a review document d is the sign of the average sentiment orientation of its constituent features . 123 to represent this estimate formally , we introduce the following notation : w is a dictionary of features : ( w123; : : : ; wp ) .
each features respective sentiment orientation is repre - sented as an entry in the vector ^ ( cid : 123 ) of length p :
^ ( cid : 123 ) j = ^ ( cid : 123 ) ( wj )
given a collection of n review documents , the i - th each di is also represented as a vector of length p , with dij equal to the number of times that feature wj occurs in di .
the length of a document is its total
turneys classier for the i - th documents senti -
ment si can now be written :
number of features , jdij =pp ^si = sign pp
using a carefully chosen collection of features , this classier produces correct results on 123% of a collection of 123 movie reviews , where 123 are labeled positive and 123 negative .
although this is not a particularly encouraging result , movie reviews tend to be a difcult domain .
accuracy on senti - ment classication in other domains exceeds 123%
123note that not all words or phrases need to be considered as features .
in turney ( 123 ) , features are selected according to
pr ( xjs = 123 ) ( cid : 123 ) 123
pk123f ( cid : 123 ) 123;123g pr ( xjs = k ) ( cid : 123 ) k
for a labeled sample of data ( xi; si ) ; i = 123; : : : ; n , a classs marginal probability ( cid : 123 ) k can be estimated trivially as the proportion of training samples be - longing to the class .
thus the critical aspect of clas - sication by bayes theorem is to estimate the con - ditional distribution of x given s .
naive bayes sim - plies this problem by making a naive assump - tion : within a class , the different feature values are taken to be independent of one another .
as a result , the estimation problem is reduced to
( cid : 123 ) naive bayes for a multinomial distribution
we consider a bag of words model for a docu - ment that belongs to class k , where features are as - sumed to result from a sequence of jdij independent multinomial draws with outcome probability vector qk = ( qk123; : : : ; qkp ) .
given a collection of documents with labels , ( di; si ) ; i = 123; : : : ; n , a natural estimate for qkj is the fraction of all features in documents of class k that equal wj :
^qkj = pi : si=k dij
in the two - class case , the logit transformation provides a revealing representation of the class pos - terior probabilities of the naive bayes model .
dlogit ( sjd ) , log cpr ( s = 123jd ) cpr ( s = ( cid : 123 ) 123jd )
= ^ ( cid : 123 ) 123 +
where ^ ( cid : 123 ) 123 = log
^ ( cid : 123 ) j = log
observe that the estimate for the logit in equation 123 has a simple structure : it is a linear function of d .
models that take this form are commonplace in 123 turneys classier as naive bayes although naive bayes classication requires a la - beled corpus of documents , we show in this sec - tion that turneys approach corresponds to a naive bayes model .
the necessary documents and their corresponding labels are built from the spans of text that surround the anchor words excellent and poor .
more formally , a labeled corpus may be produced
by the following procedure :
for a particular anchor ak , locate all of the sites
on the internet where it occurs .
from all of the pages within a site , gather the features that occur within ten words of an oc - currence of ak , with any particular feature in - cluded at most once .
this list comprises a new document , representing that site . 123
label this document +123 if ak = excellent , - 123
if ak = poor .
when a naive bayes model is t to the corpus described above , it results in a vector ^ ( cid : 123 ) of length p , consisting of coefcient estimates for all fea - tures .
in propositions 123 and 123 below , we show that turneys estimates of sentiment orientation ^ ( cid : 123 ) are closely related to ^ ( cid : 123 ) , and that both estimates produce
proposition 123 turneys classier is identical to a naive bayes classier t on this corpus , with ( cid : 123 ) 123 = ( cid : 123 ) ( cid : 123 ) 123 = 123 : 123
proof : a naive bayes classier typically assigns an observation to its most probable class .
this is equiv - alent to classifying according to the sign of the es - timated logit .
so for any document , we must show that both the logit estimate and the average senti - ment orientation are identical in sign .
when ( cid : 123 ) 123 = 123 : 123 , ( cid : 123 ) 123 = 123
thus the estimated logit
this is a positive multiple of turneys classier ( equation 123 ) , so they clearly match in sign .
123 a more versatile model 123 desired extensions by understanding turneys model within a naive bayes framework , we are able to interpret its out - put as a probability model for document classes .
in the presence of labeled examples , this insight also makes it possible to estimate the intercept term ( cid : 123 ) 123
further , we are able to view this model as a mem - ber of a broad class : linear estimates for the logit .
this understanding facilitates further extensions , in particular , utilizing the following :
^ ( cid : 123 ) = c123 ^ ( cid : 123 )
where c123 =
labeled documents
more anchor words
proof : because a feature is restricted to at most one occurrence in a document ,
dij = n ( w;ak )
then from equations 123 and 123 :
^ ( cid : 123 ) j = log
123if both anchors occur on a site , then there will actually be
two documents , one for each sentiment
the reason for using labeled documents is straightforward; labels offer validation for any cho - sen model .
using additional anchors is desirable in part because it is inexpensive to produce lists of words that are believed to reect positive sentiment , perhaps by reference to a thesaurus .
in addition , a single anchor may be at once too general and too
an anchor may be too general in the sense that many common words have multiple meanings , and not all of them reect a chosen sentiment orien - tation .
for example , poor can refer to an objec - tive economic state that does not necessarily express negative sentiment .
as a result , a word such as income appears 123 times as frequently with poor as excellent , even though it does not convey nega - tive sentiment .
similarly , excellent has a technical
meaning in antiquity trading , which causes it to ap - pear 123 times as frequently with f urniture .
an anchor may also be too specic , in the sense that there are a variety of different ways to express sentiment , and a single anchor may not capture them all .
so a word like pretentious carries a strong negative sentiment but co - occurs only slightly more frequently ( 123 times ) with excellent than poor .
likewise , f ascination generally reects a positive sentiment , yet it appears slightly more frequently ( 123 times ) with poor than excellent .
123 other sources of unlabeled data the use of additional anchors has a drawback in terms of being resource - intensive .
a feature set may contain many words and phrases , and each of them requires a separate altavista query for every chosen anchor word .
in the case of 123 , 123 features and ten queries per minute , downloads for a single anchor word require over two days of data collection .
an alternative approach is to access a large collection of documents directly .
then all co - occurrences can be counted in a single pass .
although this approach dramatically reduces the amount of data available , it does offer several ad -
( cid : 123 ) increased query options search engine queries of the form phrase near anchor may not produce all of the desired co - occurrence counts .
for instance , one may wish to run queries that use stemmed words , hy - phenated words , or punctuation marks .
one may also wish to modify the denition of near , or to count individual co - occurrences , rather than counting sites that contain at least
( cid : 123 ) topic matching across the internet as a whole , features may not exhibit the same cor - relation structure as they do within a specic domain .
by restricting attention to documents within a domain , one may hope to avoid co - occurrences that are primarily relevant to other
( cid : 123 ) reproducibility on a xed corpus , counts of word occurrences produce consistent results .
due to the dynamic nature of the internet , numbers may uctuate .
123 co - occurrences and derived features the naive bayes coefcient estimate ^ ( cid : 123 ) j may itself be interpreted as an intercept term plus a linear com - bination of features of the form log n ( wj ;ak ) .
of labeled occurrences correlation
123 - 123 123 - 123 123 - 123 123 - 123 123 - 123 123 - 123
figure 123 : correlation between supervised and un - supervised coefcient estimates
^ ( cid : 123 ) j = log
= log c123 + log n ( j;exc : ) ( cid : 123 ) log n ( j;pr : )
we generalize this estimate as follows : for a col - lection of k different anchor words , we consider a general linear combination of logged co - occurrence
( cid : 123 ) k log n ( wj ;ak )
in the special case of a naive bayes model , ( cid : 123 ) k = 123 when the k - th anchor word ak conveys positive sentiment , ( cid : 123 ) 123 when it conveys negative sentiment .
replacing the logit estimate in equation 123 with
an estimate of this form , the model becomes :
dlogit ( sjd ) = ^ ( cid : 123 ) 123 +
= ^ ( cid : 123 ) 123 +
= ( cid : 123 ) 123 +
dj ( cid : 123 ) k log n ( wj ;ak )
dj log n ( wj ;ak )
this model has only k + 123 parameters : ( cid : 123 ) 123; ( cid : 123 ) 123; : : : ; ( cid : 123 ) k .
these can be learned straightfor - wardly from labeled documents by a method such as logistic regression .
observe that a document receives a score for each j=123 dj log n ( wj ;ak ) .
effectively , the predictor variables in this model are no longer counts of the original features dj .
rather , they are
unsupervised vs .
supervised coefficients
traditional naive bayes coefs .
figure 123 : unsupervised versus supervised coef -
inner products between the entire feature vector d and the logged co - occurence vector n ( w;ak ) .
in this respect , the vector of logged co - occurrences is used to produce derived feature .
123 data analysis 123 accuracy of unsupervised coefcients that uses the lynx by means of a perl script browser , version 123 . 123rel . 123 , we download altavista hit counts for queries of the form target near anchor .
the initial list of targets consists of 123 , 123 word types extracted from the pang cor - pus of 123 labeled movie reviews .
after pre - processing , this number is reduced to 123 , 123
in figure 123 , we compare estimates produced by two naive bayes procedures .
for each feature wj , we estimate ( cid : 123 ) j by using turneys procedure , and by tting a traditional naive bayes model to the labeled documents .
the traditional estimates are smoothed by assuming a beta prior distribution that is equivalent to having four previous observations of wj in documents of each class .
where c123 =
123 +pi : si=123 dij 123 +pi : si= ( cid : 123 ) 123 dij 123p +pi : si=123 jdij 123p +pi : si= ( cid : 123 ) 123 jdij
here , dij is used to indicate feature presence :
dij = ( cid : 123 ) 123 if wj appears in di
123we eliminate extremely rare words by requiring each target to co - occur at least once with each anchor .
in addition , certain types , such as words containing hyphens , apostrophes , or other punctuation marks , do not appear to produce valid counts , so they are discarded .
figure 123 : selected anchor words
we choose this tting procedure among several can - didates because it performs well in classifying test
in figure 123 , each entry in the right - hand col - umn is the observed correlation between these two estimates over a subset of features .
for features that occur in ve documents or fewer , the corre - lation is very weak ( 123 ) .
this is not surpris - ing , as it is difcult to estimate a coefcient from such a small number of labeled examples .
corre - lations are stronger for more common features , but never strong .
as a baseline for comparison , naive bayes coefcients can be estimated using a subset of their labeled occurrences .
with two independent sets of 123 - 123 occurrences , naive bayes coefcient estimates had a correlation of 123 .
figure 123 is a scatterplot of the same coefcient estimates for word types that appear in 123 to 123 documents .
the great majority of features do not have large coefcients , but even for the ones that do , there is not a tight correlation .
123 additional anchors we wish to learn how our model performance de - pends on the choice and number of anchor words .
selecting from wordnet synonym lists ( fellbaum , 123 ) , we choose ve positive anchor words and ve negative ( figure 123 ) .
this produces a total of 123 different possible pairs for use in producing co -
figure 123 shows the classication performance of unsupervised procedures using the 123 labeled pang documents as test data .
coefcients ^ ( cid : 123 ) j are es - timated as described in equation 123
several differ - ent experimental conditions are applied .
the meth - ods labeled count use the original un - normalized coefcients , while those labeled norm .
have been normalized so that the number of co - occurrences with each anchor have identical variance .
results are shown when rare words ( with three or fewer oc - currences in the labeled corpus ) are included and omitted .
the methods pair and 123 describe whether all ten anchor coefcients are used at once , or just the ones that correspond to a single pair of
count pair >123 norm .
pair >123
misclass .
st . dev
misclassification versus sample size
figure 123 : classication error rates for different
of labeled documents
anchor words .
for anchor pairs , the mean error across all 123 pairs is reported , along with its stan -
patterns are consistent across the different condi - tions .
a relatively large improvement comes from using all ten anchor words .
smaller benets arise from including rare words and from normalizing
models that use the original pair of anchor words , excellent and poor , perform slightly better than the average pair .
whereas mean performance ranges from 123% to 123% , misclassication rates for this pair of anchors ranges from 123% to 123% .
123 a smaller unlabeled corpus as described in section 123 , there are several rea - sons to explore the use of a smaller unlabeled cor - pus , rather than the entire internet .
in our experi - ments , we use additional movie reviews as our doc - uments .
for this domain , pang makes available
because this corpus offers dramatically fewer in - stances of anchor words , we modify our estimation procedure .
rather than discarding words that rarely co - occur with anchors , we use the same feature set as before and regularize estimates by the same pro - cedure used in the naive bayes procedure described
using all features , and ten anchor words with nor - malized scores , test error is 123% .
this suggests that comparable results can be attained while re - ferring to a considerably smaller unlabeled corpus .
rather than requiring several days of downloads , the count of nearby co - occurrences was completed in under ten minutes .
because this procedure enables fast access to counts , we explore the possibility of dramatically enlarging our collection of anchor words .
we col -
figure 123 : misclassication with labeled docu - ments .
the solid curve represents a latent fac - tor model with estimated coefcients .
the dashed curve uses a naive bayes classier .
the two hor - izontal lines represent unsupervised estimates; the upper one is for the original unsupervised classier , and the lower is for the most successful unsuper -
lect data for the complete set of wordnet syn - onyms for the words good , best , bad , boring , and dreadf ul .
this yields a total of 123 anchor words , 123 positive and 123 negative .
when all of these an - chors are used in conjunction , test error increases to 123% .
one possible difculty in using this auto - mated procedure is that some synonyms for a word do not carry the same sentiment orientation .
for in - stance , intense is listed as a synonym for bad , even though its presence in a movie review is a strongly
123 methods with supervision as demonstrated in section 123 , each anchor word ak is associated with a coefcient ( cid : 123 ) k .
pervised models , these coefcients are assumed to be known .
however , when labeled documents are available , it may be advantageous to estimate them .
figure 123 compares the performance of a model with estimated coefcient vector ( cid : 123 ) , as opposed to unsupervised models and a traditional supervised approach .
when a moderate number of labeled doc - uments are available , it offers a noticeable improve -
the supervised method used for reference in this case is the naive bayes model that is described in section 123 .
naive bayes classication is of partic - ular interest here because it converges faster to its asymptotic optimum than do discriminative meth - ods ( ng , a .
and jordan , m . , 123 ) .
further , with
123this corpus is freely available on the following website :
123in the labeled pang corpus , intense appears in 123 positive
reviews and only 123 negative ones .
satoshi morinaga , kenji yamanishi , kenji tateishi , and toshikazu fukushima .
mining prod - uct reputations on the web .
and jordan , m .
on discriminative vs .
generative classiers : a comparison of logis - tic regression and naive bayes .
advances in neu - ral information processing systems , 123
kamal nigam , andrew k .
mccallum , sebastian thrun , and tom m .
mitchell .
text clas - sication from labeled and unlabeled documents using em .
machine learning , 123 ( 123 / 123 ) : 123
classication using machine learning techniques .
in proceedings of the 123 confer - ence on empirical methods in natural language
thumbs up ?
turney and m . l .
littman .
unsupervised learning of semantic orientation from a hundred -
peter turney .
thumbs up or thumbs down ? semantic orientation applied to unsupervised classication of reviews .
in proceedings of the 123th annual meeting of the association for computational linguistics ( acl123 ) , pages 123 123 , philadelphia , pennsylvania .
association for
janyce wiebe .
learning subjective adjec - tives from corpora .
in proc .
123th national con - ference on articial intelligence ( aaai - 123 ) ,
jian zhang and yiming yang .
robustness of regularized linear classication methods in text categorization .
in proceedings of the 123th an - nual international acm sigir conference ( si -
a larger number of labeled documents , its perfor - mance on this corpus is comparable to that of sup - port vector machines and maximum entropy mod - els ( pang et al . , 123 ) .
the coefcient vector ( cid : 123 ) is estimated by regular - ized logistic regression .
this method has been used in other text classication problems , as in zhang and yang ( 123 ) .
in our case , the regularization123 is introduced in order to enforce the beliefs that :
( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) 123 , if a123 , a123 synonyms ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 , if a123 , a123 antonyms
for further information on regularized model tting , see for instance , hastie et al .
( 123 ) .
in business settings , there is growing interest in learning product reputations from the internet .
for such problems , it is often difcult or expensive to obtain labeled data .
as a result , a change in mod - eling strategies is needed , towards approaches that require less supervision .
in this paper we pro - vide a framework for allowing human - provided in - formation to be combined with unlabeled docu - ments and labeled documents .
we have found that this framework enables improvements over existing techniques , both in terms of the speed of model es - timation and in classication accuracy .
as a result , we believe that this is a promising new approach to problems of practical importance .
