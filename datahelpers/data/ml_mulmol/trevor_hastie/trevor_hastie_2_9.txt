we introduce a technique for extending the classical method of linear discriminant analysis to data sets where the predictor variables are curves or functions .
this procedure , which we call functional linear discriminant analysis ( flda ) , is particularly useful when only fragments of the curves are observed .
all the techniques associated with lda can be extended for use with flda .
in particular flda can be used to produce classications on new ( test ) curves , give an estimate of the discriminant function between classes , and provide a one or two dimensional pictorial representation of a set of curves .
we also extend this procedure to provide generalizations of quadratic and regularized discriminant analysis .
some key words : classication; filtering; functional data; linear discriminant analysis; low dimensional representa - tion; reduced rank; regularized discriminant analysis; sparse curves .
linear discriminant analysis ( lda ) is a popular procedure which dates back as far as fisher ( 123 ) .
let x be a q - dimensional vector representing an observation from one of several possible classes .
linear discriminant analysis can be used to classify x if the class is unknown .
alternatively , it can be used to characterize the way that classes differ via a discriminant function .
there are several different ways of describing lda .
one is using probability models .
suppose that the ith class has density fi x i .
then bayes formula tells us that
and prior probability p
j f j x
it is relatively simple to show that the rule that classies to the largest conditional probability will make the smallest expected number of misclassications .
this is known as the bayes rule or classier .
if we further assume that the ith class has a gaussian distribution with mean m i and covariance s then it can be shown that classifying to the maximum conditional probability is equivalent to classifying to
figure 123 : data measurements of spinal bone mineral density ( g females and the grey lines males .
cm123 ) for 123 individuals .
the black lines represent
where li is the discriminant function
note that li is a linear function of x .
when the maximum likelihood estimates for m at the linear discriminant analysis procedure .
i and s are used we arrive
123 lda on functional data
lda can be implemented on data of any nite dimension but cannot be directly applied to innite - dimensional data such as functions or curves .
provided the entire curve has been observed , this can be overcome by dis - cretizing the time interval .
however , this generally results in highly correlated high - dimensional data which makes the within - class covariance matrix difcult to estimate .
there are two common solutions to this prob - lem .
the rst is to use some form of regularization , such as adding a diagonal matrix to the covariance matrix .
see for example dipillo ( 123 ) , dipillo ( 123 ) , campbell ( 123 ) , friedman ( 123 ) and hastie et al .
( 123 ) .
we call this the regularization method .
the second is to choose a nite - dimensional basis , f , and nd the best projection of each curve onto this basis .
the resulting basis coefcients can then be used as a nite - dimensional representation making it possible to use lda , or any other procedure , on the basis coefcients .
we call this the ltering method .
unfortunately , it is often the case that only a fragment of each curve has been observed .
consider , for ex - ample , the data illustrated in figure 123
these data consist of measurements of spinal bone mineral density for 123 individuals taken at various ages , a subset of the data presented in bachrach et al .
( 123 ) .
even though , in aggregate , there are 123 observations measured over a period of almost two decades , we only have 123 - 123 measurements for each individual , typically measured over no more than a couple of years .
in this situation
both of the common approaches to discriminant analysis can break down .
the regularization method is not feasible because discretizing would result in a large number of missing observations in each dimension .
the ltering method also has several potential problems .
the rst is that an assumption is made of a common covariance matrix for each curves basis coefcients .
however , if the curves are measured at different time points , as is the case in the growth curve data of figure 123 , the coefcients will all have different covariances .
one would ideally like to put more weight on accurate basis coefcients but the ltering method does not allow such an approach .
a second problem is that with extremely sparse data sets some of the basis coef - cients may have innite variance , making it impossible to estimate the entire curve .
for example , with the spinal bone density data , each individual curve has so few observations that it is not possible to t any rea - sonable basis .
in this case the method fails and there is no way to proceed .
for the sparse data considered in this paper these are serious problems .
123 general functional model
the regularization and ltering approaches can both be viewed as methods for tting the following general functional model .
let g t be the curve of an individual randomly drawn from the ith class .
assume that , if
is in class i , it is distributed as a gaussian process with
one typically never observes an individual over the entire curve; rather one samples the curve with error at distinct time points t123 tn .
we assume that the measurement errors are uncorrelated with mean zero and constant variance s 123
let y be the vector of observations of g t
cov g t
at times t123
i as given in ( 123 ) and s
many func - the bayes rule for classifying this curve is given by ( 123 ) with m tional classication procedures are simply methods for tting this model , i . e .
estimating m and then using the classication rule given by ( 123 ) .
for example , the regularization approach attempts to es - by producing sample estimates along a ne lattice of time points .
alternatively , the ltering method forms estimates by modeling m using basis functions .
however , we saw in section 123 that , when confronted with sparse data sets , both methods can produce poor ts to the model .
123 the flda approach
in this paper we present an alternative method for tting the functional model of 123 , which copes well with sparse data .
we call this method functional linear discriminant analysis ( flda ) .
the procedure uses a spline curve plus random error to model observations from each individual .
the spline is parameterized using a basis function multiplied by a q - dimensional coefcient vector .
this effectively transforms all the data into a single q - dimensional space .
finally , the coefcient vector is modeled using a gaussian distribution with common covariance matrix for all classes , in analogy with lda .
the observed curves can then be pooled to
figure 123 : ( a ) a simulated data set of 123 curves from 123 different classes .
( b ) the transformed curves after removing the random components .
estimate the covariance and mean for each class .
this makes it possible to form accurate estimates for each individual curve based on only a few observations .
this has several advantages over the regularization and ltering methods .
first , as it does not rely on forming individual estimates for each curve , it can be used on sparse data sets such as the growth curve data .
second , by producing an estimate for the covariance kernel it is possible to estimate the variance of the basis coefcient for each curve and automatically put more weight on the more accurate coefcients .
as a simple illustration of the effectiveness of flda in separating curves from different classes consider figure 123 ( a ) .
this is a plot of curves from a simulated data set .
class 123 curves are plotted as black lines and class 123 as grey .
each class has a different mean function and the curves are generated by combining the class mean with a random curve plus random normal noise .
from visual inspection alone there is no obvious separation between classes .
however , figure 123 ( b ) provides a plot of the transformed curves after using the flda procedure to remove the random component from each curve .
now the separation is clear so when the same procedure is applied to a new curve it will clearly be identiable which group it falls into and it can be classied with high accuracy .
the level of accuracy depends on the signal to noise ratio , which is high in this case .
however , the key point is that the strong signal is not apparent in figure 123 ( a ) and only emerges as a result of using the flda procedure .
in the classical two - class lda setting this transformation amounts to projecting observations onto the line segment spanned by the means .
the flda model and classication procedure are presented in sections 123 and 123
one of the reasons for the popularity of lda is that it can be used for a variety of tasks .
it can be used to project high - dimensional data into a low dimension and hence produce a graphical representation .
furthermore it can also be used for classication and to produce a discriminant function to identify areas of discrimination between classes .
in section 123 we show how flda can be used to generalize each of these tools to functional data .
section 123 ex - plains how the standard flda framework can be extended to include rank - reduced and non - identical within - class covariance matrices .
the latter of these extensions provides a functional generalization of quadratic
123 the flda model
in this section we develop the flda model by generalizing the lda model given in 123 to handle functional
123 a generalization to functional data let gi j t corresponding vectors of observations and measurement errors at times ti j123
be the true value at time t for the jth individual or curve from the ith class .
let yi j and e
ti jni j .
then we begin with
i j be the
where k is the number of classes and mi is the number of individuals in the ith class .
the measurement errors are assumed to have mean zero , constant variance s 123 and be uncorrelated with each other and gi j .
these assumptions implicitly mean that we are assuming that the time points that we have failed to observe are missing at random .
as we only have a nite amount of data we need to place some restrictions on gi j in order to t this model .
a common approach to modeling functional data is to represent the functions using a exible basis ( ramsay and silverman 123 , chapter 123 ) .
we choose to use natural cubic spline functions because of their desirable mathematical properties and easy implementation ( de boor , 123; green and silverman ,
gi j t
is a spline basis with dimension q and h
where s t leads to a more restricted model ,
i j is a q - dimensional vector of spline coefcients
notice that the problem of modeling yi j has reduced to one of modeling h i j is a q - dimensional variable , so a natural approach is to model it using the gaussian distribution assumed for the standard lda
however , h
s ti j123
s ti jni j
i j n 123
if we assume that the error terms are also normally distributed this gives
123 rank reduced lda
i j n 123
i j n 123
a reduced - rank version of lda is often performed by transforming or projecting the variables into a lower - dimensional subspace and classifying in this subspace .
the subspace is chosen to maximize the between - class covariance relative to the within - class covariance .
these transformed variables are called linear dis - criminants or canonical variables .
anderson ( 123 ) and hastie and tibshirani ( 123 ) outline an alternative procedure using the constraint
i are respectively q - and h - dimensional vectors , and l
where l 123 and a sets of authors show that using maximum likelihood to t the gaussian lda model of 123 with the added constraint ( 123 ) and classifying to the maximum posterior probability is identical to the classication from the reduced rank lda procedure .
h matrix , h
is a q
the same rank constraint can be placed on the means in ( 123 ) .
this gives the nal form of the flda model ,
i j n 123
i j n 123
in which l 123 , l tions on l
and the a
i are confounded if no constraint is imposed .
therefore we place the following restric -
l t sts
sg st and s is the basis matrix evaluated over a ne lattice of points .
the constraint provides a form of normalization for the linear discriminants .
more details will be given in the following section .
in practice the lattice should include , at least , all time points in the data set .
for example the spinal bone density 123 years so the lattice covered the same data was measured in 123 period .
this model is identical to the general functional model of 123 with
123th of a year increments from age 123
123 to 123
123 classifying curves
tg s t
in this section we rst detail a maximum likelihood procedure for tting ( 123 ) and then a method for forming classications by combining ( 123 ) and ( 123 ) to form an estimate of the bayes classier .
123 fitting the model fitting the flda model involves estimating l 123
g and s 123
notice that ( 123 ) implies
yi j n si j
si jg st
since observations from different individuals are assumed to be independent , the joint distribution of the observed curves is
123 yi j
a natural approach to tting the model is to maximize ( 123 ) over l 123 maximizing this likelihood is a difcult non - convex optimization problem .
if the g
g and s 123
unfortunately , directly i j had been observed , how -
ever , the joint likelihood of yi j and g
i j would simplify to
123s 123 yi j
123s ni j
t yi j
maximizing this likelihood is much less complex , and suggests treating the g i j as missing data and imple - menting the em algorithm ( dempster et al . , 123; laird and ware , 123 ) .
the em algorithm involves al - ternately calculating the expected value of the missing data g i j and maximizing the joint likelihood .
the e step is performed using the equation
i jsi j
i j yi j
si jl 123
while the m step involves maximizing
t yi j
ni j log
holding g i j xed .
further details can be obtained from the web site www - rcf . usc . edu / gareth .
as with all
em algorithms the likelihood will increase at each iteration but it is possible to reach a local rather than global maximum .
this can be a problem for very sparse data sets such as the bone mineral density data .
however , the problem is generally eliminated by enforcing a rank constraint on g as discussed in 123 .
other model selection questions arise in practice , such as the choice of q , the dimension of the spline basis .
there are several possible procedures that have been applied to models of this type .
one is to calculate the cross - validated likelihood for various dimensions and choose the model corresponding to the maximum ( james et al . , 123 ) .
aic and bic are two other , less computationally expensive , procedures that have also proved successful on this sort of data ( rice and wu , 123 ) .
in practice the nal classication appears to be relatively robust to any reasonable choice of dimension but this is an area of ongoing research .
notice that under the standard reduced - rank lda model ,
hence , using bayes formula , the probability of class i given x is proportional to
that classifying an observation x using reduced - rank lda is identical to classifying to
note that the second line follows from the fact that l ts
this means
it can be shown that
x and a
i are equal to the linear discriminants of x and m
i that lda produces , up to an
the same approach is used for flda .
by combining ( 123 ) and ( 123 ) we see that the posterior probability that
a curve y was generated from class i is proportional to
where sy is the spline basis matrix for y and
so y will be classied to
notice , however , that if one lets
hence ( 123 ) is equivalent to
l t sy
123 l t sy
just as with standard lda , therefore ( 123 ) corresponds to classifying to the class whose mean is closest to our test point in the reduced y .
notice also that if y has been measured space where distance is measured using the inverse covariance of
i are , up to an additive constant , the linear discriminants of y and m
y and a
l t sy
over the entire time period , so that sy
and ( 123 ) reduces to
123 applications of flda
in this section we show how three of the most important tools that lda provides , namely , low dimensional representation , discrimination functions and classication , can be replicated using flda .
123 low dimensional representation of curves
one of the reasons for the popularity of lda is that it provides the ability to view high - dimensional data by projecting it onto a low - dimensional space .
this allows one to visually determine the discrimination between
figure 123 : ( a ) linear discriminants for each curve of the spinal bone density data , plotted against the average age people were measured at .
( b ) estimates of the standard error of the linear discriminants for each curve plotted against the average age .
there is a clear trend of increasing variability with age .
classes .
as mentioned in 123 , in a standard nite - dimensional setting the linear discriminant of x equals
up to an additive constant .
as cov distance between different observations is euclidean and can be easily calculated by visual inspection .
i , the transformed variables all have identity covariance , so the
this provides a natural approach to projecting functional data into a low - dimensional space .
in the flda y is the linear discriminant for y and that if y i .
however , if only fragments of the curve have been
y , given in ( 123 ) .
recall that
model the analogue of has been observed over the entire interval cov
l t sy
this makes direct comparison of points more difcult because the covariance structure may no longer be diagonal and , in general , curves measured at different time points will have different covariances .
however , 123 , so the linear discriminant is a scalar , the only effect this has is that each point has a different standard error .
figures 123 - 123 provide examples of this .
figure 123 ( a ) shows linear discriminants for each curve from the growth curve data of figure 123 , plotted versus the average age for observations from each individual .
for a two - class situation , such as this , the plot also provides a simple classication rule; curves with positive linear discriminant are classied as male and curves with negative linear discriminant as female .
the plot reveals some interesting properties of the data .
curves measured at ages below eighteen years are relatively well - separated while individuals measured at older ages have little discernible separation .
this trend is also apparent upon close examination of the original curves .
the two solid vertical lines either side of zero give the tted values for the a is .
they represent the class centroids in the transformed space .
their close proximity to each other relative to the variability of the linear discriminants indicate little overall separation between classes .
however , recall that , as a result
figure 123 : ( a ) plot of the linear discriminants for each curve on the spinal bone density data versus the estimated standard errors .
( b ) a linear discriminant plot for a simulated data set .
it shows a fairly clear difference between the classes but also a signicant overlap .
of ( 123 ) , the linear discriminant of y will have a standard error of one if the curve is measured over the entire interval .
in fact the separation between the centroids is about 123 123 standard deviations .
this implies that the confusion between genders is a result of the small number of observations per individual , which cause the standard error to increase dramatically .
a clear separation could be achieved with more observations .
figure 123 ( b ) gives the standard error for each linear discriminant versus average age of observation .
a curve with measurements over the entire time period would have standard error 123 , so this plot gives an indication of the amount of information lost by only observing the curve at a limited number of time points .
the standard errors range from over ten to eighty , indicating that a great deal of accuracy has been sacriced .
the increased variability also explains the poor separation at older ages where the standard errors are large relative to the distance between the class centroids .
once the model has been t , the standard error can be calculated for a curve observed at an arbitrary set of time points .
this provides a method for deciding on an optimal design in terms of locating a nite number of observations for an individual to minimize the standard error .
figure 123 gives two plots which combine the linear discriminants and their standard errors together .
this gives an easy method for deciding on the reliability of a given observation .
for example points with high standard error should be treated with caution .
we call these linear discriminant plots .
the left and right vertical dotted lines indicate the class centroids while the center lines are the class discriminators .
figure 123 ( a ) shows that the points with relatively low standard error have far better separation than those with a large standard error .
figure 123 ( b ) provides a similar plot for a simulated data set consisting of 123 curves measured at the same set of time points as that of the data set illustrated in figure 123
notice that even though each curve has been measured at fairly evenly spaced points throughout the time interval the standard errors still range up to ten .
the two classes are relatively well separated but there is still some clear overlap .
the distance between the class centroids is 123 standard deviations , indicating that one could achieve near perfect separation by sampling the curves at a wider range of time points .
figures 123 and 123 are further linear discriminant plots .
they were produced by using ethnicity as the class
figure 123 : linear discriminants for the spinal bone density data using ethnicity as the class variable .
( a ) the dotted lines represent , from left to right , class centroids for blacks , hispanics , whites , and asians .
notice that blacks and asians are fairly well separated while hispanics and whites are not .
( b ) the solid lines represent decision boundaries for classifying a given curve .
variable on a subset of the growth curve data , all females .
a plot of the growth curves for each ethnicity ( not shown ) indicates that there may be no clear separation between classes .
this is borne out by figure 123 ( a ) which gives a plot of linear discriminant versus average age .
there is signicant overlap between the classes .
however , it is still possible to gain some information .
the four vertical dotted lines represent the class cen - troids for , from left to right , blacks , hispanics , whites and asians .
it is clear that there is very little separation between hispanics and whites while blacks and asians are relatively well separated .
this is highlighted by figure 123 which shows linear discriminants and a plot of the raw data for blacks and asians alone .
the differ - ences are now clear .
figure 123 ( b ) is identical to 123 ( a ) except that the three discrimination boundaries are plotted in place of the class centroids .
the discrimination boundaries divide the space into four regions .
points in the leftmost region are classied as black , the next as hispanic , followed by white and nally asian .
the ability of lda to perform classication is of equal importance to its ability to explain discrimination between classes .
in 123 we showed that to classify a curve using flda one need only produce ( 123 ) and classify using ( 123 ) .
when all classes have equal weight and y is one dimensional this classication rule simplies to
while classication is not the primary goal on the spinal bone density data , we apply ( 123 ) to it to illustrate the procedure .
when using gender as the class variable the overall training error rate comes out at 123 however , the rate increases substantially to 123 123% for ages under 123
this conforms to our expectations from figure 123 ( a ) which shows much better discrimination for lower
123% for ages over 123 and decreases to 123
figure 123 : ( a ) linear discriminants for blacks and asians .
while there is still some overlap the separation is far clearer .
the vertical line gives the classication boundary .
( b ) a plot of the raw data for blacks and asians .
notice that blacks tend to have a higher bone density .
the two solid lines represent the means for blacks and asians while the dotted lines are for hispanics and whites .
table 123 gives the confusion matrix when ethnicity is used as the class variable .
it shows the true ethnicity and the corresponding classication for each of the 123 individuals .
for example , 123 of the 123 asians were classied as asian while 123 of the 123 hispanics were classied as black .
while the overall training error rate is 123 123% , a large fraction of the errors are among hispanics and whites , while asians and blacks are relatively well classied .
when asians and blacks are considered alone the error rate drops to 123% .
in table 123 we present the results from a simulation study where the flda procedure is compared with two other classiers .
the rst is the ltering method of 123 .
recall that the ltering method consists of tting a exible basis , in this case cubic splines , to each curve and then classifying by using lda on the basis coefcients .
the ltering method provides a simple comparison to flda .
the second is the bayes classier which is optimal if the true distribution of the classes is known .
it provides the best case error rate
table 123 : confusion matrix of classications for the four ethnicities .
the numbers in parentheses give the percentages of each ethnicity receiving the corresponding classication .
asians and blacks have relatively little confusion while hispanics and whites have a great deal .
table 123 : test error rates from the simulation study for various different fractions of missing data .
the numbers in parentheses indicate estimated standard errors for the error rates .
study consisted of a three - class problem .
for each class , 123 curves were generated according to the flda model ( 123 ) and sampled on a ne grid of 123 equally - spaced points .
then , for each curve , a random subset , 123 - 123% , of the observations , were removed to replicate curve fragments .
multiple data sets were created and the flda and ltering procedures were applied to each .
error rates were then calculated on a separate , test set , of 123 curves .
furthermore the bayes error rate , which is the lowest possible , was also calculated on this test set .
over the 123 time points the average deviation of mean curves between classes 123 and 123 and 123 while it was twice this number between classes 123 and 123
the standard between classes 123 and 123 was 123 deviation of the error terms was s 123
finally the average standard deviation over the 123 time points of the random curves sg was approximately 123 123
the rst gure gives a guide as to the signal while the last two indicate the noise .
table 123 provides a summary of the test error rates .
as one would expect , all three sets of error rates increase with the fraction of missing data .
of far more interest is the similarity between the flda and bayes error rates .
even with 123% of the data removed the difference is only 123 123% .
with less than 123% of the data removed the ltering and flda methods give comparable results .
however , the ltering method deteriorates rapidly until at 123% its error rate is close to that of the naive classier which randomly assigns a class label based on the prior probability for each class .
note that at 123% the ltering method could not even be applied to several of the simulated data sets because individual curves could not be tted .
123 class discrimination functions
in a standard two - class lda setting the discriminant function is dened as
tl t sts
determining the classication of a point .
in the flda setting m
is the within group covariance matrix .
this function gives the weight put on each dimension in so the functional analogue is
sg st and s is the spline basis matrix evaluated on a ne grid of points over the entire time period .
equation ( 123 ) can be used to produce a discriminant function for any set of data .
figure 123 provides examples from the growth curve data .
figure 123 ( a ) gives the discriminant function using gender as the class variable .
there is a strong negative peak before age 123 and a large positive peak afterwards; this indicates a phase shift between genders and explains why there is far better separation for the earlier ages .
figure 123 ( b ) gives a similar plot using ethnicity as the class variable .
again most of the discrimination appears to be in the early years .
a comparison of discriminant functions produced from the simulation study of 123 , using both the flda and ltering approaches , is given in figure 123
the population discriminant function is shown in black while
figure 123 : discriminant functions for the growth curve data of figure 123
( a ) using gender as the class variable .
there is a strong indication of phase shift .
( b ) using ethnicity as the class variable .
its estimates are in grey .
figures 123 ( a ) and ( b ) present results from the study with 123% of the data removed while figures 123 ( c ) and ( d ) present results with 123% removed .
for each , the top plot shows the discriminant functions from 123 different simulations using flda , while the bottom plot gives the corresponding graph for the ltering approach .
as adding or multiplying the discriminant function by a constant leaves the clas - sication unchanged , the estimates have been transformed to produce the least squares t to the population discriminant function .
it is clear that in both cases the flda approach is producing more accurate discrim - inant functions , reected in the decreased error rates of table 123
under the standard flda model
cov yi j
si jg st
in this section we consider a number of possible extensions to this model by exploring different assumptions for g and hence the covariance matrix of yi j .
123 reduced rank covariance matrices under ( 123 ) , no restrictions are placed on the structure of g .
in practice , the likelihood function for data sets such as the spinal bone density data has a large number of local maxima which make this model difcult to t .
as a result , for even moderate q , one can produce a highly variable t to the data .
james et al .
( 123 ) show that such problems can be reduced by enforcing a rank constraint on g .
a rank p constraint is equivalent to
q dq t and
cov yi j
si jq dq tst
flda discriminant functions
flda discriminant functions
filtering discriminant functions
filtering discriminant functions
figure 123 : results from two different sets of simulations .
plots ( a ) and ( b ) show the true discriminant function ( black line ) and estimates ( grey lines ) from 123 different simulations using flda ( a ) and ltering ( b ) methods with 123% of each curve unobserved .
plots ( c ) and ( d ) show the equivalent results with 123% of each curve unobserved .
is a q
q ) matrix and d is a diagonal matrix .
james et al .
( 123 ) suggest several methods for choosing the rank and conclude that the optimal rank for
the results from section 123 were produced using such a reduced rank model with p
these data is p because this gave a far better t to the data .
123 functional quadratic discriminant analysis the flda procedure , in analogy with lda , makes an assumption of a common covariance matrix , g i j vectors from each class .
this can result in a considerable reduction in variance for classes with a small sample size .
however , the assumption can cause a considerable increase in bias if the covariances are not common .
in a standard setting quadratic discriminant analysis ( qda ) provides a less restrictive procedure by allowing different covariance matrices .
in a similar manner the flda model of 123 can be generalized by removing the assumption of a common covariance term , g
this gives the covariance structure
cov yi j
i is the covariance matrix for class i .
the posterior probability is now proportional to
by tting this model and classifying to arg mini di y quadratic discriminant analysis ( fqda ) is produced .
a generalization of qda , which we call functional
123 functional regularized discriminant analysis
it is well known that lda can perform badly if the assumption of a common within - class covariance matrix is violated , while qda generally requires a larger sample size ( wald and kronmal , 123 ) .
a small sample size causes a covariance matrix to be produced which is close to singular and hence excessive weight is placed on the directions corresponding to small eigenvalues .
regularization has been highly successful in this sort of poorly - posed inverse problem ( titterington ( 123 ) , osullivan ( 123 ) ) .
friedman ( 123 ) suggests the following regularization approach .
let si ni be the within class sample covariance matrix for class i and let ni is used for qda .
a compromise between the two approaches can be achieved by setting the within - class covariance matrix
n is the pooled covariance matrix which is used for lda while si
then s
w123 s and ni w123
a second level of regularization , namely shrinkage towards the identity matrix , is provided through
where q is the dimension of the space .
g is used as the within - class covariance matrix for the ith class .
friedman calls this approach regularized discriminant analysis ( rda ) .
rda has been shown to outperform both lda and qda in a large variety of situations .
a generalization to functional regularized discriminant analysis ( frda ) can be achieved using the fol -
lowing covariance structure
cov yi j
si j g
is dened as in ( 123 ) .
the choice of w123 and w123 is made using cross - validation .
applying cross - validation to the frda model is potentially computationally expensive .
however , in the rda setting , an algebraic update allows for a signicantly faster implementation ( friedman , 123 ) .
this update can be used in the flda setting by tting the fqda model , treating the resulting g i js as q dimensional data and tting rda to estimate w123 and w123 , and nally tting the frda model with w123 and w123 xed .
frda contains both flda and fqda as sub models .
by setting both w123 and w123 equal to zero the fqda 123 produces the flda model .
furthermore , by setting 123 a functional generalization of the nearest - means classier is produced where an observation is
123 and w123
model is produced .
while setting w123
assigned to the closest class mean , in euclidean distance .
we have presented a method , which we call functional linear discriminant analysis ( flda ) , for generaliz - ing linear discriminant analysis to functional data .
flda possesses all the usual lda tools , including a low - dimensional graphical summary of the data , and classication of new curves .
when the functional data have been measured over a large number of time points the procedure provides similar results to the ltering method introduced in section 123 .
however , when only fragments of the function are available the flda approach can still produce favorable outcomes while the ltering method fails completely .
flda can also be generalized in a number of ways .
a reduced rank version can be implemented when the data are very sparse and a quadratic version can be used when an assumption of a common covariance matrix is inappropriate .
a regularized version , which is a compromise between flda and fqda , is also available .
another possible generalization , which we have not considered , is to model heterogeneous variance and autocorrelation in the error terms .
this may well improve the classication accuracy of the method provided enough time points have been observed per curve to provide accurate estimates .
unfortunately , allowing s 123 to vary would make it impossible to enforce the constraint ,
l t sts
in turn ( 123 ) allows gures such as which ensures that , if a curve is measured at all time points , cov 123 ( b ) to provide a measure of the amount of information lost through missing observations .
if s 123 was allowed to vary this interpretation would no longer be feasible .
the authors would like to thank the editor and referees for many constructive suggestions .
trevor hastie was partially supported by grants from the national science foundation and the national institutes of health .
