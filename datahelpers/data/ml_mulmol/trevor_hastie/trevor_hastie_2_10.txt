abstract .
we propose a new method for class prediction in dna microarray studies based on an enhancement of the nearest prototype classier .
our technique uses shrunken centroids as prototypes for each class to identify the subsets of the genes that best characterize each class .
the method is general and can be applied to other high - dimensional classication problems .
the method is illustrated on data from two gene expression studies : lymphoma and cancer cell lines .
key words and phrases : sample classication , gene expression arrays .
class prediction with high - dimensional features is an important problem and has recently received a great deal of attention in the context of dna microarrays .
the task is to classify and predict category of a sample , based on its gene expression prole .
recent proposals for this problem include golub et al .
( 123 ) , hedenfalk et al .
( 123 ) , hastie , tibshirani , botstein and brown ( 123 ) and the articial neural network approach in khan et al .
( 123 ) .
the microarray problem is a unique and challeng - ing classication task because there are a large number of inputs ( genes ) from which to predict classes and a relatively small number of samples .
it is especially im - portant to identify which genes contribute toward the
robert tibshirani is professor , departments of health research and policy , and statistics , stanford univer - sity , stanford , california 123 - 123 ( e - mail : tibs@ stat . stanford . edu ) .
trevor hastie is professor , depart - ments of statistics , and health research and policy , stanford university , stanford , california 123 - 123 ( e - mail : hastie@stat . stanford . edu ) .
balasubramanian narasimhan is senior research associate , depart - ments of statistics , and health research and pol - icy , stanford university , stanford , california 123 - 123 ( e - mail : naras@stat . stanford . edu ) .
gilbert chu is professor , departments of biochemistry , and med - ical oncology , stanford university , stanford , califor - nia 123 - 123 ( e - mail : chu@cmgm . stanford . edu ) .
classication .
this can aid in biological understand - ing of the disease process and is also important in de - velopment of clinical tests for early diagnosis .
in this article we propose a simple approach to the problem that performs well and is easy to understand and inter -
as an example , we consider data from alizadeh et al .
( 123 ) , which is available from the authors web site .
these data consist of expression measurements on 123 , 123 genes from samples of 123 lymphoma patients .
the samples are classied into diffuse large b - cell lymphoma and leukemia ( dlcl ) , follicular lymphoma ( fl ) and chronic lymphocytic leukemia ( cll ) .
we selected a random subset of 123 samples and set them aside as a test set; the remaining 123 samples formed the training set .
we began with a nearest centroid classication .
figure 123 ( light grey bars ) shows the training - set cen - troids ( average expression of each gene ) within each of the three classes .
the overall average expression of the corresponding gene has been subtracted , so that these values are differences from the overall centroid .
to apply the nearest centroid classication , we take the gene expression prole of the test sample and compute its squared distance from each of the three class centroids .
the predicted class is the one whose centroid is closest to the expression prole of the test sample .
this procedure makes zero errors on the 123 test samples , but has the major drawback that it uses all 123 , 123 genes .
class prediction by nearest shrunken centroids
centroids ( grey ) and shrunken centroids ( red ) for the lymphoma / leukemia data set .
each centroid has the overall centroid subtracted; hence , what we see are contrasts .
the horizontal units are log ratios of expression .
going from left to right , the number of training samples is 123 , 123 and 123
the order of the genes is determined by hierarchical clustering .
we propose the nearest shrunken centroid method , which uses denoised versions of the centroids as prototypes for each class .
the optimally shrunken centroids , derived using a method described below , are shown as red bars in figure 123
classication is then made to the nearest ( shrunken ) centroid .
the resulting procedure has zero test errors .
in addition , only 123 genes have a nonzero red bar for one or more classes in figure 123 and , hence , are the only ones that contribute
toward the classication .
the amount of shrinkage is determined by cross - validation .
in the preceding example , the ( unshrunken ) nearest centroid method had the same error rate as the nearest shrunken centroid procedure .
this is not always the case .
table 123 shows results taken from tibshirani , hastie , narasimhan and chu ( 123 ) on classication of small round blue cell tumors .
the data are taken from khan et al .
( 123 ) .
there are 123 test samples
tibshirani , hastie , narasimhan and chu
results on classication of small round blue cell tumors
test error rate
number of genes used
nearest shrunken centroids regularized discriminant analysis
and 123 , 123 genes .
the neural network and regularized discriminant analysis methods used in the table are described in section 123
we gave a brief description of the nearest shrunken centroid method in tibshirani , hastie , narasimhan and chu ( 123 ) , focussing on the biological ndings from two different applications .
here we give a broader and more thorough statistical treatment .
in section 123 we describe the basic method .
we detail our procedure for adaptive choice of thresholds in section 123
additional issues and comparisons are discussed in sections 123 , including application of the method to capturing heterogeneity with an abnormal class compared to a control class , in section 123
finally we conclude with a brief discussion in section 123
nearest shrunken centroids
123 details of the proposal let xij be the expression for genes i = 123 , 123 , .
, p and samples j = 123 , 123 , .
each sample belongs to one of k classes 123 , 123 , .
let ck be indices of the nk samples in class k .
the ith component of the centroid for class k is xik = ( cid : 123 ) xij / nk , the mean expression in class k for gene i; the ith component of the overall centroid is xi = ( cid : 123 )
j=123 xij / n .
our idea is to shrink the class centroids toward the overall centroid .
however , we rst normalize by the within - class standard deviation for each gene
dik = xik xi
where si is the pooled within - class standard deviation for gene i ,
123 / nk 123 / n makes the denominator in equation ( 123 ) equal to the estimated standard error of
and mk =
soft threshold function .
the numerator .
thus dik is a t - statistic for gene i , comparing class k to the average class .
( in fact , we also add a regularization parameter s123 to the values si; see section 123 ) we can write
our proposal shrinks each dik toward zero , giving d and new shrunken centroids or prototypes
xik = xi + mksi dik .
= xi + mksi d
the shrinkage we use is called soft thresholding : the absolute value of each dik is reduced by an amount and is set to zero if the result is less than zero .
algebraically , this is expressed as
= sign ( dik ) ( |dik| ) + ,
where the subscript plus means positive part ( t+ = if t > 123 and zero otherwise ) .
this is shown in figure 123
since many of the xik will be noisy and close to the overall mean xi , soft thresholding usually produces better ( more reliable ) estimates of the true means ( donoho and johnstone , 123 ) .
the proposed method has the attractive property that many of the components ( genes ) are eliminated as far as class prediction is concerned if the shrinkage parameter is large enough .
specically , if causes dik to shrink to zero for all classes k , then the centroid for gene i is xi , the same for all classes .
thus gene i does not contribute
class prediction by nearest shrunken centroids
= 123 there are about 123 , 123 active genes .
the ik in each class are numbers of genes with nonzero d ( 123 , 123 , 123 ) .
note that the selection of genes for a given value of is carried out separately for each of the 123 cross - validation trials .
this is important to avoid selection bias and an unrealistically optimistic cross - validation error rate .
as pointed out by ambroise and mclach - lan ( 123 ) , a number of authors have made the mis - take of selecting genes based on all of the training data ( expression values and classes ) and then subject - ing only the selected genes to cross - validation .
this can produce a wildly optimistic estimate for misclas - sication error : it is easy to simulate two - class exam - ples in which the class labels are independent of the ex - pression values ( test error = 123% ) , but cross - validation after selection reports an error of zero .
formula ( 123 ) takes into account the size of each class and effectively applies a larger threshold to a smaller ( higher variance ) class .
even after this adjustment , some classes may be farther away than others from the overall centroid and , hence , may be easier to distinguish .
in this case , many of the nonzero genes for that class may not be needed for accurate classication .
thus we might try to vary the class thresholds to minimize the total number of nonzero genes needed to achieve a given error rate .
the details of how we do this are discussed in section 123
in this case the procedure increased the thresholds for the rst and third classes , and was very successful : as shown in the bottom panel of figure 123 , it reduced the number of genes to just 123 without increasing the test error .
123 finding the predictors that matter
figure 123 shows the shrunken differences dik for the 123 genes that have at least one nonzero difference .
figure 123 shows the heat map of the chosen 123 genes .
within each of the horizontal partitions , we have or - dered the genes by hierarchical clustering , and sim - ilarly for the samples within each vertical partition .
clear separation of the classes is evident .
the top set of genes characterizes cll with some genes overex - pressed and others underexpressed .
similarly the mid - dle set of genes characterizes fl .
the genes in the bot - tom set of the gure are overexpressed in dlcl , and underexpressed in fl and cll .
123 the log - likelihood
it is quite common to have a small number of samples in each class , especially when the number of
lymphoma / leukemia data : training error ( tr , blue ) , cross - validation error ( cv , green ) and test error ( te , red ) as the threshold parameter is varied .
in the top panel , the default soft threshold scaling is used : a solution with = 123 and 123 , 123 genes is chosen .
in the bottom panel , adaptive threshold scaling was used; the value = 123 is chosen , resulting in a subset of just 123 genes , with the same test error rate as in the top panel .
to the nearest centroid computation .
we chose by cross - validation , as illustrated below .
note that the standardization by si in ( 123 ) has the effect of giving higher weight to genes that have stable expression within samples of the same class .
this same standardization is inherent in other common statistical methods , such as linear discriminant analysis ( see section 123 ) .
the top panel of figure 123 shows the training , 123 - fold cross - validation and test errors as the shrinkage parameter is varied .
the top of the plot indicates the number of genes retained ( for the training data ) at that particular threshold .
the left end of the gure represents no shrinkage , while the right end represents complete shrinkage .
the test error is minimized near = 123; when the curve is at near the minimum , we typically chose the largest value of ( smallest number of genes ) that achieves the minimal error .
the upper axis shows the number of active genes with at least one nonzero component d ik , as is varied
tibshirani , hastie , narasimhan and chu
shrunken differences dik for the 123 genes that have at least one nonzero difference .
classes is large .
this can result in a cross - validation curve that has discrete jumps and high variability .
to help with this problem , we can use the mean cross - validated log - likelihood rather than misclassi - cation error .
since our model produces class probabil - ity estimates ( see equation ( 123 ) in section 123 ) , the log - likelihood of a test sample x ) .
the mean log - likelihood curve is typically smoother than the misclassication error curve .
with class label y
figure 123 shows the test set log - likelihood and mis - classication error curves for the lymphoma data .
( this is for illustration only; we are not suggesting use of the test error to select . ) they give a similar picture , although the choice of the smallest model where the log - likelihood starts to dip yields more genes than that from the misclassication error curve .
in the next sec - tion we make use of the log - likelihood in estimation of
123 class probabilities and discriminant functions
we classify test samples to the closest shrunken centroid , again standardizing by si .
we also make a correction for the relative abundance of members of each class .
details are given next .
suppose we have a test sample ( vector ) with expres -
we dene the dis -
123 log k .
sion levels x criminant score for class k as
) = p ( cid : 123 )
the rst part of ( 123 ) is simply the standardized squared distance of x to the kth shrunken centroid .
the second part is a correction based on the class prior probability k , where the overall proportion of class k in the population
k=123 k = 123
this prior gives
class prediction by nearest shrunken centroids
classication rule is then
) = ( cid : 123 )
) = min
if the smallest distances are close and hence ambigu - ous , the prior correction gives a preference for larger classes , since they potentially account for more er - rors .
we usually estimate the k by the sample priors k = nk / n .
if the sample prior is not representative of the population , then more realistic priors or even uni - form priors k = 123 / k can instead be used .
we can use the discriminant scores to construct estimates of the class probabilities by analogy to gaussian linear
the left panel of figure 123 displays these probabilities for the lymphoma data .
for illustration , we used the largest value of ( = 123 ) that minimizes the test error in the bottom panel of figure 123 , rather than the cross - validation - minimizing value of 123 used earlier .
the value = 123 yields 123 genes .
we derived the probabilities using the centroids that were dened by applying this value of to the test set .
in figure 123 , the value = 123 gives exactly the same test error ( in fact , the same class predictions ) as = 123 , but gives a higher log - likelihood value .
the estimated probabilities resulting from = 123 are shown in the right panel of figure 123
these probabilities are more extreme than those in the left panel .
the rightmost probabilities are preferred , since they produce a higher log - likelihood score .
heat map of the chosen 123 genes .
within each of the horizontal partitions , we have ordered the genes by hierarchical clustering , and similarly for the samples within each vertical partition .
the data for all 123 samples are shown .
test set mean log - likelihood curve ( red ) and test set misclassication error curve ( green ) .
the latter has been translated so that it ts in the same plotting region .
the broken line shows where the log - likelihood curve starts to dip , while the dotted line shows where the misclassication error starts to rise .
tibshirani , hastie , narasimhan and chu
estimated test set probabilities using the 123 gene model from minimizing misclassication error ( left ) and the 123 gene model from maximizing the log - likelihood ( right ) .
probabilities are partitioned by the true class .
there are no classication errors in the test set .
adaptive choice of thresholds
in this section we describe the procedure for adaptive threshold choice in the nearest shrunken centroid method .
we dene a scaling vector ( 123 , 123 , .
, k ) and initially set k = 123 for all k .
these scalings are included in the denominator of expression ( 123 ) , that is ,
dik = xik xi
we scale the values so that minj ( j ) = 123 : values greater than 123 mean that a larger threshold is effectively used for class k .
to test this procedure further , we simulated some data consisting of 123 samples in each of four classes and 123 , 123 genes .
we ran two different simulations , with the results shown in the top and bottom pan - els of figure 123
for a concise description , let r ( a , n ) represent the number a repeated n times .
all ex -
we applied the following procedure :
find the class k with the largest number of training
decrease k by 123% and then rescale all j so that
errors averaged over the grid of values used .
minj ( j ) = 123
repeat the above steps for a number of iterations ( here 123 ) and nd the solution that gives the lowest average error , among the values of ( 123 , 123 ,
this procedure is based entirely on the training set and does not use information from cross - validation or a test set .
it is admittedly heuristic , but does produce useful results in practice .
for the lymphoma data , we obtained the solution ( 123 , 123 , 123 ) = ( 123 , 123 , 123 ) , which is the value we used to produce figures 123 and 123
most of the errors in the original solution occurred in class fl; the new thresholds are larger for classes dlcl and cll , and hence many fewer genes are used to discriminate these classes .
remarkably , the total number of genes used has decreased from 123 , 123 to 123 without raising the test
simulated data : mean 123 standard deviation of the test error over ve simulations , for default ( equal ) thresholds ( red ) and adaptive thresholds ( green ) .
in the setup for the top panel , the class centroids are unevenly spaced; in the bottom panel , the within - class variances are unequal .
class prediction by nearest shrunken centroids
centroids for each of four classes for the two simulation scenarios .
the standard deviations for each class are indicated at the top of the plot .
pression values were independent gaussian with vari - ance 123
in the rst simulation , the class centroids were ( r ( 123 , 123 ) , r ( 123 , 123 ) ) , ( r ( 123 , 123 ) , r ( 123 , 123 ) ) , ( r ( 123 , 123 ) , r ( 123 , 123 ) , r ( 123 , 123 ) ) and ( r ( 123 , 123 ) , r ( 123 , 123 ) , r ( 123 , 123 ) , r ( 123 , 123 ) ) .
the centroids are shown in the top panel of figure 123
thus the rst class is far from the others , in the space spanned by the rst 123 genes .
the top panel of figure 123 shows the mean 123 standard deviation of the test error over ve simulations .
the methods used were default ( equal ) thresholds ( red ) and adaptive thresholds ( green ) .
the average values of the adaptive threshold were 123 , 123 , 123 and 123 .
the adaptive threshold method generally has lower test error .
in the second simulation , the means in the four classes were ( r ( 123 , 123 ) , r ( 123 , 123 ) ) , r ( 123 , 123 ) , r ( 123 , 123 ) ) , ( r ( 123 , 123 ) , r ( 123 , 123 ) , r ( 123 , 123 ) ) and ( r ( 123 , 123 ) , r ( 123 , 123 ) , r ( 123 , 123 ) ) .
the centroids are shown in the bottom panel of figure 123
the standard deviations in each class were 123 , 123 , 123 and 123 .
thus each class centroid is equidis - tant from the overall centroid ( the origin ) , but the within - class standard deviations are different .
the bot - tom of figure 123 shows the results : again the adaptive threshold does better in terms of test error; the aver - age values of the adaptive threshold were 123 , 123 , 123 and 123 .
with equal thresholds , the majority of nonzero genes were in class 123 : under the adaptive thresholds , the distribution was more balanced .
soft versus hard thresholding
an alternative to the soft thresholding ( 123 ) would be to keep all differences greater in absolute value than and discard the others; that is ,
= dik i ( |dik| > ) .
this is sometimes known as hard thresholding .
it dif - fers from soft thresholding in that differences greater than are unchanged , rather than shrunken toward zero by the amount .
one drawback of hard thresh - olding is its jumpy nature : as the threshold is in - creased , a gene with a full contribution dik suddenly is set to zero .
to investigate the relative behavior of hard versus soft thresholding , we generated standard normal ex - pression data for 123 , 123 genes and 123 samples , with 123 samples in each of two classes .
for the rst 123 genes , we added a random effect i n ( 123 , 123 ) to each expression level in class 123 for each gene i .
hence 123 of the 123 , 123 genes are differentially expressed in the two classes by varying amounts .
this experiment was repeated 123 times and the results were averaged .
the left panel of figure 123 shows the test error for hard and soft thresholding , as the threshold is var - ied , while the right panel displays the mean squared
i ( i i ) 123 / p , where i = ( cid : 123 ) ij / 123
in the left panel , we see that soft thresh - olding yields lower test error at its minimum; the right
tibshirani , hastie , narasimhan and chu
simulated data in two classes .
left : test misclassication error as the threshold is varied , using hard thresholding ( h ) and soft ( i i ) 123 / p , where i and i are the true and estimated difference in expression between thresholding ( s ) .
right : the estimation error class 123 and class 123 for gene i .
results are averages over 123 simulations : standard error of the average is about 123 in the left panel and 123 in the right panel .
panel shows that soft thresholding does a much better job of estimating the gene expression differences .
national cancer institute cancer lines
and subclass discovery
here we describe how to use nearest centroid shrink - age to discover subclasses .
we consider data from ross et al .
( 123 ) that consist of measurements on 123 , 123 genes on 123 cell lines .
the samples have been catego - rized into eight different cancer classes : breast ( bre ) , cns , colon ( col ) , leukemia ( leu ) , melanoma ( mel ) , non - small cell lung cancer ( nsc ) , ovarian ( ova ) and renal ( ren ) .
we randomly chose a training set of size 123 and a test set of size 123 , so that the classes were well represented in both sets .
default ( equal ) soft thresh - olding was used , with the prior probabilities set to the sample class proportions .
the results are shown in fig - ure 123
the best cross - validated error rate occurs at about 123 , 123 genes , giving a test error of 123 / 123
adap - tive thresholding failed to improve this result .
we also tried both support vector machines ( ra - maswamy et al . , 123 ) and regularized discriminant analysis ( section 123 ) .
both gave ve errors on the test set .
however , neither method gave a simple picture of
next we show a generalization of the nearest shrun - ken centroid approach that facilitates the discovery of potentially important subclasses .
it may be valuable bi - ologically to look for distinct subclasses of diseases in microarray analyses .
we can generalize the nearest shrunken centroid procedure to facilitate the discov - ery of subclasses .
consider the problem illustrated in
figure 123
the values indicate average gene expres - sion .
there are two subclasses in class 123 , and each of these can be distinguished from class 123 based on a small set of genes .
however , nearest shrunken cen - troids will fail here , because the overall centroids for each class are the same .
linear separating classiers , such as support vector machines ( svm ) , and linear discriminant analysis will also do poorly here .
either could be made to work with a suitable nonlinear trans - formation of the features ( or choice of kernel for the
nci cancer cell lines : training , cross - validation and test
class prediction by nearest shrunken centroids
nci subclass results : test errors ( out of 123 samples ) for nearest shrunken centroid model with no subclasses ( second column from left ) and two subclasses per class ( third column from left ) ; the columns on the right show the resulting
number of errors when a pair of subclasses for a given class is fused into one subclass
fusing subclasses for each class
svm ) ; while these may give low prediction error , they may not reveal the biologically important subclasses that are present .
for any class , our idea is to apply r - means clusters to the samples in that class , resulting in r subclasses for that class .
doing this for each of the k classes results in a total of k r subclasses .
we apply nearest
shrunken centroids to this r k class problem .
if the predicted class from this large problem is h , then our nal predicted class is the class k that contains with typical sample sizes , the choice r = 123 will be most reasonable .
table 123 shows the results on the national cancer institute ( nci ) data .
without subclasses , the test error rates start to rise when fewer than 123 , 123 or 123 , 123 genes are used .
using subclasses , we achieve about the same error rate with as few as 123 genes .
the right part of the table shows that for 123 the subclasses are most important for bre , cns , col , mel and ren .
the 123 gene solution is displayed in figure 123 and shows some distinct subclasses among some of the main classes .
capturing heterogeneity
in discriminating an abnormal from a normal group , the average gene expression may not differ between the groups .
however , the variability in expres - sion may be greater in the abnormal group , due to het - erogeneity in the abnormal population .
this is illus - trated in figure 123
nearest centroid classication will not work in this case , since the class centroids are not separated .
the subclass method of the previous section might help : we propose an alternative approach here .
= |xij mi| , where mi is the mean expression for gene i in the normal group .
then we apply nearest shrunken centroids to the new
we dene new features x
to illustrate this , we generated the expression of 123 , 123 genes in 123 samples123 from a normal group
two class problem with distinct subclasses .
numbers indicate the average gene expression .
tibshirani , hastie , narasimhan and chu
nci subclass results .
shown are pairs of centroids for each class for the genes that survived the thresholding .
and 123 from an abnormal group .
all expression values were generated independently as standard gaussian except for the rst 123 genes in the abnormal group , which had mean zero , but standard deviation 123
an independent test set of size 123 was also generated .
illustration of heterogeneity in gene expression .
abnor - mal group a has the same average gene expression as the normal group n , but shows larger variability .
nearest centroid shrinkage on the transformed features ij showed a test error rate of near zero , with 123 or more nonzero genes .
figure 123 compares the results of nearest shrunken centroids on the raw expression values xij and the transformed expression values x nearest centroid shrinkage on the raw values does poorly with an error rate greater than 123% , while use of the transformed values reduces the error rate to near
by transforming to the distance from the normal ij might also provide centroid , the use of the features x discrimination in situations where the abnormal class is not heterogeneous , but is instead mean - shifted .
the right panel of figure 123 investigates this .
the expression of the rst 123 genes in the abnormal class has mean 123 and standard deviation 123 ( versus 123 and 123 for the normal class ) .
now nearest shrunken centroids on the raw features is much more powerful , while use of the transformed features works poorly .
we conclude that use of neither the raw nor transformed features dominates the other , and both should be tried on a given
we have successfully used the heterogeneity model toxicity from radiation sensitivity using transcriptional responses to dna damage in lymphoid cells ( rieger et al . , 123 ) .
class prediction by nearest shrunken centroids
left : test error for data simulated from the heterogeneous two - class problem , using nearest shrunken centroids on raw expression values ( red ) and transformed expression values |xij mi| ( blue ) .
right : same as in left panel , but data are simulated from the mean - shifted
homogeneous two - class problem .
relationship to other approaches
) = ( x
the discriminant scores ( 123 ) are similar to those used in linear discriminant analysis ( lda ) , which arise from using the mahalanobis metric to compute the distance xk ) 123 log k .
here we use vector notation and w is the pooled within - class covariance matrix .
with thousands of genes and tens of samples ( p ( cid : 123 ) n ) , w is huge and any sample estimate will be singular ( and hence its inverse is undened ) .
our scores can be seen to be a heavily restricted form of lda , necessary to cope with the large number of variables ( genes ) .
the differences are the following : we assume a diagonal within - class covariance ma - trix for w ; without this , lda would be ill - condition - ed and fail .
we use shrunken centroids rather than centroids as a prototype for each class .
as the shrinkage parameter increases , an increas - ing number of genes will have all their d k = 123 , .
, k , due to the soft thresholding in ( 123 ) .
such genes contribute no discriminatory information in ( 123 ) , and in fact cancel in equation ( 123 ) .
both our scores ( 123 ) and the lda scores ( 123 ) are
linear in x i .
if we expand the square in ( 123 ) , discard 123 ( since they are independent of the terms involving x the class index k and hence do not contribute toward class discrimination ) and multiply by 123 / 123 , we get
) = p ( cid : 123 )
+ log k ,
) = ( x
which is linear in x
xk ) t ( w + i )
discriminant scores have the equivalent linear form
rule classies to the largest k ( x 123 xk 123
because of the sign change , our ) .
likewise the lda regularized discriminant analysis ( rda; friedman , 123 ) leaves the centroids alone and modies the covariance matrix in a different way , where is a parameter ( like our ) .
the fattened w + i is nonsingular , and as gets large , this proce - dure approaches the nearest centroid procedure ( with no variance scaling or centroid shrinking ) .
a slightly modied version uses w +d , where d = diag ( s123 123 , s123 .
, s123 p ) .
as gets large , this approaches the variance weighted nearest centroid procedure .
in practice , we normalize this regularized covariance by dividing by 123 + , leading to the convex combination ( 123 ) w + d , where = / ( 123 + ) .
although the relative dis - tances do not change , this is important when making the adjustment for the class priors .
although rda shows some promise , it is more com - plicated than our nearest shrunken centroid procedure .
furthermore , in the process of its regularization , it does not select a subset of genes as the shrunken centroid procedure does .
we are considering other hybrid ap - proaches of rda and nearest centroids in ongoing re -
nearest centroid classifier
as discussed in the previous section , the nearest cen - troid classier is equivalent to fishers linear discrimi -
tibshirani , hastie , narasimhan and chu
simulation results : bias and variance ( top panels ) and mean - squared error and misclassication error ( bottom panels ) for linear discriminant analysis and the nearest centroid classier .
details of the simulation are given in the text .
the nearest centroid classier outperforms lda because of its smaller variance .
nant analysis if we restrict the within - class covariance matrix to be diagonal .
when is this restriction a good
consider a two class microarray problem with p genes and n samples .
for simplicity we consider the standard ( unshrunken ) nearest centroid classier and standard ( full within covariance ) lda .
the recent the - sis of levina ( 123 ) did some theoretical comparisons of these methods .
she assumed p , n and p / n ( 123 , 123 ) , and analyzed the worst case error of each method .
the relative performance of the two methods depends on the correlation structure of the features ( samples ) .
her results show that if p is a large fraction of n , for a large class of correlation structures , nearest centroid classication outperforms full lda .
now in our problem , usually we have p ( cid : 123 ) n : in that case , lda is not even dened without some regular - ization .
hence to proceed we assume that p is a little less than n and hope that what we learn will extend to the case p > n .
let xj be a p - vector of gene ex - pression values in class j .
suppose x123 n ( 123 , ) and x123 n ( , ) , where is a full ( nondiagonal ) matrix .
then lda uses the maximum likelihood unbiased esti - 123 , while nearest centroid uses a biased es - 123 in a timate .
however , the lda method estimates
multivariate manner , and hence will tend to have higher variance .
what is the resulting biasvariance tradeoff and how does it translate into misclassication error ? we did an experiment with p = 123 and n = 123 , with 123 samples in each of two classes .
we set the ij th ele - , where was varied from 123 to 123 .
ment of to each of the components of the mean vector was set to 123 at random : such a mixed vector is needed to give full lda a potential advantage over lda with a di - agonal covariance .
for each simulation , an indepen - dent test set of size 123 was also generated .
the re - sults of 123 simulations from this model are shown in figure 123
bias , variance and mean - squared error refer 123
for small correlations , the un - to estimation of derlying ( diagonal covariance ) model for nearest cen - troids is approximately correct and the method wins; lda shows a small improvement in bias for larger correlations , but this is more than offset by the in - creased variance .
overall the nearest centroid method has lower mean - squared error and test misclassication error in all cases .
now for real microarray problems , p ( cid : 123 ) n , and both lda and nearest centroid methods can be improved by appropriate regularization or shrinkage .
we have
class prediction by nearest shrunken centroids
not included regularization in the above comparison , but the above results suggest that the biasvariance tradeoff will cause the nearest centroid method to outperform full lda .
the nearest shrunken centroid classier is poten - in any high - dimensional classication problem .
in addition to its application to gene expres - sion arrays , it could also be applied to other kinds of emerging genomic data , including mass spectroscopy for protein measurements , tissue arrays and single nu - cleotide polymorphism arrays .
our proposal can also be applied in conjunction with unsupervised methods .
for example , it is now standard to use hierarchical clustering methods on expression arrays to discover clusters in the samples ( eisen , spellman , brown and botstein , 123 ) .
the methods described here can identify subsets of the genes that succinctly characterize each cluster .
finally , we touch on computational
computations involved in the nearest shrunken centroid method are straightforward .
one important detail : in the denominator of the statistics dik in equation ( 123 ) we add the same positive constant s123 to each of the si values .
this guards against the possibility of large dik values arising by chance from genes at very low expression levels .
we set s123 equal to the median value of the si over the set of genes .
a similar strategy was used in the signicance analysis of microarrays ( sam ) methodology of tusher , tibshirani and chu ( 123 ) .
we have developed a package in the excel and r language called prediction analysis for microarrays .
it implements all of the nearest shrunken centroids methodology discussed in this article and is available at the website http : / / www - stat . stanford . edu / tibs / pam .
