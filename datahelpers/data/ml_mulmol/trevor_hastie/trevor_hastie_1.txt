the standard 123 - norm svm is known for its good performance in two - in this paper , we consider the 123 - norm svm .
we argue that the 123 - norm svm may have some advantage over the standard 123 - norm svm , especially when there are redundant noise features .
we also propose an efcient algorithm that computes the whole solution path of the 123 - norm svm , hence facilitates adaptive selection of the tuning parameter for the 123 - norm svm .
in standard two - class classication problems , we are given a set of training data ( x123 , y123 ) , .
( xn , yn ) , where the input xi rp , and the output yi ( 123 , 123 ) is binary .
we wish to nd a classcation rule from the training data , so that when given a new input x , we can assign a class y from ( 123 , 123 ) to it .
to handle this problem , we consider the 123 - norm support vector machine ( svm ) : ( cid : 123 ) ( cid : 123 ) 123 = |123| + + |q| s ,
where d = ( h123 ( x ) , .
hq ( x ) ) is a dictionary of basis functions , and s is a tuning parame - ter .
the solution is denoted as 123 ( s ) and ( s ) ; the tted model is
f ( x ) = 123 +
the classication rule is given by sign ( f ( x ) ) .
the 123 - norm svm has been successfully used in ( 123 ) and ( 123 ) .
we argue in this paper that the 123 - norm svm may have some advantage over the standard 123 - norm svm , especially when there are redundant noise features .
to get a good tted model f ( x ) that performs well on future data , we also need to select an appropriate tuning parameter s .
in practice , people usually pre - specify a nite set of values for s that covers a wide range , then either use a separate validation data set or use
cross - validation to select a value for s that gives the best performance among the given set .
in this paper , we illustrate that the solution path ( s ) is piece - wise linear as a function of s ( in the rq space ) ; we also propose an efcient algorithm to compute the exact whole solution path ( ( s ) , 123 s ) , hence help us understand how the solution changes with s and facilitate the adaptive selection of the tuning parameter s .
under some mild assumptions , we show that the computational cost to compute the whole solution path ( s ) is o ( nq min ( n , q ) 123 ) in the worst case and o ( nq ) in the best case .
before delving into the technical details , we illustrate the concept of piece - wise linearity of the solution path ( s ) with a simple example .
we generate 123 training data in each of two classes .
the rst class has two standard normal independent inputs x123 , x123
the second class also has two standard normal independent inputs , but conditioned on 123 x123
the dictionary of basis functions is d = ( ) .
the solution path ( s ) as a function of s is shown in figure 123
any segment between two adjacent vertical lines is linear .
hence the right derivative of ( s ) with respect to s is piece - wise constant ( in rq ) .
the two solid paths are for x123 123 , which are the two relevant features .
123 and x123
figure 123 : the solution path ( s ) as a function of s .
in section 123 , we motivate why we are interested in the 123 - norm svm .
in section 123 , we describe the algorithm that computes the whole solution path ( s ) .
in section 123 , we show some numerical results on both simulation data and real world data .
123 regularized support vector machines
the standard 123 - norm svm is equivalent to t a model that
where is a tuning parameter .
in practice , people usually choose hj ( x ) s to be the basis functions of a reproducing kernel hilbert space .
then a kernel trick allows the dimension of the transformed feature space to be very large , even innite in some cases ( i . e .
q = ) , without causing extra computational burden ( ( 123 ) and ( 123 ) ) .
in this paper , however , we will concentrate on the basis representation ( 123 ) rather than a kernel representation .
notice that ( 123 ) has the form loss + penalty , and is the tuning parameter that controls the tradeoff between loss and penalty .
the loss ( 123 yf ) + is called the hinge loss , and
the penalty is called the ridge penalty .
the idea of penalizing by the sum - of - squares of the parameters is also used in neural networks , where it is known as weight decay .
the ridge penalty shrinks the tted coefcients towards zero .
it is well known that this shrinkage has the effect of controlling the variances of , hence possibly improves the tted models prediction accuracy , especially when there are many highly correlated features ( 123 ) .
so from a statistical function estimation point of view , the ridge penalty could possibly explain the success of the svm ( ( 123 ) and ( 123 ) ) .
on the other hand , computational learning theory has associated the good performance of the svm to its margin maximizing property ( 123 ) , a property of the hinge loss .
( 123 ) makes some effort to build a connection between these two in this paper , we replace the ridge penalty in ( 123 ) with the l123 - norm of , i . e .
penalty ( 123 ) , and consider the 123 - norm svm problem :
which is an equivalent lagrange version of the optimization problem ( 123 ) - ( 123 ) .
the lasso penalty was rst proposed in ( 123 ) for regression problems , where the response y is continuous rather than categorical .
it has also been used in ( 123 ) and ( 123 ) for classication problems under the framework of svms .
similar to the ridge penalty , the lasso penalty also shrinks the tted coefcients s towards zero , hence ( 123 ) also benets from the reduction in tted coefcients variances .
another property of the lasso penalty is that because of the l123 nature of the penalty , making sufciently large , or equivalently s sufciently small , will cause some of the coefcients js to be exactly zero .
for example , when s = 123 in figure 123 , only three tted coefcients are non - zero .
thus the lasso penalty does a kind of continuous feature selection , while this is not the case for the ridge penalty .
in ( 123 ) , none of the js will be equal to zero .
it is interesting to note that the ridge penalty corresponds to a gaussian prior for the js , while the lasso penalty corresponds to a double - exponential prior .
the double - exponential density has heavier tails than the gaussian density .
this reects the greater tendency of the lasso to produce some large tted coefcients and leave others at 123 , especially in high dimensional problems .
recently , ( 123 ) consider a situation where we have a small number of training data , e . g .
n = 123 , and a large number of basis functions , e . g .
q = 123 , 123
( 123 ) argue that in the sparse scenario , i . e .
only a small number of true coefcients js are non - zero , the lasso penalty works better than the ridge penalty; while in the non - sparse scenario , the true coefcients js have a gaussian distribution , neither the lasso penalty nor the ridge penalty will t the coefcients well , since there is too little data from which to estimate these non - zero coefcients .
this is the curse of dimensionality taking its toll .
based on these observations , ( 123 ) further propose the bet on sparsity principle for high - dimensional problems , which encourages using lasso penalty .
section 123 gives the motivation why we are interested in the 123 - norm svm .
to solve the 123 - norm svm for a xed value of s , we can transform ( 123 ) - ( 123 ) into a linear programming problem and use standard software packages; but to get a good tted model f ( x ) that performs well on future data , we need to select an appropriate value for the tuning paramter s .
in this section , we propose an efcient algorithm that computes the whole solution path ( s ) , hence facilitates adaptive selection of s .
123 piece - wise linearity
if we follow the solution path ( s ) of ( 123 ) - ( 123 ) as s increases , we will notice that since both i ( 123 yi fi ) + and ( cid : 123 ) ( cid : 123 ) 123 are piece - wise linear , the karush - kuhn - tucker conditions will not change when s increases unless a residual ( 123 yi fi ) changes from non - zero to zero , or a tted coefcient j ( s ) changes from non - zero to zero , which correspond to the non - i ( 123 yi fi ) + and ( cid : 123 ) ( cid : 123 ) 123
this implies that the derivative of ( s ) with smooth points of respect to s is piece - wise constant , because when the karush - kuhn - tucker conditions do not change , the derivative of ( s ) will not change either .
hence it indicates that the whole solution path ( s ) is piece - wise linear .
see ( 123 ) for details .
thus to compute the whole solution path ( s ) , all we need to do is to nd the joints , i . e .
the asterisk points in figure 123 , on this piece - wise linear path , then use straight lines to interpolate them , or equivalently , to start at ( 123 ) = 123 , nd the right derivative of ( s ) , let s increase and only change the derivative when ( s ) gets to a joint .
initial solution ( i . e .
s = 123 )
the following notation is used .
let v = ( j : j ( s ) ( cid : 123 ) = 123 ) , e = ( i : 123 yi fi = 123 ) , l = ( i : 123 yi fi > 123 ) and u for the right derivative of v ( s ) : ( cid : 123 ) u ( cid : 123 ) 123 = 123 and v ( s ) denotes the components of ( s ) with indices in v .
without loss of generality , we assume # ( yi = 123 ) # ( yi = 123 ) ; then 123 ( 123 ) = 123 , j ( 123 ) = 123
to compute the path that ( s ) follows , we need to compute the derivative of ( s ) at 123
we consider a modied problem :
( 123 yifi ) + +
( cid : 123 ) ( cid : 123 ) 123 s , fi = 123 +
notice that if yi = 123 , the loss is still ( 123 yifi ) +; but if yi = 123 , the loss becomes ( 123 yifi ) .
in this setup , the derivative of ( s ) with respect to s is the same no matter what value s is , and one can show that it coincides with the right derivative of ( s ) when s is sufciently small .
hence this setup helps us nd the initial derivative u of ( s ) .
solving ( 123 ) - ( 123 ) , which can be transformed into a simple linear programming problem , we get initial v , e and l .
|v| should be equal to |e| .
we also have :
s starts at 123 and increases .
123 main algorithm
the main algorithm that computes the whole solution path ( s ) proceeds as following :
increase s until one of the following two events happens :
a training point hits e , i . e .
123 yifi ( cid : 123 ) = 123 becomes 123 yifi = 123 for some i .
a basis function in v leaves v , i . e .
j ( cid : 123 ) = 123 becomes j = 123 for some j .
let the current 123 , and s be denoted by old
123 , old and sold .
for each j
where u123 , uj and uj are the unknowns .
we then compute :
) uj + |uj|
/ v , we solve :
v ujhj ( xi ) + ujhj ( xi ) = 123 for i e v sign ( old ( cid : 123 ) e , we solve : v sign ( old
v ujhj ( xi ) = 123 for i e\ ( i
ujhj ( xi ) + ujhj ( xi )
) uj = 123
for each i
where u123 and uj are the unknowns .
we then compute :
from step 123 and step 123
there are q|v|+
compare the computed values of loss
|e| = q + 123 such values .
choose the smallest negative loss
if the smallest loss if the smallest negative loss
s corresponds to a j
is non - negative , the algorithm terminates; else
hence ,
in step 123 , we update
v v ( j
if the smallest negative loss s corresponds to a i ( cid : 123 ) ) , l l ( i in either of the last two cases , ( s ) changes as :
123 ( sold + s ) v ( sold + s )
in step 123 , we update u and
( cid : 123 ) ) if necessary .
and we go back to step 123
in the end , we get a path ( s ) , which is piece - wise linear .
due to the page limit , we omit the proof that this algorithm does indeed give the exact whole solution path ( s ) of ( 123 ) - ( 123 ) ( see ( 123 ) for detailed proof ) .
instead , we explain a little what each step of the algorithm tries to do .
step 123 of the algorithm indicates that ( s ) gets to a joint on the solution path and the right derivative of ( s ) needs to be changed if either a residual ( 123 yi fi ) changes from non - zero to zero , or the coefcient of a basis function j ( s ) changes from non - zero to zero , when s increases .
then there are two possible types of actions that the algorithm can take : ( 123 ) add a basis function into v , or ( 123 ) remove a point from e .
step 123 computes the possible right derivative of ( s ) if adding each basis function hj ( x ) into v .
step 123 computes the possible right derivative of ( s ) if removing each point i from e .
the possible right derivative of ( s ) ( determined by either ( 123 ) or ( 123 ) ) is such that the training points in e are kept in e when s increases , until the next joint ( step 123 ) occurs .
loss / s indicates how fast the loss will decrease if ( s ) changes according to u .
step 123 takes the action corresponding to the smallest negative loss / s .
when the loss can not be decreased , the algorithm terminates .
test error ( se )
table 123 : simulation results of 123 - norm and 123 - norm svm
123 no noise input 123 noise inputs 123 noise inputs 123 noise inputs 123 noise inputs
123 computational cost we have proposed an algorithm that computes the whole solution path ( s ) .
a natural question is then what is the computational cost of this algorithm ? suppose |e| = m at a joint on the piece - wise linear solution path , then it takes o ( qm123 ) to compute step 123 and step 123 of the algorithm through sherman - morrison updating formula .
if we assume the training data are separable by the dictionary d , then all the training data are eventually going to have loss ( 123 yi fi ) + equal to zero .
hence it is reasonable to assume the number of joints on the piece - wise linear solution path is o ( n ) .
since the maximum value of m is min ( n , q ) and the minimum value of m is 123 , we get the worst computational cost is o ( nq min ( n , q ) 123 ) and the best computational cost is o ( nq ) .
notice that this is a rough calculation of the computational cost under some mild assumptions .
simulation results ( section 123 ) actually indicate that the number of joints tends to be o ( min ( n , q ) ) .
123 numerical results
in this section , we use both simulation and real data results to illustrate the 123 - norm svm .
123 simulation results
the data generation mechanism is the same as the one described in section 123 , except that we generate 123 training data in each of two classes , and to make harder problems , we sequentially augment the inputs with additional two , four , six and eight standard normal noise inputs .
hence the second class almost completely surrounds the rst , like the skin surrounding the oragne , in a two - dimensional subspace .
the bayes error rate for this prob - lem is 123 , irrespective of dimension .
in the original input space , a hyperplane cannot nomial kernel , hence the dictionary of basis functions is d = ( separate the classes; we use an enlarged feature space corresponding to the 123nd degree poly - 123 , .
we generate 123 test data to compare the 123 - norm svm and the standard 123 - norm svm .
the average test errors over 123 simulations , with different numbers of noise inputs , are shown in table 123
for both the 123 - norm svm and the 123 - norm svm , we choose the tuning parameters to minimize the test error , to be as fair as possible to each method .
for comparison , we also include the results for the non - penalized svm .
123xjxj ( cid : 123 ) , x123
j , j , j
from table 123 we can see that the non - penalized svm performs signicantly worse than the penalized ones; the 123 - norm svm and the 123 - norm svm perform similarly when there is no noise input ( line 123 ) , but the 123 - norm svm is adversely affected by noise inputs ( line 123 - line 123 ) .
since the 123 - norm svm has the ability to select relevant features and ignore redundant features , it does not suffer from the noise inputs as much as the 123 - norm svm .
table 123 also shows the number of basis functions q and the number of joints on the piece - wise linear solution path .
notice that q < n and there is a striking linear relationship between |d| and #joints ( figure 123 ) .
figure 123 also shows the 123 - norm svm result for one simulation .
number of bases
figure 123 : left and middle panels : 123 - norm svm when there are 123 noise inputs .
the left panel is the piece - wise linear solution path ( s ) .
the two upper paths correspond to x123 123 , which are the relevant features .
the middle panel is the test error along the solution path .
the dash lines correspond to the minimum of the test error .
the right panel illustrates the linear relationship between the number of basis functions and the number of joints on the solution path when q < n .
123 and x123
123 real data results
in this section , we apply the 123 - norm svm to classication of gene microarrays .
classi - cation of patient samples is an important aspect of cancer diagnosis and treatment .
the 123 - norm svm has been successfully applied to microarray cancer diagnosis problems ( ( 123 ) and ( 123 ) ) .
however , one weakness of the 123 - norm svm is that it only predicts a cancer class label but does not automatically select relevant genes for the classication .
often a primary goal in microarray cancer diagnosis is to identify the genes responsible for the classica - tion , rather than class prediction .
( 123 ) and ( 123 ) have proposed gene selection methods , which we call univariate ranking ( ur ) and recursive feature elimination ( rfe ) ( see ( 123 ) ) , that can be combined with the 123 - norm svm .
however , these procedures are two - step procedures that depend on external gene selection methods .
on the other hand , the 123 - norm svm has an inherent gene ( feature ) selection property due to the lasso penalty .
hence the 123 - norm svm achieves the goals of classication of patients and selection of genes simultaneously .
we apply the 123 - norm svm to leukemia data ( 123 ) .
this data set consists of 123 training data and 123 test data of two types of acute leukemia , acute myeloid leukemia ( aml ) and acute lymphoblastic leukemia ( all ) .
each datum is a vector of p = 123 , 123 genes .
we use the the jth genes expression level , as the basis function , i . e .
original input xj , i . e .
the tuning parameter is chosen according to 123 - fold cross - validation , then the nal model is tted on all the training data and evaluated on the test data .
the number of joints on the solution path is 123 , which appears to be o ( n ) ( cid : 123 ) o ( q ) .
the results are summarized in table 123
we can see that the 123 - norm svm performs similarly to the other methods in classication and it has the advantage of automatically selecting relevant genes .
we should notice that the maximum number of genes that the 123 - norm svm can select is upper bounded by n , which is usually much less than q in microarray problems .
we have considered the 123 - norm svm in this paper .
we illustrate that the 123 - norm svm may have some advantage over the 123 - norm svm , especially when there are redundant features .
the solution path ( s ) of the 123 - norm svm is a piece - wise linear function in the tuning
table 123 : results on microarray classication
123 - norm svm ur 123 - norm svm rfe
cv error test error
# of genes
parameter s .
we have proposed an efcient algorithm to compute the whole solution path ( s ) of the 123 - norm svm , and facilitate adaptive selection of the tuning parameter s .
hastie was partially supported by nsf grant dms - 123 , and nih grant roi - ca - 123 - 123
tibshirani was partially supported by nsf grant dms - 123 , and nih grant
