in this paper we study boosting methods from a new perspective .
we build on recent work by efron et al .
to show that boosting approximately ( and in some cases exactly ) minimizes its loss criterion with an l123 constraint on the coefcient vector .
this helps understand the success of boosting with early stopping as regularized tting of the loss criterion .
for the two most commonly used crite - ria ( exponential and binomial log - likelihood ) , we further show that as the constraint is relaxedor equivalently as the boosting iterations proceedthe solution converges ( in the separable case ) to an l123 - optimal separating hyper - plane .
we prove that this l123 - optimal separating hyper - plane has the property of maximizing the minimal l123 - margin of the training data , as dened in the boosting liter - ature .
an interesting fundamental similarity between boosting and kernel support vector machines emerges , as both can be described as methods for regularized optimization in high - dimensional predictor space , using a computational trick to make the calculation practical , and converging to margin - maximizing solutions .
while this statement describes svms exactly , it applies to boosting keywords : boosting , regularized optimization , support vector machines , margin maximization
introduction and outline
boosting is a method for iteratively building an additive model
ft ( x ) =
th jt ( x ) ;
where h jt 123 h a large ( but we will assume nite ) dictionary of candidate predictors or weak learners; and h jt is the basis function selected as the best candidate to modify the function at stage t .
the model ft can equivalently be represented by assigning a coefcient to each dictionary
c ( cid : 123 ) 123 saharon rosset , ji zhu and trevor hastie .
rosset , zhu and hastie
function h 123 h rather than to the selected h jt s only :
ft ( x ) =
h j ( x ) ( cid : 123 ) b ( t )
where j = jh j and b ( t ) vector b ( t ) as a vector in r j or , equivalently , as the hyper - plane which has b interpretation will play a key role in our exposition .
the b representation allows us to interpret the coefcient ( t ) as its normal
j = ( cid : 123 )
jt = j a
some examples of common dictionaries are : ( cid : 123 ) the training variables themselves , in which case h j ( x ) = x j .
this leads to our additive model ft being just a linear model in the original data .
the number of dictionary functions will be j = d , the dimension of x .
( cid : 123 ) polynomial dictionary of degree p , in which case the number of dictionary functions will be
j = ( cid : 123 ) p + d
( cid : 123 ) decision trees with up to k terminal nodes , if we limit the split points to data points ( or mid - way between data points as cart does ) .
the number of possible trees is bounded from above ( trivially ) by j ( cid : 123 ) ( np ) k ( cid : 123 ) 123k123
note that regression trees do not t into our framework , since they will give j =
the boosting idea was rst introduced by freund and schapire ( 123 ) , with their adaboost algorithm .
adaboost and other boosting algorithms have attracted a lot of attention due to their great success in data modeling tasks , and the mechanism which makes them work has been presented and analyzed from several perspectives .
friedman et al .
( 123 ) develop a statistical perspective , which ultimately leads to viewing adaboost as a gradient - based incremental search for a good additive model ( more specically , it is a coordinate descent algorithm ) , using the exponential loss function c ( y; f ) = exp ( ( cid : 123 ) yf ) , where y 123 f ( cid : 123 ) 123;123g .
the gradient boosting ( friedman , 123 ) and anyboost ( mason et al . , 123 ) generic algorithms have used this approach to generalize the boosting idea to wider families of problems and loss functions .
in particular , friedman et al .
( 123 ) have pointed out that the binomial log - likelihood loss c ( y; f ) = log ( 123 + exp ( ( cid : 123 ) yf ) ) is a more natural loss for classication , and is more robust to outliers and misspecied data .
a different analysis of boosting , originating in the machine learning community , concentrates on the effect of boosting on the margins yif ( xi ) .
for example , schapire et al .
( 123 ) use margin - based arguments to prove convergence of boosting to perfect classication performance on the training data under general conditions , and to derive bounds on the generalization error ( on future , unseen
in this paper we combine the two approaches , to conclude that gradient - based boosting can be described , in the separable case , as an approximate margin maximizing process .
the view we de - velop of boosting as an approximate path of optimal solutions to regularized problems also justies early stopping in boosting as specifying a value for regularization parameter .
we consider the problem of minimizing non - negative convex loss functions ( in particular the exponential and binomial log - likelihood loss functions ) over the training data , with an l123 bound on the model coefcients :
b ( c ) = arg min
c ( yi; h ( xi ) 123b ) :
boosting as a regularized path
where h ( xi ) = ( h123 ( xi ) ; h123 ( xi ) ; : : : ; hj ( xi ) ) 123 and j = jh j . 123
; 123t in ( 123 ) , with e
hastie et al .
( 123 , chapter 123 ) have observed that slow gradient - based boosting ( i . e . , we set t = e small ) tends to follow the penalized path b ( c ) as a function of c , under some mild conditions on this path .
in other words , using the notation of ( 123 ) , ( 123 ) , this implies that kb ( c=e ) ( cid : 123 ) b ( c ) k vanishes with e , for all ( or a wide range of ) values of c .
figure 123 illustrates this equivalence between e - boosting and the optimal solution of ( 123 ) on a real - life data set , using squared error loss as the loss function .
in this paper we demonstrate this equivalence further and formally
figure 123 : exact coefcient paths ( left ) for l123 - constrained squared error regression and boosting
coefcient paths ( right ) on the data from a prostate cancer study
state it as a conjecture .
some progress towards proving this conjecture has been made by efron et al .
( 123 ) , who prove a weaker local result for the case where c is squared error loss , under some mild conditions on the optimal path .
we generalize their result to general convex loss functions .
combining the empirical and theoretical evidence , we conclude that boosting can be viewed as
an approximate incremental method for following the l123 - regularized path .
we then prove that in the separable case , for both the exponential and logistic log - likelihood
loss functions , b ( c ) =c converges as c !
to an optimal separating hyper - plane b described by
b = arg max
in other words , b maximizes the minimal margin among all vectors with l123 - norm equal to 123 this result generalizes easily to other lp - norm constraints .
for example , if p = 123 , then b describes the optimal separating hyper - plane in the euclidean sense , i . e . , the same one that a non - regularized support vector machine would nd .
combining our two main results , we get the following characterization of boosting :
our notation assumes that the minimum in ( 123 ) is unique , which requires some mild assumptions .
to avoid notational complications we use this slightly abusive notation throughout this paper .
in appendix b we give explicit conditions for uniqueness of this minimum .
the margin maximizing hyper - plane in ( 123 ) may not be unique , and we show that in that case the limit b
is still dened
and it also maximizes the second minimal margin .
see appendix b . 123 for details .
rosset , zhu and hastie
e - boosting can be described as a gradient - descent search , approximately following the path of l123 - constrained optimal solutions to its loss criterion , and converging , in the separable case , to a margin maximizer in the l123 sense .
note that boosting with a large dictionary h ( in particular if n < j = jh j ) guarantees that the data will be separable ( except for pathologies ) , hence separability is a very mild assumption here .
as in the case of support vector machines in high dimensional feature spaces , the non - regularized optimal separating hyper - plane is usually of theoretical interest only , since it typically represents an over - tted model .
thus , we would want to choose a good regularized model .
our results indicate that boosting gives a natural method for doing that , by stopping early in the boosting process .
fur - thermore , they point out the fundamental similarity between boosting and svms : both approaches allow us to t regularized models in high - dimensional predictor space , using a computational trick .
they differ in the regularization approach they takeexact l123 regularization for svms , approximate l123 regularization for boosting - and in the computational trick that facilitates ttingthe kernel trick for svms , coordinate descent for boosting .
123 related work
schapire et al .
( 123 ) have identied the normalized margins as distance from an l123 - normed sep - arating hyper - plane .
their results relate the boosting iterations success to the minimal margin of the combined model .
ratsch et al .
( 123b ) take this further using an asymptotic analysis of ad - aboost .
they prove that the normalized minimal margin , mini yi ( cid : 123 ) tj , is asymptoti - cally equal for both classes .
in other words , they prove that the asymptotic separating hyper - plane is equally far away from the closest points on either side .
this is a property of the margin maximizing separating hyper - plane as we dene it .
both papers also illustrate the margin maximizing effects of adaboost through experimentation .
however , they both stop short of proving the convergence to optimal ( margin maximizing ) solutions .
motivated by our result , ratsch and warmuth ( 123 ) have recently asserted the margin - maximizing
properties of e - adaboost , using a different approach than the one used in this paper .
their results relate only to the asymptotic convergence of innitesimal adaboost , compared to our analysis of the regularized path traced along the way and of a variety of boosting loss functions , which also leads to a convergence result on binomial log - likelihood loss .
the convergence of boosting to an optimal solution from a loss function perspective has been analyzed in several papers .
ratsch et al .
( 123a ) and collins et al .
( 123 ) give results and bounds on the convergence of training - set loss , ( cid : 123 ) tht ( xi ) ) , to its minimum .
however , in the separable case convergence of the loss to 123 is inherently different from convergence of the linear separator to the optimal separator .
any solution which separates the two classes perfectly can drive the expo - nential ( or log - likelihood ) loss to 123 , simply by scaling coefcients up linearly .
two recent papers have made the connection between boosting and l123 regularization in a slightly different context than this paper .
zhang ( 123 ) suggests a shrinkage version of boosting which converges to l123 regularized solutions , while zhang and yu ( 123 ) illustrate the quantitative relation - ship between early stopping in boosting and l123 constraints .
boosting as a regularized path
boosting as gradient descent
generic gradient - based boosting algorithms ( friedman , 123; mason et al . , 123 ) attempt to nd a good linear combination of the members of some dictionary of basis functions to optimize a given loss function over a sample .
this is done by searching , at each iteration , for the basis function which gives the steepest descent in the loss , and changing its coefcient accordingly .
in other words , this is a coordinate descent algorithm in rj , where we assign one dimension ( or coordinate ) for the coefcient of each dictionary function .
assume we have data fxi; yign
i=123 with xi 123 rd , a loss ( or cost ) function c ( y; f ) , and a set of dictionary functions fh j ( x ) g : rd ! r .
then all of these algorithms follow the same essential
algorithm 123 generic gradient - based boosting algorithm
set b ( 123 ) = 123
for t = 123 : t ,
( a ) let fi = b ( t ( cid : 123 ) 123 ) 123h ( xi ) ; i = 123; : : : ; n ( the current t ) .
( b ) set wi = ( c ) identify jt = argmax j j ( cid : 123 ) ( d ) set b ( t )
i wih jt ( xi ) ) and b ( t )
; i = 123; : : : ; n .
jt = b ( t ( cid : 123 ) 123 )
jt ( cid : 123 ) a
i wih j ( xi ) j .
k = b ( t ( cid : 123 ) 123 )
; k 123= jt .
here b ( t ) is the current coefcient vector and a
t > 123 is the current step size .
notice that ( cid : 123 )
i wih jt ( xi ) =
as we mentioned , algorithm 123 can be interpreted simply as a coordinate descent algorithm in weak learner space .
implementation details include the dictionary h of weak learners , the loss function c ( y; f ) , the method of searching for the optimal jt and the way in which a t is determined . 123 for example , the original adaboost algorithm uses this scheme with the exponential loss c ( y; f ) = exp ( ( cid : 123 ) yf ) , and an implicit line search to nd the best a t once a direction jt has been chosen ( see hastie et al . , 123; mason et al . , 123 ) .
the dictionary used by adaboost in this formulation would be a set of candidate classiers , i . e . , h j ( xi ) 123 f ( cid : 123 ) 123; +123gusually decision trees are used in practice .
123 practical implementation of boosting
the dictionaries used for boosting are typically very largepractically inniteand therefore the generic boosting algorithm we have presented cannot be implemented verbatim .
in particular , it is not practical to exhaustively search for the maximizer in step 123 ( c ) .
instead , an approximate , usually greedy search is conducted to nd a good candidate weak learner h jt which makes the rst order decline in the loss large ( even if not maximal among all possible models ) .
in the common case that the dictionary of weak learners is comprised of decision trees with up to k nodes , the way adaboost and other boosting algorithms solve stage 123 ( c ) is by building a
the sign of a
t will always be ( cid : 123 ) sign ( ( cid : 123 )
i wih jt ( xi ) ) , since we want the loss to be reduced .
in most cases , the dictionary
h is negation closed , and so it can be assumed wlog that the coefcients are always positive and increasing
rosset , zhu and hastie
decision tree to a re - weighted version of the data , with the weights jwij .
thus they rst replace step 123 ( c ) with minimization of
jwij123fyi 123= h jt ( xi ) g;
which is easily shown to be equivalent to the original step 123 ( c ) .
they then use a greedy decision - tree building algorithm such as cart or c123 to build a k - node decision tree which minimizes this quantity , i . e . , achieves low weighted misclassication error on the weighted data .
since the tree is built greedilyone split at a timeit will not be the global minimizer of weighted misclassication error among all k - node decision trees .
however , it will be a good t for the re - weighted data , and can be considered an approximation to the optimal tree .
this use of approximate optimization techniques is critical , since much of the strength of the boosting approach comes from its ability to build additive models in very high - dimensional predic - tor spaces .
in such spaces , standard exact optimization techniques are impractical : any approach which requires calculation and inversion of hessian matrices is completely out of the question , and even approaches which require only rst derivatives , such as coordinate descent , can only be
123 gradient - based boosting as a generic modeling tool
as friedman ( 123 ) ; mason et al .
( 123 ) mention , this view of boosting as gradient descent allows us to devise boosting algorithms for any function estimation problemall we need is an appro - priate loss and an appropriate dictionary of weak learners .
for example , friedman et al .
( 123 ) suggested using the binomial log - likelihood loss instead of the exponential loss of adaboost for binary classication , resulting in the logitboost algorithm .
however , there is no need to limit boosting algorithms to classicationfriedman ( 123 ) applied this methodology to regression es - timation , using squared error loss and regression trees , and rosset and segal ( 123 ) applied it to density estimation , using the log - likelihood criterion and bayesian networks as weak learners .
their experiments and those of others illustrate that the practical usefulness of this approachcoordinate descent in high dimensional predictor spacecarries beyond classication , and even beyond super -
the view we present in this paper , of coordinate - descent boosting as approximate l123 - regularized tting , offers some insight into why this approach would be good in general : it allows us to t regu - larized models directly in high dimensional predictor space .
in this it bears a conceptual similarity to support vector machines , which exactly t an l123 regularized model in high dimensional ( rkh )
123 loss functions
the two most commonly used loss functions for boosting classication models are the exponential and the ( minus ) binomial log - likelihood :
loglikelihood : cl ( y; f ) = log ( 123 + exp ( ( cid : 123 ) yf ) ) :
ce ( y; f ) = exp ( ( cid : 123 ) yf ) ;
these two loss functions bear some important similarities to each other .
as friedman et al .
( 123 ) show , the population minimizer of expected loss at point x is similar for both loss functions and is
boosting as a regularized path
figure 123 : the two classication loss functions
f ( x ) = c ( cid : 123 ) log ( cid : 123 ) p ( y = 123jx ) p ( y = ( cid : 123 ) 123jx ) ( cid : 123 ) ;
where ce = 123=123 for exponential loss and cl = 123 for binomial loss .
more importantly for our purpose , we have the following simple proposition , which illustrates the strong similarity between the two loss functions for positive margins ( i . e . , correct classica -
yf ( cid : 123 ) 123 ) 123 : 123ce ( y; f ) ( cid : 123 ) cl ( y; f ) ( cid : 123 ) ce ( y; f ) :
in other words , the two losses become similar if the margins are positive , and both behave like proof consider the functions f123 ( z ) = z and f123 ( z ) = log ( 123+z ) for z 123 ( 123;123 ) .
then f123 ( 123 ) = f123 ( 123 ) = 123 ,
thus we can conclude 123 : 123 f123 ( z ) ( cid : 123 ) f123 ( z ) ( cid : 123 ) f123 ( z ) .
now set z = exp ( ( cid : 123 ) y f ) and we get the desired
for negative margins the behaviors of ce and cl are very different , as friedman et al .
( 123 )
have noted .
in particular , cl is more robust against outliers and misspecied data .
123 line - search boosting vs .
e - boosting as mentioned above , adaboost determines a this would be
t using a line search .
in our notation for algorithm 123
t = argmina
c ( yi; fi + a h jt ( xi ) ) :
rosset , zhu and hastie
the alternative approach , suggested by friedman ( 123 ) ; hastie et al .
( 123 ) , is to shrink all a to a single small value e .
this may slow down learning considerably ( depending on how small e is ) , but is attractive theoretically : the rst - order theory underlying gradient boosting implies that the weak learner chosen is the best increment only locally .
it can also be argued that this ap - proach is stronger than line search , as we can keep selecting the same h jt repeatedly if it remains optimal and so e - boosting dominates line - search boosting in terms of training error .
in practice , this approach of slowing the learning rate usually performs better than line - search in terms of prediction error as well ( see friedman , 123 ) .
for our purposes , we will mostly assume e nitesimally small , so the theoretical boosting algorithm which results is the limit of a series of boosting algorithms with shrinking e .
in regression terminology , the line - search version is equivalent to forward stage - wise modeling , infamous in the statistics literature for being too greedy and highly unstable ( see friedman , 123 ) .
this is intuitively obvious , since by increasing the coefcient until it saturates we are destroying signal which may help us select other good predictors .
lp margins , support vector machines and boosting
we now introduce the concept of margins as a geometric interpretation of a binary classication model .
in the context of boosting , this view offers a different understanding of adaboost from the gradient descent view presented above .
in the following sections we connect the two views .
123 the euclidean margin and the support vector machine consider a classication model in high dimensional predictor space : f ( x ) = ( cid : 123 ) j .
we say that the model separates the training data fxi; yign i=123 if sign ( f ( xi ) ) = yi; 123i .
from a geometrical perspective this means that the hyper - plane dened by f ( x ) = 123 is a separating hyper - plane for this data , and we dene its ( euclidean ) margin as
j h j ( x ) b
m123 ( b ) = min
the margin - maximizing separating hyper - plane for this data would be dened by b which max - imizes m123 ( b ) .
figure 123 shows a simple example of separable data in two dimensions , with its margin - maximizing separating hyper - plane .
the euclidean margin - maximizing separating hyper - plane is the ( non regularized ) support vector machine solution .
its margin maximizing properties play a central role in deriving generalization error bounds for these models , and form the basis for a rich literature .
123 the l123 margin and its relation to boosting instead of considering the euclidean margin as in ( 123 ) we can dene an l p margin concept as
mp ( b ) = min
of particular interest to us is the case p = 123
figure 123 shows the l123 margin maximizing separating hyper - plane for the same simple example as figure 123
note the fundamental difference between
boosting as a regularized path
figure 123 : a simple data example , with two observations from class o and two observations from
class x .
the full line is the euclidean margin - maximizing separating hyper - plane .
figure 123 : l123 margin maximizing separating hyper - plane for the same data set as figure 123
the difference between the diagonal euclidean optimal separator and the vertical l123 optimal separator illustrates the sparsity effect of optimal l123 separation
rosset , zhu and hastie
the two solutions : the l123 - optimal separator is diagonal , while the l123 - optimal one is vertical .
to understand why this is so we can relate the two margin denitions to each other as
from this representation we can observe that the l123 margin will tend to be big if the ratio kb k123 big .
this ratio will generally be big if b is sparse .
to see this , consider xing the l123 norm of the vector and then comparing the l123 norm of two candidates : one with many small components and the othera sparse onewith a few large components and many zero components .
it is easy to see that the second vector will have bigger l123 norm , and hence ( if the l123 margin for both vectors is equal ) a bigger l123 margin .
a different perspective on the difference between the optimal solutions is given by a theorem due to mangasarian ( 123 ) , which states that the l p margin maximizing separating hyper plane maximizes the lq distance from the closest points to the separating hyper - plane , with 123 q = 123
thus the euclidean optimal separator ( p = 123 ) also maximizes euclidean distance between the points and the hyper - plane , while the l123 optimal separator maximizes l distance .
this interesting result gives another intuition why l123 optimal separating hyper - planes tend to be coordinate - oriented ( i . e . , have sparse representations ) : since l projection considers only the largest coordinate distance , some coordinate distances may be 123 at no cost of decreased l distance .
schapire et al .
( 123 ) have pointed out the relation between adaboost and the l123 margin .
they prove that , in the case of separable data , the boosting iterations increase the boosting margin of the model , dened as
in other words , this is the l123 margin of the model , except that it uses the a rather than the b geometric representation for the model .
the two representations give the same l123 norm if there is sign consistency , or monotonicity in the coefcient paths traced by the model , i . e . , if at every iteration t of the boosting algorithm
123= 123 ) sign ( a
t ) = sign ( b
as we will see later , this monotonicity condition will play an important role in the equivalence between boosting and l123 regularization .
the l123 - margin maximization view of adaboost presented by schapire et al .
( 123 ) and a whole plethora of papers that followedis important for the analysis of boosting algorithms for two distinct reasons :
( cid : 123 ) it gives an intuitive , geometric interpretation of the model that adaboost is looking fora model which separates the data well in this l123 - margin sense .
note that the view of boosting as gradient descent in a loss criterion doesnt really give the same kind of intuition : if the data is separable , then any model which separates the training data will drive the exponential or binomial loss to 123 when scaled up :
m123 ( b ) > 123 = ) ( cid : 123 )
123xi ) ! 123 as d !
boosting as a regularized path
( cid : 123 ) the l123 - margin behavior of a classication model on its training data facilitates generation of generalization ( or prediction ) error bounds , similar to those that exist for support vector machines ( schapire et al . , 123 ) .
the important quantity in this context is not the margin but the normalized margin , which considers the conjugate norm of the predictor vectors :
when the dictionary we are using is comprised of classiers then kh ( xi ) k ( cid : 123 ) 123 always and thus the l123 margin is exactly the relevant quantity .
the error bounds described by schapire et al .
( 123 ) allow using the whole l123 margin distribution , not just the minimal margin .
how - ever , boostings tendency to separate well in the l123 sense is a central motivation behind their
from a statistical perspective , however , we should be suspicious of margin - maximization as a method for building good prediction models in high dimensional predictor space .
margin maxi - mization in high dimensional space is likely to lead to over - tting and bad prediction performance .
this has been observed in practice by many authors , in particular breiman ( 123 ) .
our results in the next two sections suggest an explanation based on model complexity : margin maximization is the limit of parametric regularized optimization models , as the regularization vanishes , and the reg - ularized models along the path may well be superior to the margin maximizing limiting model , in terms of prediction performance .
in section 123 we return to discuss these issues in more detail .
boosting as approximate incremental l123 constrained fitting in this section we introduce an interpretation of the generic coordinate - descent boosting algorithm as tracking a path of approximate solutions to l123 - constrained ( or equivalently , regularized ) versions of its loss criterion .
this view serves our understanding of what boosting does , in particular the connection between early stopping in boosting and regularization .
we will also use this view to get a result about the asymptotic margin - maximization of regularized classication models , and by analogy of classication boosting .
we build on ideas rst presented by hastie et al .
( 123 , chapter 123 ) and efron et al .
( 123 ) .
given a convex non - negative loss criterion c ( ( cid : 123 ) ; ( cid : 123 ) ) , consider the 123 - dimensional path of optimal
solutions to l123 constrained optimization problems over the training data :
b ( c ) = arg min
c ( yi; h ( xi ) 123b ) :
as c varies , we get that b ( c ) traces a 123 - dimensional optimal curve through rj .
if an optimal solution for the non - constrained problem exists and has nite l123 norm c123 , then obviously b ( c ) = b ( c123 ) = b ; 123c > c123
in the case of separable 123 - class data , using either ce or cl , there is no nite - norm optimal solution .
rather , the constrained solution will always have k b ( c ) k123 = c .
iterations .
this will give an a
a different way of building a solution which has l123 norm c , is to run our e - boosting algorithm ( c=e ) vector which has l123 norm exactly c .
for the norm of the geometric representation b ( c=e ) to also be equal to c , we need the monotonicity condition ( 123 ) to hold as well .
this condition will play a key role in our exposition .
we are going to argue that the two solution paths b ( c ) and b ( c=e ) are very similar for e small .
let us start by observing this similarity in practice .
figure 123 in the introduction shows an example of
rosset , zhu and hastie
figure 123 : another example of the equivalence between the lasso optimal solution path ( left ) and e - boosting with squared error loss .
note that the equivalence breaks down when the path of variable 123 becomes non - monotone
this similarity for squared error loss tting with l123 ( lasso ) penalty .
figure 123 shows another example in the same mold , taken from efron et al .
( 123 ) .
the data is a diabetes study and the dictionary used is just the original 123 variables .
the panel on the left shows the path of optimal l123 - constrained solutions b ( c ) and the panel on the right shows the e - boosting path with the 123 - dimensional dictio - nary ( the total number of boosting iterations is about 123 ) .
the 123 - dimensional path through r123 is described by 123 coordinate curves , corresponding to each one of the variables .
the interesting phenomenon we observe is that the two coefcient traces are not completely identical .
rather , they agree up to the point where variable 123 coefcient path becomes non monotone , i . e . , it violates ( 123 ) ( this point is where variable 123 comes into the model , see the arrow on the right panel ) .
this example illustrates that the monotonicity conditionand its implication that ka k123 = kb k123is critical for the equivalence between e - boosting and l123 - constrained optimization .
the two examples we have seen so far have used squared error loss , and we should ask ourselves whether this equivalence stretches beyond this loss .
figure 123 shows a similar result , but this time for the binomial log - likelihood loss , cl .
we used the spam data set , taken from the uci repository ( blake and merz , 123 ) .
we chose only 123 predictors of the 123 to make the plots more interpretable and the computations more accommodating .
we see that there is a perfect equivalence between the exact constrained solution ( i . e . , regularized logistic regression ) and e - boosting in this case , since the paths are fully monotone .
to justify why this observed equivalence is not surprising , let us consider the following l123 - increment to a given
locally optimal monotone direction problem of nding the best monotone e model b 123 :
min c ( b )
kb k123 ( cid : 123 ) kb 123k123 ( cid : 123 ) e ; jb j ( cid : 123 ) jb 123j ( component - wise ) :
boosting as a regularized path
exact constrained solution
figure 123 : exact coefcient paths ( left ) for l123 - constrained logistic regression and boosting coefcient paths ( right ) with binomial log - likelihood loss on ve variables from the spam data set .
the boosting path was generated using e = 123 : 123 and 123 iterations .
here we use c ( b ) as shorthand for ( cid : 123 )
ic ( yi; h ( xi ) 123b ) .
a rst order taylor expansion gives us
c ( b ) = c ( b 123 ) + ( cid : 123 ) c ( b 123 ) 123 ( b ( cid : 123 ) b 123 ) + o ( e 123 ) :
and given the l123 constraint on the increase in kb k123 , it is easy to see that a rst - order optimal solution ( and therefore an optimal solution as e ! 123 ) will make a coordinate descent step , i . e .
j 123= b 123; j ) j ( cid : 123 ) c ( b 123 ) jj = max
j ( cid : 123 ) c ( b 123 ) kj;
assuming the signs match , i . e . , sign ( b 123 j ) = ( cid : 123 ) sign ( ( cid : 123 ) c ( b 123 ) j ) .
so we get that if the optimal solution to ( 123 ) without the monotonicity constraint happens to be monotone , then it is equivalent to a coordinate descent step .
and so it is reasonable to expect that if the optimal l123 regularized path is monotone ( as it indeed is in figures 123 , 123 ) , then an innitesimal e - boosting algorithm would follow the same path of solutions .
furthermore , even if the optimal path is not monotone , we can still use the formulation ( 123 ) to argue that e - boosting would tend to follow an approximate l123 - regularized path .
the main difference between the e - boosting path and the true optimal path is that it will tend to delay becoming non - monotone , as we observe for variable 123 in figure 123
to understand this specic phenomenon would require analysis of the true optimal path , which falls outside the scope of our discussionefron et al .
( 123 ) cover the subject for squared error loss , and their discussion applies to any continuously differentiable convex loss , using second - order approximations .
we can employ this understanding of the relationship between boosting and l123 regularization to construct lp boosting algorithms by changing the coordinate - selection criterion in the coordinate descent algorithm .
we will get back to this point in section 123 , where we design an l123 boosting
the experimental evidence and heuristic discussion we have presented lead us to the following
conjecture which connects slow boosting and l123 - regularized optimization :
rosset , zhu and hastie
conjecture 123 consider applying the e - boosting algorithm to any convex loss function , generating a path of solutions b ( e ) ( t ) .
then if the optimal coefcient paths are monotone 123c < c123 , i . e . , if 123 j; jb ( c ) jj is non - decreasing in the range c < c123 , then
b ( e ) ( c123=e ) = b ( c123 ) :
efron et al .
( 123 , theorem 123 ) prove a weaker local result for the case of squared error loss only .
we generalize their result to any convex loss .
however this result still does not prove the global convergence which the conjecture claims , and the empirical evidence implies .
for the sake of brevity and readability , we defer this proof , together with concise mathematical denition of the different types of convergence , to appendix a .
in the context of real - life boosting , where the number of basis functions is usually very large , and making e small enough for the theory to apply would require running the algorithm forever , these results should not be considered directly applicable .
instead , they should be taken as an intu - itive indication that boostingespecially the e versionis , indeed , approximating optimal solutions to the constrained problems it encounters along the way .
lp - constrained classication loss functions having established the relation between boosting and l123 regularization , we are going to turn our attention to the regularized optimization problem .
by analogy , our results will apply to boosting as well .
we concentrate on ce and cl , the two classication losses dened above , and the solution paths of their lp constrained versions :
b ( p ) ( c ) = arg min
where c is either ce or cl .
as we discussed below equation ( 123 ) , if the training data is separable in span ( h ) , then we have k b ( p ) ( c ) kp = c for all values of c .
consequently
kp = 123 :
we may ask what are the convergence points of this sequence as c ! shows that these convergence points describe l p - margin maximizing separating hyper - planes .
theorem 123 assume the data is separable , i . e . , 123b s : t : 123i; yib then for both ce and cl , every convergence point of if the lp - margin - maximizing separating hyper - plane is unique , then it is the unique convergence
c corresponds to an lp - margin - maximizing
the following theorem
123h ( xi ) > 123
= arg max
b ( p ) = lim
proof this proof applies to both ce and cl , given the property in ( 123 ) .
consider two separating candidates b 123 and b 123 such that kb 123kp = kb 123kp = 123
assume that b 123 separates better , i . e .
m123 : = min
123h ( xi ) > m123 : = min
123h ( xi ) > 123 :
then we have the following simple lemma :
boosting as a regularized path
lemma 123 there exists some d = d ( m123; m123 ) such that 123d > d , db 123 incurs smaller loss than db 123 , in other words :
123h ( xi ) ) < ( cid : 123 )
given this lemma , we can now prove that any convergence point of c must be an lp - margin maximizing separator .
assume b ( cid : 123 ) is a convergence point of .
denote its minimal margin on the data by m ( cid : 123 ) .
if the data is separable , clearly m ( cid : 123 ) > 123 ( since otherwise the loss of db ( cid : 123 ) does not even converge to 123 as d !
now , assume some b with kb kp = 123 has bigger minimal margin m > m ( cid : 123 ) .
by continuity of the
minimal margin in b
, there exists some open neighborhood of b ( cid : 123 ) : kb ( cid : 123 ) b ( cid : 123 ) k123 < d g
nb ( cid : 123 ) = fb
and an e > 123 , such that
123h ( xi ) < m ( cid : 123 ) e ; 123b 123 nb ( cid : 123 ) :
now by the lemma we get that there exists some d = d ( m; m ( cid : 123 ) e ) such that d b for any d > d; b 123 nb ( cid : 123 ) .
therefore b ( cid : 123 ) cannot be a convergence point of
loss than db
we conclude that any convergence point of the sequence
c must be an lp - margin maximiz - ing separator .
if the margin maximizing separator is unique then it is the only possible convergence point , and therefore
b ( p ) = lim
= arg max
proof of lemma using ( 123 ) and the denition of ce , we get for both loss functions :
123h ( xi ) ) ( cid : 123 ) nexp ( ( cid : 123 ) d ( cid : 123 ) m123 ) :
now , since b 123 separates better , we can nd our desired
d = d ( m123; m123 ) =
logn + log123
m123 ( cid : 123 ) m123
123d > d; nexp ( ( cid : 123 ) d ( cid : 123 ) m123 ) < 123 : 123exp ( ( cid : 123 ) d ( cid : 123 ) m123 ) :
and using ( 123 ) and the denition of ce again we can write
123 : 123exp ( ( cid : 123 ) d ( cid : 123 ) m123 ) ( cid : 123 ) ( cid : 123 )
combining these three inequalities we get our desired result :
123d > d;
123h ( xi ) ) ( cid : 123 ) ( cid : 123 )
rosset , zhu and hastie
we thus conclude that if the lp - margin maximizing separating hyper - plane is unique , the nor - malized constrained solution converges to it .
in the case that the margin maximizing separating hyper - plane is not unique , we can in fact prove a stronger result , which indicates that the limit of the regularized solutions would then be determined by the second smallest margin , then by the third and so on .
this result is mainly of technical interest and we prove it in appendix b , section 123
123 implications of theorem 123
we now briey discuss the implications of this theorem for boosting and logistic regression .
123 . 123 boosting implications
combined with our results from section 123 , theorem 123 indicates that the normalized boosting path with either ce or cl used as lossapproximately converges to a separating hyper - plane
( cid : 123 ) u ( cid : 123 ) t a u , which attains
123h ( xi ) = max
kb k123 min
where di is the ( signed ) euclidean distance from the training point i to the separating hyper - plane .
in other words , it maximizes euclidean distance scaled by an l123 norm .
as we have mentioned already , this implies that the asymptotic boosting solution will tend to be sparse in representation , due to the fact that for xed l123 norm , the l123 norm of vectors that have many 123 entries will generally be larger .
in fact , under rather mild conditions , the asymptotic solution b = limc ! b ( 123 ) ( c ) =c , will have at most n ( the number of observations ) non - zero coefcients , if we use either cl or ce as the loss .
see appendix b , section 123 for proof .
123 . 123 logistic regression implications
recall , that the logistic regression ( maximum likelihood ) solution is undened if the data is sepa - rable in the euclidean space spanned by the predictors .
theorem 123 allows us to dene a logistic regression solution for separable data , as follows :
set a high constraint value cmax 123
find b ( p ) ( cmax ) , the solution to the logistic regression problem subject to the constraint kb k p ( cid : 123 ) cmax .
the problem is convex for any p ( cid : 123 ) 123 and differentiable for any p > 123 , so interior point methods can be used to solve this problem .
now you have ( approximately ) the lp - margin maximizing solution for this data , described by
this is a solution to the original problem in the sense that it is , approximately , the convergence point of the normalized lp - constrained solutions , as the constraint is relaxed .
boosting as a regularized path
of course , with our result from theorem 123 it would probably make more sense to simply nd the optimal separating hyper - plane directlythis is a linear programming problem for l123 separation and a quadratic programming problem for l123 separation .
we can then consider this optimal separator as a logistic regression solution for the separable data .
we now apply boosting to several data sets and interpret the results in light of our regularization and
123 spam data set
we now know if the data are separable and we let boosting run forever , we will approach the same optimal separator for both ce and cl .
however if we stop earlyor if the data is not separable the behavior of the two loss functions may differ signicantly , since ce weighs negative margins exponentially , while cl is approximately linear in the margin for large negative margins ( see fried - man et al . , 123 ) .
consequently , we can expect ce to concentrate more on the hard training data , in particular in the non - separable case .
figure 123 illustrates the behavior of e - boosting with both
figure 123 : behavior of boosting with the two loss functions on spam data set
loss functions , as well as that of adaboost , on the spam data set ( 123 predictors , binary response ) .
we used 123 node trees and e = 123 : 123
the left plot shows the minimal margin as a function of the l123 norm of the coefcient vector kb k123
binomial loss creates a bigger minimal margin initially , but the minimal margins for both loss functions are converging asymptotically .
adaboost initially lags behind but catches up nicely and reaches the same minimal margin asymptotically .
the right plot shows the test error as the iterations proceed , illustrating that both e - methods indeed seem to over - t eventually , even as their separation ( minimal margin ) is still improving .
adaboost did not signicantly over - t in the 123 iterations it was allowed to run , but it obviously would have if it were allowed to run on .
we should emphasize that the comparison between adaboost and e - boosting presented consid - ers as a basis for comparison the l123 norm , not the number of iterations .
in terms of computational complexity , as represented by the number of iterations , adaboost reaches both a large minimal mar -
rosset , zhu and hastie
gin and good prediction performance much more quickly than the slow boosting approaches , as adaboost tends to take larger steps .
123 simulated data
to make a more educated comparison and more compelling visualization , we have constructed an example of separation of 123 - dimensional data using a 123 - th degree polynomial dictionary ( 123 func - tions ) .
the data consists of 123 observations of each class , drawn from a mixture of gaussians , and presented in figure 123
also presented , in the solid line , is the optimal l123 separator for this data in this dictionary ( easily calculated as a linear programming problem - note the difference from the l123 optimal decision boundary , presented in section 123 , figure 123 ) .
the optimal l123 separator has only 123 non - zero coefcients out of 123
boost 123 iter boost 123*123 iter
figure 123 : articial data set with l123 - margin maximizing separator ( solid ) , and boosting models af - ter 123 iterations ( dashed ) and 123 iterations ( dotted ) using e = 123 : 123
we observe the convergence of the boosting separator to the optimal separator
we ran an e - boosting algorithm on this data set , using the logistic log - likelihood loss cl , with e = 123 : 123 , and figure 123 shows two of the models generated after 123 and 123 ( cid : 123 ) 123 iterations .
we see that the models seem to converge to the optimal separator .
a different view of this convergence is given in figure 123 , where we see two measures of convergence : the minimal margin ( left , maximum value obtainable is the horizontal line ) and the l123 - norm distance between the normalized models ( right ) , given by
is the optimal separator with l123 norm 123 and b ( t ) is the boosting model after t iterations .
we can conclude that on this simple articial example we get nice convergence of the logistic -
boosting model path to the l123 - margin maximizing separating hyper - plane .
we can also use this example to illustrate the similarity between the boosted path and the path
of l123 optimal solutions , as we have discussed in section 123
boosting as a regularized path
figure 123 : two measures of convergence of boosting model path to optimal l123 separator : minimal margin ( left ) and l123 distance between the normalized boosting coefcient vector and the optimal model ( right )
figure 123 : comparison of decision boundary of boosting models ( broken ) and of optimal con -
strained solutions with same norm ( full )
figure 123 shows the class decision boundaries for 123 models generated along the boosting path , compared to the optimal solutions to the constrained logistic regression problem with the same bound on the l123 norm of the coefcient vector .
we observe the clear similarities in the way the solutions evolve and converge to the optimal l123 separator .
the fact that they differ ( in some cases signicantly ) is not surprising if we recall the monotonicity condition presented in section 123 for exact correspondence between the two model paths .
in this case if we look at the coefcient paths
rosset , zhu and hastie
( not shown ) , we observe that the monotonicity condition is consistently violated in the low norm ranges , and hence we can expect the paths to be similar in spirit but not identical .
we can now summarize what we have learned about boosting from the previous sections :
( cid : 123 ) boosting approximately follows the path of l123 - regularized models for its loss criterion
( cid : 123 ) if the loss criterion is the exponential loss of adaboost or the binomial log - likelihood loss of logistic regression , then the l123 regularized model converges to an l123 - margin maximizing separating hyper - plane , if the data are separable in the span of the weak learners
we may ask , which of these two points is the key to the success of boosting approaches .
one empirical clue to answering this question , can be found in breiman ( 123 ) , who programmed an algorithm to directly maximize the margins .
his results were that his algorithm consistently got signicantly higher minimal margins than adaboost on many data sets ( and , in fact , a higher margin distribution beyond the minimal margin ) , but had slightly worse prediction performance .
his conclusion was that margin maximization is not the key to adaboosts success .
from a statistical perspective we can embrace this conclusion , as reecting the importance of regularization in high - dimensional predictor space .
by our results from the previous sections , margin maximization can be viewed as the limit of parametric regularized models , as the regularization vanishes . 123 thus we would generally expect the margin maximizing solutions to perform worse than regularized models .
in the case of boosting , regularization would correspond to early stopping of the boosting
123 boosting and svms as regularized optimization in high - dimensional predictor spaces
our exposition has led us to view boosting as an approximate way to solve the regularized optimiza -
123h ( xi ) ) + l kb k123
which converges as l ! 123 to b ( 123 ) , if our loss is ce or cl .
in general , the loss c can be any convex differentiable loss and should be dened to match the problem domain .
support vector machines can be described as solving the regularized optimization problem ( see
friedman et al . , 123 , chapter 123 )
( 123 ( cid : 123 ) yib
123h ( xi ) ) + + l kb k123
which converges as l ! 123 to the non - regularized support vector machine solution , i . e . , the optimal euclidean separator , which we denoted by b ( 123 ) .
an interesting connection exists between these two approaches , in that they allow us to solve
the regularized optimization problem in high dimensional predictor space :
it can be argued that margin - maximizing models are still regularized in some sense , as they minimize a norm criterion among all separating models .
this is arguably the property which still allows them to generalize reasonably well in many cases .
boosting as a regularized path
( cid : 123 ) we are able to solve the l123 - regularized problem approximately in very high dimension via boosting by applying the approximate coordinate descent trick of building a decision tree ( or otherwise greedily selecting a weak learner ) based on re - weighted versions of the data .
( cid : 123 ) support vector machines facilitate a different trick for solving the regularized optimization problem in high dimensional predictor space : the kernel trick .
if our dictionary h spans a reproducing kernel hilbert space , then rkhs theory tells us we can nd the regularized solutions by solving an n - dimensional problem , in the space spanned by the kernel represen - ters fk ( xi;x ) g .
this fact is by no means limited to the hinge loss of ( 123 ) , and applies to any convex loss .
we concentrate our discussion on svm ( and hence hinge loss ) only since it is by far the most common and well - known application of this result .
so we can view both boosting and svm as methods that allow us to t regularized models in high dimensional predictor space using a computational shortcut .
the complexity of the model built is controlled by regularization .
these methods are distinctly different than traditional statistical approaches for building models in high dimension , which start by reducing the dimensionality of the problem so that standard tools ( e . g . , newtons method ) can be applied to it , and also to make over - tting less of a concern .
while the merits of regularization without dimensionality reduction like ridge regression or the lassoare well documented in statistics , computational issues make it impractical for the size of problems typically solved via boosting or svm , without computational
we believe that this difference may be a signicant reason for the enduring success of boosting
and svm in data modeling , i . e . :
working in high dimension and regularizing is statistically preferable to a two - step procedure of rst reducing the dimension , then tting a model in the reduced space .
it is also interesting to consider the differences between the two approaches , in the loss ( exible vs .
hinge loss ) , the penalty ( l123 vs .
l123 ) , and the type of dictionary used ( usually trees vs .
these differences indicate that the two approaches will be useful for different situations .
for ex - ample , if the true model has a sparse representation in the chosen dictionary , then l123 regularization may be warranted; if the form of the true model facilitates description of the class probabilities via a logistic - linear model , then the logistic loss cl is the best loss to use , and so on .
the computational tricks for both svm and boosting limit the kind of regularization that can be used for tting in high dimensional space .
however , the problems can still be formulated and solved for different regularization approaches , as long as the dimensionality is low enough :
( cid : 123 ) support vector machines can be tted with an l123 penalty , by solving the 123 - norm version of the svm problem , equivalent to replacing the l123 penalty in ( 123 ) with an l123 penalty .
in fact , the 123 - norm svm is used quite widely , because it is more easily solved in the linear , non - rkhs , situation ( as a linear program , compared to the standard svm which is a quadratic program ) and tends to give sparser solutions in the primal domain .
( cid : 123 ) similarly , we describe below an approach for developing a boosting algorithm for tting
approximate l123 regularized models .
both of these methods are interesting and potentially useful .
however they lack what is arguably the most attractive property of the standard boosting and svm algorithms : a computational trick to allow tting in high dimensions .
rosset , zhu and hastie
123 . 123 an l123 boosting algorithm we can use our understanding of the relation of boosting to regularization and theorem 123 to for - mulate lp - boosting algorithms , which will approximately follow the path of l p - regularized solutions and converge to the corresponding lp - margin maximizing separating hyper - planes .
of particular interest is the l123 case , since theorem 123 implies that l123 - constrained tting using cl or ce will build a regularized path to the optimal separating hyper - plane in the euclidean ( or svm ) sense .
to construct an l123 boosting algorithm , consider the equivalent optimization problem ( 123 ) , and
change the step - size constraint to an l123 constraint :
kb k123 ( cid : 123 ) kb 123k123 ( cid : 123 ) e :
it is easy to see that the rst order solution to this problem entails selecting for modication the coordinate which maximizes
( cid : 123 ) c ( b 123 ) k
and that subject to monotonicity , this will lead to a correspondence to the locally l123 - optimal direc -
following this intuition , we can construct an l123 boosting algorithm by changing only step 123 ( c )
of our generic boosting algorithm of section 123 to
123 ( c ) * identify jt which maximizes j ( cid : 123 )
i wih jt ( xi ) j
note that the need to consider the current coefcient ( in the denominator ) makes the l123 algorithm appropriate for toy examples only .
in situations where the dictionary of weak learner is prohibitively large , we will need to gure out a trick like the one we presented in section 123 , to allow us to make an approximate search for the optimizer of step 123 ( c ) * .
another problem in applying this algorithm to large problems is that we never choose the same dictionary function twice , until all have non - 123 coefcients .
this is due to the use of the l123 penalty , where the current coefcient value affects the rate at which the penalty term is increasing .
in par - ticular , if b j = 123 then increasing it causes the penalty term kb k123 to increase at rate 123 , to rst order ( which is all the algorithm is considering ) .
the convergence of our l123 boosting algorithm on the articial data set of section 123 is illustrated in figure 123
we observe that the l123 boosting models do indeed approach the optimal l123 separator .
it is interesting to note the signicant difference between the optimal l123 separator as presented in figure 123 and the optimal l123 separator presented in section 123 ( figure 123 ) .
summary and future work
in this paper we have introduced a new view of boosting in general , and two - class boosting in particular , comprised of two main points :
( cid : 123 ) we have generalized results from efron et al .
( 123 ) and hastie et al .
( 123 ) , to describe
boosting as approximate l123 - regularized optimization .
( cid : 123 ) we have shown that the exact l123 - regularized solutions converge to an l123 - margin maximizing
boosting as a regularized path
boost 123*123 iter boost 123 iter
figure 123 : articial data set with l123 - margin maximizing separator ( solid ) , and l123 - boosting models after 123 ( cid : 123 ) 123 iterations ( dashed ) and 123 iterations ( dotted ) using e = 123 : 123
we observe the convergence of the boosting separator to the optimal separator
we hope our results will help in better understanding how and why boosting works .
it is an interest - ing and challenging task to separate the effects of the different components of a boosting algorithm :
( cid : 123 ) loss criterion
( cid : 123 ) dictionary and greedy learning method
( cid : 123 ) line search / slow learning
and relate them to its success in different scenarios .
the implicit l123 regularization in boosting may also contribute to its success , as it has been shown that in some situations l123 regularization is inherently superior to others ( see donoho et al . , 123 ) .
an important issue when analyzing boosting is over - tting in the noisy data case .
to deal with over - tting , ratsch et al .
( 123b ) propose several regularization methods and generalizations of the original adaboost algorithm to achieve a soft margin by introducing slack variables .
our results indicate that the models along the boosting path can be regarded as l123 regularized versions of the optimal separator , hence regularization can be done more directly and naturally by stopping the boosting iterations early .
it is essentially a choice of the l123 constraint parameter c .
many other questions arise from our view of boosting .
among the issues to be considered :
( cid : 123 ) is there a similar separator view of multi - class boosting ? we have some tentative results to
indicate that this might be the case if the boosting problem is formulated properly .
( cid : 123 ) can the constrained optimization view of boosting help in producing generalization error
bounds for boosting that would be more tight than the current existing ones ?
rosset , zhu and hastie
we thank stephen boyd , brad efron , jerry friedman , robert schapire and rob tibshirani for helpful discussions .
we thank the referees for their thoughtful and useful comments .
this work was partially supported by stanford graduate fellowship , grant dms - 123 from the national science foundation , and grant roi - ca - 123 - 123 from the national institutes of health .
appendix a .
local equivalence of innitesimal e - boosting and l123 - constrained
as before , we assume we have a set of training data ( x123; y123 ) ; ( x123; y123 ) ; : : : ( xn; yn ) , a smooth cost function c ( y; f ) , and a set of basis functions ( h123 ( x ) ; h123 ( x ) ; : : : hj ( x ) ) .
we denote by b ( s ) be the optimal solution of the l123 - constrained optimization problem :
c ( yi; h ( xi ) 123b )
kb k123 ( cid : 123 ) s :
suppose we initialize the e - boosting version of algorithm 123 , as described in section 123 , at b ( s ) and run the algorithm for t steps .
let b ( t ) denote the coefcients after t steps .
the global convergence conjecture 123 in section 123 implies that 123d s > 123 :
b ( d s=e ) ! b ( s + d s ) as e ! 123
under some mild assumptions .
instead of proving this global result , we show here a local result by looking at the derivative of b ( s ) .
our proof builds on the proof by efron et al .
( 123 , theorem 123 ) of a similar result for the case that the cost is squared error loss c ( y; f ) = ( y ( cid : 123 ) f ) 123
theorem 123 below shows that if we start the e - boosting algorithm at a solution b ( s ) of the l123 - constrained optimization problem ( 123 ) ( 123 ) , the direction of change of the e - boosting solution will agree with that of the l123 - constrained optimization problem .
theorem 123 assume the optimal coefcient paths b
j ( t ) 123 j are also monotone as e - boosting proceeds , then
j ( s ) 123 j are monotone in s and the coefcient
b ( t ) ( cid : 123 ) b ( s )
t ( cid : 123 ) e
b ( s ) as e ! 123; t !
; t ( cid : 123 ) e ! 123 :
proof first we introduce some notations
h j = ( h j ( x123 ) ; : : : h j ( xn ) ) 123
be the jth basis function evaluated at the n training data .
f = ( f ( x123 ) ; : : : f ( xn ) ) 123
be the vector of current t .
; : : : ( cid : 123 )
boosting as a regularized path
be the current generalized residual vector as dened in friedman ( 123 ) .
c j = h123 be the current correlation between h j and r .
j = 123; : : : j
a = f j : jc jj = max
be the set of indices for the maximum absolute correlation .
for clarity , we re - write this e - boosting algorithm , starting from b ( s ) , as a special case of algo -
rithm 123 , as follows :
( 123 ) initialize b ( 123 ) = b ( s ) ;f123 = f;r123 = r .
( 123 ) for t = 123 : t
( a ) find jt = argmax j jh123
( c ) update ft and rt .
t; jt b
t ( cid : 123 ) 123; jt + e
( cid : 123 ) sign ( c jt )
notice in the above algorithm , we start from b ( s ) , rather than 123
as proposed in efron et al .
( 123 ) , we consider an idealized e - boosting case : and t ( cid : 123 ) e ! 123 , under the monotone paths condition , section 123 and section 123 of efron et al .
( 123 ) showed
as e ! 123 , t !
ft ( cid : 123 ) f123
t ( cid : 123 ) e rt ( cid : 123 ) r123 t ( cid : 123 ) e
where u and v satisfy two constraints :
( constraint 123 ) u is in the convex cone generated by fsign ( c j ) h j : j 123 ag , i . e . :
u = ( cid : 123 )
pjsign ( c j ) h j; pj ( cid : 123 ) 123 :
( constraint 123 ) v has equal correlation with sign ( c j ) h j; j 123 a :
jv = l a for j 123 a :
the rst constraint is true because the basis functions in ac will not be able to catch up in terms of jc jj for sufciently small t ( cid : 123 ) e ; the pjs are non - negative because the coefcient paths b j ( t ) are monotone .
the second constraint can be seen by taking a taylor expansion of c ( y; f ) around f123 to the quadratic term , letting t ( cid : 123 ) e go to zero and applying the result for the squared error loss from efron et al .
( 123 ) .
once the two constraints are established , we notice that
vi = ( cid : 123 )
rosset , zhu and hastie
hence we can plug the constraint 123 into the constraint 123 and get the following set of equations :
aw hap = l a123;
ha = ( ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) sign ( c j ) h j ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ) ; j 123 a;
w = diag ( cid : 123 )
p = ( ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) pj ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ) 123 ; j 123 a :
if h is of rank jaj ( we will get back to this issue in details in appendix b ) , then p , or equivalently u and v , are uniquely determined up to a scale number .
now we consider the l123 - constrained optimization problem ( 123 ) ( 123 ) .
let f ( s ) be the tted
vector and r ( s ) be the corresponding residual vector .
since f ( s ) and r ( s ) are smooth , dene
u ( cid : 123 ) ( cid : 123 ) limd s ! 123 v ( cid : 123 ) ( cid : 123 ) limd s ! 123
f ( s + d s ) ( cid : 123 ) f ( s )
r ( s + d s ) ( cid : 123 ) r ( s )
lemma 123 under the monotone coefcient paths assumption , u ( cid : 123 ) and v ( cid : 123 ) also satisfy constraints 123
proof write the coefcient b
j as b +
j ( cid : 123 ) b ( cid : 123 ) j , where j = b j = 123 j = ( cid : 123 ) b j = 123;b ( cid : 123 )
( cid : 123 ) b +
j > 123; j < 123 :
the l123 - constrained optimization problem ( 123 ) ( 123 ) is then equivalent to
b +;b ( cid : 123 )
kb +k123 + kb ( cid : 123 ) k123 ( cid : 123 ) s;b + ( cid : 123 ) 123;b ( cid : 123 ) ( cid : 123 ) 123 :
c ( cid : 123 ) yi; h ( xi ) 123 ( b + ( cid : 123 ) b ( cid : 123 ) ) ( cid : 123 ) ;
the corresponding lagrangian dual is
c ( cid : 123 ) yi; h ( xi ) 123 ( b + ( cid : 123 ) b ( cid : 123 ) ) ( cid : 123 ) + l
( cid : 123 ) s ( cid : 123 )
j + b ( cid : 123 )
where l ( cid : 123 ) 123;l +
j ( cid : 123 ) 123;l ( cid : 123 )
j ( cid : 123 ) 123 are lagrange multipliers .
by differentiating the lagrangian dual , we get the solution of ( 123 ) ( 123 ) needed to satisfy the
following karush - kuhn - tucker conditions :
j r + l ( cid : 123 ) l +
j = 123;
boosting as a regularized path
j r + l ( cid : 123 ) l ( cid : 123 )
j = 123;
j = 123; j = 123 :
let c j = h123
j r and a = f j : jc jj = max j jc jjg .
we can see the following facts from the karush - kuhn -
( fact 123 ) use ( 123 ) , ( 123 ) and l ( cid : 123 ) 123;l +
j ;l ( cid : 123 )
j ( cid : 123 ) 123 , we have jc jj ( cid : 123 ) l
j 123= 123 , then jc jj = l and j 123 a .
for example , suppose b +
123= 123 , then l +
j = 123 and
( fact 123 ) if b ( 123 ) implies c j = l ( fact 123 ) if b
j 123= 123 , sign ( b
j ) = sign ( c j ) .
we also note that : j and b ( cid : 123 ) the same time .
( cid : 123 ) b +
j can not both be non - zero , otherwise l +
j = l ( cid : 123 )
j = 123 , ( 123 ) and ( 123 ) can not hold at
( cid : 123 ) it is possible that b
j = 123 and j 123 a .
this only happens for a nite number of s values , where
basis h j is about to enter the model .
for sufciently small d s , since the second derivative of the cost function c ( y; f ) is nite , a will
stay the same .
since j 123 a if b
j 123= 123 , the change in the tted vector is
f ( s + d s ) ( cid : 123 ) f ( s ) = ( cid : 123 )
q jh j :
since sign ( b sign ( c j ) .
hence we have
j ) = sign ( c j ) and the coefcients b
j change monotonically , sign ( q j ) will agree with
f ( s + d s ) ( cid : 123 ) f ( s )
pjsign ( c j ) h j :
this implies u ( cid : 123 ) satises constraint 123
the claim v ( cid : 123 ) satises constraint 123 follows directly from fact 123 , since both r ( s + d s ) and r ( s ) satisfy constraint 123
completion of proof of theorem ( 123 ) : we further notice that in both the e - boosting case and the constrained optimization case , we have ( cid : 123 ) j123a pj = 123 by denition and the monotone coefcient paths condition , hence u and v are uniquely determined , i . e . :
u = u ( cid : 123 ) and v = v ( cid : 123 ) :
to translate the result into b ( s ) and b ( t ) , we notice f ( x ) = h ( x ) 123b that for ( cid : 123 ) conditions for when this is true in appendix b .
efron et al .
( 123 ) showed b ( s ) to be well dened , a can have at most n elements , i . e . , jaj ( cid : 123 ) n .
we give sufcient
ha = ( ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) h j ( xi ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ) ; i = 123; : : : n; j 123 a
rosset , zhu and hastie
be a n ( cid : 123 ) jaj matrix , which we assume is of rank jaj .
then ( cid : 123 )
b ( s ) is given by
hence the theorem is proved .
b ( s ) = ( cid : 123 ) h123 aw ha ( cid : 123 ) ( cid : 123 ) 123 h123 aw ha ( cid : 123 ) ( cid : 123 ) 123 h123
t ( cid : 123 ) e
b ( t ) ( cid : 123 ) b ( s )
appendix b .
uniqueness and existence results
in this appendix , we give some details on the properties of regularized solution paths .
in section b . 123 we formulate and prove sparseness and uniqueness results on l123 - regularized solutions for any convex loss .
in section b . 123 we extend theorem 123 of section 123which proved the margin maximizing property of the limit of lp - regularized solutions , as regularization variesto the case that the margin maximizing solution is not unique .
b . 123 sparseness and uniqueness of l123 - regularized solutions and their limits consider the l123 - constrained optimization problem :
in this section we give sufcient conditions for the following properties of the solutions of ( 123 ) :
existence of a sparse solution ( with at most n non - zero coefcients ) ,
non - existence of non - sparse solutions with more than n non - zero coefcients ,
uniqueness of the solution ,
convergence of the solutions to sparse solution , as c increases .
theorem 123 assume that the unconstrained solution for problem ( 123 ) has l123 norm bigger than c .
then there exists a solution of ( 123 ) which has at most n non - zero coefcients .
proof as lemma 123 in the appendix a , we will prove the theorem using the karush - kuhn - tucker ( kkt ) formulation of the optimization problem .
the chain rule for differentiation gives us that
where h j and r ( b ) are dened in the appendix a; r ( b ) is the generalized residual vector .
using this simple relationship and fact 123 of lemma 123 we can write a system of equations for all non - zero coefcients at the optimal constrained solution as follows ( denote by a the set of indices for
ar ( b ) = l
( cid : 123 ) signb a :
boosting as a regularized path
in other words , we get jaj equations in jaj variables , corresponding to the non - zero b
however , each column of the matrix ha is of length n , and so ha can have at most n linearly independent columns , rank ( ha ) ( cid : 123 ) n .
assume now that we have an optimal solution for ( 123 ) with jaj > n .
then there exists l 123 a such that
hl = ( cid : 123 )
substituting ( 123 ) into the lth row in ( 123 ) we get
jh j ) 123r ( b ) = l
but from ( 123 ) we know that h123
jr ( b ) = l
j ; 123 j 123 a , meaning we can re - phrase ( 123 ) as
j ( cid : 123 ) signb
j ( cid : 123 ) signb
l = 123 :
in other words , we get that hl is a linear combination of the columns of ha ( cid : 123 ) flg which must obey the specic numeric relation in ( 123 ) .
now we can construct an alternative optimal solution for ( 123 ) with one less non - zero coefcient ,
start from b 123
dene the direction g j = a
l = ( cid : 123 ) signb
in coefcient space implied by ( 123 ) , that is : j ( cid : 123 ) signb
l ; 123 j 123 a ( cid : 123 ) flg
move in direction g until some coefcient in a hits zero , i . e . , dene :
d ( cid : 123 ) = min ( cid : 123 ) d > 123 : 123 j 123 a s . t
jd = 123 ( cid : 123 )
( we know that d ( cid : 123 ) ( cid : 123 ) jb
set b = b + d ( cid : 123 ) g
then from ( 123 ) we get that b
123h ( xi ) = b kb k123 = kb k123 ( cid : 123 ) ( cid : 123 )
123h ( xi ) ; 123i and from ( 123 ) we get that
jd ( cid : 123 ) j ( cid : 123 ) jb
j + g = kb k123 ( cid : 123 ) d ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 )
j ( cid : 123 ) signb
l ! = kb k123 :
so b generates the same t as b and has the same l123 norm , therefore it is also an optimal solution , with at least one less non - zero coefcient ( from the denition of d ( cid : 123 ) ) .
we can obviously apply this process repeatedly until we get a solution with at most n non - zero
this theorem has the following immediate implication :
rosset , zhu and hastie
corollary 123 if there is no set of more than n dictionary functions which obeys the equalities ( 123 , 123 ) on the training data , then any solution of ( 123 ) has at most n non - zero coefcients .
this corollary implies , for example , that if the basis functions come from a continuous non - redundant distribution ( which means that any equality would hold with probability 123 ) then with probability 123 any solution of ( 123 ) has at most n non - zero coefcients .
theorem 123 assume that there is no set of more than n dictionary functions which obeys the equal - ities ( 123 , 123 ) on the training data .
in addition assume :
the loss function c is strictly convex ( squared error loss , cl and ce obviously qualify ) ,
no set of dictionary functions of size ( cid : 123 ) n is linearly dependent on the training data .
then the problem ( 123 ) has a unique solution .
proof the previous corollary tells us that any solution has at most n non - zero coefcients .
now assume b 123; b 123 are both solutions of ( 123 ) .
from strict convexity of the loss we get that
h ( x ) 123b 123 = h ( x ) 123b 123 = h ( x ) 123 ( a
123 + ( 123 ( cid : 123 ) a ) b 123 ) ; 123 ( cid : 123 ) a ( cid : 123 ) 123;
and from convexity of the l123 norm we get
123 + ( 123 ( cid : 123 ) a ) b 123k123 ( cid : 123 ) kb 123k123 = kb 123k123 = c :
123 + ( 123 ( cid : 123 ) a ) b 123 ) must also be a solution .
thus , the total number of variables with non - zero coefcients in either b 123 or b 123 cannot be bigger than n , since then ( a 123 + ( 123 ( cid : 123 ) a ) b 123 ) , would have > n non - zero coefcients for almost all values of a , contradicting corollary 123
thus , by ignoring all coefcients which are 123 in both b 123 and b 123 we get that both b 123 and b 123 can be represented in the same n ( cid : 123 ) dimensional ( maximum ) sub - space of rj .
which leads to a contradiction between ( 123 ) and assumption 123
: 123 ( cid : 123 ) c ( cid : 123 ) g of normalized solutions to the problem ( 123 ) .
corollary 123 consider a sequence f assume that all these solutions have at most n non - zero coefcients .
then any limit point of the sequence has at most n non - zero coefcients .
proof this is a trivial consequence of convergence .
assume by contradiction b ( cid : 123 ) is a convergence jj : b ( cid : 123 ) point with more than n non - zero coefcients .
let k = argmin jfjb ( cid : 123 ) j 123= 123g .
then for any vector b with at most n non - zero coefcients we know that k b ( cid : 123 ) b ( cid : 123 ) k ( cid : 123 ) jb ( cid : 123 ) jj > 123 so we get a contradiction
boosting as a regularized path
b . 123 uniqueness of limiting solution in theorem 123 when margin maximizing separator is
recall , that we are interested in convergence points of the normalized regularized solutions theorem 123 proves that any such convergence point corresponds to an l p - margin maximizing sep - arating hyper - plane .
we now extend it to the case that this rst - order separator is not unique , by extending the result to consider the second smallest margin as a tie breaker .
we show that any convergence point maximizes the second smallest margin among all models with maximal minimal margin .
if there are also ties in the second smallest margin , then any limit point maximizes the third smallest margin among all models which still remain , and so on .
it should be noted that the minimal margin is typically not attained by one observation only in margin maximizing models .
in case of ties in the smallest margins our reference to smallest , second smallest etc .
tie - breaking ( i . e . , our decision on which one of the tied margins is considered smallest , and which one second smallest is of no consequence ) .
theorem 123 assume that the data is separable and that the margin - maximizing separating hyper - c will correspond to a plane , as dened in ( 123 ) is not unique .
then any convergence point of margin - maximizing separating hyper - plane which also maximizes the second smallest margin .
proof the proof is essentially the same as that of theorem 123
we outline it below .
from theorem 123 we know that we only need to consider margin - maximizing models as limit points .
thus let b 123 , b 123 be two margin maximizing models with lp norm 123 , but let b 123 have a bigger second smallest margin .
assume that b 123 attains its smallest margin on observation i123 and b 123 attains the same smallest margin on observation i123
now dene
m123 = min
yih ( xi ) 123b 123 > min
yih ( xi ) 123b 123 = m123 :
then we have that lemma 123 of theorem 123 holds for b 123 and b 123 ( the proof is exactly the same , except that we ignore the smallest margin observation for each model , since these always contribute the same amount to the combined loss ) .
let b ( cid : 123 ) be a convergence point .
we know b ( cid : 123 ) maximizes the margin from theorem 123
now assume b also maximizes the margin but has bigger second - smallest margin than b ( cid : 123 ) .
then we can proceed exactly as the proof of theorem 123 , considering only n ( cid : 123 ) 123 observations for each model and using our modied lemma 123 , to conclude that b ( cid : 123 ) cannot be a convergence point ( again note that the smallest margin observation always contributes the same to the loss of both models ) .
in the case that the two smallest margins still do not dene a unique solution , we can continue up the list of margins , applying this result recursively .
the conclusion is that the limit of the normal - ized , lp - regularized models maximizes the margins , and not just the minimal margin .
the only case when this convergence point is not unique is , therefore , the case that the whole order statistic of the optimal separator is not unique .
it is an interesting research question to investigate under which conditions this scenario is possible .
rosset , zhu and hastie
