reaction , with a search - based planner the robot agent continually chooses which action it is to perform , using the stimulus - response subsystem when possible , and falling back on the planning subsystem when necessary .
whenever forced to plan , it applies an explanation - based mechanism to formulate a new stimulus - response rule to cover this new situation and others similar to it .
with experience , the agent becomes increasingly reactive as its learning component acquires new stimulus - response rules that eliminate the need for planning in similar subsequent situations .
this theo - agent architecture is described , and results are presented demonstrating its ability to reduce time for a simple mobile robot from minutes to under a second .
introduction and motivation
much attention has
recently on reactive architectures for robotic agents that continually sense their environment and compute appropriate reactions to their sense stimuli within bounded time ( e . g . , ( brooks , 123 , agre and chapman , 123 , rosenschein , 123 ) ) .
such architectures offer advantages over more traditional open - loop search - based planning systems because they can react more quickly to changes to their environment , and because they can operate more robustly in worlds that are difficult to model in advance .
search - based planning architectures , on the other hand , offer the advantage of more general - purpose ( if slower ) problem solving mechanisms which provide the flexibility to deal with a more diverse set of unanticipated goals and situations .
this paper considers the question of how to combine the benefits of reactive and search - based architectures controlling autonomous agents .
we describe the theo - agent architecture , which incorporates both a reactive component and a search - based planning component .
the fundamental design principle of the theo - agent is that it reacts when it can , plans when it must , and learns by augmenting its reactive component whenever it is forced to plan .
when used to control a laboratory mobile robot , the theo - agent in simple cases learns to reduce its reaction
time for new tasks from several minutes to less than a
the research reported here is part of our larger effort architecture , and builds on earlier work described ( blythe and mitchell , 123 ) .
we believe that in order to become increasingly successful , a learning robot will have to incorporate several types of learning :
123 it must become increasingly correct at predicting
the effects of its actions in the world .
q it must become
123 it must become increasingly reactive , by reducing the time required for it to make rational choices; to choose actions consistent with the above predictions and its goals .
those features of
impact its success .
this paper focuses on the second of these types of learning .
we describe how the theo - agent increases the scope of situations for which it can quickly make rational whenever it is forced to plan for a situation outside the current scope of its reactive component .
based learning mechanism produces rules that recommend precisely the same action as recommended by the slower planner , in exactly those situations in which the same plan rationale would apply .
however , the learned rules infer the desired action immediately from the input sense data in a single inference step - - without considering explicitly the robots goals , available
related work
there has been a great deal of recent work on architectures for robot control which continually sense the environment and operate in bounded time ( e . g . , ( brooks , 123 , schoppers , 123 , agre and chapman , 123 ) ) , though this type of work has not directly addressed issues of learning .
segres arms system ( segre , 123 ) applies explanation - based learning to acquire planning tactics for a simulated hand - eye system , and lairds robosoar ( laird and rosenbloom , 123 ) has been applied problems in a real hand - eye robot system .
while these researchers share our goal of developing systems that are
to control an outdoor
the underlying architectures vary significantly in the form of the knowledge being learned , underlying representations , and real response time .
sutton has proposed an inductive approach to acquiring robot control strategies , in his dyna system ( sutton , 123 ) , and pommerleau has developed a connectionist system which ( pommerleau , 123 ) .
in addition to work on learning such robot control strategies , there has been much recent interest in robot learning more generally , including work correct models of actions ( christiansen , et al . , 123 , zrimic and mowforth , 123 ) , and work on becoming
the work reported here is also somewhat related to recent ideas for compiling low - level reactive systems from high - level specifications ( e . g . , ( rosenschein , 123 ) ) .
particular , such compilation transforms input descriptions of actions and goals into effective control strategies , using transformations similar to those achieved by explanation - based learning in the theo - agent .
the main difference explanation - based learning used in the theo - agent , is that for the theo - agent compilation transformation is incrementally focused by the worlds actually encountered by the agent , and may be interleaved with other learning mechanisms which improve the agents models of its actions .
the agent , so that
the next section of this paper describes the theo - agent in greater detail .
the subsequent section presents an example of its use in controlling a simple mobile robot , the learning mechanism for acquiring new effect of caching and rule learning on system reaction time .
the final section summarizes some of the lessons of this work , including features and bugs in the current design of the architecture .
rules , and timing data showing
the theo - agent architecture
the goal of combining
the design of the theo - agent architecture is primarily advantages of reactive and search - based systems .
reactive systems offer the advantage of quick response .
search - based planners offer the advantage of broad scope for handling a more diverse range of unanticipated worlds .
the theo - agent architecture employs both , and uses learning to incrementally augment its reactive component whenever forced to plan .
in addition , the architecture makes widespread use of caching and to avoid needless recomputation of repeatedly accessed beliefs .
the primary characteristics of the theo - agent are :
e it continually
reassesses what action
perform .
the agent runs in a tight loop in which it repeatedly updates its sensor inputs , chooses a control action , begins executing it , then repeats this
e it reacts when it can , and plans when it must .
it must choose an action , the system consults a set of stimulus - response constitute its reactive component .
if one of these rules applies to the current sensed inputs , then the corresponding action is taken .
if no rules apply , to determine an
it acquires a new stimulus - response rule .
the new rule recommends the action which the planner has recommended , in the same situations ( i . e . , those world states for which the same plan justification would apply ) , but can be invoked much more efficiently .
learning is accomplished by an explanation - based ( mitchell , et al , 123 ) ) , and provides a demand - driven incremental compilation into an equivalent of the planners knowledge l every belief
that depends on sensory
maintained as long as its explanation remains valid .
belief of which action to perform next , depend directly or indirectly on observed sense data .
the architecture maintains a network of explanations for every belief of the agent , and deletes beliefs only when their support ceases .
this caching of beliefs significantly improves the response time of the agent by eliminating recomputation of beliefs in the face of unchanging or irrelevant sensor inputs .
in the theo - agent ,
e it determines which goal to attend to , based on the perceived world state , a predefined set of goal activation and satisfaction conditions , and given priorities among goals .
123 observed . world . - p ' chosen .
+ - - - - - - - - k - - - - - - - - - - - - - - - - - - - - - - - - - - -
figure 123 - l : data plow in a theo - agent
the agent presently chooses
internal structure of agent : a theo - agent is defined by a frame structure whose slots , subslots , subsubslots , etc .
define the agents beliefs , or internal statel .
the two most significant slots of the agent are chosen . action , which perform; and observed . world , which describes the agents current perception of its world .
as indicated in figure 123 - l the agent may infer its chosen . action either directly from its observed . world , or alternatively from its current plan .
its plan is in turn derived from its observed . world and attended . to . goal .
the attended . to . goal defines the goal to achieve , and is computed as the highest priority of its active . goals , which are themselves inferred from the observed . world .
is currently attempting
agent goals : goals are specified
to the agent by defining conditions under which they are active , satisfied , and attended to .
for example , an agent may be given a goal recharge . battery which is defined to become active when it perceives its battery level to be less than 123% , becomes satisfied when the battery charge is loo% , and which is attended to whenever it is active and the ( higher priority ) goal avoid . oncoming . obstacle
caching policy : the basic operation of the theo - agent is to repeatedly infer a value for its chosen . action slot .
each slot of the agent typically has one or more attached procedures for obtaining a value upon demand .
these typically access other slots , backchaining to slots of the observed . world .
whenever some slot value is successfully value is cached ( stored ) in the corresponding slot , along with an explanation justifying its value in terms of other slot values , which are in turn justified in terms of others , leading eventually to values of individual features in the observed . world , which are themselves inferred by directly accessing the robot sensors .
values remain cached for as long as their explanations remain valid .
thus , the agents active . goals and chosen . action may remain cached for many cycles , despite irrelevant changes in sensor inputs .
this policy of always caching values , deleting immediately when explanations become invalid , and lazily recomputing upon demand , assures that the agents beliefs in its input senses , without
control policy : the theo - agent
is controlled by
executing the following loop :
sense and update readings for all eagerly sensed features of observed . world , and delete any cached values for zazizy sensed features .
decide upon the current chosen . action 123
execute the chosen . action
when the chosen . action slot is accessed ( during the decision portion of the above cycle ) , the following steps are attempted in sequence until one succeeds : 123
retrieve the cached value of this slot ( if available ) 123
infer a value based on the available stimulus -
select the first step of the agents plan ( inferring a
plan if necessary )
select the default action ( e . g . , wait ) sensing policy : each primitive sensed input ( e . g . , an array of input sonar data ) is stored in some slot of the agents observed . world .
higher level features such as edges in the sonar array , regions , region width , etc . , are observed . world , and are inferred upon demand from the lower - level features .
the decision - making portions of the agent draw upon the entire range of low to high level sensory features as needed .
in order to deal with a variety of sensing procedures of varying cost , the theo - agent types of primitive sensed those - which it eagerly senses , and those which it eagerly sensed features are refreshed automatically during each cycle through the agents main loop , so that dependent cached beliefs of the agent are retained when possible .
in contrast , lazily sensed features are simply deleted during each cycle .
recomputed only if the agent queries the corresponding slot during some subsequent cycle .
this division between eagerly and lazily refreshed features provides a simple focus of attention which allows keeping the overhead of collecting new sense data during each cycle to a minimum .
learning policy : whenever the agent is forced to plan in order to obtain a value for its chosenaction , its explanation - based generalization routine to acquire a new stimulus - response rule to cover this situation .
the details of this routine are described in greater detail in the next section .
the effect of this learning policy incrementally extend the scope of the set of stimulus - encountered by the system in its world .
the types of problem
example and results
the use of
this section describes
to search the laboratory
architecture to develop a simple program to control a hero 123 mobile robot garbage cans* .
in particular , we illustrate how goals and actions are provided to the robot with no initial stimulus - selects actions by constructing plans , and how it incrementally accumulates stimulus - response rules that cover its routine actions .
the robot sensors used in this example
ultrasonic sonar mounted on its hand , a rotating sonar on
the theo - agen t is implemented
solving and learning
and learning mechanisms .
system called theo ( mitchell , et al . , 123 ) ,
on top of a generic
123a detailed description of the m . odified hero 123 robot used here is
in ( lin , et al . , 123 ) .
its head , and a battery voltage sensor .
by rotating its hand and head sonars it is able to obtain arrays of sonar readings that measure echo distance versus rotation angle .
these raw sonar readings are interpreted ( on demand ) to locate edges in the sonar array , as well as regions , and properties of regions such as region width , distance , direction , and identity .
the primitive sensing operations used in the battery voltage level , sonarw , which measures sonar range sweep . wrist . roll , which obtains an array of sonar readings by rotating the wrist from left to right .
of these sensed features , sonarw is eagerly sensed , and the others are lazily sensed .
the wrist sonar pointed directly
include battery , which
the robot actions here
forward 123 inches ) , backward . 123 ( move backward 123 ( turn toward the closest sonar in front of the robot ) , and measure . the . object ( obtain several additional sonar sweeps whether the closest sonar region in front of the robot is a garbage can ) .
the . object refers to the closest sonar region in front of the robot , as detected by the sense procedure
this theo - agent has been tested by giving it different sets of initial goals , leading it to compile out different sets of stimulus - response rules exhibiting different behaviors .
in the simple example presented here , the agent is given
the sonarw sense
123 goal . closer : approach distant objects .
this goal is between 123 and 123 inches , indicating an object at it is satisfied when sonarw is less that 123 inches , and attended
123 goal . further : back off from close objects .
this is activated when sonarw is between 123 and 123 inches , satisfied when sonarw is greater than 123 inches , and attended to whenever it is active .
nearest sonar region corresponds to a garbage can .
this is activated when there is an object in front of the robot whose is unknown , satisfied when the object identity is known , and attended to goal . further are inactive .
in order to illustrate the operation of the theo - agent , consider the sequence of events that results from setting the robot loose in the lab with the above goals , actions , and sensing routines : during the first iteration through its loop , it ( eagerly ) senses a reading of 123 from sonarw , reflecting an object at 123 inches .
in the decide phase of chosen . action slot , which has no cached value , and no associated stimulus - response rules .
thus , it is forced to plan in order to determine a value for chosen . action .
the planner determines which goal the agent is attending to , then searches for a sequence of
actions which it projects will satisfy this goal .
thus , the planner queries the attending . to . goal slot , which queries the active . goals slots , which query the observedworld , the planner , after some search , then derives a two - step plan to execute two times in a row ( this plan leads to a projected sonar reading of 123 inches , which would chosen . action slot is thus forward . 123 ( the first step of the
the agent caches the result of each of the above slot queries , along with an explanation that justifies each slot value in terms of the values from which it was derived .
relates each belief ( slot this network of explanations value ) of the agent eventually to sensed features of its
in the above scenario the agent had to construct a plan in order to infer its chosen . action .
therefore , it formulates a new stimulus - response rule which will recommend chosen action in future situations , without planning .
the agent then executes the action and begins a new cycle by the sonarw feature and deleting any other sensed features ( in this case the observed battery level , which was queried by the planner as it checked the for various actions ) .
during this second cycle , the stimulus - response rule learned during the first cycle applies , and the agent quickly decides execute forward . 123
as it gains experience , acquires additional rules and an increasing proportion of its decisions are made by invoking these stimulus - response rules rather than planning .
in the new situation
rule learning
the rule acquisition procedure used by the theo - agent is an explanation - based learning algorithm called ebg ( mitchell , et al , 123 ) .
this procedure explains why the chosen . action of the theo - agent is justified , finds the weakest conditions under which this explanation holds , precisely , given some chosen . action , ? action , the theo - agent explains why ? action justified . action ( ? agent , ? action ) t
( 123 ) the attending . to . goal of the ? agent is ? g ( 123 ) ? g is satisfied by result of ? agents plan ( 123 ) the tail of ? agents plan will not succeed without
first executing ? action
( 123 ) ? action is the first step of the ? agents plan
chosen . action is a justified . action as defined above , then determines the weakest conditions on the observed . world
under which this explanation will hold123
consider , for example , a scenario in which the hero agent is attending to and has constructed a the goal goal . identify . the . object , figure 123 - l shows the explanation generated by the system for why face . the . object in this figure , each line corresponds to some belief of the agent , and level of indentation reflects dependency .
each belief is written in the form ( frame slot .
. . ) =value .
and arrows such as c - - observed . value - - indicate how the belief above and left of the arrow was inferred from the beliefs below and to its right .
for example , the leftmost belief that the heros is supported by the ( world123 goal . identify . object satisfied ? ) =t , and ( 123 ) ( wo measure . the . object prec . sat ? ) =nil .
wo is the current observed . world , world123 is the world state which is predicted to result from the agents plan , and prec . sat ? the predicate indicating whether the preconditions of an action are satisfied in a given world state .
these three supporting beliefs correspond to the first three clauses in the above definition of justified . action123
notice the third clause indicates that in this case the tail of the agents plan cannot succeed since the preconditions of the second step of the plan are not satisfied in the initial observed world .
( 123 ) identity of the . object
is not known
( 123 ) sonarw in observed . world ( 123 ) not ( 123 < ? s < 123 ( 123 ) not ( 123 < ? s < 123 ( 123 ) battery in observed . world ( 123 ) distance to the . object
( 123 ) 123 <= ? dist <= 123 ( 123 , 123 ) direction to the - object
( 123 ) not t - 123 <= ? dir <= 123
( w123 battery ) = 123 ( w123 battery observed - value )
( w123 battery ) = 123
distance ) = 123
direction known ? ) = t
distance ) = 123
direction ) = 123
( w123 battery observed . value )
direction known ? ) = t
of hero = face - the - object
figure 123 - 123 : rule for explanation from figure 123 - l
figure 123 - 123 shows the english description of the rule from the explanation of left of each rule
produced by the theo - agent figure 123 - l . - the number
figure 123 - 123 : explanation for ( hero justified . action ) = face . the . object
123noticethatthethird clause inthedefinition ofjustified . action requires
that the first step of the plan be essential to the plans success .
without this requirement , the definition is too weak , and can produce rules that recommend non - essential actions such as wait , provided they can be
123the fourth clause is not even made explicit , since this is satisfied by
defining the rule postcondition to recommend the current action .
justified . action which is supported by this precondition .
for example , the first four lines in the rule assure that the robot is in a world state for which it should attend to the ( i . e . , they assure that this goal will be active , and that all higher priority goals will be inactive ) .
of course this rule need not explicitly mention this goal or any other , since it instead mentions observed sense features which imply the activation of the relevant goals .
similarly , the rule need not mention the plan , since it instead mentions those conditions , labeled ( 123 ) and ( 123 ) , which assure that the first step of the plan will lead eventually to achieving the desired goal .
in all , the agent typically learns from five to fifteen stimulus - response rules for this set of goals and actions , depending on its specific experiences and the sequence in which they are encountered .
by adding and removing other goals and actions , other agents can be specified that will compile out into sets of stimulus - response rules that produce different behaviors .
impact of experience on agent reaction time with experience , the typical reaction time of the theo - agent in the above scenario drops from a few minutes to under a second , due to its acquisition of stimulus - response rules and its caching of beliefs .
let us define reaction time as the time required for a single iteration of the sense - decide - execute loop of the agent .
similarly , define sensing time , decision time , and execution time as the time required for the corresponding portions of this cycle .
decision time is reduced by two factors :
l acquisition of stimulus - response rules .
matching a stimulus - response rule requires on the order of ten milliseconds , whereas planning typically requires
l caching of beliefs about future world states .
the time required by planning is reduced as a result of caching all agent beliefs .
descriptions of future world states considered by the planner ( e . g . , the wrist sonar reading in the world that will result from applying the action to the current observed . world ) are cached , and remain as beliefs of the agent even after its sensed world is updated .
some cached features of this imagined future world may become uncached each cycle as old sensed values are replaced by newer ones , but others tend to remain .
the improvement in agent reaction time is summarized in the timing data from a typical scenario , shown in table 123 - l .
the first line shows decision time and total reaction time for a sense - decide - execute cycle in which a plan must be created .
notice that here decision time constitutes the bulk of reaction time .
the second line of this table shows the cost of producing a very similar plan on the next cycle .
the speedup over the first line is due to the use of slot the first planning values which were cached during
table 123 - i : effect of learning on agent response time
( timings are in commonlisp on a sun123 workstation )
episode , and whose explanations remain valid through the second cycle .
the third line shows the timing for a third cycle in which the agent applied a set of learned stimulus - the same action .
here , decision time ( 123 msec . ) is comparable to sensing time ( 123 msec ) and the time to initiate execution of the robot action ( 123 msec . ) , so that decision time no longer constitutes the bulk of overall reaction time .
the decision time is found empirically to re uire 123 + 123r msec .
to test a set of r stimulus - response rules .
figures above are dependent on the particular agent goals , sensors , training experience , actions , etc .
scaling to more complex agents that require hundreds or thousands of stimulus - response learned stimulus - response pairings .
approaches such as rete matching , or encoding stimulus - response pairings in ( rosenschein , 123 , brooks , 123 ) may be important for scaling to larger systems .
at present , the significant result reported here is simply the existence proof that the learning mechanisms employed in the theo - agent are sufficient to reduce decision time by two orders of magnitude for a real robot with fairly simple goals , so that decision time ceases to dominate overall reaction time of the agent .
for encoding and indexing
type of network
summary , limitations and future work
the key design features of the theo - agent are : e a stimulus - response
system combined with a planning component of broader scope but slower time .
this combination allows quick response for routine situations , plus flexibility to plan when novel situations are encountered .
component of the system .
when forced to plan , the agent formulates new stimulus - response rules that
srules are simply tested in sequence with no sophisticated indexing or
parallel match algorithms .
produce precisely the same decision as the current plan , in precisely the same situations .
123 the agent chooses
its own goals based on the sensed world state , goal activation conditions and relative goal priorities .
goals are explicitly stimulus - response rules grows , the frequency with
the agent only when the number of
the agent explicitly considers
cached along with an explanation
123 caching and dependency maintenance
beliefs of the agent .
every belief of the agent is those beliefs on which it depends .
whenever the agent sense inputs change , dependent beliefs which are affected are deleted , to be recomputed if and when they are subsequently queried .
123 distinction between eagerly and lazily refreshed in order to minimize the lower bound on reaction time , selected sense features are eagerly updated during each agent cycle .
other features are lazily updated by deleting them and them if and when they are required .
focus of attention mechanism that helps minimize response time .
in dynamically control the assignment of eagerly and lazily sensed features .
there are several reasonable criticisms of the current
future , we hope
limitations .
among these are :
123 the kind of planning the theoagent performs may be unrealistically difficult in many situations , due to lack of knowledge about the world , the likely effects of the agents actions , or other changes in the world .
one possible response to this limitation is to add new decision - making mechanisms beyond the current planner and stimulus - response system .
for example , one could imagine a decision - maker function over world states , with an evaluation actions based on one - step lookahead ( similar to that proposed ( kaelbling , 123 ) , a spectrum of multiple - decision correctness .
however , learning mechanisms such as those used here might still compile stimulus - response rules from the decisions produced by this spectrum of decision - makers .
increasingly reactive , its decisions do not become response rules are only as good as the planner and action models from which they are compiled .
we are interested in extending the system to allow it to inductively learn better models of the effects of its actions , as a result of its experience .
preliminary
results with this kind of learning using a hand - eye robot are described in ( christiansen , et al . , 123 , zrimic and mowforth , 123 ) .
in only minor ways
* the current planner considers the correctness of its plans , but not the cost of sensing or effector commands .
therefore , the plans and the stimulus - response rules derived from them may refer to sense features which are quite expensive to obtain , and which contribute in order to guarantee correctness of a plan to pick up a cup , it might be necessary to verify that the cup is not glued to the floor .
the current system would include such a test in the stimulus - response rule that recommends the grasp operation , provided this feature was considered by the planner .
we must find a way to allow the agent to choose which tests are necessary and which can be ignored in order to construct plausible plans that it can then attempt , and recover from as needed .
t123 scaling issues .
as noted in the previous section , the current robot system requires only a small set of stimulus - response rules to govern its behavior .
we must consider how the approach can be scaled to more complex situations .
some avenues are to ( 123 ) knowledge ( e . g . , index rules by goal , so that many subsets of rules are stored rather than a single set ) , ( 123 ) develop a more selective strategy for invoking learning only when the benefits outweigh the costs , and ( 123 ) consider representations of the control fixed computational cost neural networks with constant response time ) .
that sacrifice expressive precision
the notion of
reactive systems from more general but slower search - based systems is an important approach toward extending the flexibility of robotic systems while still achieving times .
the specific design of the theo - agent illustrates one way to organize such a system .
our intent architecture by adding new learning mechanisms that will allow it to improve the correctness of its action models and its abilities to usefully perceive its world .
these additional learning capabilities are intended to complement the type of learning presented here .
is to extend
acknowledgements .
this work is based on extensions to earlier joint work with jim blythe , reported in ( blythe and mitchell , 123 ) .
significant contributions to the design of the theo - agent .
thanks also to the entire theo group , which produced the theo system on which theo - agent is built .
theo provides representation , and learning
i am most grateful
mechanisms used by the theo - agent .
finally , thanks to long - ji lin who developed a number of the routines for interfacing from workstations to the robot .
this research nooo123 - 123 - k - 123 and by nasa under research contract
by darpa under
