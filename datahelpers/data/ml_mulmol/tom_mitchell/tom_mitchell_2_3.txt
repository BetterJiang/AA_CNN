abstract .
learning provides a useful tool for the automatic design of autonomous robots .
recent research on learning robot control has predominantly focussed on learning single tasks that were studied in isolation .
if robots encounter a multitude of control learning tasks over their entire lifetime , however , there is an opportunity to transfer knowledge between them .
in order to do so , robots may learn the invariants of the individual tasks and environments .
this task - independent knowledge can be employed to bias generalization when learning control , which reduces the need for real - world experimentation .
we argue that knowledge transfer is essential if robots are to learn control with moderate learning times in complex scenarios .
two approaches to lifelong robot learning which both capture invariant knowledge about the robot and its environments are presented .
both approaches have been evaluated using a hero - 123 mobile robot .
learning tasks included navigation in unknown indoor environments and a simple nd - and - fetch task .
123 why learning ?
traditionally , it has often been assumed in robotics research that accurate a priori knowledge about the robot , its sensors , and most important its environment is avail - able .
by assuming the availability of accurate models of both the world and the robot itself , the kind of environments such robots can operate in , and consequently the kind of tasks such robots can solve , is limited .
limitations especially arise from four
knowledge bottleneck .
a human designer had to provide accurate models of
the world and the robot .
engineering bottleneck .
even if sufciently detailed knowledge is available , making it computer - accessible , i . e . , hand - coding explicit models of robot hard - ware , sensors and environments , has often been found to require unreasonable amounts of programming time .
tractability bottleneck .
it was early recognized that many realistic robot do - mains are too complex to be handled efciently ( schwartz et al . , 123 ) , ( canny ,
123 this paper is also available as technical report iai - tr - 123 - 123 , university of bonn , dept .
of computer science iii , march 123
123 ) , ( bylander , 123 ) .
computational tractability turned out to be a severe ob - stacle for designing control structures for complex robots in complex domains , and robots were far from being reactive .
precision bottleneck .
the robot device must be precise enough to accurately
execute plans that were generated using the internal models of the world .
recent research on autonomous robots has changed the design of autonomous agents and pointed out a promising direction for future research in robotics ( see for exam - ple ( brooks , 123 ) and several papers in ( maes , 123 ) ) .
reactivity and real - time operation have received considerably more attention than , for example , optimal - ity and completeness .
many approaches have dropped the assumption that perfect world knowledge is availablesome systems even operate in the extreme where no domain - specic initial knowledge is available at all .
consequently , todays robots are facing gradually unknown , hostile environments , they have to orient themselves , explore their environments autonomously , recover from failures , and they have to solve whole families of tasks .
if robots lack initial knowledge about themselves and their environments , learning becomes inevitable .
the term learning refers to a variety of algorithms that are characterized by their ability to replace missing or incorrect world knowledge by experimentation , observation , and generalization .
learning robots thus collect parts of their domain knowledge themselves and improve over time .
they are less depen - dent on a human instructor to provide this knowledge beforehand .
a learning robot architecture is typically exible enough to deal with a whole class of environments , robots and / or tasks ( goals ) .
consequently the internal prior knowledge , if available at all , is often too weak to solve a concrete problem off - line .
in order to reach a goal , learning robots rely on the interaction with their environment to extract in - formation .
different learning strategies differ mainly in three aspects : their way to do experimentation ( exploration ) , their way to generalize from the observed experi - ence , and the type and amount of prior knowledge that constrains the space of their internal hypothesizes about the world .
there will be no optimal , general learning technique for autonomous robots , since learning techniques are characterized by a trade - off between the degree of exibility , given by the size of the gaps in the domain knowledge , and the amount of observations required for lling these gaps .
generally speaking , the more universal a robot learning architecture , the more exper - imentation we expect the robot to take to learn successfully , and vice versa .
however , the advantage of applying learning strategies to autonomous robot agents is obvious : learning robots can operate in a whole class of initially unknown environments , and they can compensate for changes , since they can re - adjust their internal belief about the environment and themselves .
moreover , all empirically learned knowledge is grounded in the real world .
it has long been recognized in ai that learning is a key feature for making autonomous agents capable of solving more complex tasks in more realistic environments .
recent research has produced a variety of rigorous learning techniques that allow a robot to acquire huge chunks of knowledge by it - self .
see for example ( mel , 123 ) , ( moore , 123 ) , ( pomerleau , 123 ) , ( tan , 123 ) ,
figure 123 the robot control learning problem .
a robot agent is able to perceive the state of its environment by its sensors , and change it using its effectors ( actions ) .
a reward function that maps sensations to rewards measures the performance of the robot .
the control learning problem is the problem of nding a control policy that generates actions such that the reward is maximized over time .
( mahadevan and connell , 123 ) , ( lin , 123b ) , and ( kuipers and byun , 123 ) .
what exactly is the concrete problem addressed by robot learning ? let us outline a general denition of learning robot control .
assume the robot acts in an environment w ( the world ) .
each time step , the environment is in a certain state z z .
by state we mean the sum of all quantities in the environment that may change during the lifetime of the robot .
the robot is able to ( partially ) perceive the state of its environment through its sensors .
it is also able to act using its effectors , as shown in figure 123
let s denes the set of all possible sensations , and a the set of all actions that the robot can execute .
actions ( including zero - actions , if the robot does nothing ) change the state of the world .
hence , the environment can be understood as a mapping w : z a ( cid : 123 ) ! z from states and actions to states .
for example , imagine an autonomous mobile robot whose task it is to keep the oor clean .
the worlds such a robot will be acting in are buildings , including the obstacles surrounding the robot , the oors , the dirt on the oors , humans that walk by , and so on .
appropriate sensors might include a camera mounted on the robots arm , and the set of all sensations might be the space of all camera images .
actions could be go forward , turn , switch on / off vacuum , and lift arm .
in order to dene the goals of the robot , we assume that the robot is given a reward function r : s ( cid : 123 ) ! ir , that maps sensations to scalar reward values .
the reward evaluates the success of the robot to solve its tasks : in the most simple form the reward is positive ( say 123 ) , if the robot reaches its goals , it is negative ( - 123 ) if the robot fails , and zero otherwise .
positive reward corresponds to pleasure , and negative reward represents pain .
in the mobile robot domain , for example , the reward may be positive if there is no dirt on the oor , and negative reward may be received if the robot collides with the furniture or runs out of battery power .
the control learning
problem is the problem of nding a control function f that generates actions such that the reward is maximized over time .
more formally , a control learning problem can be described in the following way :
control learning problem : hs; a; w; ri ( cid : 123 ) ! f : s ! a such that f maximizes r over time
this formulation of the robot control learning problem has been extensively studied in the eld of reinforcement learning .
( sutton , 123 ) , ( barto et al . , 123 ) .
thus far , most approaches that emerged from this eld have studied robot learning with a minimal set of assumptions : the robot is able to sense , it is able to execute actions , actions have an effect on future sensations , and there is a pre - given reward function that denes the goals of the robot .
the goal of reinforcement learning is to maximize the reward over time .
note that this general denition lacks any specications about the robot at hand , its environment , or the kind of reward functions the robot is expected to face .
hence , approaches that are able to adaptively solve such a robot control learning problem are general learning techniques .
perhaps the most restricting assumption found in most approaches to reinforcement learning is that the robot is able to sense the state of the world reliably .
if this is the case , it sufces to learn the policy as a function of the most recent sensation to action : f : s ! a , i . e . , the control policy is purely reactive .
as barto et al .
pointed out ( barto et al . , 123 ) , the problem of learning a control policy can then be attacked by dynamic programming techniques ( bellman , 123 ) .
as it turns out , even if robots have access to complete state descriptions of their environments , learning control in complex robot worlds with large state spaces is practically not feasible .
this is because it takes often too much experimentation to acquire the knowledge required for maximizing reward , i . e . , to ll the huge knowledge gaps , and robot hardware is slow .
one might argue that better learning techniques have to be invented that decrease the learning complexity , while still being general .
althoughthere is certainly space for better learning and generalization techniques that gradually decrease the amount of experimentation required , it seems unlikely that such techniques will ever be applicable to complex , real - world robot environments with sparse reward and difcult tasks .
the complexity of knowledge acquisition is inherent in the formulation of the problem .
real - world experimentation will be the central bottleneck of any general learning technique that does not utilize prior knowledge about the robot and its environment .
why do natural agents such as humans learn so much better than articial agents ? maybe a better question to ask is : is the learning problem faced by natural agents any simpler than that of articial ones ? we will not attempt to give general answers to these general questions .
we will , however , point out the importance of knowledge transfer and lifelong learning problems in order to make robots to learn more complex
123 the necessity of lifelong agent learning
humans typically encounter a multitude of control learning problems over their entire lifetime , and so do robots .
henceforth , in this paper we will be interested in the lifelong learning problem faced by a robot agent , in which it must learn a collection of control policies for a variety of related control tasks .
each of these control problems , hs; a; wi; rii involves the same robot with the same set of sensors , effectors , and may vary only in the particular environment wi , and in the reward function ri that denes the goal states for this problem .
for example , an industrial mobile robot might face multiplelearning tasks such as shipping packages , delivering mail , supervising critical processes , guarding its work - place at night , and so on .
in this scenario the environment will be the same for all tasks , but the reward function varies .
alternatively , a window - cleaning robot that is able to climb fronts of buildings and move arbitrarily on walls and windows might have the very single task of cleaning windows .
it will , however , face multiple fronts and windows ( environments ) over its lifetime .
in the lifelong learning problem , for each different environment and each reward function the robot agent must nd a different control policy , fi .
the lifelong learning problem of the agent therefore corresponds to a set of control learning problems :
lifelong learning problem : fhs; a; wi; rii ( cid : 123 ) ! fijfi : s ! ag such that fi maximizes ri over time
of course the agent could approach the lifelong learning problem by handling each control learning problem independently .
however , there is an opportunity for the agent to do considerably better .
because these control learning problems are dened in terms of the same s , a , and potentially the same w , the agent should be able to reduce the difculty of solving the i - th control learning problem by using knowledge it acquired from solving earlier control learning problems .
for example , a robot that must learn to deliver laser printer output and to collect trash in the same environment should be able to use much of what it has learned from the rst task to simplify learning the second .
the problem of lifelong learning offers the opportunity for synergy among the different control learning problems , which can speed - up learning over the lifetime of the agent .
viewing robot learning as a lifelong learning problem motivates research on boot - strapping learning algorithms that transfer learned knowledge from one learning task to another .
bootstrapping learning algorithms might start with low - complexity learning problems , and gradually increase the problem solving power to harder prob - lems .
similar to humans , future robots might rst have to learn simple tasks ( such as low - level navigation , hand - eye coordination ) , and , once successful , draw their at - tention to increasingly more difcult and complex learning tasks ( such as picking up
figure 123 the robot we used in all experiments described in this paper is a wheeled hero - 123 robot with a manipulator and a gripper .
it is equipped with two sonar sensors , one on the top of the robot that can be directed by a rotating mirror to give a full 123 sweep ( 123 values ) , and one on the wrist that can be rotated by the manipulator to give a 123 sweep .
sonar sensors return approximate echo distances .
such sensors are inexpensive but very noisy .
and delivering laser printer output ) .
as , for example , singh ( singh , 123b ) and lin ( lin , 123b ) demonstrate , learning related control tasks with increasing complexity can result in a remarkable synergy between these control learning tasks , and an im - proved problem solving power .
in their experiments , simulated mobile robots were able to solve more complex navigation tasks when the robots were given simpler , related learning tasks beforehand .
they also report that their systems were unable to learn the same complex tasks in isolation , pointing out the importance of knowledge transfer for robot learning .
the reminder of the paper is organized as follows .
in the next two sections , we will briey present two approaches to the lifelong learning problem .
both approaches have been implemented and evaluated using a hero - 123 mobile robot with a ma - nipulator shown in figure 123
in the rst approach , called explanation - based neural network learning ( ebnn ) , we will assume that the environment of the agent stays the same for all control learning tasks .
this allows the robot to learn task - independent action models .
once learned , these action models provide a means of transferring knowledge across control learning tasks .
in ebnn , such action models are used to explain and analyze observations .
in section 123 , we drop the assumption of static environments .
instead , we describe a mobile autonomous robot that has to solve the
figure 123 episode : starting with the initial state s123 , the action sequence a123; a123; : : : ; an ( cid : 123 ) 123 was observed to produce the nal reward rn .
the robot agent uses its action models , which capture previous experiences in the same environment , to explain the observed episode and thus bias learning .
see text for an explanation .
same task in different , related environments .
we demonstrate how this robot might learn and transfer environment - independent knowledge that captures the characteris - tics of its sensors , as well as invariant characteristics of the environments .
in section 123 we will review some related approaches to robot learning which also utilize previ - ously learned knowledge .
as we will see , there are several types of techniques that can be grouped into categories .
the paper is concluded by section 123
123 explaining observations in terms of prior experiences
as dened in the previous section , the lifelong learning problem faced by a learning robot agent is the problem of learning collections of tasks in families of environments over the entire lifetime .
in this section we will draw our attention to a particular sub - type of lifelong learning problems , namely to the lifelong learning problem of robots that spend their whole life exclusively in the same environment .
this restricted class of lifelong learning scenarios plays an important role in autonomous agent research .
many prospective robot agents , such as housekeeping robots , industrial robot arms or articial insects , face a variety of learning problems in the very same environment .
in such scenarios , knowledge about the environment can be reused , since the environment stays the same for each task .
the explanation - based neural network learning algorithm ( ebnn ) , which will be presented in this section , transfers task - independent knowledge via learned action models which are learned empirically during problem solving .
123 learning action models
robots observe their environments ( and themselves ) by the effects of their actions .
in other words , each time an action is performed , the robot may use its sensors
to sense the way the world has changed .
let us assume for simplicity that the robot is able to accurately sense the state of the world .
as pointed out in the previous section , learning control reduces to learning a reactive controller123
if the environment is sufciently predictable , neural network learning techniques such as the back - propagation training procedure ( rumelhart et al . , 123 ) can be used to model the environment .
more specically , each time an action is performed , the previous state denoted by s , the action a and the next state s form a training example for the action model network , denoted by m :
h s; a i ( cid : 123 ) ! s
action models123 are thus functions of the type m : s a ( cid : 123 ) ! s .
since we assume that the environment is the same for all tasks , action models capture invariants that allow for transferring knowledge from one task to another .
each individual control learning problem requires a different policy , i . e . , learning a control function fi : s ( cid : 123 ) ! a that , when employed by the agent , maximizes the corresponding reward ri .
in general , fi is difcult to learn directly .
following the ideas of reinforcement learning ( ( samuel , 123 ) , ( sutton , 123 ) , ( barto et al . , 123 ) , ( watkins , 123 ) ) , we decompose the problem of learning fi into the problem of learning an evaluation function , qi , dened over states and actions .
qi : s a ( cid : 123 ) ! ir , where qis; a is the expected future cumulative reward achieved after executing action a in state s .
assume for a moment the agent had learned an accurate evaluation function qi .
then it can easily use this function to select optimal actions that maximize reward over time .
given some state , s , that the agent nds itself in , it computes its control action a by considering its available actions to determine which of them produces the highest qi value :
a = argmaxaa qis; a
since qis; a measures the future cumulative reward , a is the optimal action if qis; a is correct .
the problem of learning a policy is thus reduced to the problem of learning an evaluation function .
how can an agent learn the evaluation function qi ? to see how training data for learning qi might be obtained , consider the scenario depicted in figure 123
the robot
123 we intentionally avoid the complex problem of incomplete and noisy perception perception , since the algorithm presented in this section is kind of orthogonal to research on these issues .
see ( bachrach and mozer , 123 ) , ( chrisman , 123 ) , ( lin and mitchell , 123 ) , ( mozer and bachrach , 123 ) , ( rivest and schapire , 123 ) , ( tan , 123 ) , ( whitehead and ballard , 123 ) for approaches to learning with incomplete perception .
123 see for example ( barto et al . , 123 ) , ( jordan , 123 ) , ( munro , 123 ) , ( thrun , 123 ) for
more approaches to learning action models with neural networks .
agent begins at the initial state s123 and performs the action sequence a123; a123; : : : ; an .
after action an it receives the reward rn = 123 : 123 which , in this example , indicates that the action sequence was considerably successful .
at a rst glance , this episode can be used by the agent to derive training examples for the evaluation function qi by associating the nal reward123 with each state - action pair :
h s123; a123 i ( cid : 123 ) ! rn = 123 : 123 h s123; a123 i ( cid : 123 ) ! rn = 123 : 123
h sn ( cid : 123 ) 123; an ( cid : 123 ) 123 i ( cid : 123 ) ! rn = 123 : 123
an inductive learning method , such as the back - propagation training procedure , can use such training examples to learn qi .
as the number of training examples grows , the agents internal version of qi will improve , resulting in it choosing increasingly effective actions . 123 notice that for each control learning problem hs; a; wi; rii , the agent must learn a distinct qi , since the reward differs for different tasks .
123 the explanation - based neural network learning algorithm
how can the agent use its previously learned knowledge , namely the neural network action models , to guide learning of the evaluation function qi ? as pointed out earlier in this paper , we are interested in the lifelong learning problem .
since we assume throughout this section that the environment is the same for all individual control learning problems , neural network action models capture important domain knowledge that is independent of the particular control learning problem at hand .
in the explanation - based neural network learning algorithm ( ebnn ) , the agent uses these action models to bias learning of the control functions .
123 for simplication of the notation , we assume that reward will only be received at the end of an episode .
ebnn can be applied to arbitrary reward functions .
see ( mitchell and thrun , 123b ) for more details .
123 note that more sophisticated learning schemes for learning evaluation functions have been developed .
in his dissertation , watkins ( watkins , 123 ) describes q - learning , a scheme for learning evaluation function qisk; ak recursively .
in q - learning train - ing patterns are derived based on the maximum possible q value at the next state : hsk; aki ( cid : 123 ) ! maxa qsk+123; a .
indeed , in all our experiments we applied a linear combi - nation of watkins recursive scheme and the non - recursive scheme described in the paper .
this combination is strongly related to suttons t d algorithms ( sutton , 123 ) .
since the exact procedure is not essential for the ideas presented in this paper , we will omit any details .
a second extension , also used widely , is to discount reward over time .
if actions are to be chosen such that the number of actions is minimal , reward is typically discounted with a discount factor 123
the resulting control policy consequently prefers sooner reward to more distant reward .
see ( mitchell and thrun , 123b ) or ( thrun and mitchell , 123 ) for a more detailed description of these issues .
figure 123 fitting slopes : let f be a target function for which three examples hx123; fx123i , hx123; fx123i , and hx123; fx123i are known .
based on these points the learner might generate the hypothesis g .
if the output - input derivatives are also known , the learner can do much better : h .
ebnn works as follows .
suppose the robot faces the control learning problem num - ber i , i . e . , it has to learn the evaluation function qi .
the learning scheme described above provides training values for the desired evaluation function .
repetitive exper - imentation allows the robot to collect enough data to learn the desired qi .
however , this process does not utilize the knowledge represented in the action models .
as - sume the agent has learned already accurate action models that model the effect of actions on the state of the environment .
of course , these action models will only be approximately correct , since they are learned inductively from a nite amount of training data .
in ebnn , the agent employs these action models to explain , analyze and generalize better from the observed episodes .
this is done in the following three
explain .
an explanation is a post - facto prediction of the observed state - action sequence ( dejong and mooney , 123 ) , ( mitchell et al . , 123 ) , ( mitchell and thrun , 123a ) .
starting with the initial state - action pair s123; a123 , the agent post - facto predicts subsequent states up to the nal state sn using its neural network action models .
since the action models are only approximately correct , predic - tions will deviate from the observed states .
analyze .
having explained the whole episode , the explanation is analyzed to extract further information that is useful for learning the evaluation function qi .
in particular , the agent analyzes how a small change of the states features will affect the nal reward , and thus the value of the evaluation function .
this is done by extracting partial derivatives ( slopes ) of the target function qi with respect to the observed states in the episode : first , the agent computes the partial derivative of the nal reward with respect to the nal state sn .
notice that the reward function rs , including its derivative , is assumed to be given to the agent .
these slopes are now propagated backwards through the chain of action model inferences .
neural network action models represent differentiable functions .
using the chain rule of differentiation , the agent computes the partial derivative of the nal reward with respect to the preceding state sn ( cid : 123 ) 123 by multiplying the partial derivative of the reward with the derivative of the neural network action
this process is iterated , yielding all partial derivatives of the nal reward along the whole episode :
here m : s a ( cid : 123 ) ! s denotes the neural network action model .
the reward - state slopes analyze the importance of the state features for the nal reward .
state features believed ( by the action models ) to be irrelevant to achieving the nal reward will have partial derivatives of zero , whereas large derivative values indicate the presence of strongly relevant features .
the analytically extracted slopes approximate the slopes of the target evaluation function qi .
figure 123 illustrates the importance of the slope informa - tion of the target function .
suppose the unknown target function is the function f depicted in figure 123a , and suppose that three training examples are given : x123 , x123 and x123
an arbitrary continuous function approximator , for example a neural network , might hypothesize the function g shown in figure 123b .
if the slopes at these points are known as well , then the resulting function might be much better , as illustrated in figure 123c .
ebnn uses a combined learning scheme utilizing both types of training infor - mation .
the target values ( c . f .
equation 123 ) for learning qi are generated from observation , whereas the target slopes , given by
r h sn ( cid : 123 ) 123; an ( cid : 123 ) 123 i ( cid : 123 ) !
r h s123; a123 i ( cid : 123 ) !
sk; ak ;
are extracted from analyzing the observations using domain knowledge acquired in previous control learning tasks .
both sources of training information , the target values and the target slopes , are used to update the weights and biases of the target network . 123 consequently , the domain knowledge is used to bias the generalization .
since this bias is knowledgeable , it will partially replace the need for real - world experimentation , hence accelerate learning .
123 as simard and colleagues pointed out , the back - propagation algorithm can be extended to t target slopes as well as target values ( simard et al . , 123 ) .
their algorithm tangent prop incrementally updates weights and biases of a neural network such that both the value and the slope error are simultaneously minimized .
123 accommodating imperfect action models
initial experiments with ebnn on a simulated robot navigation task showed a signicant speedup in learning when the robot agent had access to highly accurate action models ( mitchell and thrun , 123b ) .
if the action models are not sufciently accurate , however , the robot performance can seriously suffer from the analysis .
this is because the extracted slopes might very well be wrong and mislead generalization .
this observations raises an essential question for research on lifelong agent learning and knowledge transfer : how can a robot agent deal with incorrect prior knowledge ? clearly , if the agent lacks training experience , any inductively learned bias might be poor and misleading .
but even in the worst case , a learning mechanism that employs previously learned domain knowledge should not take more time for learning control than a learning mechanism that does not utilize prior knowledge at all .
how can the learner avoid the damaging effects arising from poor prior knowledge ?
in ebnn , malicious slopes are identied and their inuence is gradually reduced .
more specically , the accuracy of the extracted slopes is estimated based upon the observed prediction error of the action models .
for example , if the action models perfectly post - facto predict the observed episode , the estimated accuracy of all slopes will be 123
likewise , if for some of the action models in the chain of model derivatives have produced inaccurate state predictions , the corresponding estimated accuracy will be close to zero .
the accuracies of the slopes are now used when training the target network .
since tangent prop allows to weight each training pattern individually , the estimated accuracies can be used to determine the ratio with which value learning and slope learning are weighted when learning the target concept .
more specically , in ebnn the step - size for weight updates is multiplied by the estimated slope accuracy when learning slopes .
as illustrated elsewhere ( mitchell and thrun , 123b ) , weighting slope training by their accuracies was found to successfully reduce the impact of malicious slopes resulting from inaccurate action models .
we evaluated ebnn using nine different sets of action models that were trained with different amounts of training data .
with well - trained action models in the simulated robot domain , the same speedup was observed as before .
with increasing inaccurate action models , the performance of ebnn approached that of standard reinforcement learning without knowledge transfer .
in these experiments , ebnn degraded gracefully with increasing errors in the action models .
these results are intriguing since they indicate the feasibility of lifelong learning algorithms that are able to benet from previously learned bias , even if this bias is poor .
123 a concrete example : learning to pick up a cup
we will present some initial results obtained with our hero - 123 robot .
thus far , we predominantly investigated the effect of previously learned knowledge on the
figure 123 the robot uses its manipulator and a sonar sensor to sense the distance to a cup and to pick it up .
learning speed for new control learning tasks .
we therefore trained the action mod - els beforehand with manually collected training data .
the robot agent had to learn a policy for approaching and grasping a cup .
the robot at hand , shown again in figure 123 , used a hand - mounted sonar sensor to observe its environment .
sonar data was preprocessed to estimate the direction and distance to the closest object , which was used as the world state description .
robot actions were forward ( inches ) , turn ( degrees ) , and grab .
positive reward was received for successful grasps , and negative reward for unsuccessful grasps as well as for losing sight of the cup .
in this experiment , actions were modeled by three separate networks , one for each action .
the networks for the parameterized actions forward ( inches ) and turn ( degrees ) predicted the distance and the orientation to the cup ( one hidden layer with 123 units ) , whereas the model for grab predicted the probability that a pre - given , open - loop grasping routing would manage to pick up the cup ( four hidden units ) .
all action models were pre - learned from approximately two hundred training episodes containing an average of ve steps each , which were manually collected beforehand .
figure 123 shows as an example the action model for the grab action .
since this particular action model modeled the probability of success of the grasping routine , the reward function was simply the identity mapping rs = s ( with the constant derivative 123 ) .
the evaluation function q was also modeled by three distinct networks , one for each action ( 123 hidden units each ) .
after learning the action models , the six training episodes for the evaluation networks shown in figure 123 were provided by a human teacher who controlled the robot .
we applied a version of t d ( sutton , 123 ) with = 123 : 123 and watkins q - learning ( watkins , 123 )
123 deg 123 123 deg 123
figure 123 action model for the action model grab .
the x and y axis measures again the angle and distance to the cup .
the z axis plots the expected success of the grasp action , i . e . , the probability that the grasping succeeds .
with experience replay ( lin , 123a ) for learning control .
figure 123 illustrates the learned q function for the grab action with ( right row ) and without ( left row ) employing the action models and ebnn .
in this initial stage of learning , when little data is yet available , the generalization bias from the pre - learned action models is apparent .
although none of the q functions has yet converged , the q functions learned using ebnn have a shape that is more correct , and which is unlikely to be guessed based solely on the few observed training points with no initial knowledge .
for example , even after presenting six episodes the plain learning procedure predicts positive reward solely based upon the angle of the cup , whereas the combined ebnn method has already learned that grasping will fail if the cup is too far away .
this information , however , is not represented in the training episodes ( figure 123 ) , since there is no single example of an attempt to grasp a cup far away .
it rather seems that the slopes of the model were copied into the target evaluation function .
this illustrates the effect of the slopes in ebnn : the evaluation functions learned with ebnn discovered the correlation of the distance of the cup and the success of the grab action from the neural network action model .
if these action models were learned in an earlier control learning tasks , there would be a signicant synergy effect between them .
the ebnn results in this section are initial .
they are presented because they indicate that task - independent knowledge , once learned , can be successfully transferred by ebnn when learning the grasping task .
they are also presented since they evaluated ebnn in a real robot domain , unlike the results presented in ( mitchell and thrun , 123b ) , ( thrun and mitchell , 123 ) .
however , thus far we did not collect enough
figure 123 six training episodes for learning control , labeled by 123 to 123
the horizontal axis measures the angle of the cup , relative to the robots body , and the vertical axis measures the distance to the cup in a logarithmic scale .
successful grasps are labeled with + , unsuccessful with .
notice that some of the episodes included forwarding and turning .
training data to learn a complete policy for approaching and grasping the cup with either method .
future research will characterize the synergy during the full course of learning until convergence .
it will also experiment with families of related tasks , such as approaching and grasping different objects , including cups that lie on the
123 ebnn and lifelong robot learning
what lesson does ebnn teach us in the context of lifelong agent learning ? in this section we made the restricting assumption that all control learning problems of the robot agent play in the very same environment .
if this is the case , any type of models of the robot and its environment are promising candidates for transferring task - invariant knowledge between the individual control learning problems .
in ebnn , task - independent information is represented by neural network action models .
these models bias generalization when learning control via the process of explaining and analyzing observed episodes .
ebnn is a method for lifelong agent learning , since it learns and re - uses learned knowledge that is independent of the particular control learning problem at hand .
although the initial experiments described in this paper do not fully demonstrate this point , ebnn is able to efciently replace real - world
without analytical training , 123 episodes
with analytical training , 123 episodes
figure 123 evaluation functions for the action grab after presenting 123 ( upper row ) and 123 ( lower row ) training episodes , and without ( left column ) and with ( right column ) using the action model for biasing the generalization .
experimentation by previously learned bias .
in related experiments with a simulated robot presented elsewhere ( mitchell and thrun , 123b ) , we observed a signicant speed - up by a factor of 123 to 123 when pre - learned action models were employed .
it is important to mention that we expect ebnn to be even more efcient if the dimension of the state space is higher .
this is because for each single observation ebnn extracts a d - dimensional slope vector from the environment , if d if the dimension of the input space .
our conjecture is that with accurate domain theories the generalization improvement scales linearly with the number of instance features ( e . g . , we would expect a three order of magnitude improvement for a network with 123 input features ) , since each derivative extracted by ebnn can be viewed roughly as summarizing information equivalent to one new training example for each of the d input dimensions .
this conjecture is roughly consistent with our experiments in
the simulated robot domain .
the relative performance improvement might increase even more if higher - order derivatives are extracted , which is not yet the case in ebnn .
it is important to notice , however , that the quality of the action models is crucial for the performance of ebnn .
with inaccurate action models , ebnn will not perform better than a purely inductive learning procedure .
this does not surprise .
approaches to the lifelong learning problem will always be at most as efcient as non - transferring approaches the robot solves its rst control learning problem in its life .
the desired synergy effect occurs later , when the agent has learned an appropriate domain - specic bias , when it is more mature .
123 lifelong learning in multiple environments
in the previous section we focussed on a certain type of lifelong robot learning problems , namely problems which deal with a single environment .
we will now focus on a more general type of lifelong - learning scenarios , in which the environments differ for the individual control learning tasks .
as pointed out in section 123 , there are quite a few robot learning scenarios where a robot has to learn control in whole families of environments .
for example , a vacuum cleaning robot might have to learn to clean different buildings .
alternatively , an autonomous vehicle might have to learn navigation in several types of terrain .
multi - environment scenarios provide less possibilities for transferring knowledge .
at a rst glance , transferring knowledge seems hard , if the environment is not the same for all control learning tasks .
but even in this type problems there are invariants that may be learned and used as a bias .
the key observation is that all lifelong learning scenarios involve the same robot , the same effectors , the same sensors , although they might have to deal with a variety of environments and control learning tasks therein .
approaches to this type of lifelong learning problems thus aim at learning the characteristics of the sensors and effectors of the robot , as well as invariants in the environments , if there are any .
the principle of learning and transferring task - independent knowledge is the same as in the previous sectionjust the type of knowledge that is transferred differs .
so far , there has been little systematic research on this general type of lifelong robot learning scenarios .
in the remainder of this section we will not describe a general learning mechanism , but a particular approach to learning the characteristics of the robots sensors and the environments .
using the hero - 123 robot as an example , we will demonstrate how inverse sensor models can represent a knowledgeable bias which is independent of the particular environment at hand .
figure 123 the robot explores an unknown environment .
note the obstacle in the middle of the laboratory .
our lab causes many malicious sonar values , and is a hard testbed for sonar - based navigation .
for example , some of the chairs absorb sound almost completely , and are thus hard to detect by sonar .
123 learning to interpret sensations
what kind of invariants can be learned across multiple environments ? two observa - tions are crucial for the approach described here .
first , the robot and its sensors are the same for each environment .
second , there might be regularities in the environ - ments that can be learned as well .
in this section we will describe a neural network approach to learning the characteristics of the robots sensors , as well as those of typical indoor environments .
the task of the mobile hero - 123 robot is to explore unknown buildings ( thrun , 123 ) .
facing a new indoor environment such as the laboratory environment depicted in figure 123 , the robot has to wander around and to use its sensors to avoid collisions .
in the exploration task the robot uses two of its sensors : a rotating sonar sensor is mounted on the head of the robot , as shown in figure 123
the robot also monitors its wheels encoders to detect stuck or slipping wheels .
negative reward is received for collision which can be detected using the wheel encoders .
positive reward is received for entering regions where the robot has not been before .
initially , the robot does not possess any knowledge about its sensors and the environments it will face throughout its life .
sensations are uninterpreted 123 - dimensional vectors of oats ( sonar scans ) , along with a single bit that encodes the state of the wheels .
in order to simplify learning , we assume that the robot has access to its x - y - coordinates
figure 123 task - independent knowledge : ( a ) sensor interpretation network r and ( b ) condence network c .
in a global reference frame ( measures the orientation of the robot ) . 123 navigation in unknown environments is clearly a lifelong robot learning problem .
initially , the robot has to experience collisions , since the initial knowledge does not sufce to prevent from them .
collisions will be penalized by negative reward .
in order to transfer knowledge , the robot then has to learn how to interpret its sensors in order to prevent collisions .
this knowledge is re - used for each environment the robot will face over its lifetime .
after some initial experimentation , the robot should be able to maneuver in new , unknown world while successfully avoiding collisions with
we will now describe a pair of networks which learn sensor - specic knowledge that can be re - used across multiple environment .
the sensor interpretation function , denoted by r , maps sensor informationin our case a sonar scanto reward infor - mation .
more specically , r evaluates for arbitrary locations close to the robot the probability for a collision , based on a single sonar scan .
figure 123a shows r .
input to the network is a vector of sonar values , together with the coordinates of the query point x; y relative to the robots local coordinate frame .
the output of the net - work is 123 , if the interpretation predicts a collision for this point , and 123 , if the network predicts free - space .
this function can be learned by standard supervised learning algorithms , if the robot keeps track of all sensor readings and of all locations where it collided ( and where it did not collide ) .
as the robot operates , it constructs maps that label occupied regions and free - space , which is used to form training examples for the sensor interpretation network r .
as usual , the robot uses back - propagation to learn r .
in order to prevent the precious robot hardware from real - world collisions , we
123 if the robot wheels are perfect , this x - y - position can be calculated internally by dead reckoning .
the robot at hand , however , is not precise enough , and after 123 to 123 minutes of operation the real coordinated usually deviate signicantly from the internal estimates .
an approach to compensating such control errors is briey described in ( thrun , 123 ) .
( a ) sensor interpretation
figure 123 capturing domain knowledge in the networks r and c .
( a ) sensor interpretations and ( b ) condence values are shown for the following examples : 123
hallway , 123
hallway with open door .
hallway with human walking by , 123
corner of a room , 123
corner with obstacle , 123
several obstacles .
lines indicate sonar measure - ments ( distances ) , and the region darkness represents in ( a ) the expected collision reward for surrounding areas ( dark values indicate negative reward ) , and in ( b ) the condence level ( dark values indicate low condence ) .
designed a robot simulator and used it for generating the training patterns for the interpretation network .
in the simulator the whole robot environment is known , and training examples that map sensations to occupancy information can be extracted easily .
in a few minutes the simulated robot explored its simulated world , collecting
figure 123 map building : ( a ) raw , noisy sensor input on an exploration path with 123 measurements .
( b ) resulting model , corresponding to the lab shown in figure 123b ( lab doorway is toward bottom left ) .
the path of the robot is also plotted ( from right to left ) , demonstrating the exploration of the initially unknown lab .
in the next steps , the robot will pass the door of the lab and explore the hallway .
a total of 123 123 training examples .
six examples for sensor interpretation using r are shown in figure 123a .
the circle in the center represents the robot , and the lines orthogonal to the robot represent distances sensed by the sonars .
the probability of negative reward , as predicted by the r , is displayed in the circular regions around the robot : the darker the region , the higher the probability of collision .
as can be seen from this gure , the network r has successfully learned how to interpret sonar signals .
if sonar values are small ( meaning that the sonar signal bounced back early ) , the network predicts an obstacle nearby .
likewise , large value readings are interpreted as free - space .
the network has also learned invariants in the training environments .
for example , typical sizes of walls are known .
in figure 123a - 123 , for example , the robot predicts a long obstacle of a certain width , but behind this obstacle it predicts a considerably low probability for collision .
this prediction is surprising , given that sonar sensors cannot see through obstacles .
in the training environments , however , regions behind walls happened to be free fairly often , which explains the x - ray predictions by the network .
this provides clear evidence that the sensor interpretation network does not only represent knowledge about the robots sensors , but also knowledge about certain invariants of the environments at hand .
figure 123 lab and oor : ( a ) model , and ( b ) condence map .
123 building maps
we will now motivate the need for a second network , the so - called condence network c , which is related to r .
as can bee seen from figure 123a , a single sonar scan can be used to build a small local map around the robot .
if the robot moves around and takes several readings , the resulting interpretations can be combined to form a map123 of the world .
however , there will be points in the world where the interpretations from multiple sensations will disagree .
there are two main reasons for conicting interpretations : first , sonar sensors , like any sensor , are noisy devices .
they often fail to detect certain objects such as smooth surfaces or objects with absorbing surfaces like some of our ofce chairs .
second , sonars are blind for areas behind obstacles .
hence interpretations for regions behind an obstacle will be inaccuratethey truly reect the average probability of occupancy ( the prior ) .
this observation makes it necessary to design a mechanism that resolves conicts between different predictions .
we will approach this problem by explicitly modeling the reliability of the inter - pretations , and using the reliability as weighting factor when combining multiple interpretations .
more specically , consider a testing phase for the interpretation net - work r .
for some of the testing examples , r will manage to predict the target value closely .
for others , however , there will be a signicant residual error for the
123 see ( moravec , 123 ) and ( elfes , 123 ) for related approaches to map building and robot
figure 123 map compiled during the rst aaai autonomous robot competition in san jose , july 123
the map reects the knowledge of the robot after stage 123 of the competition .
in this and the previous stage , the robot had to nd and approach ten visually marked poles , while avoiding collision with obstacles .
lines mark the boundary of the competition area .
the map is somehow inaccurate since it was built on two separate days , and the locations of the robot and the obstacles were not quite identical .
we compensated for such inaccuracies by decaying the condence over
reasons given above .
this error is used to train a second network c , which is called condence network .
the input to c is the same as the input to r .
the target output is the ( normalized ) prediction error of r .
after training , c thus predicts the expected deviation of the sensor interpretations , denoted by cs; x; y .
the condence into these interpretation rs; x; y is thus given by ( cid : 123 ) ln cs; x; y .
when multiple sensor interpretations are combined , the individual interpretation values are averaged , weighted by their condence value .
figure 123b shows condence values for the interpretations showed in figure 123a .
here dark regions correspond to low condence .
likewise , light regions indicate high condence .
as can be seen from these examples , the condence in regions behind obstacles is generally low .
low condence is also predicted for boundary regions between free - space and obstacles , this does not surprise , since sonar values detect objects in a 123 cone .
they do not tell where in the cone an object was found .
consequently , the ne - structure of objects is hard to predict .
similar to the sensor interpretation network , the con -
dence network represents knowledge about the sensors and the environments of the robot that can be transferred across environments .
it is important to notice that both networks , r and c , represent learned knowledge that is independent of the particular environment at hand .
these networks act as a bias in the modeling process , when the robot constructs an internal model of a new environment .
the figures 123 to 123 show some example maps that were obtained for different environments .
we used the networks r and c to nd models of our lab ( 123 ) .
a simple non - adaptive algorithm was used for exploration that basically planned minimal - cost plans to the closest unexplored region .
models of the lab and the hallway are shown in figures 123 and 123
although both networks were trained in simulation , they successfully prevented the robot from collidingwith obstacles .
the same networks r and c were used on a autonomous robot competition , that was held during the aaai conference in july 123 in san jose .
here the task and environment differed from the environments the robot had seen previously : the environment was a large arena lled with paper boxes , and the task was to nd and to navigate to 123 visually marked poles .
the robot had no information about the location of the obstacles and the poles .
it had to explore and model the environment by itself .
the nal implementation ( we named the robot odysseus ) was far more complex than what is described here .
odysseus navigation was map - based , and the adaptive map building procedure was a component of odysseus control .
the robot employed the same networks that were generated by the simulator and tested in our experiments in the lab .
after a total of 123 minutes operation during two separate stages of the competition the robot produced the map shown in figure 123
the networks produced maps that were accurate enough to protect the robot from any collision with an obstacle .
they also allowed the robot to nd close - to - optimal paths to the goal locations .
123 sensor interpretation and lifelong robot learning
what is the contribution of this approach to the problem of lifelong agent learning ? of course , learning sensor interpretation does not necessarily have to be viewed as a method for learning bias , and the presented method is by no means a general learning scheme for lifelong learning problems .
however , for the purpose of this paper we will characterize map building in terms of lifelong robot learning .
obviously in each new environment the robot clearly has to learn control .
this is because initially , when the robot faces a new , unknown environment , its knowledge does clearly not sufce to generate actions that maximize reward .
the robot then gradually learns a policy for action generation step - by - step , and the internal maps provide the freedom for learning , the knowledge gaps lled during the course of interaction with the world .
building internal two - dimensional occupancy maps , together with the static planning routine that generates action using these maps , is learning control .
thus , learning neural network sensor interpretations offers a promising perspective for research in lifelong robot learning in multiple environments .
although both the environments and the goals differed for the individual robot control tasks in
our experiments , we have demonstrated that knowledge transfer could drastically reduce real - world experimentation .
the robot did know about collisions in both real - world environments without ever having experienced one , solely based on previously learned knowledge .
we believe that methods which acquire and utilize models of the sensors , effectors ( not demonstrated here ) , and invariants in the environments are promising candidates for efcient robot learning in more complex lifelong learning
123 related work
approaches to knowledge transfer in the lifelong learning problems can be viewed as techniques for acquiring function approximation bias .
various researchers have noted the importance of learning bias and transferring knowledge across multiple robot control learning tasks .
they can roughly be grouped into the following cate -
learning models .
action models are perhaps the most straightforward way to learn and transfer task - independent knowledge , if all individual control learning tasks deal with a single environment .
approaches that utilize action models dif - fer in the type of action models they employ , and the way the action models are used to bias learning control .
sutton ( sutton , 123 ) presents a system that learns action models , like ebnn .
he uses these models for synthesizing hypothetical experiences that rene the control policy .
in his experiments he found a tremen - dous speedup in learning control when using these action models .
ebnn differs from this approach in that it uses its action models for explaining real - world ob - servations , and in that provides a mechanism to recover from errors in the action models .
lin ( lin , 123a ) describes a mechanism where past experience is mem - orized and repeatedly replayed when learning control .
the collection of past experience forms a non - generalizing action model .
lin also reports signicant speedups when using his replay mechanism .
as mentioned above , experience replay was also used for neural network training in ebnn .
thus far , there has been little research on learning models that can act as a bias in lifelong learning problems with multiple robot environments .
learning behaviors and abstractions .
a second way of learning and transfer - ring knowledge across tasks are behaviors .
behaviors are controllers ( policies ) with low complexityoften the term behavior refers to reactive controllers .
re - inforcement learning , for example , can be viewed as a technique for learning behaviors .
behaviors form abstract action spaces , since the basic actions of the robot might be replaced by the action of invoking a behavior .
thus , with appro - priate behaviors abstract action spaces can be formed , and hierarchies of actions can be identied .
learning behaviors accelerates learning control by restricting the search space of all possible policies , mainly for two reasons .
first , the num - ber of behaviors is often smaller than the number of actions .
second , behaviors
typically are selected for longer periods of time .
the latter argument is usually more important and provides a stronger bias for learning control .
singh ( singh , 123b ) , ( singh , 123a ) reports a technique to learn complex tasks by rst learning controllers for simple tasks based on reinforcement learning .
these controllers represent reactive behaviors .
the high - level policy is learned in the abstract action space formed by the low level behaviors .
in order to rep - resent even the high - level controller as a purely reactive function , singh makes several restricting assumptions on the type of tasks and sensor information .
in his doctoral thesis , lin ( lin , 123b ) describes a related scheme for learning be - haviors , action hierarchies and abstraction .
he assumes that a human instructor teaches a robot a set of elemental behaviors which sufce for all tasks which the robot will face over its lifetime .
unlike singh , his approach does not guarantee that optimal controller can be learned in the limit .
recently , dayan and hinton ( dayan and hinton , 123 ) proposed a system that uses a pre - given hierarchical decomposition to learn control on different levels of abstraction .
each level of abstraction differs in the grain - size of the sensory information , resulting in dif - ferently specialized controllers .
since their system learns reactive controllers on each level using reinforcement learning algorithms , it is unclear for what type of problems this procedure will learn successfully , since essential information may be missed when providing incomplete sensor information to purely reactive
learning inductive function approximation bias .
another , more straightfor - ward approach to learning and transferring knowledge is to learn the inductive bias of the function approximators used for learning control directly .
atkeson ( atkeson , 123 ) presents a scheme for learning distance measures for instance - based , local approximation schemes .
in his algorithm , scaling factors are learned that allows to weight different input features differently .
sutton ( sutton , 123 ) reports a family of learning schemes that allow to learn inductive bias similar to kalman lters ( kalman , 123 ) .
although he did not describe his methods in the context of learning control , his research has been motivated by transferring knowledge across multiple control learning tasks .
learning representations .
representations , together with inductive bias , de - termine the way a function approximator generalizes from examples .
many re - searchers have focussed on learning appropriate representations in order to learn bias .
for example , pratt ( pratt , 123 ) describes several approaches that allow to re - use learned representations in hidden units of neural networks .
although she could empirically demonstrate that this transfer could signicantly reduce the number of training epochs required for the convergence of the back - propagation algorithm , she only found occasional improvements in the generalization .
a sim - ilar technique is reported by sharkey and sharkey ( sharkey and sharkey , 123 ) .
some researchers have studied knowledge transfer if several tasks are learned simultaneously .
for example , suddarth and kergosien ( suddarth and kergosien , 123 ) demonstrated that multiple learning tasks of certain types can success - fully guide and improve generalization .
in his approach , he gives hints to neural
networks in form of additional output units that learn a closely related task .
these hints constrain the internal representation developed by the network .
in a more general way , caruana ( caruana , 123 ) recently proposed to learn whole collections of tasks in parallel , using a shared internal representation .
he con - jectures that multi - task learning will make neural network learning algorithms scale to more complex learning tasks .
both approaches to the lifelong learning problem described in this paper fall into the rst category .
in ebnn , the robot learns action models which bias generalization in the evaluation functions .
sensor interpretation functions are inverse models of the sensors and the environments of the robot .
in the robot exploration tasks , the robot thus learns models of itself and typical aspects of its environments , which bias the construction of the individual maps .
as pointed out earlier , action models are well - suited if the environment is the same for all learning tasks , whereas sensor models are appropriate for lifelong agent learning in multiple environments .
in this paper we have presented a lifelong learning perspective for autonomous robots .
we propose not to study robot learning problems in isolation , but in the context of the multitude of learning problems that a robot will face over its lifetime .
lifelong robot learning opens the opportunity for transfer of learned knowledge .
this knowledge may be used as a bias when learning control .
although control learning methods that allow the transfer of knowledge are more complex as most algorithms that solve isolated control learning problems , robot learning itself be - comes easier .
robots that memorize and transfer knowledge rely less on real - world experimentation and thus learn faster .
this is because previously learned knowledge may act as a knowledgeable bias that may partially replace the pure syntactic bias of inductive learning algorithms .
we have demonstrated with two concrete approaches the potential synergy effect of knowledge transfer .
these approaches addressed two main types of lifelong agent learning scenarios , namely those that are dened in a single environment , and those that are not .
we strongly believe that knowledge transfer is essential for scaling robot learning algorithms to more realistic and complex domains .
exploiting previously learned knowledge simplies learning control .
these results support our fundamental claim that learning becomes easier , if it is embedded into a lifelong learning context .
we thank the cmu robot learning group and the odysseus team at cmu for invalu - able discussion that contributed to this research .
we also thank ryusuke masuoka
figure 123 ( a ) the university of bonn robot rhino ( manufactured by real world interface , inc . ) , ( b ) map ( approximately 123 123 meters ) constructed by rhino at the aaai - 123 robot competition , using the technique described in this paper .
for his invaluable help in rening ebnn .
this research was sponsored in part by the avionics lab , wright research and de - velopment center , aeronautical systems division ( afsc ) , u .
air force , wright - patterson afb , oh 123 - 123 under contract f123 - 123 - c - 123 , arpa order no .
123 and by a grant from siemens corporation .
the views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofcial policies , either expressed or implied , of the u . s .
government or siemens corp .
since this paper was submitted , ebnn was successfully applied to a variety of real - world learning tasks .
in ( mitchell and thrun , 123 , thrun , 123 , thrun , 123a ) , result of applying ebnn to mobile robot navigation using the cmu xavier robot are reported .
ebnn has also been applied to robot perception ( mitchell et al . , 123 ) ,
( osullivan et al . , to appear ) , object recognition ( thrun and mitchell , 123 ) and the game of chess ( thrun , 123b ) .
in ( thrun and mitchell , 123 ) , a denition of the lifelong learning problem in the context of supervised learning can be found .
the approach to interpreting sonar sensors for building occupancy maps reported in sect .
123 has been , with slight modications , successfully employed in the university of bonns entry rhino at the 123 aaai mobile robot competition ( buhmann et al . , to appear ) .
currently , maps are routinely built for large indoor areas .
