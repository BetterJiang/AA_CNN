we consider here the problem of building a never - ending lan - guage learner; that is , an intelligent computer agent that runs forever and that each day must ( 123 ) extract , or read , informa - tion from the web to populate a growing structured knowl - edge base , and ( 123 ) learn to perform this task better than on the previous day .
in particular , we propose an approach and a set of design principles for such an agent , describe a par - tial implementation of such a system that has already learned to extract a knowledge base containing over 123 , 123 beliefs with an estimated precision of 123% after running for 123 days , and discuss lessons learned from this preliminary attempt to build a never - ending learning agent .
we describe here progress toward our longer - term goal of producing a never - ending language learner .
by a never - ending language learner we mean a computer system that runs 123 hours per day , 123 days per week , forever , performing two tasks each day : 123
reading task : extract information from web text to further populate a growing knowledge base of structured facts
learning task : learn to read better each day than the day before , as evidenced by its ability to go back to yester - days text sources and extract more information more ac - the thesis underlying this research is that the vast redun - dancy of information on the web ( e . g . , many facts are stated multiple times in different ways ) will enable a system with the right learning mechanisms to succeed .
one view of this research is that it is a case study in lifelong , or never - ending learning .
a second view is that it is an attempt to advance the state of the art of natural language processing .
a third view is that it is an attempt to develop the worlds largest struc - tured knowledge base one that reects the factual content of the world wide web , and that would be useful to many ai
in this paper , we rst describe a general approach to building a never - ending language learner that uses semi - copyright c ( cid : 123 ) 123 , association for the advancement of articial intelligence ( www . aaai . org ) .
all rights reserved .
supervised learning methods , an ensemble of varied knowl - edge extraction methods , and a exible knowledge base rep - resentation that allows the integration of the outputs of those methods .
we also discuss design principles for implement - ing this approach .
we then describe a prototype implementation of our ap - proach , called never - ending language learner ( nell ) .
at present , nell acquires two types of knowledge : ( 123 ) knowl - edge about which noun phrases refer to which specied semantic categories , such as cities , companies , and sports teams , and ( 123 ) knowledge about which pairs of noun phrases satisfy which specied semantic relations , such as hasof - location ) .
nell learns to acquire these two types of knowledge in a variety of ways .
it learns free - form text patterns for extracting this knowledge from sentences on the web , it learns to extract this knowledge from semi - structured web data such as tables and lists , it learns morphological regularities of instances of categories , and it learns probabilistic horn clause rules that enable it to infer new instances of relations from other relation instances that it has already learned .
finally , we present experiments showing that our imple - mentation of nell , given an initial seed ontology dening 123 categories and 123 relations and left to run for 123 days , populates this ontology with 123 , 123 new facts with esti - mated precision of 123% .
the main contributions of this work are : ( 123 ) progress to - ward an architecture for building a never - ending learning agent , and a set of design principles that help successfully implement that architecture , ( 123 ) a web - scale experimental evaluation of an implementation of that architecture , and ( 123 ) one of the largest and most successful implementations of bootstrap learning to date .
our approach is organized around a shared knowledge base ( kb ) that is continuously grown and used by a collection of learning / reading subsystem components that implement complementary knowledge extraction methods .
the starting kb denes an ontology ( a collection of predicates dening categories and relations ) , and a handful of seed examples for each predicate in this ontology ( e . g . , a dozen example cities ) .
the goal of our approach is to continuously grow this kb by reading , and to learn to read better .
icate instances from text resources , and another which learns to infer relation instances from other beliefs in the kb .
this provides multiple , independent sources of the same types of beliefs .
use coupled semi - supervised learning methods to lever - age constraints between predicates being learned ( carl - son et al .
to provide opportunities for coupling , arrange categories and relations into a taxonomy that de - nes which categories are subsets of which others , and which pairs of categories are mutually exclusive .
addi - tionally , specify the expected category of each relation ar - gument to enable type - checking .
subsystem components and the ki can benet from methods that leverage cou -
distinguish high - condence beliefs in the kb from lower - condence candidates , and retain source justica - tions for each belief .
use a uniform kb representation to capture candidate facts and promoted beliefs of all types , and use associ - ated inference and learning mechanisms that can operate on this shared representation .
ai has a long history of research on autonomous agents , problem solving , and learning , e . g . , soar ( laird , newell , and rosenbloom 123 ) , prodigy ( carbonell et al .
123 ) , eurisko ( lenat 123 ) , act - r ( anderson et al .
123 ) , and icarus ( langley et al .
in comparison , our fo - cus to date has been on semi - supervised learning to read , with less focus on problem - solving search .
nevertheless , earlier work provides a variety of design principles upon which we have drawn .
for example , the role of the kb in our approach is similar to the role of the blackboard in early systems for speech recognition ( erman et al .
123 ) , and the frame - based representation of our kb is a reimple - mentation of the theo system ( mitchell et al .
123 ) which was originally designed to support integrated representation , inference and learning .
there is also previous research on life - long learning , such as thrun and mitchell ( 123 ) , which focuses on using previ - ously learned functions ( e . g . , a robots next - state function ) to bias learning of new functions ( e . g . , the robots control func - tion ) .
banko and etzioni ( 123 ) consider a lifelong learning setting where an agent builds a theory of a domain , and ex - plore different strategies for deciding which of many possi - ble learning tasks to tackle next .
although our current sys - tem uses a simpler strategy of training all functions on each iteration , choosing what to learn next is an important capa - bility for lifelong learning .
our approach employs semi - supervised bootstrap learn - ing methods , which begin with a small set of labeled data , train a model , then use that model to label more data .
yarowsky ( 123 ) uses bootstrap learning to train classiers for word sense disambiguation .
bootstrap learning has also been employed successfully in many applications , includ - ing web page classication ( blum and mitchell 123 ) , and named entity classication ( collins and singer 123 ) .
figure 123 : our never - ending language learner ( nell ) architec - ture .
see approach for an overview of the approach implemented in nell , and implementation for subsystem details .
category and relation instances added to the kb are parti - tioned into candidate facts and beliefs .
the subsystem com - ponents can read from the kb and consult other external re - sources ( e . g . , text corpora or the internet ) , and then propose new candidate facts .
components supply a probability for each proposed candidate and a summary of the source evi - dence supporting it .
the knowledge integrator ( ki ) exam - ines these proposed candidate facts and promotes the most strongly supported of these to belief status .
this ow of pro - cessing is depicted in figure 123
in our initial implementation , this loop operates itera - tively .
on each iteration , each subsystem component is run to completion given the current kb , and then the ki makes decisions on which newly proposed candidate facts to pro - mote .
the kb grows iteration by iteration , providing more and more beliefs that are then used by each subsystem com - ponent to retrain itself to learn to read better on the next iter - ation .
in this way , our approach can be seen as implementing a coupled , semi - supervised learning method in which mul - tiple components learn and share complementary types of knowledge , overseen by the ki .
one can view this approach as an approximation to an expectation maximization ( em ) algorithm in which the e step involves iteratively estimating the truth values for a very large set of virtual candidate be - liefs in the shared kb , and the m step involves retraining the various subsystem component extraction methods .
this kind of iterative learning approach can suffer if label - ing errors accumulate .
to help mitigate this issue , we will allow the system to interact with a human for 123 min - utes each day , to help it stay on track .
however , in the work reported here , we make limited use of human input .
the following design principles are important in imple -
menting our approach : use subsystem components that make uncorrelated errors .
when multiple components make uncorrelated errors , the probability that they all make the same error is the product of their individual error probabilities , resulting in much lower error rates .
learn multiple types of inter - related knowledge .
for ex - ample , we use one component that learns to extract pred -
candidate factsbeliefsdata resources ( e . g . , corpora ) knowledgeintegratorsubsystem componentscplcsealrlcmcknowledge base bootstrap learning approaches can often suffer from se - mantic drift , where labeling errors in the learning process can accumulate ( riloff and jones 123; curran , murphy , and scholz 123 ) .
there is evidence that constraining the learning process helps to mitigate this issue .
for example , if classes are mutually exclusive , they can provide negative examples for each other ( yangarber 123 ) .
relation argu - ments can also be type - checked to ensure that they match expected types ( pasca et al .
carlson et al .
( 123 ) employ such strategies and use multiple extraction methods , which are required to agree .
carlson et al .
refer to the idea of adding many constraints between functions being learned as coupled semi - supervised learning .
chang , ratinov , and roth ( 123 ) also showed that enforcing constraints given as domain knowledge can improve semi - supervised learning .
pennacchiotti and pantel ( 123 ) present a framework for combining the outputs of an ensemble of extraction methods , which they call ensemble semantics .
multi - ple extraction systems provide candidate category instances , which are then ranked using a learned function that uses features from many different sources ( e . g . , query logs , wikipedia ) .
their approach uses a more sophisticated rank - ing method than ours , but is not iterative .
thus , their ideas are complementary to our work , as we could use their rank - ing method as part of our general approach .
other previous work has demonstrated that pattern - based and list - based extraction methods can be combined in a syn - ergistic fashion to achieve signicant improvements in re - call ( etzioni et al .
downey , etzioni , and soder - land ( 123 ) presented a probabilistic model for using and training multiple extractors where the extractors ( in their work , different extraction patterns ) make uncorrelated er - rors .
it would be interesting to apply a similar probabilistic model to cover the setting in this paper , where there are mul - tiple extraction methods which themselves employ multiple extractors ( e . g . , textual patterns , wrappers , rules ) .
nahm and mooney ( 123 ) rst demonstrated that infer - ence rules could be mined from beliefs extracted from text .
our work can also be seen as an example of multi - task learning in which several different functions are trained to - gether , as in ( caruana 123; yang , kim , and xing 123 ) , in order to improve learning accuracy .
our approach involves a kind of multi - task learning of multiple types of functions 123 functions in total in the experiments reported here in which different methods learn different functions with overlapping inputs and outputs , and where constraints pro - vided by the ontology ( e . g . , athlete is a subset of person , and mutually exclusive with city ) support accurate semi - supervised learning of the entire ensemble of functions .
we have implemented a preliminary version of our ap - proach .
we call this implementation never - ending lan - guage learner ( nell ) .
nell uses four subsystem com - ponents ( figure 123 ) : coupled pattern learner ( cpl ) : a free - text extractor which learns and uses contextual patterns like mayor of x and x plays for y to extract instances of categories
and relations .
cpl uses co - occurrence statistics between noun phrases and contextual patterns ( both dened using part - of - speech tag sequences ) to learn extraction patterns for each predicate of interest and then uses those patterns to nd additional instances of each predicate .
relation - ships between predicates are used to lter out patterns that are too general .
cpl is described in detail by carl - son et al .
( 123 ) .
probabilities of candidate instances ex - tracted by cpl are heuristically assigned using the for - mula 123c , where c is the number of promoted patterns that extract a candidate .
in our experiments , cpl was given as input a corpus of 123 billion sentences , which was generated by using the opennlp package123 to extract , to - kenize , and pos - tag sentences from the 123 million web page english portion of the clueweb123 data set ( callan and hoy 123 ) .
coupled seal ( cseal ) : a semi - structured extractor which queries the internet with sets of beliefs from each category or relation , and then mines lists and tables to extract novel instances of the corresponding predicate .
cseal uses mutual exclusion relationships to provide negative examples , which are used to lter out overly gen - eral lists and tables .
cseal is also described by carlson et al .
( 123 ) , and is based on code provided by wang and cohen ( 123 ) .
given a set of seed instances , cseal per - forms queries by sub - sampling beliefs from the kb and using these sampled seeds in a query .
cseal was con - gured to issue 123 queries for each category and 123 queries for each relation , and to fetch 123 web pages per query .
candidate facts extracted by cseal are assigned proba - bilities using the same method as for cpl , except that c is the number of unltered wrappers that extract an instance .
coupled morphological classier ( cmc ) : a set of bi - nary l123 - regularized logistic regression modelsone per categorywhich classify noun phrases based on vari - ous morphological features ( words , capitalization , afxes , parts - of - speech , etc . ) .
beliefs from the kb are used as training instances , but at each iteration cmc is restricted to predicates which have at least 123 promoted instances .
as with cseal , mutual exclusion relationships are used to identify negative instances .
cmc examines candidate facts proposed by other components , and classies up to 123 new beliefs per predicate per iteration , with a mini - mum posterior probability of 123 .
these heuristic mea - sures help to ensure high precision .
rule learner ( rl ) : a rst - order relational learning al - gorithm similar to foil ( quinlan and cameron - jones 123 ) , which learns probabilistic horn clauses .
these learned rules are used to infer new relation instances from other relation instances that are already in the kb .
our implementation of the knowledge integrator ( ki ) promotes candidate facts to the status of beliefs using a hard - coded , intuitive strategy .
candidate facts that have high con - dence from a single source ( those with posterior > 123 ) are promoted , and lower - condence candidates are promoted if
they have been proposed by multiple sources .
ki exploits re - lationships between predicates by respecting mutual exclu - sion and type checking information .
in particular , candidate category instances are not promoted if they already belong to a mutually exclusive category , and relation instances are not promoted unless their arguments are at least candidates for the appropriate category types ( and are not already be - lieved to be instances of a category that is mutually exclusive with the appropriate type ) .
in our current implementation , once a candidate fact is promoted as a belief , it is never de - moted .
the ki is congured to promote up to 123 instances per predicate per iteration , but this threshold was rarely hit in our experiments .
the kb in nell is a reimplementation of the theo frame - based representation ( mitchell et al .
123 ) based on tokyo cabinet123 , a fast , lightweight key / value store .
the kb can handle many millions of values on a single machine .
we conducted an experimental evaluation to explore the fol - can nell learn to populate many different categories ( 123+ ) and relations ( 123+ ) for dozens of iterations of learning and maintain high precision ? how much do the different components contribute to the
promoted beliefs held by nell ?
the input ontology used in our experiments included 123 categories each with 123 seed instances and 123 seed pat - terns for cpl ( derived from hearst patterns ( hearst 123 ) ) .
categories included locations ( e . g . , mountains , lakes , cities , museums ) , people ( e . g . , scientists , writers , politicians , mu - sicians ) , animals ( e . g . , reptiles , birds , mammals ) , organiza - tions ( e . g . , companies , universities , web sites , sports teams ) , and others .
123 relations were included , also with 123 seed instances and 123 negative instances each ( which were typically generated by permuting the arguments of seed in - stances ) .
relations captured relationships between the dif - ferent categories ( e . g . , teamplayssport , bookwriter , compa -
in our experiments , cpl , cseal , and cmc ran once per iteration .
rl was run after each batch of 123 iterations , and the proposed output rules were ltered by a human .
manual approval of these rules took only a few minutes .
to estimate the precision of the beliefs in the kb pro - duced by nell , beliefs from the nal kb were randomly sampled and evaluated by several human judges .
cases of disagreement were discussed in detail before a decision was made .
facts which were once true but are not currently ( e . g . , a former coach of a sports team ) were considered to be cor - rect for this evaluation , as nell does not currently deal with temporal scope in its beliefs .
spurious adjectives ( e . g . , todays chicago tribune ) were allowed , but rare .
musicartistgenre ( nirvana , grunge ) ( dan fouts , nfl ) ( will smith , seven pounds ) cpl ( acrobat reader , file ) athleteplayssport ( scott shields , baseball )
( dublin airport , ireland )
table 123 : example beliefs promoted by nell .
after running for 123 days , nell completed 123 iterations of execution .
123 , 123 beliefs were promoted across all pred - icates , 123% of which were instances of categories and 123% of relations .
example beliefs from a variety of predicates , along with the source components that extracted them , are shown in table 123
following an initial burst of almost 123 , 123 beliefs pro - moted during the rst iteration , nell continued to promote a few thousand more on every successive iteration , indicat - ing strong potential to learn more if it were left to run for a longer time .
figure 123 shows different views of the pro - motion activity of nell over time .
the left - hand gure shows overall numbers of promotions for categories and re - lations in each iteration .
category instances are promoted fairly steadily , while relation instance promotions are spiky .
this is mainly because the rl component only runs every 123 iterations , and is responsible for many of the relation promo - tions .
the right - hand gures are stacked bar plots showing the proportion of predicates with various levels of promo - tion activity during different spans of iterations .
these plots show that instances are promoted for many different cate - gories and relations during the whole run of nell .
to estimate the precision of beliefs promoted during var - ious stages of execution , we considered three time periods : iterations 123 , iterations 123 , and iterations 123
for each of these time periods , we uniformly sampled 123 be - liefs promoted during those periods and judged their cor - rectness .
the results are shown in table 123
during the three periods , the promotion rates are very similar , with between 123 , 123 and 123 , 123 instances promoted .
there is a down - ward trend in estimated precision , going from 123% to 123% to 123% .
taking a weighted average of these three estimates of precision based on numbers of promotions , the overall estimated precision across all 123 iterations is 123% .
figure 123 : promotion activity for beliefs over time .
left : the number of beliefs promoted for all category and relation predicates in each iteration .
periodic spikes among relation predicates occur every 123 iterations after the rl component runs .
center and right : stacked bar plots detailing the proportion of predicates ( and counts of predicates , shown inside the bars ) at various levels of promotion activity over time for categories and relations .
note that , while some predicates become dormant early on , the majority continue to show healthy levels of promotion activity even in later iterations of learning .
estimated precision ( % )
table 123 : estimates of precision ( from 123 sampled beliefs ) and numbers of promoted beliefs across all predicates during iterations 123 , 123 , and 123
note that the estimates of precision only consider beliefs promoted during a time period and ignore beliefs
only a few items were debated by the judges : examples are right posterior , which was judged to not refer to a body part , and green leafy salad , which was judged accept - able as a type of vegetable .
proceedings was promoted as a publication , which we considered incorrect ( it was most likely due to noun - phrase segmentation errors within cpl ) .
two errors were due to languages ( klingon language and mandarin chinese language ) being promoted as ethnic groups .
( southwest , san diego ) was labeled as an in - correct instance of the hasofcesin relation , since south - west airlines does not have an ofcial corporate ofce there .
many system errors were subtle; one might expect a non - native reader of english to make similar mistakes .
to estimate precision at the predicate level , we randomly chose 123 categories and 123 relations which had at least 123 pro - moted instances .
for each chosen predicate , we sampled 123 beliefs from iterations 123 , 123 , and 123 , and judged their correctness .
table 123 shows these predicates and , for each time period , the estimates of precision and the num - ber of beliefs promoted .
most predicates are very accurate , with precision exceeding 123% .
two predicates in partic - ular , cardgame and producttype , fare much worse .
the cardgame category seems to suffer from the abundance of web spam related to casino and card games , which results in parsing errors and other problems .
as a result of this noise , nell ends up extracting strings of adjectives and nouns like deposit casino bonuses free online list as incorrect in -
figure 123 : source counts for beliefs promoted by nell after 123 it - erations .
numbers inside nodes indicate the number of beliefs pro - moted based solely on that component .
numbers on edges indicate beliefs promoted based on evidence from multiple components .
stances of cardgame .
most errors for the producttype rela - tion came from associating product names with more gen - eral nouns that are somehow related to the product but do not correctly indicate what kind of thing the product is , e . g . , ( microsoft ofce , pc ) .
some of these producttype be - liefs were debated by the judges , but were ultimately labeled incorrect , e . g . , ( photoshop , graphics ) .
in our ontology , the category for the second argument of producttype is a general item super - category in the hierarchy; we posit that a more specic product type category might lead to more restrictive type checking .
as described in the implementation section , nell uses a knowledge integrator which promotes high - condence single - source candidate facts , as well as candidate facts with multiple lower - condence sources .
figure 123 illustrates the impact of each component within this integration strategy .
each component is shown containing a count which is the number of beliefs that were promoted based on that source alone having high condence in that belief .
lines connect - ing components are labeled with counts that are the number of beliefs promoted based on those components each hav - ing some degree of condence in that candidate .
cpl and cseal each were responsible for many promoted beliefs on their own .
however , more than half of the beliefs pro - moted by ki were based on multiple sources of evidence .
123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 categories relations 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123% 123% 123% 123% 123% 123% 123 - 123 123 - 123 123 - 123 123 - 123 123 - 123 123 - 123 123+ promotions 123 - 123 123 - 123 123 - 123 none 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123% 123% 123% 123% 123% 123% 123 - 123 123 - 123 123 - 123 123 - 123 123 - 123 123 - 123 123+ promotions 123 - 123 123 - 123 123 - 123 none # of promoted beleifsiterationiterationsiterations% of predicatescategory activityrelation activityoverall activitycategoriesrelationspredicates with : cpl123 , 123cseal123 , 123cmc123rl123 , 123 , 123 , 123 , 123 predicate
table 123 : for selected categories ( top ) and relations ( bottom ) , estimates of precision ( from 123 sampled beliefs ) and counts for beliefs promoted during iterations 123 , 123 , and 123
teamplaysinleague x ranks second in y
hearts full of x cup of aromatic x op - ed page of x
y classic x
table 123 : example free - text patterns learned by cpl .
x and y represent placeholders for noun phrases to be extracted .
while rl was not responsible for many promoted beliefs , those that it did propose with high condence appear to be largely independent from those of the other components .
rl learned an average of 123 novel rules per iteration , of which 123% were approved .
123% of the approved rules implied at least one candidate instance that had not yet been implied by another rule , and those rules implied an average of 123 such instances .
to give a sense of what is being learned by the differ - ent components used in nell , we provide examples for each component .
table 123 shows contextual patterns learned by cpl .
table 123 shows web page wrappers learned by cseal .
example weights from the logistic regression clas - siers learned by cmc are shown in table 123
finally , exam - ple rules induced by rl are shown in table 123
supplementary online materials several types of sup - plementary materials from our evaluation are posted online123 , including : ( 123 ) all promoted instances , ( 123 ) all categories , re - lations , and seed instances , ( 123 ) all labeled instances sampled for estimating precision , ( 123 ) all patterns promoted by cpl , and ( 123 ) all rules learned by rl .
pos=dt jj nn
table 123 : example feature weights induced by the morphology classier .
positive and negative weights indicate positive and neg - ative impacts on predicted probabilities , respectively .
note that mountain and college have different weights when they begin or end an instance .
the learned model uses part - of - speech features to identify typical music group names ( e . g . , the beatles , the ra - mones ) , as well as prexes to disambiguate art movements from , say , academic elds and religions .
these results are promising .
nell maintained high preci - sion for many iterations of learning with a consistent rate of knowledge accumulation , all with a very limited amount of human guidance .
we consider this to be signicant progress toward our goal of building a never - ending language learner .
in total , nell learned 123 coupled functions , since 123 differ - ent subsystems ( cmc , cpl , and cseal ) learn about 123 categories , and 123 different subsystems ( cpl , cseal , and rl ) learn about 123 relations .
<a href=d author . aspx ? a= ( x ) > - < / li> <li> ( x ) by ( y ) &#123;
table 123 : examples of web page extraction templates learned by the cseal subsystem .
( x ) and ( y ) represent placeholders for instances to be extracted ( categories have only one placeholder; relations have two ) .
athleteplayssport ( x , basketball ) athleteinleague ( x , nba ) athleteinleague ( x , y ) cityinstate ( x , y ) newspaperincity ( x , new york ) companyeconomicsector ( x , media ) , generalizations ( x , blog )
teamwontrophy ( x , stanley cup ) athleteplaysforteam ( x , z ) , teamplaysinleague ( z , y ) citycapitalofstate ( x , y ) , cityincountry ( x , usa )
table 123 : example horn clauses induced by the rule learner .
probabilities indicate the conditional probability that the literal to the left of is true given that the literals to the right are satised .
each rule captures an empirical regularity among the relations mentioned by the rule .
the rule marked with was rejected during human inspection .
the stated goal for the system is to each day read more of the web to further populate its kb , and to each day learn to read more facts more accurately .
as the kb growth over the past 123 days illustrate , the system does read more be - liefs each day .
each day it also learns new extraction rules to further populate its kb , new extractors based on morpho - logical features , new horn clause rules that infer unread be - liefs from other beliefs in the kb , and new url - specic extractors that leverage html structure .
although nells ongoing learning allows it to extract more facts each day , the precision of the extracted facts declines slowly over time .
in part this is due to the fact that the easiest extractions oc - cur during early iterations , and later iterations demand more accurate extractors to achieve the same level of precision .
however , it is also the case that nell makes mistakes that lead to learning to make additional mistakes .
although we consider the current system promising , much research re - mains to be done .
the importance of our design principle of using compo - nents which make mostly independent errors is generally supported by the results .
more than half of the beliefs were promoted based on evidence from multiple sources .
how - ever , in looking at errors made by the system , it is clear that cpl and cmc are not perfectly uncorrelated in their errors .
as an example , for the category bakedgood , cpl learns the pattern x are enabled in because of the believed instance cookies .
this leads cpl to extract persistent cookies as a candidate bakedgood .
cmc outputs high probability for phrases that end in cookies , and so persistent cookies is promoted as a believed instance of bakedgood .
this behavior , as well as the slow but steady decline in precision of beliefs promoted by nell , suggests an oppor - tunity for leveraging more human interaction in the learning process .
currently , such interaction is limited to approving or rejecting inference rules proposed by rl .
however , we plan to explore other forms of human supervision , limited to approximately 123 minutes per day .
in particular , ac -
tive learning ( settles 123 ) holds much promise by allowing nell to ask queries about its beliefs , theories , or even features about which it is uncertain .
for example , a pattern like x are enabled in is only likely to occur with a few instances of the bakedgood category .
this could be a poor pattern that leads to semantic drift , or it could be an oppor - tunity to discover some uncovered subset of the bakedgood category .
if nell can adequately identify such opportuni - ties for knowledge , a human can easily provide a label for this single pattern and convey a substantial amount of infor - mation in just seconds .
previous work has shown that label - ing features ( e . g . , context patterns ) rather than instances can lead to signicant improvements in terms of reducing human annotation time ( druck , settles , and mccallum 123 ) .
we have proposed an architecture for a never - ending lan - guage learning agent , and described a partial implementa - tion of that architecture which uses four subsystem com - ponents that learn to extract knowledge in complimentary ways .
after running for 123 days , this implementation pop - ulated a knowledge base with over 123 , 123 facts with an estimated precision of 123% .
these results illustrate the benets of using a diverse set of knowledge extraction methods which are amenable to learning , and a knowledge base which allows the storage of candidate facts as well as condent beliefs .
there are many opportunities for improvement , though , including : ( 123 ) self - reection to decide what to do next , ( 123 ) more effective use of 123 minutes of daily human interaction , ( 123 ) discov - ery of new predicates to learn , ( 123 ) learning additional types of knowledge about language , ( 123 ) entity - level ( rather than string - level ) modeling , and ( 123 ) more sophisticated proba - bilistic modeling throughout the implementation .
langley , p . ; mckusick , k .
b . ; allen , j .
a . ; iba , w .
f . ; and thompson , k .
a design for the icarus architecture .
sigart bull .
123 ( 123 ) : 123
lenat , d .
eurisko : a program that learns new heuristics and domain concepts .
123 ( 123 - 123 ) : 123
mitchell , t .
m . ; allen , j . ; chalasani , p . ; cheng , j . ; etzioni , o . ; ringuette , m .
n . ; and schlimmer , j .
theo : a framework for self - improving systems .
for intelli - nahm , u .
y . , and mooney , r .
a mutually benecial integration of data mining and information extraction .
of aaai .
pasca , m . ; lin , d . ; bigham , j . ; lifchits , a . ; and jain , a .
names and similarities on the web : fact extraction in the fast lane .
in proc .
of acl .
pennacchiotti , m . , and pantel , p .
entity extraction via ensemble semantics .
in proc .
of emnlp .
quinlan , j .
r . , and cameron - jones , r .
foil : a midterm report .
in proc .
of ecml .
riloff , e . , and jones , r .
learning dictionaries for in - formation extraction by multi - level bootstrapping .
in proc .
settles , b .
active learning literature survey .
computer sciences technical report 123 , university of wisconsin thrun , s . , and mitchell , t .
lifelong robot learning .
in robotics and autonomous systems , volume 123 , 123
wang , r .
c . , and cohen , w .
character - level analy - sis of semi - structured documents for set expansion .
in proc .
yang , x . ; kim , s . ; and xing , e .
heterogeneous mul - titask learning with joint sparsity constraints .
in nips 123
yangarber , r .
counter - training in discovery of se - mantic patterns .
in proc .
of acl .
yarowsky , d .
unsupervised word sense disambigua - tion rivaling supervised methods .
in proc .
of acl .
this work is supported in part by darpa ( under con - tract numbers fa123 - 123 - 123 - 123 and af123 - 123 - c - 123 ) , google , a yahoo ! fellowship to andrew carlson , and the brazilian research agency cnpq .
we also gratefully ac - knowledge jamie callan for the clueweb123 web crawl and yahoo ! for use of their m123 computing cluster .
we thank the anonymous reviewers for their helpful comments .
