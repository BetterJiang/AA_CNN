unsupervised learning algorithms aim to discover the structure hidden in the data , and to learn representations that are more suitable as input to a supervised machine than the raw input .
many unsupervised methods are based on reconstructing the input from the representation , while constraining the representation to have cer - tain desirable properties ( e . g .
low dimension , sparsity , etc ) .
others are based on approximating density by stochastically reconstructing the input from the repre - sentation .
we describe a novel and efcient algorithm to learn sparse represen - tations , and compare it theoretically and experimentally with a similar machine trained probabilistically , namely a restricted boltzmann machine .
we propose a simple criterion to compare and select different unsupervised machines based on the trade - off between the reconstruction error and the information content of the representation .
we demonstrate this method by extracting features from a dataset of handwritten numerals , and from a dataset of natural image patches .
we show that by stacking multiple levels of such machines and by training sequentially , high - order dependencies between the input observed variables can be captured .
one of the main purposes of unsupervised learning is to produce good representations for data , that can be used for detection , recognition , prediction , or visualization .
good representations eliminate irrelevant variabilities of the input data , while preserving the information that is useful for the ul - timate task .
one cause for the recent resurgence of interest in unsupervised learning is the ability to produce deep feature hierarchies by stacking unsupervised modules on top of each other , as pro - posed by hinton et al .
( 123 ) , bengio et al .
( 123 ) and our group ( 123 , 123 ) .
the unsupervised module at one level in the hierarchy is fed with the representation vectors produced by the level below .
higher - level representations capture high - level dependencies between input variables , thereby improving the ability of the system to capture underlying regularities in the data .
the output of the last layer in the hierarchy can be fed to a conventional supervised classier .
a natural way to design stackable unsupervised learning systems is the encoder - decoder paradigm ( 123 ) .
an encoder transforms the input into the representation ( also known as the code or the feature vector ) , and a decoder reconstructs the input ( perhaps stochastically ) from the repre - sentation .
pca , auto - encoder neural nets , restricted boltzmann machines ( rbms ) , our previous sparse energy - based model ( 123 ) , and the model proposed in ( 123 ) for noisy overcomplete channels are just examples of this kind of architecture .
the encoder / decoder architecture is attractive for two rea - sons : 123
after training , computing the code is a very fast process that merely consists in running the input through the encoder; 123
reconstructing the input with the decoder provides a way to check that the code has captured the relevant information in the data .
some learning algorithms ( 123 ) do not have a decoder and must resort to computationally expensive markov chain monte carlo ( mcmc ) sam - pling methods in order to provide reconstructions .
other learning algorithms ( 123 , 123 ) lack an encoder , which makes it necessary to run an expensive optimization algorithm to nd the code associated with each new input sample .
in this paper we will focus only on encoder - decoder architectures .
in general terms , we can view an unsupervised model as dening a distribution over input vectors y through an energy function e ( y , z , w ) :
p ( y |w ) = zz
p ( y , z|w ) = rz ee ( y , z , w ) ry , z ee ( y , z , w )
where z is the code vector , w the trainable parameters of encoder and decoder , and is an arbitrary positive constant .
the energy function includes the reconstruction error , and perhaps other terms as well .
for convenience , we will omit w from the notation in the following .
training the machine to model the input distribution is performed by nding the encoder and decoder parameters that minimize a loss function equal to the negative log likelihood of the training data under the model .
for a single training sample y , the loss function is
l ( w , y ) =
the rst term is the free energy f ( y ) .
assuming that the distribution over z is rather peaked , it can be simpler to approximate this distribution over z by its mode , which turns the marginalization over z into a minimization :
l ( w , y ) = e ( y , z ( y ) ) +
where z ( y ) is the maximum likelihood value z ( y ) = argminze ( y , z ) , also known as the optimal code .
we can then dene an energy for each input point , that measures how well it is reconstructed by the model :
f ( y ) = e ( y , z ( y ) ) = lim
the second term in equation 123 and 123 is called the log partition function , and can be viewed as a penalty term for low energies .
it ensures that the system produces low energy only for input vectors that have high probability in the ( true ) data distribution , and produces higher energies for all other input vectors ( 123 ) .
the overall loss is the average of the above over the training set .
regardless of whether only z or the whole distribution over z is considered , the main difculty with this framework is that it can be very hard to compute the gradient of the log partition function in equation 123 or 123 with respect to the parameters w .
efcient methods shortcut the computation by drastically and cleverly reducing the integration domain .
for instance , restricted boltzmann ma - chines ( rbm ) ( 123 ) approximate the gradient of the log partition function in equation 123 by sampling values of y whose energy will be pulled up using an mcmc technique .
by running the mcmc for a short time , those samples are chosen in the vicinity of the training samples , thereby ensuring that the energy surface forms a ravine around the manifold of the training samples .
this is the basis of the contrastive divergence method ( 123 ) .
the role of the log partition function is merely to ensure that the energy surface is lower around training samples than anywhere else .
the method proposed here eliminates the log partition function from the loss , and replaces it by a term that limits the volume of the input space over which the energy surface can take a low value .
this is performed by adding a penalty term on the code rather than on the input .
while this class of methods does not directly maximize the likelihood of the data , it can be seen as a crude approximation of it .
to understand the method , we rst note that if for each vector y , there exists a corresponding optimal code z ( y ) that makes the reconstruction error ( or energy ) f ( y ) zero ( or near zero ) , the model can perfectly reconstruct any input vector .
this makes the energy surface at and indiscriminate .
on the other hand , if z can only take a small number of different values ( low entropy code ) , then the energy f ( y ) can only be low in a limited number of places ( the y s that are reconstructed from this small number of z values ) , and the energy cannot
more generally , a convenient method through which at energy surfaces can be avoided is to limit the maximum information content of the code .
hence , minimizing the energy f ( y ) together with the information content of the code is a good substitute for minimizing the log partition function .
a popular way to minimize the information content in the code is to make the code sparse or low - dimensional ( 123 ) .
this technique is used in a number of unsupervised learning methods , including pca , auto - encoders neural network , and sparse coding methods ( 123 , 123 , 123 , 123 ) .
in sparse methods , the code is forced to have only a few non - zero units while most code units are zero most of the time .
sparse - overcomplete representations have a number of theoretical and practical advantages , as demonstrated in a number of recent studies ( 123 , 123 , 123 ) .
in particular , they have good robustness to noise , and provide a good tiling of the joint space of location and frequency .
in addition , they are advantageous for classiers because classication is more likely to be easier in higher dimensional spaces .
this may explain why biology seems to like sparse representations ( 123 ) .
in our context , the main advantage of sparsity constraints is to allow us to replace a marginalization by a minimization , and to free ourselves from the need to minimize the log partition function explicitly .
in this paper we propose a new unsupervised learning algorithm called sparse encoding symmetric machine ( sesm ) , which is based on the encoder - decoder paradigm , and which is able to produce sparse overcomplete representations efciently without any need for lter normalization ( 123 , 123 ) or code saturation ( 123 ) .
as described in more details in sec .
123 and 123 , we consider a loss function which is a weighted sum of the reconstruction error and a sparsity penalty , as in many other unsupervised learning algorithms ( 123 , 123 , 123 ) .
encoder and decoder are constrained to be symmetric , and share a set of linear lters .
although we only consider linear lters in this paper , the method allows the use of any differentiable function for encoder and decoder .
we propose an iterative on - line learning algorithm which is closely related to those proposed by olshausen and field ( 123 ) and by us previously ( 123 ) .
the rst step computes the optimal code by minimizing the energy for the given input .
the second step updates the parameters of the machine so as to minimize the energy .
in sec .
123 , we compare sesm with rbm and pca .
following ( 123 ) , we evaluate these methods by measuring the reconstruction error for a given entropy of the code .
in another set of experiments , we train a classier on the features extracted by the various methods , and measure the classication error on the mnist dataset of handwritten numerals .
interestingly , the machine achieving the best recognition performance is the one with the best trade - off between rmse and entropy .
in sec .
123 , we compare the lters learned by sesm and rbm for handwritten numerals and natural image patches .
in sec . 123 . 123 , we describe a simple way to produce a deep belief net by stacking multiple levels of sesm modules .
the representational power of this hierarchical non - linear feature extraction is demonstrated through the unsupervised discovery of the numeral class labels in the high - level code .
in this section we describe a sparse encoding symmetric machine ( sesm ) having a set of linear l - ters in both encoder and decoder .
however , everything can be easily extended to any other choice of parameterized functions as long as these are differentiable and maintain symmetry between encoder and decoder .
let us denote with y the input dened in rn , and with z the code dened in rm , where m is in general greater than n ( for overcomplete representations ) .
let the lters in encoder and decoder be the columns of matrix w rn m , and let the biases in the encoder and decoder be denoted by benc rm and bdec rn , respectively .
then , encoder and decoder compute :
fenc ( y ) = w t y + benc ,
fdec ( z ) = w l ( z ) + bdec
where the function l is a point - wise logistic non - linearity of the form :
with g xed gain .
the system is characterized by an energy measuring the compatibility between pairs of input y and latent code z , e ( y , z ) ( 123 ) .
the lower the energy , the more compatible ( or likely ) is the pair .
we dene the energy as :
l ( x ) = 123 / ( 123 + exp ( gx ) ) ,
e ( y , z ) = ekz fenc ( y ) k123
123 + ky fdec ( z ) k123
during training we minimize the following loss :
l ( w , y ) = e ( y , z ) + sh ( z ) + rkw k123
the rst term tries to make the output of the encoder as similar as possible to the code z .
the second term is the mean - squared error between the input y and the reconstruction provided by the decoder .
123 + sh ( z ) + rkw k123
= ekz fenc ( y ) k123
123 + ky fdec ( z ) k123
acts independently on each code unit and it is dened as h ( z ) = pm
the third term ensures the sparsity of the code by penalizing non zero values of code units; this term i=123 log ( 123+l123 ( zi ) ) , ( correspond - ing to a factorized student - t prior distribution on the non linearly transformed code units ( 123 ) through the logistic of equation 123 ) .
the last term is an l123 regularization on the lters to suppress noise and favor more localized lters .
the loss formulated in equation 123 combines terms that characterize also other methods .
for instance , the rst two terms appear in our previous model ( 123 ) , but in that work , the weights of encoder and decoder were not tied and the parameters in the logistic were up - dated using running averages .
the second and third terms are present in the decoder - only model proposed in ( 123 ) .
the third term was used in the encoder - only model of ( 123 ) .
besides the already - mentioned advantages of using an encoder - decoder architecture , we point out another good feature of this algorithm due to its symmetry .
a common idiosyncrasy for sparse - overcomplete methods using both a reconstruction and a sparsity penalty in the objective function ( second and third term in equation 123 ) , is the need to normalize the basis functions in the decoder during learning ( 123 , 123 ) with somewhat ad - hoc technique , otherwise some of the basis functions collapse to zero , and some blow up to innity .
because of the sparsity penalty and the linear reconstruction , code units become tiny and are compensated by the lters in the decoder that grow without bound .
even though the overall loss decreases , training is unsuccessful .
unfortunately , simply normalizing the lters makes less clear which objective function is minimized .
some authors have proposed quite expensive meth - ods to solve this issue : by making better approximations of the posterior distribution ( 123 ) , or by using sampling techniques ( 123 ) .
in this work , we propose to enforce symmetry between encoder and decoder ( through weight sharing ) so as to have automatic scaling of lters .
their norm cannot possibly be large because code units , produced by the encoder weights , would have large values as well , producing bad reconstructions and increasing the energy ( the second term in equation 123 and
123 learning algorithm
learning consists of determining the parameters in w , benc , and bdec that minimize the loss in equation 123
as indicated in the introduction , the energy augmented with the sparsity constraint is minimized with respect to the code to nd the optimal code .
no marginalization over code distribu - tion is performed .
this is akin to using the loss function in equation 123
however , the log partition function term is dropped .
instead , we rely on the code sparsity constraints to ensure that the energy surface is not at .
since the second term in equation 123 couples both z and w and bdec , it is not straightforward to minimize this energy with respect to both .
on the other hand , once z is given , the minimization with respect to w is a convex quadratic problem .
vice versa , if the parameters w are xed , the optimal code z that minimizes l can be computed easily through gradient descent .
this suggests the following iterative on - line coordinate descent learning algorithm : 123
for a given sample y and parameter setting , minimize the loss in equation 123 with respect to z by gradient descent to obtain the optimal code z 123
clamping both the input y and the optimal code z found at the previous step , do one step of gradient descent to update the parameters .
unlike other methods ( 123 , 123 ) , no column normalization of w is required .
also , all the parameters are updated by gradient descent unlike in our previous work ( 123 ) where some parameters are updated using a moving average .
after training , the system converges to a state where the decoder produces good reconstructions from a sparse code , and the optimal code is predicted by a simple feed - forward propagation through
123 comparative coding analysis
in the following sections , we mainly compare sesm with rbm in order to better understand their differences in terms of maximum likelihood approximation , and in terms of coding efciency and as explained in the introduction , rbms minimize an approximation of the negative log likelihood of the data under the model .
an rbm is a binary stochastic symmetric machine dened
although this is not by an energy function of the form : e ( y , z ) = z t w t y bt obvious at rst glance , this energy can be seen as a special case of the encoder - decoder architecture that pertains to binary data vectors and code vectors ( 123 ) .
training an rbm minimizes an approxima - tion of the negative log likelihood loss function 123 , averaged over the training set , through a gradient descent procedure .
instead of estimating the gradient of the log partition function , rbm training uses contrastive divergence ( 123 ) , which takes random samples drawn over a limited region around the training samples .
the loss becomes :
l ( w , y ) =
because of the rbm architecture , given a y , the components of z are independent , hence the sum over congurations of z can be done independently for each component of z .
sampling y in the neighborhood is performed with one , or a few alternated mcmc steps over y , and z .
this means that only the energy of points around training samples is pulled up .
hence , the likelihood function takes the right shape around the training samples , but not necessarily everywhere .
however , the code vector in an rbm is binary and noisy , and one may wonder whether this does not have the effect of surreptitiously limiting the information content of the code , thereby further minimizing the log partition function as a bonus .
rbm and sesm have almost the same architecture because they both have a symmetric encoder and decoder , and a logistic non - linearity on the top of the encoder .
however , rbm is trained using ( approximate ) maximum likelihood , while sesm is trained by simply minimizing the average energy f ( y ) of equation 123 with an additional code sparsity term .
sesm relies on the sparsity term to prevent at energy surfaces , while rbm relies on an explicit contrastive term in the loss , an approximation of the log partition function .
also , the coding strategy is very different because code units are noisy and binary in rbm , while they are quasi - binary and sparse in sesm .
features extracted by sesm look like object parts ( see next section ) , while features produced by rbm lack an intuitive interpretation because they aim at modeling the input distribution and they are used in a
123 experimental comparison
p n ky fdec ( z ) k123
in the rst experiment we have trained sesm , rbm , and pca on the rst 123 digits in the mnist training dataset ( 123 ) in order to produce codes with 123 components .
similarly to ( 123 ) we have collected test image codes after the logistic non linearity ( except for pca which is linear ) , and we have measured the root mean square error ( rmse ) and the entropy .
sesm was run for different values of the sparsity coefcient s in equation 123 ( while all other parameters are left unchanged , see 123 , where z is the uniformly next section for details ) .
the rmse is dened as 123 quantized code produced by the encoder , p is the number of test samples , and is the estimated variance of units in the input y .
assuming to encode the ( quantized ) code units independently and with the same distribution , the lower bound on the number of bits required to encode each of them p m , where ci is the number of counts in the i - th bin , and q is the number of quantization levels .
the number of bits per pixel is then equal to : m n hc . u . .
unlike in ( 123 , 123 ) , the reconstruction is done taking the quantized code in order to measure the robustness of the code to the quantization noise .
as shown in g .
123 - c , rbm is very robust to noise in the code because it is trained by sampling .
the opposite is true for pca which achieves the lowest rmse when using high precision codes , but the highest rmse when using a coarse quantization .
sesm seems to give the best trade - off between rmse and entropy .
123 - d / f compare the features learned by sesm and rbm .
despite the similarities in the architecture , lters look quite different in general , revealing two different coding strategies : distributed for rbm , and sparse for sesm .
is given by : hc . u
p m log123
in the second experiment , we have compared these methods by means of a supervised task in order to assess which method produces the most discriminative representation .
since we have available also the labels in the mnist , we have used the codes ( produced by these machines trained unsupervised ) as input to the same linear classier .
this is run for 123 epochs to minimize the squared error between outputs and targets , and has a mild ridge regularizer .
123 - a / b show the result of these experiments in addition to what can be achieved by a linear classier trained on the raw pixel data .
note that : 123 ) training on features instead of raw data improves the recognition ( except for pca
symmetric sparse coding rbm pca
pca : quantization in 123 bins pca : quantization in 123 bins rbm : quantization in 123 bins rbm : quantization in 123 bins sparse coding : quantization in 123 bins sparse coding : quantization in 123 bins
figure 123 : ( a ) - ( b ) error rate on mnist training ( with 123 , 123 and 123 samples per class ) and test set produced by a linear classier trained on the codes produced by sesm , rbm , and pca .
the entropy and rmse refers to a quantization into 123 bins .
the comparison has been extended also to the same classier trained on raw pixel data ( showing the advantage of extracting features ) .
the error bars refer to 123 std .
of the error rate for 123 random choices of training datasets ( same splits for all methods ) .
the parameter s in eq .
123 takes values : 123 , 123 , 123 , 123 , 123 .
( c ) comparison between sesm , rbm , and pca when quantizing the code into 123 and 123 bins .
( d ) random selection from the 123 linear lters that were learned by sesm ( s = 123 ) .
( e ) some pairs of original and reconstructed digit from the code produced by the encoder in sesm ( feed - forward propagation through encoder and decoder ) .
( f ) random selection of lters learned by rbm .
( g ) back - projection in image space of the lters learned in the second stage of the hierarchical feature extractor .
the second stage was trained on the non linearly transformed codes produced by the rst stage machine .
the back - projection has been performed by using a 123 - of - 123 code in the second stage machine , and propagating this through the second stage decoder and rst stage decoder .
the lters at the second stage discover the class - prototypes ( manually ordered for visual convenience ) even though no class label was ever used during training .
( h ) feature extraction from 123x123 natural image patches : some lters that were learned .
when the number of training samples is small ) , 123 ) rbm performance is competitive overall when few training samples are available , 123 ) the best performance is achieved by sesm for a sparsity level which trades off rmse for entropy ( overall for large training sets ) , 123 ) the method with the best rmse is not the one with lowest error rate , 123 ) compared to a sesm having the same error rate rbm is more costly in terms of entropy .
this section describes some experiments we have done with sesm .
the coefcient e in equation 123 has always been set equal to 123 , and the gain in the logistic have been set equal to 123 in order to achieve a quasi - binary coding .
the parameter s has to be set by cross - validation to a value which depends on the level of sparsity required by the specic application .
123 handwritten digits
123 - b / e shows the result of training a sesm with s is equal to 123 .
training was performed on 123 digits scaled between 123 and 123 , by setting r to 123 ( in equation 123 ) with a learning rate equal to 123 ( decreased exponentially ) .
filters detect the strokes that can be combined to form a digit .
even if the code unit activation has a very sparse distribution , reconstructions are very good ( no minimization in code space was performed ) .
123 . 123 hierarchical features
a hierarchical feature extractor can be trained layer - by - layer similarly to what has been proposed in ( 123 , 123 ) for training deep belief nets ( dbns ) .
we have trained a second ( higher ) stage machine on the non linearly transformed codes produced by the rst ( lower ) stage machine described in the previous example .
we used just 123 codes to produce a higher level representation with just 123 components .
since we aimed to nd a 123 - of - 123 code we increased the sparsity level ( in the second stage machine ) by setting s to 123
despite the completely unsupervised training procedure , the feature detectors in the second stage machine look like digit prototypes as can be seen in g .
the hierarchical unsupervised feature extractor is able to capture higher order correlations among the input pixel intensities , and to discover the highly non - linear mapping from raw pixel data to the class labels .
changing the random initialization can sometimes lead to the discover of two different shapes of 123 without a unit encoding the 123 , for instance .
nevertheless , results are qualitatively very similar to this one .
for comparison , when training a dbn , prototypes are not recovered because the learned code is distributed among units .
123 natural image patches
a sesm with about the same set up was trained on a dataset of 123 123x123 natural image patches randomly extracted from the berkeley segmentation dataset ( 123 ) .
the input images were simply scaled down to the range ( 123 , 123 ) , without even subtracting the mean .
we have considered a 123 times overcomplete code with 123 units .
the parameters s , r and the learning rate were set to 123 , 123 , and 123 respectively .
some lters are localized gabor - like edge detectors in different positions and orientations , other are more global , and some encode the mean value ( see g
there are two strategies to train unsupervised machines : 123 ) having a contrastive term in the loss function minimized during training , 123 ) constraining the internal representation in such a way that training samples can be better reconstructed than other points in input space .
we have shown that rbm , which falls in the rst class of methods , is particularly robust to channel noise , it achieves very low rmse and good recognition rate .
we have also proposed a novel symmetric sparse encoding method following the second strategy which : is particularly efcient to train , has fast inference , works without requiring any withening or even mean removal from the input , can provide the best recognition performance and trade - off between entropy / rmse , and can be easily extended to a hierarchy discovering hidden structure in the data .
we have proposed an evaluation protocol to compare different machines which is based on rmse , entropy and , eventually , error rate when also
labels are available .
interestingly , the machine achieving the best performance in classication is the one with the best trade - off between reconstruction error and entropy .
a future avenue of work is to understand the reasons for this coincidence , and deeper connections between these two strategies .
we wish to thank jonathan goodman , geoffrey hinton , and yoshua bengio for helpful discussions .
this work was supported in part by nsf grant iis - 123 toward category - level object recognition , nsf itr - 123 new directions in predictive learning , and onr grant n123 - 123 - 123 - 123 integration and representation of high dimensional data .
