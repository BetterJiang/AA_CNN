memory - based classification algorithms such as radial basis func ( cid : 123 ) tions or k - nearest neighbors typically rely on simple distances ( eu ( cid : 123 ) clidean , dot product . . .
) , which are not particularly meaningful on pattern vectors .
more complex , better suited distance measures are often expensive and rather ad - hoc ( elastic matching , deformable templates ) .
we propose a new distance measure which ( a ) can be made locally invariant to any set of transformations of the input and ( b ) can be computed efficiently .
we tested the method on large handwritten character databases provided by the post office and the nist .
using invariances with respect to translation , rota ( cid : 123 ) tion , scaling , shearing and line thickness , the method consistently outperformed all other systems tested on the same databases .
distance - based classification algorithms such as radial basis functions or k - nearest neighbors often rely on simple distances ( such as euclidean distance , hamming distance , etc . ) .
as a result , they suffer from a very high sensitivity to simple transformations of the input patterns that should leave the classification unchanged ( e . g .
translation or scaling for 123d images ) .
this is illustrated in fig .
123 where an unlabeled image of a " 123 " must be classified by finding the closest prototype image out of two images representing respectively a " 123 " and a " 123 " .
according to the euclidean distance ( sum of the squares of the pixel to pixel differences ) , the " 123 " is closer even though the " 123 " is much more similar once it has been rotated and thickened .
the result is an incorrect classification .
the key idea is to construct a distance measure which is invariant with respect to some chosen transformations such as translation , rotation and others .
the special case of linear transformations has been well studied in statistics and is sometimes referred to as procrustes analysis
efficient pattern recognition using a new transformation distance
figure 123 : what is a good similarity measure ? according to the euclidean distance the pattern to be classified is more similar to prototype b .
a better distance measure would find that prototype a is closer because it differs mainly by a rotation and a thickness transformation , two transformations which should leave the classification
( sibson , 123 ) .
it has been applied to on - line character recognition ( sinden and this paper considers the more general case of non - linear transformations such as geometric transformations of gray - level images .
remember that even a simple image translation corresponds to a highly non - linear transformation in the high ( cid : 123 ) dimensional pixel space l .
in previous work ( simard et al . , 123b ) , we showed how a neural network could be trained to be invariant with respect to selected transfor ( cid : 123 ) mations of the input .
vve now apply similar ideas to distance - based classifiers .
' ' ' ' hen a pattern p is transformed ( e . g .
rotated ) with a transformation s that depends on one parameter a ( e . g .
the angle of the rotation ) , the set of all the transformed patterns sp = ( x i 123 such that x = s ( 123 , p ) ) is a one - dimensional curve in the vector space of the inputs ( see fig .
in certain cases , such as rotations of digitized images , this curve must be made continuous using smoothing techniques ( see ( simard et al . , 123b ) ) .
when the set of transformations is parameterized by n parameters ai ( rotation , translation , scaling , etc . ) , sp is a manifold of at most n dimensions .
the patterns in sp that are obtained through small transformations of p , i . e .
the part of sp that is close to p , can be approximated by a plane tangent to the manifold sp at the point p .
small transformations of p can be obtained by adding to p a linear combination of vectors that span the tangent plane ( tangent vectors ) .
the images at the bottom of fig .
123 were obtained by that procedure .
tangent vectors for a transformation s can easily be computed by finite difference ( evaluating os ( a , p ) / oa ) ; more details can be found in ( simard et al . , 123b; simard et al . , 123a ) .
as we mentioned earlier , the euclidean distance between two patterns p and e is in general not appropriate because it is sensitive to irrelevant transformations of p and of e .
in contrast , the distance v ( e , p ) defined to be the minimal dis ( cid : 123 ) tance between the two manifolds sp and se is truly invariant with respect to the transformation used to generate sp and se .
unfortunately , these manifolds have no analytic expression in general , and finding the distance between them is a hard optimization problem with multiple local minima .
besides , t . rue invariance is not
123 if the ima . ge of a " 123 " is translated vertica . lly upward , the middle top pixel will oscillate
from black to white three times .
simard , cun , and denker
true rotations of p
transformations at p
_ _ . . . . . . . . . .
figure 123 : top : small rotations of an original digitized image of the digit " 123 " .
middle : representation of the effect of the rotation in pixel space ( if there were only 123 pixels ) .
bottom : images obtained by moving along the tangent to the transformation curve for the same original digitized image p by adding various amounts ( a ) of the tangent vector ( t . v . ) .
necessarily desirable since a rotation of a " 123 " into a " 123 " does not preserve the correct our approach consists of approximating the non - linear manifold sp and se by linear surfaces and computing the distance d ( e , p ) defined to be the minimum distance between them .
this solves three problems at once : 123 ) linear manifolds have simple analytical expressions which can be easily computed and stored , 123 ) finding the minimum distance between linear manifolds is a simple least squares problem which can be solved efficiently and , 123 ) this distance is locally invaria . nt but not globally invariant .
thus the distance between a " 123 " and a slightly rota . ted " 123 " is small but the distance between a " 123 " and a " 123 " is la . rge .
the different .
distan ces between p and e are represented schematically in fig .
the figure represents two patterns p and e in 123 - dimensional space .
the ma . nifolds generated by s are represented by one - dimensional curves going through e and p respectively .
the linear approximations to the manifolds are represented by lines tangent to the curves at e and p .
these lines do not intersect in 123 dimensions and the shortest distance between them ( uniquely defined ) is d ( e , p ) .
the distance between the two non - linear transformation curves vee , p ) is also shown on the an efficient implementation of the tangent distance d ( e , p ) will be given in the
efficient pattern recognition using a new transformation distance
figure 123 : illustration of the euclidean distance and the tangent distance between p and e
next section .
although the tangent distance can be applied to any kind of pat ( cid : 123 ) terns represented as vectors , we have concentrated our efforts on applications to image recognition .
comparison of tangent distance with the best known competing method will be described .
finally we will discuss possible variations on the tangent distance and how it can be generalized to problems other than pattern recognition .
in this section we describe formally the computation of the tangent distance .
let the function s which map u , a to s ( a , u ) be a differentiable transformation of the input space , depending on a vector a of parameter , verifying s ( o , u ) = ' it .
if u is a 123 dimensional image for instance , s ( a , u ) could be a rotation of u by the angle & .
if we are interested in all transformations of images which conserve distances ( isometry ) , 123 ( a , u ) would be a rotation by a r followed by a translation by ax , a y of the image u .
in this case & = ( ar , ax , a y ) is a vector of parameters of dimension 123
in general , & = ( ao , . .
" am - d is of dimension m .
since 123 is differentiable , the set stl .
= ( x i 123a for which x = 123 ( a , ' it ) ) is a differen ( cid : 123 ) tiable manifold which can be approximated to the first order by a hyperplane ttl . .
this hyperplane is tangent to stl .
at u and is generated by the columns of matrix
= 123 ( & ~ ' it ) i = ( 123 ( & , u ) , . . .
, 123 ( a , u ) )
which are vectors tangent to the manifold .
if e and p are two patterns to be compared , the respective tangent planes te and tp can be used to define a new distance d between these two patterns .
the tangent distance d ( e , p ) between e and p is defined by
d ( e , p ) = min
iix - yw
the equation of the tangent planes te and tp is given by :
simard , cun , and denker
where le and lp are the matrices containing the tangent vectors ( see eq .
123 ) and the vectors a e and ap are the coordinates of e ' and p ' in the corresponding tangent planes .
the quantities le and lp are attributes of the patterns so in many cases they can be precomputed and stored .
computing the tangent distance
amounts to solving a linear least squares problem .
the optimality condition is that the partial derivatives of d ( e , p ) with respect to a p and ae should be zero :
od ( ~ , p ) = 123 ( e ' ( ae ) _ p ' ( ap t le = 123 od ( ~ , p ) = 123 ( p ' ( ap ) _ e ' ( aet lp = 123
substituting e ' and p ' by their expressions yields to the following linear system of equations , which we must solve for ap and ile :
l; ( e - p - lpilp + leae ) = 123 lf ( e - p - lpap + leile ) = 123
the solution of this system is
( lpel " e123l ~ - l; ) ( e - p ) = ( lpel " e123lep - lpp ) ap ( leplp ~ l; - l ~ ) ( e - p ) = ( lee - leplp ~ lpe ) ae
where lee = lft le , lpe = l ~ le ' l ep = l ~ lp and lpp = l ~ lp .
lu decompositions 123 lee and lpp can be precomputed .
the most expensive part in solving this system is evaluating lep ( lpe can be obtained by transposing lep ) .
it requires me x mp dot products , where me is the number of tangent vectors for e and mp is the number of tangent vectors for p .
once lep has been computed , ilp and ile can be computed by solving two ( small ) linear system of respectively me and mp equations .
the tangent distance is obtained by computing iie ' ( ae ) - p ' ( ap ) 123 using the value of a p and ile in equations 123 and 123
if n is the length of vector e ( or p ) , the algorithm described above requires roughly n ( me+l ) ( mp+l ) +123 ( m ~ +m ~ ) multiply - adds .
approximations to the tangent distance can be computed more
before giving the results of handwritten digit recognition experiments , we would like to demonstrate the property of " local invariance " of tangent distance .
a 123 by 123 pixel image similar to the " 123 " in fig 123 was translated by various amounts .
the tangent distance ( using the tangent vector corresponding to horizonta . l translations ) and the euclidean distance between the original image and its translated version were measured as a function of the size k ( in pixels ) of the translation .
the result is plotted in fig .
it is clear that the euclidean distance starts increasing linearly with k while the tangent distance remains very small for translations as large as two pixels .
this indicates that , while euclidean distance is not invariant to trans ( cid : 123 ) lation , tangent distance is locally invariant .
the extent of the invariance can be
efficient pattern recognition using a new transformation distance
~ ~ ~ ~ 123 # of pixels by which image is translated
figure 123 : euclidean and tangent distances between a 123x123 handwritten digit image and its translated version as a function of the amount of translation measured in
increased by smoothing the original image , but significant features may be blurred away , leading to confusion errors .
the figure is not symmetric for large translations because the translated image is truncated to the 123 by 123 pixel field of the original image .
in the following experiments , smoothing was done by convolution with a gaussian of standard deviation u = 123 .
this value , which was estimated visually , turned out to be nearly optimal ( but not critical ) .
123 handwritten digit recognition
experiments were conducted to evaluate the performance of tangent distance for handwritten digit recognition .
an interesting characteristic of digit images is that we can readily identify a set of local transformations which do not affect the identity of the character , while covering a large portion of the set of possible instances of the character .
seven such image transformations were identified : x and y translations , rotation , scaling , two hyperbolic transformations ( which can generate shearing and squeezing ) , and line thickening or thinning .
the first six transformations were chosen to span the set of all possible linear coordinate transforms in the imn ~ e plane ( nevertheless , they correspond to highly non - linear transforms in pixel space ) .
additional transformations have been tried with less success .
the simplest possible use of tangent distance is in a nearest neighbor classifier .
a set of prototypes is selected from a training set , and stored in memory .
when a test pattern is to be classified , the j ( nearest prototypes ( in terms of tangent dis ( cid : 123 ) tance ) are found , and the pattern is given the class that has the majority among the neighbors .
in our applications , the size of the prototype set is in the neighborhood of 123 , 123
in principle , classifying a pattern would require computing 123 , 123 tan ( cid : 123 ) gent distances , leading to excessive classification times , despite the efficiency of the tangent distance computation .
fortunately , two patterns that are very far apart in terms of euclidean distance are likely to be far apart in terms of tangent distance .
therefore we can use euclidean distance as a " prefilter " , and eliminate prototypes that are unlikely to be among the nearest neighbors .
v ' le used the following 123 - step classification procedure : 123 ) the euclidean distance is computed between the test pattern and all the prototypes , 123 ) the closest 123 prototypes are selected , 123 ) the tangent distance between these 123 prototypes and the test pattern is computed
simard , cun , and denker
human t - dlst nnet
human t - dlst nnet
figure 123 : comparison of the error rate of tangent nearest neighbors and other methods on two handwritten digit databases
and 123 ) the most represented label among the j ( closest prototype is outputed .
this procedure is two orders of magnitude faster than computing all 123 , 123 tangent distances , and yields the same performance .
us postal service database : in the first experiment , the database consisted of 123 by 123 pixel size - normalized images of handwritten digits , coming from us mail envelopes .
the entire training set of 123 examples of was used as the prototype set .
the test set contained 123 patterns .
the best performance was obtained with the " one nearest neig ~ bor " rule .
the results are plotted in fig .
the error rate of the method is 123% .
two members of our group labeled the test set by hand with an error rate of 123% ( using one of their labelings as the truth to test the other also yielded 123% error rate ) .
this is a good indicator of the level of difficulty of this task123 .
the performance of our best neural network ( le cun et al . , 123 ) was 123% .
the performance of one nearest neighbor with the euclidean distance was 123% .
these results show that tangent distance performs substantially better than both standard k - nearest neighbor and neural networks .
nist database : the second experiment was a competition organized by the n 123 , ( cid : 123 ) tional institute of standards and technology .
the object of the competition was to classify a test set of 123 , 123 handwritten digits , given a training set of 123 , 123 patterns .
a total of 123 algorithms were submitted from 123 companies from 123 differ ( cid : 123 ) ent countries .
since the training set was so big , a very simple procedure was used to select about 123 , 123 patterns as prototypes .
the procedure consists of creating a new database ( empty at the beginning ) , and classifying each pattern of the large database using the new database as a prototype set .
each time an error is made , the pattern is added to the new database .
more than one pass may have to be made before the new database is stable .
since this filtering process would take too long with 123 , 123 prototypes , we split the large database into 123 smaller databases of 123 , 123 patterns each , filtered those ( to about 123 patterns ) and concatenated the result , yielding a database of roughly 123 , 123 patterns .
this procedure has many drawbacks , and in particular , it is very good at picking up mislabeled characters in the training set .
to counteract this unfortunate effect , a 123 nearest neighbors procedure was used with tangent distance .
the organizers decided to collect the
123this is an extremely difficult test set .
procedures that achieve less than 123% error on
other handwritten digit tasks barely achieve less than 123% on this one
efficient pattern recognition using a new transformation distance
training set and the test set among two very different populations ( census bureau workers for the training set , high - school students for the test set ) , we therefore re ( cid : 123 ) port results on the official nist test set ( named " hard test set " ) , and on a subset of the official training set , which we kept aside for test purposes ( the " easy test set " ) .
the results are shown in fig .
the performance is much worse on the hard test set since the distribution was very different from that of the training set .
out of the 123 participants who used the nist training database , tangent distance finished first .
the overall winner did not use the training set provided by nist ( he used a much larger proprietary training set ) , and therefore was not affected by the different distributions in the training set and test set .
the tangent distance algorithm described in the implementation section can be improved / adjusted in at least four different ways : 123 ) approximating the tangent distance for better speed 123 ) modifying the tangent distance itself , 123 ) changing the set of transformations / tangent vectors and 123 ) using the tangent distance with clas ( cid : 123 ) sification algorithms other than k - nearest neighbors , perhaps in combination , to minimize the number of prototypes .
we will discuss each of these aspects in turn .
approximation : the distance between two hyperplanes te and tp going through p and e can be approximated by computing the projection peep ) of ponto te and pp ( e ) of e onto tp .
the distance iipe ( p ) - pp ( e ) 123i can be computed in o ( n ( me + mp multiply - adds and is a fairly good approximation of d ( e , p ) .
this approximation can be improved at very low cost by computing the closest points between the lines defined by ( e , peep ~ and ( p , pp ( e .
this approximation was used with no loss of performance to reduce the number of computed tangent distance from 123 to 123 ( this involves an additional " prefilter " ) .
in the case of images , another time - saving idea is to compute tangent distance on progressively smaller sets of progressively higher resolution images .
changing the distance : one may worry that the tangent planes of e and p may be parallel and be very close at a very distant region ( a bad side effect of the linear a . pproximation ) .
this effect can be limited by imposing a constraint of the form ilaeii < f ( e and liapli < f ( p .
this constraint was implemented but did not yield better results .
the reason is that tangent planes are mostly orthogonal in high dimensional space and the norms of ( ( aeii and ! lapll are already small .
the tangent distance can be normalized by dividing it by the norm of the vectors .
this improves the results slightly because it offsets side effects introduced in some transformations such as scaling .
indeed , if scaling is a transformation of interest , there is a potential danger of finding the minimum distance between two images after they have been scaled down to a single point .
the linear approximation of the scaling transformation does not reach this extreme , but still yields a slight degradation of the performance .
the error rate reported on the usps database can be improved to 123% using this normalization ( which was not tried on nist ) .
tangent distance can be viewed as one iteration of a newton - type algorithm which finds the points of minimum distance on the true transformation manifolds .
the vectors ae and ap are the coordinates of the two closest points in the respective tangent spaces , but they can also be interpreted for real ( non - linear ) transforma ( cid : 123 ) tions .
if ae; is the amount of the translation tangent vector that must be added to e to make it as close as possible to p , we can compute the true translation of image e by ae , ; pixels .
in other words , e ' ( ae ) and pl ( ap ) are projected onto
simard , cun , and denker
close points of se and sp .
this involves a resampling but can be done efficiently .
once this new image has been computed , the corresponding tangent vectors can be computed for this new image and the process can be repeated .
eventually this will converge to a local minimum in the distance between the two transformation manifold of p and e .
the tangent distance needs to be normalized for this iteration process to work .
a priori knowledge : the a priori knowledge used for tangent vectors depends greatly on the application .
for character recognition , thickness was one of the most important transformations , reducing the error rate from 123% to 123% .
such a transformation would be meaningless in , say , speech or face recognition .
other transformations such as local rubber sheet deformations may be interesting for character recognition .
transformations can be known a priori or learned from the other algorithms , reducing the number of prototypes : tangent distance is a general method that can be applied to problems other than image recognition , with classification methods other than k - nearest neighbors .
many distance - ba . sed classification schemes could be used in conjunction with tangent distance , among them lvq ( kohonen , 123 ) , and radial basis functions .
since all the operators in ( cid : 123 ) volved in the tangent distance are differentiable , it is possible to compute the partial derivative of the tangent distance ( between an object and a prototype ) with respect to the tangent vectors , or with respect to the prototype .
therefore the tangent distance operators can be inserted in gradient - descent based adaptive machines ( of which lvq and ref are particular cases ) .
the main advantage of learning the prototypes or the tangent vectors is that fewer prototypes may be needed to reach the same ( or superior ) level of performance as , say , regular k - nearest neighbors .
in conclusion , tangent distance can greatly improve many of the distance - based algorithms .
we have used tangent distance in the simple k - nearest neighbor al ( cid : 123 ) gorithm and outperformed all existing techniques on standard classification tasks .
this surprising success is probably due the fact that a priori knowledge can be very effectively expressed in the form of tangent vectors .
fortuna . tely , many algorithms are based on computing distances and can be adapted to express a priori knowledge in a similar fashion .
promising candidates include parzen windows , learning vector quantization and radial basis functions .
