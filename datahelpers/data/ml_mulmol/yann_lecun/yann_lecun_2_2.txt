abstract .
we address the problem of learning good features for under - standing video data .
we introduce a model that learns latent represen - tations of image sequences from pairs of successive images .
the convolu - tional architecture of our model allows it to scale to realistic image sizes whilst using a compact parametrization .
in experiments on the norb dataset , we show our model extracts latent ow elds which correspond to the transformation between the pair of input frames .
we also use our model to extract low - level motion features in a multi - stage architecture for action recognition , demonstrating competitive performance on both the kth and hollywood123 datasets .
key words : unsupervised learning , restricted boltzmann machines , con - volutional nets , optical ow , video analysis , activity recognition
while the dominant methodology for visual recognition from images and video relies on hand - crafted features , there has been a growing interest in methods that learn low - level and mid - level features , either in supervised ( 123 ) , unsupervised ( 123 123 ) , or semi - supervised settings ( 123 ) .
in recent years , feature - learning methods have focused on learning multiple layers of feature hierarchies to extract increasingly abstract representations at each stage .
this has been generally done by compos - ing modules of the same architecture such as restricted boltzmann machines ( rbm ) ( 123 ) , autoencoders ( 123 ) , or various forms of encoder - decoder networks ( 123 , 123 , 123 ) each of which are trained unsupervised and therefore can take advantage of large amounts of unlabeled image data .
the resulting deep architectures are then globally trained discriminatively , with the idea that the rst phase of unsupervised feature learning has provided an initialization that is much more salient for high - level tasks than the usual random initialization .
most of the above methods do not exploit the pictorial nature of the input , and have been applied to relatively small image patches ( typically less than 123 pixels ) , because they do not scale well with the size of the input .
this can be addressed by using a convolutional architecture ( 123 ) , which exploits the fact that salient motifs can appear anywhere in the image .
this idea has been recently used in the context of rbms ( 123 , 123 ) .
by employing successive stages of weight - sharing
graham w .
taylor , rob fergus , yann lecun , and christoph bregler
and feature - pooling , deep convolutional architectures can achieve stable latent representations at each layer , that preserve locality , provide invariance to small variations of the input , and drastically reduce the number of free parameters .
to date , most of the work on unsupervised feature extraction has focused on static images but little attention has been given to learning about the way that images from videos change over time .
the few works that address the prob - lem ( e . g .
( 123 , 123 ) ) are trained on isolated patches ( not convolutionally ) , and suer from the same limitations as static methods .
in this paper , we propose a model that can extract motion - sensitive features from pairs of images ( i . e .
neighbour - ing frames of video ) .
the features can capture both static and dynamic content .
our model is trained convolutionally which enables it to work on high - resolution images .
we rst apply it to synthetic data and show that it learns to represent ow - like features when the type of transformations are restricted .
we then use it to extract useful features for human activity recognition in a multi - stage archi - tecture that achieves state - of - the - art performance on the kth actions dataset .
results are also shown on the challenging hollywood123 action recognition dataset .
123 related work
our work extends the gated rbm ( grbm ) model proposed by memisevic and hinton ( 123 ) .
the grbm is able to extract distributed , domain - specic represen - tations of image patch transformations .
due to its tensor parameterization , it is not practical to apply this model to patches larger than about pn 123q123 since the number of parameters grows as opn 123q .
therefore , it has only been applied to low - resolution synthetic images of shifting pixels or pca - reduced samples of low - resolution video .
while the model has been shown to improve digit classi - cation by learning the types of transformations to which the classier should remain invariant , we are not aware of is application to a discriminative task on real video .
memisevic and hinton have recently proposed a factored form of the grbm ( 123 ) that drastically reduces the number of free parameters by replac - ing the three - way weight tensor with three low - rank matrices .
in the present work , we take an alternative convolutional approach to scaling up the model , which achieves the additional benet of translation invariance .
sutskever and hinton ( 123 ) proposed a type of temporal rbm for video .
using synthetic videos of bouncing balls , they trained a model which was then able to generate similar videos , but did not apply their work to discriminative tasks .
the signal from the past only provides a type of temporal bias to the hidden variables , which is fundamentally dierent from our third - order rbm , where past inputs modulate the interactions between the current input and the latent feature representation .
building on the rapidly growing literature on sparse over - complete decompo - sitions of image patches ( 123 ) , cadieu and olshausen ( 123 ) have proposed a two - layer probabilistic model that learns complex motion features from video .
in contrast to our model , they explicitly separate static amplitude and dynamic phase at the rst layer .
the second layer then learns high - order dependencies among the phase variables .
dean et al .
( 123 ) have recently proposed learning spatio - temporal
convolutional learning of spatio - temporal features
descriptors by recursively applying the feature - sign sparse coding algorithm ( 123 ) to 123d patches of videos extracted at detected interest points .
like our work , their descriptors are adaptive , but their method is trained at the patch level .
state - of - the - art methods for activity recognition use engineered motion and texture descriptors extracted around interest points detected by spatio - temporal corner detectors .
the descriptors are then vector - quantized , pooled over time and space into a bag , and fed to an svm classier .
among the best performing methods are 123 ) laptev et al . s spatio - temporal interest points ( stip ) ( 123 ) used in conjunction with the hog / hof descriptor that computes histograms of spatial gradients and optic ow accumulated in local space - time neighbourhoods ( 123 ) ; 123 ) dollar et al . s cuboids approach ( 123 ) used in conjunction with several dierent descriptor types; and 123 ) willems et al . s approach ( 123 ) which uses the determinant of the hessian as a saliency measure and computes a weighted sum of haar wavelet responses within local rectangular sub - volumes .
in contrast to these approaches , we perform a type of implicit , rather than explicit interest point detection and focus on learning descriptors rather than hand - crafting them .
we also bypass the quantization step in favor of several additional layers of feature extraction that provide a distributed representation of each video .
jhuang et al .
( 123 ) propose an approach similar in spirit to ours , using multiple levels of feature detectors at increasing spatio - temporal scale .
however , like ( 123 ) , they forgo learning until the very last stage : low and mid - level features are engineered .
123 unsupervised learning of spatio - temporal features
we rst describe a related approach , the gated restricted boltzmann machine , which models image patches but does not scale to realistic - sized images or video .
we then describe our convolutional model .
123 the gated restricted boltzmann machine ( grbm )
the gated restricted boltzmann machine ( 123 ) diers from other conditional rbm architectures ( e . g .
( 123 , 123 ) ) in that its inputs change the eective weights of the model instead of simply adjusting the eective biases of visible or latent variables ( see figure 123 ( left ) ) .
this is achieved by dening an energy function that captures third - order interactions among three types of binary stochastic variables : inputs , x , outputs , y , and latents , z :
e py , z; xq
where wijk are the components of a parameter tensor , w , which is learned .
to model ane and not just linear dependencies , biases b and c are included .
when learning from video , x and y are image patches ( expressed as vectors ) at identical spatial locations in sequential frames , and z is a latent representation
graham w .
taylor , rob fergus , yann lecun , and christoph bregler
left : a gated rbm .
right : a convolutional gated rbm using probabilistic
of the transformation between x and y .
the energy of any joint conguration ty , z; xu is converted to a conditional probability by normalizing :
ppy , z|xq exp pepy , z; xqq ( zpxq
where the partition function , zpxq y , z exp pepy , z; xqq is intractable to compute exactly since it involves a sum over all possible congurations of the output and latent variables .
however , we do not need to compute this quantity to perform either inference or learning .
given an input - output pair of image patches , tx , yu , it follows from eq .
123 and 123 that
ppzk 123|x , yq p
where pzq 123 ( p123 exppzqq is the logistic .
maximizing the marginal conditional likelihood , ppy|xq , over parameters tw , b , cu is dicult for all but the smallest models due to the intractability of computing z .
learning , however , still works well if we approximately follow the gradient of another function called the contrastive divergence ( cd ) ( 123 ) .
123 the convolutional gated restricted boltzmann machine
grbms represent the input and output as a vector , and thus ignore the pictorial structure of images .
weights that encode a particular local transformation must be re - learned to detect that same transformation at multiple locations .
we now describe a form of grbm that shares weights at all locations in an image .
inference is performed eciently through convolution , so we refer to the model as a convolutional grbm ( convgrbm ) .
the model is illustrated in figure 123 ( right ) .
in our description of the grbm , we suggested that x and y were time - adjacent patches from video , but they could have been any arbitrary vectors .
x ( input ) y ( output ) zklatent featurelayer pkpoolinglayernxnxnynynznznpnppkzkm , nnxwnxwnywnywwwx ( input ) y ( output ) zlatents convolutional learning of spatio - temporal features
here , we assume that x is a nx nx binary image and y is a ny ny binary image .
we assume square , binary images to simplify the presentation but provide details of using real - valued images in the supplemental material .
in the grbm we had k binary latent variables .
now we have k nz nz binary latent feature k123 ) .
let m and n be spatial indices to each 123d feature map , such maps ( z tzkuk that a single feature is described as zk m , n .
the indices m and n not only index a particular 123d feature , but they also dene 123 ) an n y w local region in y from w n y w region of x which modulates which this feature receives input , and 123 ) a n x w n x the interaction between all k features at location m , n and the n y region in y .
alternatively , we can think of each of the k features at index m , n w pixels in x as contributing a local log - linear patch model between the n x and the n y w pixels in y where the location of these local regions is specied by m , n .
the number of local autoregressive models that can be blended is exponential in the number of feature maps .
w n y
w n x
w n y
for the remainder of our discussion , we will make two assumptions : 123 ) the input and output images are the same dimensions , nx ny ( this holds true for neighbouring frames in video ) ; and 123 ) the lter dimensions in the input and the w .
these assumptions are not necessary , but they output are the same , n x greatly simplify bookkeeping and therefore the presentation that follows .
w n y
the convgrbm has the following energy function :
e py , z; xq
where we use a per - map bias , bk , for the latent variables and single output bias , c .
123 is similar to the energy function of a convolutional rbm ( 123 ) , except that what was previously a lter weight with 123 indices : r , s , k has been replaced by a conditional lter weight , pxqk r , s , u , vxmu123 , nv123 , with 123 indices .
the additional indices m , n denote the local region in x which modulates the lter .
note that while m , n index the entire feature map , u , v and r , s index within the local regions of x and y , respectively .
r , s , m , n n x
u , v w k
as in the grbm , the probability of jointly observing y and z given x is given
the conditional distributions for z|y , x and y|z , x naturally follow :
m , n 123|x , yq p
ppyi , j 123|x , zq p
w r 123 and s123 n y
where r123 n y w s 123 represent a ipping of the lter indices ( i . e .
correlation rather than convolution ) , and z is the result of zero - padding z such that its rst n y w 123 rows and columns are zero .
note that in
graham w .
taylor , rob fergus , yann lecun , and christoph bregler
123 an output unit yi , j makes a bottom - up contribution to several elements ( m , n ) in all k feature maps .
therefore , in top - down reconstruction ( eq .
123 ) we must ensure that each output unit receives input from all feature map elements to which it has contributed , through the same conditional lter weight that was used bottom - up .
to account for border eects , it is convenient to dene pxq as a zero - padded version of pxq whose dimensions are n y w ny ny k .
as with convolutional rbms , we can express both inference ( eq .
123 ) and reconstruction ( eq .
123 ) in terms of convolution operations ( see the supplemental material for details ) .
while inference in a convolutional rbm requires a single 123d convolution of the data with the lters , inference in the convgrbm requires a 123d convolution of the output and data for each element of the conditioning window : i . e .
n x w convolutions .
the same holds true for reconstruction ( replacing data with feature maps ) .
note , however , that a fully - connected ( i . e .
non - convolutional ) grbm requires nx nx more operations during inference than a standard rbm .
restricting connections to be local clearly makes a huge dierence in eciency , especially when the ratio of pixels to lter size is high .
w n x
w n y
probabilistic max pooling most object recognition systems use a pooling operation that combines nearby values in input or feature space through a max , average or histogram operator .
this provides the system with some invariance to small local distortions and reduces the computational burden .
traditional pool - ing layers , however , are designed for feed - forward architectures like convolutional nets and do not support generative models such as rbms that include top - down feedback .
lee et al .
( 123 ) thus introduced probabilistic max - pooling in the context of convolutional rbms .
we adopt their approach , and summarize it here .
recall that we have k feature maps connected to the visible input and out - put .
we introduce a layer on top of the feature maps , called the pooling layer , which also has k maps , each connected 123 - 123 to a feature map .
however , the maps of the pooling layer are reduced in spatial resolution by a constant factor c in each dimension ( e . g .
123 or 123 ) .
more precisely , each feature map zk is partitioned into non - overlapping c c blocks , and each block is connected to exactly one , in the pooling layer ( i . e .
np nz ( c ) .
here , we have adopted binary unit , pk the notation of ( 123 ) where indexes the pooling units and also dene a block formally as b tpm , nq : zm , n belongs to the block u .
the connection between pooling unit p and the features in block b is constrained such that at most one of the features in a block is on , and if any of the features in block b is on , then p must be on , otherwise p is o .
this leads to a modied , constrained , energy function :
e py , z; xq
m , n 123 , @k , .
convolutional learning of spatio - temporal features
changing the energy function results in a change to the inference procedure .
note that each unit in feature map k receives the following bottom - up signal from the input and output :
due to the factorial form of eq .
123 , we can sample each of the blocks independently as a multinomial function of their inputs :
m , n 123|x , yq 123exp ipzk
where the normalization constant is 123 pm123 , n123qpb
123|x , yq 123
123 experiments on synthetic data : norb
one way to evaluate third - order rbms is by experimenting in a domain where optical ow is controlled and regular ( e . g .
the shifting pixels experiments of ( 123 ) ) .
in this section , we describe a domain for experimentation that is of in - creased complexity yet still controlled .
the small norb dataset ( 123 ) has 123 object categories ( humans , airplanes , cards , trucks , animals ) , and 123 dierent ob - ject instances for each training and test .
each object instance has 123 azimuths , 123 camera - elevations , and 123 illuminations , for a total of 123 training samples and 123 test samples .
traditionally norb has been used to evaluate object recognition .
since our goal is to extract useful transformation features from pairs of images we use the dataset dierently than intended .
the azimuth , elevation , and illumination changes in the norb dataset are at xed intervals and corresponding labels for each image are available .
there - fore , we created synthetic videos where an object underwent forward or reverse transformation in one of the dimensions while the others were held xed .
before generating the videos , we downsampled each image to 123 pixels , and pre - processed it using local contrast normalization ( lcn ) as described in ( 123 ) .
the lcn operation involves a 123 smoothing lter , so each resulting image is 123
we then trained a convgrbm with real - valued outputs and 123 binary feature w 123
the model trained on all maps .
the lter dimensions were n x azimuth changes of 123 , and all camera elevation changes of 123
it was trained for 123 complete passes through the training set , using standard cd ( 123 ) learning .
figure 123 shows the result of performing 123 image analogies .
each analogy is represented by a group of six small greyscale images and one larger optical ow image .
to perform an analogy , the model is presented with a pair of images each from an object instance it has never seen before , and asked to apply the same inferred transformation to a random target image , also which it has never seen before .
we can also visualize the ow implicit in the hidden units and conditional on the pair , by drawing , for each input pixel , an arrow to the output pixel to which it is most strongly connected according to the learned lters , w ( marginalized over the binary feature maps ) .
much information is
w n y
graham w .
taylor , rob fergus , yann lecun , and christoph bregler
potentially lost in this representation ( 123 ) : the transformation encoded by the feature maps can be much richer than what is expressed by optical ow alone .
analogies .
each group of six greyscale images from left to right , top to bot - tom represent : input image; output image; models reconstruction of output; random target image; ground truth of random target ( i . e .
by searching for the example that corresponds to the transformation between image and output ) ; inferred transformation applied to targets .
examples 123 - 123 show changes in azimuth; 123 - 123 show changes in camera elevation .
a representation of inferred max ow elds is shown for each example .
123 experiments on human activity recognition
recognition of human activity from video is a challenging problem that has re - ceived an increasing amount of attention from the computer vision community in recent years .
the ability to parse high - level visual information has wide - ranging applications that include surveillance and security , the aid of people with special needs and the understanding and interpretation of non - verbal communication .
convolutional learning of spatio - temporal features
an overview of our multi - stage architecture for human activity recognition .
see text for a description of each stage .
we approach the problem with a multi - stage architecture ( see figure 123 ) that combines convolutional and fully - connected layers .
at the lowest layer , a con - volutional grbm extracts features from every successive pair of frames .
we observe that most features are motion - sensitive , but others capture static in - formation .
this is particularly useful in providing context in more challenging datasets ( 123 ) and will aid in applying our method to other tasks , such as scene recognition from video .
a subset of the feature maps inferred from the kth actions dataset are shown in figure 123
the features are extremely diverse : many capture limb movement , others capture edge content , and one seems particu - larly apt at segmenting person from background ( we note that the background is generally uniformly textured in kth ) .
to capture mid - level spatio - temporal cues , we apply a traditional ( i . e .
feed - forward ) convolutional layer that uses 123d spatio - temporal lters .
a connectivity table indicates which of the 123d convolutional layer output maps are connected to each convgrbm pooling map .
our convolutional layer is a 123d extension of the architecture advocated by jarrett et al .
( 123 ) : ltering , followed by a tanh nonlinearity , followed by absolute value rectication , followed by a local con - trast normalization layer , followed by average pooling and subsampling .
both the abspq and tanhpq are performed element - wise , so their extension to 123d is straightforward .
the lcn and pooling / subsampling layers each employ a lter - ing operation , which we perform in 123d instead of 123d .
the output of the second convolutional layer is a series of 123d feature maps .
to cope with variable - length sequences , we perform an additional max pooling in the temporal dimension .
this ensures that the mid - level features can be reduced to a vector of consistent size .
this representation is followed by one or more fully - connected layers ( we use 123 or 123 in our experiments ) .
the topmost layer is a softmax ( multinomial ) layer corresponding to discrete activity labels , and inter - mediate layers use a tanh nonlinearity .
the convgrbm is trained unsupervised using cd , while the upper layers are trained by backpropagation .
we do not backpropagate through the rst layer following unsupervised training , though this could be done to make the low - level features more discriminative .
123 kth actions dataset
the kth actions dataset ( 123 ) is the most commonly used dataset in evaluating human action recognition .
it consists of 123 subjects performing six actions : walk - ing , jogging , running , boxing , hand waving , and hand clapping under 123 scenarios
convgrbm123 - dconvolutionabsrecti ( cid : 123 ) cationlocalcontrast norm . averagespatial poolingmax temporalpoolingfullyconnectedactivity labelimagepairs 123
graham w .
taylor , rob fergus , yann lecun , and christoph bregler
( outdoors , outdoors with scale variation , outdoors with dierent clothes and in - doors ) .
each sequence is further divided into shorter clips for a total of 123 sequences .
we use the original evaluation methodology : assigning 123 subjects to a training set , 123 to a validation set , and the remaining 123 subjects to a test set so that our results are directly comparable to the recent survey by wang et al .
preprocessing we maintained the original frame rate ( 123fps ) and spatial resolution 123 in all of our experiments .
all videos then underwent 123d local contrast normalization ( an extension of ( 123 ) ) .
x n y
unsupervised learning .
we trained a convgrbm with nz 123 feature maps and a pooling factor of c 123
filter sizes were n x x 123
we chose 123 as it was a number amenable to gpu - based computing , and it was close to the minimal patch size ( 123 123 ) suggested by wang et al .
we have not tried other patch sizes .
weights were updated in mini - batches of 123 pairs of subsequent frames ( the order of pairs was randomly permuted as to balance the mini - batches ) .
we made 123 complete passes over all videos in the training set .
supervised learning we trained a convolutional net with 123 123 l - ters ( randomly initialized ) on top of the features extracted by the convgrbm .
each feature map of the convolutional net received input from 123 randomly cho - sen pooling maps from the rst layer .
architectural choices were motivated by a desire to extract mid - level spatio - temporal features; the local connectivity used is standard practice ( 123 ) .
the nonlinearities we used were identical to those in ( 123 ) with the exception of extending contrast normalization and downsampling to 123d : lcn was performed using a 123 smoothing lter , followed by 123 average downsampling .
we also tried a more traditional network architecture which did not use absolute value rectication and lcn .
we found that it slightly decreased accuracy ( by about 123%; less drastic than reported in ( 123 ) for static object recogni - tion ) .
the pooling layer was then subjected to a further max - pooling over time , the output was vectorized and connected to one or two fully - connected layers .
all layers ( except the convgrbm ) used online backpropagation123
we made 123 complete passes through the training set .
table 123 compares our approach to the prior art using dense sampling ( i . e .
no interest - point detection ) and k - means quantization .
we report mean accuracy over all six actions .
our method , to the best of our knowledge , gives the best mean accuracy on kth amongst methods that do not use interest - point de - tection .
the currently best performing method ( 123 ) uses the stip interest - point detector and hog / hof or hof descriptors ( 123 and 123% , respectively ) .
due to the high ratio of background pixels to subject pixels in kth , and the limited number of actions ( that dont require context information ) , interest - point meth - ods tend to perform extremely well on kth .
evidence already indicates that dense - sampling outperforms interest - points on more challenging datasets ( 123 ) .
to demonstrate the advantage of low - level feature extraction with convgrbms ,
we have replaced the rst layer with a standard 123d convolutional layer ( 123f123
123 the choice of using online learning here was simply a matter of convenience due to variable sequence lengths .
since the convgrbm is trained on pairs of frames ( rather than whole sequences ) it is easier to train in mini - batches .
convolutional learning of spatio - temporal features
table 123
kth action dataset : classication performance using dense sampling .
integers preceding a module indicate the number of feature maps in that module .
superscripts indicate lter sizes or downsampling ratio ( chosen by context ) .
convgrbm is our proposed method , trained unsupervised .
fcsg is a standard convolutional layer : a set of convolution lters ( c ) followed by a sigmoid / tanh nonlinearity ( s ) , and gain coecients ( g ) .
r / n / pa is abs rectication , followed by local contrast normalization , followed by average pooling .
the number of fully - connected layers are either 123 which corresponds to logistic regression ( log reg ) or 123 , which corresponds to a multi - layer
accuracy convolutional architectures
- see table 123 ) .
by using lters of size 123 and a 123 pooling layer , we have matched the architecture of the convgrbm as best as possible to perform this comparison .
the entire network is trained by backpropagation .
we note that this fully feed - forward approach performs considerably worse .
123 hollywood123 dataset
the hollywood123 dataset ( 123 ) consists of a collection of video clips contain - ing 123 classes of human action ex - tracted from 123 movies .
it totals ap - proximately 123 hours of video and contains approximately 123 samples per action .
it provides a more real - istic and challenging environment for human action recognition by contain - ing varying spatial resolution , camera zoom , scene cuts and compression ar -
table 123
hollywood123 dataset : average precision ( ap ) using dense sampling .
prior art ( 123 ) :
performance is evaluated as suggested by marszalek et al .
( 123 ) : by computing the average precision ( ap ) for each of the action classes and reporting mean ap over all actions .
following ( 123 ) , we downsampled the spatial resolution of every video clip ( which varies between clips ) by a factor of 123
videos were then zero - padded to have a constant spatial resolution .
we did no temporal downsampling .
all videos then underwent 123d local contrast normalization .
similar to the kth dataset , we trained a convgrbm with nz 123 feature x 123
the maps and a pooling factor of c 123
filter sizes were n x convgrbm was trained for 123 complete passes over all videos in the training dataset and used a sparsity regularization term in the cd updates ( 123 ) that encouraged the hidden units to have a mean activation of 123 .
x n y
graham w .
taylor , rob fergus , yann lecun , and christoph bregler
instead of applying a convolutional network to extract mid - level features , we sampled the feature maps of the convgrbm with a stride of 123 pixels in each direction , and formed local temporal groups of 123 frames .
we then used the method described in ( 123 ) to learn a dictionary of 123 basis vectors , and encode the temporal groups as sparse linear coecients of the bases .
each video then yielded a varying number of sparse vectors ( given dierent lengths ) so we applied max - pooling over the temporal dimension .
a svm ( with rbf kernel ) was then trained ( per - activity ) on the top - level representation .
since hollywood123 videos may contain more than one activity , this approach allowed us to avoid training a separate 123d convolutional net per - activity .
we achieve a mean ap of 123% using dense sampling , learned convgrbm low - level features and sparse coding with 123 elements .
to the best of our knowledge , the only superior published result is 123% which uses dense sampling with hog / hof features and quantization ( 123 ) .
however , our result outperforms other popular methods such as cuboids ( 123% ) and willems et al .
( 123% ) ( published in ( 123 ) ) .
we also expect that an approach that combined our learned features with hog / hof descriptors could perform well .
gated rbms extract latent representations that are useful for video understand - ing tasks .
however , they do not scale well to realistic resolutions and must learn separate feature detectors at all locations in a frame .
in the spirit of recent work exploring convolutional deep architectures , we have introduced the convolutional gated rbm .
we showed that it learned to represent optical ow and performed image analogies in a controlled , synthetic environment .
in a more challenging set - ting , human activity recognition , it extracted useful motion - sensitive features , as well as segmentation and edge - detection operators that allowed it to perform competitively against the state - of - the - art as part of a multi - stage architecture .
acknowledgments .
we thank the oce of naval research ( onr n123 , onr n123 ) , and google for supporting this research .
