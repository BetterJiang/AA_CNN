in many machine learning applications , one has access , not only to training data , but also to some high - level a priori knowledge about the desired be ( cid : 123 ) havior of the system .
for example , it is known in advance that the output of a character recognizer should be invariant with respect to small spa ( cid : 123 ) tial distortions of the input images ( translations , rotations , scale changes , we have implemented a scheme that allows a network to learn the deriva ( cid : 123 ) tive of its outputs with respect to distortion operators of our choosing .
this not only reduces the learning time and the amount of training data , but also provides a powerful language for specifying what generalizations we wish the network to perform .
in machine learning , one very often knows more about the function to be learned than just the training data .
an interesting case is when certain directional deriva ( cid : 123 ) tives of the desired function are known at certain points .
for example , an image
simard , victorri , le cun , and denker
figure 123 : top : small rotations of an original digital image of the digit " 123 " ( center ) .
middle : representation of the effect of the rotation in the input vector space space ( assuming there are only 123 pixels ) .
bottom : images obtained by moving along the tangent to the transformation curve for the same original digital image ( middle ) .
recognition system might need to be invariant with respect to small distortions of the input image such as translations , rotations , scalings , etc . ; a speech recognition system n . ight need to be invariant to time distortions or pitch shifts .
words , the derivative of the system ' s output should be equal to zero when the input is transformed in certain ways .
given a large amount of training data and unlimited training time , the system could learn these invariances from the data alone , but this is often infeasible .
the limitation on data can be overcome by training the system with additional data obtained by distorting ( translating , rotating , etc . ) the original patterns ( baird , 123 ) .
the top of fig .
123 shows artificial data generated by rotating a digital image of the digit " 123 " ( with the original in the center ) .
this procedure , called the " distortion model " , has two drawbacks .
first , the user must choose the magnitude of distortion and how many instances should be generated .
second , and more importantly , the distorted data is highly correlated with the original data .
this makes traditional learning algorithms such as back propagation very inefficient .
the distorted data carries only a very small incremental amount of information , since the distorted patterns are not very different from the original ones .
it may not be possible to adjust the learning system so that learning the invariances proceeds at a reasonable rate while learning the original points is non - divergent .
the key idea in this paper is that it is possible to directly learn the effect on the output of distorting the input , independently from learning the undistorted
tangent prop - a formalism for specifying selected invariances in an adaptive network
figure 123 : learning a given function ( solid line ) from a limited set of example ( xl to x123 ) .
the fitted curves are shown in dotted line .
top : the only constraint is that the fitted curve goes through the examples .
bottom : the fitted curves not only goes through each examples but also its derivatives evaluated at the examples agree with the derivatives of the given function .
patterns .
when a pattern p is transformed ( e . g .
rotated ) with a transformation s that depends on one parameter a ( e . g .
the angle of the rotation ) , the set of all the transformed patterns s ( p ) = ( sea , p ) va ) is a one dimensional curve in the vector space of the inputs ( see fig .
in certain cases , such as rotations of digital images , this curve must be made continuous using smoothing techniques , as will be shown below .
when the set of transformations is parameterized by n parameters ai ( rotation , translation , scaling , etc . ) , s ( p ) is a manifold of at most n dimensions .
the patterns in s ( p ) that are obtained through small transformations of p , i . e .
the part of s ( p ) that is close to p , can be approximated by a plane tangent to the manifold s ( p ) at point p .
small transformations of p can be obtained by adding to p a linear combination of vectors that span the tangent plane ( tangent vectors ) .
the images at the bottom of fig .
123 were obtained by that procedure .
more importantly , the tangent vectors can be used to specify high order constraints on the function to be learned , as explained below .
to illustrate the method , consider the problem of learning a single - valued function f from a limited set of examples .
123 ( left ) represents a simple case where the desired function f ( solid line ) is to be approximated by a function g ( dotted line ) from four examples ( ( xi , f ( xi ) ) ) i=123 , 123 , 123 , 123
as exemplified in the picture , the fitted function g largely disagrees with the desired function f between the examples .
if the functions f and g are assumed to be differentiable ( which is generally the case ) , the approximation g can be greatly improved by requiring that g ' s derivatives evaluated at the points ( xd are equal to the derivatives of f at the same points ( fig .
123 right ) .
this result can be extended to multidimensional inputs .
in this case , we can impose the equality of the derivatives of f and g in certain directions , not necessarily in all directions of the input space .
such constraints find immediate use in traditional learning problems .
it is often the case that a priori knowledge is available on how the desired function varies with
simard , victorri , le cun , and denker
rotated by ex
figure 123 : how to compute a tangent vector for a given transformation ( in this case
respect to some transformations of the input .
it is straightforward to derive the corresponding constraint on the directional derivatives of the fitted function g in the directions of the transformations ( previously named tangent vectors ) .
typical examples can be found in pattern recognition where the desired classification func ( cid : 123 ) tion is known to be invariant with respect to some transformation of the input such as translation , rotation , scaling , etc . , in other words , the directional derivatives of the classification function in the directions of these transformations is zero .
the implementation can be divided into two parts .
the first part consists in com ( cid : 123 ) puting the tangent vectors .
this part is independent from the learning algorithm used subsequently .
the second part consists in modifying the learning algorithm ( for instance backprop ) to incorporate the information about the tangent vectors .
part i : let x be an input pattern and s be a transformation operator acting on the input space and depending on a parameter a .
if s is a rotation operator for instance , then s ( a , x ) denotes the input x rotated by the angle a .
we will require that the transformation operator s be differentiable with respect to a and x , and that s ( o , x ) = x .
the tangent vector is by definition 123s ( a , x ) / 123a .
it can be approximated by a finite difference , as shown in fig .
in the figure , the input space is a 123 by 123 pixel image and the patterns are images of handwritten digits .
the transformations considered are rotations of the digit images .
the tangent vector is obtained in two steps .
first the image is rotated by an infinitesimal amount a .
this is done by computing the rotated coordinates of each pixel and interpolating the gray level values at the new coordinates .
this operation can be advantageously combined with some smoothing using a convolution .
a convolution with a gaussian provides an efficient interpolation scheme in o ( nm ) multiply - adds , where nand m are the ( gaussian ) kernel and image sizes respectively .
the next step is to subtract ( pixel by pixel ) the rotated image from the original image and to divide the result
tangent prop - a formalism for specifying selected invariances in an adaptive network
by the scalar 123 ( see fig .
if ie types of transformations are considered , there will be ie different tangent vectors per pattern .
for most algorithms , these do not require any storage space since they can be generated as needed from the original pattern at negligible cost .
part it : tangent prop is an extension of the backpropagation algorithm , allowing it to learn directional derivatives .
other algorithms such as radial basis functions can be extended in a similar fashion .
to implement our idea , we will modify the usual weight - update rule : is replaced with ~ w = - 123 ) ow ( e + j . ter )
~ w = - 123 ) ow
where 123 ) is the learning rate , e the usual objective function , er an additional objec ( cid : 123 ) tive function ( a regularizer ) that measures the discrepancy between the actual and desired directional derivatives in the directions of some selected transformations , and j . t is a weighting coefficient .
let x be an input pattern , y = g ( x ) be the input - output function of the network .
the regularizer er is of the form
where er ( x ) is
: e e trainingset
here , ki ( x ) is the desired directional derivative of g in the direction induced by transformation si applied to pattern x .
the second term in the norm symbol is the actual directional derivative , which can be rewritten as
= g ' ( x ) .
osi ( o , x )
where g ' ( x ) is the jacobian of g for pattern x , and osi ( o , x ) joo is the tangent vector associated to transformation si as described in part i .
multiplying the tangent vector by the jacobian involves one forward propagation through a " linearized " version of the network .
in the special case where local invariance with respect to the si ' s is desired , ki ( x ) is simply set to o .
composition of transformations : the theory of lie groups ( gilmore , 123 ) ensures that compositions of local ( small ) transformations si correspond to linear combinations of the corresponding tangent vectors ( the local transformations si have a structure of lie algebra ) .
consequently , if er ( x ) = 123 is verified , the network derivative in the direction of a linear combination of the tangent vectors is equal to the same linear combination of the desired derivatives .
in other words if the network is successfully trained to be locally invariant with respect to , say , horizontal translation and vertical translations , it will be invariant with respect to compositions we have derived and implemented an efficient algorithm , " tangent prop " , for per ( cid : 123 ) forming the weight update ( eq .
it is analogous to ordinary backpropagation ,
simard , victorri , le cun , and denker
figure 123 : forward propagated variables ( a , x , a , e ) , and backward propagated vari ( cid : 123 ) ables ( b , y , p , t / j ) in the regular network ( roman symbols ) and the jacobian ( lin ( cid : 123 ) earized ) network ( greek symbols )
but in addition to propagating neuron activations , it also propagates the tangent vectors .
the equations can be easily derived from fig
a ~ = ~ wl x ' . - l
x ~ = u ( ad
tangent forward propagation :
, _ ~ , ~ ' - 123 ai - l . . . j ww " i
e ! = u ' ( a ~ ) a ~
tangent gradient backpropagation :
( 123 - ~ w ' +123i . l+123 i - l . . . j
b ' - ~ w123+ 123yl+123 i - l . . . j
123 ( e ( w , up ) + i ' er ( w , up , tp ) ) _ 123 - 123 , + ~ ' - l . i . '
- xi yi
tangent prop - - a formalism for specifying selected invariances in an adaptive network
the test set
training set size
figure 123 : generalization performance curve as a function of the training set size for the tangent prop and the backprop algorithms
the regularization parameter jj is tremendously important , because it determines the tradeoff between minimizing the usual objective function and minimizing the directional derivative error .
two experiments illustrate the advantages of tangent prop .
the first experiment is a classification task , using a small ( linearly separable ) set of 123 binarized hand ( cid : 123 ) written digit .
the training sets consist of 123 , 123 , 123 , 123 , 123 or 123 patterns , and the training set contains the remaining 123 patterns .
the patterns are smoothed using a gaussian kernel with standard deviation of one half pixel .
for each of the training set patterns , the tangent vectors for horizontal and vertical translation are computed .
the network has two hidden layers with locally connected shared weights , and one output layer with 123 units ( 123 connections , 123 free parame ( cid : 123 ) ters ) ( le cun , 123 ) .
the generalization performance as a function of the training set size for traditional backprop and tangent prop are compared in fig .
we have conducted additional experiments in which we implemented not only translations but also rotations , expansions and hyperbolic deformations .
this set of 123 gener ( cid : 123 ) ators is a basis for all linear transformations of coordinates for two dimensional images .
it is straightforward to implement other generators including gray - ievel ( cid : 123 ) shifting , " smooth " segmentation , local continuous coordinate transformations and independent image segment transformations .
the next experiment is designed to show that in applications where data is highly
simard , victorri , le cun , and denker
av " ge nmse vi 123ge
nmse vi .
o 123 123 123 123 123 123 123 123 123 123
123 123 123 123 123 123 123 123 123 123
- 123 . 123 + - - _+_ - _ - - +_ - _+_ - _ - _
figure 123 : comparison of the distortion model ( left column ) and tangent prop ( right column ) .
the top row gives the learning curves ( error versus number of sweeps through the training set ) .
the bottom row gives the final input - output function of the network; the dashed line is the result for unadorned back prop .
tangent prop - a formalism for specifying selected invariances in an adaptive network
correlated , tangent prop yields a large speed advantage .
since the distortion model implies adding lots of highly correlated data , the advantage of tangent prop over the distortion model becomes clear .
the task is to approximate a function that has plateaus at three locations .
we want to enforce local invariance near each of the training points ( fig .
123 , bottom ) .
the network has one input unit , 123 hidden units and one output unit .
two strategies are possible : either generate a small set of training point covering each of the plateaus ( open squares on fig .
123 bottom ) , or generate one training point for each plateau ( closed squares ) , and enforce local invariance around them ( by setting the desired derivative to 123 ) .
the training set of the former method is used as a measure the performance for both methods .
all parameters were adjusted for approximately optimal performance in all cases .
the learning curves for both models are shown in fig .
123 ( top ) .
each sweep through the training set for tangent prop is a little faster since it requires only 123 forward propagations , while it requires 123 in the distortion model .
as can be seen , stable performance is achieved after 123 sweeps for the tangent prop , versus 123 for the distortion model .
the overall speedup is therefore tangent prop in this example can take advantage of a very large regularization term .
the distortion model is at a disadvantage because the only parameter that effec ( cid : 123 ) tively controls the amount of regularization is the magnitude of the distortions , and this cannot be increased to large values because the right answer is only invariant under small distortions .
when a priori information about invariances exists , this information must be made available to the adaptive system .
there are several ways of doing this , including the distortion model and tangent prop .
the latter may be much more efficient in some applications , and it permits separate control of the emphasis and learning rate for the invariances , relative to the original training data points .
training a system to have zero derivatives in some directions is a powerful tool to express invariances to transformations of our choosing .
tests of this procedure on large - scale applications ( handwritten zipcode recognition ) are in progress .
