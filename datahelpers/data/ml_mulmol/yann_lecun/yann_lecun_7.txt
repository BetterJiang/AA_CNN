we introduce dropconnect , a generalization of dropout ( hinton et al . , 123 ) , for regular - izing large fully - connected layers within neu - ral networks .
when training with dropout , a randomly selected subset of activations are set to zero within each layer .
dropcon - nect instead sets a randomly selected sub - set of weights within the network to zero .
each unit thus receives input from a ran - dom subset of units in the previous layer .
we derive a bound on the generalization per - formance of both dropout and dropcon - nect .
we then evaluate dropconnect on a range of datasets , comparing to dropout , and show state - of - the - art results on several image recognition benchmarks by aggregating mul - tiple dropconnect - trained models .
neural network ( nn ) models are well suited to do - mains where large labeled datasets are available , since their capacity can easily be increased by adding more layers or more units in each layer .
however , big net - works with millions or billions of parameters can easily overt even the largest of datasets .
correspondingly , a wide range of techniques for regularizing nns have been developed .
adding an ( cid : 123 ) 123 penalty on the network weights is one simple but eective approach .
other forms of regularization include : bayesian methods ( mackay , 123 ) , weight elimination ( weigend et al . , 123 ) and early stopping of training .
in practice , us - ing these techniques when training big networks gives superior test performance to smaller networks trained
proceedings of the 123 th international conference on ma - chine learning , atlanta , georgia , usa , 123
w&cp volume 123
copyright 123 by the author ( s ) .
recently , hinton et al .
proposed a new form of regular - ization called dropout ( hinton et al . , 123 ) .
for each training example , forward propagation involves ran - domly deleting half the activations in each layer .
the error is then backpropagated only through the remain - ing activations .
extensive experiments show that this signicantly reduces over - tting and improves test per - formance .
although a full understanding of its mech - anism is elusive , the intuition is that it prevents the network weights from collaborating with one another to memorize the training examples .
in this paper , we propose dropconnect which general - izes dropout by randomly dropping the weights rather than the activations .
like dropout , the technique is suitable for fully connected layers only .
we compare and contrast the two methods on four dierent image
to demonstrate our method we consider a fully con - nected layer of a neural network with input v = ( v123 , v123 , .
, vn ) t and weight parameters w ( of size d n ) .
the output of this layer , r = ( r123 , r123 , .
, rd ) t is computed as a matrix multiply between the input vector and the weight matrix followed by a non - linear activation function , a , ( biases are included in w with a corresponding xed input of 123 for simplicity ) :
r = a ( u ) = a ( w v )
dropout was proposed by ( hinton et al . , 123 ) as a form of regularization for fully connected neural network layers .
each element of a layers output is kept with probability p , otherwise being set to 123 with probability ( 123 p ) .
extensive experiments show that dropout improves the networks generalization ability , giving improved test performance .
when dropout is applied to the outputs of a fully con -
regularization of neural networks using dropconnect
figure 123
( a ) : an example model layout for a single dropconnect layer .
after running feature extractor g ( ) on input x , a random instantiation of the mask m ( e . g .
( b ) ) , masks out the weight matrix w .
the masked weights are multiplied with this feature vector to produce u which is the input to an activation function a and a softmax layer s .
for comparison , ( c ) shows an eective weight mask for elements that dropout uses when applied to the previous layers output ( red columns ) and this layers output ( green rows ) .
note the lack of structure in ( b ) compared to ( c ) .
nected layer , we can write eqn
r = m ( cid : 123 ) a ( w v )
where ( cid : 123 ) denotes element wise product and m is a bi - nary mask vector of size d with each element , j , drawn independently from mj bernoulli ( p ) .
many commonly used activation functions such as tanh , centered sigmoid and relu ( nair and hinton , 123 ) , have the property that a ( 123 ) = 123
thus , eqn .
123 could be re - written as , r = a ( m ( cid : 123 ) w v ) , where dropout is applied at the inputs to the activation function .
dropconnect is the generalization of dropout in which each connection , rather than each output unit , can be dropped with probability 123 p .
dropconnect is similar to dropout as it introduces dynamic sparsity within the model , but diers in that the sparsity is on the weights w , rather than the output vectors of a layer .
in other words , the fully connected layer with dropconnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage .
note that this is not equivalent to setting w to be a xed sparse matrix during training .
for a dropconnect layer , the output is given as :
r = a ( ( m ( cid : 123 ) w ) v )
where m is a binary matrix encoding the connection information and mij bernoulli ( p ) .
each element of the mask m is drawn independently for each exam - ple during training , essentially instantiating a dier - ent connectivity for each example seen .
additionally ,
the biases are also masked out during training .
from eqn .
123 and eqn .
123 , it is evident that dropconnect is the generalization of dropout to the full connection structure of a layer123
the paper structure is as follows : we outline details on training and running inference in a model using drop - connect in section 123 , followed by theoretical justica - tion for dropconnect in section 123 , gpu implementa - tion specics in section 123 , and experimental results in
model description
we consider a standard model architecture composed of four basic components ( see fig
feature extractor : v = g ( x; wg ) where v are the out - put features , x is input data to the overall model , and wg are parameters for the feature extractor .
we choose g ( ) to be a multi - layered convolutional neural network ( cnn ) ( lecun et al . , 123 ) , with wg being the convolutional lters ( and biases ) of the cnn .
dropconnect layer : r = a ( u ) = a ( ( m ( cid : 123 ) w ) v ) where v is the output of the feature extractor , w is a fully connected weight matrix , a is a non - linear activation function and m is the binary mask matrix .
softmax classication layer : o = s ( r; ws ) takes as input r and uses parameters ws to map this to a k dimensional output ( k being the number of classes ) .
cross entropy loss : a ( y , o ) = ( cid : 123 ) k
i=123 yilog ( oi ) takes probabilities o and the ground truth labels y as input .
123this holds when a ( 123 ) = 123 , as is the case for tanh and
dropconnect weights w ( d x n ) b ) dropconnect mask m features v ( n x 123 ) u ( d x 123 ) a ) model layout activation function a ( u ) outputs r ( d x 123 ) feature extractor g ( x;wg ) input x softmax layer s ( r;ws ) predictions o ( k x 123 ) c ) effective dropout mask m previous layer mask current layer output mask regularization of neural networks using dropconnect
the overall model f ( x; , m ) therefore maps input data x to an output o through a sequence of operations given the parameters = ( wg , w , ws ) and randomly - drawn mask m .
the correct value of o is obtained by summing out over all possible masks m :
o = em ( f ( x; , m ) ) =
p ( m ) f ( x; , m )
this reveals the mixture model interpretation of drop - connect ( and dropout ) , where the output is a mixture of 123|m| dierent networks , each with weight p ( m ) .
if p = 123 , then these weights are equal and o =
m s ( a ( ( m ( cid : 123 ) w ) v ) ; ws )
m f ( x; , m ) = 123|m|
training the model described in section 123 begins by selecting an example x from the training set and ex - tracting features for that example , v .
these features are input to the dropconnect layer where a mask ma - trix m is rst drawn from a bernoulli ( p ) distribution to mask out elements of both the weight matrix and the biases in the dropconnect layer .
a key compo - nent to successfully training with dropconnect is the selection of a dierent mask for each training exam - ple .
selecting a single mask for a subset of training examples , such as a mini - batch of 123 examples , does not regularize the model enough in practice .
since the memory requirement for the m s now grows with the size of each mini - batch , the implementation needs to be carefully designed as described in section 123
once a mask is chosen , it is applied to the weights and biases in order to compute the input to the activa - tion function .
this results in r , the input to the soft - max layer which outputs class predictions from which cross entropy between the ground truth labels is com - puted .
the parameters throughout the model then can be updated via stochastic gradient descent ( sgd ) by backpropagating gradients of the loss function with respect to the parameters , a ( cid : 123 ) .
to update the weight matrix w in a dropconnect layer , the mask is ap - plied to the gradient to update only those elements that were active in the forward pass .
additionally , when passing gradients down to the feature extractor , the masked weight matrix m ( cid : 123 ) w is used .
a summary of these steps is provided in algorithm 123
inference time , we need to compute r = m a ( ( m ( cid : 123 ) w ) v ) , which naively requires the evaluation of 123|m| dierent masks plainly infeasible .
the dropout work ( hinton et al . , 123 ) made the ap - m ( m ( cid : 123 ) w ) v ) ,
m a ( ( m ( cid : 123 ) w ) v ) a ( ( cid : 123 )
algorithm 123 sgd training with dropconnect
input : example x , parameters t123 from step t 123 , output : updated parameters t extract features : v g ( x; wg ) random sample m mask : mij bernoulli ( p ) compute activations : r = a ( ( m ( cid : 123 ) w ) v ) compute output : o = s ( r; ws ) dierentiate loss a ( cid : 123 ) update softmax layer : ws = ws a ( cid : 123 ) update dropconnect layer : w = w ( m ( cid : 123 ) a ( cid : 123 ) update feature extractor : wg = wg a ( cid : 123 )
with respect to parameters :
algorithm 123 inference with dropconnect
input : example x , parameters , # of samples z .
output : prediction u extract features : v g ( x; wg ) moment matching of u :
123 vm ( u )
for z = 123 : z do %% draw z samples
for i = 123 : d do %% loop over units in r
sample from 123d gaussian ui , z n ( i , 123
pass result r = ( cid : 123 ) z
z=123 rz / z to next layer
averaging before the activation rather than after .
although this seems to work in practice , it is not jus - tied mathematically , particularly for the relu activa -
we take a dierent approach .
consider a single unit ui before the activation function a ( ) : ui = j ( wijvj ) mij .
this is a weighted sum of bernoulli variables mij , which can be approximated by a gaus - sian via moment matching .
the mean and variance of the units u are : em ( u ) = pw v and vm ( u ) = p ( 123 p ) ( w ( cid : 123 ) w ) ( v ( cid : 123 ) v ) .
we can then draw samples from this gaussian and pass them through the activa - tion function a ( ) before averaging them and present - ing them to the next layer .
algorithm 123 summarizes the method .
note that the sampling can be done ef - ciently , since the samples for each unit and exam - ple can be drawn in parallel .
this scheme is only an approximation in the case of multi - layer network , it works well in practise as shown in experiments .
123consider u n ( 123 , 123 ) , with a ( u ) = max ( u , 123 ) .
a ( em ( u ) ) = 123 but em ( a ( u ) ) = 123 /
regularization of neural networks using dropconnect
bit ( tex123d aligned memory ) cublas + read mask weight
table 123
performance comparison between dierent implementations of our dropconnect layer on nvidia gtx123 gpu relative to a 123ghz intel xeon ( compiled with - o123 ag ) .
input dimension and output dimension are 123 and mini - batch size is 123
as reference we provide traditional matrix multiplication using the cublas library .
model generalization bound we now show a novel bound for the rademacher com - plexity of the model r ( cid : 123 ) ( f ) on the training set ( see appendix for derivation ) :
where max|ws| bs , max|w| b , k is the num - ber of classes , r ( cid : 123 ) ( g ) is the rademacher complexity of the feature extractor , n and d are the dimensionality of the input and output of the dropconnect layer re - spectively .
the important result from eqn .
123 is that the complexity is a linear function of the probability p of an element being kept in dropconnect or dropout .
when p = 123 , the model complexity is zero , since the input has no inuence on the output .
when p = 123 , it returns to the complexity of a standard model .
implementation details
our system involves three components implemented on a gpu : 123 ) a feature extractor , 123 ) our dropconnect layer , and 123 ) a softmax classication layer .
for 123 and 123 we utilize the cuda - convnet package ( krizhevsky , 123 ) , a fast gpu based convolutional network library .
we implement a custom gpu kernel for performing the operations within the dropconnect layer .
our code is available at http : / / / cs . nyu . edu / ~ wanli /
a typical fully connected layer is implemented as a matrix - matrix multiplication between the input vec - tors for a mini - batch of training examples and the weight matrix .
the diculty in our case is that each training example requires its own random mask ma - trix applied to the weights and biases of the dropcon - nect layer .
this leads to several complications :
for a weight matrix of size d n , the corresponding mask matrix is of size d n b where b is the size of the mini - batch .
for a 123 fully connected layer with mini - batch size of 123 , the matrix would be too large to t into gpu memory if each element is stored as a oating point number , requiring 123g of memory .
once a random instantiation of the mask is created , it is non - trivial to access all the elements required during the matrix multiplications so as to maximize perfor -
the rst problem is not hard to address .
each ele - ment of the mask matrix is stored as a single bit to encode the connectivity information rather than as a oat .
the memory cost is thus reduced by 123 times , which becomes 123m for the example above .
this not only reduces the memory footprint , but also reduces the bandwidth required as 123 elements can be accessed with each 123 - byte read .
we overcome the second prob - lem using an ecient memory access pattern using 123d texture aligned memory .
these two improvements are crucial for an ecient gpu implementation of drop - connect as shown in table 123
here we compare to a naive cpu implementation with oating point masks and get a 123 speedup with our ecient gpu design .
we evaluate our dropconnect model for regularizing deep neural networks trained for image classication .
all experiments use mini - batch sgd with momentum on batches of 123 images with the momentum param - eter xed at 123 .
we use the following protocol for all experiments un - less otherwise stated :
augment the dataset by :
tions of the training sequence .
123 ) randomly selecting cropped regions from the images , 123 ) ipping images horizontally , 123 ) introducing 123% scaling and rotation train 123 independent networks with random permuta - manually decrease the learning rate if the network stops improving as in ( krizhevsky , 123 ) according to a schedule determined on a validation set .
train the fully connected layer using dropout , drop - at inference time for dropconnect we draw z = 123
connect , or neither ( no - drop ) .
regularization of neural networks using dropconnect
samples at the inputs to the activation function of the fully connected layer and average their activations .
to anneal the initial learning rate we choose a xed multiplier for dierent stages of training .
we report three numbers of epochs , such as 123 - 123 - 123 to dene our schedule .
we multiply the initial rate by 123 for the rst such number of epochs .
then we use a multiplier of 123 for the second number of epochs followed by 123 again for this second number of epochs .
the third number of epochs is used for multipliers of 123 , 123 , 123 , and 123 in that order , after which point we report our results .
we determine the epochs to use for our schedule using a validation set to look for plateaus in the loss function , at which point we move to the next multiplier
once the 123 networks are trained we report two num - bers : 123 ) the mean and standard deviation of the classi - cation errors produced by each of the 123 independent networks , and 123 ) the classication error that results when averaging the output probabilities from the 123 networks before making a prediction .
we nd in prac - tice this voting scheme , inspired by ( ciresan et al . , 123 ) , provides signicant performance gains , achiev - ing state - of - the - art results in many standard bench - marks when combined with our dropconnect layer .
the mnist handwritten digit classication task ( le - cun et al . , 123 ) consists of 123 black and white im - ages , each containing a digit 123 to 123 ( 123 - classes ) .
each digit in the 123 , 123 training images and 123 , 123 test images is normalized to t in a 123 123 pixel box while preserving their aspect ratio .
we scale the pixel values to the ( 123 , 123 ) range before inputting to our models .
for our rst experiment on this dataset , we train mod - els with two fully connected layers each with 123 out - put units using either tanh , sigmoid or relu activation functions to compare to dropout in ( hinton et al . , 123 ) .
the rst layer takes the image pixels as input , while the second layers output is fed into a 123 - class softmax classication layer .
in table 123 we show the performance of various activations functions , compar - ing no - drop , dropout and dropconnect in the fully connected layers .
no data augmentation is utilized in this experiment .
we use an initial learning rate of 123 and train for 123 - 123 - 123 epochs using our schedule .
from table 123 we can see that both dropout and drop - 123in all experiments the bias learning rate is 123 the learning rate for the weights .
additionally weights are ini - tialized with n ( 123 , 123 ) random values for fully connected layers and n ( 123 , 123 ) for convolutional layers .
table 123
mnist classication error rate for models with two fully connected layers of 123 neurons each .
no data augmentation is used in this experiment .
connect perform better than not using either method .
dropconnect mostly performs better than dropout in this task , with the gap widening when utilizing the voting over the 123 models .
to further analyze the eects of dropconnect , we show three explanatory experiments in fig .
123 using a 123 - layer fully connected model on mnist digits .
123a shows test performance as the number of hidden units in each layer varies .
as the model size increases , no - drop overts while both dropout and dropconnect improve performance .
dropconnect consistently gives a lower error rate than dropout .
123b shows the ef - fect of varying the drop rate p for dropout and drop - connect for a 123 - 123 unit network .
both methods give optimal performance in the vicinity of 123 , the value used in all other experiments in the paper .
our sampling approach gives a performance gain over mean inference ( as used by hinton ( hinton et al . , 123 ) ) , but only for the dropconnect case .
in fig .
123c we plot the convergence properties of the three methods throughout training on a 123 - 123 network .
we can see that no - drop overts quickly , while dropout and dropconnect converge slowly to ultimately give supe - rior test performance .
dropconnect is even slower to converge than dropout , but yields a lower test error in the end .
in order to improve our classication result , we choose a more powerful feature extractor network described in ( ciresan et al . , 123 ) ( relu is used rather than tanh ) .
this feature extractor consists of a 123 layer cnn with 123 - 123 feature maps in each layer respectively .
the last layers output is treated as input to the fully con - nected layer which has 123 relu units on which no - drop , dropout or dropconnect are applied .
we re - port results in table 123 from training the network on a ) the original mnist digits , b ) cropped 123 123 im - ages from random locations , and c ) rotated and scaled versions of these cropped images .
we use an initial
regularization of neural networks using dropconnect
figure 123
using the mnist dataset , in a ) we analyze the ability of dropout and dropconnect to prevent overtting as the size of the 123 fully connected layers increase .
b ) varying the drop - rate in a 123 - 123 network shows near optimal performance around the p = 123 proposed by ( hinton et al . , 123 ) .
c ) we show the convergence properties of the train / test sets .
see text for discussion .
learning rate of 123 with a 123 - 123 - 123 epoch sched - ule , no momentum and preprocess by subtracting the
table 123
mnist classication error .
previous state of the art is 123 . 123 % ( zeiler and fergus , 123 ) for a single model without elastic distortions and 123% with elastic distor - tions and voting ( ciresan et al . , 123 ) .
we note that our approach surpasses the state - of - the - art result of 123% ( ciresan et al . , 123 ) , achieving a 123% error rate , without the use of elastic distortions ( as used by ( ciresan et al . , 123 ) ) .
cifar - 123 is a data set of natural 123x123 rgb images ( krizhevsky , 123 ) in 123 - classes with 123 , 123 images for training and 123 , 123 for testing .
before inputting these images to our network , we subtract the per - pixel mean computed over the training set from each image .
the rst experiment on cifar - 123 ( summarized in table 123 ) uses the simple convolutional network fea - ture extractor described in ( krizhevsky , 123 ) ( layers - 123sec . cfg ) that is designed for rapid training rather than optimal performance .
on top of the 123 - layer feature extractor we have a 123 unit fully connected layer which uses no - drop , dropout , or dropconnect .
no data augmentation is utilized for this experiment .
since this experiment is not aimed at optimal perfor - mance we report a single models performance with - out voting .
we train for 123 - 123 - 123 epochs with an ini - tial learning rate of 123 and their default weight de - cay .
dropconnect prevents overtting of the fully con - nected layer better than dropout in this experiment .
table 123
cifar - 123 classication error using the simple feature extractor described in ( krizhevsky , 123 ) ( layers - 123sec . cfg ) and with no data augmentation .
table 123 shows classication results of the network us - ing a larger feature extractor with 123 convolutional layers and 123 locally connected layers as described in ( krizhevsky , 123 ) ( layers - conv - local - 123pct . cfg ) .
a 123 neuron fully connected layer with relu activations is added between the softmax layer and feature extrac - tor .
following ( krizhevsky , 123 ) , images are cropped to 123x123 with horizontal ips and no rotation or scal - ing is performed .
we use an initial learning rate of 123 and train for 123 - 123 - 123 epochs with their de - fault weight decay .
model voting signicantly im - proves performance when using dropout or dropcon - nect , the latter reaching an error rate of 123% .
ad - ditionally , we trained a model with 123 networks with dropconnect and achieved a state - of - the - art result of 123% , indicating the power of our approach .
the street view house numbers ( svhn ) dataset in - cludes 123 , 123 images ( both training set and extra set ) and 123 , 123 testing images ( netzer et al . , 123 ) .
simi - lar to mnist , the goal is to classify the digit centered in each 123x123 rgb image .
due to the large variety of colors and brightness variations in the images , we pre -
123 . 123 . 123 . 123 . 123 . 123hidden units% test error nodropdropoutdropconnect123 . 123 . 123 . 123 . 123 . 123 . 123 . 123% of elements kept% test error dropout ( mean ) dropconnect ( mean ) dropout ( sampling ) dropconnect ( sampling ) 123epochcross entropy nodrop trainnodrop testdropout traindropout testdropconnect traindropconnect test regularization of neural networks using dropconnect
error ( % ) 123 network
table 123
cifar - 123 classication error using a larger fea - ture extractor .
previous state - of - the - art is 123% ( snoek et al . , 123 ) .
voting with 123 dropconnect networks pro - duces an error rate of 123% , signicantly beating the process the images using local contrast normalization as in ( zeiler and fergus , 123 ) .
the feature extractor is the same as the larger cifar - 123 experiment , but we instead use a larger 123 unit fully connected layer with relu activations between the softmax layer and the feature extractor .
after contrast normalizing , the training data is randomly cropped to 123 123 pixels and is rotated and scaled .
we do not do horizontal ips .
table 123 shows the classication performance for 123 models trained with an initial learning rate of 123 for a 123 - 123 - 123 epoch schedule .
due to the large training set size both dropout and dropconnect achieve nearly the same performance as no - drop .
however , using our data augmentation tech - niques and careful annealing , the per model scores eas - ily surpass the previous 123% state - of - the - art result of ( zeiler and fergus , 123 ) .
furthermore , our vot - ing scheme reduces the relative error of the previous state - of - to - art by 123% to achieve 123% error .
error ( % ) 123 network
table 123
svhn classication error .
the previous state - of - the - art is 123% ( zeiler and fergus , 123 ) .
in the nal experiment we evaluate our models on the 123 - fold norb ( jittered - cluttered ) dataset ( lecun et al . , 123 ) , a collection of stereo images of 123d mod - els .
for each image , one of 123 classes appears on a random background .
we train on 123 - folds of 123 , 123 images each and the test on a total of 123 , 123 images .
the images are downsampled from 123 to 123 as in ( ciresan et al . , 123 ) .
we use the same feature extractor as the larger cifar - 123 experiment .
there is a 123 unit fully con - nected layer with relu activations placed between the softmax layer and feature extractor .
rotation and scaling of the training data is applied , but we do not crop or ip the images as we found that to hurt per -
table 123
norm classication error for the jittered - cluttered dataset , using 123 training folds .
the previous state - of - art is 123% ( ciresan et al . , 123 ) .
formance on this dataset .
we trained with an initial learning rate of 123 and anneal for 123 - 123 - 123 epochs .
in this experiment we beat the previous state - of - the - art result of 123% using no - drop , dropout and drop - connect with our voting scheme .
while dropout sur - passes dropconnect slightly , both methods improve over no - drop in this benchmark as shown in table 123
we have presented dropconnect , which generalizes hinton et al .
s dropout ( hinton et al . , 123 ) to the en - tire connectivity structure of a fully connected neural network layer .
we provide both theoretical justica - tion and empirical results to show that dropconnect helps regularize large neural network models .
results on a range of datasets show that dropconnect often outperforms dropout .
while our current implementa - tion of dropconnect is slightly slower than no - drop or dropout , in large models models the feature extractor is the bottleneck , thus there is little dierence in over - all training time .
dropconnect allows us to train large models while avoiding overtting .
this yields state - of - the - art results on a variety of standard benchmarks using our ecient gpu implementation of dropcon -
denition 123 ( dropconnect network ) .
given data ( x123 , x123 , .
, x ( cid : 123 ) ) with labels set s with ( cid : 123 ) entries : ( y123 , y123 , .
, y ( cid : 123 ) ) , we dene the dropconnect network m p ( m ) f ( x; , m ) =
as a mixture model : o = ( cid : 123 )
em ( f ( x; , m ) )
each network f ( x; , m ) has weights p ( m ) and net - work parameters are = ( ws , w , wg ) .
ws are the softmax layer parameters , w are the dropconnect layer parameters and wg are the feature extractor pa - rameters .
further more , m is the dropconnect layer
now we reformulate the cross - entropy loss on top of the softmax into a single parameter function that com - bines the softmax output and labels , as a logistic .
regularization of neural networks using dropconnect
denition 123 ( logistic loss ) .
the following loss func - tion dened on k - class classication is call the logistic
ay ( o ) = ( cid : 123 )
= oi + ln
layer has the linear transformation function h and ac - tivation function a .
by lemma 123 and lemma 123 , we know the network complexity is bounded by :
r ( cid : 123 ) ( h g ) c
where c = 123 for identity neuron and c = 123 for others .
lemma 123
let fm be the class of real functions that depend on m , then r ( cid : 123 ) ( em ( fm ) ) em
( cid : 123 ) r ( cid : 123 ) ( fm ) ( cid : 123 ) r ( cid : 123 ) ( fm )
r ( cid : 123 ) ( p ( m ) fm ) ( cid : 123 )
m p ( m ) fm
r ( cid : 123 ) ( em ( fm ) )
m |p ( m ) | r ( cid : 123 ) ( fm ) = em
theorem 123 ( dropconnect network complexity ) .
consider the dropconnect neural network dened in denition 123
let r ( cid : 123 ) ( g ) be the empirical rademacher complexity of the feature extractor and r ( cid : 123 ) ( f ) be the empirical rademacher complexity of the whole net - work .
in addition , we assume : 123
weight parameter of dropconnect layer |w| bh 123
weight parameter of s , i . e .
|ws| bs ( l123 - norm of it is bounded by then we have : r ( cid : 123 ) ( f ) p
( cid : 123 ) r ( cid : 123 ) ( f ( x; , m )
y ( o ) 123
where y is binary vector with ith bit set on lemma 123
logistic loss function a has the following properties : 123 ) ay ( 123 ) = ln k , 123 ) 123 a ( cid : 123 ) y ( o ) 123 , and denition 123 ( rademacher complexity ) .
for a sample s = ( x123 , .
, x ( cid : 123 ) ) generated by a distribution d on set x and a real - valued function class f in domain x , the empirical rademacher complexity of f is the random
r ( cid : 123 ) ( f ) = e
if ( xi ) | | x123 , .
, x ( cid : 123 )
where sigma = ( 123 , .
, ( cid : 123 ) ) are independent uniform ( 123 ) - valued ( rademacher ) random variables .
the rademacher complexity of f is r ( cid : 123 ) ( f ) = es
( cid : 123 ) r ( cid : 123 ) ( f )
bound derivation lemma 123 ( ( ledoux and talagrand , 123 ) ) .
let f be class of real functions and h = ( fj ) k j=123 be a k - dimensional function class .
if a : rk r is a lips - chitz function with constant l and satises a ( 123 ) = 123 , then r ( cid : 123 ) ( a h ) 123kl r ( cid : 123 ) ( f ) lemma 123 ( classier generalization bound ) .
gener - alization bound of a k - class classier with logistic loss function is directly related rademacher complexity of i=123 ayi ( oi ) + 123k r ( cid : 123 ) ( f ) + 123 lemma 123
for all neuron activations : sigmoid , tanh and relu , we have : r ( cid : 123 ) ( a f ) 123 r ( cid : 123 ) ( f ) lemma 123 ( network layer bound ) .
let g be the class of real functions rd r with input dimension f , i . e .
j=123 and hb is a linear transform function g = ( fj ) d parametrized by w with ( cid : 123 ) w ( cid : 123 ) 123 b , then r ( cid : 123 ) ( hg )
i=123 ih g ( xi )
r ( cid : 123 ) ( hg ) = e
i f j ( xi )
i=123 if ( xi )
remark 123
given a layer in our network , we denote the function of all layers before as g = ( fj ) d
r ( cid : 123 ) ( f ) = r ( cid : 123 ) ( em ( f ( x; , m ) ) em
( cid : 123 ) r ( cid : 123 ) ( a hm g ) ( cid : 123 ) r ( cid : 123 ) ( hm g )
where hm = ( m ( cid : 123 ) w ) v .
equation ( 123 ) is based on lemma 123 , equation ( 123 ) is based on lemma 123 and equation ( 123 ) follows from lemma 123
( cid : 123 ) r ( cid : 123 ) ( hm g )
iw t dm g ( xi )
where dm in equation ( 123 ) is an diagonal matrix with diagonal elements equal to m and inner prod - uct properties lead to equation ( 123 ) .
thus , we have :
regularization of neural networks using dropconnect
weigend , d .
rumelhart , and b .
huberman .
generalization by weight - elimination with applica - tion to forecasting .
in nips , 123
zeiler and r .
fergus .
stochastic pooling for regualization of deep convolutional neural networks .
in iclr , 123
