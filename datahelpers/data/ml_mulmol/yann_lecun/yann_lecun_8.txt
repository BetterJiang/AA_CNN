abstract intelligent tasks , such as visual perception , auditory perception , and language understanding require the construction of good internal representations of the world ( or features ) , which must be invariant to irrelevant variations of the input while , preserving relevant information .
a major question for machine learning is how to learn such good features auto - matically .
convolutional networks ( convnets ) are a biologically - inspired trainable architecture that can learn invariant features .
each stage in a convnets is composed of a lter bank , some non - linearities , and feature pooling layers .
with multiple stages , a convnet can learn multi - level hierarchies of features .
while convnets have been successfully deployed in many commercial applications from ocr to video surveillance , they require large amounts of labeled training samples .
we describe new unsu - pervised learning algorithms , and new non - linear stages that allow convnets to be trained with very few labeled samples .
applications to visual object recognition and vision navigation for off - road mobile robots are described .
learning internal representations
independently of pose , scale ,
one of the key questions of vision science ( natural and articial ) is how to produce good internal representations of the visual world .
what sort of internal representation would allow an articial vision system to detect and classify objects conformation , and clutter ? more interestingly , how could an articial vision system learn appropriate internal representa - tions automatically , the way animals and human seem to learn by simply looking at the world ? in the time - honored approach to computer vision ( and to pattern recognition in general ) , the question is avoided : internal representations are produced by a hand - crafted feature extractor , whose output is fed to a trainable classier .
while the issue of learning features has been a topic of interest for many years , considerable progress has been achieved in the last few years with the development of so - called deep learning methods .
good internal representations are hierarchical .
in vision , pixels are assembled into edglets , edglets into motifs , motifs into parts , parts into objects , and objects into scenes .
this suggests that recognition architectures for vision ( and for other modalities such as audio and natural language ) should have multiple trainable stages stacked on top of each other , one for each level in the feature hierarchy .
this raises two new questions : what to put in each stage ? and how to train such deep , multi - stage architectures ? convolutional networks ( convnets ) are an answer to the rst question .
until recently , the answer to the second question was to use gradient - based supervised learning , but recent research in deep learning has produced a number of unsupervised methods which greatly reduce the need for labeled samples .
convolutional networks ( 123 ) , ( 123 ) are trainable multistage architectures composed of multiple stages .
the input and output of each stage are sets of arrays called feature maps .
for example , if the input is a color image , each feature map would be a 123d array containing a color channel of the input image ( for an audio input each feature map would be a 123d array , and for a video or volumetric image , it would be a 123d array ) .
at the output , each feature map represents a particular feature
a typical convnet architecture with two feature stages
extracted at all locations on the input .
each stage is composed of three layers : a lter bank layer , a non - linearity layer , and a feature pooling layer .
a typical convnet is composed of one , two or three such 123 - layer stages , followed by a classication module .
each layer type is now described for the case of image filter bank layer - f : the input is a 123d array with n123 123d feature maps of size n123 n123
each component is denoted xijk , and each feature map is denoted xi .
the output is also a 123d array , y composed of m123 feature maps of size m123 m123
a trainable lter ( kernel ) kij in the lter bank has size l123 l123 and connects input feature map xi to output feature map yj .
the module computes yj = bj + pi kij xi where is the 123d discrete convolution operator and bj is a trainable bias parameter .
each lter detects a particular feature at every location on the input .
hence spatially translating the input of a feature detection layer will translate the output but leave it non - linearity layer : in traditional convnets this simply consists in a pointwise tanh ( ) sigmoid function applied to each site ( ijk ) .
however , recent implementations have used more sophisticated non - linearities .
a useful one for natural im - age recognition is the rectied sigmoid rabs : abs ( gi . tanh ( ) ) where gi is a trainable gain parameter .
the rectied sigmoid is sometimes followed by a subtractive and divisive local normal - ization n , which enforces local competition between adjacent features in a feature map , and between features at the same spatial location .
the subtractive normalization operation for a given site xijk computes : vijk = xijk pipq wpq . xi , j+p , k+q , where wpq is a normalized truncated gaussian weighting window ( typically of size 123x123 ) .
the divisive normalization computes yijk = vijk / max ( mean ( jk ) , jk ) where jk = i , j+p , k+q ) 123 / 123
the local contrast normalization layer is inspired by visual neuroscience models ( 123 ) , ( 123 ) .
feature pooling layer : this layer treats each feature map separately .
in its simplest instance , called pa , it computes the average values over a neighborhood in each feature map .
the neighborhoods are stepped by a stride larger than 123 ( but smaller than or equal the pooling neighborhood ) .
this results in a reduced - resolution output feature map which is robust to small variations in the location of features in the previous layer .
the average operation is sometimes replaced by a max pm .
traditional convnets use a pointwise tanh ( ) after the pooling layer , but more recent models do not .
some convnets dispense with the separate pooling layer entirely , but use strides larger than one in the lter bank layer to reduce
123 - 123 - 123 - 123 - 123 / 123 / $123 123 ieee
an example of feature extraction stage of the type f rabsn pa .
an input image ( or a feature map ) is passed through a lter bank , followed by abs ( gi .
tanh ( ) ) , local subtractive and divisive contrast normalization , and
the resolution ( 123 ) , ( 123 ) .
in some recent versions of convnets , the pooling also pools similar feature at the same location , in addition to the same feature at nearby locations ( 123 ) .
supervised training is performed using a form of stochastic gradient descent to minimize the discrepancy between the desired output and the actual output of the network .
all the coefcient of all the lters in all the layers are updated simultaneously by the learning procedure .
the gradients are computed with the back - propagation method .
details of the procedure are given in ( 123 ) , and methods for efcient training are detailed in ( 123 ) .
history and applications
convnets can be seen as a representatives of a wide class of models that we will call multi - stage hubel - wiesel architectures .
the idea is rooted in hubel and wiesels classic 123 work on the cats primary visual cortex .
it identied orientation - selective simple cells with local receptive elds , whose role is similar to the convnets lter bank layers , and complex cells , whose role is similar to the pooling layers .
the rst such model to be simulated on a computer was fukushimas neocognitron ( 123 ) , which used a layer - wise , un - supervised competitive learning algorithm for the lter banks , and a separately - trained supervised linear classier for the output layer .
the innovation in ( 123 ) , ( 123 ) was to simplify the architecture and to use the back - propagation algorithm to train the entire system in a supervised fashion .
the approach was very successful for such tasks as ocr and handwrit - ing recognition .
an operational bank check reading system built around convnets was developed at at&t in the early 123s ( 123 ) .
it was rst deployed commercially in 123 , running on a dsp board in check - reading atm machines in europe and the us , and was deployed in large bank check reading machines in 123
by the late 123s it was reading over 123% of all the checks in the us .
this motivated microsoft to deploy convnets in a number of ocr and handwriting recognition systems ( 123 ) , ( 123 ) , ( 123 ) including for arabic ( 123 ) and chinese characters ( 123 ) .
supervised convnets have also been used for object detection in images , with record accuracy and real - time performance ( 123 ) , ( 123 ) , ( 123 ) , ( 123 ) , google recently deployed a convnet to detect faces and license plate in streetview images so as to protect privacy ( 123 ) .
nec has deployed convnet - based system in japan for tracking customers in supermarket and recognizing their gender and age .
vidient technologies has developed a convnet - based video surveillance system deployed in several airports in the us .
france telecom has deployed convnet - based face detection systems for video - conference and other systems ( 123 ) .
other experimental detection applications in - clude hands / gesture ( 123 ) , logos and text ( 123 ) .
a big advantage of convnets for detection is their computational efciency : even though the system is trained on small windows , it sufces to extend the convolutions to the size of the input image and replicate the output layer to compute detections at every location .
supervised convnets have also been used for vision - based obstacle avoidance for off - road mobile robots ( 123 )
participants in the recent darpa - sponsored lagr program on vision - based navigation for off - road robots used convnets for long - range obstacle detection ( 123 ) , ( 123 ) .
in ( 123 ) , the system is pre - trained off - line using a combination of unsupervised learning ( as described in section ii ) and supervised learning .
it is then adapted on - line , as the robot runs , using labels provided by a short - range stereovision system ( see videos at http : / / www . cs . nyu . edu / yann / research / lagr ) .
inter - esting new applications include image restoration ( 123 ) and image segmentation , particularly for biological images ( 123 ) .
the big advantage over mrfs is the ability to take a large context window into account .
stunning results were obtained at mit for reconstructing neuronal circuits from an stack of brain slice images a few nanometer thick
over the years , other instances of the multi - stage hubel - wiesel architecture have appeared that are in the tradition of the neocognitron : unlike supervised convnets , they use a combination of hand - crafting , and simple unsupervised methods to design the lter banks .
notable examples include mozers visual models ( 123 ) , and the so - called hmax family of models from t .
poggios lab at mit ( 123 ) , ( 123 ) , which uses hard - wired gabor lters in the rst stage , and a simple unsupervised random template selection algorithm for the second stage .
all stages use point - wise non - linearities and max pooling .
from the same institute , pinto et al .
( 123 ) have identied the most appropriate non - linearities and normaliza - tions by running systematic experiments with a a single - stage architecture using gpu - based parallel hardware .
unsupervised learning of convnets
training deep , multi - stage architectures using supervised gradient back propagation requires many labeled samples .
however in many problems labeled data is scarce whereas un - labeled data is abundant .
recent research in deep learning ( 123 ) , ( 123 ) , ( 123 ) has shown that unsupervised learning can be used to train each stage one after the other using only unlabeled data , reducing the requirement for labeled samples signi - cantly .
in ( 123 ) , using abs and normalization non - linearities , unsupervised pre - training , and supervised global renement has been shown to yield excellent performance on the caltech - 123 dataset with only 123 training samples per category ( more on this below ) .
in ( 123 ) , good accuracy was obtained on the same set using a very different unsupervised method based on sparse restricted boltzmann machines .
several works at nec have also shown that using auxiliary tasks ( 123 ) , ( 123 ) helps regularizing the system and produces excellent performance .
unsupervised training with predictive sparse decomposition the unsupervised method we propose , to learn the lter coefcients in the lter bank layers , is called predictive sparse decomposition ( psd ) ( 123 ) .
similar to the well - known sparse coding algorithms ( 123 ) , inputs are approximated as a sparse linear combination of dictionary elements .
in conventional sparse coding for any given input x , one needs to run an expensive optimization algorithm to nd z ( the basis pursuit problem ) .
psd trains a feed - forward regressor ( or encoder ) c ( x , k ) to quickly approximate the sparse solution z .
during training , the feature vector z is obtained by e ( z , w , k ) = kx w zk123
123 + kzk123 + kz c ( x , k ) k123
where w is the matrix whose columns are the dictionnary elements and k are the lters .
for each training sample x , one rst nds z that minimizes e , then w and k are
average recognition rates on caltech - 123
rabs n pa rabs pa n pm
to lower e .
once adjusted by stochastic gradient descent training is complete , the feature vector for a given input is simply obtained with z = c ( x , k ) , hence the process is extremely fast ( feed - forward ) .
results on object recognition
in this section , various architectures and training procedures are compared to determine which non - linearities are prefer - able , and which training protocol makes a difference .
generic object recognition using caltech 123 dataset : we use a two - stage system where , the rst stage is composed of an f layer with 123 lters of size 123 123 , followed by different combinations of non - linearities and pooling .
the second - stage feature extractor is fed with the output of the rst stage and extracts 123 output features maps , each of which combines a random subset of 123 feature maps from the previous stage using 123 123 kernels .
hence the total number of convolution kernels is 123 123 = 123
table i summarizes the results for the experiments , where u and r denotes unsupervised pre - training and random initialization respectively , and + denotes supervised ne - tuning of the whole system .
excellent accuracy of 123% is obtained using unsupervised pre - training and supervised renement with abs normalization non - linearities .
the result is on par with the popular model based on sift and pyramid match kernel svm ( 123 ) .
it is clear that abs and normalization are cruciala for achieving good performance .
this is an extremely important fact for users of convolutional networks , which traditionally only use tanh ( ) .
astonishingly , random lters without any lter learning whatsoever achieve decent performance ( 123% for r ) , as long as abs and normalization are present ( rabs n pa ) .
a more detailed study on this particular case can be found 123
comparing experiments from rows r vs r+ , u vs u + , we see that supervised ne tuning consistently improves the performance , particularly with weak non - linearities .
it seems that unsupervised pre - training ( u , u + ) is crucial when newly proposed non - linearities are not in place .
handwritten digit classication using mnist dataset :
using the evidence gathered in previous experiments , we used a two - stage system with a two - layer fully - connected classier .
the two convolutional stages were pre - trained unsupervised , and rened supervised .
an error rate of 123% was achieved ont he test set .
to our knowledge , this is the lowest error rate ever reported on the original mnist dataset , without distortions or preprocessing .
the best previously reported error rate was 123% ( 123 ) .
connection with other approaches in object recognition
many recent successful object recognition systems can also be seen as single or multi - layer feature extraction systems fol - lowed by a classier .
most common feature extraction systems like sift ( 123 ) , hog ( 123 ) are composed of lterbanks ( oriented edge detectors at multiple scales ) followed by non - linearities ( winner take all ) and pooling ( histogramming ) .
a pyramid
match kernel ( pmk ) svm ( 123 ) classifer can also be seen as another layer of feature extraction since it performs a k - means based feature extraction followed by local histogramming .
hardware and software implementations implementing convnets in software is best achieved using the modular , object - oriented approach suggested in ( 123 ) .
each basic module ( convolution , pooling , etc ) is implemented as a class with three member module . fprop ( input , output ) , which computes the output from the input , module . bprop ( input , output ) , which back - propagates gradients from the outputs back to the inputs and the internal trainable parameters , and op - tionally module . bbprop ( input , output ) , which may back - propagate second diagonal derivatives for the implemen - tation of second - order optimization algorithms ( 123 ) .
several software implementations of convnets are built around this concept , and have four basic capabilities : 123
a ex - ible multi - dimensional array library with basic operations such as dot products , and convolutions , 123
a class hierarchy of basic learning machine building blocs ( e . g .
multiple convolutions non - linear transforms , cost functions , .
a set of classes for energy - based inference ( 123 ) , gradient - based optimization , and performance measurement .
three available convnet implementations use this concept .
the rst one is part of the lush system , a lisp dialect with an interpreter and compiler with an easy interface to c ( 123 ) .
the second one is eblearn , a c++ machine learning library with class hierarchy to the lush implementation ( 123 ) .
third is torch - 123 ( 123 ) a c library with an interpreter front end based on lua .
all three systems come with facilities to manipulate large datasets , images , and videos .
the rst hardware implementations of convnets date back to the early 123s with bell labs anna chip , a mixed analog - digital processor that could compute 123 simultaneous 123 123 convolutions at a peak rate of 123 multiply - accumulate operations per second ( 123 ) , ( 123 ) , with 123 bit resolution on the states and 123 bits on the weights .
more recently , a group from the canon corporation developed a prototype convnet chip for low - power intelligent cameras ( 123 ) .
some current approaches rely on addressed - event representation ( aer ) convolvers , the advantage of not requiring multipliers to compute the convolutions .
caviar is the leading such project , with a performance of 123g connections / sec ( 123 ) .
fpga implementations of convnets appeared in the mid - 123s with ( 123 ) , which used low - accuracy arithmetic to avoid implementing full - edged multipliers .
fortunately , re - cent dsp - oriented fpgas include large numbers of hard - wired mac units , which allow extremely fast and low power implementations of convnets .
the cnp developed in our group ( 123 ) achieves 123gops for 123x123 kernels , with an archi - tecture that implements entire convnets , including pre / post - processing , and is entirely programmable .
an actual face de - tection application was demonstrated on this system , achieving 123fps on vga images ( 123 ) .
the convolutional network architecture is a remarkably versatile , yet conceptually simple paradigm that can be applied to a wide spectrum of perceptual tasks .
while traditional convnet trained with supervised learning are very effective , training them require a large number of labeled training samples .
we have shown that using simple architectural tricks such as rectication and contrast normalization , and using
unsupervised pre - training of each lter bank , the need for labeled samples is considerably reduced .
because of their applicability to a wide range of tasks , and because of their rel - atively uniform architecture , convnets are perfect candidates for hardware implementations , and embedded applications , as demonstrated by the increasing amount of work in this area .
we expect to see many new embedded vision systems based on convnets in the next few years .
despite the recent progress in deep learning , one of the major challenges of computer vision , machine learning , and ai in general in the next decade will be to devise methods that can automatically learn good features hierarchies from unlabeled and labeled data in an integrated fashion .
current and future research will focus on performing unsupervised learning on multiple stages simultaneously , on the integration of unsupervised and unsupervised learning , and on using the feed - back path implemented by the decoders to perform visual inference , such as pattern completion and disambiguation .
