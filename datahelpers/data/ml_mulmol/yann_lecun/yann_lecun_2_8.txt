energy - based models ( ebms ) capture dependencies between variables by as - sociating a scalar energy to each conguration of the variables .
inference consists in clamping the value of observed variables and nding congurations of the re - maining variables that minimize the energy .
learning consists in nding an energy function in which observed congurations of the variables are given lower energies than unobserved ones .
the ebm approach provides a common theoretical frame - work for many learning models , including traditional discriminative and genera - tive approaches , as well as graph - transformer networks , conditional random elds , maximum margin markov networks , and several manifold learning methods .
probabilistic models must be properly normalized , which sometimes requires evaluating intractable integrals over the space of all possible variable congura - tions .
since ebms have no requirement for proper normalization , this problem is naturally circumvented .
ebms can be viewed as a form of non - probabilistic factor graphs , and they provide considerably more exibility in the design of architec - tures and training criteria than probabilistic approaches .
123 introduction : energy - based models
the main purpose of statistical modeling and machine learning is to encode depen - dencies between variables .
by capturing those dependencies , a model can be used to answer questions about the values of unknown variables given the values of known
energy - based models ( ebms ) capture dependencies by associating a scalar en - ergy ( a measure of compatibility ) to each conguration of the variables .
inference , i . e . , making a prediction or decision , consists in setting the value of observed variables
and nding values of the remaining variables that minimize the energy .
learning con - sists in nding an energy function that associates low energies to correct values of the remaining variables , and higher energies to incorrect values .
a loss functional , mini - mized during learning , is used to measure the quality of the available energy functions .
within this common inference / learning framework , the wide choice of energy func - tions and loss functionals allows for the design of many types of statistical models , both probabilistic and non - probabilistic .
energy - based learning provides a unied framework for many probabilistic and non - probabilistic approaches to learning , particularly for non - probabilistic training of graphical models and other structured models .
energy - based learning can be seen as an alternative to probabilistic estimation for prediction , classication , or decision - making tasks .
because there is no requirement for proper normalization , energy - based ap - proaches avoid the problems associated with estimating the normalization constant in probabilistic models .
furthermore , the absence of the normalization condition allows for much more exibility in the design of learning machines .
most probabilistic mod - els can be viewed as special types of energy - based models in which the energy function satises certain normalizability conditions , and in which the loss function , optimized by learning , has a particular form .
this chapter presents a tutorial on energy - based models , with an emphasis on their use for structured output problems and sequence labeling problems .
section 123 intro - duces energy - based models and describes deterministic inference through energy min - imization .
section 123 introduces energy - based learning and the concept of the loss func - tion .
a number of standard and non - standard loss functions are described , including the perceptron loss , several margin - based losses , and the negative log - likelihood loss .
the negative log - likelihood loss can be used to train a model to produce conditional probability estimates .
section 123 shows how simple regression and classication mod - els can be formulated in the ebm framework .
section 123 concerns models that contain latent variables .
section 123 analyzes the various loss functions in detail and gives suf - cient conditions that a loss function must satisfy so that its minimization will cause the model to approach the desired behavior .
a list of good and bad loss functions is given .
section 123 introduces the concept of non - probabilistic factor graphs and infor - mally discusses efcient inference algorithms .
section 123 focuses on sequence labeling and structured output models .
linear models such as max - margin markov networks and conditional random elds are re - formulated in the ebm framework .
the liter - ature on discriminative learning for speech and handwriting recognition , going back to the late 123s and early 123s , is reviewed .
this includes globally trained systems that integrate non - linear discriminant functions , such as neural networks , and sequence alignment methods , such as dynamic time warping and hidden markov models .
hier - archical models such as the graph transformer network architecture are also reviewed .
finally , the differences , commonalities , and relative advantages of energy - based ap - proaches , probabilistic approaches , and sampling - based approximate methods such as contrastive divergence are discussed in section 123
n e ( y , x )
figure 123 : a model measures the compatibility between observed variables x and variables to be predicted y using an energy function e ( y , x ) .
for example , x could be the pixels of an image , and y a discrete label describing the object in the image .
given x , the model produces the answer y that minimizes the energy e .
123 energy - based inference
let us consider a model with two sets of variables , x and y , as represented in fig - ure 123
variable x could be a vector containing the pixels from an image of an object .
variable y could be a discrete variable that represents the possible category of the ob - ject .
for example , y could take six possible values : animal , human gure , airplane , truck , car , and none of the above .
the model is viewed as an energy function which measures the goodness ( or badness ) of each possible conguration of x and y .
the output number can be interpreted as the degree of compatibility between the values of x and y .
in the following , we use the convention that small energy values correspond to highly compatible congurations of the variables , while large energy values corre - spond to highly incompatible congurations of the variables .
functions of this type are given different names in different technical communities; they may be called contrast functions , value functions , or negative log - likelihood functions .
in the following , we will use the term energy function and denote it e ( y , x ) .
a distinction should be made between the energy function , which is minimized by the inference process , and the loss functional ( introduced in section 123 ) , which is minimized by the learning process .
in the most common use of a model , the input x is given ( observed from the world ) , and the model produces the answer y that is most compatible with the observed x .
more precisely , the model must produce the value y , chosen from a set y , for which e ( y , x ) is the smallest :
y = argminy y e ( y , x ) .
when the size of the set y is small , we can simply compute e ( y , x ) for all possible values of y y and pick the smallest .
figure 123 : several applications of ebms : ( a ) face recognition : y is a high - cardinality discrete variable; ( b ) face detection and pose estimation : y is a collection of vectors with location and pose of each possible face; ( c ) image segmentation : y is an image in which each pixel is a discrete label; ( d - e ) handwriting recognition and sequence labeling : y is a sequence of symbols from a highly structured but potentially innite set ( the set of english sentences ) .
the situation is similar for many applications in natural language processing and computational biology; ( f ) image restoration : y is a high - dimensional continuous variable ( an image ) .
in general , however , picking the best y may not be simple .
figure 123 depicts sev - eral situations in which y may be too large to make exhaustive search practical .
in figure 123 ( a ) , the model is used to recognize a face .
in this case , the set y is discrete and nite , but its cardinality may be tens of thousands ( chopra et al . , 123 ) .
in fig - ure 123 ( b ) , the model is used to nd the faces in an image and estimate their poses .
the set y contains a binary variable for each location indicating whether a face is present at that location , and a set of continuous variables representing the size and orienta - tion of the face ( osadchy et al . , 123 ) .
in figure 123 ( c ) , the model is used to segment a biological image : each pixel must be classied into one of ve categories ( cell nu - cleus , nuclear membrane , cytoplasm , cell membrane , external medium ) .
in this case , y contains all the consistent label images , i . e .
the ones for which the nuclear mem - branes are encircling the nuclei , the nuclei and cytoplasm are inside the cells walls , etc .
the set is discrete , but intractably large .
more importantly , members of the set must satisfy complicated consistency constraints ( ning et al . , 123 ) .
in figure 123 ( d ) , the model is used to recognize a handwritten sentence .
here y contains all possible sentences of the english language , which is a discrete but innite set of sequences of symbols ( lecun et al . , 123a ) .
in figure 123 ( f ) , the model is used to restore an image ( by cleaning the noise , enhancing the resolution , or removing scratches ) .
the set y contains all possible images ( all possible pixel combinations ) .
it is a continuous and
for each of the above situations , a specic strategy , called the inference procedure , must be employed to nd the y that minimizes e ( y , x ) .
in many real situations , the inference procedure will produce an approximate result , which may or may not be the global minimum of e ( y , x ) for a given x .
in fact , there may be situations where e ( y , x ) has several equivalent minima .
the best inference procedure to use often depends on the internal structure of the model .
for example , if y is continuous and e ( y , x ) is smooth and well - behaved with respect to y , one may use a gradient - based optimization algorithm .
if y is a collection of discrete variables and the energy func - tion can be expressed as a factor graph , i . e .
a sum of energy functions ( factors ) that depend on different subsets of variables , efcient inference procedures for factor graphs can be used ( see section 123 ) ( kschischang et al . , 123 , mackay , 123 ) .
a popular ex - ample of such a procedure is the min - sum algorithm .
when each element of y can be represented as a path in a weighted directed acyclic graph , then the energy for a partic - ular y is the sum of values on the edges and nodes along a particular path .
in this case , the best y can be found efciently using dynamic programming ( e . g with the viterbi algorithm or a ) .
this situation often occurs in sequence labeling problems such as speech recognition , handwriting recognition , natural language processing , and biolog - ical sequence analysis ( e . g .
gene nding , protein folding prediction , etc ) .
different situations may call for the use of other optimization procedures , including continuous optimization methods such as linear programming , quadratic programming , non - linear optimization methods , or discrete optimization methods such as simulated annealing , graph cuts , or graph matching .
in many cases , exact optimization is impractical , and one must resort to approximate methods , including methods that use surrogate energy functions ( such as variational methods ) .
123 what questions can a model answer ?
in the preceding discussion , we have implied that the question to be answered by the model is what is the y that is most compatible with this x ? , a situation that occurs in prediction , classication or decision - making tasks .
however , a model may be used to answer questions of several types :
prediction , classication , and decision - making : which value of y is most com - patible with this x ? this situation occurs when the model is used to make hard decisions or to produce an action .
for example , if the model is used to drive a robot and avoid obstacles , it must produce a single best decision such as steer left , steer right , or go straight .
ranking : is y123 or y123 more compatible with this x ? this is a more complex task than classication because the system must be trained to produce a complete ranking of all the answers , instead of merely producing the best one .
this situ - ation occurs in many data mining applications where the model is used to select multiple samples that best satisfy a given criterion .
detection : is this value of y compatible with x ? typically , detection tasks , such as detecting faces in images , are performed by comparing the energy of a face label with a threshold .
since the threshold is generally unknown when the system is built , the system must be trained to produce energy values that increase as the image looks less like a face .
conditional density estimation : what is the conditional probability distribution over y given x ? this case occurs when the output of the system is not used directly to produce actions , but is given to a human decision maker or is fed to the input of another , separately built system .
we often think of x as a high - dimensional variable ( e . g .
an image ) and y as a discrete variable ( e . g .
a label ) , but the converse case is also common .
this occurs when the model is used for such applications as image restoration , computer graphics , speech and language production , etc .
the most complex case is when both x and y
123 decision making versus probabilistic modeling
for decision - making tasks , such as steering a robot , it is merely necessary that the sys - tem give the lowest energy to the correct answer .
the energies of other answers are irrelevant , as long as they are larger .
however , the output of a system must sometimes be combined with that of another system , or fed to the input of another system ( or to a human decision maker ) .
because energies are uncalibrated ( i . e .
measured in arbitrary units ) , combining two , separately trained energy - based models is not straightforward : there is no a priori guarantee that their energy scales are commensurate .
calibrating energies so as to permit such combinations can be done in a number of ways .
however , the only consistent way involves turning the collection of energies for all possible out - puts into a normalized probability distribution .
the simplest and most common method
for turning a collection of arbitrary energies into a collection of numbers between 123 and 123 whose sum ( or integral ) is 123 is through the gibbs distribution :
p ( y |x ) =
ryy ee ( y , x ) ,
where is an arbitrary positive constant akin to an inverse temperature , and the denom - inator is called the partition function ( by analogy with similar concepts in statistical physics ) .
the choice of the gibbs distribution may seem arbitrary , but other proba - bility distributions can be obtained ( or approximated ) through a suitable re - denition of the energy function .
whether the numbers obtained this way are good probability estimates does not depend on how energies are turned into probabilities , but on how e ( y , x ) is estimated from data .
it should be noted that the above transformation of energies into probabilities is
only possible if the integral ryy ee ( y , x ) converges .
this somewhat restricts the
energy functions and domains y that can be used .
more importantly , there are many practical situations where computing the partition function is intractable ( e . g .
when y has high cardinality ) , or outright impossible ( e . g .
when y is a high dimensional variable and the integral has no analytical solution ) .
hence probabilistic modeling comes with a high price , and should be avoided when the application does not require
123 energy - based training : architecture and loss func -
training an ebm consists in nding an energy function that produces the best y for any x .
the search for the best energy function is performed within a family of energy functions e indexed by a parameter w
e = ( e ( w , y , x ) : w w ) .
the architecture of the ebm is the internal structure of the parameterized energy func - tion e ( w , y , x ) .
at this point , we put no particular restriction on the nature of x , y , w , and e .
when x and y are real vectors , e could be as simple as a linear com - bination of basis functions ( as in the case of kernel methods ) , or a set of neural net architectures and weight values .
section gives examples of simple architectures for common applications to classication and regression .
when x and y are variable - size images , sequences of symbols or vectors , or more complex structured objects , e may represent a considerably richer class of functions .
sections 123 , 123 and 123 discuss several examples of such architectures .
one advantage of the energy - based approach is that it puts very little restrictions on the nature of e .
to train the model for prediction , classication , or decision - making , we are given a set of training samples s = ( ( x i , y i ) : i = 123 .
p ) , where x i is the input for the i - th training sample , and y i is the corresponding desired answer .
in order to nd the best energy function in the family e , we need a way to assess the quality of any
particular energy function , based solely on two elements : the training set , and our prior knowledge about the task .
this quality measure is called the loss functional ( i . e .
a function of function ) and denoted l ( e , s ) .
for simplicity , we often denote it l ( w , s ) and simply call it the loss function .
the learning problem is simply to nd the w that minimizes the loss :
for most cases , the loss functional is dened as follows :
w = min
l ( e , s ) =
l ( y i , e ( w , y , x i ) ) + r ( w ) .
it is an average taken over the training set of a per - sample loss functional , denoted l ( y i , e ( w , y , x i ) ) , which depends on the desired answer y i and on the energies obtained by keeping the input sample xed and varying the answer y .
thus , for each sample , we evaluate a slice of the energy surface .
the term r ( w ) is the regularizer , and can be used to embed our prior knowledge about which energy functions in our family are preferable to others ( in the absence of training data ) .
with this denition , the loss is invariant under permutations of the training samples and under multiple repetitions of the training set .
naturally , the ultimate purpose of learning is to produce a model that will give good answers for new input samples that are not seen during training .
we can rely on general results from statistical learning theory which guarantee that , under simple interchangeability conditions on the samples and general conditions on the family of energy functions ( nite vc dimension ) , the deviation between the value of the loss after minimization on the training set , and the loss on a large , separate set of test samples is bounded by a quantity that converges to zero as the size of training set increases ( vapnik , 123 ) .
123 designing a loss functional
intuitively , the per - sample loss functional should be designed in such a way that it assigns a low loss to well - behaved energy functions : energy functions that give the lowest energy to the correct answer and higher energy to all other ( incorrect ) answers .
conversely , energy functions that do not assign the lowest energy to the correct answers would have a high loss .
characterizing the appropriateness of loss functions ( the ones that select the best energy functions ) is further discussed in following sections .
considering only the task of training a model to answer questions of type 123 ( pre - diction , classication and decision - making ) , the main intuition of the energy - based ap - proach is as follows .
training an ebm consists in shaping the energy function , so that for any given x , the inference algorithm will produce the desired value for y .
since the inference algorithm selects the y with the lowest energy , the learning procedure must shape the energy surface so that the desired value of y has lower energy than all other ( undesired ) values .
figures 123 and 123 show examples of energy as a function of y for a given input sample x i in cases where y is a discrete variable and a continuous scalar variable .
we note three types of answers :
figure 123 : how training affects the energies of the possible answers in the discrete case : the energy of the correct answer is decreased , and the energies of incorrect answers are increased , particularly if they are lower than that of the correct answer .
figure 123 : the effect of training on the energy surface as a function of the answer y in the con - tinuous case .
after training , the energy of the correct answer y i is lower than that of incorrect
n ( y )
y i : the correct answer
y i : the answer produced by the model , i . e .
the answer with the lowest energy .
y i : the most offending incorrect answer , i . e .
the answer that has the lowest energy among all the incorrect answers .
to dene this answer in the continuous case , we can simply view all answers within a distance of y i as correct , and all answers beyond that distance as incorrect .
with a properly designed loss function , the learning process should have the effect of pushing down on e ( w , y i , x i ) , and pulling up on the incorrect energies , par - ticularly on e ( w , y i , x i ) .
different loss functions do this in different ways .
section 123 gives sufcient conditions that the loss function must satisfy in order to be guaranteed to shape the energy surface correctly .
we show that some widely used loss functions do not satisfy the conditions , while others do .
to summarize : given a training set s , building and training an energy - based model
involves designing four components :
the architecture : the internal structure of e ( w , y , x ) .
the inference algorithm : the method for nding a value of y that minimizes
e ( w , y , x ) for any given x .
the loss function : l ( w , s ) measures the quality of an energy function using the
the learning algorithm : the method for nding a w that minimizes the loss
functional over the family of energy functions e , given the training set .
properly designing the architecture and the loss function is critical .
any prior knowl - edge we may have about the task at hand is embedded into the architecture and into the loss function ( particularly the regularizer ) .
unfortunately , not all combinations of architectures and loss functions are allowed .
with some combinations , minimizing the loss will not make the model produce the best answers .
choosing the combinations of architecture and loss functions that can learn effectively and efciently is critical to the energy - based approach , and thus is a central theme of this tutorial .
123 examples of loss functions
we now describe a number of standard loss functions that have been proposed and used in the machine learning literature .
we shall discuss them and classify them as good or bad in an energy - based setting .
for the time being , we set aside the regularization term , and concentrate on the data - dependent part of the loss function .
123 . 123 energy loss
the simplest and the most straightforward of all the loss functions is the energy loss .
for a training sample ( x i , y i ) , the per - sample loss is dened simply as :
lenergy ( y i , e ( w , y , x i ) ) = e ( w , y i , x i ) .
energy : ec / ei
figure 123 : the hinge loss ( left ) and log loss ( center ) penalize e ( w , y i , x i ) e ( w , y i , x i ) lin - early and logarithmically , respectively .
the square - square loss ( right ) separately penalizes large values of e ( w , y i , x i ) ( solid line ) and small values of e ( w , y i , x i ) ( dashed line ) quadrati -
this loss function , although very popular for things like regression and neural network training , cannot be used to train most architectures : while this loss will push down on the energy of the desired answer , it will not pull up on any other energy .
with some architectures , this can lead to a collapsed solution in which the energy is con - stant and equal to zero .
the energy loss will only work with architectures that are designed in such a way that pushing down on e ( w , y i , x i ) will automatically make the energies of the other answers larger .
a simple example of such an architecture is e ( w , y i , x i ) = ||y i g ( w , x i ) ||123 , which corresponds to regression with mean - squared error with g being the regression function .
123 . 123 generalized perceptron loss
the generalized perceptron loss for a training sample ( x i , y i ) is dened as
lperceptron ( y i , e ( w , y , x i ) ) = e ( w , y i , x i ) min
e ( w , y , x i ) .
this loss is always positive , since the second term is a lower bound on the rst term .
minimizing this loss has the effect of pushing down on e ( w , y i , x i ) , while pulling up on the energy of the answer produced by the model .
while the perceptron loss has been widely used in many settings , including for models with structured outputs such as handwriting recognition ( lecun et al . , 123a ) and parts of speech tagging ( collins , 123 ) , it has a major deciency : there is no mech - anism for creating an energy gap between the correct answer and the incorrect ones .
hence , as with the energy loss , the perceptron loss may produce at ( or almost at ) energy surfaces if the architecture allows it .
consequently , a meaningful , uncollapsed result is only guaranteed with this loss if a model is used that cannot produce a at energy surface .
for other models , one cannot guarantee anything .
123 . 123 generalized margin losses
several loss functions can be described as margin losses; the hinge loss , log loss , lvq123 loss , minimum classication error loss , square - square loss , and square - exponential loss all use some form of margin to create an energy gap between the correct answer and the
incorrect answers .
before discussing the generalized margin loss we give the following
denition 123 let y be a discrete variable .
then for a training sample ( x i , y i ) , the most offending incorrect answer y i is the answer that has the lowest energy among all answers that are incorrect :
y i = argminy yandy 123=y i e ( w , y , x i ) .
if y is a continuous variable then the denition of the most offending incorrect answer can be dened in a number of ways .
the simplest denition is as follows .
denition 123 let y be a continuous variable .
then for a training sample ( x i , y i ) , the most offending incorrect answer y i is the answer that has the lowest energy among all answers that are at least away from the correct answer :
y i = argminy y , ky y ik>e ( w , y , x i ) .
the generalized margin loss is a more robust version of the generalized perceptron loss .
it directly uses the energy of the most offending incorrect answer in the contrastive
lmargin ( w , y i , x i ) = qm ( cid : 123 ) e ( w , y i , x i ) , e ( w , y i , x i ) ( cid : 123 ) .
here m is a positive parameter called the margin and qm ( e123 , e123 ) is a convex function whose gradient has a positive dot product with the vector ( 123 , 123 ) in the region where e ( w , y i , x i ) + m > e ( w , y i , x i ) .
in other words , the loss surface is slanted toward low values of e ( w , y i , x i ) and high values of e ( w , y i , x i ) wherever e ( w , y i , x i ) is not smaller than e ( w , y i , x i ) by at least m .
two special cases of the generalized margin loss are given below :
hinge loss : a particularly popular example of generalized margin loss is the hinge loss , which is used in combination with linearly parameterized en - ergies and a quadratic regularizer in support vector machines , support vector markov models ( altun and hofmann , 123 ) , and maximum - margin markov net - works ( taskar et al . , 123 ) :
lhinge ( w , y i , x i ) = max ( cid : 123 ) 123 , m + e ( w , y i , x i ) e ( w , y i , x i ) ( cid : 123 ) ,
where m is the positive margin .
the shape of this loss function is given in figure 123
the difference between the energies of the correct answer and the most offending incorrect answer is penalized linearly when larger than m .
the hinge loss only depends on energy differences , hence individual energies are not constrained to take any particular
log loss : a common variation of the hinge loss is the log loss , which can be seen
as a soft version of the hinge loss with an innite margin ( see figure 123 , center ) :
llog ( w , y i , x i ) = log ( cid : 123 ) 123 + ee ( w , y i , x i ) e ( w , y i , x i ) ( cid : 123 ) .
lvq123 loss : one of
the very rst proposals for discriminatively train -
loss has been advocated is a version of kohonens lvq123 loss .
( driancourt et al . , 123a , driancourt and gallinari , 123b , driancourt and gallinari , 123a , driancourt , 123 , mcdermott , 123 , mcdermott and katagiri , 123 ) :
and bottou since
llvq123 ( w , y i , x i ) = min ( cid : 123 ) 123 , max ( cid : 123 ) 123 ,
e ( w , y i , x i ) e ( w , y i , x i )
e ( w , y i , x i )
where is a positive parameter .
lvq123 is a zero - margin loss , but it has the peculiarity of saturating the ratio between e ( w , y i , x i ) and e ( w , y i , x i ) to 123 + .
this mitigates the effect of outliers by making them contribute a nominal cost m to the total loss .
this loss function is a continuous approximation of the number of classication errors .
unlike generalized margin losses , the lvq123 loss is non - convex in e ( w , y i , x i ) and e ( w , y i , x i ) .
mce loss : the minimum classication error loss was originally proposed by juang et al .
in the context of discriminative training for speech recognition sys - tems ( juang et al . , 123 ) .
the motivation was to build a loss function that also ap - proximately counts the number of classication errors , while being smooth and differ - entiable .
the number of classication errors can be written as :
( cid : 123 ) e ( w , y i , x i ) e ( w , y i , x i ) ( cid : 123 ) ,
where is the step function ( equal to zero for negative arguments , and 123 for positive arguments ) .
however , this function is not differentiable , and therefore very difcult to optimize .
the mce loss softens it with a sigmoid :
lmce ( w , y i , x i ) = ( cid : 123 ) e ( w , y i , x i ) e ( w , y i , x i ) ( cid : 123 ) ,
where is the logistic function ( x ) = ( 123 + ex ) 123
as with the lvq123 loss , the satu - ration ensures that mistakes contribute a nominal cost to the overall loss .
although the mce loss does not have an explicit margin , it does create a gap between e ( w , y i , x i ) and e ( w , y i , x i ) .
the mce loss is non - convex .
square - square loss : unlike the hinge loss ,
the energy of rately ( lecun and huang , 123 , hadsell et al . , 123 ) :
the correct answer and the most offending answer
the square - square loss treats
lsqsq ( w , y i , x i ) = e ( w , y i , x i ) 123 + ( cid : 123 ) max ( 123 , m e ( w , y i , x i ) ) ( cid : 123 ) 123
large values of e ( w , y i , x i ) and small values of e ( w , y i , x i ) below the margin m are both penalized quadratically ( see figure 123 ) .
unlike the margin loss , the square - square loss pins down the correct answer energy at zero and pins down the incor - rect answer energies above m .
therefore , it is only suitable for energy functions that are bounded below by zero , notably in architectures whose output module measures some sort of distance .
chopra et al . , 123 , osadchy et al . , 123 ) : the square - exponential loss is similar to the square - square instead of a quadratic term it has the exponential of the negative energy of the most offending incorrect answer :
it only differs in the contrastive term :
( lecun and huang , 123 ,
lsqexp ( w , y i , x i ) = e ( w , y i , x i ) 123 + ee ( w , y i , x i ) ,
where is a positive constant .
unlike the square - square loss , this loss has an innite margin and pushes the energy of the incorrect answers to innity with exponentially
123 . 123 negative log - likelihood loss
the motivation for the negative log - likelihood loss comes from probabilistic modeling .
it is dened as :
lnll ( w , y i , x i ) = e ( w , y i , x i ) + f ( w , y , x i ) .
where f is the free energy of the ensemble ( e ( w , y , x i ) , y y ) :
f ( w , y , x i ) =
exp ( cid : 123 ) e ( w , y , x i ) ( cid : 123 ) ( cid : 123 ) .
where is a positive constant akin to an inverse temperature .
this loss can only be used if the exponential of the negative energy is integrable over y , which may not be the case for some choices of energy function or y .
the form of the negative log - likelihood loss stems from a probabilistic formulation of the learning problem in terms of the maximum conditional probability principle .
given the training set s , we must nd the value of the parameter that maximizes the conditional probability of all the answers given all the inputs in the training set .
assum - ing that the samples are independent , and denoting by p ( y i|x i , w ) the conditional probability of y i given x i that is produced by our model with parameter w , the condi - tional probability of the training set under the model is a simple product over samples :
p ( y 123 , .
, y p |x 123 , .
, x p , w ) =
p ( y i|x i , w ) .
applying the maximum likelihood estimation principle , we seek the value of w that maximizes the above product , or the one that minimizes the negative log of the above
p ( y i|x i , w ) =
log p ( y i|x i , w ) .
using the gibbs distribution ( equation 123 ) , we get :
p ( y i|x i , w ) =
e ( w , y i , x i ) + logzyy
the nal form of the negative log - likelihood loss is obtained by dividing the above expression by p and ( which has no effect on the position of the minimum ) :
lnll ( w , s ) =
xi=123 ( cid : 123 ) e ( w , y i , x i ) +
ee ( w , y , x i ) ( cid : 123 ) .
while many of the previous loss functions involved only e ( w , y i , x i ) in their con - trastive term , the negative log - likelihood loss combines all the energies for all val - ues of y in its contrastive term f ( w , y , x i ) .
this term can be interpreted as the helmholtz free energy ( log partition function ) of the ensemble of systems with ener - gies e ( w , y , x i ) , y y .
this contrastive term causes the energies of all the answers to be pulled up .
the energy of the correct answer is also pulled up , but not as hard as it is pushed down by the rst term .
this can be seen in the expression of the gradient for a single sample :
lnll ( w , y i , x i )
e ( w , y i , x i )
e ( w , y , x i )
where p ( y |x i , w ) is obtained through the gibbs distribution :
p ( y |x i , w ) ,
p ( y |x i , w ) =
ryy ee ( w , y , x i ) .
hence , the contrastive term pulls up on the energy of each answer with a force propor - tional to the likelihood of that answer under the model .
unfortunately , there are many interesting models for which computing the integral over y is intractable .
evaluating this integral is a major topic of research .
considerable efforts have been devoted to ap - proximation methods , including clever organization of the calculations , monte - carlo sampling methods , and variational methods .
while these methods have been devised as approximate ways of minimizing the nll loss , they can be viewed in the energy - based framework as different strategies for choosing the y s whose energies will be pulled
interestingly , the nll loss reduces to the generalized perceptron loss when ( zero temperature ) , and reduces to the log loss ( eq .
123 ) when y has two elements ( e . g .
it was also used by bengio et al .
the nll loss has been used extensively by many authors under various in the neural network classication literature , it is known as the cross - to train an entropy loss ( solla et al . , 123 ) .
energy - based language model ( bengio et al . , 123 ) .
it has been widely used un - der the name maximum mutual information estimation for discriminatively train - ing speech recognition systems since the late 123s , including hidden markov models with mixtures of gaussians ( bahl et al . , 123 ) , and hmm - neural net hy - brids ( bengio et al . , 123 , bengio et al . , 123 , haffner , 123 , bengio , 123 ) .
also been used extensively for global discriminative training of handwriting recog - nition systems that integrate neural nets and hidden markov models under the names maximum mutual information ( bengio et al . , 123 , lecun and bengio , 123 , bengio et al . , 123 , lecun et al . , 123 , bottou et al . , 123 ) and discriminative for - ward training ( lecun et al . , 123a ) .
finally , it is the loss function of choice for train - ing other probabilistic discriminative sequence labeling models such as input / output hmm ( bengio and frasconi , 123 ) , conditional random elds ( lafferty et al . , 123 ) , and discriminative random elds ( kumar and hebert , 123 ) .
minimum empirical error loss : some authors have argued that the negative log likelihood loss puts too much emphasis on mistakes : eq .
123 is a product whose value
is dominated by its smallest term .
hence , ljolje et al .
( ljolje et al . , 123 ) proposed the minimum empirical error loss , which combines the conditional probabilities of the samples additively instead of multiplicatively :
lmee ( w , y i , x i ) = 123 p ( y i|x i , w ) .
substituting equation 123 we get :
lmee ( w , y i , x i ) = 123
ee ( w , y i , x i )
ryy ee ( w , y , x i ) .
as with the mce loss and the lvq123 loss , the mee loss saturates the contribution of any single error .
this makes the system more robust to label noise and outliers , which is of particular importance to such applications such as speech recognition , but it makes the loss non - convex .
as with the nll loss , mee requires evaluating the
123 simple architectures
to substantiate the ideas presented thus far , this section demonstrates how simple mod - els of classication and regression can be formulated as energy - based models .
this sets the stage for the discussion of good and bad loss functions , as well as for the discussion of advanced architectures for structured prediction .
e ( w , y , x )
e ( w , y , x )
e ( w , y , x ) =
d ( gw ( x ) , y )
y gw ( x )
figure 123 : simple learning models viewed as ebms : ( a ) a regressor : the energy is the dis - crepancy between the output of the regression function gw ( x ) and the answer y .
the best inference is simply y = gw ( x ) ; ( b ) a simple two - class classier : the set of possible an - swers is ( 123 , +123 ) .
the best inference is y = sign ( gw ( x ) ) ; ( c ) a multiclass classier : the discriminant function produces one value for each of the three categories .
the answer , which can take three values , controls the position of a switch , which connects one output of the dis - criminant function to the energy function .
the best inference is the index of the smallest output component of gw ( x ) .
figure 123 ( a ) shows a simple architecture for regression or function approximation .
the energy function is the squared error between the output of a regression function gw ( x ) and the variable to be predicted y , which may be a scalar or a vector :
e ( w , y , x ) =
||gw ( x ) y ||123
the inference problem is trivial : the value of y that minimizes e is equal to gw ( x ) .
the minimum energy is always equal to zero .
when used with this architecture , the energy loss , perceptron loss , and negative log - likelihood loss are all equivalent because the contrastive term of the perceptron loss is zero , and that of the nll loss is constant ( it is a gaussian integral with a constant variance ) :
lenergy ( w , s ) =
e ( w , y i , x i ) =
||gw ( x i ) y i||123
this corresponds to standard regression with mean - squared error .
a popular form of regression occurs when g is a linear function of the parameters :
gw ( x ) =
wkk ( x ) = w t ( x ) .
the k ( x ) are a set of n features , and wk are the components of an n - dimensional parameter vector w .
for concision , we use the vector notation w t ( x ) , where w t denotes the transpose of w , and ( x ) denotes the vector formed by each k ( x ) .
with this linear parameterization , training with the energy loss reduces to an easily solvable least - squares minimization problem , which is convex :
w = argminw " 123
||w t ( x i ) y i||123# .
in simple models , the feature functions are hand - crafted by the designer , or separately trained from unlabeled data .
in the dual form of kernel methods , they are dened as k ( x ) = k ( x , x k ) , k = 123 .
p , where k is the kernel function .
in more complex models such as multilayer neural networks and others , the s may themselves be pa - rameterized and subject to learning , in which case the regression function is no longer a linear function of the parameters and hence the loss function may not be convex in
123 two - class classier
figure 123 ( b ) shows a simple two - class classier architecture .
the variable to be pre - dicted is binary : y = ( 123 , +123 ) .
the energy function can be dened as :
e ( w , y , x ) = y gw ( x ) ,
where gw ( x ) is a scalar - valued discriminant function parameterized by w .
inference
y = argminy ( 123 , 123 ) y gw ( x ) = sign ( gw ( x ) ) .
learning can be done using a number of different loss functions , which include the perceptron loss , hinge loss , and negative log - likelihood loss .
substituting equations 123 and 123 into the perceptron loss ( eq .
123 ) , we get :
lperceptron ( w , s ) =
xi=123 ( cid : 123 ) sign ( gw ( x i ) ) y i ( cid : 123 ) gw ( x i ) .
the stochastic gradient descent update rule to minimize this loss is :
w w + ( cid : 123 ) y i sign ( gw ( x i ) ( cid : 123 )
gw ( x i )
where is a positive step size .
if we choose gw ( x ) in the family of linear models , the energy function becomes e ( w , y , x ) = y w t ( x ) and the perceptron loss
lperceptron ( w , s ) =
xi=123 ( cid : 123 ) sign ( w t ( x i ) ) y i ( cid : 123 ) w t ( x i ) ,
and the stochastic gradient descent update rule becomes the familiar perceptron learn -
ing rule : w w + ( cid : 123 ) y i sign ( w t ( x i ) ) ( cid : 123 ) ( x i ) .
the hinge loss ( eq .
123 ) with the two - class classier energy ( eq .
123 ) yields :
lhinge ( w , s ) =
max ( 123 , m + 123y igw ( x i ) ) .
using this loss with gw ( x ) = w t x and a regularizer of the form ||w ||123 gives the familiar linear support vector machine .
the negative log - likelihood loss ( eq .
123 ) with equation 123 yields :
lnll ( w , s ) =
xi=123hy igw ( x i ) + log ( cid : 123 ) ey igw ( x i ) + ey igw ( x i ) ( cid : 123 ) i .
using the fact that y = ( 123 , +123 ) , we obtain :
lnll ( w , s ) =
log ( cid : 123 ) 123 + e123y igw ( x i ) ( cid : 123 ) ,
which is equivalent to the log loss ( eq .
using a linear model as described above , the loss function becomes :
lnll ( w , s ) =
log ( cid : 123 ) 123 + e123y iw t ( x i ) ( cid : 123 ) .
this particular combination of architecture and loss is the familiar logistic regression
123 multiclass classier
figure 123 ( c ) shows an example of architecture for multiclass classication for 123 classes .
a discriminant function gw ( x ) produces an output vector ( g123 , g123 , .
, gc ) with one component for each of the c categories .
each component gj can be interpreted as a penalty for assigning x to the jth category .
a discrete switch module selects which of the components is connected to the output energy .
the position of the switch is controlled by the discrete variable y ( 123 , 123 , .
, c ) , which is interpreted as the j=123 ( y j ) gj , where ( y j ) is the kronecker delta function : ( u ) = 123 for u = 123; ( u ) = 123 otherwise .
inference consists in setting y to the index of the smallest component of gw ( x ) .
category .
the output energy is equal to e ( w , y , x ) = pc
the perceptron loss , hinge loss , and negative log - likelihood loss can be directly
translated to the multiclass case .
123 implicit regression
e ( w , y , x )
||g123w123 ( x ) g123w123 ( y ) ||123
g123w123 ( y )
figure 123 : the implicit regression architecture .
x and y are passed through two functions .
this architecture allows multiple values of y to have low energies for a given
the architectures described in the previous section are simple functions of y with a single minimum within the set y .
however , there are tasks for which multiple answers are equally good .
examples include robot navigation , where turning left or right may get around an obstacle equally well , or a language model in which the sentence segment the cat ate the can be followed equally well by mouse or bird .
more generally , the dependency between x and y sometimes cannot be expressed as a function that maps x to y ( e . g . , consider the constraint x 123+y 123 = 123 ) .
in this case , which we call implicit regression , we model the constraint that x and y must satisfy and design the energy function such that it measures the violation of the constraint .
both x and y can be passed through functions , and the energy is a function of their outputs .
a simple example is :
e ( w , y , x ) =
||gx ( wx , x ) gy ( wy , y ) ||123
for some problems , the function gx must be different from the function gy .
in other cases , gx and gy must be instances of the same function g .
an interesting example is the siamese architecture ( bromley et al . , 123 ) : variables x123 and x123 are passed through two instances of a function gw .
a binary label y determines the con - straint on gw ( x123 ) and gw ( x123 ) : if y = 123 , gw ( x123 ) and gw ( x123 ) should be equal , and if y = 123 , gw ( x123 ) and gw ( x123 ) should be different .
in this way , the regres - sion on x123 and x123 is implicitly learned through the constraint y rather than explicitly learned through supervision .
siamese architectures are used to learn similarity metrics with labeled examples .
when two input samples x123 and x123 are known to be similar ( e . g .
two pictures of the same person ) , y = 123; when they are different , y = 123
siamese architectures were originally designed for signature verication ( bromley et al . , 123 ) .
more recently they have been used with the square - exponential loss ( eq .
123 ) to learn a similarity metric with application to face recognition ( chopra et al . , 123 ) .
they have also been used with the square - square loss ( eq .
123 ) for unsupervised learning of mani - folds ( hadsell et al . , 123 ) .
in other applications , a single non - linear function combines x and y .
an example of such architecture is the trainable language model of bengio et al ( bengio et al . , 123 ) .
under this model , the input x is a sequence of a several successive words in a text , and the answer y is the the next word in the text .
since many different words can follow a particular word sequence , the architecture must allow multiple values of y to have low energy .
the authors used a multilayer neural net as the function g ( w , x , y ) , and chose to train it with the negative log - likelihood loss .
because of the high cardinal - ity of y ( equal to the size of the english dictionary ) , they had to use approximations ( importance sampling ) and had to train the system on a cluster machine .
the current section often referred to architectures in which the energy was linear or quadratic in w , and the loss function was convex in w , but it is important to keep in mind that much of the discussion applies equally well to more complex architectures , as we will see later .
123 latent variable architectures
energy minimization is a convenient way to represent the general process of reasoning and inference .
in the usual scenario , the energy is minimized with respect to the vari - ables to be predicted y , given the observed variables x .
during training , the correct value of y is given for each training sample .
however there are numerous applications where it is convenient to use energy functions that depend on a set of hidden variables z whose correct value is never ( or rarely ) given to us , even during training .
for ex - ample , we could imagine training the face detection system depicted in figure 123 ( b ) with data for which the scale and pose information of the faces is not available .
for these architectures , the inference process for a given set of variables x and y involves minimizing over these unseen variables z :
e ( y , x ) = min
e ( z , y , x ) .
such hidden variables are called latent variables , by analogy with a similar concept in probabilistic modeling .
the fact that the evaluation of e ( y , x ) involves a minimiza -
tion over z does not signicantly impact the approach described so far , but the use of latent variables is so ubiquitous that it deserves special treatment .
in particular , some insight can be gained by viewing the inference process in the
presence of latent variables as a simultaneous minimization over y and z :
y = argminy y , zz e ( z , y , x ) .
latent variables can be viewed as intermediate results on the way to nding the best output y .
at this point , one could argue that there is no conceptual difference between the z and y variables : z could simply be folded into y .
the distinction arises during training : we are given the correct value of y for a number of training samples , but we are never given the correct value of z .
latent variables are very useful in situations where a hidden characteristic of the process being modeled can be inferred from observations , but cannot be predicted di - rectly .
one such example is in recognition problems .
for example , in face recognition the gender of a person or the orientation of the face could be a latent variable .
knowing these values would make the recognition task much easier .
likewise in invariant object recognition the pose parameters of the object ( location , orientation , scale ) or the illumi - nation could be latent variables .
they play a crucial role in problems where segmenta - tion of the sequential data must be performed simultaneously with the recognition task .
a good example is speech recognition , in which the segmentation of sentences into words and words into phonemes must take place simultaneously with recognition , yet the correct segmentation into phonemes is rarely available during training .
similarly , in handwriting recognition , the segmentation of words into characters should take place simultaneously with the recognition .
the use of latent variables in face recognition is discussed in this section , and section 123 describes a latent variable architecture for
123 an example of latent variable architecture
to illustrate the concept of latent variables , we consider the task of face detection , beginning with the simple problem of determining whether a face is present or not in a small image .
imagine that we are provided with a face detecting function gface ( x ) which takes a small image window as input and produces a scalar output .
it outputs a small value when a human face lls the input image , and a large value if no face is present ( or if only a piece of a face or a tiny face is present ) .
an energy - based face detector built around this function is shown in figure 123 ( a ) .
the variable y controls the position of a binary switch ( 123 = face , 123 = non - face ) .
the output energy is equal to gface ( x ) when y = 123 , and to a xed threshold value t when y = 123 :
e ( y , x ) = y gface ( x ) + ( 123 y ) t .
the value of y that minimizes this energy function is 123 ( face ) if gface ( x ) < t and 123
let us now consider the more complex task of detecting and locating a single face in a large image .
we can apply our gface ( x ) function to multiple windows in the large image , compute which window produces the lowest value of gface ( x ) , and detect a
e ( w , y , x )
e ( w , z , y , x )
gf ace ( x ) gf ace ( x ) gf ace ( x )
figure 123 : ( a ) : architecture of an energy - based face detector .
given an image , it outputs a small value when the image is lled with a human face , and a high value equal to the threshold t when there is no face in the image .
( b ) : architecture of an energy - based face detector that simultaneously locates and detects a face in an input image by using the location of the face as a latent variable .
face at that location if the value is lower than t .
this process is implemented by the energy - based architecture shown in figure 123 ( b ) .
the latent location variable z selects which of the k copies of the gface function is routed to the output energy .
the energy function can be written as
e ( z , y , x ) = y " k
( z k ) gface ( xk ) # + ( 123 y ) t ,
where the xks are the image windows .
locating the best - scoring location in the image consists in minimizing the energy with respect to y and z .
the resulting value of y will indicate whether a face was found , and the resulting value of z will indicate the
123 probabilistic latent variables
when the best value of the latent variable for a given x and y is ambiguous , one may consider combining the contributions of the various possible values by marginalizing over the latent variables instead of minimizing with respect to those variables .
when latent variables are present , the joint conditional distribution over y and z
given by the gibbs distribution is :
p ( z , y |x ) =
marginalizing over z gives :
ryy , zz ee ( y , z , x ) .
p ( y |x ) = rzz ee ( z , y , x ) ryy , zz ee ( y , z , x ) .
finding the best y after marginalizing over z reduces to :
y = argminy y
this is actually a conventional energy - based inference in which the energy function has merely been redened from e ( z , y , x ) to f ( z ) = 123 is the free energy of the ensemble ( e ( z , y , x ) , z z ) .
the above inference formula by marginalization reduces to the previous inference formula by minimization when
logrzz ee ( z , y , x ) , which
123 analysis of loss functions for energy - based models
this section discusses the conditions that a loss function must satisfy so that its mini - mization will result in a model that produces the correct answers .
to give an intuition of the problem , we rst describe simple experiments in which certain combinations of architectures and loss functions are used to learn a simple dataset , with varying results .
a more formal treatment follows in section 123 .
123 good and bad loss functions
consider the problem of learning a function that computes the square of a number : y = f ( x ) , where f ( x ) = x 123
though this is a trivial problem for a learning machine , it is useful for demonstrating the issues involved in the design of an energy function and loss function that work together .
for the following experiments , we use a training set of 123 samples ( x i , y i ) where y i = x i123 , randomly sampled with a uniform distribution between 123 and +123
first , we use the architecture shown in figure 123 ( a ) .
the input x is passed through a parametric function gw , which produces a scalar output .
the output is compared with the desired answer using the absolute value of the difference ( l123 norm ) :
e ( w , y , x ) = ||gw ( x ) y ||123
any reasonable parameterized family of functions could be used for gw .
for these experiments , we chose a two - layer neural network with 123 input unit , 123 hidden units ( with sigmoids ) and 123 output unit .
figure 123 ( a ) shows the initial shape of the energy
e ( w , y , x )
e ( w , y , x )
||gw ( x ) y ||123
||g123w123 ( x ) g123w123 ( y ) ||123
g123w123 ( y )
figure 123 : ( a ) : a simple architecture that can be trained with the energy loss .
( b ) : an implicit regression architecture where x and y are passed through functions g123w123 tively .
training this architecture with the energy loss causes a collapse ( a at energy surface ) .
a loss function with a contrastive term corrects the problem .
function in the space of the variables x and y , using a set of random initial parameters w .
the dark spheres mark the location of a few training samples .
first , the simple architecture is trained with the energy loss ( eq
lenergy ( w , s ) =
e ( w , y i , x i ) =
||gw ( x ) y ||123
this corresponds to a classical form of robust regression .
the learning process can be viewed as pulling down on the energy surface at the location of the training samples ( the spheres in figure 123 ) , without considering the rest of the points on the energy surface .
the energy surface as a function of y for any x has the shape of a v with xed slopes .
by changing the function gw ( x ) , the apex of that v can move around for different x i .
the loss is minimized by placing the apex of the v at the position y = x 123 for any value of x , and this has the effect of making the energies of all other answers larger , because the v has a single minimum .
figure 123 shows the shape of the energy surface at xed intervals during training with simple stochastic gradient descent .
the energy surface takes the proper shape after a few iterations through the training set .
using more sophisticated loss functions such as the nll loss or the perceptron loss would produce exactly the same result as the energy loss because , with this simple architecture , their contrastive term is constant .
consider a slightly more complicated architecture , shown in figure 123 ( b ) , to learn and y is the same dataset .
in this architecture x is passed through function g123w123 .
for the experiment , both functions were two - layer passed through function g123w123 neural networks with 123 input unit , 123 hidden units and 123 output units .
the energy is
figure 123 : the shape of the energy surface at four intervals while training the system in fig - ure 123 ( a ) with stochastic gradient descent to minimize the energy loss .
the x axis is the input , and the y axis the output .
the energy surface is shown ( a ) at the start of training , ( b ) after 123 epochs through the training set , ( c ) after 123 epochs , and ( d ) after 123 epochs .
the energy surface has attained the desired shape where the energy around training samples ( dark spheres ) is low and energy at all other points is high .
the l123 norm of the difference between their 123 - dimensional outputs :
e ( w , x , y ) = ||g123w123 ( x ) g123w123 ( y ) ||123 ,
where w = ( w123w123 ) .
training this architecture with the energy loss results in a col - lapse of the energy surface .
figure 123 shows the shape of the energy surface during training; the energy surface becomes essentially at .
what has happened ? the shape of the energy as a function of y for a given x is no longer xed .
with the energy loss , there is no mechanism to prevent g123 and g123 from ignoring their inputs and producing identical output values .
this results in the collapsed solution : the energy surface is at and equal to zero everywhere .
figure 123 : the shape of the energy surface at four intervals while training the system in fig - ure 123 ( b ) using the energy loss .
along the x axis is the input variable and along the y axis is the answer .
the shape of the surface ( a ) at the start of the training , ( b ) after 123 epochs through the training set , ( c ) after 123 epochs , and ( d ) after 123 epochs .
clearly the energy is collapsing to a at
now consider the same architecture , but trained with the square - square loss :
l ( w , y i , x i ) = e ( w , y i , x i ) 123 ( cid : 123 ) max ( 123 , m e ( w , y i , x i ) ) ( cid : 123 ) 123
here m is a positive margin , and y i is the most offending incorrect answer .
the second term in the loss explicitly prevents the collapse of the energy by pushing up on points
whose energy threatens to go below that of the desired answer .
figure 123 shows the shape of the energy function during training; the surface successfully attains the desired
figure 123 : the shape of the energy surface at four intervals while training the system in fig - ure 123 ( b ) using square - square loss .
along the x - axis is the variable x and along the y - axis is the variable y .
the shape of the surface at ( a ) the start of the training , ( b ) after 123 epochs over the training set , ( c ) after 123 epochs , and ( d ) after 123 epochs .
the energy surface has attained the desired shape : the energies around the training samples are low and energies at all other points
figure 123 : the shape of the energy surface at four intervals while training the system in fig - ure 123 ( b ) using the negative log - likelihood loss .
along the x axis is the input variable and along the y axis is the answer .
the shape of the surface at ( a ) the start of training , ( b ) after 123 epochs over the training set , ( c ) after 123 epochs , and ( d ) after 123 epochs .
the energy surface has quickly attained the desired shape .
another loss function that works well with this architecture is the negative log -
l ( w , y i , x i ) = e ( w , y i , x i ) +
ee ( w , y , x i ) ( cid : 123 ) .
the rst term pulls down on the energy of the desired answer , while the second term pushes up on all answers , particularly those that have the lowest energy .
note that the energy corresponding to the desired answer also appears in the second term .
the shape of the energy function at various intervals using the negative log - likelihood loss is shown in figure 123
the learning is much faster than the square - square loss .
the minimum is deeper because , unlike with the square - square loss , the energies of the in - correct answers are pushed up to innity ( although with a decreasing force ) .
however ,
each iteration of negative log - likelihood loss involves considerably more work because pushing up every incorrect answer is computationally expensive when no analytical expression for the derivative of the second term exists .
in this experiment , a simple sampling method was used : the integral is approximated by a sum of 123 points regu - larly spaced between - 123 and +123 in the y direction .
each learning iteration thus requires computing the gradient of the energy at 123 locations , versus 123 locations in the case of the square - square loss .
however , the cost of locating the most offending incorrect answer must be taken into account for the square - square loss .
an important aspect of the nll loss is that it is invariant to global shifts of energy values , and only depends on differences between the energies of the y s for a given x .
hence , the desired answer may have different energies for different x , and may not be zero .
this has an important consequence : the quality of an answer cannot be measured by the energy of that answer without considering the energies of all other answers .
in this section we have seen the results of training four combinations of architec - tures and loss functions .
in the rst case we used a simple architecture along with a simple energy loss , which was satisfactory .
the constraints in the architecture of the system automatically lead to the increase in energy of undesired answers while de - creasing the energies of the desired answers .
in the second case , a more complicated architecture was used with the simple energy loss and the machine collapsed for lack of a contrastive term in the loss .
in the third and the fourth case the same architecture was used as in the second case but with loss functions containing explicit contrastive terms .
in these cases the machine performed as expected and did not collapse .
123 sufcient conditions for good loss functions
in the previous section we offered some intuitions about which loss functions are good and which ones are bad with the help of illustrative experiments .
in this section a more formal treatment of the topic is given .
first , a set of sufcient conditions are stated .
the energy function and the loss function must satisfy these conditions in order to be guaranteed to work in an energy - based setting .
then we discuss the quality of the loss functions introduced previously from the point of view of these conditions .
123 conditions on the energy
generally in energy - based learning , the inference method chooses the answer with minimum energy .
thus the condition for the correct inference on a sample ( x i , y i ) is
condition 123 for sample ( x i , y i ) , the machine will give the correct answer for x i if
e ( w , y i , x i ) < e ( x , y , x i ) , y y and y 123= y i .
in other words , the inference algorithm will give the correct answer if the energy of the desired answer y i is less than the energies of all the other answers y .
to ensure that the correct answer is robustly stable , we may choose to impose that its energy be lower than energies of incorrect answers by a positive margin m .
if y i denotes the most offending incorrect answer , then the condition for the answer to be correct by a margin m is as follows .
condition 123 for a variable y and sample ( x i , y i ) and positive margin m , the infer - ence algorithm will give the correct answer for x i if
e ( w , y i , x i ) < e ( w , y i , x i ) m .
123 sufcient conditions on the loss functional
if the system is to produce the correct answers , the loss functional should be designed in such a way that minimizing it will cause e ( w , y i , x i ) to be lower than e ( w , y i , x i ) by some margin m .
since only the relative values of those two energies matter , we only need to consider the shape of a slice of the loss functional in the 123d space of those two energies .
for example , in the case where y is the set of integers from 123 to k , the loss functional can be written as :
l ( w , y i , x i ) = l ( y i , e ( w , 123 , x i ) , .
, e ( w , k , x i ) ) .
the projection of this loss in the space of e ( w , y i , x i ) and e ( w , y i , x i ) can be viewed as a function q parameterized by the other k 123 energies :
l ( w , y i , x i ) = q ( ey ) ( e ( w , y i , x i ) , e ( w , y i , x i ) ) ,
where the parameter ( ey ) contains the vector of energies for all values of y except y i and y i .
we assume the existence of at least one set of parameters w for which condition 123 is satised for a single training sample ( x i , y i ) .
clearly , if such a w does not exist , there cannot exist any loss function whose minimization would lead to condition 123
for the purpose of notational simplicity let us denote the energy e ( w , y i , x i ) associated with the training sample ( x i , y i ) by ec ( as in correct energy ) and e ( w , y i , x i ) by ei ( as in incorrect energy ) .
consider the plane formed by ec and ei .
as an illustration , figure 123 ( a ) shows a 123 - dimensional plot of the square - square loss function in which the abscissa is ec and the ordinate is ei .
the third axis gives the value of the loss for the corresponding values of ec and ei .
in general , the loss function is a family of 123d surfaces in this 123d space , where each surface corresponds to one particular conguration of all the energies except ec and ei .
the solid red line in the gure corresponds to the points in the 123d plane for which ec = ei .
the dashed blue line correspond to the margin line ec +m = ei .
let the two half planes ec +m < ei and ec + m ei be denoted by hp123 and hp123 respectively .
let r be the feasible region , dened as the set of values ( ec , ei ) corresponding to all possible values of w w .
this region may be non - convex , discontinuous , open , or one - dimensional and could lie anywhere in the plane .
it is shown shaded in
+ m = e
figure 123 : figure showing the various regions in the plane of the two energies ec and ei .
ec are the ( correct answer ) energies associated with ( x i , y i ) , and ei are the ( incorrect answer ) energies associated with ( x i , y i ) .
figure 123
as a consequence of our assumption that a solution exists which satises conditions 123 , r must intersect the half plane hp123
let two points ( e123 , e123 ) and ( e
( e123 , e123 ) hp123 ( that is , e123 + m < e123 ) and ( e 123 ) hp123 ( that is , e are now ready to present the sufcient conditions on the loss function .
123 ) belong to the feasible region r , such that
123 + m e
condition 123 let ( x i , y i ) be the ith training example and m be a positive margin .
minimizing the loss function l will satisfy conditions 123 or 123 if there exists at least one point ( e123 , e123 ) with e123 + m < e123 such that for all points ( e
123 + m e
123 ) with e
q ( ey ) ( e123 , e123 ) < q ( ey ) ( e
where q ( ey ) is given by
l ( w , y i , x i ) = q ( ey ) ( e ( w , y i , x i ) , e ( w , y i , x i ) ) .
in other words , the surface of the loss function in the space of ec and ei should be such that there exists at least one point in the part of the feasible region r intersecting the half plane hp123 such that the value of the loss function at this point is less than its value at all other points in the part of r intersecting the half plane hp123
note that this is only a sufcient condition and not a necessary condition .
there may be loss functions that do not satisfy this condition but whose minimization still satises condition 123
table 123 : a list of loss functions , together with the margin which allows them to satisfy con - dition 123
a margin > 123 indicates that the loss satises the condition for any strictly positive margin , and none indicates that the loss does not satisfy the condition .
loss ( equation # )
energy loss ( 123 )
e ( w , y i , x i )
e ( w , y i , x i ) miny y e ( w , y , x i )
max ( cid : 123 ) 123 , m + e ( w , y i , x i ) e ( w , y i , x i ) ( cid : 123 ) log ( cid : 123 ) 123 + ee ( w , y i , x i ) e ( w , y i , x i ) ( cid : 123 ) min ( cid : 123 ) m , max ( 123 , e ( w , y i , x i ) e ( w , y i , x i ) ( cid : 123 ) ( cid : 123 ) 123 + e ( e ( w , y i , x i ) e ( w , y i , x i ) ) ( cid : 123 ) 123 square - square ( 123 ) e ( w , y i , x i ) 123 ( cid : 123 ) max ( 123 , m e ( w , y i , x i ) ) ( cid : 123 ) 123
e ( w , y i , x i ) 123 + ee ( w , y i , x i ) e ( w , y i , x i ) + 123
logryy ee ( w , y , x i ) 123 ee ( w , y i , x i ) / ryy ee ( w , y , x i )
123 which loss functions are good or bad
table 123 lists several loss functions , together with the value of the margin with which they satisfy condition 123
the energy loss is marked none because it does not satisfy condition 123 for a general architecture .
the perceptron loss and the lvq123 loss satisfy it with a margin of zero .
all others satisfy condition 123 with a strictly positive value of
123 . 123 energy loss
the energy loss is a bad loss function in general , but there are certain forms of energies for which it is a good loss function .
for example consider an energy function of the
e ( w , y i , x i ) =
( y i k ) ||u k gw ( x i ) ||123
this energy passes the output of the function gw through k radial basis functions ( one corresponding to each class ) whose centers are the vectors u k .
if the centers u k are xed and distinct then the energy loss satises condition 123 and hence is a good loss
to see this , consider the two - class classication case ( the reasoning for k > 123
follows along the same lines ) .
the architecture of the system is shown in figure 123
e ( w , y , x ) =
( y k ) ||u k
di = ||u i
figure 123 : the architecture of a system where two rbf units with centers u 123 and u 123 are placed on top of the machine gw , to produce distances d123 and d123
figure 123 : ( a ) : when using the rbf architecture with xed and distinct rbf centers , only the shaded region of the ( ec , ei ) plane is allowed .
the non - shaded region is unattainable because the energies of the two outputs cannot be small at the same time .
the minimum of the energy loss is at the intersection of the shaded region and vertical axis .
( b ) : the 123 - dimensional plot of the energy loss when using the rbf architecture with xed and distinct centers .
lighter shades indicate higher loss values and darker shades indicate lower values .
let d = ||u 123u 123||123 , d123 = ||u 123gw ( x i ) ||123 , and d123 = ||u 123gw ( x i ) ||123
since u 123 and u 123 are xed and distinct , there is a strictly positive lower bound on d123 + d123 for all gw .
being only a two - class problem , ec and ei correspond directly to the energies of the two classes .
in the ( ec , ei ) plane no part of the loss function exists in where ec + ei d .
the region where the loss function is dened is shaded in figure 123 ( a ) .
the exact shape of the loss function is shown in figure 123 ( b ) .
one can see from the gure that as long as d m , the loss function satises condition 123
we conclude that this is a good loss function .
however , when the rbf centers u 123 and u 123 are not xed and are allowed to be learned , then there is no guarantee that d123 + d123 d .
then the rbf centers could become equal and the energy could become zero for all inputs , resulting in a collapsed energy surface .
such a situation can be avoided by having a contrastive term in the loss
123 . 123 generalized perceptron loss
the generalized perceptron loss has a margin of zero .
therefore , it could lead to a col - lapsed energy surface and is not generally suitable for training energy - based models .
however , the absence of a margin is not always fatal ( lecun et al . , 123a , collins , 123 ) .
first , the set of collapsed solutions is a small piece of the parameter space .
second , although nothing prevents the system from reaching the collapsed solutions , nothing drives the system toward them either .
thus the probability of hitting a collapsed solu - tion is quite small .
123 . 123 generalized margin loss
+ m = e
+ m = e
figure 123 : ( a ) the square - square loss in the space of energies ec and ei ) .
the value of the loss monotonically decreases as we move from hp123 into hp123 , indicating that it satises condition 123
( b ) the square - exponential loss in the space of energies ec and ei ) .
the value of the loss monotonically decreases as we move from hp123 into hp123 , indicating that it satises
we now consider the square - square and square - exponential losses .
for the two - class case , the shape of the surface of the losses in the space of ec and ei is shown in figure 123
one can clearly see that there exists at least one point ( e123 , e123 ) in hp123 such
q ( ey ) ( e123 , e123 ) < q ( ey ) ( e
for all points ( e
123 ) in hp123
these loss functions satisfy condition 123
123 . 123 negative log - likelihood loss
it is not obvious that the negative log - likelihood loss satises condition 123
the proof
c , e*
b = ( e*
c + m + )
ec + m = ei
a = ( e*
c + m )
g = gc + gi
ec = ei
figure 123 : figure showing the direction of gradient of the negative log - likelihood loss in the feasible region r in the space dened by the two energies ec and ei .
for any xed parameter w and a sample ( x i , y i ) consider the gradient of the loss with respect to the energy ec of the correct answer y i and the energy ei of the most offending incorrect answer y i .
we have
l ( w , y i , x i )
l ( w , y i , x i )
ee ( w , y i , x i )
py y ee ( w , y , x i ) , py y ee ( w , y , x i ) .
ee ( w , y i , x i )
clearly , for any value of the energies , gc > 123 and gi < 123
the overall direction of the gradient at any point in the space of ec and ei is shown in figure 123
one can conclude that when going from hp123 to hp123 , the loss decreases monotonically .
now we need to show that there exists at least one point in hp123 at which the loss c + m ) be a point on the margin
is less than at all the points in hp123
let a = ( e
line for which the loss is minimum
c is the value of the correct energy at this point .
c = argmin ( q ( ey ) ( ec , ec + m ) ) .
since from the above discussion , the negative of the gradient of the loss q ( ey ) at all points ( and in particular on the margin line ) is in the direction which is inside hp123 , by monotonicity of the loss we can conclude that
c + m ) q ( ey ) ( ec , ei ) ,
where ec + m > ei .
consider a point b at a distance away from the point ( e
c + m ) , and inside
hp123 ( see figure 123 ) .
that is the point
c , e
c + m + ) .
using the rst order taylors expansion on the value of the loss at this point , we get
c , e
c + m + )
c + m )
c + m ) + ( cid : 123 ) q ( ey )
from the previous discussion the second term on the right hand side is negative .
so for sufciently small we have
c , e
c + m + ) < q ( ey ) ( e
c + m ) .
thus we conclude that there exists at least one point in hp123 at which the loss is less than at all points in hp123
note that the energy of the most offending incorrect answer ei is bounded above by the value of the energy of the next most offending incorrect answer .
thus we only need to consider a nite range of eis and the point b cannot be at innity .
123 efcient inference : non - probabilistic factor graphs
this section addresses the important issue of efcient energy - based inference .
se - quence labeling problems and other learning problem with structured outputs can often be modeled using energy functions whose structure can be exploited for efcient infer -
learning and inference with ebms involves a minimization of the energy over the set of answers y and latent variables z .
when the cardinality of y z is large , this minimization can become intractable .
one approach to the problem is to exploit the structure of the energy function in order to perform the minimization efciently .
one case where the structure can be exploited occurs when the energy can be expressed as a
e ( y , z , x )
eb ( x , z123 , z123 )
eb ( x , 123 , 123 )
a ( x , 123 )
b ( x , 123 , 123 )
d ( 123 , 123 )
eb ( x , 123 , 123 )
figure 123 : top : a log domain factor graph .
the energy is a sum of factors that take differ - ent subsets of variables as inputs .
bottom : each possible conguration of z and y can be represented by a path in a trellis .
here z123 , z123 , and y123 are binary variables , while y123 is ternary .
sum of individual functions ( called factors ) that each depend on different subsets of the variables in y and z .
these dependencies are best expressed in the form of a factor graph ( kschischang et al . , 123 , mackay , 123 ) .
factor graphs are a general form of graphical models , or belief networks .
graphical models are normally used to represent probability distributions over vari - ables by directly encoding the dependency relationships between variables .
at rst glance , it is difcult to dissociate graphical models from probabilistic modeling ( wit - ness their original name : bayesian networks ) .
however , factor graphs can be studied outside the context of probabilistic modeling , and ebm learning applies to them .
a simple example of a factor graph is shown in figure 123 ( top ) .
the energy func -
tion is the sum of four factors :
e ( y , z , x ) = ea ( x , z123 ) + eb ( x , z123 , z123 ) + ec ( z123 , y123 ) + ed ( y123 , y123 ) ,
where y = ( y123 , y123 ) are the output variables and z = ( z123 , z123 ) are the latent variables .
each factor can be seen as representing soft constraints between the values of its input variables .
the inference problem consists in nding :
( y , z ) = argminyy , zz ( ea ( x , z123 ) + eb ( x , z123 , z123 ) + ec ( z123 , y123 ) + ed ( y123 , y123 ) ) .
this factor graph represents a structured output problem , because the factor ed en - codes dependencies between y 123 and y 123 ( perhaps by forbidding certain combinations
lets assume that z123 , z123 , and y123 are discrete binary variables , and y123 is a ternary variable .
the cardinality of the domain of x is immaterial since x is always observed .
the number of possible congurations of z and y given x is 123 123 123 123 = 123
a naive minimization algorithm through exhaustive search would evaluate the entire energy function 123 times ( 123 single factor evaluations ) .
however , we notice that for a given x , ea only has two possible input congurations : z123 = 123 and z123 = 123
sim - ilarly , eb and ec only have 123 possible input congurations , and ed has 123
hence , there is no need for more than 123 + 123 + 123 + 123 = 123 single factor evaluations .
the set of possible congurations can be represented by a graph ( a trellis ) as shown in fig - ure 123 ( bottom ) .
the nodes in each column represent the possible values of a single variable .
each edge is weighted by the output energy of the factor for the correspond - ing values of its input variables .
with this representation , a single path from the start node to the end node represents one possible conguration of all the variables .
the sum of the weights along a path is equal to the total energy for the corresponding con - guration .
hence , the inference problem can be reduced to searching for the shortest path in this graph .
this can be performed using a dynamic programming method such as the viterbi algorithm , or the a* algorithm .
the cost is proportional to the number of edges ( 123 ) , which is exponentially smaller than the number of paths in general .
to compute e ( y , x ) = minzz e ( y , z , x ) , we follow the same procedure , but we re - strict the graph to the subset of arcs that are compatible with the prescribed value of
the above procedure is sometimes called the min - sum algorithm , and it is the log domain version of the traditional max - product for graphical models .
the procedure can easily be generalized to factor graphs where the factors take more than two variables
logzy y , zz
as inputs , and to factor graphs that have a tree structure instead of a chain structure .
however , it only applies to factor graphs that are bipartite trees ( with no loops ) .
when loops are present in the graph , the min - sum algorithm may give an approximate solu - tion when iterated , or may not converge at all .
in this case , a descent algorithm such as simulated annealing could be used .
as mentioned in section 123 , variables can be handled through minimization or through marginalization .
the computation is identical to the one required for comput - ing the contrastive term of the negative log - likelihood loss ( the log partition function ) , hence we will make no distinctions .
the contrastive term in the negative log - likelihood loss function is :
when no latent variables are present .
at rst , this seems intractable , but the computation can be factorized just like with the min - sum algorithm .
the result is the so - called forward algorithm in the log domain .
values are propagated forward , starting at the start node on the left , and following the arrows in the trellis .
each node k computes a quantity k :
where ejk is the energy attached to the edge linking node j to node k .
the nal at the end node is the quantity in eq .
the procedure reduces to the min - sum algorithm for large values of .
in a more complex factor graph with factors that take more than two variables as in - put , or that have a tree structure , this procedure generalizes to a non - probabilistic form of belief propagation in the log domain .
for loopy graphs , the procedure can be iter - ated , and may lead to an approximate value for eq .
123 , if it converges at all ( yedidia et al . , 123 ) .
the above procedures are an essential component for constructing models with
structures and / or sequential output .
123 ebms versus internally normalized models
it is important to note that at no point in the above discussion did we need to manip - ulate normalized probability distributions .
the only quantities that are manipulated are energies .
this is in contrast with hidden markov models and traditional bayesian nets .
in hmms , the outgoing transition probabilities of a node must sum to 123 , and the emission probabilities must be properly normalized .
this ensures that the overall dis - tribution over sequences is normalized .
similarly , in directed bayesian nets , the rows of the conditional probability tables are normalized .
ebms manipulate energies , so no normalization is necessary .
when energies are transformed into probabilities , the normalization over y occurs as the very last step
in the process .
this idea of late normalization solves several problems associated with the internal normalization of hmms and bayesian nets .
the rst problem is the so - called label bias problem , rst pointed out by bottou ( bottou , 123 ) : transitions leaving a given state compete with each other , but not with other transitions in the model .
hence , paths whose states have few outgoing transitions tend to have higher probability than paths whose states have many outgoing transitions .
this seems like an articial constraint .
to circumvent this problem , a late normalization scheme was rst proposed by denker and burges in the context of handwriting and speech recogni - tion ( denker and burges , 123 ) .
another avor of the label bias problem is the miss - ing probability mass problem discussed by lecun et al .
in ( lecun et al . , 123a ) .
they also make use of a late normalization scheme to solve this problem .
normalized mod - els distribute the probability mass among all the answers that are explicitly modeled by the system .
to cope with junk or other unforeseen and un - modeled inputs , designers must often add a so - called background model that takes some probability mass away from the set of explicitly - modeled answers .
this could be construed as a thinly dis - guised way of removing the normalization constraint .
to put it another way , since every explicit normalization is another opportunity for mishandling unforeseen events , one should strive to minimize the number of explicit normalizations in a model .
a recent demonstration of successful handling of the label bias problem through normalization removal is the comparison between maximum entropy markov models by mccallum , freitag and pereira ( mccallum et al . , 123 ) , and conditional random elds by lafferty , mccallum and pereira ( lafferty et al . , 123 ) .
the second problem is controlling the relative importance of probability distribu - tions of different natures .
in hmms , emission probabilities are often gaussian mix - tures in high dimensional spaces ( typically 123 to 123 ) , while transition probabilities are discrete probabilities over a few transitions .
the dynamic range of the former is considerably larger than that of the latter .
hence transition probabilities count for almost nothing in the overall likelihood .
practitioners often raise the transition prob - abilities to some power in order to increase their inuence .
this trick is difcult to justify in a probabilistic framework because it breaks the normalization .
in the energy - based framework , there is no need to make excuses for breaking the rules .
arbitrary coefcients can be applied to any subset of energies in the model .
the normalization can always be performed at the end .
the third problem concerns discriminative learning .
discriminative training often uses iterative gradient - based methods to optimize the loss .
it is often complicated , ex - pensive , and inefcient to perform a normalization step after each parameter update by the gradient method .
the ebm approach eliminates the problem ( lecun et al . , 123a ) .
more importantly , the very reason for internally normalizing hmms and bayesian nets is somewhat contradictory with the idea of training them discriminatively .
the normal - ization is only necessary for generative models .
123 ebms for sequence labeling and structured out -
the problem of classifying or labeling sequences of symbols or sequences of vec - tors has long been a topic of great interest in several technical communities .
the earliest and most notable example is speech recognition .
discriminative learning methods were proposed to train hmm - based speech recognition systems in the late 123s ( bahl et al . , 123 , ljolje et al . , 123 ) .
these methods for hmms brought about a considerable improvement in the accuracy of speech recognition systems , and re - mains an active topic of research to this day .
