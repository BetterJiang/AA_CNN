we present an application of back - propagation networks to hand ( cid : 123 ) written digit recognition .
minimal preprocessing of the data was required , but architecture of the network was highly constrained and specifically designed for the task .
the input of the network consists of normalized images of isolated digits .
the method has 123 % error rate and about a 123% reject rate on zipcode digits provided by the u . s .
postal service .
the main point of this paper is to show that large back - propagation ( bp ) net ( cid : 123 ) works can be applied to real image - recognition problems without a large , complex preprocessing stage requiring detailed engineering .
unlike most previous work on the subject ( denker et al . , 123 ) , the learning network is directly fed with images , rather than feature vectors , thus demonstrating the ability of bp networks to deal with large amounts of low level information .
previous work performed on simple digit images ( le cun , 123 ) showed that the architecture of the network strongly influences the network ' s generalization ability .
good generalization can only be obtained by designing a network architecture that contains a certain amount of a priori knowledge about the problem .
the basic de ( cid : 123 ) sign principle is to minimize the number of free parameters that must be determined by the learning algorithm , without overly reducing the computational power of the network .
this principle increases the probability of correct generalization because
handwritten digit recognition with a back - propagation network
tl ( if ! ? - ( ) ( ) rt ' r . a . . 123 ~ cj - >i
figure 123 : examples of original zip codes from the testing set .
it results in a specialized network architecture that has a reduced entropy ( denker et al . , 123; patarnello and carnevali , 123; tishby , levin and solla , 123; le cun , 123 ) .
on the other hand , some effort must be devoted to designing appropriate constraints into the architecture .
123 zipcode recognition the handwritten digit - recognition application was chosen because it is a relatively simple machine vision task : the input consists of black or white pixels , the digits are usually well - separated from the background , and there are only ten output categories .
yet the problem deals with objects in a real two - dimensional space and the mapping from image space to category space has both considerable regularity and considerable complexity .
the problem has added attraction because it is of great practical value .
the database used to train and test the network is a superset of the one used in the work reported last year ( denker et al . , 123 ) .
we emphasize that the method of solution reported here relies more heavily on automatic learning , and much less on hand - designed preprocessing .
the database consists of 123 segmented numerals digitized from handwritten zip ( cid : 123 ) codes that appeared on real u . s .
mail passing through the buffalo , n . y .
post office .
examples of such images are shown in figure 123
the digits were written by many different people , using a great variety of sizes , writing styles and instruments , with widely varying levels of care .
this was supplemented by a set of 123 printed dig ( cid : 123 ) its coming from 123 different fonts .
the training set consisted of 123 handwritten digits plus 123 printed digits .
the remaining 123 handwritten and 123 printed digits were used as the test set .
the printed fonts in the test set were different from the printed fonts in the training set . one important feature of this database , which
le cun , boser , denker , henderson , howard , hubbard and jackel
figure 123 : examples of normalized digits from the testing set .
is a common feature to all real - world databases , is that both the training set and the testing set contain numerous examples that are ambiguous , unclassifiable , or
acquisition , binarization , location of the zip code , and preliminary segmentation were performed by postal service contractors ( wang and srihari , 123 ) .
some of these steps constitute very hard tasks in themselves .
the segmentation ( separating each digit from its neighbors ) would be a relatively simple task if we could assume that a character is contiguous and is disconnected from its neighbors , but neither of these assumptions holds in practice .
many ambiguous characters in the database are the result of mis - segmentation ( especially broken 123 ' s ) as can be seen on figure 123
at this point , the size of a digit varies but is typically around 123 by 123 pixels .
since the input of a back - propagation network is fixed size , it is necessary to normalize the size of the characters .
this was performed using a linear transformation to make the characters fit in a 123 by 123 pixel image .
this transformation preserves the aspect ratio of the character , and is performed after extraneous marks in the image have been removed .
because of the linear transformation , the resulting image is not binary but has multiple gray levels , since a variable number of pixels in the original image can fall into a given pixel in the target image .
the gray levels of each image are scaled and translated to fall within the range - 123 to 123
123 the network the remainder ofthe recognition is entirely performed by a multi - layer network .
all of the connections in the network are adaptive , although heavily constrained , and are trained using back - propagation .
this is in contrast with earlier work ( denker et al . , 123 ) where the first few layers of connections were hand - chosen constants .
the input of the network is a 123 by 123 normalized image and the output is composed
handwritten digit recognition with a back - propagation network
of 123 units : one per class .
when a pattern belonging to class i is presented , the desired output is +123 for the ith output unit , and - 123 for the other output units .
figure 123 : input image ( left ) , weight vector ( center ) , and resulting feature map ( right ) .
the feature map is obtained by scanning the input image with a single neuron that has a local receptive field , as indicated .
white represents - 123 , black represents + 123
a fully connected network with enough discriminative power for the task would have far too many parameters to be able to generalize correctly .
therefore a restricted connection - scheme must be devised , guided by our prior knowledge about shape recognition .
there are well - known advantages to performing shape recognition by detecting and combining local features .
we have required our network to do this by constraining the connections in the first few layers to be local .
in addition , if a feature detector is useful on one part of the image , it is likely to be useful on other parts of the image as well .
one reason for this is that the salient features of a distorted character might be displaced slightly from their position in a typical char ( cid : 123 ) acter .
one solution to this problem is to scan the input image with a single neuron that has a local receptive field , and store the states of this neuron in corresponding locations in a layer called a feature map ( see figure 123 ) .
this operation is equivalent to a convolution with a small size kernel , followed by a squashing function .
the process can be performed in parallel by implementing the feature map as a plane of neurons whose weight vectors are constrained to be equal .
that is , units in a feature map are constrained to perform the same operation on different parts of the image .
an interesting side - effect of this weight sharing technique , already described in ( rumelhart , hinton and williams , 123 ) , is to reduce the number of free param ( cid : 123 ) eters by a large amount , since a large number of units share the same weights .
in addition , a certain level of shift invariance is present in the system : shifting the input will shift the result on the feature map , but will leave it unchanged otherwise .
in practice , it will be necessary to have multiple feature maps , extracting different features from the same image .
le cun , boser , denker , henderson , howard , hubbard and jackel
x x x x x
123 x x x
123 123 123
x x x
x x x x x
table 123 : connections between h123 and h123
the idea of local , convolutional feature maps can be applied to subsequent hidden layers as well , to extract features of increasing complexity and abstraction .
inter ( cid : 123 ) estingly , higher level features require less precise coding of their location .
reduced precision is actually advantageous , since a slight distortion or translation of the in ( cid : 123 ) put will have reduced effect on the representation .
thus , each feature extraction in our network is followed by an additional layer which performs a local averaging and a subsampling , reducing the resolution of the feature map .
this layer introduces a certain level of invariance to distortions and translations .
a functional module of our network consists of a layer of shared - weight feature maps followed by an averaging / subsampling layer .
this is reminiscent of the neocognitron architecture ( fukushima and miyake , 123 ) , with the notable difference that we use backprop ( rather than unsupervised learning ) which we feel is more appropriate to this sort of classification problem .
the network architecture , represented in figure 123 , is a direct extension of the ones described in ( le cun , 123; le cun et al . , 123a ) .
the network has four hidden layers respectively named hi , h123 , h123 , and h123
layers hi and h123 are shared - weights feature extractors , while h123 and h123 are averaging / subsampling layers .
although the size of the active part of the input is 123 by 123 , the actual input is a 123 by 123 plane to avoid problems when a kernel overlaps a boundary .
hi is composed of 123 groups of 123 units arranged as 123 independent 123 by 123 feature maps .
these four feature maps will be designated by hi . l , hi . 123 , hi . 123 and hia .
each unit in a feature map takes its input from a 123 by 123 neighborhood on the input plane .
as described above , corresponding connections on each unit in a given feature map are constrained to have the same weight .
in other words , all of the 123 units in h123 uses the same set of 123 weights ( including the bias ) .
of course , units in another map ( say hi . 123 ) share another set of 123 weights .
layer h123 is the averaging / subsampling layer .
it is composed of 123 planes of size 123 by 123
each unit in one of these planes takes inputs on 123 units on the corresponding plane in hi .
receptive fields do not overlap .
all the weights are constrained to be equal , even within a single unit .
therefore , h123 performs a local averaging and a 123 to 123 subsampling of hi in each direction .
layer h123 is composed of 123 feature maps .
each feature map contains 123 units arranged in a 123 by 123 plane .
as before , these feature maps will be designated as h123 , h123 . . .
the connection scheme between h123 and h123 is quite similar to the one between the input and hi , but slightly more complicated because h123 has multiple 123 - d maps .
each unit receptive field is composed of one or two 123 by
handwritten digit recognition with a back . propagation network
figure 123 : network architecture with 123 layers of fully - adaptive connections .
le cun , boser , denker , henderson , howard , hubbard and jackel
123 neighborhoods centered around units that are at identical positions within each h123 maps .
of course , all units in a given map are constrained to have identical weight vectors .
the maps in h123 on which a map in h123 takes its inputs are chosen according to a scheme described on table 123
according to this scheme , the network is composed of two almost independent modules .
layer h123 plays the same role as layer h123 , it is composed of 123 groups of 123 units arranged in 123 by 123 planes .
the output layer has 123 units and is fully connected to h123
in summary , the network has 123 units , 123 connections , and 123 independent parameters .
this architecture was derived using the optimal brain damage technique ( le cun et al . , 123b ) starting from a previous architecture ( le cun et al . , 123a ) that had 123 times more free parameters .
after 123 training passes the error rate on training set ( 123 handwritten plus 123 printed digits ) was 123% and the mse was . 123
on the whole test set ( 123 handwritten plus 123 printed characters ) the error rate was 123% and the mse was 123 .
all the classification errors occurred on handwritten characters .
in a realistic application , the user is not so much interested in the raw error rate as in the number of rejections necessary to reach a given level of accuracy .
in our case , we measured the percentage of test patterns that must be rejected in order to get 123% error rate .
our rejection criterion was based on three conditions : the activity level of the most - active output unit should by larger than a given threshold t 123 , the activity level of the second most - active unit should be smaller than a given threshold t123 , and finally , the difference between the activity levels of these two units should be larger than a given threshold td .
the best percentage of rejections on the complete test set was 123% for 123% error .
on the handwritten set only , the result was 123% rejections for 123 % error .
it should be emphasized that the rejection thresholds were obtained using performance measures on the test set .
about half the substitution errors in the testing set were due to faulty segmentation , and an additional quarter were due to erroneous assignment of the desired category .
some of the remaining images were ambiguous even to humans , and in a few cases the network misclassified the image for no discernible reason .
even though a second - order version of back - propagation was used , it is interesting to note that the learning takes only 123 passes through the training set .
we think this can be attributed to the large amount of redundancy present in real data .
a complete training session ( 123 passes through the training set plus test ) takes about 123 days on a sun sp arcstation 123 using the sn123 connectionist simulator ( bottou and le cun , 123 ) .
after successful training , the network was implemented on a commercial digital signal processor board containing an at&t dsp - 123c general purpose dsp chip with a peak performance of 123 million multiply - add operations per second on 123 bit floating point numbers .
the dsp operates as a coprocessor in a pc connected to a video camera .
the pc performs the digitization , binarization and segmentation
handwritten digit recognition with a back - propagation network
figure 123 : atypical data .
the network classifies these correctly , even though they are quite unlike anything in the training set .
of the image , while the dsp performs the size - normalization and the classification .
the overall throughput of the digit recognizer including image acquisition is 123 to 123 classifications per second and is limited mainly by the normalization step .
on normalized digits , the dsp performs more than 123 classifications per second .
back - propagation learning was successfully applied to a large , real - world task .
our results appear to be at the state of the art in handwritten digit recognition .
the network had many connections but relatively few free parameters .
the network architecture and the constraints on the weights were designed to incorporate geo ( cid : 123 ) metric knowledge about the task into the system .
because of its architecture , the network could be trained on a low - level representation of data that had minimal preprocessing ( as opposed to elaborate feature extraction ) .
because of the redun ( cid : 123 ) dant nature of the data and because of the constraints imposed on the network , the learning time was relatively short considering the size of the training set .
scaling properties were far better than one would expect just from extrapolating results of back - propagation on smaller , artificial problems .
preliminary results on alphanu ( cid : 123 ) meric characters show that the method can be directly extended to larger tasks .
the final network of connections and weights obtained by back - propagation learn ( cid : 123 ) ing was readily implementable on commercial digital signal processing hard ware .
throughput rates , from camera to classified image , of more than ten digits per second were obtained .
we thank the us postal service and its contractors for providing us with the zip ( cid : 123 ) code database .
we thank henry baird for useful discussions and for providing the
