interpolated kneser - ney is one of the best smoothing methods for n - gram language models .
previ - ous explanations for its superiority have been based on intuitive and empirical justications of specic properties of the method .
we propose a novel interpretation of interpolated kneser - ney as approxi - mate inference in a hierarchical bayesian model consisting of pitman - yor processes .
as opposed to past explanations , our interpretation can recover exactly the formulation of interpolated kneser - ney , and performs better than interpolated kneser - ney when a better inference procedure is used .
probabilistic language models are used extensively in a variety of linguistic applications .
standard exam - ples include speech recognition , handwriting recognition , machine translation and spelling correction .
the basic task is to model the probability distribution over sentences .
most researchers take the approach of modelling the conditional distribution of words given their histories , and piecing these together to form the joint distribution over the whole sentence ,
p ( word123 , word123 , .
, wordt ) =
p ( wordi | word123 , .
, wordi123 ) .
the class of n - gram models form the bulk of such models .
the basic assumption here is that the conditional probability of a word given its history can be simplied to its probability given a reduced context consisting of only the past n 123 words ,
p ( wordi | word123 , .
, wordi123 ) = p ( wordi | wordin +123 , .
, wordi123 )
even for modest values of n the number of parameters involved in n - gram models is still tremendous .
for example typical applications use n = 123 and has a o ( 123 ) word vocabulary , leading to o ( 123 ) parameters .
as a result direct maximum - likelihood parameter tting will severely overt to our training data , and smoothing techniques are indispensible for the proper training of n - gram models .
a large number of smoothing techniques have been proposed in the literature; see chen and goodman ( 123 ) ; goodman ( 123 ) ; rosenfeld ( 123 ) for overviews , while more recent proposals include charniak ( 123 ) ; bilmes and kirchhoff ( 123 ) ; bengio et al .
( 123 ) ; xu and jelinek ( 123 ) and blitzer et al .
( 123 ) .
in an extensive and systematic survey of smoothing techniques for n - grams , chen and goodman ( 123 ) showed that interpolated kneser - ney and its variants were the most successful smoothing techniques at the time .
although more recent techniques have exhibited better performance , interpolated kneser - ney is still an important technique now as the better performances have only been achieved by combining more elaborate models with it .
interpolated kneser - ney involves three concepts : it interpolates linearly between higher and lower order n - grams , it alters positive word counts by subtracting a constant amount ( absolute discounting ) , and it uses an unusual estimate of lower order n - grams .
a number of explanations for why interpolated kneser - ney works so well has been given in the liter - ature .
kneser and ney ( 123 ) , chen and goodman ( 123 ) and goodman ( 123 ) showed that the unusual estimate of lower order n - grams follows from interpolation , absolute discounting , and a constraint on word marginal distributions .
goodman ( 123 ) further showed that n - gram models which does not preserve these word marginal distributions cannot be optimal .
empirical results in chen and goodman ( 123 ) demon - strated that interpolation works better than other ways of combining higher and lower order n - grams and that absolute discounting is a good approximation to the optimal discount .
finally , a different approach by goodman ( 123 ) showed that back - off kneser - ney is similar to a maximum - entropy model with exponential
we will give a new interpretation of interpolated kneser - ney as an approximate inference method in a bayesian model .
the model we propose is a straightforward hierarchical bayesian model ( gelman et al .
123 ) , where each hidden variable represents the distribution over next words given a particular context .
these variables are related hierarchically such that the prior mean of a hidden variable corresponding to a context is the word distribution given the context consisting of all but the earliest word ( we will make clear what we mean by this in the later parts of the paper ) .
the hidden variables are distributed according to a well - studied nonparametric generalization of the dirichlet distribution variously known as the two - parameter poisson - dirichlet process or the pitman - yor process ( pitman and yor 123; ishwaran and james 123; pitman 123 ) ( in this paper we shall refer to this as the pitman - yor process for succinctness ) .
as we shall show in this paper , this hierarchical structure corresponds exactly to the technique of inter - polating between higher and lower order n - grams .
our interpretation has the advantage over past interpre - tations in that we can recover the exact form of interpolated kneser - ney .
in addition , in comparison with the maximum - entropy view , where interpolated kneser - ney in fact does better than the model in which it is supposed to approximate , we show in experiments that our model works better than interpolated kneser - ney if we use more accurate inference procedures .
as our model is fully bayesian , we also reap other advantages of bayesian methods , e . g .
we can easily use the model as part of a more elaborate model .
bayesian techniques are not new in natural language processing and language modelling given the prob - abilistic nature of most approaches .
maximum - entropy models have found many uses relating features of inputs to distributions over outputs ( rosenfeld 123; berger et al .
123; mccallum et al .
123; lafferty et al .
use of priors is widespread and a number of studies have been conducted comparing different types of priors ( brand 123; chen and rosenfeld 123; goodman 123 ) .
even hierarchical bayesian models have been applied to language modellingmackay and peto ( 123 ) have proposed one based on dirichlet distributions .
our model is a natural generalization of this model using pitman - yor processes rather than
bayesian models have not been more widely adopted in the language modelling community because the models proposed so far have performed poorly in comparison to other smoothing techniques .
the major contributions of our work are in proposing a bayesian model with excellent performance , and in establishing the direct correspondence between interpolated kneser - ney , a well - established smoothing technique , and the bayesian approach .
we expect this connection to be useful both in terms of giving a principled statistical
footing to smoothing techniques and in suggesting even better performing bayesian models .
goldwater et al .
( 123 ) observed that pitman - yor processes generate power - law distributions and argued that since such distributions occur frequently in natural languages , they are more suited for natural languages is it thus perhaps unsurprising that our model has performance superior to the hierarchical dirichlet language model of mackay and peto ( 123 ) .
in fact , goldwater et al .
( 123 ) have independently noted this relationship between a hierarchical pitman - yor process and interpolated kneser - ney , but have not corroborated this with further investigations and experimental results .
in the following section we will give a detailed description of interpolated kneser - ney and modied kneser - ney .
we review the pitman - yor process as it pertains to language modelling in section 123
in sec - tion 123 we propose the hierarchical pitman - yor language model and relate it to interpolated kneser - ney .
experimental results establishing the performance of the model in terms of cross - entropy is reported in sec - tion 123 , and we conclude with some discussion in section 123
finally we delegate details of some additional properties of the model and inference using markov chain monte carlo sampling to the appendices .
123 interpolated kneser - ney and its variants
in this section we introduce notations and describe in detail the n - gram language modelling task , interpo - lated kneser - ney and a modied version which performs better .
our sources of information are chen and goodman ( 123 ) and goodman ( 123 ) which are excellent reviews of state - of - the - art smoothing techniques and language models .
we assume that we have a closed set of words in our vocabolary w , which is of size v .
for a word w w and a context consisting of a sequence of n 123 words u w n123 let cuw be the number of occurrences of w following u in our training corpus .
the naive maximum likelihood probability for a word w following u is
u ( w ) =
where cu = pw123 cuw123
instead , interpolated kneser - ney estimates the probability of word w following
context u by discounting the true count cuw by a xed amount d|u| depending on the length |uw| if cuw > 123 ( otherwise the count remains 123 ) .
further , it interpolates the estimated probability of word w with lower order m - gram probabilities .
this gives ,
u ( w ) =
max ( 123 , cuw d|u| )
where tu = # ( w123 | cuw123 > 123 ) is the number of distinct words w123 following context u in the training corpus , ( u ) is the context consisting of all words in u except the rst and p ikn ( u ) ( w ) are the lower order m - gram probabilities .
the value of tu is chosen simply to make the probability estimates sum to 123
finally , interpolated kneser - ney uses modied sets of counts for the lower order m - gram probabilities .
in particular , for a context u123 of length m < n 123 and words w123 and w , let
tw123u123w = ( 123 if cw123u123w > 123;
123 if cw123u123w = 123;
cu123w = tu123w =xw123
where w123u123 is the context formed by concatenating w123 and u123
the lower order m - gram probabilities are estimated as in ( 123 ) using the modied counts of ( 123 ) .
a different value of discount dm123 is used for each length m and these can either be estimated using formulas or by using cross - validation .
modied kneser - ney is an improvement upon interpolated kneser - ney where the amount of discount is allowed more variability .
in the empirical studies in chen and goodman ( 123 ) and church and gale ( 123 ) it was found that the optimal amount of discount that should be used changes slowly as a function of the counts cuw .
this was used as one of the reasons for absolute discounting in chen and goodman ( 123 ) .
in the same study it was also noticed that the optimal discounts for low values of cuw differ substantially from those with higher values .
modied kneser - ney uses different values of discounts for different counts , one each for cuw = 123 , 123 , .
, c ( max ) 123 and another for cuw c ( max ) .
the same formulas for ( 123 ) and ( 123 ) are used .
modied kneser - ney reduces to interpolated kneser - ney when c ( max ) = 123 , while chen and goodman ( 123 ) uses c ( max ) = 123 as a good compromise between diminishing improvements and increasing
the unusual counts in interpolated kneser - ney can be derived by preserving marginal word distribu - tions .
let p emp ( u ) be the empirical probability of word sequence u among sequences of length n 123
let w123 and w be words and u123 be a word sequence of length m = n 123
assuming the form of ( 123 ) and the following marginal constraints ,
we can derive that
p emp ( w123u123 ) p ikn
w123 u123 ( w ) = p emp ( u123w )
where cu123w is as given in ( 123 ) .
finally , rather than using ( 123 ) we should discount these new counts and interpolate with even lower order m - gram probabilities , i . e .
recursively apply ( 123 ) and ( 123 ) .
satisfying the marginal constraints ( 123 ) is reasonable since the n - gram probabilities should be consistent with the statistics of the word counts .
in fact goodman ( 123 ) showed that if these constraints are not satised then the n - gram probability estimates cannot be optimal ( the converse is not true; satisfying these constraints does not imply optimality ) .
taking the marginal constraints view further , goodman ( 123 ) showed that a back - off version of kneser - ney can be seen as an approximation to a maximum - entropy model with approximately satised marginal constraints and an exponential prior on the parameters of the model .
however this view of interpolated kneser - ney in terms of marginal constraints is limited in scope for a few reasons .
firstly , the maximum - entropy model of which back - off kneser - ney is supposed to approximate in fact performs worse than back - off kneser - ney which is in turn worse than interpolated kneser - ney .
secondly , modied kneser - ney , which performs better than interpolated kneser - ney does not satisfy these
123 pitman - yor processes
we go through the properties of the pitman - yor process relevant to language modelling in this section .
for more in depth discussion we refer to pitman and yor ( 123 ) ; ishwaran and james ( 123 ) ; pitman ( 123 ) , while jordan ( 123 ) gives a high - level tutorial of this branch of statistics and probability theory from a machine learning perspective .
the pitman - yor process py ( d , , g123 ) is a distribution over distributions over a probability space x .
it has three parameters : a discount parameter 123 d < 123 , a strength parameter > d and a base distribu - tion g123 over x .
the base distribution can be understood as a putative mean of draws from py ( d , , g123 ) , while both and d control the amount of variability around the base distribution g123
an explicit construc - tion of draws g123 py ( d , , g123 ) from a pitman - yor process is given by the stick - breaking construction
( sethuraman 123; ishwaran and james 123 ) .
this construction shows that g123 is a weighted sum of an innite sequence of point masses ( with probability one ) .
let v123 , v123 , .
and 123 , 123 , .
be two sequence of independent random variables with distributions ,
vk beta ( 123 d , + kd )
for k = 123 , 123 ,
then the following construction gives a draw from py ( d , , g123 ) :
( 123 v123 ) ( 123 vk123 ) vkk
where is a point mass located at .
the stick - breaking construction is useful as it is mathematically elegant and it gives us a direct visualization of pitman - yor processes .
a different perspective on the pitman - yor process is given by the chinese restaurant process .
this describes the properties of the pitman - yor process in terms of distributions over draws from g123 , which is itself a distribution over x .
though indirect , this perspective is more useful for our purpose of language modelling , since draws from g123 will correspond to words whose distributions we wish to model .
let x123 , x123 , .
be a sequence of identical and independent draws from g123
the analogy is that of a sequence of customers ( xis ) visiting a restaurant ( corresponding to g123 ) with an unbounded number of tables .
the chinese restaurant process assigns a distribution over the seating arrangement of the customers .
the rst customer sits at the rst available table , while each of the other customers sits at the kth occupied table with probability proportional to ck d , where ck is the number of customers already sitting there , and she sits at a new unoccupied table with probability proportional to + dt , where t is the current number of occupied tables .
to generate draws for x123 , x123 , .
. , associate with each table k an independent draw k g123 from the base distribution g123 and set the drawn value of xi to be k if customer i sat at table k .
the k draws can be thought of as dishes , with customers sitting at each table eating the dish on the table .
the resulting
conditional distribution of the next draw after a sequence of c =pk ck draws is thus :
xc+123 | x123 .
, xc , seating arrangement
the sequence x123 , x123 , .
as generated by the chinese restaurant process can be shown to be exchangeable .
that is , the distribution assigned by the chinese restaurant process to x123 , x123 , .
is invariant to permuting the order of the sequence .
de finettis theorem on exchangeable sequences then shows that there must be a distribution over distributions g123 such that x123 , x123 , .
are conditionally independent and identical draws from g123 ( pitman 123 ) .
the pitman - yor process is one such distribution over g123
consider using the pitman - yor process as a prior for unigram word distributions .
we use a uniform distribution over our xed vocabulary w of v words as the base distribution g123 , that is , each word in w is equiprobable under g123 , while the draw from the pitman - yor process g123 is the desired unigram distribution over words .
we have a training corpus consisting of cw occurrences of word w w , which corresponds to knowing that cw customers are eating dish w in the chinese restaurant representation .
given this informa -
tion , we infer the seating arrangement of the c = pw cw customers in the restaurant .
in particular , let tw
be the number of tables serving dish w in the seating arrangement ( since the vocabulary w is nite there is positive probability that multiple tables serve the same dish ) .
the predictive probability of a new word given the seating arrangement is given by ( 123 ) , which evaluates to
p ( xc+123 = w | seating arrangement ) =
by collecting terms in ( 123 ) corresponding to each dish w .
the actual predictive probability is then ( 123 ) averaged over the posterior probability over seating arrangements .
we see that there are two opposing effects on word counts cw in the pitman - yor process .
the second term adds to word counts , while the discount term in the rst fraction dtw subtracts from word counts .
when d = 123 the pitman - yor process reduces to a dirichlet distribution , and we only have the usual additive pseudo - counts of the dirichlet distribution .
if d > 123 , we have discounts , and the additive term can be understood as interpolation with the uniform distribution .
further assuming that tw = 123 , i . e .
only one table serves dish w , we obtain absolute discounting .
in the appendix we show that tws grow as o ( c d
w ) instead .
123 hierarchical pitman - yor language models
in the previous section we already see how we can obtain absolute discounting and interpolation using the pitman - yor process .
in this section we describe a language model based on a hierarchical extension of the pitman - yor process , and show that we can recover interpolated kneser - ney as approximate inference in the model .
the hierarchical pitman - yor process is a generalization of the hierarchical dirichlet process , and the derivation described here is a straightforward generalization of those in teh et al .
( 123 ) .
we are interested building a model of distributions over the current word given various contexts .
given a context u consisting of a sequence of up to n 123 words , let gu ( w ) be the distribution over the current word w .
since we wish to infer gu ( w ) from our training corpus , the bayesian nonparametric approach we take here is to assume that gu ( w ) is itself a random variable .
we use a pitman - yor process as the prior for gu ( w ) , in particular ,
gu ( w ) py ( d|u| , |u| , g ( u ) ( w ) )
where ( u ) is the sufx of u consisting of all but the rst word .
the strength and discount parameters depend on the length of the context , just as in interpolated kneser - ney where the same discount parameter is used for each length of context .
the base distribution is g ( u ) ( w ) , the distribution over the current word given all but the earliest word in the context .
that is , we believe that without observing any data the earliest word is the least important in determining the distribution over the current word .
since we do not know g ( u ) ( w ) either , we recursively place a prior over g ( u ) ( w ) using ( 123 ) , but now with parameters | ( u ) | , d| ( u ) | and base distribution g ( ( u ) ) ( w ) and so on .
finally the prior for g ( w ) , the distribution over current word given the empty context is given a prior of
g ( w ) py ( d123 , 123 , g123 )
where g123 is the global base distribution , which is assumed to be uniform over the vocabulary w of v words .
the structure of the prior is that of a sufx tree of depth n , where each node corresponds to a context consisting of up to n 123 words , and each child corresponds to adding a different word to the beginning of the context .
as we shall see , this choice of the prior structure expresses our belief that words appearing later in a context have more inuence over the distribution over the current word .
we can apply the chinese restaurant representation to the hierarchical pitman - yor language model to draw words from the prior .
the basic observation is that to draw words from gu ( w ) using the chinese restaurant representation the only operation we need of the base distribution g ( u ) ( w ) is to draw words from it .
since g ( u ) ( w ) is itself distributed according to a pitman - yor process , we can use another chinese restaurant to draw words from that .
this is recursively applied until we need a draw from the global base distribution g123 , which is easy since it assigns equal probability to each word in the vocabulary .
in summary
we have a restaurant corresponding to each gu ( w ) , which has an unbounded number tables and has a sequence of customers corresponding to words drawn from gu ( w ) .
each table is served a dish ( corresponds to a word drawn from the base distribution g ( u ) ( w ) ) , while each customer eats the dish served at the table she sat at ( the word drawn for her is the same as the word drawn for the table ) .
the dish served at the table is in turn generated by sending a customer to the parent restaurant in a recursive fashion .
notice that there are two types of customers in each restaurant , the independent ones arriving by themselves , and those sent by a child restaurant .
further , every table at every restaurant is associated with a customer in the parent restaurant , and every dish served in the restaurants can be traced to a draw from g123 in this way .
in the rest of the paper we index restaurants ( contexts ) by u , dishes ( words in our vocabulary ) by w , and tables by k .
let cuwk be the number of customers in restaurant u sitting at table k eating dish w ( cuwk = 123 if table k does not serve dish w ) , and let tuw be the number of tables in restaurant u serving dish w .
we denote marginal counts by dots , for example cuk is the number of customers sitting around table k in restaurant u , cuw is the number eating dish w in restaurant u ( number of occurrences of word w in context u ) , and tu is the number of tables in restaurant u .
in language modelling , the training data consists knowing the number of occurrences of each word w after each context u of length n 123 ( we pad the beginning of each sentence with begin - sentence symbols ) .
this corresponds to knowing the number cuw of customers eating dish w in restaurant u , for each u with length n 123
these customers are the only independent ones in the restaurants , the others are all sent by child restaurants .
as a result only the values of cuw with |u| = n 123 are xed by the training data , other values vary depending on the seating arrangement in each restaurant , and we have the following relationships among the cuws and tuw :
( tuw = 123
123 tuw cuw
if cuw = 123; if cuw > 123;
cuw = xu123 : ( u123 ) =u
algorithm 123 gives details of how the chinese restaurant representation can be used to generate words given contexts in terms of a function which draws a new word by calling itself recursively .
notice the self - reinforcing property of the hierarchical pitman - yor language model : the more a word w has been drawn in context u , the more likely will we draw w again in context u .
in fact word w will be reinforced for other contexts that share a common sufx with u , with the probability of drawing w increasing as the length of the common sufx increases .
this is because w will be more likely under the context of the common sufx
the chinese restaurant representation can also be used for inference in the hierarchical pitman - yor language model .
appendix a . 123 gives the joint distribution over seating arrangements in the restaurants ,
table 123 : routine to draw a new word given context u using the chinese restaurant representation .
if j = 123 , return word w w with probability g123 ( w ) = 123 / v .
else with probabilities proportional to :
max ( 123 , cuwk d|u| ) : sit customer at table k ( increment cuwk ) ;
return word w .
|u| + d|u|tu : let w drawword ( ( u ) ) ;
sit customer at an unoccupied table knew serving dish w ( increment tuw , set cuwknew = 123 ) ;
while appendix b gives an inference routine based upon gibbs sampling which returns samples from the posterior distribution over seating arrangements .
appendix c gives an auxiliary sampling routine for the strength and discount parameters .
given a sample from the posterior seating arrangement and parameters , the predictive probability of the next draw from gu ( w ) is given by recursively applying ( 123 ) .
for the global base distribution the predictive probability is simply
( w | seating arrangement ) = g123 ( w )
while for each context u the predictive probability of the next word after context u given the seating ar -
( w | seating arrangement ) =
|u| + cu
|u| + d|u|tu
|u| + cu
( u ) ( w | seating arrangement )
to form our n - gram probability estimates , we simply average ( 123 ) over the posterior of the seating arrange - ments and parameters .
from ( 123 ) the correspondence to interpolated kneser - ney is now straightforward .
suppose that the strength parameters are all |u| = 123
consider an approximate inference scheme for the hierarchical pitman - yor language model where we simply set
tuw = ( 123 if cuw = 123;
123 if cuw 123;
cuw = xu123 : ( u123 ) =u
( 123 ) says that there is at most one table in each restaurant serving each dish .
the predictive probabilities given by ( 123 ) now directly reduces to the predictive probabilities given by interpolated kneser - ney ( 123 ) .
as a result we can interpret interpolated kneser - ney as this particular approximate inference scheme in the hierarchical pitman - yor language model .
appendix a describes some additional properties of the hierarchical pitman - yor language model .
123 experimental results
we performed experiments on the hierarchical pitman - yor language model under two circumstances : tri - grams on a 123 million word corpus derived from apnews123
and bigrams on a 123 million word corpus derived from the penn treebank portion of the wsj dataset123 on the trigram apnews dataset , we compared our model to interpolated and modied kneser - ney on cross - entropies and studied the growth of discounts as functions of trigram counts .
on the simpler bigram wsj dataset , we studied the effect on cross - entropies of varying the strength and discount parameters and related our results to the hierarchical dirichlet language model .
we also showed that our proposed sampler converges very quickly .
we compared the hierarchical pitman - yor language model against interpolated kneser - ney and mod - ied kneser - ney with c ( max ) = 123 and 123 on the trigram apnews dataset .
we varied the training set size between approximately 123 million and 123 million words by six equal increments .
for all three versions of interpolated kneser - ney , we rst determined the discount parameters by conjugate gradient descent in the
123this is the same dataset as in bengio et al .
( 123 ) .
the training , validation and test sets consist of about 123 million , 123 million
and 123 million words respectively , while the vocabulary size is 123
123this is the same dataset as in xu and jelinek ( 123 ) and blitzer et al .
( 123 ) .
we split the data into training , validation and test
sets by randomly assigning bigrams to each with probabilities . 123 , . 123 , . 123 respectively .
the vocabulary size is 123
comparison of different models
growth of discounts with counts
modified kneserney , c ( max ) =123 modified kneserney , c ( max ) =123
number of words in training and validation sets ( millions )
modified kneserney , c ( max ) =123 modified kneserney , c ( max ) =123
figure 123 : left : cross - entropy on test set ( lower better ) .
the training set size is varied on the x - axis while the y - axis shows the cross - entropy ( in natural logarithm ) .
each line corresponds to a language model .
right : average discount as a function of trigram counts .
for the hierarchical pitman - yor language model the reported discount for a count c is d123 times the number of tables averaged over posterior samples of seating arrangement and over all trigrams that occurred c times in the full training set .
the last entry is averaged over all trigrams that occurred at least 123 times .
cross - entropy on the validation set ( chen and goodman 123 ) .
at the optimal values , we folded the valida - tion set into the training set to obtain the nal trigram probability estimates .
for the hierarchical pitman - yor language model we inferred the posterior distribution over seating arrangement and the strength and dis - count parameters given both the training and validation set123
we used a sampling routine which alternates between updating the seating arrangement ( appendix b ) and the parameters ( appendix c ) .
since the pos - terior is very well - behaved , we only used 123 iterations for burn - in , and 123 iterations to collect posterior samples .
on the full 123 million word training set ( includes data from the validation set ) this took less than 123 hours on 123ghz pentium iiis .
the cross - entropy results are given in figure 123 ( left ) .
as expected the hierarchical pitman - yor language model performs better than interpolated kneser - ney , supporting our claim that interpolated kneser - ney is just an approximation inference scheme in the hierarchical pitman - yor language model .
interestingly , the hierarchical pitman - yor language model performs slightly worse than the modied versions of kneser - ney .
in figure 123 ( right ) we showed the average discounts returned by the hierarchical pitman - yor language model as a function of the observed count of trigrams in the training set .
we also showed the discounts returned by the interpolated and modied kneser - ney models .
we see that the average discounts returned by the hierarchical pitman - yor language model grows slowly as a function of the trigram counts .
appendix a . 123 shows that the average discount grows as a power - law with index d123 and this is reected well by the gure .
the growth of the average discounts also matches relatively closely with that of the optimal discounts in figure 123 of chen and goodman ( 123 ) ,
in the second set of experiments we investigated the effect of the strength and discount parameters on the
123this is one of the advantages of a bayesian procedure , we need not use a separate validation set to determine parameters of the model .
instead we can include the validation set in the training set and infer both the hidden variables and parameters in a single phase of training .
performance with varying
performance with varying d
figure 123 : left : cross entropy on test data as 123 is varied and with other parameters held at the optimal settings found by interpolated kneser - ney .
right : varying d123 instead .
performance of the hierarchical pitman - yor language model in case of bigrams on a 123 million word dataset .
we rst found optimal settings for the four parameters 123 , 123 , d123 and d123 by optimizing the performance of interpolated kneser - ney on a validation set123
then for each parameter we varied it while keeping the others xed at its optimal .
we found that the model is only sensitive to d123 but is insensitive to d123 , 123 and 123
results for 123 and d123 are shown in figure 123
the model is insensitive to the strength parameters because in most cases these are very small compared with the count and discount terms in the predictive probabilities ( 123 ) .
in fact , we had repeated both trigram and bigram experiments with m set to 123 for each m , and the results were identical .
the model is insensitive to d123 for two reasons : its effect on the predictive probabilities ( 123 ) is small , and most values of tw = 123 or 123 so the discount term corresponding to d123 in ( 123 ) is cancelled out by the additive term involving the uniform base distribution g123 over the vocabulary .
when d123 = 123 the hierarchical pitman - yor language model reduces down to the hierarchical dirichlet language model of mackay and peto ( 123 ) , and as seen in figure 123 ( right ) this performs badly .
we have described using a hierarchical pitman - yor process as a language model and derived estimates of n - gram probabilities based on this model that are generalizations of interpolated kneser - ney .
setting some variables and parameters to specic values reduces the formula for n - gram probabilities to those in interpolated kneser - ney , hence we may interpret interpolated kneser - ney as approximate inference in this model .
in experiments we have also shown that cross - entropies attained by the model are better than those obtained by interpolated kneser - ney .
the hierarchical dirichlet language model of mackay and peto ( 123 ) was an inspiration for our work .
though mackay and peto ( 123 ) had the right intuition to look at smoothing techniques as the outcome of hierarchical bayesian models , the use of the dirichlet distribution as a prior was shown to lead to non - competitive cross - entropy results .
as a result the language modelling community seemed to have dismissed
123we can use average values of the parameters as returned by the hierarchical pitman - yor language model as well , the parameter
values are similar and does not affect our results .
bayesian methods as theoretically nice but impractical methods .
our model is a nontrivial but direct general - ization of the hierarchical dirichlet language model that gives state - of - the - art performance .
we have shown that with a suitable choice of priors ( namely the pitman - yor process ) , bayesian methods can be competi - tive with the best smoothing techniques .
in fact we have shown that one of the best smoothing techniques , namely interpolated kneser - ney , is a great approximation to a bayesian model .
the hierarchical pitman - yor process is a natural generalization of the recently proposed hierarchical dirichlet process ( teh et al .
the hierarchical dirichlet process was proposed to solve a clustering problem instead and it is interesting to note that such a direct generalization leads us to a well - established solution for a different problem , namely interpolated kneser - ney .
this indicates the naturalness of this class of models .
both the hierarchical dirichlet process and the hierarchical pitman - yor process are examples of bayesian nonparametric processes .
these have recently received much attention in the statistics and machine learning communities because they can relax previously strong assumptions on the parametric forms of bayesian models yet retain computational efciency , and because of the elegant way in which they handle the issues of model selection and structure learning in graphical models .
the hierarchical pitman - yor language model is only the rst step towards comprehensive bayesian solu - tions to many tasks in natural language processing .
we envision that a variety of more sophisticated models which make use of the hierarchical pitman - yor process can be built to solve many problems .
foremost in our agenda are extensions of the current model that achieve better cross - entropy for language modelling , and verifying experimentally that this translates into reduced word error rates for speech recognition .
i wish to thank the lee kuan yew endowment fund for funding , joshua goodman for answering many questions regarding interpolated kneser - ney and smoothing techniques , john blitzer and yoshua bengio for help with datasets and hal daume iii for comments on an earlier draft .
