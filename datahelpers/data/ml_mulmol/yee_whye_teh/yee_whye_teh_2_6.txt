the indian buffet process ( ibp ) is an exchangeable distribution over binary ma - trices used in bayesian nonparametric featural models .
in this paper we propose a three - parameter generalization of the ibp exhibiting power - law behavior .
we achieve this by generalizing the beta process ( the de finetti measure of the ibp ) to the stable - beta process and deriving the ibp corresponding to it .
we nd interest - ing relationships between the stable - beta process and the pitman - yor process ( an - other stochastic process used in bayesian nonparametric models with interesting power - law properties ) .
we derive a stick - breaking construction for the stable - beta process , and nd that our power - law ibp is a good model for word occurrences in
the indian buffet process ( ibp ) is an innitely exchangeable distribution over binary matrices with a nite number of rows and an unbounded number of columns ( 123 , 123 ) .
it has been proposed as a suitable prior for bayesian nonparametric featural models , where each object ( row ) is modeled with a potentially unbounded number of features ( columns ) .
applications of the ibp include bayesian nonparametric models for ica ( 123 ) , choice modeling ( 123 ) , similarity judgements modeling ( 123 ) , dyadic data modeling ( 123 ) and causal inference ( 123 ) .
in this paper we propose a three - parameter generalization of the ibp with power - law behavior .
using the usual analogy of customers entering an indian buffet restaurant and sequentially choosing dishes from an innitely long buffet counter , our generalization with parameters > 123 , c > and ( 123 , 123 ) is simply as follows :
customer 123 tries poisson ( ) dishes .
subsequently , customer n + 123 :
tries dish k with probability mk tries poisson ( ( 123+c ) ( n+c+ )
( n+123+c ) ( c+ ) ) new dishes .
n+c , for each dish that has previously been tried;
where mk is the number of previous customers who tried dish k .
the dishes and the customers correspond to the columns and the rows of the binary matrix respectively , with an entry of the matrix being one if the corresponding customer tried the dish ( and zero otherwise ) .
the mass parameter controls the total number of dishes tried by the customers , the concentration parameter c controls the number of customers that will try each dish , and the stability exponent controls the power - law behavior of the process .
when = 123 the process does not exhibit power - law behavior and reduces to the usual two - parameter ibp ( 123 ) .
many naturally occurring phenomena exhibit power - law behavior , and it has been argued that using models that can capture this behavior can improve learning ( 123 ) .
recent examples where this has led to signicant improvements include unsupervised morphology learning ( 123 ) , language modeling ( 123 )
and image segmentation ( 123 ) .
these examples are all based on the pitman - yor process ( 123 , 123 , 123 ) , a generalization of the dirichlet process ( 123 ) with power - law properties .
our generalization of the ibp extends the ability to model power - law behavior to featural models , and we expect it to lead to a wealth of novel applications not previously well handled by the ibp .
the approach we take in this paper is to rst dene the underlying de finetti measure , then to derive the conditional distributions of bernoulli process observations with the de finetti measure integrated out .
this automatically ensures that the resulting power - law ibp is innitely exchangeable .
we call the de finetti measure of the power - law ibp the stable - beta process .
it is a novel generalization of the beta process ( 123 ) ( which is the de finetti measure of the normal two - parameter ibp ( 123 ) ) with characteristics reminiscent of the stable process ( 123 , 123 ) ( in turn related to the pitman - yor process ) .
we will see that the stable - beta process has a number of properties similar to the pitman - yor process .
in the following section we rst give a brief description of completely random measures , a class of random measures which includes the stable - beta and the beta processes .
in section 123 we introduce the stable - beta process , a three parameter generalization of the beta process and derive the power - law ibp based on the stable - beta process .
based on the proposed model , in section 123 we construct a model of word occurrences in a document corpus .
we conclude with a discussion in section 123
123 completely random measures
in this section we give a brief description of completely random measures ( 123 ) .
let be a measure space with its - algebra .
a random variable whose values are measures on ( , ) is referred to as a random measure .
a completely random measure ( crm ) over ( , ) is a random mea - sure such that ( a ) ( b ) for all disjoint measurable subsets a , b .
that is , the ( random ) masses assigned to disjoint subsets are independent .
an important implication of this property is that the whole distribution over is determined ( with usually satised technical assumptions ) once the distributions of ( a ) are given for all a .
crms can always be decomposed into a sum of three independent parts : a ( non - random ) measure , an atomic measure with xed atoms but random masses , and an atomic measure with random atoms and masses .
crms in this paper will only contain the second and third components .
in this case we can write in the form ,
where uk , vl > 123 are the random masses , k are the xed atoms , l are the random atoms , and n , m n ( ) .
to describe fully it is sufcient to specify n and ( k ) , and to describe the joint distribution over the random variables ( uk ) , ( vl ) , ( l ) and m .
each uk has to be independent from everything else and has some distribution fk .
the random atoms and their weights ( vl , l ) are jointly drawn from a 123d poisson process over ( 123 , ) with some nonatomic rate measure called the levy measure .
the rate measure has to satisfy a number of technical properties; see ( 123 , ) ( du d ) = m < then the number of random atoms m in is poisson distributed with mean m , otherwise there are an innite number of random atoms .
if is described by and ( k , fk ) n
( 123 , 123 ) for details .
if ( cid : 123 )
k=123 as above , we write ,
123 the stable - beta process
in this section we introduce a novel crm called the stable - beta process ( sbp ) .
it has no xed atoms while its levy measure is dened over ( 123 , 123 ) :
( 123 + c )
123 ( du d ) =
where the parameters are : a mass parameter > 123 , a concentration parameter c > , a stability exponent 123 < 123 , and a smooth base distribution h .
the mass parameter controls the overall mass of the process and the base distribution gives the distribution over the random atom locations .
( 123 ) ( c + ) u123 ( 123 u ) c+123duh ( d )
the mean of the sbp can be shown to be e ( ( a ) ) = h ( a ) for each a , while var ( ( a ) ) = 123+c h ( a ) .
thus the concentration parameter and the stability exponent both affect the variability of the sbp around its mean .
the stability exponent also governs the power - law behavior of the sbp .
when = 123 the sbp does not have power - law behavior and reduces to a normal two - parameter beta process ( 123 , 123 ) .
when c = 123 the stable - beta process describes the random atoms with masses < 123 in a stable process ( 123 , 123 ) .
the sbp is so named as it can be seen as a generalization of both the stable and the beta processes .
both the concentration parameter and the stability exponent can be generalized to functions over though we will not deal with this generalization here .
123 posterior stable - beta process
consider the following hierarchical model :
iid , for i = 123 ,
the random measure is a sbp with no xed atoms and with levy measure ( 123 ) , while zi bernoullip ( ) is a bernoulli process with mean ( 123 ) .
this is also a crm : in a small neighborhood d around it has a probability ( d ) of having a unit mass atom in d; otherwise it does not have an atom in d .
if has an atom at the probability of zi having an atom at as well is ( ( ) ) .
if has a smooth component , say 123 , zi will have random atoms drawn from a poisson process with rate measure 123
in typical applications to featural models the atoms in zi give the features associated with data item i , while the weights of the atoms in give the prior probabilities of the corresponding features occurring in a data item .
we are interested in both the posterior of given z123 , .
, zn , as well as the conditional distribu - tion of zn+123|z123 , .
, zn with marginalized out .
let k be the k unique atoms among z123 , .
, zn with atom k occurring mk times .
theorem 123 of ( 123 ) shows that the posterior of given z123 , .
, zn is still a crm , but now including xed atoms given by k .
its updated levy measure and the distribution of the mass at each xed atom
, zn crm ( n , (
n ( du d ) =
( 123 + c )
( 123 ) ( c + ) u123 ( 123 u ) n+c+123duh ( d ) , ( mk ) ( n mk + c + ) umk123 ( 123 u ) nmk+c+123du .
( n + c )
intuitively , the posterior is obtained as follows .
firstly , the posterior of must be a crm since both the prior of and the likelihood of each zi| factorize over disjoint subsets of .
secondly , must have xed atoms at each k since otherwise the probability that there will be atoms among z123 , .
, zn at precisely k is zero .
the posterior mass at k is obtained by multiplying a bernoulli likelihood umk ( 123 u ) nmk ( since there are mk occurrences of the atom k among z123 , .
, zn ) to the prior 123 ( dud k ) in ( 123 ) and normalizing , giving us ( 123b ) .
finally , outside of these k atoms there are no other atoms among z123 , .
we can think of this as n observations of 123 among n iid bernoulli variables , so a likelihood of ( 123 u ) n is multiplied into 123 ( without normalization ) , giving the updated levy measure in ( 123a ) .
let us inspect the distributions ( 123 ) of the xed and random atoms in the posterior in turn .
the k has a distribution fnk which is simply a beta distribution with parameters ( mk random mass at , n mk + c + ) .
this differs from the usual beta process in the subtraction of from mk and addition of to n mk + c .
this is reminiscent of the pitman - yor generalization to the dirichlet process ( 123 , 123 , 123 ) , where a discount parameter is subtracted from the number of customers seated around each table , and added to the chance of sitting at a new table .
on the other hand , the levy measure of the random atoms of is still a levy measure corresponding to an sbp with updated
( 123 + c ) ( n + c + ) ( n + 123 + c ) ( c + ) ,
c ( cid : 123 ) c + n ,
note that the update depends only on n , not on z123 , .
in summary , the posterior of is simply an independent sum of an sbp with updated parameters and of xed atoms with beta distributed masses .
observe that the posterior is not itself a sbp .
in other words , the sbp is not conjugate to bernoulli process observations .
this is different from the beta process and again reminiscent of pitman - yor processes , where the posterior is also a sum of a pitman - yor process with updated parameters and xed atoms with random masses , but not a pitman - yor process ( 123 ) .
fortunately , the non - conjugacy of the sbp does not preclude efcient inference .
in the next subsections we de - scribe an indian buffet process and a stick - breaking construction corresponding to the sbp .
efcient inference techniques based on both representations for the beta process can be straightforwardly generalized to the sbp ( 123 , 123 , 123 ) .
123 the stable - beta indian buffet process
we can derive an indian buffet process ( ibp ) corresponding to the sbp by deriving , for each n , the distribution of zn+123 conditioned on z123 , .
, zn , with marginalized out .
this derivation is straightforward and follows closely that for the beta process ( 123 ) .
for each of the atoms posterior of (
k ) given z123 , .
, zn is beta distributed with mean mk
k ) = 123|z123 , .
, zn ) = e ( (
k ) |z123 , .
, zn ) = mk
metaphorically speaking , customer n + 123 tries dish k with probability mk atoms .
let \ (
in a small neighborhood d around , we have :
now for the random
p ( zn+123 ( d ) = 123|z123 , .
, zn ) = e ( ( d ) |z123 , .
, zn ) = ( 123 ) ( c + ) u123 ( 123 u ) n+c+123duh ( d ) ( 123 + c )
( 123 + c )
( 123 ) ( c + ) h ( d ) ( 123 + c ) ( n + c + ) ( n + 123 + c ) ( c + ) h ( d )
since zn+123 is completely random and h is smooth , the above shows that on \ ( 123 , .
, zn+123 is simply a poisson process with rate measure ( 123+c ) ( n+c+ ) ( n+123+c ) ( c+ ) h .
in particular , it will have ( n+123+c ) ( c+ ) ) new atoms , each independently and identically distributed according to h .
in the ibp metaphor , this corresponds to customer n+123 trying new dishes , with each dish associ - ated with a new draw from h .
the resulting indian buffet process is as described in the introduction .
it is automatically innitely exchangeable since it was derived from the conditional distributions of the hierarchical model ( 123 ) .
multiplying the conditional probabilities of each zn given previous ones together , we get the joint probability of z123 , .
, zn with marginalized out :
p ( z123 , .
, zn ) = exp
where there are k atoms ( dishes ) k among z123 , .
, zn with atom k appearing mk times , and h is the density of h .
( 123 ) is to be contrasted with ( 123 ) in ( 123 ) .
the kh ! terms in ( 123 ) are absent as we have to distinguish among these kh dishes in assigning each of them a distinct atom ( this also contributes the h ( k ) terms ) .
the fact that ( 123 ) is invariant to permuting the ordering among z123 , .
, zn also indicates the innite exchangeability of the stable - beta ibp .
123 stick - breaking constructions
in this section we describe stick - breaking constructions for the sbp generalizing those for the beta process .
the rst is based on the size - biased ordering of atoms induced by the ibp ( 123 ) , while
the second is based on the inverse levy measure method ( 123 ) , and produces a sequence of random atoms of strictly decreasing masses ( 123 ) .
the size - biased construction is straightforward : we use the ibp to generate the atoms ( dishes ) in the sbp; each time a dish is newly generated the atom is drawn from h and its mass from fnk .
this leads to the following procedure :
for n = 123 , 123 , .
. : for k = 123 ,
jn poisson ( ( 123+c ) ( n123+c+ ) vnk beta ( 123 , n 123 + c + ) ,
the inverse levy measure is a general method of generating from a poisson process with non - uniform rate measure .
it essentially transforms the poisson process into one with uniform rate , generates a sample , and transforms the sample back .
this method is more involved for the sbp because the inverse transform has no analytically tractable form .
the levy measure 123 of the sbp factorizes into a product 123 ( du d ) = l ( du ) h ( d ) of a - nite measure l ( du ) = ( 123 ) ( c+ ) u123 ( 123u ) c+123du over ( 123 , 123 ) and a probability measure h over .
this implies
that we can generate a sample ( vl , l ) l=123 of the random atoms of and their masses by rst sam - l=123 poissonp ( l ) from a poisson process on ( 123 , 123 ) with rate measure l , and pling the masses ( vl ) associating each vl with an iid draw l h ( 123 ) .
now consider the mapping t : ( 123 , 123 ) ( 123 , )
t ( u ) =
( 123 + c )
( 123 ) ( c + ) u123 ( 123 u ) c+123du .
l=123 poissonp ( l ) if and only if ( t ( vl ) )
t is bijective and monotonically decreasing .
the mapping theorem for poisson processes ( 123 ) shows that ( vl ) l=123 poissonp ( l ) where l is l=123 poissonp ( l ) can be easily drawn by letting lebesgue measure on ( 123 , ) .
a sample ( tl ) i=123 ei for all l .
transforming back with vl = t 123 ( tl ) , l=123 poissonp ( l ) .
as t123 , t123 , .
is an increasing sequence and t is decreasing , we have ( vl ) v123 , v123 , .
is a decreasing sequence of masses .
deriving the density of vl given vl123 , we get :
el exponential ( 123 ) and setting tl = ( cid : 123 ) l p ( vl|vl123 ) = ( cid : 123 ) ( cid : 123 ) dtl
in general these densities do not simplify and we have to resort to solving for t 123 ( tl ) numerically .
there are two cases for which they do simplify .
for c = 123 , = 123 , the density function reduces to p ( vl|vl123 ) = v123 l123 , leading to the stick - breaking construction of the single parameter ibp ( 123 ) .
in the stable process case when c = 123 and ( cid : 123 ) = 123 , the density of vl simplies to :
p ( vl | vl123 ) = ( 123 )
= ( 123 ) v123
doing a change of values to yl = v
, we get : p ( yl|yl123 ) = 123
( cid : 123 ) ( cid : 123 ) vl123
that is , each yl is exponentially distributed with rate 123 and offset by yl123
for general values of the parameters we do not have an analytic stick breaking form .
however note that the weights generated using this method are still going to be strictly decreasing .
123 power - law properties
the sbp has a number of appealing power - law properties .
in this section we shall assume > 123 since the case = 123 reduces the sbp to the usual beta process with less interesting power - law properties .
derivations are given in the appendix .
figure 123 : power - law properties of the stable - beta indian buffet process .
firstly , the total number of dishes tried by n customers is o ( n ) .
the left panel of figure 123 shows this for varying .
secondly , the number of customers trying each dish follows a zipfs law ( 123 ) .
this is shown in the right panel of figure 123 , which plots the number of dishes km versus the number of customers m trying each dish ( that is , km is the number of dishes k for which mk = m ) .
asymptotically we can show that the proportion of dishes tried by m customers is o ( m123 ) .
note that these power - laws are similar to those observed for pitman - yor processes .
one aspect of the sbp which is not power - law is the number of dishes each customer tries .
this is simply poisson ( ) distributed .
it seems difcult obtain power - law behavior in this aspect within a crm framework , because of the fundamental role played by the poisson process .
123 word occurrence models with stable - beta processes
in this section we use the sbp as a model for word occurrences in document corpora .
let n be the number of documents in a corpus .
let zi ( ( ) ) = 123 if word type occurs in document i and 123 otherwise , and let ( ( ) ) be the occurrence probability of word type among the documents in the corpus .
we use the hierarchical model ( 123 ) with a sbp prior123 on and with each document modeled as a conditionally independent bernoulli process draw .
the joint distribution over the word occurrences z123 , .
, zn , with integrated out , is given by the ibp joint probability ( 123 ) .
we applied the word occurrence model to the 123newsgroups dataset .
following ( 123 ) , we modeled the training documents in each of the 123 newsgroups as a separate corpus with a separate sbp .
we use the popularity of each word type across all 123 newsgroups as the base distribution123 : for each word type let n be the number of documents containing and let h ( ( ) ) n .
in the rst experiment we compared the sbp to the beta process by tting the parameters , c and of both models to each newsgroup by maximum likelihood ( in beta process case is xed at 123 ) .
we expect the sbp to perform better as it is better able to capture the power - law statistics of the document corpora ( see figure 123 ) .
the ml values of the parameters across classes did not vary much , taking values = 123 123 , c = 123 123 and = 123 123 .
in comparison , the parameters values obtained by the beta process are = 123 123 and c = 123 123 .
note that the estimated values for c are signicantly larger than for the sbp to allow the beta process to model the fact that many words occur in a small number of documents ( a consequence of the power - law
123words are discrete objects .
to get a smooth base distribution we imagine appending each word type with
a u ( 123 , 123 ) variate .
this does not affect the modelling that follows .
123the appropriate technique , as proposed by ( 123 ) , would be to use a hierarchical sbp to tie the word occur - rence probabilities across the newsgroups .
however due to difculties dealing with atomic base distributions we cannot dene a hierarchical sbp easily ( see discussion ) .
123number of customersmean number of dishes tried ! =123 , c=123 " =123 " =123 " =123 " =123number of customers trying each dishnumber of dishes ! =123 , c=123 , " =123 figure 123 : power - law properties of the 123newsgroups dataset .
the faint dashed lines are the distribu - tions of words in the documents in each class , the solid curve is the mean of these lines .
the dashed lines are the means of the word distributions generated by the ml parameters for the beta process ( pink ) and the sbp ( green ) .
table 123 : classication performance of sbp and beta process ( bp ) .
the jth column ( denoted 123 : j ) shows the cumulative rank j classication accuracy of the test documents .
the three numbers after the models are the percentages of training , validation and test sets respectively .
assigned to classes :
bp - 123 / 123 / 123 sbp - 123 / 123 / 123 bp - 123 / 123 / 123 sbp - 123 / 123 / 123
statistics of word occurrences; see figure 123 ) .
we also plotted the characteristics of data simulated from the models using the estimated ml parameters .
the sbp has a much better t than the beta process to the power - law properties of the corpora .
in the second experiment we tested the two models on categorizing test documents into one of the 123 newsgroups .
since this is a discriminative task , we optimized the parameters in both models to maximize the cumulative ranked classication performance .
the rank j classication performance is dened to be the percentage of documents where the true label is among the top j predicted classes ( as determined by the ibp conditional probabilities of the documents under each of the 123 newsgroup classes ) .
as the cost function is not differentiable , we did a grid search over the parameter space , using 123 values of , c and each , and found the parameters maximizing the objective function on a validation set separate from the test set .
to see the effect of sample size on model performance we tried splitting the documents in each newsgroup into 123% training , 123% validation and 123% test sets , and into 123% training , 123% validation and 123% test sets .
we repeated the experiment ve times with different random splits of the dataset .
the ranked classication rates are shown in table 123
figure 123 shows that the sbp model has generally higher classication performances than the beta process .
we have introduced a novel stochastic process called the stable - beta process .
the stable - beta process is a generalization of the beta process , and can be used in nonparametric bayesian featural models with an unbounded number of features .
as opposed to the beta process , the stable - beta process has a number of appealing power - law properties .
we developed both an indian buffet process and a stick - breaking construction for the stable - beta process and applied it to modeling word occurrences in document corpora .
we expect the stable - beta process to nd uses modeling a range of natural phenomena with power - law properties .
123number of documentscumulative number of words bpsbpdata123number of documents per wordnumber of words bpsbpdata figure 123 : differences between the classication rates of the sbp and the beta process .
the perfor - mance of the sbp was consistently higher than that of the beta process for each of the ve runs .
we derived the stable - beta process as a completely random measure with levy measure ( 123 ) .
would be interesting and illuminating to try to derive it as an innite limit of nite models , however we were not able to do so in our initial attempts .
a related question is whether there is a natural denition of the stable - beta process for non - smooth base distributions .
until this is resolved in the positive , we are not able to dene hierarchical stable - beta processes generalizing the hierarchical beta processes ( 123 ) .
another avenue of research we are currently pursuing is in deriving better stick - breaking construc - tions for the stable - beta process .
the current construction requires inverting the integral ( 123 ) , which is expensive as it requires an iterative method which evaluates the integral numerically within each
we thank the gatsby charitable foundation for funding , romain thibaux , peter latham and tom grifths for interesting discussions , and the anonymous reviewers for help and feedback .
a derivation of power - law properties we will make large n and k assumptions here , and make use of stirlings approximation ( n+123 )
123n ( n / e ) n , which is accurate in the larger n regime .
the expected number of dishes is ,
= o ( n )
e+123 ( 123 + 123
we are interested in the joint distribution of the statistics ( k123 , .
, kn ) , where km is the number of dishes tried by exactly m customers and where there are a total of n customers in the restaurant .
as there are ( k123 , .
, kn ) , we have ( ignoring constant terms and collecting terms in ( 123 ) with mk = m ) ,
i+c ) i+c ( i + c + 123 ) 123 m=123 km as well , we see that ( k123 , .
, kn ) is multinomial with the prob - ability of a dish having m customers being proportional to the term in large parentheses .
for large m ( and even larger n ) , this probability simplies to ,
( cid : 123 ) km congurations of the ibp with the same statistics
p ( k123 , .
, kn|n )
conditioning on k = ( cid : 123 ) n
= o ( cid : 123 ) m123 ( cid : 123 ) .
( m+123 ) ) = o
