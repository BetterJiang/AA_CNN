bayesian nonparametric models
peter orbanz , cambridge university
yee whye teh , university college london
related keywords : bayesian methods , prior probabilities , dirichlet process ,
a bayesian nonparametric model is a bayesian model on an in ? nite - dimensional parameter space .
the parameter space is typically chosen as the set of all possi - ble solutions for a given learning problem .
for example , in a regression problem the parameter space can be the set of continuous functions , and in a density esti - mation problem the space can consist of all densities .
a bayesian nonparametric model uses only a ? nite subset of the available parameter dimensions to explain a ? nite sample of observations , with the set of dimensions chosen depending on the sample , such that the e ? ective complexity of the model ( as measured by the number of dimensions used ) adapts to the data .
classical adaptive problems , such as nonparametric estimation and model selection , can thus be formulated as bayesian inference problems .
popular examples of bayesian nonparametric models include gaussian process regression , in which the correlation structure is re ? ned with growing sample size , and dirichlet process mixture models for clustering , which adapt the number of clusters to the complexity of the data .
bayesian nonparametric models have recently been applied to a variety of ma - chine learning problems , including regression , classi ? cation , clustering , latent variable modeling , sequential modeling , image segmentation , source separation and grammar induction .
motivation and background
most of machine learning is concerned with learning an appropriate set of pa - rameters within a model class from training data .
the meta level problems of determining appropriate model classes are referred to as model selection or model adaptation .
these constitute important concerns for machine learning practitioners , chie ? y for avoidance of over - ? tting and under - ? tting , but also for discovery of the causes and structures underlying data .
examples of model se - lection and adaptation include : selecting the number of clusters in a clustering problem , the number of hidden states in a hidden markov model , the number
of latent variables in a latent variable model , or the complexity of features used in nonlinear regression .
nonparametric models constitute an approach to model selection and adap - tation , where the sizes of models are allowed to grow with data size .
this is as opposed to parametric models which uses a ? xed number of parameters .
for example , a parametric approach to density estimation would be to ? t a gaus - sian or a mixture of a ? xed number of gaussians by maximum likelihood .
a nonparametric approach would be a parzen window estimator , which centers a gaussian at each observation ( and hence uses one mean parameter per observa - tion ) .
another example is the support vector machine with a gaussian kernel .
the representer theorem shows that the decision function is a linear combina - tion of gaussian radial basis functions centered at every input vector , and thus has a complexity that grows with more observations .
nonparametric methods have long been popular in classical ( non - bayesian ) statistics ( 123 ) .
they often per - form impressively in applications and , though theoretical results for such models are typically harder to prove than for parametric models , appealing theoretical properties have been established for a wide range of models .
bayesian nonparametric methods provide a bayesian framework for model selection and adaptation using nonparametric models .
a bayesian formulation of nonparametric problems is nontrivial , since a bayesian model de ? nes prior and posterior distributions on a single ? xed parameter space , but the dimen - sion of the parameter space in a nonparametric approach should change with sample size .
the bayesian nonparametric solution to this problem is to use an in ? nite - dimensional parameter space , and to invoke only a ? nite subset of the available parameters on any given ? nite data set .
this subset generally grows with the data set .
in the context of bayesian nonparametric models , ※in ? nite - dimensional§ can therefore be interpreted as ※of ? nite but unbounded dimension§ .
more precisely , a bayesian nonparametric model is a model that ( 123 ) constitutes a bayesian model on an in ? nite - dimensional parameter space and ( 123 ) can be evaluated on a ? nite sample in a manner that uses only a ? nite subset of the available parameters to explain the sample .
we make the above description more concrete in the next section when we describe a number of standard machine learning problems and the correspond - ing bayesian nonparametric solutions .
as we will see , the parameter space in ( 123 ) typically consists of functions or of measures , while ( 123 ) is usually achieved by marginalizing out surplus dimensions over the prior .
random functions and measures , and more generally probability distributions on in ? nite - dimensional random objects , are called stochastic processes; examples we will encounter include gaussian processes , dirichlet processes and beta processes .
bayesian nonparametric models are often named after the stochastic processes they con - tain .
the examples are then followed by theoretical considerations , including formal constructions and representations of the stochastic processes used in bayesian nonparametric models , exchangeability , and issues of consistency and convergence rate .
we conclude this article with future directions and a reading
density function over data items x of the form p ( x ) = ( cid : 123 ) k p ( x ) = ( cid : 123 ) p ( x|西 ) g ( 西 ) d西 , where g = ( cid : 123 ) k
clustering with mixture models .
bayesian nonparametric generalizations of ? nite mixture models provide an approach for estimating both the number of components in a mixture model and the parameters of the individual mix - ture components simultaneously from data .
finite mixture models de ? ne a k=123 佞kp ( x|西k ) , where 佞k is the mixing proportion and 西k are parameters associated with compo - nent k .
the density can be written in a non - standard manner as an integral : k=123 佞k虫西k is a discrete mixing distribution encapsulating all the parameters of the mixture model and 虫西 is a dirac dis - tribution ( atom ) centered at 西 .
bayesian nonparametric mixtures use mixing distributions consisting of a countably in ? nite number of atoms instead :
this gives rise to mixture models with an in ? nite number of components .
when applied to a ? nite training set , only a ? nite ( but varying ) number of components will be used to model the data , since each data item is associated with exactly one component but each component can be associated with multiple data items .
inference in the model then automatically recovers both the number of compo - nents to use and the parameters of the components .
being bayesian , we need a prior over the mixing distribution g , and the most common prior to use is a dirichlet process ( dp ) .
the resulting mixture model is called a dp mixture .
formally , a dirichlet process dp ( 艮 , h ) parametrized by a concentration paramter 艮 > 123 and a base distribution h is a prior over distributions ( probabil - ity measures ) g such that , for any ? nite partition a123 , .
, am of the parameter space , the induced random vector ( g ( a123 ) , .
, g ( am ) ) is dirichlet distributed with parameters ( 艮h ( a123 ) , .
, 艮h ( am ) ) ( see the theory section for a discussion of subtleties involved in this de ? nition ) .
it can be shown that draws from a dp will be discrete distributions as given in ( 123 ) .
the dp also induces a distribution over partitions of integers called the chinese restaurant process ( crp ) , which directly describes the prior over how data items are clustered under the dp mixture .
for more details on the dp and the crp , see dp entry ( ? ) .
nonlinear regression .
the aim of regression is to infer a continuous function from a training set consisting of input - output pairs ( ( ti , xi ) ) n approaches parametrize the function using a ? nite number of parameters and attempt to infer these parameters from data .
the prototypical bayesian non - parametric approach to this problem is to de ? ne a prior distribution over con - tinuous functions directly by means of a gaussian process ( gp ) .
as explained in gp entry ( ? ) , a gp is a distribution on an in ? nite collection of random variables xt , such that the joint distribution of each ? nite subset xt123 , .
, xtm is a mul - tivariate gaussian .
a value xt taken by the variable xt can be regarded as the value of a continuous function f at t , that is , f ( t ) = xt .
given the training set , the gaussian process posterior is again a distribution on functions , conditional
on these functions taking values f ( t123 ) = x123 , .
, f ( tn ) = xn .
latent feature models .
latent feature models represent a set of objects in terms of a set of latent features , each of which represents an independent degree of variation exhibited by the data .
such a representation of data is sometimes referred to as a distributed representation .
in analogy to nonparametric mix - ture models with an unknown number of clusters , a bayesian nonparametric approach to latent feature modeling allows for an unknown number of latent features .
the stochastic processes involved here are known as the indian bu ? et process ( ibp ) and the beta process ( bp ) .
draws from bps are random discrete measures , where each of an in ? nite number of atoms has a mass in ( 123 , 123 ) but the masses of atoms need not sum to 123
each atom corresponds to a feature , with the mass corresponding to the probability that the feature is present for an object .
we can visualize the occurrences of features among objects using a binary matrix , where the ( i , k ) entry is 123 if object i has feature k and 123 oth - erwise .
the distribution over binary matrices induced by the bp is called the hidden markov models .
hidden markov models ( hmms ) are popular mod - els for sequential or temporal data , where each time step is associate with a state , with state transitions dependent on the previous state .
an in ? nite hmm is a bayesian nonparametric approach to hmms , where the number of states is unbounded and allowed to grow with the sequence length .
it is de ? ned using one dp prior for the transition probabilities going out from each state .
to en - sure that the set of states reachable from each outgoing state is the same , the base distributions of the dps are shared and given a dp prior recursively .
the construction is called a hierarchical dirichlet process ( hdp ) ; see below .
density estimation .
a nonparametric bayesian approach to density estima - tion requires a prior on densities or distributions .
however , the dp is not useful in this context , since it generates discrete distributions .
a useful density estima - tor should smooth the empirical density ( such as a parzen window estimator ) , which requires a prior that can generate smooth distributions .
priors applicable in density estimation problems include dp mixture models and p∩olya trees .
dp mixture models : since the mixing distribution in the dp mixture is random , the induced density p ( x ) is random thus the dp mixture can be used as a prior over densities .
despite the fact that these are now primarily used in machine learning as clustering models , they were in fact originally proposed for
p∩olya trees are priors on probability distributions that can generate both discrete and piecewise continuous distributions , depending on the choice of pa - rameters .
p∩olya trees are de ? ned by a recursive in ? nitely deep binary subdi - vision of the domain of the generated random measure .
each subdivision is associated with a beta random variable which describes the relative amount of mass on each side of the subdivision .
the dp is a special case of a p∩olya tree corresponding to a particular parametrization .
for other parametrizations the resulting random distribution can be smooth so is suitable for density estima - power - law phenomena .
many naturally occurring phenomena exhibit power -
law behavior .
examples include natural languages , images and social and ge - netic networks .
an interesting generalization of the dp , called the pitman - yor process , pyp ( 艮 , d , h ) , has recently been successfully used as models of such power - law data .
the pitman - yor process augments the dp by a third parame - ter d ㏒ ( 123 , 123 ) .
when d = 123 the pyp is a dp ( 艮 , h ) , while when 艮 = 123 it is a so called normalized stable process .
sequential modeling .
hmms model sequential data using latent variables representing the underlying state of the system , and assuming that each state only depends on the previous state ( the so called markov property ) .
in some applications , for example language modeling and text compression , we are inter - ested in directly modeling sequences without using latent variables , and without making any markov assumptions , i . e .
modeling each observation conditional on all previous observations in the sequence .
since the set of potential sequences of previous observations is unbounded , this calls for nonparametric models .
a hierarchical pitman - yor process can be used to construct a bayesian nonpara - metric solution whereby the conditional probabilities are coupled hierarchically .
dependent and hierarchical models .
most of the bayesian nonparametric models described above are applied in settings where observations are homo - geneous or exchangeable .
in many real world settings observations are often not homogeneous , in fact they are often structured in interesting ways .
for example , the data generating process might change over time thus observations at di ? erent times are not exchangeable , or observations might come in distinct groups with those in the same group being more similar than across groups .
signi ? cant recent e ? orts in bayesian nonparametrics research have been placed in developing extensions that can handle these non - homogeneous set - tings .
dependent dirichlet processes are stochastic processes , typically over a spatial or temporal domain , which de ? ne a dirichlet process ( or a related ran - dom measure ) at each point with neighboring dps being more dependent .
these are used for spatial modeling , nonparametric regression , as well as for modeling temporal changes .
alternatively , hierarchical bayesian nonparametric models like the hierarchical dp aim to couple multiple bayesian nonparametric mod - els within a hierarchical bayesian framework .
the idea is to allow sharing of statistical strength across multiple groups of observations .
among other appli - cations , these have been used in the in ? nite hmm , topic modeling , language modeling , word segmentation , image segmentation and grammar induction .
for an overview of various dependent bayesian nonparametric models and their ap - plications in biostatistics please consult ( 123 ) .
( 123 ) is an overview of hierarchical bayesian nonparametric models as well as a variety of applications in machine
as we saw in the preceding examples , bayesian nonparametric models often make use of priors over functions and measures .
because these spaces typically have an uncountable number of dimensions , extra care has to be taken to de ? ne
the priors properly and to study the asymptotic properties of estimation in the in this section we give an overview of the basic concepts involved in the theory of bayesian nonparametric models .
we start with a discussion of the importance of exchangeability in bayesian parametric and nonparametric statistics .
this is followed by representations of the priors and issues of convergence .
the underlying assumption of all bayesian methods is that the parameter spec - ifying the observation model is a random variable .
this assumption is subject to much criticism , and at the heart of the bayesian versus non - bayesian debate that has long divided the statistics community .
however , there is a very general type of observations for which the existence of such a random variable can be derived mathematically : for so - called exchangeable observations , the bayesian assumption that a randomly distributed parameter exists is not a modeling assumption , but a mathematical consequence of the data＊s properties .
formally , a sequence of variables x123 , x123 , .
, xn over the same probability space ( x , ? ) is exchangeable if their joint distribution is invariant to permuting the variables .
that is , if p is the joint distribution and 佛 any permutation of ( 123 , .
, n ) , then p ( x123 = x123 , x123 = x123 .
xn = xn ) = p ( x123 = x佛 ( 123 ) , x123 = x佛 ( 123 ) .
xn = x佛 ( n ) ) ( 123 )
an in ? nite sequence x123 , x123 , .
is in ? nitely exchangeable if x123 , .
, xn is ex - changeable for every n ∣ 123
in this paper we will mean in ? nite exchangeability whenever we write exchangeability .
exchangeability re ? ects the assumption that the variables do not depend on their indices although they may be depen - dent among themselves .
this is typically a reasonable assumption in machine learning and statistical applications , even if the variables are not themselves iid ( independently and identically distributed ) .
exchangeability is a much weaker assumption than iid; iid variables are automatically exchangeable .
if 西 parametrizes the underlying distribution , and one assumes a prior dis - tribution over 西 , then the resulting marginal distribution over x123 , x123 , .
with 西 marginalized out will still be exchangeable .
a fundamental result credited to de finetti ( 123 ) states that the converse is also true .
that is , if x123 , x123 , .
is ( in ? nitely ) exchangeable , then there is a random 西 such that :
p ( x123 , .
, xn ) =
for every n ∣ 123
in other words , the seemingly innocuous assumption of ex - changeability automatically implies the existence of a hierarchical bayesian model with 西 being the random latent parameter .
this the crux of the fun - damental importance of exchangeability to bayesian statistics .
in de finetti＊s theorem it is important to stress that 西 can be in ? nite dimen - sional ( it is typically a random measure ) , thus the hierarchical bayesian model
( 123 ) is typically a nonparametric one .
for example , the blackwell - macqueen urn scheme ( related to the crp ) is exchangeable thus implicitly de ? nes a ran - dom measure , namely the dp ( see the dp entry ( ? ) for more details ) .
in this sense , we will see below that de finetti＊s theorem is an alternative route to kol - mogorov＊s extension theorem , which implicitly de ? nes the stochastic processes underlying bayesian nonparametric models .
in ? nite dimensions , a probability model is usually de ? ned by a density function or probability mass function .
in in ? nite - dimensional spaces , this approach is not generally feasible , for reasons explained below .
to de ? ne or work with a bayesian nonparametric model , we have to choose alternative mathematical weak distributions .
a weak distribution is a representation for the dis - tribution of a stochastic process , that is , for a probability distribution on an in ? nite - dimensional sample space .
if we assume that the dimensions of the space are indexed by t ㏒ t , the stochastic process can be regarded as the joint distribution p of an in ? nite set of random variables ( xt ) t㏒t .
for any ? nite subset s ? t of dimensions , the joint distribution ps of the corresponding subset ( xt ) t㏒s of random variables is a ? nite - dimensional marginal of p .
the weak distribution of a stochastic process is the set of all its ? nite - dimensional marginals , that is , the set ( ps : s ? t , |s| < ／ ) .
for example , the customary de ? nition of the gaussian process as an in ? nite collection of random variables , each ? nite subset of which has a joint gaussian distribution , is an example of a weak distribution representation .
in contrast to the explicit representations to be described below , this representation is generally not generative , because it represents the distribution rather than a random draw , but is more widely
apparently , just de ? ning a weak distribution in this manner need not im - ply that it is a valid representation of a stochastic process .
a given collection of ? nite - dimensional distributions represents a stochastic process only ( 123 ) if a process with these distributions as its marginals actually exists , and ( 123 ) if it is uniquely de ? ned by the marginals .
the mathematical result which guarantees that weak distribution representations are valid is the kolmogorov extension theorem ( also known as the daniell - kolmogorov theorem or the kolmogorov consistency theorem ) .
suppose that a collection ( ps : s ? t , |s| < ／ ) of distributions is given .
if all distributions in the collection are marginals of each other , that is , if ps123 is a marginal of ps123 whenever s123 ? s123 , the set of distribu - tions is called a projective family .
the kolmogorov extension theorem states that , if the set t is countable , and if the distributions ps form a projective family , then there exists a uniquely de ? ned stochastic process with the collec - tion ( ps ) as its marginal distributions .
in other words , any projective family for a countable set t of dimensions is the weak distribution of a stochastic pro - cess .
conversely , any stochastic process can be represented in this manner , by computing its set of ? nite - dimensional marginals .
the weak distribution representation assumes that all individual random variable xt of the stochastic process take values in the same sample space ? .
the stochastic process p de ? ned by the weak distribution is then a probability distribution on the sample space ? t , which can be interpreted as the set of all function f : t ↙ ? .
for example , to construct a gp we might choose t = q and ? = r to obtain real - valued functions on the countable space of rational numbers .
since q is dense in r , the function f can then be extended to all of r by continuity .
to de ? ne the dp as a distribution over probability measures on r , we note that a probability measure is a set function that maps ※random events§ , i . e .
elements of the borel 佛 - algebra b ( r ) of r , into probabilities in ( 123 , 123 ) .
we could therefore choose a weak distribution consisting of dirichlet distributions , and set t = b ( r ) and ? = ( 123 , 123 ) .
however , this approach raises a new problem because the set b ( r ) is not countable .
as in the gp , we can ? rst de ? ne the dp on a countable ※base§ for b ( r ) then extend to all random events by continuity of measures .
more precise descriptions are unfortunately beyond the scope of this entry .
explicit representations .
explicit representations directly describe a ran - dom draw from a stochastic process , rather than describing its distribution .
a prominent example of an explicit representation is the so - called stick - breaking representation of the dirichlet process . the discrete random measure g in ( 123 ) is completely determined by the two in ? nite sequences ( 佞k ) k㏒n and ( 西k ) k㏒n .
the stick - breaking representation of the dp generates these two sequences by drawing 西k ‵ h iid and vk ‵ beta ( 123 , 艮 ) for k = 123 , 123 , .
the coe ? cients 佞k are j=123 ( 123 ? vk ) .
the measure g so obtained can be then computed as 佞k = vk shown to be distributed according to a dp ( 艮 , g123 ) .
similar representations can be derived for the pitman - yor process and the beta process as well .
explicit representations , if they exist for a given model , are typically of great practical importance for the derivation of algorithms .
implicit representations .
a third representation of in ? nite dimensional models is based on de finetti＊s theorem .
any exchangeable sequence x123 , .
, xn uniquely de ? nes a stochastic process 西 , called the de finetti measure , making the xi＊s iid .
if the xi＊s are su ? cient to de ? ne the rest of the model and their conditional distributions are easily speci ? ed , then it is su ? cient to work directly with the xi＊s and have the underlying stochastic process implicitly de ? ned .
ex - amples include the chinese restaurant process ( an exchangeable distribution over partitions ) with the dp as the de finetti measure , and the indian bu ? et process ( an exchangeable distribution over binary matrices ) with the bp being the corresponding de finetti measure .
these implicit representations are useful in practice as they can lead to simple and e ? cient inference algorithms .
finite representations .
a fourth representation of bayesian nonparametric models is as the in ? nite limit of ? nite ( parametric ) bayesian models .
for exam - ple , dp mixtures can be derived as the in ? nite limit of ? nite mixture models with particular dirichlet priors on mixing proportions , gps can be derived as the in ? nite limit of particular bayesian regression models with gaussian priors , while bps can be derived as from the limit of an in ? nite number of indepen -
dent beta variables .
these representations are sometimes more intuitive for practitioners familiar with parametric models .
however not all bayesian non - parametric models can be expressed in this fashion , and they do not necessarily make clear the mathematical subtleties involved .
consistency and convergence rates
recent work in mathematical statistics examines the convergence properties of bayesian nonparametric models , and in particular the questions of consistency and convergence rates .
intuitively , a consistent estimation procedure is a method that will result in a correct estimate if it has access to an in ? nite amount of data , that is , a procedure that will be accurate unless it is hampered by insu ? cient sample size .
in bayesian statistics , an estimate is a distribution over possible parameter values ( the posterior ) , and hence consistency in bayesian models is de ? ned by demanding that the posterior converges to a delta peak at the true parameter .
a classic theorem by j .
doob shows that for any bayesian model , the set of all possible true parameter values for which the model will be consistent has probability one under the prior .
in other words , if the true parameter ( and hence the data distribution ) is chosen at random from the prior , we will always end up with a consistent model , and a bayesian who is certain about the prior need not worry about inconsistency .
this result does not hold anymore if the true parameter value is not in the domain of the prior .
in nonparametric mod - els , which have to spread out their probability mass over an in ? nite - dimensional space , this can result in seemingly unreasonable behavior of the model .
for ex - ample , the dirichlet process may be used as a prior in density estimation .
its support consists of the discrete distributions , which means that if the data is drawn from any smooth distribution , the true model is not in the support of the prior ( so doob＊s theorem does not apply ) .
as shown by diaconis and freedman ( 123 ) , the posterior will not necessarily concentrate in a region close to the true model .
intuitively , the implication is that we cannot generally assume in non - parametric models that the e ? ect of the prior will eventually become negligible if only we see enough data .
however , this does not mean that nonparametric bayesian models are generally inconsistent : a large and growing literature in mathematical statistics shows that consistency can be guaranteed by proper choice of an adequate nonparametric model ( 123 ) .
recent results , notably by van der vaart and ghosal , apply modern meth - ods of mathematical statistics to study the convergence properties of bayesian nonparametric models ( see ( 123 ) for further references ) .
consistency has been es - tablished for a number of models , including gaussian processes and dirichlet process mixtures .
a particularly interesting aspect of this line of work are re - sults on convergence rates , which specify how rapidly the posterior concentrates with growing sample size , depending on the complexity of the model and on how much probability mass the prior places around the true solution .
to make such results quantitative requires a measure for the complexity of a bayesian non - parametric model .
this is done by means of complexity measures developed in
empirical process theory and statistical learning theory , such as metric entropies , covering numbers and bracketing , some of which are well - known in theoretical machine learning .
examples of such results include the consistency of dirichlet process mixture models for density estimation if both the target density and the parametric mixture components are smooth , and a range of consistency results for regression and density estimation with gaussian processes .
for all of these results , convergence rates can be speci ? ed as well ( references are given in ( 123 ) ) .
a large class of in ? nite - dimensional models which do behave well even if the true parameter is not in the domain of the prior is identi ? ed in ( 123 ) .
in this case , the posterior will concentrate in the region of the prior support which is closest to the true parameter in a kullback - leibler sense .
there are two aspects to inference in bayesian nonparametric models : the ana - lytic tractability of posteriors for the stochastic processes embedded in bayesian nonparametric models , and practical inference algorithms for the overall mod - els .
bayesian nonparametric models typically include stochastic processes such as the gaussian process and the dirichlet process .
these processes have an in ? nite number of dimensions thus na“ ? ve algorithmic approaches to computing posteriors is generally infeasible .
fortunately , these processes typically have analytically tractable posteriors , so all but ? nitely many of the dimensions can be analytically integrated out e ? ciently .
the remaining dimensions , along with the parametric parts of the models , can then be handled by the usual infer - ence techniques employed in parametric bayesian modeling , including markov chain monte carlo , sequential monte carlo , variational inference , and message - passing algorithms like expectation propagation .
the precise choice of approx - imations to use will depend on the speci ? c models under consideration , with speed / accuracy trade - o ? s between di ? erent techniques generally following those for parametric models .
in the following , we will given two examples to illus - trate the above points , and discuss a few theoretical issues associated with the analytic tractability of stochastic processes .
in gaussian process regression , we model the relationship between an input x and an output y using a function f , so that y ‵ f ( x ) +  where  is iid gaussian noise .
given a gp prior over f and a ? nite training data set ( ( xi , yi ) ) n wish to compute the posterior over f .
here we can use the weak representation of f and note that ( f ( xi ) ) n i=123 is simply a ? nite - dimensional gaussian with mean and covariance given by the mean and covariance functions of the gp .
inference for ( f ( xi ) ) n i=123 is then straightforward .
the approach can be thought of equivalently as marginalizing out the whole function except its values on the training inputs .
note that although we only have the posterior over ( f ( xi ) ) n this is su ? cient to reconstruct the function evaluated at any other point x123 ( say
i=123 given ( f ( xi ) ) n
the test input ) , since f ( x123 ) is gaussian and independent of the training data i=123
in gp regression the posterior over ( f ( xi ) ) n can be computed exactly .
in gp classi ? cation or other regression settings with nonlinear likelihood functions , the typical approach is to use sparse methods based on variational approximations or expectation propagation; see gp entry ( ? ) for details .
our second example involves dirichlet process mixture models .
recall that the dp induces a clustering structure on the data items .
if our training set consists of n data items , since each item can only belong to one cluster , there are at most n clusters represented in the training set .
even though the dp mixture itself has an in ? nite number of potential clusters , all but ? nitely many of these are not associated with data , thus the associated variables need not be explicitly represented at all .
this can be understood either as marginalizing out these variables , or as an implicit representation which can be made explicit whenever required by sampling from the prior .
this idea is applicable for dp mixtures using both the chinese restaurant process and the stick - breaking rep - resentations .
in the crp representation , each data item xi is associated with a cluster index zi , and each cluster k with a parameter 西 ? k ( these parameters can be marginalized out if h is conjugate to f ) , and these are the only latent vari - ables that need be represented in memory .
in the stick - breaking representation , clusters are ordered by decreasing prior expected size , with cluster k associated with a parameter 西 ? k and a size 佞k .
each data item is again associated with a cluster index zi , and only the clusters up to k = max ( z123 , .
, zn ) need be represented .
all clusters with index > k need not be represented since their posterior conditioning on ( ( xi , zi ) ) n
i=123 is just the prior .
on bayes equations and conjugacy
it is worth noting that the posterior of a bayesian model is , in abstract terms , de ? ned as the conditional distribution of the parameter given the data and the hyperparameters , and this de ? nition does not require the existence of a if a bayes equation exists for the model , the posterior can equivalently be de ? ned as the left - hand side of the bayes equation .
however for some stochastic processes , notably the dp on an uncountable space such as r , it is not possible to de ? ne a bayes equation even though the posterior is still a well - de ? ned mathematical object .
technically speaking , existence of a bayes equation requires the family of all possible posteriors to be dominated by the prior , but this is not the case for the dp .
that posteriors of these stochastic processes can be evaluated at all is solely due to the fact that they admit an
the particular form of tractability exhibited by many stochastic processes in the literature is that of a conjugate posterior , that is , the posterior belongs to the same model family as the prior , and the posterior parameters can be computed as a function of the prior hyperparameters and the observed data .
for example , the posterior of a dp ( 艮 , g123 ) under observations 西123 , .
, 西n is again a dirichlet process , dp ( 艮 + n ,
艮+n ( 艮g123 + ( cid : 123 ) 虫西i ) ) .
similarly the posterior of a gp under
observations of f ( x123 ) , .
, f ( xn ) is still a gp .
it is this conjugacy that allows practical inference in the examples above .
a bayesian nonparametric model is conjugate if and only if the elements of its weak distribution , i . e .
its ? nite - dimensional marginals , have a conjugate structure as well ( 123 ) .
in particular , this characterizes a class of conjugate bayesian nonparametric models whose weak distributions consist of exponential family models .
note however that lack of conjugacy do not imply intractable posteriors .
an example is given by the pitman - yor process , where the posterior is given by a sum of a ? nite number of atoms and a pitman - yor process independent from the atoms .
since mcmc sampling algorithms for dirichlet process mixtures became avail - able in the 123s and made latent variable models with nonparametric bayesian components applicable to practical problems , the development of bayesian non - parametrics has experienced explosive growth ( 123 , 123 ) .
arguably , though , the results available so far have only scratched the surface .
the repertoire of avail - able models is still mostly limited to using the gaussian process , the dirichlet process , the beta process , and generalizations derived from those .
ple , bayesian nonparametric models may be de ? ned on any in ? nite - dimensional mathematical object of possible interest to machine learning and statistics .
pos - sible examples are kernels , in ? nite graphs , special classes of functions ( e . g .
piece - wise continuous or sobolev functions ) , and permutations .
aside from the obvious modeling questions , two major future directions are to make bayesian nonparametric methods available to a larger audience of re - searchers and practitioners through the development of software packages , and to understand and quantify the theoretical properties of available methods .
general - purpose software package
there is currently signi ? cant growth in the application of bayesian nonparamet - ric models across a variety of application domains both in machine learning and in statistics .
however signi ? cant hurdles still exist , especially the expense and expertise needed to develop computer programs for inference in these complex models .
one future direction is thus the development of software packages that can compile e ? cient inference algorithms automatically given model speci ? ca - tions , thus allowing a much wider range of modeler to make use of these models .
current developments include the r dppackage123 , the hierarchical bayesian com - piler123 , adaptor grammars123 , the mit - church project123 , as well as e ? orts to add bayesian nonparametric models to the repertoire of current bayesian modeling
environments like openbugs123 and infer . net123
statistical properties of models
recent work in mathematical statistics provides some insight into the quan - titative behavior of bayesian nonparametric models ( cf theory section ) .
the elegant , methodical approach underlying these results , which quanti ? es model complexity by means of empirical process theory and then derives convergence rates as a function of the complexity , should be applicable to a wide range of models .
so far , however , only results for gaussian processes and dirichlet pro - cess mixtures have been proven , and it will be of great interest to establish properties for other priors .
some models developed in machine learning , such as the in ? nite hmm , may pose new challenges to theoretical methodology , since their study will probably have to draw on both the theory of algorithms and mathematical statistics .
once a wider range of results is available , they may in turn serve to guide the development of new models , if it is possible to establish how di ? erent methods of model construction a ? ect the statistical properties of the constructed model .
dirichlet processes , gaussian processes , bayesian methods , prior probabilities .
in addition to the references embedded in the text above , we recommend the books ( 123 , 123 ) and the review articles ( 123 , 123 ) on bayesian nonparametrics .
references for dps and dp mixture models can be found in the dp entry ( ? ) , for gps in the gp entry ( ? ) , while for most of the other examples can be found in the chapter ( 123 ) of the book ( 123 ) .
wasserman .
all of nonparametric statistics .
springer , 123
dunson .
nonparametric bayes applications to biostatistics .
hjort , c .
holmes , p .
m“uller , and s .
walker , editors , bayesian non - parametrics .
cambridge university press , 123
teh and m .
jordan .
hierarchical bayesian nonparametric models with applications .
hjort , c .
holmes , p .
m“uller , and s .
walker , editors , bayesian nonparametrics .
cambridge university press , 123
de finetti .
funzione caratteristica di un fenomeno aleatorio .
atti della r .
academia nazionale dei lincei , serie 123
memorie , classe di scienze fisiche , mathematice e naturale , 123 , 123
diaconis and d .
freedman .
on the consistency of bayes estimates ( with
discussion ) .
annals of statistics , 123 ( 123 ) : 123牢123 , 123
ghosal .
the dirichlet process , related priors and posterior asymptotics .
hjort , c .
holmes , p .
m“uller , and s .
walker , editors , bayesian non - parametrics .
cambridge university press , 123
kleijn and a .
van der vaart .
misspeci ? cation in in ? nite -
dimensional bayesian statistics .
annals of statistics , 123 : 123牢123 , 123
orbanz .
construction of nonparametric bayesian models from para - in advances in neural information processing
metric bayes equations .
escobar and m .
bayesian density estimation and inference using mixtures .
journal of the american statistical association , 123 : 123牢
( 123 ) r .
markov chain sampling methods for dirichlet process mixture models .
journal of computational and graphical statistics , 123 : 123牢123 ,
( 123 ) n .
hjort , c .
holmes , p .
m“uller , and s .
walker , editors .
bayesian nonpara - metrics .
number 123 in cambridge series in statistical and probabilistic mathematics .
cambridge university press , 123
( 123 ) j .
ghosh and r .
ramamoorthi .
bayesian nonparametrics .
springer ,
( 123 ) s .
walker , p .
damien , p .
laud , and a .
bayesian nonparametric inference for random distributions and related functions .
journal of the royal statistical society , 123 ( 123 ) : 123牢123 , 123
( 123 ) p .
m“uller and f .
quintana .
nonparametric bayesian data analysis .
statistical science , 123 ( 123 ) : 123牢123 , 123
