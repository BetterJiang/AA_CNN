nonparametric bayesian mixture models , in partic - ular dirichlet process ( dp ) mixture models , have shown great promise for density estimation and data clustering .
given the size of todays datasets , computational efciency becomes an essential in - gredient in the applicability of these techniques to real world data .
we study and experimentally com - pare a number of variational bayesian ( vb ) ap - proximations to the dp mixture model .
in partic - ular we consider the standard vb approximation where parameters are assumed to be independent from cluster assignment variables , and a novel col - lapsed vb approximation where mixture weights are marginalized out .
for both vb approximations we consider two different ways to approximate the dp , by truncating the stick - breaking construction , and by using a nite mixture model with a sym - metric dirichlet prior .
mixture modeling remains one of the most useful tools in statistics , machine learning and data mining for applications involving density estimation or clustering .
one of the most prominent recent developments in this eld is the application of nonparametric bayesian techniques to mixture modeling , which allow for the automatic determination of an appropriate number of mixture components .
current inference algorithms for such models are mostly based on gibbs sampling , which suffer from a number of drawbacks .
most importantly , gibbs sampling is not efcient enough to scale up to the large scale problems we face in modern - day data mining .
secondly , sam - pling requires careful monitoring of the convergence of the markov chain , both to decide on the number of samples to be ignored for burn - in and to decide how many samples are needed to reduce the variance in the estimates .
these con - siderations have lead researchers to develop deterministic al - ternatives which trade off variance with bias and are easily monitored in terms of their convergence .
moreover , they can
this material is based in part upon work supported by the national science foundation under grant number d / hs - 123
ywt thanks the lee kuan yew endowment fund for funding .
be orders of magnitude faster than sampling , especially when special data structures such as kd trees are used to cache cer - tain sufcient statistics ( moore , 123; verbeek et al . , 123; kurihara et al . , 123 ) .
( blei and jordan , 123 ) recently applied the framework of variational bayesian ( vb ) inference to dirichlet process ( dp ) mixture models and demonstrated signicant computational gains .
their model was formulated entirely in the truncated stick - breaking representation .
the choice of this representa - tion has both advantages and disadvantages .
for instance , it is very easy to generalize beyond the dp prior and use much more exible priors in this representation .
on the ip side , the model is formulated in the space of explicit , non - exchangeable cluster labels ( instead of partitions ) .
in other words , randomly permuting the labels changes the probabil - ity of the data .
this then requires samplers to mix over cluster labels to avoid bias ( porteous et al . , 123 ) .
in this paper we propose and study alternative approaches to vb inference in dp mixture models beyond that proposed in ( blei and jordan , 123 ) .
there are three distinct contri - butions in this paper : in proposing an improved vb algo - rithm based on integrating out mixture weights , in comparing the stick - breaking representation against the nite symmet - ric dirichlet approximation to the dp , and in the maintain - ing optimal ordering of cluster labels in the stick - breaking vb algorithms .
these lead to a total of six different algo - rithms , including the one proposed in ( blei and jordan , 123 ) .
we experimentally evaluate these six algorithms and compare against gibbs sampling .
in section 123 we explore both the truncated stick - breaking approximation and the nite symmetric dirichlet prior as - nite dimensional approximations to the dp .
as opposed to the truncated stick - breaking approximation , the nite symmetric dirichlet model is exchangeable over cluster labels .
theoret - ically this has important consequences , for example a gibbs sampler is not required to mix over cluster labels if we are computing averages over quantities invariant to cluster label permutations ( as is typically the case ) .
in section 123 we explore the idea of integrating out the mixture weights , hence collapsing the model to a lower di - mensional space .
this idea has been shown to work well for lda models ( teh et al . , 123 ) where strong dependencies ex - ist between model parameters and assignment variables .
such dependencies exist between mixture weights and assignment
variables in our mixture model context as well , thus collaps - ing could also be important here .
this intuition is reected in the observation that the variational bound on the log evidence is guaranteed to improve .
in section 123 we derive the vb update equations corre - sponding to the approximations in section 123
we also con - sider optimally reordering cluster labels in the stick - breaking vb algorithms .
as mentioned , the ordering of the cluster la - bels is important for models formulated in the stick - breaking representation .
in the paper ( blei and jordan , 123 ) this issue was ignored .
here we also study the effect of cluster reorder - ing on relevant performance measures such as the predictive
the above considerations lead us to six vb inference meth - ods , which we evaluate in section 123
the methods are : 123 ) the truncated stick - breaking representation with standard vb ( tsb ) , 123 ) the truncated stick - breaking representation with collapsed vb ( ctsb ) , 123 ) the nite symmetric dirichlet rep - resentation with standard vb ( fsd ) , 123 ) the nite symmetric dirichlet presentation with collapsed vb ( cfsd ) , and 123 ) and 123 ) being tsb and ctsb with optimal reordering ( o - tsb and
123 four approximations to the dp
we describe four approximations to the dp in this section .
these four approximations are obtained by a combination of truncated stick - breaking / nite symmetric dirichlet approxi - mations and whether the mixture weights are marginalized out or not .
based on these approximations we describe the six vb inference algorithms in the next section .
the most natural representation of dps is using the chi - nese restaurant process , which is formulated in the space of partitions .
partitions are groupings of the data independent of cluster labels , where each data - point is assigned to exactly 123 group .
this space of partitions turns out to be problem - atic for vb inference , where we wish to use fully factorized variational distributions on the assignment variables , q ( z ) = n q ( zn ) .
since the assignments z123 = 123 , z123 = 123 , z123 = 123 rep - resent the same partition ( 123 , 123 ) ( 123 ) as z123 = 123 , z123 = 123 , z123 = 123 , there are intricate dependencies between the assignment vari - ables and it does not make sense to use the factorization above .
we can circumvent this by using nite dimensional approximations for the dp , which are formulated in the space of cluster labels ( not partitions ) and which are known to closely approximate the dp prior as the number of explic - itly maintained clusters grows ( ishwaran and james , 123; ishwaran and zarepour , 123 ) .
these nite approximations are what will we discuss next .
123 tsb and fsd approximations
in the rst approximation we use the stick - breaking represen - tation for the dp ( ishwaran and james , 123 ) and truncate it
after t terms ,
vi b ( vi; 123 , ) vt = 123
i = vi
i = 123 , . . . , t 123
i = 123 , . . . , t
where b ( v; 123 , ) is a beta density for variable v with para - i=123 i = 123
in - meters 123 and , and one can verify that corporating this into a joint probability over data items x = ( xn ) , n = 123 , . . . , n , cluster assignments z = ( zn ) , n = 123 , . . . , n , stick - breaking weights v = ( vi ) , i = 123 , . . . , t and cluster parameters = ( i ) , i = 123 , . . . , t we nd
p ( x , z , v , ) =
p ( xn|zn ) p ( zn| ( v ) )
p ( i ) b ( vi; 123 , )
where ( v ) are the mixture weights as dened in ( 123 ) .
in this representation the cluster labels are not interchangeable , i . e .
changing labels will change the probability value in ( 123 ) .
note also that as t the approximation becomes exact .
a second approach to approximate the dp is by assuming a nite ( but large ) number of clusters , k , and using a sym - metric dirichlet prior d on ( ishwaran and zarepour , 123 ) ,
this results in the joint model ,
p ( x , z , , ) =
p ( xn|zn ) p ( zn| )
k ) ( 123 )
the essential difference with the stick - breaking representa - tion is that the cluster labels remain interchangeable under this representation , i . e .
changing cluster labels does not change the probability ( porteous et al . , 123 ) .
the limit k is somewhat tricky because in the transition k we switch to the space of partitions , where states that result from cluster relabelings are mapped to the same par - tition .
for example , both z123 = 123 , z123 = 123 , z123 = 123 and z123 = 123 , z123 = 123 , z123 = 123 are mapped to the same partition
in gure 123 we show the prior average cluster sizes under the truncated stick - breaking ( tsb ) representation ( left ) and under the nite symmetric dirichlet ( fsd ) prior ( middle ) for two values of the truncation level and number of clusters re - spectively .
from this gure it is apparent that the cluster labels in the tsb prior are not interchangeable ( the proba - bilities are ordered in decreasing size ) , while they are inter - changeable for the fsd prior .
as we increase t and k these priors approximate the dp prior with increasing accuracy .
one should note however , that they live in different spaces .
the dp itself is most naturally dened in the space of parti - tions , while both tsb and fsd are dened in the space over cluster labels .
however , tsb and fsd also live in different
truncated stickbreaking representation
finite symmetric dirichlet prior
truncated stickbreaking representation 123
figure 123 : average cluster size for three nite approximations to the dp prior .
left : truncated stick - breaking prior ( tsb ) as given in ( 123 ) .
middle : finite symmetric dirichlet prior ( fsd ) .
right : stick - breaking representation corresponding to the fsd prior .
in each gure we show results for two truncation levels : t / k = 123 ( left bars ) and t / k = 123 ( right bars ) .
spaces ! more precisely , one can transform a sample from the fsd prior into the stick - breaking representation by per - forming a size - biased permutation of the mixture weights ( i . e .
after every sample from d ( ) we sample an ordering according to without replacement ) .
as it turns out , for - nite k this does not exactly recover the left hand gure in 123 , but rather samples from a prior very closely related to it shown in the right pane of gure 123
this prior is given by a stick - breaking construction as in eqn . ( 123 ) with stick - lengths
vi b ( vi; 123 +
conversely , we can obtain samples from the fsd prior by applying a random , uniformly distributed permutation on the cluster weights obtained from eqn . ( 123 ) .
although these two stick - breaking constructions are slightly different , for large enough k , t they are very similar and we do not expect any difference in terms of performance between the two .
123 marginalizing out the mixture weights the variational bayesian approximations discussed in the next section assume a factorized form for the posterior dis - tribution .
this means that we assume that parameters are in - dependent of assignment variables .
this is clearly a very bad assumption because changes in will have a considerable impact on z .
ideally , we would integrate out all the parame - ters , but this is too computationally expensive .
there is how - ever a middle ground : we can marginalize out from both methods without computational penalty if we make another approximation which will be discussed in section 123 .
for both tsb and fsd representations the joint collapsed model over x , z , is given by ,
p ( x , z , ) =
with different distributions over cluster labels p ( z ) in both cases .
for the tsb representation we have ,
( 123 + ni ) ( + n>i )
( 123 + + ni )
i ( zn = i )
i ( zn > i )
and ni = ni + n>i .
for fsd we nd instead ,
k=123 ( nk +
( n + ) (
123 variational bayesian inference the variational bayesian inference algorithm ( attias , 123; ghahramani and beal , 123 ) lower bounds the log marginal likelihood by assuming that parameters and hidden variables are independent .
the lower bound is given by ,
l ( x ) b ( x ) =
p ( x , z , )
where is either ( , v ) , ( , ) or ( ) in the various dp approximations discussed in the previous section .
approxi - mate inference is then achieved by alternating optimization of this bound over q ( z ) and q ( ) .
in the following we will spell out the details of vb inference for the proposed four methods .
for the tsb prior we use ,
qtsb ( z , , v ) =
where q ( v ) is not used in the tsb model with v marginalized out .
for the fsd prior we use ,
qfsd ( z , , ) =
as well , q ( ) is left out for the collapsed version .
123 bounds on the evidence given the variational posteriors we can construct bounds on the log marginal likelihood by inserting q into eqn . ( 123 )
ter some algebra we nd the following general form ,
q ( zn ) q ( zn ) log p ( xn|zn )
+ extra term
q ( zn ) log q ( zn )
where the extra term depends on the particular method .
for the tsb prior we have ,
on the other hand for the fsd prior we nd ,
q ( zn ) q ( ) log p ( zn| )
for both collapsed versions these expressions are replaced by ,
123 vb update equations given these bounds it is now not hard to derive update equa - tions for the various methods .
due to space constraints we will refer to the papers ( blei and jordan , 123; ghahramani and beal , 123; penny , 123; yu et al . , 123 ) for more details on the update equations for the un - collapsed methods and fo - cus on the novel collapsed update equations .
below we will provide the general form of the update equations where we do not assume anything about the par - ticular form of the prior p ( i ) .
the equations become par - ticularly simple when we choose this prior in the conju - gate exponential family .
explicit update equations for q ( i ) can be found in the papers ( ghahramani and beal , 123; blei and jordan , 123; penny , 123; yu et al . , 123 ) .
for q ( i ) we nd the same update for both methods ,
q ( i ) p ( i ) exp
q ( zn = i ) log p ( xn|i )
while for q ( zn ) we nd the update
q ( zm ) log p ( zn|zn )
q ( zn ) log p ( xn|zn )
where the conditional p ( zn|zn ) is different for the fsd and tsb priors .
for the tsb prior we use ( 123 ) , giving the condi -
p ( zn = i|zn ) =
123 + n n
123 + + n n
123 + + n n
i = ni i ( zn = i ) , n n
where n n >i = n>i i ( zn > i ) are the corresponding counts with zn removed .
in contrast , for the fsd prior we have ,
p ( zn = k|zn ) =
123 gaussian approximation the expectation required to compute the update ( 123 ) seems intractable due to the exponentially large space of all assign - ments for z .
it can in fact be computed in polynomial time using convolutions , however this solution still tended to be too slow to be practical .
a much more efcient approximate solution is to observe that both random variables ni and n>i are sums over bernoulli variables : ni = n i ( zn = i ) and n i ( zn > i ) .
using the central limit theorem these sums are expected to be closely approximated by gaussian distributions with means and variances given by ,
q ( zn = i )
q ( zn = j )
q ( zn = i ) ( 123 q ( zn = i ) )
q ( zn = j )
q ( zn = k )
to apply this approximation to the computation of the average in ( 123 ) , we use the following second order taylor expansion ,
e ( f ( m ) ) f ( e ( m ) ) +
this approximation has been observed to work extremely well in practice , even for small values of m .
123 optimal cluster label reordering as discussed in section 123 the stick - breaking prior assumes a certain ordering of the clusters ( more precisely , a size - biased ordering ) .
since a permutation of the cluster labels changes the probability of the data , we should choose the optimal per - mutation resulting in the highest probability for the data .
the optimal relabelling of the clusters is given by the one that or - ders the cluster sizes in decreasing order ( this is true since the average prior cluster sizes are also ordered ) .
in our ex - periments we assess the effect of reordering by introducing algorithms o - tsb and o - ctsb which always maintain this optimal labelling of the clusters .
note that optimal ordering was not maintained in ( blei and jordan , 123 ) .
figure 123 : average log probability per data - point for test data as a function of n .
figure 123 : average log probability per data - point for test data as a function of t ( for tsb methods ) or k ( for fsd methods ) .
figure 123 : relative average log probability per data - point for test data as a function of n .
figure 123 : relative average log probability per data - point for test data as a function of t ( for tsb methods ) or k ( for fsd methods ) .
in the following experiments we compared the six algorithms discussed in the main text in terms of their log - probability on held out test data .
the probability for a test point , xt , is then
where the expectation e ( p ( zt|ztrain ) ) q ( ztrain ) is computed using the techniques introduced in section 123 .
all experiments were conducted using gaussian mixtures with vague priors on the parameters .
in the rst experiment we generated synthetic data from a mixture of 123 gaussians in 123 dimensions with a separation
coefcient123 c = 123
we studied the accuracy of each algorithm as a function of the number of data cases and the truncation level of the approximation .
in gures 123 and 123 we show the results as we vary n ( keeping t and k xed at 123 ) while in gures 123 and 123 we plot the results as we vary t and k ( keep - ing n xed at 123 ) .
we plot both the absolute value of the log probability of test data and the value relative to a gibbs sam - pler ( gs ) .
we 123 iterations for burn - in , and run another 123 iterations for inference .
error bars are computed on the rela - tive values in order to subtract variance caused by the differ - ent splits ( i . e .
we measure variance on paired experiments ) .
123following ( dasgupta , 123 ) , a gaussian mixture is c - separated if for each pair ( i , j ) of components we have ||mi mj ||123 ) , where max denotes the maximum eigen - value of their covariance .
figure 123 : 123 most populated clusters found by the various al - gorithms in descending order of e ( ni ) .
algorithms were trained on a random subset of 123 , 123 images from mnist and dimen - sionality reduced to 123 dimensions using pca .
log probability of 123 , 123 test images are given by , l = 123 123 ( tsb ) , l = 123 123 ( o - tsb ) , l = 123 123 ( ctsb ) , l = 123 123 ( o - ctsb ) , l = 123 123 ( fsd ) , and l = 123 123 ( cfsd ) .
standard error over differences relative to o - ctsb are given by : dl = 123 123 ( tsb ) , dl = 123 123 ( o - tsb ) , dl = 123 123 ( ctsb ) , dl = 123 123 ( fsd ) , and dl = 123 123 ( cfsd ) .
results were averaged over 123 independently sampled train - ing / testing datasets , where the number of test instances was always xed at 123
in the second experiment we have run the algorithms on subsets of mnist .
images of size 123 123 were dimension - ality reduced to 123 pca dimensions as a preprocessing step .
we trained all algorithms on 123 splits of the data , each split containing 123 data - cases for training and 123 , 123 data - cases for testing .
truncation levels were set to 123 for all algorithms .
unfortunately , the dataset was too large to obtain results with gibbs sampling .
all algorithms typically nd between 123 and 123 clusters .
results are shown in gure 123
in this paper we explored six different ways to perform vari - ational bayesian inference in dp mixture models .
besides an empirical study of these algorithms our contribution has been to introduce a new family of collapsed variational algorithms where the mixture weights are marginalized out .
to make these algorithms efcient , we used the central limit theorem to approximate the required averages .
we can draw three conclusions from our study .
firstly , there is very little difference between variational bayesian in - ference in the reordered stick - breaking representation and the nite mixture model with symmetric dirichlet priors .
sec - ondly , label reordering is important for the stick - breaking representation .
thirdly , variational approximations are much
123for n=123 and t or k=123 , tsb , o - tsb , ctsb , o - ctsb , fsd , cfsd and gs took 123 , 123 , 123 , 123 , 123 , 123 and 123 , 123 seconds on average , respectively .
note that the computational com - plexities of the variational algorithms are the same .
more efcient computationally than gibbs sampling , with al - most no loss in accuracy .
we are currently working towards models where the para - meters are marginalized out as well .
we expect this to have a more signicant impact on test accuracy than the current setup which only marginalizes over , especially when clus - ters are overlapping .
unfortunately , it seems this will come at the cost of increased computation .
collapsed variational inference has also been applied to lda models ( teh et al . , 123 ) , where preliminary results indicate signicant performance improvement .
we are cur - rently also exploring collapsed variational inference for hier - archical dp models ( teh et al . , 123 ) .
