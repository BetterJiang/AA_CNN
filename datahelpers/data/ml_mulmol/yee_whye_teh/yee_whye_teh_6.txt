hierarchical modeling is a fundamental concept in bayesian statistics .
the basic idea is that parameters are endowed with distributions which may themselves introduce new parameters , and this construction recurses .
in this review we discuss the role of hierarchical modeling in bayesian non - parametrics , focusing on models in which the innite - dimensional parame - ters are treated hierarchically .
for example , we consider a model in which the base measure for a dirichlet process is itself treated as a draw from another dirichlet process .
this yields a natural recursion that we refer to as a hierarchical dirichlet process .
we also discuss hierarchies based on the pitman - yor process and on completely random processes .
we demonstrate the value of these hierarchical constructions in a wide range of practical applications , in problems in computational biology , computer vision and natural language processing .
hierarchical modeling is a fundamental concept in bayesian statistics .
the basic idea is that parameters are endowed with distributions which may themselves introduce new parameters , and this construction recurses .
a common motif in hierarchical modeling is that of the conditionally independent hierarchy , in which a set of parameters are coupled by making their distributions depend
on a shared underlying parameter .
these distributions are often taken to be identical , based on an assertion of exchangeability and an appeal to de finettis
hierarchies help to unify statistics , providing a bayesian interpretation of frequentist concepts such as shrinkage and random eects .
hierarchies also provide ways to specify non - standard distributional forms , obtained as integrals over underlying parameters .
they play a role in computational practice in the guise of variable augmentation .
these advantages are well appreciated in the world of parametric modeling , and few bayesian parametric modelers fail to make use of some aspect of hierarchical modeling in their work .
nonparametric bayesian models also typically include many classical nite - dimensional parameters , including scale and location parameters , and hierar - chical modeling concepts are often invoked in specifying distributions for these parameters .
for example , the dirichlet process dp ( , g123 ) involves a concentra - tion parameter , which is generally given a prior distribution in nonparametric ( and semiparametric ) models that make use of the dirichlet process .
moreover , the base measure , g123 , is often taken to be a parametric distribution and its parameters are endowed with prior distributions as well .
in this chapter we discuss a more thoroughgoing exploitation of hierarchi - cal modeling ideas in bayesian nonparametric statistics .
the basic idea is that rather than treating distributional parameters such as g123 parametrically , we treat them nonparametrically .
in particular , the base measure g123 in the dirich - let process can itself be viewed as a random draw from some distribution on measuresspecically it can be viewed as a draw from the dirichlet process .
this yields a natural recursion that we refer to as a hierarchical dirichlet pro - cess .
our focus in this chapter is on nonparametric hierarchies of this kind , where the tools of bayesian nonparametric modeling are used recursively .
the motivations for the use of hierarchical modeling ideas in the nonpara - metric setting are at least as strong as they are in the parametric setting .
in particular , nonparametric models involve large numbers of degrees of freedom , and hierarchical modeling ideas provide essential control over these degrees of freedom .
moreover , hierarchical modeling makes it possible to take the build - ing blocks provided by simple stochastic processes such as the dirichlet process and construct models that exhibit richer kinds of probabilistic structure .
this breathes life into the nonparametric framework .
the chapter is organized as follows .
in section 123 , we discuss the hierarchical dirichlet process , showing how it can be used to link multiple dirichlet processes .
we present several examples of real - world applications in which such models are natural .
section 123 shows how the hierarchical dirichlet process can be used to build nonparametric hidden markov models; these are hidden markov models in which the cardinality of the state space is unbounded .
we also discuss extensions to nonparametric hidden markov trees and nonparametric probabilistic context free grammars .
in section 123 we consider a dierent nonparametric hierarchy based on the pitman - yor model , showing that it is natural in domains such as natural language processing in which data often exhibit power - law behavior .
section 123 discusses the beta process , an alternative to the dirichlet process
which yields sparse featural representations .
we show that the counterpart of the chinese restaurant process is a distribution on sparse binary matrices known as the indian buet process .
we also consider hierarchical models based on the beta process .
in section 123 , we consider some semiparametric models that are based on nonparametric hierarchies .
finally , in section 123 we present an overview of some of the algorithms that have been developed for posterior inference in hierarchical bayesian nonparametric models .
in all of these cases , we use practical applications to motivate these con - structions and to make our presentation concrete .
our applications range from problems in biology to computational vision to natural language processing .
several of the models that we present provide state - of - the - art performance in these application domains .
this wide range of successful applications serves notice as to the growing purview of bayesian nonparametric methods .
123 hierarchical dirichlet processes the dirichlet process ( dp ) is useful in models for which a component of the model is a discrete random variable of unknown cardinality .
the canonical example of such a model is the dp mixture model , where the discrete variable is a cluster indicator .
the hierarchical dirichlet process ( hdp ) is useful in problems in which there are multiple groups of data , where the model for each group of data incorporates a discrete variable of unknown cardinality , and where we wish to tie these variables across groups ( teh et al . , 123 ) .
for example , the hdp mixture model allows us to share clusters across multiple clustering the basic building block of a hierarchical dirichlet process is a recursion in which the base measure g123 for a dirichlet process g dp ( , g123 ) is itself a draw from a dirichlet process : g123 dp ( , h ) .
this recursive construction has the eect of constraining the random measure g to place its atoms at the discrete locations determined by g123
the major application of such a construction is to the setting of conditionally independent hierarchical models of grouped data .
more formally , consider an indexed collection of dps , ( gj ) , one for each of a countable set of groups and dened on a common probability space ( , ) .
the hierarchical dirichlet process ties these random measures probabilistically by letting them share their base measure and letting this base measure be random :
g123 | , h dp ( , h ) gj | , g123 dp ( , g123 )
for j j ,
where j is the index set .
this conditionally independent hierarchical model induces sharing of atoms among the random measures gj since each inherits its set of atoms from the same g123
to understand the precise nature of the sharing induced by the hdp it is helpful to consider representations akin to the stick - breaking and chinese restaurant representations of the dp .
we consider these representations in the next three subsections before turning to a discussion of applications of the hdp .
figure 123 : the hdp stick - breaking construction .
the left panel depicts a draw of , and the remaining panels depict draws of 123 , 123 and 123 conditioned on
note that the recursive construction of the hdp can be generalized to ar - bitrary hierarchies in the obvious way .
each gj is given a dp prior with base measure gpa ( j ) , where pa ( j ) is the parent index of j in the hierarchy .
as in the two - level hierarchy in eq .
( 123 ) , the set of atoms at the top level is shared throughout the hierarchy , while the multi - level hierarchy allows for a richer de - pendence structure on the weights of the atoms .
section 123 presents an instance of such a hierarchy in the setting of pitman - yor processes .
other ways to couple multiple dirichlet processes have been proposed in the literature; in particular the dependent dirichlet process of maceachern et al .
( 123 ) provides a general formalism .
ho et al .
( 123 ) gives a complementary view of the hdp and its pitman - yor generalizations in terms of coagulation operators .
see teh et al .
( 123 ) and chapter ? ? for overviews .
123 stick - breaking construction in this section we develop a stick - breaking construction for the hdp .
this rep - resentation provides a concrete representation of draws from an hdp and it provides insight into the sharing of atoms across multiple dps .
we begin with the stick - breaking representation for the random base mea - sure g123 , where g123 dp ( , h ) .
given that this base measure is distributed according to a dp , we have ( sethuraman , 123; ishwaran and james , 123 , also see section ? ? in chapter ? ? ) :
for k = 123 ,
vk | beta ( 123 , )
k = vk k | h h .
123 . 123 . 123weights123 . 123 . 123 . 123 . 123 . 123 . 123 we refer to the joint distribution on the innite sequence ( 123 , 123 , .
. ) as the gem ( ) distribution ( pitman , 123 ) ( gem stands for griths , engen and
the random measures gj are also distributed ( conditionally ) according to a dp .
moreover , the support of each gj is contained within the support of g123
thus the stick - breaking representation for gj is a reweighted sum of the atoms
the problem reduces to nding a relationship between the weights = ( 123 , 123 , .
. ) and j = ( j123 , j123 , .
let us interpret these weight vectors as probability measures on the discrete space ( 123 , .
taking partitions over integers in - duced by partitions on , the dening property of the dp ( ferguson , 123 )
some algebra then readily yields the following explicit construction for j con - ditioned on :
j | , dp ( , ) .
vjk | , 123 , .
, k beta
jk = vjk
for k = 123 ,
figure 123 shows a sample draw of along with draws from 123 , 123 and 123 given from eq .
( 123 ) we see that the mean of k is e ( k ) = k123 ( 123 + ) k which decreases exponentially in k .
the mean for j is simply its base measure ; thus e ( jk ) = e ( k ) = k123 ( 123+ ) k as well .
however the law of total variance shows that jk has higher variance than k : var ( jk ) = e ( k ( 123k ) ) +var ( k ) > var ( k ) .
the higher variance is reected in figure 123 by the sparser nature of j relative
123 chinese restaurant franchise the chinese restaurant process ( crp ) describes the marginal probabilities of the dp in terms of a random partition obtained from a sequence of customers sitting at tables in a restaurant .
there is an analogous representation for the hdp which we refer to as a chinese restaurant franchise ( crf ) .
in a crf the metaphor of a chinese restaurant is extended to a set of restaurants , one for each index in j .
the customers in the jth restaurant sit at tables in the same manner as the crp , and this is done independently in the restaurants
coupling among restaurants is achieved via a franchise - wide menu .
the rst customer to sit at a table in a restaurant chooses a dish from the menu and all subsequent customers who sit at that table inherit that dish .
dishes are chosen with probability proportional to the number of tables ( franchise - wide ) which have previously served that dish .
more formally , label the ith customer in the jth restaurant with a random variable ji that is distributed according to gj .
similarly , let jt denote a random variable corresponding to the tth table in the jth restaurant; these variables are drawn independently and identically distributed ( iid ) according to g123
finally , the dishes are iid variables k distributed according to the base measure h .
we couple these variables as follows .
each customer sits at one table and each table serves one dish; let customer i in restaurant j sit at table tji , and let table t serve dish kjt .
then let ji =
let njtk be the number of customers in restaurant j seated around table t and being served dish k , let mjk be the number of tables in restaurant j serving dish k , and let k be the number of unique dishes served in the entire franchise .
we denote marginal counts with dots; e . g . , njk is the number of customers in restaurant j served dish k .
to show that the crf captures the marginal probabilities of the hdp , we integrate out the random measures gj and g123 in turn from the hdp .
we start by integrating out the random measure gj; this yields a set of conditional distributions for the ji described by a polya urn scheme :
ji | j123 , .
, j , i123 , , g123 mj ( cid : 123 )
a draw from this mixture can be obtained by drawing from the terms on the right - hand side with probabilities given by the corresponding mixing propor - tions .
if a term in the rst summation is chosen then the customer sits at an already occupied table : we increment njt , set ji = jt and let tji = t for the chosen t .
if the second term is chosen then the customer sits at a new table : g123 , set ji = we increment mj by one , set njmj = 123 , draw tji = mj .
jt is drawn iid from g123 in the polya urn scheme in eq .
( 123 ) , and this is the only reference to g123 in that equation .
thus we can readily integrate out g123 as well , obtaining a polya urn scheme for the
notice that each
j , t123 , , h k ( cid : 123 )
where we have presumed for ease of notation that j = ( 123 , .
, |j | ) .
as promised , we see that the kth dish is chosen with probability proportional to the number of tables franchise - wide that previously served that dish ( mk ) .
the crf is useful in understanding scaling properties of the clustering in - duced by an hdp .
in a dp the number of clusters scales logarithmically ( an - toniak , 123 ) .
thus mj o ( log nj ) where mj and nj are respectively the
) = o ( log (
from a dp , we have that k o ( log ( cid : 123 )
total number of tables and customers in restaurant j .
since g123 is itself a draw we assume that there are j groups and that the groups ( the customers in the dierent restaurants ) have roughly the same size n , nj o ( n ) , we see that k o ( log ) .
thus the number of clusters scales doubly logarithmically in the size of each group , and logarithmi - cally in the number of groups .
the hdp thus expresses a prior belief that the number of clusters grows very slowly in n .
if this prior belief is inappropriate for a given problem , there are alternatives; in particular , in section 123 . 123 we discuss a hierarchical model that yields power - law scaling .
+ log j + log log n
) = o ( log
j log n
j log nj
123 posterior structure of the hdp the chinese restaurant franchise is obtained by integrating out the random measures gj and then integrating out g123
integrating out the random measures gj yields a chinese restaurant for each group as well as a sequence of iid draws from the base measure g123 , which are used recursively in integrating out g123
having obtained the crf , it is of interest to derive conditional distributions that condition on the crf; this not only illuminates the combinatorial structure of the hdp but it also prepares the ground for a discussion of inference algorithms ( see section 123 ) , where it can be useful to instantiate the crf explicitly .
k ) k=123 , . . . , k , the table tji at which the ith customer sits , and the dish kjt served at the tth table .
as functions of the state of the crf , we also have the numbers of customers n = ( njtk ) , the numbers of tables m = ( mjk ) , the customer labels = ( ji ) jt ) .
the relationship between the customer labels and the table labels and the table labels is given as follows : consider the distribution of g123 conditioned on the state of the crf .
g123 is independent from the rest of the crf when we condition on the iid draws because the restaurants interact with g123 only via the iid draws .
the posterior thus follows from the usual posterior for a dp given iid draws :
the state of the crf consists of the dish labels
and ji =
g123 | , h ,
are determined given
note that values for m and the unique values and their counts among constructed as follows ( using the dening property of a dp ) :
, since they are simply 123
a draw from eq .
( 123 ) can be
123 , 123 , .
, k | , g123 ,
dirichlet ( , m123 ,
123 | , h dp ( , h )
g123 = 123g ( cid : 123 )
123here we make the simplifying assumption that h is a continuous distribution so that
draws from h are unique .
if h is not continuous then additional bookkeeping is required .
we see that the posterior for g123 is a mixture of atoms corresponding to the dishes and an independent draw from dp ( , h ) .
conditioning on this draw of g123 as well as the state of the crf , the posteri - ors for the gj are independent .
in particular , the posterior for each gj follows from the usual posterior for a dp , given its base measure g123 and iid draws j :
gj | , g123 , j dp
note that nj and j .
making use of the decomposition of g123 into g ( cid : 123 )
are simply the unique values and their counts among the 123 and atoms located at the
, a draw from eq .
( 123 ) can thus be constructed as follows :
j123 , j123 , .
, jk | , j dirichlet ( 123 , 123 + nj123 , .
, k + njk )
j | , g123 dp ( 123 , g ( cid : 123 )
gj = j123g ( cid : 123 )
we see that gj is a mixture of atoms at dp , where the concentration parameter depends on 123
k and an independent draw from a
the posterior over the entire hdp is obtained by averaging the conditional distributions of g123 and gj over the posterior state of the chinese restaurant franchise given .
this derivation shows that the posterior for the hdp can be split into a discrete part and a continuous part .
the discrete part consists of atoms at , with dierent weights on these atoms for each dp .
the the unique values continuous part is a separate draw from an hdp with the same hierarchical structure as the original hdp and global base measure h , but with altered concentration parameters .
the continuous part consists of an innite series of atoms at locations drawn iid from h .
although we have presented this posterior representation for a two - level hierarchy , the representation extends immediately to general hierarchies .
123 applications of the hdp in this section we consider several applications of the hdp .
these models use the hdp at dierent depths in an overall bayesian hierarchy .
in the rst example the random measures obtained from the hdp are used to generate data directly , and in the second and third examples these random measures generate latent
123 . 123 information retrieval the growth of modern search engines on the world wide web has brought new attention to a classical problem in the eld of information retrieval ( ir ) how
should a collection of documents be represented so that relevant documents can be returned in response to a query ? ir researchers have studied a wide variety of representations and have found empirically that a representation known as term frequency - inverse document frequency , or tf - idf , yields reasonably high - quality rankings of documents ( salton and mcgill , 123 ) .
the general intuition is that the relevance of a document to a query should be proportional to the frequency of query terms it contains ( term frequency ) , but that query terms that appear in many documents should be downweighted since they are less informative ( inverse document frequency ) .
cowans ( 123 , 123 ) has shown that the hdp provides statistical justi - cation for the intuition behind tf - idf .
let xji denote the ith word in the jth document in some corpus of documents , where the range of xji is a discrete vocabulary .
consider the following simple model for documents :
g123 | , h dp ( , h ) gj | , g123 dp ( , g123 )
xji | gj gj
for j j for i = 123 ,
where h is the global probability measure over the vocabulary and where nj is the number of words in the jth document .
( note that nj = nj where the latter refers to the general notation introduced in section 123; here and elsewhere we use nj as a convenient shorthand . ) in this model , gj is a discrete measure over the vocabulary associated with document j and g123 is a discrete measure over the vocabulary that acts to tie together word usages across the corpus .
the model is presented as a graphical model in the left panel of figure 123
following marginal probabilities for words in the jth document :
integrating out g123 and the gj as discussed in section 123 , we obtain the
p123 ( ) = m ( cid : 123 )
j + p123 ( )
j is the term frequencythe number of occurrences of in document j is the number of tables serving dish in restaurant j in the crf ( note that the need for the specialized prime notation in this case is driven by the fact that is a discrete space in this example .
in particular , for each there may be multiple k such that k = .
the term frequency n ( cid : 123 ) = njk is the number of customers eating dish regardless of which menu entry they picked .
similarly , m ( cid : 123 )
j = ( cid : 123 )
if we make the approximation that the number of tables serving a partic - ular dish in a particular restaurant is at most one , then m ( cid : 123 ) is the document frequencythe number of documents containing word in the corpus .
we now rank documents by computing a relevance score r ( j , q ) the log probability
figure 123 : graphical representations of hdp - based models .
left : an hdp model for information retrieval .
center : an hdp mixture model for haplotype phasing .
right : the hdp - lda model for topic or admixture modeling .
xjig123gjhi=123 , . . . , njjjji123xjig123gjhi=123 , . . . , njjjji123jixjig123gjhi=123 , . . . , njjj of a query q under each document j :
r ( j , q ) = ( cid : 123 )
j + ) + log ( p123 ( ) )
in this score the rst term is akin to a tf - idf score , the second term is a nor - malization penalizing large documents , and the third term can be ignored as it does not depend on document identity j .
thus we see that a simple application of the hdp provides a principled justication for the use of inverse document frequency and document length normalization .
moreover , in small - scale experi - ments , cowans ( 123 , 123 ) found that this score improves upon state - of - the - art relevance scores ( robertson et al . , 123; hiemstra and kraaij , 123 ) .
123 . 123 multi - population haplotype phasing we now consider a class of applications in which the hdp provides a distribution on latent parameters rather than on the observed data .
haplotype phasing is an interesting problem in statistical genetics that can be formulated as a mixture model ( stephens et al . , 123 ) .
consider a set of m binary markers along a chromosome .
chromosomes come in pairs for humans , so let i123 and i123 denote the binary - valued vectors of markers for a pair of chromosomes for the ith individual .
these vectors are referred to as haplotypes , and the elements of these vectors are referred to as alleles .
a genotype xi is a vector which records the unordered pair of alleles for each marker; that is , the association of alleles to chromosome is lost .
the haplotype phasing problem is to restore haplotypes ( which are useful for predicting disease associations ) from genotypes ( which are readily assayed experimentally whereas haplotypes
under standard assumptions from population genetics , we can write the
probability of the ith genotype as a mixture model :
p ( xi ) = ( cid : 123 )
p ( i123 ) p ( i123 ) p ( xi | i123 , i123 ) ,
where h is the set of haplotypes in the population and where p ( xi | i123 , i123 ) reects the loss of order information as well as possible measurement error .
given that the cardinality of h is unknown , this problem is naturally formulated as a dp mixture modeling problem where a cluster is a haplotype ( xing et al . ,
let us now consider a multi - population version of the haplotype phasing problem in which the genotype data can be classied into ( say ) asian , european and african subsets .
here it is natural to attempt to identify the haplotypes in each population and to share these haplotypes among populations .
this can be
achieved with the following hdp mixture model :
g123 | , h dp ( , h ) gj | , g123 dp ( , g123 )
ji123 , ji123 | gj xji | ji123 , ji123 fji123 , ji123
for each population j j for each individual i = 123 ,
where ji123 , ji123 denote the pair of haplotypes for the ith individual in the jth population .
the model is presented as a graphical model in the center panel of figure 123
xing et al .
( 123 ) showed that this model performs eectively in multi - population haplotype phasing , outperforming methods that lump together the multiple populations or treat them separately .
123 . 123 topic modeling a topic model or mixed membership model is a generalization of a nite mixture model in which each data point is associated with multiple draws from a mixture model , not a single draw ( blei et al . , 123; erosheva , 123 ) .
as we will see , while the dp is the appropriate tool to extend nite mixture models to the nonparametric setting , the appropriate tool for nonparamametric topic models is the hdp .
to motivate the topic model formulation , consider the problem of modeling the word occurrences in a set of newspaper articles ( e . g . , for the purposes of classifying future articles ) .
a simple clustering methodology might attempt to place each article in a single cluster .
but it would seem more useful to be able to cross - classify articles according to topics; for example , an article might be mainly about italian food , but it might also refer to health , history and the weather .
moreover , as this example suggests , it would be useful to be able to assign numerical values to the degree to which an article treats each topic .
topic models achieve this goal as follows .
dene a topic to be a probability distribution across a set of words taken from some vocabulary w .
a document is modeled as a probability distribution across topics .
in particular , let us assume the following generative model for the words in a document .
first choose a probability vector from the k - dimensional simplex , and then repeatedly ( 123 ) select one of the k topics with probabilities given by the components of and ( 123 ) choose a word from the distribution dened by the selected topic .
the vector thus encodes the expected fraction of words in a document that are allocated to each of the k topics .
in general a document will be associated with multiple
another natural example of this kind of problem arises in statistical genet - ics .
assume that for each individual in a population we can assay the state of each of m markers , and recall that the collection of markers for a single individual is referred to as a genotype .
consider a situation in which k sub - populations which have hitherto remained separate are now thoroughly mixed ( i . e . , their mating patterns are that of a single population ) .
individual geno - types will now have portions that arise from the dierent subpopulations
is referred to as admixture .
we can imagine generating a new admixed geno - type by xing a distribution across subpopulations and then repeatedly ( 123 ) choosing a subpopulation according to and ( 123 ) choosing the value of a marker ( an allele ) from the subpopulation - specic distribution on the alleles for that marker .
this formulation is essentially isomorphic to the document modeling formulation .
( the dierence is that in the document setting the observed words are generally assumed to be exchangeable , whereas in the genetics setting each marker has its own distribution over alleles ) .
to fully specify a topic model we require a distribution for .
taking this distribution to be symmetric dirichlet , we obtain the latent dirichlet allocation ( lda ) model , developed by blei et al .
( 123 ) and pritchard et al .
( 123 ) as a model for documents and admixture , respectively .
this model has been widely used not only in the elds of information retrieval and statistical genetics , but also in computational vision , where a topic is a distribution across visual primitives , and an image is modeled as a distribution across topics ( fei - fei and
let us now turn to the problem of developing a bayesian nonparametric version of lda in which the number of topics is allowed to be open - ended .
as we have alluded to , this requires the hdp , not merely the dp .
to see this , consider the generation of a single word in a given document .
according to lda , this is governed by a nite mixture model , in which one of k topics is drawn and then a word is drawn from the corresponding topic distribution .
generating all of the words in a single document requires multiple draws from this nite mixture .
if we now consider a dierent document , we again have a nite mixture , with the same mixture components ( the topics ) , but with a dierent set of mixing proportions ( the document - specic vector ) .
thus we have multiple nite mixture models .
in the nonparametric setting they must be linked so that the same topics can appear in dierent documents .
we are thus led to the following model , which we refer to as hdp - lda :
g123 | , h dp ( , h ) gj | , g123 dp ( , g123 )
ji | gj gj xji | ji fji
for each document j j for each word i = 123 ,
where xji is the ith word in document j , h is the prior distribution over topics is the distribution over words .
the model is presented as a graphical model in the right panel of figure 123
note that the atoms present in the ran - dom distribution g123 are shared among the random distributions gj .
thus , as desired , we have a collection of tied mixture models , one for each document .
topic models can be generalized in a number of other directions .
for exam - ple , in applications to document modeling it is natural to ask that topics occur at multiple levels of resolution .
thus , at a high level of resolution , we might wish to obtain topics that give high probability to words that occur throughout the documents in a corpus , while at a lower level we might wish to nd topics that are focused on words that occur in specialized subsets of the documents .
a bayesian nonparametric approach to obtaining this kind of abstraction hier - archy has been presented by blei et al .
( 123 ) .
in the model presented by these authors , topics are arranged into a tree , and a document is modeled as a path down the tree .
this is achieved by dening the tree procedurally in terms of a linked set of chinese restaurants .
123 hidden markov models with innite state spaces hidden markov models ( hmms ) are widely used to model sequential data and time series data ( rabiner , 123 ) .
an hmm is a doubly - stochastic markov chain in which a state sequence , 123 , 123 , .
, , is drawn according to a markov chain on a discrete state space with transition kernel ( t , t+123 ) .
a corresponding sequence of observations , x123 , x123 , .
, x , is drawn conditionally on the state se - quence , where for all t the observation xt is conditionally independent of the other observations given the state t .
we let ft ( xt ) denote the distribution of xt conditioned on the state t; this is referred to as the emission distribution .
in this section we show how to use bayesian nonparametric ideas to obtain an innite hmman hmm with a countably innite state space ( beal et al . , 123; teh et al . , 123 ) .
the idea is similar in spirit to the passage from a nite mixture model to a dp mixture model .
however , as we show , the appropriate nonparametric tool is the hdp , not the dp .
the resulting model is thus referred to as the hierarchical dirichlet process hidden markov model ( hdp - hmm ) .
we present both the hdp formulation and a stick - breaking formulation in this section; the latter is particularly helpful in understanding the relationship to nite hmms .
it is also worth noting that a chinese restaurant franchise ( crf ) representation of the hdp - hmm can be developed , and indeed beal et al .
( 123 ) presented a precursor to the hdp - hmm that was based on an urn model akin to the crf .
to understand the need for the hdp rather than the dp , note rst that a classical hmm species a set of nite mixture distributions , one for each value of the current state t .
indeed , given t , the observation xt+123 is chosen by rst picking a state t+123 and then choosing xt+123 conditional on that state .
thus the transition probability ( t , t+123 ) plays the role of a mixing proportion and the emission distribution ft plays the role of the mixture component .
it is natural to consider replacing this nite mixture model by a dp mixture model .
in so doing , however , we must take into account the fact that we obtain a set of dp mixture models , one for each value of the current state .
if these dp mixture models are not tied in some way , then the set of states accessible in a given value of the current state will be disjoint from those accessible for some other value of the current state .
we would obtain a branching structure rather than a chain structure .
the solution to this problem is straightforwardwe use the hdp to tie the dps .
more formally , let us consider a collection of random transition kernels ,
figure 123 : hdp hidden markov model .
( g : ) , drawn from an hdp : g123 | , h dp ( , h ) g | , g123 dp ( , g123 )
where h is a base measure on the probability space ( , t ) .
as we shall see , the random base measure g123 allows the transitions out of each state to share 123 be a predened initial state .
the the same set of next states .
let 123 = conditional distributions of the sequence of latent state variables 123 , .
, and observed variables x123 , .
, x are :
t | t123 , gt123
xt | t ft
for t = 123 ,
a graphical model representation for the hdp - hmm is shown in figure 123
we have dened a probability model consisting of an uncountable number of dps , which may raise measure - theoretic concerns .
these concerns can be dealt with , however , essentially due to the fact that the sample paths of the hdp - hmm only ever encounter a nite number of states .
to see this more clearly , and to understand the relationship of the hdp - hmm to the parametric hmm , it is helpful to consider a stick - breaking representation of the hdp - hmm .
this representation is obtained directly from the stick - breaking representation of the
123x123x123xg123gh underlying hdp :
k | h h
| , dp ( , ) .
for l = 123 , 123 ,
for k = 123 ,
123 and the atoms
k are shared across g123 and the transition distributions g
all states visited by the hmm are drawn from the transition distributions , the states possibly visited by the hmm with positive probability ( given g123 ) will consist only of the initial state 123 , .
relating to the parametric hmm , we see that the transition probability from state
k is given by k and the distribution on the observations is given by f this relationship to the parametric hmm can be seen even more clearly if we k with the integer k , for k = 123 , 123 , .
, , and if we introduce identify the state integer - valued variables zt to denote the state at time t .
in particular , if t = is the state at time t , we let zt take on value k , and write k instead of the hdp - hmm can now be expressed as : zt | zt123 , zt123 xt | zt ,
with priors on the parameters and transition probabilities given by eq .
this construction shows explicitly that the hdp - hmm can be interpreted as an hmm with a countably innite state space .
a diculty with the hdp - hmm as discussed thus far is that it tends to be poor at capturing state persistence; it has a tendency to create redundant states and rapidly switch among them .
this may not be problematic for ap - plications in which the states are nuisance variables and it is overall predictive likelihood that matters , but it can be problematic for segmentation or parsing applications in which the states are the object of inference and when state per - sistence is expected .
this problem can be solved by giving special treatment to self - transitions .
in particular , let g denote the transition kernel associated with state .
fox et al .
( 123 ) proposed the following altered denition of g ( compare to eq .
( 123 ) ) :
g | , , g123 , dp
where is a point mass at and where is a parameter that determines the extra mass placed on a self - transition .
to see in more detail how this aects state persistence , consider the stick - breaking weights associated with one of the countably many states k that can be visited by the hmm .
the stick - breaking is altered as follows ( compare to eq .
( 123 ) ) : representation of g
| , , dp
fox et al .
( 123 ) further place a vague gamma prior on + and a beta prior on / ( + ) .
the hyperparameters of these distributions allow prior control of state persistence .
see also beal et al .
( 123 ) , who develop a related prior within the framework of their hierarchical urn scheme .
123 applications of the hdp - hmm in the following sections we describe a number of applications and extensions of the hdp - hmm .
an application that we will not discuss but is worth mention - ing is the application of hdp - hmms to the problem of modeling recombination hotspots and ancestral haplotypes for short segments of single nucleotide poly - morphisms ( xing and sohn , 123 ) .
123 . 123 speaker diarization speech recognition has been a major application area for classical parametric hmms ( huang et al . , 123 ) .
in a typical application , several dozen states are used , roughly corresponding to the number of phoneme - like segments in speech .
the observations xt are spectral representations of speech over short time slices .
in many applications , however , the number of states is more fundamentally part of the inferential problem and it does not suce to simply x an arbi - trary value .
consider an audio recording of a meeting in which the number of people participating in the meeting is unknown a priori .
the problem of speaker diarization is that of segmenting the audio recording into time intervals associated with individual speakers ( wooters and huijbregts , 123 ) .
here it is natural to consider an hdp - hmm model , where a state corresponds to an individual speaker and the observations are again short - term spectral represen - tations .
posterior inference in the hdp - hmm yields estimates of the spectral content of each speakers voice , an estimate of the number of speakers partici - pating in the meeting , and a diarization of the audio stream .
such an application of the hdp - hmm has been presented by fox et al .
( 123 ) , who showed that the hdp - hmm approach yielded a state - of - the - art diarization method .
a noteworthy aspect of their work is that they found that the special treatment of self - transitions discussed in the previous section was essential; without this special treatment the hdp - hmms tendency to rapidly switch among redundant states led to poor speaker diarization performance .
123 . 123 word segmentation as another application of the hdp - hmm to speech , consider the problem of segmenting an audio stream into a sequence of words .
speech is surprisingly continuous with few obvious breaks between words and the problem of word seg - mentationthat of identifying coherent segments of words and their bound - aries in continuous speechis nontrivial .
goldwater et al .
( 123b ) proposed a statistical approach to word segmentation based upon the hdp - hmm .
the latent states of the hmm correspond to words .
an hdp - hmm rather than a parametric hmm is required for this problem , since there are an unbounded number of potential words .
in the model , an utterance is viewed as a sequence of phonemes , 123 , 123 , .
the sequence is modeled by an hdp - hmm in which words are the latent states .
a word is itself a sequence of phonemes .
the model specication is as follows .
first , the number of words n is drawn from a geometric distribution .
then a sequence of n words , 123 , 123 , .
, n , is drawn from an hdp - hmm :
g123 | , h dp ( , h ) g | , g123 dp ( , g123 )
i | i123 , gi123
for i = 123 ,
where 123 g is a draw from an initial state distribution .
each g is the transition distribution over next words , given the previous word .
this is dened for every possible word , with the set of all possible words ( including the empty word 123 which serves as an initial state for the markov chain ) .
the base measure h over words is a simple independent phonemes model : the length of the word , l 123 , is rst drawn from another geometric distribution , then each phoneme ri is drawn independently from a prior over phonemes :
h ( = ( r123 , r123 , .
, rl ) ) = 123 ( 123 123 ) l123
where h123 is a probability measure over individual phonemes .
the probability of the observed utterance is then a sum over probabilities of sequences of words such that their concatenation is 123 , 123 ,
goldwater et al .
( 123b ) have shown that this hdp - hmm approach leads
to signicant improvements in segmentation accuracy .
123 . 123 trees and grammars a number of other structured probabilistic objects are amenable to a nonpara - metric treatment based on the hdp .
in this section we briey discuss some recent developments which go beyond the chain - structured hmm to consider objects such as trees and grammars .
a hidden markov tree ( hmt ) is a directed tree in which the nodes correspond to states , and in which the probability of a state depends ( solely ) on its unique
parent in the tree .
to each state there is optionally associated an observation , where the probability of the observation is conditionally independent of the other observations given the state ( chou et al . , 123 ) .
we can generalize the hdp - hmm to a hierarchical dirichlet process hidden markov tree ( hdp - hmt ) model in which the number of states is unbounded .
this is achieved by a generalization of the hdp - hmm model in which the tran - sition matrix along each edge of the hmt is replaced with sets of draws from a dp ( one draw for each row of the transition matrix ) and these dps are tied with the hdp .
this model has been applied to problems in image processing ( denoising , scene recognition ) in which the hdp - hmt is used to model corre - lations among wavelet coecients in multiresolution models of images ( kivinen et al . , 123a , b ) .
as a further generalization of the hdp - hmm , several groups have considered nonparametric versions of probabilistic grammars ( johnson et al . , 123; liang et al . , 123; finkel et al . , 123 ) .
these grammars consist of collections of rules , of the form a bc , where this transition from a symbol a to a pair of symbols bc is modeled probabilistically .
when the number of grammar symbols is unknown a priori , it is natural to use the hdp to generate symbols and to tie together the multiple occurrences of these symbols in a parse tree .
123 hierarchical pitman - yor processes as discussed in chapter ? ? , a variety of alternatives to the dp have been ex - plored in the bayesian nonparametrics literature .
these alternatives can provide a better t to prior beliefs than the dp .
it is therefore natural to consider hi - erarchical models based on these alternatives .
in this section we shall describe one such hierarchical model , the hierarchical pitman - yor ( hpy ) process , which is based on the pitman - yor process ( also known as the two - parameter poisson - dirichlet process ) .
we briey describe the pitman - yor process here; section ? ? in chapter ? ? as well as perman et al .
( 123 ) , pitman and yor ( 123 ) and ish - waran and james ( 123 ) present further material on the pitman - yor process .
in section 123 . 123 we describe an application of the hpy process to language modeling .
section 123 . 123 presents a spatial extension of the hpy process and an application to image segmentation .
123 pitman - yor processes the pitman - yor process is a two - parameter generalization of the dp , with a dis - count parameter 123 d < 123 and a concentration parameter > d .
when d = 123 the pitman - yor process reduces to a dp with concentration parameter .
we write g py ( d , , h ) if g is a pitman - yor process with the given parameters and base measure h .
the stick - breaking construction and the chinese restau - rant process have natural generalizations in the pitman - yor process .
a draw g
from the pitman - yor process has the following stick - breaking construction :
where the atoms
k are drawn iid from h , and the weights are obtained as
vk | d , beta ( 123 d , + kd )
for k = 123 ,
k = vk
we refer to the joint distribution over 123 , 123 , .
as the gem ( d , ) distribution , this being a two - parameter generalization of the one - parameter gem ( ) asso - ciated with the dp .
suppose that h is a smooth distribution and let 123 , 123 , .
be iid draws from g .
marginalizing out g , the distribution of i conditioned on 123 , .
, i123 follows a generalization of the polya urn scheme :
i | 123 , .
, i123 , d , , h k ( cid : 123 )
+ i 123
+ + kd
+ i 123 h ,
t is the tth unique value among 123 , .
, i123 , there being nt occurrences t , and k such unique values .
in the chinese restaurant analogy , each i is t corresponds to a table , and customer i sits at table t if i = there are two salient properties of this generalized chinese restaurant process .
first , the rich - gets - richer property of the original chinese restaurant process is preserved , which means that there are a small number of large tables .
second , there are a large number of small tables since the probability of occupying new tables grows along with the number of occupied tables , and the discount d decreases the probabilities of new customers sitting at small tables .
when 123 < d < 123 the pitman - yor process yields power - law behavior ( pitman , 123; goldwater et al . , 123a; teh , 123a , see also chapter ? ? ) .
it is this power - law behavior which makes the pitman - yor process more suitable than the dp for many applications involving natural phenomena .
the power - law nature of the pitman - yor process can be expressed in several ways .
first , under eq .
( 123 ) we have e ( k ) o ( k123 / d ) if 123 < d < 123 , which indicates that cluster sizes decay according to a power law .
second , zipfs law can be derived from the chinese restaurant process; that is , the proportion of tables with n customers scales as o ( n123d ) .
finally the chinese restaurant process also yields heaps law , where the total number of tables in a restaurant with n customers scales as o ( nd ) .
note that the discount parameter d is the key parameter governing the power - law behavior .
these various power laws are illustrated in figure 123
figure 123 : power - law behavior of the pitman - yor process .
left : e ( k ) vs .
middle : number of tables in restaurant vs .
number of customers .
right : number of tables vs .
number of customers at each table .
each plot shows the results of 123 draws ( small dots ) and their mean ( large dots ) .
the log - log plots are well approximated by straight lines , indicating power laws .
123 hierarchical pitman - yor processes the hierarchical pitman - yor ( hpy ) process is dened in the obvious manner :
g123 | , , h py ( , , h ) gj | d , , g123 py ( d , , g123 )
for j j ,
where g123 is the common base measure shared across the dierent pitman - yor processes gj , and is itself given a pitman - yor process prior .
similarly to the hdp , this hierarchical construction generalizes immediately to a multiple - level
recall that one of the useful facts about the hdp is that it can be represented using both a stick - breaking representation and a chinese restaurant franchise representation .
it would be of interest to consider generalizations of these ob - jects to the hpy process .
as we shall see in the following , the chinese restaurant franchise can be readily generalized to an hpy analog .
unfortunately , however , there is no known analytic form for the stick - breaking representation of the hpy recall that in the chinese restaurant franchise representation , each gj cor - responds to a restaurant , draws ji gj correspond to customers , tables t in jt g123 , and dishes correspond to draws restaurant j corresponds to draws k h .
let njtk be the number of customers in restaurant j seated at table t
and eating dish k , mjk be the number of tables in restaurant j serving dish k , and k be the number of dishes served throughout the franchise .
the conditional distributions given by the chinese restaurant franchise for the hpy process are
ji | j123 , .
, j , i123 , , d , g123 mj ( cid : 123 ) j , t123 , , , h k ( cid : 123 )
+ + mjd + + k
which is a natural generalization of the crf for the hdp ( cf .
( 123 ) and
123 applications of the hierarchical pitman - yor process in this section we describe an application of the hpy process to language mod - eling and another application to image segmentation .
123 . 123 language modeling statistical models of sentences in a natural language ( e . g .
english ) are an in - dispensable component of many systems for processing linguistic data , includ - ing speech recognition , handwriting recognition and machine translation sys - tems ( manning and schutze , 123 ) .
in this section we describe an application of the hierarchical pitman - yor process in statistical language modeling .
most statistical language models treat sentences as drawn from markov mod - els of xed order larger than one .
that is , the probability of a sentence consisting of a sequence of words ( 123 , 123 , .
, ) is modeled as
, ) =
p ( t | tn+123 , .
, t123 ) ,
where for simplicity n+123 , .
, 123 are special start - of - sentence symbols , and n 123 is one plus the order of the markov model .
such models are known as n - gram models .
in typical applications n = 123 , corresponding to a second - order markov model and a context consisting of just the previous two words .
in natural languages the size of the vocabulary typically consists of more than 123 words .
this means that in a 123 - gram model the number of param - eters is in excess of 123 , making maximum likelihood estimation infeasible .
in fact a nave prior treating parameters corresponding to dierent contexts independently performs badly as wellit is important to model dependencies across dierent contexts for a language model to be successful .
in the language modeling community such dependencies are achieved by a variety of heuristic smoothing algorithms , which combine the counts associated with dierent con - texts in various ways ( chen and goodman , 123 ) .
it is also possible to take a hierarchical bayesian point of view on smoothing , and indeed such an approach was considered in a parametric setting by mackay and peto ( 123 ) .
however , word occurrences in natural languages tend to follow
power laws , and a nonparametric model such as the hpy process provides a more natural prior for this domain ( teh , 123a , b; goldwater et al . , 123a ) .
indeed , the most successful heuristic smoothing methods are closely related to an hpy model .
given a context u consisting of a sequence of words , let gu be the distri - bution over the next word following the context u .
that is , gu ( ) = p ( t = | tn+123 , .
, t123 = u ) in eq .
we place a pitman - yor prior on gu , with base measure gpa ( u ) , where pa ( u ) is the context with the rst word dropped
gu | d|u| , |u| , gpa ( u ) py ( d|u| , |u| , gpa ( u ) ) .
the parameters of the pitman - yor process depend on the length of the context |u| .
we recursively place a pitman - yor prior on gpa ( u ) , dropping words from the front of the context until g , the distribution over next words given the empty context .
finally we place a pitman - yor prior on g :
g | d123 , 123 , g123 py ( d123 , 123 , g123 ) ,
where g123 is the uniform distribution over the vocabulary .
the structure of this hierarchical prior reects the notion that more recent words in the context are more informative in predicting the next word .
teh ( 123a , b ) applied the hpy language model to a 123 - million word corpus , and found that it produces state - of - the - art prediction results , closely matching results using interpolated and modied kneser - ney , two of the most widely - used smoothing algorithms ( chen and goodman , 123 ) .
moreover , the hpy language model has been shown to outperform modied kneser - ney in the con - text of an application to dialog transcription ( huang and renals , 123 ) .
these results are unsurprising , as teh ( 123a , b ) and goldwater et al .
( 123a ) showed that interpolated kneser - ney can be derived as an approximation to the crf representation of the hpy language model .
in particular , interpolated kneser - ney assumes that the number of tables in each restaurant serving each dish is at most one .
this is the same approximation as in section 123 . 123
123 . 123 image segmentation models based on the pitman - yor process have also had impact in the eld of image processing , a eld that shares with the language modeling domain the fact that power laws characterize many of the statistics within the domain .
in par - ticular , using a database of images that were manually segmented and labeled by humans ( oliva and torralba , 123 ) , sudderth and jordan ( 123 ) have shown that both the segment sizes and the label occurrences ( e . g . , sky , grass ) fol - low long - tailed distributions that are well captured by the pitman - yor process .
this suggests considering models in which the marginal distributions at each site in an image are governed by pitman - yor processes .
moreover , to share information across a collection of images it is natural to consider hpy priors .
in this section we describe a model based on such an hpy prior ( sudderth
and jordan , 123 ) .
our focus is the problem of image segmentation , where the observed data are a collection of images ( an image is a collection of gray - scale or color values at each point in a two - dimensional grid ) and the problem is to output a partition of each image into segments ( a segment is a coherent region of the image , as dened by human labelings ) .
let us consider a generative model for image texture and color , simplifying at rst in two ways : ( 123 ) we focus on a single image and ( 123 ) we neglect the issue of spatial dependency within the image .
thus , for now we focus simply on obtaining pitman - yor marginal statistics for segment sizes and segment labels within a single image .
let us suppose that the image is represented as a large collection of sites , where a site is a local region in the image ( often referred to as a pixel or a super - pixel ) .
let gem ( d , ) be a draw from the two - parameter gem distribution .
for each site i , let ti denote the segment assignment of site i , where ti discrete ( ) are independent draws from .
given a large number of sites of equal size , the total area assigned to segment t will be roughly t , and segment sizes will follow pitman - yor statistics .
we also assign a label to each segment , again using a two - parameter gem distribution .
in particular , let gem ( , ) be a distribution across labels .
for each segment t we label the segment by drawing kt discrete ( ) indepen - dently .
we also let k denote an appearance model123 for label type k , where k are drawn from some prior distribution h .
putting this together , the label assigned to site i is denoted kti .
the visual texture and color at site i are then generated by a draw from the distribution
to obtain a spatially dependent pitman - yor process , sudderth and jordan ( 123 ) adapt an idea of duan et al .
( 123 ) , who used a latent collection of gaussian processes to dene a spatially dependent set of draws from a dirich - let process .
in particular , to each index t we associate a zero - mean gaussian process , ut .
at a given site i , we thus have an innite collection of gaussian random variables , ( uti ) t=123 , . . . , .
by an appropriate choice of thresholds for this innite sequence of gaussian variables , it is possible to mimic a draw from the distribution ( by basing the selection on the rst gaussian variable in the se - quence that is less than its threshold ) .
indeed , for a single site , this is simply a change - of - variables problem from a collection of beta random variables to a collection of gaussian random variables .
the gaussian process framework cou - ples the choice of segments at nearby sites via the covariance function .
figure 123 gives an example of three draws from this model , showing the underlying ran - dom distribution ( truncated to four values ) , the corresponding collection of draws from gaussian processes ( again truncated ) , and the resulting segmented
this framework applies readily to multiple images by coupling the label k across multiple images .
letting j j distribution and appearance models index the images in the collection , we associate a segment distribution j with each image and associate a set of gaussian processes with each image to describe
123this parameter is generally a multinomial parameter encoding the probabilities of various
discrete - valued texture and color descriptors .
figure 123 : draws from dependent pitman - yor processes .
top : the random pro - portions j .
middle : draws from gaussian processes , one for each entry in j .
bottom : resulting segmentation .
the segmentation of that image .
the image segmentation problem can be cast as posterior inference in this hpy - based model .
given an image represented as a collection of texture and color descriptors , we compute the maximum a posteriori set of segments for the sites .
sudderth and jordan ( 123 ) have shown that this procedure yields a state - of - the - art unsupervised image segmentation algorithm .
123 the beta process and the indian buet pro -
the dp mixture model embodies the assumption that the data can be parti - tioned or clustered into discrete classes .
this assumption is made particularly clear in the chinese restaurant representation , where the table at which a data point sits indexes the class ( the mixture component ) to which it is assigned .
if we represent the restaurant as a binary matrix in which the rows are the data points and the columns are the tables , we obtain a matrix with a single one in each row and all other elements equal to zero .
a dierent assumption that is natural in many settings is that objects can be
described in terms of a collection of binary features or attributes .
for example , we might describe a set of animals with features such as diurnal / nocturnal , avian / non - avian , cold - blooded / warm - blooded , etc .
forming a binary matrix in which the rows are the objects and the columns are the features , we obtain a matrix in which there are multiple ones in each row .
we will refer to such a representation as a featural representation .
a featural representation can of course be converted into a set of clusters if desired : if there are k binary features , we can place each object into one of 123k clusters .
in so doing , however , we lose the ability to distinguish between classes that have many features in common and classes that have no features in common .
also , if k is large , it may be infeasible to consider models with 123k parameters .
using the featural representation , we might hope to construct models that use on the order of k parameters to describe 123k classes .
in this section we discuss a bayesian nonparametric approach to featural representations .
in essence , we replace the dirichlet / multinomial probabilities that underlie the dirichlet process with a collection of beta / bernoulli draws .
this is achieved via the beta process , a stochastic process whose realizations provide a countably innite collection of coin - tossing probabilities .
we also discuss some other representations of the beta process that parallel those for the dp .
in particular we describe a stick - breaking construction as well as an analog of the chinese restaurant process known as the indian buet process .
123 the beta process and the bernoulli process the beta process is an instance of a general class of stochastic processes known as completely random measures ( kingman , 123 , see also chapter ? ? ) .
the key property of completely random measures is that the random variables obtained by evaluating a random measure on disjoint subsets of the probability space are mutually independent .
moreover , draws from a completely random measure are discrete ( up to a xed deterministic component ) .
thus we can represent such a draw as a weighted collection of atoms on some probability space , as we do for the dp .
( note , however , that the dp is not a completely random measure because the weights are constrained to sum to one for the dp; thus , the independence assertion does not hold for the dp .
the dp can be obtained by normalizing a completely random measure ( specically the gamma process;
applications of the beta process in bayesian nonparametric statistics have mainly focused on its use as a model for random hazard functions ( hjort , 123 , see also chapter ? ? ) .
in this case , the probability space is the real line and it is the cumulative integral of the sample paths that is of interest ( yielding a random , nondecreasing step function ) .
in the application of the beta process to featural representations , on the other hand , it is the realization itself that is of interest and the underlying space is no longer restricted to be the real line .
following thibaux and jordan ( 123 ) , let us thus consider a general proba - bility space ( , ) endowed with a nite base measure b123 ( note that b123 is not a probability measure; it does not necessarily integrate to one ) .
intuitively we
path and the red curve is the corresponding cumulative integral ( cid : 123 ) x
figure 123 : ( a ) a draw b bp ( 123 , u ( 123 , 123 ) ) .
the set of blue spikes is the sample 123 samples from bep ( b ) , one sample per row .
note that a single sample is a set of unit - weight atoms .
wish to partition into small regions , placing atoms into these regions accord - ing to b123 and assigning a weight to each atom , where the weight is a draw from a beta distribution .
a similar partitioning occurs in the denition of the dp , but in that case the aggregation property of dirichlet random variables immediately yields a consistent set of marginals and thus an easy appeal to kolmogorovs theorem .
because the sum of two beta random variables is not a beta random variable , the construction is somewhat less straightforward in the beta process
the general machinery of completely random processes deals with this issue in an elegant way .
consider rst the case in which b123 is absolutely continuous and dene the levy measure on the product space ( 123 , 123 ) in the following
( d , d ) = c123 ( 123 ) c123db123 ( d ) ,
where c > 123 is a concentration parameter .
now sample from a nonhomogeneous poisson process with the levy measure as its rate measure .
this yields a set of atoms at locations ( 123 , 123 ) , ( 123 , 123 ) .
dene a realization of the beta
is an atom at k with k its mass in b .
we denote this stochastic process as b bp ( c , b123 ) .
figure 123 ( a ) provides an example of a draw from bp ( 123 , u ( 123 , 123 ) ) , where u ( 123 , 123 ) is the uniform distribution on ( 123 , 123 ) .
we obtain a countably innite set of atoms from this construction because the levy measure in eq .
( 123 ) is - nite with innite mass .
indeed , consider
123draw partitioning the product space ( 123 , 123 ) into stripes having equal integral under this density .
these stripes have the same nite rate under the poisson process , and there are an innite number of such stripes .
note also that the use of a limiting form of the beta density implies that most of the atoms are associated with very small weights .
campbells theorem shows that the sum of these
weights is nite with probability one , since ( cid : 123 ) ( d , d ) < .
if b123 contains atoms , then these are treated separately .
in particular , denote the measure of the kth atom as qk ( assumed to lie in ( 123 , 123 ) ) .
the realization b necessarily contains that atom , with the corresponding weight k dened as an independent draw from beta ( cqk , c ( 123 qk ) ) .
the overall realization b is a sum of the weighted atoms coming from the continuous component and the discrete component of b123
let us now dene a bernoulli process bep ( b ) with an atomic base measure b as a stochastic process whose realizations are collections of atoms of unit mass on .
atoms can only appear at the locations of atoms of b .
whether or not an atom appears is determined by independent tosses of a coin , where the probability of success is the corresponding weight of the atom in b .
after n draws from bep ( b ) we can ll a binary matrix that has n rows and an innite number of columns ( corresponding to the atoms of b arranged in some order ) .
most of the entries of the matrix are zero while a small ( nite ) number of the entries are equal to one .
figure 123 ( b ) provides an example .
the beta process and the bernoulli process are conjugate .
consider the
b | c , b123 bp ( c , b123 ) zi | b bep ( b ) ,
for i = 123 ,
where z123 , .
, zn are conditionally independent given b .
the resulting posterior distribution is itself a beta process , with updated parameters :
b | z123 , .
, zn , c , b123 bp
c + n ,
a a + ( cid : 123 )
this formula can be viewed as an analog of standard nite - dimensional beta / bernoulli updating .
indeed , given a prior beta ( a , b ) , the standard update takes the form i zi .
( 123 ) , c plays the role of a + b and
i zi and b b + n ( cid : 123 )
cb123 is analogous to a .
123 the indian buet process recall that the chinese restaurant process can be obtained by integrating out the dirichlet process and considering the resulting distribution over partitions .
in the other direction , the dirichlet process is the random measure that is guar - anteed ( by exchangeability and de finettis theorem ) to underlie the chinese restaurant process .
in this section we discuss the analog of these relationships for the beta process .
we begin by dening a stochastic process known as the indian buet process ( ibp ) .
the ibp was originally dened directly as a distribution on ( equivalence classes of ) binary matrices by griths and ghahramani ( 123 ) and ghahra - mani et al .
( 123 ) .
the ibp is an innitely exchangeable distribution on these equivalence classes , thus it is of interest to discover the random measure that must underlie the ibp according to de finettis theorem .
thibaux and jordan ( 123 ) showed that the underlying measure is the beta process; that is , the ibp is obtained by integrating over the beta process b in the hierarchy in eq .
the ibp is dened as follows .
consider an indian buet with a countably - innite number of dishes and customers that arrive in sequence in the buet line .
let z denote a binary - valued matrix in which the rows are customers and the columns are the dishes , and where z nk = 123 if customer n samples dish k .
the rst customer samples poisson ( ) dishes , where = b123 ( ) is the total mass of b123
a subsequent customer n samples dish k with probability mk mk is the number of customers who have previously sampled dish k; that is , nk bernoulli ( mk c+n123 ) .
having sampled from the dishes previously sampled by other customers , customer n then goes on to sample an additional number of new dishes determined by a draw from a poisson ( to derive the ibp from the beta process , consider rst the distribution eq .
( 123 ) for n = 123; in this case the base measure is simply b123
drawing from b bp ( b123 ) and then drawing z123 bep ( b ) yields atoms whose locations are dis - tributed according to a poisson process with rate b123; the number of such atoms is poisson ( ) .
now consider the posterior distribution after z123 , .
, zn123 have been observed .
the updated base measure is i=123 zi .
treat the discrete component and the continuous component separately .
the discrete i=123 zi , can be reorganized as a sum over the unique values of the atoms; let mk denote the number of times the kth atom appears in one of the previous zi .
we thus obtain draws k beta ( ( c+n123 ) qk , ( c+n123 ) ( 123qk ) ) , where qk = mk c+n123 and thus ( under bernoulli sampling ) this atom appears in zn with probability mk c+n123
from the continu - c+n123 ) new atoms .
equating atoms with dishes , and rows of z with draws zn , we have obtained exactly the probabilistic specication of the ibp .
the expected value of k is mk
c+n123 b123 , we generate poisson (
c+n123 ) distribution .
c+n123 b123 + 123
123 stick - breaking constructions the stick - breaking representation of the dp is an elegant constructive character - ization of the dp as a discrete random measure ( chapter ? ? ) .
this construction can be viewed in terms of a metaphor of breaking o lengths of a stick , and it can also be interpreted in terms of a size - biased ordering of the atoms .
this section , we consider analogous representations for the beta process .
draws b bp ( c , b123 ) from the beta process are discrete with probability one , which gives hope that such representations exist .
indeed , we will show that there are two stick - breaking constructions of b , one based on a size - biased ordering of the atoms ( thibaux and jordan , 123 ) , and one based on a stick - breaking representation known as the inverse levy measure ( wolpert and ickstadt , 123 ) .
the size - biased ordering of thibaux and jordan ( 123 ) follows straightfor - wardly from the discussion in section 123 .
recall that the indian buet process is dened via a sequence of draws from bernoulli processes .
for each draw , a poisson number of new atoms are generated , and the corresponding weights in the base measure b have a beta distribution .
this yields the following truncated
kn | c , b123 poisson (
nk | c beta ( 123 , c + n 123 ) nk | b123 b123 / .
for n = 123 , .
, for k = 123 ,
it can be shown that this size - biased construction bn converges to b with prob - ability one .
the expected total weight contributed at step n is while the expected total weight remaining , in b bn , is c+n .
the expected total weight remaining decreases to zero as n , but at a relatively slow rate .
note also that we are not guaranteed that atoms contributed at later stages of the construction will have small weightthe sizes of the weights need not be in decreasing order .
the stick - breaking construction of teh et al .
( 123 ) can be derived from the inverse levy measure algorithm of wolpert and ickstadt ( 123 ) .
this algorithm starts from the levy measure of the beta process , and generates a sequence of weights of decreasing size using a nonlinear transformation of a one - dimensional poisson process to one with uniform rate .
in general this approach does not lead to closed forms for the weights; inverses of the incomplete beta function need to be computed numerically .
however for the one - parameter beta process ( where c = 123 ) we do obtain a simple closed form :
for k = 123 ,
vk | beta ( 123 , ) k | b123 b123 / .
again bk b as k , but in this case the expected weights decrease exponentially to zero .
further , the weights are generated in strictly decreasing order , so we are guaranteed to generate the larger weights rst .
figure 123 : stick - breaking construction for the dp and the one - parameter bp .
the lengths i are the weights for the dp and the lengths i are the weights for the bp .
the stick - breaking construction for the one - parameter beta process has an intriguing connection to the stick - breaking construction for the dp .
in par - ticular , both constructions use the same beta - distributed breakpoints vk; the dierence is that for the dp we use the lengths of the sticks just broken o as the weights while for the beta process we use the remaining lengths of the sticks .
this is depicted graphically in figure 123
123 hierarchical beta processes recall the construction of the hierarchical dirichlet process : a set of dirichlet processes are coupled via a random base measure .
a similar construction can be carried out in the case of the beta process : let the common base measure for a set of beta processes be drawn from an underlying beta process ( thibaux and jordan , 123 ) .
under this hierarchical bayesian nonparametric model , the featural representations that are chosen for one group will be related to the featural representations that are used for other groups .
we accordingly dene a hierarchical beta process ( hbp ) as follows :
b123 | , a bp ( , a ) bj | c , b123 bp ( c , b123 ) zji | bj bep ( bj )
for j j for i = 123 ,
where j is the set of groups and there are nj individuals in group j .
the hyperparameter c controls the degree of coupling among the groups : values of c yield realizations bj that are closer to b123 and thus a greater degree of overlap among the atoms chosen in the dierent groups .
as an example of the application of the hbp , thibaux and jordan ( 123 ) considered the problem of document classication , where there are |j | groups of documents and where the goal is to classify a new document into one of these groups .
in this case , zji is a binary vector that represents the presence or absence in the ith document of each of the words in the vocabulary .
the hbp yields a form of regularization in which the group - specic word probabilities
123 are shrunk towards each other .
this can be compared to standard laplace smoothing , in which word probabilities are shrunk towards a xed reference point .
such a reference point can be dicult to calibrate when there are rare words in a corpus , and thibaux and jordan ( 123 ) showed empirically that the hbp yielded better predictive performance than laplace smoothing .
123 applications of the beta process in the following sections we describe a number of applications of the beta process to hierarchical bayesian featural models .
note that this is a rather dierent class of applications than the traditional class of applications of the beta process to random hazard functions .
123 . 123 sparse latent variable models latent variable models play an essential role in many forms of statistical anal - ysis .
many latent variable models take the form of a regression on a latent vector; examples include principal component analysis , factor analysis and in - dependent components analysis .
paralleling the interest in the regression liter - ature in sparse regression models , one can also consider sparse latent variable models , where each observable is a function of a relatively small number of la - tent variables .
the beta process provides a natural way of constructing such models .
indeed , under the beta process we can work with models that dene a countably - innite number of latent variables , with a small , nite number of variables being active ( i . e . , non - zero ) in any realization .
consider a set of n observed data vectors , x123 , .
we use a beta process to model a set of latent features , z123 , .
, zn , where we capture interactions among the components of these vectors as follows :
b | c , b123 bp ( c , b123 ) zi | b bep ( b )
for i = 123 ,
as we have seen , realizations of beta and bernoulli processes can be expressed as weighted sums of atoms :
we view k as parametrizing feature k , while zi denotes the features that are active for item i .
in particular , z ik = 123 if feature k is active for item i .
the data point xi is modeled as follows :
yik | h h
xi | zi , , yi f ( k , yik ) k : z
for k = 123 ,
where yik is the value of feature k if it is active for item i , and the distribu - depends only on the active features , their values , and their
note that this approach denes a latent variable model with an innite number of sparse latent variables , but for each data item only a nite number of latent variables are active .
the approach would often be used in a predictive setting in which the latent variables are integrated out , but if the sparseness pattern is of interest per se , it is also possible to compute a posterior distribution over the latent variables .
observation of the linear combination ( cid : 123 )
there are several specic examples of this sparse latent variable model in the literature .
one example is an independent components analysis model with an innite number of sparse latent components ( knowles and ghahramani , 123; teh et al . , 123 ) , where the latent variables are real - valued and xi is a noisy ikyikk .
another example is the noisy - or model of wood et al .
( 123 ) , where the latent variables are binary and are interpreted as presence or absence of diseases , while the observations xi are binary vectors indicating presence or absence of symptoms .
123 . 123 relational models the beta process has also been applied to the modeling of relational data ( also known as dyadic data ) .
in the relational setting , data are relations among pairs of objects ( getoor and taskar , 123 ) ; examples include similarity judgments between two objects , protein - protein interactions , user choices among a set of options , and ratings of products by customers .
we rst consider the case in which there is a single set of objects and relations are dened among pairs of objects in that set .
formally , dene an observation as a relation xij between objects i and j in a collection of n objects .
each object is modeled using a set of latent features as in eq .
( 123 ) and eq .
the observed relation xij between objects i and j then has a conditional distribution that is dependent only on the features active in objects i and j .
for example , navarro and griths ( 123 ) modeled subjective similarity judgments between objects jk; note that this is a weighted sum of features active in both objects .
chu et al .
( 123 ) modeled high - throughput protein - protein interaction screens where the observed bind - ing anity of proteins i and j is related to the number of overlapping features jk , with each feature interpreted as a potential protein complex con - sisting of proteins containing the feature .
gorur et al .
( 123 ) proposed a non - parametric elimination by aspects choice model where the probability of a user choosing object i over object j is modeled as proportional to a weighted sum , jk ) , across features active for object i that are not active for object j .
note that in these examples , the parameters of the model , k , are the atoms of the beta process .
i and j as normally distributed with mean ( cid : 123 )
relational data involving separate collections of objects can be modeled with the beta process as well .
meeds et al .
( 123 ) modeled movie ratings , where the collections of objects are movies and users , and the relational data consists of
ratings of movies by users .
the task is to predict the ratings of movies not yet rated by users , using these predictions to recommend new movies to users .
these tasks are called recommender systems or collaborative ltering .
meeds et al .
( 123 ) proposed a featural model where movies and users are modeled using separate ibps .
let z be the binary matrix of movie features and y the matrix of user features .
the rating of movie i by user j is modeled as jl .
note that this dyadic model cannot be represented using two independent beta processes , since there is a parameter kl for each combination of features in the two ibps .
the question of what random measure underlies this model is an interesting one .
normally distributed with mean ( cid : 123 )
123 semiparametric models the nonparametric priors introduced in previous sections can be combined with more traditional nite - dimensional priors , as well as hierarchies of such priors .
in the resulting semiparametric models , the object of inference may be the nite - dimensional parameter , with the nonparametric component treated as a nuisance parameter to be integrated out .
in other cases , the nite - dimensional parameters are to be integrated out and aspects of the nonparametric component are the inferential focus .
in this section we describe two such semiparametric models based on the hdp .
the rst model couples the stick - breaking represen - tation of the hdp with a gaussian hierarchy , while the other is based on the chinese restaurant franchise representation of the hdp .
123 hierarchical dps with random eects an important characteristic of the hdp is that the same atoms appear in dif - ferent dps , allowing clusters to be shared across the dierent groups .
the hier - archical dp with random eects ( hdp+re ) model of kim and smyth ( 123 ) generalizes the hdp by allowing atoms in dierent dps to dier from each other to better capture group - specicity of cluster parameters .
this model is based on the stick - breaking representation for hdps .
we begin with the standard representation for the common random base measure g123 dp ( , h ) :
k | h h
for k = 123 ,
for each group j j , the weights and atoms for the group - specic gj dier from g123 in the following way :
j | dp ( , )
for j j for k = 123 ,
where t is a distribution centered at ; for example , t might be a normal distribution with mean .
kim and smyth ( 123 ) used the hdp+re model to model bumps in func - tional magnetic resonance imaging ( fmri ) data .
fmri analyses report areas of high metabolic activity in the brain that are correlated with external stim - uli in an attempt to discover the function of local brain regions .
such areas of activities often show up as bumps in fmri images , and each bump can be modeled well using a normal density .
an fmri image then consists of multiple bumps and can be modeled with a dp mixture .
each individual brain might have slightly dierent structure and might react dierently to the same stimuli , while each fmri machine has dierent characteristics .
the hdp+re model naturally captures such variations while sharing statistical strength across indi - viduals and machines .
123 analysis of densities and transformed dps in this section we describe another approach to introducing group - specic pa - rameters within the dp framework .
the common base measure g123 is still given a dp prior as in eq .
( 123 ) , while the group - specic random measures are dened
gj | h123 dp ( , h123 )
for j j .
in the particular case in which h and t are normal distributions with xed variances , this model has been termed the analysis of densities ( ande ) model by tomlinson and escobar ( 123 ) , who used it for sharing statistical strength among multiple density estimation problems .
sudderth et al .
( 123 ) called the model given by eq .
( 123 ) and eq .
( 123 ) a transformed dp .
the transformed dp is very similar to an hdp , the dierence being that the atoms in g123 are replaced by distributions parametrized by the atoms .
if these distributions are smooth the measures gj will not share atoms as in the hdp .
instead each atom in gj is drawn from t with probability k .
identifying an atom of gj with k , the chinese restaurant franchise representa - tion for the hdp can be generalized to the transformed dp .
we have customers ( draws from gj ) going into restaurants ( gj ) and sitting around tables ( draws
from h123 ) , while tables are served dishes ( atoms in g123 ) from a franchise - wide menu ( g123 ) .
in the hdp the actual dish served at the dierent tables that order the same dish are identical .
for the transformed dp the dishes that are served at dierent tables ordering the same dish on the menu can take on distinct
sudderth et al .
( 123 ) used the transformed dp as a model for visual scene analysis that can simultaneously segment , detect and recognize objects within the scenes .
each image is rst preprocessed into a set of low - level descriptors of local image appearances , and eq .
( 123 ) and eq .
( 123 ) are completed with a mixture model for these descriptors :
ji | gj gj xji | ji fji
for j j and i = 123 ,
where xji is one of nj image descriptors in image j and fji over image descriptors parameterized by ji .
is a distribution
the chinese restaurant franchise representation of the transformed dp trans - lates to a hierarchical representation of visual scenes , with scenes consisting of multiple objects and objects consisting of descriptors of local image appear - ances .
to see this , note that customers ( xji ) are clustered into tables ( object instances ) , and tables are served dishes from a global menu ( each object in - stance belongs to an object category ) .
there could be multiple tables in the same restaurant serving variations ( dierent seasonings ) on a given dish from the global menu .
this corresponds to the fact that there could be multiple in - stances of the same object category in a single visual scene , with each instance being in a dierent location or having dierent poses or lighting conditions ( thus yielding transformed versions of an object category template ) .
inference for hierarchical bayesian nonpara -
in this section we discuss algorithmic aspects of inference for the hierarchical bayesian nonparametric models that we have discussed in earlier sections .
our treatment will be brief and selective; in particular , we focus on relatively simple algorithms that help to convey basic methodology and provide a sense of some of the options that are available .
an underlying theme of this section is that the various mathematical representations available for nonparametric models including stick - breaking representations , urn models and truncationscan be combined in various ways to yield a wide range of possible algorithmic imple -
while we focus on sampling - based inference methods throughout this sec - tion , we also note that there is a growing literature on variational methods for in - ference in hierarchical bayesian nonparametric models; examples include liang et al .
( 123 ) ; sudderth and jordan ( 123 ) and teh et al .
( 123 ) .
inference for hierarchical dirichlet processes
we begin by considering posterior inference for a simple hdp mixture model .
in this model , the random measures gj are drawn from an hdp model according to eq .
( 123 ) , and this hdp prior is completed as follows :
ji | gj gj xji | ji fji
for i = 123 ,
where the ith observation in the jth group is denoted xji and where this obser - vation is drawn from a distribution fji indexed by ji .
the latent parameter ji is drawn from gj and can be viewed as indexing the mixture component associated with the data point xji .
we shall assume that h is conjugate to f for simplicity .
nonconjugate models can be treated by adapting techniques from the dp mixture literature ( cf .
neal , 123 ) .
teh et al .
( 123 ) presented sampling algorithms for the hdp mixture model based on both the crf representation and the stick - breaking representation .
in the following section we describe the crf - based sampler .
we then turn to an alternative sampler that is based on the posterior representation of the hdp described in section 123 .
123 . 123 chinese restaurant franchise sampler recall our notation for the crf representation of the hdp .
customer i in restaurant j is associated with an iid draw from gj and sits at table tji .
table t in restaurant j is associated with an iid draw from g123 and serves a dish kjt from a franchise - wide menu .
dish k is associated with an iid draw from h .
if h is an absolutely continuous measure then each such dish is unique with probability one .
there are njtk customers in restaurant j sitting at table t and eating dish k , and there are mjk tables in restaurant j serving dish k .
given this setup , we describe a gibbs sampler in which the table and dish assignment variables are iteratively sampled conditioned on the state of all other variables .
the variables consist of ( tji ) jj , i=123 , . . . , nj and ( kjt ) jj , t=123 , . . . , mj .
the parameters ji are integrated out analytically ( recall our assumption of conju - gacy ) .
consider the assignment of customer i in restaurant j to a table tji .
to resample tji we make use of exchangeability and imagine customer i being the last customer to enter restaurant j .
the customer can sit at an already occupied table , can sit at a new table and be served an existing dish , or can sit at a new table and be served a new dish .
the probabilities of these events are :
with probability nji tji = t tji = tnew , kjtnew = k tji = tnew , kjtnew = knew with probability
where tnew and knew denote a new table and new dish , respectively , and where superscript ji denotes counts in which customer i in restaurant j is removed from the crf ( if this empties a table we also remove that table from the crf along with the dish served on it ) .
the fractional terms are the conditional priors given by the crf in eq .
( 123 ) and eq .
( 123 ) , and fk ( ( xji ) ) is dened using the following general notation :
where d is an arbitrary index set , where dk = ( j ( cid : 123 ) i ( cid : 123 ) : kj ( cid : 123 ) t j ( cid : 123 ) i ( cid : 123 ) = k ) denotes the set of indices of data items currently associated with dish k , and where h ( ) and f ( ) denote the densities of h and f respectively .
in particular , fk ( ( xji ) ) is the marginal conditional probability of the singleton data point xji in cluster k , given all of the other data points currently assigned to cluster k .
the gibbs update for the dish kjt served at table t in restaurant j is derived
similarly .
the probabilities of the relevant events in this case are : fk ( ( xji : tji = t ) ) fknew ( ( xji : tji = t ) ) .
with probability mjt
knew with probability
while the computational cost of the gibbs updates is generally dominated by the computation of the marginal conditional probabilities fk ( ) , the number of possible events that can occur at one gibbs step is one plus the total number of tables or dishes in all restaurants that are ancestors of j , and this number can be large in deep or wide hierarchies .
a drawback of the crf sampler is that it couples sampling in the various restaurants ( since all dps are integrated out ) .
this coupling makes deriving a crf sampler for certain models ( e . g .
the hdp - hmm ) dicult .
an alternative is to construct samplers that use a mixed representationsome dps in stick - breaking representation and some in crp representationand thereby decouple the restaurants ( teh et al . , 123 ) .
the crf - based sampler can be easily extended to arbitrary hierarchies .
it can also be extended to the hierarchical pitman - yor process discussed in
123 . 123 posterior representation sampler in section 123 we showed that the posterior of the hdp consists of a discrete part corresponding to mixture components associated with data and a contin - uous part corresponding to components not associated with data .
this repre - sentation can be used to develop a sampler which represents only the discrete part explicitly .
in particular , referring to eq .
( 123 ) and eq .
( 123 ) , the posterior
representation sampler maintains only the weights and ( j ) jj .
( the atoms k ) k=123 , . . . , k can be integrated out in the conjugate setting . ) we also make use of cluster index variables zji , dened so that ji = the sampler iterates between two phases : the sampling of the cluster indices ( zji ) , and the sampling of the weights and ( j ) .
the sampling of the cluster indices is a simple variation on the gibbs updates in the crf sampler described in section 123 . 123
in particular , we dene the following gibbs conditionals :
( i . e . , zji = kjtji
with probability jkfk ( ( xji ) )
knew with probability j123fknew ( ( xji ) ) .
if a new component knew is chosen , the corresponding atom is instantiated in the sampler .
specically , the weights corresponding to this new atom can be generated as follows :
for j j
v123 | beta ( , 123 ) k+123 ) = ( 123v123 , 123 ( 123 v123 ) )
vj | , 123 , v123 beta ( 123v123 , 123 ( 123 v123 ) ) j123 , new
j k+123 ) = ( j123vj , j123 ( 123 vj ) ) .
finally we set zji = k + 123 and increment k .
the second phase resamples the weights ( j ) jj and conditioned on the cluster indices ( zji ) .
the approach is to rst integrate out the random measures , leaving a crp representation as in section 123 , then the weights ( j ) jj and can be sampled conditionally on the state of the crf using eq .
( 123 ) and eq .
because we are conditioning on ( zji ) , and customers with dierent values of zji cannot be assigned to the same table , each restaurant eectively gets split into independent sub - restaurants , one for each value of k .
also the related direct assignment sampler in teh et al .
( 123 ) . ) let njk be the number of observations in group j assigned to component k , and let mjk be the random number of tables in a sub - restaurant with njk customers and concentration parameter k .
the ( mjk ) are mutually independent and thus a draw for each of them can be simulated using the crp .
we can now sample the and ( j ) using eq .
( 123 ) and eq
inference for hdp hidden markov models
the posterior representation sampler of section 123 . 123 can also be used to derive a gibbs sampler for the hdp - hmm .
consider the formulation of the hdp - hmm given in eq .
( 123 ) and eq .
( 123 ) where we make use of a sequence of latent indicator variables z123 , .
we again assume that h is conjugate to f .
note that the posterior of the hdp prior for the model ( given z123 , .
, z ) can be decomposed into a discrete part consisting of k atoms ( corresponding to the k states currently visited by z123 , .
, z ) , as well as a continuous part consisting of unused atoms .
the weights on the k atoms ( equivalently the transition
probabilities among the k states currently used by the hdp - hmm ) can be constructed from a crf representation of the hdp :
( 123 , 123 , .
, k ) dirichlet ( , m123 ,
for j = 123 ,
( j123 , j123 , .
, jk ) dirichlet ( 123 , 123 + nj123 , .
, k + njk ) where njk is the number of transitions from state j to state k ( equivalently the number of customers eating dish k in restaurant j ) , while mk is the number of tables serving dish k in the crf representation of the hdp .
the conditional probabilities for the gibbs update of zt are as follows : with probability zt123kkzt+123 knew with probability zt123zt+123
the three factors on the right - hand side are the probability of transitioning into the current state , the probability of transitioning out of the current state , and the conditional probability of the current observation xt respectively .
the zt+123 factor arises because transitions from the new state knew have not been observed before so we need to use the conditional prior mean .
the weights and transition probabilities j can be updated as for the posterior representation sampler for plain hdps .
this simple gibbs sampler can converge very slowly due to strong depen - dencies among the latent states ( scott , 123 ) .
to obtain a faster algorithm we would like to update the latent states in a block via the forward - backward al - gorithm for hmms; the traditional form of this algorithm cannot , however , be applied directly to the hdp - hmm since there are an innite number of possi - ble states .
the solution is to limit the number of states to a nite number so that the forward - backward algorithm becomes feasible .
fox et al .
( 123 ) pro - posed doing this via a truncation of the stick - breaking process ( cf .
ishwaran and james , 123 ) , while van gael et al .
( 123 ) proposed a slice sampling approach which adaptively limits the number of states to a nite number ( neal , 123;
inference for beta processes
in this section we describe a gibbs sampler for the beta process latent variable model described in section 123 . 123
this sampler is based on the stick - breaking representation of the beta process .
recall that the model is dened in terms of a set of feature weights ( k ) k=123 , . . . , and the atoms ( feature parameters ) ( k ) k=123 , . . . , .
moreover , corresponding to each data item xi , we have a set of binary feature activities ( z and latent feature values ( yik ) k=123 , . . . , .
the observed data item xi depends on the conditional distributions dening the model are given in eq .
( 123 ) and ik = 123| k ) = k .
gibbs sampling in this model is straight - eq .
( 123 ) , where p ( z forward except for a few diculties which we describe below along with their
the main diculty with a gibbs sampler is that there are an innite number of random variables that need to be sampled .
to circumvent this problem , teh et al .
( 123 ) propose to use slice sampling ( neal , 123; walker , 123 ) to adaptively truncate the representation to a nite number of features .
consider an auxiliary variable s with conditional distribution :
s| z , ( k ) k=123 , . . . , uniform
where the supremum in the range of s is the smallest feature weight k among the currently active features .
conditioned on the current state of the other variables a new value for s can easily be sampled .
conditioned on s , features for which k < s are forced to be inactive since making them active would make s lie outside its range .
this means that we only need to update the nite number of features for which k > s .
this typically includes all the active features , along with a small number of inactive features ( needed for the sampler to explore the use of new features ) .
a related issue concerns the representation of the model within the nite memory of the computer .
using the auxiliary variable s it is clear that we need only represent features 123 , .
, k , where k is such that k+123 < s; that is , the model is truncated after feature k .
as the values of s and the feature weights change over the course of gibbs sampling this value of k changes as well .
if k is decreased we simply delete the last few features , while if k is increased we sample the variables k , k and yik corresponding to these new features from their conditional distributions given the current state of the represented
the nal issue is the problem of sampling the feature weights 123 , .
unlike the case of dps , it is easier in this case to work with the weights directly instead of the stick - breaking variables vk .
in particular , teh et al .
( 123 ) showed that the joint probability for the weights is :
, k ) = i ( 123 k 123 123 ) k
where i ( ) = 123 if the predicate is true and 123 otherwise .
for k = 123 , .
, k 123 the conditional probability of k given the other variables can be computed from eq .
( 123 ) and the conditional probability of z nk given k .
for k we also have to condition on z ik = 123 for all i and k > k; this probability can be computed using the levy - khintchine representation for the beta process ( teh et al . , 123 ) .
inference for hierarchical beta processes
in this section we present an inference algorithm for the hierarchical beta pro - cess given in eq .
the observed data are the variables zji; these binary vectors denote ( in the language of document classication ) the presence or ab - sence of words in document i of group j .
the underlying measure space is
interpreted as the vocabulary .
( each element in is referred to as a word ) .
let 123 , .
, k denote the words that are observed among the documents .
that is , these are the such that zji ( ) = 123 for some i and j .
because both the beta and bernoulli processes are completely random mea - sures , the posterior over b123 and bj for j j decomposes into a discrete part over the observed vocabulary ( 123 , .
, k ) and a continuous part over \ ( 123 , .
the discrete part further factorizes over each observed word k .
thus it is sucient to focus separately on inference for each observed word and for the continuous part corresponding to unobserved words .
for a xed k , let a = a ( k ) , 123 = b123 ( k ) , j = bj ( k ) and zji = zji ( k ) .
the slice of the hbp corresponding to k has the following joint distribution :
123 | c123 , a beta ( c123a , c123 ( 123 a ) ) j | cj , 123 beta ( cj123 , cj ( 123 123 ) )
zji | j bernoulli ( j )
for j j for i = 123 ,
note that the prior over 123 is improper if a is continuous and a = 123
this beta hierarchy is a special case of the nite dirichlet hierarchy of eq .
( 123 ) and eq .
( 123 ) and it is straightforward to use the posterior representation sampler described in section 123 to sample from the posterior given the observed zji .
thibaux and jordan ( 123 ) described an alternative where the j are integrated out and rejection sampling is used to sample from 123
finally , we consider the continuous part of the posterior .
this component is not simply the prior , since we have to condition on the fact that no words in \ ( 123 , .
, k ) have been observed among the documents .
thibaux and jordan ( 123 ) solved this problem by noting that the posterior factors over the levels indexed by n in the size - biased ordering in eq .
focusing on each level separately , they derived a posterior distribution on the number of atoms in each level , combining this with the posterior over the level - specic weights to obtain the overall posterior .
our goal in this chapter has been to place hierarchical modeling in the same central role in bayesian nonparametrics that it plays in other areas of bayesian statistics .
indeed , one of the principal arguments for hierarchical modeling in parametric statistics is that it provides control over the large numbers of degrees of freedom that arise , for example , in random eects models .
such an argument holds a fortiori in the nonparametric setting .
nonparametric priors generally involve hyperparameters , some of which are nite - dimensional and some of which are innite - dimensional .
sharing the nite - dimensional parameter among multiple draws from such a prior is a natural modeling strategy that mimics classical hierarchical modeling concepts .
our contention , however , that this form of control is far too limited , and that the innite - dimensional parameters should generally also be shared .
we have
made this point principally by considering examples in applied problem domains .
in domains such as computational vision , information retrieval and genetics , nonparametric models provide natural descriptions of the complex objects under study; in particular , it is natural to describe an image , a document or a genome as the realization of a stochastic process .
now , in considering collections of such objects it is natural to want to share details of the realization among the objects in the collectionwe wish to share parts of objects , features , recurring phrases and motifs .
this can be achieved by coupling multiple draws from a nonparametric prior via their innite - dimensional parameters .
another advantage of hierarchical modeling in the classical setting is that it expands the repertoire of distributional forms that can be considered .
for example , heavy - tailed distributions can be obtained by placing a prior on the scale parameter of lighter - tailed distributions .
although this point has been little explored to date in the nonparametric setting , we expect that it will be a fruitful direction for further research .
in particular , there are stringent com - putational constraints that limit the nonparametric repertoire , and hierarchical constructions oer one way forward .
indeed , as we have seen , computationally oriented constructions such as urn models and stick - breaking representations often carry over naturally to hierarchical nonparametric models .
finally , it is worth noting a diculty that is raised by hierarchical mod - eling .
although bayesian hierarchies help to control hyperparameters , they do not remove the need to specify distributions for hyperparameters .
indeed , when hyperparameters are placed high in a hierarchy it can be dicult to give operational meaning to such hyperparameters .
one approach to coping with this issue involves considering the marginal probabilities that are induced by a nonparametric prior .
for example , we argued that the marginals induced by a pitman - yor prior exhibit long tails that provide a good match to the power - law behavior found in textual data and image statistics .
further research is needed to develop this kind of understanding for a wider range of hierarchical bayesian nonparametric models and problem domains .
we would like to thank david blei , jan gasthaus , sam gershman , tom grif - ths , kurt miller , vinayak rao and erik sudderth for their helpful comments on the manuscript .
