we propose a semiparametric model for regression and classi ( cid : 123 ) cation problems in - volving multiple response variables .
the model makes use of a set of gaussian processes to model the relationship to the inputs in a nonparametric fashion .
conditional depen - dencies between the responses can be captured through a linear mixture of the driving processes .
this feature becomes important if some of the responses of predictive interest are less densely supplied by observed data than related auxiliary ones .
we propose an e ( cid : 123 ) cient approximate inference scheme for this semiparametric model whose complexity is linear in the number of training data points .
123 semiparametric latent factor models
we are interested in predicting multiple responses yc 123 yc; c = 123; : : : ; c from covariates x 123 x , and we would like to model the responses as conditionally dependent .
in statistical terminology , we would like to \share statistical strength " between the yc; in machine learning parlance this is often referred to as \transfer of learning . " as we demonstrate empirically , such sharing can be especially powerful if the data for the responses is partially missing .
models related to the one proposed here are used in geostatistics and spatial prediction under the name of co - kringing ( 123 ) , and an example helps to demonstrate what we want to achieve with our technique .
after an accidental uranium spill , a spatial map of uranium concentration is sought .
we can take soil samples at locations of choice and measure their uranium content , then use kriging , gaussian process regression or another spatial prediction technique to infer a map .
however , carbon concentration is easier to measure than uranium so the space can be sampled more densely .
moreover , it is known that these two responses are often signi ( cid : 123 ) cantly correlated .
in co - kriging we setup a joint spatial model for several responses with the aim of improving our prediction of one of them .
the model to be de - scribed here can be used in the same context , but goes beyond simple co - kriging in several ways .
first , by using latent random ( cid : 123 ) elds our model can represent conditional dependencies between responses directly .
this is more ( cid : 123 ) exible and expressive than schemes in which re - gression functions for each response are mixed in a posthoc manner ( 123 ) , because the latent
( cid : 123 ) elds are ( cid : 123 ) tted using the data from all responses and can be used to model chacteristics of the dependencies rather than marginal relationships only .
second , the true nature of the dependencies does not have to be known in advance but can be learned from training data using empirical bayesian techniques .
writing y = ( yc ) c and introducing a latent variable v 123 rc , our model comes with a
p ( yjv ) =yc
we intend to model the prior p ( vjx ) using gaussian processes .
the simplest possibility is to assume that the vc are independent given x , i . e .
p ( vjx ) = qc p ( vcjx ) .
in this case we can represent p ( vcjx ) as a gaussian process ( gp ) with mean function 123 and covariance function ~ k ( c ) :
this model will be called the baseline model in the sequel .
e ( cid : 123 ) vc ( x ) vc123 ( x123 ) ( cid : 123 ) = ( cid : 123 ) c;c123 ~ k ( c ) ( x; x123 ) :
note that under the baseline model , the inference and learning task simply decompose into c independent ones .
the components of v are independent a posteriori , so even if there are dependencies in the data the prediction under the baseline model cannot pro ( cid : 123 ) t from them .
while this is often appropriate if the data is complete in all components yc , it can behave suboptimal in situations where part of the yc data is missing .
on the other end of the spectrum , we can model p ( vjx ) as a set of dependent gaussian processes with c ( c + 123 ) =123 cross - covariance functions .
tasks such as inference , hyperparam - eter learning and prediction can be performed in much the same way as in a single process model .
this model and related algorithms will be called the naive method .
approximate inference in nonparametric models typically scales superlinearly in the number of variables which can be dependent a posteriori .
if n is the number of training datapoints , we have to deal with n variables at a time in the baseline method , but with as many as c n in the naive one .
the latter scaling is usually not acceptable for large c n .
in this paper we propose a model in which vjx can be dependent in a ( cid : 123 ) exible ( and adaptive ) way , yet inference and learning is more tractable than for the naive model .
the key is to restrict the dependencies in a way which can be exploited in inference .
we introduce a second latent variable u 123 rp .
here and in the following it is understood that for typical applications of our model we will have p ( cid : 123 ) c .
for a mixing matrix ( cid : 123 ) 123 rc;p we set
v = ( cid : 123 ) u + v ( 123 )
where u and v ( 123 ) are independent .
the components v ( 123 ) c have independent gp priors with mean 123 and covariance function ~ k ( c ) , and the components up have independent zero - mean gp priors with kernel k ( p ) .
the baseline model is a special case ( p = 123 ) , but for p > 123 the vc will be dependent a posteriori .
note that the dependencies themselves are represented by nonparametric latent random ( cid : 123 ) elds u .
we refer to this setup as semiparametric latent factor model ( slfm ) , owing to the fact that the model combines nonparametric ( the processes u; v ) and parametric elements ( the mixing matrix ( cid : 123 ) j ) .
note that by integrating out the u processes , we obtain induced cross - covariance functions for x 123 ! v :
( cid : 123 ) c;p ( cid : 123 ) c123;pk ( p ) ( x; x123 ) :
e ( vc ( x ) vc123 ( x123 ) ) = ( cid : 123 ) c;c123 ~ k ( c ) ( x; x123 ) +xp
we can therefore perform inference and prediction using the naive method .
one goal of this paper is to exploit the structure in this particular setup in order to obtain a signi ( cid : 123 ) cantly more e ( cid : 123 ) cient method .
suppose we observe some independently and identically distributed data d = f ( x i; yi ) j i = 123; : : : ; ng .
we are interested in approximating the posterior p ( v ( cid : 123 ) jx ( cid : 123 ) ; d ) .
let v = ( vi;c ) i;c; vi;c = vc ( xi ) .
from the sampling model it is clear that for a test point x ( cid : 123 ) , v ( cid : 123 ) = v ( x ( cid : 123 ) ) is independent of d given v .
therefore ,
p ( v ( cid : 123 ) jd ) =z p ( v ( cid : 123 ) jv ) p ( vjd ) dv
which can be computed straightforwardly if a gaussian approximation to p ( vjd ) is known .
in section 123 we show how such an approximation can be represented and computed in an e ( cid : 123 ) cient way .
our framework can deal with missing values for yi;c e ( cid : 123 ) ortlessly .
while in the following we treat y = ( yi;c ) i;c as completely given for notational convenience , incorporating missing values amounts to nothing more than reducing the dimensionality of the vector y .
in this case , we let n be the maximum number of yi;c given for any single class c .
variables vi;c corresponding to a missing yi;c do not have direct evidence associated with them , but are constrained through u .
if yi = ( yi;c ) c is missing completely , the corresponding datapoint can be removed .
123 approximate inference
in this section we show how a sparse approximation to the posterior can be represented and updated as we condition on evidence .
we make use of the informative vector machine ( ivm ) ( 123 , 123 ) framework which allows for inference approximations within time and memory requirements scaling only linearly in the number n of datapoints .
the latter has been used successfully in the context of models with a single gp and its application to the baseline model of section 123 is straightforward , but the application to a model with coupled processes is novel and of substantial additional complexity .
123 the single process ivm
we provide a brief introduction to the principles behind the single process ivm scheme applied to binary classi ( cid : 123 ) cation .
details can be found in ( 123 , 123 ) .
the ivm scheme has so far been applied to models featuring a single gp .
as mentioned in section 123 , we replace the posterior p ( vjd ) by a gaussian approximation q ( v ) .
in the single process case , v = ( v123; : : : ; vn ) .
possible likelihoods include the gaussian p ( yjv ) = n ( yjv; ( cid : 123 ) 123 ) for regression and the probit for classi ( cid : 123 ) cation :
p ( yjv ) = ( cid : 123 ) ( y ( v + ( cid : 123 ) ) ) ; ( cid : 123 ) ( x ) =z x
n ( tj123; 123 ) dt :
in the ivm scheme , we have
q ( v ) / p ( v ) exp ( cid : 123 ) ( cid : 123 )
vt dv + bt v ( cid : 123 ) ; d = diag ( ( cid : 123 ) i )
where p ( v ) = n ( 123; ~ k ) is the prior .
the gaussian term parameterized by the site pa - rameters b; d is called likelihood approximation .
in general , we would allow ( cid : 123 ) i > 123 for all i , but this leads to costs of o ( n123 ) for training and o ( n123 ) for each prediction .
in con - trast , the ivm scheme is a sparse approximation in which we constrain ( cid : 123 ) i = bi = 123 for all i 123 i ( cid : 123 ) f123; : : : ; ng; jij = d ( cid : 123 ) n , which leads to o ( n d123 ) training time and o ( d123 ) cost for each prediction ( in the ivm algorithm ) .
the choice of the active set i is done sequentially using greedy forward selection with an information - theoretic criterion : among all remaining patterns outside the current i , we choose the one which maximizes the ( instantaneous ) information gain realized by including the pattern into i ( note that this score is relative to the current approximation q , which is why the scheme is sequential ) .
in order to ease notation we write i ( cid : 123 ) ;idi i; ( cid : 123 ) ; d 123 rd;d instead of d and i ( cid : 123 ) ;ib; b 123 rd instead of b .
we brie ( cid : 123 ) y remind the reader of the ivm representation which is derived in ( 123 , 123 ) .
our ability to score all remaining patterns for each inclusion comes at a cost of o ( n d ) memory and is the reason for the o ( n d123 ) time scaling .
if q ( u ) = n ( h; a ) , then we see from eq .
123 that
i+d123=123 ~ k i d123=123 = llt ;
+ i ( cid : 123 ) ;i di i; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123
= ~ k ( cid : 123 ) m m t ; m = ~ k ( cid : 123 ) ;id123=123l ( cid : 123 ) t ;
a = ( cid : 123 ) ~ k furthermore h = m ( cid : 123 ) ; ( cid : 123 ) = l ( cid : 123 ) 123d ( cid : 123 ) 123=123b .
here , l is lower triangular with positive diagonal ( i . e .
a cholesky factor ) , and the rows of m 123 rn;d are called m - stubs .
in order to score all remaining points , we require the posterior marginal means h and variances diag a .
a scheme to update l; m ; ( cid : 123 ) ; h and diag a after inclusion of a new point into i is given in ( 123 , 123 ) , the cost is o ( n d ) .
the computation of the site parameters ( cid : 123 ) i; bi for a pattern i to be included requires a so - called adf projection ( or moment matching ) ( 123 , 123 ) and typically a one - dimensional numerical quadrature if gaussian expectations over the likelihood p ( yju ) are not analytically tractable .
importantly , these computations can be done to high accuracy with no signi ( cid : 123 ) cant additional cost , because they are one - dimensional ( see section 123 ) .
we then make use of the conditional inference scheme as a subroutine in order to drive hyperparameter learning ( the parameters of k and the intercept ( cid : 123 ) ) .
this is done using a variational bound similar to what we employ in this paper here .
a look at eq .
123 reveals how to compute the latent predictive distribution q ( u ( cid : 123 ) ) = n ( u ( cid : 123 ) jh ( cid : 123 ) ; a ( cid : 123 ) ) , namely a ( cid : 123 ) = ~ k ( x ( cid : 123 ) ; x ( cid : 123 ) ) ( cid : 123 ) km ( cid : 123 ) k123 ; m ( cid : 123 ) = l ( cid : 123 ) 123d123=123 ( ~ k ( x ( cid : 123 ) ; xi ) ) i123i ;
h ( cid : 123 ) = mt
and the predictive distribution is then obtained as p ( y ( cid : 123 ) jx ( cid : 123 ) ; d ) = eq ( p ( y ( cid : 123 ) ju ( cid : 123 ) ) ) which is computed by one - dimensional quadrature in the general case , or can be computed analyti - cally for the probit likelihood .
123 gaussian process belief propagation
in this section we show how the ivm representation of section 123 can be combined with the standard belief propagation algorithm for inference in parametric graphical models in order to obtain an e ( cid : 123 ) cient inference machinery for the slfm .
if we simply apply the ivm technology to the naive method mentioned in section 123 , we have c n variables of which we select an active set of size c d ( say ) , so that the running time complexity is o ( c 123 n d123 ) , and the memory requirements are o ( c 123 n d ) .
this should be compared to o ( c n d123 ) time and o ( c n d ) memory required for the baseline model if we
select d active points for each c .
the ratio of c 123 is due to the fact that the naive method does not exploit the structure in the e ( cid : 123 ) ective v prior at all .
in parametric graphical models , conditional independence statements between variables of a domain are asserted .
under this model assumption , there are algorithms which can exploit certain markovian aspects of this structure in order to perform inference very e ( cid : 123 ) ciently .
a typical assumption for such models is that conditioned on all parameters ( of the conditional distributions in the graph ) , data cases are independent .
strictly speaking there are two di - mensions of conditional dependence in such models , the one between di ( cid : 123 ) erent variables of the domain ( the model dimension ) , and the one between datapoints ( the data dimension ) .
for parametric models , the latter is usually trivial . 123 in contrast to that , in nonparametric models the data dimension has a very rich structure in that typically there is no ( cid : 123 ) nite num - ber of parameters which render the data independent .
this means that usually there are no su ( cid : 123 ) cient statistics in which the data can be described in a subtantially compressed manner , implying the scaling with the number of training points .
however , common nonparametric process models have so far almost exclusively been proposed for very simple model dimen - sions , by essentially focussing on a single real variable .
in cases where this is not su ( cid : 123 ) cient , strong assumptions such as complete posterior independence of all ( cid : 123 ) elds are used ( leading to methods such as our baseline ) , or the variables are taken to be fully coupled ( leading to the naive method ) .
in this paper we are interested in a model which has nontrivial structure along both dimen - sions .
we propose to combine structured graphical models tools along the model dimension with sparse inference approximations along the data dimension in order to speed up the total inference and learning process .
let us specify what we require the representation to deliver .
denote v c = ( vi;c ) i 123 rn .
within the ivm framework , active sets are selected based on current marginal posterior distributions .
in order to generalize this to our model , the representation has to maintain posterior means and variances for all vi;c .
the key idea is to make use of the structure of the graphical model along the model dimensional , i . e .
for p ( u; v ) .
if we treat u as a single variable , we have a tree - structured network ( see figure 123 ) .
this allows us to employ the ( exact ) belief propagation ( bp ) algorithm ( cite ! ! ) in order to maintain marginals over the vc as more and more evidence is accommodated .
note that running bp e ( cid : 123 ) ciently on the network of figure 123 is not straightforward due to the dimensionality of the variables involved ( along the data dimension ) .
we need to combine the basic message passing with the ivm framework in order to obtain an acceptable scaling .
just as in the case of the single process ivm , we will perform inference sequentially , including a certain number of active patterns from the sample one at a time .
the ability to maintain marginals of the vi;c at any time will be crucial to select which point to include next .
the evidence potentials vc 123 ! p ( ycjvc ) are non - gaussian in general .
we will replace them by low rank gaussian factors in the same way as in the single process ivm ( our notation is described in appendix a ) :
( cid : 123 ) v ( vc ) = n u ( cid : 123 ) i ( cid : 123 ) ;icb ( c ) ; i ( cid : 123 ) ;icd ( c ) i ic; ( cid : 123 ) ( cid : 123 ) ;
where ic ( cid : 123 ) f123; : : : ; ng is the active set and b ( c ) ; d ( c ) the site parameters .
initially , ic = ;
123the graphical symbol for this conditional data independence is the plate .
figure 123 : slfm as a tree - structured graphical model
and ( cid : 123 ) v ( cid : 123 ) 123
the edge potentials are
( cid : 123 ) u ! v ( vc; u ) = p ( vcju ) = n ( cid : 123 ) ( ( cid : 123 ) t
c ( cid : 123 ) i ) u; ~ k
where ( cid : 123 ) c = ( cid : 123 ) t
( cid : 123 ) u ( u ) = p ( u ) = n ( 123; k )
where k = diag ( k ( p ) ) p .
now suppose new evidence is introduced in the sense that j is included into ic with site parameters bj;c; ( cid : 123 ) j;c .
this will change the message vc sends to u
mvc ! u ( u ) / z ( cid : 123 ) v ( vc ) ( cid : 123 ) u ! v ( vc; u ) dvc
which in turns modi ( cid : 123 ) es the messages u sends to vc123; c123 123= c :
mu ! vc123 ( vc123 ) / z yc123=c123
mvc123 ! u ( u ) ( cid : 123 ) u ( u ) ( cid : 123 ) u ! v ( vc123; u ) du :
the message mu ! vc remains the same .
finally , all marginals have to be updated :
q ( vc123 ) / ( cid : 123 ) v ( vc123 ) mu ! vc123 ( vc123 ) ;
q ( vc ) because ( cid : 123 ) v ( vc ) changed , and q ( vc123 ) because mu ! vc123 changed , c123 123= c .
the details for the propagation scheme will be worked out in the following section .
we will see that the key problem of applying bp to our nonparametric setup is that messages have to represented by a number of parameters which grows as new evidence is incorporated .
this situation is very di ( cid : 123 ) erent from bp on a parametric graphical model where messages have a ( cid : 123 ) xed size depending on the size of the su ( cid : 123 ) cient statistics of potentials and the graph
123 the representation
in this section we work out the details for the propagation scheme introduced in section 123 and give the representation for the posterior approximation q .
we ( cid : 123 ) rst note that a direct implementation of bp as described in the previous section does not signi ( cid : 123 ) cantly improve the scaling of the naive method mentioned there .
the reason is that if the active sets ic can be chosen independently , they may end up to be disjoint and have a combined size of about c d .
since each vi;c directly in ( cid : 123 ) uences all variables ui , we basically need about p c d of the components of u in order to represent each of the messages mu ! vc , and this leads to prohibitive costs .
on the other hand , restricting all active sets to be the same would mean a drawback compared to the independent baseline .
we opt for an approximate inference method which limits the number of u components that the message mvc ! u depends upon .
to this end let dc = jicj denote the ( cid : 123 ) nal active set sizes . 123 let d ( cid : 123 ) minfdcg and i ( cid : 123 ) ic for all c with d = jij .
in our inference approximation we will use the bp algorithm introducing evidence sequentially , but in a sense squeeze the messages from vc to u through the bottleneck of ui = ( ui;p ) i123i;p 123 rp d .
the common active set i is selected in the beginning , in what we call common inclusion phase .
once it has attained the ( cid : 123 ) nal size d , we set ic = i for all c and continue to add elements to the ic independently .
each inclusion is equivalent to introducing new evidence , but the corresponding message mvc ! u is restricted not to depend on uicni .
this idea will be made more precise below .
we note that our inference approximation may not lead to a consistent joint posterior approximation q ( v ) , but it does lead to joint marginal posteriors q ( v c ) over the di ( cid : 123 ) erent classes .
this is not much di ( cid : 123 ) erent from some general extensions of bp on loopy networks where a consistent joint posterior approximation cannot be extracted .
the complete representation for q is quite complicated and consists of a sequence of representations as used for the single process ivm .
we denote the representations by r123 ( c ) ;r123 ( c ) ;r123 ( c ) ;r123; c = 123; : : : ; c .
we aim to use consistent notation which is the same as used in section 123 or in ( 123 , 123 ) .
namely , l will be lower - triangular cholesky factors of matrices a , ( cid : 123 ) will be additional vectors of the form ( cid : 123 ) = l ( cid : 123 ) 123c , and m will be matrices of the form b l ( cid : 123 ) t .
for notational simplicity below we will order the components in the vector ui a di ( cid : 123 ) erent way than usual ( see appendix a ) .
recall that for v; u , etc .
we have that v = ( v123;123; : : : ; vn;123; v123;123; : : : ; vn;c ) t .
however for ui we use the ordering ui = ( ui123;123; : : : ui123;p ; ui123;123; : : : ; uid;p ) t where i = fi123; : : : ; idg .
let ( cid : 123 ) be the permutation ma - trix which converts from the latter to the standard ordering , so that ( cid : 123 ) ui is in standard ordering .
note that ( cid : 123 ) t ( ( cid : 123 ) c ( cid : 123 ) i ) = ( i ( cid : 123 ) ( cid : 123 ) c ) , so that ( ( cid : 123 ) c ( cid : 123 ) i ) in the standard ordering becomes ( i ( cid : 123 ) ( cid : 123 ) c ) in the ui ordering .
if k = diag ( k ( p ) ) p 123 rp n;p n is the kernel matrix in the standard ordering , we de ( cid : 123 ) ne ^k = ( cid : 123 ) t k ( cid : 123 ) which is the kernel matrix in the ordering used for ui .
note that although ^k is just as sparse as k it does not have block - diagonal structure .
in general , the superscript \^ " indicates that the ui ordering is used .
123for simplicity we denote both the current and ( cid : 123 ) nal active set size by dc .
in complexity statements , dc
is the ( cid : 123 ) nal size .
in order to represent mvc ! u we need a ivm representation of size dc :
r123 ( c ) : l ( 123;c ) l ( 123;c ) t = a ( 123;c ) = i + d ( c ) 123=123 ~ k ( cid : 123 ) ( 123;c ) = l ( 123;c ) ( cid : 123 ) 123d ( c ) ( cid : 123 ) 123=123b ( c ) ; e ( c ) = d ( c ) 123=123l ( 123;c ) ( cid : 123 ) t :
in the common inclusion phase ,
ic = i .
if p ( c ) = suppose that we are still ( ( cid : 123 ) c ( cid : 123 ) i ) d ( c ) 123=123l ( 123;c ) ( cid : 123 ) t , shown in appendix b . 123 that mvc ! u is given by n u ( uji ( cid : 123 ) ;ip ( c ) ( cid : 123 ) ( 123;c ) ; i ( cid : 123 ) ;ip ( c ) p ( c ) t i i; ( cid : 123 ) ) , so the message depends on ui only .
the \bottle - neck " approximation we are using ensures that mvc ! u will always depend on ui only , even if eventually ic n i 123= ; .
namely , if
( where we assume that i is a pre ( cid : 123 ) x of ic ) , the message is de ( cid : 123 ) ned to be
= ( i ( cid : 123 ) ( cid : 123 ) c ) e ( c )
123 : : : d; ( cid : 123 ) 123 rp d;dc
mvc ! u ( ui ) = n u ( cid : 123 ) ^p
the representation r123 ( c ) is needed to form the message mu ! vc , it basically represents the
rc ( u ) / p ( u ) yc123=c
because all mvc123 ! u are functions of ui we have rc ( u ) = rc ( ui ) p ( u n uijui ) .
r123 ( c ) there - fore needs to be of size p d only , and its size grows only during the common inclusion phase .
its exact form is determined by what is required to ( cid : 123 ) nally compute the single marginals of q ( vc ) .
to this end , we need ivm representations r123 ( c ) of size dc .
the di ( cid : 123 ) erence between r123 ( c ) and r123 ( c ) lies in the \prior distributions " used for the ivm representation .
for r123 ( c ) this is n ( vcj123; ~ k ) which does not depend on messages coming from u .
for r123 ( c ) this is replaced by the \e ( cid : 123 ) ective prior " n ( vcj ( cid : 123 ) ( c ) ; ( cid : 123 ) ( c ) ) whose parameters depend on the message mu ! vc , so that r123 ( c ) has to be modi ( cid : 123 ) ed whenever the message changes .
apart from that , the both representations share the same evidence potential ( cid : 123 ) v ( vc ) which is modi ( cid : 123 ) ed with each inclusion into ic .
a glance at eq .
123 reveals that we have
( cid : 123 ) ( c ) = ~ k
c ( cid : 123 ) i ) varrc ( u ) ( ( cid : 123 ) c ( cid : 123 ) i ) :
by a standard formula ,
varrc ( u ) = erc ( varp ( ujui ) ) + varrc ( ep ( ujui ) )
= k ( cid : 123 ) k ( cid : 123 ) ;i ( cid : 123 ) k ( cid : 123 ) 123
i ( cid : 123 ) k ( cid : 123 ) 123
i ( cid : 123 ) varrc ( ui ) ( cid : 123 ) t k ( cid : 123 ) 123
where we used that rc ( ujui ) = p ( ujui ) .
next , let dnc =pc123=c dc and ( c ) ( cid : 123 ) 123 rp d;dnc; = ( cid : 123 ) ( cid : 123 ) ( 123;123 ) t : : : ( cid : 123 ) ( 123;c ( cid : 123 ) 123 ) t ( cid : 123 ) ( 123;c+123 ) t : : : ( cid : 123 ) ( 123;c ) t ( cid : 123 ) t
: : : ^p
: : : ^p
i ( cid : 123 ) k i; ( cid : 123 )
the order of the columns of ^p recall that ^k i = ( cid : 123 ) t ki ( cid : 123 ) .
we have
is not important as long as ^ ( cid : 123 )
follows the same ordering .
varrc ( ui ) = ( cid : 123 ) ^k
i + ^p
; erc ( ui ) = varrc ( ui ) ^p
i ( cid : 123 ) k ( cid : 123 ) 123
i ( cid : 123 ) varrc ( ui ) ( cid : 123 ) t k ( cid : 123 ) 123
i = k ( cid : 123 ) 123
plugging this into eq .
123 we have
( cid : 123 ) ( c ) = ~ k
c ( cid : 123 ) i ) k ( ( cid : 123 ) c ( cid : 123 ) i ) ( cid : 123 ) ( ( cid : 123 ) t
i ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ^k i + ^k i c ( cid : 123 ) i ) m ( 123 ) m ( 123 ) t ( ( cid : 123 ) c ( cid : 123 ) i ) + m ( 123;c ) m ( 123;c ) t
( nc ) t ^k i ( cid : 123 ) ( cid : 123 ) 123
where m ( 123 ) does not depend on c and actually has a simple blockdiagonal structure .
the role of r123 ( c ) is the maintenance of m ( 123;c ) ( see eq .
we also have ( cid : 123 ) ( c ) = ( ( cid : 123 ) t
c ( cid : 123 ) i ) erc ( ep ( ujui ) ) = ( ( cid : 123 ) t
c ( cid : 123 ) i ) k ( cid : 123 ) ;ik ( cid : 123 ) 123
i + ^p
i ( cid : 123 ) ( cid : 123 ) ^k
r123 ( c ) : l ( 123;c ) l ( 123;c ) t = a ( 123;c ) = ^k i + ^k i
( cid : 123 ) ( 123;c ) = l ( 123;c ) ( cid : 123 ) 123 ^k i m ( 123;c ) = ( ( cid : 123 ) t
c ( cid : 123 ) i ) k ( cid : 123 ) ;i ( cid : 123 ) l ( 123;c ) ( cid : 123 ) t 123 rn;p d this representation is of size o ( n p d ) .
from eq .
123 it is easy to see that
( nc ) t ^k i;
( cid : 123 ) ( c ) = m ( 123;c ) ( cid : 123 ) ( 123;c ) :
r123 is required to maintain m ( 123 ) which is needed in eq
r123 : l ( 123 ) l ( 123 ) t = k i; m ( 123 ) = k ( cid : 123 ) ;il ( 123 ) ( cid : 123 ) t :
since all matrices here are blockdiagonal , the representation size is only o ( n p d ) .
k ( p ) may be ill - conditioned as d gets large , so we use the common remedy of replacing k ( p ) finally , r123 ( c ) is a normal ivm representation based on the \e ( cid : 123 ) ective prior " n ( ( cid : 123 ) ( c ) ; ( cid : 123 ) ( c ) ) :
i + " i for some small " > 123
r123 ( c ) : l ( 123;c ) l ( 123;c ) t = a ( 123;c ) = i + d ( c ) 123=123 ( cid : 123 ) ( c )
m ( 123;c ) = ( cid : 123 ) ( c )
d ( c ) 123=123l ( 123;c ) ( cid : 123 ) t 123 rn;dc;
ic ( cid : 123 ) : ( cid : 123 ) ( 123;c ) = l ( 123;c ) ( cid : 123 ) 123 ( cid : 123 ) d ( c ) ( cid : 123 ) 123=123b ( c ) ( cid : 123 ) d ( c ) 123=123 ( cid : 123 ) ( c )
the size is o ( n pc dc ) for all r123 ( c ) .
we also maintain ( cid : 123 ) ( c ) and diag ( cid : 123 ) ( c ) explicitly in r123 ( c ) .
it easy to see that the gaussian posterior q ( vc ) is given by
eq ( vc ) = ( cid : 123 ) ( c ) + m ( 123;c ) ( cid : 123 ) ( 123;c ) ; varq ( vc ) = ( cid : 123 ) ( c ) ( cid : 123 ) m ( 123;c ) m ( 123;c ) t :
both h ( c ) = eq ( vc ) and a ( c ) = diag varq ( vc ) are maintained explicitly with r123 ( c ) ( their maintenance is in fact the prime reason for all of the representations ) .
the size of the combined representation is o ( n ( pc dc + d c p ) ) .
this should be compared to o ( n pc dc ) for the baseline method and to o ( n c pc dc ) for the naive method .
the update of the representation after the inclusion of a point into i is most easily described in terms of some computation primitives which are required in the context of low - rank updates of matrices .
we assume in general that
a = llt 123 rp;p; m = bl ( cid : 123 ) t 123 rq;p;
where a is symmetric positive de ( cid : 123 ) nite .
the primitives update l ! l123; m ! m 123 after certain modi ( cid : 123 ) cations a ! a123; b ! b123
123 . 123 primitive cholext
cholext is used if a grows by a number of rows / columns .
namely ,
a123 = ( cid : 123 ) a a ( cid : 123 ) ; ( cid : 123 )
( cid : 123 ) ; ( cid : 123 ) a ( cid : 123 ) ( cid : 123 ) ; b123 = ( b b ( cid : 123 ) ) ; a ( cid : 123 ) ; ( cid : 123 ) 123 rp;r :
cholext ( l; m ; a ( cid : 123 ) ; ( cid : 123 ) ; a ( cid : 123 ) ; b ( cid : 123 ) ) computes
l123 = ( cid : 123 ) l
( cid : 123 ) ; ( cid : 123 ) l ( cid : 123 ) ( cid : 123 ) ; m 123 = ( m m ( cid : 123 ) ) ( cid : 123 ) = a ( cid : 123 ) ( cid : 123 ) lt
l ( cid : 123 ) ; ( cid : 123 ) = l ( cid : 123 ) 123a ( cid : 123 ) ; ( cid : 123 ) ; l ( cid : 123 ) lt
( cid : 123 ) ; ( cid : 123 ) l ( cid : 123 ) ; ( cid : 123 ) ; m ( cid : 123 ) = ( b ( cid : 123 ) ( cid : 123 ) m l ( cid : 123 ) ; ( cid : 123 ) ) l ( cid : 123 ) t
the complexity is o ( p123r + qpr ) if r < p .
123 . 123 primitive chollrup
here , a123 = a + sv v t ; s 123 f ( cid : 123 ) 123; +123g; v 123 rp;r .
let p = l ( cid : 123 ) 123v which has to be com - puted if not given .
we can call chollrup ( l; m ; v ; s; f alse ) or chollrup ( l; m ; p ; s; true ) .
an algorithm for chollrup can be found in ( 123 ) or ( 123 ) .
the version for s = ( cid : 123 ) 123 is numerically less stable for a given condition number of l , so if positive and negative low - rank updates have to be done , it is recommended to do all positive ones ( cid : 123 ) rst .
the complexity is o ( p123r ) for the computation of p and o ( p123r + pqr ) for the rest ( if r < p ) .
we refer to m ! m 123 as m being dragged along .
123 update of the representation
the representation has to be updated after new inclusions are made to the ic .
this changes the evidence potentials and therefore some of the messages .
the process is di ( cid : 123 ) erent during the common inclusion phase and afterwards .
the former is more complicated and will be discussed here , for the latter we comment on how it di ( cid : 123 ) ers from the former .
during the common inclusion phase , an elementary step consists of the inclusion of j into i with corresponding new site parameters ( bj;c ) c; ( ( cid : 123 ) j;c ) c .
we will comment in ref ! ! on how j and the new parameters are determined .
we make use of the following conventions .
if x is a quantity before the update , x123 denotes its value after the update .
if the update proceeds in more than one step , x123 becomes x before the second step .
for example , i 123 = i ( fjg .
123 . 123 update of r123 ( c ) and r123 first , r123 ( c ) are updated just like in the single process ivm :
cholext ( cid : 123 ) l ( 123;c ) ; ( cid : 123 ) ( 123;c ) t ; ( cid : 123 ) 123=123
j;c d ( c ) 123=123 ~ k
i;j; 123 + ( cid : 123 ) j;c ~ k
note that d = dc , and that d ( c ) 123=123l ( 123;c ) ( cid : 123 ) t is upper triangular .
let e ( c ) 123 rd+123 be the new column of e ( c ) , i . e .
where ( lt l ) is the new row of l ( 123;c ) .
the new ^p the bottom of ^p
( c ) 123 is obtained by appending 123 123 rp;d to , then the new column ( i ( cid : 123 ) ( cid : 123 ) c ) e ( c ) to the right . 123 the update cost is
o ( pc dc ) = o ( c d ) .
next we update r123
since l ( 123 ) = diag ( l ( 123 ) p ; k ( p )
p ; m ( 123 )
i;j ; k ( p )
; k ( p )
( cid : 123 ) ;j ( cid : 123 ) ; p = 123; : : : ; p
p ) p; m ( 123 ) = diag ( m ( 123 )
p ) p , the update consists of
( recall that we implicitly add a small " > 123 to the kernel diagonal ) .
the cost is o ( n p d ) .
123 . 123 update of r123 ( c ) ; r123 ( c ) ( ( cid : 123 ) rst part ) next we need to update r123 ( c ) ; r123 ( c ) to account for the change in mu ! vc .
we do this in two steps .
first , we make a low - rank adjustment to a ( 123;c ) ( a chollrup to r123 ( c ) ) , second we extend a ( 123;c ) by a row / column ( a cholext to r123 ( c ) ) .
both result in low - rank adjustments to r123 ( c ) which are done directly after the changes to r123 ( c ) .
the ( cid : 123 ) rst step can be described as follows
) 123 is obtained from ^p
the right .
also , let ( ^ ( cid : 123 )
e ( nc ) = ( cid : 123 ) ( i ( cid : 123 ) ( cid : 123 ) c123 ) e ( c123 ) ( cid : 123 ) c123=c 123 rp ( d+123 ) ;c ( cid : 123 ) 123 : ( cid : 123 ) ) t with ( cid : 123 ) ( cid : 123 ) 123 rc ( cid : 123 ) 123
) 123 = ( ^ ( cid : 123 )
by appending 123 123 rp;d ( c ( cid : 123 ) 123 ) to the bottom , then e ( nc ) to
= a ( 123;c ) + v v t ; v = ^k i;i 123e ( nc ) 123 rp d;c ( cid : 123 ) 123 :
) 123 is obtained by appending 123 123 rp to ^p
= ^k i
^k i;i 123 ( cid : 123 ) ^p
, then adding e ( nc ) ( cid : 123 ) ( cid : 123 ) .
therefore
+ ^k i;i 123e ( nc ) ( cid : 123 ) ( cid : 123 ) :
w = l ( 123;c ) ( cid : 123 ) 123v ; w = l ( 123;c ) ( cid : 123 ) 123k i;i 123e ( nc ) ( cid : 123 ) ( cid : 123 ) = w ( cid : 123 ) ( cid : 123 ) :
then , r123 ( c ) is updated as
chollrup ( cid : 123 ) l ( 123;c ) ; ( cid : 123 ) m ( 123;c ) t ; ( cid : 123 ) ( 123;c ) + w ( cid : 123 ) t
; w ; +123; true ( cid : 123 ) :
123we do not explicitly indicate the dimensionality of i within each ( ( cid : 123 ) c ( cid : 123 ) i ) or ( ( cid : 123 ) t
c ( cid : 123 ) i ) , it is always clear
from the context ( here it is dc + 123 ) .
the cost is o ( n p c 123 d ) for all r123 ( c ) .
note that some quantities required for the subsequent update of r123 ( c ) have to be computed before m ( 123;c ) is modi ( cid : 123 ) ed ( notably ~ m ) .
to update r123 ( c ) accordingly , we ( cid : 123 ) rst need to work out what changes the update of r123 ( c ) implies for ( cid : 123 ) ( c ) ; ( cid : 123 ) ( c ) . 123 this is done in appendix b . 123
if we let
the result is
llt = i + w t w ;
( ( cid : 123 ) ( c ) ) 123 = ( cid : 123 ) ( c ) ( cid : 123 ) ~ m ~ m ( ( cid : 123 ) ( c ) ) 123 = ( cid : 123 ) ( c ) + ( cid : 123 ) ( c )
( cid : 123 ) ; ( cid : 123 ) ( c )
~ m = m ( 123;c ) w l ( cid : 123 ) t ;
( cid : 123 ) = ~ m l ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) w t ( cid : 123 ) ( 123;c ) ( cid : 123 ) :
we update diag ( cid : 123 ) ( c ) by subtracting diag ( ~ m ~ m then r123 ( c ) is updated as
if q = l ( 123;c ) ( cid : 123 ) 123d ( c ) 123=123 ~ m ic; ( cid : 123 ) 123 rdc;c ( cid : 123 ) 123 ,
; q; ( cid : 123 ) 123; true ! :
; ( cid : 123 ) ( cid : 123 ) ( 123;c ) ( cid : 123 ) l ( 123;c ) ( cid : 123 ) 123d ( c ) 123=123 ( ( cid : 123 ) ( c )
chollrup l ( 123;c ) ; ( cid : 123 ) ( cid : 123 ) m ( 123;c ) ( cid : 123 ) ~ m qt ( cid : 123 ) t it is clear that we have to compute ~ m explicitly in order to obtain ~ m qt e ( cid : 123 ) ciently .
the computation of ~ m is o ( n c p d ) , so the cost for each r123 ( c ) is o ( n c ( dc + p d ) ) , the total cost adds up to o ( n c ( c p d +pc dc ) ) which is o ( n c 123 p d ) during the common inclusion 123 . 123 update of r123 ( c ) ; r123 ( c ) ( second part ) next we extend a ( 123;c ) by p new rows / columns .
let b ( nc ) = ^p by padding with zeros at the bottom and right , then adding
which is updated
( cid : 123 ) = e ( nc ) e ( nc ) t :
= ^k i 123;j + ^k i 123;i b ( nc ) ^k i;j + ^k i 123;i 123b ( nc )
^k i 123;j
( again , " > 123 needs to be added to the kernel diagonal ) .
we can use that k j;i ( cid : 123 ) ( i ( cid : 123 ) ( cid : 123 ) c ) =
the update of r123 ( c ) is
cholext ( cid : 123 ) l ( 123;c ) ; ( cid : 123 ) m ( 123;c ) t ; ( cid : 123 ) ( 123;c ) ( cid : 123 ) t
c ( cid : 123 ) i ) k ( cid : 123 ) ;j ( cid : 123 ) t
; ^k j;i 123 ^p
; ( a ( 123;c ) 123 ) 123 : : : d;d+123; ( a ( 123;c ) 123 ) d+123;
which costs o ( n p 123 d ) ( recall that the variables l ( 123;c ) ; m ( 123;c ) ; p ( nc ) , etc .
denote the values after the ( cid : 123 ) rst update step has been done , not the initial values at the beginning of the
123we do not yet accommodate the update of r123 , this is done in the second step .
let m ( cid : 123 ) 123 rn;p denote the new columns of m ( 123;c ) , ( cid : 123 ) ( cid : 123 ) 123 rp the new entries of ( cid : 123 ) ( 123;c ) , and m ( cid : 123 ) ( cid : 123 ) 123 rnp;p the new columns of m ( 123 ) ( the latter does not depend on c and is block -
( ( cid : 123 ) ( c ) ) 123 = ( cid : 123 ) ( c ) ( cid : 123 ) ( ( cid : 123 ) t
c ( cid : 123 ) i ) m ( cid : 123 ) ( cid : 123 ) m t
( cid : 123 ) ( cid : 123 ) ( ( cid : 123 ) c ( cid : 123 ) i ) + m ( cid : 123 ) m t
which indicates how diag ( cid : 123 ) ( c ) is updated .
furthermore ,
( ( cid : 123 ) ( c ) ) 123 = ( cid : 123 ) ( c ) + m ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) :
in order to update r123 ( c ) we need two rank - p chollrup calls , we do the positive one ( cid : 123 ) rst .
the derivation is given in appendix b . 123
let q = l ( 123;c ) ( cid : 123 ) 123d ( c ) 123=123 ( m ( cid : 123 ) ) ic; ( cid : 123 ) 123 rdc;p , then
; q; +123; true ! :
chollrup l ( 123;c ) ; ( cid : 123 ) ( cid : 123 ) m ( 123;c ) + m ( cid : 123 ) qt ( cid : 123 ) t
next , with q = l ( 123;c ) ( cid : 123 ) 123d ( c ) 123=123 ( ( cid : 123 ) t
; ( cid : 123 ) ( 123;c ) ( cid : 123 ) q ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) t c ( cid : 123 ) i ) ( m ( cid : 123 ) ( cid : 123 ) ) ic; ( cid : 123 ) 123 rdc;p we have
; q; ( cid : 123 ) 123; true ! :
c ( cid : 123 ) i ) m ( cid : 123 ) ( cid : 123 ) qt ( cid : 123 ) t
chollrup l ( 123;c ) ; ( cid : 123 ) ( cid : 123 ) m ( 123;c ) ( cid : 123 ) ( ( cid : 123 ) t both cost o ( n p pc dc ) in total ( for all c ) .
123 . 123 update of r123 ( c ) and marginals finally , we update r123 ( c ) to incorporate the new evidence ( everything so far has only been done to update its \e ( cid : 123 ) ective prior " ) .
this is a standard ivm update , we only have to evaluate a new column of ( cid : 123 ) ( c ) :
( cid : 123 ) ;j = ~ k
( cid : 123 ) ;j + ( ( cid : 123 ) t
c ( cid : 123 ) i ) k ( cid : 123 ) ;j ( cid : 123 ) c ( cid : 123 ) ( ( cid : 123 ) t
c ( cid : 123 ) i ) m ( 123 ) ( cid : 123 ) m ( 123 )
( cid : 123 ) c + m ( 123;c ) m ( 123;c )
where we used that ( ( cid : 123 ) c ( cid : 123 ) i ) i ( cid : 123 ) ;j = i ( cid : 123 ) ;j ( cid : 123 ) c .
this computation is o ( n p c d ) for all c
cholext ( cid : 123 ) l ( 123;c ) ; ( cid : 123 ) m ( 123;c ) t ; ( cid : 123 ) ( 123;c ) ( cid : 123 ) t
t ; ( cid : 123 ) ( cid : 123 ) 123=123
bj;c ( cid : 123 ) ( cid : 123 ) 123=123
i;j; 123 + ( cid : 123 ) j;c ( cid : 123 ) ( c )
costing o ( n pc dc ) in total .
finally , the posterior means and variances h ( c ) ; a ( c ) are recom - puted from scratch ( based on the new m ( 123;c ) matrices ) at total cost of o ( n pc dc ) .
the cost is dominated by the ( cid : 123 ) rst step updates of r123 ( c ) , namely of the m ( 123;c ) matrices , and by the ( cid : 123 ) rst step updates of r123 ( c ) ( the m ( 123;c ) matrices ) .
for d inclusions it amounts to o ( n d123 c 123 p ) .
the memory cost is dominated by o ( n d c p ) for the m ( 123;c ) matrices .
after the common inclusion phase , patterns are included w . r . t .
speci ( cid : 123 ) c classes , say j into i c .
that means that the update process described above uses rank 123 updates instead of rank
c ( cid : 123 ) 123
as for section 123 . 123 , de ( cid : 123 ) ne e ( c ) = ( cid : 123 ) l ( cid : 123 ) 123e ( c )
123 : : : d; ( cid : 123 ) l 123 rd
( c ) 123 = ( cid : 123 ) ^p
; ( i ( cid : 123 ) ( cid : 123 ) c ) e ( c ) ( cid : 123 ) ;
remain the same for c123 123= c .
r123 is not modi ( cid : 123 ) ed ( it is in fact not needed anymore ) .
furthermore , k i remains the same and ^p are modi ( cid : 123 ) ed by appending ( i ( cid : 123 ) ( cid : 123 ) c ) e ( c ) 123= c .
in section 123 . 123 , r123 ( c ) ; r123 ( c ) are not updated ( since they do not change ) .
e ( nc123 ) = e ( c ) , and v ; w have a single column .
the r123 ( c123 ) updates cost o ( n p c d ) , and the r123 ( c123 ) updates are o ( n ( p c d + pc dc ) ) .
this is cheaper by a factor c than in the common inclusion phase , because the message updates are rank 123 instead of rank c ( cid : 123 ) 123
the second update step of section 123 . 123 does not exist here , since i does not grow anymore .
the ( cid : 123 ) nal update of r123 ( c ) ( for the single c ) costs o ( n ( p d + dc ) ) .
finally , h ( c123 ) and a ( c123 ) are recomputed from scratch ( for all c123 ) at cost o ( npc dc ) .
o n p c d +xc
therefore , the overall running time complexity is
and the memory requirements are
o n p c d +xc
in that case , the memory requirements of our method are the same as for the baseline up to a constant factor .
however , it seems that modelling conditional dependencies between
in large sample situations it makes sense to require p c d to be of the same order as pc dc .
classes comes at a signi ( cid : 123 ) cant additional price of at least o ( n ( pc dc ) 123 ) as compared to o ( n pc d123
c ) for the independent baseline .
on the other hand , our method is faster than the
naive implementation by a factor of c .
note that if the active sets and site parameters are ( cid : 123 ) xed , then the complete representation can be computed in
o n xc
c + p d c p d +xc
which is signi ( cid : 123 ) cantly faster and actually fairly close to what the independent baseline re - quires .
therefore , in contrast to the situation for earlier ivm variants , conditional inference with active set selection comes at a signi ( cid : 123 ) cantly higher cost than without . 123 the problem is
that while r123 ( c ) is of limited size p d , for each of the pc dc inclusions c ( cid : 123 ) 123 of them have
to be updated by rank 123
in other words , the matrices ^p are of size p d , but are in fact updated dnc > p d times by rank 123
each such update has to be worked through the
123even for other ivm variants the active set selection takes signi ( cid : 123 ) cantly more time in practice , but does
not have a higher complexity .
whole representation in order to make sure that the marginals h ( c ) ; a ( c ) are up - to - date all the time .
delaying these updates does not help .
we would have to delay the updates for a class c more than p d times before it would be cheaper to simply recompute r123 ( c ) ; r123 ( c )
in order to predict on test data , the dominant bu ( cid : 123 ) ers scaling as o ( n ) are not required .
we need to compute the marginals of q on the test point which is done just as above for the training points : compute m ( 123;c ) , ( cid : 123 ) ( c ) ( cid : 123 ) ;ic , m ( 123 ) , and m ( 123;c ) w . r . t .
the test points .
the cost is the same as computing the representation for the training set from scratch ( with ( cid : 123 ) xed active sets and site parameters ) , but with n replaced by the number of test points m :
o m xc
c + p d c p d +xc
again , this is fairly close to the requirements of the baseline method .
if m is large , the computation can be done in multiple chunks . 123 predictive distributions can be computed from the marginals using gaussian quadrature in general .
123 selection of points .
computing site parameters
in this section we show how new site parameters are computed and how the candidate j for the next inclusion into i is found .
the principal tool for models with non - gaussian likelihood factors p ( ycjvc ) is the adf projection mentioned in section 123 .
as mentioned in section 123 , there are two di ( cid : 123 ) erent phases in which patterns are included into the active sets .
during the initial common inclusion phase , each pattern is included into all ic in parallel , so that ic = i for all c during this phase .
during the second phase , additional patterns are included w . r . t .
speci ( cid : 123 ) c classes c only .
both the forward selection of points to be included and the computation of site parameters is relatively straightforward in the second phase , while we need additional approximations during the common phase .
we concentrate on describing the initial common phase .
; bj;c = yj;c ( cid : 123 ) ( cid : 123 ) 123
suppose that during the common inclusion phase , we have selected j for inclusion into i and want to compute the new site parameters ( bj;c ) c; ( ( cid : 123 ) j;c ) c .
first note that for a gaussian likelihood , the site parameters are ( cid : 123 ) xed in advance , i . e .
if p ( yj;cjvj;c ) = n ( yj;cjvj;c; ( cid : 123 ) 123 then ( cid : 123 ) j;c = ( cid : 123 ) ( cid : 123 ) 123 .
therefore , we assume that p ( yj;cjvj;c ) is not gaussian .
the idea behind adf is as follows .
let q ( v ) be a gaussian and f ( vj ) a positive non - gaussian potential depending only on a small number vj of components of v .
if ^p ( v ) / q ( v ) f ( vj ) , the goal is to approximate ^p by a gaussian q123 ( v ) .
in adf , this is achieved by moment matching , i . e .
^p and q123 have the same mean and covariance matrix .
due to the special form of f , it is easy to see that q123 ( v ) = q123 ( vj ) q ( v n vjjvj ) , so we only need to match the moments of ^p ( vj ) / q ( vj ) f ( vj ) .
this is feasible in general if jvjj is very small , in our case it will be a single component .
note that we only need to know q ( v j ) in order to do the adf projection .
^p is called tilted distribution .
123on the other hand , with specialized code for large matrix computations , the chunks should be made as
large as memory resources permit .
in the second phase , we can apply adf directly to obtain the site parameters , but during the common phase we would have to base it on vj = ( vj;c ) c .
this would require to com - pute a c - dimensional gaussian expectation over a non - gaussian function which is hard in general .
even for a gaussian likelihood , we would have to have access to q ( v j ) which is a joint marginal spanning all of the vc .
the representation developed above cannot be used to obtain these joint marginals e ( cid : 123 ) ciently , and we do not know of a way to obtain a large number of these joint marginals which is substantially more e ( cid : 123 ) cient than the naive method mentioned at the beginning of section 123 .
we need a way of computing the site parameters which uses the single marginals q ( vj;c ) only .
the simplest choice is to compute the parameters for ( j; c ) under the assumption that j is included into ic only , requiring a one - dimensional adf projection and q ( vj;c ) only .
since for each inclusion we use the old q ( vj;c ) marginal which does not incorporate the new information from the other vj;c123 this is an approximation .
its e ( cid : 123 ) ect on the overall approximation could be tested by comparing it with other more complicated schemes in which the site parameters are computed in some ordering over c and the computations are interleaved with marginal updates .
we have not done this so far .
if ^p ( vj;c ) / p ( yj;cjvj;c ) q ( vj;c ) , q123 ( vj;c ) is the gaussian which minimizes d ( ^p k ( cid : 123 ) ) .
if q123 ( vj;c ) = n ( ^hj;c; ^aj;c ) , these parameters can be computed to high accuracy using one - dimensional gauss - hermite quadrature if the likelihood p ( yj;cjvj;c ) is reasonably smooth .
in the case of the probit likelihood ( eq .
123 ) or the gaussian likelihood we are mainly interested in here , the computation is analytic .
details are provided in appendix c .
the new site parameters then follow directly from
q123 ( vj;c ) / q ( vj;c ) exp ( ( cid : 123 ) ( cid : 123 ) j;cv123
j;c=123 + bj;cvj;c ) :
finally , we need a procedure for selecting a good inclusion candidate j among the indices not already in i .
just as in the single process ivm we make use of greedy forward selection where the selection criterion is an information measure : select the point whose subsequent inclusion changes the posterior most , i . e .
which introduces the most new information into q .
during the second phase we need to score pairs ( j; c ) .
a convenient criterion based on the marginal q ( vj;c ) is the information gain ( cid : 123 ) j;c = ( cid : 123 ) d ( q123 ( vj;c ) k q ( vj;c ) ) , where q123 is the marginal after the inclusion of ( j; c ) .
( cid : 123 ) j;c can be computed in o ( 123 ) given q ( vj;c ) .
during the common inclusion phase , a good generalization of the information gain would depend on joint marginals q ( vj ) which we cannot obtain feasibly .
however , a forward selection criterion which can be computed very e ( cid : 123 ) ciently is essential in order to be able to score most ( or all ) of the remaining patterns for each inclusion .
in our case , we are looking for a criterion which depends on the single marginals q ( vj;c ) only , since we can a ( cid : 123 ) ord to update all of them after each inclusion using our representation .
for the common inclusion phase , we propose to use the c information gain values ( cid : 123 ) j;c in order to construct an appropriate single score ( cid : 123 ) j .
possibilities are the average
or the maximum
j = c ( cid : 123 ) 123xc
j = max
note that q123 ( vj;c ) is not the marginal after the inclusion of j .
the latter would require to propagate the new evidence to u and back to the vc .
alternatively , one could replace the
q123 by these new marginals , however they may be costly to compute .
we will explore this possibility in future work .
123 joint common inclusion phase
as mentioned in section 123 , there are two di ( cid : 123 ) erent phases for inclusions into the active sets , and during the ( cid : 123 ) rst common inclusion phase our inability of computing joint marginals of vj = ( vj;c ) c leads to further somewhat unsatisfactory approximations .
in section 123 . 123 we determined the overall complexity and noted that a realistic scaling is obtained if p c d ( cid : 123 ) pc dc .
under this assumption conditional inference with active set selection requires about o ( n ( p c d ) 123 ) time .
in this section we suggest a way of working with joint marginals during the common inclusion phase while still matching this scaling .
note that the common inclusion phase requires o ( n p ( c d ) 123 ) and is clearly dominated by the second phase .
if we run the common phase in the naive way mentioned at the beginning of section 123 it scales as o ( n c ( c d ) 123 ) which is o ( n ( p c d ) 123 ) if p 123 ( cid : 123 ) c .
recall that the naive method simply uses the single process ivm representation of section 123 over n c variables v with the kernel induced by ~ k ( c ) ; k ( p ) .
the memory requirement for the naive method is o ( n d c 123 ) which is by a factor c=p too large .
however , we can limit the memory requirements by reducing the number of candidates which are scored for each inclusion .
suppose the ( common ) active set i has size d123 ( cid : 123 ) d .
the matrix m 123 rm;cd123 dominates the memory requirements , it has one row for every candidate which can be scored .
if m ( cid : 123 ) n p d=d123 , then a bu ( cid : 123 ) er of n d c p elements is su ( cid : 123 ) cient .
for the ( cid : 123 ) rst few inclusions we can set m = c n .
eventually , rows have to removed from m after each inclusion .
when selecting these rows priority is given to the ones attaining the worst
based on the stub matrix m we can compute the joint distribution q ( v j ) = n ( hj; aj ) ; vj 123 rc for every inclusion candidate j .
if the likelihood p ( yjv ) is gaus - sian , then the tilted distribution ^p ( vj ) is gaussian , i . e .
q123 = ^p , and the information gain ( cid : 123 ) j = ( cid : 123 ) d ( q123 ( vj ) k q ( vj ) ) can be computed analytically .
in general , computing ( cid : 123 ) j requires c - dimensional quadrature which quickly becomes very costly or inaccurate .
we suggest the following greedy proxy .
start with q123 ( vj ) = q ( vj ) ; d123 = ; , and ( cid : 123 ) 123 = 123
for c = 123; : : : ; c
for all c123 123 f123; : : : ; cgn dc ( cid : 123 ) 123
here , ^pc;c123 ( vj ) / p ( yc123jvc123 ) qc ( cid : 123 ) 123 ( vj ) and q123 adf projection of ^pc;c123
note that ( cid : 123 ) c;c123 projections are one - dimensional only .
if c123 is the argument minimizing these ( cid : 123 ) c;c123 ( cid : 123 ) c ( cid : 123 ) 123 + ( cid : 123 ) c;c123
c;c123 is the gaussian c;c123 ( vc123 ) k qc ( cid : 123 ) 123 ( vc123 ) ) , and that the adf , set ( cid : 123 ) c =
and dc = dc ( cid : 123 ) 123 ( fc123g .
the ( cid : 123 ) nal score value for j is ( cid : 123 ) c .
123 limiting resource requirements
it is important to note that the scaling of o ( n ) for the inference approximation is only due to the fact that we wish to consider all remaining points as inclusion candidates for the active sets ic for all classes .
in a situation of limited resources , we may reduce the size of the sample or restrict the active set sizes dc , but a third option is to score only subsets of
points as candidates for later inclusions .
it is quite likely that later inclusion decisions are less important than earlier ones , so that this way of limiting resource requirements may be much less intrusive than the other ones .
in this section we outline a simple scheme which can be used to this end .
let jc ( cid : 123 ) f123; : : : ; ng be the selection index for class c , and nc = jjcj .
for the ( cid : 123 ) rst inclusions , ideally for all of the common inclusion phase , nc = n for all c , but during later stages the nc decrease following some schedule which may be aimed at keeping the memory cost below a ( cid : 123 ) xed threshold at all times .
if we only insist on considering the points in jc as inclusion jc; ( cid : 123 ) ; m ( 123;c ) candidates into ic , we only need to maintain h ( c ) have to be stored and updated .
a point which is excluded from jc should not be re - included later on , because it is easy to show that doing so costs no less than just leaving it in jc in the ( cid : 123 ) rst place .
interestingly we can use the scores of all points in jc in order to decide which subset to exclude , for example by retaining the top - scorers .
some randomization is recommended as well .
in fact the problem of maintaining jc is similar to organizing a cache , with the important di ( cid : 123 ) erence that the scores we care about can be evaluated e ( cid : 123 ) ciently for all elements all the time , so we may draw on existing strategies developed for the latter .
jc which means that only m ( 123;c )
jc ; a ( c )
123 parameter learning
in this section we show how free parameters can be learned automatically from the data d in an empirical bayesian manner .
it turns out that the conditional inference approximation developed above is required to drive this optimization .
we can separate the unobserved variables in our model in two categories : primary and sec - ondary parameters .
the former are the latent processes v ( ( cid : 123 ) ) , or equivalently their values v at the training points , the latter consist of ( cid : 123 ) ; ( cid : 123 ) and the parameters of the covari - ance functions ~ k ( c ) ; k ( p ) .
secondary parameters are sometimes called hyperparameters in a bayesian setting , although in a semiparametric setup such as ours one is typically careful to distinguish parameters such as ( cid : 123 ) from \nuisance " parameters ( the kernel parameters in our context ) .
for the former we are explicitly interested in a point estimate , while the latter would be best integrated out and are estimated only if such a marginalization proves intractable ( such as in our case ) .
denote the vector of all secondary parameters by ( cid : 123 ) .
empirical bayesian techniques combine marginalization and maximization in the sense that primary parameters are integrated out , but hyperparameters are maximized over .
in our case , the marginal likelihood is
p ( yj ( cid : 123 ) ) =z p ( yjv ) p ( vj ( cid : 123 ) ) dv :
in the maximum likelihood ii ( ml - ii ) method we choose the hyperparameters ^ ( cid : 123 ) as ( local ) maximizer of p ( yj ( cid : 123 ) ) or p ( y; ( cid : 123 ) ) .
the latter requires the speci ( cid : 123 ) cation of a hyperprior p ( ( cid : 123 ) ) and implies that ml - ii can also be seen as maximum a posteriori ( map ) technique in that the posterior p ( ( cid : 123 ) jy ) is maximized .
the computation of log p ( yj ( cid : 123 ) ) is as hard as doing conditional inference ( i . e .
computing marginals of the conditional posterior p ( vjy; ( cid : 123 ) ) ) .
in terms of statistical physics , log p ( yj ( cid : 123 ) ) is the log partition function of the model .
interestingly a general variational framework
allows us to use a method for approximate inference in order to lower bound the log partition function .
we can then maximize the lower bound in order to determine ^ ( cid : 123 ) .
note that log p ( yj ( cid : 123 ) ) is a convex function of v 123 ! log p ( y; vj ( cid : 123 ) ) .
by legendre - fenchel duality ( 123 , 123 )
log p ( yj ( cid : 123 ) ) ( cid : 123 ) eq ( log p ( y; vj ( cid : 123 ) ) ) + h ( q ( v ) ) = eq ( log p ( yjv; ( cid : 123 ) ) ) ( cid : 123 ) d ( q ( v ) k p ( vj ( cid : 123 ) ) )
for any distribution q ( v ) .
the maximizer q ( v ) for the lower bound is the posterior p ( vjy; ( cid : 123 ) ) ( in general duality terms , v 123 ! log p ( y; vj ( cid : 123 ) ) and p ( vjy; ( cid : 123 ) ) are dually cou - pled ) , but any other posterior approximation gives a valid lower bound .
in this paper , we use the particular q ( v ) described in section 123 as variational distribution .
by a slight abuse of notation let vi denote the vector of all components vi;c; c = 123; : : : ; c; i 123 ic .
in order to compute the relative entropy term , we note that q ( v n v ijvi ) = p ( v n vijvi ) ,
d ( q ( v ) k p ( v ) ) = d ( q ( vi ) k p ( vi ) ) :
recall that the e ( cid : 123 ) cient representation we use for q ( v ) allows access to the marginals q ( vc ) ; c = 123; : : : ; c only , so especially q ( vi ) is not available .
if vi;c = ( vi;c ) i123ic , we make use of the additional bounding step
d ( q ( vi ) k p ( vi ) ) ( cid : 123 ) xc
d ( q ( vi;c ) k p ( vi;c ) ) :
thus , the learning criterion to be minimized is
eq ( ( cid : 123 ) log p ( yi;cjvi;c ) ) + d ( q ( vi;c ) k p ( vi;c ) ) ! :
it turns out that given the representation of the q ( v c ) , the criterion and its gradient w . r . t .
( cid : 123 ) can be computed e ( cid : 123 ) ciently if the dependence of fic; b ( c ) ; d ( c ) g on ( cid : 123 ) is neglected .
collect the latter variables in the inner state s .
the computation of r ( cid : 123 ) g is involved , it is sketched in appendix d .
importantly , the time complexity is the same as for conditional inference , and the computation is actually signi ( cid : 123 ) cantly cheaper because software for large matrix computations can be employed .
furthermore , if the bu ( cid : 123 ) ers required for conditional inference are overwritten , the additional memory requirements are subdominant .
we use a simple double - loop optimization strategy .
in the inner loop , we ( cid : 123 ) x the inner state s and perform gradient - based minimization of g using a quasi - newton method .
in the outer loop , s is re - selected as described in section 123
there is no obvious stopping criterion for the outer loop , so we run for a ( cid : 123 ) xed number of outer loop iterations , or until no more improvement is recorded in the inner loop .
we close this section with a number of comments .
first , note that our practice di ( cid : 123 ) ers from most other variational methods in which all of q is kept ( cid : 123 ) xed for an update of ( cid : 123 ) .
q depends on both the inner state s and ( cid : 123 ) , and the latter dependence is strong .
if we ( cid : 123 ) xed all of q during the inner loop , the criterion could not be improved much .
in a variant of our double - loop method we could split s into the active set indices and the site parameters and recompute the latter more often .
however , it is important to note that in the case of gaussian likelihoods p ( yjv ) for the regression model , the site parameters are ( cid : 123 ) xed anyway , so that s contains the active set indices only .
in this important special case , both proposals are the same .
note that , as opposed to many standard variational methods , the optimization strategy is not a pure descent method .
re - selection of s can actually lead to an increase in g .
the problem is that \closeness " to the true posterior p ( vjy; ( cid : 123 ) ) is measured in di ( cid : 123 ) erent ways in the variational bound and in our sparse approximation .
in other words , the active sets i c are not selected with the minimization of g in mind .
futhermore , the optimization is not convex in any sense .
the overall process involves the selection of subsets which really is a discrete optimization .
since our goal is to improve upon a \practically intractable " yet just cubic scaling , invoking any of the heavy machinery to deal with combinatorial problems would certainly not be sensible .
but even for ( cid : 123 ) xed s , the inner loop problem is not convex in general .
convexity is only obtained in rather unnatural parameterizations of the kernel matrices whose relevance in practice is unclear .
it would be very interesting to ( cid : 123 ) nd convex relaxations of our problem which retain the statistical features to some quanti ( cid : 123 ) able extent .
in this section we describe the notation used in this paper .
a . 123 matrices and vectors
vectors a 123 rn and matrices a 123 rn;m are set in boldface .
we write a = ( ai ) i = ( a123; : : : ; an ) t and a = ( ai;j ) i;j for the components .
ordered set subscripts are used to extract corresponding parts of vectors or matrices : for i ( cid : 123 ) f123; : : : ; ng; j ( cid : 123 ) f123; : : : ; mg we have ai;j = ( ai;j ) i123i;j123j and ai = ( ai ) i123i .
we write short " ( cid : 123 ) " for the whole range and i ( instead of fig ) for single element sets , i . e .
ai = ai; a ( cid : 123 ) ;j = ( ai;j ) i; a ( cid : 123 ) ; ( cid : 123 ) = a , etc .
we also abbreviate bi;i to bi .
the matrices i ( cid : 123 ) ;i and i i; ( cid : 123 ) ( with ( cid : 123 ) = f123; : : : ; ng ) should be regarded as distribution and selection operators respectively .
i ( cid : 123 ) ;ib for b 123 rd is the vector in rn with bk at position ik and 123 elsewhere , if i = fi123; : : : ; idg .
furthermore , i i; ( cid : 123 ) a = ( ai123; : : : ; aid ) t .
note that for matrices we have i k; ( cid : 123 ) ai ( cid : 123 ) ;i = ak;i .
in this paper we need to use matrices and vectors with double indexes ( i; c ) or ( i; p ) .
in order to be able to present our derivations in a reasonable ( cid : 123 ) uid manner , we choose a \lightweight " notation which may seem unfamiliar and ambiguous to the reader at ( cid : 123 ) rst .
in general we treat double indexes as ( cid : 123 ) at with i ( the index over datapoints ) meant to be the inner one , changing faster than than the index c over classes or p over latent processes .
for example , u = ( ui;p ) i;p = ( u123;123; : : : ; un;123; : : : ; un;p ) t .
we also write ui = ( ui;p ) p and up = ( ui;p ) i , in this context we will exclusively use i; j as indexes over datapoints , c as index over classes , and p as index over latent processes .
subset indexing is \overloaded " to the double index case as follows .
if i; j ( cid : 123 ) f123; : : : ; ng and a 123 rnp;np , then ai;j is short for ai ( cid : 123 ) f123; : : : ;p g;j ( cid : 123 ) f123; : : : ;p g .
thus , selection is done only on the index over datapoints .
however , if an index p or c is used , selection is meant to be w . r . t .
latent process or class .
for example , if a 123 rnc;np , then ac;p = ( a ( i;c ) ; ( j;p ) ) i;j 123 rn;n .
the same holds for obvious variants such as p123; c123; p123; c123 , etc .
a . 123 other notation
we need to manipulate gaussians in complicated ways and make use of the convenient notation introduced in ( 123 )
n u ( xjr; v ) = exp ( cid : 123 ) ( cid : 123 )
xt v x + rt x ( cid : 123 )
denote an unnnormalized gaussian .
here , v must be symmetric but need not be positive de ( cid : 123 ) nite .
if so , we have
n u ( xjr; v ) = n ( cid : 123 ) xjv ( cid : 123 ) 123r; v ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) v ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 )
rt v ( cid : 123 ) 123r ( cid : 123 ) :
b derivations for representation and updates
in this section we collect derivations for the representation and the scheme to update it after inclusions .
the subsections are referenced from the main text and are not self - contained .
b . 123 the message mvc ! u
the message mvc ! u is de ( cid : 123 ) ned in eq .
if ( cid : 123 ) = ( ( cid : 123 ) t
c ( cid : 123 ) i ) u , then rt ( cid : 123 ) r ( cid : 123 )
z ( cid : 123 ) v ( vc ) ( cid : 123 ) u ! v ( vc; u ) dvc / exp ( cid : 123 ) 123
where r = i ( cid : 123 ) ;ib ( c ) + ~ k ( cid : 123 ) m ( 123;c ) m ( 123;c ) t ( a collection of useful formulae for manipulating gaussians can be found in ( 123 ) , sect .
a . 123 ) .
plugging these in , some algebra gives
+ i ( cid : 123 ) ;id ( c ) i i; ( cid : 123 ) ) ( cid : 123 ) 123 = ~ k
( cid : 123 ) and ( cid : 123 ) = ( ~ k
mvc ! u / exp ( cid : 123 ) ( cid : 123 ) ( 123;c ) t ( cid : 123 ) ( cid : 123 )
( cid : 123 ) t ( cid : 123 ) ( cid : 123 ) ; ( cid : 123 ) = l ( 123;c ) ( cid : 123 ) 123d ( c ) 123=123 ( cid : 123 ) i = l ( 123;c ) ( cid : 123 ) 123d ( c ) 123=123 ( ( cid : 123 ) t
c ( cid : 123 ) i ) i i; ( cid : 123 ) u :
with the de ( cid : 123 ) nition of p ( c ) ( eq .
123 ) we have ( cid : 123 ) = p ( c ) t ui ( recall that we use a di ( cid : 123 ) erent component ordering for ui ) .
b . 123 first update of ( cid : 123 ) ( c )
( cid : 123 ) a ( 123;c ) + v v t ( cid : 123 ) ( cid : 123 ) 123
= a ( 123;c ) ( cid : 123 ) 123 ( cid : 123 ) m m t ; m = l ( 123;c ) ( cid : 123 ) t w l ( cid : 123 ) t ; llt = i + w t w :
( cid : 123 ) m ( 123;c ) m ( 123;c ) t ( cid : 123 ) 123
= m ( 123;c ) m ( 123;c ) t ( cid : 123 ) ~ m ~ m
~ m = ( ( cid : 123 ) t
c ( cid : 123 ) i ) k ( cid : 123 ) ;i ( cid : 123 ) m = m ( 123;c ) w l ( cid : 123 ) t 123 rn;c ( cid : 123 ) 123
which shows how to update ( cid : 123 ) ( c ) .
~ m is required explicitly in the subsequent r123 ( c ) update , the cost is o ( n c p d ) .
furthermore ,
( ( cid : 123 ) ( c ) ) 123 = ( ( cid : 123 ) t
= ( cid : 123 ) ( c ) + ( cid : 123 ) ( c )
( cid : 123 ) ; ( cid : 123 ) ( c )
c ( cid : 123 ) i ) k ( cid : 123 ) ;i ( cid : 123 ) ( cid : 123 ) a ( 123;c ) ( cid : 123 ) 123 ( cid : 123 ) m m t ( cid : 123 ) ^k i;i 123 ( cid : 123 ) ^p ( cid : 123 ) = m ( 123;c ) w ( cid : 123 ) ~ m m t ^k i;i 123 ( cid : 123 ) ^p = l ( cid : 123 ) 123w t ( cid : 123 ) ( cid : 123 ) ( 123;c ) + w ( cid : 123 ) :
m t ^k i;i 123 ( cid : 123 ) ^p
using the facts w = w ( cid : 123 ) ( nc )
and i + w t w = llt , some algebra gives
( cid : 123 ) = ~ m l ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) w t ( cid : 123 ) ( 123;c ) ( cid : 123 ) :
b . 123 second part of r123 ( c ) update recall the ( cid : 123 ) ( c ) update from eq .
we need to adjust the r123 ( c ) representation to incorpo - rate this change and the one of ( cid : 123 ) ( c ) .
this is done in two steps , ( cid : 123 ) rst a positive chollrup for m ( cid : 123 ) and the ( cid : 123 ) ( c ) change , then a negative chollrup for ( ( cid : 123 ) t c ( cid : 123 ) i ) m ( cid : 123 ) ( cid : 123 ) .
recall our convention that after each of these steps , the new variables x123 overwrite the old ones x .
for the positive update we need q = l ( 123;c ) ( cid : 123 ) 123d ( c ) 123=123 ( m ( cid : 123 ) ) ic; ( cid : 123 ) .
as for m ( 123;c ) , we ( cid : 123 ) rst replace
( cid : 123 ) ;ic which amounts to
m ( 123;c ) ! m ( 123;c ) + m ( cid : 123 ) qt ;
then drag it along the chollrup in order to replace l ( 123;c ) ( cid : 123 ) t by ( l ( 123;c ) 123 ) ( cid : 123 ) t .
as for ( cid : 123 ) ( 123;c ) , we
ic which amounts to
( cid : 123 ) ( 123;c ) ! ( cid : 123 ) ( 123;c ) ( cid : 123 ) l ( 123;c ) ( cid : 123 ) 123d ( c ) 123=123 ( m ( cid : 123 ) ) ic; ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ;
then drag it along .
for the negative chollrup we need q = l ( 123;c ) ( cid : 123 ) 123d ( c ) 123=123 ( ( cid : 123 ) t
c ( cid : 123 ) i ) ( m ( cid : 123 ) ( cid : 123 ) ) ic; ( cid : 123 ) and replace
m ( 123;c ) ! m ( 123;c ) ( cid : 123 ) ( ( cid : 123 ) t
c ( cid : 123 ) i ) m ( cid : 123 ) ( cid : 123 ) qt
before dragging it along .
( cid : 123 ) ( 123;c ) is simply dragged along .
both updates are of rank p , so the individual cost is o ( n p dc ) ( which is also the cost of computing m ( cid : 123 ) qt and ( ( cid : 123 ) t i ) m ( cid : 123 ) ( cid : 123 ) qt ) .
c details for adf projection
in this section we provide details for the adf projection discussed in section 123 .
recall that we need to compute mean and variance of the tilted distribution / p ( yjv ) n ( vjh; a ) .
z = e ( p ( yjv ) ) ; ( cid : 123 ) =
( cid : 123 ) = ( cid : 123 )
dh123 log z;
where e ( ( cid : 123 ) ) is over n ( vjh; a ) .
it is easy to see that mean ^h and variance ^a of the tilted distribution are given by
^h = h + ( cid : 123 ) a;
^a = a ( 123 ( cid : 123 ) a ( cid : 123 ) ) :
furthermore , the information gain criterion described in section 123 can be computed as
j;c = ( cid : 123 )
123 ( cid : 123 ) ( cid : 123 ) log ( 123 ( cid : 123 ) a ( cid : 123 ) ) + 123 ( cid : 123 ) a ( cid : 123 ) ( cid : 123 ) 123 + ( cid : 123 ) 123a ( cid : 123 ) :
if the adf projection is used for an inclusion , the new site parameters can be determined
they are given as
n ( vj^h; ^a ) / n ( vjh; a ) exp ( cid : 123 ) ( cid : 123 ) dv123=123 + bv ( cid : 123 ) ;
123 ( cid : 123 ) a ( cid : 123 )
h ( cid : 123 ) + ( cid : 123 ) 123 ( cid : 123 ) a ( cid : 123 )
it is interesting to note that if p ( yjv ) is ( strictly ) log - concave , then one can show that log z is ( strictly ) concave in h , so that ( cid : 123 ) ( cid : 123 ) 123 ( ( cid : 123 ) > 123 ) .
for such a likelihood , d will always be set to a nonnegative number .
in our implementation we reject inclusions if the corresponding adf update leads to a very small d . 123 for a general smooth likelihood p ( yjv ) , ( cid : 123 ) and ( cid : 123 ) can be computed by gauss - hermite quadra - ture .
for the probit likelihood ( eq .
123 ) , the computations are analytic .
we have
z = ev ( cid : 123 ) n ( h;a ) ;n ( cid : 123 ) n ( 123;123 ) ( cid : 123 ) ifn ( cid : 123 ) y ( v+ ( cid : 123 ) ) g ( cid : 123 ) = ( cid : 123 ) ( z ) ;
z = y ( h + ( cid : 123 ) ) s; s =
p123 + a
( cid : 123 ) = ( cid : 123 ) ys
ys = exp ( log n ( z ) ( cid : 123 ) log ( cid : 123 ) ( z ) ) ys ( ( cid : 123 ) zys ( cid : 123 ) ( cid : 123 ) ) = ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) + 123 + a ( cid : 123 ) :
h + ( cid : 123 )
for the gaussian likelihood p ( yju ) = n ( yju; ( cid : 123 ) 123 ) , we have z = n ( yjh; a + ( cid : 123 ) 123 ) , so that
y ( cid : 123 ) h a + ( cid : 123 ) 123 ;
a + ( cid : 123 ) 123 :
d learning criterion and gradient
in this section we provide details for the computation of the learning criterion g developed in section 123 and its gradient .
the complete derivation is lengthy and some details are let g = g123 + g123 with
eq ( ( cid : 123 ) log p ( yi;cjvi;c ) ) ; g123 =xc
g123 ( c ) = d ( q ( vi;c ) k p ( vi;c ) ) :
123our selection criterion actually favours updates which lead to larger d .
recall that vi;c = ( vi;c ) i123ic .
we have g123 = ( cid : 123 ) 123t z with
zi;c = eq ( log p ( yi;cjvi;c ) ) ; q ( vi;c ) = n ( hi;c; ai;c ) ; h ( c ) = ( hi;c ) i; a ( c ) = ( ai;c ) i :
recall that h ( c ) ; a ( c ) denote marginal means and variances of q ( vc )
i;c eq ( ( log p ( yi;cjvi;c ) ) ~ vi;c ) ;
ci;c = ( cid : 123 ) a ( cid : 123 ) 123=123 ~ vi;c = a ( cid : 123 ) 123=123
( vi;c ( cid : 123 ) hi;c ) ;
123ai;c ( cid : 123 ) zi;c ( cid : 123 ) eq ( cid : 123 ) ( log p ( yi;cjvi;c ) ) ~ v123
dzi;c = ( cid : 123 ) gi;cdai;c ( cid : 123 ) ci;cdhi;c; i : e : dg123 = gt ( da ) + ct ( dh ) =xc ( cid : 123 ) gt
c ( cid : 123 ) da ( c ) ( cid : 123 ) + ct
c ( cid : 123 ) dh ( c ) ( cid : 123 ) ( cid : 123 ) :
due to the multi - part representation and the nontrivial message ( cid : 123 ) ow , the gradient com - putation is very challenging .
first , we need to ( cid : 123 ) nd the general form of dg in terms of accumulator matrices .
if we ignore the parameters ( ( cid : 123 ) c ) c of the likelihood for the moment , we can infer from the representation that
t ( cid : 123 ) d diag ~ k t ( cid : 123 ) d ~ k t ( cid : 123 ) d ~ k
ic ( cid : 123 ) :
tr z ( 123 )
tr z ( 123 )
t ( cid : 123 ) d diag k ( p ) ( cid : 123 ) + tr z ( 123 ) t ( d ( cid : 123 ) ) t ( cid : 123 ) dk ( p ) t ( cid : 123 ) dk ( p )
separately , because in every iteration c , all of the z ( 123 )
the computation starts with a loop over c = 123; : : : ; c in which the accumulator matrices are computed .
here , the dominant matrices z ( 123 ) overwrite the bu ( cid : 123 ) ers used for m ( 123;c ) .
we maintain z ( 123 ) c123 are modi ( cid : 123 ) ed , and we cannot do these modi ( cid : 123 ) cations on the z ( 123 ) c123 because they only come available once m ( 123;c123 ) is not required anymore .
the gradient is computed in subsequent loops over c and p .
in the following we sketch how the accumulators are updated during a ( cid : 123 ) xed iteration c of the ( cid : 123 ) rst loop .
operations marked with ( ( cid : 123 ) ) are done only once , say for c = 123
if p appears , the operation is done for every p = 123; : : : ; p ( if nothing else is said ) .
from ivm learning in the single process case , we know how to formulate dh ( c ) ; da ( c ) in terms of d ( cid : 123 ) ( c ) ( cid : 123 ) ;ic; d diag ( cid : 123 ) ( c ) and d ( cid : 123 ) ( c ) .
thus dg123 can be written as linear expression in these terms , and the same is true for a part of dg123 as we show now .
our ( cid : 123 ) rst goal is to write dg123 + dg123;part =xc ( cid : 123 ) gt
( cid : 123 ) ;ic ( cid : 123 ) + f ( c ) t ( cid : 123 ) d ( cid : 123 ) ( c ) ( cid : 123 ) ( cid : 123 ) :
c ( cid : 123 ) d diag ( cid : 123 ) ( c ) ( cid : 123 ) + tr ~ z
with ~ z c; f ( c ) to be determined .
we then compute the remaining part of dg123 , and ( cid : 123 ) nally deal with the principal part ( eq
( cid : 123 ) ( c ) = d ( c ) 123=123l ( 123;c ) ( cid : 123 ) t ( cid : 123 ) ( 123;c ) ; f ( c ) = cc ( cid : 123 ) i ( cid : 123 ) ;ic
= m ( 123;c ) l ( 123;c ) ( cid : 123 ) 123d ( c ) 123=123;
where we overwrite m ( 123;c ) by ~ m
we have
dm ( 123;c ) m ( 123;c ) t = h ( cid : 123 ) d ( cid : 123 ) ( c ) dm ( 123;c ) ( cid : 123 ) ( 123;c ) = h ( cid : 123 ) d ( cid : 123 ) ( c )
( cid : 123 ) ;ic ( cid : 123 ) ( cid : 123 ) ( c ) + ( h ( cid : 123 ) i ) ( cid : 123 ) d ( cid : 123 ) ( c ) ( cid : 123 ) ; h = i ( cid : 123 ) ~ m
dh ( c ) = h ( cid : 123 ) d ( cid : 123 ) ( c ) ( cid : 123 ) + h ( cid : 123 ) d ( cid : 123 ) ( c ) da ( c ) = ( cid : 123 ) d diag ( cid : 123 ) ( c ) ( cid : 123 ) ( cid : 123 ) diag ( i + h ) ( cid : 123 ) d ( cid : 123 ) ( c )
noting that f ( c ) = h t cc and gt
c diag b = tr ( diag gc ) b , we have
~ z c = ( cid : 123 ) 123 ( diag gc ) ~ m
+ i ( cid : 123 ) ;ic
( diag gc ) ~ m
+ f ( c ) ( cid : 123 ) ( c ) t
which overwrites ~ m
the cost is o ( n d123
c ) for each c .
as for g123 ( c ) , let
t ( c ) = ~ k
ic + ( ( cid : 123 ) t
c ( cid : 123 ) i ) k ic ( ( cid : 123 ) c ( cid : 123 ) i ) 123 rdc;dc
ic ( cid : 123 ) m ( 123;c ) we have that p ( vi;c ) = n ( 123; t ( c ) ) and q ( vi;c ) = n ( h ( c )
a ( c ) = eq ( vi;c ) = ( cid : 123 ) ( c )
ic; ( cid : 123 ) m ( 123;c ) ic ; a ( c ) ) .
if r ( c ) = t ( c ) ( cid : 123 ) 123a ( c ) , then
123 ( cid : 123 ) ( cid : 123 ) log jr ( c ) j + tr r ( c ) ( cid : 123 ) dc + h ( c )
t t ( c ) ( cid : 123 ) 123h ( c )
ic ( cid : 123 ) :
we make use of the cholesky decomposition of t ( c ) to compute expressions involving t ( c ) ( cid : 123 ) 123
d log jr ( c ) j = tr a ( c ) ( cid : 123 ) 123 ( cid : 123 ) da ( c ) ( cid : 123 ) ( cid : 123 ) tr t ( c ) ( cid : 123 ) 123 ( cid : 123 ) dt ( c ) ( cid : 123 ) ; d tr r ( c ) = ( cid : 123 ) tr t ( c ) ( cid : 123 ) 123a ( c ) t ( c ) ( cid : 123 ) 123 ( cid : 123 ) dt ( c ) ( cid : 123 ) + tr t ( c ) ( cid : 123 ) 123 ( cid : 123 ) da ( c ) ( cid : 123 )
dt ( c ) = d ~ k
ic ( cid : 123 ) + 123xp
and with t ( c ) = t ( c ) ( cid : 123 ) 123h ( c )
ic we have
t t ( c ) ( cid : 123 ) 123h ( c )
ic = 123t ( c ) t ( cid : 123 ) dh ( c )
ic ( cid : 123 ) ( cid : 123 ) t ( c ) t ( cid : 123 ) dt ( c ) ( cid : 123 ) t ( c ) ;
ic ( cid : 123 ) ; dg123 ( c ) = tr j ( 123;c ) t ( cid : 123 ) da ( c ) ( cid : 123 ) + tr j ( 123;c ) t ( cid : 123 ) dt ( c ) ( cid : 123 ) + t ( c ) t ( cid : 123 ) dh ( c )
j ( 123;c ) =
123 ( cid : 123 ) t ( c ) ( cid : 123 ) 123 ( cid : 123 ) a ( c ) ( cid : 123 ) 123 ( cid : 123 ) ; j ( 123;c ) =
ic ( cid : 123 ) ( cid : 123 ) 123 sym ~ m
123 ( cid : 123 ) t ( c ) ( cid : 123 ) 123 ( cid : 123 ) t ( c ) ( cid : 123 ) 123a ( c ) t ( c ) ( cid : 123 ) 123 ( cid : 123 ) t ( c ) t ( c ) t ( cid : 123 ) : ic ( cid : 123 ) + ~ m
ic ( cid : 123 ) ~ m
we can incorporate the dh ( c ) the computation of f ( c ) ( eq .
123 ) and ~ z c ( eq .
the da ( c ) is then incorporated by
ic part by replacing cc in eq .
123 by cc + i ( cid : 123 ) ;ict ( c ) which a ( cid : 123 ) ects
~ z c ~ z c + i ( cid : 123 ) ;ic ( cid : 123 ) ~ m
ic; ( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) t
j ( 123;c ) ( cid : 123 ) ~ m
ic; ( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) :
the dt ( c ) part results in the following direct gradient contributions :
c + = j ( 123;c ) ; z ( 123 )
p + = ( cid : 123 ) 123
c;pi ( cid : 123 ) ;icj ( 123;c )
( cid : 123 ) ;123 : : : d; z ( 123 )
c;p+ = ( cid : 123 ) 123
c;pi ( cid : 123 ) ;icj ( 123;c )
z ( 123 ) + = ( cid : 123 ) 123 ( cid : 123 ) c;p tr j ( 123;c ) k ( p )
denote the parts in eq .
123 by dg123;123 ( c ) ; dg123;123 ( c ) ; dg123;123 ( c ) respectively
( cid : 123 ) ( c ) = ~ ( cid : 123 )
+ m ( 123;c ) m ( 123;c ) t ;
c ( cid : 123 ) i ) ( cid : 123 ) k ( cid : 123 ) m ( 123 ) m ( 123 ) t ( cid : 123 ) ( ( cid : 123 ) c ( cid : 123 ) i ) :
( cid : 123 ) ( c ) does not depend on ~ ( cid : 123 )
consider only the ~ ( cid : 123 )
variation for the moment
= m ( 123 ) l ( 123 ) ( cid : 123 ) 123 = diag ( cid : 123 ) m ( 123 )
which overwrites m ( 123 ) once not needed anymore .
the cost is o ( n p d123 ) .
from dg123;123 ( c ) we
c + = gc;
p + = ~ g p ( ( cid : 123 ) ) ;
~ g p =xc z ( 123 ) + = ( cid : 123 ) 123 ( cid : 123 ) c;p ( cid : 123 ) diag k ( p ) ( cid : 123 ) diag m ( 123 ) p + = ( cid : 123 ) 123 ( diag ~ g p ) ~ m
p + i ( cid : 123 ) ;i ~ m
p m ( 123 )
( diag ~ g p ) ~ m
and from dg123;123 ( c ) we have
c + = ~ z c; z ( 123 )
c;p+ = ( cid : 123 ) 123
z ( 123 ) + = ( cid : 123 ) 123 ( cid : 123 ) c;p tr ~ z
c ( cid : 123 ) k ( p ) c;p ( cid : 123 ) ( ~ z c ) ( cid : 123 ) ;123 : : : d ( cid : 123 ) ~ z c
p + = ( cid : 123 ) 123
c;p ( ~ z c ) ( cid : 123 ) ;d+123 : : : dc; p m ( 123 ) ( cid : 123 ) ;ic ( cid : 123 ) m ( 123 )
ic;p ( cid : 123 ) i ( cid : 123 ) ;ic
p + i ( cid : 123 ) ;i ( cid : 123 ) ~ z
here and elsewhere , if ( say ) m ( 123 ) = ( mi; ( j;p ) ) i; ( j;p ) , then m ( 123 ) the ~ z c in the c loop and do the d ~ ( cid : 123 )
ic;p = ( mi; ( j;p ) ) i123ic;j 123 rdc;d .
accumulator is accessed only here , so it is easiest to compute updates in one batch after this loop .
the total
overwrites ~ z c .
the z ( 123 )
cost of the batch ( given ~ z c ) is o ( n p d pc dc ) .
note that we need the kernel matrix k ( p )
except in cases where kernel evaluations are very expensive , these matrices should be recomputed here on the ( cid : 123 ) y in order to save memory ( part of the kernel matrix is required in eq .
123 as well ) .
next the dm ( 123;c ) m ( 123;c ) t part
c ( cid : 123 ) i ) k ( cid : 123 ) ;i = ( cid : 123 ) d ( cid : 123 ) t
c ( cid : 123 ) i ( cid : 123 ) k ( cid : 123 ) ;i + ( ( cid : 123 ) t
c ( cid : 123 ) i ) ( dk ( cid : 123 ) ;i ) ;
we see that for a d ( ( cid : 123 ) t ( tr bt k ( p )
c ( cid : 123 ) i ) k ( cid : 123 ) ;i part , a contribution z ( 123 )
p + = ( cid : 123 ) c;pb is paired with z ( 123 ) + =
( cid : 123 ) ;i ) c;p , so we only need to deal with the dk ( cid : 123 ) ;i part explicitly
= m ( 123;c ) l ( 123;c ) ( cid : 123 ) 123 ( cid : 123 ) t 123 rn;p d
which overwrites m ( 123;c ) , the cost is o ( n p 123 d123 )
dm ( 123;c ) m ( 123;c ) t =xp
123 ( cid : 123 ) c;p sym ( cid : 123 ) dk ( p )
( ignoring d ( cid : 123 ) c ) .
we deal with the da ( 123;c ) part later
sc = ~ m
( diag gc ) ~ m
f ( c ) = ~ z
123 rdc;p d;
( cid : 123 ) p;p123 123 rp d;p d;
which comes at cost o ( n p 123 d123 ) for sc and o ( n p d dc ) for f ( c ) .
the contribution of da ( 123;c ) ( through dg123;123 ( c ) ; dg123;123 ( c ) ) is
dg123;123 ( c ) + dg123;123 ( c ) = tr ( cid : 123 ) ( cid : 123 ) sc ( cid : 123 ) ~ m
ignoring da ( 123;c ) we have the contributions
ic; ( cid : 123 ) f ( c ) ( cid : 123 ) ( cid : 123 ) d ( cid : 123 ) a ( 123;c ) ( cid : 123 ) t ( cid : 123 ) :
p + = ( cid : 123 ) c;pb; b = 123 ( diag gc ) ~ m
p + ~ z c
ic;p + i ( cid : 123 ) ;icf ( c )
z ( 123 ) + = ( cid : 123 ) tr bt k ( p )
at cost o ( n p d dc ) .
the relationship through b may be exploited in an implementation .
b occurs as intermediate in the computation of s c .
next we look at dg123;123 ( c )
" ( c ) = ( cid : 123 ) l ( 123;c ) ( cid : 123 ) t ( cid : 123 ) ( 123;c ) 123 rp d;
c ( cid : 123 ) i ) k ( cid : 123 ) ;i ( cid : 123 ) " ( c ) ( cid : 123 ) ~ m
again , da ( 123;c ) is dealt with later
( 123;c ) ( cid : 123 ) d ( cid : 123 ) a ( 123;c ) ( cid : 123 ) t ( cid : 123 ) " ( c ) + ~ m
( 123;c ) ( cid : 123 ) dk i ( cid : 123 ) ^p
q ( c ) = ~ m
f ( c ) 123 rp d;
the contribution is
for the ( cid : 123 ) rst part in eq .
123 we have
dg123;123 ( c ) = tr ( cid : 123 ) ( cid : 123 ) " ( c ) q ( c ) t ( cid : 123 ) ( cid : 123 ) d ( cid : 123 ) a ( 123;c ) ( cid : 123 ) t ( cid : 123 ) :
p + = ( cid : 123 ) c;pb; b = f ( c ) " ( c )
t ; z ( 123 ) + = ( cid : 123 ) tr bt k ( p )
where again we use the pairing of z ( 123 ) p and z ( 123 ) contributions .
note that this contribution can be fused with eq .
123 by adding the corresponding b matrices .
for the third part in eq .
123 we note that in ^p
, the rows are in ui ordering , so that
( ( cid : 123 ) c123 ( cid : 123 ) i ) e ( c123 )
b ( c ) = e ( c )
123 : : : d; ( cid : 123 ) e ( c ) t 123 rd;dc; ( cid : 123 ) ( c ) = e ( c ) ( cid : 123 ) ( 123;c ) 123 rdc :
then , dk i gives the contribution
p + = i ( cid : 123 ) ;iq ( c )
the contribution for d ( cid : 123 ) ^p
makes use of eq
z ( 123 ) + = ( cid : 123 ) q ( c ) c123 + = ( cid : 123 ) b ( c123 ) t xp
p ! ( cid : 123 ) ( c123 ) t ! ( c123 123= c ) :
here , only the rows c123 123= c of z ( 123 ) are updated .
it remains to deal with the da ( 123;c ) part which is
dg123 ( c ) = tr u ( c ) ( cid : 123 ) d ( cid : 123 ) a ( 123;c ) ( cid : 123 ) t ( cid : 123 ) ; u ( c ) = ( cid : 123 ) sc ( cid : 123 ) ~ m
here , u ( c ) can be replaced by sym u ( c ) which we do .
we have
ic; ( cid : 123 ) f ( c ) ( cid : 123 ) " ( c ) q ( c ) t 123 rp d;p d :
d ( cid : 123 ) a ( 123;c ) ( cid : 123 ) t = ( dk i ) + 123 sym ( dk i ) ( cid : 123 ) ^p
then ignoring d ^p
q ( c ) = u ( c ) k i ( cid : 123 ) ^p
, the contribution is
( cid : 123 ) t ki + k i ( cid : 123 ) d ( cid : 123 ) ^p ( cid : 123 ) t 123 rp d;p d;
( cid : 123 ) t ( cid : 123 ) k i :
where we used that u ( c ) is symmetric .
recall that u ( c ) ( p; p ) in u ( c ) .
the remaining term is
p + = i ( cid : 123 ) ;i ( cid : 123 ) u ( c )
p + 123q ( c )
p ( cid : 123 ) ; p denotes the d ( cid : 123 ) d block at position
tr k i u ( c ) ki ( cid : 123 ) d ( cid : 123 ) ^p ( cid : 123 ) t = xc123=c ( cid : 123 ) 123 sym ( d ( cid : 123 ) c123 ( cid : 123 ) i ) b ( c123 ) ( cid : 123 ) ( ( cid : 123 ) c123 ( cid : 123 ) i ) b ( c123 ) ( cid : 123 ) d ~ k
c123 ( cid : 123 ) i ) ic123 ( cid : 123 ) b ( c123 ) t ( ( cid : 123 ) t
c123 ( cid : 123 ) i ) ( cid : 123 ) :
w ( c ) = k i ( ( cid : 123 ) c ( cid : 123 ) i ) b ( c ) = ( cid : 123 ) ( cid : 123 ) c;pk ( p )
i b ( c ) ( cid : 123 ) p 123 rp d;dc;
then the contribution is
c123 + = ( cid : 123 ) w ( c123 ) t u ( c ) w ( c123 ) ; z ( 123 ) + = ( cid : 123 ) 123 tr w ( c123 )
t u ( c )
( cid : 123 ) ;p k ( p )
this completes the description of dg w . r . t .
parameters of the covariance functions .
the computational cost is estimated under the assumption that n ( cid : 123 ) dc for all c and p < c .
the dominating operations are the computation of ~ m and sc at o ( n c p 123 d123 ) , and the computation of the ~ ( cid : 123 ) batch and computation of ~ m
and ~ z c at o ( n pc d123
of the f ( c ) at o ( n p d pc dc ) .
thus , the complexity is c + p d c p d +xc
o n xc
which is the same as cost as computing the representation without selecting the active sets ( see section 123 . 123 ) .
the gradient w . r . t .
likelihood parameters depends on the details .
for example , if the p ( y cjvc ) have intercept parameters ( cid : 123 ) c in the sense that p ( ycjvc ) = f ( yc; vc + ( cid : 123 ) c ) , it is easy to see
dg123 = xi
if p ( ycjvc ) = n ( ycjvc; ( cid : 123 ) 123
c ) , then
ai;c + ( yi;c ( cid : 123 ) hi;c ) 123
we close the section providing some further details for the gradient computation in our implementation .
first of all , it is actually not sensible to store the accumulators z ( 123 ) rn;dc ( cid : 123 ) d which would come at a total memory cost of o ( n p pc dc ) .
fortunately , the z ( 123 )
are updated in eq .
123 and eq .
123 only , so it is easy to compute the gradient contributions on the ( cid : 123 ) y .
in general we separate accumulation from the ( cid : 123 ) nal traces with kernel derivative matrices for the sake of a simple implementation .
d . 123 derivative w . r . t .
( cid : 123 )
c for regression model
if the slfm is used for regression with a gaussian noise model p ( ycjvc ) = n ( ycjvc; ( cid : 123 ) 123 c ) the site parameters b ( c ) ; d ( c ) depend on ( ( cid : 123 ) 123 c ) c and the ( cid : 123 ) xed target values y only .
in this case we can refrain from ( cid : 123 ) xing the site parameters during the inner loop optimization , so that only the active set indicators ic are kept ( cid : 123 ) xed .
in this section we compute the derivative of g w . r . t .
the noise variances .
denote lc = log ( cid : 123 ) 123
we accumulate @g=@lc in zc .
note that d ( c ) = ( cid : 123 ) ( cid : 123 ) 123
c i and b ( c ) = ( cid : 123 ) ( cid : 123 ) 123
dd ( c ) 123=123 = ( cid : 123 )
dd ( c ) ( cid : 123 ) 123=123b ( c ) = ( cid : 123 )
in the following we make use of these simple relationships , furthermore of the fact that d ( c ) / i and therefore commutes with all matrices .
we also concentrate exclusively on the
some algebra gives
dm ( 123;c ) m ( 123;c ) t = ( cid : 123 ) ( cid : 123 ) 123
dm ( 123;c ) ( cid : 123 ) ( 123;c ) = ( cid : 123 ) ( cid : 123 ) 123
123 results in the contribution
zc+ = ( cid : 123 ) 123
( diag gc ) ~ m
contributions from dg123 are through da ( c ) ; dh ( c ) ic; ( cid : 123 ) j ( 123;c ) ~ m
zc+ = ( cid : 123 ) 123
ic; ( cid : 123 ) ( cid : 123 ) t ( c ) t ~ m
ic; ( cid : 123 ) ( cid : 123 ) ( c ) ( cid : 123 ) :
the next contribution is through d ^p in the contribution
we have d ( cid : 123 ) ( c )
c b ( c ) ( cid : 123 ) ( c ) ( dlc ) , resulting
finally for the d ^p
part we note that
123; : : : ;d = ( cid : 123 ) ( cid : 123 ) 123
a ; c123 123= c :
t = ( cid : 123 ) ( cid : 123 ) 123
c b ( c ) b ( c ) t ;
resulting in the contribution
c123 tr w ( c123 ) t u ( c ) w ( c123 ) ( cid : 123 ) ; c123 123= c :
note all of the zc accumulations use terms which have already been computed for the main gradient .
the contribution through the likelihood factors ( eq .
123 ) has to be added to zc as
