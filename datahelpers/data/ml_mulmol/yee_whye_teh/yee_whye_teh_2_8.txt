we introduce a new probability distribution over a potentially innite number of binary markov chains which we call the markov indian buffet process .
this pro - cess extends the ibp to allow temporal dependencies in the hidden variables .
we use this stochastic process to build a nonparametric extension of the factorial hid - den markov model .
after constructing an inference scheme which combines slice sampling and dynamic programming we demonstrate how the innite factorial hidden markov model can be used for blind source separation .
when modeling discrete time series data , the hidden markov model ( 123 ) ( hmm ) is one of the most widely used and successful tools .
the hmm denes a probability distribution over observations y123 , y123 , yt using the following generative model : it assumes there is a hidden markov chain s123 , s123 , , st with st ( 123 k ) whose dynamics is governed by a k by k stochastic transition matrix .
at each timestep t , the markov chain generates an output yt using some likelihood model f parametrized by a state dependent parameter st .
we can write the probability distribution induced by the hmm as follows123
p ( y123 : t , s123 : t ) =
st123 , stf ( yt; st ) .
figure 123 shows the graphical model for the hmm .
one shortcoming of the hidden markov model is the limited representational power of the latent variables .
one way to look at the distribution dened by the hmm is to write down the marginal distribution of yt given the previous latent state st123
st123 , stf ( yt; st ) .
equation ( 123 ) illustrates that the observations are generated from a dynamic mixture model .
the factorial hidden markov model ( fhmm ) , developed in ( 123 ) , addresses the limited representational power of the hidden markov model .
the fhmm extends the hmm by representing the hidden state
123to make the notation more convenient , we assume w . l . o . g .
that for all our models , all latent chains start in
a dummy state that is in the 123 state .
for the hmm s123 = 123 , for the fhmm s ( m )
123 = 123 for all m .
figure 123 : the hidden markov model
figure 123 : the factorial hidden markov model
( cid : 123 ) yt as follows : m latent chains s ( 123 ) ( cid : 123 ) s ( 123 ) ( cid : 123 ) ( cid : 123 )
in a factored form .
this way , information from the past is propagated in a distributed manner through a set of parallel markov chains .
the parallel chains can be viewed as latent features which evolve over time according to markov dynamics .
formally , the fhmm denes a probability distribution over observations y123 ( cid : 123 ) y123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) s ( m ) evolve according to markov dynamics and at each timestep t , the markov chains generate an output yt using some likelihood model f parameterized by a joint state - dependent parameter ( cid : 123 ) s .
the graphical model in gure 123 shows how the fhmm is a special case of a dynamic bayesian network .
the fhmm has been successfully applied in vision ( 123 ) , audio processing ( 123 ) and natural language processing ( 123 ) .
unfortunately , the dimensionality m of our factorial representation or equivalently , the number of parallel markov chains , is a new free parameter for the fhmm which we would prefer learning from data rather than specifying it beforehand .
recently , ( 123 ) introduced the basic building block for nonparametric bayesian factor models called the indian buffet process ( ibp ) .
the ibp denes a distribution over innite binary matrices z where element znk denotes whether datapoint n has feature k or not .
the ibp can be combined with distributions over real numbers or integers to make the features useful for practical problems .
in this work , we derive the basic building block for nonparametric bayesian factor models for time series which we call the markov indian buffet process ( mibp ) .
using this distribution we build a nonparametric extension of the fhmm which we call the innite factorial hidden markov model ( ifhmm ) .
this construction allows us to learn a factorial representation for time series .
in the next section , we develop the novel and generic nonparametric mibp distribution .
section 123 describes how to use the mibp do build the ifhmm .
which in turn can be used to perform inde - pendent component analysis on time series data .
section 123 shows results of our application of the ifhmm to a blind source separation problem .
finally , we conclude with a discussion in section 123
123 the markov indian buffet process
similar to the ibp , we dene a distribution over binary matrices to model whether a feature at time t is on or off .
in this representation rows correspond to timesteps and the columns to features or markov chains .
we want the distribution over matrices to satisfy the following two properties : ( 123 ) the potential number of columns ( representing latent features ) should be able to be arbitrary large; ( 123 ) the rows ( representing timesteps ) should evolve according to a markov process .
below , we will formally derive the mibp distribution in two steps : rst , we describe a distribution over binary matrices with a nite number of columns .
we choose the hyperparameters carefully so we can easily integrate out the parameters of the model .
in a second phase , we take the limit as the number of features goes to innity in a manner analogous to ( 123 ) s derivation of innite mixtures .
123 a nite model
let s represent a binary matrix with t rows ( datapoints ) and m columns ( features ) .
stm represents the hidden state at time t for markov chain m .
each markov chain evolves according to the transition
( cid : 123 ) 123 ( cid : 123 ) am am
123 ( cid : 123 ) bm bm
w ( m ) =
ij = p ( st+123 , m = j|stm = i ) .
we give the parameters of w ( m ) distributions am where w ( m ) beta ( / m , 123 ) and bm beta ( , ) .
each chain starts with a dummy zero state s123m = 123
the hidden state sequence for chain m is generated by sampling t steps from a markov chain with transition matrix w ( m ) .
summarizing , the generative specication for this process is
m ( 123 , 123 , , m ) : am beta
bm beta ( , ) ,
s123m = 123 ,
next , we evaluate the probability of the state matrix s with the transition matrix parameters w ( m ) m be the number of 123 marginalized out .
we introduce the following notation , let c123 123 , 123 123 , 123 123 and 123 123 transitions respectively , in binary chain m ( including the transition from the dummy state to the rst state ) .
we can then write
m , c123
m , c123
m , c123
p ( s|a , b ) =
m ( 123 bm ) c123
we integrate out a and b with respect to the conjugate priors dened in equation ( 123 ) and nd
p ( s| , , ) =
m + c123 m + c123
m + c123
m + 123 ) ( + ) ( + c123 m + 123 ) ( ) ( ) ( + + c123
m ) ( + c123 m ) , m + c123
where ( x ) is the gamma function .
123 taking the innite limit analogous to the ibp , we compute the limit for m of the nite model in equation ( 123 ) .
the probability of a single matrix in the limit as m is zero .
this is not a problem since we are only interested in the probability of a whole class of matrices , namely those matrices that can be transformed into each other through column permutations .
in other words , our factorial model is exchangeable in the columns as we dont care about the ordering of the features .
hence , we compute the innite limit for left - ordered form ( lof ) - equivalence classes ( 123 ) .
the left - ordered form of a binary s matrix can be dened as follows : we interpret one column of length t as encoding a binary number : column m encodes the number 123t123s123m +123t123s123m ++ st m .
we call the number which a feature encodes the history of the column .
then , we denote with mh the number of columns in the matrix s that have the same history .
we say a matrix is a lof - matrix if its columns are sorted in decreasing history values .
let s be a lof - matrix , then we denote with ( s ) the set of all matrices that can be transformed into s using only column permutations; we call ( s ) the lof - equivalence class .
one can check that the number of elements in the lof - equivalence class of s is equal to
we thus nd the probability of the equivalence class of s to be
p ( ( s ) ) = ( cid : 123 )
p ( s| , , )
m ) ( + c123 m ) .
( 123 ) m + c123 this form allows us to compute a meaningful limit as m .
a writeup on the technical details of this computation can be found on the authors website .
the end result has the following form
m + 123 ) ( + ) ( + c123 m + 123 ) ( ) ( ) ( + + c123
m + c123 m + c123
m + c123
m p ( ( s ) ) =
m ) ( + c123 m ) , m + c123 where ht denotes the tth harmonic number and m+ denotes the number of markov chains that switch on at least once between 123 and t , i . e .
m+ is the effective dimension of our model .
m ) ! ( ) ( ) ( + + c123
m ! ( + ) ( + c123
m + c123
123 properties of the distribution
first of all , it is interesting to note from equation ( 123 ) that our model is exchangeable in the columns and markov exchangeable123 in the rows .
next , we derive the distribution in equation ( 123 ) through a stochastic process that is analogous to the indian buffet process but slightly more complicated for the actors involved .
in this stochastic process , t customers enter an indian restaurant with an innitely long buffet of dishes organized in a line .
the rst customer enters the restaurant and takes a serving from each dish , starting at the left of the buffet and stopping after a poisson ( ) number of dishes as his plate becomes overburdened .
a waiter stands near the buffet and takes notes as to how many people have eaten which dishes .
the tth customer enters the restaurant and starts at the left of the buffet .
at dish m , he looks at the customer in front of him to see whether he has served himself that dish .
m + ) / ( + + c123
if so , he asks the waiter how many people have previously served themselves dish m when the person in front of them did ( the waiters replies to him the number c123 m ) and how many people didnt serve themselves dish m when the person in front of them did ( the waiter m ) .
the customer then serves himself dish m with probability replies to him the number c123 m + c123 otherwise , he asks the waiter how many people have previously served themselves dish m m ) and when the person in front of them did not ( the waiters replies to him the number c123 how many people didnt serve themselves dish m when the person in front of them did not m ) .
the customer then serves himself dish m either ( the waiter replies to him the number c123 with probability c123
m + c123
t=123 m ( t )
the customer then moves on to the next dish and does exactly the same .
after the customer has passed all dishes people have previously served themselves from , he tries poisson ( / t ) new dishes .
if we denote with m ( t ) the number of new dishes tried by the tth customer , the probability of any particular matrix being produced by this process is
m ) ( + c123 m ) .
m + c123 we can recover equation ( 123 ) by summing over all possible matrices that can be generated using the markov indian buffet process that are in the same lof - equivalence class .
it is straightforward of these .
multiplying this by equation ( 123 ) we recover to check that there are exactly equation ( 123 ) .
this construction shows that the effective dimension of the model ( m+ ) follows a poisson ( ht ) distribution .
m + 123 ) ( + ) ( + c123 m + 123 ) ( ) ( ) ( + + c123
m + c123 m + c123
m + c123
123 a stick breaking representation
although the representation above is convenient for theoretical analysis , it is not very practical for inference .
interestingly , we can adapt the stick breaking construction for the ibp ( 123 ) to the mibp .
this will be very important for the ifhmm as it will allow us to use a combination of slice sampling and dynamic programming to do inference .
the rst step in the stick breaking construction is to nd the distribution of a ( 123 ) > a ( 123 ) > , the order statistics of the parameters a .
since the distribution on the variables am in our model are identical to the distribution of the feature parameters in the ibp model , we can use the result in ( 123 ) that these variables have the following distribution
a ( 123 ) beta ( , 123 ) ,
p ( a ( m ) |a ( m123 ) ) = a
i ( 123 a ( m ) a ( m123 ) ) .
the variables bm are all independent draws from a beta ( , ) distribution which is independent of m .
hence if we denote with b ( m ) the b variable corresponding to the mth largest a value ( in other words : the b value corresponding to a ( m ) ) then it follows that b ( m ) beta ( , ) .
123a sequence is markov exchangeable if its distribution is invariant under permutations of the transitions .
figure 123 : the innite factorial hidden markov model
123 the innite factorial hidden markov model
in this section , we explain how to use the mibp as a building block in a full blown probabilistic model .
the mibp provides us with a matrix s which we interpret as an arbitrarily large set of par - allel markov chains .
first we augment our binary representation with a more expressive component which can describe feature specic properties .
we do this by introducing a base distribution h from which we sample a parameter ( cid : 123 ) m ( cid : 123 ) h for each markov chain .
this is a rather exible setup as the base distribution can introduce a parameter for every chain and every timestep , which we will illustrate in section 123 .
now that we have a model with a more expressive latent structure , we want to add a likelihood model f which describes the distribution over the observations conditional on the latent structure .
formally , f ( yt ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) st ( cid : 123 ) ( cid : 123 ) ) describes the probability of generating yt given the model parameters ( cid : 123 ) and the current latent feature state st ( cid : 123 ) ( cid : 123 ) .
we note that there are two important conditions which the likelihood must satisfy in order for the limit m ( cid : 123 ) ( cid : 123 ) to be valid : ( 123 ) the likelihood must be invariant to permutations of the features , ( 123 ) the likelihood cannot depend on ( cid : 123 ) m if stm = 123
figure 123 shows the graphical model for our construction which we call the innite factorial hidden markov model ( ifhmm ) .
in the following section , we describe one particular choice of base distribution and likelihood model which performs independent component analysis on time series .
123 the independent component analysis ifhmm
independent component analysis ( 123 ) ( ica ) means different things to different people .
originally invented as an algorithm to unmix a signal into a set of independent signals , it will be more insightful for our purpose to think of ica in terms of the probabilistic model which we describe below .
as we explain in detail in section 123 , we are interested in ica to solve the blind source separation problem .
assume that m signals are represented through the vectors xm; grouping them we can represent the signals using the matrix x = ( x123x123 ( cid : 123 ) ( cid : 123 ) xm ) .
next , we linearly combine the signals using a mixing matrix w to generate the observed signal y = xw .
additionally , we will assume iid y ) noise added : y = xw + ( cid : 123 ) .
a variety of fast algorithms exist which unmix the observations y and recover the signal x .
how - ever , crucial to these algorithms is that the number of signals is known in advance .
( 123 ) used the ibp to design the innite independent component analysis ( iica ) model which learns an appropri - ate number of signals from exchangeable data .
our ica ifhmm model extends the iica for time the ica ifhmm generative model can be described as follows : we sample s ( cid : 123 ) mibp and point - wise multiply ( denoted by ( cid : 123 ) ) it with a signal matrix x .
each entry in x is an iid sample from a laplace ( 123 ( cid : 123 ) 123 ) distribution .
one could choose many other distributions for x , but since in section 123 we will model speech data , which is known to be heavy tailed , the laplace distribution is a conve - nient choice .
speakers will be speaking infrequently so pointwise multiplying a heavy tailed distri - bution with a sparse binary matrix achieves our goal of producing a sparse heavy tailed distribution .
next , we introduce a mixing matrix w which has a row for each signal in s ( cid : 123 ) x and a column for each observed dimension in y .
the entries for w are sampled iid from a normal ( 123 ( cid : 123 ) ( cid : 123 ) 123 distribution .
finally , we combine the signal and mixing matrices as in the nite case to form the
observation matrix y : y = ( s ( cid : 123 ) x ) w + where is normal ( 123 , 123 y ) iid noise for each element .
in terms of the general ifhmm model dened in the previous section , the base distribution h is a joint distribution over columns of x and rows of w .
the likelihood f performs the pointwise multiplication , mixes the signals and adds the noise .
it can be checked that our likelihood satises the two technical conditions for proper ifhmm likelihoods described in section 123
inference for nonparametric models requires special treatment as the potentially unbounded dimen - sionality of the model makes it hard to use exact inference schemes .
traditionally , in nonparametric factor models inference is done using gibbs sampling , sometimes augmented with metropolis hast - ings steps to improve performance .
however , it is commonly known that naive gibbs sampling in a time series model is notoriously slow due to potentially strong couplings between successive time steps ( 123 ) .
in the context of the innite hidden markov model , a solution was recently proposed in ( 123 ) , where a slice sampler adaptively truncates the innite dimensional model after which a dy - namic programming performs exact inference .
since a stick breaking construction for the ifhmm is readily available , we can use a very similar approach for the ifhmm .
the central idea is the following : we introduce an auxiliary slice variable with the following distribution
it is not essential that we sample from the uniform distribution , in fact for some of our experiments we use the more exible beta distribution .
the resulting joint distribution is
p ( , a , b , s ) = p ( |a , s ) p ( a , b , s ) .
it is clear from the equation above that one recovers the original mibp distribution when we integrate out .
however , when we condition the joint distribution on we nd
p ( s|y , , a , b ) p ( s|y , a , b )
i ( 123 minm : t , stm=123 am )
which forces all columns of s for which am < to be in the all zero state .
since there can only be a nite number of am > , this effectively implies that we need only resample a nite number of columns of s .
we now describe our algorithm in the context of the ica ifhmm : we start with an initial s matrix and sample a , b .
next , conditional on our initial s and the data y , we sample the ica parameters x and w .
we then start an iterative sampling scheme which involves the following steps :
we sample the auxiliary slice variable .
this might involve extending the representation
of s , x and w ,
for all the represented features , we sample s , x and w , 123
we resample the hyperparameters ( y , w , , , ) of our model , 123
we compact our representation by removing all unused features .
we experimented with 123 different algorithms for step 123
the rst , a naive gibbs sampler , did not perform well as we expected .
the second algorithm , which we used for our experiments , is a blocked gibbs sampler which xes all but one column of s and runs a forward - ltering backward - sampling sweep on the remaining column .
this allows us to analytically integrate out one column of x in the dynamic program and resample it from the posterior afterwards .
w can be sampled exactly conditional on x , s and y .
a third algorithm runs dynamic programming on multiple chains at once .
we originally designed this algorithm as it has the potential to merge two features in one sweep .
however , we found that because we cannot integrate out x and w in this setting , the inference was not faster than our second algorithm .
note that because the bulck of the computation is used for estimating x and w , the dynamic programming based algorithms are effectively as fast as the naive gibbs sampler .
a prototype implementation of the ifhmm sampler in matlab or . net can be obtained from the rst author .
( a ) ground truth
( b ) ica ifhmm
( d ) ica ifhmm
figure 123 : blind speech separation experiment; gures represent which speaker is speaking at a cer - tain point in time : columns are speakers , rows are white if the speaker is talking and black otherwise .
the left gure is ground truth , the next two gures in are for the 123 microphone experiment , the right two gures are for the 123 microphone experiment .
y ) noise with y = 123 .
to test our model and inference algorithms , we address a blind speech separation task , also known as the cocktail party problem .
more specically , we record multiple people who are simultane - ously speaking , using a set of microphones .
given the mixed speech signals , the goal is to separate out the individual speech signals .
key to our presentation is that we want to illustrate that using nonparametric methods , we can learn the number of speakers from a small amount of data .
our rst experiment learns to recover the signals in a setting with more microphones then speakers , our second experiment uses less microphones then speakers .
the experimental setup was the following : we downloaded data from 123 speakers from the speech separation challenge website123
the data for each speaker consists of 123 sentences which we ap - pended with random pauses in between each sentence .
figure 123 ( a ) illustrates which person is talking at what point in time .
next , we articially mix the data 123 times .
each mixture is a linear combi - nation of each of the 123 speakers using uniform ( 123 , 123 ) mixing weights .
we centered the data to have zero mean and unit variance and added iid normal ( 123 , 123 in our rst experiment we compared the ica ifhmm with the iica model using all 123 microphones .
we subsample the data so we learn from 123 datapoints .
we initialized the samplers for both models with an initial s matrix with 123 features , 123% random entries on .
we use a gamma ( 123 , 123 ) prior on .
in both models , we use a inversegamma ( 123 , 123 ) prior for y and w .
finally , for the ifhmm , we chose a gamma ( 123 , 123 ) prior on and a gamma ( 123 , 123 ) prior on to encode our belief that people speak for larger stretches of time , say the time to pronounce a sentence .
we ran the samplers for 123 iterations and then gathered 123 samples every 123 iterations .
for both the ica ifhmm and iica models , we average the 123 samples and rearrange the features to have maximal overlap with the ground truth features .
figure 123 ( b ) shows that the ica ifhmm model recognizes that the data was generated from 123 speakers .
visual inspection of the recovered s matrix also shows that the model discovers who is speaking at what time .
123 ( c ) illustrated the results of the iica model on the same data .
although the model discovers some structure in the data , it fails to nd the right number of speakers ( it nds 123 ) and does a poor job in discovering which speaker is active at which time .
we computed the average mutual information between the 123 columns of the true s matrix and the rst 123 columns of the recovered s matrices .
we nd that the ifhmm has an average mutual information of 123 compared to 123 for the iica model .
the difference between the two models is strictly limited to the difference between using the ibp versus mibp .
we want to emphasize that although one could come up with ad - hoc heuristics to smooth the iica results , the ica ifhmm is a principled probabilistic model that does a good job at comparable computational in a second experiment , we chose to perform blind speech separation using only the rst 123 micro - phones .
we subsampled a noiseless version of the data to get 123 datapoints .
we ran both the ica ifhmm and iica inference algorithms using exactly the same settings as in the previous experi -
figure 123 ( d ) and 123 ( e ) show the average of 123 samples , rearranged to match the ground truth .
in this setting both methods fail to identify the number of speakers although the ica ifhmm clearly performs better .
the ica ifhmm nds one too many signal : the spurious signal is very similar to the third signal which suggests that the error is a problem of the inference algorithm and not so much of the model itself .
the iica on the other hand performs poorly : it is very hard to nd any structure in the recovered z matrix .
we compared the mutual information as described above and nd that the ifhmm has a mutual information of 123 compared to 123 for the iica model .
the success of the hidden markov model set off a wealth of extensions to adapt it to particular situations .
( 123 ) introduced a factorial hidden markov model which explicitly models dynamic latent features while in ( 123 ) a nonparametric version of the the hidden markov model was presented .
in this paper we complete the square by presenting a nonparametric factorial hidden markov model .
we introduced a new stochastic process for latent feature representation of time series called the markov indian buffet process .
we showed how this stochastic process can be used to build a nonparametric extension of the fhmm which we call the ifhmm .
another issue which deserves further exploration is inference : in ( 123 ) it was found that a structured variational method provides a good balance between accuracy and computational effort .
an interesting open problem is whether we can adapt the structured variational method to the ifhmm .
finally , analogous to the two - parameter ibp ( 123 ) we would like to add one more degree of exibility to control the 123 123 transition probability more nely .
although the derivation of the mibp with this extra parameter is straightforward , we as yet lack a stick breaking construction for this model which is crucial for our
