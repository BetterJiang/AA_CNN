we present a new way of extending independent components analysis ( ica ) to overcomplete representations .
in contrast to the causal generative extensions of ica which maintain marginal independence of sources , we dene features as deterministic ( linear ) functions of the inputs .
this assumption results in marginal dependencies among the features , but conditional independence of the features given the inputs .
by assigning energies to the features a probability distribution over the input states is dened through the boltzmann distribution .
free parameters of this model are trained using the contrastive divergence objective ( hinton , 123 ) .
when the number of features is equal to the number of input dimensions this energy - based model reduces to noiseless ica and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data .
in additional experiments we train overcomplete energy - based models to extract features from various standard data - sets containing speech , natural images , hand - written digits and tions , sparse representations
independent components analysis , density estimation , overcomplete representa -
there have been two dominant ways of understanding ica , one based on a bottom - up , ltering view and the other based on a top - down , causal generative view .
in the information maximization approach ( bell and sejnowski , 123 , shriki et al . , 123 ) the aim is to maximize the mutual infor - mation between the observations and the non - linearly transformed outputs of a set of linear lters .
in the causal generative view ( pearlmutter and parra , 123 , mackay , 123 , cardoso , 123 ) , on the
c ( cid : 123 ) 123 yee whye teh , max welling , simon osindero and geoffrey e .
hinton .
teh et al .
linear components analysis
figure 123 : different methods for non - gaussian linear components analysis .
other hand , the aim is to build a density model in which independent , non - gaussian sources are linearly combined to produce the observations .
the main point of this paper is to show that there is a third , energy - based approach to un - derstanding ica which combines a bottom - up , ltering view with the goal of tting a probability density to the observations .
the parameters of an energy - based model specify a deterministic map - ping from an observation vector x to a feature123 vector and the feature vector determines a global energy , e ( x ) .
the probability density of x is dened by
where z is a normalization factor - the integral of the numerator over all possible observation vec - tors .
the energy - based approach is interesting because it suggests a novel and tractable way of extending ica to overcomplete and multi - layer models .
the relationship between the three approaches is depicted in figure 123
in general , they are quite different , but they all become equivalent for the square and noiseless case , that is , when the number of sources or features equals the number of observations and there is no observation noise .
while complete representations have been applied successfully to a wide range of problems , researchers have recently argued for overcomplete representations where there are more sources or features than observations .
apart from greater model exibility , reported advantages include improved robustness in the presence of noise ( simoncelli et al . , 123 ) , more compact and more easily interpretable codes ( mallat and zhang , 123 ) and superresolution ( chen et al . , 123 ) .
the natural way to extend the causal generative approach to the overcomplete case is to retain the assumption that the sources are independent when the model is used to generate data and to
when discussing energy - based models , we use the term feature rather than source for reasons that will become
clear when we discuss extensions to the overcomplete case .
sparse overcomplete energy - based models
over source vectors ( before an observation )
over source vectors ( after an observation )
table 123 : independence properties of three types of models .
accept the consequence that an observation vector creates a posterior distribution over a multiplicity of source vectors .
in this posterior distribution , the sources are conditionally dependent due to the effect known as explaining - away and , in general , the distribution has the unfortunate property that it is computationally intractable .
the natural way to extend the information maximization approach to overcomplete representa - tions is to retain both the simple , deterministic , feedforward ltering of observations and the mutual information objective function ( shriki et al . , 123 ) .
however , because the manifold of possible lter outputs typically does not consist of the whole space ( except in the square case ) , the equivalence with causal generative models breaks down .
when our energy - based approach to ica is made overcomplete , it continues to be a proper density model and it retains the computationally convenient property that the features are a deter - ministic function of the observation vector .
however , it abandons the marginal independence of the features ( which is why we do not call them sources ) .
a useful way of understanding the dif - ference between energy - based density models and causal generative density models is to compare their independence properties .
table 123 summarizes the similarities and differences .
the table reminds us that the different views are equivalent in the square case , and hence , in the absence of observations , the sources are marginally independent .
further , the posterior distribution over source vectors conditioned on an observation vector collapses to a point in the absence of noise , so the sources are trivially independent in the posterior distribution .
in the causal generative approach this conditional independence of the sources is seen as a fortuitous consequence of using as many sources as observations and avoiding noise in the observations , and is not retained in the overcomplete case .
in the energy - based view , the conditional independence of the features is treated as a basic assumption that remains true even in the overcomplete case .
we can consider the energy contributed by the activity of a feature of an energy - based model as the negative log probability of a one - dimensional , non - gaussian distribution .
however not all combinations of feature activities can
teh et al .
occur because the lower - dimensional observation space only maps to a restricted manifold in the
the marginal dependence of the features in an overcomplete , energy - based model can be under - stood by considering an illuminating but innitely inefcient way of generating unbiased samples from the energy - based density model .
first we sample the features independently from their prior distributions ( the negative exponentials of their individual energy contributions ) and then we reject cases in which the feature activities do not correspond to valid observation vectors .
this process of rejecting - away creates dependencies among the activities of different features .
for some applications , such as unmixing sound sources , the causal generative approach is clearly more appropriate than the energy - based approach because we have a strong prior belief that the sources are marginally independent .
in many other applications , however , the real aim is to model the probability density of the data , or to discover interpretable structure in the data , or to extract a representation that is more useful for controlling action than the raw data itself .
in these applications , there is no a priori reason for preferring the causal generative approach to the energy - based approach that characterizes each observation vector by representing the degree to which it satises a set of learned features .
square ica
in this section we will briey review standard models for ica .
one of the rst expositions on ica ( comon , 123 ) used the entropy of linearly transformed input vectors as a contrast function to nd statistically independent directions in input space .
indeed many , if not all , ica algorithms ulti - mately reduce to optimizing some sort of contrast function; this overview will not mention them all .
rather we will focus on reviewing two general approaches to ica , namely the causal generative approach ( pearlmutter and parra , 123 , mackay , 123 , cardoso , 123 ) and the information max - imization approach ( bell and sejnowski , 123 , shriki et al . , 123 ) .
subsequent sections will then compare these canonical approaches with our proposed energy - based approach , and in particular will explore the consequences of making the different models overcomplete .
consider a real valued input , denoted by x , of dimensionality d , and an m - dimensional source or feature vector , denoted by s .
in this section we will consider the special case where the number of input dimensions is equal to that of the sources or features , i . e .
d = m .
123 the causal generative approach
in the causal generative approach , the sources s are assumed to be independent , that is , the distribu - tion p ( s ) factorizes as
while the inputs are simply linear combinations of the sources .
moreover , we will assume for now that there is no noise on the inputs , i . e .
x = as;
where a is a square invertible matrix called the mixing matrix .
inverting this relationship we have
s = w x
w = a ( cid : 123 ) 123;
sparse overcomplete energy - based models
where the inverse mixing matrix w will be called the lter matrix since each row acts as a linear lter of the inputs .
the aim is now to recover the statistically independent source signals s from the linearly mixed observations x .
this turns out to be possible only if the statistical properties of the sources are non - gaussian .
thus , we shall assume that the probability distribution of the sources will be modelled by non - gaussian prior distributions pi ( si ) .
since the relation between sources and inputs is deter - ministic and one - to - one , we may view it as a change of coordinates .
deriving an expression for the probability distribution of the inputs x can therefore be accomplished by transforming expression ( 123 ) to x - space , using the jacobian of that transformation ,
i x ) jdetw j;
i are the rows of w .
learning proceeds by averaging the log - likelihood for the above model over a data distribution123 p123 ( x ) and using the derivatives of it with respect to w for gradient ascent :
+ ( cid : 123 ) w ( cid : 123 ) t ( cid : 123 ) i j ;
where wi j is the jth entry of wi and ( w ( cid : 123 ) t ) i j is the i jth entry of the matrix w ( cid : 123 ) t .
123 the information maximization approach
an alternative , more neurally plausible approach to ica was put forward by bell and sejnowski ( 123 ) . 123 in that paper it was assumed that a certain transformation was applied to the inputs ,
yi = fi ( wt
i = 123 : : : m;
with fi ( ( cid : 123 ) ) being a monotone squashing function such as a sigmoid and wi a set of linear lters .
it was then argued that maximizing the mutual information123 between outputs y and inputs x , which is equivalent to maximizing the entropy of y due to the deterministic relation ( 123 ) , would lead to independent components .
this effect can be understood through the decomposition
hi ( yi ) ( cid : 123 ) i ( y123; : : : ; ym ) ;
with h ( y ) the entropy of y , hi ( yi ) the individual entropies , and i the mutual information among yis .
maximizing the joint entropy thus involves maximizing the individual entropies of the yis and minimizing the mutual information between the yis , i . e .
making the yis independent .
this approach can best be described as a ltering approach , since each yi is just a squashed i x .
this is in contrast with the causal generative approach where
version of the lter outputs si = wt we instead think of x as being generated by s in a top - down manner .
this data distribution is the underlying distribution from which our observed data is sampled .
in practice , we replace
this by the empirical distribution over the training set .
in fact this information maximization approach to ica was proposed rst , followed by the causal generative ap -
proach .
we presented the two approaches in reverse order here for more intuitive exposition .
note that this mutual information is measured with respect to the data distribution p123
teh et al .
123 equivalence of the two approaches
for square representations the information maximization approach turns out to be equivalent to the causal generative one if we interpret fi ( ( cid : 123 ) ) to be the cumulative distribution function of pi ( ( cid : 123 ) ) ( pearlmutter and parra , 123 , mackay , 123 , cardoso , 123 ) .
this can be seen by observing that the entropy h ( y ) can be written as a negative kl divergence using a change of variables as follows :
h ( y ) = ( cid : 123 ) z dy p123 ( y ) log p123 ( y ) = ( cid : 123 ) z dx p123 ( x ) log jdetj ( x ) j and j ( x ) is the jacobian of the transformation between y and x ,
where p123 ( y ) = p123 ( x )
ji j ( x ) =
using some basic algebra it can be shown that jdetj ( x ) j is in fact exactly equal to equation ( 123 ) , since fi satises f 123 i ( ( cid : 123 ) ) = pi ( ( cid : 123 ) ) , being the cumulative function of pi .
therefore maximizing the entropy in equation ( 123 ) is indeed equivalent to maximizing the log likelihood of the model ( 123 ) .
note also that if the sources are actually distributed according to pi ( ( cid : 123 ) ) and mixed using a , then the transforma - tion ( 123 ) maps the input variables to independent , uniformly distributed variables over the range of fi ( ( cid : 123 ) ) , i . e .
the interval ( 123;123 ) in case of a sigmoid .
this geometric interpretation will be helpful in
123 square ica with input noise
in the previous section we have shown that the causal generative approach , in the special case of a square mixing matrix and no noise , is equivalent to the information maximization approach .
this equivalence will break down , however , when we consider a noise model for the inputs .
in that case , there is no longer a deterministic relationship between the inputs and the sources .
it is , however , still straightforward to write a probabilistic model for the joint distribution over sources and inputs ,
p ( x;s ) = p ( xjs )
unfortunately , even for isotropic gaussian noise it is no longer true that given a mixing matrix a the optimal reconstruction of the sources is simply given by equation ( 123 ) .
instead , one typically computes the maximum a posteriori ( map ) value ( or the mean , depending on the objective ) of the posterior distribution p ( sjx ) .
overcomplete generalizations of ica
the equivalence between the two approaches also breaks down when there are more sources than input dimensions , i . e .
when we consider overcomplete representations of the data .
we will now review overcomplete generalizations of ica based on both approaches .
123 the causal generative approach
arguably the most natural way to extend the ica framework to overcomplete representations is through the causal generative approach .
the corresponding directed graphical model is depicted in
sparse overcomplete energy - based models
figure 123a .
for noiseless inputs , nding the most probable state s corresponding to a particular input x now translates into the following optimization problem :
smap = argmax
such that x = as ) :
the above problem is typically hard and it can only be solved efciently for certain choices of pi ( ( cid : 123 ) ) .
for instance lewicki and sejnowski ( 123 ) argued that by choosing the priors to be laplacian the problem can be mapped to a standard linear program .
one can soften this optimization problem by introducing a noise model for the inputs .
for instance , using a spherical gaussian noise model with noise variance s 123 we nd the following joint probability density distribution over sources and inputs :
p ( x;s ) = p ( xjs ) p ( s ) = nx ( cid : 123 ) as;s 123i ( cid : 123 )
this leads to the following maximization problem to reconstruct the sources from the inputs :
smap = argmax
123s 123 jx ( cid : 123 ) asj123 +
maximum likelihood learning for the above noisy model using the em procedure involves av - eraging over the posterior distribution p ( sjx ) .
unfortunately this inference problem is intractable in general and approximations are needed .
in the literature one can nd a whole range of approx - imate inference techniques applied to this problem .
in olshausen and field ( 123 ) the posterior is approximated by a delta function at its map value .
thus at every iteration of learning and for every data vector the maximization in equation ( 123 ) needs to be performed . 123 in lewicki and sejnowski ( 123 ) it was argued that the approximation can be signicantly improved if a gaussian distribution around this map value was constructed by matching the second derivatives locally ( i . e .
the laplace approximation ) .
attias ( 123 ) and girolami ( 123 ) use a variational approach which replaces the true posterior with a tractable approximation which is itself adapted to better approximate the pos - terior .
finally , mcmc sampling methods , such as gibbs sampling may be employed to solve the inference problem approximately ( olshausen and millman , 123 ) .
a notably different variation on the generative theme is the bayesian approach taken by hyvari - nen and inki ( 123 ) .
there , a prior distribution p ( a ) over possible mixing matrices a is introduced which favors orthogonal basis vectors ( columns of a ) .
they argue that the role of the jacobian jdetw j = 123=jdetaj in equation ( 123 ) is precisely to encourage orthogonality among basis vectors and that it is therefore a reasonable assumption to remove this jacobian in favor of the extra prior .
the resultant expression is then easily extended to overcomplete representations .
we want to stress that causal generative models will almost always lead to very difcult infer - ence problems .
in contrast , generating unbiased samples from the distribution p ( x ) is relatively straightforward , since we rst sample source values independently from their priors and subse - quently sample the input variables according to the conditional gaussian in equation ( 123 ) .
in fact the situation is slightly better using a variational point of view .
one can show that one can also improve a bound on the log - likelihood by jointly maximizing over s and a .
we also note that an extra condition on the mixing matrix is needed to prevent it from collapsing to 123
teh et al .
inputs : x auxiliary vars : z
figure 123 : ( a ) directed graphical model corresponding to the causal generative approach to ica .
( b ) undi - rected graphical model for an ebm .
( c ) directed graphical model representation for an ebm with auxiliary variables clamped at 123
figure 123 : mapping used by the information maximization approach given by equation ( 123 ) .
123 the information maximization approach
in section 123 an information maximization approach to ica was discussed for the simple case when the number of inputs is equal to the number of sources and no noise is assumed on the inputs .
a natural question is whether that objective can be generalized to overcomplete representations .
one possibility advocated by shriki et al .
( 123 ) is to dene again the parametrized nonlinear mapping ( 123 ) between inputs and outputs and to maximize their mutual information ( which amounts to maxi - mizing the entropy of the outputs ) .
note that this approach is best classied as a ltering approach , and that inputs are mapped one - to - one onto a subset of all possible outputs , i . e .
the image of that mapping forms a lower dimensional manifold in output space ( see figure 123 ) .
shriki et al .
( 123 ) showed that this objective translates into maximizing the following expression for the entropy ,
h ( y ) = ( cid : 123 ) z dx p123 ( x ) log
where j ( x ) is the jacobian dened in equation ( 123 ) , and p123 ( x ) is the data distribution .
energy - based models
by interpreting ica as a ltering model of the inputs , we now describe a very different way of generalizing ica to overcomplete representations .
energy - based models ( ebm ) preserve the com -
sparse overcomplete energy - based models
putationally attractive property that the features u are simple deterministic functions of the inputs , instead of stochastic latent variables as in a causal generative model .
as a consequence , even in the overcomplete setting the posterior p ( ujx ) collapses to a point , which stands in sharp contrast to overcomplete causal models which dene a posterior distribution over the sources .
in fact , for overcomplete ebms , not all feature values are allowed , since not all values lie in the image of the mapping from x to u .
this is similar to the information maximization approach but very different from the causal generative approach where all source values are allowed .
let ui ( x;wi ) be the mapping from x to feature ui with parameters wi .
the features are used for
assigning an energy e ( x ) , to each possible observation vector x , as follows :
the probability of x is dened in terms of its energy through the boltzmann distribution123
where z denotes the normalization constant ( or partition function ) ,
standard ica with non - gaussian priors pi ( si ) is implemented by having the same number of sources as input dimensions and using
ui ( x;wi ) = wt
ei ( ui ) = ( cid : 123 ) log pi ( ui ) :
furthermore , in this special case of standard ica the normalization term in equation ( 123 ) is tractable and simplies to
where the rows of w are the lters wt
the above energy - based model suggests thinking about ica as a ltering model instead of a causal generative model .
that is , observations are linearly ltered rather than independent sources being linearly mixed .
hinton and teh ( 123 ) interpreted these lters as linear constraints , with the energies serving as costs for violating the constraints .
using energies corresponding to heavy tailed distributions with a sharp peak at zero means that the constraints should be frequently approxi - mately satised , but will not be strongly penalized if they are grossly violated .
in this new approach it is very natural to include more constraints than input dimensions .
note however , that the marginal independence among the sources which was a modelling assumption for overcomplete causal models , is no longer true for the features in the ebms in general .
instead , since the posterior p ( ujx ) reduces to a point , the features given the inputs are trivially independent :
p ( ujx ) = ( cid : 123 )
d ( ui ( cid : 123 ) ui ( x;wi ) ) ;
we note that the additive form of the energy leads to a product form for the probability distribution , which was called
a product of experts ( poe ) model in ( hinton , 123 ) .
teh et al .
where ui ( x;wi ) is the feature computed in equation ( 123 ) .
the semantics of such probabilistic mod - els is consistent with that of undirected graphical models as depicted in figure 123b .
the above means that inference in ebms is trivial .
on the other hand , sampling from the distribution p ( x ) is dif - cult and involves mcmc in general .
this is precisely opposite to causal generative models where inference is hard but sampling easy .
123 relating ebms to causal generative ica
we will now discuss how the proposed overcomplete ebms relate to the causal generative approach to ica .
intuitively , an ebm can be interpreted as the conditional distribution obtained from a larger square ica model when we observe a number of variables .
this relationship explains how is it that in ebms the features u are conditionally independent given x , but marginally dependent without
in the previous section we have already argued that when the number of input dimensions matches the number of features , an ebm is strictly equivalent to standard ica as described in in the following we will assume that there are more features than input dimensions ( i . e .
m > d ) .
consider an ica model where we have added m ( cid : 123 ) d auxiliary input dimensions z .
we will de - note the total input space by v = ( x;z ) .
we will also add additional lters from the new z variables to all features and denote them by f , i . e .
the total lter matrix is now g = ( w jf ) .
we will assume that the new lters are chosen such that g is invertible , i . e .
that the new enlarged space is fully spanned .
for this enlarged ica model we can again write the probability distribution as in equation ( 123 ) , here
i x + ft
i are the rows of f .
next , we write the probability density for the conditional distribution ,
r p ( x123;z ) dx123 ;
p ( xjz = 123 ) =
i x123 ) dx123 :
where the jdetgj terms have cancelled .
if we choose the auxiliary variables z = 123 then this can be
the above is of course just an ebm where the partition function is given by
z =z ( cid : 123 )
i x123 ) dx123 :
note that the above derivation is independent of the precise choice of the lters f as long as they span the extra dimensions .
in the previous subsection we have seen that an ebm may be interpreted as an undirected graphi - cal model with conditional independence of the features given the inputs .
from the above discussion we may conclude that we can also interpret the ebm as a conditional distribution p ( xjz = 123 ) on a directed graphical model , where m ( cid : 123 ) d auxiliary variables z have been clamped at 123 ( see figure 123c ) .
by clamping the extra nodes at 123 we introduce dependencies among the features through the
sparse overcomplete energy - based models
phenomenon of explaining away .
in other words , the features are marginally dependent when x is unobserved .
when x is observed , the whole input vector ( x;z ) is now observed so the posterior distribution over the features again collapse to a point , trivially implying conditional independence .
123 relating ebms to information maximization
in section 123 we saw that in the information maximization approach to overcomplete representa -
tions one maximizes the entropy of equation ( 123 ) .
the fact that the quantity pdet ( j ( x ) t j ( x ) ) in
that equation is not normalized in general , as opposed to the complete case , prevents expression ( 123 ) from being a negative kl divergence .
if we therefore dene the probability density
where z is the normalization constant , then minimizing the kl divergence kl ( p123jjp ) is equivalent to maximizing the log - likelihood of the model p ( x ) .
importantly , p ( x ) is consistent with the denition of an ebm if we choose as the energy
e ( x ) = ( cid : 123 ) log ( cid : 123 ) qdet ( j ( x ) t j ( x ) ) ( cid : 123 ) = ( cid : 123 )
tr ( cid : 123 ) log ( cid : 123 ) j ( x ) t j ( x ) ( cid : 123 ) ( cid : 123 ) :
the energy - based density model p ( x ) in equation ( 123 ) has a simple interpretation in terms of the mapping ( 123 ) .
this mapping is depicted in figure 123 where it is shown that the x - coordinates dene a parametrization of the manifold .
it is not hard to show that the distribution p ( x ) is transformed precisely to a uniform distribution p ( y ) = 123=z on the manifold in y - space , where the normalization constant z may thus be interpreted as the volume of this manifold .
minimizing the kl divergence kl ( p123jjp ) can therefore be interpreted as mapping the data to a manifold in a higher dimensional embedding space , in which the data are distributed as uniformly as possible .
the relation between information maximization and the above energy - based approach is summarized by the following
h ( y ) = ( cid : 123 ) kl ( p123 ( x ) jjp ( x ) ) + log ( manifold - volume ) :
the rst term describes the t of the model p ( x ) to data , while the second term is simply the entropy of the uniform distribution p ( y ) on the manifold .
relative to the energy - based approach , maximizing the mutual information will have a stronger preference to increase the volume of the manifold , since this is directly related to the entropy of p ( y ) .
note that in the square case the manifold is exactly the whole image space ( 123;123 ) m , hence its volume is always xed at 123 , and equa - tion ( 123 ) reduces exactly to the kl divergence kl ( p123 ( x ) kp ( x ) ) .
in the overcomplete case , experiments will have to decide which approach is preferrable and
under what circumstances .
parameter estimation for energy - based models
in section 123 we proposed energy - based models as probabilistic models for overcomplete represen - tations .
we did however not discuss how to t the free parameters of such models ( e . g .
lters wt efciently to data .
in this section we will address that issue .
first , we describe the usual maximum likelihood method of training such models .
for overcom - plete models , we show that maximum likelihood is not a practical solution , because of the non - trivial
teh et al .
partition function .
in light of this , we propose another estimation method for energy - based models called contrastive divergence ( hinton , 123 ) .
this is a biased method , but we will show that the bias is acceptably small compared with the gain in efciency in training overcomplete models , and the ease with which we can generalize the method to new and more intricate models .
let p123 ( x ) be the distribution of the observed data , and p
( x ) = p ( x ) be the model distribution given in equation ( 123 ) ( the notation will become apparent later in the section ) .
we would like p to approximate p123 as well as possible .
the standard measure of the difference between p123 and p is the kullback - leibler ( kl ) divergence :
) =z p123 ( x ) log
because p123 is xed , minimizing the kl divergence is equivalent to maximizing the log likelihood of the data under the model p .
for energy - based models given by equation ( 123 ) , the derivative of the kl divergence with respect to a weight wi j is
wi j ( cid : 123 ) p123
wi j ( cid : 123 ) p
where h ( cid : 123 ) iq is the expectation operator under distribution q .
learning can now proceed by using the derivative in equation ( 123 ) for gradient descent in the kl divergence between the data distribution and the model distribution ,
d wi j ( cid : 123 ) ( cid : 123 )
the above update rule can be understood as lowering the energy surface at locations where there are data ( rst term in equation ( 123 ) ) and at the same time raising the energy surface at locations where there are no data but the model predicts high probability ( second term in equation ( 123 ) ) .
this will eventually result in an energy surface with low energy ( high probability ) in regions where there are data present and high energy ( low probability ) everywhere else .
the second term on the rhs of equation ( 123 ) is obtained by taking the derivative of the log partition function ( equation ( 123 ) ) with respect to wi j .
in the square ica case , the log partition func - tion is exactly given by log jdetw ( cid : 123 ) 123j , hence the second term evaluates to the i jth entry of the matrix w ( cid : 123 ) t .
however if the model is overcomplete , there is no analytic form for the partition function so exact computation is generally intractable .
instead , since the second term is an expectation under the model distribution p , one possibility is to use markov chain monte carlo ( mcmc ) techniques to approximate the average using samples from p ( see neal , 123 ) .
this method inherits both the advantages and drawbacks associated with mcmc sampling .
the obtained estimate is consistent ( i . e .
the bias decreases to zero as the length of the chains is increased ) , and it is very easily adapt - able to other more complex models .
the main drawback is that the method is very expensive the markov chain has to be run for many steps before it approaches the equilibrium distribution p and it is hard to estimate how many steps are required .
also , the variance of the mcmc estimator is usually high .
to reduce the variance many independent samples are needed , incurring additional computational costs .
therefore , estimating the derivative ( 123 ) accurately by mcmc sampling is slow and can be unreliable due to high variance .
however , in the following we will argue that it is unnecessary to estimate the derivatives aver - aged over the equilibrium distribution in order to train an energy - based model from data .
instead , we
sparse overcomplete energy - based models
will average the derivatives over a different distribution , resulting from truncating the markov chain after a xed number of steps .
this idea , called contrastive divergence learning , was rst proposed by hinton ( 123 ) to improve both computational efciency and reduce the variance at the expense of introducing a bias for the estimates of the parameters with respect to the maximum likelihood
there are two ideas involved in contrastive divergence learning .
the rst one is to start the markov chain at the data distribution p123 rather than to initialize the markov chain at some vague distribution ( e . g .
a gaussian with large variances ) .
the reason usually given for using vague initial distributions is that every mode of the equilibrium distribution has a chance of being visited by some chain .
this can help to overcome a problematic feature of many markov chains a low mixing rate; once a chain enters a mode of the distribution it is hard to escape to a different mode .
however , we argue that starting at the data distribution is preferable since the training data already contains examples from the various modes that the model distribution ought to have .
towards the end of learning , when the modes of the model distribution roughly correspond to the modes in the data distribution , the number of samples in each mode approximately matches the number of data vectors in each mode .
this further reduces the variance of the derivative estimates .
a possible danger with this technique is that certain spurious ( empty ) modes which are accidentally created during learning may go unnoticed .
the second idea of contrastive divergence is to run the markov chain for only a few iterations rather than until equilibrium .
because the chains are started at the data distribution , even after only a few iterations , any consistent tendency to move away from the data distribution provides valuable information that can be used to adapt the parameters of the model .
intuitively , the parameters of the model should be updated so that the markov chain does not tend to move away from the data distribution ( since we want the markov chain to equilibrate to the data distribution ) .
combining the two ideas described above and dening pn ( x ) to be the distribution of the random variable at the nth iteration of the markov chain , 123 the contrastive divergence learning algorithm is implemented by using the following quantity to update the lters wi j :
d wi j ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) e ( x ) wi j ( cid : 123 ) p123
wi j ( cid : 123 ) pn
relative to maximum likelihood learning ( equations ( 123 ) and ( 123 ) ) we have replaced the equilib - rium distribution p with pn , and the markov chain is initialized at the data distribution p123
the gure below gives pseudo - code for contrastive divergence learning .
notice that in order to compute the average in the second term of equation ( 123 ) we used samples produced by markov chains initialized at the corresponding data vectors used in the rst term .
this , rather than uniformly sampling the initial states of the markov chains from the data vectors , further reduces the variance .
if in addition to the lter weights wi j additional parameters are present , for instance to model the shape of the energies ei , similar update rules as equation ( 123 ) can be used to t them to data .
for standard ica , this would correspond to learning the shape of the prior densities .
this explains the notation p123 for the initial distribution of the markov chain and p
for the limit distribution of pn as
teh et al .
contrastive divergence learning for energy - based models
compute the gradient of the total energy with respect to the parameters and average over the
run mcmc samplers for n steps , starting at every data vector dk , keeping only the last sample
data cases dk .
sk of each chain .
compute the gradient of the total energy with respect to the parameters and average over the
update the parameters using
d wi j = ( cid : 123 )
wi j ! ;
is the learning rate and n the number of samples in each mini - batch .
in the ideal situation that the model distribution p
is exible enough to perfectly model the data distribution123 p123 , and we use a markov chain that properly mixes , then contrastive divergence learning has a xed point at the maximum likelihood solution , i . e .
when p = p123
this is not hard to see , since at the maximum likelihood solution , the markov chain will not change the model dis - tribution , which implies that the derivatives in equation ( 123 ) precisely cancel .
in general however , we expect contrastive divergence learning to trade - off variance with bias ( see also williams and agakov , 123 ) .
apart from this , it may also happen that for certain markov chains spurious xed points exist in contrastive divergence learning ( for some examples see mackay , 123 ) .
although we have argued that contrastive divergence learning seems a sensible way to t energy - based models to data , we have not shown that it corresponds to gradient descent on a cost function , which is desirable to prove convergence .
now we will show a slightly weaker statement , namely that the update ( 123 ) corresponds to an approximate gradient descent step on a cost function .
dene the contrastive divergence cost function ( hinton , 123 ) as
cd = kl ( p123kp
) ( cid : 123 ) kl ( pnkp
note that this consists of the usual kl divergence between the data distribution and the model distribution , subtracted by the kl divergence between the n - step distribution pn and the model distribution .
using properties of markov chains one can show that the n - step distribution is always closer to the equilibrium model distribution , so that cd is always non - negative , with cd = 123 exactly when p123 = p
taking derivatives of the contrastive divergence cost function with respect to the lter weights
wi j we nd the following gradient :
wi j ( cid : 123 ) p123
wi j ( cid : 123 ) pn
in the case of nite data , we replace the data distribution by the empirical distribution , which is a mixture of delta - functions .
in this case , any smooth model distribution will not be able to perfectly t the empirical data distribution and the above argument fails .
in fact , we may expect to incur a certain bias with respect to the maximum likelihood
sparse overcomplete energy - based models
the rst two terms in equation ( 123 ) are identical to the ones proposed for the learning algorithm in equation ( 123 ) .
the last term represents the effect that changes in wi j have on the contrastive divergence via the effect on pn , i . e the effect on the markov chain itself when the parameters wi j are altered .
this term is hard to compute but fortunately it is typically very small and simulations by hinton ( 123 ) suggest that it can be safely ignored .
the results later in this paper further support
experiment : blind source separation
to assess the performance of contrastive divergence as a learning algorithm , we compared a hybrid monte carlo implementation of contrastive divergence with an exact sampling algorithm as well as the bell and sejnowski ( 123 ) algorithm on a standard blind source separation problem .
the model has the same number of input and source dimensions , 123 and the energy of the model is dened
ei ( si ) = ( cid : 123 ) log ( s ( si ) ( 123 ( cid : 123 ) s ( si ) ) ) ;
where s ( s ) = 123= ( 123 + exp ( ( cid : 123 ) s ) ) is the sigmoid function .
this model is strictly equivalent to the noiseless ica model with sigmoidal outputs used by bell and sejnowski ( 123 ) .
the data consisted of 123 , 123 - second stereo cd recordings of music , sampled at 123 : 123 khz . 123 each recording was monoized , down - sampled by a factor of 123 , randomly permuted over the time - index and rescaled to unit variance .
the resulting 123 samples in 123 channels were linearly mixed using the standard instamix routine with b = 123 : 123 ( 123 on the diagonal and 123=123 off the diagonal ) , 123 and whitened before presentation to the various learning algorithms .
we compared three different ways of computing or estimating the gradient ( 123 ) :
algorithm hmc : we used a hybrid monte carlo implementation of contrastive divergence .
this implementation uses 123 step of hybrid monte carlo simulation to sample from p123 ( x ) , which in turn consists of 123 leap frog steps , with the step sizes adapted at the end of each simulation so that the acceptance rate is about 123% .
see neal ( 123 ) for further detail on hybrid monte
algorithm equil : for noiseless ica , it is possible to sample efciently from the true equilibrium distribution using the causal generative view .
these samples can then be used to estimate the second term of ( 123 ) .
to be fair , we used a number of samples equal to the number of data vectors in each mini - batch .
algorithm exact : we can also compute the partition function using equation ( 123 ) and evaluate the
second term of equation ( 123 ) exactly .
this is precisely bell and sejnowskis algorithm .
parameter updates were performed on mini - batches of 123 data vectors .
the learning rate was annealed from 123 : 123 down to 123 : 123 in 123 iterations of learning , 123 while a momentum factor of
note however that recovering more sound sources than input dimensions ( sensors ) is not possible with our energy -
based model , since the feautures are not marginally independent .
prepared by barak pearlmutter .
these data are available at http : / / sound . media . mit . edu / ica - bench / .
this consisted of 123 iterations each at 123 : 123 , 123 : 123 , 123 : 123 , 123 : 123 and 123 : 123
teh et al .
figure 123 : evolution of the amari distance for the various algorithms , averaged over 123 runs .
note that hmc converged just as fast as the exact sampling algorithm equil , while the exact algorithm exact is only slightly faster .
the sudden changes in amari distance are due to the annealing schedule .
comparison of final amari distances
figure 123 : final amari distances for the various algorithms , averaged over 123 runs .
the boxes have lines at the lower quartile , median , and upper quartile values .
the whiskers show the extent of the rest of the data .
outliers are denoted by + .
this plot shows that the deterministic method exact performs slightly better than the sampling methods hmc and equil , probably due to the variance induced by the sampling .
more importantly , it shows that learning with brief sampling ( hmc ) performs equally well as learning with samples from the equilibrium distribution ( equil ) .
123 : 123 was used to speed up convergence .
the initial weights were sampled from a gaussian with a standard deviation of 123 : 123
sparse overcomplete energy - based models
during learning we monitored the amari distance123 to the true unmixing matrix .
in figures 123 and 123 we show the results of the various algorithms on the sound separation task .
the main conclusion of this experiment is that we do not need to sample from the equilibrium distribution in order to learn the lters w .
this validates the ideas behind cd learning .
experiments : feature extraction
we present examples of the features delivered by our algorithm on several standard datasets .
firstly we demonstrate performance on typical ica tasks of determining an overcomplete set of features of speech and natural images .
then , we show the algorithm applied to the cedar cdrom dataset of handwritten digits and lastly , we present the feature vectors learned when the algorithm is applied to the feret database of human faces .
for all the experiments described in this section we use an energy function of the form
ei ( ui ( x;wi ) ) = g
i log ( cid : 123 ) 123 + ( wt
i x ) 123 ( cid : 123 ) ;
which corresponds to modelling the data with a product of one - dimensional student - t distributions of degree ( 123g ( cid : 123 ) 123 ) ( hinton and teh , 123 ) .
this energy function was chosen for its simplicity yet versatility in describing super - gaussian distributions .
however , the algorithmic formulation allows the use of arbitrary energy functions and results may be improved by a more systematic tailoring of the energy function to particular datasets .
to test whether the model could extract meaningful lters from speech data we used recordings of 123 male speakers from the timit database , uttering the sentence
dont ask me to carry an oily rag like that .
the sentences were down - sampled to 123khz , and 123;123 123 : 123ms segments ( each segment corre - sponding to 123 samples ) were extracted from random locations .
before presentation to the learning algorithm the data was centred and sphered .
the features were trained using contrastive divergence with one step of hybrid monte carlo sampling consisting of 123 leap frog steps .
mini - batches of size 123 were used , while the learning rate was annealed from 123 : 123 to 123 : 123 over 123 iterations .
the lters were initialized at small random values and momentum was used to speed up convergence .
in figure 123 we show 123 of the 123 features in the whitened domain together with their power spectra .
recall that since there are 123 times more lters extracted as dimensions in the input space , the energy - based model is no longer equivalent to a causal ica model .
figure 123 shows the distribution of power over time and frequency .
there seems to be interesting structure around 123 : 123khz , where the lters are less localized and more nely tuned in frequency than average .
this phenomenon is also reported by abdallah and plumbley ( 123 ) .
teh et al .
figure 123 : ( a ) filters found by the 123 ( cid : 123 ) overcomplete ebm .
the 123 lters in the rst row are the ones with largest power , indicating that they represent important features .
the 123 lters in the second row are randomly drawn from the remaining 123 lters .
( b ) corresponding
figure 123 : distribution of power over time and frequency .
first the envelope of each lter ( the absolute value of its hilbert transform ) was computed and squared .
next , the squared envelope and the power spectrum were thresholded by mapping all values greater than half the peak value to one and the rest to zero .
gaps smaller than 123 samples in time and 123 samples in frequency were lled in .
finally , the outer product of the two templates were computed , weighted by the total power of the lter , and added to the diagram .
123 natural image patches
we tested our algorithm on the standard ica task of determining the independent components of natural images .
the data set used is the imlog123 data set of van hateren and van der schaaf
the amari distance ( amari et al . , 123 ) measures a distance between two matrices a and b up to permutations and
scalings : ( cid : 123 ) ( cid : 123 ) n
i=123 ( cid : 123 ) n
this data set is available at ftp : / / hlab . phys . rug . nl / pub / samples / imlog .
maxk j ( ab ( cid : 123 ) 123 ) ikj + j ( ab ( cid : 123 ) 123 ) i jj
maxk j ( ab ( cid : 123 ) 123 ) k jj ( cid : 123 ) ( cid : 123 ) 123n123
sparse overcomplete energy - based models
figure 123 : learned lters for natural images .
is were initialized at 123
both wi and g
( 123 ) .
the logarithm of the pixel intensities was rst taken and then the image patches were centred and whitened .
there were 123 patches and each patch was 123 ( cid : 123 ) 123 in size .
we trained a network with 123 ( cid : 123 ) 123 = 123 features , using contrastive divergence with 123 step of hybrid monte carlo sampling consisting of 123 leap frog steps .
the step size was adaptive so that the acceptance rate is approximately 123% .
both wi and g i are unconstrained , but a small weight decay of 123 ( cid : 123 ) 123 was used for wi to encourage the features to localize .
the wis were initialized to random vectors of length 123 , while the g i were trained with a learning rate of 123 and momentum factor of 123 .
we found however that the result is not sensitive to the settings of these parameters .
a random sample of 123 learned features in the whitened domain is shown in figure 123
they were roughly ordered by increasing spatial frequency .
by hand , we counted a total of 123 features which have not localized either in the spatial or frequency domain .
most of the other features can be described well with gabor functions .
to further analyze the set of learned lters , we tted a gabor function of the form used by lewicki and olshausen ( 123 ) to each feature and extracted parameters like frequency , location and extent in the spatial and frequency domains .
these are summarized in figures 123 and 123 , and show that the lters form a nice tiling of both the spatial and frequency domains .
we see from figures 123 and 123 that lters are learned at multiple scales , with larger features typically being of lower frequency .
however we also see an over emphasis of horizontal and vertical lters .
this effect has been observed in previous papers ( van hateren and van der schaaf , 123 , lewicki and olshausen , 123 ) , and is probably due to pixellation .
teh et al .
figure 123 : the spatial layout and size of the lters , which are described by the position and size of
figure 123 : a polar plot of frequency tuning and orientation selectivity of the learned lters , with the centre of each cross at the peak frequency and orientation response , and crosshairs describing the 123=123 - bandwidth .
sparse overcomplete energy - based models
figure 123 : learned lters for cedar digits .
filters are plotted in whitened space for clarity .
123 cedar digits
we used 123x123 real valued digits from the br set on the cedar cdrom #123
there are 123 digits available , divided equally into 123 classes .
the mean image from the entire dataset was subtracted from each datum , and the digits were whitened with zca .
a network with 123 features was trained in the same manner as for natural image patches .
a random subset of learned lters is shown in figure 123
to make it easier to discern the structure of the learned lters , we present them in the zca whitened domain rather than in pixel space .
we note the supercial similarity between these lters and those found from the natural scene experiments .
however , in addition to straight edge lters we also see several curved lters .
we interpret the results as a set of stroke detectors , modelling a space of strokes that gives rise to the full digit set .
123 feret faces we used the full nist feret database of frontal face images . 123 the data was rst pre - processed in the standard manner by aligning the faces , normalising the pixel intensities and cropping a central oval shaped region . 123 then as an additional preprocessing step we centred the data and performed pca whitening , retaining the projections onto the leading 123 eigenvectors as the input dimensions
teh et al .
figure 123 : ( a ) 123 eigenfaces with largest eigenvalue plotted rowwise in descending eigenvalue ( b ) subset of 123 type ii feature vectors .
the top row are hand picked , the bottom row randomly selected .
( c ) subset of 123 type i feature vectors , all randomly
to the algorithm .
we trained a network with 123 features using contrastive divergence with hybrid monte carlo sampling ( 123 sets of 123 leap frog steps ) .
both the wis and g is were unconstrained .
the wis were initialized uniformly over vectors of norm 123
the g is were initialized at 123
a learning rate of 123 was used for the wi , whilst a learning rate of 123 was used for the g
figure 123a shows the 123 leading eigenvectors plotted as face images , and figure 123b shows a subset of 123 lters as learned by our algorithm .
in bartlett et al .
( 123 ) two kinds of square ica were applied to a lower resolution version of the feret database .
in relation to their work , the lters shown in figure 123b correspond to type ii ica ( each face constitutes an input line ) rather than type i ica ( the value of a pixel across all faces constitutes an input line . ) there seem to be both similarities and notable differences between our features and their type ii results .
as with bartlett et al .
( 123 ) , many of the lters that we learn are somewhat global in the sense that most pixels have a nonzero weight .
however , in addition to these global features and in contradistinction to their type ii results , we also develop features with most of their weight concentrated in localised sub - regions for instance focusing on glasses , eyes , smiling mouths , moustaches , etc .
furthermore , as well as global features that can perhaps be described as archetypical faces we also see global features which appear to mainly capture structure in the illumination of a face .
lastly , figure 123c illustrates the results when our algorithm is applied in a type i manner ( using the pixels of the leading 123 principal components as inputs rather than the original pixel values ) .
this approach leads to features that are all highly localised in space .
our results are again qualitatively similar to those described by bartlett et al .
( 123 ) .
sparse overcomplete energy - based models
future work will show whether the overcomplete feature sets that our algorithm delivers can be
usefully employed in a face or expression recognition system .
in this paper we have re - interpreted the standard ica algorithm as an energy - based model and studied its extension to overcomplete representations .
we have shown that parameters of an ebm , such as the lter weights and those parametrizing the energy function , can be efciently estimated using contrastive divergence learning .
through a number of experiments on standard data sets we have shown that ebms can efciently extract useful features in high dimensions .
contrary to causal generative models for overcomplete ica , the features of an ebm exhibit marginal dependencies .
the advantage of allowing these dependencies in the model is fast infer - in causal generative models , the assumption of marginal independence often leads to in - tractible inference which needs to be approximated using some iterative , data dependent scheme .
the role of these iterations can be understood as suppressing the activity of less relevant features , thus producing a sparse code .
therefore , for causal generative models , overcomplete representa - tions are expected to produce very compact ( or sparse ) codes , a fact which is often emphasized as desirable ( olshausen and field , 123 ) .
perhaps surprisingly , we have shown that such a slow iterative process is not required for producing sparse and overcomplete representations .
however the above does suggest enriching ebms with inhibitory lateral connections to achieve the goal of further suppressing less relevant features in order to produce an even sparser represen - tation .
preliminary experiments using a mean eld approach to implement these lateral inhibitions have been successful in learning good density models , but are slow due to the iterative optimization for every data case .
another powerful generalization of ebms is a hierarchical non - linear architecture in which the output activities are computed with a feed - forward neural network ( figure 123 ) and each layer may contribute to the total energy ( for related work see also hyvrinen and hoyer , 123 ) .
to t this model to data , backpropagation is used to compute gradients of the energy with respect to both the data vector ( to be used in hybrid monte carlo sampling ) , and the weights ( to be used for weight updates ) .
since this algorithm applies backpropagation in an unsupervised setting and combines it with contrastive divergence learning we have named it contrastive backpropagation ( hinton et al . ,
indeed , the contrastive backpropagation learning procedure is quite exible .
it puts no con - straints other than smoothness on the activation functions or the energy functions . 123 the procedure can be easily modied to use recurrent neural networks that contain directed cycles by running each forward pass for some predetermined number of steps and dening the energy to be any smooth function of the time history of the activations .
backpropagation through time ( rumelhart et al . , 123 , werbos , 123 ) can then be used to obtain the required derivatives .
the data - vector can also change during the forward pass through a recurrent network .
this makes it possible to model se - quential data , such as video sequences , by running the network forward in time for a whole sequence and then running it backward in time to compute the derivatives required for hybrid monte carlo sampling and for updating the weights .
in welling et al .
( 123 ) , a two - layer model was studied where the second layer performed a local averaging of the non - linearly transformed activities of the rst layer .
this resulted in a topographic
there is also the necessary assumption that the energy is dened such that the boltzmann distribution is normalizable .
teh et al .
figure 123 : architecture of a hierarchical non - linear energy - based model .
non - linearities are in - dicated by sigmoidal units in output layer 123
energies can be contributed by output variables in both layers , and the number of output variables need not correspond to the number of input variables .
ordering of the lters , where orientation , location and frequency are changing smoothly from one lter to the next .
the energy - based approach to ica we have presented stems from previous work on products of experts ( poes ) ( hinton , 123 ) .
in fact our model is a type of poe in which each energy term corresponds to one expert .
there is also an interesting link between ebms and maximum entropy models ( della pietra et al . , 123 , zhu et al . , 123 ) .
indeed , the probability distribution of a maximum entropy model is also dened as a boltzmann distribution equation ( 123 ) over a sum of energy contributions ei ( x ) which are written as ,
ei ( x ) = l
where ui ( x ) are xed features of the model and where the weights l i are the parameters of the model to be t to data .
in this sense , the proposed ebm can be interpreted as a maximum entropy model with exible learned features and a different energy function .
in conclusion we believe that the ebm provides a exible modelling tool which can be trained
efciently to uncover useful structure in data .
we would like to thank peter dayan , sam roweis , zoubin ghahramani and maneesh sahani for helpful discussions , carl rasmussen for making minimize . m available , and the reviewers and dave mackay for helpful comments .
