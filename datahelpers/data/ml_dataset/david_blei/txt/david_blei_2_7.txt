Online Variational Inference for the Hierarchical Dirichlet Process

Chong Wang
David M. Blei
Computer Science Department, Princeton University

{chongw,jpaisley,blei}@cs.princeton.edu

John Paisley

Abstract

The hierarchical Dirichlet process (HDP) is a
Bayesian nonparametric model that can be used
to model mixed-membership data with a poten-
tially inﬁnite number of components. It has been
applied widely in probabilistic topic modeling,
where the data are documents and the compo-
nents are distributions of terms that reﬂect recur-
ring patterns (or “topics”) in the collection. Given
a document collection, posterior inference is used
to determine the number of topics needed and to
characterize their distributions. One limitation
of HDP analysis is that existing posterior infer-
ence algorithms require multiple passes through
all the data—these algorithms are intractable for
very large scale applications. We propose an on-
line variational inference algorithm for the HDP,
an algorithm that is easily applicable to massive
and streaming data. Our algorithm is signiﬁcantly
faster than traditional inference algorithms for the
HDP, and lets us analyze much larger data sets.
We illustrate the approach on two large collections
of text, showing improved performance over on-
line LDA, the ﬁnite counterpart to the HDP topic
model.

1

INTRODUCTION

The hierarchical Dirichlet process (HDP) [1] is a powerful
mixed-membership model for the unsupervised analysis of
grouped data. Applied to document collections, the HDP
provides a nonparametric topic model where documents are
viewed as groups of observed words, mixture components
(called topics) are distributions over terms, and each docu-
ment exhibits the topics with different proportions. Given
a collection of documents, the HDP topic model ﬁnds a
low-dimensional latent structure that can be used for tasks

like classiﬁcation, exploration, and summarization. Unlike
its ﬁnite counterpart, latent Dirichlet allocation [2], the HDP
topic model infers the number of topics from the data.
Posterior inference for the HDP is intractable, and much
research is dedicated to developing approximate inference
algorithms [1, 3, 4]. These methods are limited for massive
scale applications, however, because they require multiple
passes through the data and are not easily applicable to
streaming data.1 In this paper, we develop a new approx-
imate inference algorithm for the HDP. Our algorithm is
designed to analyze much larger data sets than the existing
state-of-the-art allows and, further, can be used to analyze
streams of data. This is particularly apt to the HDP topic
model. Topic models promise to help summarize and orga-
nize large archives of texts that cannot be easily analyzed
by hand and, further, could be better exploited if available
on streams of texts such as web APIs or news feeds.
Our method—online variational Bayes for the HDP— was
inspired by the recent online variational Bayes algorithm
for LDA [7]. Online LDA allows LDA models to be ﬁt to
massive and streaming data, and enjoys signiﬁcant improve-
ments in computation time without sacriﬁcing model quality.
Our motivation for extending this algorithm to the HDP is
that LDA requires choosing the number of topics in advance.
In a traditional setting, where ﬁtting multiple models might
be viable, the number of topics can be determined with cross
validation or held-out likelihood. However, these techniques
become impractical when the data set size is large, and they
become impossible when the data are streaming. Online
HDP provides the speed of online variational Bayes with
the modeling ﬂexibility of the HDP.
The idea behind online variational Bayes in general is to opti-
mize the variational objective function of [8] with stochastic
optimization [9]. Optimization proceeds by iteratively tak-
ing a random subset of the data, and updating the variational
parameters with respect to the subset. Online variational
Bayes is particularly efﬁcient when using the natural gradi-
ent [10] on models in which traditional variational Bayes

Appearing in Proceedings of the 14th International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS) 2011, Fort Laud-
erdale, FL, USA. Volume 15 of JMLR: W&CP 15. Copyright 2011
by the authors.

1One exception that may come to mind is the particle ﬁlter [5,
6]. However, this algorithm still requires periodically resampling
a variable for every data point. Data cannot be thrown away as in
a true streaming algorithm.

     752Online Variational Inference for the Hierarchical Dirichlet Process

can be performed by simple coordinate ascent [11]. (This
is the property that allowed [7] to derive an efﬁcient online
variational Bayes algorithm for LDA.) In this setting, on-
line variational Bayes is signiﬁcantly faster than traditional
variational Bayes [12], which must make multiple passes
through the data.
The challenge we face is that the existing coordinate as-
cent variational Bayes algorithms for the HDP require com-
plicated approximation methods or numerical optimiza-
tion [3, 4, 13]. We will begin by reviewing Sethuraman’s
stick-breaking construction of the HDP [14]. We show that
this construction allows for coordinate-ascent variational
Bayes without numerical approximation, which is a new
and simpler variational inference algorithm for the HDP. We
will then use this approach in an online variational Bayes
algorithm, allowing the HDP to be applied to massive and
streaming data. Finally, on two large archives of scientiﬁc
articles, we will show that the online HDP topic model pro-
vides a signiﬁcantly better ﬁt than online LDA. Online vari-
ational Bayes lets us apply Bayesian nonparametric models
at much larger scales.

2 A STICK BREAKING CONSTRUCTION

OF THE HDP

We describe the stick-breaking construction of the HDP [14]
using the Sethuraman’s construction for the DP [15]. This is
amenable to simple coordinate-ascent variational inference,
and we will use it to develop online variational inference for
the HDP.
A two-level hierarchical Dirichlet process (HDP) [1] (the
focus of this paper) is a collection of Dirichlet processes
(DP) [16] that share a base distribution G0, which is also
drawn from a DP. Mathematically,

G0 ∼ DP(γH)
Gj ∼ DP(α0G0), for each j,

(1)
(2)

where j is an index for each group of data. A notable feature
of the HDP is that all DPs Gj share the same set of atoms
and only the atom weights differ. This is a result of the
almost sure discreteness of the top-level DP.
In the HDP topic model—which is the focus of this paper—
we model groups of words organized into documents. The
variable wjn is the nth word in the jth document; the base
distribution H is a symmetric Dirichlet over the vocabulary
simplex; and the atoms of G0, which are independent draws
from H, are called topics.
The HDP topic model contains two additional steps to gener-
ate the data. First we generate the topic associated with the
nth word in the jth document; then we generate the word
from that topic,

θjn ∼ Gj, wjn ∼ Mult(θjn).

(3)

The discreteness of the corpus-level draw G0 ensures that
all documents share the same set of topics. The document-
level draw Gj inherits the topics from G0, but weights them
according to document-speciﬁc topic proportions.

Teh’s Stick-breaking Construction. The deﬁnition of
the HDP in Eq. 1 is implicit. [1] propose a more construc-
tive representation of the HDP using two stick-breaking
representations of a Dirichlet distribution [15]. For the
corpus-level DP draw, this representation is

(cid:81)k−1
k ∼ Beta(1, γ),
β(cid:48)
l=1 (1 − β(cid:48)
βk = β(cid:48)
G0 =(cid:80)∞
l),
φk ∼ H,

k=1 βkδφk .

k

(4)

jk ∼ Beta
π(cid:48)
πjk = π(cid:48)

Gj =(cid:80)∞

k=1 with weights β = (βk)∞

Thus, G0 is discrete and has support at the atoms φ =
(φk)∞
k=1. The distribution for β
is also written as β ∼ GEM(γ) [17].
The construction of each document-level Gj is

(cid:16)
(cid:16)
1 −(cid:80)k
(cid:81)k−1
α0βk, α0
l=1 (1 − π(cid:48)

jl),

(cid:17)(cid:17)

l=1 βl

,

(5)

jk
k=1 πjkδφk ,
k=1 are the same atoms as G0 in Eq. 4.

where φ = (φk)∞
This construction is difﬁcult to use in an online variational
inference algorithm. Online variational inference is partic-
ularly efﬁcient when the model is also amenable to coordi-
nate ascent variational inference, and where each update is
available in closed form. In the construction above, the stick-
breaking weights are tightly coupled between the bottom
and top-level DPs. As a consequence, it is not amendable to
closed form variational updates [3, 4].

Sethuraman’s Stick-breaking Construction. To ad-
dress this issue, we describe an alternative stick-breaking
construction for the HDP that allows for closed-form
coordinate-ascent variational inference due to its full conju-
gacy. (This construction was also brieﬂy described in [14].)
The construction is formed by twice applying Sethuraman’s
stick-breaking construction of the DP. We again construct
the corpus-level base distribution G0 as in Eq. 4. The differ-
ence is in the document-level draws. We use Sethuraman’s
construction for each Gj,
ψjt ∼ G0,
(cid:81)t−1
jt ∼ Beta(1, α0),
π(cid:48)
Gj =(cid:80)∞
l=1 (1 − π(cid:48)
πjt = π(cid:48)
jt
t=1 πjtδψjt,

jl),

(6)

Notice that each document-level atom (i.e., topic) ψjt maps
to a corpus-level atom φk in G0 according to the distribution

     753Chong Wang

John Paisley

David M. Blei

3 ONLINE VARIATIONAL INFERENCE

FOR THE HDP

With Sethuraman’s construction of the HDP in hand, we now
turn to our original aim—approximate posterior inference
in the HDP for massive and streaming data. Given a large
collection of documents, our goal is to approximate the
posterior distribution of its latent topic structure.
We will use online variational inference [11]. Traditional
variational inference approximates the posterior over the
hidden variables by positing a simpler distribution which is
optimized to be close in Kullback-Leibler (KL) divergence
to the true posterior [8]. This problem is (approximately)
solved by optimizing a function equal up to a constant to the
KL of interest. In online variational inference, we optimize
that function with stochastic approximation.
Online variational inference enjoys a close relationship with
coordinate-ascent variational inference. Consider a model
with latent variables and observations for which the poste-
rior is intractable to compute. One strategy for variational
inference is the mean-ﬁeld approach: posit a distribution
where each latent variable is independent and governed by
its own parameter, and optimize the variational parameters
with coordinate ascent.
Now, suppose that those coordinate ascent updates are avail-
able in closed form and consider updating them in parallel.
(Note this is no longer coordinate ascent.) It turns out that
the vector of parallel coordinate updates is exactly the nat-
ural gradient of the variational objective function under
conjugate priors [11]. This insight makes stochastic opti-
mization of the variational objective, based on a subset of
the data under analysis, a simple and efﬁcient alternative to
traditional coordinate-ascent.
Let us now return to the HDP topic model. We will ﬁrst
show that Sethuraman’s representation of the HDP above
allows for closed-form coordinate-ascent updates for vari-
ational inference. Then, we will derive the corresponding
online algorithm, which provides a scalable method for HDP
posterior inference.

3.1 A New Coordinate-ascent Variational Inference

When applied to Bayesian nonparametric models, vari-
ational methods are usually based on stick-breaking
representations—these representations provide a concrete
set of hidden variables on which to place an approximate
posterior [18, 19, 3]. Furthermore, the approximate pos-
terior is usually truncated. The user ﬁrst sets a truncation
on the number of topics to allow, and then relies on vari-
ational inference to infer a smaller number that are used
in the data. (Two exceptions are found in [20, 21], who
developed methods that allow the truncation to grow.) Note
that setting a truncation level is different from asserting a
number of components in a model. When set large, the

Figure 1: Illustration of the Sethuraman’s stick-breaking
construction of the two-level HDP. In the ﬁrst level, φk ∼ H
and β ∼ GEM(γ); in the second level, πj ∼ GEM(α0),
cjt ∼ Mult(β) and ψjt = φcjt.

deﬁned by G0. Further note there will be multiple document-
level atoms ψjt which map to the same corpus-level atom
φk, but we can verify that Gj contains all of the atoms in
G0 almost surely.
A second way to represent the document-level atoms ψj =
(ψjt)∞
t=1 is to introduce a series of indicator variables, cj =
(cjt)∞
t=1, which are drawn i.i.d.,

cjt ∼ Mult(β),

where β ∼ GEM(γ) (as mentioned above). Then let

ψjt = φcjt,

(7)

(8)

Thus, we do not need to explicitly represent the document
atoms ψj. This further simpliﬁes online inference.
The property that multiple document-level atoms ψjt can
map to the same corpus-level atom φk in this representa-
tion is similar in spirit to the Chinese restaurant franchise
(CRF) [1], where each restaurant can have multiple tables
serving the same dish φk. In the CRF representation, a
hierarchical Chinese restaurant process allocates dishes to
tables. Here, we use a series of random indicator variables
cj to represent this structure. Figure 1 illustrates the concept.

Given the representation in Eq. 6, the generative process for
the observed words in jth document, wjn, is as follows,

zjn ∼ Mult(πj),
θjn = ψjzjn = φcjzjn
wjn ∼ Mult(θjn).

,

(9)
(10)
(11)

The indicator zjn selects topic parameter ψjt, which maps
to one topic φk through the indicators cj. This also provides
the mapping from topic θjn to φk, which we need in Eq. 3.

     754Online Variational Inference for the Hierarchical Dirichlet Process

HDP assumptions encourage the approximate posterior to
use fewer components.
We use a fully factorized variational distribution and per-
form mean-ﬁeld variational inference. The hidden vari-
ables that we are interested in are the top-level stick propor-
tions β(cid:48) = (β(cid:48)
k)∞
k=1, bottom-level stick proportions π(cid:48)
j =
(π(cid:48)
jt)∞
t=1 for each
Gj. We also infer atom/topic distributions φ = (φk)∞
k=1,
topic index zjn for each word wjn. Thus our variational
distribution has the following form,

t=1 and the vector of indicators cj = (cjt)∞

q(β(cid:48), π(cid:48), c, z, φ) = q(β(cid:48))q(π(cid:48))q(c)q(z)q(φ).

(12)

This further factorizes into

q(c) =(cid:81)
q(z) =(cid:81)
q(φ) =(cid:81)

(cid:81)
(cid:81)
k q(φk|λk),

t q(cjt|ϕjt),
n q(zjn|ζjn),

j

j

where the variational parameters are ϕjt (multinomial), ζjn
(multinomial) and λk (Dirichlet). The factorized forms of
q(β(cid:48)) and q(π(cid:48)) are

q(β(cid:48)) =(cid:81)K−1
q(π(cid:48)) =(cid:81)
(cid:81)T−1
k=1 q(β(cid:48)
t=1 q(π(cid:48)

j

k|uk, vk),

jt|ajt, bjt),

(13)

K = 1) = 1 and q(π(cid:48)

where (uk, bk) and (ajt, bjt) are parameters of beta distri-
butions. We set the truncations for the corpus and document
levels to K and T . Here, T can be set much smaller than
K, because in practice each document Gj requires far fewer
topics than those needed for the entire corpus (i.e., the atoms
of G0). With this truncation, our variational distribution has
q(β(cid:48)
Using standard variational theory [8], we lower bound the
marginal log likelihood of the observed data D = (wj)D
using Jensen’s inequality,
log p(D|γ, α0, η) ≥ Eq [log p(D, β(cid:48), π(cid:48), c, z, φ)] + H(q)
j)p(π(cid:48)

(cid:2)log(cid:0)p(wj|cj, zj, φ)p(cj|β(cid:48))p(zj|π(cid:48)
j))(cid:9)

+ H(q(cj)) + H(q(zj)) + H(q(π(cid:48)

jT = 1) = 1, for all j.

=(cid:80)

(cid:8)Eq

j=1

j

+ Eq [log p(β(cid:48))p(φ)] + H(q(β(cid:48))) + H(q(φ))

= L(q),
(14)
where H(·) is the entropy term for the variational distribu-
tion. This is the variational objective function, which up to a
constant is equivalent to the KL to the true posterior. Taking
derivatives of this lower bound with respect to each vari-
ational parameter, we can derive the following coordinate
ascent updates.
Document-level Updates: At the document level we up-
date the parameters to the per-document stick, the parame-
ters to the per word topic indicators, and the parameters to

j|α0)(cid:1)(cid:3)

(cid:17)

.

n

n ζjnt,

s=t+1 ζjns,

ζjnt ∝ exp

the per document topic indices,

ajt = 1 +(cid:80)
bjt = α0 +(cid:80)
ϕjtk ∝ exp ((cid:80)
(cid:16)(cid:80)K

uk = 1 +(cid:80)
vk = γ +(cid:80)
λkw = η +(cid:80)

(cid:80)T
(15)
(16)
n ζjntEq [log p(wjn|φk)] + Eq [log βk]) ,
(17)
k=1 ϕjtkEq [log p(wjn|φk)] + Eq [log πjt]
(18)
Corpus-level Updates: At the corpus level, we update the
parameters to top-level sticks and the topics,

(cid:80)T
(cid:80)T
(cid:80)K
t=1 ϕjtk ((cid:80)
(cid:80)T
k] +(cid:80)k−1
(cid:3) +(cid:80)t−1
(cid:2)log π(cid:48)
(cid:3) = Ψ(ajt) − Ψ(ajt + bjt),
(cid:2)log π(cid:48)
(cid:2)log(1 − π(cid:48)
jt)(cid:3) = Ψ(bjt) − Ψ(ajt + bjt),
Eq [log p(wjn = w|φk)] = Ψ(λkw) − Ψ((cid:80)

The expectations involved above are taken under the varia-
tional distribution q, and are
Eq [log βk] = Eq [log β(cid:48)
Eq [log β(cid:48)
Eq [log(1 − β(cid:48)
Eq [log πjt] = Eq
Eq
Eq

(cid:2)log(1 − π(cid:48)
js)(cid:3) ,

k)] = Ψ(vk) − Ψ(uk + vk),

k] = Ψ(uk) − Ψ(uk + vk),

Eq [log(1 − β(cid:48)

n ζjntI[wjn = w]) ,

(19)

(20)

(21)

l=k+1 ϕjtl,

t=1 ϕjtk,

l)] ,

Eq

s=1

j

j

j

t=1

l=1

jt

jt

w λkw),

where Ψ(·) is the digamma function.
Unlike previous variational inference methods for the
HDP [3, 4], this method only contains simple closed-form
updates due to the full conjugacy of the stick-breaking con-
struction. (We note that, even in the batch setting, this is a
new posterior inference algorithm for the HDP.)

3.2 Online Variational Inference

We now develop online variational inference for an HDP
topic model.
In online variational inference, we apply
stochastic optimization to the variational objective. We
subsample the data (in this case, documents), compute an
approximation of the gradient based on the subsample, and
follow that gradient with a decreasing step-size. The key in-
sight behind efﬁcient online variational inference is that co-
ordinate ascent updates applied in parallel precisely form the
natural gradient of the variational objective function [11, 7].
Our approach is similar to that described in [7]. Let D be
the total number of documents in the corpus, and deﬁne the
variational lower bound for document j as
Lj = Eq

(cid:2)log(cid:0)p(wj|cj, zj, φ)p(cj|β(cid:48))p(zj|π(cid:48)

j|α0)(cid:1)(cid:3)

j)p(π(cid:48)

+ H(q(cj)) + H(q(zj)) + H(q(π(cid:48)
+ 1

D [Eq [log p(β(cid:48))p(φ)] + H(q(β(cid:48))) + H(q(φ))] .

j))

     755Chong Wang

John Paisley

David M. Blei

We have taken the corpus-wide terms and multiplied them
by 1/D. With this expression, we can see that the lower
bound L in Eq. 14 can be written as

L =(cid:80)

j Lj = Ej[DLj],

where the expectation is taken over the empirical distribution
of the data set. The expression DLj is the variational lower
bound evaluated with D duplicate copies of document j.
With the objective construed as an expectation over our
data, online HDP proceeds as follows. Given the exist-
ing corpus-level parameters, we ﬁrst sample a document
j and compute its optimal document-level variational pa-
rameters (aj, bj, ψj, ζj) by coordinate ascent (see Eq. 15
to 18.). Then, take the gradient of the corpus-level param-
eters (λ, u, v) of DLj, which is a noisy estimate of the
gradient of the expectation above. We follow that gradient
according to a decreasing learning rate, and repeat.
Natural Gradients. The gradient of the variational objec-
tive contains, as a component, the covariance matrix of the
variational distribution. This is a computational problem
in topic modeling because each set of topic parameters in-
volves a V ×V covariance matrix, where V is the size of the
vocabulary (e.g., 5,000). The natural gradient [10]—which
is the inverse of the Riemannian metric multiplied by the
gradient—has a simple form in the variational setting [11]
that allows for fast online inference.
Multiplying the gradient by the inverse of Riemannian met-
ric cancels the covariance matrix of the variational distri-
bution, leaving a natural gradient which is much easier to
work with. Speciﬁcally, the natural gradient is structurally
equivalent to the coordinate updates of Eq 19 to 21 taken
in parallel. (And, in stochastic optimization, we treat the
sampled document j as though it is the whole corpus.) Let
∂λ(j), ∂u(j) and ∂v(j) be the natural gradients for DLj.
Using the analysis in [11, 7], the components of the natural
gradients are

∂λkw(j) = −λkw + η + D(cid:80)T
∂uk(j) = −uk + 1 + D(cid:80)T
∂vk(j) = −vk + γ + D(cid:80)T

t=1 ϕjtk ((cid:80)
(cid:80)K

t=1 ϕjtk,

t=1

l=k+1 ϕjtl.

(22)
(23)
(24)

In online inference, an appropriate learning rate ρto is
needed to ensure the parameters to converge to a stationary
point [11, 7]. Then the updates of λ, u and v become

λ ← λ + ρto∂λ(j),
u ← u + ρto∂u(j)
v ← v + ρto∂v(j),
where the learning rate ρto should satisfy

(cid:80)∞
to=1 ρto = ∞, (cid:80)∞

to=1 ρ2
to

(25)
(26)
(27)

(28)

< ∞,

n ζjntI[wjn = w]) ,

1: Initialize λ = (λk)K

k=1, u = (uk)K−1

k=1 and v =

(vk)K−1

k=1 randomly. Set to = 1.

2: while Stopping criterion is not met do
3:
4:

Fetch a random document j from the corpus.
Compute aj, bj, ϕj and ζj using variational infer-
ence using document-level updates, Eq. 15 to 18.
Compute the natural gradients, ∂λ(j), ∂u(j) and
∂v(j) using Eq. 22 to 24.
Set ρto = (τ0 + to)−κ, to ← to + 1.
Update λ, u and v using Eq. 25 to 27.

5:

6:
7:
8: end while

Figure 2: Online variational inference for the HDP

which ensures convergence [9]. In our experiments, we use
ρto = (τ0 + to)−κ, where κ ∈ (0.5, 1] and τ0 > 0. Note
that the natural gradient is essential to the efﬁciency of the
algorithm. The online variational inference algorithm for
the HDP topic model is illustrated in Figure 2.
Mini-batches. To improve stability of the online learning
algorithm, practitioners typically use multiple samples to
compute gradients at a time—a small set of documents in
our case. Let S be a small set of documents and S = |S|
be its size. In this case, rather than computing the natural
j∈S Lj. The update

gradients using DLj, we use (D/S)(cid:80)

equations can then be similarly derived.

4 EXPERIMENTAL RESULTS

In this section, we evaluate the performance of online varia-
tional HDP compared with batch variational HDP and online
variational LDA.2

4.1 Data and Metric

Data Sets. Our experiments are based on two datasets:
• Nature: This dataset contains 352,549 documents, with
about 58 million tokens and a vocabulary size of 4,253.
These articles are from the years 1869 to 2008.

• PNAS: The Proceedings of the National Academy of
Sciences (PNAS) dataset contains 82,519 documents,
with about 46 million tokens and a vocabulary size of
6,500. These articles are from the years 1914 to 2004.

Standard stop words and those words that appear too fre-
quently or too rarely are removed.

Evaluation Metric. We use the following evaluation met-
ric to compare performance. For each dataset, we held out
2000 documents as a test set Dtest, with the remainder as
training data Dtrain. For testing, we split document wj
in Dtest into two parts, wj = (wj1, wj2), and compute
2http://www.cs.princeton.edu/˜blei/downloads/onlineldavb.tar

     756Online Variational Inference for the Hierarchical Dirichlet Process

the predictive likelihood of the second part wj2 (10% of
the words) conditioned on the ﬁrst part wj1 (90% of the
words) and on the training data. This is similar to the met-
rics used in [3, 22], which tries to avoid comparing different
hyperparameters. The metric is

likelihoodpw =

(cid:80)

j∈Dtest

(cid:80)

log p(wj2|wj1,Dtrain)
j∈Dtest

|wj2|

,

where |wj2| is the number of tokens in wj2 and “pw” means
“per-word.” Exact computation is intractable, and so we use
the following approximation. For all algorithms, let ¯φ be
the variational expectation of φ given Dtrain. For LDA,
let ¯πj be the variational expectation given wj2 and α be
its Dirichlet hyperparameter for topic proportions. The
predictive marginal probability of wj1 is approximated by

p(wj2|wj1,Dtrain) ≈(cid:81)

w∈wj2

k ¯πjk

¯φkw.

(cid:80)

To use this approximation for the HDP, we set the Dirichlet
¯β, where ¯β is the variational
hyperparameter to ¯α = α0
expectation of β, obtained from the variational expectation
of β(cid:48).

4.2 Results

Experimental Settings. For the HDP, we set γ =
α0 = 1, although using priors is also an option. We
set the top-level truncation K = 150 and the second
level truncation T = 15. Here T (cid:28) K, since docu-
ments usually don’t have many topics. For online vari-
ational LDA, we set its Dirichlet hyperparameter α =
(1/K, . . . , 1/K), where K is the number of topics; we set
K = {20, 40, 60, 80, 100, 150}.3 We set τ0 = 64 based
on the suggestions in [7], and vary κ = {0.6, 0.8, 1.0} and
the batch size S = {16, 64, 256, 1024, 2048}. We collected
experimental results during runs of 6 hours each.4

Nature Corpus.
In Figure 3, we plot the per-word log
likelihood as a function of computation time for online HDP,
online LDA, and batch HDP. (For the online algorithms,
we set κ = 0.6 and the batch size was S = 256.) This
ﬁgure shows that online HDP performs better than online
LDA. The HDP uses about 110 topics out of its potential
150. In contrast, online LDA uses almost all the topics and
exhibits overﬁtting at 150 topics. Note that batch HDP is
only trained on a subset of 20, 000 documents—otherwise
it is too slow—and its performance suffers.
In Figure 4, we plot the per-word likelihood after 6 hours of
computation, exploring the effect of batch size and values
of κ. We see that, overall, online HDP performs better
than online LDA. (This matches the reported results in [3],
which compares batch variational inference for the HDP and

3This is different from the top level truncation K in the HDP.
4The python package will be available at ﬁrst author’s home-

page.

Figure 3: Experimental results on Nature with κ = 0.6 and
batch size S = 256 (for the online algorithms). Points are
sub-sampled for better view. The label “oLDA-20” indicates
online LDA with 20 topics. (Not all numbers of topics are
shown; see Figure 4 for more details.) Online HDP performs
better than online LDA and batch HDP.

LDA.) Further, we found that small κ favors larger batch
sizes. (This matches the results seen for online LDA in [7].)
We also ran online HDP on the full Nature dataset using
only one pass (with κ = 0.6 and a batch size S = 1024) by
sequentially processing the articles from the year 1869 to
2008. Table 1 tracks the most probable ten words from two
topics as we encounter more articles in the collection. Note
that the HDP here is not a dynamic topic model [23, 24];
we show these results to demonstrate the online inference
process.
These results show that online inference for streaming data
ﬁnds different topics at different speeds, since the relevant
information for each topic does not come at the same time.
In this sequential setting, some topics are rarely used until
there are documents that can provide enough information to
update them (see the top topic in Table 1). Other topics are
updated throughout the stream because relevant documents
occur throughout the whole collection (see the bottom topic
in Table 1).

PNAS Corpus We ran the same experiments on the PNAS
corpus. Since PNAS is smaller than Nature, we were able
to run batch HDP on the whole data set. Figure 5 shows
the result with κ = 0.6 and batch size S = 2048. Online
HDP performs better than online LDA. Here batch HDP
performs a little better than online HDP, but online HDP is
much faster. Figure 6 plots the comparison between online
HDP and online LDA across different batch sizes and values
of κ.

5 DISCUSSION

We developed an online variational inference algorithm for
the hierarchical Dirichlet process topic model. Our algo-

     757naturetime (in seconds, log scale)per−word log likelihood−8.0−7.8−7.6−7.4−7.2−7.0lllllllllllllllllllllllllllllllllllll101.5102102.5103103.5104algorithmlllllloHDPbHDPoLDA−20oLDA−60oLDA−100oLDA−150Chong Wang

John Paisley

David M. Blei

Figure 4: Comparisons of online LDA and online HDP on the Nature corpus under various settings of batch size S and
parameter κ (kappa), run for 6 hours each. (Some lines for online HDP and points for online LDA do not appear due to
ﬁgure limits.) The best result among all is achieved by online HDP.

Figure 6: Comparisons of online LDA and online oHDP on the PNAS corpus under various settings of batch size S and
parameter κ (kappa), run for 6 hours each. (Some lines for online HDP and points for online LDA do not appear due to
ﬁgure limits.) The best result among all is achieved by online HDP.

     758naturetopiclikelihood−7.3−7.2−7.1−7.0−6.9−6.8−7.3−7.2−7.1−7.0−6.9−6.8−7.3−7.2−7.1−7.0−6.9−6.8S=16lllllllllllllll20406080100120140S=64lllllllllllllllll20406080100120140S=256llllllllllllllllll20406080100120140S=1024llllllllllllllllll20406080100120140S=2048llllllllllllllllll20406080100120140kappa=0.6kappa=0.8kappa=1algorithmloLDAloHDPpnastopiclikelihood−8.2−8.1−8.0−7.9−7.8−8.2−8.1−8.0−7.9−7.8−8.2−8.1−8.0−7.9−7.8S=16lllllllllllll20406080100120140S=64llllllllllllllllll20406080100120140S=256llllllllllllllllll20406080100120140S=1024llllllllllllllllll20406080100120140S=2048lllllllllllllllll20406080100120140kappa=0.6kappa=0.8kappa=1algorithmloLDAloHDPOnline Variational Inference for the Hierarchical Dirichlet Process

40,960
author
series
vol
due
your
latter
think
sun
sea
feet

81,920
author
series
due
sea
vol
latter
hand
carried
fact
appear

stars
star
observatory
sun
magnitude
solar
comet
spectrum
motion
photographs

stars
observatory
star
sun
magnitude
solar
motion
comet
eclipse
spectrum

122,880
due
series
distribution
author
sea
carried
statistical
sample
average
soil

stars
observatory
sun
star
solar
astronomical
greenwich
earth
eclipse
magnitude

163,840
weight
due
birds
response
series
average
sample
soil
population
frequency

204,800
rats
response
blood
sec
weight
dose
mice
average
food
controls

245,760
rats
mice
response
dose
drug
brain
injection
food
saline
females

286,720
rats
response
dose
saline
injection
brain
females
treated
food
rat

stars
observatory
solar
sun
astronomical
star
greenwich
eclipse
instrument
royal

stars
observatory
solar
sun
astronomical
star
earth
radio
greenwich
motion

stars
observatory
radio
star
optical
objects
magnitude
solar
positions

star
arc
emission
stars
optical
spectrum
image
images
ray
plates magnitude

327,680
rats
brain
memory
dopamine
mice
subjects
neurons
drug
induced
response

stars
galaxy
star
emission
galaxies
optical
redshift
images
image
objects

352,549
neurons
rats
memory
brain
dopamine
response
mice
behavioural
training
responses

galaxies
stars
galaxy
star
emission
optical
redshift
spectrum
images
objects

Table 1: The top ten words from two topics, displayed after different numbers of documents have been processed for
inference. The two topics are separated by the dashed line. The ﬁrst line of the table indicates the number of articles seen so
far (beginning from the year 1869). The topic on the top (which could be labeled “neuroscience research on rats”) does
not have a clear meaning until we have analyzed 204,800 documents. This topic is rarely used in the earlier part of the
corpus and few documents provide useful information about it. In contrast, the topic on the bottom (which could be labeled
“astronomy research”) has a clearer meaning from the beginning. This subject is discussed earlier in Nature history.

rithm is based on a stick-breaking construction of the HDP
that allows for closed-form coordinate ascent variational
inference, which is a key factor in developing the online al-
gorithm. Our experimental results show that for large-scale
applications, the online variational inference for the HDP
can address the model selection problem for LDA and avoid
overﬁtting.
The application of natural gradient learning to online vari-
ational inference may be generalized to other Bayesian
nonparametric models, as long as we can construct varia-
tional inference algorithms with closed form updates un-
der conjugacy. For example, the Indian Buffet process
(IBP) [25, 26, 27] might be another model that can use
an efﬁcient online variational inference algorithm for large
and streaming data sets.

Acknowledgement. Chong Wang is supported by Google
PhD fellowship. David M. Blei is supported by ONR 175-
6343, NSF CAREER 0745520, AFOSR 09NL202, the Al-
fred P. Sloan foundation, and a grant from Google.

References

[1] Teh, Y., M. Jordan, M. Beal, et al. Hierarchical Dirich-
let processes. Journal of the American Statistical As-
sociation, 101(476):1566–1581, 2007.

[2] Blei, D., A. Ng, M. Jordan. Latent Dirichlet allocation.

Figure 5: Experimental results on PNAS κ = 0.6 and batch
size S = 2048 (for the online algorithms). Points are sub-
sampled for better view. (Not all numbers of topics are
shown, please see Figure 6 for more details.) Online HDP
performs better than online LDA, and slightly worse than
batch HDP. Unlike in the Nature experiment, batch HDP is
trained on the whole training set.

     759pnastime (in seconds, log scale)per−word log likelihood−8.6−8.4−8.2−8.0−7.8llllllllllllllllllllllllllllllllllll102102.5103103.5104algorithmlllllloHDPbHDPoLDA−20oLDA−60oLDA−100oLDA−150Chong Wang

John Paisley

David M. Blei

[18] Blei, D., M. Jordan. Variational methods for the Dirich-
let process. In 21st International Conference on Ma-
chine Learning. 2004.

[19] Kurihara, K., M. Welling, Y. Teh. Collapsed varia-
In IJCAI.

tional Dirichlet process mixture models.
2007.

[20] Kurihara, K., M. Welling, N. Vlassis. Accelerated vari-
ational Dirichlet process mixtures. In B. Sch¨olkopf,
J. Platt, T. Hoffman, eds., Advances in Neural Infor-
mation Processing Systems 19, pages 761–768. MIT
Press, Cambridge, MA, 2007.

[21] Wang, C., D. Blei. Variational inference for the nested
Chinese restaurant process. In Y. Bengio, D. Schuur-
mans, J. Lafferty, C. K. I. Williams, A. Culotta, eds.,
Advances in Neural Information Processing Systems
22, pages 1990–1998. 2009.

[22] Asuncion, A., M. Welling, P. Smyth, et al. On smooth-
ing and inference for topic models. In Uncertainty in
Artiﬁcial Intelligence. 2009.

[23] Blei, D., J. Lafferty. Dynamic topic models. In In-
ternational Conference on Machine Learning, pages
113–120. ACM, New York, NY, USA, 2006.

[24] Wang, C., D. Blei, D. Heckerman. Continuous time
dynamic topic models. In Uncertainty in Artiﬁcial
Intelligence (UAI). 2008.

[25] Grifﬁths, T., Z. Ghahramani. Inﬁnite latent feature
models and the Indian buffet process. In Y. Weiss,
B. Sch¨olkopf, J. Platt, eds., Advances in Neural Infor-
mation Processing Systems 18, pages 475–482. MIT
Press, Cambridge, MA, 2006.

[26] Teh, Y., D. Gorur, Z. Ghahramani. Stick-breaking
In 11th
construction for the Indian buffet process.
Conference on Artiﬁcial Intelligence and Statistics.
2007.

[27] Doshi-Velez, F., K. Miller, J. Van Gael, et al. Vari-
ational inference for the Indian buffet process.
In
Proceedings of the Intl. Conf. on Artiﬁcial Intelligence
and Statistics, pages 137–144. 2009.

Journal of Machine Learning Research, 3:993–1022,
2003.

[3] Teh, Y., K. Kurihara, M. Welling. Collapsed vari-
In Neural Information

ational inference for HDP.
Processing Systems. 2007.

[4] Liang, P., S. Petrov, D. Klein, et al. The inﬁnite PCFG
using hierarchical Dirichlet processes. In Empirical
Methods in Natural Language Processing. 2007.

[5] Canini, K., L. Shi, T. Grifﬁths. Online inference of
topics with latent Dirichlet allocation. In Artiﬁcial
Intelligence and Statistics. 2009.

[6] Rodriguez, A. On-line learning for the inﬁnite hid-
den Markov model. Tech. Rep. UCSC-SOE-10-27,
Department of Applied Mathematics and Statistics,
University of California at Santa Cruz, 2010.

[7] Hoffman, M., D. Blei, F. Bach. Online inference for

latent Drichlet allocation. In NIPS. 2010.

[8] Jordan, M., Z. Ghahramani, T. Jaakkola, et al. Intro-
duction to variational methods for graphical models.
Machine Learning, 37:183–233, 1999.

[9] Robbins, H., S. Monro. A stochastic approxima-
tion method. The Annals of Mathematical Statistics,
22(3):pp. 400–407, 1951.

[10] Amari, S. Natural gradient works efﬁciently in learn-

ing. Neural computation, 10(2):251–276, 1998.

[11] Sato, M. Online model selection based on the varia-
tional Bayes. Neural Computation, 13(7):1649–1681,
2005.

[12] Attias, H. A variational Bayesian framework for graph-
ical models. In Advances in Neural Information Pro-
cessing Systems 12. 2000.

[13] Boyd-Graber, J., D. Blei. Syntactic topic models. In

Neural Information Processing Systems. 2009.

[14] Fox, E., E. Sudderth, M. Jordan, et al. An HDP-HMM
for systems with state persistence. In International
Conference on Machine Learning. 2008.

[15] Sethuraman, J. A constructive deﬁnition of Dirichlet

priors. Statistica Sinica, 4:639–650, 1994.

[16] Ferguson, T. A Bayesian analysis of some nonpara-
metric problems. The Annals of Statistics, 1:209–230,
1973.

[17] Pitman, J. Poisson-Dirichlet and GEM invariant dis-
tributions for split-and-merge transformations of an
interval partition. Combinatorics, Probability, and
Computing, 11:501–514, 2002.

     760