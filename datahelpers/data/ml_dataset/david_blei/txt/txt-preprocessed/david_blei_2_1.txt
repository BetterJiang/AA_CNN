Abstract

Motivated by large-scale multimedia applications we propose to learn mappings
from high-dimensional data to binary codes that preserve semantic similarity.
Binary codes are well suited to large-scale applications as they are storage ef-
cient and permit exact sub-linear kNN search. The framework is applicable
to broad families of mappings, and uses a exible form of triplet ranking loss.
We overcome discontinuous optimization of the discrete mappings by minimizing
a piecewise-smooth upper bound on empirical loss, inspired by latent structural
SVMs. We develop a new loss-augmented inference algorithm that is quadratic in
the code length. We show strong retrieval performance on CIFAR-10 and MNIST,
with promising classication results using no more than kNN on the binary codes.

Introduction

1
Many machine learning algorithms presuppose the existence of a pairwise similarity measure on
the input space. Examples include semi-supervised clustering, nearest neighbor classication, and
kernel-based methods, When similarity measures are not given a priori, one could adopt a generic
function such as Euclidean distance, but this often produces unsatisfactory results. The goal of
metric learning techniques is to improve matters by incorporating side information, and optimizing
parametric distance functions such as the Mahalanobis distance [7, 12, 30, 34, 36].
Motivated by large-scale multimedia applications, this paper advocates the use of discrete mappings,
from input features to binary codes. Compact binary codes are remarkably storage efcient, allow-
ing one to store massive datasets in memory. The Hamming distance, a natural similarity measure
on binary codes, can be computed with just a few machine instructions per comparison. Further, it
has been shown that one can perform exact nearest neighbor search in Hamming space signicantly
faster than linear search, with sublinear run-times [15, 25]. By contrast, retrieval based on Ma-
halanobis distance requires approximate nearest neighbor (ANN) search, for which state-of-the-art
methods (e.g., see [18, 23]) do not always perform well, especially with massive, high-dimensional
datasets when storage overheads and distance computations become prohibitive.
Most approaches to discrete (binary) embeddings have focused on preserving the metric (e.g. Eu-
clidean) structure of the input data, the canonical example being locality-sensitive hashing (LSH)
[4, 17]. Based on random projections, LSH and its variants (e.g., [26]) provide guarantees that metric
similarity is preserved for sufciently long codes. To nd compact codes, recent research has turned
to machine learning techniques that optimize mappings for specic datasets (e.g., [20, 28, 29, 32, 3]).
However, most such methods aim to preserve Euclidean structure (e.g. [13, 20, 35]).
In metric learning, by comparison, the goal is to preserve semantic structure based on labeled at-
tributes or parameters associated with training exemplars. There are papers on learning binary hash
functions that preserve semantic similarity [29, 28, 32, 24], but most have only considered ad hoc
datasets and uninformative performance measures, for which it is difcult to judge performance with
anything but the qualitative appearance of retrieval results. The question of whether or not it is pos-
sible to learn hash functions capable of preserving complex semantic structure, with high delity,
has remained unanswered.

1

To address this issue, we introduce a framework for learning a broad class of binary hash functions
based on a triplet ranking loss designed to preserve relative similarity (c.f. [11, 5]). While certainly
useful for preserving metric structure, this loss function is very well suited to the preservation of
semantic similarity. Notably, it can be viewed as a form of local ranking loss. It is more exible
than the pairwise hinge loss of [24], and is shown below to produce superior hash functions.
Our formulation is inspired by latent SVM [10] and latent structural SVM [37] models, and it gen-
eralizes the minimal loss hashing (MLH) algorithm of [24]. Accordingly, to optimize hash function
parameters we formulate a continuous upper-bound on empirical loss, with a new form of loss-
augmented inference designed for efcient optimization with the proposed triplet loss on the Ham-
ming space. To our knowledge, this is one of the most general frameworks for learning a broad class
of hash functions. In particular, many previous loss-based techniques [20, 24] are not capable of
optimizing mappings that involve non-linear projections, e.g., by neural nets.
Our experiments indicate that the framework is capable of preserving semantic structure on chal-
lenging datasets, namely, MNIST [1] and CIFAR-10 [19]. We show that k-nearest neighbor (kNN)
search on the resulting binary codes retrieves items that bear remarkable similarity to a given query
item. To show that the binary representation is rich enough to capture salient semantic structure,
as is common in metric learning, we also report classication performance on the binary codes.
Surprisingly, on these datasets, simple kNN classiers in Hamming space are competitive with so-
phisticated discriminative classiers, including SVMs and neural networks. An important appeal of
our approach is the scalability of kNN search on binary codes to billions of data points, and of kNN
classication to millions of class labels.

2 Formulation
The task is to learn a mapping b(x) that projects p-dimensional real-valued inputs x  Rp onto
q-dimensional binary codes h  H  {1, 1}q, while preserving some notion of similarity. This
mapping, referred to as a hash function, is parameterized by a real-valued vector w as

(1)
where sign(.) denotes the element-wise sign function, and f (x; w) : Rp  Rq is a real-valued
transformation. Different forms of f give rise to different families of hash functions:

b(x; w) = sign (f (x; w)) ,

1. A linear transform f (x) = W x, where W  Rqp and w  vec(W ), is the simplest and most
well-studied case [4, 13, 24, 33]. Under this mapping the kth bit is determined by a hyperplane
in the input space whose normal is given by the kth row of W . 1

2. In [35], linear projections are followed by an element-wise cosine transform, i.e. f (x) =
cos(W x). For such mappings the bits correspond to stripes of 1 and 1 regions, oriented
parallel to the corresponding hyperplanes, in the input space.

3. Kernelized hash functions [20, 21].
4. More complex hash functions are obtained with multilayer neural networks [28, 32]. For
example, a two-layer network with a p(cid:48)-dimensional hidden layer and weight matrices W1 
Rp(cid:48)p and W2  Rqp(cid:48)
can be expressed as f (x) = tanh(W2 tanh(W1x)), where tanh(.)
is the element-wise hyperbolic tangent function.

Our Hamming distance metric learning framework applies to all of the above families of hash func-
tions. The only restriction is that f must be differentiable with respect to its parameters, so that one
is able to compute the Jacobian of f (x; w) with respect to w.
2.1 Loss functions
The choice of loss function is crucial for learning good similarity measures. To this end, most ex-
isting supervised binary hashing techniques [13, 22, 24] formulate learning objectives in terms of
pairwise similarity, where pairs of inputs are labelled as either similar or dissimilar. Similarity-
preserving hashing aims to ensure that Hamming distances between binary codes for similar (dis-
similar) items are small (large). For example, MLH [24] uses a pairwise hinge loss function. For

1For presentation clarity, in linear and nonlinear cases, we omit bias terms. They are incorporated by adding

one dimension to the input vectors, and to the hidden layers of neural networks, with a xed value of one.

2

(cid:26) [(cid:107)hg(cid:107)H   + 1 ]+

two binary codes h, g  H with Hamming distance2 (cid:107)hg(cid:107)H, and a similarity label s  {0, 1},
the pairwise hinge loss is dened as:

for s = 1
for s = 0

(similar)
(dissimilar) ,

(cid:96)pair(h, g, s) =

[   (cid:107)hg(cid:107)H + 1 ]+

(2)
where []+  max(, 0), and  is a Hamming distance threshold that separates similar from dis-
similar codes. This loss incurs zero cost when a pair of similar inputs map to codes that differ by
less than  bits. The loss is zero for dissimilar items whose Hamming distance is more than  bits.
One problem with such loss functions is that nding a suitable threshold  with cross-validation is
slow. Furthermore, for many problems one cares more about the relative magnitudes of pairwise
distances than their precise numerical values. So, constraining pairwise Hamming distances over
all pairs of codes with a single threshold is overly restrictive. More importantly, not all datasets are
amenable to labeling input pairs as similar or dissimilar. One way to avoid some of these problems is
to dene loss in terms of relative similarity. Such loss functions have been used in metric learning [5,
11], and, as shown below, they are also naturally suited to Hamming distance metric learning.
To dene relative similarity, we assume that the training data includes triplets of items (x, x+, x),
such that the pair (x, x+) is more similar than the pair (x, x). Our goal is to learn a hash function
b such that b(x) is closer to b(x+) than to b(x) in Hamming distance. Accordingly, we propose a
ranking loss on the triplet of binary codes (h, h+, h), obtained from b applied to (x, x+, x):

(cid:96)triplet(h, h+, h) = (cid:2)(cid:107)hh+(cid:107)H  (cid:107)hh(cid:107)H + 1(cid:3)

(3)
This loss is zero when the Hamming distance between the more-similar pair, (cid:107)h h+(cid:107)H, is at
least one bit smaller than the Hamming distance between the less-similar pair, (cid:107)h h(cid:107)H. This
loss function is more exible than the pairwise loss function (cid:96)pair, as it can be used to preserve
rankings among similar items, for example based on Euclidean distance, or perhaps using path
distance between category labels within a phylogenetic tree.

+ .

3 Optimization

Given a training set of triplets, D =(cid:8)(xi, x+

i )(cid:9)n

i , x

loss and a simple regularizer on the vector of unknown parameters w:

(cid:88)

L(w) =

(cid:96)triplet

(x,x+,x)D

i=1, our objective is the sum of the empirical

(cid:0) b(x; w), b(x+; w), b(x; w)(cid:1) +

(cid:107)w(cid:107)2
2 .


2

(4)

This objective is discontinuous and non-convex. The hash function is a discrete mapping and em-
pirical loss is piecewise constant. Hence optimization is very challenging. We cannot overcome the
non-convexity, but the problems owing to the discontinuity can be mitigated through the construction
of a continuous upper bound on the loss.
The upper bound on loss that we adopt is inspired by previous work on latent structural SVMs [37].
The key observation that relates our Hamming distance metric learning framework to structured
prediction is as follows,

b(x; w) = sign (f (x; w))

= argmax

hTf (x; w) ,

(5)
where H  {1, +1}q. The argmax on the RHS effectively means that for dimensions of f (x; w)
with positive values, the optimal code should take on values +1, and when elements of f (x; w) are
negative the corresponding bits of the code should be 1. This is identical to the sign function.
3.1 Upper bound on empirical loss
The upper bound on loss that we exploit for learning hash functions takes the following form:

hH

(cid:0) b(x; w), b(x+; w), b(x; w)(cid:1) 
(cid:0) g, g+, g(cid:1) + gTf (x; w) + g+T
(cid:8)hTf (x; w)(cid:9)  max

max
g,g+,g
 max

(cid:110)

(cid:110)

(cid:96)triplet

hT
2The Hamming norm (cid:107)v(cid:107)H is dened as the number of non-zero entries of vector v.

f (x+; w)

h+T

h

h+

h

f (x+; w) + gT

f (x; w)

(cid:111)  max

(cid:110)

f (x; w)

,

(6)

(cid:111)

(cid:111)

(cid:96)triplet

3

where g, g+, g, h, h+, and h are constrained to be q-dimensional binary vectors. To prove
the inequality in Eq. 6, note that if the rst term on the RHS were maximized3 by (g, g+, g) =
(b(x), b(x+), b(x)), then using Eq. 5, it is straightforward to show that Eq. 6 would become an
equality. In all other cases of (g, g+, g) which maximize the rst term, the RHS can only be as
large or larger than when (g, g+, g) = (b(x), b(x+), b(x)), hence the inequality holds.
Summing the upper bound instead of the loss in Eq. 4 yields an upper bound on the regularized
empirical loss in Eq. 4.
Importantly, the resulting bound is easily shown to be continuous and
piecewise smooth in w as long as f is continuous in w. The upper bound of Eq. 6 is a generalization
of a bound introduced in [24] for the linear case, f (x) = W x. In particular, when f is linear in
w, the bound on regularized empirical loss becomes piecewise linear and convex-concave. While
the bound in Eq. 6 is more challenging to optimize than the bound in [24], it allows us to learn
hash functions based on non-linear functions f, e.g. neural networks. While the bound in [24] was
dened for (cid:96)pair-type loss functions and pairwise similarity labels, the bound here applies to the more
exible class of triplet loss functions.

3.2 Loss-augmented inference
To use the upper bound in Eq. 6 for optimization, we must be able to nd the binary codes given by

(cid:110)

(cid:0) g, g+, g(cid:1) + gTf (x) + g+T

f (x+) + gT

f (x)

.

(7)

(cid:111)

(g, g+, g) = argmax
(g,g+,g)

(cid:96)triplet

In the structured prediction literature this maximization is called loss-augmented inference. The
challenge stems from the 23q possible binary codes over which one has to maximize the RHS.
Fortunately, we can show that this loss-augmented inference problem can be solved efciently for
the class of triplet loss functions that depend only on the value of

d(g, g+, g)  (cid:107)gg+(cid:107)H  (cid:107)gg(cid:107)H .

Importantly, such loss functions do not depend on the specic binary codes, but rather just the
differences. Further, note that d(g, g+, g) can take on only 2q +1 possible values, since it is an
integer between q and +q. Clearly the triplet ranking loss only depends on d since

(cid:0)g, g+, g(cid:1) = (cid:96) (cid:48)(cid:0)d(g, g+, g)(cid:1) , where

(cid:96) (cid:48)() = [   1 ]+ .

(8)

(cid:96)triplet

For this family of loss functions, given the values of f (.) in Eq. 7, loss-augmented inference can be
performed in time O(q2). To prove this, rst consider the case d(g, g+, g) = m, where m is an
integer between q and q. In this case we can replace the loss augmented inference problem with

gTf (x) + g+T

f (x+) + gT

f (x)

s.t. d(g, g+, g) = m .

(9)

(cid:110)

(cid:96) (cid:48)(m) + max
g,g+,g

(cid:111)

One can solve Eq. 9 for each possible value of m. It is straightforward to see that the largest of those
2q + 1 maxima is the solution to Eq. 7. Then, what remains for us is to solve Eq. 9.
To solve Eq. 9, consider the ith bit for each of the three codes, i.e. a = g[i], b = g+[i], and c =
g[i], where v[i] denotes the ith element of vector v. There are 8 ways to select a, b and c, but no
matter what values they take on, they can only change the value of d(g, g+, g) by 1, 0, or +1.
Accordingly, let ei  {1, 0, +1} denote the effect of the ith bits on d(g, g+, g). For each value
of ei, we can easily compute the maximal contribution of (a, b, c) to Eq. 9 by:

(cid:8)af (x)[i] + bf (x+)[i] + cf (x)[i](cid:9)

(10)

such that a, b, c  {1, +1} and (cid:107)ab(cid:107)H  (cid:107)ac(cid:107)H = ei.

Therefore, to solve Eq. 9, we aim to select values for ei, for all i, such that(cid:80)q
(cid:80)q

i=1 ei = m and
i=1 cont(i, ei) is maximized. This can be solved for any m using a dynamic programming algo-
rithm, similar to knapsack, in O(q2). Finally, we choose m that maximizes Eq. 9 and set the bits to
the congurations that maximized cont(i, ei).

cont(i, ei) = max
a,b,c

3For presentation clarity we will sometimes drop the dependence of f and b on w, and write b(x) and f (x).

4

3.3 Perceptron-like learning
Our learning algorithm is a form of stochastic gradient descent, where in the tth iteration we sample
a triplet (x, x+, x) from the dataset, and then take a step in the direction that decreases the upper
bound on the triplets loss in Eq. 6. To this end, we randomly initialize w(0). Then, at each iteration
t + 1, given w(t), we use the following procedure to update the parameters, w(t+1):

1. Select a random triplet (x, x+, x) from dataset D.
2. Compute (h, h+, h) = (b(x; w(t)), b(x+; w(t)), b(x; w(t))) using Eq. 5.
3. Compute (g, g+, g), the solution to the loss-augmented inference problem in Eq. 7 .
4. Update model parameters using

(cid:21)
w(t+1) = w(t) + 
,
where  is the learning rate, and f (x)/w  f (x; w)/w|w=w(t)  R|w|q is the trans-
pose of the Jacobian matrix, where |w| is the number of parameters.

(cid:16)h g(cid:17) w(t)

(cid:16)h+ g+(cid:17)

(cid:20) f (x)

(cid:16)h g

f (x)

(cid:17)

w

+

f (x+)

w

w

+

This update rule can be seen as gradient descent in the upper bound of the regularized empirical loss.
Although the upper bound in Eq. 6 is not differentiable at isolated points (owing to the max terms),
in our experiments we nd that this update rule consistently decreases both the upper bound and the
actual regularized empirical loss L(w).

4 Asymmetric Hamming distance
When Hamming distance is used to score and retrieve the nearest neighbors to a given query, there is
a high probability of a tie, where multiple items are equidistant from the query in Hamming space.
To break ties and improve the similarity measure, previous work suggests the use of an asymmetric
Hamming (AH) distance [9, 14]. With an AH distance, one stores dataset entries as binary codes (for
storage efciency) but the queries are not binarized. An asymmetric distance function is therefore
dened on a real-valued query vector, v  Rq, and a database binary code, h  H. Computing AH
distance is slightly less efcient than Hamming distance, and efcient retrieval algorithms, such as
[25], are not directly applicable. Nevertheless, the AH distance can also be used to re-rank items
retrieved using Hamming distance, with a negligible increase in run-time. To improve efciency
further when there are many codes to be re-ranked, AH distance from the query to binary codes can
be pre-computed for each 8 or 16 consecutive bits, and stored in a query-specic lookup table.
In this work, we use the following asymmetric Hamming distance function

1
4

(cid:107) h  tanh(Diag(s) v)(cid:107)2
2 ,

AH(h, v; s) =

(11)
where s  Rq is a vector of scaling parameters that control the slope of hyperbolic tangent applied
to different bits; Diag(s) is a diagonal matrix with the elements of s on its diagonal. As the scaling
factors in s approach innity, AH and Hamming distances become identical. Here we use the AH
distance between a database code b(x(cid:48)) and the real-valued projection for the query f (x). Based
on our validation sets, the AH distance of Eq. 11 is relatively insensitive to values in s. For the
experiments we simply use s to scale the average absolute values of the elements of f (x) to be 0.25.
5
In practice, the basic learning algorithm described in Sec. 3 is implemented with several modica-
tions. First, instead of using a single training triplet to estimate the gradients, we use mini-batches
comprising 100 triplets and average the gradient. Second, for each triplet (x, x+, x), we replace
x with a hard example by selecting an item among all negative examples in the mini-batch that is
closest in the current Hamming distance to b(x). By harvesting hard negative examples, we ensure
that the Hamming constraints for the triplets are not too easily satised. Third, to nd good binary
codes, we encourage each bit, averaged over the training data, to be mean-zero before quantization
(motivated in [35]). This is accomplished by adding the following penalty to the objective function:

Implementation details

(cid:0)f (x; w)(cid:1)(cid:107)2

2 ,

1
2

(cid:107) mean

x

5

(12)

Figure 1: MNIST precision@k: (left) four methods (with 32-bit codes); (right) three code lengths.

where mean(f (x; w)) denotes the mean of f (x; w) across the training data. In our implementation,
for efciency, the stochastic gradient of Eq. 12 is computed per mini-batch using the Jacobian matrix
in the update rule (see Sec. 3.3). Empirically, we observe that including this term in the objective
improves the quality of binary codes, especially with the triplet ranking loss.
We use a heuristic to adapt learning rates, known as bold driver [2]. For each mini-batch we evaluate
the learning objective before the parameters are updated. As long as the objective is decreasing we
slowly increase the learning rate , but when the objective increases,  is halved. In particular, after
every 25 epochs, if the objective, averaged over the last 25 epochs, decreased, we increase  by 5%,
otherwise we decrease  by 50%. We also used a momentum term; i.e. the previous gradient update
is scaled by 0.9 and then added to the current gradient.
All experiments are run on a GPU for 2, 000 passes through the datasets. The training time for
our current implementation is under 4 hours of GPU time for most of our experiments. The two
exceptions involve CIFAR-10 with 6400-D inputs and relatively long code-lengths of 256 and 512
bits, for which the training times are approximated 8 and 16 hours respectively.

6 Experiments
Our experiments evaluate Hamming distance metric learning using two families of hash functions,
namely, linear transforms and multilayer neural networks (see Sec. 2). For each, we examine two
loss functions, the pairwise hinge loss (Eq. 2) and the triplet ranking loss (Eq. 3).
Experiments are conducted on two well-known image corpora, MNIST [1] and CIFAR-10 [19].
Ground-truth similarity labels are derived from class labels; items from the same class are deemed
similar4. This denition of similarity ignores intra-class variations and the existence of sub-
categories, e.g. styles of handwritten fours, or types of airplanes. Nevertheless, we use these coarse
similarity labels to evaluate our framework. To that end, using items from the test set as queries,
we report precision@k, i.e. the fraction of k closest items in Hamming distance that are same-class
neighbors. We also show kNN retrieval results for qualitative inspection. Finally, we report Ham-
ming (H) and asymmetric Hamming (AH) kNN classication rates on the test sets.
Datasets. The MNIST [1] digit dataset contains 60, 000 training and 10, 000 test images (2828
pixels) of ten handwritten digits (0 to 9). Of the 60, 000 training images, we set aside 5, 000 for
validation. CIFAR-10 [19] comprises 50, 000 training and 10, 000 test color images (3232 pixels).
Each image belongs to one of 10 classes, namely airplane, automobile, bird, cat, deer, dog, frog,
horse, ship, and truck. The large variability in scale, viewpoint, illumination, and background clutter
poses a signicant challenge for classication. Instead of using raw pixel values, we borrow a bag-
of-words representation from Coates et al [6]. Its 6400-D feature vector comprises one 1600-bin
histogram per image quadrant, the codewords of which are learned from 66 image patches. Such
high-dimensional inputs are challenging for learning similarity-preserving hash functions. Of the
50, 000 training images, we set aside 5, 000 for validation.
MNIST: We optimize binary hash functions, mapping raw MNIST images to 32, 64, and 128-bit
codes. For each test code we nd the k closest training codes using Hamming distance, and report
precision@k in Fig. 1. As one might expect, the non-linear mappings5 signicantly outperform lin-
ear mappings. We also nd that the triplet loss (Eq. 3) yields better performance than the pairwise

4Training triplets are created by taking two items from the same class, and one item from a different class.
5The two-layer neural nets for Fig. 1 and Table 1 had 1 hidden layer with 512 units. Weights were initialized

randomly, and the Jacobian with respect to the parameters was computed with the backprop algorithm [27].

6

101001000100000.870.90.930.960.99kPrecision @k  Twolayer net, tripletTwolayer net, pairwiseLinear, tripletLinear, pairwise [24]101001000100000.870.90.930.960.99kPrecision @k  128bit, linear, triplet64bit, linear, triplet32bit, linear, tripletEuclidean distanceDistance

.

g
n
i
m
m
a
H

g
n
i
m
m
a
H

Hash function, Loss
Linear, pairwise hinge [24]
Linear, triplet ranking
Two-layer Net, pairwise hinge
Two-layer Net, triplet ranking
Linear, pairwise hinge
Linear, triplet ranking
Two-layer Net, pairwise hinge
Two-layer Net, triplet ranking
Baseline
Deep neural nets with pre-training [16]
Large margin nearest neighbor [34]
RBF-kernel SVM [8]
Neural network [31]
Euclidean 3NN

m
y
s
A

64 bits
3.16
3.06
1.45
1.38
2.78
2.90
1.36
1.29

128 bits

2.61
2.44
1.44
1.27
2.46
2.51
1.35
1.20

kNN
2 NN
2 NN
30 NN
30 NN
3 NN
3 NN
30 NN
30 NN

32 bits
4.66
4.44
1.50
1.45
4.30
3.88
1.50
1.45

Error
1.2
1.3
1.4
1.6
2.89

Table 1: Classication error rates on MNIST test set.

loss (Eq. 2). The sharp drop in precision at k = 6000 is a consequence of the fact that each digit in
MNIST has approximately 6000 same-class neighbors. Fig. 1 (right) shows how precision improves
as a function of the binary code length. Notably, kNN retrieval, for k > 10 and all code lengths,
yields higher precision than Euclidean NN on the 784-D input space. Further, note that these Euclid-
ian results effectively provide an upper bound on the performance one would expect with existing
hashing methods that preserve Eucliean distances (e.g., [13, 17, 20, 35]).
One can also evaluate the delity of the Hamming space represenation in terms of classication
performance from the Hamming codes. To focus on the quality of the hash functions, and the speed
of retrieval for large-scale multimedia datasets, we use a kNN classier; i.e. we just use the retrieved
neighbors to predict class labels for each test code. Table 1 reports classication error rates using
kNN based on Hamming and asymmetric Hamming distance. Non-linear mappings, even with
only 32-bit codes, signicantly outperform linear mappings (e.g.with 128 bits). The ranking hinge
loss also improves upon the pairwise hinge loss, even though the former has no hyperparameters.
Table 1 also indicates that AH distance provides a modest boost in performance. For each method
the parameter k in the kNN classier is chosen based on the validation set.
For baseline comparison, Table 1 reports state-of-the-art performance on MNIST with sophisticated
discriminative classiers (excluding those using examplar deformations and convolutional nets).
Despite the simplicity of a kNN classier, our model achieves error rates of 1.29% and 1.20% using
64- and 128-bit codes. This is compared to 1.4% with RBF-SVM [8], and to 1.6%, the best published
neural net result for this version of the task [31]. Our model also out performs the metric learning
approach of [34], and is competitive with the best known Deep Belief Network [16]; although they
used unsupervised pre-training while we do not.
The above results show that our Hamming distance metric learning framework can preserve suf-
cient semantic similarity, to the extent that Hamming kNN classication becomes competitive with
state-of-the-art discriminative methods. Nevertheless, our method is not solely a classier, and it
can be used within many other machine learning algorithms.
In comparison, another hashing technique called iterative quantization (ITQ) [13] achieves 8.5%
error on MNIST and 78% accuracy on CIFAR-10. Our method compares favorably, especially on
MNIST. However, ITQ [13] inherently binarizes the outcome of a supervised classier (Canonical
Correlation Analysis with labels), and does not explicitly learn a similarity measure on the input
features based on pairs or triplets.
CIFAR-10: On CIFAR-10 we optimize hash functions for 64, 128, 256, and 512-bit codes. The
supplementary material includes precision@k curves, showing superior quality of hash functions
learned by the ranking loss compared to the pairwise loss. Here, in Fig. 2, we depict the quality
of retrieval results for two queries, showing the 16 nearest neighbors using 256-bit codes, 64-bit
codes (both learned with the triplet ranking loss), and Euclidean distance in the original 6400-D
feature space. The number of class-based retrieval errors is much smaller in Hamming space, and
the similarity in visual appearance is also superior. More such results, including failure modes, are
shown in the supplementary material.

7

(Hamming on 256 bit codes)

(Hamming on 64 bit codes)

(Euclidean distance)

Figure 2: Retrieval results for two CIFAR-10 test images using Hamming distance on 256-bit and
64-bit codes, and Euclidean distance on bag-of-words features. Red rectangles indicate mistakes.

Hashing, Loss
Linear, pairwise hinge [24]
Linear, pairwise hinge
Linear, triplet ranking
Linear, triplet ranking

Distance

H
AH
H
AH

kNN
7 NN
8 NN
2 NN
2 NN

64 bits
72.2
72.3
75.1
75.7

128 bits

256 bits

512 bits

72.8
73.5
75.9
76.8

73.8
74.3
77.1
77.5

74.6
74.9
77.9
78.0

Baseline
One-vs-all linear SVM [6]
Euclidean 3NN

Accuracy

77.9
59.3

Table 2: Recognition accuracy on the CIFAR-10 test set (H  Hamming, AH  Asym. Hamming).

Table 2 reports classication performance (showing accuracy instead of error rates for consistency
with previous papers). Euclidean NN on the 6400-D input features yields under 60% accuracy,
while kNN with the binary codes obtains 76 78%. As with MNIST data, this level of perfor-
mance is comparable to one-vs-all SVMs applied to the same features [6]. Not surprisingly, training
fully-connected neural nets on 6400-dimensional features with only 50, 000 training examples is
challenging and susceptible to over-tting, hence the results of neural nets on CIFAR-10 were not
competitive. Previous work [19] had some success training convolutional neural nets on this dataset.
Note that our framework can easily incorporate convolutional neural nets, which are intuitively bet-
ter suited to the intrinsic spatial structure of natural images.

7 Conclusion

We present a framework for Hamming distance metric learning, which entails learning a discrete
mapping from the input space onto binary codes. This framework accommodates different families
of hash functions, including quantized linear transforms, and multilayer neural nets. By using a
piecewise-smooth upper bound on a triplet ranking loss, we optimize hash functions that are shown
to preserve semantic similarity on complex datasets.
In particular, our experiments show that a
simple kNN classier on the learned binary codes is competitive with sophisticated discriminative
classiers. While other hashing papers have used CIFAR or MNIST, none report kNN classication
performance, often because it has been thought that the bar established by state-of-the-art classiers
is too high. On the contrary our kNN classication performance suggests that Hamming space can
be used to represent complex semantic structures with high delity. One appeal of this approach is
the scalability of kNN search on binary codes to billions of data points, and of kNN classication to
millions of class labels.

8

