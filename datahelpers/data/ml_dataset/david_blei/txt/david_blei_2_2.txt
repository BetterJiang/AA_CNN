Bayesian Nonparametric Matrix Factorization for Recorded Music

Matthew D. Hoffman
David M. Blei
Perry R. Cook
Princeton University, Department of Computer Science, 35 Olden St., Princeton, NJ, 08540 USA

MDHOFFMA@CS.PRINCETON.EDU
BLEI@CS.PRINCETON.EDU
PRC@CS.PRINCETON.EDU

Abstract

Recent research in machine learning has focused
on breaking audio spectrograms into separate
sources of sound using latent variable decom-
positions. These methods require that the num-
ber of sources be speciﬁed in advance, which is
not always possible. To address this problem,
we develop Gamma Process Nonnegative Matrix
Factorization (GaP-NMF), a Bayesian nonpara-
metric approach to decomposing spectrograms.
The assumptions behind GaP-NMF are based
on research in signal processing regarding the
expected distributions of spectrogram data, and
GaP-NMF automatically discovers the number of
latent sources. We derive a mean-ﬁeld variational
inference algorithm and evaluate GaP-NMF on
both synthetic data and recorded music.

1. Introduction
Recent research in machine learning has focused on break-
ing audio spectrograms into separate sources of sound us-
ing latent variable decompositions. Such decompositions
have been applied to identifying individual instruments and
notes, e.g., for music transcription (Smaragdis & Brown,
2003), to predicting hidden or distorted signals (Bansal
et al., 2005), and to source separation (F´evotte et al., 2009).
A problem with these methods is that the number of sources
must be speciﬁed in advance, or found with expensive tech-
niques such as cross-validation. This problem is particu-
larly relevant when analyzing music. We want the discov-
ered latent components to correspond to real-world sound
sources, and we cannot expect the same number of sources
to be present in every recording.
In this article, we develop Gamma Process Nonnegative
Matrix Factorization (GaP-NMF), a Bayesian nonparamet-

Appearing in Proceedings of the 27 th International Conference
on Machine Learning, Haifa, Israel, 2010. Copyright 2010 by the
author(s)/owner(s).

ric (BNP) approach to decomposing spectrograms. We
posit a generative probabilistic model of spectrogram data
where, given an observed audio signal, posterior inference
reveals both the latent sources and their number.
The central computational challenge posed by our model
is posterior inference. Unlike other BNP factorization
methods, our model is not composed of conjugate pairs of
distributions—we chose our distributions to be appropriate
for spectrogram data, not for computational convenience.
We use variational inference to approximate the posterior,
and develop a novel variational approach to inference in
nonconjugate models. Variational inference approximates
the posterior with a simpler distribution, whose parameters
are optimized to be close to the true posterior (Jordan et al.,
1999). In mean-ﬁeld variational inference, each variable is
given an independent distribution, usually of the same fam-
ily as its prior. Where the model is conjugate, optimization
proceeds by an elegant coordinate ascent algorithm. Re-
searchers usually appeal to less efﬁcient scalar optimization
where conjugacy is absent. We instead use a bigger varia-
tional family than the model initially asserts. We show that
this gives an analytic coordinate ascent algorithm, of the
kind usually limited to conjugate models.
We evaluated GaP-NMF on several problems—extracting
the sources from music audio, predicting the signal in miss-
ing entries of the spectrogram, and classical measures of
Bayesian model ﬁt. Our model performs as well as or better
than the current state-of-the-art. It ﬁnds simpler representa-
tions of the data with equal statistical power, without need-
ing to explore many ﬁts over many numbers of sources, and
thus with much less computation.

2. GaP-NMF Model
We model the Fourier power spectrogram X of an audio
signal. The spectrogram X is an M by N matrix of non-
negative reals; the cell Xmn is the power of our input au-
dio signal at time window n and frequency bin m. Each
column of the power spectrogram is obtained as follows.
First, take the discrete Fourier transform of a window of

Bayesian Nonparametric Matrix Factorization for Recorded Music

2(M − 1) samples. Next, compute the squared magnitude
of the complex value in each frequency bin. Finally, keep
only the ﬁrst M bins, since the remaining bins contain only
redundant information.
We assume the audio signal is composed of K static sound
sources. As a consequence, we can model the observed
spectrogram X with the product of two non-negative ma-
trices: an M by K matrix W describing these sources
and a K by N matrix H controlling how the amplitude
of each source changes over time (Smaragdis & Brown,
2003). Each column of W is the average power spectrum
of an audio source; cell Wmk is the average amount of en-
ergy source k exhibits at frequency m. Each row of H is
the time-varying gain of a source; cell Hkn is the gain of
source k at time n. These matrices are unobserved.
Abdallah & Plumbley (2004) and F´evotte et al. (2009)
show that (under certain assumptions) mixing K sound
sources in the time domain, with average power spectra de-
ﬁned by the columns of W and gains speciﬁed by the rows
of H, yields a mixture whose spectrogram X is distributed

Xmn ∼ Exponential((cid:80)

k WmkHkn).

(1)

Previous spectrogram decompositions assume the number
of components K is known. In practice, this is rarely true.
Our goal is to develop a method that infers both the char-
acters and number of latent audio sources from data. We
develop a Bayesian nonparametric model with an inﬁnite
number of latent components, a ﬁnite number of which are
active when conditioned on observed data.
We now describe the Gamma Process Nonnegative Matrix
Factorization model (GaP-NMF). As in previous matrix de-
composition models, the spectrogram X arises from hid-
den matrices W and H. In addition, the model includes
a hidden vector of non-negative values θ, where each ele-
ment θl is the overall gain of the corresponding source l.
The key idea is that we allow for the possibility of a large
number of sources L, but place a sparse prior on θ. During
posterior inference, this prior biases the model to use no
more sources than it needs.
Speciﬁcally, GaP-NMF assumes that X is drawn according
to the following generative process:
Wml ∼ Gamma(a, a)
Hln ∼ Gamma(b, b)
θl ∼ Gamma(α/L, αc)

Xmn ∼ Exponential((cid:80)

l θlWmlHln).

(2)

As the truncation level L increases towards inﬁnity, the
vector θ approximates an inﬁnite sequence drawn from a
gamma process with shape parameter α and inverse-scale
parameter αc (Kingman, 1993). A property of this se-

quence is that the number of elements K greater than some
number  > 0 is ﬁnite almost surely. Speciﬁcally:

K ∼ Poisson

x−1e−xαcdx

.

(3)

(cid:18)1

(cid:90) ∞

c



(cid:19)

For truncation levels L that are sufﬁciently large relative to
the shape parameter α, we likewise expect that only a few
of the L elements of θ will be substantially greater than 0.
During posterior inference, this property leads to a prefer-
ence for explanations that use relatively few components.
Note that the expected value of Xmn under this model is
constant with respect to L, α, a, and b:

Ep[Xmn] =(cid:80)

Ep[θl]Ep[Wml]Ep[Hln] = 1
c .

(4)

l

This equation suggests the heuristic of setting the expected
mean of the spectrogram X under the prior equal to its
empirical mean ¯X by setting c = 1/ ¯X.

3. Variational Inference
Posterior inference is the central computational problem
for analyzing data with the GaP-NMF model. Given an ob-
served spectrogram X, we want to compute the posterior
distribution p(θ, W , H|X, α, a, b, c). Exact Bayesian in-
ference is intractable. We appeal to mean-ﬁeld variational
inference (Jordan et al., 1999).
Variational
inference is a deterministic alternative to
Markov Chain Monte Carlo (MCMC) methods that re-
places sampling with optimization. It has permitted efﬁ-
cient large-scale inference for several Bayesian nonpara-
metric models (e.g. Blei & Jordan, 2004; Doshi-Velez et al.,
2009; Paisley & Carin, 2009). Variational inference algo-
rithms approximate the true posterior distribution with a
simpler variational distribution controlled by free param-
eters. These parameters are optimized to make the vari-
ational distribution close (in Kullback-Leibler divergence)
to the true posterior of interest. Mean-ﬁeld variational in-
ference uses a fully factorized variational distribution—i.e.,
under the variational distribution all variables are indepen-
dent.
In conjugate models this permits easy coordinate
ascent updates using variational distributions of the same
families as the prior distributions.
Less frequently, variational methods are applied to non-
conjugate models, which allow increased model expres-
sivity at the price of greater algorithmic challenges. Our
model is such a model. The usual strategy is to use a fac-
torized variational distribution with the same families as the
priors, bound or approximate the objective function, and
use numerical techniques to optimize difﬁcult parameters
(Blei & Lafferty, 2006; Braun & McAuliffe, 2008).
We use a different strategy. We adopt an expanded fam-
ily for our variational distributions, one that generalizes the

Bayesian Nonparametric Matrix Factorization for Recorded Music

priors’ family. This allows us to derive analytic coordinate
ascent updates for the variational parameters, eliminating
the need for numerical optimization.

3.1. Variational Objective Function

It is standard in mean-ﬁeld variational inference to give
each variable a variational distribution from the same fam-
ily as its prior distribution (Jordan et al., 1999). We instead
use the more ﬂexible Generalized Inverse-Gaussian (GIG)
family (Jørgenson, 1982):

q(Wml) = GIG(γ(W )
ml
q(Hln) = GIG(γ(H)
q(θl) = GIG(γ(θ)

, ρ(W )
ml
ln , ρ(H)
, ρ(θ)

, τ (W )
ml )
ln , τ (H)
ln )
).

, τ (θ)

l

l

l

(5)

The GIG distribution is an exponential family distribution
with sufﬁcient statistics x, 1/x, and log x, and its PDF (in
canonical exponential family form) is

GIG(y; γ, ρ, τ) =

exp{(γ − 1) log y − ρy − τ /y}ργ/2

2τ γ/2Kγ(2

√

ρτ)

,

(6)
for x ≥ 0, ρ ≥ 0, and τ ≥ 0. (Kν(x) denotes a modiﬁed
Bessel function of the second kind.)
Note that the GIG family’s sufﬁcient statistics (y, 1/y, and
log y) are a superset of those of the gamma family (y and
log y), and so the gamma family is a special case of the
GIG family where γ > 0, τ → 0.
To compute the bound in equation 8, we will need the ex-
pected values of each Wml, Hln, and θl and of their recip-
rocals under our variational GIG distributions. For a vari-
able y ∼ GIG(γ, ρ, τ) these expectations are

E[y] =

√
√
Kγ+1(2
ρτ)
√
√
Kγ(2
ρτ)

τ
ρ

; E

√
√
Kγ−1(2
ρτ)
√
√
Kγ(2
ρτ)

τ

ρ

.

=

(cid:20) 1

(cid:21)

y

(7)
Having chosen a fully factorized variational family, we can
lower bound the marginal likelihood of the input spectro-
gram under the GaP-NMF model (Jordan et al., 1999):

log p(X|α, a, b, c) ≥ Eq[log p(X|W , H, θ)]
+ Eq[log p(W|a)] − Eq[log q(W )]
+ Eq[log p(H|b)] − Eq[log q(H)]
+ Eq[log p(θ|α, c)] − Eq[log q(θ)].

(8)

The difference between the left and right sides of equation
8 is the Kullback-Leibler (KL) divergence between the true
posterior and the variational distribution q. Thus, maximiz-
ing this bound with respect to q minimizes the KL diver-
gence between q and our posterior distribution of interest.
The second, third, and fourth lines of equation 8 can be
computed using the expectations in equation 7.

The likelihood term in equation 8 expands to

Eq[log p(X|W , H, θ)] =

(cid:20)

(cid:80)

(cid:88)

m,n

Eq

(cid:21)

−Xmn
l θlWmlHln

− Eq

(cid:34)

log(cid:88)

l

(cid:35)

θlWmlHln

.

(9)

We cannot compute either of the expectations on the right.
However, we can compute lower bounds on both of them.
says that for any vector φ such that φl ≥ 0 and(cid:80)
First, the function −x−1 is concave. Jensen’s inequality
= −(cid:80)
l φl = 1
− 1P
(10)
(cid:20)
(cid:20) −Xmn

We use this inequality to derive a bound on the ﬁrst expec-
tation in equation 9:

≥ −(cid:80)
≥(cid:88)

= − 1P

l φ2
l

(cid:21)

(cid:21)

l φl

1
xl

.

(11)

1
xl
φl

l xl

l φl

xl
φl

φ2

lmn

Eq

Eq

(cid:80)

−Xmn
l θlWmlHln

θlWmlHln

l

Second, the function − log x is convex. We can therefore
bound the second expectation in equation 9 using a ﬁrst-
order Taylor approximation about an arbitrary (positive)
point ωmn as in (Blei & Lafferty, 2006) 1:

(cid:34)

log(cid:88)

−Eq

(cid:35)

≥

(cid:20)

θlWmlHln

l

− log(ωmn) + 1 − 1
ωmn

(12)

Eq [θlWmlHln] .

(cid:88)

l

We use equations 11 and 12 to bound equation 9:
Eq[log p(X|W , H, θ)] ≥

(cid:88)

(cid:88)

− Xmn

φ2

lmn

Eq

m,n

l

θlWmlHln

(cid:21)

(13)

− log(ωmn) + 1 − 1
ωmn

Eq [θlWmlHln] .

1

(cid:88)

l

Note that this bound involves the expectations both of the
model parameters and of their reciprocals under the vari-
ational distribution q. Since both y and 1/y are sufﬁcient
statistics of GIG(y; γ, ρ, τ), this will not pose a problem
during inference, as it would if we were to use variational
distributions from the gamma family.
We denote as L the sum of the likelihood bound in equa-
tion 13 and the second, third, and fourth lines of equation

1Braun & McAuliffe (2008) observe that this bound is max-
imized when the Taylor approximation is taken around the ex-
pected value of the argument to the logarithm function, which
corresponds to the 0th-order delta method. However, retaining
the “redundant” parameter ωmn permits faster and simpler up-
dates for our other parameters.

Bayesian Nonparametric Matrix Factorization for Recorded Music

8. L lower bounds the likelihood p(X|α, a, b, c). Our vari-
ational inference algorithm maximizes this bound over the
free parameters, yielding an approximation q(W , H, θ) to
the true posterior p(W , H, θ|X, α, a, b, c).

We iterate between updating bound parameters and vari-
ational parameters according to equations 14, 15, 16, 17,
and 18. Each update tightens the variational bound on
log p(X|α, a, b, c), ultimately reaching a local optimum.

3.2. Coordinate Ascent Optimization
We maximize the bound L using coordinate ascent, iter-
atively optimizing each parameter while holding all other
parameters ﬁxed. There are two sets of parameters to opti-
mize: those used to bound the likelihood term in equation
9 and those that control the variational distribution q.

3.2.1. TIGHTENING THE LIKELIHOOD BOUND

In equations 11 and 12, we derived bounds on the in-
tractable expectations in equation 9. After updating the
variational distributions on each set of parameters W , H,
and θ, we update φ and ω to re-tighten these bounds.
Using Lagrange multipliers, we ﬁnd that the optimal φ is

(cid:20)

(cid:21)−1

φlmn ∝ Eq

1

θlWmlHln

.

(14)

The bound in equation 12 is tightest when
Eq [θlWmlHln] .

ωmn =(cid:80)

l

(15)

I.e., this bound is tightest when we take the Taylor approxi-
mation about the expected value of the function’s argument.

3.2.2. OPTIMIZING THE VARIATIONAL DISTRIBUTIONS
The derivative of L with respect to any of γ(W )
τ (W )
ml

equals 0 when

, ρ(W )
ml

ml

, or

ml = a + Eq[θl](cid:80)
(cid:105)(cid:80)
(cid:104) 1

ρ(W )

n Xmnφ2

lmn

Eq

θl

Eq[Hln]
ωmn

;

(cid:104) 1

n

(cid:105)

Hln

.

(16)

γ(W )
ml = a;
ml = Eq
τ (W )

Simultaneously updating the parameters γ(W ), ρ(W ), and
τ (W ) according to equation 16 will maximize L with re-
spect to those parameters.
Similarly, the derivative of L with respect to any of γ(H)
ln ,
ln , or τ (H)
ρ(H)

ln

equals 0 and L is maximized when
Eq[Wml]

ln = b + Eq[θl](cid:80)
(cid:105)(cid:80)
(cid:104) 1

(cid:104) 1

ρ(H)

(cid:105)

ωmn

m

m Xmnφ2

lmn

Eq

Wml

θl

;

γ(H)
ln = b;
ln = Eq
τ (H)

.

(17)

Finally, the derivative of L with respect to any of γ(θ)
or τ (θ)

equals 0 and L is maximized when

l

, ρ(θ)

l

,

l

l = αc +(cid:80)
(cid:80)

n Xmnφ2

ρ(θ)

m

lmn

m

(cid:80)
(cid:104)

n

Eq

l =(cid:80)

γ(θ)
l = α
L;
τ (θ)

Eq[WmlHln]

ωmn

;

(cid:105)

1

WmlHln

.

(18)

(cid:80)

3.3. Accelerating Inference
Paisley & Carin (2009) observed that if Eq[θl] becomes
small for some component l, then we can safely skip the
updates for the variational parameters associated with that
(In our experiments we used 60 dB below
component.
Eq[θl] as a threshold.) This heuristic allows the use of
large truncation levels L (yielding a better approximation to
an inﬁnite gamma process) without incurring too severe a
performance penalty. The ﬁrst few iterations will be expen-
sive, but the algorithm will require less time per iteration as
it becomes clear that only a small number of components
(relative to L) are needed to explain the data.

l

4. Evaluation
We conducted several experiments to assess the decompo-
sitions provided by the GaP-NMF model. We tested GaP-
NMF’s ability to recover the true parameters used to gener-
ate a synthetic spectrogram, compared the marginal likeli-
hoods of real songs under GaP-NMF to the marginal likeli-
hoods of those songs under a simpler version of the model,
evaluated GaP-NMF’s ability to predict held-out data with
a bandwidth expansion task, and evaluated GaP-NMF’s
ability to separate individual notes from mixed recordings.
We compared GaP-NMF to two variations on the same
model:
Finite Bayesian model. This is a ﬁnite version of the GaP-
NMF model ﬁt using the same variational updates but with-
out the top-level gain parameters θ. This simpler model’s
generative process is

Wmk ∼ Gamma(a, ac); Hkn ∼ Gamma(b, b);

Xmn ∼ Exponential((cid:80)

k WmkHkn),

(19)
where k ∈ {1, . . . , K} and the model order K is chosen
a priori. The hyperparameters a, b, and c are set to the
same values as in the GaP-NMF model in all experiments.
We will refer to this model as GIG-NMF, for Generalized
Inverse-Gaussian Nonnegative Matrix Factorization.
Finite non-Bayesian model. This model ﬁts W and H
to maximize the likelihood in equation 1. F´evotte et al.
(2009) derive iterative multiplicative updates to maximize
this likelihood, calling the resulting algorithm Itakura-Saito
Nonnegative Matrix Factorization (IS-NMF).
We also compared GaP-NMF to the two nonnegative ma-
trix factorization (NMF) algorithms described by Lee &
Seung (2001). Both of these algorithms also attempt to ap-

Bayesian Nonparametric Matrix Factorization for Recorded Music

proximately decompose the spectrogram X into an M by
K matrix W and a K by N matrix H so that X ≈ W H.
The ﬁrst algorithm, which we refer to as EU-NMF, mini-
mizes the sum of the squared Euclidean distances between
the elements of X and W H. The second algorithm, which
we refer to as KL-NMF, minimizes the generalized KL-
divergence between X and W H. KL-NMF (and its exten-
sions) in particular is widely used to analyze audio spectro-
grams (e.g. Smaragdis & Brown, 2003; Bansal et al., 2005).
We focus on approaches that explain power spectrograms
in terms of components that can be interpreted as audio
power spectra. Other approaches may be useful for some
tasks, but they do not decompose mixed audio signals into
their component sources. This requirement excludes, for
example, standard linear Gaussian factor models, whose
latent factors cannot be interpreted as audio spectra unless
audio signals are allowed to have negative power.
We normalized all spectrograms to have a maximum value
of 1.0. (The high probability densities in our experiments
result from low-power bins in the spectrograms.) To avoid
numerical issues, we forced the values of the spectrograms
to be at least 10−8, 80 dB below the peak value of 1.0.
In all experiments, we initialized the variational parameters
ρ for each W , H, and θ with random draws from a gamma
distribution with shape parameter 100 and inverse-scale pa-
rameter 1000, the variational parameters τ to 0.1, and each
γ(W )
mk = a, γ(H)
k = α/K. This yields a dif-
fuse and smooth initial variational posterior, which helped
avoid local optima. We ran variational inference until the
variational bound increased by less than 0.001%. The GIG-
NMF and IS-NMF algorithms were optimized to the same
criterion. KL-NMF and EU-NMF were iterated until their
cost functions decreased by less than 0.01 and 0.001, re-
spectively. (We found no gains in performance from letting
EU-NMF or KL-NMF run longer.) All algorithms were
implemented in MATLAB2.
We found GaP-NMF to be insensitive to the choice of α,
and so we set α = 1 in all reported experiments.

kn = b, and γ(θ)

4.1. Synthetic Data

We evaluated the GaP-NMF model’s ability to correctly
discover the latent bases that generated a matrix X, and
how many such bases exist. To test this, we ﬁt GaP-NMF
to random matrices X drawn according to the process:

Wmk ∼ Gamma(0.1, 0.1);
Hkn ∼ Gamma(0.1, 0.1);

Xmn ∼ Exponential((cid:80)

k WmkHkn),

(20)

2MATLAB code

http://www.cs.princeton.edu/˜mdhoffma

for GaP-NMF

is

available

at

Figure 1. True synthetic bases (left) and expected values under the
variational posterior of the nine bases found by the model (right).
Brighter denotes more active. The 36-dimensional basis vectors
are presented in 6 × 6 blocks for visual clarity.
where m ∈ {1, . . . , M = 36}, n ∈ {1, . . . , N = 300},
k ∈ {1, . . . , K} for K = 9.
We ran variational inference with the truncation level L
set to 50, and hyperparameters α = 1, a = b = 0.1,
c = 1/ ¯X (where ¯X is the mean of X). After convergence,
only nine of these components were associated with the ob-
served data. (The smallest element of θ associated with one
of these nine components was 0.06, while the next largest
element was 2.4 × 10−8). Figure 1 shows that the latent
components discovered by the model correspond closely to
those used to generate the data.

4.2. Marginal Likelihood

We want to evaluate the ability of GaP-NMF to choose a
good number of components to model recorded music. To
determine a “good” number of components, we use varia-
tional inference to ﬁt GIG-NMF with various orders K and
examine the resulting variational bounds on the marginal
log-likelihood log p(X|a, b, c).
As above, we set the prior parameters for the GaP-NMF
model to α = 1, a = b = 0.1, and c = 1/ ¯X. We set the
prior parameters for the simpliﬁed model to a = b = 0.1
and c = 1/ ¯X. The value of 0.1 for a and b was chosen
because it gave slightly better bounds than higher or lower
values. The results were not very sensitive to α.
We computed power spectrograms from three songs: Pink
Moon by Nick Drake, Funky Kingston by Toots and the
Maytals, and a clip from the Kreutzer Sonata by Ludwig
van Beethoven. These analyses used 2048-sample (46 ms)
Hann windows with no overlap, yielding spectrograms of
1025 frequency bins by 2731, 6322, and 2584 time win-
dows, respectively. We ﬁt variational posteriors for GaP-
NMF and GIG-NMF, conditioning on these spectrograms.
We used a truncation level L of 100 for the nonparametric
model, and values of K ranging from 1 to 100 for the ﬁnite
GIG-NMF model.
The computational cost of ﬁtting the GaP-NMF model was
lower than the cost of ﬁtting GIG-NMF with K = 100
(thanks to the accelerated inference trick in section 3.3),

Bayesian Nonparametric Matrix Factorization for Recorded Music

Figure 2. Left: Bounds on log p(X|prior) for the nonparametric GaP-NMF model and its parametric counterpart GIG-NMF with differ-
ent numbers of latent components K. Ticks on the horizontal lines showing the bound for the GaP-NMF model indicate the number of
components K used to explain the data. For all three songs the values of K chosen by GaP-NMF are close to the optimal value of K for
the parametric model. Right: Geometric mean of the likelihood assigned to each censored observation by the nonparametric, ﬁnite, and
unregularized models. Ticks again indicate the number of components K used to explain the data. The unregularized models overﬁt.
EU-NMF performs badly, with likelihoods orders of magnitude lower than the other models.

and much lower than the cost of repeatedly ﬁtting GIG-
NMF with different values of K. For example, on a single
core of a 2.3 GHz AMD Opteron 2356 Quad-Core Proces-
sor, ﬁtting the 100-component GIG-NMF model to Pink
Moon took 857 seconds, while ﬁtting the GaP-NMF model
to the same song took 645 seconds.
The results are summarized in ﬁgure 2 (left). The GaP-
NMF model used 50, 53, and 38 components to explain the
spectrograms of Funky Kingston, the Kreutzer Sonata, and
Pink Moon respectively. In each case the value of K chosen
by GaP-NMF was close to the best value of K tested for the
GIG-NMF model. This suggests that GaP-NMF performs
automatic order selection as well as the more expensive ap-
proach of ﬁtting multiple ﬁnite-order models.

4.3. Bandwidth Expansion

One application of statistical spectral analysis is band-
width expansion, the problem of inferring what the high-
frequency content of a signal is likely to be given only the
low-frequency content of the signal (Bansal et al., 2005).
This task has applications to restoration of low-bandwidth
audio and lossy audio compression. This is a missing data
problem. We compared the ability of different models and
inference algorithms to predict the held-out data.
We computed a power spectrogram from 4000 1024-
sample (23 ms) Hann windows taken from the middles
of the same three songs used to evaluate marginal like-
lihoods: Funky Kingston, the Kreutzer Sonata, and Pink
Moon. For each song, this yielded a 513 × 4000 spec-
trogram X describing 93 seconds of the song. We ran
ﬁve-fold cross-validation to compare GaP-NMF’s predic-
tions of the missing high-frequency content to those of
GIG-NMF, EU-NMF, and IS-NMF. (It is more difﬁcult to

evaluate KL-NMF’s ability to predict missing data, since it
does not correspond to a probabilistic model of continuous
data.) We divided each spectrogram into ﬁve contiguous
800-frame sections. For each fold, we censored the top two
octaves (i.e., the top 384 out of 513 frequency bins) of one
of those sections. We then predicted the values in the cen-
sored bins based on the data in the uncensored bins.
The prior hyperparameters for the Bayesian models were
set to a = b = 1, c = 1/ ¯X, and α = 1 (for GaP-NMF).
We chose a higher value for a and b for this experiment
since stronger smoothing can improve the models’ ability
to generalize to held-out data.
For each ﬁt model, we computed an estimate Xpred
mn of each
missing value Xmiss
mn . For the models ﬁt using variational
inference, we used the expected value of the missing data
under the variational posterior q, Eq[Ep[Xmiss
mn ]]. For the
GaP-NMF model, this expectation is

mn = Eq[Ep[Xmiss
Xpred

Eq[θkWmkHkn],

and for the GIG-NMF model it is

mn ]] =(cid:80)
mn ]] =(cid:80)

k

mn = Eq[Ep[Xmiss
Xpred

Eq[WmkHkn].
IS-NMF and EU-NMF we predicted Xpred

k

mn =

(cid:80)

For

k WmkHkn.

To evaluate the quality of ﬁt for IS-NMF, GaP-NMF, and
GIG-NMF, we compute the likelihood of each unobserved
element Xmiss
mn under an exponential distribution with mean
Xpred
mn . To evaluate EU-NMF, we ﬁrst compute the mean
squared error of the estimate of the observed data σ2 =
Mean[(Xobs − [W H]obs)2]. We then compute the like-
lihood of each unobserved element Xmiss
mn under a normal
distribution with mean Xpred

mn and variance σ2.

Klog p(X)87000000875000008800000088500000890000008950000090000000396000003980000040000000402000004040000040600000408000004350000044000000445000004500000020406080100Funky KingstonKreutzer SonataPink MoonKGeometric Mean Probabilityof Hidden Data05000001000000150000020000000e+002e+064e+066e+068e+061e+070e+001e+062e+063e+064e+0620406080100Funky KingstonKreutzer SonataPink MoonmodelGaP-NMFEU-NMFGIG-NMFIS-NMFBayesian Nonparametric Matrix Factorization for Recorded Music

Figure 2 (right) plots the geometric mean of the likelihood
of each unobserved element of X for the nonparametric
model and for models ﬁt with different numbers of com-
ponents K. The Bayesian models do very well compared
with the unregularized models, which overﬁt badly for any
number of components K greater than 1. GaP-NMF used
fewer components to explain the songs than in the previous
experiment, which we attribute to the stronger smoothing,
smaller number of observations, and smaller window size.

4.4. Blind Monophonic Source Separation

GaP-NMF can also be applied to blind source separation,
where the goal is to recover a set of audio signals that
combined to produce a mixed audio signal. For exam-
ple, we may want to separate a polyphonic piece of music
into notes to facilitate transcription (Smaragdis & Brown,
2003), denoising, or upmixing (F´evotte et al., 2009).
The GaP-NMF model assumes that the audio signal is a
linear combination of L sources (some of which have ex-
tremely low gain). Given the complex magnitude spectro-
gram X c of the original audio and an estimate of the model
parameters W , H, and θ, we can compute maximum-
likelihood estimates of the spectrograms of the L unmixed
sources using Wiener ﬁltering (F´evotte et al., 2009):

ˆXlmn = X c

mn

(cid:80)

θlWmlHln

i∈{1,...,L} θiWmiHin

,

(21)

where ˆXlmn is the estimate of the complex magnitude
spectrum of the lth source at time n and frequency n. We
can invert these spectrograms to obtain estimates of the au-
dio signals that are combined in the mixed audio signal.
We evaluated GaP-NMF’s ability to separate signals from
two synthesized pieces of music. We used synthetic music
rather than live performances so that we could easily isolate
each note. The pieces we used were a randomly generated
four-voice clarinet piece using 21 unique notes, and a two-
part Bach invention synthesized using a physical model of
a vibraphone using 36 unique notes.
We compared the Signal-to-Noise Ratios (SNRs) of the
separated tracks for the GaP-NMF model with those ob-
tained using GIG-NMF, IS-NMF, EU-NMF, and KL-NMF.
For the ﬁnite models we also used Wiener ﬁltering to sepa-
rate the tracks, dropping θ from equation 21.
The models do not provide any explicit information about
the correspondence between sources and notes. To decide
which separated signal to associate with which note, we
adopt the heuristic of assigning each note to the component
k whose gain signal H k has the highest correlation with the
power envelope of the true note signal. We only consider
the V components that make the largest contribution to the
mixed signal, where V is the true number of notes.

Figure 3. Average Signal-to-Noise Ratios (SNRs) across notes in
the source separation task. Approaches based on the exponential
likelihood model do well, EU-NMF and KL-NMF do less well.
Ticks on the horizontal lines showing GaP-NMF’s performance
denote the ﬁnal number of components K used to explain the data.

Figure 3 shows the average SNRs of the tracks correspond-
ing to individual notes for each piece. The approaches
based on the exponential likelihood model do comparably
well. The KL-NMF and EU-NMF models perform con-
siderably worse, and are sensitive to the model order K.
GaP-NMF decomposed the clarinet piece into 34 compo-
nents, and the Bach invention into 42 components. In both
cases, some of these components were used to model the
temporal evolution of the instrument sounds.

5. Related Work
GaP-NMF is closely related to recent work in Bayesian
nonparametrics and probabilistic interpretations of NMF.

Bayesian Nonparametrics Most of the literature on
Bayesian nonparametric latent factor models focuses on
conjugate linear Gaussian models in conjunction with the
Indian Buffet Process (IBP) (Grifﬁths & Ghahramani,
2005) or the Beta Process (BP) (Thibaux & Jordan, 2007),
using either MCMC or variational methods for posterior
inference (e.g. Doshi-Velez et al., 2009; Paisley & Carin,
2009)). (An exception that uses MCMC in non-conjugate
inﬁnite latent factor models is (Teh et al., 2007).)
A standard linear Gaussian likelihood model is not appro-
priate for audio spectrogram data, whereas the exponential
likelihood model has theoretical justiﬁcation and gives in-
terpretable components. The nonlinearity and lack of con-
jugacy of the exponential likelihood model make inference
using an IBP or BP difﬁcult. Our use of a gamma process
prior allows us to derive an inﬁnite latent factor model that
is appropriate for audio spectrograms and permits a simple
and efﬁcient variational inference algorithm.

Probabilistic NMF F´evotte & Cemgil

(2009) sug-

KSignal-to-Noise Ratio-20-15-10-50-10-50520406080100120140Bach InventionClarinet PiecemodelGaP-NMFEU-NMFGIG-NMFIS-NMFKL-NMFBayesian Nonparametric Matrix Factorization for Recorded Music

gest the outline of a variational inference algorithm for
Itakura-Saito NMF based on the space-alternating gener-
alized expectation-maximization algorithm in F´evotte et al.
(2009). This approach introduces K×M×N complex hid-
den variables whose posteriors must be estimated. In our
informal experiments, this gave a much looser variational
bound, much longer convergence times, and a less ﬂexible
approximate posterior than the variational inference algo-
rithm presented in this paper.
Our approach of weighting the contribution of each com-
ponent k by a parameter θk resembles the strategy of Au-
tomatic Relevance Determination (ARD), which has been
used in a Maximum A Posteriori (MAP) estimation algo-
rithm for a different NMF cost function (Tan & F´evotte,
2009). Though similar in spirit, this ARD approach is less
amenable to fully Bayesian inference.

6. Discussion
We developed the GaP-NMF model, a Bayesian nonpara-
metric model capable of determining the number of la-
tent sources needed to explain an audio spectrogram. We
demonstrated the effectiveness of the GaP-NMF model on
several problems in analyzing and processing recorded mu-
sic. Although this paper has focused on analyzing music,
GaP-NMF is equally applicable to other types of audio,
such as speech or environmental sounds.

Acknowledgments
We thank the reviewers for their helpful observations and
suggestions. David M. Blei is supported by ONR 175-
6343, NSF CAREER 0745520.

References
Abdallah, S.A. and Plumbley, M.D. Polyphonic music tran-
scription by non-negative sparse coding of power spec-
tra. In Proc. 5th Int’l Conf. on Music Information Re-
trieval (ISMIR), pp. 10–14, 2004.

Bansal, D., Raj, B., and Smaragdis, P. Bandwidth expan-
sion of narrowband speech using non-negative matrix
In Proc. 9th European Conf. on Speech
factorization.
Communication and Technology, 2005.

Blei, D. and Jordan, M. Variational methods for the Dirich-
let process. In Proc. 21st Int’l Conf. on Machine Learn-
ing, 2004.

Blei, D. and Lafferty, J. Correlated topic models.

In
Advances in Neural Information Processing Systems 18
(NIPS) 18, pp. 147–154. MIT Press, 2006.

Braun, M. and McAuliffe, J.

Variational

inference

for large-scale models of discrete choice.
(0712.2526), 2008.

arXiv,

Doshi-Velez, F., Miller, K.T., Van Gael, J., and Teh,
Y.W. Variational inference for the indian buffet process.
In Proc. 13th Int’l Conf. on Artiﬁcial Intelligence and
Statistics, pp. 137–144, 2009.

F´evotte, C. and Cemgil, A.T. Nonnegative matrix factor-
izations as probabilistic inference in composite models.
In Proc. 17th European Signal Processing Conf. (EU-
SIPCO), Glasgow, Scotland, 2009.

F´evotte, C., Bertin, N., and Durrieu, J.L. Nonnegative ma-
trix factorization with the Itakura-Saito divergence: With
application to music analysis. Neural Computation, 21
(3):793–830, 2009.

Grifﬁths, T.L. and Ghahramani, Z.

Inﬁnite latent feature
In Advances in
models and the indian buffet process.
Neural Information Processing Systems 17 (NIPS), pp.
475–482. MIT Press, 2005.

Jordan, M., Ghahramani, Z., Jaakkola, T., and Saul, L. In-
troduction to variational methods for graphical models.
Machine Learning, 37:183–233, 1999.

Jørgenson, Bent. Statistical properties of the generalized
Springer-Verlag, New

inverse-Gaussian distribution.
York, 1982.

Kingman, J.F.C. Poisson processes. Oxford University

Press, USA, 1993.

Lee, D.D. and Seung, H.S. Algorithms for non-negative
matrix factorization. In Advances in Neural Information
Processing Systems 13 (NIPS), pp. 556–562. MIT; 1998,
2001.

Paisley, J. and Carin, L. Nonparametric factor analysis with
beta process priors. In Proc. 26th Int’l Conf. on Machine
Learning, 2009.

Smaragdis, P. and Brown, J.C. Non-negative matrix fac-
torization for polyphonic music transcription. In IEEE
Workshop on Applications of Signal Processing to Audio
and Acoustics, pp. 177–180, 2003.

Tan, V.Y.F. and F´evotte, C. Automatic relevance deter-
In Proc.
mination in nonnegative matrix factorization.
Workshop on Signal Processing with Adaptative Sparse
Structured Representations (SPARS09), 2009.

Teh, Y., Gorur, D., and Ghahramani, Z. Stick-breaking con-
struction for the Indian buffet process. In 11th Conf. on
Artiﬁcial Intelligence and Statistics, 2007.

Thibaux, R. and Jordan, M. Hierarchical beta processes
and the Indian buffet process. In 11th Conf. on Artiﬁcial
Intelligence and Statistics, 2007.

