Abstract

Topic models have been used successfully for a va-
riety of problems, often in the form of application-
specic extensions of the basic Latent Dirichlet
Allocation (LDA) model. Because deriving these
new models in order to encode domain knowledge
can be difcult and time-consuming, we propose
the Foldall model, which allows the user to spec-
ify general domain knowledge in First-Order Logic
(FOL). However, combining topic modeling with
FOL can result in inference problems beyond the
capabilities of existing techniques. We have there-
fore developed a scalable inference technique using
stochastic gradient descent which may also be use-
ful to the Markov Logic Network (MLN) research
community. Experiments demonstrate the expres-
sive power of Foldall, as well as the scalability of
our proposed inference method.

1 Introduction
Building upon the success of Latent Dirichlet Allocation
(LDA) [Blei et al., 2003], a large number of latent-topic-
model variants have been proposed for many application do-
mains. Often, these variants are custom-built by incorpo-
rating external knowledge specic to the target domain, see
e.g., [Wang et al., 2009; Gerrish and Blei, 2010]. However,
deriving a custom latent topic model, along with an ef-
cient inference scheme, requires machine-learning expertise
not common among application domain experts. Further-
more, such effort must be duplicated whenever a new type of
domain knowledge is to be used, preventing domain experts
from taking full advantage of topic modeling approaches.
Previous work has integrated word-level knowledge into topic
models for both batch [Andrzejewski et al., 2009; Petter-
son et al., 2010] and interactive [Hu et al., 2011] settings,
but these approaches do not incorporate constraints involving
documents, topics, or general side information.
The main contribution of this paper is Foldall (First-Order
Logic latent Dirichlet ALLocation), a framework for incorpo-
rating general domain knowledge into LDA. A domain expert
only needs to specify her domain knowledge as First-Order
Logic (FOL) rules, and Foldall will automatically incorpo-
rate them into LDA inference to produce topics shaped by

Mark Craven
University of

WisconsinMadison

craven@biostat.wisc.edu

Benjamin Recht

University of

WisconsinMadison
brecht@cs.wisc.edu

both the data and the rules. This approach enables domain
experts to focus on high-level modeling goals instead of the
low-level issues involved in creating a custom topic model.
In fact, some previous topic model variants can be expressed
within the Foldall framework.
Internally, Foldall converts the FOL rules into a Markov
Random Field, and combines it with the LDA probabilistic
model. As such, it can be viewed as an instance of a Hy-
brid Markov Logic Network (HMLN) [Wang and Domingos,
2008], which is itself a generalization of a Markov Logic Net-
work (MLN) [Richardson and Domingos, 2006]. However,
existing inference schemes developed for HMLNs and MLNs
do not scale well for typical topic modeling applications. An-
other contribution of this paper is a scalable stochastic op-
timization algorithm for Foldall, which is potentially useful
for general MLN research, too.
2 The Foldall Framework
We now briey review the standard LDA model [Blei et al.,
2003]. While we describe variables in terms of text (e.g.,
words and documents), note that both LDA and Foldall are
general and can be applied to non-text data as well. Let w =
w1 . . . wN be a text corpus containing N tokens, with d =
d1 . . . dN being the document indices of each word token and
z = z1 . . . zN being the hidden topic assignments of each
token. Each topic t = 1 . . . T is represented by a multinomial
t over a W -word-type vocabulary. The s have a Dirichlet
prior with parameter . Likewise, each document j = 1 . . . D
is associated with a multinomial j over topics, with another
Dirichlet prior with parameter . The generative model is
P (w, z, ,  | , , d) 

(cid:32) T(cid:89)

(cid:33) D(cid:89)

(cid:32) N(cid:89)

(cid:33)

p(t|)

p(j|)

zi(wi)di(zi)

(1)

t

j

i

where zi(wi) is the wi-th element in vector zi, and di(zi)
is the zi-th element in vector di. One important goal of topic
modeling is to estimate the topics  given a corpus (w, d).
The key to our Foldall framework is to allow domain
knowledge, specied in FOL, to inuence the values of the
hidden topics z, indirectly inuencing  and . FOL provides
a powerful and exible way to specify domain knowledge.
For example, an analyst working on a congressional debate

i : W(i, taxes)  Speaker(di, Rep)  Z(i, 77),

corpus where each speech is a document may specify the rule
(2)
which states that for any word token wi = taxes that ap-
pears in a speech by a Republican, the corresponding latent
topic should be zi = 77. We briey review some FOL con-
cepts [Domingos and Lowd, 2009] for Foldall.

We dene logical predicates for each of the standard LDA
variables, letting Z(i, t) be true if the hidden topic zi = t,
and false otherwise. Likewise, W(i, v) and D(i, j) are true if
wi = v and di = j, respectively. In addition, Foldall can
incorporate other variables beyond those modeled by stan-
dard LDA. In our previous example, a domain expert denes
a predicate Speaker(di, Rep), which is true if the speaker
for document di is a member of the Republican political party.
We use o to collectively denote these other observed variables
and their corresponding logical predicate values.

The domain expert species her background knowledge in
the form of a weighted FOL knowledge base using these pred-
icates: KB = {(1, 1), . . . , (L, L)}. The KB is in Con-
junctive Normal Form, consisting of L pairs where each rule
l is an FOL clause, and l  0 is its weight which the do-
main expert sets to represent the importance of l. Thus, in
