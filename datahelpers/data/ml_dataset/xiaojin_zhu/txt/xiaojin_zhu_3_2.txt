NAACLHLT2012The2012ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguageTechnologiesProceedingsoftheConferenceJune3-8,2012Montr´eal,Canadaii

ProductionandManufacturingbyOmnipress,Inc.2600AndersonStreetMadison,WI53707USAPLATINUMSPONSORS:GOLDSPONSORS:SILVERSPONSORS:iii

BRONZESPONSORS:SUPPORTERS:c(cid:13)2012TheAssociationforComputationalLinguisticsOrdercopiesofthisandotherACLproceedingsfrom:AssociationforComputationalLinguistics(ACL)209N.EighthStreetStroudsburg,PA18360USATel:+1-570-476-8006Fax:+1-570-476-0860acl@aclweb.orgISBN978-1-937284-20-6/1-937284-20-4iv

Preface:GeneralChairItismypleasureandhonortowelcomeyoutothe2012NAACLHumanLanguageTechnologiesConferenceinbeautifulMontreal,Canada(inourCanadianspirit,letmeadd,bienvenue!).Theorganizingcommitteehasputinagreatdealofeffortontheprogramsintheupcomingweek.Ihopeyouwillenjoywhattheconferencehastooffer!Thecoreoftheconferenceisthe3-daymaintechnicalprogramconsistingoforalandposterpresentationsofpapers,keynoteaddresses,andanovel“NLPIdol”session.IamveryfortunatetohavethiskeypieceoftheconferenceinthehandsofEricFosler-Lussier,EllenRiloff,andSrinivasBangalore,threeextremelydedicatedandcapableprogramco-chairs.Eric,Ellen,andSrinimanagedthewholeprocesssowellthatIrealizedearlyonintheconference-planningprocessthatIcouldleavethisimportantpartoftheconferenceentirelyintheirhands.Themaintechnicalprogramthatyouwillseehereisthefruitoftheirlabor,withtheassistanceof22areachairsandhundredsofpeoplewhoreviewedorsubmittedpapers,orboth.Thankyouall!TobeheldinconjunctionwiththemainconferencepostersessionistheDemonstrationsprogram.IwouldliketothankAriaHaghighiandYaserAl-Onaizanforselectingadozeninterestingsystemdemosaspartofthisprogram.AnothercomponentofthepostersessionispresentationsfromtheStudentResearchWorkshop.Iamgratefultoco-chairsRivkaLevitanandMyleOttaswellasfacultyadvisorsRogerLevyandAniNenkovaforidentifyingourrisingstars,andfororganizingaroundtablediscussiononpeerreviewstandardsandpracticesforallparticipants.FortheTutorialsprogram,IthankJacobEisensteinandRaduFlorianformanagingthesubmissionandreviewingprocesstoidentify8diverseandinterestinghalf-daytutorialsforpresentation.Continuinganewtradition,theWorkshopsprogramiscoordinatedamongtheEACL,NAACL,andACLconferences,withajointsubmissionandreviewprocess.IamindebtedtotheNAACLWorkshopco-chairsColinCherryandMonaDiabwhoworkedaspartofthe*ACLworkshopcommitteetoselectastrongsuiteof16workshopsforNAACLandputinsigniﬁcanteffortworkingwiththeworkshoporganizerstobringthewholeprogramtogether.Thankyoualsototheorganizersofthe*SEMconference,EnekoAgirre,JohanBos,andMonaDiab,forchoosingtocollocatetheirﬁrstconferencewithNAACLHLT.TheUSBkeythatcontainstheentireproceedingsofthisconferenceistheproductionofPublicationsco-chairsNizarHabashandWilliamSchuler.Specialthanksgotothemfortheireffortsinassemblingallthematerialsandworkingtokeepeveryoneonschedulefortheproductionoftheproceedings.IwouldliketothankPublicitychairSmarandaMuresanforhereffortsinhelpingtoattractsubmissionsandattendeesfromvariouscommunities,andExhibitschairJoelTetreaultforhelpingtoarrangeshow-and-tellspaceforoursponsorsandotherexhibitors.ThewebsitethatyouundoubtedlyconsultedcountlesstimesbeforegettingtoMontrealhasbeendesignedandmaintainedbyDirkHovy.AheartfeltthankyoutoDirkfortakingonthistask,andtoLucyRoarkfordesigningtheNAACLHLT2012logo.Fortheﬁnancialaspectsoftheconference,Iwouldliketothankourcorporate,academic,andgovernmentsponsorsfortheircontributions,andourNorthAmericansponsorshipco-chairsMichaelGamonandPatrickPantelfortheroletheyplayed.TheNAACLexecutiveboardhasbeenverysupportiveandhelpfulduringtheplanningofthisv

conference.Iamverygratefulfortheguidanceandsuggestionstheboardmembers,especiallythetwochairsRebeccaHwaandChrisCallison-Burch,haveprovided.Finally,theconferencewouldnotbehappeningwithouttheexpertiseanddedicationofPriscillaRasmussen,ACL’sbusinessmanagerandthisconference’sLocalArrangementsChair.WorkingwithlocaladvisorycommitteemembersSabineBerglerandGuyLapalme,Priscillaistakingonlocalarrangementsresponsibilitiesusuallysplitamongmultiplefacultymembersanddoingafabulousjob.Withhervastarrayofexperienceineveryaspectoforganizinga*ACLconference,Priscillahasbeenmygo-topersonandlife-saverforthelast9months.Ijustwanttosay,thankyouPriscilla,it’sbeengreatworkingwithyou!Forthoseofyouwhomadeitthisfar,here’saconference-appropriatepunforyouramusement.Q:WhatdolinguistscallSanta’selves?A:SubordinatedClauses.Enjoytheconference!JenniferChu-CarrollIBMT.J.WatsonResearchCenterNAACLHLT2012GeneralChairvi

Preface:ProgramChairsWelcometoNAACLHLT2012!Thisyear’sconferencebringstoMontrealanexcitingarrayofworkrangingacrossthehumanlanguagetechnologydisciplines.Themainconferencefeaturesbothoralandpostersessionsforfullandshortpapers;themainconferenceisprecededbyeighttutorialsandfollowedbysixteenseparateworkshopsaswellastheFirstJointConferenceonLexicalandComputationalSemantics(*SEM).Withinthemainprogram,wearepleasedtoannounceseveralspecialevents.Wehavetwoexcellentinvitedspeakersstartingofftwodaysofourprogram.OnMondaymorning,wewillhearfromEduardHovy,DirectoroftheHumanLanguageTechnologyGroup,InformationSciencesInstituteoftheUniversityofSouthernCalifornia,whowillspeakabout“ANewSemantics:MergingPropositionalandDistributionalInformation.”Wednesdaymorning’sinvitedspeakerisJamesW.Pennebaker,CentennialLiberalArtsProfessorandChairofPsychologyattheUniversityofTexasatAustin;histalkisentitled“A,is,I,and,the:Howoursmallestwordsrevealthemostaboutwhoweare.”Twospecialdiscussion-orientedeventsarealsoplanned—onMondayduringthelunchhour,theStudentResearchWorkshopwillbehostingapaneldiscussionon“ReviewingPractices,”whichisopentoallconferenceparticipants.WewillalsohaveaspecialfuneventTuesdayafternooncalled“NLPIdol,”whereseniorresearcherswilltrytoconvinceapanelofjudges(andtheaudience!)thatweshouldbepayingattentiontoaforgottenlineofresearchfromthepastbypresentingpapers“pluckedfromobscurity.”Thisyear,196fullpapersweresubmittedtotheconference,with61papersbeingaccepted(a31%acceptancerate);105shortpapersweresubmitted,with36acceptances(34%acceptance).Thebreakdownofpapersbyareaofsubmission(basedonauthordesignation)andacceptances(inparentheses)wereasfollows:Author-assignedPaperCategory#FullPapers#ShortPapersDiscourse,Dialogue,andPragmatics14(7)8(3)DocumentCategorization/TopicClustering13(1)7(2)End-to-endLanguageProcessingSystems4(3)4(3)InformationExtraction17(4)6(0)InformationRetrievalandQuestionAnswering7(3)5(0)LanguageResources,NovelEvaluationMethods8(5)8(3)MachineLearningforLanguageProcessing21(11)8(2)MachineTranslation26(8)18(5)PhonologyandMorphology,WordSegmentation7(1)5(3)Semantics25(5)7(3)SentimentAnalysisandOpinionMining12(1)8(3)SocialMediaAnalysisandProcessing7(2)3(1)SpokenLanguageProcessing8(2)4(2)SummarizationandGeneration8(1)5(3)SyntacticTaggingandChunking3(1)1(0)SyntaxandParsing16(6)8(3)vii

(Aspartofthereviewandassignmentprocess,someofthepaperswererecategorizedbytheprogramchairs,sotheacceptancenumbersbasedonauthorcategorizationdonotnecessarilymatchtheassignmentofpapersintheprogram.)Theoralandposterslotswereallocatedbasedonthesuggestionsofreviewersandareachairsforappropriatepresentationstyle;bothpresentationtypescarrythesamestatus.Fourteenfullandsevenshortpaperswillbepresentedduringaneveningpostersession,withbuffetdinner,inconjunctionwiththeDemosessionandtheStudentResearchWorkshopposters.Precedingthepostersessionwillbearepriseoftheone-minutemadnesssessionintroducedatNAACLHLT2010,inwhichattendeescanseeanoverviewoftheposterpresentations.OralsessionswillbeheldinthreeparallelsessionsonMondayandWednesday,withfourparallelsessionsonTuesday.Wehaveexpandedpresentationtimesto30-minuteslotsforfullpapers,and20-minuteslotsforshortpapers,tofacilitatemorediscussionofpapers.Weareexcitedthattheconferenceisabletopresentsuchadynamicarrayofpapers,andwouldliketothanktheauthorsfortheirﬁnework.Thereviewprocessfortheconferencewasdouble-blind,andincludedanauthorresponseperiodforclarifyingreviewers’questions.Wewereverypleasedtohavetheassistanceof476reviewersindecidingtheprogram.Weareespeciallythankfulforthereviewerswhospenttimereadingtheauthorresponsesandengagingotherreviewersinthediscussionboard.Constructingtheprogramwouldnothavebeenpossiblewithout22excellentareachairsformingtheSeniorProgramCommittee:RobertoBasili,GuiseppeCarenini,YejinChoi,ChristineDoran,JasonEisner,GeorgeFoster,RoxanaGirju,HengJi,SadaoKurohashi,MattLease,DianeLitman,DeepakRavichandran,GiuseppeRiccardi,RichardRose,GiorgioSatta,FeiSha,SuzanneStevenson,DavidTraum,ScottYih,LukeZettlemoyer,BowenZhou,andJerryZhu.Areachairswereresponsibleforrecruitingreviewers,managingpaperassignments,collatingreviewerresponses,handlingpapersforotherareachairsorprogramchairswhohadconﬂictsofinterest,makingrecommendationsforpaperacceptanceorrejection,andnominatingbestpapersfromtheirareas.Weareverygratefulforthetimeandenergythattheyhaveputintotheprogram.TheBestPaperAwardsessionstartsoffTuesdaymorning;thisyearwearepleasedtopresentthreeawards—forbestfullpaper,bestshortpaper,andbeststudentpaper.Thisyear’swinnersare:•BestFullPaperAward:VinePruningforEfﬁcientMulti-PassDependencyParsing,AlexanderRushandSlavPetrov•BestShortPaperAward:Trait-BasedHypothesisSelectionForMachineTranslation,JacobDevlinandSpyrosMatsoukas•IBMBestStudentPaperAward:Cross-lingualWordClustersforDirectTransferofLinguisticStructure,OscarT¨ackstr¨om,RyanMcDonald,JakobUszkoreitWewouldliketothankreviewersandareachairsfornominatingthebestpapercandidates.Asubsetofareachairswithexpertiseintheareasofthenominatedpaperswereinvaluableinhelpingtheprogramchairsinthedecisionprocess.Inparticular,wewouldliketothankChristineDoran,JasonEisner,GeorgeFoster,DianeLitman,GiorgioSatta,LukeZettlemoyer,andBowenZhoufortheirassistanceviii

inthedecisionprocess.WewouldliketonotethattheBestFullPaperandIBMBestStudentPaperawardeesbothhavestudentsasﬁrstauthors.TheauthorswillbepresentedwithacertiﬁcateandcashprizeattheopeningofTuesday’ssession.WegratefullyacknowledgeIBM’ssupportfortheStudentBestPaperAward.Thereareanumberofotherpeoplethatweinteractedwithwhodeserveaheartythanksforthesuccessoftheprogram.RichGerberandtheSTARTteamatSoftconfhavebeeninvaluableforhelpinguswiththemechanicsofthereviewingprocess.NizarHabashandWilliamSchuler,aspublicationsco-chairs,havebeenveryhelpfulinassemblingtheﬁnalprogramandcoordinatingthepublicationsoftheworkshopproceedings.Thereareseveralcrucialpartsoftheoverallprogramthatweretheresponsibilityofvariouscontributors,includingRivkaLevitan,MyleOtt,RogerLevy,andAniNenkova(StudentResearchWorkshop);JacobEisensteinandRaduFlorian(TutorialChairs);ColinCherryandMonaDiab(WorkshopChairs);andAriaHaghighiandYaserAl-Onaizan(DemoChairs).WewouldalsoliketothankChrisCallison-Burch,RebeccaHwa,andtheNAACLExecutiveBoardforguidanceduringtheprocess.DirkHovywasalsoavaluableteammemberinhelpingusdisseminateinformationasWebmaster.Deservingspecialmentionistheever-unﬂappablePriscillaRasmussen,whoisdoingdoubledutythisconferenceaslocalarrangementschairandgeneralbusinessmanager.Priscillamakeseverythingsheisinvolvedwithgomoresmoothly,andwehavereliedonheradvicegreatlyduringtherun-uptotheconference.Finally,wewouldliketothankourGeneralChair,JenniferChu-Carroll,forentrustinguswiththisjob,forwalkingusthroughsomeofthemorestickymoments,andforbeingagreatsoundingboardfordifferentideas.Inparticular,herguidancewascrucialindevelopingtheconceptfortheNLPIdolsession.Wehopethatyouenjoytheconference!EricFosler-Lussier,TheOhioStateUniversityEllenRiloff,UniversityofUtahSrinivasBangalore,AT&TResearchix

GeneralChair:JenniferChu-Carroll,IBMT.J.WatsonResearchCenterProgramCo-Chairs:EricFosler-Lussier,TheOhioStateUniversityEllenRiloff,UniversityofUtahSrinivasBangalore,AT&TAreaChairs:RobertoBasili,UniversityofRome:SemanticsGuiseppeCarenini,UniversityofBritishColumbia:SummarizationandGenerationYejinChoi,StonyBrookUniversity:SentimentAnalysisandOpinionMiningChristyDoran,MITRE:LanguageResourcesNovelEvaluationMethodsJasonEisner,JohnsHopkinsUniversity:PhonologyandMorphologyWordSegmentationGeorgeFoster,NationalResearchCouncilCanada:MachineTranslationRoxanaGirjuUniversityofIllinoisUrbana-Champaign:SocialMediaAnalysisandProcessingHengJi,CityUniversityofNewYork:InformationExtractionSadaoKurohashi,KyotoUniversity:SyntacticTaggingandChunkingMattLease,UniversityofTexasAustin:InformationRetrievalandQuestionAnsweringDianeLitman,UniversityofPittsburgh:DiscourseDialogueandPragmaticsDeepakRavichandran,Google:InformationExtractionGuiseppeRicardi,UniversityofTrento:SpokenLanguageProcessingRichardRose,McGillUniversity:SpokenLanguageProcessingGiorgioSatta,UniversityofPadova:SyntaxandParsingFeiSha,UniversityofSouthernCalifornia:MachineLearningforLanguageProcessingSuzanneStevenson,UniversityofToronto:SemanticsDavidTraum,UniversityofSouthernCalifornia:End-to-endLanguageProcessingSystemsScottYih,Microsoft:MachineLearningforLanguageProcessingLukeZettlemoyer,UniversityofWashington:SyntaxandParsingBowenZhou,IBM:MachineTranslationJerryZhu,UniversityofWisconsin:DocumentCategorization/TopicClusteringLocalArrangements:Chair:PriscillaRasmussen,ACLBusinessOfﬁce,acl-AT-aclweb.orgAdvisorycommittee:SabineBergler,ConcordiaUniversityGuyLapalme,Universit´edeMontr´ealx

WorkshopsCo-Chairs:ColinCherry,NationalResearchCouncilofCanadaMonaDiab,ColumbiaUniversityTutorialsCo-Chairs:JacobEisenstein,CMURaduFlorian,IBMT.J.WatsonResearchCenterDemosCo-Chairs:AriaHaghighi,PrismaticYaserAl-Onaizan,IBMT.J.WatsonResearchCenterStudentWorkshop:Co-chairs:RivkaLevitan,ColumbiaUniversityMyleOtt,CornellUniversityFacultyAdvisors:RogerLevy,UCSDAniNenkova,UniversityofPennsylvaniaPublications:NizarHabash,ColumbiaUniversityWilliamSchuler,OSUPublicity:SmarandaMuresan,RutgersUniversityExhibits:JoelTetreault,EducationTestingServicesWebmaster:DirkHovy,USC/ISIAmericasSponsorshipCo-chairs:MichaelGamon,MicrosoftResearchPatrickPantel,MicrosoftResearchxi

ProgramCommitteeMembers:HuaAiGregoryAistCemAkkayaAfraAlishahiCeciliaO.AlmGiambattistaAmatiIonAndroutsopoulosDavidAndrzejewskiMarkArehartVictoriaArranzJavierArtilesRonArtsteinAbhishekArunJavedAslamGiuseppeAttardiNecipFazilAyanMichielBacchianiTimothyBaldwinKrisztianBalogEvaBanikMicheleBankoKenBarkerRobertoBayardoNickBelkinAnjaBelzEmilyBenderMichaelBenderskyPaulBennettEdwardBensonSabineBerglerShaneBergsmaRaffaellaBernardiStevenBethardRahulBhagatAlanW.BlackRoiBlancoJohnBlitzerBranimirBoguraevDanBohusJohanBosAlexandreBouchard-CoteGosseBoumaJordanBoyd-GraberKristyBoyerThorstenBrantsUlfBrefeldFabioBrugnaraSabineBuchholzBillByrneAoifeCahillClarieCardieAndrewCarlsonMarineCarpuatJohnCarrollFranciscoCasacubertaDiamantinoCaseiroTommasoCaselliMauroCettoloJoyceChaiYlliasChaliMing-WeiChangPi-ChuanChangEugeneCharniakCiprianChelbaBoxingChenJohnChenZhengChenWenliangChenColinCherryDavidChiangYejinChoiChristosChristodoulopoulosWeiChuPhilippCimianoStephenClarkAlexClarkMartinCmejrekShayCohenRonanCollobertSherriCondonGaoCongMichaelConnorPaulCookMarkCoreStephenCoxMarkCravenDaniloCroceAronCulottaWalterDaelemansGeraldineDamnatiHoaTrangDangDipanjanDasSajibDasguptaHalDaumeIIIAdriadeGispertRenatoDeMoriRodolfoDelmonteDinaDemner-FushmanSteveDeNeefeJohnDeNeroHongboDengNicholasDiakopoulosLauraDietzChrysanneDiMarcoXiaowenDingDeniseDiPersioDougDowneyMarkusDreyerKevinDuhChrisDyerMarcDymetmanMyroslavaDzikovskaJensEdlundKojiEguchiJacobEisensteinCharlesElkanJonathanElsasMichaElsnerAhmadEmamiHakanErdoganHuiFangFaisalFarooqAfsanehFazlyAnnaFeldmanDonghuiFengElenaFilatovaxii

KatjaFilippovaTimFininJennyFinkelDanFlickingerRaduFlorianKateForbes-RileyJenniferFosterFrancescaFrontiniMichelGalleyKuzmanGanchevKavitaGanesanJianfengGaoQinGaoAlbertGattNiyuGeDmitriyGenzelKallirroiGeorgilaPanayiotisGeorgiouMatthewGerberShaliniGhoshArnabGhoshalDanielGildeaKevinGimpelRoxanaGirjuAlﬁoGliozzoAmirGlobersonVibhavGogateYoavGoldbergSharonGoldwaterCarlosGomezRodriguezJulioGonzaloJoaoGracaAgustinGravanoNancyGreenGregoryGrefenstetteRalphGrishmanMarkusGuheIrynaGurevychNizarHabashGholamrezaHaffariDilekHakkani-TurKeithHallDavidHallOlivierHamonSandraHarabaginLisaHarperHelenHastieTimothyJ.HazenXiaodongHeMartiHearstJeffreyHeinzAurelieHerbelotSanjikaHewavitharanaSiljaHildebrandGraemeHirstBarboraHladkaHieuHoangJuliaHockenmaierMinlieHuangZhongqiangHuangJimmyHuangYangHuiNancyIdeAmyIsardAbrahamIttycheriahEmilyJamisonJingJiangRichardJohanssonHowardJohnsonMarkJohnsonKristiinaJokinenDougJonesPamelaJordanShaﬁqJotyNobuhiroKajiHiroshiKanayamaDaisukeKawaharaAnnaKazantsevaAlistairKennedyJosephKeshetTracyKingBrianKingsburyKatrinKirchhoffDietrichKlakowAlexandreKlementievPhilippKoehnRobKoelingStanleyKokAlexanderKollerGrzegorzKondrakTerryKooZornitsaKozarevaTakuKudohSandraKueblerMarcoKuhlmannRolandKuhnSethKulickShankarKumarOrenKurlandPhilippeLanglaisGuyLapalmeMirellaLapataAlonLavieMattLeaseJohnLehmannAlessandroLenciJamesLesterGregorLeuschAntonLeuskiHaiboLiShoushanLiHangLiZhifeiLiFangtaoLiMuLiFennieLiangPercyLiangElizabethLiddyDingLiuZhiyuanLiuQunLiuYangLiuYanLiuEduardoLleidaSolanoOierLopezdeLacalleAnnieLouisPengfeiLuBinLuYueLuKlausMachereyWolfgangMachereyNitinMadnaniAndreasMalettiArindamMandalGideonMannChristopherManningDanielMarcuJamesMartinxiii

AndreMartinsYuvalMartonSameerMaskeySpyrosMatsoukasTakuyaMatsuzakiMikeMaxwellJonMayJamesMayﬁeldDianaMaynardDianaMcCarthyRyanMcDonaldPaulMcNameeDanMelamedChrisMellishArulMenezesDonaldMetzlerHaitaoMiRadaMihalceaKeithJ.MillerAndriyMnihSaifMohammadBobMooreAlessandroMoschittiDragosMunteanuSmarandaMuresanGabrielMurrayMeenakshiNagarajanTetsujiNakagawaRobertoNavigliMark-JanNederhofAniNenkovaAlenaNeviarouskayaHweeTouNgAlexandruNiculescu-MizilJian-YunNieJoakimNivreStephanOepenConstantinOrasanBeatriceOshikaChristopherPalSinnoJialinPanBoPangSoo-MinPantelCecileParisChrisParisienSParthasarathyMariusPascaSiddharthPatwardhanMichaelPaulAdamPaulsAnselmoPenasMarcoPennacchiottiSlavPetrovFabioPianesiEmilyPitlerPaulPiwekThierryPoibeauJoePolifroniHoifungPoonMartinPopelAdrianPopescuMajaPopovicMattPostSameerPradhanJohnPragerMarkPrzybockiStephenPulmanMatthewPurverYanjunQiSilviaQuarteroniChrisQuirkDragomirRadevPiyushRaiOwenRambowDelipRaoAriRappoportLev-ArieRatinovSujithRaviSravanaReddyInesRehbeinEhudReiterJasonRiggleLauraRimellAlanRitterAntonioRoqueCarolynRoseAndrewRosenbergAfshinRostamizadehDanRothMartaRuizJosefRuppenhoferIreneRussoKenjiSagaeAliciaSagaeHoracioSaggionMuratSaraclarAnoopSarkarManabuSassanoHassanSawafFrankSchilderDavidSchlangenSabineSchulteimWaldeHolgerSchwenkDoniaScottFr´ed´eriqueSegondSatoshiSekineHendraSetiawanBurrSettlesWadeShenShumingShiMichelSimardVikasSindhwaniKevinSmallDavidSmithRonnieSmithBenjaminSnyderMorganSondereggerLuciaSpeciaCarolineSporlederMatthewStoneVeselinStoyanovCarloStrapparavaStephanieStrasselKristinaStriegnitzMichaelStrubeJianSuLVenkataSubramaniamAmarnagSubramanyaRichardSutcliffeCharlesSuttonHiroyaTakamuraParthaTalukdarJoergTiedemannChristophTillmannIvanTitovNorikoTomuroSaraTonelliAudreyTongxiv

CigdemToprakKristinaToutanovaVivianTsangOrenTsurYoshimasaTsuruokaGokhanTurJosephTurianOscarT¨ackstr¨omJacobUszkoreitLonnekevanderPlasIelkavanderSluisGertjanvanNoordLucyVanderwendeAshishVenugopalGuidoVetereDavidVilarAlineVillavicencioEllenVoorheesXiaojunWanStephenWanLidanWangChongWangHaifengWangNigelWardTaroWatanabeBonnieWebberFuliangWengMichaelWhiteRichardWicentowskiJasonWilliamsTheresaWilsonGuillaumeWisniewskiKamfaiWongFrankWoodDekaiWuYunqingXiaBingXiangDeyiXiongPengXuNianwenXueFanYangMuyunYangTianbaoYangRomanYangarberAlexYatesAinurYessenalinaYisongYueMurawakiYugoFranc¸oisYvonFabioMassimoZanzottoBe˜natZapirainRichardZensZhongwuZhaiChengxiangZhaiHaoZhangYingZhangMinZhangLeiZhangYiZhangTiejunZhaoBingZhaoJingZhengZhi-HuaZhouDengyongZhouGuodongZhouMingZhouJerryZhuXiaodanZhuImedZitouniChengqingZongGeoffreyZweigDiarmuid´OS´eaghdhaLiljaØvrelidTable of Contents

Multiple Narrative Disentanglement: Unraveling Inﬁnite Jest

Byron Wallace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1

Acoustic-Prosodic Entrainment and Social Behavior

Rivka Levitan, Agustin Gravano, Laura Willson, Stefan Benus, Julia Hirschberg and Ani Nenkova

11

Identifying High-Level Organizational Elements in Argumentative Discourse

Nitin Madnani, Michael Heilman, Joel Tetreault and Martin Chodorow . . . . . . . . . . . . . . . . . . . . . 20

Fast Inference in Phrase Extraction Models with Belief Propagation

David Burkett and Dan Klein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

Continuous Space Translation Models with Neural Networks

Hai-Son Le, Alexandre Allauzen and Franc¸ois Yvon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

Machine Translation of Arabic Dialects

Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas, Richard Schwartz,
John Makhoul, Omar F. Zaidan and Chris Callison-Burch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

Entity Clustering Across Languages

Spence Green, Nicholas Andrews, Matthew R. Gormley, Mark Dredze and Christopher D. Man-
ning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

Multi-Event Extraction Guided by Global Constraints

Roi Reichart and Regina Barzilay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

Reference Scope Identiﬁcation in Citing Sentences

Amjad Abu Jbara and Dragomir Radev . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80

Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement Detector for an Uncertainty-
Adaptive Spoken Dialogue System

Kate Forbes-Riley, Diane Litman, Heather Friedberg and Joanna Drummond . . . . . . . . . . . . . . . . 91

Exploring Content Features for Automated Speech Scoring

Shasha Xie, Keelan Evanini and Klaus Zechner. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .103

Hello, Who is Calling?: Can Words Reveal the Social Nature of Conversations?

Anthony Stark, Izhak Shafran and Jeffrey Kaye. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .112

Minimum-Risk Training of Approximate CRF-Based NLP Systems

Veselin Stoyanov and Jason Eisner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120

Unsupervised Learning on an Approximate Corpus

Jason Smith and Jason Eisner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131

xv

Structured Perceptron with Inexact Search

Liang Huang, Suphan Fayong and Yang Guo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142

Segmentation Similarity and Agreement

Chris Fournier and Diana Inkpen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152

HyTER: Meaning-Equivalent Semantics for Translation Evaluation

Markus Dreyer and Daniel Marcu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162

Apples to Oranges: Evaluating Image Annotations from Natural Language Processing Systems

Rebecca Mason and Eugene Charniak . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172

Re-examining Machine Translation Metrics for Paraphrase Identiﬁcation

Nitin Madnani, Joel Tetreault and Martin Chodorow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182

A Dependency Treebank of Classical Chinese Poems

John Lee and Yin Hei Kong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191

Towards Effective Tutorial Feedback for Explanation Questions: A Dataset and Baselines

Myroslava O. Dzikovska, Rodney D. Nielsen and Chris Brew . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200

Topical Segmentation: a Study of Human Performance and a New Measure of Quality.

Anna Kazantseva and Stan Szpakowicz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211

Structured Ramp Loss Minimization for Machine Translation

Kevin Gimpel and Noah A. Smith . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221

Implicitly Intersecting Weighted Automata using Dual Decomposition

Michael J. Paul and Jason Eisner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232

Transliteration Mining Using Large Training and Test Sets

Ali El-Kahki, Kareem Darwish, Mohamed Abdul-Wahab and Ahmed Taei . . . . . . . . . . . . . . . . . 243

Optimized Online Rank Learning for Machine Translation

Taro Watanabe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253

Every sensible extended top-down tree transducer is a multi bottom-up tree transducer

Andreas Maletti . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263

NOMIT: Automatic Titling by Nominalizing

C´edric Lopez, Violaine Prince and Mathieu Roche . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274

Correcting Comma Errors in Learner Essays, and Restoring Commas in Newswire Text

Ross Israel, Joel Tetreault and Martin Chodorow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .284

The Challenges of Parsing Chinese with Combinatory Categorial Grammar

Daniel Tse and James R. Curran . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295

xvi

Using Supertags and Encoded Annotation Principles for Improved Dependency to Phrase Structure
Conversion

Seth Kulick, Ann Bies and Justin Mott. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .305

Getting More from Morphology in Multilingual Dependency Parsing

Matt Hohensee and Emily M. Bender. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .315

Stylometric Analysis of Scientiﬁc Articles

Shane Bergsma, Matt Post and David Yarowsky . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327

Using paraphrases for improving ﬁrst story detection in news and Twitter

Sasa Petrovic, Miles Osborne and Victor Lavrenko. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .338

Insertion and Deletion Models for Statistical Machine Translation

Matthias Huck and Hermann Ney . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347

TransAhead: A Computer-Assisted Translation and Writing Tool

Chung-chi Huang, Ping-che Yang, Keh-jiann Chen and Jason S. Chang . . . . . . . . . . . . . . . . . . . . 352

Correction Detection and Error Type Selection as an ESL Educational Aid

Ben Swanson and Elif Yamangil . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357

Getting More from Segmentation Evaluation

Martin Scaiano and Diana Inkpen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362

G2P Conversion of Proper Names Using Word Origin Information

Sonjia Waxmonsky and Sravana Reddy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367

Evaluating a Morphological Analyser of Inuktitut

Jeremy Nicholson, Trevor Cohn and Timothy Baldwin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372

Intra-Speaker Topic Modeling for Improved Multi-Party Meeting Summarization with Integrated Ran-
dom Walk

Yun-Nung Chen and Florian Metze . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377

Towards Using EEG to Improve ASR Accuracy

Yun-Nung Chen, Kai-Min Chang and Jack Mostow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382

A Comparative Investigation of Morphological Language Modeling for the Languages of the European
Union

Thomas Mueller, Hinrich Schuetze and Helmut Schmid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386

Leveraging supplemental representations for sequential transduction

Aditya Bhargava and Grzegorz Kondrak . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396

A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction

Kairit Sirts and Tanel Alum¨ae . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407

Encouraging Consistent Translation Choices

Ferhan Ture, Douglas W. Oard and Philip Resnik . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417

xvii

Batch Tuning Strategies for Statistical Machine Translation

Colin Cherry and George Foster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427

Real-time Incremental Speech-to-Speech Translation of Dialogs

Srinivas Bangalore, Vivek Kumar Rangarajan Sridhar, Prakash Kolan, Ladan Golipour and Aura
Jimenez . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437

Parsing Time: Learning to Interpret Time Expressions

Gabor Angeli, Christopher Manning and Daniel Jurafsky . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446

Fine-Grained Focus for Pinpointing Positive Implicit Meaning from Negated Statements

Eduardo Blanco and Dan Moldovan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456

Taxonomy Induction Using Hierarchical Random Graphs

Trevor Fountain and Mirella Lapata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466

Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure

Oscar T¨ackstr¨om, Ryan McDonald and Jakob Uszkoreit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477

Training Dependency Parser Using Light Feedback

Avihai Mejer and Koby Crammer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488

Vine Pruning for Efﬁcient Multi-Pass Dependency Parsing

Alexander Rush and Slav Petrov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498

Active Learning for Coreference Resolution

Florian Laws, Florian Heimerl and Hinrich Sch¨utze . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508

Space Efﬁciencies in Discourse Modeling via Conditional Random Sampling

Brian Kjersten and Benjamin Van Durme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513

Predicting Overt Display of Power in Written Dialogs

Vinodkumar Prabhakaran, Owen Rambow and Mona Diab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518

Co-reference via Pointing and Haptics in Multi-Modal Dialogues

Lin Chen and Barbara Di Eugenio. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .523

Trait-Based Hypothesis Selection For Machine Translation

Jacob Devlin and Spyros Matsoukas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528

Improved Reordering for Shallow-n Grammar based Hierarchical Phrase-based Translation

Baskaran Sankaran and Anoop Sarkar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533

Automatic Parallel Fragment Extraction from Noisy Data

Jason Riesa and Daniel Marcu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538

Tuning as Linear Regression

Marzieh Bazrafshan, Tagyoung Chung and Daniel Gildea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543

xviii

Ranking-based readability assessment for early primary children’s literature

Yi Ma, Eric Fosler-Lussier and Robert Lofthus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548

How Text Segmentation Algorithms Gain from Topic Models

Martin Riedl and Chris Biemann . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553

Identifying Comparable Corpora Using LDA

Judita Preiss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558

Behavioral Factors in Interactive Training of Text Classiﬁers

Burr Settles and Xiaojin Zhu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563

Better Evaluation for Grammatical Error Correction

Daniel Dahlmeier and Hwee Tou Ng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568

Are You Sure? Conﬁdence in Prediction of Dependency Tree Edges

Avihai Mejer and Koby Crammer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573

Concavity and Initialization for Unsupervised Dependency Parsing

Kevin Gimpel and Noah A. Smith . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577

Multimodal Grammar Implementation

Katya Alahverdzhieva, Dan Flickinger and Alex Lascarides. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .582

Portable Features for Classifying Emotional Text

Saif Mohammad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587

Stance Classiﬁcation using Dialogic Properties of Persuasion

Marilyn Walker, Pranav Anand, Rob Abbott and Ricky Grant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592

Context-Enhanced Citation Sentiment Detection

Awais Athar and Simone Teufel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 597

Predicting Responses to Microblog Posts

Yoav Artzi, Patrick Pantel and Michael Gamon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 602

The Intelius Nickname Collection: Quantitative Analyses from Billions of Public Records

Vitor Carvalho, Yigit Kiran and Andrew Borthwick . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607

A comparison of models of word meaning in context

Georgiana Dinu, Stefan Thater and Soeren Laue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 611

Measuring Word Relatedness Using Heterogeneous Vector Space Models

Wen-tau Yih and Vahed Qazvinian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 616

Expectations of Word Sense in Parallel Corpora

Xuchen Yao, Benjamin Van Durme and Chris Callison-Burch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621

xix

Why Not Grab a Free Lunch? Mining Large Corpora for Parallel Sentences to Improve Translation
Modeling

Ferhan Ture and Jimmy Lin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626

Summarization of Historical Articles Using Temporal Event Clustering

James Gung and Jugal Kalita . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 631

Comparing HMMs and Bayesian Networks for Surface Realisation

Nina Dethlefs and Heriberto Cuayahuitl . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 636

On The Feasibility of Open Domain Referring Expression Generation Using Large Scale Folksonomies
Fabi´an Pacheco, Pablo Duboue and Mart´ın Dom´ınguez . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .641

Structured Event Retrieval over Microblog Archives

Donald Metzler, Congxing Cai and Eduard Hovy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 646

Learning from Bullying Traces in Social Media

Jun-Ming Xu, Kwang-Sung Jun, Xiaojin Zhu and Amy Bellmore. . . . . . . . . . . . . . . . . . . . . . . . . .656

Grammatical structures for word-level sentiment detection

Asad Sayeed, Jordan Boyd-Graber, Bryan Rusk and Amy Weinberg . . . . . . . . . . . . . . . . . . . . . . . 667

Graph-Based Lexicon Expansion with Sparsity-Inducing Penalties

Dipanjan Das and Noah A. Smith . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 677

Uniﬁed Expectation Maximization

Rajhans Samdani, Ming-Wei Chang and Dan Roth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 688

Low-Dimensional Discriminative Reranking

Jagadeesh Jagarlamudi and Hal Daume III . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 699

Autonomous Self-Assessment of Autocorrections: Exploring Text Message Dialogues

Tyler Baldwin and Joyce Chai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 710

Translation-Based Projection for Multilingual Coreference Resolution

Altaf Rahman and Vincent Ng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 720

Exploring Semi-Supervised Coreference Resolution of Medical Concepts using Semantic and Temporal
Features

Preethi Raghavan, Eric Fosler-Lussier and Albert Lai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 731

Mind the Gap: Learning to Choose Gaps for Question Generation

Lee Becker, Sumit Basu and Lucy Vanderwende . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 742

Unsupervised Concept-to-text Generation with Hypergraphs

Ioannis Konstas and Mirella Lapata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 752

Detecting Visual Text

Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Karl Stratos, Kota
Yamaguchi, Yejin Choi, Hal Daume III, Alex Berg and Tamara Berg . . . . . . . . . . . . . . . . . . . . . . . . . . . 762

xx

Unsupervised Translation Sense Clustering

Mohit Bansal, John DeNero and Dekang Lin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 773

Shared Components Topic Models

Matthew R. Gormley, Mark Dredze, Benjamin Van Durme and Jason Eisner. . . . . . . . . . . . . . . .783

Textual Predictors of Bill Survival in Congressional Committees

Tae Yano, Noah A. Smith and John D. Wilkerson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 793

xxi

Conference Program

Monday, June 4, 2012

(7:30-5:00) Registration

(7:30-9:00) Breakfast

(9:00-9:15) Welcoming remarks

(9:15-10:30) Keynote: Eduard Hovy – “A New Semantics: Merging Proposi-
tional and Distributional Information”

(10:30-11:00) Coffee Break

Session Mon-1E: (11:00-12:30) Discourse, Dialog, and Pragmatics I

11:00–11:30 Multiple Narrative Disentanglement: Unraveling Inﬁnite Jest

Byron Wallace

11:30–12:00

Acoustic-Prosodic Entrainment and Social Behavior
Rivka Levitan, Agustin Gravano, Laura Willson, Stefan Benus, Julia Hirschberg
and Ani Nenkova

12:00–12:30

Identifying High-Level Organizational Elements in Argumentative Discourse
Nitin Madnani, Michael Heilman, Joel Tetreault and Martin Chodorow

Session Mon-1W: (11:00-12:30) Machine Translation I

11:00–11:30

Fast Inference in Phrase Extraction Models with Belief Propagation
David Burkett and Dan Klein

11:30–12:00 Continuous Space Translation Models with Neural Networks

Hai-Son Le, Alexandre Allauzen and Franc¸ois Yvon

12:00–12:30 Machine Translation of Arabic Dialects

Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas,
Richard Schwartz, John Makhoul, Omar F. Zaidan and Chris Callison-Burch

xxiii

Monday, June 4, 2012 (continued)

Session Mon-1D: (11:00-12:30) Information Extraction

11:00–11:30

Entity Clustering Across Languages
Spence Green, Nicholas Andrews, Matthew R. Gormley, Mark Dredze and Christopher D.
Manning

11:30–12:00 Multi-Event Extraction Guided by Global Constraints

Roi Reichart and Regina Barzilay

12:00–12:30

Reference Scope Identiﬁcation in Citing Sentences
Amjad Abu Jbara and Dragomir Radev

(12:30-1:15) Student lunch sponsored by IBM and the Student Research Workshop
(students only)

(1:15-2:15) SRW Panel: Reviewing Practices (open to all)

Session Mon-2E: (2:30-4:00) Spoken Language Processing

2:30–3:00

3:00–3:30

3:30–4:00

Intrinsic and Extrinsic Evaluation of an Automatic User Disengagement Detector for an
Uncertainty-Adaptive Spoken Dialogue System
Kate Forbes-Riley, Diane Litman, Heather Friedberg and Joanna Drummond

Exploring Content Features for Automated Speech Scoring
Shasha Xie, Keelan Evanini and Klaus Zechner

Hello, Who is Calling?: Can Words Reveal the Social Nature of Conversations?
Anthony Stark, Izhak Shafran and Jeffrey Kaye

xxiv

Monday, June 4, 2012 (continued)

2:30–3:00

3:00–3:30

3:30–4:00

2:30–3:00

3:00–3:30

3:30–4:00

Session Mon-2W: (2:30-4:00) Machine Learning I

Minimum-Risk Training of Approximate CRF-Based NLP Systems
Veselin Stoyanov and Jason Eisner

Unsupervised Learning on an Approximate Corpus
Jason Smith and Jason Eisner

Structured Perceptron with Inexact Search
Liang Huang, Suphan Fayong and Yang Guo

Session Mon-2D: (2:30-4:00) Language Resources and Evaluations

Segmentation Similarity and Agreement
Chris Fournier and Diana Inkpen

HyTER: Meaning-Equivalent Semantics for Translation Evaluation
Markus Dreyer and Daniel Marcu

Apples to Oranges: Evaluating Image Annotations from Natural Language Processing
Systems
Rebecca Mason and Eugene Charniak

(4:00-4:30) Coffee Break

(4:30-5:30) Posters and Demos: One-Minute Madness

xxv

Monday, June 4, 2012 (continued)

(6:00-9:00) Poster and Demo Session (with Buffet Dinner)

Session Mon-P: Posters: Full Papers

Re-examining Machine Translation Metrics for Paraphrase Identiﬁcation
Nitin Madnani, Joel Tetreault and Martin Chodorow

A Dependency Treebank of Classical Chinese Poems
John Lee and Yin Hei Kong

Towards Effective Tutorial Feedback for Explanation Questions: A Dataset and Baselines
Myroslava O. Dzikovska, Rodney D. Nielsen and Chris Brew

Topical Segmentation: a Study of Human Performance and a New Measure of Quality.
Anna Kazantseva and Stan Szpakowicz

Structured Ramp Loss Minimization for Machine Translation
Kevin Gimpel and Noah A. Smith

Implicitly Intersecting Weighted Automata using Dual Decomposition
Michael J. Paul and Jason Eisner

Transliteration Mining Using Large Training and Test Sets
Ali El-Kahki, Kareem Darwish, Mohamed Abdul-Wahab and Ahmed Taei

Optimized Online Rank Learning for Machine Translation
Taro Watanabe

Every sensible extended top-down tree transducer is a multi bottom-up tree transducer
Andreas Maletti

NOMIT: Automatic Titling by Nominalizing
C´edric Lopez, Violaine Prince and Mathieu Roche

Correcting Comma Errors in Learner Essays, and Restoring Commas in Newswire Text
Ross Israel, Joel Tetreault and Martin Chodorow

xxvi

Monday, June 4, 2012 (continued)

Using Supertags and Encoded Annotation Principles for Improved Dependency to Phrase
Structure Conversion
Seth Kulick, Ann Bies and Justin Mott

Stylometric Analysis of Scientiﬁc Articles
Shane Bergsma, Matt Post and David Yarowsky

Using paraphrases for improving ﬁrst story detection in news and Twitter
Sasa Petrovic, Miles Osborne and Victor Lavrenko

Session Mon-PS: Posters: Short Papers

TransAhead: A Computer-Assisted Translation and Writing Tool
Chung-chi Huang, Ping-che Yang, Keh-jiann Chen and Jason S. Chang

Correction Detection and Error Type Selection as an ESL Educational Aid
Ben Swanson and Elif Yamangil

Getting More from Segmentation Evaluation
Martin Scaiano and Diana Inkpen

G2P Conversion of Proper Names Using Word Origin Information
Sonjia Waxmonsky and Sravana Reddy

Evaluating a Morphological Analyser of Inuktitut
Jeremy Nicholson, Trevor Cohn and Timothy Baldwin

Intra-Speaker Topic Modeling for Improved Multi-Party Meeting Summarization with In-
tegrated Random Walk
Yun-Nung Chen and Florian Metze

Towards Using EEG to Improve ASR Accuracy
Yun-Nung Chen, Kai-Min Chang and Jack Mostow

Posters: Student Research Workshop

Domain-Speciﬁc Semantic Relatedness From Wikipedia: Can A Course Be Transferred?
Beibei Yang and Jesse M. Heines

Automatic Animacy Classiﬁcation
Samuel Bowman and Harshit Chopra

Finding the Right Supervisor: Expert-Finding in a University Domain
Fawaz Alarfaj, Udo Kruschwitz, David Hunter and Chris Fox

xxvii

Monday, June 4, 2012 (continued)

Indexing Google 1T for low-turnaround wildcarded frequency queries
Steinar Kaldager

Automatic Humor Classiﬁcation on Twitter
Yishay Raz

Beauty Before Age? Applying Subjectivity to Automatic English Adjective Ordering
Felix Hill

Automatic Metrics for Genre-speciﬁc Text Quality
Annie Louis

A Weighting Scheme for Open Information Extraction
Yuval Merhav

Choosing an Evaluation Metric for Parser Design
Woodley Packard

Using Ontology-based Approaches to Representing Speech Transcripts for Automated
Speech Scoring
Miao Chen

Deep Unsupervised Feature Learning for Natural Language Processing
Stephan Gouws

Uniﬁed Extraction of Health Condition Descriptions
Ivelina Nikolova

Posters: Demo

DeSoCoRe: Detecting Source Code Re-Use across Programming Languages
Enrique Flores, Alberto Barr´on-Cede˜no, Paolo Rosso and Lidia Moreno

A Graphical User Interface for Feature-Based Opinion Mining
Pedro Paulo Balage Filho, Caroline Brun and Gilbert Rondeau

Navigating Large Comment Threads with CoFi
Christine Doran, Guido Zarrella and John C. Henderson

SurfShop: combing a product ontology with topic model results for online window-
shopping.
Zoﬁa Stankiewicz and Satoshi Sekine

xxviii

Monday, June 4, 2012 (continued)

An Interactive Humanoid Robot Exhibiting Flexible Sub-Dialogues
Heriberto Cuay´ahuitl and Ivana Kruijff-Korbayov´a

MSR SPLAT, a language analysis toolkit
Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami Suzuki, Kristina Toutanova,
Michael Gamon, Wen-tau Yih, Colin Cherry and Lucy +Vanderwende

Incremental Speech Understanding in a Multi-Party Virtual Human Dialogue System
David DeVault and David Traum

A Robust Shallow Temporal Reasoning System
Ran Zhao, Quang Do and Dan Roth

AttitudeMiner: Mining Attitude from Online Discussions
Amjad Abu-Jbara, Ahmed Hassan and Dragomir Radev

xxix

Tuesday, June 5, 2012

(7:30-5:00) Registration

(7:30-9:00) Breakfast

Session Tue-3: (9:00-10:30) Best Paper Awards Session

9:10–9:30

Trait-Based Hypothesis Selection For Machine Translation
Jacob Devlin and Spyros Matsoukas

9:30–10:00

Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure
Oscar T¨ackstr¨om, Ryan McDonald and Jakob Uszkoreit

10:00–10:30

Vine Pruning for Efﬁcient Multi-Pass Dependency Parsing
Alexander Rush and Slav Petrov

(10:30-11:00) Coffee Break

Session Tue-4E: (11:00-12:30) Phonology and Morphology

11:00–11:30

A Comparative Investigation of Morphological Language Modeling for the Languages of
the European Union
Thomas Mueller, Hinrich Schuetze and Helmut Schmid

11:30–12:00

Leveraging supplemental representations for sequential transduction
Aditya Bhargava and Grzegorz Kondrak

12:00–12:30

A Hierarchical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induc-
tion
Kairit Sirts and Tanel Alum¨ae

xxx

Tuesday, June 5, 2012 (continued)

Session Tue-4C: (11:00-12:30) Machine Translation II

11:00–11:30

Encouraging Consistent Translation Choices
Ferhan Ture, Douglas W. Oard and Philip Resnik

11:30–12:00

Batch Tuning Strategies for Statistical Machine Translation
Colin Cherry and George Foster

12:00–12:30

Real-time Incremental Speech-to-Speech Translation of Dialogs
Srinivas Bangalore, Vivek Kumar Rangarajan Sridhar, Prakash Kolan, Ladan Golipour and
Aura Jimenez

Session Tue-4W: (11:00-12:30) Semantics I

11:00–11:30

Parsing Time: Learning to Interpret Time Expressions
Gabor Angeli, Christopher Manning and Daniel Jurafsky

11:30–12:00

Fine-Grained Focus for Pinpointing Positive Implicit Meaning from Negated Statements
Eduardo Blanco and Dan Moldovan

12:00–12:30

Taxonomy Induction Using Hierarchical Random Graphs
Trevor Fountain and Mirella Lapata

Session Tue-4D: (11:00-12:30) Syntax and Parsing

11:00–11:30 Getting More from Morphology in Multilingual Dependency Parsing

Matt Hohensee and Emily M. Bender

11:30–12:00

Training Dependency Parser Using Light Feedback
Avihai Mejer and Koby Crammer

12:00–12:30

The Challenges of Parsing Chinese with Combinatory Categorial Grammar
Daniel Tse and James R. Curran

xxxi

Tuesday, June 5, 2012 (continued)

(12:30-2:00) Lunch

(2:00-3:30) NLP Idol: Plucked from Obscurity

(3:30-4:00) Coffee Break

Session Tue-5E: (4:00-5:20) Short papers: Discourse

Active Learning for Coreference Resolution
Florian Laws, Florian Heimerl and Hinrich Sch¨utze

Space Efﬁciencies in Discourse Modeling via Conditional Random Sampling
Brian Kjersten and Benjamin Van Durme

Predicting Overt Display of Power in Written Dialogs
Vinodkumar Prabhakaran, Owen Rambow and Mona Diab

Co-reference via Pointing and Haptics in Multi-Modal Dialogues
Lin Chen and Barbara Di Eugenio

Session Tue-5C: (4:00-5:20) Short papers: MT

Insertion and Deletion Models for Statistical Machine Translation
Matthias Huck and Hermann Ney

Improved Reordering for Shallow-n Grammar based Hierarchical Phrase-based Transla-
tion
Baskaran Sankaran and Anoop Sarkar

Automatic Parallel Fragment Extraction from Noisy Data
Jason Riesa and Daniel Marcu

Tuning as Linear Regression
Marzieh Bazrafshan, Tagyoung Chung and Daniel Gildea

4:00–4:20

4:20–4:40

4:40–5:00

5:00–5:20

4:00–4:20

4:20–4:40

4:40–5:00

5:00–5:20

xxxii

Tuesday, June 5, 2012 (continued)

Session Tue-5W: (4:00-5:20) Short papers: Document Categorization and Topic
Modeling

Ranking-based readability assessment for early primary children’s literature
Yi Ma, Eric Fosler-Lussier and Robert Lofthus

How Text Segmentation Algorithms Gain from Topic Models
Martin Riedl and Chris Biemann

Identifying Comparable Corpora Using LDA
Judita Preiss

Behavioral Factors in Interactive Training of Text Classiﬁers
Burr Settles and Xiaojin Zhu

Session Tue-5D: (4:00-5:20) Short papers: Syntax

Better Evaluation for Grammatical Error Correction
Daniel Dahlmeier and Hwee Tou Ng

Are You Sure? Conﬁdence in Prediction of Dependency Tree Edges
Avihai Mejer and Koby Crammer

Concavity and Initialization for Unsupervised Dependency Parsing
Kevin Gimpel and Noah A. Smith

Multimodal Grammar Implementation
Katya Alahverdzhieva, Dan Flickinger and Alex Lascarides

4:00–4:20

4:20–4:40

4:40–5:00

5:00–5:20

4:00–4:20

4:20–4:40

4:40–5:00

5:00–5:20

xxxiii

Tuesday, June 5, 2012 (continued)

(7:00) Banquet at Le Windsor Ballroom

Wednesday, June 6, 2012

(7:30-5:00) Registration

(7:30-9:00) Breakfast

(9:00-10:15) Keynote: James W. Pennebaker – “A, is, I, and, the: How our smallest
words reveal the most about who we Are”

(10:15-10:40) Coffee Break

Session Wed-6E: (10:40-12:00) Short papers: Sentiment and Social Media

10:40–11:00

Portable Features for Classifying Emotional Text
Saif Mohammad

11:00–11:20

Stance Classiﬁcation using Dialogic Properties of Persuasion
Marilyn Walker, Pranav Anand, Rob Abbott and Ricky Grant

11:20–11:40 Context-Enhanced Citation Sentiment Detection

Awais Athar and Simone Teufel

11:40–12:00

Predicting Responses to Microblog Posts
Yoav Artzi, Patrick Pantel and Michael Gamon

xxxiv

Wednesday, June 6, 2012 (continued)

Session Wed-6C: (10:40-12:00) Short papers: Semantics

10:40–11:00

The Intelius Nickname Collection: Quantitative Analyses from Billions of Public Records
Vitor Carvalho, Yigit Kiran and Andrew Borthwick

11:00–11:20

A comparison of models of word meaning in context
Georgiana Dinu, Stefan Thater and Soeren Laue

11:20–11:40 Measuring Word Relatedness Using Heterogeneous Vector Space Models

Wen-tau Yih and Vahed Qazvinian

11:40–12:00

Expectations of Word Sense in Parallel Corpora
Xuchen Yao, Benjamin Van Durme and Chris Callison-Burch

Session Wed-6W: (10:40-12:00) Short papers: Summarization

10:40–11:00 Why Not Grab a Free Lunch? Mining Large Corpora for Parallel Sentences to Improve

Translation Modeling
Ferhan Ture and Jimmy Lin

11:00–11:20

Summarization of Historical Articles Using Temporal Event Clustering
James Gung and Jugal Kalita

11:20–11:40 Comparing HMMs and Bayesian Networks for Surface Realisation

Nina Dethlefs and Heriberto Cuayahuitl

11:40–12:00 On The Feasibility of Open Domain Referring Expression Generation Using Large Scale

Folksonomies
Fabi´an Pacheco, Pablo Duboue and Mart´ın Dom´ınguez

xxxv

Wednesday, June 6, 2012 (continued)

(12:00-1:00) Lunch

(1:00-2:00) Business Meeting

2:10–2:40

2:40–3:10

3:10–3:40

2:10–2:40

2:40–3:10

3:10–3:40

Session Wed-7E: (2:10-3:40) Sentiment and Social Media

Structured Event Retrieval over Microblog Archives
Donald Metzler, Congxing Cai and Eduard Hovy

Learning from Bullying Traces in Social Media
Jun-Ming Xu, Kwang-Sung Jun, Xiaojin Zhu and Amy Bellmore

Grammatical structures for word-level sentiment detection
Asad Sayeed, Jordan Boyd-Graber, Bryan Rusk and Amy Weinberg

Session Wed-7C: (2:10-3:40) Machine Learning II

Graph-Based Lexicon Expansion with Sparsity-Inducing Penalties
Dipanjan Das and Noah A. Smith

Uniﬁed Expectation Maximization
Rajhans Samdani, Ming-Wei Chang and Dan Roth

Low-Dimensional Discriminative Reranking
Jagadeesh Jagarlamudi and Hal Daume III

xxxvi

Wednesday, June 6, 2012 (continued)

2:10–2:40

2:40–3:10

3:10–3:40

4:10–4:40

4:40–5:10

Session Wed-7W: (2:10-3:40) Discourse, Dialog, and Pragmatics II

Autonomous Self-Assessment of Autocorrections: Exploring Text Message Dialogues
Tyler Baldwin and Joyce Chai

Translation-Based Projection for Multilingual Coreference Resolution
Altaf Rahman and Vincent Ng

Exploring Semi-Supervised Coreference Resolution of Medical Concepts using Semantic
and Temporal Features
Preethi Raghavan, Eric Fosler-Lussier and Albert Lai

(3:40-4:10) Coffee Break

Session Wed-8E: (4:10-5:10) Summarization

Mind the Gap: Learning to Choose Gaps for Question Generation
Lee Becker, Sumit Basu and Lucy Vanderwende

Unsupervised Concept-to-text Generation with Hypergraphs
Ioannis Konstas and Mirella Lapata

Session Wed-8C: (4:10-5:10) Semantics II

4:10–4:40

Detecting Visual Text
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Karl Stratos,
Kota Yamaguchi, Yejin Choi, Hal Daume III, Alex Berg and Tamara Berg

4:40–5:10

Unsupervised Translation Sense Clustering
Mohit Bansal, John DeNero and Dekang Lin

xxxvii

Wednesday, June 6, 2012 (continued)

4:10–4:40

4:40–5:10

Session Wed-8W: (4:10-5:10) Document Categorization and Topic Modeling

Shared Components Topic Models
Matthew R. Gormley, Mark Dredze, Benjamin Van Durme and Jason Eisner

Textual Predictors of Bill Survival in Congressional Committees
Tae Yano, Noah A. Smith and John D. Wilkerson

xxxviii

2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1–10,

Montr´eal, Canada, June 3-8, 2012. c(cid:13)2012 Association for Computational Linguistics

1

MultipleNarrativeDisentanglement:UnravelingInﬁniteJestByronC.WallaceTuftsUniversityandTuftsMedicalCenterBoston,MAbyron.wallace@gmail.comAbstractManyworks(ofbothﬁctionandnon-ﬁction)spanmultiple,intersectingnarratives,eachofwhichconstitutesastoryinitsownright.InthisworkIintroducethetaskofmultiplenar-rativedisentanglement(MND),inwhichtheaimistoteasethesenarrativesapartbyassign-ingpassagesfromatexttothesub-narrativestowhichtheybelong.Themotivatingexam-pleIuseisDavidFosterWallace’sﬁctionaltextInﬁniteJest.Iselectedthisbookbecauseitcontainsmultiple,interweavingnarrativeswithinitssprawling1,000-pluspages.Ipro-poseandevaluateanovelunsupervisedap-proachtoMNDthatismotivatedbythetheoryofnarratology.Thismethodachievesstrongempiricalresults,successfullydisentanglingthethreadsinInﬁniteJestandsigniﬁcantlyoutperformingbaselinestrategiesindoingso.1IntroductionBothﬁctionalandnon-ﬁctionaltextsoftencom-prisemultiple,intersectingandinter-relatednarra-tivearcs.Thisworkconsidersthetaskofidentifyingthe(sub-)narrativeslatentwithinanarrativetextandthesetofpassagesthatcomprisethem.Asamo-tivatingexample,IconsiderDavidFosterWallace’sopusInﬁniteJest(Wallace,1996),1whichcontainsseveraldisparatesub-narrativesinterleavedthrough-outitsvoluminous(meta-)story.Bysub-narrativeImean,loosely,thatthesethreadsconstitutetheirownindependentstories,coherentontheirown(i.e.,1Norelation.withoutthebroadercontextoftheoverarchingnarra-tive).Irefertothetaskofidentifyingtheseindepen-dentthreadsanduntanglingthemfromoneanotherasmultiplenarrativedisentanglement(MND).Thetaskisoftheoreticalinterestbecausedisen-tanglementisanecessarypre-requisitetomakingsenseofnarrativetexts,aninterestingdirectioninNLPthathasreceivedanincreasingamountofatten-tion(Elsonetal.,2010;ElsonandMcKeown,2010;Celikyilmazetal.,2010;ChambersandJurafsky,2008;ChambersandJurafsky,2009).Recogniz-ingthe(main)narrativethreadscomprisingaworkprovidesacontextforinterpretingthetext.Disen-tanglementmaythusbeviewedastheﬁrststepinaliteraryprocessing‘pipeline’.Identifyingthreadsandassigningthemtopassagesmayhelpinauto-maticplotsummarization,socialnetworkconstruc-tionandotherliteraryanalysistasks.Computationalapproachestoliteraturelooktomakenarrativesenseofunstructuredtext,i.e.,constructmodelsthatrelatecharactersandeventschronologically:disentangle-mentisattheheartofthisre-construction.ButMNDisalsopotentiallyofmorepragmaticimport:disentanglementmaybeusefulforidentify-ingandextractingdisparatethreadsin,e.g.,anews-magazinearticlethatcoversmultiple(related)sto-ries.2Consideranarticlecoveringapoliticalrace.Itwouldlikelycontainmultiplesub-narratives(thestoryofonecandidate’sriseandfall,ascandalinapoliticalparty,etc.)thatmaybeofinterestindepen-dentlyoftheparticularraceathand.Narrativedis-2Whilenarrativecolloquiallytendstorefertoﬁctionaltexts,thenarrativevoiceisalsofrequentlyusedinnon-ﬁctionalcon-texts(Bal,1997).2

entanglementthushasapplicationsoutsideofcom-putationalmethodsforﬁction.Inthiswork,ItreatMNDasanunsupervisedlearningtask.Givenablockofnarrativetext,theaimistoidentifythetopksub-narrativestherein,andthentoextractthepassagescomprisingthem.Theproposedtaskissimilarinspirittotheprob-lemofchatdisentanglement(ElsnerandCharniak,2010),inwhichtheaimistoassigneachutteranceinachattranscriptiontoanassociatedconversationalthread.Indeed,themainobjectiveisthesame:dis-entanglefragmentsofamonolithictextintochrono-logicallyordered,independentlycoherent‘threads’.Despitetheirsimilarities,however,narrativedisen-tanglementisaqualitativelydifferenttaskthanchatdisentanglement,asIhighlightinSection3.Itakeinspirationfromtheliterarycommunity,whichhasstudiedthetheoreticalunderpinningsofthenarrativeformatlength(Prince,1982;Prince,2003;Abbott,2008).IrelyespeciallyontheseminalworkofBal(1997),Narratology,whichprovidesacomprehensivetheoreticalframeworkfortreatingnarratives.Thisnarratologicaltheorymotivatesmystrategyofnarrativemodeling,inwhichIﬁrstex-tracttheentitiesineachpassageofatext.IthenuncoverthelatentnarrativecompositionsofthesepassagesbyperforminglatentDirichletallocation(LDA)(Bleietal.,2003)overtheextractedentities.Themaincontributionsofthisworkareasfol-lows.First,Iintroducethetaskofmultiplenarrativedisentanglement(MND).Second,motivatedbythetheoryofnarratology(Section2)Iproposeanovel,unsupervisedmethodforthistask(Section5)anddemonstrateitssuperiorityoverbaselinestrategiesempirically(Section6).Finally,Imakeavailableacorpusforthistask:thetextofInﬁniteJestmanuallyannotatedwithnarrativetags(Section4).2NarratologyInowintroducesomeusefuldeﬁnitionsandcon-cepts(Table1)centraltothetheoryofnarratology(Bal,1997).Theseconstructsmotivatemyapproachtothetaskofdisentanglement.Thesedeﬁnitionsimplythattheobservednarra-tivetexthasbeengeneratedwithrespecttosomenumberoflatentfabulas.Astoryisaparticulartellingofanunderlyingfabula,i.e.,asequenceofActoranagentthatperformsactions.Ac-torsarenotnecessarilypersons.Fabulaaseriesoflogicallyandchronolog-icallyrelatedeventsthatarecausedorexperiencedbyactors.Storyaninstantiationofafabula,toldinaparticularstyle(astorytellsafab-ula).Storiesarenotnecessarilytoldinchronologicalorder.Focalizeraspecialactorfromwhosepointofviewthestoryistold.Table1:Asmallglossaryofnarratology.eventsinvolvingactors.Figure1schematizestherelationshipsbetweentheaboveconstructs.Thedottedlinebetweenauthorandfabulaimpliesthatauthorssometimesgeneratethefabula,sometimesnot.Inparticular,anauthormayre-tellawidelyknownfabula(e.g.,Hamlet);perhapsfromadif-ferentperspective.Consider,forexample,theplayRosencrantzandGuildensternareDead(Stoppard,1967),anarrativethatre-tellsthefabulaofHamletfromtheperspectiveofthetitularcharacters(bothofwhomplayaminorpartinHamletitself).Fromanarratologicalview,thisstoryisaninstantiationoftheHamletfabulaimbuedwithnovelaspects(e.g.,thefocalizersinthistellingareRosencrantzandGuildenstern,ratherthanHamlet).Innon-ﬁctionalworksthefabulacorrespondstotheactualeventse-quenceasithappened,andthusisnotinventedbytheauthor(saveforcasesofoutrightfabrication).Fabulasareessentiallyactor-driven.Further,ac-torstendtooccupyparticularplaces,andindeedBal(1997)highlightslocationsasoneofthedeﬁningel-ementsoffabulas.Giventheseobservations,itthusseemsfruitfultoattempttoidentifytheagentsandlocations(orentities)ineachpassageofatextasaﬁrststeptowarddisentanglement.IwillreturntothisintuitionwhenIpresentthenarrativemodelingmethodinSection5.First,Iplacethepresentworkincontextbyrelatingittoexistingworkonminingliteratureandchatdisentanglement.3RelationshiptoExistingWorkMostsimilartoMNDisthetaskofchatdisentan-glement(Shenetal.,2006;ElsnerandCharniak,2010;ElsnerandCharniak,2011),whereinutter-ances(perhapsoverheardatacocktailparty)areto3

FabulaStorySymbols (e.g., text)AuthorFigure1:Aschematicofthenarratologytheory.Thedottedlinebetweenauthorandfabulaimpliesthatwhengeneratinganarrativetext,anauthormayinventafabula,ormaydrawuponanexistingone.Together,theauthorandfabulajointlygiverisetothestory,whichiscommu-nicatedviathetext.beassignedtoconversationalthreads.Thereare,however,importantdifferencesbetweenthesetwotasks.Notably,utterancesinachatbelongtoasinglediscussionthread,motivating‘hard’assignmentsofutterancestothreads,e.g.,usinggraph-partitioning(ElsnerandCharniak,2010)ork-meanslikeap-proaches(Shenetal.,2006).Narratives,however,oftenintersect:asinglepassagemaybelongtomul-tiplenarrativethreads.Thismotivatessoft,proba-bilisticassignmentsofpassagestothreads.More-over,narrativesareinherentlyhierarchical.Thelat-tertwoobservationssuggestthatprobabilisticgen-erativemodelsareappropriateforMND.Therehasalsobeenrecentinterestingrelatedworkintheunsupervisedinductionofnarrativeschemas(ChambersandJurafsky,2008;ChambersandJurafsky,2009).Inthiswork,theauthorspro-posedthetaskof(automatically)discoveringtheeventscomprisinganarrativechain.HerenarrativeeventchainsweredeﬁnedbyChambersandJuraf-sky(2008)aspartiallyorderedsetsofeventsinvolv-ingthesameprotagonist.Whilesimilarinthattheseworksattempttomakesenseofnarrativetexts,thetaskathandisquitedifferent.Inparticular,narrativeschemainductionpre-supposesasinglenarrativethread.Indeed,theau-thorsexplicitlymaketheassumptionthatasingleprotagonistparticipatesinalloftheeventsforminganarrativechain.Thusthediscoveredchainsde-scribeactionsexperiencedbytheprotagonistlocal-izedwithinaparticularnarrativestructure.Bycon-trast,inthisworkItreatnarrativetextsasinstan-tiationsoffabulas,inlinewithBal(1997).Fab-ulascanbeviewedasdistributionsovercharac-ters,eventsandotherentities;thisconceptualiza-tionofwhatconstitutesanarrativeisbroaderthanChambersandJurafsky(2008).inducingnarrativeschemas(ChambersandJurafsky,2009)maybeviewedasapossiblenextstepinanarrativeinduc-tionpipeline,subsequenttodisentanglingthetextcomprisingindividualnarrativethreads.Indeed,thelattertaskmightbeviewedasattemptingtoauto-maticallyre-constructthefabulalatentinaspeciﬁcnarrativethread.Elsewhere,Elsonetal.(2010)proposedamethodforextractingsocialnetworksfromliterarytexts.Theirmethodreliesondialoguedetection.Thisisusedtoconstructagraphrepresentingsocialinter-actions,inwhichanedgeconnectingtwocharac-tersimpliesthattheyhaveinteractedatleastonce;theweightoftheedgeencodesthefrequencyoftheirinteractions.Theirmethodisapipelinedpro-cesscomprisingthreesteps:characteridentiﬁcation,speechattributionand,ﬁnally,graphconstruction.Theirresultsfromtheapplicationofthismethodtoalargecollectionofnovelscalledintoquestionalong-heldliteraryhypothesis:namelythatthereisaninversecorrelationbetweenthenumberofchar-actersinanovelandtheamountofdialogueitcon-tains(Moretti,2005)(itseemsthereisnot).Byan-sweringaliteraryquestionempirically,theirworkdemonstratesthepowerofcomputationalmethodsforliteratureanalysis.4Corpus(InﬁniteJest)Iintroduceanewcorpusforthetaskofmultiplenar-rativedisentanglement(MND):DavidFosterWal-lace’snovelInﬁniteJest(Wallace,1996)thatIhavemanuallyannotatedwithnarrativetags.3InﬁniteJestisaninstructiveexampleforexperimentingwithMND,asthestorymovesfrequentlybetweenafewmostlyindependent–thoughultimatelyconnectedandoccasionallyintersecting–narrativethreads.3Availableathttp://github.com/bwallace/computationaljest.Ialsonotethatthetextcomprises∼100pagesoffootnotes,butIdidnotannotatethese.4

Annotation,i.e.,manuallyassigningtexttooneormorenarratives,istrickydueprimarilytohav-ingtomakedecisionsaboutnewthreaddesignationandlabelgranularity.4Startwiththeﬁrst.Thereisaninherentsubjectivityindecidingwhatconsti-tutesanarrativethread.Inthiswork,Iwaslib-eralinmakingthisdesignation,intotalassigning49uniquenarrativelabels.Mostofthesetellthestoryofparticular(minor)characters,whoarethemselvesactorsina‘higher-level’narrative–aspreviouslymentioned,narrativestructuresareinherentlyhier-archical.Thismotivatesmyliberalintroductionofnarratives:lesserthreadsaresubsumedbytheirpar-entnarratives,andcanthussimplybeignoredduringanalysisifoneisuninterestedinthem.Indeed,thisworkfocusesonlyonthethreemainnarrativesinthetext(seebelow).Granularityposesanotherchallenge.Atwhatleveloughtthetextbeannotated?Shouldeachsen-tencebetaggedwithassociatedthreads?Eachpara-graph?Iletcontextguidethisdecision:insomecasestagsspanasinglesentence;moreoftentheyspanparagraphs.Asanexample,considerthefol-lowingexampleofannotatedtext,whereintheAFRbrieﬂynarrativeintersectsthestoryoftheETA(seeTable2).<AFR>Marathewaschargedwiththisopera-tion’sdetails...<ETA>AdirectassaultupontheAcademyofTennisitselfwasimpossible.A.F.R.sfearnothinginthishemisphereexcepttallandsteephillsides....</ETA></AFR>Heretheellipsesspansseveralparagraphs.PrecisionprobablymatterslessthancontextinMND:identi-fyingonlysentencesthatinvolveaparticularsub-narrative,sanscontext,wouldprobablynotbeuse-ful.Becausetheappropriatelevelofgranularityde-pendsonthecorpusathand,thetaskofsegmentingthetextintousefulchunksisasub-taskofMND.Irefertothesegmentedpiecesoftextaspassagesandsaythatapassagebelongstoallofthenarrativethreadsthatappearanywherewithinit.Henceintheaboveexample,thepassagecontainingthisexcerptwouldbedesignatedasbelongingtoboththeETAandAFRthreads.4Thesecomplexitiesseemtobeinherenttodisentanglementtasksingeneral:ElsnerandCharniak(2010)describeanaloguesissuesinthecaseofchat.AFRThisisthetaleofthewheelchairassassins,aQu`eb`ecoisterroristgroup,andtheirattemptstoseizeanoriginalcopyofadangerousﬁlm.Fo-calizer:Marathe.EHDRHTheEnnetHouseDrugRecoveryHouse(sic).Thisnarrativeconcernsthegoing-onsatadrugrecoveryhouse.Focalizer:DonGately.ETAThisnarrativefollowsthestudentsandfacultyattheEnﬁeldTennisAcademy.Focalizer:Hal.Table2:Briefsummariesofthemainnarrativescompris-ingInﬁniteJest.narrative#ofpassagesprevalenceAFR3016%EHDRH4223%ETA6938%Table3:Summarystatisticsforthethreemainnarratives.InﬁniteJestisnaturallysegmentedbybreaks,i.e.,blanklinesinthetextwhichtypicallyindicatesomesortofcontext-shift(functionally,thesearelikemini-chapters).Thereare182suchbreaksinthebook,demarcating183passages.Eachofthesecomprisesabout16,000wordsandcontainsanav-erageof4.6(outof49)narratives,accordingtomyannotations.TherearethreemainnarrativethreadsinInﬁniteJest,summarizedbrieﬂyinTable2.5Iamnotaloneindesignatingtheseasthecentralplot-linesinthebook.6Nearlyalloftheotherthreadsinthetextaresubsumedbythese(togetherthethreecover72%ofthepassagesinthebook).ThesethreemainthreadsareidealforevaluatinganMNDsystem,forafewreasons.First,theyarelargelyindependentofoneanother,i.e.,overlaponlyoccasionally(thoughtheydooverlap).Second,theyarerelativelyunam-biguous:itismostlyclearwhenapassagetellsapieceofoneofthesestory-lines,andwhenitdoesnot.Thesenarrativesarethuswell-deﬁned,provid-ingaminimal-noisedatasetforthetaskofMND.ThatIamthesingleannotatorofthecorpus(andhenceinter-annotatoragreementcannotbeassessed)isunfortunate;thedifﬁcultyofﬁndingsomeonebothqualiﬁedandwillingtoannotatethe1000+pagebookprecludedthispossibility.Ihopetoaddress5Iincludetheseonlyforinterestedreaders:thedescriptionsarenottechnicallyimportantfortheworkhere,andonemayequivalentlysubstitute‘narrative1’,‘narrative2’,etc.6e.g.,http://www.sampottsinc.com/ij/ﬁle/IJDiagram.pdf.5

Figure2:ThethreemainnarrativesinInﬁniteJest.Acoloredboximpliesthatthecorrespondingnarrativeispresentinthepassageatthatlocationinthetext;thesearescaledrelativetothepassagelength.thisshortcominginfuturework.Figure2depictsthelocationanddurationofthesesub-narrativeswithinthetext.Passagesrunalongthebottomaxis.Acoloredboxindicatesthatthecorrespondingnarrativeispresentinthepassagefoundatthatlocationinthebook.Passagesarenor-malizedbytheirlength:awideboximpliesalongpassage.TheaimofMND,then,istoautomaticallyinferthisstructurefromthenarrativetext.5NarrativeModelingforMultipleNarrativeDisentanglementTheproposedmethodismotivatedbythetheoryofnarratology(Bal,1997),reviewedinSection2.SpeciﬁcallyIassumethatpassagesaremixturesofdifferentnarrativeswithassociatedunderlyingfabu-las.Fabulas,inturn,areviewedasdistributionsoverentities.Entitiesaretypicallyactors,butmayalsobelocations,etc.;theyarewhatfabulasareabout.Theideaistoinferfromtheobservedpassagestheprobablelatentfabulas.Thisisagenerativeviewofnarrativetexts,whichlendsitselfnaturallytoatopic-modelingapproach(SteyversandGrifﬁths,2007).Further,thisgenera-tivevantageallowsonetoexploitthemachineryoflatentDiricheletallocation(LDA)(Bleietal.,2003).LDAisagenerativemodelfortexts(anddiscretedata,ingeneral)inwhichitisassumedthateachdocumentinacorpusreﬂectsamixtureof(latent)topics.Thewordsinthetextarethusassumedtobegeneratedbythesetopics:topicsaremultinomialsoverwords.Graphically,thismodelisdepictedbyFigure3.Alloftheparametersinthismodelmustbeestimated;onlythewordsindocumentsareob-served.Touncoverthetopicmixtureslatentindoc-LATENTDIRICHLETALLOCATION!zw"#MNFigure1:GraphicalmodelrepresentationofLDA.Theboxesare“plates”representingreplicates.Theouterplaterepresentsdocuments,whiletheinnerplaterepresentstherepeatedchoiceoftopicsandwordswithinadocument.wherep(zn|")issimply"ifortheuniqueisuchthatzin=1.Integratingover"andsummingoverz,weobtainthemarginaldistributionofadocument:p(w|!,#)=￿p("|!)￿N$n=1%znp(zn|")p(wn|zn,#)￿d".(3)Finally,takingtheproductofthemarginalprobabilitiesofsingledocuments,weobtaintheproba-bilityofacorpus:p(D|!,#)=M$d=1￿p("d|!)￿Nd$n=1%zdnp(zdn|"d)p(wdn|zdn,#)￿d"d.TheLDAmodelisrepresentedasaprobabilisticgraphicalmodelinFigure1.Astheﬁguremakesclear,therearethreelevelstotheLDArepresentation.Theparameters!and#arecorpus-levelparameters,assumedtobesampledonceintheprocessofgeneratingacorpus.Thevariables"daredocument-levelvariables,sampledonceperdocument.Finally,thevariableszdnandwdnareword-levelvariablesandaresampledonceforeachwordineachdocument.ItisimportanttodistinguishLDAfromasimpleDirichlet-multinomialclusteringmodel.Aclassicalclusteringmodelwouldinvolveatwo-levelmodelinwhichaDirichletissampledonceforacorpus,amultinomialclusteringvariableisselectedonceforeachdocumentinthecorpus,andasetofwordsareselectedforthedocumentconditionalontheclustervariable.Aswithmanyclusteringmodels,suchamodelrestrictsadocumenttobeingassociatedwithasingletopic.LDA,ontheotherhand,involvesthreelevels,andnotablythetopicnodeissampledrepeatedlywithinthedocument.Underthismodel,documentscanbeassociatedwithmultipletopics.StructuressimilartothatshowninFigure1areoftenstudiedinBayesianstatisticalmodeling,wheretheyarereferredtoashierarchicalmodels(Gelmanetal.,1995),ormorepreciselyascon-ditionallyindependenthierarchicalmodels(KassandSteffey,1989).SuchmodelsarealsooftenreferredtoasparametricempiricalBayesmodels,atermthatrefersnotonlytoaparticularmodelstructure,butalsotothemethodsusedforestimatingparametersinthemodel(Morris,1983).In-deed,aswediscussinSection5,weadopttheempiricalBayesapproachtoestimatingparameterssuchas!and#insimpleimplementationsofLDA,butwealsoconsiderfullerBayesianapproachesaswell.997Figure3:ThegraphicalmodeloflatentDirichletallo-cation(LDA;FigurefromBleietal.(2003)).Θparam-eterizesthemultinomialgoverningtopics,i.e.,zs.Theobservedwordswarethenassumedtobedrawnfromamultinomialconditionedonz.HeretheplatesdenotethatthereareN(observed)wordsandMtopics.uments,standardinferenceprocedurescanbeusedforparameterestimation(Jordanetal.,1999).IproposethefollowingapproachforMND,whichIwillrefertoasnarrativemodeling.(ThispipelineisalsodescribedbyFigure4).1.Segmenttherawtextintopassages.Itisatthelevelofthisunitthatnarrativeswillbeassigned:ifagivennarrativetagisanywhereinapassage,thatpassageisdeemedasbeingapartofsaidnarrative.7Inmanycases(includingthepresentone)thisstepwillberelativelytrivial;e.g.,segmentingthetextintochaptersorparagraphs.2.(Automatically)extractfromeachoftheseseg-mentsnamedentities.Theideaisthattheseincludetheprimaryplayersintherespectivenarratives,i.e.,importantactorsandlocations.3.PerformlatentDiricheletanalysis(LDA)overtheentitiesextractedin(2).Whenthistopicmod-7Thisisanalogoustoamulti-labelscenario.6

elingisperformedovertheentities,ratherthanthetext,Ishallrefertoitasnarrativemodeling.Asmentionedabove,Step(1)willbetask-speciﬁc:whatconstitutesapassageisinherentlysubjective.Inmanycases,however,thetextwilllenditselftoa‘natural’segmenting,e.g.,atthechapter-level.Standardstatisticaltechniquesfornamedentityrecognition(NER)canbeusedforStep(2)(McCallumandLi,2003).Algorithm1ThestoryofLDAoverextractedenti-tiesformultiplenarrativedisentanglement.Drawamixtureofnarrativethreadsθ∼Dir(α)foreachentityinthepassageeidoDrawanarrativethreadti∼Multinomial(θ)Draweifromp(ei|ti)endforsegmenternarrative textpassages NER extractorextracted entities for passages narrative modelingFigure4:TheMNDpipeline.Forthenarrativemodel-ingStep(3),IuseLDA(Bleietal.,2003);thegenerativestoryfornar-rativemodelingistoldbyAlgorithm1.8Thissquareswiththenarra-tologicalview:entitiesareobservedinthetextwithprobabilitypropor-tionaltotheirlikelihoodofbeingdrawnfromthecorrespondinglatentfabu-las(whichweareattempt-ingtorecover).Focus-ingontheseentities,ratherthantherawtext,iscru-cialifoneistobecompat-iblewiththenarratologicalview.Thetextismerelyaparticulartellingoftheunderlyingfabula,madenoisybystoryspeciﬁcas-pects;extractingentitiesfromthepassageseffec-tivelyremovesthisnoise,allowingthemodeltoop-erateoveraspacemorecloselytiedtothefabulas.Inthefollowingsection,Idemonstratethatthisshifttotheentity-spacesubstantiallyboostsMNDperfor-mance.8LiuandLiu(2008)havealsoproposedtopicmodelsoverNEs,thoughinaverydifferentcontext.Theaimistouncoverthetopkmostsalientnar-rativethreadsinatext,wherekisauser-providedparameter.Indeedonemustspecifythenumberofthreadsheorsheisinterestedinidentifying(anddis-entangling),becausebecause,duetothehierarchicalnatureofnarratives,thereisnosingle‘rightnumber’ofthem.Considerthattheinputblockoftextcon-stitutesaperfectlylegitimate(meta-)narrativeonitsown,forexample.Arelatedissuethatmustbead-dressedisthatofdecidingwhentoassignapassagetomultiplethreads.Thatis,giventhe(estimated)narrativemixturesforeachpassageasaninput,towhich(ifany)narrativethreadsoughtthispassagebeassigned?Myapproachtothisistwo-fold.First,Isetathresholdprobabilityαsuchthatapassagepican-notbeassignedtoanarrativethreadtiftheesti-matedmixturecomponentis≤α.Iuseα=1/k,asthisvalueimpliesthatthepassageisdominatedbyotherthreads(inthecasethatallkthreadscontributeequallytoapassage,thecorrespondingmixtureel-ementswouldallbe1/k).Second,Ienforceacon-straintthatinordertobeassignedtothenarrativet,apassagemustcontainatleastoneofthetoplenti-tiesinvolvedint(accordingtothenarrativemodel).Thisconstraintencodestheintuitionthatthemainactors(andlocations)thatconstituteagivenfabulaare(extremely)likelytobepresentinanygivenpas-sageinwhichitislatent.Isetl=100,reﬂectingintuition.TheseweretheﬁrstvaluesIusedforbothoftheseparameters;Ididnottunethemtothecor-pusathand.Idid,however,experimentwithothervaluesaftertheprimaryanalysistoassesssensitiv-ity.Theproposedalgorithmisnotterriblysensitivetoeitherparameter,thoughbothexertinﬂuenceintheexpecteddirections:increasingαdecreasesre-call,aspassagesarelesslikelytobeassignedtonar-ratives.Decreasinglhasasimilareffect,butdoesnotsubstantiallyimpactperformanceunlessextremevaluesareused.95.1FocalizerDetectionRecallthatthefocalizerofanarrativeistheagentresponsibleforperception:itisfromtheirpointofviewthatthestoryistold(Bal,1997).Onecaneas-ilyexploitthenarrativemodelingmethodaboveto9Fewerthan10ormorethan500,forexample.7

automaticallyidentifythe(main)focalizeroftheun-coverednarratives.10Tothisend,Isimplyidentifythehighestrankingentityfromeachnarrativethathasalsobeenlabeledasa‘person’(asopposed,e.g.,toan‘organization’).6EmpiricalResultsInowpresentexperimentalresultsovertheInﬁniteJestcorpus,describedinSection4.Thetaskhereistouncoverthethreemainnarrativesinthetext,de-pictedinFigure2.Toimplementtheproposednar-rativemodelingmethod(Section5),Iﬁrstchunkedthetextintopassages,delineatedinJestbybreaksinthetext.IperformedentityextractionoverthesepassagesusingtheNLTKtoolkit(Birdetal.,2009).IthenperformedLDAviaMallet(McCallum,2002)toestimatethenarrativemixturecomponentsofeachpassage.recall=TP/(TP+FN)(1)precision=TP/(TP+FP)(2)F=2·precision·recallprecision+recall(3)Icomparethenarrativemodelingapproachpre-sentedintheprecedingsectiontothreebaselines.Thesimplestofthese,round-robinandall-samearesimilartothebaselinesusedforchatdisentan-glement(ElsnerandCharniak,2010).Respectively,thesestrategiesdesignateeachpassageas:belong-ingtothenextnarrativeinagivensequence(‘narra-tive1’,‘narrative2’,‘narrative3’),and,belongingtothemajoritynarrative.InbothcasesIshowthebestresultattainableusingthemethod:thusinthecaseoftheformer,Ireportthebestscoringresultsfromall3!possiblethreadsequences(withrespecttomacro-averagedF-score)andinthelattercaseIusethetruemajoritynarrative.Ialsoevaluateasimpletopic-modelingbaseline,whichisthesameasnarrativemodeling,exceptthat:1)LDAisperformedoverthefull-text(ratherthantheextractedentities)and,2)thereisnoconstraintenforcingthatpassagesreferenceanentityassoci-atedwiththeassignednarrative.Ievaluateresultswithrespecttoper-narrativerecall,precisionandF-score(Equations1-3)(whereTP=truepositive,10Technically,theremaybemultiplefocalizersinanarrative,butmoreoftenthereisonlyone.FN=falsenegative,etc.).Ialsoconsidermicro-andmacro-averagesofthese.Tocalculatethemicro-average,oneconsiderseachpassageatatimebycountinguptheTPs,FPs,TNsandFNsthereinforeachnarrativeundercon-sideration(w.r.t.themodelbeingevaluated).Themicro-averageisthencalculatedusingthesetalliedcounts.Notethatinthiscasecertainnarrativesmaycontributemoretotheoverallresultthanothers,e.g.thosethatarecommon.Bycontrast,tocalculatethemacro-average,oneconsiderseachnarrativeinturnandcalculatestheaverageofthemetricsofinterest(recall,precision)w.r.t.thisnarrativeoverallpas-sages.Anaverageisthentakenoverthesemeanper-formances.Thiscapturestheaverageperformanceofamodeloverallofthenarratives,irrespectiveoftheirprevalence;inthiscase,eachthreadcon-tributesequallytotheoverallresult.Finally,notethatnoneofthemethodsexplicitlylabelsthenarra-tivestheyuncover:thisassignmentcanbemadebysimplymatchingthereturnednarrativestothethreadlabels(e.g.,ETA)thatmaximizeperformance.Thislabelingisstrictlyaesthetic;theaimistorecoverthelatentnarrativethreadsintext,nottolabelthem.Table4presentsthemainempiricalresults.Nei-therofthesimplebaselinemethods(round-robinandall-same)performedverywell.Bothcases,forexample,completelyfailedtoidentifytheEHDRHthread(thoughthisishardlysurprisinglyintheall-samecase,whichidentiﬁesonlyonethreadbydef-inition).Themacro-averagedprecisionsandF-measuresarethusundeﬁnedinthesecases(thesegiverisetoadenominatorof0).Withrespecttomicro-averagedperformance,all-sameachievesasubstantiallyhigherF-scorethanround-robinhere,thoughingeneralthiswillbecontingentonhowdominatedthetextisbythemajoritythread.Nextconsiderthetwomoresophisticatedstrate-gies,includingtheproposednarrativemodelingmethod.Startwiththeperformanceoffull-textTM,i.e.,performingstandardtopic-modelingoverthefull-text.Thismethodimprovesconsiderablyonthebaselines,achievingamacro-averagedF-scoreof.545.11Butthenarrativemodelingmethod(Sec-tion5)performssubstantiallybetter,boostingthe11Inthefull-textcase,Ievaluatedtheperformanceofeverypossibleassignmentoftopicstothreads,andreportthebestscoringresult.8

Figure5:Theunsupervisedre-constructionofthethreemainnarrativesusingthenarrativemodelingapproach.Hatchedboxesdenotefalse-positives(designatingapassageasbelongingtoanarrativewhenitdoesnot);emptyboxesfalsenegatives(failingtoassignapassagetonarrativetowhichitbelongs).Figure6:Resultsusingfull-texttopicmodeling(seeabovecaption).macro-averagedF-scorebyover15points(apercentgainofnearly30%).Figures5and6depicttheunsupervisedre-constructionofthenarrativethreadsusingnarrativemodelingandthefull-texttopicmodelingapproach,respectively.Recallthattheaimistore-constructthenarrativesdepictedinFigure2.Intheseplots,anemptyboxrepresentsafalsenegative(i.e.,impliesthatthispassagecontainedthecorrespondingnarra-tivebutthiswasnotinferredbythemodel),andahatchedboxdenotesafalsepositive(themodelas-signedthepassagetothecorrespondingnarrative,butthepassagedidnotbelongtoit).Onecanseethatthenarrativemodelingmethod(Figure5)re-constructsthehiddenthreadsmuchbetterthandoesthefull-texttopicmodelingapproach(Figure6).OncecanseethatthelattermethodhasparticulartroublewiththeEHDRHthread.IalsoexperimentedwiththefocalizerdetectionmethodproposedinSection5.1.Thissimplestrat-egyachieved100%accuracyonthethreemainnar-ratives,correctlyidentifyingbynameeachofthecorrespondingfocalizers(seeTable2).6.1AMoreEntangledThreadTheprecedingresultsarepositive,insofarasthepro-posedmethodsubstantiallyimprovesonbaselinesandisabletodisentanglethreadswithrelativelyhighﬁdelity.Theseresultsconsideredthethreemainnarrativesthatcomprisethenovel(Figure2).ThisisthesortofstructureIbelievewillbemostcom-moninnarrativedisentanglement,asitislikelythatonewillmostlybeinterestedinextractingcoherentthreadsthatarelargelyindependentofoneanother.Thatsaid,Iwillnextconsideramoreentangledthreadtoseeifthemethodhandlesthesewell.Morespeciﬁcally,IintroducethenarrativeINC,whichre-latesthestoryoftheIncandenzafamily.Thisfamilyis(arguably)thefocusofthenovel.ThestoryoftheIncandenza’soverlapsextremelyfrequentlywiththethreemain,mostlyindependentnarrativescon-sideredthusfar(seeFigure6).ThisthreadisthusdifﬁcultfromanMNDperspective.Iapplythesamemethodsasabovetothistask,re-questingfour(ratherthanthree)sub-narratives,i.e.,k=4.ResultsaresummarizedinTable5.12Weob-12Iomitthetwobaselinestrategiesduetospaceconstraints;9

round-robinall-samefull-textTMnarrativemodelingnarrativerecallprec.Frecallprec.Frecallprec.Frecallprec.FAFR0.4330.2100.2830.000undef.undef.0.9000.3000.4500.9330.3590.519EHDRH0.000undef.undef.0.000undef.undef.0.7860.4020.5320.9290.7360.821ETA0.3690.3480.3931.0000.3750.5450.6670.6390.6530.8550.6940.766macro-avg.0.260undef.undef.0.333undef.undef.0.7520.4470.5450.9060.5960.702micro-avg.0.2620.3000.2800.4890.3750.4250.7520.4340.5510.8940.5830.706Table4:EmpiricalresultsusingdifferentstrategiesforMND.Thetopthreerowscorrespondtoperformanceforindividualnarratives;thebottomtwoprovidemicro-andmacro-averages,whicharetakenovertheindividualpassagesandthenarrative-levelresults,respectively.Figure7:TheINCnarrativethread(green,top).Thisnarrativeissubstantiallymoreentangledthantheothers,i.e.,morefrequentlyintersectswiththeothernarratives.full-textTMnarrativemodelingnarrativerecallprec.Frecallprec.FAFR0.600.300.400.830.500.63EHDRH0.830.570.670.790.750.77ETA0.670.690.680.670.890.76INC0.570.460.510.430.750.54macro-avg.0.670.500.560.680.720.67micro-avg.0.650.500.570.620.720.67Table5:Resultswhenthefourthnarrative,moreentan-glednarrative(INC)isadded.servethatthenarrativemodelingstrategyagainbeststhebaselinestrategies,achievingamacro-averagedF-scoreofabout10pointsgreaterthanthatachievedusingthefull-textTMmethod(a∼20%gain).Focalizeridentiﬁcationistrickyinthiscasebe-causetherearemultiplefocalizers.HoweverInotethatusingtheproposedstrategy,fourmembersoftheIncandenzaclanrankinthetopﬁveentitiesas-sociatedwiththisnarrative,anencouragingresult.13bothperformedworsethanthedisplayedmethods.13Theﬁfthtop-rankingentityisJoelle,agirlwhoplaysanimportantpartinthefamilysaga.7ConclusionsIhaveintroducedthetaskofmultiplenarrativedis-entanglement(MND),andprovidedanewannotatedcorpusforthistask.Iproposedanovelmethod(narrativemodeling)forMNDthatismotivatedbythetheoryofnarratology.IdemonstratedthatthismethodisabletodisentanglethenarrativethreadscomprisingInﬁniteJestandthatitsubstantiallyout-performsbaselinesintermsofdoingso.Ialsoex-tendedthemethodtoautomaticallyidentifynarra-tivefocalizers,andshowedthatitispossibletodosowithnear-perfectaccuracy.Interestingfuturedirectionsincludeexploringsupervisednarrativedisentanglement,combiningMNDwithnarrativeinduction(ChambersandJuraf-sky,2009)andapplyingMNDtonon-ﬁctionaltexts.AcknowledgmentsThankstoKevinSmallandCarlaBrodleyforsug-gestingimprovementstothiswork,andtoallofthemembersoftheInmanSquareExistentialistBookClubforinsightfuldiscussionsaboutJest.10

ReferencesH.P.Abbott.2008.TheCambridgeintroductiontonar-rative.CambridgeUnivPr.MBal.1997.Narratology:Introductiontothetheoryofnarrative,3rded.UniversityofTorontoPress.S.Bird,E.Klein,andE.Loper.2009.NaturallanguageprocessingwithPython.O’ReillyMedia.D.M.Blei,A.Y.Ng,andM.I.Jordan.2003.Latentdirichletallocation.TheJournalofMachineLearningResearch,3:993–1022.A.Celikyilmaz,D.Hakkani-Tur,H.He,G.Kondrak,andD.Barbosa.2010.Theactortopicmodelforextractingsocialnetworksinliterarynarrative.InNIPSWork-shop:MachineLearningforSocialComputing.N.ChambersandD.Jurafsky.2008.Unsupervisedlearningofnarrativeeventchains.ProceedingsofACL-08:HLT,pages789–797.N.ChambersandD.Jurafsky.2009.Unsupervisedlearningofnarrativeschemasandtheirparticipants.InProceedingsoftheJointConferenceofthe47thAnnualMeetingoftheACLandthe4thInternationalJointConferenceonNaturalLanguageProcessingoftheAFNLP,pages602–610.AssociationforCompu-tationalLinguistics.M.ElsnerandE.Charniak.2010.Disentanglingchat.ComputationalLinguistics,36(3):389–409.M.ElsnerandE.Charniak.2011.Disentanglingchatwithlocalcoherencemodels.InProceedingsofthe49thAnnualMeetingoftheAssociationforCompu-tationalLinguistics:HumanLanguageTechnologies,volume1,pages1179–1189.AssociationforCompu-tationalLinguistics.D.K.ElsonandK.R.McKeown.2010.Automaticattri-butionofquotedspeechinliterarynarrative.InPro-ceedingsofAAAI.D.K.Elson,N.Dames,andK.R.McKeown.2010.Ex-tractingsocialnetworksfromliteraryﬁction.InPro-ceedingsofthe48thAnnualMeetingoftheAssociationforComputationalLinguistics,pages138–147.Asso-ciationforComputationalLinguistics.M.I.Jordan,Z.Ghahramani,T.S.Jaakkola,andL.K.Saul.1999.Anintroductiontovariationalmethodsforgraphicalmodels.Machinelearning,37(2):183–233.Y.LiuandF.Liu.2008.Unsupervisedlanguagemodeladaptationviatopicmodelingbasedonnamedentityhypotheses.InAcoustics,SpeechandSignalProcess-ing,2008.ICASSP2008.IEEEInternationalConfer-enceon,pages4921–4924.IEEE.A.McCallumandW.Li.2003.Earlyresultsfornamedentityrecognitionwithconditionalrandomﬁelds,fea-tureinductionandweb-enhancedlexicons.InPro-ceedingsoftheseventhconferenceonNaturallan-guagelearningatHLT-NAACL2003-Volume4,pages188–191.AssociationforComputationalLinguistics.AndrewKachitesMcCallum.2002.Mal-let:Amachinelearningforlanguagetoolkit.http://mallet.cs.umass.edu.F.Moretti.2005.Graphs,Maps,Trees:Abstractmodelsforaliteraryhistory.VersoBooks.G.Prince.1982.Narratology:Theformandfunctioningofnarrative.MoutonBerlin.G.Prince.2003.Adictionaryofnarratology.UniversityofNebraskaPress.D.Shen,Q.Yang,J.T.Sun,andZ.Chen.2006.Threaddetectionindynamictextmessagestreams.InPro-ceedingsofthe29thannualinternationalACMSIGIRconferenceonResearchanddevelopmentininforma-tionretrieval,pages35–42.ACM.M.SteyversandT.Grifﬁths.2007.Probabilistictopicmodels.Handbookoflatentsemanticanalysis,427(7):424–440.T.Stoppard.1967.Rosencrantz&Guildensternaredead:aplayinthreeacts.SamuelFrenchTrade.D.F.Wallace.1996.InﬁniteJest.LittleBrown&Co.2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 11–19,

Montr´eal, Canada, June 3-8, 2012. c(cid:13)2012 Association for Computational Linguistics

11

Acoustic-ProsodicEntrainmentandSocialBehaviorRivkaLevitan1,Agust´ınGravano2,LauraWillson1,˘StefanBe˘nu˘s3,JuliaHirschberg1,AniNenkova41Dept.ofComputerScience,ColumbiaUniversity,NewYork,NY10027,USA2DepartamentodeComputaci´on(FCEyN),UniversidaddeBuenosAires,Argentina3ConstantinethePhilosopherUniversity&InstituteofInformatics,SlovakAcademyofSciences,Slovakia4Dept.ofComputerandInformationScience,UniversityofPennsylvania,Philadelphia,PA19104,USArlevitan@cs.columbia.edu,gravano@dc.uba.ar,law2142@barnard.edu,sbenus@ukf.sk,julia@cs.columbia.edu,nenkova@seas.upenn.eduAbstractInconversation,speakershavebeenshowntoentrain,orbecomemoresimilartoeachother,invariousways.Wemeasureentrain-mentoneightacousticfeaturesextractedfromthespeechofsubjectsplayingacooperativecomputergameandassociatethedegreeofen-trainmentwithanumberofmanually-labeledsocialvariablesacquiredusingAmazonMe-chanicalTurk,aswellasobjectivemeasuresofdialoguesuccess.Weﬁndthatmale-femalepairsentrainonallfeatures,whilemale-malepairsentrainonlyonparticularacousticfea-tures(intensitymean,intensitymaximumandsyllablespersecond).Wefurtherdeterminethatentrainmentismoreimportanttotheper-ceptionoffemale-malesocialbehaviorthanitisforsame-genderpairs,anditismoreimpor-tanttothesmoothnessandﬂowofmale-maledialoguethanitisforfemale-femaleormixed-genderpairs.Finally,weﬁndthatentrainmentismorepronouncedwhenintensityorspeak-ingrateisespeciallyhighorlow.1IntroductionEntrainment,alsotermedalignment,adaptation,primingorcoordination,isthephenomenonofconversationalpartnersbecomingmoresimilartoeachotherinwhattheysay,howtheysayit,andotherbehavioralphenomena.Entrainmenthasbeenshowntooccurfornumerousaspectsofspo-kenlanguage,includingspeakers’choiceofre-ferringexpressions(Brennan&Clark,1996);lin-guisticstyle(Niederhoffer&Pennebaker,2002;Danescu-Niculescu-Miziletal.,2011);syntacticstructure(Reitteretal.,2006);speakingrate(Lev-itan&Hirschberg,2011);acoustic/prosodicfea-turessuchasfundamentalfrequency,intensity,voicequality(Levitan&Hirschberg,2011);andphonet-ics(Pardo,2006).Entrainmentinmanyofthesedimensionshasalsobeenassociatedwithdifferentmeasuresofdialoguesuccess.Forexample,ChartrandandBargh(1999)demonstratedthatmimicryofpostureandbehaviorledtoincreasedlikingbetweenthedialoguepar-ticipantsaswellasasmootherinteraction.Theyalsofoundthatnaturallyempatheticindividualsex-hibitedagreaterdegreeofmimicrythandidoth-ers.Nenkovaetal.(2008)foundthatentrainmentonhigh-frequencywordswascorrelatedwithnat-uralness,tasksuccess,andcoordinatedturn-takingbehavior.Natale(1975)showedthatanindivid-ual’ssocialdesirability,or“propensitytoactinasocialmanner,”canpredictthedegreetowhichthatindividualwillmatchherpartner’svocalinten-sity.Levitanetal.(2011)showedthatentrainmentonbackchannel-precedingcuesiscorrelatedwithshorterlatencybetweenturns,fewerinterruptions,andahigherdegreeoftasksuccess.Inastudyofmarriedcouplesdiscussingproblemsintheirrela-tionships,Leeetal.(2010)foundthatentrainmentmeasuresderivedfrompitchfeaturesweresigniﬁ-cantlyhigherinpositiveinteractionsthaninnega-tiveinteractionsandwerepredictiveofthepolarityoftheparticipants’attitudes.Thesestudieshavebeenmotivatedbytheoreti-calmodelssuchasGiles’CommunicationAccom-modationTheory(Giles&Coupland,1991),whichproposesthatspeakerspromotesocialapprovalor12

efﬁcientcommunicationbyadaptingtotheirinter-locutors’communicativebehavior.Anothertheoryinformingtheassociationofentrainmentanddia-loguesuccessisthecoordination-rapporthypoth-esis(Tickle-Degnen&Rosenthal,1990),whichpositsthatthedegreeoflikingbetweenconversa-tionalpartnersshouldbecorrelatedwiththedegreeofnonverbalcoordinationbetweenthem.Motivatedbysuchtheoreticalproposalsandem-piricalﬁndings,wehypothesizedthatentrainmentonacoustic/prosodicdimensionssuchaspitch,in-tensity,voicequalityandspeakingratemightalsobecorrelatedwithpositiveaspectsofperceivedsocialbehaviorsaswellasotherperceivedchar-acteristicsofefﬁcient,well-coordinatedconversa-tions.Inthispaperwedescribeaseriesofex-perimentsinvestigatingtherelationshipbetweenob-jectiveacoustic/prosodicdimensionsofentrainmentandmanually-annotatedperceptionofasetofso-cialvariablesdesignedtocaptureimportantas-pectsofconversationalpartners’socialbehaviors.Sincepriorresearchonotherdimensionsofentrain-menthassometimesobserveddifferencesindegreeofentrainmentbetweenfemale-female,male-maleandmixedgendergroups(Bilous&Krauss,1988;Pardo,2006;Namyetal.,2002),wealsoexam-inedourdataforvariationbygenderpair,consid-eringfemale-female,male-male,andfemale-malepairsofspeakersseparately.Ifpreviousﬁndingsextendtoacoustic/prosodicentrainment,wewouldexpectfemale-femalepairstoentraintoagreaterdegreethanmale-malepairsandfemalepartnersinmixedgenderpairstoentrainmorethantheirmalecounterparts.Sincepriorﬁndingspositthatentrain-mentleadstosmootherandmorenaturalconversa-tions,wewouldalsoexpectdegreeofentrainmenttocorrelatewithperceptionofothercharacteristicsdescriptiveofsuchconversations.BelowwedescribethecorpusandannotationsusedinthisstudyandhowoursocialannotationswereobtainedinSections2and3.Wenextdiscussourmethodandresultsfortheprevalenceofentrain-mentamongdifferentgendergroups(Section4).InSections5and6,wepresenttheresultsofcorrelat-ingacousticentrainmentwithsocialvariablesandobjectivesuccessmeasures,respectively.Finally,inSection7,weexploreentrainmentincasesofoutlierfeaturevalues.2TheColumbiaGamesCorpusTheColumbiaGamesCorpus(Gravano&Hirsch-berg,2011)consistsofapproximatelyninehoursofspontaneousdialoguebetweenpairsofsubjectsplayingaseriesofcomputergames.Sixfemalesandsevenmalesparticipatedinthecollectionofthecor-pus;elevenofthesubjectsreturnedonadifferentdayforanothersessionwithanewpartner.Duringthecourseofeachsession,apairofspeak-ersplayedthreeCardsgamesandoneObjectsgame.TheworkdescribedherewascarriedoutontheOb-jectsgames.Thissectionofeachsessiontook7m12sonaverage.Wehaveatotalof4h19mofOb-jectsgamespeechinthecorpus.ForeachtaskinanObjectsgame,theplayerssawidenticalcollectionsofobjectsontheirscreens.However,oneplayer(theDescriber)hadanaddi-tionaltargetobjectpositionedamongtheotherob-jects,whiletheother(theFollower)hadthesameobjectatthebottomofherscreen.TheDescriberwasinstructedtodescribethepositionofthetargetobjectsothattheFollowercouldplaceitinexactlythesamelocationonherscreen.Points(upto100)wereawardedbasedonhowwelltheFollower’star-getlocationmatchedthedescribers.Eachpairofpartnerscompleted14suchtasks,alternatingroleswitheachtask.Thepartnerswereseparatedbyacurtaintoensurethatallcommunicationwasoral.Theentirecorpushasbeenorthographicallytran-scribedandwordsalignedwiththespeechsource.IthasalsobeenToBI-labeled(Silvermanetal.,1992)forprosodicevents,aswellaslabeledforturn-takingbehaviors.3AnnotationofSocialVariablesInordertostudyhowentrainmentinvariousdimen-sionscorrelatedwithperceivedsocialbehaviorsofoursubjects,weaskedAmazonMechanicalTurk1annotatorstolabelthe168Objectsgamesinourcor-pusforanarrayofsocialbehaviorsperceivedforeachofthespeakers,whichwetermhere“socialvariables.”EachHumanIntelligenceTask(HIT)presentedtotheAMTworkersforannotationconsistedofasin-gleObjectsgametask.TobeeligibleforourHITs,1http://www.mturk.com13

annotatorshadtohavea95%successrateonpre-viousAMTHITsandtobelocatedintheUnitedStates.Theyalsohadtocompleteasurveyestab-lishingthattheywerenativeEnglishspeakerswithnohearingimpairments.Theannotatorswerepaid$0.30foreachHITtheycompleted.Overhalfoftheannotatorscompletedfewerthanﬁvehits,andonlyfourcompletedmorethantwenty.Theannotatorslistenedtoanaudioclipofthetask,whichwasaccompaniedbyananimationthatdisplayedabluesquareoragreencircledependingonwhichspeakerwascurrentlytalking.Theywerethenaskedtoansweraseriesofquestionsabouteachspeaker:DoesPersonA/Bbelieves/heisbetterthanhis/herpartner?Makeitdifﬁcultforhis/herpartnertospeak?Seemengagedinthegame?Seemtodis-likehis/herpartner?Iss/heboredwiththegame?Directingtheconversation?Frustratedwithhis/herpartner?Encouraginghis/herpartner?Makinghim/herselfclear?Planningwhats/heisgoingtosay?Polite?Tryingtobeliked?Tryingtodomi-natetheconversation?Theywerealsoaskedques-tionsaboutthedialogueasawhole:Doesitﬂownaturally?Aretheparticipantshavingtroubleun-derstandingeachother?Whichpersondoyoulikemore?Whowouldyouratherhaveasapartner?Aseriesofcheckquestionswithobjectivelyde-terminableanswers(e.g.“WhichspeakeristheDe-scriber?”)wereincludedamongthetargetquestionstoensurethattheannotatorswerecompletingthetaskwithintegrity.HITsforwhichtheannotatorfailedtoanswerthecheckquestionscorrectlyweredisqualiﬁed.Eachtaskwasratedbyﬁveuniqueannotatorswhoanswered”yes”or”no”toeachquestion,yieldingascorerangingfrom0to5foreachsocialvari-able,representingthenumberofannotatorswhoan-swered”yes.”Afullerdescriptionoftheannotationforsocialvariablescanbefoundin(Gravanoetal.,2011).Inthisstudy,wefocusouranalysisonannotationsoffoursocialvariables:•Isthespeakertryingtobeliked?•Isthespeakertryingtodominatetheconversa-tion?•Isthespeakergivingencouragementtohis/herpartner?•Istheconversationawkward?Wecorrelatedannotationsofthesevariableswithanarrayofacoustic/prosodicfeatures.4AcousticentrainmentWeexaminedentrainmentinthisstudyineightacoustic/prosodicfeatures:•Intensitymean•Intensitymax•Pitchmean•Pitchmax•Jitter•Shimmer•Noise-to-harmonicsratio(NHR)•SyllablespersecondIntensityisanacousticmeasurecorrelatedwithperceivedloudness.Jitter,shimmer,andnoise-to-harmonicsratiosarethreemeasuresofvoicequality.Jitterdescribesvaryingpitchinthevoice,whichisperceivedasaroughsound.Shimmerdescribesﬂuc-tuationofloudnessinthevoice.Noise-to-harmonicsratioisassociatedwithperceivedhoarseness.Allfeatureswerespeaker-normalizedusingz-scores.Foreachtask,wedeﬁneentrainmentbetweenpartnersoneachfeaturefasENTp=−|speaker1f−speaker2f|wherespeaker[1,2]frepresentsthecorrespondingspeaker’smeanforthatfeatureoverthetask.Wesaythatthecorpusshowsevidenceofen-trainmentonfeaturefifENTp,thesimilaritiesbe-tweenpartners,aresigniﬁcantlygreaterthanENTx,thesimilaritiesbetweennon-partners:ENTx=−Pi|speaker1f−Xi,f||X|whereXisthesetofspeakersofsamegenderandroleasthespeaker’spartnerwhoarenotpairedwiththespeakerinanysession.Werestrictthecompar-isonstospeakersofthesamegenderandroleasthespeaker’spartnertocontrolforthefactthatdiffer-encesmaysimplybeduetodifferencesingenderorrole.Theresultsofaseriesofpairedt-testscompar-ingENTpandENTxforeachfeaturearesummarizedinTable1.14

FeatureFFMMFMIntensitymeanXXXIntensitymaxXXXPitchmeanXPitchmaxXJitterXXShimmerXXNHRXSyllablespersecXXXTable1:Evidenceofentrainmentforgenderpairs.Atickindicatesthatthedatashowsevidenceofentrainmentonthatrow’sfeatureforthatcolumn’sgenderpair.Weﬁndthatfemale-femalepairsinourcorpusentrainon,indescendingorderofsigniﬁcance,jitter,intensitymax,intensitymean,syllablespersecondandshimmer.TheydonotentrainonpitchmeanormaxorNHR.Male-malepairsshowtheleastevidenceofentrainment,entrainingonlyoninten-sitymean,intensitymax,andsyllablespersecond,supportingthehypothesisthatentrainmentislessprevalentamongmales.Female-malepairsentrainon,againindescendingorderofsigniﬁcance,inten-sitymean,intensitymax,jitter,syllablespersecond,pitchmean,NHR,shimmer,andpitchmax–infact,oneveryfeatureweexamine,withsigniﬁcanceval-uesineachcaseofp<0.01.Tolookmorecloselyattheentrainmentbehaviorofmalesandfemalesinmixed-genderpairs,wede-ﬁneENT2pasfollows:ENT2p=−Pi|Pi,f−Ti,f||T|whereTisthesetofthepause-freechunksofspeechthatbeginaspeaker’sturns,andPisthecorrespond-ingsetofpause-freechunksthatendtheinterlocu-tor’sprecedingturns.UnlikeENTp,thismeasureisasymmetric,allowingustoconsidereachmemberofapairseparately.WecompareENT2pforeachfeatureformalesandfemalesofmixedgenderpairs.Contrarytoourhy-pothesisthatfemalesinmixed-genderpairswouldentrainmore,wefoundnosigniﬁcantdifferencesinpartnergender.Femalesinmixed-genderpairsdonotmatchtheirinterlocutor’spreviousturnanymorethandomales.ThismaybeduetothefactFeatureFMMMFpIntensitymean↑↓3.830.02Intensitymax↑↓4.010.02Syllablespersec↓↓2.560.08Table2:Effectsofgenderpaironentrainment.Anarrowpointingupindicatesthatthegroup’snormalizedentrain-mentforthatfeatureisgreaterthanthatoffemale-femalepairs;anarrowpointingdownindicatesthatitissmaller.that,asshowninTable1,theoveralldifferencesbe-tweenpartnersinmixed-genderpairsarequitelow,andsoneitherpartnermaybedoingmuchturn-by-turnmatching.However,asweexpected,entrainmentisleastprevalentamongmale-malepairs.Althoughweex-pectedfemale-femalepairstoexhibitthehighestprevalenceofentrainment,theydonotshowevi-denceofentrainmentonpitchmean,pitchmaxorNHR,whilefemale-malepairsentrainoneveryfea-ture.Infact,althoughENTpforthesefeaturesisnotsigniﬁcantlysmallerbetweenfemale-femalepairsthanbetweenfemale-malepairs,ENTx,theoverallsimilarityamongnon-partnersforthesefeatures,issigniﬁcantlylargerbetweenfemalesthanbetweenfemalesandmales.Thedegreeofsimilaritybetweenfemale-femalepartnersisthereforeattributabletotheoverallsimilaritybetweenfemalesratherthantheeffectofentrainment.Allthreetypesofpairsexhibitentrainmentonin-tensitymean,intensitymax,andsyllablespersec-ond.Welookmorecloselyintothegender-baseddifferencesinentrainmentbehaviorwithanANOVAwiththeratioofENTptoENTxasthedependentvariableandgenderpairastheindependentvariable.NormalizingENTpbyENTxallowsustocomparethedegreeofentrainmentacrossgenderpairs.Re-sultsareshowninTable2.Male-malepairshavelowerentrainmentthanfemale-femalepairsforev-eryfeature;female-malepairshavehigherentrain-mentthanfemale-femalepairsforintensitymeanandmaxandlowerforsyllablespersecond(p<0.1).Theseresultsareconsistentwiththegeneralﬁndingthatmale-malepairsentraintheleastandfemale-malepairsentrainthemost.15

5EntrainmentandsocialbehaviorWenextcorrelateeachofthesocialvariablesde-scribedinSection3withENTpforoureightacous-ticfeatures.BasedonCommunicationAccommo-dationTheory,wewouldexpectgivesencourage-ment,avariablerepresentingadesirablesocialchar-acteristic,tobepositivelycorrelatedwithentrain-ment.Conversely,conversationawkwardshouldbenegativelycorrelatedwithentrainment.WenotethatTryingtobelikedisnegativelycorrelatedwiththelikemorevariableinourdata–thatis,annotatorswerelesslikelytopreferspeakerswhomtheyper-ceivedastryingtobeliked.Thisreﬂectsthein-tuitionthatsomeoneoverlyeagertobelikedmaybeperceivedasannoyingandsociallyinept.How-ever,similarity-attractiontheorystatesthatsimilar-itypromotesattraction,andsomeonemightthere-foreentraininordertoobtainhispartner’ssocialapproval.ThisideaissupportedbyNatale’sﬁnd-ingthattheneedforsocialapprovalispredictiveofthedegreeofaspeaker’sconvergenceoninten-sity(Natale,1975).Wecanthereforeexpecttryingtobelikedtopositivelycorrelatewithentrainment.Speakerswhoareperceivedastryingtodominatemaybeoverlyentrainingtotheirinterlocutorsinwhatissometimescalled“dependencyoveraccom-modation.”Dependencyoveraccommodationcausestheinterlocutortoappeardependentonthespeakerandgivestheimpressionthatthespeakeriscontrol-lingtheconversation(West&Turner,2009).Theresultsofourcorrelationsofsocialvari-ableswithacoustic/prosodicentrainmentaregen-erallyconsonantwiththeseintuitions.Althoughitisnotstraightforwardtocomparecorrelationcoefﬁ-cientsofgroupsforwhichwehavevaryingamountsofdata,forpurposesofassessingtrends,wewillconsideracorrelationstrongifitissigniﬁcantatthep<0.00001level,moderateatthep<0.01level,andweakatthep<0.05level.Theresultsaresum-marizedinTable3;wepresentonlythesigniﬁcantresultsforspaceconsiderations.Forfemale-femalepairs,givingencouragementisweaklycorrelatedwithentrainmentonintensitymaxandshimmer.Conversationawkwardisweaklycorrelatedwithentrainmentonjitter.Formale-malepairs,tryingtobelikedismoderatelycorrelatedwithentrainmentonintensitymeanandweaklycor-relatedwithentrainmentonjitterandNHR.Giv-ingencouragementismoderatelycorrelatedwithentrainmentonintensitymean,intensitymax,andNHR.Forfemale-malepairs,tryingtobelikedismoderatelycorrelatedwithentrainmentonpitchmean.Givingencouragementisstronglycorre-latedwithentrainmentonintensitymeanandmaxandmoderatelycorrelatedwithentrainmentonpitchmeanandshimmer.However,itisnegativelycor-relatedwithentrainmentonjitter,althoughthecor-relationisweak.Conversationawkwardisweaklycorrelatedwithentrainmentonjitter.Asweexpected,givingencouragementiscorre-latedwithentrainmentforallthreegendergroups,andtryingtobelikediscorrelatedwithentrainmentformale-maleandfemale-malegroups.However,tryingtodominateisnotcorrelatedwithentrainmentonanyfeature,andconversationawkwardisactu-allypositivelycorrelatedwithentrainmentonjitter.Entrainmentonjitterisaclearoutlierhere,withallofitscorrelationscontrarytoourhypotheses.Inadditiontobeingpositivelycorrelatedwithconver-sationawkward,itistheonlyfeaturetobenega-tivelycorrelatedwithgivingencouragement.Entrainmentiscorrelatedwiththemostsocialvariablesforfemale-malepairs;thesecorrelationsarealsothestrongest.Wethereforeconcludethatacousticentrainmentisnotonlymostprevalentformixed-genderpairs,itisalsomoreimportanttotheperceptionoffemale-malesocialbehaviorthanitisforsame-genderpairs.6EntrainmentandobjectivemeasuresofdialoguesuccessWenowexamineacoustic/prosodicentrainmentinourcorpusaccordingtofourobjectivemeasuresofdialoguesuccess:themeanlatencybetweenturns,thepercentageofturnsthatareinterruptions,thepercentageofturnsthatareoverlaps,andthenumberofturnsinatask.Highlatencybetweenturnscanbeconsideredasignofanunsuccessfulconversation,withpoorturn-takingbehaviorindicatingapossiblelackofrapportanddifﬁcultyincommunicationbetweenthepart-ners.Ahighpercentageofinterruptions,anotherex-ampleofpoorturn-takingbehavior,maybeasymp-tomoforareasonforhostilityorawkwardnessbe-16

SocialAcousticdfrpFemale-FemaleGivingInt.max-0.240.03enc.Shimmer-0.240.03Conv.Jitter-0.230.03awkwardMale-MaleTryingtoInt.mean-0.300.006belikedJitter-0.270.01NHR-0.230.03GivingInt.mean-0.390.0003enc.Int.max-0.310.005NHR-0.300.005Female-MaleTryingtoPitchmean-0.260.001belikedGivingInt.mean-0.362.8e-06enc.Int.max-0.317.7e-05Pitchmean-0.230.003Jitter0.190.02Shimmer-0.160.04Conv.Jitter-0.170.04awkwardTable3:Correlationsbetweenentrainmentandsocialvariables.tweenpartners.Weexpectthesemeasurestobeneg-ativelycorrelatedwithentrainment.Conversely,ahighpercentageofoverlapsmaybeasymptomofawell-coordinatedconversationthatisﬂowingeas-ily.Intheguidelinesfortheturn-takingannotationoftheGamesCorpus(Gravano,2009),overlapsaredeﬁnedascasesinwhichSpeaker2takestheﬂoor,overlappingwiththecompletionofSpeaker1’sut-terance.Overlapsrequirethesuccessfulreadingofturn-takingcuesandbydeﬁnitionprecludeawkwardpauses.Weexpectahighpercentageofoverlapstocorrelatepositivelywithentrainment.Thenumberofturnsinataskcanbeinterpretedeitherpositivelyornegatively.Ahighnumberisnegativeinthatitisthesignofaninefﬁcientdia-logue,onewhichtakesmanyturnexchangestoac-complishtheobjective.However,itmayalsobethesignofeasy,ﬂowingdialoguebetweenthepart-ners.Inourdomain,itmayalsobeasignofahigh-achievingpairwhoareplacingtheobjectmeticu-ObjectiveAcousticdfrpFemale-FemaleLatencyInt.mean0.220.04Int.max0.310.005Pitchmean0.240.02Jitter0.290.007Shimmer0.330.002Syllables/sec0.390.0002#TurnsInt.max-0.300.006Shimmer-0.340.002NHR-0.240.03Syllables/sec-0.280.01%OverlapsInt.max-0.230.04Shimmer-0.300.005%InterruptionsShimmer-0.330.005Male-MaleLatencyInt.mean0.578.8e-08Int.max0.430.0001Pitchmean0.522.4e-06Pitchmax0.615.7e-09Jitter0.654.5e-10NHR0.400.0004#TurnsInt.mean-0.290.0002Pitchmean-0.320.003Pitchmax-0.290.007NHR-0.477.9e-06Syllables/sec-0.250.02%OverlapsInt.mean-0.390.0002Int.max-0.390.0002%InterruptionsNHR-0.330.002Female-Male#TurnsInt.mean-0.240.003Int.max-0.190.02Shimmer-0.160.04%OverlapsShimmer-0.260.001Table4:Correlationsbetweenentrainmentandobjectivevariables.louslyinordertosecureeverysinglepoint.Wethereforeexpectthenumberofturnstobepositivelycorrelatedwithentrainment.Asbefore,wecon-sideracorrelationstrongifitissigniﬁcantatthep<0.00001level,moderateatthep<0.01level,andweakatthep<0.05level.Thesigniﬁcantcor-relationsarepresentedinTable4.Forfemale-femalepairs,meanlatencybetween17

turnsisnegativelycorrelatedwithentrainmentonallvariablesexceptpitchmaxandNHR.Thecorrela-tionsareweakforintensitymeanandpitchmeanandmoderateforintensitymax,jitter,shimmer,andsyllablespersecond.Thenumberofturnsismoder-atelycorrelatedwithentrainmentonintensitymaxandshimmerandweaklycorrelatedwithentrain-mentonsyllablespersecond.Contrarytoourexpec-tations,thepercentageofinterruptionsispositively(thoughmoderately)correlatedwithentrainmentonshimmer;thepercentageofoverlapsismoderatelycorrelatedwithentrainmentonshimmerandweaklycorrelatedwithentrainmentonintensitymax.Male-malepairsshowthemostcorrelationsbe-tweenentrainmentandobjectivemeasuresofdia-loguesuccess.Thelatencybetweenturnsisneg-ativelycorrelatedwithentrainmentonallvariablesexceptshimmerandsyllablespersecond;thecorre-lationsaremoderateforintensitymaxandNHRandstrongfortherest.Thenumberofturnsinataskispositivelycorrelatedwithentrainmentoneveryvariableexceptintensitymean,jitterandshimmer:stronglyforNHR;moderatelyforintensitymean,pitchmean,andpitchmax;andweaklyforsyllablespersecond..Thepercentageofoverlapsismoder-atelycorrelatedwithentrainmentonintensitymeanandmax.Thepercentageofinterruptionsismoder-atelycorrelatedwithentrainmentonNHR.Forfemale-malepairs,thenumberofturnsismoderatelycorrelatedwithentrainmentonintensitymeanandweaklycorrelatedwithentrainmentonin-tensitymaxandshimmer.Thepercentageofover-lapsismoderatelycorrelatedwithentrainmentonshimmer.Forthemostpart,thedirectionsofthecorrela-tionswehavefoundareinaccordancewithourhy-potheses.Latencyisnegativelycorrelatedwithen-trainmentandoverlapsandthenumberofturnsarepositivelycorrelated.Apuzzlingexceptionisthepercentageofinterruptions,whichispositivelycor-relatedwithentrainmentonshimmer(forfemale-femalepairs)andNHR(formale-malepairs).Whilethestrongestcorrelationswereformixed-genderpairsforthesocialvariables,weﬁndthatthestrongestcorrelationsforobjectivevariablesareformale-malepairs,whichalsohavethegreat-estnumberofcorrelations.Itthereforeseemsthatwhileentrainmentismoreimportanttothepercep-tionofsocialbehaviorformixed-genderpairsthanitisforsame-genderpairs,itismoreimportanttothesmoothnessandﬂowofdialogueformale-malepairsthanitisforfemale-femaleorfemale-malepairs.7EntrainmentinoutliersSinceacousticentrainmentisgenerallyconsideredanunconsciousphenomenon,itisinterestingtocon-sidertasksinwhichaparticularfeatureofaperson’sspeechisparticularlysalient.Thiswilloccurwhenafeaturedifferssigniﬁcantlyfromthenorm–forex-ample,whenaperson’svoiceisunusuallyloudorsoft.ChartrandandBargh(1999)suggestthatthepsychologicalmechanismbehindtheentrainmentistheperception-behaviorlink,theﬁndingthattheactofobservinganother’sbehaviorincreasesthelike-lihoodoftheobserver’sengaginginthatbehavior.Basedonthisﬁnding,wehypothesizethatapart-nerpaircontainingone“outlier”speakerwillexhibitmoreentrainmentonthesalientfeature,sincethatfeatureismorelikelytobeobservedandthereforeimitated.Weconsidervaluesinthe10thor90thpercentileforafeature“outliers.”WecanconsiderENTx,thesimilaritybetweenaspeakerandthespeakersofherpartner’sroleandgenderwithwhomsheisneverpaired,the“baseline”valueforthesimilaritybe-tweenaspeakerandherinterlocutorwhennoen-trainmentoccurs.ENTp−ENTx,thedifferencebe-tweenthesimilarityexistingbetweenpartnersandthebaselinesimilarity,isthenameasureofhowmuchentrainmentexistsrelativetobaseline.WecompareENTp−ENTxfor“normal”versus“outlier”speakers.ENTpshouldbesmallerforout-lierspeakers,sincetheirinterlocutorsarenotlikelytobesimilarlyunusual.However,ENTxshouldalsobelowerforoutlierspeakers,sincebydeﬁnitiontheydivergefromthenorm,whilethenormalspeakersbydeﬁnitionrepresentthenorm.Itisthereforerea-sonabletoexpectENTp−ENTxtobethesameforoutlierspeakersandnormalspeakers.IfENTp−ENTxishigherforoutlierspeakers,thatmeansthatENTpishigherthanweexpect,andentrainmentisgreaterrelativetobaselineforpairscontaininganoutlierspeaker.IfENTp−ENTxislowerforoutlierspeakers,thatmeansthatENTpis18

AcoustictdfpIntensitymean5.6694.261.7e-07Intensitymax8.29152.055.5e-14Pitchmean-1.2076.82N.S.Pitchmax-0.8476.76N.S.Jitter0.3670.23N.S.Shimmer2.64102.230.02NHR-0.92137.34N.S.Syllablespersec2.4172.600.02Table5:T-testsforrelativeentrainmentforoutliervs.normalspeakers.lowerthanweexpect,andpairscontaininganoutlierspeakerentrainlessthandopairsofnormalspeak-ers,evenallowingforthefactthattheirusualvaluesshouldbefurtheraparttobeginwith.Theresultsfort-testscomparingENTp−ENTxfor“normal”versus“outlier”speakersareshowninTable5.Outlierpairshavehigherrelativeen-trainmentthandonormalpairsforintensitymeanandmax,shimmer,andsyllablespersecond.Thismeansthatspeakersconfrontedwithaninterlocutorwhodivergeswidelyfromthenormforthosefourfeaturesmakealargeradjustmenttotheirspeechinordertoconvergetothatinterlocutor.AnANOVAshowsthatrelativeentrainmentonintensitymaxishigherinoutliercasesformale-malepairsthanforfemale-femalepairsandevenhigherforfemale-malepairs(F=11.33,p=5.3e-05).RelativeentrainmentonNHRinthesecasesislowerformale-malepairsthanforfemale-femalepairsandhigherforfemale-malepairs(F=11.41,p=6.5e-05).Relativeentrainmentonsyllablespersecondislowerformale-malepairsandhigherforfemale-malepairs(F=5.73,p=0.005).TheseresultsdifferslightlyfromtheresultsinTable2fordifferencesinentrainmentinthegeneralcaseamonggenderpairs,reinforcingtheideathatcasesinwhichfea-turevaluesdivergewidelyfromthenormareuniqueintermsofentrainmentbehavior.8ConclusionOurstudyofentrainmentonacoustic/prosodicvari-ablesyieldsnewﬁndingsaboutentrainmentbe-haviorforfemale-female,male-male,andmixed-genderdyads,aswellastheassociationofentrain-mentwithperceivedsocialcharacteristicsandob-jectivemeasuresofdialoguesmoothnessandefﬁ-ciency.Weﬁndthatentrainmentisthemostpreva-lentformixed-genderpairs,followedbyfemale-femalepairs,withmale-malepairsentrainingtheleast.Entrainmentisthemostimportanttotheper-ceptionofsocialbehaviorofmixed-genderpairs,anditisthemostimportanttotheefﬁciencyandﬂowofmale-maledialogues.Forthemostpart,thedirectionsofthecorrela-tionsofentrainmentwithsuccessvariablesaccordwithhypothesesmotivatedbytherelevantliterature.Givingencouragementandtryingtobelikedarepositivelycorrelatedwithentrainment,asareper-centageofoverlapsandnumberofturns.Meanla-tency,asymptomofapoorly-runconversation,isnegativelyassociatedwithentrainment.However,severalexceptionssuggestthattheassociationsarenotstraightforwardandfurtherresearchmustbedonetofullyunderstandtherelationshipbetweenentrainment,socialcharacteristicsanddialoguesuc-cess.Inparticular,theexplanationbehindtheas-sociationsofentrainmentoncertainvariableswithcertainsocialandobjectivemeasuresisaninterest-ingdirectionforfuturework.Finally,weﬁndthatin“outlier”caseswhereaparticularspeakerdivergeswidelyfromthenormforintensitymean,intensitymax,orsyllablespersec-ond,entrainmentismorepronounced.Thissupportsthetheorythattheperception-behaviorlinkisthemechanismbehindentrainmentandprovidesapos-sibledirectionforresearchintowhyspeakersentrainoncertainfeaturesandnotothers.Infutureworkwewillexplorethisdirectionandgomorethoroughlyintoindividualdifferencesinentrainmentbehavior.AcknowledgmentsThismaterialisbaseduponworksupportedinpartbyNSFIIS-0307905,NSFIIS-0803148,UBACYT20020090300087,ANPCYTPICT-2009-0026,CONICET,VEGANo.2/0202/11;andtheEUSF(ITMS26240220060).ReferencesAmazonMechanicalTurk,http://www.mturk.com.FrancesR.BilousandRobertM.Krauss1988.Dom-inanceandaccommodationintheconversationalbe-19

havioursofsame-andmixed-genderdyads.LanguageandCommunication,8(3/4):183–194.SusanE.BrennanandHerbertH.Clark.1996.Concep-tualPactsandLexicalChoiceinConversation.Jour-nalofExperimentalPsychology:Learning,MemoryandCognition,22(6):1482–1493.TanyaL.ChartrandandJohnA.Bargh.1999.TheChameleonEffect:ThePerception-BehaviorLinkandSocialInteraction.JournalofPersonalityandSocialPsychology,76(6):893–910.CristianDanescu-Niculescu-Mizil,MichaelGamon,andSusanDumais.2011.MarkMyWords!LinguisticStyleAccommodationinSocialMedia.ProceedingsofWWW2011.H.GilesandN.Coupland.1991.Language:ContextsandConsequences.PaciﬁcGrove,CA:Brooks/Cole.Agust´ınGravano.2009.Turn-TakingandAfﬁrmativeCueWordsinTask-OrientedDialogue.Ph.D.thesis,ColumbiaUniversity,NewYork.Agust´ınGravanoandJuliaHirschberg.2011.Turn-takingcuesintask-orienteddialogue.ComputerSpeechandLanguage,25(3):601–634.Agust´ınGravano,RivkaLevitan,LauraWillson,˘StefanBe˘nu˘s,JuliaHirschberg,AniNenkova.2011.Acous-ticandProsodicCorrelatesofSocialBehavior.Inter-speech2011.Chi-ChunLee,MatthewBlack,AthanasiosKatsama-nis,AdamLammert,BrianBaucom,AndrewChris-tensen,PanayiotisG.Georgiou,ShrikanthNarayanan.2010.QuantiﬁcationofProsodicEntrainmentinAf-fectiveSpontaneousSpokenInteractionsofMarriedCouples.EleventhAnnualConferenceoftheInterna-tionalSpeechCommunicationAssociation.RivkaLevitan,Agust´ınGravano,andJuliaHirschberg.2011.EntrainmentinSpeechPrecedingBackchan-nels.ProceedingsofACL/HLT2011.RivkaLevitanandJuliaHirschberg.2011.Measuringacoustic-prosodicentrainmentwithrespecttomulti-plelevelsanddimensions.ProceedingsofInterspeech2011.LauraL.Namy,LynneC.Nygaard,DeniseSauerteig.2002.Genderdifferencesinvocalaccommodation:theroleofperception.JournalofLanguageandSo-cialPsychology,21(4):422–432.MichaelNatale.1975.ConvergenceofMeanVocalIn-tensityinDyadicCommunicationasaFunctionofSo-cialDesirability.JournalofPersonalityandSocialPsychology,32(5):790–804.AniNenkova,Agust´ınGravano,andJuliaHirschberg.2008.High-frequencywordentrainmentinspokendi-alogue.ProceedingsofACL/HLT2008.KateG.NiederhofferandJamesW.Pennebaker.2002.Linguisticstylematchinginsocialinteraction.Jour-nalofLanguageandSocialPsychology,21(4):337–360.JenniferS.Pardo.2006.Onphoneticconvergencedur-ingconversationalinteraction.JournaloftheAcousti-calSocietyofAmerica,119(4):2382–2393.DavidReitter,JohannaD.Moore,andFrankKeller.1996.PrimingofSyntacticRulesinTask-OrientedDi-alogueandSpontaneousConversation.Proceedingsofthe28thAnnualConferenceoftheCognitiveScienceSociety.KimSilverman,MaryBeckman,JohnPitrelli,MoriOs-tendorf,ColinWightman,PattiPrice,JanetPierrehum-bert,JuliaHirschberg.1992.TOBI:AStandardforLabelingEnglishProsody.ICSLP-1992,867-870.LindaTickle-DegnenandRobertRosenthal.1990.TheNatureofRapportanditsNonverbalCorrelates.Psy-chologicalInquiry,1(4):285–293.RichardWest&LynnTurner.2009.IntroducingCommunicationTheory:AnalysisandApplication.McGraw-HillHumanities/SocialSciences/Languages,4thedition.2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20–28,

Montr´eal, Canada, June 3-8, 2012. c(cid:13)2012 Association for Computational Linguistics

20

IdentifyingHigh-LevelOrganizationalElementsinArgumentativeDiscourseNitinMadnaniMichaelHeilmanJoelTetreaultEducationalTestingServicePrinceton,NJ,USA{nmadnani,mheilman,jtetreault}@ets.orgMartinChodorowHunterCollegeofCUNYNewYork,NY,USAmartin.chodorow@hunter.cuny.eduAbstractArgumentativediscoursecontainsnotonlylanguageexpressingclaimsandevidence,butalsolanguageusedtoorganizetheseclaimsandpiecesofevidence.Differentiatingbe-tweenthetwomaybeusefulformanyappli-cations,suchasthosethatfocusonthecontent(e.g.,relationextraction)ofargumentsandthosethatfocusonthestructureofarguments(e.g.,automatedessayscoring).Weproposeanautomatedapproachtodetectinghigh-levelorganizationalelementsinargumentativedis-coursethatcombinesarule-basedsystemandaprobabilisticsequencemodelinaprincipledmanner.Wepresentquantitativeresultsonadatasetofhuman-annotatedpersuasiveessays,andqualitativeanalysesofperformanceones-saysandonpoliticaldebates.1IntroductionWhenpresentinganargument,awriterorspeakerusuallycannotsimplystatealistofclaimsandpiecesofevidence.Instead,thearguermustexplic-itlystructurethoseclaimsandpiecesofevidence,aswellasexplainhowtheyrelatetoanopponent’sar-gument.Considerexample1below,adaptedfromanessayrebuttinganopponent’sargumentthatgriz-zlybearslivedinaspeciﬁcregionofCanada.Theargumentstatesthatbasedontheresultoftherecentresearch,thereproba-blyweregrizzlybearsinLabrador.Itmayseemreasonableatﬁrstglance,butac-tually,therearesomelogicalmistakesinit....Thereisapossibilitythattheywereathirdkindofbearapartfromblackandgrizzlybears.Also,theexplorerac-countswererecordedinthenineteenthcentury,whichwasmorethan100yearsago....Insum,theconclusionofthisargumentisnotreasonablesincetheac-countandtheresearcharenotconvinc-ingenough....Theargumentbeginsbyexplicitlyrestatingtheopponent’sclaim,prefacingtheclaimwiththephrase“Theargumentstatesthat.”Then,thesec-ondsentenceexplicitlymarkstheopponent’sargu-mentasﬂawed.Lateron,thephrase“Thereisapossibilitythat”indicatesthesubsequentclausein-troducesevidencecontrarytotheopponent’sclaim.Finally,thesentence“Insum,...”sumsupthear-guer’sstanceinrelationtotheopponent’sclaim.1Asillustratedintheaboveexample,argumenta-tivediscoursecanbeviewedasconsistingoflan-guageusedtoexpressclaimsandevidence,andlanguageusedtoorganizethem.Webelievethatdifferentiatingorganizationalelementsfromcontentwouldbeusefulforanalyzingpersuasivediscourse.1ThewordAlsosignalsthatadditionalevidenceisabouttobepresentedandshouldalsobemarkedasshell.However,itwasnotmarkedinthisspeciﬁccasebyourhumanannotator(§3.2).21

Werefertosuchorganizationalelementsasshell,in-dicatingthattheydifferfromthespeciﬁcclaimsandevidence,or“meat,”ofanargument.Inthiswork,wedeveloptechniquesfordetectingshellintexts.Weenvisionpotentialapplicationsinpoliticalsci-ence(e.g.,tobetterunderstandpoliticaldebates),in-formationextractionorretrieval(e.g.,tohelpasys-temfocusoncontentratherthanorganization),andautomatedessayscoring(e.g.,toanalyzethequalityofatest-taker’sargument),thoughadditionalworkisneededtodetermineexactlyhowtointegrateourapproachintosuchapplications.Detectingorganizationalelementscouldalsobeaﬁrststepinparsinganargumenttoinferitsstructure.Wefocusonthisinitialstep,leavingtheotherstepsofcategorizationofspans(astowhethertheyevalu-atetheopponent’sclaims,connectone’sownclaims,etc.),andtheinferenceofargumentationstructuretofuturework.Beforedescribingourapproachtoidentifyingshell,webeginbydeﬁningit.Shellreferstose-quencesofwordsusedtorefertoclaimsandevi-denceinpersuasivewritingorspeaking,providinganorganizationalframeworkforanargument.Itmaybeusedbythewriterorthespeakerinthefol-lowingways:•todeclareone’sownclaims(e.g.,“Thereisthepossibilitythat”)•torestateanopponent’sclaims(e.g.,“Theargu-mentstatesthat”)•toevaluateanopponent’sclaims(e.g.,“Itmayseemreasonableatﬁrstglance,butactually,therearesomelogicalmistakesinit”)•topresentevidenceandrelateittospeciﬁcclaims(e.g.,“Toillustratemypoint,Iwillnowgivetheexampleof”)Therearemanywaysofanalyzingdiscourse.Themostrelevantisperhapsrhetoricalstructuretheory(RST)(MannandThompson,1988).Toourknowl-edge,theRSTparserfromMarcu(2000)istheonlyRSTparserreadilyavailableforexperimentation.TheparseristrainedtomodeltheRSTcorpus(Carl-sonetal.,2001),whichtreatscompleteclauses(i.e.,clauseswiththeirobligatorycomplements)astheel-ementaryunitsofanalysis.Thus,theparsertreatstheﬁrstsentenceinexample1asasingleunitanddoesnotdifferentiatebetweenthemainandsubordi-nateclauses.Incontrast,ourapproachdistinguishesthesequence“Theargumentstatesthat...”asshell(whichisusedheretorestatetheexternalclaim).Furthermore,weidentifytheentiresecondsentenceasshell(here,usedtoevaluatetheexternalclaim),whereastheRSTparsersplitsthesentenceintotwoclauses,“Itmayseem...”and“butactually...”,linkedbya“contrast”relationship.2Finally,ourapproachfocusesonexplicitmarkersoforganiza-tionalstructureinarguments,whereasRSTcoversabroaderrangeofdiscourseconnections(e.g.,elabo-ration,backgroundinformation,etc.),includingim-plicitones.(Notethatadditionalrelatedworkisde-scribedin§6.)Thisworkmakesthefollowingcontributions:•Wedescribeaprincipledapproachtothetaskofdetectinghigh-levelorganizationalelementsinargumentativediscourse,combiningrulesandaprobabilisticsequencemodel(§2).•Weconductexperimentstovalidatetheapproachonanannotatedsampleofessays(§3,§4).•Wequalitativelyexplorehowtheapproachper-formsinanewdomain:politicaldebate(§5).2DetectionMethodsInthissection,wedescribethreeapproachestotheproblemofshelldetection:arule-basedsystem(§2.1),asupervisedprobabilisticsequencemodel(§2.2),andasimplelexicalbaseline(§2.3).2.1Rule-basedsystemWebeginbydescribingaknowledge-basedap-proachtodetectingorganizationalelementsinargu-mentativediscourse.Thisapproachusesasetof25hand-writtenregularexpressionpatterns.3Inordertodevelopthesepatterns,wecreatedasampleof170annotatedessaysacross57distinctprompts.4Theessayswerewrittenbytest-takersofastandardizedtestforgraduateadmissions.Thissampleofessayswassimilarinnaturetobutdidnotoverlapwiththosediscussedinothersections2WeusedtheRSTparserofMarcu(2000)toanalyzetheoriginalessayfromwhichtheexamplewasadapted.3WeusethePyParsingtoolkittoparsesentenceswiththegrammarfortherulesystem.4Promptsareshorttextsthatpresentanargumentorissueandasktesttakerstorespondtoit,eitherbyanalyzingthegivenargumentortakingastanceonthegivenissue.22

MODAL→do|don’t|can|cannot|will|would|...ADVERB→strongly|totally|fundamentally|vehemently|...AGREEVERB→disagree|agree|concur|...AUTHORNOUN→writer|author|speaker|...SHELL→I[MODAL][ADVERB]AGREEVERBwiththeAUTHORNOUNFigure1:Anexamplepatternthatrecognizesshelllanguagedescribingtheauthor’spositionwithrespecttoanoppo-nent’s,e.g.,ItotallyagreewiththeauthororIwillstronglydisagreewiththespeaker.ofthepaper(§2.2,§3.2).Theannotationswerecar-riedoutbyindividualsexperiencedinscoringper-suasivewriting.Noformalannotationguidelineswereprovided.Besidesshelllanguage,therewereotherannotationsrelevanttoessayscoring.How-ever,weignoredthemforthisstudybecausetheyarenotdirectlyrelevanttothetaskofshelllanguagedetection.Fromthissample,wecomputedlistsofn-grams(n=1,2,...,9)thatoccurredmorethanonceinessaysfromatleasthalfofthe57distinctessayprompts.Wethenwroterulestorecognizetheshelllanguagepresentinthen-gramlists.Additionalruleswereaddedtocoverinstancesofshellthatweobservedintheannotatedessaysbutthatwerenotfrequentenoughtoappearinthen-gramanalysis.Weuse“Rules”torefertothismethod.2.2SupervisedSequenceModelThenextapproachwedescribeisasupervised,prob-abilisticsequencemodelbasedonconditionalran-domﬁelds(CRFs)(Laffertyetal.,2001),usingasmallnumberofgeneralfeaturesbasedonlexicalfrequencies.WeassumeaccesstoalabeleddatasetofNexamples(w,y)indexedbyi,containingse-quencesofwordsw(i)andsequencesoflabelsy(i),withindividualwordsandlabelsindexedbyj(§3describesourdevelopmentandtestingsets).y(i)isasequenceofbinaryvalues,indicatingwhethereachwordw(i)jinthesequenceisshell(y(i)j=1)ornot(y(i)j=0).FollowingLaffertyetal.(2001),weﬁndaparametervectorθthatmaximizesthefollowinglog-likelihoodobjectivefunction:L(θ|w,y)=NXi=1logp(cid:16)y(i)|w(i),θ(cid:17)(1)=NXi=1(cid:16)θ>f(w(i),y(i))−logZ(i)(cid:17)ThenormalizationconstantZiisasumoverallpossiblelabelsequencesfortheithexample,andfisafeaturefunctionthattakespairsofwordandla-belsequencesandreturnsavectoroffeaturevalues,equalindimensionstothenumberofparametersinθ.5Thefeaturevaluesforthejthwordandlabelpairareasfollows(thesearesummedoverallelementstocomputethevaluesofffortheentiresequence):•Therelativefrequencyofw(i)jintheBritishNa-tionalCorpus.•Therelativefrequencyofw(i)jinasetof100,000essays(seebelow).•Eightbinaryfeaturesforwhethertheabovefre-quenciesmeetorexceedthefollowingthresholds:10{−6,−5,−4,−3}.•Theproportionofpromptsforwhichw(i)jap-pearedinatleastoneessayaboutthatpromptinthesetof100,000.•Threebinaryfeaturesforwhethertheabovepro-portionofpromptsmeetsorexceedsthefollowingthresholds:{0.25,0.50,0.75}.•Abinaryfeaturewithvalue1ifw(i)jconsistsonlyoflettersa-z,and0otherwise.Thisfeaturedis-tinguishespunctuationandnumbersfromotherto-kens.5WeusedCRFsuite0.12(Okazaki,2007)toimplementtheCRFmodel.23

•Abinaryfeaturewithvalue1iftherule-basedsys-tempredictsthatw(i)jisshell,and0otherwise.•Abinaryfeaturewithvalue1iftherule-basedsys-tempredictsthatw(i)j−1isshell,and0otherwise.•Twobinaryfeaturesforwhetherornotthecurrenttokenwastheﬁrstorlastinthesentence,respec-tively.•Fourbinaryfeaturesforthepossibletransitionsbetweenpreviousandcurrentlabels(y(i)jandy(i)j−1,respectively).Todeﬁnethefeaturesrelatedtoessaypromptsandlexicalfrequenciesinessays,wecreatedasetof100,000essaysfromalargersetofessayswrittenbytest-takersofastandardizedtestforgraduatead-missions(thesamedomainasin§2.1).Theessayswerewritteninresponseto228differentpromptsthataskedstudentstoanalyzevariousissuesorar-guments.Weuseadditionalessayssampledfromthissourcelatertoacquireannotatedtrainingandtestdata(§3.2).Wedevelopedtheabovefeaturesetusingcross-validationonourdevelopmentset(§3).Theintu-itionbehinddevelopingthewordfrequencyfeaturesisthatshelllanguagegenerallyconsistsofchunksofwordsthatoccurfrequentlyinpersuasivelanguage(e.g.,“claims,”“conclude”)butnotnecessarilyasfrequentlyingeneraltext(e.g.,theBNC).These-quencemodelcanalsolearntodispreferchangesofstate,suchthatmulti-wordsubsequencesarelabeledasshelleventhoughsomeoftheindividualwordsinthesubsequencearestopwords,punctuation,etc.Notetherearearelativelysmallnumberofpa-rametersinthemodel,6whichallowsustoestimateparametersonarelativelysmallsetoflabeleddata.Webrieﬂyexperimentedwithaddingan‘2penaltyonthemagnitudeofθinEquation2,butthisdidnotseemtoimproveperformance.Whenmakingpredictionsˆy(i)aboutthelabelse-quenceforanewsentence,themostcommonap-proachistoﬁndthemostlikelysequenceoflabelsygiventhewordsw(i),foundwithViterbidecoding:6Therewere42parametersinourimplementationofthefullCRFmodel.Excludingthefourtransitionfeatures,eachofthe19featureshadtwoparameters,oneforthepositiveclassandoneforthenegativeclass.Havingtwoparametersforeachisunnecessary,butwearenotawareofhowtohavethecrfsuitetoolkitavoidtheseextrafeatures.ˆy(i)=argmaxypθ(y|w(i))(2)Weuse“CRFv”torefertothisapproach.Weusethesufﬁx“+R”todenotemodelsthatincludethetworule-basedsystempredictionfeatures,andweuse“-R”todenotemodelsthatexcludethesetwofeatures.Indevelopment,weobservedthatthisdecodingapproachseemedtoverystronglypreferlabelinganentiresentenceasshellornot,whichisoftennotdesirablesinceshelloftenappearsatjustthebegin-ningsofsentences(e.g.,“Theargumentstatesthat”).Wethereforetestanalternativepredictionrulethatworksattheword-level,ratherthansequence-level.Thisapproachlabelseachwordasshellifthesumoftheprobabilitiesofallpathsinwhichthewordwaslabeledasshell—thatis,themarginalprobability—exceedssomethresholdλ.Wordsarelabeledasnon-shellotherwise.Speciﬁcally,anindi-vidualwordw(i)jislabeledasshell(i.e.,ˆy(i)j=1)accordingtothefollowingequation,where1(q)isanindicatorfunctionthatreturns1ifitsargumentqistrue,and0otherwise.ˆy(i)j=1  Xypθ(y|w(i))yj!≥λ!(3)Wetuneλusingthedevelopmentset,asdiscussedin§3.Weuse“CRFm”torefertothisapproach.2.3LexicalBaselineAsasimplebaseline,wealsoevaluatedamethodthatlabelswordsasshelliftheyappearfrequentlyinpersuasivewriting—speciﬁcally,inthesetof100,000unannotatedessaysdescribedin§2.2.Inthisapproach,wordtokensaremarkedasshelliftheybelongedtothesetofkmostfrequentwordsfromtheessays.Usingthedevelopmentsetdiscussedin§3.2,wetestedvaluesofkin{100,200,...,1000}.Settingk=700ledtothehighestF1.Weuse“TopWords”torefertothismethod.24

3ExperimentsInthissection,wediscussthedesignofourexper-imentalevaluationandpresentresultsonourdevel-opmentset,whichweusedtoselecttheﬁnalmeth-odstoevaluateontheheld-outtestset.3.1MetricsInourexperiments,weevaluatedtheperformanceoftheshelldetectionmethodsbycomparingtoken-levelsystempredictionstohumanlabels.Shelllan-guagetypicallyoccursasfairlylongsequencesofwords,butidentifyingtheexactspanofasequenceofshellseemslessimportantthaninrelatedtag-gingtasks,suchasnamedentityrecognition.There-fore,ratherthanevaluatingbasedonspans(eitherwithexactorapartialcreditsystem),wemeasuredperformanceatthewordtoken-levelusingstandardmetrics:precision,recall,andtheF1measure.Forexample,forprecision,wecomputedthepropor-tionoftokenspredictedasshellbyasystemthatwerealsolabeledasshellinourhuman-annotateddatasets.3.2AnnotatedDataToevaluatethemethodsdescribedin§2,wegath-eredannotationsfor200essaysthatwerenotinthelarger,unannotatedsetdiscussedin§2.2.Wesplitthissetofessaysintoadevelopmentsetof150es-says(68,601wordtokens)andaheld-outtestsetof50essays(21,277wordtokens).Anindividualwithextensiveexperienceatscoringpersuasivewritingandfamiliaritywithshelllanguageannotatedallto-kensintheessayswithjudgmentsofwhethertheywereshellornot(incontrastto§2.1,thisannotationonlyinvolvedlabelingshelllanguage).Fromtheﬁrstannotator’sjudgmentsonthedevel-opmentset,wecreatedasetofannotationguidelinesandtrainedasecondannotator.Thesecondanno-tatormarkedtheheld-outtestsetsothatwecouldmeasurehumanagreement.Comparingthetwoan-notators’testsetannotations,weobservedagree-mentofF1=0.736andCohen’sκ=0.699(wedonotuseκinourexperimentsbutreportitheresinceitisacommonmeasureofhumanagreement).Exceptformeasuringagreement,wedidnotusethesecondannotator’sjudgmentsinourexperiments.77Intheversionofthispapersubmittedforreview,wemea-recallprecision0.00.20.40.60.81.0l0.00.20.40.60.81.0linesCRFm-RCRFm+RpointslCRFm-RCRFm+RCRFv-RCRFv+RRulesTopWordsFigure2:Precisionandrecallofthedetectionmethodsatvariousthresholds,computedthroughcross-validationonthedevelopmentset.Pointsindicateperformancefortherule-basedandbaselinesystemaswellaspointswhereF1ishighest.3.3Cross-validationResultsTodeveloptheCRF’sfeatureset,totunehyperpa-rameters,andtoselectthemostpromisingsystemstoevaluateonthetestset,werandomlysplitthesen-tencesfromthedevelopmentsetintotwohalvesandconductedtestswithtwo-foldcross-validation.WetestedthresholdsfortheCRFatλ={0.01,0.02,...,1.00}.Figure2showstheresultsonthedevelopmentset.Fortherule-basedsystem,whichdidnotrequirela-beleddata,performanceiscomputedontheentiredevelopmentset.FortheCRFapproaches,thepre-cisionandrecallwerecomputedafterconcatenatingpredictionsoneachofthecross-validationfolds.TheTopWordsbaselineperformedquitepoorly,withF1=0.205.Therule-basedsystemperformedmuchbetter,withF1=0.382,butstillnotaswellastheCRFsystems.TheCRFsystemsthatpre-dictmaximumsequenceshadF1=0.382withouttherule-basedsystemfeatures(CRFv−R),andF1=0.467withtherule-basedfeatures(CRFv+R).TheCRFsystemsthatmadepredictionsfrommarginalscoresperformedbest,withF1=0.516withouttherule-basedfeatures,andF1=0.551withtherule-basedfeatures.Thus,boththerule-basedsys-suredtestsetagreementwithjudgmentsfromathirdindivid-ual,whowasinformallytrainedbytheﬁrst,withouttheformalguidelines.Agreementwassomewhatlower:F1=0.668andκ=0.613.25

MethodPRF1LenTopWords0.1250.7590.214∗2.80Rules0.5610.3600.439∗4.99CRFv−R0.7290.2680.392∗15.67CRFv+R0.7630.3690.498∗13.30CRFm−R0.5860.5740.5809.00CRFm+R0.5560.6700.6079.96Human0.6850.7960.736∗7.91Table1:Performanceontheheld-outtestset,intermsofprecision(P),recall(R),F1measure,andaveragelengthintokensofsequencesofoneormorewordslabeledasshell(Len).∗indicatesF1scoresthatarestatisticallyreliablydifferentfromCRFm+Ratthep<0.01level.temfeaturesandthemarginalpredictionapproachledtogainsinperformance.FromanexaminationofthepredictionsfromtheCRFm+RandCRFm−Rsystems,itappearsthatamajorcontributionofthefeaturesderivedfromtherule-basedsystemistohelpthehybridCRFm+Rsystemavoidtaggingentiresentencesasshellwhenonlypartsofthemareactuallyshell.Forexam-ple,considerthesentence“Accordingtothisstate-ment,thespeakerassertsthattechnologycannotonlyinﬂuencebutalsodeterminesocialcustomsandethics”(typographicalerrorsincluded).CRFm−Rtagseverythingupto“determine”asshell,whereastherule-basedsystemandCRFm+Rcorrectlystopafter“assertsthat.”4TestSetResultsNext,wepresentresultsontheheld-outtestset.FortheCRFmsystems,weusedthethresholdsthatledtothehighestF1scoresonthedevelopmentset(λ=0.26forCRFm+Randλ=0.32forCRFm−R).Table1presentstheresultsforallsys-tems,alongwithresultscomparingthesecondanno-tator’slabels(“Human”)tothegoldstandardlabelsfromtheﬁrstannotator.Thesamepatternemergedasonthedevelopmentset,withCRFm+Rperformingthebest.TheF1scoreof0.607fortheCRFm+Rsystemwasrel-ativelyclosetotheF1scoreof0.736foragree-mentbetweenhumanannotators.TotestwhetherCRFm+R’srelativelyhighperformancewasduetochance,wecomputed99%conﬁdenceintervalsforthedifferencesinF1scorebetweenCRFm+Randeachoftheothermethods.Weusedthebias-correctedandaccelerated(BCa)Bootstrap(EfronandTibshirani,1993)with10,000roundsofresam-plingatthesentencelevelforeachcomparison.Adifferenceisstatisticallyreliableattheαlevel(i.e.,p<α)ifthe(1−α)%conﬁdenceintervalforthedifferencedoesnotcontainzero,whichcorrespondstothenullhypothesis.Statisticallyreliablediffer-encesareindicatedinTable1.TheonlysystemthatdidnothaveareliablylowerF1scorethanCRFm+RwasCRFm−R,thoughduetotherelativelysmallsizeofourtestset,wedonottakethisasstrongev-idenceagainstusingtherule-basedsystemfeaturesintheCRF.WenotethatwhiletheCRFm+Rsystemhadlowerprecision(0.556)thantheCRFv+Rsystem(0.763),itsthresholdλcouldbetunedtopreferhighpreci-sionratherthanthebestdevelopmentsetF1.Suchtuningcouldbeveryimportantdependingontherel-ativecostsoffalsepositivesandfalsenegativesforaparticularapplication.Wealsocomputedthemeanlengthofsequencesofoneormorecontiguouswordslabeledasshell.Herealso,weobservedthattheCRFm+Rapproachprovidedaclosematchtohumanperformance.Themeanlengthsofshellfortheﬁrstandsecondanno-tatorswere8.49and7.91tokens,respectively.FortheCRFm+Rapproach,themeanlengthwasslightlyhigherat9.96tokens,butthiswasmuchclosertothemeansofthehumanannotatorsthanthemeanfortheCRFv+Rsystem,whichwas13.30tokens.Fortherule-basedsystem,themeanlengthwas4.99to-kens,indicatingthatitcapturesshortsequencessuchas“Inaddition,”moreoftenthantheothersystems.5ObservationsaboutaNewDomainInthissection,weapplyoursystemtoacorpusoftranscriptsofpoliticaldebates8inordertounder-standwhetherthesystemcangeneralizetoanewdomainwithasomewhatdifferentstyleofargu-mentation.Ouranalysesareprimarilyqualitativeinnatureduetothelackofgold-standardannota-tions.Wechosetwohistoricallywell-knowndebates8TheLincoln–Douglasdebatesweredownloadedfromhttp://www.bartleby.com/251/.Theotherdebatesweredownloadedfromhttp://debates.org/.26

(Lincoln–Douglasfrom1858andKennedy–Nixonfrom1960)andtwodebatesthatoccurredmorere-cently(Gore–Bushfrom2000andObama–McCainfrom2008).Thesedebatesrangeinlengthfrom38,000wordtokensto65,000wordtokens.Politicaldebatesaresimilartothepersuasivees-saysweusedaboveinthatdebateparticipantsstatetheirownclaimsandevidenceaswellasevaluatetheiropponents’claims.Theyaredifferentfromes-saysinthattheyarespokenratherthanwritten—meaningthattheycontainmoredisﬂuencies,collo-quiallanguage,etc.—andthattheycoverdifferentsocialandeconomicissues.Also,thedebatesareinsomesenseadialoguebetweentwopeople.WetaggedallthedebatesusingtheCRFm+Rsys-tem,usingthesameparametersasforthetestsetexperiments(§4).First,weobservedthatasmallerpercentageoftokensweretaggedasshellinthedebatesthanintheessays.Fortheannotatedessaytestset(§3.2),thepercentageoftokenstaggedasshellwas14.0%(11.6%werelabeledasshellbytheﬁrstannota-tor).Incontrast,thepercentageoftokenstaggedasshellwas4.2%forLincoln–Douglas,5.4%forKennedy–Nixon,4.6%forGore–Bush,and4.8%forObama–McCain.Itisnotcompletelyclearwhetherthesmallerpercentagestaggedasshellareduetoalackofcoveragebytheshelldetectorormoresub-stantialdifferencesinthedomain.However,itseemsthatthesedebatesgenuinelyin-cludelessshell.Onepotentialreasonisthatmanyoftheessaypromptsaskedtest-takerstorespondtoaparticularargument,leadingtoresponsescontainingmanyphrasessuchas“Thespeakerclaimsthat”and“However,theargumentlacksspeciﬁcity...”.Weanalyzedthesystem’spredictionsandex-tractedasetofexamples,someofwhichappearinTable2,showingtruepositives,wheremostofthetokensappeartobelabeledcorrectlyasshell;falsepositives,wheretokenswereincorrectlylabeledasshell;andfalsenegatives,wherethesystemmissedtokensthatshouldhavebeenmarked.Table2alsoprovidessomeexamplesfromourde-velopmentset,forcomparison.Weobservedmanyinstancesofcorrectlymarkedshell,includingmanythatappearedverydifferentinstylethanthelanguageusedinessays.Forex-ample,Lincolndemonstratesanaggressivestyleinthefollowing:“Now,Isaythatthereisnocharitablewaytolookatthatstatement,excepttoconcludethatheisactuallycrazy.”Also,Bushemploysasome-whatatypicalsentencestructurehere:“It’snotwhatIthinkanditsnotmyintentionsandnotmyplan.”However,thesystemalsoincorrectlytaggedse-quencesasshell,particularlyinshortsentences(e.g.,“Areweasstrongasweshouldbe?”).Italsomissedshell,partiallyorentirely,suchasinthefollowingexample:“Butlet’sgetbacktothecoreissuehere.”Theseresultssuggestthatalthoughthereispoten-tialforimprovementinadaptingtonewdomains,ourapproachtoshelldetectionatleastpartiallygen-eralizesbeyondourinitialdomainofpersuasivees-saywriting.6RelatedWorkTherehasbeenmuchpreviousworkonanalyzingdiscourse.Inthissection,wedescribesimilaritiesanddifferencesbetweenthatworkandours.Rhetoricalstructuretheory(MannandThomp-son,1988)isperhapsthemostrelevantareaofwork.See§1foradiscussion.Inresearchonintentionalstructure,GroszandSidner(1986)proposethatanydiscourseiscom-posedofthreeinteractingcomponents:thelinguisticstructuredeﬁnedbytheactualutterances,theinten-tionalstructuredeﬁnedbythepurposesunderlyingthediscourse,andanattentionalstructuredeﬁnedbythediscourseparticipants’focusofattention.De-tectingshellmayalsobeseenastryingtoidentifyexplicitcuesofintentionalstructureinadiscourse.Additionally,thecategorizationofshellspansastowhethertheyevaluatetheopponentsclaims,connectonesownclaims,etc.,maybeseenasdeterminingwhatGroszandSidenercall“discoursesegmentpur-poses”(i.e.,theintentionsunderlyingthesegmentscontainingtheshellspans).Wecanalsoviewshelldetectionasthetaskofidentifyingphrasesthatindicatecertaintypesofspeechacts(Searle,1975).Inparticular,weaimtoidentifymarkersofassertivespeechacts,whichde-clarethatthespeakerbelievesacertainproposition,andexpressivespeechacts,whichexpressattitudestowardpropositions.Shellalsooverlapswiththeconceptofdiscoursemarkers(Hutchinson,2004),suchas“however”or27

LINCOLN(L)—DOUGLAS(D)DEBATESTPL:Now,Isaythatthereisnocharitablewaytolookatthatstatement,excepttoconcludethatheisactuallycrazy.L:TheﬁrstthingIseeﬁttonoticeisthefactthat...FPD:Hebecamenotedastheauthoroftheschemeto...D:...suchamendmentsweretobemadetoitaswouldrenderituselessandinefﬁcient...FND:Iwishtoimpressituponyou,thateverymanwhovotedforthoseresolutions...L:Thatstatementhemakes,too,intheteethoftheknowledgethatIhadmadethestipulationtocomedownhere...KENNEDY(K)—NIXON(N)DEBATESTPN:IfavorthatbecauseIbelievethat’sthebestwaytoaidourschools...N:Andinourcase,Idobelievethatourprogramswillstimulatethecreativeenergiesof...FPN:Weareforprograms,inaddition,whichwillseethatourmedicalcarefortheaged...K:Areweasstrongasweshouldbe?FNK:IshouldmakeitclearthatIdonotthinkwe’redoingenough...N:WhydidSenatorKennedytakethatpositionthen?WhydoItakeitnow?BUSH(B)—GORE(G)DEBATESTPB:It’snotwhatIthinkanditsnotmyintentionsandnotmyplan.G:AndFEMAhasbeenamajorﬂagshipprojectofourreinventinggovernmentefforts.AndIagree,itworksextremelywellnow.FPB:Firstofall,mostofthisisatthestatelevel.G:Anditfocusesnotonlyonincreasingthesupply,whichIagreewehavetodo,butalsoon...FNB:Myopponentthinksthegovernment—thesurplusisthegovernment’smoney.That’snotwhatIthinkG:Istronglysupportlocalcontrol,sodoesGovernorBush.OBAMA(O)—MCCAIN(M)DEBATESTPM:Butthepointis—thepointis,wehaveﬁnallyseenRepublicansandDemocratssittingdownandnegotiatingtogether...O:AndoneofthethingsIthinkwehavetodoismakesurethatcollegeisaffordable...FPO:...butintheshorttermthere’sanoutlayandwemaynotseethatmoneyforawhile.O:Wehavetodothatnow,becauseitwillactuallymakeourbusinessesandourfamiliesbetteroff.FNO:SoIthinkthelessontobedrawnisthatweshouldneverhesitatetousemilitaryforce...tokeeptheAmericanpeoplesafe.O:Butlet’sgetbacktothecoreissuehere.PERSUASIVEESSAYS(DEVELOPMENTSET,SPELLINGERRORSINCLUDED)TPHowever,theargumentlacksspeciﬁcityandreliesontoomanyquestionableassumptionstomakeastrongcaseforadoptinganexpensiveandlogisticallycomplicatedprogram.Ibelievethatbothoftheseclaimshavebeenmadeinhaseandotherfactorsneedtobeconsidered.FPSincetheyareallfarfromnow,theproveisnotstrongenoughtosupporttheconclusion.Asweknowthatonemindcannotthinkastheotherdoes.FNHistoryhasproventhat...Thegivenissuewhichstatesthatinanyﬁeldofinquiry...isacontroversionalone.Table2:ExamplesofCRFm+Rperformance.Underliningmarkstokenspredictedtobeshell,andboldfontindicatesshellaccordingtohumanjudgments(ourjudgmentsforthedebatetranscripts,andtheannotator’sjudgmentsforthedevelopmentset).Examplesincludetruepositives(TP),falsepositives(FP),andfalsenegatives(FN).NotethatsomeFPandFNexamplesincludepartiallyaccuratepredictions.28

“therefore.”Discoursemarkers,however,aretyp-icallyonlysinglewordsorshortphrasesthatex-pressalimitednumberofrelationships.Ontheotherhand,shellcancapturelongersequencesthatex-pressmorecomplexrelationshipsbetweenthecom-ponentsofanargumentativediscourse(e.g.,“Butlet’sgetbacktothecoreissuehere”signalsthatthefollowingpointismoreimportantthanthepreviousone).Therearealsovariousotherapproachestoana-lyzingarguments.Notably,muchrecenttheoreti-calresearchonargumentationhasfocusedonar-gumentationschemes(Waltonetal.,2008),whicharehigh-levelstrategiesforconstructingarguments(e.g.,argumentfromconsequences).Recently,FengandHirst(2011)developedautomatedmethodsforclassifyingtextsbyargumentationscheme.Insim-ilarwork,Anandetal.(2011)useargumentationschemestoidentifytacticsinblogposts(e.g.,moralappeal,socialgeneralization,appealstoexternalau-thoritiesetc.).Althoughshelllanguagecancertainlybefoundinpersuasivewriting,itisusedtoorga-nizethepersuader’stacticsandclaimsratherthantoexpressthem.Forexample,considerthefollow-ingsentence:“Itmustbethecasethatthisdietworkssinceitwasrecommendedbysomeonewholost20poundsonit.”Inshelldetection,wefocusonthelexico-syntacticlevel,aimingtoidentifytheboldwordsasshell.Incontrast,workonargumenta-tionschemesfocusesatahigherlevelofabstraction,aimingtoclassifythesentenceasanattempttoper-suadebyappealingtoanexternalauthority.7ConclusionsInthispaper,wedescribedourapproachtodetect-inglanguageusedtoexplicitlystructureanarguer’sclaimsandpiecesofevidenceaswellasexplainhowtheyrelatetoanopponent’sargument.Weim-plementedarule-basedsystem,asupervisedproba-bilisticsequencemodel,andaprincipledhybridver-sionofthetwo.Wepresentedevaluationsofthesesystemsusinghuman-annotatedessays,andweob-servedthatthehybridsequencemodelsystemper-formedthebest.Wealsoappliedoursystemtopo-liticaldebatesandfoundevidenceofthepotentialtogeneralizetonewdomains.AcknowledgmentsWewouldliketothanktheannotatorsforhelpinguscreatetheessaydatasets.WewouldalsoliketothankJamesCarlson,PaulDeane,YokoFutagi,BeataBeigmanKlebanov,MelissaLopez,andtheanonymousreviewersfortheirusefulcommentsonthepaperandannotationscheme.ReferencesP.Anand,J.King,J.Boyd-Graber,E.Wagner,C.Martell,D.Oard,andP.Resnik.2011.Believeme–wecandothis!annotatingpersuasiveactsinblogtext.InProc.ofAAAIWorkshoponComputationalModelsofNaturalArgument.L.Carlson,D.Marcu,andM.E.Okurowski.2001.Buildingadiscourse-taggedcorpusintheframeworkofrhetoricalstructuretheory.InProc.oftheSecondSIGdialWorkshoponDiscourseandDialogue.B.EfronandR.Tibshirani.1993.AnIntroductiontotheBootstrap.ChapmanandHall/CRC.V.W.FengandG.Hirst.2011.Classifyingargumentsbyscheme.InProc.ofACL.BarbaraJ.GroszandCandaceL.Sidner.1986.Atten-tion,Intentions,andtheStructureofDiscourse.Com-put.Linguist.,12(3):175–204.B.Hutchinson.2004.Acquiringthemeaningofdis-coursemarkers.InProc.ofACL.J.Lafferty,A.McCallum,andF.Pereira.2001.Con-ditionalrandomﬁelds:Probabilisticmodelsforseg-mentingandlabelingsequencedata.InProc.ofICML.W.C.MannandS.A.Thompson.1988.Rhetoricalstructuretheory:Towardafunctionaltheoryoftextorganization.Text,8(3).D.Marcu.2000.TheTheoryandPracticeofDiscourseParsingandSummarization.MITPress.N.Okazaki.2007.CRFsuite:afastimplementationofconditionalrandomﬁelds(CRFs).J.R.Searle.1975.Aclassiﬁcationofillocutionaryacts.LanguageinSociety,5(1).D.Walton,C.Reed,andF.Macagno.2008.Argumenta-tionSchemes.CambridgeUniversityPress.2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 29–38,

Montr´eal, Canada, June 3-8, 2012. c(cid:13)2012 Association for Computational Linguistics

29

FastInferenceinPhraseExtractionModelswithBeliefPropagationDavidBurkettandDanKleinComputerScienceDivisionUniversityofCalifornia,Berkeley{dburkett,klein}@cs.berkeley.eduAbstractModelingoverlappingphrasesinanalign-mentmodelcanimprovealignmentqualitybutcomeswithahighinferencecost.Forexample,themodelofDeNeroandKlein(2010)usesanITGconstraintandbeam-basedViterbidecodingfortractability,butisstillslow.Weﬁrstshowthattheirmodelcanbeapproximatedusingstructuredbeliefpropaga-tion,withagaininalignmentqualitystem-mingfromtheuseofmarginalsindecoding.Wethenconsideramoreﬂexible,non-ITGmatchingconstraintwhichislessefﬁcientforexactinferencebutmoreefﬁcientforBP.Withthisnewconstraint,weachievearelativeerrorreductionof40%inF5anda5.5xspeed-up.1IntroductionModernstatisticalmachinetranslation(MT)sys-temsmostcommonlyinfertheirtransferrulesfromword-levelalignments(Koehnetal.,2007;LiandKhudanpur,2008;Galleyetal.,2004),typicallyusingadeterministicheuristictoconvertthesetophrasealignments(Koehnetal.,2003).Therehavebeenmanyattemptsoverthelastdecadetodevelopmodel-basedapproachestothephrasealignmentproblem(MarcuandWong,2002;Birchetal.,2006;DeNeroetal.,2008;Blunsometal.,2009).How-ever,mostofthesehavemetwithlimitedsuccesscomparedtothesimplerheuristicmethod.Onekeyproblemwithtypicalmodelsofphrasealignmentisthattheychooseasingle(latent)segmentation,givingrisetoundesirablemodelingbiases(DeNeroetal.,2006)andreducingcoverage,whichinturnreducestranslationquality(DeNeefeetal.,2007;DeNeroetal.,2008).Ontheotherhand,theextrac-tionheuristicidentiﬁesmanyoverlappingoptions,andachieveshighcoverage.Inresponsetotheseeffects,therecentphrasealignmentworkofDeNeroandKlein(2010)mod-elsextractionsets:collectionsofoverlappingphrasepairsthatareconsistentwithanunderlyingwordalignment.Theirextractionsetmodelisempiricallyveryaccurate.However,theabilitytomodelover-lapping–andthereforenon-local–featurescomesatahighcomputationalcost.DeNeroandKlein(2010)handlethisinpartbyimposingastructuralITGconstraint(Wu,1997)ontheunderlyingwordalignments.Thispermitsapolynomial-timealgo-rithm,butitisstillO(n6),withalargeconstantfactoroncethestatespaceisappropriatelyenrichedtocaptureoverlap.Therefore,theyuseaheavilybeamedViterbisearchproceduretoﬁndareason-ablealignmentwithinanacceptabletimeframe.Inthispaper,weshowhowtousebeliefpropagation(BP)toimproveonthemodel’sITG-basedstruc-turalformulation,resultinginanewmodelthatissimultaneouslyfasterandmoreaccurate.First,giventhemodelofDeNeroandKlein(2010),wedecomposeitintofactorsthatadmitanefﬁcientBPapproximation.BPisaninferencetechniquethatcanbeusedtoefﬁcientlyapproxi-mateposteriormarginalsonvariablesinagraphicalmodel;herethemarginalsofinterestarethephrasepairposteriors.BPhasonlyrecentlycomeintouseintheNLPcommunity,butithasbeenshowntobeeffectiveinothercomplexstructuredclassiﬁcationtasks,suchasdependencyparsing(SmithandEis-ner,2008).TherehasalsobeensomepriorsuccessinusingBPforbothdiscriminative(NiehuesandVogel,2008)andgenerative(Cromi`eresandKuro-hashi,2009)wordalignmentmodels.ByaligningallphrasepairswhoseposteriorunderBPexceedssomeﬁxedthreshold,ourBPapproxi-mationofthemodelofDeNeroandKlein(2010)can30

achieveacomparablephrasepairF1.Furthermore,becausewehaveposteriormarginalsratherthanasingleViterbiderivation,wecanexplicitlyforcethealignertochoosedenserextractionsetssimplybyloweringthemarginalthreshold.Therefore,wealsoshowsubstantialimprovementsoverDeNeroandKlein(2010)inrecall-heavyobjectives,suchasF5.Moreimportantly,wealsoshowhowtheBPfac-torizationallowsustorelaxtheITGconstraint,re-placingitwithanewsetofconstraintsthatper-mitawiderfamilyofalignments.ComparedtoITG,theresultingmodelislessefﬁcientforexactinference(whereitisexponential),butmoreefﬁ-cientforourBPapproximation(whereitisonlyquadratic).OurnewmodelperformsevenbetterthantheITG-constrainedmodelonphrasealign-mentmetricswhilebeingfasterbyafactorof5.5x.2ExtractionSetModelsFigure1showspartofanalignedsentencepair,in-cludingtheword-to-wordalignments,andtheex-tractedphrasepairslicensedbythosealignments.Formally,givenasentencepair(e,f),aword-levelalignmentaisacollectionoflinksbetweentargetwordseiandsourcewordsfj.Followingpastwork,wefurtherdividewordlinksintotwocategories:sureandpossible,showninFigure1assolidandhatchedgreysquares,respectively.Werepresentaasagridofternarywordlinkvariablesaij,eachofwhichcantakethevaluesuretorepresentasurelinkbetweeneiandfj,posstorepresentapossiblelink,orofftorepresentnolink.Anextractionsetπisasetofalignedphrasepairstobeextractedfrom(e,f),showninFigure1asgreenroundedrectangles.Werepresentπasasetofbooleanvariablesπghk‘,whicheachhavethevaluetruewhenthetargetspan[g,h]isphrase-alignedtothesourcespan[k,‘].Followingpreviousworkonphraseextraction,welimitthesizeofπbyimposingaphraselengthlimitd:πonlycontainsavariableπghk‘ifh−g<dand‘−k<d.Thereisadeterministicmappingπ(a)fromawordalignmenttotheextractionsetlicensedbythatwordalignment.Wewillbrieﬂydescribeithere,andthenpresentourfactorizedmodel.e3e4e5e6e7f5f6f7f8f9σf5=[7,7]σf6=[5,6]σf7=[5,6]σf8=[4,4]σf9=[−1,∞]Figure1:Aschematicrepresentationofpartofasen-tencepair.Solidgreysquaresindicatesurelinks(e.g.a48=sure),andhatchedsquarespossiblelinks(e.g.a67=poss).Roundedgreenrectanglesareextractedphrasepairs(e.g.π5667=true).Targetspansareshownasblueverticallinesandsourcespansasredhorizontallines.Becausethereisasurelinkata48,σf8=[4,4]doesnotincludethepossiblelinkata38.However,f7onlyhaspossiblelinks,soσf7=[5,6]isthespancontainingthose.f9isnull-aligned,soσf9=[−1,∞],whichblocksallphrasepairscontainingf9frombeingextracted.2.1ExtractionSetsfromWordAlignmentsThemappingfromawordalignmenttothesetoflicensedphrasepairsπ(a)isbasedonthestandardruleextractionproceduresusedinmostmodernsta-tisticalsystems(Koehnetal.,2003;Galleyetal.,2006;Chiang,2007),butextendedtohandlepos-siblelinks(DeNeroandKlein,2010).Westartbyusingatoﬁndaprojectionfromeachtargetwordeiontoasourcespan,representedasblueverticallinesinFigure1.Similarly,sourcewordsprojectontotargetspans(redhorizontallinesinFigure1).π(a)containsaphrasepairiffeverywordinthetargetspanprojectswithinthesourcespanandviceversa.Figure1containsanexampleford=2.Formally,themappingintroducesasetofspansσ.Werepresentthespansasvariableswhosevaluesareintervals,whereσei=[k,‘]meansthatthetar-getwordeiprojectstothesourcespan[k,‘].Thesetoflegalvaluesforσeiincludesanyintervalwith0≤k≤‘<|f|and‘−k<d,plusthespecialin-terval[−1,∞]thatindicateseiisnull-aligned.Thespanvariablesforsourcewordsσfjhavetargetspans[g,h]asvaluesandaredeﬁnedanalogously.ForasetIofpositions,wedeﬁnetherangefunc-31

tion:range(I)=([−1,∞]I=∅[mini∈Ii,maxi∈Ii]else(1)Foraﬁxedwordalignmentawesetthetargetspanvariableσei:σei,s=range({j:aij=sure})(2)σei,p=range({j:aij6=off})(3)σei=σei,s∩σei,p(4)AsillustratedinFigure1,thissetsσeitothemin-imalspancontainingallthesourcewordswithasurelinktoeiifthereareany.Otherwise,becauseofthespecialcaseforrange(I)whenIisempty,σei,s=[−1,∞],soσeiistheminimalspancontainingallposs-alignedwords.Ifallwordlinkstoeiareoff,indicatingthateiisnull-aligned,thenσeiis[−1,∞],preventingthealignmentofanyphrasepairscon-tainingei.Finally,wespecifywhichphrasepairsshouldbeincludedintheextractionsetπ.Giventhespansσbasedona,π(a)setsπghk‘=trueiffeverywordineachphrasalspanprojectswithintheother:σei⊆[k,‘]∀i∈[g,h](5)σfj⊆[g,h]∀j∈[k,‘]2.2FormulationasaGraphicalModelWescoretriples(a,π,σ)asthedotproductofaweightvectorwthatparameterizesourmodelandafeaturevectorφ(a,π,σ).Thefeaturevectordecom-posesintowordalignmentfeaturesφa,phrasepairfeaturesφπandtargetandsourcenullwordfeaturesφe∅andφf∅:1φ(a,π,σ)=Xi,jφa(aij)+Xg,h,k,‘φπ(πghk‘)+Xiφe∅(σei)+Xjφf∅(σfj)(6)ThisfeaturefunctionisexactlythesameasthatusedbyDeNeroandKlein(2010).2However,while1Inadditiontotheargumentswewriteoutexplicitly,allfea-turefunctionshaveaccesstotheobservedsentencepair(e,f).2AlthoughthenullwordfeaturesarenotdescribedinDeN-eroandKlein(2010),alloftheirreportedresultsincludethesefeatures(DeNero,2010).theyformulatedtheirinferenceproblemasasearchforthehighestscoringtriple(a,π,σ)foranob-servedsentencepair(e,f),wewishtoderiveacon-ditionalprobabilitydistributionp(a,π,σ|e,f).Wedothiswiththestandardtransformationforlinearmodels:p(a,π,σ|e,f)∝exp(w·φ(a,π,σ)).DuetothefactorizationinEq.(6),thisexponentiatedformbecomesaproductoflocalmultiplicativefactors,andhenceourmodelformsanundirectedgraphicalmodel,orMarkovrandomﬁeld.Inadditiontothescoringfunction,ourmodelalsoincludesconstraintsonwhichtriples(a,π,σ)havenonzeroprobability.DeNeroandKlein(2010)implicitlyincludedtheseconstraintsintheirrepre-sentation:insteadofsetsofvariables,theyusedastructuredrepresentationthatonlyencodestriples(a,π,σ)satisfyingboththemappingπ=π(a)andthestructuralconstraintthatacanbegeneratedbyablockITGgrammar.However,ourinferencepro-cedure,BP,requiresthatwerepresent(a,π,σ)asanassignmentofvaluestoasetofvariables.Therefore,wemustexplicitlyencodeallconstraintsintothemultiplicativefactorsthatdeﬁnethemodel.Toac-complishthis,inadditiontothesoftscoringfactorswehavealreadymentioned,ourmodelalsoincludesasetofhardconstraintfactors.Hardconstraintfac-torsenforcetherelationshipsbetweenthevariablesofthemodelbytakingavalueof0whenthecon-straintstheyencodeareviolatedandavalueof1whentheyaresatisﬁed.Thefullfactorgraphrep-resentationofourmodel,includingbothsoftscor-ingfactorsandhardconstraintfactors,isdrawnschematicallyinFigure2.2.2.1SoftScoringFactorsThescoringfactorsalltaketheformexp(w·φ),andsocanbedescribedintermsoftheirrespectivelocalfeaturevectors,φ.Dependingonthevaluesofthevariableseachfactordependson,thefactorcanbeactiveorinactive.Featuresareonlyextractedforactivefactors;otherwiseφisemptyandthefactorproducesavalueof1.SURELINK.EachwordalignmentvariableaijhasacorrespondingSURELINKfactorLijtoincor-poratescoresfromthefeaturesφa(aij).Lijisac-tivewheneveraij=sure.φa(aij)includesposte-riorsfromunsupervisedjointlytrainedHMMwordalignmentmodels(Liangetal.,2006),dictionary32

a11a21LijL11L21a12ai1Li1L12L22a22a1jaijL1jA(a)ITGfactoragkahkag￿ah￿ag|f|ah|f|a|e|ka|e|￿Pghk￿Rghk￿πghk￿SegSehNehNegσegσehSfkSf￿Nf￿Nfkσfkσf￿(b)SPANandEXTRACTfactorsFigure2:AfactorgraphrepresentationoftheITG-basedextractionsetmodel.Forvisualclarity,wedrawthegraphseparatedintotwocomponents:onecontainingthefactorsthatonlyneighborwordlinkvariables,andonecontainingtheremainingfactors.andidenticalwordfeatures,apositiondistortionfea-ture,andfeaturesfornumbersandpunctuation.PHRASEPAIR.Foreachphrasepairvariableπghk‘,scoresfromφπ(πghk‘)comefromthefactorRghk‘,whichisactiveifπghk‘=true.Mostofthemodel’sfeaturesareonthesefactors,andincluderelativefrequencystatistics,lexicaltemplateindica-torfeatures,andindicatorsfornumbersofwordsandChinesecharacters.SeeDeNeroandKlein(2010)foramorecomprehensivelist.NULLWORD.Wecandetermineifawordisnull-alignedbylookingatitscorrespondingspanvariable.Thus,weincludefeaturesfromφe∅(σei)inafactorNeithatisactiveifσei=[−1,∞].Thefeaturesaremostlyindicatorsforcommonwords.TherearealsofactorsNfjforsourcewords,whicharedeﬁnedanalogously.2.2.2HardConstraintFactorsWeencodethehardconstraintsonrelationshipsbetweenvariablesinourmodelusingthreefami-liesoffactors,showngraphicallyinFigure2.TheSPANandEXTRACTfactorstogetherensurethatπ=π(a).TheITGfactorencodesthestructuralconstraintona.SPAN.First,foreachtargetwordeiweincludeafactorSeitoensurethatthespanvariableσeihasavaluethatagreeswiththeprojectionofthewordalignmenta.AsshowninFigure2b,Seidependsonσeiandallthewordalignmentvariablesaijincolumniofthewordalignmentgrid.Seihasvalue1ifftheequalityinEq.(4)holds.OurmodelalsoincludesafactorSfjtoenforcetheanalogousrela-tionshipbetweeneachσfjandcorrespondingrowjofa.EXTRACT.Foreachphrasepairvariableπghk‘wehaveafactorPghk‘toensurethatπghk‘=trueiffitislicensedbythespanprojectionsσ.AsshowninFigure2b,inadditiontoπghk‘,Pghk‘dependsontherangeofspanvariablesσeifori∈[g,h]andσfjforj∈[k,‘].Pghk‘issatisﬁedwhenπghk‘=trueandtherelationsinEq.(5)allhold,orwhenπghk‘=falseandatleastoneofthoserelationsdoesnothold.ITG.Finally,toenforcethestructuralconstraintona,weincludeasingleglobalfactorAthatde-pendsonallthewordlinkvariablesina(seeFig-ure2a).Aissatisﬁediffaisinthefamilyofblockinversetransductiongrammar(ITG)align-ments.TheblockITGfamilypermitsmultiplelinkstobeon(aij6=off)foraparticularwordeiviatermi-nalblockproductions,butensuresthateverywordis33

inatmostonesuchterminalproduction,andthatthefullsetofterminalblockproductionsisconsistentwithITGreorderingpatterns(Zhangetal.,2008).3RelaxingtheITGConstraintTheITGfactorcanbeviewedasimposingtwodif-ferenttypesofconstraintsonallowablewordalign-mentsa.First,itrequiresthateachwordisalignedtoatmostonerelativelyshortsubspanoftheothersentence.Thisisalinguisticallyplausiblecon-straint,asitisrarelythecasethatasinglewordwilltranslatetoanextremelylongphrase,ortomultiplewidelyseparatedphrases.3TheotherconstraintimposedbytheITGfactoristheITGreorderingconstraint.Thisconstraintisimposedprimarilyforreasonsofcomputationaltractability:thestandarddynamicprogramforbi-textparsingdependsonITGreordering(Wu,1997).Whilethisconstraintisnotdramaticallyrestric-tive(Haghighietal.,2009),itisplausiblethatre-movingitwouldpermitthemodeltoproducebetteralignments.Wetestedthishypothesisbydevelop-inganewmodelthatenforcesonlytheconstraintthateachwordaligntoonelimited-lengthsubspan,whichcanbeviewedasageneralizationoftheat-most-one-to-oneconstraintfrequentlyconsideredintheword-alignmentliterature(Taskaretal.,2005;Cromi`eresandKurohashi,2009).Ournewmodelhasalmostexactlythesameformasthepreviousone.TheonlydifferenceisthatAisreplacedwithanewfamilyofsimplerfactors:ONESPAN.Foreachtargetwordei(andeachsourcewordfj)weincludeahardconstraintfactorUei(respectivelyUfj).Ueiissatisﬁediff|σei,p|<d(lengthlimit)andeitherσei,p=[−1,∞]or∀j∈σei,p,aij6=off(nogaps),withσei,pasinEq.(3).Fig-ure3showstheportionofthefactorgraphfromFig-ure2aredrawnwiththeONESPANfactorsreplacingtheITGfactor.AsFigure3shows,thereisnolongeraglobalfactor;eachUeidependsonlyonthewordlinkvariablesfromcolumni.3ShortgapscanbeaccomodatedwithinblockITG(andinourmodelarerepresentedaspossiblelinks)aslongasthetotalalignedspandoesnotexceedtheblocksize.a11a21LijL11L21a12ai1Li1L12L22a22a1jaijL1jUf1Uf2UfjUe1Ue2UeiFigure3:ONESPANfactors4BeliefPropagationBeliefpropagationisageneralizationofthewellknownsum-productalgorithmforundirectedgraph-icalmodels.Wewillprovideonlyaproceduralsketchhere,butagoodintroductiontoBPforin-ferenceinstructuredNLPmodelscanbefoundinSmithandEisner(2008),andChapters16and23ofMacKay(2003)containageneralintroductiontoBPinthemoregeneralcontextofmessage-passingalgorithms.Atahighlevel,eachvariablemaintainsalocaldistributionoveritspossiblevalues.Theselocaldis-tributionareupdatedviamessagespassedbetweenvariablesandfactors.ForavariableV,N(V)de-notesthesetoffactorsneighboringVinthefac-torgraph.Similarly,N(F)isthesetofvariablesneighboringthefactorF.DuringeachroundofBP,messagesaresentfromeachvariabletoeachofitsneighboringfactors:q(k+1)V→F(v)∝YG∈N(V),G6=Fr(k)G→V(v)(7)andfromeachfactortoeachofitsneighboringvari-ables:r(k+1)F→V(v)∝XXF,XF[V]=vF(XF)YU∈N(F),U6=Vq(k)U→F(v)(8)whereXFisapartialassignmentofvaluestojustthevariablesinN(F).34

Marginalbeliefsattimekcanbecomputedbysimplymultiplyingtogetherallreceivedmessagesandnormalizing:b(k)V(v)∝YG∈N(V)r(k)G→V(v)(9)Althoughmessagescanbeupdatedaccordingtoanyschedule,generallyoneiterationofBPupdateseachmessageonce.Theprocessiteratesuntilsomestoppingcriterionhasbeenmet:eitheraﬁxednum-berofiterationsorsomeconvergencemetric.Forourmodels,wesaythatBPhasconvergedwheneverPV,v(cid:16)b(k)V(v)−b(k−1)V(v)(cid:17)2<δforsomesmallδ>0.4Whilewehavenotheoreticalconvergenceguarantees,itusuallyconvergeswithin10iterationsinpractice.5EfﬁcientBPforExtractionSetModelsIngeneral,theefﬁciencyofBPdependsdirectlyonthearityofthefactorsinthemodel.Performedna¨ıvely,thesuminEq.(8)willtaketimethatgrowsexponentiallywiththesizeofN(F).Forthesoft-scoringfactors,whicheachdependonlyonasinglevariable,thisisn’taproblem.However,ourmodelalsoincludesfactorswhosearitygrowswiththein-putsize:forexample,explicitlyenumeratingallas-signmentstothewordlinkvariablesthattheITGfactordependsonwouldtakeO(3n2)time.5TorunBPinareasonabletimeframe,weneedefﬁcientfactor-speciﬁcpropagatorsthatcanexploitthestructureofthefactorfunctionstocomputeout-goingmessagesinpolynomialtime(Duchietal.,2007;SmithandEisner,2008).Fortunately,allofourhardconstraintspermitdynamicprogramsthataccomplishthispropagation.Spacedoesnotpermitafulldescriptionofthesedynamicprograms,butwewillbrieﬂysketchtheintuitionsbehindthem.SPANandONESPAN.MarginalbeliefsforSeiorUeicanbecomputedinO(nd2)time.Thekeyobser-vationisthatforanylegalvalueσei=[k,‘],SeiandUeirequirethataij=offforallj/∈[k,‘].6Thus,westartbycomputingtheproductofalltheoffbeliefs:4Wesetδ=0.001.5Forallasymptoticanalysis,wedeﬁnen=max(|e|,|f|).6Foreaseofexposition,weassumethatallalignmentsareeithersureoroff;themodiﬁcationstoaccountforthegeneralcasearestraightforward.FactorRuntimeCountTotalSURELINKO(1)O(n2)O(n2)PHRASEPAIRO(1)O(n2d2)O(n2d2)NULLWORDO(nd)O(n)O(n2d)SPANO(nd2)O(n)O(n2d2)EXTRACTO(d3)O(n2d2)O(n2d5)ITGO(n6)1O(n6)ONESPANO(nd2)O(n)O(n2d2)Table1:Asymptoticcomplexityforallfactors.¯b=Qjqaij(off).Then,foreachoftheO(nd)legalsourcespans[k,‘]wecanefﬁcientlyﬁndajointbe-liefbysummingoverconsistentassignmentstotheO(d)linkvariablesinthatspan.EXTRACT.MarginalbeliefsforPghk‘canbecomputedinO(d3)time.ForeachoftheO(d)targetwords,wecanﬁndthetotalincomingbeliefthatσeiiswithin[k,‘]bysummingovertheO(d2)values[k0,‘0]where[k0,‘0]⊆[k,‘].Likewiseforsourcewords.Multiplyingtogethertheseper-wordbeliefsandthebeliefthatπghk‘=trueyieldsthejointbe-liefofaconsistentassignmentwithπghk‘=true,whichcanbeusedtoefﬁcientlycomputeoutgoingmessages.ITG.Tobuildoutgoingmessages,theITGfac-torAneedstocomputemarginalbeliefsforallofthewordlinkvariablesaij.ThesecanallbecomputedinO(n6)timebyusingastandardbitextparsertoruntheinside-outsidealgorithm.ByusinganormalformgrammarforblockITGwithnulls(Haghighietal.,2009),weensurethatthereisa1-1correspon-dencebetweentheITGderivationstheparsersumsoverandwordalignmentsathatsatisfyA.TheasymptoticcomplexityforallthefactorsisshowninTable1.Thetotalcomplexityforinferenceineachmodelissimplythesumofthecomplexitiesofitsfactors,sothecomplexityoftheITGmodelisO(n2d5+n6),whilethecomplexityoftherelaxedmodelisjustO(n2d5).Thecomplexityofexactin-ference,ontheotherhand,isexponentialindfortheITGmodelandexponentialinbothdandnfortherelaxedmodel.35

6TrainingandDecodingWeuseBPtocomputemarginalposteriors,whichweuseattrainingtimetogetexpectedfeaturecountsandattesttimeforposteriordecoding.Foreachsen-tencepair,wecontinuetopassmessagesuntileithertheposteriorsconverge,orsomemaximumnumberofiterationshasbeenreached.7AfterrunningBP,themarginalsweareinterestedincanallbecom-putedwithEq.(9).6.1TrainingWetrainthemodeltomaximizetheloglikelihoodofmanuallyword-alignedgoldtrainingsentencepairs(withL2regularization).Becauseπandσaredeter-minedwhenaisobserved,themodelhasnolatentvariables.Therefore,thegradienttakesthestandardformforloglinearmodels:OLL=φ(a,π,σ)−(10)Xa0,π0,σ0p(a0,π0,σ0|e,f)φ(a0,π0,σ0)−λwThefeaturevectorφcontainsfeaturesonsurewordlinks,extractedphrasepairs,andnull-alignedwords.Approximateexpectationsofthesefeaturescanbeefﬁcientlycomputedusingthemarginalbe-liefsbaij(sure),bπghk‘(true),andbσei([−1,∞])andbσfj([−1,∞]),respectively.WelearnedourﬁnalweightvectorwusingAdaGrad(Duchietal.,2010),anadaptivesubgradientversionofstandardstochas-ticgradientascent.6.2TestingWeevaluateourmodelbymeasuringprecisionandrecallonextractedphrasepairs.Thus,thedecod-ingproblemtakesasentencepair(e,f)asinput,andmustproduceanextractionsetπasoutput.Ourap-proach,posteriorthresholding,isextremelysimple:wesetπghk‘=trueiffbπghk‘(true)≥τforsomeﬁxedthresholdτ.Notethatthisdecodingmethoddoesnotrequirethattherebeanyunderlyingwordalignmentalicensingtheresultingextractionsetπ,87SeeSection7.2foranempiricalinvestigationofthismaxi-mum.8Thiswouldbetrueevenifwecomputedposteriorsex-actly,butisespeciallytruewithapproximatemarginalsfromBP,whicharenotnecessarilyconsistent.butthestructureofthemodelissuchthattwocon-ﬂictingphrasepairsareunlikelytosimultaneouslyhavehighposteriorprobability.Mostpubliclyavailabletranslationsystemsex-pectword-levelalignmentsasinput.Thesecanalsobegeneratedbyapplyingposteriorthreshold-ing,aligningtargetworditosourcewordjwhen-everbaij(sure)≥t.97ExperimentsOurexperimentsareperformedonChinese-to-Englishalignment.WetrainedandevaluatedallmodelsontheNISTMT02testset,whichconsistsof150trainingand191testsentencesandhasbeenusedpreviouslyinalignmentexperiments(AyanandDorr,2006;Haghighietal.,2009;DeNeroandKlein,2010).TheunsupervisedHMMwordalignerusedtogeneratefeaturesforthemodelwastrainedon11.3millionwordsofFBISnewswiredata.Wetestthreemodels:theViterbiITGmodelofDeNeroandKlein(2010),ourBPITGmodelthatusestheITGfactor,andourBPRelaxedmodelthatreplacestheITGfactorwiththeONESPANfactors.Inallofourexperiments,thephraselengthdwassetto3.107.1PhraseAlignmentWetestedthemodelsbycomputingprecisionandrecallonextractedphrasepairs,relativetothegoldphrasepairsofuptolength3inducedbythegoldwordalignments.FortheBPmodels,wetradeoffprecisionandrecallbyadjustingthedecodingthresholdτ.TheViterbiITGmodelwastrainedtooptimizeF5,arecall-biasedmeasure,soinadditiontoF1,wealsoreporttherecall-biasedF2andF5measures.ThemaximumnumberofBPiterationswassetto5fortheBPITGmodelandto10fortheBPRelaxedmodel.ThephrasealignmentresultsareshowninFig-ure4.TheBPITGmodelperformscomparablytotheViterbiITGmodel.However,becauseposteriordecodingpermitsexplicittradeoffsbetweenpreci-sionandrecall,itcandomuchbetterintherecall-biasedmeasures,eventhoughtheViterbiITGmodelwasexplicitlytrainedtomaximizeF5(DeNeroand9Forourexperiments,wesett=0.2.10BecausetheruntimeoftheViterbiITGmodelgrowsexpo-nentiallywithd,itwasnotfeasibletoperformcomparisonsforhigherphraselengths.36

betaprf20.690.7420.730982360 65 70 75 80 60 65 70 75 80 85 Recall Precision Viterbi ITG BP ITG BP Relaxed ModelBestScoresSentencesF1F2F5perSecondViterbiITG71.673.174.00.21BPITG71.874.883.50.11BPRelaxed72.675.284.51.15Figure4:Phrasealignmentresults.AportionofthePre-cision/RecallcurveisplottedfortheBPmodels,withtheresultfromtheViterbiITGmodelprovidedforreference.Klein,2010).TheBPRelaxedmodelperformsthebestofall,consistentlyachievinghigherrecallforﬁxedprecisionthaneitheroftheothermodels.Be-causeofitslowerasymptoticruntime,itisalsomuchfaster:over5timesasfastastheViterbiITGmodelandover10timesasfastastheBPITGmodel.117.2TimingBPapproximatesmarginalposteriorsbyiterativelyupdatingbeliefsforeachvariablebasedoncur-rentbeliefsaboutothervariables.Theiterativena-tureofthealgorithmpermitsustomakeanexplicitspeed/accuracytradeoffbylimitingthenumberofiterations.WetestedthistradeoffbylimitingbothoftheBPmodelstorunfor2,3,5,10,and20iter-ations.TheresultsareshowninFigure5.NeithermodelbeneﬁtsfromrunningmoreiterationsthanusedtoobtaintheresultsinFigure4,buteachcanbespedupbyafactorofalmost1.5xinexchangeforamodest(<1F1)dropinaccuracy.11ThespeedadvantageofViterbiITGoverBPITGcomesfromViterbiITG’saggressivebeaming.SpeedF12.0833333361.3267.61.5873015971.91.1494252972.60.9615384672.667 68 69 70 71 72 73 0.5 1 2 4 8 16 Best F1 Time (seconds per sentence) Viterbi ITG BP ITG BP Relaxed 67 68 69 70 71 72 73 0.0625 0.125 0.25 0.5 1 2 Best F1 Speed (sentences per second) Viterbi ITG BP ITG BP Relaxed Figure5:Speed/accuracytradeoff.Thespeedaxisisonalogarithmicscale.Fromfastesttoslowest,datapointscorrespondtomaximumsof2,5,10,and20BPitera-tions.F1fortheBPRelaxedmodelwasverylowwhenlimitedto2iterations,sothatdatapointisoutsidethevisibleareaofthegraph.ModelBLEURelativeHourstoImprove.Train/AlignBaseline32.8+0.05ViterbiITG33.5+0.7831BPRelaxed33.6+0.839Table2:Machinetranslationresults.7.3TranslationWerantranslationexperimentsusingMoses(Koehnetal.,2007),whichwetrainedona22.1mil-lionwordparallelcorpusfromtheGALEprogram.WecomparedalignmentsgeneratedbythebaselineHMMmodel,theViterbiITGmodelandtheRe-laxedBPmodel.12Thesystemsweretunedandevaluatedonsentencesuptolength40fromtheNISTMT04andMT05testsets.Theresults,showninTable2,showthattheBPRelaxedmodelachivesa0.8BLEUimprovementovertheHMMbaseline,comparabletothatoftheViterbiITGmodel,buttak-ingafractionofthetime,13makingtheBPRelaxedmodelapracticalalternativeforrealtranslationap-plications.12FollowingasimpliﬁedversionoftheproceduredescribedbyDeNeroandKlein(2010),weaddedrulecountsfromtheHMMalignmentstotheextractionsetaligners’counts.13SomeofthespeeddifferencebetweentheBPRelaxedandViterbiITGmodelscomesfrombetterparallelizabilityduetodrasticallyreducedmemoryoverheadoftheBPRelaxedmodel.37

8ConclusionForperforminginferenceinastate-of-the-art,butin-efﬁcient,alignmentmodel,beliefpropagationisaviablealternativetogreedysearchmethods,suchasbeaming.BPalsoresultsinmodelsthataremuchmorescalable,byreducingtheasymptoticcomplex-ityofinference.Perhapsmostimportantly,BPper-mitstherelaxationofartiﬁcialconstraintsthataregenerallytakenforgrantedasbeingnecessaryforefﬁcientinference.Inparticular,arelativelymod-estrelaxationoftheITGconstraintcandirectlybeappliedtoanymodelthatusesITG-basedinference(e.g.ZhangandGildea,2005;CherryandLin,2007;Haghighietal.,2009).AcknowledgementsThisprojectisfundedbyanNSFgraduateresearchfellowshiptotheﬁrstauthorandbyBBNunderDARPAcontractHR0011-06-C-0022.ReferencesNecipFazilAyanandBonnieJ.Dorr.2006.Goingbe-yondAER:AnextensiveanalysisofwordalignmentsandtheirimpactonMT.InACL.AlexandraBirch,ChrisCallison-Burch,andMilesOs-borne.2006.Constrainingthephrase-based,jointprobabilitystatisticaltranslationmodel.InAMTA.PhilBlunsom,TrevorCohn,ChrisDyer,andMilesOs-borne.2009.Agibbssamplerforphrasalsynchronousgrammarinduction.InACL-IJCNLP.ColinCherryandDekangLin.2007.Inversiontransduc-tiongrammarforjointphrasaltranslationmodeling.InNAACLWorkshoponSyntaxandStructureinStatisti-calTranslation.DavidChiang.2007.Hierarchicalphrase-basedtransla-tion.ComputationalLinguistics,33(2):201–228.FabienCromi`eresandSadaoKurohashi.2009.Analignmentalgorithmusingbeliefpropagationandastructure-baseddistortionmodel.InEACL.SteveDeNeefe,KevinKnight,WeiWang,andDanielMarcu.2007.Whatcansyntax-basedMTlearnfromphrase-basedMT?InEMNLP-CoNLL.JohnDeNeroandDanKlein.2010.Discriminativemod-elingofextractionsetsformachinetranslation.InACL.JohnDeNero,DanGillick,JamesZhang,andDanKlein.2006.Whygenerativephrasemodelsunderperformsurfaceheuristics.InNAACLWorkshoponStatisticalMachineTranslation.JohnDeNero,AlexandreBouchard-Cˆot´e,andDanKlein.2008.SamplingalignmentstructureunderaBayesiantranslationmodel.InEMNLP.JohnDeNero.2010.PersonalCommunication.JohnDuchi,DannyTarlow,GalElidan,andDaphneKoller.2007.Usingcombinatorialoptimizationwithinmax-productbeliefpropagation.InNIPS2006.JohnDuchi,EladHazan,andYoramSinger.2010.Adaptivesubgradientmethodsforonlinelearningandstochasticoptimization.InCOLT.MichelGalley,MarkHopkins,KevinKnight,andDanielMarcu.2004.What’sinatranslationrule?InHLT-NAACL.MichelGalley,JonathanGraehl,KevinKnight,DanielMarcu,SteveDeNeefe,WeiWang,andIgnacioThayer.2006.Scalableinferenceandtrainingofcontext-richsyntactictranslationmodels.InCOLING-ACL.AriaHaghighi,JohnBlitzer,JohnDeNero,andDanKlein.2009.BetterwordalignmentswithsupervisedITGmodels.InACL-IJCNLP.PhilippKoehn,FranzJosefOch,andDanielMarcu.2003.Statisticalphrase-basedtranslation.InACL.PhilippKoehn,HieuHoang,AlexandraBirch,ChrisCallison-Burch,MarcelloFederico,NicolaBertoldi,BrookeCowan,WadeShen,ChristineMoran,RichardZens,ChrisDyer,OndrejBojar,AlexandraCon-stantin,andEvanHerbst.2007.Moses:Opensourcetoolkitforstatisticalmachinetranslation.InACL.ZhifeiLiandSanjeevKhudanpur.2008.Ascalabledecoderforparsing-basedmachinetranslationwithequivalentlanguagemodelstatemaintenance.InACLSSST.PercyLiang,BenTaskar,andDanKlein.2006.Align-mentbyagreement.InHLT-NAACL.DavidJ.C.MacKay.2003.Informationtheory,infer-ence,andlearningalgorithms.CambridgeUnivPress.DanielMarcuandDanielWong.2002.Aphrase-based,jointprobabilitymodelforstatisticalmachinetransla-tion.InEMNLP.JanNiehuesandStephanVogel.2008.Discriminativewordalignmentviaalignmentmatrixmodeling.InACLWorkshoponStatisticalMachineTranslation.DavidA.SmithandJasonEisner.2008.Dependencyparsingbybeliefpropagation.InEMNLP.BenTaskar,SimonLacoste-Julien,andDanKlein.2005.Adiscriminativematchingapproachtowordalign-ment.InEMNLP.DekaiWu.1997.Stochasticinversiontransductiongrammarsandbilingualparsingofparallelcorpora.ComputationalLinguistics,23(3):377–404.HaoZhangandDanielGildea.2005.Stochasticlexical-izedinversiontransductiongrammarforalignment.InACL.38

HaoZhang,ChrisQuirk,RobertC.Moore,andDanielGildea.2008.Bayesianlearningofnon-compositionalphraseswithsynchronousparsing.InACL:HLT.2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48,

Montr´eal, Canada, June 3-8, 2012. c(cid:13)2012 Association for Computational Linguistics

39

ContinuousSpaceTranslationModelswithNeuralNetworksLeHaiSonandAlexandreAllauzenandFranc¸oisYvonUniv.Paris-Sud,FranceandLIMSI/CNRSrueJohnvonNeumann,91403Orsaycedex,FranceFirstname.Lastname@limsi.frAbstractTheuseofconventionalmaximumlikelihoodestimateshinderstheperformanceofexistingphrase-basedtranslationmodels.Forlackofsufﬁcienttrainingdata,mostmodelsonlycon-siderasmallamountofcontext.Asapar-tialremedy,weexplorehereseveralcontin-uousspacetranslationmodels,wheretransla-tionprobabilitiesareestimatedusingacon-tinuousrepresentationoftranslationunitsinlieuofstandarddiscreterepresentations.Inordertohandlealargesetoftranslationunits,theserepresentationsandtheassociatedesti-matesarejointlycomputedusingamulti-layerneuralnetworkwithaSOULarchitecture.InsmallscaleandlargescaleEnglishtoFrenchexperiments,weshowthattheresultingmod-elscaneffectivelybetrainedandusedontopofan-gramtranslationsystem,deliveringsig-niﬁcantimprovementsinperformance.1IntroductionThephrase-basedapproachtostatisticalmachinetranslation(SMT)isbasedonthefollowinginfer-encerule,which,givenasourcesentences,selectsthetargetsentencetandtheunderlyingalignmentamaximizingthefollowingterm:P(t,a|s)=1Z(s)exp(cid:16)KXk=1λkfk(s,t,a)(cid:17),(1)whereKfeaturefunctions(fk)areweightedbyasetofcoefﬁcients(λk),andZisanormalizingfac-tor.Thephrase-basedapproachdiffersfromotherapproachesbythehiddenvariablesofthetranslationprocess:thesegmentationofaparallelsentencepairintophrasepairsandtheassociatedphrasealign-ments.Thisformulationwasintroducedin(Zensetal.,2002)asanextensionofthewordbasedmod-els(Brownetal.,1993),thenlatermotivatedwithinadiscriminativeframework(OchandNey,2004).Onemotivationforintegratingmorefeaturefunc-tionswastoimprovetheestimationofthetranslationmodelP(t|s),whichwasinitiallybasedonrelativefrequencies,thusyieldingpoorestimates.Thisisbecausetheunitsofphrase-basedmod-elsarephrasepairs,madeofasourceandatar-getphrase;suchunitsareviewedastheeventsofdiscreterandomvariables.Theresultingrepresenta-tionsofphrases(orwords)thusentirelyignorethemorphological,syntacticandsemanticrelationshipsthatexistamongthoseunitsinbothlanguages.Thislackofstructurehindersthegeneralizationpowerofthemodelandreducesitsabilitytoadapttootherdomains.Anotherconsequenceisthatphrase-basedmodelsusuallyconsideraveryrestrictedcontext1.ThisisageneralissueinstatisticalNaturalLan-guageProcessing(NLP)andmanypossiblereme-dieshavebeenproposedintheliterature,suchas,forinstance,usingsmoothingtechniques(ChenandGoodman,1996),orworkingwithlinguisticallyen-riched,ormoreabstract,representations.Instatisti-callanguagemodeling,anotherlineofresearchcon-sidersnumericalrepresentations,trainedautomat-icallythroughtheuseofneuralnetwork(seeeg.1typicallyasmallnumberofprecedingphrasepairsforthen-grambasedapproach(CregoandMari˜no,2006),ornocon-textatall,forthestandardapproachof(Koehnetal.,2007).40

(Collobertetal.,2011)).Aninﬂuentialproposal,inthisrespect,istheworkof(Bengioetal.,2003)oncontinuousspacelanguagemodels.Inthisap-proach,n-gramprobabilitiesareestimatedusingacontinuousrepresentationofwordsinlieuofstan-darddiscreterepresentations.Experimentalresults,reportedforinstancein(Schwenk,2007)showsig-niﬁcantimprovementsinspeechrecognitionappli-cations.Recently,thismodelhasbeenextendedinseveralpromisingways(Mikolovetal.,2011;Kuoetal.,2010;Liuetal.,2011).InthecontextofSMT,Schwenketal.(2007)istheﬁrstattempttoesti-matetranslationprobabilitiesinacontinuousspace.However,becauseoftheproposedneuralarchitec-ture,theauthorsonlyconsideraveryrestrictedsetoftranslationunits,andthereforereportonlyaslightimpactontranslationperformance.Therecentpro-posalof(Leetal.,2011a)seemsespeciallyrelevant,asitisable,throughtheuseofclass-basedmodels,tohandlearbitrarilylargevocabulariesandopensthewaytoenhancedneuraltranslationmodels.Inthispaper,weexplorevariousneuralarchitec-turesfortranslationmodelsandconsiderthreedif-ferentwaystofactorthejointprobabilityP(s,t)differingbytheunits(respectivelyphrasepairs,phrasesorwords)thatareprojectedincontinuousspaces.Whilethesedecompositionsaretheoreti-callystraightforward,theywerenotconsideredinthepastbecauseofdatasparsityissuesandoftheresultingweaknessesofconventionalmaximumlike-lihoodestimates.Ourmaincontributionisthentoshowthatsuchjointdistributionscanbeefﬁcientlycomputedbyneuralnetworks,evenforverylargecontextsizes;andthattheiruseyieldssigniﬁcantperformanceimprovements.Thesemodelsareeval-uatedinan-bestrescoringstepusingtheframeworkofn-grambasedsystems,withinwhichtheyinte-grateeasily.Note,howeverthattheycouldbeusedwithanyphrase-basedsystem.Therestofthispaperisorganizedasfollows.Weﬁrstrecollect,inSection2,then-grambasedap-proach,anddiscussvariousimplementationsofthisframework.Wethendescribe,inSection3,theneu-ralarchitecturedevelopedandexplainhowitcanbemadetohandlelargevocabularytasksaswellaslan-guagemodelsoverbilingualunits.Weﬁnallyre-port,inSection4,experimentalresultsobtainedonalarge-scaleEnglishtoFrenchtranslationtask.2Variationsonthen-gramapproachEventhoughn-gramtranslationmodelscanbeintegratedwithinstandardphrase-basedsystems(Niehuesetal.,2011),then-grambasedframe-workprovidesamoreconvenientwaytointroduceourworkandhasalsobeenusedtobuildthebase-linesystemsusedinourexperiments.Inthen-grambasedapproach(CasacubertaandVidal,2004;Mari˜noetal.,2006;CregoandMari˜no,2006),trans-lationisdividedintwosteps:asourcereorderingstepandatranslationstep.Sourcereorderingisbasedonasetoflearnedrewriterulesthatnon-deterministicallyreordertheinputwordssoastomatchthetargetordertherebygeneratingalatticeofpossiblereorderings.Translationthenamountstoﬁndingthemostlikelypathinthislatticeusingan-gramtranslationmodel2ofbilingualunits.2.1Thestandardn-gramtranslationmodeln-gramtranslationmodels(TMs)relyonaspe-ciﬁcdecompositionofthejointprobabilityP(s,t),wheresisasequenceofIreorderedsourcewords(s1,...,sI)andtcontainsJtargetwords(t1,...,tJ).Thissentencepairisfurtherassumedtobede-composedintoasequenceofLbilingualunitscalledtuplesdeﬁningajointsegmentation:(s,t)=u1,...,uL.Intheapproachof(Mari˜noetal.,2006),thissegmentationisaby-productofsourcereorder-ing,andultimatelyderivesfrominitialwordandphrasealignments.Inthisframework,thebasictranslationunitsaretuples,whicharetheanalogousofphrasepairs,andrepresentamatchingu=(s,t)betweenasourcesandatargettphrase(seeFig-ure1).Usingthen-gramassumption,thejointprob-abilityofasegmentedsentencepairdecomposesas:P(s,t)=LYi=1P(ui|ui−1,...,ui−n+1)(2)Aﬁrstissuewiththismodelisthattheelementaryunitsarebilingualpairs,whichmeansthattheunder-lyingvocabulary,hencethenumberofparameters,canbequitelarge,evenforsmalltranslationtasks.Duetodatasparsityissues,suchmodelsarebound2Likeinthestandardphrase-basedapproach,thetranslationprocessalsoinvolvesadditionalfeaturefunctionsthatarepre-sentedbelow.41

tofacesevereestimationproblems.Anotherprob-lemwith(2)isthatthesourceandtargetsidesplaysymmetricroles,whereasthesourcesideisknown,andthetargetsidemustbepredicted.2.2Afactoredn-gramtranslationmodelToovercomesomeoftheseissues,then-gramprob-abilityinequation(2)canbefactoredbydecompos-ingtuplesintwo(sourceandtarget)parts:P(ui|ui−1,...,ui−n+1)=P(ti|si,si−1,ti−1,...,si−n+1,ti−n+1)×P(si|si−1,ti−1...,si−n+1,ti−n+1)(3)Decomposition(3)involvestwomodels:theﬁrsttermrepresentsaTM,thesecondtermisbestviewedasareorderingmodel.Inthisformulation,theTMonlypredictsthetargetphrase,givenitssourceandtargetcontexts.Anotherbeneﬁtofthisformulationisthattheel-ementaryeventsnowcorrespondeithertosourceortotargetphrases,butnevertopairsofsuchphrases.Theunderlyingvocabularyisthusobtainedastheunion,ratherthanthecrossproduct,ofphrasein-ventories.Finallynotethatthen-gramprobabilityP(ui|ui−1,...,ui−n+1)couldalsofactoras:P(si|ti,si−1,ti−1,...,si−n+1,ti−n+1)×P(ti|si−1,ti−1,...,si−n+1,ti−n+1)(4)2.3AwordfactoredtranslationmodelAmoreradicalwaytoaddressthedatasparsityis-suesistotake(sourceandtarget)wordsasthebasicunitsofthen-gramTM.Thismayseemtobeastepbackwards,sincethetransitionfromword(Brownetal.,1993)tophrase-basedmodels(Zensetal.,2002)isconsideredasoneofthemainrecentimprovementinMT.Oneimportantmotivationforconsideringphrasesratherthanwordswastocapturelocalcon-textintranslationandreordering.Itshouldthenbestressedthatthedecompositionofphrasesinwordsisonlyre-introducedhereasawaytomitigatetheparameterestimationproblems.Translationunitsarestillpairsofphrases,derivedfromabilingualsegmentationintuplessynchronizingthesourceandtargetn-gramstreams,asdeﬁnedbyequation(3).Infact,theestimationpolicydescribedinsection3willactuallyallowustodesignn-grammodelswithlongercontextsthanistypicallypossibleinthecon-ventionaln-gramapproach.Letskidenotethekthwordofsourcetuplesi.ConsideringagaintheexampleofFigure1,s111istothesourcewordnobel,s411istothesourcewordpaix,andsimilarlyt211isthetargetwordpeace.Weﬁnallydenotehn−1(tki)thesequencemadeofthen−1wordsprecedingtkiinthetargetsentence:inFigure1,h3(t211)thusreferstothethreewordcon-textreceivethenobelassociatedwiththetargetwordpeace.Usingthesenotations,equation(3)isrewrit-tenas:P(s,t)=LYi=1h|ti|Yk=1P(cid:0)tki|hn−1(tki),hn−1(s1i+1)(cid:1)×|si|Yk=1P(cid:0)ski|hn−1(t1i),hn−1(ski)(cid:1)i(5)Thisdecompositionreliesonthen-gramassump-tion,thistimeatthewordlevel.Therefore,thismodelestimatesthejointprobabilityofasentencepairusingtwoslidingwindowsoflengthn,oneforeachlanguage;however,themovesofthesewin-dowsremainsynchronizedbythetuplesegmenta-tion.Moreover,thecontextisnotlimitedtothecur-rentphrase,andcontinuestoincludewordsinad-jacentphrases.UsingtheexampleofFigure1,thecontributionofthetargetphraset11=nobel,peacetoP(s,t)usinga3-grammodelisP(cid:0)nobel|[receive,the],[la,paix](cid:1)×P(cid:0)peace|[the,nobel],[la,paix](cid:1).Likewise,thecontributionofthesourcephrases11=nobel,de,la,paixis:P(cid:0)nobel|[receive,the],[recevoir,le](cid:1)×P(cid:0)de|[receive,the],[le,nobel](cid:1)×P(cid:0)la|[receive,the],[nobel,de](cid:1)×P(cid:0)paix|[receive,the],[de,la](cid:1).Abeneﬁtofthisnewformulationisthattheinvolvedvocabulariesonlycontainwords,andarethusmuchsmaller.Thesemodelsarethuslessboundtobeaf-fectedbydatasparsityissues.WhiletheTMdeﬁnedbyequation(5)derivesfromequation(3),avariationcanbeequivalentlyderivedfromequation(4).42

 s̅8: à  t̅8: to  s̅9: recevoir  t̅9: receive  s̅10: le  t̅10: the  s̅11: nobel de la paix  t̅11: nobel peace  s̅12: prix  t̅12: prize  u8  u9  u10  u11  u12 S :   .... T :   .... àrecevoirleprix nobel de la paixorg :   ............Figure1:ExtractofaFrench-Englishsentencepairsegmentedinbilingualunits.Theoriginal(org)Frenchsentenceappearsatthetopoftheﬁgure,justabovethereorderedsourcesandtargett.Thepair(s,t)decomposesintoasequenceofLbilingualunits(tuples)u1,...,uL.Eachtupleuicontainsasourceandatargetphrase:siandti.3TheSOULmodelIntheprevioussection,wedeﬁnedthreedifferentn-gramtranslationmodels,basedrespectivelyonequations(2),(3)and(5).Asdiscussedabove,amajorissuewithsuchmodelsistoreliablyestimatetheirparameters,thenumbersofwhichgrowexpo-nentiallywiththeorderofthemodel.Thisproblemisaggravatedinnaturallanguageprocessing,duetowellknowndatasparsityissues.Inthiswork,wetakeadvantageoftherecentproposalof(Leetal.,2011a):usingaspeciﬁcneuralnetworkarchitecture(theStructuredOUtputLayermodel),itbecomespossibletohandlelargevocabularylanguagemod-elingtasks,asolutionthatweadaptheretoMT.3.1LanguagemodelinginacontinuousspaceLetVbeaﬁnitevocabulary,n-gramlanguagemod-els(LMs)deﬁnedistributionsoverﬁnitesequencesoftokens(typicallywords)wL1inV+asfollows:P(wL1)=LYi=1P(wi|wi−1i−n+1)(6)Modelingthejointdistributionofseveraldiscreterandomvariables(suchaswordsinasentence)isdifﬁcult,especiallyinNLPapplicationswhereVtypicallycontainsdozensofthousandswords.Inspiteofthesimplifyingn-gramassump-tion,maximumlikelihoodestimationremainsun-reliableandtendstounderestimatetheproba-bilityofveryraren-grams.Smoothingtech-niques,suchasKneser-NeyandWitten-Bellback-offschemes(see(ChenandGoodman,1996)foranempiricaloverview,and(Teh,2006)foraBayesianinterpretation),performback-offtolowerorderdis-tributions,thusprovidinganestimatefortheproba-bilityoftheseunseenevents.Oneofthemostsuccessfulalternativetodateistousedistributedwordrepresentations(Bengioetal.,2003),wheredistributionallysimilarwordsarerep-resentedasneighborsinacontinuousspace.Thisturnsn-gramsdistributionsintosmoothfunctionsofthewordrepresentations.Theserepresentationsandtheassociatedestimatesarejointlycomputedinamulti-layerneuralnetworkarchitecture.Fig-ure2providesapartialrepresentationofthiskindofmodelandhelpsﬁguringouttheirprinciples.TocomputetheprobabilityP(wi|wi−1i−n+1),then−1contextwordsareprojectedinthesamecontinu-ousspaceusingasharedmatrixR;thesecontinuouswordrepresentationsarethenconcatenatedtobuildasinglevectorthatrepresentsthecontext;afteranon-lineartransformation,theprobabilitydistribu-tioniscomputedusingasoftmaxlayer.Themajordifﬁcultywiththeneuralnetworkap-proachremainsthecomplexityofinferenceandtraining,whichlargelydependsonthesizeoftheoutputvocabulary(i.e.thenumberofwordsthathavetobepredicted).Onepracticalsolutionistore-stricttheoutputvocabularytoashort-listcomposedofthemostfrequentwords(Schwenk,2007).How-ever,theusualsizeoftheshort-listisunder20k,whichdoesnotseemsufﬁcienttofaithfullyrepre-sentthetranslationmodelsofsection2.3.2PrinciplesofSOULTocircumventthisproblem,StructuredOutputLayer(SOUL)LMsareintroducedin(Leetal.,2011a).FollowingMnihandHinton(2008),theSOULmodelcombinestheneuralnetworkapproachwithaclass-basedLM(Brownetal.,1992).Struc-