Online Manifold Regularization:

A New Learning Setting and Empirical Study

Andrew B. Goldberg1, Ming Li2, and Xiaojin Zhu1

1 Department of Computer Sciences, University of Wisconsin-Madison

Madison, WI, USA. {goldberg, jerryzhu}@cs.wisc.edu

2 National Key Laboratory for Novel Software Technology, Nanjing University

Nanjing, China. lim@lamda.nju.edu.cn

Abstract. We consider a novel “online semi-supervised learning” set-
ting where (mostly unlabeled) data arrives sequentially in large volume,
and it is impractical to store it all before learning. We propose an on-
line manifold regularization algorithm. It diﬀers from standard online
learning in that it learns even when the input point is unlabeled. Our
algorithm is based on convex programming in kernel space with stochas-
tic gradient descent, and inherits the theoretical guarantees of standard
online algorithms. However, na¨ıve implementation of our algorithm does
not scale well. This paper focuses on eﬃcient, practical approximations;
we discuss two sparse approximations using buﬀering and online random
projection trees. Experiments show our algorithm achieves risk and gen-
eralization accuracy comparable to standard batch manifold regulariza-
tion, while each step runs quickly. Our online semi-supervised learning
setting is an interesting direction for further theoretical development,
paving the way for semi-supervised learning to work on real-world life-
long learning tasks.

1 Introduction

Consider a robot with a video camera. The robot continuously takes high frame-
rate video of its surroundings, and wants to learn the names of various objects
in the video. However, like a child learning in the real world, the robot receives
names from humans only very rarely. The robot is thus in a semi-supervised
learning situation: most objects are unlabeled, while only a few are labeled by
humans.

There are several challenges that distinguish this situation from standard
semi-supervised learning. The robot cannot aﬀord to store the massive amount
of mostly unlabeled video before learning; it requires an “anytime classiﬁer”
that is ready to use at all times, yet is continuously improving; training must
be cheap; and since the world is changing, it must adapt to non-stationarity in
classiﬁcation.

These challenges are well-studied in online learning. However, our situation
is also diﬀerent from standard online learning. Online learning (classiﬁcation)
traditionally assumes that every input point is fully labeled; it cannot take ad-
vantage of unlabeled data. But in the robot case, the vast majority of the input

will be unlabeled. It seems wasteful to throw away the unlabeled input, as it
may contain useful information.

We address this situation by combining semi-supervised learning with on-
line learning. The resulting online semi-supervised learning algorithm is based
on convex programming with stochastic gradient descent in kernel space. This
combination is novel. To the best of our knowledge, the closest prior work is
the multiview hidden Markov perceptron ([1], Section 4), which heuristically
combines multiview learning with online perceptron. However, that work did
not enjoy the theoretical guarantees aﬀorded by the online learning literature,
nor did it directly apply to other semi-supervised learning methods. In contrast,
our method can lift any batch semi-supervised learning methods with convex
regularized risks to the online setting. As a special case, we will discuss online
manifold regularization in detail.

The focus of the present work is to introduce a novel learning setting, and
to develop practical algorithms with experimental veriﬁcation. It is important
to consider the eﬃciency issues, as we do in Section 3, for the algorithm to
be practically relevant. Our online semi-supervised learning algorithm inherits
no-regret bounds from online convex programming but does not provide new
bounds. It is our hope that the novel setting where most of the incoming data
stream is unlabeled will inspire future work on improved bounds. Some of the
future directions are laid out at the end of the paper.

2 Online Semi-Supervised Learning

We build online semi-supervised learning with two main ingredients: online con-
vex programming [2] and regularized risk minimization for semi-supervised learn-
ing (see the overview in [3, 4]). Although kernel-based online convex program-
ming is well-understood [5], we are not aware of prior application in the semi-
supervised learning setting.

Consider an input sequence x1 . . . xT , where xt ∈ Rd is the feature vector
of the t-th data point. Most (possibly even the vast majority) of the points are
unlabeled. Only occasionally is a point xt accompanied by its label yt ∈ Y. This
setting diﬀers dramatically from traditional online learning where all points are
labeled. Let K be a kernel over x and HK the corresponding reproducing kernel
Hilbert space (RKHS) [6]. Our goal is to learn a good predictor f ∈ HK from
the sequence. Importantly, learning proceeds in an iterative fashion:

1. At time t an adversary picks xt and yt, not necessarily from any distribution
P (x, y) (although we will later assume iid for predicting future data). The
adversary presents xt to the learner.

2. The learner makes prediction ft(xt) using its current predictor ft.
3. With a small probability pl, the adversary reveals the label yt. Otherwise,

the adversary abstains, and xt remains unlabeled.

4. The learner updates its predictor to ft+1 based on xt and the adversary’s

feedback yt, if any.

We hope the functions f1 . . . fT “do well” on the sequence, and on future
input if the data is indeed iid. The exact performance criteria is deﬁned below.

2.1 Batch Semi-Supervised Risks

Before introducing our online learning algorithm, we ﬁrst review batch semi-
supervised learning, where the learner has access to the labeled and unlabeled
data all at once. A unifying framework for batch semi-supervised learning is risk
minimization with specialized “semi-supervised” regularizers. That is, one seeks
the solution f ∗ = argminf ∈HK J(f ), where the batch semi-supervised regularized
risk is

J(f ) =

1
l

T

Xt=1

δ(yt)c(f (xt), yt) +

λ1
2 kfk2

K + λ2Ω(f ),

where l is the number of labeled points, δ(yt) is an indicator function equal to 1
if yt is present (labeled) and 0 otherwise, c is a convex loss function, λ1, λ2 are
regularizer weights, kfkK is the RKHS norm of f , and Ω is the semi-supervised
regularizer which depends on f and x1 . . . xT . Speciﬁc choices of Ω lead to fa-
miliar semi-supervised learning methods:

i) Manifold regularization [7–9]:

Ω =

1
2T

T

Xs,t=1

(f (xs) − f (xt))2wst.

The edge weights wst deﬁne a graph over the T points, e.g., a fully connected
graph with Gaussian weights wst = e−kxs−xtk2/2σ2
. In this case, Ω is known as
the energy of f on the graph. It encourages label smoothness over the graph:
similar examples (large w) tend to have similar labels.

ii) Multiview learning [10–12] optimizes multiple functions f1 . . . fM simulta-

neously. The semi-supervised regularizer

Ω =

M

T

Xi,j=1

Xt=1

(fi(xt) − fj(xt))2

penalizes diﬀerences among the learners’ predictions for the same point.

iii) Semi-supervised support vector machines (S3VMs) [13–15]:

Ω =

1
T − l

T

Xt=1

(1 − δ(yt)) max(1 − |f (xt)|, 0).

This is the average “hat loss” on unlabeled points. The hat loss is zero if f (x)
is outside (−1, 1), and is the largest when f (x) = 0. It encourages the deci-
sion boundary f (x) = 0 to be far away from any unlabeled points (outside the
margin), thus avoiding cutting through dense unlabeled data regions.

2.2 From Batch to Online

A key observation is that for certain semi-supervised learning methods, the batch
risk J(f ) is the sum of convex functions in f . These methods include mani-
fold regularization and multiview learning, but not S3VMs whose hat loss is
non-convex. For these convex semi-supervised learning methods, one can derive
a corresponding online semi-supervised learning algorithm using online convex
programming. The remainder of the paper will focus on manifold regularization,
with the understanding that online versions of multiview learning and other
convex semi-supervised learning methods can be derived similarly.

We follow the general approach in [2, 5]. Recall the batch risk for our version

of manifold regularization in Section 2.1 is

J(f ) =

1
l

T

Xt=1

δ(yt)c(f (xt), yt) +

λ1
2 kfk2

K +

λ2
2T

T

Xs,t=1

(f (xs) − f (xt))2wst, (1)

and f ∗ is the batch solution that minimizes J(f ). In online learning, the learner
only has access to the input sequence up to the current time. We thus deﬁne the
instantaneous regularized risk Jt(f ) at time t to be

Jt(f ) =

T
l

δ(yt)c(f (xt), yt) +

λ1
2 kfk2

K + λ2

t−1

Xi=1

(f (xi) − f (xt))2wit.

(2)

The last term in Jt(f ) involves the graph edges from xt to all previous points
up to time t. The astute reader might notice that this poses a computational
challenge—we will return to this issue in Section 3. While T appears in (2), Jt(f )
depends only on the ratio T /l. This is the empirical estimate of the inverse label
probability 1/pl, which we assume is given and easily determined based on the
rate at which humans can label the data at hand.

All the Jt’s are convex. They are intimately connected to the batch risk J:

Proposition 1 J(f ) = 1

t=1 Jt(f ).

T PT

Our online algorithm constructs a sequence of functions f1 . . . fT . Let f1 = 0.
The online algorithm simply performs a gradient descent step that aims to reduce
the instantaneous risk in each iteration:

ft+1 = ft − ηt

∂Jt(f )

∂f

(cid:12)(cid:12)(cid:12)(cid:12)ft

.

(3)

The step size ηt needs to decay at a certain rate, e.g., ηt = 1/√t. Under mild
conditions, this seemingly na¨ıve online algorithm has a remarkable guarantee
that on any input sequence, there is asymptotically “no regret” compared to the
batch solution f ∗. Speciﬁcally, let the average instantaneous risk incurred by
the online algorithm be Jair(T ) ≡ 1
t=1 Jt(ft). Note Jair involves a varying

T PT

sequence of functions f1 . . . fT . As a standard quality measure in online learning,
we compare Jair to the risk of the best ﬁxed function in hindsight:

Jair(T ) − min
= Jair(T ) − min

f

f

T

Jt(f )

1
Xt=1
T
J(f ) = Jair(T ) − J(f ∗),

where we used Proposition 1. This diﬀerence is known as the average regret. Ap-
plying Theorem 1 in [2] results in the no-regret guarantee lim supT →∞ Jair(T )−
J(f ∗) ≤ 0. It is in this sense that the online algorithm performs as well as the
batch algorithm on the sequence.
To compute (3) for manifold regularization, we ﬁrst express the functions

f1 . . . fT using a common set of representers x1 . . . xT [16]

ft =

t−1

Xi=1

α(t)
i K(xi,·).

(4)

The problem of ﬁnding ft+1 becomes computing the coeﬃcients α(t+1)
, . . . , α(t+1)
Again, this will be a computational issue when T is large, and will be addressed
in Section 3. We extend the kernel online supervised learning approach in [5] to
semi-supervised learning by writing the gradient ∂Jt(f )/∂f in (3) as

1

t

.

T
l

δ(yt)c′(f (xt), yt)K(xt,·) + λ1f

+2λ2

t−1

Xi=1

(f (xi) − f (xt))wit(K(xi,·) − K(xt,·)),

(5)

where we used the reproducing property of RKHS in computing the derivative:
∂f (x)/∂f = ∂hf, K(x,·)i/∂f = K(x,·). c′ is the (sub)gradient of the loss func-
tion c. For example, when c(f (x), y) is the hinge loss max(1− f (x)y, 0), we may
deﬁne c′(f (x), y) = −y if f (x)y ≤ 1, and 0 otherwise. Putting (5) back in (3),
and replacing ft with its kernel expansion (4), it can be shown that ft+1 has the
following coeﬃcients:

α(t+1)
i

α(t+1)
t

= (1 − ηtλ1)α(t)
= 2ηtλ2

t−1

Xi=1

(ft(xi) − ft(xt))wit − ηt

i − 2ηtλ2(ft(xi) − ft(xt))wit,

i = 1 . . . t − 1

T
l

δ(yt)c′(f (xt), yt).

(6)

We now have a basic online manifold regularization algorithm; see Algorithm 1.

When the data is iid, the generalization risk of the average function ¯f =
t=1 ft approaches that of f ∗ [17]. The average function ¯f involves all
representers x1, . . . , xT . For basic online manifold regularization, it is possible
to incrementally maintain the exact ¯f as time increases. However, for the sparse

1/T PT

Algorithm 1 Online Manifold Regularization

Parameters: edge weight function w, kernel K, weights λ1, λ2, loss function c, label
ratio T /l, step sizes ηt
Initialize t = 1, f1 = 0
loop

receive xt, predict ft(xt) using (4)
(occasionally) receive yt
update ft to ft+1 using (6)
store xt, let t = t + 1

end loop

approximations introduced below, the basis changes over time. Therefore, in
those cases ¯f can be maintained only approximately using matching pursuit [18].
In our experiments, we compare the classiﬁcation accuracy of ¯f vs. f ∗ on a
separate test set, which is of practical interest.

3 Sparse Approximations

Unfortunately, Algorithm 1 will not work in practice because it needs to store
every input point and soon runs out of memory; it also has time complexity
O(T 2). In particular, the instantaneous risk (2) and the kernel representation (4)
both involve the sequence up to the current time. To be useful, it is imperative
to sparsify both terms. In this section, we present two distinct approaches for
this purpose: i) using a small buﬀer of points, and ii) constructing a random
projection tree that represents the manifold structure.

3.1 Buﬀering

Buﬀering (e.g., [19] and the references therein) keeps a limited number of points.
Let the buﬀer size be τ . The simplest buﬀering strategy replaces the oldest point
xt−τ in the buﬀer with the incoming point xt. With buﬀering, the approximate
instantaneous risk is

Jt(f ) =

T
l

δ(yt)c(f (xt), yt) +

λ1
2 kfk2

K + λ2

t
τ

t−1

Xi=t−τ

(f (xi) − f (xt))2wit,

(7)

where the scaling factor t/τ keeps the magnitude of the graph regularizer com-
parable to the unbuﬀered version. In terms of manifold regularization, buﬀering
corresponds to a dynamic graph on the points in the buﬀer. Similarly, the kernel
expansion now has τ terms:

ft =

t−1

Xi=t−τ

α(t)
i K(xi,·).

With buﬀering, the function update involves two steps. In the ﬁrst step, we
update ft to an intermediate function f ′ represented by a basis of τ +1 elements,
consisting of the old buﬀer and the new point xt:

f ′ =

t−1

Xi=t−τ

iK(xi,·) + α′
α′

tK(xt,·)

i = (1 − ηtλ1)α(t)
α′
Xi=t−τ
α′
t = 2ηtλ2

t
τ

t−1

i − 2ηtλ2(ft(xi) − ft(xt))wit,
(ft(xi) − ft(xt))wit − ηt

T
l

i = t − τ . . . t − 1

δ(yt)c′(f (xt), yt).

(8)

Second, we evict xt−τ from the buﬀer, add xt to the buﬀer, and approximate f ′
(which uses τ + 1 basis functions) with ft+1 (which uses τ basis functions):

t

α(t+1) kf ′ − ft+1k2
min

s.t. ft+1 =

α(t+1)
i

Xi=t−τ +1

K(xi,·).

(9)

Intuitively, we “spread” α′
t−τ K(xt−τ ,·) to the remaining points in the buﬀer, in
an attempt to minimize the change caused by truncation. We use kernel matching
pursuit [18] to eﬃciently ﬁnd the approximate coeﬃcients α(t+1) in (9). Matching
pursuit is a greedy function approximation scheme. It iteratively selects a basis
function on which to spread the residual in α′
t−τ K(xt−τ ,·). The number of steps
(i.e., basis functions selected) can be controlled to trade-oﬀ approximation error
and speed. We run matching pursuit until the norm of the residue vector has
been suﬃciently reduced. We call the above buﬀering strategy “buﬀer.” The
overall time complexity for buﬀering is O(T ).

An alternative buﬀering strategy, “buﬀer-U,” evicts the oldest unlabeled
points in the buﬀer while keeping labeled points. This is motivated by the fact
that the labeled points tend to have larger α coeﬃcients and exert more inﬂuence
on our learned function. The oldest labeled point is evicted from the buﬀer only
when it is ﬁlled with labeled points. Note this is distinct from batch learning:
the labeled points only form a better basis, but learning is still done via gradient
descent.

3.2 Random Projection Tree

Another way to improve Algorithm 1 is to construct a sparse representation of
the manifold. While many embedding techniques exist, we require one that is
fast and can be incrementally modiﬁed. Recently random projection has been
proposed as an eﬃcient means to preserve the manifold structure (see e.g., [20,
21]). We build our algorithm upon the online version of the Random Projection
Tree (RPtree [22], Appendix I). An RPtree is a tree data structure with desirable
theoretical properties that asymptotically traces the manifold. The basic idea is
simple: as points arrive sequentially, they are spatially sorted into the RPtree
leaves. When enough points fall into a leaf, the RPtree grows by splitting the

Fig. 1. A random projection tree on the Swiss roll data. Small dots represent data
points, line segments represent the random splits in the internal nodes of the RPtree,
polygons represent the regions governed by the leaves, and ellipses represent the Gaus-
sian distributions on the data points within each leaf. We exploit the fact that these
distributions follow the manifold structure of the data.

leaf along a hyperplane with random orientation. An RPtree can be regarded as
an eﬃcient online clustering algorithm whose clusters grow over time and cover
the manifold, as shown in Figure 1. We refer the reader to [22] for details, while
presenting our extensions for semi-supervised learning below.

Let {Li}s

i=1, s ≪ t denote the leaves in the RPtree at time t. To model the
data points that have fallen into each leaf, we maintain a Gaussian distribution
N (µi, Σi) at each leaf Li, where µi and Σi are estimated incrementally as the
data points arrive. We also keep track of ni, the number of points in leaf Li. With
an RPtree, we approximate the kernel representation of ft (4) by the means of the
i K(µi,·).

Gaussian distributions associated with the tree leaves: ft = Ps

We approximate the instantaneous risk (2) by

i=1 β(t)

Jt(f ) =

T
l

δ(yt)c(f (xt), yt) +

λ1
2 kfk2

K + λ2

s

Xi=1

ni(f (µi) − f (xt))2wµit. (10)

From a graph regularization point of view, this can be understood as having a
coarser graph over the RPtree leaves. We deﬁne the edge weight wµit between
incoming point xt and each leaf Li to be

wµit = Ex∼N (µi,Σi)(cid:20)exp(cid:18)−||x − xt||2

2σ2

(cid:19)(cid:21)

(11)

= (2π)− d
exp(cid:18)−

1
2

2|Σ0|− 1
i Σ−1

2| ˜Σi|
i µi + x⊤

1

2 |Σi|− 1
2 (cid:16)µ⊤
i + Σ−1

where Σ0 = σ2I, ˜Σi = (Σ−1
0 xt, and σ is the
bandwidth of the (original point to point) weight. We call this weight scheme

0 )−1, ˜µi = Σ−1

i µi + Σ−1

t Σ−1

0 xt − ˜µ⊤

i

˜Σi ˜µi(cid:17)(cid:19) ,

“RPtree PPK” for its similarity to the probability product kernel [23]. An
even simpler approximation is to ignore the covariance structure by deﬁning
wµit = e−kµi−xtk2/2σ2
. It has computational advantages at the price of precision.
We call this weight scheme “RPtree.”

With an RPtree, the function update occurs in three steps. As space pre-
cludes a detailed discussion, we present an outline here. In the ﬁrst step, upon
receiving xt, we update ft to an intermediate function f ′ using a basis of s + 1
elements: µ1, . . . , µs and xt. This is similar to (8) in the buﬀering case. In the
second step, the RPtree itself is adjusted to account for the addition of xt. The
adjustments include updating the Gaussian parameters for the leaf xt falls into,
and potentially splitting the leaf. In the latter case, the number of leaves s will
increase to s′, and each new leaf’s mean and covariance statistics are established.
In the third step, we approximate f ′ by ft+1 using the s′ new basis elements
µ1, . . . , µs′ (s′ = s if no split happened), similar to (9). The point xt is then
discarded.

4 Experiments

We present a series of experimental results as empirical evidence that online
manifold regularization (MR) is a viable option for performing fast MR on large
data sets. We summarize our ﬁndings as follows:

1. Online MR scales better than batch MR in time and space. Although recent
advances in manifold regularization greatly improve the feasible problem size
(e.g., [24]), we believe that it takes online learning to handle unlimited input
sequences and achieve life-long learning.

2. Online MR achieves comparable performance to batch MR. This is measured

by two criteria:
(a) Jair(T ) approaches J(f ∗), both for the basic online MR algorithm, as

well as for the buﬀering and RPtree approximations.

(b) Generalization error of ¯f approaches that of f ∗ on test sets.

3. Online MR can handle concept drift (changes in P (x) and P (y|x)). The
online method (using a limited size buﬀer) can track a non-stationary distri-
bution and maintain good generalization accuracy, while the batch method
trained on all previous data fails to do so.

Our focus is on comparing online MR to batch MR, not semi-supervised
learning to supervised learning. It is known that semi-supervised learning does
not necessarily outperform supervised learning, depending on the correctness
of model assumptions. Thus, our experiments use tasks where batch MR has
proven beneﬁcial in prior work, and we demonstrate that online MR provides a
useful alternative to batch MR on these tasks.

4.1 Data Sets and Protocol

We report results on three data sets. The ﬁrst is a toy two-spirals data set. The
training sequences and test sets (of size 2000) are generated iid. The second is

the MNIST digit classiﬁcation data set [25], and we focus on two binary tasks:
0 vs. 1 and 1 vs. 2. We scaled down the images to 16 x 16 pixels (256 features).
The training sequences are randomly shuﬄed subsets of the oﬃcial training sets,
and we use the oﬃcial test sets (of size 2115 for 0 vs. 1, and 2167 for 1 vs. 2). The
third is the 361-dimensional Extended MIT face vs. non-face image classiﬁcation
data set (“Face”) [26]. We sampled a balanced subset of the data, and split this
into a training set and a test set. The same test set of size 2000 is used in all
experiments, while diﬀerent training runs use diﬀerent randomly shuﬄed subsets
of the training set. The labeled rate pl is 0.02 in all experiments, with points
assigned to each class with equal probability.

Our experimental protocol is the following:

1. Generate randomly ordered training sequences and test sets (for MNIST and

Face, the test sets are already given).

2. For batch MR, train separate versions on increasing subsequences (i.e., T =

500, 1000, 2000, . . .).

3. For online MR, train once on the entire sequence.
4. For each T , compare the corresponding batch MR f ∗ with the online classiﬁer

trained up to T .

All results are the average of ﬁve such trials. The error bars are ±1 standard
deviation.
All experiments use hinge loss c and RBF kernel K. The kernel bandwidth
parameter σK, λ1, λ2, and the edge weight parameter σ were all tuned for batch
MR using T = 500. When using a limited size buﬀer, we set τ = 300, and only
require that matching pursuit reduce the residue norm by 50%. We use a step
size of ηt = γ/√t, where γ = 0.03 for the RPtree approximation, and 0.1 for all
other methods. We implemented all methods using MATLAB and CPLEX.

4.2 Online MR Scales Better than Batch MR

We illustrate this point by comparing runtime growth curves on the spirals and
MNIST 0 vs. 1 data sets. Figure 2(left) shows that, for the spirals data set, the
growth rates of batch MR and basic online MR are quadratic as expected (in fact,
online MR has more overhead in our MATLAB implementation). Batch MR runs
out of memory after T = 5000, and we stop basic online MR at T = 4000 because
the runtime becomes excessive. On the other hand, online MR (buﬀered) and
online RPtree are linear. Though not included in the plot, online RPtree PPK has
a curve nearly identical to online MR (buﬀered). Figure 2(right) demonstrates
similar trends for the higher dimensional MNIST 0 vs. 1 data set.

4.3 Online MR Achieves Comparable Risks

We compare online MR’s average instantaneous risk Jair(T ) vs. batch MR’s risk
J(f ∗) on the training sequence. Our experiments support the theory that Jair(T )

Spirals

500

400

300

200

100

)
s
d
n
o
c
e
s
(
 
e
m
T

i

0
0

2000 4000 6000

T

500

400

300

200

100

0
 
0

MNIST 0 vs. 1

 

Batch MR
Online MR
Online MR (buffer)
Online RPtree

5000

T

10000

Fig. 2. Runtime growth curves. Batch MR and basic online MR scale quadratically,
while the sparse approximations of buﬀering and RPtree scale only linearly.

 

J(f*) Batch MR
J
air
J
air
J
air

(T) Online MR
(T) Online MR (buffer)
(T) Online RPtree

k
s
R

i

1.7

1.6

1.5

1.4

1.3

1.2

1.1

1

0.9

0.8

0.7
 
0

500

1000 1500 2000 2500 3000 3500 4000 4500

T

Fig. 3. Online MR’s average instantaneous risk Jair(T ) approaches batch MR’s risk
J(f ∗) as T increases.

converges to J(f ∗) as T increases. 3 Figure 3 compares these measures for basic
online MR and batch MR on the spirals data set. The two curves approach each
other. Jair(T ) continues to decrease beyond T = 4000 (not pictured). Figure 3
also shows that online MR (buﬀer) and online RPtree are good approximations
to basic online MR in terms of Jair.

4.4 Generalization Error of Online MR

The experiments in this section compare the averaged function ¯f of online MR
and the batch solution f ∗ in terms of generalization error on test sets. Figure 4
presents results for all the data sets. We observe that online MR buﬀer-U is the
best and consistently achieves test accuracy that is comparable to batch MR.
3 While the average regret approaches zero asymptotically, the step size of ηt = 1/√t
decays rapidly, potentially leading to slow convergence. Thus, it is possible that
long sequences (i.e., large T values) would be required for the online algorithm to
compete with the best batch algorithm. Nevertheless, our experiments show this is
not actually a problem in practice.

t

e
a
r
 
r
o
r
r
e
n
o

 

i
t

a
z

i
l

a
r
e
n
e
G

e

t

a
r
 
r
o
r
r
e

 

n
o

i
t

a
z

i
l

a
r
e
n
e
G

0.3

0.25

0.2

0.15

0.1

0.05

0
 
0

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0
 
0

 

Batch MR
Online MR
Online MR (buffer)
Online MR (buffer−U)
Online RPtree
Online RPtree (PPK)

1000

2000

3000

T

4000

5000

6000

7000

(a) Spirals

 

Batch MR
Online MR
Online MR (buffer)
Online MR (buffer−U)
Online RPtree

2000

4000

T

6000

8000

10000

e
t
a
r
 
r
o
r
r
e
 
n
o
i
t
a
z

i
l

a
r
e
n
e
G

e

t

a
r
 
r
o
r
r
e

 

n
o

i
t

a
z

i
l

a
r
e
n
e
G

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0
 
0

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0
 
0

 

Batch MR
Online MR
Online MR (buffer)
Online MR (buffer−U)
Online RPtree

2000

4000

T

6000

8000

10000

(b) Face

 

Batch MR
Online MR
Online MR (buffer)
Online MR (buffer−U)
Online RPtree

2000

4000

T

6000

8000

10000

(c) MNIST 0 vs. 1

(d) MNIST 1 vs. 2

Fig. 4. Generalization error of batch MR’s f ∗ and online MR’s ¯f as T increases. Online
MR buﬀer-U consistently achieves test accuracy comparable to batch MR.

From Figure 4(a), we observe that, for the spirals data set, all the online
methods perform nearly as well as batch MR. As is to be expected, batch MR
makes the most eﬃcient use of the data and reaches 0 test error ﬁrst, while
the online methods require only a little additional data to reach this level (after
all, standard incremental learning usually needs multiple passes over the training
set). Buﬀering and RPtree perform as well as basic online MR, showing little sign
of approximation error. Panels (b), (c), and (d) in Figure 4 show that buﬀer-U
can be much better than buﬀer. This is understandable, since matching pursuit
may provide a poor approximation to the contributions of the discarded data
point. In high dimensional space, there may be few similar data points remain-
ing in the small buﬀer, so much of the weight assigned to discarded points is
lost. Under the buﬀer-U strategy, we alleviate this issue by preserving the larger
weights on labeled points, which approximate the function better. RPtree PPK
on these high dimensional data sets involves expensive inversion of (often singu-
lar) covariance matrices and is not included in the comparison. The performance
of RPtree is no better than buﬀer-U.

e

t

a
r
 
r
o
r
r
e

 

n
o

i
t

a
z

i
l

a
r
e
n
e
G

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0
 
0

 

Batch MR
Online MR (buffer)

1000

2000

3000

T

4000

5000

6000

7000

Fig. 5. Online MR (buﬀer) has much better generalization error than batch MR when
faced with concept drift in the rotating spirals data set.

4.5 Online MR Handles Concept Drift

Lastly, we demonstrate that online MR can handle concept drift. When the
underlying distributions, both P (x) and P (y|x), change during the course of
learning, using buﬀered online MR is extremely advantageous. For this experi-
ment, we “spin” the two spirals data set so that the spirals smoothly rotate 360◦
in every 4000 points (Figure 5). All points in the space will thus change their
true labels during the sequence. We still provide only 2% of the labels to the
algorithms. The test set for a given T consists of 2000 points drawn from the
current underlying distribution.

For this experiment, we show the generalization error of batch MR’s f ∗ vs. on-
line MR (buﬀer)’s fT , since the latest function is expected to track the changes
in the data. Figure 5 illustrates that online MR (buﬀer) is able to adapt to
the changing sequence and maintain a small error rate. In contrast, batch MR
uses all data points, which now tend to conﬂict heavily (i.e., newer data from
one class overlaps with older data from the other class). As expected, the single
batch classiﬁer f ∗ is inadequate for predicting such changing data.

5 Conclusions

We presented an online semi-supervised learning algorithm that parallels man-
ifold regularization. Our algorithm is based on online convex programming in
RKHS. We proposed two sparse approximations using buﬀering and online ran-
dom projection trees to make online MR practical. The original batch manifold
regularization algorithm has time complexity at least O(T 2); so does the online

version without sparse approximation. In contrast, the RPtree approximation
has complexity O(T log T ), where each iteration requires O(log T ) leaf lookups
(the tree’s height is O(log T ) because each leaf contains a constant maximum
number of points). Buﬀering has complexity O(T ). Experiments show that our
online MR algorithm has risk and generalization error comparable to batch MR,
but scales much better. In particular, online MR (buﬀer-U) tends to have the
best performance.

There are many interesting questions remaining in this online semi-supervised
learning setting. Future work will proceed along two directions. On the empiri-
cal side, we will further speed up online MR, for example by using fast neighbor
search to reduce the number of candidate basis elements in matching pursuit. We
also plan to study practical online algorithms for other semi-supervised learning
methods, in particular those with non-convex risks like S3VMs. On the theoreti-
cal side, we plan to investigate diﬀerent regret notions that might be appropriate
for this setting, performance guarantees with concept drift, and models that do
not require all previous points.

Acknowledgements

A. Goldberg and X. Zhu were supported in part by the Wisconsin Alumni Re-
search Foundation. This work was completed while M. Li was a visiting re-
searcher at University of Wisconsin-Madison under a State Scholarship from the
Chinese Scholarship Council. The authors also thank Shuchi Chawla for helpful
discussions on online learning.

References

1. Brefeld, U., B¨uscher, C., Scheﬀer, T.: Multiview discriminative sequential learning.

In: European Conference on Machine Learning (ECML). (2005)

2. Zinkevich, M.: Online convex programming and generalized inﬁnitesimal gradient

ascent. In: ICML’03. (2003)

3. Chapelle, O., Zien, A., Sch¨olkopf, B., eds.: Semi-supervised learning. MIT Press

(2006)

4. Zhu, X.: Semi-supervised learning literature survey. Technical Report 1530, De-

partment of Computer Sciences, University of Wisconsin, Madison (2005)

5. Kivinen, J., Smola, A.J., Williamson, R.C.: Online learning with kernels. IEEE

Transactions on Signal Processing 52(8) (2004) 2165–2176

6. Sch¨olkopf, B., Smola, A.J.: Learning with Kernels. MIT Press (2002)
7. Belkin, M., Niyogi, P., Sindhwani, V.: Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. Journal of Machine Learn-
ing Research 7 (2006) 2399–2434

8. Sindhwani, V., Niyogi, P., Belkin, M.: Beyond the point cloud: from transductive

to semi-supervised learning. In: ICML’05. (2005)

9. Zhu, X., Ghahramani, Z., Laﬀerty, J.: Semi-supervised learning using Gaussian

ﬁelds and harmonic functions. In: ICML’03. (2003)

10. Blum, A., Mitchell, T.: Combining labeled and unlabeled data with co-training.

In: COLT’98. (1998)

11. Sindhwani, V., Niyogi, P., Belkin, M.: A co-regularized approach to semi-supervised

learning with multiple views. In: ICML’05. (2005)

12. Brefeld, U., Gaertner, T., Scheﬀer, T., Wrobel, S.: Eﬃcient co-regularized least

squares regression. In: ICML’06. (2006)

13. Joachims, T.: Transductive inference for text classiﬁcation using support vector

machines. In: ICML’99. (1999)

14. Chapelle, O., Sindhwani, V., Keerthi, S.S.: Branch and bound for semi-supervised

support vector machines. In: NIPS’06. (2006)

15. Collobert, R., Sinz, F., Weston, J., Bottou, L.: Large scale transductive SVMs.

The Journal of Machine Learning Research 7(Aug) (2006) 1687–1712

16. Kimeldorf, G., Wahba, G.: Some results on Tchebychean spline functions. Journal

of Mathematics Analysis and Applications 33 (1971) 82–95

17. Cesa-Bianchi, N., Conconi, A., Gentile, C.: On the generalization ability of on-
line learning algorithms. IEEE Transactions on Information Theory 50(9) (2004)
2050–2057

18. Vincent, P., Bengio, Y.: Kernel matching pursuit. Machine Learning 48(1-3) (2002)

165–187

19. Dekel, O., Shalev-Shwartz, S., Singer, Y.: The forgetron: A kernel-based perceptron

on a ﬁxed budget. In: NIPS’05. (2005)

20. Hegde, C., Wakin, M., Baraniuk, R.: Random projections for manifold learning.

In: NIPS’07. (2007)

21. Freund, Y., Dasgupta, S., Kabra, M., Verma, N.: Learning the structure of mani-

folds using random projections. In: NIPS’07. (2007)

22. Dasgupta, S., Freund, Y.: Random projection trees and low dimensional manifolds.

Technical Report CS2007-0890, University of California, San Diego (2007)

23. Jebara, T., Kondor, R., Howard, A.: Probability product kernels. Journal of
Machine Learning Research, Special Topic on Learning Theory 5 (2004) 819–844
24. Tsang, I., Kwok, J.: Large-scale sparsiﬁed manifold regularization. In: NIPS’06.

(2006)

25. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P.: Gradient-based learning applied
to document recognition. Proceedings of the IEEE 86(11) (November 1998) 2278–
2324

26. Tsang, I.W., Kwok, J.T., Cheung, P.M.: Core vector machines: Fast svm training
on very large data sets. Journal of Machine Learning Research 6 (2005) 363–392

