Semi-SupervisedRecursiveAutoencodersforPredictingSentimentDistributionsRichardSocherJeffreyPenningtonEricH.HuangAndrewY.NgChristopherD.ManningComputerScienceDepartment,StanfordUniversity,Stanford,CA94305,USASLACNationalAcceleratorLaboratory,StanfordUniversity,Stanford,CA94309,USArichard@socher.org{jpennin,ehhuang,ang,manning}@stanford.eduang@cs.stanford.eduAbstractWeintroduceanovelmachinelearningframe-workbasedonrecursiveautoencodersforsentence-levelpredictionofsentimentlabeldistributions.Ourmethodlearnsvectorspacerepresentationsformulti-wordphrases.Insentimentpredictiontaskstheserepresen-tationsoutperformotherstate-of-the-artap-proachesoncommonlyuseddatasets,suchasmoviereviews,withoutusinganypre-denedsentimentlexicaorpolarityshiftingrules.Wealsoevaluatethemodelsabilitytopredictsentimentdistributionsonanewdatasetbasedonconfessionsfromtheexperienceproject.Thedatasetconsistsofpersonaluserstoriesannotatedwithmultiplelabelswhich,whenaggregated,formamultinomialdistributionthatcapturesemotionalreactions.Oural-gorithmcanmoreaccuratelypredictdistri-butionsoversuchlabelscomparedtoseveralcompetitivebaselines.1IntroductionTheabilitytoidentifysentimentsaboutpersonalex-periences,products,moviesetc.iscrucialtoun-derstandusergeneratedcontentinsocialnetworks,blogsorproductreviews.Detectingsentimentinthesedataisachallengingtaskwhichhasrecentlyspawnedalotofinterest(PangandLee,2008).Currentbaselinemethodsoftenusebag-of-wordsrepresentationswhichcannotproperlycapturemorecomplexlinguisticphenomenainsentimentanaly-sis(Pangetal.,2002).Forinstance,whilethetwophraseswhitebloodcellsdestroyinganinfectionandaninfectiondestroyingwhitebloodcellshavethesamebag-of-wordsrepresentation,theformerisapositivereactionwhilethelaterisverynegative.Moreadvancedmethodssuchas(Nakagawaetal.,IndicesWordsSemantic RepresentationsRecursive Autoencoderi         walked      into         a        parked     carSorry, Hugs      You Rock       Teehee    I Understand    Wow, Just WowPredicted Sentiment DistributionFigure1:Illustrationofourrecursiveautoencoderarchi-tecturewhichlearnssemanticvectorrepresentationsofphrases.Wordindices(orange)arerstmappedintoasemanticvectorspace(blue).Thentheyarerecursivelymergedbythesameautoencodernetworkintoaxedlengthsentencerepresentation.Thevectorsateachnodeareusedasfeaturestopredictadistributionoversenti-mentlabels.2010)thatcancapturesuchphenomenausemanymanuallyconstructedresources(sentimentlexica,parsers,polarity-shiftingrules).Thislimitstheap-plicabilityofthesemethodstoabroaderrangeoftasksandlanguages.Lastly,almostallpreviousworkisbasedonsingle,positive/negativecategoriesorscalessuchasstarratings.Examplesaremoviereviews(PangandLee,2005),opinions(Wiebeetal.,2005),customerreviews(Dingetal.,2008)ormultipleaspectsofrestaurants(SnyderandBarzilay,2007).Suchaone-dimensionalscaledoesnotaccu-ratelyreectthecomplexityofhumanemotionsandsentiments.Inthiswork,weseektoaddressthreeissues.(i)Insteadofusingabag-of-wordsrepresentation,ourmodelexploitshierarchicalstructureandusescom-positionalsemanticstounderstandsentiment.(ii)Oursystemcanbetrainedbothonunlabeleddo-maindataandonsupervisedsentimentdataanddoesnotrequireanylanguage-specicsentimentlexica,152

parsers,etc.(iii)Ratherthanlimitingsentimenttoapositive/negativescale,wepredictamultidimen-sionaldistributionoverseveralcomplex,intercon-nectedsentiments.Weintroduceanapproachbasedonsemi-supervised,recursiveautoencoders(RAE)whichuseasinputcontinuouswordvectors.Fig.1showsanillustrationofthemodelwhichlearnsvectorrep-resentationsofphrasesandfullsentencesaswellastheirhierarchicalstructurefromunsupervisedtext.Weextendourmodeltoalsolearnadistributionoversentimentlabelsateachnodeofthehierarchy.Weevaluateourapproachonseveralstandarddatasetswhereweachievestate-of-theartperfor-mance.Furthermore,weshowresultsonthere-centlyintroducedexperienceproject(EP)dataset(Potts,2010)thatcapturesabroaderspectrumofhumansentimentsandemotions.Thedatasetcon-sistsofverypersonalconfessionsanonymouslymadebypeopleontheexperienceprojectwebsitewww.experienceproject.com.Confessionsarela-beledwithasetofvereactionsbyotherusers.Re-actionlabelsareyourock(expressingapprovement),tehee(amusement),Iunderstand,Sorry,hugsandWow,justwow(displayingshock).Forevaluationonthisdatasetwepredictboththelabelwiththemostvotesaswellasthefulldistributionoverthesenti-mentcategories.Onbothtasksourmodeloutper-formscompetitivebaselines.Asetofover31,000confessionsaswellasthecodeofourmodelareavailableatwww.socher.org.Afterdescribingthemodelindetail,weevalu-ateitqualitativelybyanalyzingthelearnedn-gramvectorrepresentationsandcomparequantitativelyagainstothermethodsonstandarddatasetsandtheEPdataset.2Semi-SupervisedRecursiveAutoencodersOurmodelaimstondvectorrepresentationsforvariable-sizedphrasesineitherunsupervisedorsemi-supervisedtrainingregimes.Theserepresenta-tionscanthenbeusedforsubsequenttasks.Werstdescribeneuralwordrepresentationsandthenpro-ceedtoreviewarelatedrecursivemodelbasedonautoencoders,introduceourrecursiveautoencoder(RAE)anddescribehowitcanbemodiedtojointlylearnphraserepresentations,phrasestructureandsentimentdistributions.2.1NeuralWordRepresentationsWerepresentwordsascontinuousvectorsofparam-eters.Weexploretwosettings.IntherstsettingwesimplyinitializeeachwordvectorxRnbysamplingitfromazeromeanGaussiandistribution:xN(0,2).ThesewordvectorsarethenstackedintoawordembeddingmatrixLRn|V|,where|V|isthesizeofthevocabulary.Thisinitializationworkswellinsupervisedsettingswhereanetworkcansubsequentlymodifythesevectorstocapturecertainlabeldistributions.Inthesecondsetting,wepre-trainthewordvec-torswithanunsupervisedneurallanguagemodel(Bengioetal.,2003;CollobertandWeston,2008).Thesemodelsjointlylearnanembeddingofwordsintoavectorspaceandusethesevectorstopredicthowlikelyawordoccursgivenitscontext.Afterlearningviagradientascentthewordvectorscap-turesyntacticandsemanticinformationfromtheirco-occurrencestatistics.InbothcaseswecanusetheresultingmatrixofwordvectorsLforsubsequenttasksasfollows.As-sumewearegivenasentenceasanorderedlistofmwords.Eachwordhasanassociatedvocabularyindexkintotheembeddingmatrixwhichweusetoretrievethewordsvectorrepresentation.Mathemat-ically,thislook-upoperationcanbeseenasasim-pleprojectionlayerwhereweuseabinaryvectorbwhichiszeroinallpositionsexceptatthekthindex,xi=LbkRn.(1)Intheremainderofthispaper,werepresentasen-tence(oranyn-gram)asanorderedlistofthesevectors(x1,...,xm).Thiswordrepresentationisbettersuitedtoautoencodersthanthebinarynumberrepresentationsusedinpreviousrelatedautoencodermodelssuchastherecursiveautoassociativemem-ory(RAAM)model(Pollack,1990;VoegtlinandDominey,2005)orrecurrentneuralnetworks(El-man,1991)sincesigmoidunitsareinherentlycon-tinuous.Pollackcircumventedthisproblembyhav-ingvocabularieswithonlyahandfulofwordsandbymanuallydeningathresholdtobinarizethere-sultingvectors.153

x1x3x4x2y1=f(W(1)[x3;x4] + b)y2=f(W(1)[x2;y1] + b)y3=f(W(1)[x1;y2] + b)Figure2:Illustrationofanapplicationofarecursiveau-toencodertoabinarytree.Thenodeswhicharenotlledareonlyusedtocomputereconstructionerrors.Astan-dardautoencoder(inbox)isre-usedateachnodeofthetree.2.2TraditionalRecursiveAutoencodersThegoalofautoencodersistolearnarepresentationoftheirinputs.Inthissectionwedescribehowtoobtainareduceddimensionalvectorrepresentationforsentences.Inthepastautoencodershaveonlybeenusedinsettingwherethetreestructurewasgivena-priori.Wereviewthissettingbeforecontinuingwithourmodelwhichdoesnotrequireagiventreestructure.Fig.2showsaninstanceofarecursiveautoencoder(RAE)appliedtoagiventree.Assumewearegivenalistofwordvectorsx=(x1,...,xm)asdescribedintheprevioussectionaswellasabinarytreestruc-tureforthisinputintheformofbranchingtripletsofparentswithchildren:(pc1c2).Eachchildcanbeeitheraninputwordvectorxioranontermi-nalnodeinthetree.FortheexampleinFig.2,wehavethefollowingtriplets:((y1x3x4),(y2x2y1),(y1x1y2)).Inordertobeabletoapplythesameneuralnetworktoeachpairofchildren,thehiddenrepresentationsyihavetohavethesamedi-mensionalityasthexis.Giventhistreestructure,wecannowcomputetheparentrepresentations.Therstparentvectory1iscomputedfromthechildren(c1,c2)=(x3,x4):p=f(W(1)[c1;c2]+b(1)),(2)wherewemultipliedamatrixofparametersW(1)Rn2nbytheconcatenationofthetwochildren.Afteraddingabiastermweappliedanelement-wiseactivationfunctionsuchastanhtotheresult-ingvector.Onewayofassessinghowwellthisn-dimensionalvectorrepresentsitschildrenistotrytoreconstructthechildreninareconstructionlayer:(cid:2)c01;c02(cid:3)=W(2)p+b(2).(3)Duringtraining,thegoalistominimizetherecon-structionerrorsofthisinputpair.Foreachpair,wecomputetheEuclideandistancebetweentheoriginalinputanditsreconstruction:Erec([c1;c2])=12(cid:12)(cid:12)(cid:12)(cid:12)[c1;c2](cid:2)c01;c02(cid:3)(cid:12)(cid:12)(cid:12)(cid:12)2.(4)ThismodelofastandardautoencoderisboxedinFig.2.Nowthatwehavedenedhowanautoen-codercanbeusedtocomputeann-dimensionalvec-torrepresentation(p)oftwon-dimensionalchildren(c1,c2),wecandescribehowsuchanetworkcanbeusedfortherestofthetree.Essentially,thesamestepsrepeat.Nowthaty1isgiven,wecanuseEq.2tocomputey2bysettingthechildrentobe(c1,c2)=(x2,y1).Again,aftercomputingtheintermediateparentvectory2,wecanassesshowwellthisvectorcapturethecontentofthechildrenbycomputingthereconstructionerrorasinEq.4.Theprocessrepeatuntilthefulltreeisconstructedandwehaveareconstructionerrorateachnonterminalnode.ThismodelissimilartotheRAAMmodel(Pollack,1990)whichalsorequiresaxedtreestructure.2.3UnsupervisedRecursiveAutoencoderforStructurePredictionNow,assumethereisnotreestructuregivenfortheinputvectorsinx.Thegoalofourstructure-predictionRAEistominimizethereconstructioner-rorofallvectorpairsofchildreninatree.Wede-neA(x)asthesetofallpossibletreesthatcanbebuiltfromaninputsentencex.Further,letT(y)beafunctionthatreturnsthetripletsofatreeindexedbysofallthenon-terminalnodesinatree.UsingthereconstructionerrorofEq.4,wecomputeRAE(x)=argminyA(x)XsT(y)Erec([c1;c2]s)(5)Wenowdescribeagreedyapproximationthatcon-structssuchatree.154

GreedyUnsupervisedRAE.Forasentencewithmwords,weapplytheautoencoderrecursively.Ittakestherstpairofneighboringvectors,denesthemaspotentialchildrenofaphrase(c1;c2)=(x1;x2),concatenatesthemandgivesthemasin-puttotheautoencoder.Foreachwordpair,wesavethepotentialparentnodepandtheresultingrecon-structionerror.Aftercomputingthescorefortherstpair,thenetworkisshiftedbyonepositionandtakesasinputvectors(c1,c2)=(x2,x3)andagaincomputesapo-tentialparentnodeandascore.Thisprocessrepeatsuntilithitsthelastpairofwordsinthesentence:(c1,c2)=(xm1,xm).Next,itselectsthepairwhichhadthelowestreconstructionerror(Erec)anditsparentrepresentationpwillrepresentthisphraseandreplacebothchildreninthesentencewordlist.Forinstance,considerthesequence(x1,x2,x3,x4)andassumethelowestErecwasobtainedbythepair(x3,x4).Aftertherstpass,thenewsequencethenconsistsof(x1,x2,p(3,4)).Theprocessrepeatsandtreatsthenewvectorp(3,4)likeanyotherinputvec-tor.Forinstance,subsequentstatescouldbeeither:(x1,p(2,(3,4)))or(p(1,2),p(3,4)).Bothstateswouldthennishwithadeterministicchoiceofcollapsingtheremainingtwostatesintooneparenttoobtain(p(1,(2,(3,4))))or(p((1,2),(3,4)))respectively.Thetreeisthenrecoveredbyunfoldingthecollapsingdeci-sions.Theresultingtreestructurecapturesasmuchofthesingle-wordinformationaspossible(inordertoallowreconstructingthewordvectors)butdoesnotnecessarilyfollowstandardsyntacticconstraints.Wealsoexperimentedwithamethodthatndsbet-tersolutionstoEq.5basedonCKY-likebeamsearchalgorithms(Socheretal.,2010;Socheretal.,2011)buttheperformanceissimilarandthegreedyversionismuchfaster.WeightedReconstruction.Oneproblemwithsimplyusingthereconstructionerrorofbothchil-drenequallyasdescribeinEq.4isthateachchildcouldrepresentadifferentnumberofpreviouslycollapsedwordsandishenceofbiggerimportancefortheoverallmeaningreconstructionofthesen-tence.Forinstanceinthecaseof(x1,p(2,(3,4)))onewouldliketogivemoreimportancetorecon-structingpthanx1.Wecapturethisdesideratumbyadjustingthereconstructionerror.Letn1,n2bethenumberofwordsunderneathacurrentpoten-tialchild,were-denethereconstructionerrortobeErec([c1;c2];)=n1n1+n2(cid:12)(cid:12)(cid:12)(cid:12)c1c01(cid:12)(cid:12)(cid:12)(cid:12)2+n2n1+n2(cid:12)(cid:12)(cid:12)(cid:12)c2c02(cid:12)(cid:12)(cid:12)(cid:12)2(6)LengthNormalization.OneofthegoalsofRAEsistoinducesemanticvectorrepresentationsthatallowustocomparen-gramsofdifferentlengths.TheRAEtriestolowerreconstructionerrorofnotonlythebigramsbutalsoofnodeshigherinthetree.Unfortunately,sincetheRAEcomputesthehiddenrepresentationsitthentriestoreconstruct,itcanjustlowerreconstructionerrorbymakingthehiddenlayerverysmallinmagnitude.Topreventsuchundesirablebehavior,wemodifythehiddenlayersuchthattheresultingparentrepresentational-wayshaslengthone,aftercomputingpasinEq.2,wesimplyset:p=p||p||.2.4Semi-SupervisedRecursiveAutoencodersSofar,theRAEwascompletelyunsupervisedandinducedgeneralrepresentationsthatcapturethese-manticsofmulti-wordphrases.Inthissection,weextendRAEstoasemi-supervisedsettinginordertopredictasentence-orphrase-leveltargetdistribu-tiont.1OneofthemainadvantagesoftheRAEisthateachnodeofthetreebuiltbytheRAEhasassoci-atedwithitadistributedvectorrepresentation(theparentvectorp)whichcouldalsobeseenasfea-turesdescribingthatphrase.Wecanleveragethisrepresentationbyaddingontopofeachparentnodeasimplesoftmaxlayertopredictclassdistributions:d(p;)=softmax(Wlabelp).(7)AssumingthereareKlabels,dRKisaK-dimensionalmultinomialdistributionandPk=1dk=1.Fig.3showssuchasemi-supervisedRAEunit.Lettkbethekthelementofthemultino-mialtargetlabeldistributiontforoneentry.Thesoftmaxlayersoutputsareinterpretedascondi-tionalprobabilitiesdk=p(k|[c1;c2]),hencethecross-entropyerrorisEcE(p,t;)=KXk=1tklogdk(p;).(8)1Forthebinarylabelclassicationcase,thedistributionisoftheform[1,0]forclass1and[0,1]forclass2.155

Reconstruction error           Cross-entropy errorW(1)W(2)W(label)Figure3:IllustrationofanRAEunitatanonterminaltreenode.Rednodesshowthesupervisedsoftmaxlayerforlabeldistributionprediction.Usingthiscross-entropyerrorforthelabelandthereconstructionerrorfromEq.6,thenalsemi-supervisedRAEobjectiveover(sentences,label)pairs(x,t)inacorpusbecomesJ=1NX(x,t)E(x,t;)+2||||2,(9)wherewehaveanerrorforeachentryinthetrainingsetthatisthesumovertheerroratthenodesofthetreethatisconstructedbythegreedyRAE:E(x,t;)=XsT(RAE(x))E([c1;c2]s,ps,t,).Theerrorateachnonterminalnodeistheweightedsumofreconstructionandcross-entropyerrors,E([c1;c2]s,ps,t,)=Erec([c1;c2]s;)+(1)EcE(ps,t;).Thehyperparameterweighsreconstructionandcross-entropyerror.Whenminimizingthecross-entropyerrorofthissoftmaxlayer,theerrorwillbackpropagateandinuenceboththeRAEparam-etersandthewordrepresentations.Initially,wordssuchasgoodandbadhaveverysimilarrepresenta-tions.ThisisalsothecaseforBrownclustersandothermethodsthatuseonlycooccurrencestatisticsinasmallwindowaroundeachword.Whenlearn-ingwithpositive/negativesentiment,thewordem-beddingsgetmodiedandcapturelesssyntacticandmoresentimentinformation.Inordertopredictthesentimentdistributionofasentencewiththismodel,weusethelearnedvectorrepresentationofthetoptreenodeandtrainasimplelogisticregressionclassier.3LearningLet=(W(1),b(1),W(2),b(1),Wlabel,L)bethesetofourmodelparameters,thenthegradientbecomes:J=1NX(x,t)E(x,t;)+.(10)Tocomputethisgradient,werstgreedilyconstructalltreesandthenderivativesforthesetreesarecom-putedefcientlyviabackpropagationthroughstruc-ture(GollerandKuchler,1996).Becausethealgo-rithmisgreedyandthederivativesofthesupervisedcross-entropyerroralsomodifythematrixW(1),thisobjectiveisnotnecessarilycontinuousandastepinthegradientdescentdirectionmaynotnec-essarilydecreasetheobjective.However,wefoundthatL-BFGSrunoverthecompletetrainingdata(batchmode)tominimizetheobjectiveworkswellinpractice,andthatconvergenceissmooth,withthealgorithmtypicallyndingagoodsolutionquickly.4ExperimentsWerstdescribethenewexperienceproject(EP)dataset,resultsofstandardclassicationtasksonthisdatasetandhowtopredictitssentimentlabeldistributions.Wethenshowresultsonothercom-monlyuseddatasetsandconcludewithananalysisoftheimportantparametersofthemodel.Inallexperimentsinvolvingourmodel,werepre-sentwordsusing100-dimensionalwordvectors.WeexplorethetwosettingsmentionedinSec.2.1.Wecompareperformanceonstandarddatasetswhenus-ingrandomlyinitializedwordvectors(randomwordinit.)orwordvectorstrainedbythemodelofCol-lobertandWeston(2008)andprovidedbyTurianetal.(2010).2ThesevectorsweretrainedonanunlabeledcorpusoftheEnglishWikipedia.NotethatalternativessuchasBrownclustersarenotsuit-ablesincetheydonotcapturesentimentinformation(goodandbadareusuallyinthesamecluster)andcannotbemodiedviabackpropagation.2http://metaoptimize.com/projects/wordreprs/156

CorpusKInstancesDistr.(+/-)Avg|W|MPQA210,6240.31/0.693MR210,6620.5/0.522EP531,675.2/.2/.1/.4/.1113EP456,129.2/.2/.1/.4/.1129Table1:Statisticsonthedifferentdatasets.Kisthenum-berofclasses.Distr.isthedistributionofthedifferentclasses(inthecaseof2,thepositive/negativeclasses,forEPtheroundeddistributionoftotalvotesineachclass).|W|istheaveragenumberofwordsperinstance.WeuseEP4,asubsetofentrieswithatleast4votes.4.1EPDataset:TheExperienceProjectTheconfessionssectionoftheexperienceprojectwebsite3letspeopleanonymouslywriteshortper-sonalstoriesorconfessions.Onceastoryisonthesite,eachusercangiveasinglevotetooneofvelabelcategories(withourinterpretation):1Sorry,Hugs:Userofferscondolencestoauthor.2.YouRock:Indicatingapproval,congratulations.3.Teehee:Userfoundtheanecdoteamusing.4.IUnderstand:Showofempathy.5.Wow,JustWow:Expressionofsurprise,shock.TheEPdatasethas31,676confessionentries,ato-talnumberof74,859votesforthe5labelsabove,theaveragenumberofvotesperentryis2.4(withavari-anceof33).Forthevecategories,thenumbersofvotesare[14,816;13,325;10,073;30,844;5,801].Sinceanentrywithlessthan4votesisnotverywellidentied,wetrainandtestonlyonentrieswithatleast4totalvotes.Thereare6,129totalsuchentries.Thedistributionovertotalvotesinthe5classesissimilar:[0.22;0.2;0.11;0.37;0.1].Theaveragelengthofentriesis129words.Someentriescon-tainmultiplesentences.Inthesecases,weaveragethepredictedlabeldistributionsfromthesentences.Table1showsstatisticsofthisandothercommonlyusedsentimentdatasets(whichwecompareoninlaterexperiments).Table2showsexampleentriesaswellasgoldandpredictedlabeldistributionsasdescribedinthenextsections.Comparedtootherdatasets,theEPdatasetcon-tainsawiderrangeofhumanemotionsthatgoesfarbeyondpositive/negativeproductormoviereviews.Eachitemislabeledwithamultinomialdistribu-3http://www.experienceproject.com/confessions.phptionoverinterconnectedresponsecategories.Thisisincontrasttomostotherdatasets(includingmulti-aspectrating)whereseveraldistinctaspectsareratedindependentlybutonthesamescale.Thetopicsrangefromgenerichappystatements,dailyclumsi-nessreports,love,loneliness,torelationshipabuseandsuicidalnotes.Asisevidentfromthetotalnum-beroflabelvotes,themostcommonuserreactionisoneofempathyandanabilitytorelatetotheau-thorsexperience.However,somestoriesdescribehorriblescenariosthatarenotcommonandhencereceivemoreoffersofcondolence.Inthefollowingsectionsweshowsomeexamplesofstorieswithpre-dictedandtruedistributionsbutrefrainfromlistingthemosthorribleexperiences.ForallexperimentsontheEPdataset,wesplitthedataintotrain(49%),development(21%)andtestdata(30%).4.2EP:PredictingtheLabelwithMostVotesThersttaskforourevaluationontheEPdatasetistosimplypredictthesingleclassthatreceivesthemostvotes.Inordertocompareournoveljointphraserepresentationandclassierlearningframe-worktotraditionalmethods,weusethefollowingbaselines:RandomSincethereareveclasses,thisgives20%accuracy.MostFrequentSelectingtheclasswhichmostfre-quentlyhasthemostvotes(theclassIunder-stand).Baseline1:BinaryBoWThisbaselineuseslogis-ticregressiononbinarybag-of-wordrepresen-tationsthatare1ifawordispresentand0oth-erwise.Baseline2:FeaturesThismodelissimilartotra-ditionalapproachestosentimentclassicationinthatitusesmanyhand-engineeredresources.Werstusedaspell-checkerandWordnettomapwordsandtheirmisspellingstosynsetstoreducethetotalnumberofwords.Wethenre-placedsentimentwordswithasentimentcat-egoryidentierusingthesentimentlexicaoftheHarvardInquirer(Stone,1966)andLIWC(Pennebakeretal.,2007).Lastly,weusedtf-idfweightingonthebag-of-wordrepresentationsandtrainedanSVM.157

KLPredicted&GoldV.Entry(Shortenedifitendswith...).03.16.16.16.33.166Ireguarlyshoplift.Igotcaughtonceandwenttojail,butIvefoundthatthiswasnotadeterrent.Idontbuygroceries,Idontbuyschoolsuppliesformykids,Idontbuygiftsformykids,wedontpayformovies,andIdontbuymostincidentalsforthehouse(cleaningsupplies,toothpaste,etc.)....03.38.04.06.35.14165iamaverysuccesfullbuissnesman.imakegoodmoneybutihavebeenaddictedtocrackfor13years.imoved1hourawayfrommydealers10yearsagotostopusingnowidontusedailybutonceaweekusallyfridaynights.iusedtouse1or2hundredadaynowiuse4or5hundredonafriday.myproblemisiamafuncationaladdict....05.14.28.14.28.147Hithere,Imaguythatlovesagirl,thesameoldbloodystory...Imetherawhileago,whilestudying,sheIssoperfect,somatureandyetsolonely,Igettoknowherandshegetaholdofme,byopeningherlifetomeandsodidIwithher,shehasbeentherstperson,maleorfemalethathasevermadethatbondwithme,....07.27.18.00.45.0911bekissingyourightnow.ishouldbewrappedinyourarmsinthedark,butinsteadiveruinedeverything.ivepiledbrickstomakeawallwheretherenevershouldhavebeenone.ifeelanachethatishouldntfeelbecauseiveneverhadyoucloseenough.wevenevertouched,butistillfeelasthoughapartofmeismissing.....0523DearLove,IjustwanttosaythatIamlookingforyou.TonightIfelttheurgetowrite,andIambecomingmoreandmorefrustratedthatIhavenotfoundyouyet.Imalsotiredofspendingsomuchheartonanolddream.....055IwishIknewsomonetotalktohere..0624IlovedherbutIscreweditup.Nowshesmovedon.Illneverhaveheragain.IdontknowifIlleverstopthinkingabouther..065iam13yearsoldandihatemyfatherheisalwasgetingdrunkanddosnotcareabouthowitaffectsmeormysistersiwanttocarebutthetruthisidontcareifhedies.136wellithinkhairywomenareattractive.355AssoonasIputclothingsonIwillgodowntoDQandgetathinmintblizzard.Ineedit.Itllmakemysoulfeelabitbetter:).366Iama45yearolddivocedwoman,andIhaventbeenonadateorhadanysignicantrelationshipin12years...yes,12yrs.thesadthingis,Imnotsomedriedupoldgrannywhoisnolongerinterestedinmen,Ijustcantmeetmen.(beforeyoujudge,noImnotterriblypicky!)Whatiswrongwithme?.636Wheniwasinkindergardeniusedtolockmyselfintheclosetandeatallthecandy.Thentheteacherfoundoutitwasoneofusandmadeusgotwodayswithoutfreetime.Itmightbealittlelatenow,butsorryguysitwasmehaha.924Mypaperisdueinlessthan24hoursandImstilldancingroundmyroom!Table2:ExampleEPconfessionsfromthetestdatawithKLdivergencebetweenourpredicteddistribution(lightblue,leftbaroneachofthe5classes)andgroundtruthdistribution(redbarandnumbersunderneath),numberofvotes.The5classesare[Sorry,Hugs;YouRock;Teehee;IUnderstand;Wow,JustWow].EvenwhentheKLdivergenceishigher,ourmodelmakesreasonablealternativelabelchoices.Someentriesareshortened.Baseline3:WordVectorsWecanignoretheRAEtreestructureandonlytrainsoftmaxlayersdi-rectlyonthepre-trainedwordsinordertoinu-encethewordvectors.ThisisfollowedbyanSVMtrainedontheaverageofthewordvec-tors.WealsoexperimentedwithlatentDirichletalloca-tion(Bleietal.,2003)butperformancewasverylow.Table3showstheresultsforpredictingtheclasswiththemostvotes.Eventheapproachthatisbasedonsentimentlexicaandotherresourcesisoutper-formedbyourmodelbyalmost3%,showingthatfortasksinvolvingcomplexbroad-rangehumansen-timent,theoftenusedsentimentlexicalackincover-ageandtraditionalbag-of-wordsrepresentationsarenotpowerfulenough.4.3EP:PredictingSentimentDistributionsWenowturntoevaluatingourdistribution-predictionapproach.InboththisandthepreviousMethodAccuracyRandom20.0MostFrequent38.1Baseline1:BinaryBoW46.4Baseline2:Features47.0Baseline3:WordVectors45.5RAE(ourmethod)50.1Table3:Accuracyofpredictingtheclasswithmostvotes.maximumlabeltask,webackpropusingthegoldmultinomialdistributionasatarget.Sincewemax-imizelikelihoodandbecausewewanttopredictadistributionthatisclosesttothedistributionoflabelsthatpeoplewouldassigntoastory,weevaluateus-ingKLdivergence:KL(g||p)=Pigilog(gi/pi),wheregisthegolddistributionandpisthepredictedone.WereporttheaverageKLdivergence,whereasmallervalueindicatesbetterpredictivepower.TogetanideaofthevaluesofKLdivergence,predict-158

Avg.Distr.BoWFeaturesWord Vec.RAE0.60.70.80.830.810.720.730.70Figure4:AverageKL-divergencebetweengoldandpre-dictedsentimentdistributions(lowerisbetter).ingrandomdistributionsgivesaanaverageof1.2inKLdivergence,predictingsimplytheaveragedistri-butioninthetrainingdatagive0.83.Fig.4showsthatourRAE-basedmodeloutperformstheotherbaselines.Table2showsEPexampleentrieswithpredictedandgolddistributions,aswellasnumbersofvotes.4.4BinaryPolarityClassicationInordertocompareourapproachtoothermeth-odswealsoshowresultsoncommonlyusedsen-timentdatasets:moviereviews4(MR)(PangandLee,2005)andopinions5(MPQA)(Wiebeetal.,2005).WegivestatisticalinformationontheseandtheEPcorpusinTable1.Wecomparetothestate-of-the-artsystemof(Nakagawaetal.,2010),adependencytreebasedclassicationmethodthatusesCRFswithhiddenvariables.Weusethesametrainingandtestingregi-men(10-foldcrossvalidation)aswellastheirbase-lines:majorityphrasevotingusingsentimentandreversallexica;rule-basedreversalusingadepen-dencytree;Bag-of-FeaturesandtheirfullTree-CRFmodel.AsshowninTable4,ouralgorithmoutper-formstheirapproachonbothdatasets.Forthemoviereview(MR)dataset,wedonotuseanyhand-designedlexica.AnerroranalysisontheMPQAdatasetshowedseveralcasesofsinglewordswhichneveroccurredinthetrainingset.Correctlyclassify-ingtheseinstancescanonlybetheresultofhavingthemintheoriginalsentimentlexicon.Hence,fortheexperimentonMPQAweaddedthesamesen-timentlexiconthat(Nakagawaetal.,2010)usedintheirsystemtoourtrainingset.Thisimprovedac-curacyfrom86.0to86.4.Usingthepre-trainedwordvectorsboostsperformancebylessthan1%com-4www.cs.cornell.edu/people/pabo/movie-review-data/5www.cs.pitt.edu/mpqa/MethodMRMPQAVotingwithtwolexica63.181.7Rule-basedreversalontrees62.982.8Bagoffeatureswithreversal76.484.1Tree-CRF(Nakagawaetal,10)77.386.1RAE(randomwordinit.)76.885.7RAE(ourmethod)77.786.4Table4:Accuracyofsentimentclassicationonmoviereviewpolarity(MR)andtheMPQAdataset.00.20.40.60.810.830.840.850.860.87Figure5:AccuracyonthedevelopmentsplitoftheMRpolaritydatasetfordifferentweightingsofreconstructionerrorandsupervisedcross-entropyerror:err=Erec+(1)EcE.paredtorandomlyinitializedwordvectors(setting:randomwordinit).Thisshowsthatourmethodcanworkwelleveninsettingswithlittletrainingdata.Wevisualizethesemanticvectorsthattherecursiveautoencoderlearnsbylistingn-gramsthatgivethehighestprobabilityforeachpolarity.Table5showssuchn-gramsfordifferentlengthswhentheRAEistrainedonthemoviereviewpolaritydataset.Ona4-coremachine,trainingtimeforthesmallercorporasuchasthemoviereviewstakesaround3hoursandforthelargerEPcorpusaround12hoursuntilconvergence.Testingofhundredsofmoviere-viewstakesonlyafewseconds.4.5Reconstructionvs.ClassicationErrorInthisexperiment,weshowhowthehyperparame-terinuencesaccuracyonthedevelopmentsetofoneofthecross-validationsplitsoftheMRdataset.Thisparameteressentiallytrade-offthesupervisedandunsupervisedpartsoftheobjective.Fig.5showsthatalargerfocusonthesupervisedobjectiveisim-portantbutthataweightof=0.2fortherecon-structionerrorpreventsoverttingandachievesthehighestperformance.159

