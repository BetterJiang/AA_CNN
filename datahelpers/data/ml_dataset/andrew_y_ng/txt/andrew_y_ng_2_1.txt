151

Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151–161,

Edinburgh, Scotland, UK, July 27–31, 2011. c(cid:13)2011 Association for Computational Linguistics

Semi-SupervisedRecursiveAutoencodersforPredictingSentimentDistributionsRichardSocherJeffreyPennington∗EricH.HuangAndrewY.NgChristopherD.ManningComputerScienceDepartment,StanfordUniversity,Stanford,CA94305,USA∗SLACNationalAcceleratorLaboratory,StanfordUniversity,Stanford,CA94309,USArichard@socher.org{jpennin,ehhuang,ang,manning}@stanford.eduang@cs.stanford.eduAbstractWeintroduceanovelmachinelearningframe-workbasedonrecursiveautoencodersforsentence-levelpredictionofsentimentlabeldistributions.Ourmethodlearnsvectorspacerepresentationsformulti-wordphrases.Insentimentpredictiontaskstheserepresen-tationsoutperformotherstate-of-the-artap-proachesoncommonlyuseddatasets,suchasmoviereviews,withoutusinganypre-deﬁnedsentimentlexicaorpolarityshiftingrules.Wealsoevaluatethemodel’sabilitytopredictsentimentdistributionsonanewdatasetbasedonconfessionsfromtheexperienceproject.Thedatasetconsistsofpersonaluserstoriesannotatedwithmultiplelabelswhich,whenaggregated,formamultinomialdistributionthatcapturesemotionalreactions.Oural-gorithmcanmoreaccuratelypredictdistri-butionsoversuchlabelscomparedtoseveralcompetitivebaselines.1IntroductionTheabilitytoidentifysentimentsaboutpersonalex-periences,products,moviesetc.iscrucialtoun-derstandusergeneratedcontentinsocialnetworks,blogsorproductreviews.Detectingsentimentinthesedataisachallengingtaskwhichhasrecentlyspawnedalotofinterest(PangandLee,2008).Currentbaselinemethodsoftenusebag-of-wordsrepresentationswhichcannotproperlycapturemorecomplexlinguisticphenomenainsentimentanaly-sis(Pangetal.,2002).Forinstance,whilethetwophrases“whitebloodcellsdestroyinganinfection”and“aninfectiondestroyingwhitebloodcells”havethesamebag-of-wordsrepresentation,theformerisapositivereactionwhilethelaterisverynegative.Moreadvancedmethodssuchas(Nakagawaetal.,IndicesWordsSemantic RepresentationsRecursive Autoencoderi         walked      into         a        parked     carSorry, Hugs      You Rock       Teehee    I Understand    Wow, Just WowPredicted Sentiment DistributionFigure1:Illustrationofourrecursiveautoencoderarchi-tecturewhichlearnssemanticvectorrepresentationsofphrases.Wordindices(orange)areﬁrstmappedintoasemanticvectorspace(blue).Thentheyarerecursivelymergedbythesameautoencodernetworkintoaﬁxedlengthsentencerepresentation.Thevectorsateachnodeareusedasfeaturestopredictadistributionoversenti-mentlabels.2010)thatcancapturesuchphenomenausemanymanuallyconstructedresources(sentimentlexica,parsers,polarity-shiftingrules).Thislimitstheap-plicabilityofthesemethodstoabroaderrangeoftasksandlanguages.Lastly,almostallpreviousworkisbasedonsingle,positive/negativecategoriesorscalessuchasstarratings.Examplesaremoviereviews(PangandLee,2005),opinions(Wiebeetal.,2005),customerreviews(Dingetal.,2008)ormultipleaspectsofrestaurants(SnyderandBarzilay,2007).Suchaone-dimensionalscaledoesnotaccu-ratelyreﬂectthecomplexityofhumanemotionsandsentiments.Inthiswork,weseektoaddressthreeissues.(i)Insteadofusingabag-of-wordsrepresentation,ourmodelexploitshierarchicalstructureandusescom-positionalsemanticstounderstandsentiment.(ii)Oursystemcanbetrainedbothonunlabeleddo-maindataandonsupervisedsentimentdataanddoesnotrequireanylanguage-speciﬁcsentimentlexica,152

parsers,etc.(iii)Ratherthanlimitingsentimenttoapositive/negativescale,wepredictamultidimen-sionaldistributionoverseveralcomplex,intercon-nectedsentiments.Weintroduceanapproachbasedonsemi-supervised,recursiveautoencoders(RAE)whichuseasinputcontinuouswordvectors.Fig.1showsanillustrationofthemodelwhichlearnsvectorrep-resentationsofphrasesandfullsentencesaswellastheirhierarchicalstructurefromunsupervisedtext.Weextendourmodeltoalsolearnadistributionoversentimentlabelsateachnodeofthehierarchy.Weevaluateourapproachonseveralstandarddatasetswhereweachievestate-of-theartperfor-mance.Furthermore,weshowresultsonthere-centlyintroducedexperienceproject(EP)dataset(Potts,2010)thatcapturesabroaderspectrumofhumansentimentsandemotions.Thedatasetcon-sistsofverypersonalconfessionsanonymouslymadebypeopleontheexperienceprojectwebsitewww.experienceproject.com.Confessionsarela-beledwithasetofﬁvereactionsbyotherusers.Re-actionlabelsareyourock(expressingapprovement),tehee(amusement),Iunderstand,Sorry,hugsandWow,justwow(displayingshock).Forevaluationonthisdatasetwepredictboththelabelwiththemostvotesaswellasthefulldistributionoverthesenti-mentcategories.Onbothtasksourmodeloutper-formscompetitivebaselines.Asetofover31,000confessionsaswellasthecodeofourmodelareavailableatwww.socher.org.Afterdescribingthemodelindetail,weevalu-ateitqualitativelybyanalyzingthelearnedn-gramvectorrepresentationsandcomparequantitativelyagainstothermethodsonstandarddatasetsandtheEPdataset.2Semi-SupervisedRecursiveAutoencodersOurmodelaimstoﬁndvectorrepresentationsforvariable-sizedphrasesineitherunsupervisedorsemi-supervisedtrainingregimes.Theserepresenta-tionscanthenbeusedforsubsequenttasks.Weﬁrstdescribeneuralwordrepresentationsandthenpro-ceedtoreviewarelatedrecursivemodelbasedonautoencoders,introduceourrecursiveautoencoder(RAE)anddescribehowitcanbemodiﬁedtojointlylearnphraserepresentations,phrasestructureandsentimentdistributions.2.1NeuralWordRepresentationsWerepresentwordsascontinuousvectorsofparam-eters.Weexploretwosettings.Intheﬁrstsettingwesimplyinitializeeachwordvectorx∈RnbysamplingitfromazeromeanGaussiandistribution:x∼N(0,σ2).ThesewordvectorsarethenstackedintoawordembeddingmatrixL∈Rn×|V|,where|V|isthesizeofthevocabulary.Thisinitializationworkswellinsupervisedsettingswhereanetworkcansubsequentlymodifythesevectorstocapturecertainlabeldistributions.Inthesecondsetting,wepre-trainthewordvec-torswithanunsupervisedneurallanguagemodel(Bengioetal.,2003;CollobertandWeston,2008).Thesemodelsjointlylearnanembeddingofwordsintoavectorspaceandusethesevectorstopredicthowlikelyawordoccursgivenitscontext.Afterlearningviagradientascentthewordvectorscap-turesyntacticandsemanticinformationfromtheirco-occurrencestatistics.InbothcaseswecanusetheresultingmatrixofwordvectorsLforsubsequenttasksasfollows.As-sumewearegivenasentenceasanorderedlistofmwords.Eachwordhasanassociatedvocabularyindexkintotheembeddingmatrixwhichweusetoretrievetheword’svectorrepresentation.Mathemat-ically,thislook-upoperationcanbeseenasasim-pleprojectionlayerwhereweuseabinaryvectorbwhichiszeroinallpositionsexceptatthekthindex,xi=Lbk∈Rn.(1)Intheremainderofthispaper,werepresentasen-tence(oranyn-gram)asanorderedlistofthesevectors(x1,...,xm).Thiswordrepresentationisbettersuitedtoautoencodersthanthebinarynumberrepresentationsusedinpreviousrelatedautoencodermodelssuchastherecursiveautoassociativemem-ory(RAAM)model(Pollack,1990;VoegtlinandDominey,2005)orrecurrentneuralnetworks(El-man,1991)sincesigmoidunitsareinherentlycon-tinuous.Pollackcircumventedthisproblembyhav-ingvocabularieswithonlyahandfulofwordsandbymanuallydeﬁningathresholdtobinarizethere-sultingvectors.153

x1x3x4x2y1=f(W(1)[x3;x4] + b)y2=f(W(1)[x2;y1] + b)y3=f(W(1)[x1;y2] + b)Figure2:Illustrationofanapplicationofarecursiveau-toencodertoabinarytree.Thenodeswhicharenotﬁlledareonlyusedtocomputereconstructionerrors.Astan-dardautoencoder(inbox)isre-usedateachnodeofthetree.2.2TraditionalRecursiveAutoencodersThegoalofautoencodersistolearnarepresentationoftheirinputs.Inthissectionwedescribehowtoobtainareduceddimensionalvectorrepresentationforsentences.Inthepastautoencodershaveonlybeenusedinsettingwherethetreestructurewasgivena-priori.Wereviewthissettingbeforecontinuingwithourmodelwhichdoesnotrequireagiventreestructure.Fig.2showsaninstanceofarecursiveautoencoder(RAE)appliedtoagiventree.Assumewearegivenalistofwordvectorsx=(x1,...,xm)asdescribedintheprevioussectionaswellasabinarytreestruc-tureforthisinputintheformofbranchingtripletsofparentswithchildren:(p→c1c2).Eachchildcanbeeitheraninputwordvectorxioranontermi-nalnodeinthetree.FortheexampleinFig.2,wehavethefollowingtriplets:((y1→x3x4),(y2→x2y1),(y1→x1y2)).Inordertobeabletoapplythesameneuralnetworktoeachpairofchildren,thehiddenrepresentationsyihavetohavethesamedi-mensionalityasthexi’s.Giventhistreestructure,wecannowcomputetheparentrepresentations.Theﬁrstparentvectory1iscomputedfromthechildren(c1,c2)=(x3,x4):p=f(W(1)[c1;c2]+b(1)),(2)wherewemultipliedamatrixofparametersW(1)∈Rn×2nbytheconcatenationofthetwochildren.Afteraddingabiastermweappliedanelement-wiseactivationfunctionsuchastanhtotheresult-ingvector.Onewayofassessinghowwellthisn-dimensionalvectorrepresentsitschildrenistotrytoreconstructthechildreninareconstructionlayer:(cid:2)c01;c02(cid:3)=W(2)p+b(2).(3)Duringtraining,thegoalistominimizetherecon-structionerrorsofthisinputpair.Foreachpair,wecomputetheEuclideandistancebetweentheoriginalinputanditsreconstruction:Erec([c1;c2])=12(cid:12)(cid:12)(cid:12)(cid:12)[c1;c2]−(cid:2)c01;c02(cid:3)(cid:12)(cid:12)(cid:12)(cid:12)2.(4)ThismodelofastandardautoencoderisboxedinFig.2.Nowthatwehavedeﬁnedhowanautoen-codercanbeusedtocomputeann-dimensionalvec-torrepresentation(p)oftwon-dimensionalchildren(c1,c2),wecandescribehowsuchanetworkcanbeusedfortherestofthetree.Essentially,thesamestepsrepeat.Nowthaty1isgiven,wecanuseEq.2tocomputey2bysettingthechildrentobe(c1,c2)=(x2,y1).Again,aftercomputingtheintermediateparentvectory2,wecanassesshowwellthisvectorcapturethecontentofthechildrenbycomputingthereconstructionerrorasinEq.4.Theprocessrepeatuntilthefulltreeisconstructedandwehaveareconstructionerrorateachnonterminalnode.ThismodelissimilartotheRAAMmodel(Pollack,1990)whichalsorequiresaﬁxedtreestructure.2.3UnsupervisedRecursiveAutoencoderforStructurePredictionNow,assumethereisnotreestructuregivenfortheinputvectorsinx.Thegoalofourstructure-predictionRAEistominimizethereconstructioner-rorofallvectorpairsofchildreninatree.Wede-ﬁneA(x)asthesetofallpossibletreesthatcanbebuiltfromaninputsentencex.Further,letT(y)beafunctionthatreturnsthetripletsofatreeindexedbysofallthenon-terminalnodesinatree.UsingthereconstructionerrorofEq.4,wecomputeRAEθ(x)=argminy∈A(x)Xs∈T(y)Erec([c1;c2]s)(5)Wenowdescribeagreedyapproximationthatcon-structssuchatree.154

GreedyUnsupervisedRAE.Forasentencewithmwords,weapplytheautoencoderrecursively.Ittakestheﬁrstpairofneighboringvectors,deﬁnesthemaspotentialchildrenofaphrase(c1;c2)=(x1;x2),concatenatesthemandgivesthemasin-puttotheautoencoder.Foreachwordpair,wesavethepotentialparentnodepandtheresultingrecon-structionerror.Aftercomputingthescorefortheﬁrstpair,thenetworkisshiftedbyonepositionandtakesasinputvectors(c1,c2)=(x2,x3)andagaincomputesapo-tentialparentnodeandascore.Thisprocessrepeatsuntilithitsthelastpairofwordsinthesentence:(c1,c2)=(xm−1,xm).Next,itselectsthepairwhichhadthelowestreconstructionerror(Erec)anditsparentrepresentationpwillrepresentthisphraseandreplacebothchildreninthesentencewordlist.Forinstance,considerthesequence(x1,x2,x3,x4)andassumethelowestErecwasobtainedbythepair(x3,x4).Aftertheﬁrstpass,thenewsequencethenconsistsof(x1,x2,p(3,4)).Theprocessrepeatsandtreatsthenewvectorp(3,4)likeanyotherinputvec-tor.Forinstance,subsequentstatescouldbeeither:(x1,p(2,(3,4)))or(p(1,2),p(3,4)).Bothstateswouldthenﬁnishwithadeterministicchoiceofcollapsingtheremainingtwostatesintooneparenttoobtain(p(1,(2,(3,4))))or(p((1,2),(3,4)))respectively.Thetreeisthenrecoveredbyunfoldingthecollapsingdeci-sions.Theresultingtreestructurecapturesasmuchofthesingle-wordinformationaspossible(inordertoallowreconstructingthewordvectors)butdoesnotnecessarilyfollowstandardsyntacticconstraints.Wealsoexperimentedwithamethodthatﬁndsbet-tersolutionstoEq.5basedonCKY-likebeamsearchalgorithms(Socheretal.,2010;Socheretal.,2011)buttheperformanceissimilarandthegreedyversionismuchfaster.WeightedReconstruction.Oneproblemwithsimplyusingthereconstructionerrorofbothchil-drenequallyasdescribeinEq.4isthateachchildcouldrepresentadifferentnumberofpreviouslycollapsedwordsandishenceofbiggerimportancefortheoverallmeaningreconstructionofthesen-tence.Forinstanceinthecaseof(x1,p(2,(3,4)))onewouldliketogivemoreimportancetorecon-structingpthanx1.Wecapturethisdesideratumbyadjustingthereconstructionerror.Letn1,n2bethenumberofwordsunderneathacurrentpoten-tialchild,were-deﬁnethereconstructionerrortobeErec([c1;c2];θ)=n1n1+n2(cid:12)(cid:12)(cid:12)(cid:12)c1−c01(cid:12)(cid:12)(cid:12)(cid:12)2+n2n1+n2(cid:12)(cid:12)(cid:12)(cid:12)c2−c02(cid:12)(cid:12)(cid:12)(cid:12)2(6)LengthNormalization.OneofthegoalsofRAEsistoinducesemanticvectorrepresentationsthatallowustocomparen-gramsofdifferentlengths.TheRAEtriestolowerreconstructionerrorofnotonlythebigramsbutalsoofnodeshigherinthetree.Unfortunately,sincetheRAEcomputesthehiddenrepresentationsitthentriestoreconstruct,itcanjustlowerreconstructionerrorbymakingthehiddenlayerverysmallinmagnitude.Topreventsuchundesirablebehavior,wemodifythehiddenlayersuchthattheresultingparentrepresentational-wayshaslengthone,aftercomputingpasinEq.2,wesimplyset:p=p||p||.2.4Semi-SupervisedRecursiveAutoencodersSofar,theRAEwascompletelyunsupervisedandinducedgeneralrepresentationsthatcapturethese-manticsofmulti-wordphrases.Inthissection,weextendRAEstoasemi-supervisedsettinginordertopredictasentence-orphrase-leveltargetdistribu-tiont.1OneofthemainadvantagesoftheRAEisthateachnodeofthetreebuiltbytheRAEhasassoci-atedwithitadistributedvectorrepresentation(theparentvectorp)whichcouldalsobeseenasfea-turesdescribingthatphrase.Wecanleveragethisrepresentationbyaddingontopofeachparentnodeasimplesoftmaxlayertopredictclassdistributions:d(p;θ)=softmax(Wlabelp).(7)AssumingthereareKlabels,d∈RKisaK-dimensionalmultinomialdistributionandPk=1dk=1.Fig.3showssuchasemi-supervisedRAEunit.Lettkbethekthelementofthemultino-mialtargetlabeldistributiontforoneentry.Thesoftmaxlayer’soutputsareinterpretedascondi-tionalprobabilitiesdk=p(k|[c1;c2]),hencethecross-entropyerrorisEcE(p,t;θ)=−KXk=1tklogdk(p;θ).(8)1Forthebinarylabelclassiﬁcationcase,thedistributionisoftheform[1,0]forclass1and[0,1]forclass2.155

Reconstruction error           Cross-entropy errorW(1)W(2)W(label)Figure3:IllustrationofanRAEunitatanonterminaltreenode.Rednodesshowthesupervisedsoftmaxlayerforlabeldistributionprediction.Usingthiscross-entropyerrorforthelabelandthereconstructionerrorfromEq.6,theﬁnalsemi-supervisedRAEobjectiveover(sentences,label)pairs(x,t)inacorpusbecomesJ=1NX(x,t)E(x,t;θ)+λ2||θ||2,(9)wherewehaveanerrorforeachentryinthetrainingsetthatisthesumovertheerroratthenodesofthetreethatisconstructedbythegreedyRAE:E(x,t;θ)=Xs∈T(RAEθ(x))E([c1;c2]s,ps,t,θ).Theerrorateachnonterminalnodeistheweightedsumofreconstructionandcross-entropyerrors,E([c1;c2]s,ps,t,θ)=αErec([c1;c2]s;θ)+(1−α)EcE(ps,t;θ).Thehyperparameterαweighsreconstructionandcross-entropyerror.Whenminimizingthecross-entropyerrorofthissoftmaxlayer,theerrorwillbackpropagateandinﬂuenceboththeRAEparam-etersandthewordrepresentations.Initially,wordssuchasgoodandbadhaveverysimilarrepresenta-tions.ThisisalsothecaseforBrownclustersandothermethodsthatuseonlycooccurrencestatisticsinasmallwindowaroundeachword.Whenlearn-ingwithpositive/negativesentiment,thewordem-beddingsgetmodiﬁedandcapturelesssyntacticandmoresentimentinformation.Inordertopredictthesentimentdistributionofasentencewiththismodel,weusethelearnedvectorrepresentationofthetoptreenodeandtrainasimplelogisticregressionclassiﬁer.3LearningLetθ=(W(1),b(1),W(2),b(1),Wlabel,L)bethesetofourmodelparameters,thenthegradientbecomes:∂J∂θ=1NX(x,t)∂E(x,t;θ)∂θ+λθ.(10)Tocomputethisgradient,weﬁrstgreedilyconstructalltreesandthenderivativesforthesetreesarecom-putedefﬁcientlyviabackpropagationthroughstruc-ture(GollerandK¨uchler,1996).Becausethealgo-rithmisgreedyandthederivativesofthesupervisedcross-entropyerroralsomodifythematrixW(1),thisobjectiveisnotnecessarilycontinuousandastepinthegradientdescentdirectionmaynotnec-essarilydecreasetheobjective.However,wefoundthatL-BFGSrunoverthecompletetrainingdata(batchmode)tominimizetheobjectiveworkswellinpractice,andthatconvergenceissmooth,withthealgorithmtypicallyﬁndingagoodsolutionquickly.4ExperimentsWeﬁrstdescribethenewexperienceproject(EP)dataset,resultsofstandardclassiﬁcationtasksonthisdatasetandhowtopredictitssentimentlabeldistributions.Wethenshowresultsonothercom-monlyuseddatasetsandconcludewithananalysisoftheimportantparametersofthemodel.Inallexperimentsinvolvingourmodel,werepre-sentwordsusing100-dimensionalwordvectors.WeexplorethetwosettingsmentionedinSec.2.1.Wecompareperformanceonstandarddatasetswhenus-ingrandomlyinitializedwordvectors(randomwordinit.)orwordvectorstrainedbythemodelofCol-lobertandWeston(2008)andprovidedbyTurianetal.(2010).2ThesevectorsweretrainedonanunlabeledcorpusoftheEnglishWikipedia.NotethatalternativessuchasBrownclustersarenotsuit-ablesincetheydonotcapturesentimentinformation(goodandbadareusuallyinthesamecluster)andcannotbemodiﬁedviabackpropagation.2http://metaoptimize.com/projects/wordreprs/156

CorpusKInstancesDistr.(+/-)Avg|W|MPQA210,6240.31/0.693MR210,6620.5/0.522EP531,675.2/.2/.1/.4/.1113EP≥456,129.2/.2/.1/.4/.1129Table1:Statisticsonthedifferentdatasets.Kisthenum-berofclasses.Distr.isthedistributionofthedifferentclasses(inthecaseof2,thepositive/negativeclasses,forEPtheroundeddistributionoftotalvotesineachclass).|W|istheaveragenumberofwordsperinstance.WeuseEP≥4,asubsetofentrieswithatleast4votes.4.1EPDataset:TheExperienceProjectTheconfessionssectionoftheexperienceprojectwebsite3letspeopleanonymouslywriteshortper-sonalstoriesor“confessions”.Onceastoryisonthesite,eachusercangiveasinglevotetooneofﬁvelabelcategories(withourinterpretation):1Sorry,Hugs:Userofferscondolencestoauthor.2.YouRock:Indicatingapproval,congratulations.3.Teehee:Userfoundtheanecdoteamusing.4.IUnderstand:Showofempathy.5.Wow,JustWow:Expressionofsurprise,shock.TheEPdatasethas31,676confessionentries,ato-talnumberof74,859votesforthe5labelsabove,theaveragenumberofvotesperentryis2.4(withavari-anceof33).Fortheﬁvecategories,thenumbersofvotesare[14,816;13,325;10,073;30,844;5,801].Sinceanentrywithlessthan4votesisnotverywellidentiﬁed,wetrainandtestonlyonentrieswithatleast4totalvotes.Thereare6,129totalsuchentries.Thedistributionovertotalvotesinthe5classesissimilar:[0.22;0.2;0.11;0.37;0.1].Theaveragelengthofentriesis129words.Someentriescon-tainmultiplesentences.Inthesecases,weaveragethepredictedlabeldistributionsfromthesentences.Table1showsstatisticsofthisandothercommonlyusedsentimentdatasets(whichwecompareoninlaterexperiments).Table2showsexampleentriesaswellasgoldandpredictedlabeldistributionsasdescribedinthenextsections.Comparedtootherdatasets,theEPdatasetcon-tainsawiderrangeofhumanemotionsthatgoesfarbeyondpositive/negativeproductormoviereviews.Eachitemislabeledwithamultinomialdistribu-3http://www.experienceproject.com/confessions.phptionoverinterconnectedresponsecategories.Thisisincontrasttomostotherdatasets(includingmulti-aspectrating)whereseveraldistinctaspectsareratedindependentlybutonthesamescale.Thetopicsrangefromgenerichappystatements,dailyclumsi-nessreports,love,loneliness,torelationshipabuseandsuicidalnotes.Asisevidentfromthetotalnum-beroflabelvotes,themostcommonuserreactionisoneofempathyandanabilitytorelatetotheau-thorsexperience.However,somestoriesdescribehorriblescenariosthatarenotcommonandhencereceivemoreoffersofcondolence.Inthefollowingsectionsweshowsomeexamplesofstorieswithpre-dictedandtruedistributionsbutrefrainfromlistingthemosthorribleexperiences.ForallexperimentsontheEPdataset,wesplitthedataintotrain(49%),development(21%)andtestdata(30%).4.2EP:PredictingtheLabelwithMostVotesTheﬁrsttaskforourevaluationontheEPdatasetistosimplypredictthesingleclassthatreceivesthemostvotes.Inordertocompareournoveljointphraserepresentationandclassiﬁerlearningframe-worktotraditionalmethods,weusethefollowingbaselines:RandomSincethereareﬁveclasses,thisgives20%accuracy.MostFrequentSelectingtheclasswhichmostfre-quentlyhasthemostvotes(theclassIunder-stand).Baseline1:BinaryBoWThisbaselineuseslogis-ticregressiononbinarybag-of-wordrepresen-tationsthatare1ifawordispresentand0oth-erwise.Baseline2:FeaturesThismodelissimilartotra-ditionalapproachestosentimentclassiﬁcationinthatitusesmanyhand-engineeredresources.Weﬁrstusedaspell-checkerandWordnettomapwordsandtheirmisspellingstosynsetstoreducethetotalnumberofwords.Wethenre-placedsentimentwordswithasentimentcat-egoryidentiﬁerusingthesentimentlexicaoftheHarvardInquirer(Stone,1966)andLIWC(Pennebakeretal.,2007).Lastly,weusedtf-idfweightingonthebag-of-wordrepresentationsandtrainedanSVM.157

KLPredicted&GoldV.Entry(Shortenedifitendswith...).03.16.16.16.33.166Ireguarlyshoplift.Igotcaughtonceandwenttojail,butI’vefoundthatthiswasnotadeterrent.Idon’tbuygroceries,Idon’tbuyschoolsuppliesformykids,Idon’tbuygiftsformykids,wedon’tpayformovies,andIdontbuymostincidentalsforthehouse(cleaningsupplies,toothpaste,etc.)....03.38.04.06.35.14165iamaverysuccesfullbuissnesman.imakegoodmoneybutihavebeenaddictedtocrackfor13years.imoved1hourawayfrommydealers10yearsagotostopusingnowidontusedailybutonceaweekusallyfridaynights.iusedtouse1or2hundredadaynowiuse4or5hundredonafriday.myproblemisiamafuncationaladdict....05.14.28.14.28.147Hithere,Imaguythatlovesagirl,thesameoldbloodystory...Imetherawhileago,whilestudying,sheIssoperfect,somatureandyetsolonely,Igettoknowherandshegetaholdofme,byopeningherlifetomeandsodidIwithher,shehasbeentheﬁrstperson,maleorfemalethathasevermadethatbondwithme,....07.27.18.00.45.0911bekissingyourightnow.ishouldbewrappedinyourarmsinthedark,butinsteadi’veruinedeverything.i’vepiledbrickstomakeawallwheretherenevershouldhavebeenone.ifeelanachethatishouldn’tfeelbecausei’veneverhadyoucloseenough.we’venevertouched,butistillfeelasthoughapartofmeismissing.....0523DearLove,IjustwanttosaythatIamlookingforyou.TonightIfelttheurgetowrite,andIambecomingmoreandmorefrustratedthatIhavenotfoundyouyet.I’malsotiredofspendingsomuchheartonanolddream.....055IwishIknewsomonetotalktohere..0624IlovedherbutIscreweditup.Nowshe’smovedon.I’llneverhaveheragain.Idon’tknowifI’lleverstopthinkingabouther..065iam13yearsoldandihatemyfatherheisalwasgetingdrunkanddo’snotcareabouthowitaffectsmeormysistersiwanttocarebutthetruthisidontcareifhedies.136wellithinkhairywomenareattractive.355AssoonasIputclothingsonIwillgodowntoDQandgetathinmintblizzard.Ineedit.It’llmakemysoulfeelabitbetter:).366Iama45yearolddivocedwoman,andIhaventbeenonadateorhadanysigniﬁcantrelationshipin12years...yes,12yrs.thesadthingis,Imnotsomedriedupoldgrannywhoisnolongerinterestedinmen,Ijustcan’tmeetmen.(beforeyoujudge,noImnotterriblypicky!)Whatiswrongwithme?.636Wheniwasinkindergardeniusedtolockmyselfintheclosetandeatallthecandy.Thentheteacherfoundoutitwasoneofusandmadeusgotwodayswithoutfreetime.Itmightbealittlelatenow,butsorryguysitwasmehaha.924Mypaperisdueinlessthan24hoursandI’mstilldancingroundmyroom!Table2:ExampleEPconfessionsfromthetestdatawithKLdivergencebetweenourpredicteddistribution(lightblue,leftbaroneachofthe5classes)andgroundtruthdistribution(redbarandnumbersunderneath),numberofvotes.The5classesare[Sorry,Hugs;YouRock;Teehee;IUnderstand;Wow,JustWow].EvenwhentheKLdivergenceishigher,ourmodelmakesreasonablealternativelabelchoices.Someentriesareshortened.Baseline3:WordVectorsWecanignoretheRAEtreestructureandonlytrainsoftmaxlayersdi-rectlyonthepre-trainedwordsinordertoinﬂu-encethewordvectors.ThisisfollowedbyanSVMtrainedontheaverageofthewordvec-tors.WealsoexperimentedwithlatentDirichletalloca-tion(Bleietal.,2003)butperformancewasverylow.Table3showstheresultsforpredictingtheclasswiththemostvotes.Eventheapproachthatisbasedonsentimentlexicaandotherresourcesisoutper-formedbyourmodelbyalmost3%,showingthatfortasksinvolvingcomplexbroad-rangehumansen-timent,theoftenusedsentimentlexicalackincover-ageandtraditionalbag-of-wordsrepresentationsarenotpowerfulenough.4.3EP:PredictingSentimentDistributionsWenowturntoevaluatingourdistribution-predictionapproach.InboththisandthepreviousMethodAccuracyRandom20.0MostFrequent38.1Baseline1:BinaryBoW46.4Baseline2:Features47.0Baseline3:WordVectors45.5RAE(ourmethod)50.1Table3:Accuracyofpredictingtheclasswithmostvotes.maximumlabeltask,webackpropusingthegoldmultinomialdistributionasatarget.Sincewemax-imizelikelihoodandbecausewewanttopredictadistributionthatisclosesttothedistributionoflabelsthatpeoplewouldassigntoastory,weevaluateus-ingKLdivergence:KL(g||p)=Pigilog(gi/pi),wheregisthegolddistributionandpisthepredictedone.WereporttheaverageKLdivergence,whereasmallervalueindicatesbetterpredictivepower.TogetanideaofthevaluesofKLdivergence,predict-158

Avg.Distr.BoWFeaturesWord Vec.RAE0.60.70.80.830.810.720.730.70Figure4:AverageKL-divergencebetweengoldandpre-dictedsentimentdistributions(lowerisbetter).ingrandomdistributionsgivesaanaverageof1.2inKLdivergence,predictingsimplytheaveragedistri-butioninthetrainingdatagive0.83.Fig.4showsthatourRAE-basedmodeloutperformstheotherbaselines.Table2showsEPexampleentrieswithpredictedandgolddistributions,aswellasnumbersofvotes.4.4BinaryPolarityClassiﬁcationInordertocompareourapproachtoothermeth-odswealsoshowresultsoncommonlyusedsen-timentdatasets:moviereviews4(MR)(PangandLee,2005)andopinions5(MPQA)(Wiebeetal.,2005).WegivestatisticalinformationontheseandtheEPcorpusinTable1.Wecomparetothestate-of-the-artsystemof(Nakagawaetal.,2010),adependencytreebasedclassiﬁcationmethodthatusesCRFswithhiddenvariables.Weusethesametrainingandtestingregi-men(10-foldcrossvalidation)aswellastheirbase-lines:majorityphrasevotingusingsentimentandreversallexica;rule-basedreversalusingadepen-dencytree;Bag-of-FeaturesandtheirfullTree-CRFmodel.AsshowninTable4,ouralgorithmoutper-formstheirapproachonbothdatasets.Forthemoviereview(MR)dataset,wedonotuseanyhand-designedlexica.AnerroranalysisontheMPQAdatasetshowedseveralcasesofsinglewordswhichneveroccurredinthetrainingset.Correctlyclassify-ingtheseinstancescanonlybetheresultofhavingthemintheoriginalsentimentlexicon.Hence,fortheexperimentonMPQAweaddedthesamesen-timentlexiconthat(Nakagawaetal.,2010)usedintheirsystemtoourtrainingset.Thisimprovedac-curacyfrom86.0to86.4.Usingthepre-trainedwordvectorsboostsperformancebylessthan1%com-4www.cs.cornell.edu/people/pabo/movie-review-data/5www.cs.pitt.edu/mpqa/MethodMRMPQAVotingwithtwolexica63.181.7Rule-basedreversalontrees62.982.8Bagoffeatureswithreversal76.484.1Tree-CRF(Nakagawaetal,’10)77.386.1RAE(randomwordinit.)76.885.7RAE(ourmethod)77.786.4Table4:Accuracyofsentimentclassiﬁcationonmoviereviewpolarity(MR)andtheMPQAdataset.00.20.40.60.810.830.840.850.860.87Figure5:AccuracyonthedevelopmentsplitoftheMRpolaritydatasetfordifferentweightingsofreconstructionerrorandsupervisedcross-entropyerror:err=αErec+(1−α)EcE.paredtorandomlyinitializedwordvectors(setting:randomwordinit).Thisshowsthatourmethodcanworkwelleveninsettingswithlittletrainingdata.Wevisualizethesemanticvectorsthattherecursiveautoencoderlearnsbylistingn-gramsthatgivethehighestprobabilityforeachpolarity.Table5showssuchn-gramsfordifferentlengthswhentheRAEistrainedonthemoviereviewpolaritydataset.Ona4-coremachine,trainingtimeforthesmallercorporasuchasthemoviereviewstakesaround3hoursandforthelargerEPcorpusaround12hoursuntilconvergence.Testingofhundredsofmoviere-viewstakesonlyafewseconds.4.5Reconstructionvs.ClassiﬁcationErrorInthisexperiment,weshowhowthehyperparame-terαinﬂuencesaccuracyonthedevelopmentsetofoneofthecross-validationsplitsoftheMRdataset.Thisparameteressentiallytrade-offthesupervisedandunsupervisedpartsoftheobjective.Fig.5showsthatalargerfocusonthesupervisedobjectiveisim-portantbutthataweightofα=0.2fortherecon-structionerrorpreventsoverﬁttingandachievesthehighestperformance.159

nMostnegativen-gramsMostpositiven-grams1bad;boring;dull;ﬂat;pointless;tv;neither;pretentious;badly;worst;lame;mediocre;lack;routine;loud;bore;barely;stupid;tired;poorly;suffers;heavy;nor;choppy;superﬁcialtouching;enjoyable;powerful;warm;moving;culture;ﬂaws;provides;engrossing;wonderful;beautiful;quiet;socio-political;thoughtful;portrait;refreshingly;chilling;rich;beautifully;solid;2howbad;bybad;dull.;forbad;tobad;boring.;,dull;arebad;thatbad;boring,;,ﬂat;pointless.;badlyby;ontv;soroutine;lackthe;mediocre.;ageneric;stupid,;abysmallypatheticthebeautiful;moving,;thoughtfuland;,inventive;solidand;abeautiful;abeautifully;andhilarious;withdazzling;providesthe;provides.;andinventive;aspowerful;movingand;amoving;apowerful3.toobad;exactlyhowbad;andneverdull;shotbutdull;ismoreboring;tothedull;dull,UNK;itisbad;orjustplain;byturnspretentious;manipulativeandcontrived;bagofstale;isabad;thewholemildly;contrivedpasticheof;fromthischoppy;stalemate-rial.funnyandtouching;asmallgem;withamoving;cuts,fast;,ﬁnemusic;smartandtaut;cultureintoa;romantic,riveting;...asolid;beautifullyacted.;,graduallyreveals;withthechilling;castofsolid;hasasolid;spareyetaudacious;...apolished;boththebeauty;5boringthananythingelse.;amajorwaste...generic;nothingihadn’talready;,UNKplotting;superﬁcial;problem?nolaughs.;,justhorriblymediocre.;dull,UNKfeel.;there’snothingexactlywrong;movieisaboutaboring;essentiallyacollectionofbitsremindedusthatafeel-good;engrossing,seldomUNK,;betweenrealisticcharactersshowinghonest;asolidpieceofjournalistic;easilythemostthoughtfulﬁctional;cute,funny,heartwarming;withwryhumorandgenuine;engrossingandultimatelytragic.;8loud,silly,stupidandpointless.;dull,dumbandderivativehorrorﬁlm.;UNK’sﬁlm,aboring,pretentious;thisﬁlmbiggestproblem?nolaughs.;ﬁlmintheserieslooksandfeelstired;dodraweasychucklesbutleadnowhere.;stupid,infantile,redundant,sloppyshotinrich,shadowyblack-and-white,devilsanescapistcon-fectionthat’spureentertainment.;,deeplyabsorbingpiecethatworksasa;...oneofthemostingeniousandentertaining;ﬁlmisariveting,briskdelight.;bringingrichermeaningtothestory’s;Table5:Examplesofn-grams(n=1,2,3,5,8)fromthetestdataofthemoviepolaritydatasetforwhichourmodelpredictsthemostpositiveandmostnegativeresponses.5RelatedWork5.1AutoencodersandDeepLearningAutoencodersareneuralnetworksthatlearnare-duceddimensionalrepresentationofﬁxed-sizein-putssuchasimagepatchesorbag-of-wordrepre-sentationsoftextdocuments.Theycanbeusedtoefﬁcientlylearnfeatureencodingswhichareusefulforclassiﬁcation.Recently,Mirowskietal.(2010)learndynamicautoencodersfordocumentsinabag-of-wordsformatwhich,likeours,combinesuper-visedandreconstructionobjectives.TheideaofapplyinganautoencoderinarecursivesettingwasintroducedbyPollack(1990).Pollack’srecursiveauto-associativememories(RAAMs)aresimilartooursinthattheyareaconnectionst,feed-forwardmodel.However,RAAMslearnvectorrepresentationsonlyforﬁxedrecursivedatastruc-tures,whereasourRAEbuildsthisrecursivedatastructure.Morerecently,(VoegtlinandDominey,2005)introducedalinearmodiﬁcationtoRAAMsthatisabletobettergeneralizetonovelcombina-tionsofpreviouslyseenconstituents.Oneofthemajorshortcomingsofpreviousapplicationsofre-cursiveautoencoderstonaturallanguagesentenceswastheirbinarywordrepresentationasdiscussedinSec.2.1.Recently,(Socheretal.,2010;Socheretal.,2011)introducedamax-marginframeworkbasedonrecur-siveneuralnetworks(RNNs)forlabeledstructureprediction.Theirmodelsareapplicabletonaturallanguageandcomputervisiontaskssuchasparsingorobjectdetection.Thecurrentworkisrelatedinthatitusesarecursivedeeplearningmodel.How-ever,RNNsrequirelabeledtreestructuresanduseasupervisedscoreateachnode.Instead,RAEslearnhierarchicalstructuresthataretryingtocaptureasmuchofthetheoriginalwordvectorsaspossible.Thelearnedstructuresarenotnecessarilysyntacti-callyplausiblebutcancapturemoreofthesemanticcontentofthewordvectors.Otherrecentdeeplearn-ingmethodsforsentimentanalysisinclude(Maasetal.,2011).5.2SentimentAnalysisPangetal.(2002)wereoneoftheﬁrsttoexperimentwithsentimentclassiﬁcation.Theyshowthatsim-plebag-of-wordsapproachesbasedonNaiveBayes,MaxEntmodelsorSVMsareofteninsufﬁcientforpredictingsentimentofdocumentseventhoughtheyworkwellforgeneraltopic-baseddocumentclassi-ﬁcation.Evenaddingspeciﬁcnegationwords,bi-gramsorpart-of-speechinformationtothesemod-elsdidnotaddsigniﬁcantimprovements.Otherdocument-levelsentimentworkincludes(Turney,2002;Daveetal.,2003;Beinekeetal.,2004;PangandLee,2004).Forfurtherreferences,see(PangandLee,2008).Insteadofdocumentlevelsentimentclassiﬁca-tion,(Wilsonetal.,2005)analyzethecontextualpolarityofphrasesandincorporatemanywellde-signedfeaturesincludingdependencytrees.Theyalsoshowimprovementsbyﬁrstdistinguishingbe-160

tweenneutralandpolarsentences.Ourmodelnatu-rallyincorporatestherecursiveinteractionbetweencontextandpolaritywordsinsentencesinauniﬁedframeworkwhilesimultaneouslylearningtheneces-saryfeaturestomakeaccuratepredictions.Otherap-proachesforsentence-levelsentimentdetectionin-clude(YuandHatzivassiloglou,2003;Grefenstetteetal.,2004;Ikedaetal.,2008).Mostpreviousworkiscenteredaroundagivensentimentlexiconorbuildingoneviaheuristics(KimandHovy,2007;EsuliandSebastiani,2007),manualannotation(DasandChen,2001)ormachinelearningtechniques(Turney,2002).Incontrast,wedonotrequireaninitialorconstructedsentimentlex-iconofpositiveandnegativewords.Infact,whentrainingourapproachondocumentsorsentences,itjointlylearnssuchlexicaforbothsinglewordsandn-grams(seeTable5).(MaoandLebanon,2007)proposeisotonicconditionalrandomﬁeldsanddif-ferentiatebetweenlocal,sentence-levelandglobal,document-levelsentiment.Theworkof(PolanyiandZaenen,2006;ChoiandCardie,2008)focusesonmanuallyconstructingsev-erallexicaandrulesforbothpolarwordsandre-latedcontent-wordnegators,suchas“preventcan-cer”,wherepreventreversesthenegativepolarityofcancer.Likeourapproachtheycapturecomposi-tionalsemantics.However,ourmodeldoessowith-outmanuallyconstructinganyrulesorlexica.Recently,(Velikovichetal.,2010)showedhowtouseaseedlexiconandagraphpropagationframe-worktolearnalargersentimentlexiconthatalsoin-cludespolarmulti-wordphrasessuchas“onceinalifetime”.Whileourmethodcanalsolearnmulti-wordphrasesitdoesnotrequireaseedsetoralargewebgraph.(Nakagawaetal.,2010)introducedanapproachbasedonCRFswithhiddenvariableswithverygoodperformance.Wecomparetotheirstate-of-the-artsystem.Weoutperformthemonthestan-dardcorporathatwetestedonwithoutrequiringexternalsystemssuchasPOStaggers,dependencyparsersandsentimentlexica.Ourapproachjointlylearnsthenecessaryfeaturesandtreestructure.Inmulti-aspectrating(SnyderandBarzilay,2007)oneﬁndsseveraldistinctaspectssuchasfoodorser-viceinarestaurantandthenratesthemonaﬁxedlinearscalesuchas1-5stars,whereallaspectscouldobtainjust1starorallaspectscouldobtain5starsindependently.Incontrast,inourmethodasingleaspect(acomplexreactiontoahumanexperience)ispredictednotintermsofaﬁxedscalebutintermsofamultinomialdistributionoverseveralintercon-nected,sometimesmutuallyexclusiveemotions.Asinglestorycannotsimultaneouslyobtainastrongreactionindifferentemotionalresponses(byvirtueofhavingtosumtoone).6ConclusionWepresentedanovelalgorithmthatcanaccuratelypredictsentence-levelsentimentdistributions.With-outusinganyhand-engineeredresourcessuchassentimentlexica,parsersorsentimentshiftingrules,ourmodelachievesstate-of-the-artperformanceoncommonlyusedsentimentdatasets.Furthermore,weintroduceanewdatasetthatcontainsdistribu-tionsoverabroadrangeofhumanemotions.Ourevaluationshowsthatourmodelcanmoreaccu-ratelypredictthesedistributionsthanothermodels.AcknowledgmentsWegratefullyacknowledgethesupportoftheDefenseAdvancedResearchProjectsAgency(DARPA)MachineReadingProgramunderAirForceResearchLaboratory(AFRL)primecontractno.FA8750-09-C-0181.Anyopinions,ﬁndings,andconclusionorrecommendationsexpressedinthismaterialarethoseoftheauthor(s)anddonotnecessarilyreﬂecttheviewofDARPA,AFRL,ortheUSgovernment.ThisworkwasalsosupportedinpartbytheDARPADeepLearningprogramundercontractnumberFA8650-10-C-7020.WethankChrisPottsforhelpwiththeEPdataset,Ray-mondHsu,BozhiSee,andAlanWuforlettingususetheirsystemasabaselineandJiquanNgiam,QuocLe,GaborAngeliandAndrewMaasfortheirfeedback.ReferencesP.Beineke,T.Hastie,C.D.Manning,andS.Vaithyanathan.2004.Exploringsentimentsummarization.InProceedingsoftheAAAISpringSymposiumonExploringAttitudeandAffectinText:TheoriesandApplications.Y.Bengio,R.Ducharme,P.Vincent,andC.Janvin.2003.Aneuralprobabilisticlanguagemodel.JournalofMa-chineLearningResearch,3:1137–1155.D.M.Blei,A.Y.Ng,andM.I.Jordan.2003.Latentdirichletallocation.JournalofMachineLearningRe-search.,3:993–1022.161

Y.ChoiandC.Cardie.2008.Learningwithcomposi-tionalsemanticsasstructuralinferenceforsubsenten-tialsentimentanalysis.InEMNLP.R.CollobertandJ.Weston.2008.Auniﬁedarchi-tecturefornaturallanguageprocessing:deepneuralnetworkswithmultitasklearning.InProceedingsofICML,pages160–167.S.DasandM.Chen.2001.Yahoo!forAmazon:Ex-tractingmarketsentimentfromstockmessageboards.InProceedingsoftheAsiaPaciﬁcFinanceAssociationAnnualConference(APFA).K.Dave,S.Lawrence,andD.M.Pennock.2003.Min-ingthepeanutgallery:Opinionextractionandseman-ticclassiﬁcationofproductreviews.InProceedingsofWWW,pages519–528.X.Ding,B.Liu,andP.S.Yu.2008.Aholisticlexicon-basedapproachtoopinionmining.InProceedingsoftheConferenceonWebSearchandWebDataMining(WSDM).J.L.Elman.1991.Distributedrepresentations,simplerecurrentnetworks,andgrammaticalstructure.Ma-chineLearning,7(2-3):195–225.A.EsuliandF.Sebastiani.2007.Pagerankingword-netsynsets:Anapplicationtoopinionmining.InProceedingsoftheAssociationforComputationalLin-guistics(ACL).C.GollerandA.K¨uchler.1996.Learningtask-dependentdistributedrepresentationsbybackpropaga-tionthroughstructure.InProceedingsoftheInterna-tionalConferenceonNeuralNetworks(ICNN-96).G.Grefenstette,Y.Qu,J.G.Shanahan,andD.A.Evans.2004.Couplingnichebrowsersandaffectanalysisforanopinionminingapplication.InProceedingsofRecherched’InformationAssist´eeparOrdinateur(RIAO).D.Ikeda,H.Takamura,L.Ratinov,andM.Okumura.2008.Learningtoshiftthepolarityofwordsforsenti-mentclassiﬁcation.InIJCNLP.S.KimandE.Hovy.2007.Crystal:Analyzingpredic-tiveopinionsontheweb.InEMNLP-CoNLL.A.L.Maas,R.E.Daly,P.T.Pham,D.Huang,A.Y.Ng,andC.Potts.2011.Learningaccurate,compact,andinterpretabletreeannotation.InProceedingsofACL.Y.MaoandG.Lebanon.2007.IsotonicConditionalRandomFieldsandLocalSentimentFlow.InNIPS.P.Mirowski,M.Ranzato,andY.LeCun.2010.Dynamicauto-encodersforsemanticindexing.InProceedingsoftheNIPS2010WorkshoponDeepLearning.T.Nakagawa,K.Inui,andS.Kurohashi.2010.Depen-dencytree-basedsentimentclassiﬁcationusingCRFswithhiddenvariables.InNAACL,HLT.B.PangandL.Lee.2004.Asentimentaleducation:Sentimentanalysisusingsubjectivitysummarizationbasedonminimumcuts.InACL.B.PangandL.Lee.2005.Seeingstars:Exploitingclassrelationshipsforsentimentcategorizationwithrespecttoratingscales.InACL,pages115–124.B.PangandL.Lee.2008.Opinionminingandsenti-mentanalysis.FoundationsandTrendsinInformationRetrieval,2(1-2):1–135.B.Pang,L.Lee,andS.Vaithyanathan.2002.Thumbsup?Sentimentclassiﬁcationusingmachinelearningtechniques.InEMNLP.J.W.Pennebaker,R.J.Booth,andM.E.Francis.2007.Linguisticinquiryandwordcount:Liwc2007opera-torsmanual.UniversityofTexas.L.PolanyiandA.Zaenen.2006.Contextualvalenceshifters.J.B.Pollack.1990.Recursivedistributedrepresenta-tions.ArtiﬁcialIntelligence,46:77–105,November.C.Potts.2010.Onthenegativityofnegation.InDavidLutzandNanLi,editors,ProceedingsofSemanticsandLinguisticTheory20.CLCPublications,Ithaca,NY.B.SnyderandR.Barzilay.2007.Multipleaspectrank-ingusingtheGoodGriefalgorithm.InHLT-NAACL.R.Socher,C.D.Manning,andA.Y.Ng.2010.Learningcontinuousphraserepresentationsandsyntacticpars-ingwithrecursiveneuralnetworks.InProceedingsoftheNIPS-2010DeepLearningandUnsupervisedFea-tureLearningWorkshop.R.Socher,C.C.Lin,A.Y.Ng,andC.D.Manning.2011.ParsingNaturalScenesandNaturalLanguagewithRecursiveNeuralNetworks.InICML.P.J.Stone.1966.TheGeneralInquirer:AComputerApproachtoContentAnalysis.TheMITPress.J.Turian,L.Ratinov,andY.Bengio.2010.Wordrep-resentations:asimpleandgeneralmethodforsemi-supervisedlearning.InProceedingsofACL,pages384–394.P.Turney.2002.Thumbsuporthumbsdown?Seman-ticorientationappliedtounsupervisedclassiﬁcationofreviews.InACL.L.Velikovich,S.Blair-Goldensohn,K.Hannan,andR.McDonald.2010.Theviabilityofweb-derivedpo-laritylexicons.InNAACL,HLT.T.VoegtlinandP.Dominey.2005.LinearRecursiveDis-tributedRepresentations.NeuralNetworks,18(7).J.Wiebe,T.Wilson,andC.Cardie.2005.Annotatingex-pressionsofopinionsandemotionsinlanguage.Lan-guageResourcesandEvaluation,39.T.Wilson,J.Wiebe,andP.Hoffmann.2005.Recogniz-ingcontextualpolarityinphrase-levelsentimentanal-ysis.InHLT/EMNLP.H.YuandV.Hatzivassiloglou.2003.Towardsanswer-ingopinionquestions:Separatingfactsfromopinionsandidentifyingthepolarityofopinionsentences.InEMNLP.