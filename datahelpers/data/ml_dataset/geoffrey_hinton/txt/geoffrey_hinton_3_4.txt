Keeping

Neural

Networks

Simple

by Minimizing

the Description

Length

of

the Weights

Geoffrey

E. Hinton

and Drew

van Camp

Department

of Computer

Science

University
10 King’s College Road

of Toronto

Toronto M5S 1A4, Canada

Abstract

well

generalize

if
in the weights
the train-
vectors of
it
is impor-
by penaliz-
contain.

simple

they

neural

learning,

error of

networks

So during

information

information

the network

the trade-off

of
information

to keep the weights

Supervised
there is much less information
than there is in the output
ing cases.
tant
ing the amount
of
The
amount
be controlled
by adding Gaussian
the noise level can be adapted
to optimize
squared
of
a method
expected
information
work that contains
units. Provided
exact derivatives
without
tions.
information
the weights
number of intereating
weights.

The idea of minimizing
is required

of computing
squared
error

that
of a neural

can be computed

in the weights.

time-consuming

noisy weights

the output

in the

during

in a weight

can
noise and
learning
between the expected
and the amount
We describe
the
of
in a net-
hidden
the

of
the amount

the derivatives
and of

units are linear,

efficiently
Monte Carlo simula-

network

leads
schemes for encoding

to a
the

the amount

of

to communicate

a layer of non-linear

1

Introduction

learning

complicated

tasks there is little

data so any reasonably

the data and give poor generalization

In many practical
training
tend to overfit
new data.
there is less information
the output
of
have considered many possible ways of
formation

available
model will
to
we need to ensure that
than there is in
cases. Researchers
the in-

To avoid overfitting

in the weights:

in the weights

the training

vectors

limiting

l

b

l

the number

Limit
in the network
(and hope that each weight does not have too much
information

of connections

in it).

the connections

Divide
weights within
“weight-sharing”
ural symmetries
(Lang, Waibel

a subset

into subsets,

and force the
this
the nat-
the task it can be very effective

is baaed on an analysis of
of

to be identical.

If

and Hinton

(1990);

LeCun

1989).

all

so that

the weights

in the network

Quantize
probability
tized value. The number
– logp,
quantization,
a difficult
does not have a smooth

a
mass, p, can be assigned to each quan-
is then
the
leads to
search space because the cost of a weight

provided we ignore the cost of defining

of bits in a weight

Unfortunately

this method

derivative.

2

Applying
Length

the Minimum

Description

Principle

it

fitting

in the

Length

to data,

of bits it

data better

architecture,

is not worth

1986) asserts

The Minimum

For supervised

the improvement

is always possible

Description
that

the model and describing

is the one that minimizes

this may make the model worse at

the best model
the combhmd
the misfit

to fit
When fitting models
by using a more complex model,
the training
but
new data.
So we need some way of deciding when extra complex-
ity in the model
data-fit.
(Rissanen,
data
describing
the model and the data.
with a predetermined
number
data-misfit
the discrepancy
put of the neural network
think
vector
only see the input
network,
set of
ceiver. For each training
discrepancy
output.
the net,
output.

Principle
of some
cost of
between
neural networks
is the
and the
takea to describe
and the out-
case. We can
the input
and a receiver who can
first
fits a neural
to the complete

to the re-
case the sender also sends the
and the correct

in terms of a sender who can see both
and the correct

is the number
between the correct output

between
By adding
the receiver

the net’s output
this discrepancy

the model cost
the weights,

The sender
architecture,

then sends the weights

takea to describe

on each training

of pm-arranged

to the output

can generate

the correct

of bits it

training

exactly

vector.

cases,

output

cost

of

the copies

fee

the

all or part
are not made
copyright
ACM

and its date

appear,

of

this material

is

or distributed

notice
and notice

for
and the
is given

otherwise,

of

the Association
or

to republish,

for Computing
requires

a fee

without

commercial

provided

to copy

that
advantage,

the publication

of
copying

Permission
granted
direct
title
that
Machinery.
and/or
permission.
ACM COLT ’93 171931CA,
a 1993 ACM 0-89791-61

is by permission
To copy

specific

USA
‘1-51931000710005

. ..$l

.50

5

of

square deviation
value of uj and summing
term of equation
misfit

cost

is:

the misfits

over all

from zero.1 Using this
caaes the last
and the data

training
a constant

2 becomes

; ~(d;

c

[

-

y;)’

1

(3)

@v

Cdata-misfit

=kN

+

:

log

t

t.

value,

1: This

v, using

a quantized

width
If
distribution,

is much narrower
the probability

Figure
ciated with
tization
Gaussian
approximated
width,
The log t term is a constant,
tion is a zero-mean Gaussian,
is proportional

shows the probability y mass asso-
a quan-
than
the
mass is well
of the height and the
is a sum of two terms.
the distribu-
the height

and if
the log of

so the log probability

by the product

to V2.

where k is a constant

that depends

only on t.

Independently
a predetermined
description
squared error
have made about
justification

length

of

of whether

value for uj,
is minimized

we use the optimal
it
is apparent
by minimizing

value or
that
the
the usual
we

function,

so the Gaussian

assumptions

coding
this error

can be viewed

aa the MDL

function.

3

Coding

the

data

misfits

4

A simple method
weights

of coding

the

scheme for

To apply
coding
Clearly,
amount
shall assume that
intervals
the data misfits
output

units.

of

the MDL

principle

we need to decide

on a

the data misfits

if
of information

the data misfits

and for
are real numbers,

the weights.
an infinite

they are very finely
t. We shall

is needed to convey them. So we
quantized,
using
also assume that
the

for each of

separately

are encoded

fixed width

bits.

of bits,

log2 p(Ay)

distribution

If we want

to minimize

mass, p(Ay),

if a sender and a re-
that as-
distribution
to each possible quan-
using
the expected

the best probability

then we can code the misfit

The coding
theorem tells us that
ceiver have agreed on a probability
signs a probability
tized data misfit, Ay,
-
number
is the correct one, but any other agreed distribution
also be used. For convenience,
output
the data misfits
that
they are drawn
bution with standard
large compared
with
sumed probability
the desired output,
output @ is then we ~ approximated
mass shown in figure

from a zero-mean Gaussian
deviation,
the quantization

of a particular
dc on training

UJ. Provided
width

to use
can
we shall assume that
for
are encoded by assuming
distri-

is
that
t,the ss-
data misfit
between
case c and the actual

by the probability

unit

1.

Cj

j

P(d;–Y;)

t—

=

&Qj

1

‘Xp

[1-(d;-@2

2C3

(1)

Using an optimal
misfit,

dj — y;,

code,

the description

length

in units of

log2 (e) bits (called

– logp(d;

– y;) = –logt+logfi+loguj+

of a data
“nats”
) is:

(d;

- @2
Zuf

“

(2)

To minimize
training

this description

length

cases,

the optimal

value of uj

summed

over all

IV
is the root mean

in just

UW, of

network

distribution.

this distribution

the same way aa we
the weights
of
and come from
de-
the

We could code the weights
code the data misfits. We assume that
are finely quantized
the trained
a zero-mean Gaussian
If
viation,
description
to the sum of their squares. So, assuming we use a Gaus-
sian with standard
the output
errors, we can minimize
the data misfits
of

for encoding
description
and the weights by minimizing

the standard
is fixed in advance,

length
of
the sum

the weights

proportional

two terms:

the total

is simply

deviation

length

of

uj

where c is an index over

training

cases.

the standard

“weight-decay”

is just
that weight-decay

This
fact
1987) can therefore
crude MDL
of the gaussians used for coding the data misfits
weights

generalization
aa a vindication
the standard

improves
be viewed
in which

are both fixed in advance.2

approach

method.

The

(Hinton,

of

this

deviations

and the

weight-decay

of standard

the distribution

An elaboration
that
can be modelled more
of several Gaussians whose means,
ing proportions

are adapted

of weights

accurately

by using
variances
as the network

in the trained

is to assume
network
a mixture
and mix-
is trained

1If

the optimal

value of Uj

is to be used,

municated before the data misfits are sent, so it
coded. However, since it
safe in ignoring
is clear

is only one number we are probably
length.

of the total description

the ratio

of

is only

4 that

‘It

it

it must be com-
too must be

this ~pect
from equation
of
this ratio,

it

the two Gaussians that matters.

Rather

ratio

gives optimal

is usually

better
performance

to estimate

it by

on a validation

the variances
than guessing
seeing which
set.

6

and Hinton,
way of coding

(Nowlan
elaborate
better
a small number

generalization.

1992).

For some tasks this more

the weights

gives considerably

This
of different

is especially
weight

true when only

values are required.

this more elaborate

scheme still

It assumes that

all

to the same tolerance,

and that

used for modelling

with

compared

However,
a serious weakness:
quantized
is small
Gaussians
takes into account
Thus it
weight
(the height
cision (the width).
network
some of
cisely without
the network.

significantly

in figure
This

the standard

the weight
the probability

1) but

it

is a terrible

suffers
the weights

from
are

this tolerance
the

of

deviations

distribution.
density

ignores
waste of bits.
to describe

of a
the pre-
A
if

very impre-
of

the predictions

is clearly much more economical
the weight

values can be described

affecting

the net work has been trained.

The next

haa considered

the
on the outputs

(1992)

in the weights

MacKay
changes
after
describes a method
into account
during
weight
against
sity and the excess data misfit
the weight.

of taking
training

can be traded

effects

of

of small
the network
section
of the weights

the precision
so that
both
caused by imprecision

its probability

the precision

of a
den-
in

After

distribution,

weight.
terior
method
data misfits
ber of bits required
bution
of a weight
(the Kullback-Liebler

learning,
Q,

the sender has a Gaussian
for

We describe

of communicating

the weights
and show that using this method

the weight.
both

pos-
a
and the
the num-
distri-
divergence

to communicate

the posterior

is equal

to the asymmetric

distance)

from P to Q.

G(P, Q) =

/

Q(w)log~dw

(5)

5.2

The

“bits

back”

argument

probability

distribution

the weight

the sender

the posterior

a set of noisy weights,

first
To communicate
collapses
for each
weight by using a source of random bits to pick a precise
some very fine tolerance
value for
t). The probability
each possible
termined
the weight.
cise weights
distribution,
precise weight, w,

value is de-
for
these pre-
them using some prior Gaussian

The sender
by coding
P,

(to within
of picking

then communicates

the communication

by the posterior

distribution

probability

so that

coat of a

is:

5

Noisy

weights

c(w) = – Iogt - log P(w)

(6)

is to add zero-mean Gaussian

the amount

of information
noise. At

in
first
seems to be even more expensive
than a precise one since it appears that
as well as a mean, and that
these. As we

for both of

a noisy weight

A standard way of limiting
a number
sight,
to communicate
we need to send a variance
we need to decide on a precision
shall see, however,
the MDL
to allow very noisy weights
cheaply,

framework
to be communicated

can be adapted
very

-

using

approach

it
in weight
that

backpropagation
is standard

When
neural network,
particular
point
in the direction
ternative
sian distribution
the mean and the variance
tors so as to reduce some cost
ourselves to distributions
pendent,
mean and one variance

so the distribution

over weight

to train

a feed forward

to start

practice

reduces the error

function.
is to start with a multivariate

at some
space and to move this point
An al-
Gaus-
vectors and to change both
vec-
of

this cloud of weight

function. We shall

restrict
in which the weights are ind+
by one

can be represented

per weight.

is the expected

and of

function

The cost
the weights
high-variance
they cause extra variance
ing these misfits more expensive

weights

are cheaper

the data misfits.

It

turns

description

length
out
to communicate

of
that
but
thus mak-

in the data misfits

to communicate.

5.1

expected

The
weights

description

length

of

the

We assume that
agreed Gaussian

the sender
prior

distribution,

and the

receiver
for
P,

have an
a given

be small

compared

t must
C(w)
big refund

is big. However,
the end.

at

with

of P so
as we shall see, we are due for a

the variance

Q,

that

sent

outputs

outputs.

algorithm

probability

the correct

the sender

the data-misfits

the precise weights,

the sender collapsed

else. Once he has the correct

received the weights and the misfits,

learning
the exact same posterior

then com-
achieved using those weights.
the receiver
But he can also
he
was used by the

Having
municates
Having
can then produce
do something
can run whatever
sender and recover
distribution,
the precise weights. 3 Now, since the receiver
sender’s
for each weight
knows the precise value that was communicated,
recover all
lapse that distribution
bits have been successfully
subtract
get
misfits.
the posterior
finely quantized

to get
knows the
and he
he can
the sender used to col-
value. So these random
and we must
cost
to
and the
to collapse

for a weight, Q,
is:

the true cost of communicating

of random bits required

them from the overall

the random bits that

communication

communicated

to a particular

The number

the model

distribution

distribution

value, w,

posterior

in order

to that

R(w) = -

logt

-

log Q(w)

(7)

expected

So the
weight
the distribution

true
is determined
Q :

description

by taking

length

for
an expectation,

a noisy
under

3 If

the

~nder

communicated
is being explained.

~

weights these can be
at a net cost of O bits using the method that

r~dom

initial

7

G(P, Q) = (C(w)

– R(w))

=

/ Q(w)

.

log ~dw
-,.

,

(8)

For Gaussians with
asymmetric

divergence

is

different means and variances,

the

G(P, Q) = log ~

+ *

[~;

-u;

+ (P,

- %)2]

(9)

P

5.3

expected

The
data misfits

description

length

of

the

errors of

Unfortunately,

the data-misfit

by the systematic

cost given in equation

networks with noisy weights,

by the noise in the weights.
feedforward

3 we
value of (d; – VI ) 2. This squared error
the network

To compute
need the expected
is caused partly
and partly
for general
expected squared errors are not easy to compute.
approximations
weights
ness of
main purposes
weights.
and if
pute the expected

the
Linear
the level of noise in the
small compared with the smooth-
the
is to allow very noisy
layer
is possible to com-

is sufficiently
the non-linearities,

there is only one hidden

units
squared

the idea which

error exactly.

this defeats

are possible

Fort unately,

the output

are linear,

one of

but

of

if

if

it

of

z~,

input,

received

the output

the Gaussian-distributed

and variance, V.h, of

by hidden
the mean, pyh and variance,

are assumed to have independent
vector we can compute

Gaussian
the mean
t-
unit h. Using a table,
Vvh,
this out-

The weights
noise, so for any input
/+,,
tal
we can then compute
of
put
is required
many different
for each pair we must use Monte Carlo sampling
merical
table
it
using Monte Carlo sampling

since
pairs of PZA and V=h must be used, and
or nu-
pv~ and Vyh. Once the
than

even though
A lot of computation

is much more efficient

is not Gaussian

twedimensional

to compute

the hidden

distributed.

integration

to create

however,

runtime.

is built,

table

unit,

this

at

Since the noise in the outputs
independent,
they independently
each linear
contribute
put units
total
case,
able with

output
variance
are linear,

unit.
to the output
their

inputs
the output,

yj, of output

the hidden

of
contribute

The noisy weights,

units.

outputs,

yj, are equal

units
variance

is
to
also
Since the out-
to the

Whj,

they receive, ~j. On a particular

training

unit

j

is a random vari-

the following mean and variance:

PYj

‘x

h

/%hhh,

(lo)

j make independent

The mean and the variance
unit
squared error
ticular
training

case is dj,

(Ej

If

).

of

the activity

contributions

of output
to the expected
j on a par-

the desired output

of
is given by:

(Ej)

(~j)

=

((dj

–

~j)2)

=

(dj –PV,)2 + Vg,

(12)

(Ej)

above to compute

we first build
to be backpropagated

provided

for each input

So,
equations
can also backpropagate
Dj
derivatives
units. As before,
for
the backward
four partial
output
the equations:

derivatives

derivatives

the exact

the exact value of
derivatives
table

vector, we can use the table and the
(Ej). We
of E =
to allow
the hidden
by p=~ and V=h but
the
the
using

the table contains
that are needed to to convert

the table is indexed
psss each cell of

of h into its input

derivatives

through

another

6

Letting

the data

determine

the prior

values that make it very expensive

the “prior”

distribution

is a single Gaussian.
to both
the sender
are communicated.

in advance we could pick
to code
allow the mean and
during
depends
kind of prior! We could try
that

terms by assuming

to be determined

so the coding-prior

of

that

must

process,

the coding-prior

the weights
be known
before the weights

So far, we have assumed
is used for coding
that
This
coding-prior
and the receiver
If we fix its mean and variance
inappropriate
the actual weights. We therefore
variance
the optimization
on the data. This is a funny
to make sense of
we start with
distributions
prior
find
take into account
prior
practice, we just
two parameters
to invent

and then we use the hyper-prior
the best coding-prior.

to a receiver who only knows

the coding-prior

ignore
of

a hyper-prior

hyper-priors.

in Bayesian

that

for

it

the mean and variance

This would
the cost of communicating

specifies
of

probability
the coding-
and the data to

automatically

the hyper-prior.

the coding-
In
the
so we do not need

the cost of communicating

6.1

A more
weights

flexible

prior

distribution

for

the

simple

flexible

structure

the posterior

enough
in the weights,

for communicating
penalty

If we use a single Gaussian
prior
noisy weights, we get a relatively
for
the asymmetric
divergence,
Unfortunately,
tion of each noisy weight.
scheme is not
to capture
of common
ample,
near 1 and the rest
the posterior
distribution
ance (to avoid the extra
in the weights) we inevitably
weights
good model of a spike around O and a spike around

the
term,
distribu-
coding
kinds
for ex-
to have values
If
has low vari-
caused by noise
for
a

pay a high code cost
can provide

for each weight
squared

to have valuea very close to O.

because no single Gaussian

that we want a few of

this
certain

the weights

Suppose,

error

prior

1.

8

different

the different

subsets

distributions,

of
the
we can
subsets. As
it makes sense to use
weights
and
If we do

since the input

scales.

the input-t-hidden
weights

different

by NowIan

should

be similar,
by an adaptive
and Hin-
vari-
to

the means,

in the mixture
values. Simultaneously,

adapt

for

for

that

the weight

coding-priors

to have different

values may have quite

(1992) has demonstrated,
coding-priors

If we know in advance
weights are likely
use different
MacKay
different
and the hidden-t~output
output
not know in advance which weights
we can model
distribution
mixture
ton (1992).
ances and mixing
model
the clusters
the weights
adapt
weights
get pulled
ters. Suppose,
sians in the mixture.
low variance
variance it
with values near 1 or O.

proportions
in the weight
to fit
towards

the optimization,

of Gaussians

as proposed

During

for eaxample,

the centers of nearby

the current mixture model

so
clus-
there are two Gaus-
has mean 1 and
and the other gaussian has mean O and low
weights

is very cheap to encode low-variance

If one gaussian

that

mixture

and Hinton
distribution
variance
density

Nowlan
posterior
negligible
probability
coding-prior
their
variance
assuming
strained
cost of communicating
to be used for coding

of
that

technique

the mean of
distribution,

(1992)

implicitly

for each weight

assumed that
has a fixed
so they focussed on maximizing
of

under

the weight

the
and
the
the
We now show how
the
the weights,
are still
con-
As before, we ignore the
is

to take into account

distribution

that

for

the mixture
the weights.

can be extended

the posterior

distributions

the posterior

distributions

to be single Gaussians.

The mixture

prior

has the form:

P(w)

= ~ 7riPj(w)

i

(15)

where ~i
asymmetric
the single Gaussian

is the mixing
divergence

proportion
between

posterior,

Q,

of Gaussian

the mixture
prior
for a noisy weight

Pi. The
and
is

G(P, Q) =

/

Q(w) b ~ ~ipi(w)~w

(16)

Q(W)

i

with

This is unfortunate

that we repeatedly

ana-
pro-
both G(P, Q)
of

The sum inside the log makes this hard to integrate
lytically.
since the optimization
cess requires
and its derivatives
there is a much more tractable
P and Q. Fortunately,
expression which is an upper bound on G and can there
fore be used in its place, This expression
the Gi (Pi, Q) the asymmetric
divergences
posterior
in the mixture

is in terms of
the
the Gaussians, Pi,

evaluate
to the parameters

distribution,
prior.

Q, and each of

between

respect

G(P1,

P2 . . .. Q)=

-

log~

fi:e-G’

(17)

i

configurations

energy of a system depends on the energies of the various
one
alternative
scheme
way to derive equation
in which
and to
then use a lemma from statistical

17 is to define a coding
resembles
a free energy
mechanics.

the code cost

the system.

Indeed,

of

7

A coding
of Gaussians

scheme

that

uses a mixture

a sender

and

a receiver

mixture

of Gauaaians

that

Suppose
agreed on a particular
tion.
terior Gaussian
lowing

coding

scheme:

The sender can now send a sample
of a weight

distribution

have

already
distribu-
from the pos
using the fol-

1, Randomly

pick one of

the Gauasians

in the mixture

with probability

ri given by

xi e-G,
‘i= ~j lrjt?-Gj

(18)

2. Communicate

the choice

If we use the mixing

ceiver.
for communicating
cost

is

of Gaussian
proportions

to the re-
as a prior
code

the choice,

the expected

(19)

3. Communicate

value to the receiver
us-
the sample
If we take into account
ing the chosen Gaussian.
the random bits that we get backwhen the receiver
from which
reconstructs
the sample was chosen,
cost of com-
municating

distribution
the expected

the sample is

the posterior

u-’

(20)

.,

So the expected
choice of Gaussian
choice is

cost ~f communicating

the
and the sample value given that

both

4. After

receiving

samplea

from all

the

posterior

(21)

algorithm

cases with

distributions

the posterior

and also receiving

can run the learning

these sampled weights,

weight distributions
the training
receiver
struct
weights
reconstruct
the random bits used to choose a Gaussian from the
that must
mixture.
be subtracted
in equation
21 is

the errors on
the
and recon-
the
to

“bits
from the expected

the Gi and hence to reconstruct

from which
the receiver

So the number

are sampled.

back”
cost

allows

all of

This

of

The way in which G depends
17 is precisely

analogous

to the way in which

the free

on the Gi

in equation

(22)

We now use a lemma
simple
bits back.

expression

for

from statistical mechanics
the expected

code cost minus

to get a
the

7.1

A lemma

from statistical

mechanics

a physical

For
Helmholtz
energy minus

system at
F,

free energy,

a temperature
is defined

the
as the expected

of

1,

the entropy

F=~riEj–~Tilog~

i

i

ri

(23)

states
is the

Ei

the system,

where i
of
probability
ity distribution
tion which minimizes
which probabilities

is an index over

the alternative

possible
is the energy of a state, and ri

of a state.

F is a function

of

over states and the probability

F is the Boltzmann

are exponentially

related

the probabil-
distribu-

distribution

in
to energies

~-E,

‘i=

~j

,.E,

the minimum

At
the free energy is equal
function:

given by the Boltzmann

to minus

the log of

F = –log~e-Ei
i

(24)

distribution,
the partition

(25)

uses a Boltzmann

If we equate esch Gaussian
st a~e of a physical
t ernative
with ~ie-
‘*. So our method
the mixture
makes ri proportional
are successfully
structs
tropy
cost
therefore

the probabilities
the Boltzmann

(including

equal

of

communicated

with

a Gaussian

distribution
The random bits

in the mixture
an al-
system, we can equate e-E*
of picking
from
because it
that
recon-
to the en-
code
to F and is
17.

exactly
so the total

when the receiver

given in equation

is equivalent

the bits back)
to the expression

ri correspond
distribution,

to e- ‘S.

9

Preliminary

Results

this

that

data.

inputs

algorithm

turn out

a thorough

refinements

by 128 parameters

We have, however,

tried
task with

The task is to predict

comparison
methods

on one very high dimensional

and alternative
further

(the input
is a single scalar

be-
and it
are required
the
very
the effec-
Each molecule
vector)
and
(the ouput
to have
could
set con-

We have not yet performed
tween
may well
that
to make it competitive.
algorithm
scarce training
tiveness of a class of peptide molecules.
is described
has an effectiveness
value), All
zero mean and unit
be expected
sisted of 105 cases and the teat set was the remaining
420 cases. We deliberately
training
in which it should
set since these are the circumstances
be most helpful
in
units.
the weights. We tried
(including
contains
This network
so it overfits
the biases of
the output
the 105 training
the
information

a network
521 adaptive
and hidden

and outputs were normalized

scales. The training

chose a very small

cases very badly

weights
units)

in the weights.

to have similar

if we do not

the weights

the amount

information

to control

4 hidden

variance

so that

with

limit

of

for

uniformly

the weights.

with” means

and separated

of 5 Gaussians

We used an adaptive mixture
coding-prior
tialized
and +0.24
their neighbors.
tributions
with mean O and standard
dard deviations
all

as our
The Gauaaians were ini-
–0.24
from
dis-
of each weight were chosen from a Gaussian
0.15.
The stan-
the weights were

spaced
by 2 standard
The initial means for

between
deviationa

deviation
for

the posteriors

the posterior

initialized

at 0.1.

of

all of

the parameters

We optimize
ing a conjugate
optimize
and cannot
ing proportions
between O and 1 and add to 1 we optimize

gradient method.
so that

the log variance

the Gauasians

to zero.

collapse

For
it cannot

of

To ensure that

go negative
the mix-
lie
the ~i where

simultaneously

us-
the variances we

in the coding-prior

(26)

8

Implementation

of

robust

com-

deeeent

function

is worrying

coding-prior,

of Gausaians

gradient
minor

because
against

are moderately

in implementing

This
are quite

is easy to make an error

an adaptive mixture
the cost

With
the derivatives
plicated
so it
them.
rithms
errors.
is hard to know how large to make the tables
used for propagating
gistic
or
demonstrate
to decide the table sizes we used the following
check. We change each parameter
by a small
check that
the gradient
that
reasonably

rJgo-
it
are
lo-
To
and
semantic
step and
of
and step size. Using this method we found
gives

for backpropagating
the implementation

Alao,
that
through

a 300 x 300 table with

derivatives.
was correct

changes by the product

interpolation

distributions

derivatives.

Gaussian

the cost

functions

accurate

function

linear

that

at

coat

almost

is initially

the output

the desired

the weights

It seems that

the mean of

no information,

it
function

the optimization

by usin~ weights

and uses the bias of

quickly makes all of

If we penalize
them,
equal and negative
to fix the output
the training
set.
reduce the combined
contain
cape from this poor solution.
tiply
.05 and gradually
ule .05,
of
of the weights and at
minate
the optimization
by leas than
2 shows all
four hidden

by the full coat of describing
the weights
unit
values in
very easy to
that
is very hard to &
To avoid this trap, we mul-
that starta at
to the sched-
.2, .3, .4, .5, .6, .7, .8, .9, 1.0. At each value
updatea
ter-
changes
Figure
the
of
It

the cost of the weights by a coefficient
increases to 1 according

the final value of 1.0 we do not

function
bits).
weights

.15,
the coefficient

the incoming
units

we do 100 conjugate

after one run of

the optimization.

until
(a nat

and outgoing

10-6 nats

is logz(e)

the cost

gradient

and it

.1,

10

‘~

-2

2

probability
the weights.

distribution
This distribution

that

by adapting
of

proportions

the means,

variances

five gauasians.

3: The final

Figure
is used for coding
is implemented
and mixing

that
is clear
ters.
Figure
has adapted
for

this weight

the weights
3 shows that
to implement
distribution.

form three

the mixture
the appropriate

fairly

sharp

clus-
of 5 Gaussians

coding-prior

The performance
comparing
with the error
ing the mean of

of

the network

can be measured

by
it achievea on the test data
guess-

by simply

be achieved

the squared error

that would

the correct

answera for

the test data:

Relative Error =

~c(dc

- y.)’

~c(dc

- ~)2

(27)

for

required

4. To set

it was necessary

the squared weights

the relative

ran-
five times using different
the initial means of
the noisy
that achieved the lowest value
error was 0.286.
the same
and did not
best
relative
with four non-

error of 0.967 for

content.

function,

to uf/a~

coefficient

in equation

appropriately

values on a portion

For
the overall

chosen valuea for
the network
cost

information
using simple weight-decay
units was .317. This

We ran the optimization
domly
weights.
of
This compares with a relative
when we used noise-free weights
network
penalize
their
The
error obtained
linear
hidden
chosen penalty
corresponds
decay coefficient
many different
and to use the remainder
which coefficient
beat coefficient
training
ror of 0.291 can be achieved
gradually
the value that
data.
huge relative
fell
of the regression coefficients
performance
sen to optimize
almost
identical
to the performance
and optimal
probably
weights,
range, so the whole network

to 0.291 when we penalized

gives optimal
is cheating.

in their
is effectively

increase the weight-decay

performance
Linear

set was used with

this coefficient.

by an amount

units operate

weight-decay

the training

the hidden

error

gave the best generalization.
had been determined

of 35.6 (gross overfitting)
the sum of

the whole

the training

a carefully
that
this weight-
to try
set
to decide
Once the
the
A lower er-
if we
and pick
on the test
gave a
this
the squarea
that was ch~
is
units
with 4 hidden
because, with small
linear

regression

coefficient

This

using weight-decay

on the test data.

central
linear.

this

But

but

set

of

of

of

11

the network.

unit.

negative

Each
The small
or
rep
The bot-
the incoming

the area of a rectangle

the weight.

represent

of

or white

rectangles

represents

one hidden

2: The final weights

Figure
large block
black
positive weights with
resenting
tom 12 rows in each block represent
weights of
the top of each block is the weight
unit
top-right

the magnitude

is the bias of

to the linear

the hidden

of a block

output

unit.

unit.

of

The central weight

at
from the hidden
the
unit.

the hidden

at

The weight

non-linear

complicated
of

demonstrate

that

our

new

allows

results

techniques

preliminary

the method

the number

of dimensions

also show that
simple weight-decay
experimental
work

These
method
us to fit quite
models even when the number
than
The results
better
than
Much more
whether
tical
the amount
the dimensionality
tioning
total
the weights
except
tive. This solution
1.0 so it
Description
scribing

the output
has a relative

of training
of

the weights.

for handling

the solution

description

Principle

Length

that

is competitive

the new method

on at
is required

training
in the input

cases is less
vector.
is slightly
least one task.
to decide
statis-
tasks in which
data is very small compared with
is also worth men-
the input.
the
all
bias are equal and nega-

value of
in which

non-linear

other

with

It

the lowest
length was the solution

with

error of approximately
the Minimum

for

is a serious embarrassment

or

for our method

of de

10

Discussion

but

in a feed forward

space. We then construct

in weight

intractable,

the weights

is a correct,

There
of determining
work. We start with a prior distribution
points
posterior
multiplying
outputs
nally we normalize
Then we use this distribution
predictions

at each point
by the probability

in the training

for new input

distribution

the prior

vectors.

to get

set given
the full posterior

neural

Bayesian method
net-
over all possible
the correct
space by
the
Fi-

of getting
those weights.4

in weight

values to make

distribution.

of weight

that

network

distribution.

a probability

the closest we can get

is to use a Monte Carlo method

This
random moves in weight

In practice,
method
the posterior
sidering
move with
resulting
shows how the gradient
propagation
method
bution.
ods is that
assumptions
tion in weight

to the ideal Bayesian
to sample from
could be done by con-
a
space and accepting
the
(1993)
by back-
can be used to get a much more efficient
distri-
of Monte Carlo meth-
simple
distribu-

impose
the shape of

unrealistically
the posterior

of obtaining
The major

samples
advantage

from the posterior

fits the desired

they
about

on how well

information

depends

provided

outputs.

do not

space.

Neal

to make simplifying

assumptions

time-consuming

can be avoided. MacKay

(1992)

about
Monte Carlo
finds a sin-

point

optimal

distribution,

If we are willing
the posterior
simulations
gle locally
covariance Gaussian
a full
distribution
around
terior
proposed
method
sian approximation
covariance matrix)
count

during

(with

but

in weight

space and constructs

approximation
that
in this paper

point.

to the pos-

The alternative

no off-diagonal

is to use a simpler Gaua-
in the
into ac-

terms

to take this distribution

the learning. With

one layer of non-linear

4This

assumes

that

the output

of

the neural

net

the mean
output

sents
final
be exactly

of a Gaussian

distribution

from which

is randomly

selected.

correct

even though

So the tlnal
of

the output

output
the net

repre-
the
could
is not.

hidden
bution
derivatives

the integration

units,
can be performed

over

the Gaussian

distri-
and the exact weight

exactly

can be computed

efficiently.

has shown

that

1s used to find a single,
space and a Gaussian
weight

distribution

the covariances

However,
are significant
manipulating

clear how much
terms in the covariance matrix.

is lost by ignoring

point,

is not

communication)

this
are significant.

It
diagonal
(personal
backpropagatlon
point
in weight
to the posterior
around
weights
that
algorithm
tribution
force the noise in the weights
pressure for
coat
function
weights
if
performing
deed suppress

the covariances
is explicitly
because in this

overestimate
they have correlated

the covariancea.

independence

simulations

will

the off-
David Mackay
if standard
optimaI

between

locally
approximation
is then constructed
different
this does not mean
when the learning
di~
the Gaussian
to
The
the
in the

try

case the learning

will
to be independent.

comes from the fact
the information
noise. We are currently

that

to see if

this pressure

does in-

of

With

sigmoid

that
its input.

When using the standard
is essential
function
a smooth
old function.
ble to use a version
described
ear
that
a smooth
to optimize
vectors

threshold
the probability
function

than it

units.

of

backpropagation

algorithm,

it
is a smooth

units use

unit

of a hidden
is why the hidden
instead

the output
This
function
noisy weights,
of

the backpropagation

however,

of a linear
it
algorithm

thresh-
is possi-

above in networks

that

have one layer of

The noise in the weights
of a threshold
its inputs.

As a result,

unit

lin-
ensures
being active is
is easier
over weight

it

a whole Gaussian

distribution

is to optimize

a single weight

vector.

11

Acknowledgements

by operating

research was funded

from NSERC. Geoffrey Hinton

This
and strategic
grants
is the Noranda
fellow of
for Advanced Research.
We thank David Mackay, Radford Neal, Chris Williams
and Rich Zemel

the Canadian

discussions.

for helpful

Institute

12 References

G. E.

(1987)

Learning
in a maaaivel y parallel

translation
network.

invariant

In Gooa, G.

and Languages

J., editors,
Eurape,

PARLE:
pages

Pamllel

Architec-

1–13,

Lecture

Notes

Hinton,
recognition
and Hartmanis,
tures
in Computer

Science, Springer-Verlag,

Berlin.

Lang, K., Waibel, A. and Hinton, G. E.
Delay Neural Network
for
3, 23-43.
Recognition.

Neuml Networks,

Architecture

(1990) A Time-
Isolated Word

B., Denker,

R. E., Hubbard,

W.

Y., Boser,

Le Cun,
D., Howard,
(1989) Back-Propagation
code Recognition.

Applied

J. S., Henderson,
and Jackel,
to Handwritten

L. D.
Zip-

Neuml Computation,

1, 541-551.

Mackay, D. J. C. (1992) A practical

Bayesian

framework

12

for backpropagation
448-472.

networks.

Neuml Computation,

4,

In Giles, C. L., Hanson, S. J. and Cowan,

learning

via stochastic

(1993) Bayesian

Neal, R. M.
namics.
D.
in Neuml
Systems 5, Morgan Kaufmann,

(Eds), Advances

dy-
J.
Processing

Information
San Mateo CA.

NowIan.
neural networks
tation,

4, 173-193.

S. J. and Hinton,

G. E.

(1992) Simplifying

by soft weight

sharing.

Neuml Compu-

Rissanen,
ing. Annals

J.

(1986) Stochastic
of Statistics,

14, 1080-1100.

Complexity

and Model-

13

