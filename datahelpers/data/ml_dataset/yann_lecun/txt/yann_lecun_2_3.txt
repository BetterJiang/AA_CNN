Ask the locals: multi-way local pooling for image

recognition

Y-Lan Boureau, Nicolas Le Roux, Francis Bach, Jean Ponce, Yann Lecun

To cite this version:

Y-Lan Boureau, Nicolas Le Roux, Francis Bach, Jean Ponce, Yann Lecun. Ask the locals:
multi-way local pooling for image recognition. ICCV‚Äô11 - The 13th International Conference
on Computer Vision, Nov 2011, Barcelone, Spain. 2011. <hal-00646816>

HAL Id: hal-00646816

https://hal.inria.fr/hal-00646816

Submitted on 30 Nov 2011

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entiÔ¨Åc research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L‚Äôarchive ouverte pluridisciplinaire HAL, est
destin¬¥ee au d¬¥epÀÜot et `a la diÔ¨Äusion de documents
scientiÔ¨Åques de niveau recherche, publi¬¥es ou non,
¬¥emanant des ¬¥etablissements d‚Äôenseignement et de
recherche fran¬∏cais ou ¬¥etrangers, des laboratoires
publics ou priv¬¥es.

Ask the locals: multi-way local pooling for image recognition

Y-Lan Boureau1,3,‚òÖ

Nicolas Le Roux1,‚Ä†

Francis Bach1,‚Ä†

Jean Ponce2,‚òÖ

Yann LeCun3

1INRIA

2Ecole Normale Sup¬¥erieure

3Courant Institute, New York University

Abstract

Invariant representations in object recognition systems
are generally obtained by pooling feature vectors over spa-
tially local neighborhoods. But pooling is not local in the
feature vector space, so that widely dissimilar features may
be pooled together if they are in nearby locations. Recent
approaches rely on sophisticated encoding methods and
more specialized codebooks (or dictionaries), e.g., learned
on subsets of descriptors which are close in feature space, to
circumvent this problem. In this work, we argue that a com-
mon trait found in much recent work in image recognition or
retrieval is that it leverages locality in feature space on top
of purely spatial locality. We propose to apply this idea in its
simplest form to an object recognition system based on the
spatial pyramid framework, to increase the performance of
small dictionaries with very little added engineering. State-
of-the-art results on several object recognition benchmarks
show the promise of this approach.

1. Introduction

Much recent work in image recognition has underscored
the importance of locality constraints for extracting good
image representations. Methods that incorporate some way
of taking locality into account deÔ¨Åne the state of the art on
many challenging image classiÔ¨Åcation benchmarks such as
Pascal VOC, Caltech-101, Caltech-256, and 15-Scenes [11,
39, 42, 43, 45].

The spatial pyramid [25] has emerged as a popular
framework to encapsulate more and more sophisticated fea-
ture extraction techniques [4, 11, 39, 41, 42, 45]. The global
representation of an image is obtained by extracting image
descriptors such as SIFT [28] or HOG [9] on a dense grid,
encoding them over some learned codebook or dictionary
(coding step), and then summarizing the distribution of the
codes in the cells of a spatial pyramid by some well-chosen
aggregation statistic (pooling step).

‚àó

WILLOW project-team, Laboratoire d‚ÄôInformatique de l‚ÄôEcole Nor-

male Sup¬¥erieure, ENS/INRIA/CNRS UMR 8548.

‚Ä†

SIERRA project-team, Laboratoire d‚ÄôInformatique de l‚ÄôEcole Nor-

male Sup¬¥erieure, ENS/INRIA/CNRS UMR 8548.

1

Several recent papers have focused on reÔ¨Åning the cod-
ing step, one purpose of which is to produce representations
that can be aggregated (pooled) without losing too much in-
formation in the process. Pooling, which has long been part
of popular recognition architectures such as convolutional
networks [26], gives robustness to small transformations of
the image. It is related to Koenderink‚Äôs concept of locally
orderless images
[24], and can be traced back to Hubel
and Wiesel‚Äôs seminal work on complex cells in the visual
cortex [18]. The simplest pooling operation consists in av-
eraging the feature vectors within a spatial neighborhood.
One fact that makes the coding step necessary is that de-
scriptors such as SIFT or HOG cannot be averaged with
their neighbors without losing a considerable amount of in-
formation: the average of several widely different SIFT fea-
tures doesn‚Äôt tell us much about the content of the underly-
ing image. Hence, coding is generally designed to produce
representations that can be added with each other without
diluting the signal. The original spatial pyramid proposal
used vector quantization (K-means), which can be seen as a
way of turning SIFT vectors into very sparse, 1-of-K codes.
When obtained by averaging over spatial neighborhoods,
the pooled codes can be interpreted as local histograms of
visual words. Recent work has proposed to replace this
hard quantization step of individual SIFT descriptors by soft
vector quantization [36], or sparse coding [41], including
sparse coding of local groups of SIFT descriptors (macro-
features) instead of single ones, in our previous work [4].

While the pooling operations are often performed over
local spatial neighborhoods, the neighborhoods may con-
tain feature vectors that are very heterogeneous, possibly
leading to the loss of a large amount of information about
the distribution of features, as illustrated in Fig. (1). Re-
stricting the pooling to feature vectors that are similar in the
multidimensional input space (or nearby) [21, 45] remedies
this problem. The usefulness of considering similar inputs
for smoothing noisy data over a homogeneous sample to re-
duce noise without throwing out the signal has long been
recognized in the image processing and denoising commu-
nities [6, 8], and has been successfully incorporated to de-
noising methods using sparse coding [30].
It is interest-
ing to note that considerations of locality often pull cod-

2

f

2

f

f1

f1

Figure 1. Cartoon representation of a distribution of descriptors that has a high curvature and is invariant to the spatial location in the
image, with two feature components (left). The center and right Ô¨Ågures show the samples projected across space in the 2D feature space.
Due to the curvature of the surface, global pooling (center) loses most of the information contained in the descriptors; the red cross (average
pooling of the samples) is far away from the lower-dimensional surface on which the samples lie. Clustering the samples and performing
pooling inside each cluster preserves information since the surface is locally Ô¨Çat (right).

ing and pooling in opposite directions: they make coding
smoother (neighbors are used to regularize coding so that
noise is harder to represent) and pooling more restrictive
(only neighbors are used so that the signal does not get av-
eraged out). This can be viewed as an attempt to distribute
smoothing more evenly between coding and pooling.

Authors of locality-preserving methods have often at-
tributed their good results to the fact that the encoding uses
only dictionary atoms that resemble the input [39, 43], or
viewed them as a trick to learn huge specialized dictionar-
ies, whose computational cost would be prohibitive with
standard sparse coding [42]. However, this view may un-
derestimate how local coding also makes pooling more lo-
cal, by tying activation of a component more strongly to a
region of the multidimensional input conÔ¨Åguration space.
We argue that more local pooling may be one factor in
the success of methods that incorporate locality constraints
into the training criterion of the codebook for sparse cod-
ing [11, 39, 43], or directly cluster the input data to learn
one local dictionary per cluster [39, 42]. The question we
attempt to answer in this paper is whether it is possible to
leverage locality in the descriptor space once the descriptors
have already been encoded. We argue that if the coding step
has not been designed in such a way that the pooling oper-
ation preserves as much information as possible about the
distribution of features, then the pooling step itself should
become more selective.

The contributions of this work are threefold. First, we
show how several recent feature extracting methods can be
viewed in a uniÔ¨Åed perspective as preventing pooling from
losing too much relevant information. Second, we demon-
strate empirically that restricting pools to codes that are
nearby not only in (2D) image space but also in descrip-
tor space, boosts the performance even with relatively small
dictionaries, yielding state-of-the-art performance or better
on several benchmarks, without resorting to more compli-
cated and expensive coding methods, or having to learn new
dictionaries. Third, we propose some promising extensions.
The paper is organized as follows. Sec. 2 introduces the

general classiÔ¨Åcation pipeline that is used in this paper and
much previous work and motivates our approach. Sec. 3
presents related work. Experiments are presented in Sec. 4.

2. General image recognition architecture and

proposed approach

As argued in our previous work [4], many modern meth-
ods for image classiÔ¨Åcation, such as convolutional and deep
belief networks [17, 20, 26, 32], bags of features [34], his-
tograms of gradients for pedestrian detection [9], or the spa-
tial pyramid [25], implement an alternating series of coding
and spatial pooling steps. ClassiÔ¨Åcation is then performed
with some standard classiÔ¨Åer such as a kernel or linear sup-
port vector machine (SVM), or logistic regression.

2.1. Feature extraction

Our setting for feature extraction is the following. Let ùêº
denote an input image. First, low-level descriptors ùíôùëñ (e.g.,
SIFT or HOG) are extracted densely at ùëÅ locations identi-
Ô¨Åed with their indices ùëñ = 1, . . . , ùëÅ. Coding is performed
at each location by applying some operator that is chosen to
ensure that the resulting codes ùú∂ùëñ retain useful information
(e.g., input data can be predicted from them), while hav-
ing some desirable properties (e.g., compactness). Here, we
focus on hard vector quantization and sparse coding, that
both minimize some regularized error between inputs and
the reconstructions that can be obtained from the codes.

Hard vector quantization is the simplest coding step,
used in the bag-of-features framework [34]. It models the
data with ùêæ clusters, representing each ùë•ùëñ by a one-of-ùêæ
encoding of its cluster assignment:
ùú∂ùëñ ‚àà {0, 1}ùêæ , ùú∂ùëñ,ùëó = 1 iff ùëó = argmin
ùëò‚â§ùêæ

‚à•ùíôùëñ ‚àí dùëò‚à•2
2,

(1)

where dùëò denotes the ùëò-th codeword of a codebook that is
usually learned by an unsupervised algorithm such as K-
means. The extreme sparseness of the codes produced (only

one component per code is non-zero) may be ill-suited to
images, and in fact better results have been obtained by re-
placing hard vector quantization by a soft probabilistic ver-
sion [36], or by sparse coding [4, 41].

Sparse coding
combination of few codewords:

[31] reconstructs the input as a linear

ùêøùëñ(ùú∂, D) ‚âú ‚à•ùíôùëñ ‚àí Dùú∂‚à•2

2 + ùúÜ‚à•ùú∂‚à•1,

ùú∂

ùú∂ùëñ = argmin

(2)
where ‚à•ùú∂‚à•1 denotes the ‚Ñì1 norm of ùú∂, ùúÜ is a parameter that
controls the sparsity of ùú∂, and D is some dictionary, which
can be obtained by K-means, or for better performance,
trained by minimizing the average of ùêøùëñ(ùú∂ùëñ, D) over all
samples, alternatively over D and the ùú∂ùëñ. It is well known
that the ‚Ñì1 penalty induces sparsity and makes the problem
tractable (e.g., [27, 29]).

A pooling operator then takes the varying number of
codes that are located within ùëÄ possibly overlapping re-
gions of interest (e.g., the cells of a spatial pyramid), and
summarizes them as a single vector of Ô¨Åxed length. The
representation for the global image is obtained by concate-
nating the representations of each region of interest, pos-
sibly with a suitable weight. We denote by ùí¥ùëö the set of
locations/indices within region ùëö. Here, we use the two
common pooling strategies of average and max pooling.

Average pooling
computes a histogram or take the aver-
age of the codes over the region (these two methods are
equivalent after normalization):

ùíâùëö =

1
‚à£ùí¥ùëö‚à£

‚àë

ùëñ‚ààùí¥ùëö

ùú∂ùíä,

(3)

where ùíâùëö is the vector representing region ùëö.

‚ÄúMax pooling‚Äù
computes the maximum of each compo-
nent instead of its average. It has recently gained popularity
due to its better performance when paired with sparse cod-
ing and simple linear classiÔ¨Åers [4, 38, 41], and its statistical
properties which make it well suited to sparse representa-
tions [5]. In our notation, max pooling is written:

ùíâùëö,ùëó = max
ùëñ‚ààùí¥ùëö

ùú∂ùëñ,ùëó, for ùëó = 1, . . . , ùêæ.

(4)

Max pooling is used for all experiments in this work,

except those in Sec. 4.2.3.

2.2. Pooling more locally across the input space

We propose to streamline the approach in [42], which
requires learning one different dictionary per cluster, and
show that simply making the pooling step more selective

can substantially enhance the performance of small dictio-
naries, and beat the state of the art on some object recog-
nition benchmarks when large dictionaries are used, with-
out requiring additional learning beyond obtaining an addi-
tional clustering codebook with ùêæ-means. Comparing the
performance of our system with that obtained with individ-
ual dictionaries allows us to quantify the relative contribu-
tions of more selective pooling and more specialized, over-
complete dictionaries.

To clarify how our local pooling scheme differs from the
usual local spatial pooling, an image feature can be viewed
as a couple ùëß = (ùíô, ùíö), where ùíö ‚àà ‚Ñù
2 denotes a pixel
location, and ùíô ‚àà ‚Ñù
ùëë is a vector, or conÔ¨Åguration, encoding
the local image structure at ùíö (e.g., a SIFT descriptor, with
ùëë = 128). A feature set ùíµ is associated with each image, its
size potentially varying from one picture to the next.

Spatial pooling considers a Ô¨Åxed ‚Äì that is, predetermined
and image-independent ‚Äì set of ùëÄ possibly overlapping im-
age regions (spatial bins) ùí¥1 to ùí¥ùëÄ . To these, we add a
Ô¨Åxed set of ùëÉ (multi-dimensional) bins ùí≥1 to ùí≥ùëÉ in the
conÔ¨Åguration space. In this work, the spatial bins are the
cells in a spatial pyramid, and the conÔ¨Åguration space bins
are the Voronoi cells of clusters obtained using ùêæ-means.

Denoting by ùëî the pooling operator (average or max in

the previous section), the pooled feature is obtained as:

ùíâ(ùëö,ùëù) = ùëî(ùëñ‚ààùí¥ùëö,ùëó‚ààùí≥ùëù)(ùú∂(ùëñ,ùëó)).

(5)

Bags of features can be viewed as a special case of this in
two ways: either by considering the 1-of-ùêæ encoding pre-
sented above, followed by global pooling in the conÔ¨Ågura-
tion space (ùëÉ = 1), or with a simplistic encoding that maps
all inputs to 1, but does Ô¨Åne conÔ¨Åguration space binning
(ùëÉ = ùêæ). Accordingly, the feature extraction in this paper
can be viewed either as extending the sparse coding spatial
pyramid by making conÔ¨Åguration space pooling local, or as
extending the hard-vector-quantized spatial pyramid by re-
placing the simplistic code by sparse coding: descriptors are
Ô¨Årst decomposed by sparse coding over a dictionary of size
ùêæ; the same descriptors are also clustered over a ùêæ-means
dictionary of size ùëÉ ; Ô¨Ånally, pooling of the sparse codes is
then performed separately for each cluster (as in aggregated
coding [21] ‚Äì see Sec. 3 ‚Äì, but with sparse codes), yielding
a feature of size ùêæ √ó ùëÉ √ó ùëÜ if there are ùëÜ spatial bins.

While this does not apply to max pooling, local pool-
ing can be viewed as implementing local bilinear classiÔ¨Åca-
tion when using average pooling and linear classiÔ¨Åcation:
the pooling operator and the classiÔ¨Åer may be swapped,
and classiÔ¨Åcation of local features then involves computing
ùë•ùë¶ùëä ùú∂, where ùú∑ùë•ùë¶ is a (ùëÜ√óùëÉ )- dimensional binary vector
ùú∑ùëá
that selects a subset of classiÔ¨Åers corresponding to the con-
Ô¨Åguration space and spatial bins, and ùëä is a (ùëÜ √ó ùëÉ ) √ó ùêæ
matrix containing one ùêæ-dimensional local classiÔ¨Åer per
row.

3. Related work about locality in feature space

We start our review of previous work with a caveat about
word choice. There exists an unfortunate divergence in the
vocabulary used by different communities when it comes
to naming methods leveraging neighborhood relationships
in feature space: what is called ‚Äùnon-local‚Äù in work in the
vein of signal processing [6, 30] bears a close relationship to
‚Äùlocal‚Äù Ô¨Åtting and density estimation [11, 33, 39, 43]. Thus,
non-local means [6] and locally-linear embedding [33] actu-
ally perform the same type of initial grouping of input data
by minimal Euclidean distance. This discrepancy stems
from the implicit understanding of ‚Äùlocal‚Äù as either ‚Äùspa-
tially local‚Äù, or ‚Äùlocal in translation-invariant conÔ¨Åguration
space‚Äù.

3.1. Preserving neighborhood relationships during

coding

Previous work has shown the effectiveness of preserving
conÔ¨Åguration space locality during coding, so that similar
inputs lead to similar codes. This can be done by explicitly
penalizing codes that differ for neighbors. The DrLIM sys-
tem of siamese networks in [16], and neighborhood com-
ponent analysis [13], learn a mapping that varies smoothly
with some property of the input by minimizing a cost which
encourages similar inputs to have similar codes (similarity
can be deÔ¨Åned arbitrarily, as locality in input space, or shar-
ing the same illumination, orientation, etc.) Exploiting im-
age self-similarities has also been used successfully for de-
noising [6, 8, 30].

Locality constraints imposed on the coding step have
been adapted to classiÔ¨Åcation tasks with good results.
Laplacian sparse coding [11] uses a modiÔ¨Åed sparse coding
step in the spatial pyramid framework. A similarity matrix
of input SIFT descriptors is obtained by computing their in-
tersection kernel, and used in an added term to the sparse
coding cost. The penalty to pay for the discrepancy be-
tween a pair of codes is proportional to the similarity of the
corresponding inputs. This method obtains state-of-the-art
results on several object recognition benchmarks. Locality-
constrained linear coding [39] (LLC) projects each descrip-
tor on the space formed by its ùëò nearest neighbors (ùëò is
small, e.g., ùëò = 5). This procedure corresponds to per-
forming the Ô¨Årst two steps of the locally linear embedding
algorithm [33] (LLE), except that the neighbors are selected
among the atoms of a dictionary rather than actual descrip-
tors, and the weights are used as features instead of being
mere tools to learn an embedding.

Sparse coding methods incorporating a locality con-
straint share the property of indirectly limiting activation
of a given component of the vectors representing descrip-
tors to a certain region of the conÔ¨Åguration space. This
may play a role in their good performance. For example, in
LLC coding, the component corresponding to a given dic-
tionary atom will be non-zero only if that atom is one of

the ùëò nearest neighbors of the descriptor being encoded; the
non-zero values aggregated during pooling then only come
from these similar descriptors. Several approaches have im-
plemented this strategy directly during the pooling step, and
are presented in the next section.

3.2. Letting only neighbors vote during pooling

Pooling involves extracting an ensemble statistic from
a potentially large group of inputs. However, pooling too
drastically can damage performance, as shown in the spa-
tial domain by the better performance of spatial pyramid
pooling [25] compared to whole-image pooling.

Different groups have converged to a procedure involv-
ing preclustering of the input to create independent bins
over which to pool the data. In fact, dividing the feature
space into bins to compute correspondences has been pro-
posed earlier by the pyramid match kernel approach [14].
However, newer work does not tile the feature space evenly,
relying instead on unsupervised clustering techniques to
adaptively produce the bins.

The methods described here all perform an initial (hard
or soft) clustering to partition the training data according
to appearance, as in the usual bag-of-words framework, but
then assigning a vector to each cluster instead of a scalar.
The representation is then a ‚Äúsuper-vector‚Äù that concate-
nates these vectors instead of being a vector that concate-
nates scalars.

Aggregated coding [21] and super-vector coding [45]
both compute, for each cluster, the average difference be-
tween the inputs in the cluster, and its centroid: (1) SIFT
descriptors xùëñ are extracted at regions of interest, (2) visual
words cùëò are learned over the whole data by ùêæ-means, (3)
descriptors of each image are clustered, (4) for each cluster
(x ‚àí cùëò) is computed, (5) the image
ùê∂ùëò, the sum
descriptor is obtained by concatenating the representations
for each cluster.

x‚ààùê∂ùëò

‚àë

If the centroids were computed using only the descrip-
tors in a query image, the representation would be all ze-
ros, because the centroids in K-means are also obtained by
averaging the descriptors in each cluster. Instead, the cen-
troids are computed using descriptors from the whole data,
implicitly representing a ‚Äúbaseline image‚Äù against which
each query image is compared. Thus, encoding relatively to
the cluster centroid removes potentially complex but non-
discriminative information. This representation performs
very well on retrieval [21] and image classiÔ¨Åcation [45]
(Pascal VOC2009) benchmarks.

Another related method [42] that obtains high accuracy
on the Pascal datasets combines the preclustering step of
aggregated and super-vector coding, with sparse decompo-
sition over individual local dictionaries learned inside each
cluster. Both approaches using preclustering for image clas-
siÔ¨Åcation [42, 45] have only reported results using gigantic
global descriptors for each image. Indeed, the high results
obtained in [42] are attributed to the possibility of learning

a very large overcomplete dictionary (more than 250,000
atoms) which would be computationally infeasible without
preclustering, but can be done by assembling a thousand or
more smaller local dictionaries. The experiments presented
in the next section seek to isolate the effect of local pooling
that is inherent in this scheme.

4. Experiments

We perform experiments on three image recognition
datasets: 15-Scenes [25], Caltech-101 [10] and Caltech-
256 [15]. All features are extracted from grayscale images.
Large images are resized to Ô¨Åt inside a 300√ó 300 box. SIFT
descriptors are extracted densely over the image, and en-
coded into sparse vectors using the SPAMS toolbox [1].
We adopt the denser 2 √ó 2 macrofeatures of our previous
work [4], extracted every 4 pixels, for the Caltech-256 and
Caltech-101 databases, and every 8 pixels for the Scenes,
except for some experiments on Caltech-256 where stan-
dard features extracted every 8 pixels are used for faster
processing. The sparse codes are pooled inside the cells of a
three-level pyramid (4√ó 4, 2√ó 2 and 1√ó 1 grids); max pool-
ing is used for all experiments except those in Sec. 4.2.3,
which compare it to other pooling schemes. We apply an
‚Ñì1.5 normalization to each vector, since it has shown slightly
better performance than no normalization in our experi-
ments (by contrast, normalizing by ‚Ñì1 or ‚Ñì2 norms worsens
performance). One-versus-all classiÔ¨Åcation is performed by
training one linear SVM for each class using LIBSVM [7],
and then taking the highest score to assign a label to the in-
put. When local pooling in the conÔ¨Åguration space is used
(ùëÉ ‚â• 1), clustering is performed using the ùêæ-means algo-
rithm to obtain cluster centers. Following the usual prac-
tice [15, 25, 39], we use 30 training images on the Caltech-
101 and Caltech-256 datasets, 100 training images on the
Scenes dataset; the remaining images are used for testing,
with a maximum of 50 and 20 test images for Caltech-
101 and Caltech-256, respectively. Experiments are run ten
times on ten random splits of training and testing data, and
the reported result is the mean accuracy and standard devi-
ation of these runs. Hyperparameters of the model (such as
the regularization parameter of the SVM or the ùúÜ parameter
of sparse coding) are selected by cross-validation within the
training set. Patterns of results are very similar for all three
datasets, so results are shown only on Caltech-101 for some
of the experiments; more complete numerical results on all
three datasets can be found in the supplemental material

‚àó

.

4.1. Pooling locally in conÔ¨Åguration space yields

state-of-the-art performance

Experiments presented in Table 1 and 2 compare the per-
formance of sparse coding with a variety of conÔ¨Åguration
space pooling schemes, with a list of published results of
http://cs.nyu.edu/‚àºylan/Ô¨Åles/publi/boureau-iccv-11-supplemental.pdf

‚àó

Boiman et al. [3]
Boureau et al. [4]
Gao et al. [11]
Jain et al. [19]
Lazebnik et al. [25]
van Gemert et al. [36]
Wang et al. [39]
Yang et al. [41]
Zhang et al. [44]
Zhou et al. [46]
ùêæ = 256, Pre, ùëÉ = 1
ùëÉ = 16
ùëÉ = 64
ùëÉ = 128
ùëÉ = 1 + 16
ùëÉ = 1 + 64

ùêæ = 256, Post,ùëÉ = 16
ùëÉ = 64
ùëÉ = 128

ùêæ = 1024,Pre, ùëÉ = 1
ùëÉ = 16
ùëÉ = 64
ùëÉ = 1 + 16
ùëÉ = 1 + 64

ùêæ = 1024,Post,ùëÉ = 16
ùëÉ = 64

Caltech 30 tr.
70.4
75.7 ¬± 1.1
‚àí
69.6
64.4 ¬± 0.8
64.1 ¬± 1.2
73.44
73.2 ¬± 0.5
66.2 ¬± 0.5
‚àí
70.5 ¬± 0.8
74.0 ¬± 1.0
75.0 ¬± 0.8
75.5 ¬± 0.8
74.2 ¬± 1.1
75.6 ¬± 0.6
75.1 ¬± 0.8
76.4 ¬± 0.8
76.7 ¬± 0.8
75.6 ¬± 0.9
76.3 ¬± 1.1
76.2 ¬± 0.8
76.9 ¬± 1.0
77.3 ¬± 0.6
77.0 ¬± 0.8
77.1 ¬± 0.7

Scenes
-
85.6 ¬± 0.2
89.8 ¬± 0.5
-
81.4 ¬± 0.5
76.7 ¬± 0.4
‚àí
80.3 ¬± 0.9
-
84.1 ¬± 0.5
78.8 ¬± 0.6
81.5 ¬± 0.8
81.1 ¬± 0.5
81.0 ¬± 0.3
81.5 ¬± 0.8
81.9 ¬± 0.7
80.9 ¬± 0.6
81.1 ¬± 0.6
81.1 ¬± 0.5
82.7 ¬± 0.7
82.7 ¬± 0.9
81.4 ¬± 0.7
83.3 ¬± 1.0
83.1 ¬± 0.7
82.9 ¬± 0.6
82.4 ¬± 0.7

Table 1. Results on Caltech-101 (30 training samples per class)
and 15-scenes for various methods. Results for our method are
given as a function of whether clustering is performed before (Pre)
or after (Post) the encoding, ùêæ: dictionary size, and ùëÉ : number of
conÔ¨Åguration space bins.

methods using grayscale images and a single type of de-
scriptor. Local pooling always improves results, except on
the Scenes for a dictionary of size ùêæ = 1024. On the
Caltech-256 benchmark, our performance of 41.7% accu-
racy with 30 training examples is similar to the best re-
ported result of 41.2% that we are aware of (for methods
using a single type of descriptors over grayscale), obtained
by locality-constrained linear codes [39], using three scales
of SIFT descriptors and a dictionary of size ùêæ = 4096.

4.1.1 Using pyramids in conÔ¨Åguration space

We examine whether it is advantageous to combine Ô¨Åne and
coarse clustering, in a way reminiscent of the levels of the
spatial pyramid. With large dictionaries, local pooling in
the conÔ¨Åguration space does not always perform better than
standard global pooling (see Tables 1 and 2). However,
combining levels of different coarseness gives performance
better than or similar to that of the best individual level, as
has been observed with the spatial pyramid [25].

This signiÔ¨Åcantly improves performance on the Caltech-
101 dataset. To the best of our knowledge, our perfor-
mance of 77.3% on the Caltech-101 benchmark, is above
all previously published results for a single descriptor type

Boiman et al. [3]
Gao et al. [11] (ùêæ = 1024)
Kim et al. [23]
van Gemert et al. [36] (ùêæ = 128)
Wang et al. [39] (ùêæ = 4096)
Yang et al. [41] (ùêæ = 1024)
ùêæ = 256,

Pre,

ùëÉ = 1
ùëÉ = 16
ùëÉ = 64
ùëÉ = 128
ùëÉ = 16
ùëÉ = 64
ùëÉ = 128
ùëÉ = 1
ùëÉ = 16
ùëÉ = 64
ùëÉ = 16

ùêæ = 256,

Post,

ùêæ = 1024,

Pre,

ùêæ = 1024,

Post,

36.3

37.0

41.2

Accuracy
35.7 ¬± 0.1
27.2 ¬± 0.5
34.0 ¬± 0.4
32.3 ¬± 0.8
38.0 ¬± 0.5
39.2 ¬± 0.5
39.7 ¬± 0.6
36.9 ¬± 0.7
39.6 ¬± 0.5
40.3 ¬± 0.6
38.1 ¬± 0.6
41.6 ¬± 0.6
41.7 ¬± 0.8
40.4 ¬± 0.6

Table 2. Recognition accuracy on Caltech 256, 30 training exam-
ples, for several methods using a single descriptor over grayscale.
For our method, results are shown as a function of whether clus-
tering is performed before (Pre) or after (Post) the encoding, ùêæ:
dictionary size, and ùëÉ : number of conÔ¨Åguration space bins.

using grayscale images ‚Äì although better performance has
been reported with color images (e.g., 78.5% ¬± 0.4 with
a saliency-based approach [22]), multiple descriptor types
(e.g., methods using multiple kernel learning have achieved
77.7% ¬± 0.3 [12], 78.0% ¬± 0.3 [2, 37] and 84.3% [40]
on Caltech-101 with 30 training examples), or subcategory
learning (83% on Caltech-101 [35]). On the Scenes bench-
mark, preclustering does improve results for small dictio-
naries (ùêæ ‚â§ 256, see supplemental material), but not for
larger ones (ùêæ = 1024). While our method outperforms
the Laplacian sparse coding approach [11] on the Caltech
256 dataset, our performance is much below that of Lapla-
cian sparse coding on the Scenes database.

4.1.2 Pre- vs. Post-Clustering

One advantage of using the same dictionary for all fea-
tures is that the clustering can be performed after the en-
coding. The instability of sparse coding could cause fea-
tures similar in descriptor space to be mapped to dissimilar
codes, which would then be pooled together. This does not
happen if clustering is performed on the codes themselves.
While pre-clustering may perform better for few clusters,
post-clustering yields better results when enough clusters
are used (ùëÉ ‚â• 64); a dictionary of size ùêæ = 1024 reaches
77.1 ¬± 0.7 accuracy on Caltech-101 with ùëÉ = 64 bins (to
be compared to 76.2 ¬± 0.8 when clustering before coding),
while a dictionary of size ùêæ = 256 yields 76.7 ¬± 0.8 with
ùëÉ = 128 bins (to be compared to 75.5¬±0.8 with precluster-
ing). Fig. 2 also shows that performance drops for larger ùëÉ ,
irrespective of whether the clustering is performed before
or after the encoding.

77

76

75

74

73

72

71

y
c
a
r
u
c
c
A

1

Dictionary
size

1024
256

Clustering

After coding
Before coding

4

256
Configuration bins, log scale

16

64

1024

Figure 2. Recognition accuracy on Caltech-101, for clustering be-
fore or after encoding. Clustering after the encoding generally per-
forms better; for both schemes, binning too Ô¨Ånely in conÔ¨Åguration
space (large ùëÉ ) hurts performance. Best viewed in color.

4.2. Gaining a Ô¨Åner understanding of local conÔ¨Ågu-

ration space pooling

In this section, we investigate how much local conÔ¨Ågu-
ration space pooling can enhance the performance of small
dictionaries, how it compares to learning one local dictio-
nary per conÔ¨Åguration bins, and what pooling and weight-
ing schemes work best in our pipeline.

4.2.1 Local pooling boosts small dictionaries

Fig. 3(a) shows results for various assignments of compo-
nents between atoms (ùêæ) and centroids (ùëÉ ). Pooling more
locally in conÔ¨Åguration space (ùëÉ > 1) can considerably
boost the performance of small dictionaries.

Unsurprisingly,

larger dictionaries consistently beat
smaller ones combined with preclustering-driven pooling,
at same total number of components; this can be seen from
the downwards slope of the gray dashed lines in Fig. 3(a)
linking data points at constant ùêæ ‚àó ùëÉ . However, if ùëÉ is
allowed to grow more, small dictionaries can outperform
larger ones. This leads to good performance with a small
dictionary; e.g., a dictionary of just ùêæ = 64 atoms cou-
pled with a preclustering along ùëÉ = 64 centroids achieves
73.0 ¬± 0.6% on Caltech-101.

4.2.2 Comparison with cluster-speciÔ¨Åc dictionaries

In addition to learning richer, more local dictionaries, learn-
ing one dictionary per cluster as done in [39, 42] inherently
leads to more local pooling. Experiments in this section
seek to disentangle these effects. As shown in Fig. 3(b),
more than half of the improvement compared to no preclus-
tering is usually due to the separate clustering rather than
more speciÔ¨Åc dictionaries. The smaller the dictionary, the

y
c
a
r
u
c
c
A

70

60

50

40

30

Dictionary

4 dics, 4 bins
1 dic, 4 bins
1 dic,
1 bin

y
c
a
r
u
c
c
A

70

60

50

40

30

20

10

0

Dictionary
size

1024
256
64
16
4

75

70

65

y
c
a
r
u
c
c
A

60

55

50

45

Pooling
Avg
Max

Weighting
1
Ni N
Ni N

1
Configuration bins, log scale

16

4

64

4

16
Dictionary size

64

256

1024

4

16

Dictionary size, log scale

64

256

(a)

(b)

(c)

Figure 3. Recognition accuracy on Caltech-101. Left: pooling locally in Ô¨Åner conÔ¨Åguration space bins can boost the performance of small
dictionaries. Dotted gray lines indicate constant product of dictionary size √ó number of conÔ¨Åguration bins. Middle: a substantial part of
the improvement observed when using multiple local dictionaries can be achieved without changing the encoding, by pooling locally in
conÔ¨Åguration space. ùëÉ = 4 conÔ¨Åguration space bins are used. Right: the best performance is obtained with max pooling and uniform
weighting. Max pooling consistently outperforms average pooling for all weighting schemes. With average pooling, weighting by the
square root of the cluster weight performs best. ùëÉ = 16 conÔ¨Åguration space bins are used. Results on the Caltech-256 and Scenes datasets
show similar patterns (see supplemental material). Best viewed in color.

larger the proportion of the improvement due to clustering.
This may be due to the fact that smaller dictionaries do not
have enough atoms to implicitly link activation of an atom
to cluster membership during coding, leaving more of that
task to the explicit local conÔ¨Åguration space pooling than
when large dictionaries are used.

4.2.3 Pooling operator and cluster weighting

When concatenating the vectors corresponding to each
pool, it is not clear whether they should be weighted accord-
ing to the prominence of the cluster, measured as the ratio
ùëÅùëñ/ùëÅ of the number ùëÅùëñ of inputs falling into cluster ùëñ, over
the total number ùëÅ of inputs. Denoting by ùë§ùëñ the weight
for cluster ùëñ, we compare three weighting schemes: iden-
tical weight (ùë§ùëñ = 1), a weight proportional to the square
root of the ratio (ùë§ùëñ =
ùëÅùëñ/ùëÅ) as proposed by Zhou et
al. [45], or the ratio itself (ùë§ùëñ = ùëÅùëñ/ùëÅ).

‚àö

As shown in Fig. 3(c), the weighting scheme assign-
ing the same weight to each cluster performs better when
max pooling is used, except for very small dictionaries.
When average pooling is used, the best weighting scheme
is the square root weighting, which empirically validates
the choice in [45], but performance is below that of max
pooling. Based on these results, max pooling with identical
weighting for all clusters has been used for all other experi-
ments in the paper.

5. Conclusion

While there is no question that making coding more sta-
ble and more speciÔ¨Åc is advantageous, the simple procedure

of clustering the data in order to make pooling local in con-
Ô¨Åguration space is a powerful tool for image recognition.
The main conclusions of this work are that (1) more local
conÔ¨Åguration space pooling in itself boosts performance,
dramatically so with smaller dictionaries; (2) it is advan-
tageous to use pyramids rather than grids, analogously to
spatial pooling; (3) with enough conÔ¨Åguration space bins,
better performance may be obtained when the clustering is
performed just before the pooling step, rather than before
the coding step; (4) performance drops if too many bins are
added.

Our ongoing efforts are focused on adapting the same
ideas to pooling across features when there is a topogra-
phy on the feature extractors, instead of simply pooling each
component separately. We hope that considerating locality
in all dimensions simultaneously (feature space, between
feature extractors, and space itself) will lead to even more
robust invariant recognition.

Acknowledgments. This work was supported by ONR
contract N00014-09-1-0473 and NSF grant EFRI/COPN-
0835878 to NYU, and by the European Research Council
(VideoWorld and Sierra grants).

References

[1] http://www.di.ens.fr/willow/SPAMS/. 5
[2] http://www.robots.ox.ac.uk/‚àºvgg/software/MKL/. 6
[3] O. Boiman, I. Rehovot, E. Shechtman, and M. Irani. In De-
In

fense of Nearest-Neighbor Based Image ClassiÔ¨Åcation.
CVPR, 2008. 5, 6

[4] Y. Boureau, F. Bach, Y. LeCun, and J. Ponce. Learning mid-

level features for recognition. In CVPR, 2010. 1, 2, 3, 5

[5] Y. Boureau, J. Ponce, and Y. LeCun. A theoretical analysis

of feature pooling in vision algorithms. In ICML, 2010. 3

[6] A. Buades, B. Coll, and J. Morel. A non-local algorithm for

image denoising. In CVPR, 2005. 1, 4

[7] C.-C. Chang and C.-J. Lin.

support vector machines, 2001.
http://www.csie.ntu.edu.tw/‚àºcjlin/libsvm. 5

LIBSVM: a library for
Software available at

[8] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image
denoising with block-matching and 3D Ô¨Åltering.
In Proc.
SPIE Electronic Imaging: Algorithms and Systems V, vol-
ume 6064, San Jose, CA, USA, January 2006. 1, 4

[9] N. Dalal and B. Triggs. Histograms of oriented gradients for

human detection. In CVPR, 2005. 1, 2

[10] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative vi-
sual models from few training examples. In CVPR Workshop
GMBV, 2004. 5

[11] S. Gao, I. Tsang, L. Chia, and P. Zhao. Local features are not
lonely‚ÄìLaplacian sparse coding for image classiÔ¨Åcation. In
CVPR, 2010. 1, 2, 4, 5, 6

[12] P. Gehler and S. Nowozin. On Feature Combination for Mul-

ticlass Object ClassiÔ¨Åcation. In ICCV, 2009. 6

[13] J. Goldberger, S. T. Roweis, G. E. Hinton, and R. Salakhut-
dinov. Neighbourhood components analysis. In NIPS, 2004.
4

[14] K. Grauman and T. Darrell. Pyramid match kernels: Dis-
In

criminative classiÔ¨Åcation with sets of image features.
ICCV, 2005. 4

[15] G. GrifÔ¨Ån, A. Holub, and P. Perona. Caltech-256 object cat-
egory dataset. Technical Report 7694, California Institute of
Technology, 2007. 5

[16] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality re-
duction by learning an invariant mapping. In CVPR, 2006.
4

[17] G. Hinton and R. R. Salakhutdinov. Reducing the dimen-
sionality of data with neural networks. Science, 313(5786),
2006. 2

[18] D. H. Hubel and T. N. Wiesel. Receptive Ô¨Åelds, binocular
interaction and functional architecture in the cat‚Äôs visual cor-
tex. J Physiol, 160:106‚Äì154, Jan 1962. 1

[19] P. Jain, B. Kulis, and K. Grauman. Fast image search for

learned metrics. In CVPR, 2008. 5

[20] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun.
What is the best multi-stage architecture for object recog-
nition? In ICCV, 2009. 2

[21] H. J¬¥egou, M. Douze, C. Schmid, and P. P¬¥erez. Aggregating
In

local descriptors into a compact image representation.
CVPR, 2010. 1, 3, 4

[22] C. Kanan and G. Cottrell. Robust classiÔ¨Åcation of objects,
faces, and Ô¨Çowers using natural image statistics. In CVPR,
2010. 6

[23] J. Kim and K. Grauman. Asymmetric Region-to-Image
Matching for Comparing Images with Generic Object Cat-
egories. In CVPR, 2010. 6

[24] J. Koenderink and A. Van Doorn. The structure of locally

orderless images. IJCV, 31(2/3):159‚Äì168, 1999. 1

[25] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of
features: Spatial pyramid matching for recognizing natural
scene categories. In CVPR, 2006. 1, 2, 4, 5

[26] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-
based learning applied to document recognition. Proceed-
ings of the IEEE, 86(11):2278‚Äì2324, November 1998. 1, 2
[27] H. Lee, A. Battle, R. Raina, and A. Y. Ng. EfÔ¨Åcient sparse

coding algorithms. In NIPS, 2006. 3

[28] D. Lowe. Distinctive image features from scale-invariant

keypoints. IJCV, 60(4):91‚Äì110, 2004. 1

[29] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online Dictio-

nary Learning for Sparse Coding. In ICML, 2009. 3

[30] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman.
In CVPR

Non-local sparse models for image restoration.
2009. 1, 4

[31] B. A. Olshausen and D. J. Field. Sparse coding with an over-
complete basis set: a strategy employed by V1? Vision Re-
search, 37:3311‚Äì3325, 1997. 3

[32] M. Ranzato, Y. Boureau, and Y. LeCun. Sparse feature learn-

ing for deep belief networks. In NIPS 2007, 2007. 2

[33] L. Saul and S. Roweis. Think globally, Ô¨Åt locally: unsuper-
vised learning of low dimensional manifolds. JMLR, 4:119‚Äì
155, 2003. 4

[34] J. Sivic and A. Zisserman. Video Google: A text retrieval

approach to object matching in videos. In ICCV, 2003. 2

[35] S. Todorovic and N. Ahuja. Learning subcategory relevances

for category recognition. In CVPR, 2008. 6

[36] J. C. van Gemert, C. J. Veenman, A. W. M. Smeulders,
and J. M. Geusebroek. Visual word ambiguity. PAMI,
32(7):1271‚Äì1283, 2010. 1, 3, 5, 6

[37] A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Mul-

tiple kernels for object detection. In ICCV, 2009. 6

[38] D. Wang, S. Hoi, and Y. He. An Effective Approach to
Pose Invariant 3D Face Recognition. Advances in Multime-
dia Modeling, pages 217‚Äì228, 2011. 3

[39] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong.
Locality-constrained linear coding for image classiÔ¨Åcation.
In CVPR, 2010. 1, 2, 4, 5, 6

[40] J. Yang, Y. Li, Y. Tian, L. Duan, and W. Gao. Group-sensitive
multiple kernel learning for object categorization. In ICCV,
2009. 6

[41] J. Yang, K. Yu, Y. Gong, and T. Huang. Linear Spatial Pyra-
mid Matching Using Sparse Coding for Image ClassiÔ¨Åcation.
In CVPR, 2009. 1, 3, 5, 6

[42] J. Yang, K. Yu, and T. Huang. EfÔ¨Åcient Highly Over-
Complete Sparse Coding using a Mixture Model. ECCV,
2010. 1, 2, 3, 4, 6

[43] K. Yu, T. Zhang, and Y. Gong. Nonlinear learning using local

coordinate coding. NIPS, 2009. 1, 2, 4

[44] H. Zhang, A. C. Berg, M. Maire, and J. Malik. SVM-KNN:
Discriminative nearest neighbor classiÔ¨Åcation for visual cat-
egory recognition. In CVPR, 2006. 5

[45] X. Zhou, K. Yu, T. Zhang, and T. Huang. Image ClassiÔ¨Åca-
tion using Super-Vector Coding of Local Image Descriptors.
ECCV, 2010. 1, 4, 7

[46] X. Zhou, X. D. Zhuang, H. Tang, M. H. Johnson, and T. S.
Huang. A novel gaussianized vector representation for natu-
ral scene categorization. In ICPR, 2008. 5

