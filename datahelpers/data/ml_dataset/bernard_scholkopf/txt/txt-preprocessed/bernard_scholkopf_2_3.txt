Abstract

A method is described which, like the kernel trick in support vector ma-
chines (SVMs), lets us generalize distance-based algorithms to operate
in feature spaces, usually nonlinearly related to the input space. This
is done by identifying a class of kernels which can be represented as
norm-based distances in Hilbert spaces. It turns out that common kernel
algorithms, such as SVMs and kernel PCA, are actually really distance
based algorithms and can be run with that class of kernels, too.
As well as providing a useful new insight into how these algorithms
work, the present work can form the basis for conceiving new algorithms.

1 Introduction

One of the crucial ingredients of SVMs is the so-called kernel trick for the computation of
dot products in high-dimensional feature spaces using simple functions dened on pairs of
input patterns. This trick allows the formulation of nonlinear variants of any algorithm that
can be cast in terms of dot products, SVMs being but the most prominent example [13, 8].
Although the mathematical result underlying the kernel trick is almost a century old [6], it
was only much later [1, 3, 13] that it was made fruitful for the machine learning community.
Kernel methods have since led to interesting generalizations of learning algorithms and to
successful real-world applications. The present paper attempts to extend the utility of the
kernel trick by looking at the problem of which kernels can be used to compute distances
in feature spaces. Again, the underlying mathematical results, mainly due to Schoenberg,
have been known for a while [7]; some of them have already attracted interest in the kernel
methods community in various contexts [11, 5, 15].

is the set of possible
is some nonempty set (the domain) that
for previously

Let us consider training data 	





Here,
outputs (e.g., in pattern recognition,  ! ), and 
the patterns are taken from. We are interested in predicting the outputs 
unseen patterns 

. This is only possible if we have some measure that tells us how "#$%


is related to the training examples. For many problems, the following approach works:
informally, we want similar inputs to lead to similar outputs. To formalize this, we have to
state what we mean by similar. On the outputs, similarity is usually measured in terms of
a loss function. For instance, in the case of pattern recognition, the situation is simple: two
outputs can either be identical or different. On the inputs, the notion of similarity is more
complex. It hinges on a representation of the patterns and a suitable similarity measure
operating on that representation.

One particularly simple yet surprisingly useful notion of (dis)similarity  the one we will
use in this paper  derives from embedding the data into a Euclidean space and utilizing
geometrical concepts. For instance, in SVMs, similarity is measured by dot products (i.e.
. Formally, the patterns are

angles and lengths) in some high-dimensional feature space 
rst mapped into using


, one tries to
pick a feature space in which the dot product can be evaluated directly using a nonlinear
function in input space, i.e. by means of the kernel trick


. To avoid working in the potentially high-dimensional space 


 and then compared using a dot product



"







"

"#$




"









(1)

facilitates a number of algorithmic and theoretical issues. It
is well established that (1) works out for Mercer kernels [3, 13], or, equivalently, positive

Often, one simply chooses a kernel
with the property that there exists some 
the above holds true, without necessarily worrying about the actual form of  already the
existence of the linear space




denite kernels [2, 14]. Here and below, indices and by default run over
Denition 1 (Positive denite kernel) A symmetric function
all 


 we have

gives rise to a positive denite Gram matrix, i.e. for which for all

 which for

such that

.

#"$% '&)(% where "$%

"*$+




(2)



!

is called a positive denite (pd) kernel.

One particularly intuitive way to construct a feature map satisfying (1) for such a kernel

proceeds, in a nutshell, as follows (for details, see [2]):

1. Dene a feature map


denotes the space of functions mapping 

-.0/

,


into 

Here, 

2. Turn it into a linear space by forming linear combinations




.

(3)

2!







43






$





5


.;


8
95:

CD

76



$





!
76
3. Endow it with a dot product 
!
<!

, and turn it into a
Hilbert space =?> by completing it in the corresponding norm.
Note that in particular, by denition of the dot product, 

A
$



, hence,
B


 , the kernel trick. This shows that pd
in view of (3), we have


"
*	#GF
measures, the canonical dot product #$B	E , 
HLK .
there also exist generalizations of the simplest dissimilarity measure, the distance H
in the feature space associated with a pd kernel
can
Clearly, the distance HM

ON
be computed using the kernel trick (1) as

. Positive denite

kernels can be thought of as (nonlinear) generalizations of one of the simplest similarity
. The question arises as to whether

kernels are, however, not the full story: there exists a larger class of kernels that can be
used as generalized distances, and the following section will describe why.



$*
@

"#$



GIQP

"B	"


GI

J


+I

(4)

HMK

"



"



"

$



"

"

2 Kernels as Generalized Distance Measures

Let us start by considering how a dot product and the corresponding distance measure are
is translation invariant

affected by a translation of the data, 

'I

'I

*

BR . Clearly, H

HLK


































/
1















3
8


1



;

3

8

















"

HMK


R

?I

*	
$"$I

is not. A short calculation shows that the effect of the translation can be

H#K as

A
*	 , still a pd kernel:
H .

while "
expressed in terms of H
?I


"
Note that this is, just like "
( . For any choice of 
?I
associated with the dissimilarity measure H
substitute instead of H
Denition 2 (Conditionally positive denite kernel) A symmetric function
 which satises (2) for all 

HMK on the right hand side of (5) to ensure that the left hand side
-

This naturally leads to the question whether (5) might suggest a connection that holds true
also in more general cases: what kind of nonlinear dissimilarity measure do we have to

becomes positive denite? The answer is given by a known result. To state it, we rst need
to dene the appropriate class of kernels.

, we thus get a similarity measure (5)

C with

K

":



$"







@


R


R

and for all 

(5)

(6)

2!

(%

is called a conditionally positive denite (cpd) kernel.

Proposition 3 (Connection pd  cpd [2]) Let 
R
on 

GI

#$
is positive denite if and only if

. Then
"


GI





"

is conditionally positive denite.

, and let
be a symmetric kernel
$

(7)

$


$


"


4N

"

The proof follows directly from the denitions and can be found in [2].

This result does generalize (5):

the negative squared distance kernel is indeed cpd, for

implies I,;
A
"*$$+



kernels of the form

HLK
A

"*$+


#
.BI

?I

I,;

(

HLK



(
H
is cpd, then so are I

8


HLK

In fact, this implies that all

HLK

(8)

are cpd (they are not pd), by application of the following result:

9I




((


.

)

still be at least cpd. For further examples of cpd kernels, cf. [2, 14, 4, 11].

To state another class of cpd kernels that are not pd, note rst that as trivial consequences

Proposition 4 ([2]) If
and I
of Denition 2, we know that (i) sums of cpd kernels are cpd, and (ii) any constant
is cpd. Therefore, any kernel of the form
particular, since pd kernels are cpd, we can take any pd kernel and offset it by and it will
and dene 
according to (7). Due to Proposition 3, 
the feature map for

A
-
*

,
Hilbert space representation,



"
H#

 , where
is cpd and
. To this end, x 
R



$

(cf. (1)), satisfying 

*N


We now return to the main ow of the argument. Proposition 3 allows us to construct

from that of the pd kernel 

is positive denite. Therefore, we may employ the

of 

4I

"*

JI,P

, is also cpd. In

#




A

hence


4I

(9)






"

"

"

"

"

"


I


R

I

R

P

I
H


H
K
N
H
H
K
N
H

R
I


H

;







I
I
H
;




I

R


&
R



I























P




R

R


R
R
;




(







H


I






;



H


I
;



;



H


N
P
;





P
;







P
H
;



&
(


I
H


P





3


I



N







=









H
K



I














Substituting (7) yields

We thus have proven the following result.

I

"




JN

"




ON

"

$





(10)




0I

HM

"

"

"





HM


-(

-




, such that

be a real-
.
, and a mapping

for all 

Proposition 5 (Hilbert space representation of cpd kernels [7, 2]) Let

, satisfying


"

ON

valued conditionally positive denite kernel on 
Then there exists a Hilbert space =
,

0I
I
"

A
If we drop the assumption
I
D(

of real-valued functions on 
"
( , the Hilbert space representation reads

JN
"

HM
for all 
is a semi-metric; it is a metric if


0I
"
$
"#$B	"

It can be shown that if


for 

HM
intuitive understanding of Proposition 3: we can then write 

as 
"
R
corresponds to the image 

We next show how to represent general symmetric kernels (thus in particular cpd kernels)
as symmetric bilinear forms
in feature spaces. This generalization of the previously
known feature space representation for pd kernels comes at a cost:
will no longer be
a dot product. For our purposes, we can get away with this. The result will give us an



. Proposition 3 thus essentially adds an origin in feature space which

of one point 
R under the feature map. For translation

invariant algorithms, we are always allowed to do this, and thus turn a cpd kernel into a pd
one  in this sense, cpd kernels are as good as pd kernels.

(11)

(12)

#$


,






[2].

#$*

#*

"*	"


"
"


"
"

, then


R


GI






"

"

"

that

be a real-
of real-valued functions
, such

Proposition 6 (Vector space representation of symmetric kernels) Let

, endowed with a symmetric bilinear form

valued symmetric kernel on 
on 













. Then there exists a linear space=
"


, and a mapping 



"
76
!

, hence it is independent of8

76
!




Proof The proof is a direct modication of the pd case. We use the map (3) and linearly
complete the image as in (4). Dene
is well-dened, although it explicitly contains the expansion coefcients (which need not
be unique), note that
,
note that
show that

"
$*

. To see that it
2!
 . Similarly, for 5

, independent of the 3
. The last two equations also

is bilinear; clearly, it is symmetric.

95%


95%

5"

95%


(13)

,


"

"






Note, moreover, that by denition of

is a reproducing kernel for the feature space

particular,

(4), we have

$*	"



$


#$


(which is not a Hilbert space): for all functions 1
Rewriting 

as 

"*	"
I
in feature space  points that do not have a preimage BR
of individual points. This is taken care of by the constraint on the sum of the 

eralization of Proposition 3: in practice, we might want to choose other points as origins
in input space, such as (usually)
the mean of a set of points (cf. [12]). This will be useful when considering kernel PCA.
Crucial is only that our reference points behaviour under translations is identical to that
in the


; in


suggests an immediate gen-

following proposition. The asterisk denotes the complex conjugated transpose.











"

"





H
K



P










=




H
K






H
K



P











I



H

(













I


I

R







=









1


;


;

3

8





1

;

8

1



1
;

3









1



1















I

R


R

(14)

C

Proof

the

be the vector of

is conditionally positive denite.


. Then

be a symmetric matrix,

satisfy
identity matrix, and let
	


9"

Proposition 7 (Exercise 2.23, [2]) Let "
all ones,
is positive denite if and only if "
: suppose 


is positive denite, i.e. for any
	

In the case
that "

 : suppose "

the orthogonal complement of
	
is a projection. Thus 
	

Moreover, being symmetric and satisfying 
to the orthogonal complement of , and
is the restriction of "
by denition of conditional positive deniteness, that is precisely the space where "



(cf. (6)), the three last terms vanish, i.e. (

, which can be seen by computing, for any
	

is conditionally positive denite. The map 

 proving

has its range in

is conditionally positive denite.


, the map 

positive denite.

	




, we have





(15)

(16)

is

,

This result directly implies a corresponding generalization of Proposition 3:

O



<!

0I

. Then


"

satisfy ;

"

Proposition 8 (Adding a general origin) Let
be a symmetric kernel, #


$
and let 


#$
<!
is positive denite if and only if
Proof Consider a set of points B


tion 7 using 

C
D*
6 ,
N)



$


Gram matrix based on 




is conditionally positive denite.




*
( .







"*$+

N)

"
$




OI


JN

!

<!

, and let "
6 . Apply Proposi-

be the

,

(17)

Example 9 (SVMs and kernel PCA) (i) The above results show that conditionally posi-
tive denite kernels are a natural choice whenever we are dealing with a translation in-
variant problem, such as the SVM: maximization of the margin of separation between two
classes of data is independent of the origins position. Seen in this light, it is not surprising
that the structure of the dual optimization problem (cf. [13]) allows cpd kernels: as noticed

in [11, 10], the constraint ;

<!

denition of conditionally positive denite kernels.

( projects out the same subspace as (6) in the



(ii) Another example of a kernel algorithm that works with conditionally positive denite
kernels is kernel PCA [9], where the data is centered, thus removing the dependence on the

origin in feature space. Formally, this follows from Proposition 7 for 

.

4







"




I



I



"

(




"




"

N



"


I


"


I



"




(

"


I





I








I





(


I


K



I

I


"














P













































6



3



Example 10 (Parzen windows) One of the simplest distance-based classication algo-

the latter and the two classes, and assign it to the one where this mean is smaller,


, we compute the mean squared distances between

We use the distance kernel trick (Proposition 5) to express the decision function as a kernel
expansion in input space: a short calculation shows that

"


0I

"*

rithms conceivable proceeds as follows. Given 
labelled with I
, and a test point 
sgn
"
H#
!



sgn

#

with the constant offset
that for some cpd kernels, such as (8),
the commonly used Gaussian kernel,

B$*



vanishes.





GI

"

!

 points labelled with N

"*


0I

HM





,  points
K

(18)

#$



4N

(19)

!



$*

. Note
( . For others, such as
is always( , thus
is a nonzero constant, in which case  also


$*

For normalized Gaussians and other kernels that are valid density models, the resulting
decision boundary can be interpreted as the Bayes decision based on two Parzen windows
density estimates of the classes; for general cpd kernels, the analogy is a mere formal one.

Example 11 (Toy experiment) In Fig. 1, we illustrate the nding that kernel PCA can be
carried out using cpd kernels. We use the kernel (8). Due to the centering that is built into

P actually is equivalent to linear
kernel PCA (cf. Example 9, (ii), and (5)), the case 8
PCA. As we decrease8
Note, moreover, that as the kernel parameter 8 gets smaller, less weight is put on large

distances, and we get more localized feature extractors (in the sense that the regions where
they have large gradients, i.e. dense sets of contour lines in the plot, get more localized).

, we obtain increasingly nonlinear feature extractors.

Figure 1: Kernel PCA on a toy dataset using the cpd kernel (8); contour plots of the feature
extractors corresponding to projections onto the rst two principal axes in feature space.

extractors increasingly nonlinear, which allows the identication of the cluster structure.

%

(%
. Notice how smaller values of 8 make the feature

From left to right: 8











H
K
I


!




H





!















P
;






I

P




;


!











P




3 Conclusion

We have described a kernel trick for distances in feature spaces. It can be used to generalize
all distance based algorithms to a feature space setting by substituting a suitable kernel
function for the squared distance. The class of kernels that can be used is larger than
those commonly used in kernel methods (known as positive denite kernels). We have
argued that this reects the translation invariance of distance based algorithms, as opposed
to genuinely dot product based algorithms. SVMs and kernel PCA are translation invariant
in feature space, hence they are really both distance rather than dot product based. We
thus argued that they can both use conditionally positive denite kernels. In the case of
the SVM, this drops out of the optimization problem automatically [11], in the case of
kernel PCA, it corresponds to the introduction of a reference point in feature space. The
contribution of the present work is that it identies translation invariance as the underlying
reason, thus enabling us to use cpd kernels in a much larger class of kernel algorithms, and
that it draws the learning communitys attention to the kernel trick for distances.

Acknowledgments. Part of the work was done while the author was visiting the Aus-
tralian National University. Thanks to Nello Cristianini, Ralf Herbrich, Sebastian Mika,
Klaus Muller, John Shawe-Taylor, Alex Smola, Mike Tipping, Chris Watkins, Bob
Williamson, Chris Williams and a conscientious anonymous reviewer for valuable input.

