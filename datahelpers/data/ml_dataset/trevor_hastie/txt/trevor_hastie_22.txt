PredictionbySupervisedPrincipalComponentsEricBAIR,TrevorHASTIE,DebashisPAUL,andRobertTIBSHIRANIInregressionproblemswherethenumberofpredictorsgreatlyexceedsthenumberofobservations,conventionalregressiontechniquesmayproduceunsatisfactoryresults.Wedescribeatechniquecalledsupervisedprincipalcomponentsthatcanbeappliedtothistypeofproblem.Supervisedprincipalcomponentsissimilartoconventionalprincipalcomponentsanalysisexceptthatitusesasubsetofthepredictorsselectedbasedontheirassociationwiththeoutcome.Supervisedprincipalcomponentscanbeappliedtoregressionandgeneralizedregres-sionproblems,suchassurvivalanalysis.Itcomparesfavorablytoothertechniquesforthistypeofproblem,andcanalsoaccountfortheeffectsofothercovariatesandhelpidentifywhichpredictorvariablesaremostimportant.Wealsoprovideasymptoticconsistencyresultstohelpsupportourempiricalﬁndings.ThesemethodscouldbecomeimportanttoolsforDNAmicroarraydata,wheretheymaybeusedtomoreaccuratelydiagnoseandtreatcancer.KEYWORDS:Geneexpression;Microarray;Regression;Survivalanalysis.1.INTRODUCTIONInthisarticlewestudyamethodforpredictinganoutcomevariableYfromasetofpredictorvariablesX1,X2,...,Xp,mea-suredoneachofNindividuals.Inthetypicalscenariothatwehaveinmind,thenumberofmeasurementspismuchlargerthanN.Intheexamplethatmotivatedourwork,X1,X2,...,XparegeneexpressionmeasurementsfromDNAmicroarrays.TheoutcomeYmightbeaquantitativevariablethatwemightas-sumetobenormallydistributed.Morecommonlyinmicroarraystudies,Yisasurvivaltime,subjecttocensoring.Oneapproachtothiskindofproblemwouldbeasupervisedpredictionmethod.Forexample,wecoulduseaformofre-gressionapplicablewhenp>N;partialleastsquares(Wold1975)wouldbeonereasonablechoice,aswouldridgeregres-sion(HoerlandKennard1970).However,Figure1illustrateswhyasemisupervisedapproachmaybemoreeffective.Weimaginethattherearetwocelltypes,andthatpatientswiththegoodcell(2)typelivelongerontheaverage.However,thereisconsiderableoverlapinthetwosetsofsurvivaltimes.Wemightthinkofsurvivaltimeasa“noisysurrogate”forcelltype.Afullysupervisedapproachwouldgivethemostweighttothosegeneshavingthestrongestrelationshipwithsurvival.Thesegenesarepartially,butnotperfectly,relatedtocelltype.Ifwecouldinsteaddiscovertheunderlyingcelltypesofthepatients(oftenreﬂectedbyasizeablesignatureofgenesactingtogetherinpathways),thenwewoulddoabetterjobofpredict-ingpatientsurvival.NowwecanextractinformationaboutimportantcelltypesfromboththerelationshipbetweenYandX1,X2,...,Xpandthecorrelationamongthepredictorsthemselves.Principalcom-ponentsanalysis(PCA)isastandardmethodformodelingcorrelation.Regressionontheﬁrstfewprincipalcomponentswouldseemlikeanaturalapproach,butthismightnotalwaysEricBairisPost-DoctoralFellow,DepartmentofStatistics,StanfordUni-versity,Stanford,CA94305,andDepartmentofNeurology,UCSF(E-mail:ebair@stat.stanford.edu).TrevorHastieisProfessor,DepartmentsofSta-tisticsandHealth,Research&Policy(E-mail:hastie@stat.stanford.edu),DebashisPaulisaGraduateStudent,DepartmentofStatistics(E-mail:debashis@stat.stanford.edu),andRobertTibshiraniisProfessor,DepartmentofStatisticsandHealth,Research&Policy(E-mail:tibs@stat.stanford.edu),StanfordUniversity,Stanford,CA94305.Theauthorsthanktheeditor,twoassociateeditors,andrefereesforsuggestionsthatsubstantiallyimprovedthisarticle.BairwassupportedinpartbyaNationalScienceFoundationgradu-ateresearchfellowship.TibshiraniwassupportedinpartbyNationalScienceFoundationgrantDMS-99-71405andNationalInstitutesofHealthcontractN01-HV-28183.HastiewassupportedinpartbyNationalScienceFoundationgrantDMS-02-04612andNationalInstitutesofHealthgrant2R01CA72028-07.workwell.TheﬁctitiousdatagiveninFigure2illustratetheproblem(ifweweretouseonlythelargestprincipalcompo-nent).Thisisaheatmapdisplaywitheachgenerepresentedbyarowandeachcolumncontainingdatafromonepatientononemicroarray.Geneexpressioniscodedfromblue(low)toyellow(high).Inthisexample,thelargestvariationisseeninthegenesmarkedA,withthesecondsetof10patientshav-inghigherexpressioninthesegenesthantheﬁrst10.ThesetofgenesmarkedBshowdifferentvariation,withthesecondandfourthblocksofpatientshavinghigherexpressioninthesegenes.Theremainderofthegenesshownosystematicvaria-tion.Atthebottomofthedisplay,theredpointsaretheﬁrsttwosingularvectorsu1andu2(principalcomponents)ofthematrixofexpressionvalues.Inmicroarraystudiesthesearesometimescalled“eigengenes”(Alter,Brown,andBotstein2000).(Thebrokenlinesrepresentthe“true”groupingmechanismthatgen-eratedthedatainthetwogroups.)NowifthegenesinAarestronglyrelatedtotheoutcomeY,thenYwillbehighlycor-relatedwiththeﬁrstprincipalcomponent.Inthisinstancewewouldexpectamodelthatusesu1topredictYtobeveryeffec-tive.However,thevariationingenesAmightreﬂectsomebio-logicalprocessthatisunrelatedtotheoutcomeY.Inthatcase,Ymightbemorehighlycorrelatedwithu2orsomehigher-orderprincipalcomponent.Thesupervisedprincipalcomponentstechniquethatwedescribeinthisarticleisdesignedtouncoversuchstructureautomatically.Thistechniquewasdescribedinabiologicalset-tingbyBairandTibshirani(2004)inthecontextofarelatedmethodknownas“supervisedclustering.”Thesupervisedprin-cipalcomponentideaissimple:Ratherthanperformingprinci-palcomponentanalysisusingallofthegenesinadataset,weuseonlythosegeneswiththestrongestestimatedcorrelationwithY.InthescenarioofFigure2,ifYwerehighlycorre-latedwiththesecondprincipalcomponentu2,thenthegenesinblockBwouldhavethehighestcorrelationwithY.Hencewewouldcomputetheﬁrstprincipalcomponentusingjustthesegenes,andthiswouldyieldu2.Asthisexampleshows,usingprincipalcomponentshelpsun-covergroupsofgenesthatexpresstogether.Biologically,oneormorecellularprocesses,accompaniedbytheircadreofexpress-inggenes,determinethesurvivaloutcome.Thissamemodelunderliesotherapproachestosupervisedlearninginmicroarraystudies,includingsupervisedgeneshaving(Hastieetal.2000)©2006AmericanStatisticalAssociationJournaloftheAmericanStatisticalAssociationMarch2006,Vol.101,No.473,TheoryandMethodsDOI10.1198/016214505000000628119120JournaloftheAmericanStatisticalAssociation,March2006Figure1.UnderlyingConceptualModel.Therearetwocelltypes;patientswiththegoodcelltypelivelongeronaverage.However,thereisconsiderableoverlapinthetwosetsofsurvivaltimes.Henceitcouldbeadvantageoustotrytouncoverthecelltypesandusethesetopredictsurvivaltime,ratherthantopredictsurvivaltimedirectly.andtreeharvesting(Hastie,Tibshirani,Botstein,andBrown2001).Thesupervisedprincipalcomponentsprocedurecanbeviewedasasimplewaytoidentifytheclustersofrelevantpre-dictorsbyselectionbasedonscorestoremovetheirrelevantsourcesofvariationandapplicationofprincipalcomponentstoidentifythegroupsofcoexpressinggenes.Asfarasweknow,BairandTibshirani(2004)weretheﬁrsttodiscusstheideaofsupervisedprincipalcomponentsindetail.Butotherauthorshavepresentedrelatedideas.Ghosh(2002)prescreenedgenesbeforeextractingprincipalcomponents,butseemedtodosoforcomputationalreasons.Jiangetal.(2004)usedasimilarideainthecontextofmergingtheresultsfromtwodifferentdatasets.NguyenandRocke(2002)andHiandGui(2004)discussedpartialleastsquares(PLS)approachestosurvivalpredictionfrommicroarraydata.Aswediscussinthisarticle,thisisarelatedbutdifferentmethod,andPLSdidnotperformaswellassupervisedprincipalcomponentsinourtests.PLSdoesnotdoaninitialthresholdingoffeatures,andthisisthekeyaspectofourprocedurethatunderliesitsgoodperfor-mance.Inthenextsectionwedeﬁnethesupervisedprincipalcom-ponentsprocedure.Section3givesabriefsummaryofourcon-sistencyresults,andSection4discussesanimportancemeasureforindividualfeaturesandareducedmodel.Section5givesanexamplefromalymphomastudy,Section6discussesalterna-tiveapproachestosemisupervisedprediction,including“geneshaving,”andSection7presentsasimulationstudycomparingthevariousmethods.Section8summarizestheresultsofsuper-visedprincipalcomponentsonsomesurvivalstudies.Section9givesdetailsofthetheoreticalresults.Thearticleconcludeswithsomegeneralizations,includingcovariateadjustmentandtheuseofunlabeleddatainSection10andadiscussionoflim-itationsandfutureworkinSection11.TheAppendixcontainsdetailsofsomeproofsforSection9.2.SUPERVISEDPRINCIPALCOMPONENTS2.1DescriptionWeassumethattherearepfeaturesmeasuredonNobserva-tions(e.g.,patients).LetXbeanN×pmatrixoffeaturemea-surements(e.g.,genes),andletybetheN-vectorofoutcomemeasurements.Weassumethattheoutcomeisaquantitativevariable;wediscussothertypesofoutcomes,suchascensoredsurvivaltimes.Hereinanutshellisthesupervisedprincipalcomponentproposal:1.Compute(univariate)standardregressioncoefﬁcientsforeachfeature.2.Formareduceddatamatrixconsistingofonlythosefea-tureswhoseunivariatecoefﬁcientexceedsathresholdθinabsolutevalue(θisestimatedbycross-validation).3.Computetheﬁrst(orﬁrstfew)principalcomponentsofthereduceddatamatrix.4.Usetheseprincipalcomponent(s)inaregressionmodeltopredicttheoutcome.Wenowgivedetailsofthemethod.AssumethatthecolumnsofX(variables)havebeencenteredtohavemean0.Writethesingularvaluedecomposition(SVD)ofXasX=UDVT,(1)whereU,D,andVareN×m,m×m,andm×p,andm=min(N−1,p)istherankofX.HereDisadiagonalmatrixcontainingthesingularvaluesdj;andthecolumnsofUaretheprincipalcomponentsu1,u2,...,um;theseareassumedtobeordered,sothatd1≥d2≥···≥dm≥0.Letsbethep-vectorofstandardizedregressioncoefﬁcientsformeasuringtheunivariateeffectofeachgeneseparatelyony,sj=xTjy(cid:2)xj(cid:2),(2)with(cid:2)xj(cid:2)=√xTjxj.Actually,ascaleestimateˆσismiss-ingineachofthesj’s,butbecauseitiscommontoall,wecanomitit.LetCθbethecollectionofindicessuchthat|sj|>θ.WedenotebyXθthematrixconsistingofthecolumnsofXcorrespondingtoCθ.TheSVDofXθisXθ=UθDθVTθ.(3)LettingUθ=(uθ,1,uθ,2,...,uθ,m),wecalluθ,1theﬁrstsu-pervisedprincipalcomponentofX,andsoon.Wenowﬁtaunivariatelinearregressionmodelwithresponseyandpredic-toruθ,1,ˆyspc,θ=¯y+ˆγ·uθ,1.(4)Notethatbecauseuθ,1isaleftsingularvectorofXθ,ithasmean0andunitnorm.Henceˆγ=uTθ,1y,andtheinter-ceptis¯y,themeanofy(expandedhereasavectorofsuchmeans).Weusecross-validationofthelog-likelihood(orlogpartial-likelihood)ratiostatistictoestimatethebestvalueofθ.Inmostexamplesinthisarticleweconsideronlytheﬁrstsupervisedprincipalcomponent;intheexamplesofSection8,weallowthepossibilityofusingmorethanonecomponent.Notethat,from(3),Uθ=XθVθD−1θ=XθWθ.(5)So,forexample,uθ,1isalinearcombinationofthecolumnsofXθ:uθ,1=Xθwθ,1.Henceourlinearregressionmodelesti-matecanbeviewedasarestrictedlinearmodelestimateusingallofthepredictorsinXθ,ˆyspc,θ=¯y+ˆγ·Xθwθ,1(6)=¯y+Xθˆβθ,(7)Bairetal.:SupervisedPrincipalComponents121Figure2.FictitiousMicroarrayDataforIllustration.Aheatmapdisplaywitheachgenerepresentedbyarow,andeachcolumngivingthedatafromonepatientononemicroarray.Geneexpressioniscodedfromblue(low)toyellow(high).ThelargestvariationisseeninthegenesmarkedA,withthesecondsetof10patientshavinghigherexpressioninthesegenes.ThesetofgenesmarkedBshowdifferentvariation,withthesecondandfourthblocksofpatientshavinghigherexpressioninthesegenes.Atthebottomofthedisplayareshowntheﬁrsttwosingularvectors(principalcomponents)ofthematrixofexpressionvalues(redpoints),andtheactualgroupinggeneratorsforthedata(dashedlines).Iftheoutcomeishighlycorrelatedwitheitherprincipalcomponent,thenthesupervisedprincipalcomponenttechniquewilldiscoverthis.whereˆβθ=ˆγwθ,1.Infact,bypaddingwθ,1with0’s(corre-spondingtothegenesexcludedbyCθ),ourestimateislinearinallpgenes.Givenatestfeaturevectorx∗,wecanmakepredictionsfromourregressionmodelasfollows:1.Centereachcomponentofx∗usingthemeanswederivedonthetrainingdata,x∗j←x∗j−¯xj.2.ˆy∗=¯y+ˆγ·x∗θTwθ,1=¯y+x∗θTˆβθ,wherex∗θistheappropriatesubvectorofx∗.Inthecaseofuncorrelatedpredictors,itiseasytoverifythatthesupervisedprincipalcomponentsprocedurehasthedesiredbehavior.Ityieldsallpredictorswhosestandardizedunivariatecoefﬁcientsexceedθinabsolutevalue.Ourproposalisalsoapplicabletogeneralizedregressionset-tings,forexample,survivaldata,classiﬁcationproblems,ordatatypicallyanalyzedbyageneralizedlinearmodel.Inthesecasesweuseascorestatisticinplaceofthestandardizedre-gressioncoefﬁcientsin(2)anduseaproportionalhazardsorappropriategeneralizedregressionin(4).Let(cid:4)j(β)bethelog-122JournaloftheAmericanStatisticalAssociation,March2006likelihoodorpartiallikelihoodrelatingthedataforasinglepre-dictorXjandtheoutcomey,andletUj(β0)=d(cid:4)/dβ|β=β0andIj(β0)=−d2(cid:4)j/dβ2|β=β0.Thenthescorestatisticforpredic-torjhastheformsj=Uj(0)2Ij(0).(8)Ofcourse,fortheGaussianlog-likelihood,thisquantityisequivalenttothestandardizedregressioncoefﬁcient(2).Onecouldconsideriteratingthesupervisedprincipalcom-ponentsprocedure.Thuswewouldﬁndfeatureswhoseinnerproductwiththecurrentsupervisedprincipalcomponentswaslargest,usethosefeaturestocomputethenewprincipalcom-ponents,andsoon.Butthisprocedurewilltendtoconvergetotheusual(unsupervised)principalcomponents,becausethereisnothingtokeepitclosetotheoutcomeaftertheﬁrststep.Aniterativeprocedurewouldmakesenseonlyifitwerebasedonacriterioninvolvingboththevarianceofthefeaturesandthegoodnessofﬁttotheoutcome.Weconsidersuchacriterioninthenextsection,althoughweultimatelydonotpursueit(forreasonsgiventhere).2.2AnUnderlyingModelWenowconsideramodeltosupportthesupervisedprinci-palcomponentsmethod.Supposethatwehavearesponsevari-ableYthatisrelatedtoanunderlyinglatentvariableUbyalinearmodel,Y=β0+β1U+ε.(9)Inaddition,wehaveexpressionmeasurementsonasetofgenesXjindexedbyj∈P,forwhichXj=α0j+α1jU+j,j∈P.(10)Theerrorsεandjareassumedtohavemean0andareinde-pendentofallotherrandomvariablesintheirrespectivemodels.WealsohavemanyadditionalgenesXk,k/∈P,whicharein-dependentofU.WecanthinkofUasadiscreteorcontinuousaspectofacelltype,whichwedonotmeasuredirectly.Prepre-sentsasetofgenescomprisingapathwayorprocessassociatedwiththiscelltype,andtheXj’sarenoisymeasurementsoftheirgeneexpression.WewouldliketoidentifyP,estimateU,andhenceﬁtthepredictionmodel(9).Thisisaspecialcaseofala-tentstructuremodelorsingle-componentfactoranalysismodel(Mardia,Kent,andBibby1979).Thesupervisedprincipalcomponentsalgorithm(SPCA)canbeseenasamethodforﬁttingthismodel:1.ThescreeningstepestimatesthesetPbyˆP=Cθ.2.GivenˆP,theSVDofXθestimatesUin(10)bythelargestprincipalcomponentuθ,1.3.Finally,theregressionﬁt(4)estimates(9).Step1isnatural,becauseonaveragetheregressioncoefﬁ-cientSj=XTjY/(cid:2)Xj(cid:2)isnon-0onlyifα1jisnon-0(assumingthatthegeneshavebeencentered).Hencethisstepshouldse-lectthegenesj∈P.Step2isnaturalifweassumethattheer-rorsjhaveaGaussiandistribution,withthesamevariance.InthiscasetheSVDprovidesthemaximumlikelihoodestimatesforthesingle-factormodel(Mardiaetal.1979).Theregressioninstep3isanobviousﬁnalstep.Infact,givenP,themodeldeﬁnedby(9)and(10)isaspecialstructuredcaseofanerrors-in-variablesmodel(Miller1986;HuffelandLemmerling2002).Onecouldsetupajointopti-mizationcriterion,minβ0,β1,{α0,j,α1,j},u1,...,uN(cid:1)Ni=1(yi−β0−β1ui)2σ2Y+(cid:2)j∈P(cid:1)Ni=1(xij−α0j−α1jui)2σ2X.(11)Thenitiseasytoshowthat(11)canbesolvedbyanaugmentedandweightedSVDproblem.Indetail,weformtheaugmenteddatamatrixXa=(y:X),(12)andassignweightω1=σ2X/σ2Ytotheﬁrstcolumnandweightωj=1totheremainingcolumns.Then,withv0=β0α0j1...α0jq,v1=β1α1j1...α1jq,(13)(withq=|P|)therank-1weightedSVDXa≈1vT0+uvT1solvestheoptimizationproblemin(11).Althoughthisap-proachmightseemmoreprincipledthanourtwo-stepproce-dure,SPCAhasadistinctadvantage.Hereˆuθ,1=Xθwθ,1,andhenceitcanbedeﬁnedforfuturex∗dataandusedforpre-dictions.Intheerrors-in-variablesapproach,ˆuEV=XAwEV,whichinvolvesyaswellandleavesnoobviousestimateforfuturedata.WereturntothismodelinSection6.Thislatent-variablemodelcanbeeasilyextendedtoaccom-modatemultiplecomponentsU1,...,Um.OnewayofdoingthisistoassumethatY=β0+M(cid:2)m=1βmUm+ε(14)andXj=α0j+M(cid:2)m=1α1jmUm+j,j∈P.(15)Fittingthismodelproceedsasbefore,exceptnowweextractMratherthanoneprincipalcomponentfromXθ.WestudythismodelmoredeeplyinSection9.2.3AnExampleTheSPCAmodelanticipatesothersourcesofvariationinthedata,unrelatedtotheresponse.Infactthesesourcescanbeevenstrongerthanthosedrivingtheresponse,totheextentthatprincipalcomponentswouldidentifythemﬁrst.Byguidingtheprincipalcomponents,SPCAextractsthedesiredcomponents.WesimulateddatafromascenariolikethatofFigure2.Weused1,000genesand40samples,allwithbaseerrormodelbeingGaussianwithunitvariance.WethendeﬁnedthemeanBairetal.:SupervisedPrincipalComponents123vectorsµ1andµ2asfollows.Wedividethesamplesintocon-secutiveblocksof10,denotedbythesets(a,b,c,d).Thenµ1i=(cid:9)−2ifi∈a∪b+2otherwise(16)andµ2i=(cid:9)−1ifi∈a∪c+1otherwise.(17)Theﬁrst200geneshavemeanstructureµ1,xij=µ1i+ij,j=1,...,200,i=1,...,40.(18)Thenext50geneshavemeanstructureµ2,xij=µ2i+ij,j=201,...,250,i=1,...,40.(19)Inallcasesij∼N(0,1),whichisalsohowtheremaining750genesaredeﬁned.Finally,theoutcomeisgeneratedasyi=α·µ1i+(1−α)·µ2i+εi,whereεiisN(0,1).TheﬁrsttwoprincipalcomponentsofXareapproximatelyµ1andµ2(seeFig.2).Wetriedvariousvaluesofα∈[0,1],asshowninFigure3.Plottedisthecorrelationofthesupervisedprincipalcompo-nentspredictorwithanindependent(testset)realizationofyasθinthescreeningprocess|sj|>θisvaried.Thenumberofgenessurvivingthescreeningisshownonthehorizontalaxis.Theextremerightendofeachplotrepresentsstandardprinci-palcomponentsregression.Whenα=0,sothattheoutcomeiscorrelatedwiththesecondprincipalcomponent,supervisedPCeasilyimprovesonprincipalcomponentsregression.Whenαreaches.5,theadvantagedisappears,butsupervisedPCdoesnoworsethanprincipalcomponentsregression.3.CONSISTENCYOFSUPERVISEDPRINCIPALCOMPONENTSInSection9weshowthatthestandardprincipalcomponentsregressionisnotconsistentasthesamplesizeandnumberoffeaturesgrow,whereassupervisedprincipalcomponentsiscon-sistentunderappropriateassumptions.Becausethedetailsarelengthy,wegiveasummaryﬁrstanddeferthefulldiscussionuntilSection9.Weconsideralatentvariablemodeloftheform(9)and(10)fordatawithNsamplesandpfeatures.WedenotethefullN×pfeaturematrixbyX,andtheN×p1blockofXbyX1,corre-spondingtothefeaturesj∈P.WeassumethatasN→∞,p/N→γ∈(0,∞)andp1/N→0“fast.”Notethatpandp1maybeﬁxedormayapproach∞.Giventhissetup,weprovethefollowing:•Let˜UbetheleadingprincipalcomponentofXandlet˜βbetheregressioncoefﬁcientofYon˜U.Then˜UisnotgenerallyconsistentforU,andlikewise˜βisnotgenerally(a)(b)(c)(d)Figure3.CorrelationBetweentheFirstSupervisedPrincipalComponentuθ,1andaTestOutcomey,astheWeightαGiventotheFirstPrincipalComponentintheDataGenerationIsVaried.Thenumberofgenesusedbytheprocedureisshownonthehorizontalaxisineachpanel.Thesharpswitch(a)and(b)correspondstothepointatwhichtheorderoftheprincipalcomponentsisreversed.124JournaloftheAmericanStatisticalAssociation,March2006consistentforβ.BecauseUisarandomvariable,inSec-tion9wedeﬁnewhatwemeanbyconsistency.•AssumethatwearegivenX1.ThenifˆUistheleadingprincipalcomponentofX1andˆβistheregressioncoefﬁ-cientofYonˆU,thesearebothconsistent.•IfX1isnotgivenbutisestimatedbythresholdinguni-variatefeaturesscores(asinthesupervisedprincipalcom-ponentsprocedure),thenthecorrespondingˆUandˆβareconsistent.WehavealsoderivedanalogousresultsforCox’spropor-tionalhazardsmodel.Detailsaregiveninatechnicalpa-peravailableathttp://www-stat.stanford.edu/˜tibs/spc/cox.ps(orpdf).4.IMPORTANCESCORESANDAREDUCEDPREDICTORHavingderivedthepredictoruθ,1,howdoweassessthecon-tributionsofthepindividualfeatures?Wedeﬁnetheimpor-tancescoreastheinnerproductbetweeneachfeatureanduθ,1,impj=(cid:12)xj,uθ,1(cid:13).(20)Featuresjwithlargevaluesof|impj|contributemosttothepredictionofy.Ifthefeaturesarestandardized,thenthisisjustthecorrelationbetweeneachgeneandthesupervisedprincipalcomponent.Insomeapplicationswewouldliketohaveamodelthatusesonlyasmallnumberoffeatures.Forexample,apredictorthatrequiresexpressionmeasurementsforafewthousandgenesisnotlikelytobeusefulinaeverydayclinicalsettings;microar-raysaretooexpensiveandcomplexforeverydayuse,andsim-plerassayslikereversetranscription–polymerasechainreactioncanmeasureonly50or100genesatatime.Inaddition,isola-tionofasmallergenesetcouldaidbiologicalunderstandingofthedisease.Thereareanumberofwaystoobtainaseriesofreducedmodels.Onewaywouldbetoapplythelasso(Tibshirani1996)tothedata(X,ˆyspc).TheLARalgorithm(Efron,Hastie,Johnstone,andTibshirani2004)providesaconvenientmethodforcomputingthelassosolutions.Onedrawbackofthisap-proachisthattheseriesofmodelstypicallywillinvolvedif-ferentsetsoffeatures,whichcanbedifﬁcultforascientisttoassimilate.Herewetakeasimplerapproach.Wedeﬁneˆured=(cid:2)|impj|>γ(cid:4)j·xj,(21)where(cid:4)j=uTθ,1xj/d1istheloadingforthejthfeatureandd1istheﬁrstsingularvaluefromtheSVD(3).Thispredictorkeepsonlyfeatureswithimportancescoresγorlarger,andweightsthesefeaturesbytheirloadings.Onecouldcomputeimportancescores,andthecorrespond-ingreducedpredictor,forallfeatures(notjusttheonesusedincomputationofthesupervisedprincipalcomponents).Forex-ample,therecouldbeafeaturenotintheﬁrstsetthathasahigherinnerproductwiththesupervisedprincipalcomponentthanafeaturethatisintheﬁrstset.However,werestrictat-tentiontothefeaturesintheﬁrstset,foracoupleofreasons.Withthisapproach,avalueofγ=0yieldstheoriginalsuper-visedprincipalcomponentspredictor,facilitatingacomparisonbetweenthefullandreducedmodels.Second,allowingthereducedmodeltousefeaturesthatareoutsidetheﬁrstsetleadsnaturallytoaniteratedversionoftheprocedureinwhichwerecomputethesupervisedprincipalcomponentusinggeneswithhighestimportancescore,computenewscoresandrepeat.However,thisprocedurewilltypicallyconvergetoausualﬁrstprincipalcomponent(i.e.,itisunsu-pervised).Hencewedonotconsiderthisiteratedversion,andrestrictattentiontogenesthatpasstheinitialthreshold.Weillustratethisideainthenextsection.5.EXAMPLE:SURVIVALOFLYMPHOMAPATIENTSThisdataset,fromRosenwaldetal.(2002),consistsof240samplesfrompatientswithdiffuselargeB-celllymphoma(DLBCL),withgeneexpressionmeasurementsfor7,399genes.Theoutcomewassurvivaltime,eitherobservedorright-censored.Werandomlydividedthesamplesintoatrainingsetofsize160andatestsetofsize80.Theresultsofvariouspro-ceduresaregiveninTable1.Weusedthegeneswithtop25Coxscores(cutoffof3.53)incomputingtheﬁrstsupervisedprin-cipalcomponent.AlthoughPLS(describedinSec.6)providesastrongpredictorofsurvival,supervisedprincipalcomponentsisevenstronger.Figure4showsthecross-validationcurveforestimatingthebestthreshold.Eachmodelistrained,andthenthelog-likelihoodratio(LR)teststatisticiscomputedontheleft-outdata.Tohavesufﬁcientdataintheleft-outdatatocomputeameaningfulLRstatistic,weusetwo-foldcross-validation(ratherthanthemoretypicalﬁve-orten-fold).Thisprocessisrepeatedﬁvetimesandtheresultsareaveraged.Inourex-periments,thismethodyieldsareasonableestimateofthebestthresholdbutoftenunderestimatesthetestsetLRstatistic(be-causethetrainingandvalidationsetsarehalfoftheactualsizes).Thisisthecasehere,wherethecross-validatedLRsta-tisticisjustsigniﬁcantbutthetestsetLRstatisticisstronglysigniﬁcant.Thisexamplealsoillustratesthattheprocedurecanbesensi-tivetothethresholdvalue.Ifweinsteadchooseathresholdof2(areasonablechoiceaccordingtoFig.4),then865genesareselected.Thecorrelationoftheresultingsupervisedprincipalcomponentwiththeonefoundwith25genesisonlyabout.5.Thesupervisedprincipalcomponentpredictorgivesapvalueof.02inthetestset;thissigniﬁcantbutnotasstrongasthatfromthe25-genepredictor.Figure5showsthetestsetlog-likelihoodratiostatisticob-tainedbyﬁttingregressionmodelsofvarioussizestotheout-putofsupervisedprincipalcomponentregression.Weseethatifthetopfewgenesareused,thenthereisnolossinpredictivepower.Figure6showsthetop25genesandtheirloadings.Detailsaregivenintheﬁgurecaption.Table1.LymphomaData:TestSetResultsfortheVariousMethodsMethodZ-scorePvalueFirstprincipalcomponent−1.04.2940Partialleastsquares2.51.0112Firstsupervisedprincipalcomponent(25genes)−2.93.0045Bairetal.:SupervisedPrincipalComponents125Figure4.LymphomaData:Cross-ValidationCurveforEstimatingtheBestThreshold.6.SOMEALTERNATIVEAPPROACHESInthissectionwediscusssomealternativeapproachestothisproblem,someclassicalandsomereﬂectingotherapproachesthatwehaveexplored.6.1RidgeRegressionRidgeregression(HoerlandKennard1970)isaclassicalre-gressionprocedurewhentherearemanycorrelatedpredictors,andonethatcouldreasonablybeappliedinthepresentsetting.Ridgeregressionﬁtsthefulllinearregressionmodelbutman-agesthelargenumberofpredictorsinthesegenomicsettingsbyregularization(HastieandTibshirani2003).Ridgeregres-sionsolvesminβ(cid:2)y−β0−Xβ(cid:2)2+λ(cid:2)β(cid:2)2,(22)wherethesecondtermshrinksthecoefﬁcientstoward0.Theregularizationparameterλcontrolstheamountofshrinkage,andforeventhesmallestλ>0,thesolutionisdeﬁnedandisunique.ItcanalsobeshownthatthisformofregularizationFigure5.LymphomaData:TestSetLog-LikelihoodRatioStatisticObtainedFromtheReducedPredictorApproximation.shrinksthecoefﬁcientsofstronglycorrelatedpredictorstowardeachother,anattractivepropertyinthissetting.Usingthesingularvaluerepresentation(1),theﬁttedvaluesfromaridgeregressionhavetheformˆyRR=¯y+X(XTX+λI)−1Xy=¯y+m(cid:2)j=1ujd2jd2j+λuTjy.(23)Ridgeregressionislikeasmoothversionofprincipalcom-ponentsregression;ratherthanretainingtheﬁrstkprincipalcomponentsanddiscardingtherest,itweightsthesuccessivecomponentsbyafactorthatdecreaseswithdecreasingeigen-valued2j.Notethatridgeregressionisalinearmethod;thatis,ˆyRRisalinearfunctionofy.Incontrast,SPCAisnonlinear,becauseoftheinitialgene-selectionstep.6.2TheLassoThelassoTibshirani(1996)isavariationonridgeregressionthatsolvesminβ(cid:2)y−β0−Xβ(cid:2)2+λp(cid:2)j=1|βj|,(24)wherethesecondtermshrinksthecoefﬁcientstoward0.Theabsolutevalueformofthepenaltyhastheattractivepropertythatitcanshrinksomecoefﬁcientsexactlyto0.The“basispur-suit”proposalofChen,Donoho,andSaunders(1998)usesthesameideainasignalprocessingcontext.Computationofthelassoismorechallengingthatcomputationofridgeregression.Problem(24)isaconvexoptimization,whichcanbeverydifﬁ-cultifthenumberoffeaturespislarge.Theleast-angleregres-sion(LARS)algorithm(Efronetal.2004)providesanefﬁcientmethodforcomputationofthelasso,exploitingthefactthatasλchanges,theproﬁlesoftheestimatesarepiecewiselin-ear.Forotherlikelihood-basedmodels,suchastheCoxmodel,theEuclideandistancein(22)isreplacedbythe(negative)log-likelihoodorlog-partial-likelihood.Thecoefﬁcientproﬁlesarenotpiecewiselinear,sotheLARSapproachcannotbeapplied.SomedetailsofthelassofornonlinearmodelshavebeengivenbyTibshirani(1996,1997).WhenpislargerthanthesamplesizeN,thenumberofnon-0coefﬁcientsinalassosolutionisatmostN(foranyλ).Al-thoughsparsesolutionsaregenerallyattractive,thesesolutionsmaybetoosparse,because,forexample,formicroarraydatatheywouldallowonlyNgenestoappearinagivenmodel.Wenowconsiderseveralapproachestosupervisedprincipalcomponentsthatmodifytheoptimizationcriterionbehindprin-cipalcomponentsanalysisinasupervisoryfashion.6.3PartialLeastSquaresPLSisonesuchapproach,withalonghistory(Wold1975;FrankandFriedman1993;Hastie,Tibshirani,andFriedman2001).PLSworksasfollows:1.Standardizeeachofthevariablestohavemean0andunitnorm,andcomputetheunivariateregressioncoefﬁcientsw=XTy.2.DeﬁneuPLS=Xw,anduseitinalinearregressionmodelwithy.126JournaloftheAmericanStatisticalAssociation,March2006Figure6.LymphomaData:HeatmapDisplayoftheTop25Genes.Thetoptworowsoftheﬁgureshowtheobservedsurvivaltimesandﬁrstsupervisedprincipalcomponent(SPC)uθ,1;forsurvivaltimesTcensoredattimec,weshowˆE(T|T≥c)basedontheKaplan–Meierestimator.Allcolumnshavebeensortedbyincreasingvalueofuθ,1.Ontherightoftheheatmapthe“loadings”wθ,1areshown[see(6)];thegenes(rows)aresortedbydecreasingvalueoftheirloading.Allgenesbutthelastonehavepositiveloadings.AlthoughPLSgoesontoﬁndsubsequentorthogonalcompo-nents,onecomponentissufﬁcientforourpurposeshere.PLSexplicitlyusesyinestimatingitslatentvariable.Interestingly,itcanbeshownthatthe(normalized)winPLSsolves(FrankandFriedman1993)max(cid:2)w(cid:2)=1corr2(y,Xw)var(Xw),(25)acompromisebetweenregressionandPCA.FrankandFriedman(1993)concludedthatthevariancetermdominates,andhencethatPLSwouldingeneralbesimilartoprincipalcomponentsregression.Wecanseethisinthecontextofconsidermodel(9)–(10).Theexpectedvaluesoftheunivari-ateregressioncoefﬁcientwjisE(wj)=β1(cid:2)jαjα2j+σ2j.(26)Nowifσ2j=0,thenthePLSdirection(cid:1)jwjxijreducestoβ1(cid:1)jxij/αj.Butinthatcase,thelatentfactorUequals(cid:1)jXi/αj,sothetwosolutionsagree(inexpectation).Hence,afterweisolatetheblockofimportantfeatures,car-ryingoutprincipalcomponentsregressionorPLSislikelytogivesimilarresults.Themainadvantageofsupervisedprinci-palcomponentsoverthestandardPLSprocedureistheuseofthresholdingtoestimatewhichfeaturesareimportant.PLSre-tainsallfeaturesandcanbeadverselyeffectedbythenoiseinBairetal.:SupervisedPrincipalComponents127theunimportantfeatures.WeincludePLSamongthecompeti-torsinourcomparisonsinthenextsections.6.4MixedVariance–CovarianceCriterionThelargestprincipalcomponentisthatnormalizedlinearcombinationz=Xvofthegeneswiththelargestsamplevari-ance.Anotherwaytosupervisethiswouldbetoseekalin-earcombinationz=Xvhavingbothlargevarianceandalarge(squared)covariancewithy,leadingtothecompromisecrite-rionmax(cid:2)v(cid:2)=1(1−α)var(z)+αcov(z,y)2,s.t.z=Xv.(27)Thisisequivalenttomax(cid:2)v(cid:2)=1(1−α)vTXTXv+αvTXTyyTXv.(28)Ifyisnormalizedtounitnorm,thenthesecondtermin(28)isaregressionsumofsquares(regressingzony)andhastheinterpretation“thevarianceofzexplainedbyy.”Thesolutionvcanbeefﬁcientlycomputedastheﬁrstrightsingularvectoroftheaugmented(N+1)×pmatrix,Xa=(cid:10)(1−α)1/2Xα1/2yTX(cid:11).(29)Byvaryingthemixingparameterα,wecontroltheamountofsupervision.Althoughthemixedcriterioncanguidethese-quenceofeigenvectors,allgeneshavenon-0loadings,whichaddsalotofvariancetothesolution.6.5SupervisedGeneShavingHastieetal.(2000)proposed“geneshaving”asamethodforclusteringgenes.Theprimaryfocusoftheirmethodwastoﬁndsmallclustersofhighlycorrelatedgenes,whoseaverageexhibitedstrongvarianceoverthesamples.Theyachievedthisthroughaniterativeprocedure,whichrepeatedlycomputedthelargestprincipalcomponentofasubsetofthegenes,butaf-tereachiteration“shaved”awayafractionofthegeneswithsmallloadings.Thisproducesasequenceofnestedsubsetsofgeneclusters,withsuccessivelystrongerpairwisecorrelationandvarianceofthelargestprincipalcomponent.Theyalsoproposedasupervisedversionofgeneshaving,whichusespreciselyamixedcriterionoftheform(28).Al-thoughthismethodhastwotuningparameters,αandthesub-setsize,hereweﬁxαtotheintermediatevalueof.5andfocusattentiononthesubsetsize.AsinSPCA,foreachsubsetthelargestprincipalcomponentisusedtorepresentitsgenes.ThismethodissimilarinﬂavortoSPCA;itproducesprinci-palcomponentsofsubsetofgenes,wherethechoiceofsubsetissupervised.SimultaneouslysearchingforsparsecomponentswithhighvarianceandcorrelationwithyisanattempttoomitfeaturesthatmightslipthroughtheSPCAscreeningstep.OurexperimentsinthenextsectionshowthatshavingcanexhibitverysimilarperformancetoSPCA,thelatterwiththeadvan-tagesofbeingsimplertodeﬁneandhavingonlyonetuningparametertoselect.6.6AnotherMixedCriterionThelargestnormalizedprincipalcomponentu1isthelargesteigenvectorofXXT.ThisfollowseasilyfromtheSVD(1)andhenceXXT=UD2UT.Intuitively,becauseuT1XXTu1=p(cid:2)j=1(cid:12)u1,xj(cid:13)2,(30)weareseekingthevectoru1closestonaveragetoeachofthexj.Anaturalsupervisedmodiﬁcationistoperturbthiscriterioninamannerthatencouragestheleadingeigenvectortoalignwithy,maxu1,(cid:2)u1(cid:2)=1(1−α)p(cid:2)j=1(cid:12)u1,xj(cid:13)2+α(cid:12)u1,y(cid:13)2.(31)Solving(31)amountstoﬁndingthelargesteigenvectorofC(y;α)=(1−α)XXT+αyyT.(32)Equivalently,onecouldformanaugmentedmatrixXawithyinthe(p+1)stcolumn.Ifweassignweightsαtothisrowand(1−α)totheﬁrstprows,thenaweightedSVDofXaisequivalenttoaneigendecompositionof(31).Wenotethatthisisexactlythesituationdescribedintheerrors-in-variablesmodel(11)–(13)inSection2.2.Asmentionedthere,theesti-mateu1involvesyaswellasthexj,andsocannotbeuseddirectlywithtestdata.Wedidnotpursusthisapproachfurther.6.7DiscussionofMethodsFigure7illustratesthemethodsdiscussedearlieronasim-ulationexamplewithN=100samplesandp=5,000fea-tures.Thedataaregeneratedaccordingtothelatentvariablemodel(35),wheretherearefourdominantprincipalcom-ponents,andtheoneassociatedwiththeresponseisrankednumber3(whenestimatedfromthedata).Themethodsareidentiﬁedintheﬁgurecaption.TheleftmostMpointcorre-spondstoprincipalcomponentsregressionusingthelargestprincipalcomponent.SPCAandshavingdomuchbetterthantheothermethods.Figure8givesusacluetowhatisgoingon.Shownaretheﬁrst1,000of5,000featureloadingsfortwoofthemethodsdemonstratedinFigure7(chosenatthebestsolutionpoints).Bothmethodscorrectlyidentiﬁedtheimportantcomponent(theonerelatedtoyinvolvingtheﬁrst50features).InaregularSVDofX,thisimportantcomponentwasdominatedbytwoothercomponents.Indetail,thetrainingdatafrommodel(35)hasfourbuilt-incomponents,withsingularvaluescomputedas99.9,88.3,80.9,and80.5.Empirically,weveriﬁedthatcom-ponentthreeisidentiﬁedwiththeresponsemechanism,butitssingularvalueisjustabovethenoiselevel(theﬁfthsingularvaluewas79.2).However,themixedcriterionalsobringswithitnoisycoefﬁcients,somewhatsmaller,foralloftheothervari-ables,whereasSPCAsetsmostoftheotherloadingsto0.ThecoefﬁcientsforshavingshowaverysimilarpatterntoSPCA,whereasthoseforridgeandPLSareverysimilartothemixedcriterionandarenotshownhere.Ourexperiencewithmanysimilarexamplesismuchthesame,althoughtheshavingmethodoccasionallygetsthewrongcomponentcompletely.SPCAtendstobemorereliableandissimplertodeﬁne,andhenceisourmethodofchoice.Thesim-ulationsinthenextsectionalsosupportthischoice.128JournaloftheAmericanStatisticalAssociation,March2006Figure7.ASimulationExampleIllustratingtheTypicalBehavioroftheDifferentMethods.Thedataaregeneratedaccordingtothemodel(35)describedinthenextsection,withN=100andp=5,000.Ridgeregression,PLS,andthemixedcriterionallsufferfromtheveryhighdimensions.Althoughnotshown,theregularizationparameterλfortheridgepointsincreasestotheright,asdoestheαforthemixedcriterion,theleftmostvaluebeing0.BothshavingandSPCAareindexedbysubsetsize.Thelinelabeled“truth”usestheknownlinearcombinationof50featuresastheregressionpredictor(◦,SPCA;,truth;,mix;,ridge;,shave;,PLS).7.SIMULATIONSTUDIESWeperformedthreesimulationstudiestocomparetheper-formanceofthemethodsthatwehaveconsidered.Wedescribetheﬁrsttwostudieshere,andthethirdonelater.EachsimulateddatasetXconsistedof5,000“genes”(rows)and100“patients”(columns).Letxijdenotethe“expressionlevel”oftheithgene(a)(b)Figure8.FeatureLoadingswforSPCA(a)andtheMixedCriterion(28)(b).Theﬁrst1,000of5,000areshown,atthe“best”solutionpoint.Theverticallineindicatesthattheﬁrst50variablesgeneratedtheresponse.Whereasbothofthesemethodswereabletooverwhelmtheﬁrsttwodominantprincipalcomponents(whichwereunrelatedtoy),SPCAisabletoignoremostofthevariables,andthemixedcriteriongivesthemallweight(albeitmoreweighttotheﬁrst50).Bairetal.:SupervisedPrincipalComponents129andjthpatient.Intheﬁrststudywegeneratedthedataasxij=3+ijifi≤50,j≤504+ijifi≤50,j>503.5+ijifi>51,(33)wheretheij’sareindependentnormalrandomvariableswithmean0andvariance1.Wealsoletyj=(cid:1)50i=1xij25+j,(34)wherethej’sareindependentnormalrandomvariableswithmean0andstandarddeviation1.5.Wedesignedthissimulationsothattherearetwotumor“subclasses.”Patients1–50belongtotumorclass1,andhaveslightlyloweraverageexpressionlevelsinthepatientswithtu-morclass2.Furthermore,becauseyisproportionaltothesumoftheexpressionleveloftheﬁrst50genes,yisslightlylowerforpatientswithtumorclass1.Theother4,950genesareunre-latedtoy.Weappliedeightmethodstothissimulateddataset:prin-cipalcomponentsregression,principalcomponentsregressionusingonlytheﬁrstprincipalcomponent,PLS(onedirection),ridgeregression,lasso,supervisedprincipalcomponents,mixedvariance–covariance,andgeneshaving.Wetrainedeachofthesemodelsusingasimulateddatasetgeneratedasdescribedearlier.Weselecttheoptimalvalueofthetuningparametersforeachmethodusing10-foldcross-validation.Thenweusedthesameproceduretogenerateanindependenttestdatasetandusedthemodelsthatwebuilttopredictyonthetestdataset.Werepeatedthisprocedure10timesandaveragedtheresults.Table2givestheerrorsproducedbyeachmodel.Weseethatgeneshavingandsupervisedprincipalcom-ponentsgenerallyproducesmallercross-validationandtesterrorsthananyoftheothermethods,withtheformerhold-ingasmalledge.PrincipalcomponentsregressionandPLSgavecomparableresults(althoughprincipalcomponentsre-gressionperformedslightlyworsewhenrestrictedtoonecom-ponent).Table2.ResultsoftheSimulationStudyBasedonthe“Easy”SimulatedDataMethodCVerrorTesterrorPCR293.4(17.21)217.6(10.87)PCR-1316.8(20.52)239.4(11.94)PLS291.6(13.11)218.2(12.03)Ridgeregression298.0(14.72)224.2(12.35)Lasso264.0(13.06)221.9(12.72)Supervisedprincipalcomponents233.2(11.23)176.4(10.14)Mixedvariance–covariance316.7(19.52)238.7(10.24)Geneshaving223.0(8.48)172.5(9.25)NOTE:Eachentryinthetablerepresentsthesquarederrorofthetestsetpredictionsaveragedover10simulations.Thestandarderrorofeacherrorestimateisinparentheses.Thepredictionmethodsare:principalcomponentsregression(PCR),PCRrestrictedtousingonlyoneprincipalcomponent(PCR-1),partialleastsquares(PLS),ridgeregression,lasso,supervisedprincipalcomponents,mixedvariance–covariance,andgeneshaving.Next,wegenerateda“harder”simulateddataset.Inthissim-ulation,wegeneratedeachxijasfollows:xij=3+ijifi≤50,j≤504+ijifi≤50,j>503.5+1.5·I(u1j<.4)+ijif51≤i≤1003.5+.5·I(u2j<.7)+ijif101≤i≤2003.5−1.5·I(u3j<.3)+ijif201≤i≤3003.5+ijifi>301.(35)Heretheuijareuniformrandomvariableson(0,1)andI(x)isanindicatorfunction.Forexample,foreachofthegenes51–100,asinglevalueu1jisgeneratedforsamplej;ifthisvalueislargerthan.4,thenallofthegenesinthatblockget1.5added.Themotivationforthissimulationisthatthereareotherclustersofgeneswithsimilarexpressionpatternsthatareunre-latedtoy.Thisislikelytobethecaseinrealmicroarraydata,becausetherearepathwaysofgenes(thatprobablyhavesimilarexpressionpatterns)thatarenotrelatedtoy.Figures7and8illustratesomeofthemethodsappliedtoarealizationfromthismodel.Werepeatedtheexperimentdescribedearlierusing(35)togeneratethedatasetsinsteadof(33).TheresultsaregiveninTable3.Mostofthemethodsperformedworseinthis“harder”experiment.Onceagain,geneshavingandsupervisedprincipalcomponentsproducedsmallererrorsthananyofthecompet-ingmethods;geneshavingshowsmuchmorevariabilitythansupervisedprincipalcomponentsinthiscase.Ourthirdsimulationstudywasquitedifferentthantheﬁrsttwo.WeusedthetrainingandtestexpressiondatasetsfromRosenwaldetal.(2002),soastoobtaingeneswith“real-life”correlation.Fixingtheexpressiondata,wegeneratedindepen-dentstandardGaussiancoefﬁcientsθj,andﬁnallygeneratedaquantitativeoutcomeyi=(cid:1)pj=1xijθj+σZ,withZstandardGaussian.Withσ=3,about30%ofthevariationintheout-comewasexplainedbythetruemodel.Multipledatasetsweregeneratedinthisway,withexpressiondataheldﬁxed.RidgeregressionistheBayesestimateinthissetup,sowewouldexpectittoperformthebest.Wewereinterestedtoseehowothermethodscompared.Table4givestheresults.Ridgeregressionisthebest,followedincross-validationerrorbyPLSandintesterrorbythelasso.Theothermethodsaresubstan-tiallyworse.Table5givestheaveragenumberofgenesusedbyTable3.ResultsoftheSimulationStudyBasedonthe“Hard”SimulatedDataMethodCVerrorTesterrorPCR302.4(17.48)327.6(14.49)PCR-1325.6(20.05)354.6(14.99)PLS299.6(17.10)321.8(16.12)Ridgeregression301.0(18.47)328.0(16.38)Lasso286.9(16.92)322.8(21.24)Supervisedprincipalcomponents242.3(15.38)268.9(10.47)Mixedvariance–covariance322.5(19.64)349.8(16.02)Geneshaving234.0(12.46)276.6(13.43)NOTE:Eachentryinthetablerepresentsthesquarederrorofthetestsetpredictionsaveragedover10simulations.Thestandarderrorofeacherrorestimateisinparentheses.ThepredictionmethodsarethesameasinTable2.130JournaloftheAmericanStatisticalAssociation,March2006Table4.ThirdSimulationStudy:GaussianPriorforTrueCoefﬁcientsMethodCVerror/1,000Testerror/1,000PCR399.423(16.617)194.489(16.298)PCR-1559.708(29.637)283.356(24.320)PLS322.513(11.142)203.375(16.978)Ridgeregression304.215(9.858)132.251(55.45)Lasso356.886(15.281)169.266(10.217)SupervisedPC417.972(16.485)203.374(16.978)Mixedcovariance(y)418.250(10.975)202.293(16.805)Mixedcovariance(ˆy)551.924(26.251)286.255(23.149)Geneshaving402.876(11.897)197.000(17.040)supervisedprincipalcomponentsandlassointhethreesimula-tionstudies.Weseethatlassousesfewergenesthansupervisedprincipalcomponentsineachcase.However,intheﬁrsttwosimulationstudies,thenumberchosenbysupervisedprincipalcomponentsisclosertotheactualnumber(50).Inaddition,ifthereareNsamplesandNislessthanthetotalnumberoffeaturesp,thenthelassocanneverchoosemorethanNfea-tures.Thiscouldbetoorestrictive,becausethereisnoreasoningeneralthatthetruenumberofimportantgenesshouldbelessthanN.8.APPLICATIONTOVARIOUSSURVIVALSTUDIESHerewecompareseveralmethodsforperformingsurvivalanalysisonrealDNAmicroarraydatasets.(Someofthesere-sultswerealsoreportedbyBairandTibshirani2004.)Weap-pliedthemethodstofourdifferentdatasets.First,weexaminedamicroarraydatasetconsistingofpatientswithdiffuselargeB-celllymphoma(Rosenwaldetal.2002).Thereare7,399genes,160trainingpatients,and80testpatientsinthisdataset.Second,weconsideredabreastcancerdataset(van’tVeeretal.2002)with4,751genesand97patients.Wepartitionedthisdatasetintoatrainingsetof44patientsandatestsetof53pa-tients.Next,weexaminedalungcancerdataset(Beeretal.2002)with7,129genesand86patients,whichwepartitionedintoatrainingsetof43patientsandatestsetof43patients.Fi-nally,weconsideredadatasetofpatientswithacutemyeloidleukemia(Bullingeretal.2004),consistingof6,283genesand116patients.Thisdatasetwaspartitionedintoatrainingsetof59patientsandatestsetof53patients.Inadditiontosupervisedprincipalcomponents,weexaminedthefollowingmethods:principalcomponentsregression,partialleastsquares,lasso,andtwoothermethodsthatwecall“mediancut”and“clusteringCox,”describedbyBairandTibshirani(2004).Bothoftheselattermethodsturntheproblemintoatwo-classclassiﬁcationproblemandthenapplythenearestshrunkencentroidclassiﬁerofTibshirani,Hastie,Narasimhan,andChu(2001).Themediancutmethodstratiﬁesthepatientsintohighriskorlowrisk,dependingonwhethertheysurvivedpastthemediansurvivaltime.The“clusteringCox”methodisTable5.AverageNumberofGenes(andstandarddeviation)forSupervisedPrincipalComponentsandLassoinEachofThreeSimulationStudiesMethodSimulation1Simulation2Simulation3SupervisedPC44.5(9.4)54.4(10.9)95.7(16.4)Lasso32.8(8.7)23.1(6.2)42.9(5.5)likesupervisedprincipalcomponents,usingtwo-meanscluster-ingappliedtothegeneswiththehighestCoxscores.ForPLS,ridgeregression,andthelasso,weallowedthepossibilityofusingmorethanonecomponent,andchosethisnumberbycross-validation.TheresultsaregiveninTable6.Overall,supervisedprincipalcomponentsperformsbetterthanthecompetingmethods.However,intheDLBCLexample,thelassodoesbest.Thisisnotsurprising,becauseouruseofthelassoasapost-processorforsupervisedprincipalcomponentsshowedthatonlyafewgenesareneededforgoodpredictioninthisexample.9.THEORETICALRESULTSInthissectionwegivedetailsofourforsupervisedprinci-palcomponentsintheGaussianregressionsetting.ConsistencyresultsforsurvivaldataarediscussedintheAppendix.9.1SetupSupposethattherowsofXareiid.Thenwecanformulateapopulationmodelasfollows.DenotingtherowsbyXTi(i=1,...,N),wehavethemodelXiiid∼Np(µ,),where(p×p)isthecovariancematrix.Withoutlossofgen-erality,weassumethatµ=0,becauseitcanbequiteaccuratelyestimatedfromthedata.SupposethatXispartitionedasX=(X1,X2),whereX1isN×p1andX2isN×p2withp1+p2=p.Assumethatthecorrespondingpartitionofisgivenby=(cid:16)1002(cid:17).(36)Supposefurtherthatwecanrepresent1(p1×p1)as1=M(cid:2)k=1λkθkθTk+σ2I,(37)whereθk(k=1,...,M)aremutuallyorthonormaleigenvec-torsandtheeigenvaluesλ1≥···≥λM>0.Hereσ2>0rep-resentsthecontributionof(isotropic)“backgroundnoise”thatisunrelatedtotheinteractionsamonggenes.ThismodelcanbedescribedasacovariancemodelforgeneexpressionsthatisanM-rankperturbationofidentity.Here1≤M≤p1−1.Wecanequivalentlyexpressthepredictorsthroughthefol-lowingfactoranalysismodel.LetPbethesetofgenesformingthecolumnsofmatrixX1.Then|P|=p1andXij=M(cid:2)k=1(cid:18)λkθjkηik+σwij,j∈P,(38)representtheexpressionmeasurementsforthegenesinthesetPofitharray(replicate),i=1,...,N[cf.(10)inSec.2.2].Hereηikiid∼N(0,1)andareindependentofwijiid∼N(0,1).OurmainassumptionisthatX1isthematrixcontainingallofthecolumnswhosevariationsarerelatedtothevariationsiny.First,assumethattheselectionprocedureissuchthatitselectsX1withprobabilitytendingtoward1asN→∞.InSection10.4weconsiderthemorerealisticscenarioinwhichBairetal.:SupervisedPrincipalComponents131Table6.ComparisonoftheDifferentMethodsonFourDifferentDatasetsFromCancerStudies(a)DLBCL(b)Breastcancer(c)Lungcancer(d)AMLMethodR2pvalueNCR2pvalueNCR2pvalueNCR2pvalueNC(1)Mediancut.05.047.13.0042.15.0016.07.0487(2)Clustering-Cox.08.006.21.0001.07.0499.08.0309(3)SPCA.11.0032.272.1×10−51.361.5×10−73.16.00133(4)PCregression.01.0242.22.00033.11.01561.08.03761(5)PLS.10.0043.18.00031.18.00441.07.04891(6)Lasso.16.0002NA.14.001NA.26.0001NA.05.0899NANOTE:Themethodsare(1)assigningsamplestoa“low-risk”ora“high-risk”groupbasedontheirmediansurvivaltime,(2)usingtwo-meansclusteringbasedonthegeneswiththelargestCoxscores,(3)supervisedprincipalcomponentsmethod,(4)principalcomponentsregression,(5)partialleastsquaresregression,and(6)lasso.ThetableliststheR2(proportionoflog-likelihoodexplained)andpvaluesforthetestsetpredictions,aswellasthenumberofcomponentsused.weestimatethissubspacefromdata.Ourkeyassumptionsre-gardingthematrix1aregivenbyconditionsA1–A2or,moregenerally,byconditionsA1(cid:16)andA2(cid:16).WeshowinSection10.2thattheseconditionsaresufﬁcientfortheconsistencyoftheor-dinaryPCA-basedestimatorsofθkandλk,k=1,...,M,whenweperformsuchaPCAonthesamplecovariancematrixofX1.Itfollowsfromthisthatwecanconsistentlyestimatethepara-metersinthePCregressionmodelfortheresponseydescribedthrough(40);seeSection10.2fordetails.A1.The“signal”eigenvaluesof1satisfy(identiﬁabilityconditionforeigenvectors)λ1>···>λM>0,andMisaﬁxedpositiveinteger.A2.p1→∞asNincreasestoinﬁnityinsuchawaythatp1/N→0.Itmaybepossiblethatthenoisevarianceσ2andthe“signal”eigenvaluesλkalsovarywithN.Underthissetting,toguaranteeconsistency,weneedtoreplaceconditionsA1andA2bythefollowing:A1(cid:16).Theeigenvaluesaresuchthatλk/λ1→ρkfork=1,...,Mwith1=ρ1>ρ2>···>ρM>0andλ1→c>0asN→∞.Moreover,σ2→σ20∈[0,∞)asN→∞.A2(cid:16).p1varieswithNinsuchawaythatσ2p1/(Nλ1)→0asN→∞.NoticethatconditionA1(cid:16)isanasymptoticidentiﬁabilitycon-ditionfortheeigenvectorsθ1,...,θM.Thisisbecauseifρk=ρk+1forsome1≤k≤M−1,thenforlargeN,andforany2×2orthogonalmatrixC,thecolumnsofthematrixC[θk:θk+1]areapproximatelytheeigenvectorsof1correspondingtoeigen-valuesλkandλk+1.Thiswouldimplyaveryspecialkindofinconsistencyintheestimatesofθkandθk+1,eventhoughwestillmaybeabletoestimatethecorrespondingeigenspacecon-sistently.Toavoidthetechnicalitiesassociatedtothissituation,werestrictourselvestoconditionA1(cid:16).Notethattheconditionλ1→c>0,takentogetherwiththeﬁrstpartofconditionA1(cid:16),impliesthatalloftheMeigenvaluesλkconvergetopositivelimits.Remark1.ConditionsA1(cid:16)andA2(cid:16)allowforthepossibilitythatλ1/σ2→∞andp1/Nconvergestoapositivelimit.Thisparticularfacetbecomesrelevantwhenwetrytoconnecttothescenariothatwedescribehere.Considerthemodel(38)andsupposethatM=1.Inthiscase,if√λ1θj1isroughlyofthesameorderofmagnitudeforallj∈P,thenλ1∼p1forp1large.Evenifotherwise,itisreasonabletobelievethatthe“signal-to-noiseratio”λ1/σ2isgoingto∞asp1→∞,becausethepresenceoflargernumberofgenesassociatedwithacommonlatentfactoryieldsagreateramountofinformation.SupposethattheSVDofX1isgivenbyX1=UDVT,whereUisN×m,Dism×m,andVisp1×m,(39)withm=min(N,p1).HereNisthenumberofobservations(patients)andp1isthedi-mension(numberofgenes).Letu1,...,umdenotethecolumnsofUandletv1,...,vmdenotethecolumnsofV.Forobviousreasons,weset(cid:19)θk=vk,k=1,...,M.Also,wedenotethedi-agonalelementsofDbyd1>···>dm.Themodelfortheresponseisy=β01√N1+K(cid:2)k=1βk1√Nηk+Z,(40)whereK≤M,1isthevectorwith1ineachcoordinate,andZ∼NN(0,τ2NI)independentofXforsomeτ∈[0,∞).Itmayseemfrom(40)thattheparametersassociatedwiththedistributionofthesamplesizedependonthesamplesizeN.Butinrealitythemodel(40)isanexactanalogofthemodelforresponsegivenby(9)and(10).Thisisseenbydividingthrough(9)by√Nandtakingα1jm=θjmin(10)andsettingUm=ηmform=1,...,M.Remark2.Notethatwealsocouldhavedescribedthemodelintermsofsimilarquantitiesforthefulldataset,thatis,X(cor-respondingly).Therearetwodifﬁcultiesassociatedwiththisformulation.First,itisnotatalllikelythatallthesystematicvariationinthegeneexpressionsisassociatedwiththevaria-tionintheresponse.Soevenifmodel(36)–(37)istrue,thereisnoguaranteethatthelargestKeigenvaluesofarethelargestKeigenvaluesof1.Thiswillresultintheadditionofspurious(i.e.,unrelatedtotheresponsey)componentstothemodel.Theseconddifﬁcultyrelatestotheaccuracyofestimation.Becausetypicallypisverylarge(infactmuchlargerthan,oratleastcomparableto,thesamplesizeN),itisalmostnevergoingtobethecasethatassumptionA2(cid:16)issatisﬁed(withp1replacedbyp).Buttheassumptionforp1isreasonable,becauseonlyafewgenesareexpectedtobeassociatedwithacertaintypeofdisease.Violationofthisconditionresultsinaninconsistency132JournaloftheAmericanStatisticalAssociation,March2006intheestimatesofθk(seethenextsectionfordetails).SotheprocedureofselectingthegenesbeforeperformingthePCAregressionisnotonlysensible,butalsoineffectnecessary.9.2ResultsonEstimationofθkandλkTodiscussconsistencyoftheeigenvectorsθk,weconsiderthequantitydist((cid:19)θk,θk),wheredistisadistancemeasurebetweentwovectorsonthep1-dimensionalunitsphere.Wecanchooseeitherdist(a,b)=∠(a,b)(i.e.,theanglebetweenaandb)ordist(a,b)=(cid:2)a−sign(aTb)·b(cid:2)2fora,b∈Sp1.FirstsupposeweperformPCAonthefulldatasetXandes-timateθkby(cid:20)θk,therestrictionofthekthrightsingularvectorofXtothecoordinatescorrespondingtothesetX1.Thenthefollowingresultassertsthatifpisverylarge,thenwemaynothaveconsistency.Theorem1(Lu2002;JohnstoneandLu2006).Supposethat(38)andconditionA1hold(andassumethatσ2andλkareﬁxed)andthatp/N→γ∈(0,∞)asN→∞.Thendist((cid:20)θk,θk)(cid:17)→0inprobabilityasN→∞;thatis,theusualPCA-basedestimateofθkisinconsistent.UnderthesameconditionsasinTheorem1,thesampleeigenvaluesarealsoinconsistentestimatesforthepopulationseigenvalues.BaikandSilverstein(2004)derivedalmost-surelimitsofthesampleeigenvaluesinasimilarsetupundermini-maldistributionalassumptions.FromnowonwardwetreatexclusivelythesingularvaluedecompositionofX1.WedenotethePCA-basedestimateofthekthlargesteigenvalueof1by(cid:19)(cid:4)k,k=1,2,...,m.Ob-servethat(cid:19)(cid:4)k=1Ndk2.Thecorrespondingpopulationquantityis(cid:4)k:=λk+σ2.Anaturalestimatorofλkis(cid:19)λk=max{(cid:19)(cid:4)k−σ2,0}ifσ2isknown.But,ifσ2isunknown,thenwecanestimatethisbyvariousstrategies.Oneapproachistousethemedianofthedi-agonalelementsof1NXT1X1asa(usuallybiased)estimateofσ2andthendeﬁne(cid:19)λk=max{(cid:19)(cid:4)k−(cid:19)σ2,0}.NowweestablishtheconsistencyforPCArestrictedtothematrixX1.Wedonotgiveacompleteproofofthisresult,be-causeitisratherlongandsomewhattechnicalinnature.ButintheAppendixwegiveanoutlineoftheproofforthecasep1/N→0and{λk}Mk=1andσ2ﬁxed.ThedetailshavebeengivenbyPaul(2005).Theorem2.Letdist(a,b)=(cid:2)a−sign(aTb)·b(cid:2)2.Leth(x):=x21+xandg(x,y):=(x−y)2xy.Assumethat(38)holdsandthatthesetPisselectedwithprobabilitytendingtoward1asN→∞.•SupposethatconditionsA1(cid:16)andA2(cid:16)hold.Then,for1≤k≤M,Edist2((cid:19)θk,θk)≤(cid:16)p1Nh(λk/σ2)+1N(cid:2)k(cid:17)=k(cid:16)1g(λk+σ2,λk(cid:16)+σ2)(cid:17)×(1+o(1)).(41)If,moreover,λ1/σ2→∞,then(cid:19)(cid:4)k=λk(1+oP(1)).•Ifσ2andtheλk’sareﬁxedandconditionsA1andA2hold,then(41)holdsand(cid:19)(cid:4)kP→(cid:4)k=λk+σ2asN→∞.9.3EstimationofβkInthissectionwediscussestimationoftheparametersβk,k=1....,K.Tosimplifytheexposition,wetreatσ2andλk’sasﬁxedandassumethatconditionsA1andA2hold.Recallthatourmodelfortheresponsevariableisy=ˆβ0+(cid:1)Kk=1ˆβkˆµk.Supposethateitherσ2isknownoraconsistentestimate(cid:19)σ2isavailable.Thendeﬁne(cid:19)λk=max{(cid:19)(cid:4)k−(cid:19)σ2,0}.Letukbeasbe-foreanddeﬁne(cid:20)ukas1√(cid:19)λk1√NX1vkif(cid:19)λk>0,andasanyﬁxedunitvector[say(1,0,...,0)T]otherwise.Deﬁneanestimateofβk(for1≤k≤K)as(cid:20)βk=(cid:20)uTky.Wecancompareitsperfor-mancewithanotherestimate(cid:19)βk=uTkywithukasbefore.Also,deﬁne(cid:19)β0=(cid:20)β0=1√N(cid:1)Nj=1yj.Observethatuk=1dkX1vk=((cid:19)(cid:4)k)−1/21√NX1(cid:19)θk=((cid:19)(cid:4)k)−1/2(cid:21)M(cid:2)l=1(cid:18)λl(θTl(cid:19)θk)1√Nηl+σ√NW(cid:19)θk(cid:22),whereWistheN×p1matrixwhoserowsarewTi(i=1,...,N).Then,because(cid:19)θk=θk+εk(asaconventionassum-ing(cid:19)θTkθk>0),where(cid:2)εk(cid:2)2=OP(√p1/N),uk=√λk(cid:18)λk+σ21√Nηk(1+oP(1))+σ(cid:18)λk+σ21√NWθk(1+oP(1))+δk,(42)where(cid:2)δk(cid:2)2=OP(√p1/N).Toprovethislaststatement,weneedonlyuseTheorem2togetherwiththefactthat(cid:2)1NWTW(cid:2)2=1+oP(1),because2(cid:23)(cid:23)(cid:23)(cid:23)1√NWεk(cid:23)(cid:23)(cid:23)(cid:23)2≤2(cid:23)(cid:23)(cid:23)(cid:23)1NWTW(cid:23)(cid:23)(cid:23)(cid:23)2(cid:2)εk(cid:2)2=OP(cid:10)p1N(cid:11)and|εTkθl|≤(cid:2)εk(cid:2)2(cid:2)θl(cid:2)2=OP(cid:10)(cid:24)p1N(cid:11)for1≤l(cid:17)=k≤M,and,ﬁnally,(cid:2)ηl(cid:2)2=√N(1+oP(1))foralll=1,...,M.From,thisitfollowsthat(cid:20)uk=1√Nηk(1+oP(1))+σ√λk1√NWθk(1+oP(1))+(cid:20)δk,(43)where(cid:2)(cid:20)δk(cid:2)2=OP(√p1/N).Notethatthevectors{Wθk:k=1,...,M}areindependentNN(0,I)andindependentof{ηk:k=1,...,M},becausetheθk’saremutuallyorthonormal.Toestablishconsistencyof(cid:20)βk,1≤k≤K,notethat[by(43)](cid:20)βk=β01√N(cid:20)uTk1+K(cid:2)l=1βl1N(cid:16)(cid:10)ηk+σ√λkWθk(cid:11)(1+oP(1))+√N(cid:20)δk(cid:17)Tηl+(cid:20)uTkZBairetal.:SupervisedPrincipalComponents133=β0(cid:10)OP(cid:10)1√N(cid:11)+oP(1)(cid:11)+βk(1+oP(1)+(cid:20)δTk1√Nηk(cid:11)+(cid:2)l(cid:17)=kβl(cid:10)OP(cid:10)1√N(cid:11)+(cid:20)δTk1√Nηl(cid:11)+OP(cid:10)1√N(cid:11)=βk(1+oP(1)),because1NηTkηl=OP(1/√N)ifk(cid:17)=land1NηTlWθk=OP(1/√N)forallk,l(byindependence),(cid:2)(cid:20)δk(cid:2)2=oP(1),and(cid:20)uTkZ=(cid:2)(cid:20)uk(cid:2)2(cid:12)(cid:20)uk(cid:2)(cid:20)uk(cid:2)2,Z(cid:13).NotethatthesecondterminthelastproductisaN(0,τ2N)randomvariable,andtheﬁrsttermis(cid:18)(λk+σ2)/λk(1+oP(1))by(43).Itiseasytoverifythat(cid:19)β0=β0(1+oP(1)).But,fromtheforegoinganalysis,itisclearthattheestimator(cid:19)βk=uTky,for1≤k≤K,isnotconsistentingeneral.Infact,(cid:19)βk=(cid:25)λkλk+σ2βk(1+oP(1))whentheλk’sandσ2areﬁxed.How-ever,asweindicatedinRemark1,itisreasonabletoassumethatλ1/σ2→∞asp1,N→∞.Thiswillensure(viatheﬁrstpartofThm.2)thatthefactor(cid:18)λk/(cid:19)(cid:4)k→1inprobabilityasN→∞whenconditionsA1(cid:16)andA2(cid:16)hold.Therefore,wehave(cid:19)βk=βk(1+oP(1))for1≤k≤K.Thisinawayvalidatestheclaimthathavingmoregenes(i.e.,largerp1)associatedwiththeresponsegivesbetterpredictability.9.4ConsistencyoftheCoordinateSelectionScheme:RegressionModelInthissectionwedescribesomesituationsunderwhichSPCAwillconsistentlyselectthesetPofcoordinates(genes)whosevariabilityisassociatedwiththatoftheresponsethroughthemodelgivenby(36),(38),and(40).Hereweworkun-dertheassumptionthatp1=O(Nα)forsomeα∈(0,1)andlogp(cid:18)logN.Thesecondassumptioncoversawiderangeofpossiblesituations.ThekeypointthatweemphasizeisthattobeabletorecoverthesetPofpredictorsassociatedwiththeresponse,wemayneedsomeidentiﬁabilityconditionsonthisset.Ourmethodmayworkundermoregeneralcircumstances,butherewerestrictourattentiontocasesthatareanalyticallytractableandrelativelysimpletointerpret.First,observethatwecanwritethevectorofunivariatescoresass=H−1XXTy,whereHX=diag((cid:2)x1(cid:2),...,(cid:2)xp(cid:2)).BecausetherowsofX2areindependentNp2(0,2)r.v.independentofX1,invoking(38),wecanexpressthenonnormalizedscorevector(cid:20)s:=XTyintheform(cid:20)s=(cid:21)((cid:1)Mk=1√λkθkηTk+σWT)y1/22Cy(cid:22),(44)whereCisap2×NmatrixwhoseentriesareiidN(0,1)inde-pendentofX1andZ(andhencey).ObservethatWTisinde-pendentofy.Forexpositionalpurposes,weworkwith(cid:20)sratherthanwiths.Thisshowsthatifweconsiderthejthelementof(cid:20)sforj∈P,then1√N(cid:20)sj=1N(cid:26)M(cid:2)k=1(cid:18)λkθjkηTk(cid:27)×(cid:26)β01+K(cid:2)k(cid:16)=1βk(cid:16)ηk(cid:16)+√NZ(cid:27)+σ√N(WTy)j=β0M(cid:2)k=1(cid:18)λkθjkOP(cid:10)1√N(cid:11)+K(cid:2)k=1βk(cid:18)λkθjk(cid:10)1+OP(cid:10)1√N(cid:11)(cid:11)+M(cid:2)k=1(cid:18)λkθjkK(cid:2)k(cid:16)(cid:17)=kβk(cid:16)OP(cid:10)1√N(cid:11)+σ(cid:26)K(cid:2)k=0βk(cid:27)OP(cid:10)1√N(cid:11)=K(cid:2)k=1βk(cid:18)λkθjk+OP(cid:10)1√N(cid:11).Butontheotherhand,ifj/∈P,then,assumingthat(cid:2)2(cid:2)2isboundedabove,1√N(cid:20)sj=1√N(cid:28)1/22Cy(cid:29)j=(cid:28)1/22(cid:29)Tj1√NCy=OP(cid:10)1√N(cid:11).Thus,forthatthe“signal”ζKj:=(cid:1)Kk=1βk√λkθjktobede-tectable,itmustbe(cid:19)1/√N.Largedeviationboundssuggestthatwecanrecoverwithsufﬁcientaccuracyonlythosecoordi-natesjforwhich|ζKj|≥c0√logN/Nforsomeconstantc0>0(whichdependsonσ,theλk’sandβk’sand(cid:2)2(cid:2)2).Potentially,manyζKj’scouldbesmallerthanthat,andhencethosecoordinateswillnotbeselectedwithahighprobability.Ifwemakethethresholdtoosmall,thenwewillincludemany“spurious”coordinates(i.e.,thosewithj/∈P),whichcancauseproblemsinestimationinvariouswaysthatwediscussedal-ready.IfK=1,thenthejthcomponentofthesignalvectorζKisproportionalto√λ1θj1.Sotheschemewillselectonlythoseco-ordinatesjforwhich√λ1|θj1|isbig.Thismaynotexhausttheset{1,...,p1},butasfarasconsistentestimationofθ1andλ1isconcerned,itisadequate.Thus,whenK=1,thecoordinateselectionschemeisconsistent.InthecasewhereK>1,wemayencounteraproblem.Thisisbecausethemethodthatwedescribedreliesonaﬁxedlinearfunctionalofthevectortj=(√λ1θj1,...,√λKθjK),viz.,ζKj.Thusevenifatleastoneentryoftjisquitebig,wemaymissthatcoordinatej.Inotherwords,whenK>1,ingeneralthereisnoguaranteethatthecoordinateselectionschemeisconsistent.AcloserlooksuggeststhatingeneralwedonothavesufﬁcientidentiﬁabilityconstraintsonthesetofpredictorsP.Onewaytoimposethisconstraintistosaythat|ζKj|isaboveathresholdoftheformc0√logN/Nwhenever(cid:2)tj(cid:2)isaboveathresholdc1√logN/Nforsomeconstantsc0,c1>0.Eventhoughthisconditionmaynotbesatisﬁedexactly,itturnsoutthatweonlyneedthefollowing,somewhatweakerconstraint:A3.ThesetofvariablesPdeterminingX1issuchthatifPN,βdenotesthesetofallj∈Pwith(cid:30)(cid:30)(cid:30)(cid:30)(cid:31)tj,β(cid:2)β(cid:2) (cid:30)(cid:30)(cid:30)(cid:30)≥c0(cid:24)logNN(45)134JournaloftheAmericanStatisticalAssociation,March2006forsomeconstantc0>0independentofβ,then(cid:2)j∈P\PN,β1λ1(cid:2)tj(cid:2)2→0asN→∞.ObservethatA3isaconstraintontheentiremodel,notjustonthedistributionofthepredictorsX.Thephysicalmeaningofthisconstraintisasfollows:ThepredictorsintheclassPthathavesigniﬁcantvariancebecausetheﬁrstKcomponentsofvariationinthemodel(38)arehighlycorrelatedwiththeresponse.Thisisbecauseσ2+(cid:2)tj(cid:2)2+(cid:1)Mk=K+1λkθ2jkisthevarianceofthejthpredictor,and1√NζKjisthecovarianceofXjandy.Italsomeansthatthecoordinatesthatwemayfailtopickhavenegligiblecontributiontotheoverallvariabilityasso-ciatedwiththeﬁrstKcomponentsofvariation.NotethatthisconditionisautomaticallysatisﬁedwhenK=1.Adifferentwaytoimposeidentiﬁabilityonthesetofpre-dictorsPistoimposeaconstraintontheparameterβ=(β1,...,βK).InthiswaywerequirethataspeciﬁcK×Kma-trixH(β),whoseentriesarepolynomialsinβk’s,hassmallconditionnumber.WedeﬁneHasfollows.TheﬁrstcolumnofH(β)isβitself.WecanusethisconstrainttoensurethatweselectallofthebigcoordinatesevenwhenK(cid:19)1.Wecouldgeneralizetheselectionschemeasfollows.Forintegersr=1,2,...,K,deﬁnethesetJrtobethesetofcoordinatesjsuchthat|s(r)j|>α(r)jwhereα(r)jisathresholdoftheorder√logNands(r)jisthejthcoordinateof1√N(XTy(r))wherethelthcoordinateofy(r)is(√Nyl)2r−1.Inparticulary(1)=√Ny,sothats(1)=s,asdeﬁnedearlier.Finally,taketheunionJ:=!Kr=1JrandtakeJtobetheﬁnalselection.Ananalysisofthisschemeshowsthatforj∈P,1√Ns(r)j=tTjHr(β)+OP(1/√N),whereHr(β)istherthcolumnofH.Then,bytheconstraintonthematrixH(β),foranyj∈P,wehavej/∈Jifandonlyif(cid:2)tj(cid:2)is“small”(meaningsmallerthanacertainthresholdoftheformc2√logN/Nforsomeconstantc2>0).Remark3.Ofthetwomethodsofimposingidentiﬁabilityconstraints,thesecondoneisadmittedlyratheradhocanddoesnothaveameaningfulgeneralizationbeyondtheregressionset-ting.However,theﬁrstconstraintmayoftenbesatisﬁedinprac-tice,becausesomepartofthevariabilityinthepredictorsmaybedirectlylinkedtothevariabilityintheresponse.Thisislikelytobetrueif,forexample,thereisacausalrelationship.10.SOMEPRACTICALISSUESANDGENERALIZATIONSHerewementionsomewaysinwhichthesupervisedprinci-palcomponentscanbeappliedinpractice.JointFittingWithOtherCovariates.Typically,theremaybecovariatesmeasuredoneachofthecases,anditmightbeofinteresttoadjustforthese.Forexample,ingeneexpressionsurvivalstudies,inadditiontothepredictorsX1,X2,...,Xp,wemighthaveavailablecovariatesz=(z1,z2,...,zk),suchastumorstageandtumortype.Theremightbeinterestinﬁnd-inggeneexpressionpredictorsthatworkindependentlyofstageandtumor;thatis,havingadjustedforthesefactors,thegeneexpressionpredictorisstillstronglyrelatedtosurvival.Tocomparethesupervisedprincipalcomponentpredictortocompetingpredictors,onecansimplyﬁtthemtogetherinapre-dictivemodelforthetestset.Inthelymphomaexample,theInternationalPrognosticIndex(IPI)(low,mediumorhigh)isawidelyusedclinicalpredictorofsurvival.Weﬁtboththesuper-visedprincipalcomponentpredictorandIPItothetestset,thendeterminedthepvalueswheneachfromremovedseparatelyfromthejointmodel.Thesewere.001forthesupervisedprinci-palcomponentand.05forIPI.ThustheeffectofthesupervisedprincipalcomponentspredictorisstronglyindependentofIPI,whereasIPIisonlymoderatelyindependentofthesupervisedprincipalcomponentspredictor.WecanalsoexplicitlyencouragethesupervisedprincipalcomponentsPCpredictortolookforvariationthatisindepen-dentofcompetingpredictors.Todothis,wedoalinearregres-sionofeachgeneonthecompetingpredictors,replacingeachgene’smeasurementsbytheresidualsfromthisprocess.Wethenapplythesupervisedprincipalcomponentsproceduretotheresidualmatrix.Thisprocessdecorrelatesthegeneexpres-sionandcompetingpredictorsandforcestheprincipalcompo-nentstobeorthogonaltothecompetingpredictors.Thissameapproachcanbeusedwithothermethods,suchasPLS.UseofUnlabelledSamples.Insomesettings,wehaveavailableboth“labeled”data(e.g.,geneexpressionproﬁleswithameasuredsurvivaltimes)andunlabeleddata(justgeneexpressionproﬁles).Infact,onemighthavemanyunlabeledsamplesandonlyafewlabeledones,becauseobtainingout-comeinformationcanbemoredifﬁcult.Inthissettingitmightbehelpfultousetheunlabeleddatainsomeway,becausetheycontaininformationaboutthecorrelationbetweenthefeatures.Becauseofthesimpleformofthesupervisedprincipalcompo-nentspredictor,thereisaneasywaytodothis.SupposethatthefeaturematricesforthelabeledandunlabeleddataareXLandXU.Intheﬁrststep,weusejustXL(andtheoutcome)tochoosethefeatures.Thenweusethefullsetoffeatures(XL,XU)tocomputeprincipalcomponents.Theaddedinfor-mationprovidedbytheunlabeledsamplescanpotentiallyim-provetheaccuracyofthesupervisedprincipalcomponents.ApplicationtoOtherDataTypes.Thesupervisedprincipalcomponentsideacanbeappliedtoothertypesofoutcomemea-sures,suchasclassiﬁcationoutcomes.Inthatcase,wecouldchoosefeatureshavingthelargestbetween-classtowithin-classvariation,thencomputetheprincipalcomponentsofthese-lecteddata.Thentheprincipalcomponentwouldbeﬁtinamul-tiplelogisticregressiontopredicttheclasslabel.Althoughthisprocedureseemspromising,wehavenotyetfoundexampleswhereitimprovesonmethodssuchasthenearestshrunkencen-troidapproach(Tibshiranietal.2001).Theexplanationmaylieinthesoft-thresholdinginherentinnearestshrunkencentroids;itmayhavethesamebeneﬁcialeffectasthethresholdinginsupervisedprincipalcomponents.11.DISCUSSIONANDLIMITATIONSSupervisedprincipalcomponentsrepresentsapromisingtoolforpredictioninregressionandgeneralizedregressionprob-lems.Itisasimpleideathathasprobablybeentriedmanytimesinpractice.Herewehaveexploreditsapplicationtogeneex-pressionstudies.Bairetal.:SupervisedPrincipalComponents135Regressionisanimportantanddifﬁcultprobleminstatis-tics;itisespeciallydifﬁcultwhenthenumberoffeaturespgreatlyexceedsthenumberorobservationsN.Overﬁttingcanoccurwithevenmoderatelycomplexmodels,andidentifyingtheimportantfeaturesisfraughtwithdangerbecauseofthelargenumberoffeatures,manyofwhichareoftenhighlycor-related.Despitethedifﬁcultyinidentifyingimportantfeatures,however,thisisahighpriorityforbiologistsingeneexpressionstudies.Supervisedprincipalcomponentsapproachesthisdifﬁcultproblemthroughasemisupervisedstrategy,lookingforgrossstructureinthedatathatalignsitselfwiththeoutcome.Onlylaterintheprocessdoesittrytoparedownthesetoffeaturestoamuchsmallerlist(throughitsimportancescores).Acru-cialpracticalaspectofthisimportancescoreisthefactthatisprovidesaﬁxedorderingoffeatures.Thuswestartwithalistof200featuresandaskforasubmodelcontainingjust20fea-tures;theconstructedmodelconsistsofthe20featuresamongthe200withthelargestimportancescores.Incontrast,usingamethodlikethelasso,thefollowingcouldhappen:Wedeliveramodelhaving200featurestoourcollaborator,whothenasksforsmallermodel,containingjust20features.Sowechangethelassoboundtoachievethisandobtainanewmodelcon-taining20features,someornoneofwhichwereintheoriginallistof200!Thisseemslikeanunsatisfactoryapproachtomodelselectioninthissetting.Despitetheencouragingperformanceofsupervisedprinci-palcomponents,thehigh-dimensionalregressionproblemisverydifﬁcultandshouldbeapproachedwithcaution.Therearemanyissuesthatneedfurtherdevelopmentandcarefulstudy.Manyofthesewerepointedoutbytheeditorsandreferees.Welistsomehere:•Theabilityofcross-validationtoselectthe“correct”setofgeneshasnotbeenestablishedtheoretically.Inpractice,itseemstoperformreasonablywellbutcansometimesexhibitlargevariability,especiallywhenthesamplesizesaresmall.WhenthenumberofprincipalcomponentsKis>1,theconditionneededtoensureelectionofthecor-rectvariablesisverydifﬁculttoverifyinpractice.Itwouldbeusefultoexploreotherapproachesformultiplecompo-nents.Withlargenumbersofhighlycorrelatedfeatures,itisimportanttolearnwhenwecanandcannotisolatetheimportantunderlyingfeatures.•Thelatentvariablemodelusedinthisarticleisareason-ablestartingpoint,butmaynotberealisticinpractice.Onemighthaveasituationinwhichtheresponseismarginallyindependentoftheactivepredictors,andyetjointlydepen-dentonthem.Anothersituationwouldhaveallpredictorsmarginallydependentontheresponse,whereasonesetisindependentoftheresponsegiventherestofthepredic-tors.Inthesecases,thesupervisedprincipalcomponentprocedurewouldfail.•Theresponsemodelconsideredinthisarticleisasim-plelinearor(generalizedlinear)model.Itwouldbeuse-fultoexaminewhethersupervisedprincipalcomponentscanperformwellwhentheresponseisamorecomplexfunctionofthelatentfactors.Onecoulduselinearcorrela-tionthresholding(asdescribedinthearticle)butthenuseasplinebasis(insteadofalinearbasis)intheresponsemodel.Inpractice,wehavefoundthatanaturalcubicsplinebasiswithtwoorthreeknotscancapturesimplenonlinearitiesintheresponsefunction.Wehavenotyetimplementedthisinoursoftware,butplantoexploreitfurther.•Akeyaspectofthemethodisthepreselectionoffeaturesaccordingtotheircorrelationwiththeoutcome.Thisal-leviatestheeffectofalargernumberofnoisyfeaturesonthepredictionmodel.Itislikelythatthispreselectioncanbeusedeffectivelywithotherregressionmethods,suchaspartialleastsquaresandridgeregression.Wehavefocusedonsupervisedprincipalcomponentsbecauseofitsstrikingsimplicity.•FurtherworkisneededintheCoxmodelsetting,becauseourresultstherearenotyetrigorous.•Supervisedprincipalcomponentsisattractivebecauseofitssimplicity.However,asmentionedearlier,othermeth-ods,suchaspartialleastsquaresandthelasso,couldbeappliedafterthresholdingofthegenes.Thesemightalsoperformwellandareworthinvestigating.Inaddition,thereareothercloselyrelatedmethodsthatshouldbeconsid-eredandcomparedwithsupervisedprincipalcomponents.TheseincludetheslicedinversedregressionapproachofDuanandLi(1991)andLi(1992)andthesufﬁcientdimensionreductionapproachesusedbyChiaromonte,Cook,andLi(2002)andCook(2004).Theﬁrstofthelattertwoarticlesrelatestocovariateadjustment,whereasthesecondtreatspredictorselection.Relatedapplica-tionstomicroarraydataincludethoseofChiaromonteandMartinelli(2002),Antoniadis,Lambert-Lacroix,andLeblanc(2003),andBuraandPfeiffer(2003).Anapplicationofsupervisedprincipalcomponentsinamed-icalsettingisdiscussedinZhaoetal.(2006).WehavewrittenExcel(PAM)andRlanguagepackages(superpc)implement-ingsupervisedprincipalcomponentsforsurvivalandregressiondata.ThesearefreelyavailableonTibshirani’swebsite(http://www-stat.stanford.edu/˜tibs/superpc).APPENDIX:OUTLINEOFPROOFOFTHEOREM2Asalreadystated,weprovetheresultunderassumptionsA1andA2.ToproveTheorem2,weneedthefollowinglemmaaboutthepertur-bationofeigenvectorsofasymmetricmatrixundersymmetricpertur-bation.LemmaA.1.Forsomem∈N,letAandBbetwosymmetricm×mmatrices.LettheeigenvaluesofmatrixAbedenotedbyλ1(A)≥···≥λm(A).Setλ0(A)=∞andλm+1(A)=−∞.Foranyr∈{1,...,m},ifλr(A)isaneigenvalueofmultiplicity1,thatis,λr−1(A)>λr(A)>λr+1(A),thendenotingbyprtheeigenvectorassociatedwiththerthlargesteigenvalue,pr(A+B)−sign(cid:28)pr(A+B)Tpr(A)(cid:29)pr(A)=−Hr(A)Bpr(A)+Rr,(A.1)whereHr(A):=(cid:1)s(cid:17)=r(λs(A)−λr(A))−1PEs(A)andPEs(A)denotestheprojectionmatrixontotheeigenspaceEscorrespondingtotheeigenvalueλs(A),(possiblymultidimensional).Further,theresidual136JournaloftheAmericanStatisticalAssociation,March2006termRrcanbeboundedby(cid:2)Rr(cid:2)≤(cid:2)Hr(A)Bpr(A)(cid:2)(cid:16)2r(1+r)1−2r(1+r)+(cid:2)Hr(A)Bpr(A)(cid:2)(1−2r(1+r))2(cid:17)ifr<√5−12102ralways,(A.2)wherer=(cid:2)B(cid:2)2min1≤s(cid:17)=r≤m|λs(A)−λr(A)|.(A.3)Proof.ThisfollowsfromareﬁnementoftheargumentgivenintheproofoflemmaA.1ofKneipandUtikal(2001).LemmaA.1givesaﬁrst-orderexpansionoftheeigenvectorofaperturbedmatrix.NowwecantakeasmatrixAthematrix1,thecovariancematrixof{Xj:j∈P},andthenwecantakeBtobethedifferenceS1−1,whereS1=1NXT1X1.NoticethatHr(1)=(cid:2)1≤s(cid:17)=r≤M1λs−λrθsθTs−1λr(cid:26)I−M(cid:2)s=1θsθTs(cid:27),andpr(1)=θr.ByLemmaA.1,weonlyneedprobabilisticboundsforthequantities(cid:2)Hr(A)Bpr(A)(cid:2)and(cid:2)S1−1(cid:2).Theﬁrstinvolvessomelengthybutstraightforwardcalculation,andforthesecondweneedaboundfortheterm(cid:2)1NWTW−I(cid:2).Forthis,weusethefollowinglemma,theproofofwhichuseslargedeviationinequalitiesforquadraticformsofGaussianrandomvariables.LemmaA.2.Supposethatn,L→∞s.t.L/n→0.LetZbedenoteanL×nmatrixwithiidN(0,1)entries.Denotebyl1andlLthelargestandthesmallesteigenvaluesof1nZZT.WehaveP(cid:10)l1−1>2(cid:28)(cid:18)log(n/L)+π(cid:29)(cid:24)Ln(cid:11)≤−1nL(L/n)(L/2)(1+o(1))(1+o(1))andP(cid:10)lL−1<−2(cid:28)(cid:18)log(n/L)+π(cid:29)(cid:24)Ln(cid:11)≤(1+−1nL)(L/n)(L/2)(1+o(1))(1+o(1)),wherenL=√2L(cid:18)log(n/L).Proof.Fora∈SL(L-dimensionalunitsphere),deﬁneg(a,Z)=1naTZZTa.Asafunctionofa,g(a,Z)isLipschitz-1withLipschitzconstant2(cid:2)1nZZT(cid:2)=2l1.Thisisbecauseg(a,Z)−g(b,Z)=(a−b)T1nZZT(a+b).LetFδbeaminimalcoveringofthesphereSLbyballsofradiusδ<1.Thenasimpleargumentshowsthat(cid:10)1δ(cid:11)L−1≤|Fδ|≤2(cid:10)πδ(cid:11)L−1.(A.4)Foraproofofthisresult,refertoproposition2ofvonLuxburg,Bousquet,andSchölkopf(2002).Now,bydeﬁnition,l1=maxa∈SLg(a,Z)andlL=mina∈SLg(a,Z).HencebythecoveringofSLbyspheresofradiusδcenteredatpointsinFδandtheLipschitzboundong,itfollowsthatmaxa∈Fδg(a,Z)≤l1≤maxa∈Fδg(a,Z)+2δl1andmina∈Fδg(a,Z)−2δl1≤lL≤mina∈Fδg(a,Z).(A.5)Nowweusethefactthatifa∈SLandentriesoftheL×nmatrixZareiidN(0,1),thenZTahasiidN(0,1)entriesandsog(a,Z)∼χ2(n)/n.Finally,werecallsomelarge-deviationinequalitiesforχ2randomvariables.Johnstone(2001)showedthatP(cid:28)χ2(n)>n(1+)(cid:29)≤e−3n2/16,0<<12,(A.6)P(cid:28)χ2(n)<n(1−)(cid:29)≤e−n2/4,0<<1,(A.7)andP(cid:28)χ2(n)>n(1+)(cid:29)≤√2√ne−n2/40<<n1/16,n≥16.(A.8)Let0<t<1and0<δ<t2(1+t);then,bytheﬁrstinequalityin(A.5),forn≥16,P(l1−1>t)≤P"maxa∈Fδg(a,Z)·(1−2δ)−1−1>t#=P"maxa∈Fδ[g(a,Z)−1]>t(1−2δ)−2δ#≤|Fδ|P(cid:10)χ2(n)n−1>t(1−2δ)−2δ(cid:11)≤2(cid:10)πδ(cid:11)N−1√2√n(t(1−2δ)−2δ)×exp(cid:16)−n4(t(1−2δ)−2δ)2(cid:17),by(A.4)and(A.8).Nowchooseδ:=δn=π√L/nandt:=tn=(2(cid:18)log(n/L)+2π)(cid:25)Ln,whichsatisfytherestrictionsfornsufﬁcientlylarge.Thent(1−2δ)−2δ=2$log(cid:10)nL(cid:11)(cid:24)Ln(cid:10)1−2π(cid:24)Ln(cid:11)−4π2Ln=2$log(cid:10)nL(cid:11)(cid:24)Ln(1−εn),whereεn=2π√L/n(1+π(log(nL))−1/2).Becauseεn=o(1)asn→∞,P(cid:10)l1−1>(cid:10)2$log(cid:10)nL(cid:11)+2π(cid:11)(cid:24)Nn(cid:11)≤√22√L(cid:18)log(n/L)(1−εn)(cid:10)nL(cid:11)(L−1)/2×exp(cid:16)−Llog(cid:10)nL(cid:11)(1−εn)2(cid:17)=1√2L(cid:18)log(n/L)(1+o(1))(cid:10)Ln(cid:11)(L/2)(1+o(1))asn→∞.(A.9)Next,usingthesecondinequalityin(A.5),fort=tnandδ=δnaschosenearlier,P(lL−1<−t)≤P"mina∈Fδg(a,Z)−1<−t+2δl1#≤P"mina∈Fδ[g(a,Z)−1]<−t+2δ(1+t)#+P(l1−1>t)≤|Fδ|P(cid:10)χ2(n)n−1<−(t(1−2δ)−2δ)(cid:11)+P(l1−1>t).Bairetal.:SupervisedPrincipalComponents137Then,using(A.4)and(A.7)exactlyasbeforetoboundtheﬁrsttermontherightsideandthenusing(A.9),weget,asn→∞,P(cid:10)lL−1<−(cid:10)2$log(cid:10)nL(cid:11)+2π(cid:11)(cid:24)Ln(cid:11)≤(cid:10)1+1√2L(cid:18)log(n/L)(1+o(1))(cid:11)(cid:10)Ln(cid:11)(L/2)(1+o(1)).(A.10)[ReceivedNovember2004.RevisedMay2005.]REFERENCESAlter,O.,Brown,P.,andBotstein,D.(2000),“SingularValueDecompositionforGenome-WideExpressionDataProcessingandModeling,”ProceedingsoftheNationalAcademyofSciencesUSA,97,10101–10106.Antoniadis,A.,Lambert-Lacroix,S.,andLeblanc,F.(2003),“EffectiveDi-mensionReductionMethodsforTumorClassiﬁcationUsingGeneExpres-sionData,”Bioinformatics,19,563–570.Baik,J.,andSilverstein,J.W.(2004),“EigenvaluesofLargeSampleCovari-anceMatricesofSpikedPopulationModels,”arXiv:math.ST.Bair,E.,andTibshirani,R.(2004),“Semi-SupervisedMethodstoPredictPa-tientSurvivalFromGeneExpressionData,”PLoSBiology,2,511–522.Beer,D.G.,Kardia,S.L.,Huang,C.-C.,Giordano,T.J.,Levin,A.M.,Misek,D.E.,Lin,L.,Chen,G.,Gharib,T.G.,Thomas,D.G.,Lizyness,M.L.,Kuick,R.,Hayasaka,S.,Taylor,J.M.,Iannettoni,M.D.,Orringer,M.B.,andHanash,S.(2002),“Gene-ExpressionProﬁlesPre-dictSurvivalofPatientsWithLungAdenocarcinoma,”NatureMedicine,8,816–824.Bullinger,L.,Döhner,K.,Bair,E.,Fröhling,S.,Schlenk,R.,Tibshirani,R.,Döhner,H.,andPollack,J.R.(2004),“GeneExpressionProﬁlingIdenti-ﬁesNewSubclassesandImprovesOutcomePredictioninAdultMyeloidLeukemia,”NewEnglandJournalofMedicine,350,1605–1616.Bura,E.,andPfeiffer,R.M.(2003),“GraphicalMethodsforClassPredictionUsingDimensionReductionTechniquesonDNAMicroarrayData,”Bioin-formatics,19,1252–1258.Chen,S.S.,Donoho,D.L.,andSaunders,M.A.(1998),“AtomicDecomposi-tionbyBasisPursuit,”SIAMJournalonScientiﬁcComputing,20,33–61.Chiaromonte,F.,Cook,R.,andLi,B.(2002),“SufﬁcientDimensionReduc-tioninRegressionsWithCategoricalPredictors,”TheAnnalsofStatistics,30,475–497.Chiaromonte,F.,andMartinelli,J.(2002),“DimensionReductionStrategiesforAnalyzingGlobalGeneExpressionDataWithaResponse,”MathematicalBiosciences,176,123–144.Cook,R.(2004),“TestingPredictorContributionsinSufﬁcientDimensionRe-duction,”TheAnnalsofStatistics,32,1062–1092.Duan,N.,andLi,K.-C.(1991),“SlicingRegression:ALink-FreeRegressionMethod,”TheAnnalsofStatistics,19,505–530.Efron,B.,Hastie,T.,Johnstone,I.,andTibshirani,R.(2004),“LeastAngleRegression,”TheAnnalsofStatistics,32,407–499.Frank,I.,andFriedman,J.(1993),“AStatisticalViewofSomeChemometricsRegressionTools”(withdiscussion),Technometrics,35,109–148.Ghosh,D.(2002),“SingularValueDecompositionRegressionModelsforClas-siﬁcationofTumorsFromMicroarrayExperiments,”PaciﬁcSymposiumonBiocomputing,7,18–29.Hastie,T.,andTibshirani,R.(2003),“EfﬁcientQuadraticRegularizationforExpressionArrays,”technicalreport,StanfordUniversity.Hastie,T.,Tibshirani,R.,Botstein,D.,andBrown,P.(2001),“SupervisedHar-vestingofExpressionTrees,”GenomeBiology,2,1–12.Hastie,T.,Tibshirani,R.,Eisen,M.,Alizadeh,A.,Levy,R.,Staudt,L.,Botstein,D.,andBrown,P.(2000),“IdentifyingDistinctSetsofGenesWithSimilarExpressionPatternsvia‘GeneShaving’,”GenomeBiology,1,1–21.Hastie,T.,Tibshirani,R.,andFriedman,J.(2001),TheElementsofStatisti-calLearning;DataMining,InferenceandPrediction,NewYork:Springer-Verlag.Hi,H.,andGui,J.(2004),“PartialCoxRegressionAnalysisforHigh-DimensionalMicroarrayGeneExpressionData,”Bioinformatics,5,1208–1215.Hoerl,A.E.,andKennard,R.(1970),“RidgeRegression:BiasedEstimationforNonorthogonalProblems,”Technometrics,12,55–67.Huffel,S.V.,andLemmerling,P.(eds.)(2002),TotalLeastSquaresandErrors-in-VariablesModeling,Dordrecht:Kluwer.Jiang,H.,Deng,Y.,Chen,H.-S.,Tao,L.,Sha,Q.,Chen,J.,Tsai,C.-J.,andZhang,S.(2004),“JointAnalysisofTwoMicroarrayGene-ExpressionDataSetstoSelectLungAdenocarcinomaMarkerGenes,”BMCBioinformatics,5,1–12.Johnstone,I.(2001),“Chi-SquareOracleInequalities,”inFestschriftforWilliamR.vanZwet,eds.M.deGunst,C.Klaassen,andA.vanderWaart,Hayward,CA:IMS,pp.399–418.Johnstone,I.,andLu,A.Y.(2006),“SparsePrincipalComponentsAnalysis,”JournaloftheAmericanStatisticalAssociation,toappear.Kneip,A.,andUtikal,K.J.(2001),“InferenceforDensityFamiliesUsingFunctionalPrincipalComponentAnalysis,”JournaloftheAmericanStatisti-calAssociation,96,519–542.Li,K.-C.(1992),“SlicedInverseRegressionforDimensionReduction”(withdiscussion),JournaloftheAmericanStatisticalAssociation,86,316–342.Lu,A.Y.(2002),“SparsePrincipalComponentsAnalysisforFunctionalData,”technicalreport,StanfordUniversity.Mardia,K.,Kent,J.,andBibby,J.(1979),MultivariateAnalysis,NewYork:AcademicPress.Miller,R.G.(1986),BeyondAnova:BasicsofAppliedStatistics,NewYork:Wiley.Nguyen,D.,andRocke,D.(2002),“PartialLeastSquaresProportionalHaz-ardRegressionforApplicationtoDNAMicroarrays,”Bioinformatics,18,1625–1632.Paul,D.(2005),“NonparametricEstimationorParametricComponents,”Ph.D.thesis,StanfordUniversity.Rosenwald,A.,Wright,G.,Chan,W.C.,Connors,J.M.,Campo,E.,Fisher,R.I.,Gascoyne,R.D.,Muller-Hermelink,H.K.,Smeland,E.B.,andStaudt,L.M.(2002),“TheUseofMolecularProﬁlingtoPredictSurvivalAf-terChemotherapyforDiffuseLargeB-CellLymphoma,”TheNewEnglandJournalofMedicine,346,1937–1947.Tibshirani,R.(1996),“RegressionShrinkageandSelectionviatheLasso,”JournaloftheRoyalStatisticalSociety,SerB.,58,267–288.(1997),“TheLassoMethodforVariableSelectionintheCoxModel,”StatisticsinMedicine,16,385–395.Tibshirani,R.,Hastie,T.,Narasimhan,B.,andChu,G.(2001),“DiagnosisofMultipleCancerTypesbyShrunkenCentroidsofGeneExpression,”Proceed-ingsoftheNationalAcademyofScience,99,6567–6572.van’tVeer,L.J.,Dai,H.,vandeVijver,M.J.,He,Y.D.,Hart,A.A.M.,Mao,M.,Peterse,H.L.,vanderKooy,K.,Marton,M.J.,Witteveen,A.T.,Schreiber,G.J.,Kerkhoven,R.M.,Roberts,C.,Linsley,P.S.,Bernards,R.,andFriend,S.H.(2002),“GeneExpressionProﬁlingPredictsClinicalOut-comeofBreastCancer,”Nature,415,530–536.vonLuxburg,U.,Bousquet,O.,andSchölkopf,B.(2002),“ACompressionApproachtoSupportVectorModelSelection,”technicalreport,Max-Planck-InstitutfürBiologischeKybernetik.Wold,H.(1975),“SoftModellingbyLatentVariables:TheNonlinearIterativePartialLeastSquares(NIPALS)Approach,”inPerspectivesinProbabilityandStatistics,inHonorofM.S.Bartlett,ed.J.Gani,pp.117–144.Zhao,H.,Ljungberg,B.,Grankvist,K.,Rasmuson,T.,Tibshirani,R.,andBrooks,J.(2006),“GeneExpressionProﬁlingPredictsSurvivalinConven-tionalRenalCellCarcinoma,”PLoSMedicine,3,1–10.