Statistics and Its Interface Volume 2 (2009) 349–360

∗
Multi-class AdaBoost
Ji Zhu†‡, Hui Zou§, Saharon Rosset and Trevor Hastie¶

Boosting has been a very successful technique for solving
the two-class classiﬁcation problem. In going from two-class
to multi-class classiﬁcation, most algorithms have been re-
stricted to reducing the multi-class classiﬁcation problem to
multiple two-class problems. In this paper, we develop a new
algorithm that directly extends the AdaBoost algorithm to
the multi-class case without reducing it to multiple two-class
problems. We show that the proposed multi-class AdaBoost
algorithm is equivalent to a forward stagewise additive mod-
eling algorithm that minimizes a novel exponential loss for
multi-class classiﬁcation. Furthermore, we show that the ex-
ponential loss is a member of a class of Fisher-consistent loss
functions for multi-class classiﬁcation. As shown in the pa-
per, the new algorithm is extremely easy to implement and
is highly competitive in terms of misclassiﬁcation error rate.

AMS 2000 subject classifications: Primary 62H30.
Keywords and phrases: boosting, exponential
multi-class classiﬁcation, stagewise modeling.

loss,

1. INTRODUCTION

Boosting has been a very successful technique for solving
the two-class classiﬁcation problem. It was ﬁrst introduced
by [8], with their AdaBoost algorithm. In going from two-
class to multi-class classiﬁcation, most boosting algorithms
have been restricted to reducing the multi-class classiﬁca-
tion problem to multiple two-class problems, e.g. [8], [19],
and [21]. The ways to extend AdaBoost from two-class to
multi-class depend on the interpretation or view of the suc-
cess of AdaBoost in binary classiﬁcation, which still remains
controversial. Much theoretical work on AdaBoost has been
based on the margin analysis, for example, see [20] and [13].
Another view on boosting, which is popular in the statistical
community, regards AdaBoost as a functional gradient de-
scent algorithm [6, 11, 17]. In [11], AdaBoost has been shown
to be equivalent to a forward stagewise additive modeling al-
gorithm that minimizes the exponential loss. [11] suggested
that the success of AdaBoost can be understood by the fact
∗

We thank the AE and a referee for their helpful comments and sug-

gestions which greatly improved our paper.
†
Corresponding author.
‡
Zhu was partially supported by NSF grant DMS-0705532.
§
Zou was partially supported by NSF grant DMS-0706733.
¶
Hastie was partially supported by NSF grant DMS-0204162.

that the population minimizer of exponential loss is one-
half of the log-odds. Based on this statistical explanation,
[11] derived a multi-class logit-boost algorithm.

The multi-class boosting algorithm by [11] looks very dif-
ferent from AdaBoost, hence it is not clear if the statis-
tical view of AdaBoost still works in the multi-class case.
To resolve this issue, we think it is desirable to derive an
AdaBoost-like multi-class boosting algorithm by using the
exact same statistical explanation of AdaBoost. In this pa-
per, we develop a new algorithm that directly extends the
AdaBoost algorithm to the multi-class case without reduc-
ing it to multiple two-class problems. Surprisingly, the new
algorithm is almost identical to AdaBoost but with a sim-
ple yet critical modiﬁcation, and similar to AdaBoost in
the two-class case, this new algorithm combines weak clas-
siﬁers and only requires the performance of each weak clas-
siﬁer be better than random guessing. We show that the
proposed multi-class AdaBoost algorithm is equivalent to a
forward stagewise additive modeling algorithm that mini-
mizes a novel exponential loss for multi-class classiﬁcation.
Furthermore, we show that the exponential loss is a mem-
ber of a class of Fisher-consistent loss functions for multi-
class classiﬁcation. Combined with forward stagewise addi-
tive modeling, these loss functions can be used to derive
various multi-class boosting algorithms. We believe this pa-
per complements [11].

1.1 AdaBoost
Before delving into the new algorithm for multi-class
boosting, we brieﬂy review the multi-class classiﬁcation
problem and the AdaBoost algorithm [8]. Suppose we are
given a set of training data (x1, c1), . . . , (xn, cn), where the
input (prediction variable) xi ∈ Rp, and the output (re-
sponse variable) ci is qualitative and assumes values in a
ﬁnite set, e.g. {1, 2, . . . , K}. K is the number of classes. Usu-
ally it is assumed that the training data are independently
and identically distributed samples from an unknown prob-
ability distribution Prob(X, C). The goal is to ﬁnd a classiﬁ-
cation rule C(x) from the training data, so that when given a
new input x, we can assign it a class label c from {1, . . . , K}.
(cid:4)
Under the 0/1 loss, the misclassiﬁcation error rate of a classi-
IC(X)=kProb(C = k|X)
.

ﬁer C(x) is given by 1−(cid:2)

(cid:3)

K

k=1 EX

It is clear that
∗

C

(x) = arg max

k

Prob(C = k|X = x)

will minimize this quantity with the misclassiﬁcation error
rate equal to 1 − EX maxk Prob(C = k|X). This classiﬁer is

known as the Bayes classiﬁer, and its error rate is the Bayes
error rate.

The AdaBoost algorithm is an iterative procedure that
∗(x) by combining
tries to approximate the Bayes classiﬁer C
many weak classiﬁers. Starting with the unweighted train-
ing sample, the AdaBoost builds a classiﬁer, for example a
classiﬁcation tree [5], that produces class labels. If a training
data point is misclassiﬁed, the weight of that training data
point is increased (boosted). A second classiﬁer is built us-
ing the new weights, which are no longer equal. Again, mis-
classiﬁed training data have their weights boosted and the
procedure is repeated. Typically, one may build 500 or 1000
classiﬁers this way. A score is assigned to each classiﬁer, and
the ﬁnal classiﬁer is deﬁned as the linear combination of the
classiﬁers from each stage. Speciﬁcally, let T (x) denote a
weak multi-class classiﬁer that assigns a class label to x,
then the AdaBoost algorithm proceeds as follows:
Algorithm 1. AdaBoost [8]

1. Initialize the observation weights wi = 1/n, i =

1, 2, . . . , n.

2. For m = 1 to M:

(a) Fit a classiﬁer T (m)(x) to the training data using

weights wi.

(b) Compute

err(m) =

(cid:6)

(cid:7)
ci (cid:4)= T (m)(xi)

wiI

n(cid:5)

i=1

n(cid:5)

/

wi.

i=1

(c) Compute

α(m) = log

(d) Set

(cid:6)

wi ← wi · exp

for i = 1, 2, . . . , n.
(e) Re-normalize wi.

.

1 − err(m)
err(m)
(cid:6)

(cid:7)(cid:7)
ci (cid:4)= T (m)(xi)

,

α(m) · I

3. Output

C(x) = arg max

k

M(cid:5)

m=1

α(m) · I(T (m)(x) = k).

When applied to two-class classiﬁcation problems, Ad-
aBoost has been proved to be extremely successful in pro-
ducing accurate classiﬁers. In fact, [1] called AdaBoost with
trees the “best oﬀ-the-shelf classiﬁer in the world.” How-
ever, it is not the case for multi-class problems, although
AdaBoost was also proposed to be used in the multi-class
case [8]. Note that the theory of [8] assumes that the error
of each weak classiﬁer err(m) is less than 1/2 (or equiva-
lently α(m) > 0), with respect to the distribution on which

350 J. Zhu et al.

it was trained. This assumption is easily satisﬁed for two-
class classiﬁcation problems, because the error rate of ran-
dom guessing is 1/2. However, it is much harder to achieve
in the multi-class case, where the random guessing error rate
is (K − 1)/K. As pointed out by the inventors of AdaBoost,
the main disadvantage of AdaBoost is that it is unable to
handle weak learners with an error rate greater than 1/2. As
a result, AdaBoost may easily fail in the multi-class case. To
illustrate this point, we consider a simple three-class simu-
lation example. Each input x ∈ R10, and the ten input vari-
ables for all training examples are randomly drawn from
a ten-dimensional standard normal distribution. The three
classes are deﬁned as:

⎧⎪⎨
⎪⎩ 1,

2,
3,

if 0 ≤(cid:2)

if χ2
if χ2

10,1/3

10,2/3

c =

≤(cid:2)
≤(cid:2)

x2
j < χ2
10,1/3,
x2
j < χ2
x2
j ,

10,2/3,

10,k/3 is the (k/3)100% quantile of the χ2

where χ2
10 distribu-
tion, so as to put approximately equal numbers of observa-
tions in each class. In short, the decision boundaries separat-
ing successive classes are nested concentric ten-dimensional
spheres. The training sample size is 3000 with approximately
1000 training observations in each class. An independently
drawn test set of 10000 observations is used to estimate the
error rate.

Figure 1 (upper row) shows how AdaBoost breaks using
ten-terminal node trees as weak classiﬁers. As we can see
(upper left panel), the test error of AdaBoost actually starts
to increase after a few iterations, then levels oﬀ around 0.53.
What has happened can be understood from the upper mid-
dle and upper right panels: the err(m) starts below 0.5; after
a few iterations, it overshoots 0.5 (α(m) < 0), then quickly
hinges onto 0.5. Once err(m) is equal to 0.5, the weights of
the training samples do not get updated (α(m) = 0), hence
the same weak classiﬁer is ﬁtted over and over again but is
not added to the existing ﬁt, and the test error rate stays
the same.

This illustrative example may help explain why Ad-
aBoost is never used for multi-class problems. Instead, for
multi-class classiﬁcation problems, [21] proposed the Ad-
aBoost.MH algorithm which combines AdaBoost and the
one-versus-all strategy. There are also several other multi-
class extensions of the boosting idea, for example, the ECOC
in [19] and the logit-boost in [11].

1.2 Multi-class AdaBoost

We introduce a new multi-class generalization of Ad-
aBoost for multi-class classiﬁcation. We refer to our algo-
rithm as SAMME — Stagewise Additive Modeling using a
Multi-class Exponential loss function — this choice of name
will be clear in Section 2. Given the same setup as that of
AdaBoost, SAMME proceeds as follows:

Figure 1. Comparison of AdaBoost and the new algorithm SAMME on a simple three-class simulation example. The training
sample size is 3000, and the testing sample size is 10000. Ten-terminal node trees are used as weak classiﬁers. The upper row

is for AdaBoost and the lower row is for SAMME.

Algorithm 2. SAMME

1. Initialize the observation weights wi = 1/n, i =

1, 2, . . . , n.

2. For m = 1 to M:

(a) Fit a classiﬁer T (m)(x) to the training data using

weights wi.

(b) Compute

err(m) =

(c) Compute

(cid:6)

(cid:7)
ci (cid:4)= T (m)(xi)

wiI

n(cid:5)

i=1

n(cid:5)

/

wi.

i=1

(1)

α(m) = log

1 − err(m)
err(m)

+ log(K − 1).

(d) Set

(cid:6)

α(m) · I

(cid:6)

(cid:7)(cid:7)
ci (cid:4)= T (m)(xi)

,

wi ← wi · exp

for i = 1, . . . , n.
(e) Re-normalize wi.

3. Output

C(x) = arg max

k

M(cid:5)

m=1

α(m) · I(T (m)(x) = k).

Note that Algorithm 2 (SAMME) shares the same simple
modular structure of AdaBoost with a simple but subtle dif-
ference in (1), speciﬁcally, the extra term log(K − 1). Obvi-
ously, when K = 2, SAMME reduces to AdaBoost. However,
the term log(K − 1) in (1) is critical in the multi-class case
(K > 2). One immediate consequence is that now in order

Multi-class AdaBoost 351

for α(m) to be positive, we only need (1− err(m)) > 1/K, or
the accuracy of each weak classiﬁer to be better than ran-
dom guessing rather than 1/2. To appreciate its eﬀect, we
apply SAMME to the illustrative example in Section 1.1. As
can be seen from Fig. 1, the test error of SAMME quickly
decreases to a low value and keeps decreasing even after 600
iterations, which is exactly what we could expect from a
successful boosting algorithm. In Section 2, we shall show
that the term log(K −1) is not artiﬁcial, it follows naturally
from the multi-class generalization of the exponential loss
in the binary case.

The rest of the paper is organized as follows: In Sec-
tion 2, we give theoretical justiﬁcation for our new algo-
rithm SAMME. In Section 3, we present numerical results
on both simulation and real-world data. Summary and dis-
cussion regarding the implications of the new algorithm are
in Section 4.

2. STATISTICAL JUSTIFICATION

In this section, we are going to show that the extra term
log(K − 1) in (1) is not artiﬁcial; it makes Algorithm 2
equivalent to ﬁtting a forward stagewise additive model us-
ing a multi-class exponential loss function. Our arguments
are in line with [11] who developed a statistical perspective
on the original two-class AdaBoost algorithm, viewing the
two-class AdaBoost algorithm as forward stagewise additive
modeling using the exponential loss function

L(y, f) = e

−yf ,

where y = (I(c = 1) − I(c = 2)) ∈ {−1, 1} in a two-class
classiﬁcation setting. A key argument is to show that the
population minimizer of this exponential loss function is one
half of the logit transform

∗

f

(x) = arg min
f (x)

EY |X=xL(y, f(x))

=

1
2

log

Prob(c = 1|x)
Prob(c = 2|x) .

Therefore, the Bayes optimal classiﬁcation rule agrees with
∗(x). [11] recast AdaBoost as a functional gra-
the sign of f
∗(x). We note that
dient descent algorithm to approximate f
besides [11], [2] and [21] also made connections between the
original two-class AdaBoost algorithm and the exponential
loss function. We acknowledge that these views have been
inﬂuential in our thinking for this paper.

2.1 SAMME as forward stagewise additive

modeling

We now show that Algorithm 2 is equivalent to forward
stagewise additive modeling using a multi-class exponential
loss function.
We start with the forward stagewise additive modeling
using a general loss function L(·,·), then apply it to the

352 J. Zhu et al.

multi-class exponential loss function. In the multi-class clas-
siﬁcation setting, we can recode the output c with a K-
dimensional vector y, with all entries equal to − 1
K−1 except
a 1 in position k if c = k, i.e. y = (y1, . . . , yK)T, and:

(cid:12)

(2)

yk =

1,
− 1

K−1 ,

if c = k,
if c (cid:4)= k.

[14] and [16] used the same coding for the multi-class sup-
port vector machine. Given the training data, we wish to
ﬁnd f(x) = (f1(x), . . . , fK(x))T such that

n(cid:5)

L(yi, f(xi))

(3)

(4)

min
f (x)
subject to

i=1

f1(x) + ··· + fK(x) = 0.
We consider f(x) that has the following form:
M(cid:5)

f(x) =

β(m)g(m)(x),

m=1

where β(m) ∈ R are coeﬃcients, and g(m)(x) are basis func-
tions. We require g(x) to satisfy the symmetric constraint:

g1(x) + ··· + gK(x) = 0.

For example, the g(x) that we consider in this paper takes
value in one of the K possible K-dimensional vectors in (2);
speciﬁcally, at a given x, g(x) maps x onto Y:

g : x ∈ Rp → Y,

where Y is the set containing K K-dimensional vectors:

(5)

Y =

⎧⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩

(cid:6)
(cid:6)
1,− 1
K−1 , . . . ,− 1
K−1
− 1
K−1 , 1, . . . ,− 1
K−1
(cid:6)

K−1 , . . . ,− 1
− 1

K−1 , 1

...

(cid:7)
(cid:7)
(cid:7)

T

T

,

,

T

⎫⎪⎪⎪⎪⎪⎪⎬
⎪⎪⎪⎪⎪⎪⎭ .

Forward stagewise modeling approximates the solution
to (3)–(4) by sequentially adding new basis functions to the
expansion without adjusting the parameters and coeﬃcients
of those that have already been added. Speciﬁcally, the al-
gorithm starts with f (0)(x) = 0, sequentially selecting new
basis functions from a dictionary and adding them to the
current ﬁt:
Algorithm 3. Forward stagewise additive modeling

1. Initialize f (0)(x) = 0.
2. For m = 1 to M:

(a) Compute

(β(m), g(m)(x))

= arg min
β,g

n(cid:5)

i=1

L(yi, f (m−1)(xi) + βg(xi)).

(b) Set

Now, we consider using the multi-class exponential loss

function

L(y, f) = exp

f (m)(x) = f (m−1)(x) + β(m)g(m)(x).
(cid:17)
(y1f1 + ··· + yKfK)

(cid:17)

(cid:16)
− 1
(cid:16)
K
− 1
K

in the above forward stagewise modeling algorithm. The
choice of the loss function will be clear in Section 2.2 and
Section 2.3. Then in step (2a), we need to ﬁnd g(m)(x) (and
β(m)) to solve:

(cid:17)
i (f (m−1)(xi) + βg(xi))

(β(m), g(m))

= arg min
β,g

n(cid:5)

exp

= arg min
β,g

i=1

n(cid:5)

i=1

yT

(cid:16)
− 1
K
(cid:16)
− 1
K

(6)

(7)

wi exp

(cid:6)

i g(xi)

βyT

(cid:7)
i f (m−1)(xi)

− 1
where wi = exp
observation weights.

K yT

(cid:17)

,

are the un-normalized

Notice that every g(x) as in (5) has a one-to-one corre-
spondence with a multi-class classiﬁer T (x) in the following
way:

(cid:12)

n(cid:5)
(cid:16)

i=1

(8)

and vice versa:

T (x) = k,

if gk(x) = 1,

(9)

gk(x) =

1,
− 1

K−1 ,

if T (x) = k,
if T (x) (cid:4)= k.

Hence, solving for g(m)(x) in (7) is equivalent to ﬁnding the
multi-class classiﬁer T (m)(x) that can generate g(m)(x).
Lemma 1. The solution to (7) is

(10)

(11)

T (m)(x)

= arg min

wiI(ci (cid:4)= T (xi)),

β(m)
(K − 1)2

K

=

err(m) =

1 − err(m)
err(m)

(cid:17)
+ log(K − 1)
(cid:7)
ci (cid:4)= T (m)(xi)

n(cid:5)

wi.

/

,

i=1

log

(cid:6)

n(cid:5)

wiI

i=1

where err(m) is deﬁned as

= exp

yTf

,

This is equal to

Based on Lemma 1, the model is then updated
f (m)(x) = f (m−1)(x) + β(m)g(m)(x),
(cid:17)

and the weights for the next iteration will be

wi ← wi · exp

β(m)yT

i g(m)(xi)

.

(cid:16)
− 1
K

(12)

wi · e

− (K−1)2

(cid:18)

i g(m)(xi)

K2 α(m)yT
wi · e
wi · e

− K−1
K α(m)
1

,

K α(m)

=

,

if ci = T (xi),
if ci (cid:4)= T (xi),

where α(m) is deﬁned as in (1) with the extra term log(K −
1), and the new weight (12) is equivalent to the weight up-
dating scheme in Algorithm 2 (2d) after normalization.

1

a

is

It

to

also

task
is

simple
K (x))T

(cid:2)
that
arg maxk(f (m)
(x), . . . , f (m)
the
m=1 α(m) · I(T (m)(x) = k) in
output C(x) = arg maxk
Algorithm 2. Hence, Algorithm 2 can be considered as
forward stagewise additive modeling using the multi-class
exponential loss function.

equivalent

check

to

M

2.2 The multi-class exponential loss

We now justify the use of the multi-class exponential
loss (6). Firstly, we note that when K = 2, the sum-to-
zero constraint indicates f = (f1,−f1) and then the multi-
class exponential loss reduces to the exponential loss used
in binary classiﬁcation. [11] justiﬁed the exponential loss by
showing that its population minimizer is equivalent to the
Bayes rule. We follow the same arguments to investigate
what is the population minimizer of this multi-class expo-
nential loss function. Speciﬁcally, we are interested in

(13)

arg min
f (x)

EY |X=x exp

(cid:16)
− 1
K

(cid:17)
(Y1f1(x) + ··· + YKfK(x))

subject to f1(x) + ··· + fK(x) = 0. The Lagrange of this
constrained optimization problem can be written as:

(cid:16)
(cid:16)

(cid:17)
(cid:17)

Prob(c = 1|x)

− f1(x)
K − 1

exp
+ ···
+ exp
− λ (f1(x) + ··· + fK(x)) ,

− fK(x)
K − 1

Prob(c = K|x)

Multi-class AdaBoost 353

where λ is the Lagrange multiplier. Taking derivatives with
respect to fk and λ, we reach

(cid:17)

(cid:16)
− f1(x)
K − 1
(cid:17)

(cid:16)
− fK(x)
K − 1

− 1
K − 1

exp

− 1
K − 1

exp

Prob(c = 1|x) − λ = 0,
...
Prob(c = K|x) − λ = 0,
f1(x) + ··· + fK(x) = 0.

...

Solving this set of equations, we obtain the population min-
imizer

k (x) = (K − 1) log Prob(c = k|x)−
∗

f

(14)

log Prob(c = k

(cid:3)|x),

K(cid:5)

k(cid:2)=1

K − 1
K

for k = 1, . . . , K. Thus,

arg max

k

∗
k (x) = arg max
f

k

Prob(c = k|x),

which is the multi-class Bayes optimal classiﬁcation rule.
This result justiﬁes the use of this multi-class exponential
loss function. Equation (14) also provides a way to recover
the class probability Prob(c = k|x) once f
∗
k (x)’s are esti-
mated, i.e.

(15)

Prob(C = k|x) =

1

K−1 f

e

for k = 1, . . . , K.

1

K−1 f

∗
k (x)

e

∗

1 (x) + ··· + c

1

K−1 f

∗
K (x)

,

2.3 Fisher-consistent multi-class loss

functions

We have shown that the population minimizer of the new
multi-class exponential loss is equivalent to the multi-class
Bayes rule. This property is shared by many other multi-
class loss functions. Let us use the same notation as in Sec-
tion 2.1, and consider a general multi-class loss function

L(y, f) = φ

(cid:17)
(y1f1 + ··· + yKfK)

(cid:17)

,

yTf

= φ

(16)
where φ(·) is a non-negative valued function. The multi-
−t. We can use the gen-
class exponential loss uses φ(t) = e
eral multi-class loss function in Algorithm 3 to minimize the
empirical loss

(cid:17)

(17)

1
n

− 1
K

i f(xi)
yT

.

However, to derive a sensible algorithm, we need to require
the φ(·) function be Fisher-consistent. Speciﬁcally, we say

354 J. Zhu et al.

(cid:16)
− 1
(cid:16)
K
− 1
K

(cid:16)

n(cid:5)

φ

i=1

φ(·) is Fisher-consistent for K-class classiﬁcation, if for ∀x
in a set of full measure, the following optimization problem

(18)

arg min
f (x)

EY |X=xφ

(cid:16)

− 1
K

(cid:17)
(Y1f1(x) + ··· + YKfK(x))

subject to f1(x) +··· + fK(x) = 0, has a unique solution ˆf,
and

(19)

arg max

k

ˆfk(x) = arg max

k

Prob(C = k|x).

We use the sum-to-zero constraint to ensure the existence
and uniqueness of the solution to (18).
Note that as n → ∞, the empirical loss in (17) becomes
(cid:17)(cid:19)
(Y1f1(x) + ··· + YKfK(x))

(20) EX

EC|X=xφ

(cid:12)

.

(cid:16)
− 1
K

Therefore, the multi-class Fisher-consistent condition basi-
cally says that with inﬁnite samples, one can exactly recover
the multi-class Bayes rule by minimizing the multi-class loss
using φ(·). Thus our deﬁnition of Fisher-consistent losses is
a multi-class generalization of the binary Fisher-consistent
loss function discussed in [15].

In the following theorem, we show that there are a class
of convex functions that are Fisher-consistent for K-class
classiﬁcation, for all K ≥ 2.
Theorem 1. Let φ(t) be a non-negative twice diﬀerentiable
(cid:3)(cid:3)(t) > 0 for ∀t, then φ is Fisher-
function. If φ
consistent for K-class classiﬁcation for ∀K ≥ 2. Moreover,
let ˆf be the solution of (18), then we have

(cid:3)(0) < 0 and φ

(cid:6)

(cid:6)

1
K−1

(cid:3)
1/φ
k(cid:2)=1 1/φ(cid:3)

K

(cid:7)
ˆfk(x)

1
K−1

ˆfk(cid:2)(x)

(cid:7) ,

(21)

Prob(C = k|x) =

(cid:2)

for k = 1, . . . , K.

Theorem 1 immediately concludes that the three most
popular smooth loss functions, namely, exponential, logit
and L2 loss functions, are Fisher-consistent for all multi-
class classiﬁcation problems regardless the number of
classes. The inversion formula (21) allows one to easily con-
struct estimates for the conditional class probabilities. Ta-
ble 1 shows the explicit inversion formulae for computing the
conditional class probabilities using the exponential, logit
and L2 losses.

With these multi-class Fisher-consistent losses on hand,
we can use the forward stagewise modeling strategy to de-
rive various multi-class boosting algorithms by minimizing
the empirical multi-class loss. The biggest advantage of the
exponential loss is that it gives us a simple re-weighting for-
mula. Other multi-class loss functions may not lead to such
a simple closed-form re-weighting scheme. One could han-
dle this computation issue by employing the computational

Table 1. The probability inversion formula

exponential
−t
φ(t) = e

logit

φ(t) = log(1 + e

−t)

L2

φ(t) = (1 − t)2

1/(1− 1
K−1
(cid:2)=1

ˆfk (x))
1/(1− 1
ˆfk
K−1

K

k

(cid:2) (x))

(cid:2)

Prob(C = k|x)

(cid:2)

(cid:2)

1

K−1

e
K
(cid:2)=1

k

e

ˆfk(x)
1
ˆf
K−1
k

(cid:2) (x)

1

K−1

ˆfk(x)
1
ˆf
K−1
k

(1+e

1+e
(cid:2)=1

K

k

(cid:2) (x)

)

trick used in [10] and [6]. For example, [24] derived a multi-
class boosting algorithm using the logit loss. A multi-class
version of the L2 boosting can be derived following the lines
in [6]. We do not explore these directions in the current pa-
per. To ﬁx ideas, we shall focus on the multi-class AdaBoost
algorithm.

3. NUMERICAL RESULTS

In this section, we use both simulation data and real-
world data to demonstrate our multi-class AdaBoost algo-
rithm. For comparison, a single decision tree (CART; [5])
and AdaBoost.MH [21] are also ﬁt. We have chosen to com-
pare with the AdaBoost.MH algorithm because it is concep-
tually easy to understand and it seems to have dominated
other proposals in empirical studies [21]. Indeed, [22] also
argue that with large samples, AdaBoost.MH has the op-
timal classiﬁcation performance. The AdaBoost.MH algo-
rithm converts the K-class problem into that of estimating
a two-class classiﬁer on a training set K times as large, with
an additional feature deﬁned by the set of class labels. It is
essentially the same as the one vs. rest scheme [11].

We would like to emphasize that the purpose of our nu-
merical experiments is not to argue that SAMME is the ul-
timate multi-class classiﬁcation tool, but rather to illustrate
that it is a sensible algorithm, and that it is the natural ex-
tension of the AdaBoost algorithm to the multi-class case.

3.1 Simulation
We mimic a popular simulation example found in [5]. This
is a three-class problem with twenty one variables, and it is
considered to be a diﬃcult pattern recognition problem with
Bayes error equal to 0.140. The predictors are deﬁned by

⎧⎨
⎩ u · v1(j) + (1 − u) · v2(j) + j, Class 1,
u · v1(j) + (1 − u) · v3(j) + j, Class 2,
u · v2(j) + (1 − u) · v3(j) + j, Class 3,

(22)

xj =

where j = 1, . . . , 21, u is uniform on (0, 1), j are standard
normal variables, and the v(cid:4) are the shifted triangular wave-
forms: v1(j) = max(6 − |j − 11|, 0), v2(j) = v1(j − 4) and
v3(j) = v1(j + 4).

The training sample size is 300 so that approximately 100
training observations are in each class. We use the classiﬁ-
cation tree as the weak classiﬁer for SAMME. The trees are
built using a greedy, top-down recursive partitioning strat-
egy, and we restrict all trees within each method to have the

same number of terminal nodes. This number is chosen via
ﬁve-fold cross-validation. We use an independent test sam-
ple of size 5000 to estimate the error rate. Averaged results
over ten such independently drawn training-test set combi-
nations are shown in Fig. 2 and Table 2.

As we can see, for this particular simulation example,
SAMME performs slightly better than the AdaBoost.MH al-
gorithm. A paired t-test across the ten independent compar-
isons indicates a signiﬁcant diﬀerence with p-value around
0.003.

3.2 Real data
In this section, we show the results of running SAMME on
a collection of datasets from the UC-Irvine machine learn-
ing archive [18]. Seven datasets were used: Letter, Nursery,
Pendigits, Satimage, Segmentation, Thyroid and Vowel.
These datasets come with pre-speciﬁed training and testing
sets, and are summarized in Table 3. They cover a wide
range of scenarios: the number of classes ranges from 3
to 26, and the size of the training data ranges from 210
to 16,000 data points. The types of input variables in-
clude both numerical and categorical, for example, in the
Nursery dataset, all input variables are categorical vari-
ables. We used a classiﬁcation tree as the weak classiﬁer
in each case. Again, the trees were built using a greedy,
top-down recursive partitioning strategy. We restricted all
trees within each method to have the same number of ter-
minal nodes, and this number was chosen via ﬁve-fold cross-
validation.

(cid:20)
Figure 3 compares SAMME and AdaBoost.MH. The test
error rates are summarized in Table 5. The standard er-
te.err · (1 − te.err)/n.te, where
rors are approximated by
te.err is the test error, and n.te is the size of the testing
data.

The most interesting result is on the Vowel dataset. This
is a diﬃcult classiﬁcation problem, and the best methods
achieve around 40% errors on the test data [12]. The data
was collected by [7], who recorded examples of the eleven
steady state vowels of English spoken by ﬁfteen speakers for
a speaker normalization study. The International Phonetic
Association (IPA) symbols that represent the vowels and the
words in which the eleven vowel sounds were recorded are
given in Table 4.

Four male and four female speakers were used to train
the classiﬁer, and then another four male and three fe-
male speakers were used for testing the performance. Each

Multi-class AdaBoost 355

Figure 2. Test errors for SAMME and AdaBoost.MH on the waveform simulation example. The training sample size is 300,
and the testing sample size is 5000. The results are averages of over ten independently drawn training-test set combinations.

Table 2. Test error rates % of diﬀerent methods on the

waveform data. The results are averaged over ten

independently drawn datasets. For comparison, a single

decision tree is also ﬁt

200

Iterations

400

600

CART error = 28.4 (1.8)

17.1 (0.6)
16.7 (0.8)

17.0 (0.5)
16.6 (0.7)

17.0 (0.6)
16.6 (0.6)

Method
Waveform

Ada.MH
SAMME

Table 3. Summary of seven benchmark datasets

Dataset
Letter
Nursery
Pendigits
Satimage
Segmentation
Thyroid
Vowel

#Train
16000
8840
7494
4435
210
3772
528

#Test
4000
3790
3498
2000
2100
3428
462

#Variables

#Classes

16
8
16
36
19
21
10

26
3
10
6
7
3
11

speaker yielded six frames of speech from eleven vowels. This
gave 528 frames from the eight speakers used as the train-
ing data and 462 frames from the seven speakers used as
the testing data. Ten predictors are derived from the digi-
tized speech in a rather complicated way, but standard in
the speech recognition world. As we can see from Fig. 3 and
Table 5, for this particular dataset, the SAMME algorithm

356 J. Zhu et al.

Table 4. The International Phonetic Association (IPA)

symbols that represent the eleven vowels

vowel word
i:
E
a:

heed O
head U
3:
hard

vowel word

vowel word
hod
I
hood A
heard Y

hid
had
hud

vowel word
hoard
C:
u:
who’d

performs almost 15% better than the AdaBoost.MH algo-
rithm.

For other datasets, the SAMME algorithm performs
slightly better
than the AdaBoost.MH algorithm on
Letter, Pendigits, and Thyroid, while slightly worse on
Segmentation. In the Segmentation data, there are only
210 training data points, so the diﬀerence might be just
due to randomness. It is also worth noting that for the
Nursery data, both the SAMME algorithm and the Ad-
aBoost.MH algorithm are able to reduce the test error
to zero, while a single decision tree has about 0.8% test
error rate. Overall, we are comfortable to say that the
performance of SAMME is comparable with that of the
AdaBoost.MH.

For the purpose of further investigation, we also merged
the training and the test sets, and randomly split them into
new training and testing sets. The procedure was repeated
ten times. Again, the performance of SAMME is comparable
with that of the AdaBoost.MH. For the sake of space, we do
not present these results.

Figure 3. Test errors for SAMME and AdaBoost.MH on six benchmark datasets. These datasets come with pre-speciﬁed

training and testing splits, and they are summarized in Table 3. The results for the Nursery data are not shown for the test

error rates are reduced to zero for both methods.

4. DISCUSSION

The statistical view of boosting, as illustrated in [11],
shows that the two-class AdaBoost builds an additive model
to approximate the two-class Bayes rule. Following the same
statistical principle, we have derived SAMME, the natural
and clean multi-class extension of the two-class AdaBoost
algorithm, and we have shown that

• SAMME adaptively implements the multi-class Bayes
rule by ﬁtting a forward stagewise additive model for
multi-class problems;
• SAMME follows closely to the philosophy of boosting,
i.e. adaptively combining weak classiﬁers (rather than
regressors as in logit-boost [11] and MART [10]) into a
powerful one;
• At each stage, SAMME returns only one weighted clas-

Multi-class AdaBoost 357

Table 5. Test error rates % on seven benchmark real

testing splits. The standard errors (in parentheses) are

(cid:20)
datasets. The datasets come with pre-speciﬁed training and
te.err · (1 − te.err)/n.te, where te.err is
approximated by
the test error, and n.te is the size of the testing data. For
comparison, a single decision tree was also ﬁt, and the tree

size was determined by ﬁve-fold cross-validation

Method
Letter

Ada.MH
SAMME
Nursery

Ada.MH
SAMME
Pendigits

Ada.MH
SAMME
Satimage

Ada.MH
SAMME
Segmentation

Ada.MH
SAMME
Thyroid

Ada.MH
SAMME
Vowel

Ada.MH
SAMME

200

Iterations

400

600

CART error = 13.5 (0.5)

3.0 (0.3)
2.6 (0.3)

2.8 (0.3)
2.4 (0.2)

2.6 (0.3)
2.3 (0.2)

CART error = 0.79 (0.14)

0
0

3.0 (0.3)
2.5 (0.3)

8.7 (0.6)
8.6 (0.6)

4.5 (0.5)
4.9 (0.5)

0
0

0
0

CART error = 8.3 (0.5)

3.0 (0.3)
2.5 (0.3)

2.8 (0.3)
2.5 (0.3)

CART error = 13.8 (0.8)

8.4 (0.6)
8.2 (0.6)

8.5 (0.6)
8.5 (0.6)

CART error = 9.3 (0.6)

4.5 (0.5)
5.0 (0.5)

4.5 (0.5)
5.1 (0.5)

CART error = 0.64 (0.14)

0.67 (0.14)
0.58 (0.13)

0.67 (0.14)
0.61 (0.13)

0.67 (0.14)
0.58 (0.13)

CART error = 53.0 (2.3)

52.8 (2.3)
43.9 (2.3)

51.5 (2.3)
43.3 (2.3)

51.5 (2.3)
43.3 (2.3)

siﬁer (rather than K), and the weak classiﬁer only needs
to be better than K-class random guessing;
• SAMME shares the same simple modular structure of

AdaBoost.

Our numerical experiments have indicated that Ad-
aBoost.MH in general performs very well and SAMME’s
performance is comparable with that of the AdaBoost.MH,
and sometimes slightly better. However, we would like to
emphasize that our goal is not to argue that SAMME is
the ultimate multi-class classiﬁcation tool, but rather to il-
lustrate that it is the natural extension of the AdaBoost
algorithm to the multi-class case. The success of SAMME
is used here to demonstrate the usefulness of the forward
stagewise modeling view of boosting.

[11] called the AdaBoost algorithm Discrete AdaBoost
and proposed Real AdaBoost and Gentle AdaBoost algo-
rithms which combine regressors to estimate the conditional
class probability. Using their language, SAMME is also a dis-
crete multi-class AdaBoost. We have also derived the corre-
sponding Real Multi-class AdaBoost and Gentle Multi-class

358 J. Zhu et al.

AdaBoost [23, 24]. These results further demonstrate the
usefulness of the forward stagewise modeling view of boost-
ing.

It should also be emphasized here that although our sta-
tistical view of boosting leads to interesting and useful re-
sults, we do not argue it is the ultimate explanation of boost-
ing. Why boosting works is still an open question. Interested
readers are referred to the discussions on [11]. [9] mentioned
that the forward stagewise modeling view of AdaBoost does
not oﬀer a bound on the generalization error as in the orig-
inal AdaBoost paper [8]. [3] also pointed out that the sta-
tistical view of boosting does not explain why AdaBoost is
robust against overﬁtting. Later, his understandings of Ad-
aBoost lead to the invention of random forests [4].

Finally, we discuss the computational cost of SAMME.
Suppose one uses a classiﬁcation tree as the weak learner,
and the depth of each tree is ﬁxed as d, then the computa-
tional cost for building each tree is O(dpn log(n)), where p
is the dimension of the input x. The computational cost for
our SAMME algorithm is then O(dpn log(n)M) since there
are M iterations.

The SAMME algorithm has been implemented in the R
computing environment, and will be publicly available from
the authors’ websites.

APPENDIX: PROOFS

Lemma 1. First, for any ﬁxed value of β > 0, using the
deﬁnition (8), one can express the criterion in (7) as:

(cid:5)

ci(cid:4)=T (xi)

β

(K−1)2

wie

(cid:5)

i

wiI(ci (cid:4)= T (xi)).

(cid:5)

− β

K−1 +

wie

(cid:5)

ci=T (xi)
− β
K−1

= e

wi +

(23)

(e

i

β

(K−1)2 − e

− β

K−1 )

Since only the last sum depends on the classiﬁer T (x), we
get that (10) holds. Now plugging (10) into (7) and solving
for β, we obtain (11) (note that (23) is a convex function
of β).

Theorem 1. Firstly, we note that under the sum-to-zero
constraint,

EY |X=xφ

(cid:16)

1
K

(cid:17)

(cid:17)
(Y1f1(x) + ··· + YKfK(x))
(cid:16)
Prob(C = 1|x) +
f1(x)
K − 1
(cid:16)
(cid:17)
fK(x)
K − 1

Prob(C = K|x).

+φ

= φ
. . .

Therefore, we wish to solve

min
f

subject to

1

φ(
. . .
+φ(

K − 1 f1(x))Prob(C = 1|x) +
K − 1 fK(x))Prob(C = 1|x)
K(cid:5)

1

fk(x) = 0.

k=1

For convenience, let pk = Prob(C = k|x), k = 1, 2, . . . , K
and we omit x in fk(x). Using the Lagrangian multiplier,
we deﬁne

Q(f) = φ(

1
K − 1 f1)p1+
··· +
1
K − 1 fK)pK +
φ(

1

K − 1 λ(f1 + . . . + fK).

Then we have
∂Q(f)

1

(cid:3)

1

=

∂fk

(24)

K − 1 φ
for k = 1, . . . , K. Since φ
verse function, denoted by ψ. Equation (24) gives
ψ(− λ

). By the sum-to-zero constraint on f, we have

K − 1 λ = 0,

(cid:3) has an in-
K−1 fk =

1

1

K − 1 fk)pk +
(
(cid:3)(cid:3)(t) > 0 for ∀t, φ
(cid:16)
− λ
pk

= 0.

(cid:17)

K(cid:5)

ψ

k=1

pk

(25)

1

∗

pk

pk

pk

=

(K−1)2 φ

> −φ

(cid:3)(cid:3)( 1
(cid:3)(0) > 0, we have λ

(cid:3) is a strictly monotone increasing function, so is
Since φ
ψ. Thus the left hand size (LHS) of (25) is a decreasing
function of λ. It suﬃces to show that equation (25) has a
∗, which is the unique root. Then it is easy to see
root λ
that ˆfk = ψ(− λ
) is the unique minimizer of (18), for the
Hessian matrix of Q(f) is a diagonal matrix and the k-th
diagonal element is ∂2Q(f )
K−1 fk) > 0. Note
∂f 2
that when λ = −φ
(cid:3)(0), then
k
ψ(− λ
(cid:3)(0)) = 0. So the LHS of (25) is negative
) < ψ (φ
when λ = −φ
(cid:3)(0) > 0. On the other hand, let us deﬁne
A = {a : φ
(cid:3)(t) → 0−
as t → ∞ (since φ is convex). If A is not empty, denote
∗ = inf A. By the fact φ
(cid:3)(0) < 0, we conclude a
> 0.
a
∗−. In both cases, we see that ∃
(cid:3)(t) → 0− as t → a
Hence φ
a small enough λ0 > 0 such that ψ(− λ0
) > 0 for all k. So
the LHS of (25) is positive when λ = λ0 > 0. Therefore there
(cid:3)(0)) such that equation (25)
must be a positive λ
holds. Now we show the minimizer ˆf agrees with the Bayes
rule. Without loss of generality, let p1 > pk for ∀k (cid:4)= 1.
Then since − λ
for ∀k (cid:4)= 1, we have ˆf1 > ˆfk for
(cid:2)
∀k (cid:4)= 1. For the inversion formula, we note pk = −
,
and

(cid:3)(a) = 0}. If A is an empty set, then φ

ˆfk)
= 1. Hence it

∗ ∈ (λ0,−φ

k=1 pj = 1 requires

> − λ

∗
λ
1
K−1

(cid:2)

K
k=1

−

φ(cid:2)(

pk

pk

p1

∗

K

∗

∗

∗
λ
1
K−1

φ(cid:2)(

ˆfk)

(cid:2)
∗ = −(

follows that λ
obtained.

K

k=1 1/φ

(cid:3)( 1
K−1

ˆfk))−1. Then (21) is

ACKNOWLEDGMENTS

We would like to dedicate this work to the memory of
Leo Breiman, who passed away while we were ﬁnalizing this
manuscript. Leo Breiman has made tremendous contribu-
tions to the study of statistics and machine learning. His
work has greatly inﬂuenced us.

Received 22 May 2009

REFERENCES

[1] Breiman, L. (1996). Bagging predictors. Machine Learning 24

123–140.

[2] Breiman, L. (1999). Prediction games and arcing algorithms.

Neural Computation 7 1493–1517.

[3] Breiman, L. (2000). Discussion of “Additive logistic regression: a
statistical view of boosting” by Friedman, Hastie and Tibshirani.
Annals of Statistics 28 374–377. MR1790002

[4] Breiman, L. (2001). Random forests. Machine Learning 45 5–32.
[5] Breiman, L., Friedman, J., Olshen, R., and Stone, C. (1984).
Classiﬁcation and Regression Trees. Wadsworth, Belmont, CA.
MR0726392

[6] B¨uhlmann, P. and Yu, B. (2003). Boosting with the (cid:2)2 loss:
regression and classiﬁcation. Journal of the American Statistical
Association 98 324–339. MR1995709

[7] Deterding, D. (1989). Speaker Normalization for Automatic

Speech Recognition. University of Cambridge. Ph.D. thesis.

[8] Freund, Y. and Schapire, R. (1997). A decision theoretic
generalization of on-line learning and an application to boost-
ing. Journal of Computer and System Sciences 55 119–139.
MR1473055

[9] Freund, Y. and Schapire, R. (2000). Discussion of “Addi-
tive logistic regression: a statistical view on boosting” by Fried-
man, Hastie and Tibshirani. Annals of Statistics 28 391–393.
MR1790002

[10] Friedman, J. (2001). Greedy function approximation: a gra-
dient boosting machine. Annals of Statistics 29 1189–1232.
MR1873328

[11] Friedman, J., Hastie, T., and Tibshirani, R. (2000). Additive
logistic regression: a statistical view of boosting. Annals of Statis-
tics 28 337–407. MR1790002

[12] Hastie, T., Tibshirani, R., and Friedman, J. (2001). The
Elements of Statistical Learning. Springer-Verlag, New York.
MR1851606

[13] Koltchinskii, V. and Panchenko, D. (2002). Empirical margin
distributions and bounding the generalization error of combined
classiﬁers. Annals of Statistics 30 1–50. MR1892654

[14] Lee, Y., Lin, Y., and Wahba, G. (2004). Multicategory support
vector machines, theory, and application to the classiﬁcation of
microarray data and satellite radiance data. Journal of the Amer-
ican Statistical Association 99 67–81. MR2054287

[15] Lin, Y. (2004). A note on margin-based loss functions in
classiﬁcation. Statistics and Probability Letters 68 73–82.
MR2064687

[16] Liu, Y. and Shen, X. (2006). Multicategory psi-learning. Journal
of the American Statistical Association 101 500–509. MR2256170
[17] Mason, L., Baxter, J., Bartlett, P., and Frean, M. (1999).
Boosting algorithms as gradient descent in function space. Neural
Information Processing Systems 12.

[18] Merz, C. and Murphy, P. (1998). UCI repository of machine

learning databases.

Multi-class AdaBoost 359

Hui Zou
School of Statistics
University of Minnesota
Minneapolis, MN 55455
USA
E-mail address: hzou@stat.umn.edu

Saharon Rosset
Department of Statistics
Tel Aviv University
Tel Aviv 69978
Israel
E-mail address: saharon@tau.ac.il

Trevor Hastie
Department of Statistics
Stanford University
Stanford, CA 94305
USA
E-mail address: hastie@stanford.edu

[19] Schapire, R. (1997). Using output codes to boost multiclass
learning problems. Proceedings of the Fourteenth International
Conference on Machine Learning. Morgan Kauﬀman.

[20] Schapire, R., Freund, Y., Bartlett, P., and Lee, W. (1998).
Boosting the margin: a new explanation for the eﬀectiveness of
voting methods. Annals of Statistics 26 1651–1686. MR1673273
[21] Schapire, R. and Singer, Y. (1999). Improved boosting algo-
rithms using conﬁdence-rated prediction. Machine Learning 37
297–336. MR1811573

[22] Zhang, T. (2004). Statistical analysis of some multi-category
large margin classiﬁcation methods. Journal of Machine Learning
Research 5 1225–1251. MR2248016

[23] Zhu, J., Rosset, S., Zou, H., and Hastie, T. (2005). Multi-
class AdaBoost. Technical Report # 430, Department of Statis-
tics, University of Michigan.

[24] Zou, H., Zhu, J., and Hastie, T. (2008). The margin vector,
admissable loss, and multiclass margin-based classiﬁers. Annals
of Applied Statistics 2 1290–1306.

Ji Zhu
Department of Statistics
University of Michigan
Ann Arbor, MI 48109
USA
E-mail address: jizhu@umich.edu

360 J. Zhu et al.

