From: KDD-97 Proceedings. Copyright © 1997, AAAI (www.aaai.org). All rights reserved. 

Discriminative 

vs  Informative 

Learning 

., 
LI  <” 

.I’ 

‘, 

Y.  Dan  Rubinstein 

and  Trevor  Hastie 

Department  of  Statistics 

Stanford  University 
Stanford,  CA  94305 
ruby@stat.stanford.edu 
trevor@stat.stanford.edu 

Abstract 

The  goal  of  pattern  classification  can  be  approached 
from  two points  of view:  informative  - where the classi- 
fier  learns the  class densities, or discriminative  - where 
the  focus  is  on  learning  the  class boundaries  without 
regard  to  the  underlying  class densities.  We  review 
and  synthesize  the  tradeoffs  between  these  two  ap- 
proaches for  simple  classifiers,  and  extend  the  results 
to  modern  techniques such as Naive  Bayes and  Gener- 
alized  Additive  Models.  Data  mining  applications  of- 
I^_  -----A.-:- 
,l:m,no:,nn,  &..,+..ron 
IJeLl uyaraw  1u IJUI: U”Lllcz,U “I Lup  uLLlL~~DI”II(III 
where the  tradeoffs  between informative  and discrimi- 
native  classifiers are especially relevant.  Experimental 
results  are provided  for  simulated  and real  data.’ 

LL- 2--.-s.. 

,cL:,L 

IGi(lr”UIG.3 

KDD 

and  Classification 

Automatic  classification  is  among the  main  goals of 
data  mining  systems  (Fayyad,  Piatetsky-Shapiro,  & 
Smyth  1996).  Given  a  database of  observations con- 
sisting  of input  (predictor)  and output  (response, i.e. 
class label) variables, a classifier seeks to learn relation- 
ships between the  predictors  and response  that  allow 
it  to  assign a new observation, whose response is un- 
known, 
classes.  The 
goal of good classification is to  minimize misclassifica- 
tions  or the  expected  cost  of  misclassifications 
if  some 
types  of  mistakes  are  more  costly  than  others. 

into  one  of  the  K  predetermined 

Classifiers can be segmented into  two  groups: 

1.  Informative:  These are  classifiers that  model  the 

in  done 
-- 

f’laraifirn.t.inn 

hv  nun.mininp 
IJ 
---I---------o 

~lslaa  rlancit;cl~ 
“A~“”  UUAIYAYA”“.  d,,,,...,,.,,.- 
the  likelihood  of  each class producing  the  features 
and assigning to  the most likely  class. Examples in- 
clude Fisher Discriminant  Analysis, Hidden Markov 
Models,  and Naive  Bayes.  Because each class den- 
sity  is  considered separately from  the  others, these 
models are relatively  easy to  train. 

‘Copyright  01997,  American  Association  for  Artificial 

Intelligence  (www,aaai.org).  All  rights  reserved. 

2.  Discriminative:  Here, no attempt  is made to  model 
the  underlying  class feature densities.  The  focus is 
on modeling the  class boundaries or the  class mem- 
bership probabilities  directly.  Examples include Lo- 
gistic Regression, Neural Networks, and Generalized 
Additive  Models.  Because  this  requires  simultane- 
ous consideration  of  all  other  classes, these mod- 
els are harder to  train,  often  involve  iterative  algo- 
rithms,  and do not  scale well. 

lated  via  Bayes  rule,  but  often  lead  to  different  decision 
rules,  especially  when  the  class  density  model  is  incor- 
rect  or  there  are  few  training  observations 
to 
the number of parameters in  the model. 

relative 

the 

training 

and  classification 

the  two  approaches 

lessons  can  be  applied 

There  are  tradeoffs  between 

in 
terms  of  ease  of 
perfor- 
mance. Precise statements can only  be made for  very 
simple  classifiers,  but 
to 
more  sophisticated  techniques.  In  this  paper  we re- 
view  the  known  statistical  results that  apply  to  sim- 
ple  non-discriminative 
classifiers  and  we  demonstrate 
how  modern  techniques can  be  categorized as being 
discriminative  or  not.  Using  Naive  Bayes and  GAM 
applied  both 
and  real  data,  we  exem- 
plify  that,  counter-intuitively,  discriminative  training 
may  not  always lead to  the  best  classifiers.  We  also 
propose  methods  of  combining 
the  two  approaches.  We 
focus on parametric techniques although similar results 
obtain  in  the  non-parametric  case.  With 
the  advent 
of  increasingly  sophisticated  classification  techniques, 
:c 
Ib 
falls  in,  because the  assumptions, problems and fixes 
for  each type  are different. 

to  simulation 

th,, 
“A&G  CIIclJUUIII~L 

:n 
:-s.r\rtnn+ 
13  ML~“llJallb 

nqtomrr,.., 
bauzEj”LJ 

+n 
IJ” 

znnl:n,,‘*z.,ha+ 
lGall*G, 

NyIIcb” 

~1nnn;fic.r 

Overview 

of  Bayesian  Classification 

Theory 

Formally, 
the  classification  problem  consists  of  assign- 
ing  a vector observation 2  E 7V  into  one of K  classes. 
The true  class is denoted by y  E { 1, . . . , K}.  The clas- 

Rubinstein 

49 

sifier is a mapping  that  assigns class labels to  observa- 
tions:  y  : z  +  (1,.  . . , K}.  There is also a cost matrix 
c(r,s),  T, s =  1,. . .) K  which describes the cost associ- 
ated with  misclassifying a member of class-r to  class-s. 
A  special case is O/l  loss, c(r, s) =  1 -  c$.,~ =  1 if  T #  s 
and 0 otherwise. 

Underlying  the  observations is  a true  joint  density 
P(x, y)  =  P(ylz)P(z)  =  P(zly)P(y)  which  is  unknown. 
The goal is to  minimize the total  cost of errors, known 
as the  overall  risk  and  this  is  achieved by  the  Bayes 
classifier (Duda  &  Hart.  1973) 

=  n-y-l 

~logPe(xilyi) 
i 

+  lOWe( 

For  the  gaussian case this  yields  the  well  known 
estimates  ?k  =  nk/n,fik  =  L%k =  &  cyiCIE X4,x  = 
i  Cf  Cyizk(xi 
-  Zk)(xi  -  &)’  where nk  is the  num- 
ber of observations from  class Ic and n =  Cf  nk.  The 
discriminant  functions  are 

xk (x>  = 

(log 2 

- 

;(Pk  +  PK)WPk 

-  /.a))  + 

y(x) 

= 

rnp-  lc 

=  m,ax-lP(Y  =  Iclx)  (O/l  loss). 

ckm)P(Y  =  4x1 

nZ=l 

(1) 

(2) 

For O/l  loss this  reduces to  classifying x  to  the class Ic 
for  which  the  class posterior  probability  P(y  =  IcIx) is 
maximum. 

In  practice,  the  true  density P(x, y)  is unknown  and 
all  that  is  available  is  a  set  of  training  observations 
for  i  =  1,. . . ,n.  Many  classification  tech- 
(xi,yi) 
niques seek to estimate the class posterior probabilities 
P(y =  klx),  since we see in  (2)  that  optimal  classifica- 
tion  can be achieved if  these are known perfectly  (for a 
discussion on the relationship  between class posteriors 
and neural net  outputs  see (Ney  1995)). 

For convenience in what  follows, we will  make use of 

the  discriminant  function 

X,(x)  =  log pq(yy _=;;; 

. 

X 

This  discriminant  preserves the  ordering  of  the  class 
posterior probabilities  and can be used instead of them 
for  classification. 
Informative 
Rather  than  estimate  the  class posteriors  P(ylx)  di- 
rectly,  the  class densities p(xlg)  and  priors  p(y)  are 
estimated.  The  operative  equation here is Bayes rule, 
which  gives the  class posteriors  in  terms  of  the  class 
densities and priors 

Classification 

P(Y =  $4  = 

PblY  =  WP(Y =  k) 

c:  P(4Y  =  rn>P(Y  =  m> * 

Typically  some model is  chosen for  the  class densi- 
ties,  for  example gaussian, Pe(XlY =  k)  =  n/(x;  pk, C) 
(heree={pi 
parameters are estimated from the data by maximizing 
the  full  log likelihood 

,...,  pK,7rr ,...,  7rK,C}),andthemodel 

oMLE 

= 

meaX  -l~hWe(xi,Yi) 

i 

50 

KDD-97 

+  1)/2  + 
and are linear  in  x.  Note that  while  Kp+P(p 
(K  -  1)  parameters  are estimated,  the  discriminants 
involve only  (K  -  l)(p  +  1) parameters. 

The important  points  with  informative  training  are 
1.  A  model PO (XIY) is assumed for  the  class densities. 
2.  The parameters are obtained  by maximizing  the full 

log likelihood  log PO (x: Y) =  log nn (ulz)sn (5). 
I 

v-“\“I 

,a”\ 

3.  A  decision boundary  is induced,  and the  model pa- 
rameters may  appear in  a way that  reduces the  ef- 
fective  number of parameters in  the  discriminant. 

Classification 

Discriminative 
The  discriminative  approach models the  class posteri- 
ors and hence the discriminants  directly.  The discrimi- 
native approach is more flexible with  regard to the class 
densities it  is  capable of  modeling.  By  only  restrict- 
ing  the  discriminant  &,(X)  =  log[p(y  =  klz)/P(y  = 
Klx>l = lwM4y  = ~)P(Y = WP(~Y = KMy  = K)l 
we are capable of modeling any class densities that  are 
exponential  ‘Ws”  of each other 

P(X[Y =  k)  =  exk(“)p(xly  =  K)  p(y  =  k)  . 

P(Y =  K) 

In  particular, 
the  informative  model,  as regards  the 
class  densities,  is  seen to  be  a  special  instance  of 
the  more  general discriminative  model.  The  example 
above was a special case with  a gaussian as the  “carri- 
er”  density 

p(xly  =  k)  =  N(x;  pi,  C)ep~+ob’s (z) 

while  the  corresponding  discriminative  model  allows 
any carrier  density 

~(43 = k) = fdx; e>e 

so long as the  discriminant  is linear. 

Parameter  estimation  in  the  discriminative  case is 

cnrr;nrl nrrt 
cI.zJ,II‘~U “L&U “J IILu,‘u”I’Y”‘~ “IA” U”IIU1”L”A~ L”b L~l.Y~~~L”“U 

h-7  mcavimininrr 

1ilmlihnnA 

rr-mrlitinn 

thn 

lna 

9 DISCR 

7L 

=  mop  -l  ~logPe(YiIxi). 

i 

On  the  one hand,  maximizing  the  conditional  likeli- 
hood is a natural  thing  to  do because it  is directly  fo- 
cused on the  class posteriors p(ylz)  which is all that  is 
required in  order to  classify well.  However, it  is ignor- 
ing part  of the data,  namely, the marginal  distribution 
p(z).  Compare to  the  full  likelihood  case where each 
observation contributes  p(~:, y)  =  p(ylz)p(z). 
The  dis- 
criminative  approach,  which  uses only  the  first  term 
on the  right  side, throws  away the  information  in  the 
marginal  density of x.  Thus,  if  the class density model 
is  correct,  the  discriminative  approach ignores useful 
information.  However, ignoring  the  class models may 
be good if  they  are incorrect.  The  table  below  sum- 
marizes the  main  comparisons between the  two  ap- 
proaches. 

Informative 
Full log likelihood  Conditional 
xi 

logpe(zi,  y;) 

Discriminative 

likelihood 
JJ  logPe(Yil~i) 
densities  Discriminant 

log 

functions  XI,(~) 
“Hard” 

Objective 
Function 

Model 
Assumptions 
Parameter 
Estimation 
Advantages  More 

Class 
p(zly  =  L) 
“Easy” 

if  model  correct, 
borrows  strength 

1 from  p(z) 

- 
if  model 

I 

I- 
_...._^^I 
IuC”I-recb. 

Disadvantages 1 Bias 

efficient  More  flexible,  ro- 
because 

bust 
fewer 

1 assumptions 

is  1 May  also  be  bi- 

I 

.-&d 

T --^..-- 
lgu”ltT3 

:, 
111- 

I 

formLion 

in  w(x) 

Logistic  Regression 

vs  Linear 

Discriminant 

Analysis 

= 

A  lot  of  insight  can  be  gained  from  examining  the 
two  class  case  where 
the  class  densities  are  assumed 
to  be  Gaussian  p~(zly 
=  N(,t~k,  C) [z]  = 
d& 
(X -  pk))  with  priors 
PS(Y =  k)  =m. 

k) 
-  #)‘C-l 

exp -(+(x 

When  the  popuiations  are  gaussian,  informative 
classification  is  more  efficient  than  discriminative, 
ie 
fewer training  observations are required or for  a fixed 
number  of  training  observations, better  classification 
is  obtained  (Efron  1975; O’Neill  1980; Ruiz-Velasco 
1991). Even when the  class densities are not  gaussian 
there are circumstances -  such as when the classes are 
well separated -  when informative  training  does about 
as well as discriminative  (Byth  &  McLachlan  1980). 

The  informative  approach requires estimating  class 
means and a pooled covariance which  requires only  a 
the  data:  The  diqcriminativc!  an- 
sippde  SWP.AII thrmeh 
------o- 
---- 
preach requires an iterative  optimization  via a gradient 
descent of the  conditional  likelihood. 

-__-  -_L- ________ 

o--  -..--= 

21-. -  Ir 

Figure  1:  Class  densities  for  3  cases of  simulation 
data.  The class boundaries derived from  many  (10000) 
training  observations for  Normal  Discriminant  Anal- 
ysis  (LDA)  and  Logistic  Regression (LOGREG)  are 
shown: points to  the left  of the boundary  are classified 
to  class 1. 

iments.  Case 3 is a gaussian class case for which we ex- 
pect LDA  to  do better  than  LOGREG  when the mod- 
els are learned using training  data.  For each case, 100 
training  sets with  5 observations per  class, i.e.  p(y  = 
1) =  p(y  =  2)  =  l/2,  were drawn according to the class 
densities pictured.  LDA  and LOGREG  classifiers were 
trained  for  each set and the  exact probability  of error 
was computed using integration  over a grid  P(error)  = 

The  table  below provides error  rates using  the  two 
procedures.  Each  column  corresponds to  a  different 
density case as depicted in figure  1. The first  two rows 
are ‘<best” in the  sense that  the model is trained  using 
the complete density, not  a sample of training  observa- 
tions.  The  remaining 
errors of the  error  rates across 100 training  sets, each 
of which contained 5 observations per  class. 

rows  are  averages  and  standard 

Rubinstein 

51 

I  case 
LDA  - best 
LOGREG  -  best 
LDA 
SECLDA) 
LO-GREG 
SE(LOGREG) 

21 
8.6 
3.1 
9.6 

31 
II 
6.7 
28.1 
6.7 
8.8 
25.2 
7.6 
0.47  0.61  0.12 
.“2.6 
8.1 
0.94  0.17  0.27 

4.1 

As expected, LDA  did  better  than  LOGREG  when 
the classes were gaussian (case 3).  An interesting result 
in  case 1 is that  LDA  does significantly  better  (25.2% 
vs  28.1%) when  it  does not  know  the  true  distribu- 
tions.  In  this  case, it  is because the  true  distribution 
is  highly  non-gaussian.  When  the  number  of  obser- 
vations  are few relative  to  their  dimensionality,  infor- 
mative  methods  may  do  surprisingly  well  even when 
the model is incorrect  (see also the  GAM/Naive  Bayes 
example below). 
StatLog 
data 
The  StatLog  experiments compared several classifica- 
tion  techniques on  various  datasets.  For most  of  the 
datasets,  logistic  discrimination  did  better  than  the 
corresponding informative  approach of  LDA  (Michie, 
Spiegelhalter,  &  Taylor  1994).  However, there  were 
several  cases, such  as  the  chromosome  dataset,  in 
which LDA  did better  than logistic discrimination.  For 
these cases, the  informative  model  apparently  makes 
use of  important 
information  in  the  marginal  density 
P(Xc>* 

Naive  Bayes  and  GAM 

Naive  Bayes  classifiers  are  a  specialized form  of  a 
Bayesian network  (John  &  Langley  1995; Langley  & 
Sage 1994) and  fall  into  the  informative  category  of 
classifiers.  The  class densities  assume independence 
among the predictors 

PblY  =  ICI  = 

fiP(dY 
j 

=  ICI 

* 

logp(xly  =  k)  =  ~logP(xjIY 

=  k) 

= 

g9r;,(z,), 
j 

fnr 

,,ng- 

\“...A” 

--‘b...“. 

thic!  T-PRE~~ 

-zu  w-v  1-w..  v  A..&  YA&.Y ^“W”V&..  T.nndc,~  OTnhn  &  T 
anrl  BTP nnim 
ley  1995) considered class densities tha.t are products 
of  univariate  gaussians as well  as  “flexible”  gaussian 
kernel densities. 

The  corresponding  discriminative  procedure 

is 
known  as  a  Generalized  Additive  Model 
(GAM) 
(Hastie  &  Tibshirani  1990).  GAM’s assume that  the 

52 

KDD-97 

log  ratio  of  class posteriors  is  additive  in  each of  the 
predictors  zj, 

j  =  l,,  . . ,p 

log  ‘(’  =  ‘lx)  =  2 
j 

P(Y =  Klx) 

fk,j(xj)  +  constb. 

1  Naive  Bayes classifiers are a specialized 

It  suffices to show that  the induced discriminant 

Theorem 
case of GAM. 
Proof 
is log additive. 
log P(Y = Vx)  = 
= 

P(Y  =  04 

log  PMY  =  k)P(Y  =  ICI 
P(ZlY =  K)P(Y  =  K) 

l%P(XlY  =  k)  - 
l%KY  =  UP(Y  =  K)l 

logp(xly  =  K)  + 

1 

&lhj(xj) 

-  SK,j(Xjcj)l + 

=  k)/P(Y  =  WI 

= 

fk,j(xj)  + constk 

iMY 
f: 
j 

0 
In  the  comparisons to  follow,  we ensure that  the  same 
representations  are  possible for  both  procedures. 
In 
particular,  for the informative  case, we model the class 
densities with  logspline densities which imply  an addi- 
tive  spline discriminant 

O,(X)  =  &Oj,nB,(2) 

j 

study 

simulation 

where B  is a natural  cubic spline basis. 
Logspline 
For  the  simulation  study  shown in  figure  2,  the  dis- 
criminant  was taken  to  be  an  additive  spline  with  5 
uniformly  spaced fixed knots.  Class 1 is a complicated 
mixture  density  (the  outer  ring),  and class 2 (the  two 
mounds in  the  middle)  is  the  exponential  tilt 
(using 
the logspline discriminant)  of class 1.  The Naive Bayes 
classifier assumes a logspline density  (see (Stone et al. 
to  appear))  separately in  each dimension and in  each 
class. Asymptotically,  the  GAM  classifier achieves the 
Ravm 
cliwr;minmt 
t-ho 
“ALU YL uv  UI”UIIIAIA~LLuII” 
YWJV”  VL-VL  AWYV \,.A,,“, 
is  log  additive  by  construction.  Asymptotically, 
the 
Naive  Bayes (NB)  classifier  does worse  (9.0%)  than 
GAM,  since the  class densities are not  a product  form. 
However, when only  a finite  sample of training  obser- 
vations is available, the Naive Bayes classifier does sur- 
prisingly  well (this behavior has been noted by Langley 

17  ‘F!Z,\  ~inre 
Ylll”” 

OFmr 

rate 

tnm 

References 

Byth,  K.,  and McLachlan,  G.  J.  1980.  Logistic  re- 
gression compared to  normal  discrimination  for non- 
normal populations.  The Australian  Journal of Statis- 
tics  22:188-196. 
Duda, R.  O., and Hart.,  P. E.  1973. Pattern  classiji- 
cation  and scene analysis. Wiley. 
Efron,  B.  1975. The efficiency of  logistic  regression 
compared to  normal  discriminant  analysis.  Journal 
of  the  American  Statistical  Association  70(352):892- 
898. 
Fayyad, U. M.;  Piatetsky-Shapiro,  G.; and Smyth,  P. 
1996. From data mining  to  knowledge discovery: An 
overview.  In  Advances  in  Knowledge Discovery  and 
Data  Mining.  The Mit  Press. 1-31. 
RYedman, J.  1996.  on  bias ,  VXimCe,  O/l-iOSS  2nd 
the  curse of  dimensionality.  Technical report,, Dept. 
of Statistics,  Stanford University. 
Hastie, T.,  and Tibshirani,  R.  1990. Generalized Ad- 
ditive  Models.  Chapman Hall. 
John, G. H.,  and Langley, P.  1995. Estimating  con- 
tinuous  distributions  in  bayesian classifiers.  In  Pro- 
ceedings of  the Eleventh  Conference  on  Uncertainty 
in 
Artifical  IntelZigence. San Mateo:  Morgan Kaufmann. 
Langley, P.,  and Sage, S.  1994.  Induction  of  selec- 
tive  bayesian classifers.  In  Proceedings of  the  Tenth 
Conference  on  Uncertainty  in  Artifkal 
Intelligence. 
Seattle, WA:  Morgan Kaufmann. 
Michie, D.;  Spiegelhalter, D.  J.;  and Taylor,  C.  1994. 
Machine  Learning, Neural  and  Statistical  Classifica- 
tion.  Ellis  Horwood. 
Ney, H.  1995. On the  probabilistic  interpretation  of 
neural network classifiers and discriminative  training 
criteria.  IEEE  Transactions  on  Pattern  Analysis  and 
Machine 
O’Neill, T. J.  1980. The general distribution  of the er- 
ror rate of a classification procedure with  application 
to  logistic  regression discrimination.  JournaE  of  the 
American  Statistical  Association 75(369):154-160. 
Ruiz-Velasco, S. 1991. Asymptotic  efficiency of logis- 
tic  regression relative to  linear discriminant  analysis. 
Biometrika  78~235-243. 
Stone,  C.  J.;  Hansen,  M.;  Kooperberg,  C.;  and 
Truong, Y. K.  to appear. Polynomial splines and their 
tensor products in  extended linear modelmg.  Annais 
of  Statistics. 

Intelligence  17(2):107-119. 

-  -i 

Rubinstein 

53 

L 

-5 

0 
traindata[, 

-l][.i] 

5 

(Langley &  Sage 1994)). In  simulation experiments, 25 
training  sets each containing 25 observations from each 
class were used to  train  both  NB  and GAM  classifiers. 
The average error rates were 11.1% for NB  and 11.4% 
for GAM  with  standard errors of 0.05 % and 0.06% re- 
spectively. Here then is an instance where informative 
training  actually  does slightly  better  than  discrimina- 
tive  training,  even though the  discriminative  model is 
correct and the informative  one is not! 

Conclusion 

Recently, Friedman (Friedman 1996) has shown that 
when it  comes to  classification, bias in the class poste- 
riors  is not  so critical  because of the  discretization  of 
the assignment rule.  So even if  the class density model 
is incorrect, i.e.  biased, it  may yet get the upper hand 
especially if  it  has lower variance estimates of the ciass 
posteriors across training  sets. 

It  is  best  to  use an informative  approach if  confi- 
dence in the model correctness is high.  This suggests a 
promising way of combining the two  approaches: par- 
tition  the feature space into two.  Train an informative 
model on those dimensions for  which it  seems correct, 
and a discriminative  model on the others. Experimen- 
tal  results on this  approach are forthcoming.  We are 
also investigating  other  techniques of  combining  the 
+...r. w...nnnAr.r~” 
l!W” pI”bcuu’“J. 

Even  when  the  goal  is  discrimination  between 
classes, it,  pays  to 
investigate  the  performance  of 
the  corresponding informative  model  which  borrows 
strength from the marginal density. 

