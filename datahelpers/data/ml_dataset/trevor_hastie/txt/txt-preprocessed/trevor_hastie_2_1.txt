Abstract

Classicationinhigh-dimensionalfeaturespaceswhereinterpreta-
tion and dimension reduction are of great importance is common in
biological and medical applications. For these applications standard
methodsasmicroarrays,1DNMR,andspectroscopyhavebecomeev-
erydaytoolsformeasuringthousandsoffeaturesinsamplesofinterest.
Furthermore, the samples are often costly and therefore many such
problemshavefewobservationsinrelationtothenumberoffeatures.
Traditionallysuchdataareanalyzedbyrstperformingafeaturese-
lection before classication. We propose a method which performs
lineardiscriminantanalysiswithasparsenesscriterionimposedsuch
that the classication, feature selection and dimension reduction is
merged into one analysis. The sparse discriminant analysis is faster
than traditional feature selection methods based on computationally
heavycriteriasuchasWilk'slambda,andtheresultsarebetterwith
regardstoclassicationratesandsparseness. Themethodisextended
tomixturesofGaussianswhichisusefulwhene.g.biologicalclusters
arepresentwithineachclass. Finally,themethodsproposedprovide
low-dimensionalviewsofthediscriminativedirections.

1

Introduction

1
Lineardiscriminantanalysis(LDA)isafavoredtoolforsupervisedclassica-
tion in many applications due to its simplicity and robustness. Comparison
studiesshowthatalargepercentage(typicallymorethan90%)oftheachiev-
able improvement in predictive accuracy, over the simple baseline model, is
achievedbyLDA(Hand,2006). Furthermore,LDAprovideslow-dimensional
projectionsofdataontothemostdiscriminativedirections. However,itfails
insomesituations:
 Whenthenumberofpredictorvariablesishighinrelationtothenum-
berofobservations(pn).
 Whenasingleprototypeperclassisinsucient.
 Whenlinearboundariesareinsucientinseparatingtheclasses.
Thementioned situationswhere LDAfails were previously addressed in pe-
nalizeddiscriminantanalysis(Hastieetal.,1995a)anddiscriminantanalysis
bygaussianmixtures(HastieandTibshrani,1996),seealso
exiblediscrim-
inant and mixture models (Hastie et al., 1995b). However, in some cases
where p  n these methods are not adequate since both sparseness and
feature selection is desired. A low number of nonzero parameters ensures
a better interpretation of the model and additionally tends to overt train-
ingdatalessthannonsparsemethodsasillustratedwiththeelasticnetand
sparseprincipalcomponents(ZouandHastie,2005;Zouetal.,2006).
It is often desirable to perform feature selection in biological or medical
applicationssuchasmicroarrays. Intheseapplicationsitisessentialtoiden-
tifyimportantfeaturesfortheproblemathandforinterpretationissuesand
to improve speed by using models with few nonzero loadings as well as fast
algorithms.
During the past decade problems in which the number of features is
much larger than the number of observations have received much attention
(Donoho,2000;Hastieetal.,2001;Dudaetal.,2001). Hereweconsiderclas-
sicationproblemsandproposeamethodforperformingrobustdiscriminant
analysis. Previously this issue has been addressed by ignoring correlations
between features and assuming independence in the multivariate Gaussian
model (naive Bayes) (Bickel and Levina, 2004). We will focus on imposing
sparsenessinthemodel(Donoho,2000)inlinewithmodelssuchaslassoand
theelasticnet(Tibshirani,1996;ZouandHastie,2005).

2

Theintroductionofasparsenesscriterioniswellknownintheregression
framework (Tibshirani, 1996; Zou and Hastie, 2005; Zou et al., 2006) and
we shall therefore consider LDA by optimal scoring which performs LDA
by regression (Hastie et al., 1995a; Ye, 2007). Furthermore, the optimal
scoring framework allows for an extension to mixtures of Gaussians (Hastie
andTibshrani,1996).
Thepaperisorganizedasfollows. SectiontwodescribesthesparseLDA
and sparse mixture discriminant analysis algorithms, introducing a modi-
cation of the elastic net algorithm to include various penalizing matrices.
Section three illustrates experimental results on a small illustrative shape
baseddatasetoffemaleandmalesilhouettesandonthreehigh-dimensional
datasets: Amicroarraydatasetplusspectral,andchemicalidenticationof
fungi. Weroundowithadiscussioninsectionfour.
2 Methodology
Lineardiscriminantanalysis(LDA)isaclassicationmethodwhichassumes
that the variables in each of the k classes are normally distributed with
means j, j =1;:::;k and equal dispersion  (see e.g. Hastie et al. (2001)).
Reduced-rankLDAhastheabilitytoprovidelow-dimensionalviewsofdata
of up to at most k 1 dimensions. These views, also called discriminant
directions,arefurthermoresortedsuchthatthedirectiondiscriminatingthe
classes most is rst and so forth. The at most k1 directions, js are the
oneswhichmaximizethevariancebetweenclassesandminimizethevariance
within classes and are orthogonal to each other. Hence, we maximize the
betweensumsofsquares,B relativetothewithinsumsofsquares,W (the
Fisher'scriterion)
argmaxj TjBj
(1)
undertheorthogonalityconstraint
TjWl= 0 l=1;:::;j1
(2)
;
1 l=j
tondthediscriminatingdirections j, j=1;:::;k1.
The methodology section is written following the notation of Penalized
Discriminant Analysis (PDA) in Hastie et al. (1995a). PDA replaces the
withinsumsofsquaresmatrixin(2)withthepenalizedtermW +2
. In

3

argmaxj TjBj1

ordertoobtainsparsenessinthesolutionweintroduceanextratermwhich
controlsthe`1-normoftheparameters. The`1-normhaspreviouslyproved
to be an eective regularization term for obtaining sparseness; see methods
such as lasso, elastic net and sparse principal component analysis (Tibshi-
rani,1996;ZouandHastie,2005;Zouetal.,2006). Thesparsediscriminant
criterionthenbecomes
(3)
under the constraint (2) with the penalized within sums of squares matrix
Wp =W +2
replacingW.
The elastic net proposed by Zou and Hastie (2005) solves a regression
problem regularized by the `2-norm and the `1-norm in a fast and eective
manner. Theelasticnetisdenedas
enj =argminj (kyXjk2
(4)
Asthesparsediscriminantcriterionisalsoregularizedbyan`2-normandan
`1-normpenaltyitseemsadvantageoustorewritethecriteriontoaregression
typeprobleminordertousetheelasticnetalgorithmforsolvingSDA.
LDAwasrewritteninHastieetal.(1995a)asaregressiontypeproblem
usingoptimalscoring. Theideabehindoptimalscoringistoturncategorical
variables1intoquantitativevariables. Optimalscoringassignsascore,jifor
eachclass iandforeachparametervector j. Theoptimalscoringproblem
isdenedas
(5)
s:t: n1kYk2
(6)
where Y isamatrixofdummyvariablesrepresentingthe k classes.
PDA adds a penalty of Tj
j to the optimal scoring problem such that
thepenalizedoptimalscoringcriterionbecomes
(^;^)pos=argmin; (n1kYXk2
(7)
;
s.t. (6),where
isasymmetricandpositivedenitematrix. Inthispaper,a
sparsenesscriterionisaddedtothepenalizedoptimalscoringcriterioninform
1The categorical variables will here be encoded asf0; 1g dummy variables.

(^;^)os = argmin; n1kYXk2
2=1 ;

2+2k
12k2
2)

2

pX
i=1jjij

2+2kjk2

2+1kjk1)

:

4

2.1 Sparse discriminant analysis by optimal scoring

ofthe`1-normoftheregressionparameters. Thenormalequationscanthus
no longer be applied and it is not possible to solve the sparse discriminant
analysis(SDA)probleminoneregressionandoneeigenvaluedecomposition
step as is the case for PDA. We propose an iterative algorithm for solving
SDA. Extending the method to mixtures of Gaussians is straightforward in
linewithHastieandTibshrani(1996).
Since the elastic net (Zou and Hastie, 2005) is used in the algorithm
wewillassumethatdataarenormalized,i.e.thefeaturesaretransformedto
havezeromeanandlengthone. Theelasticnetalgorithmusesthecorrelation
between the dependent variable and the predictors to decide which variable
toactivateineachiteration. However,itispossibletorunthealgorithmon
raw data which is comparable to performing principal component analysis
onthecovariancematrixratherthanthecorrelationmatrix.
In this section we introduce constraints to the optimal scoring problem in
(15) in order to obtain sparseness in the PDA. The score vector j assigns
a real number ji for each class i, i=1;:::;k. The scored training data Y
is an nq matrix on which we will regress the matrix of predictors Xnp
to obtain the parameters or directions pq. This leads to q components of
sparsediscriminativedirections. Wedenesparseoptimalscoringas
(8)
2+1kk1)
(9)
where
isapenalizationmatrix,asintroducedinPDA(Hastieetal.,1995a).
The `1-norm introduces sparseness as in lasso or elastic net regularization.
Inappendixtherelationbetweensparsediscriminantanalysis(3)andsparse
optimalscoring (8)isgiven.
Forxed  weobtain:
(10)
which for
= I is an elastic net problem. We will later rewrite the elastic
netformoregeneralpenaltymatrices. Forxed  theoptimalscoresare
(11)

(;)sos = argmin; n1(kYXk2
2=1 ;

sosj =argminj n1(kYjXjk2

2+2Tj
j+1kjk1)

os = argmin n1kYXk2
2=1 :

s:t: n1kYk2

2

5

2+2k
12k2

s:t: n1kYk2

2

2=1 ;

:

^ = UVT ,
^ = D12 UVT

^ = argmin n1kYD12  ^Yk2

SetD =n1YTY whichisadiagonalmatrixoftheclassproportions. Then
12wecan
theconstraint(9)canbewrittenasTD=I andsetting=D
solvethefollowingprobleminstead.
(12)
s:t: kk2
(13)
where ^Y =X. ThisisabalancedProcrustesproblemwhen Y and ^Y have
the same dimensions (for q = k). As q  k1 we pad ^Y with zeros, so
that ^Y = [X 0]. The problem can then be solved by taking the svd of
D12 YT^Y,asdescribedinEldenandPark(1999). However,asweonlyneed
to estimate U and V of the svd in order to obtain a solution, and D12
is a
diagonal matrix, taking the svd of YT^Y = USVT suces, and the solution
becomes
(14)
(15)
By analogy with the PDA case, we use heuristics from suitable normal as-
sumptionsasguidelinesforproducingposteriorprobabilitiesandaclassier.
As a graphical projection of a predictor vector x we use the set of ts Tx,
anda nearest class mean rule,where"nearest"ismeasuredusingWp,isap-
plied in the q <k1 reduced-dimensional discriminant subspace to obtain
classlabels.
Forgeneralization,wemodifytheelasticnetalgorithmtoincludeanarbitrary
penalty matrix
rather than the identity. The modied naive elastic net
solutionbecomes
(16)
:
WecantransformthenaiveelasticnetproblemintoanequivalentLasso
problemontheaugmenteddata(ZouandHastie,2005,Lemma1).
(17)

j =argminj n1(kyXjk2
X= Xp2


2+2Tj
j+1kjk1)
y= y0p



:

2.2 Modied elastic net

;

6

Thenormalequations,yieldingtheOLSsolution,tothisaugmentedproblem
are

Algorithm 1 Sparse Discriminant Analysis:

2.3 Sparse Discriminant Algorithm

(18)
Weseethatisthe
-penalizedregressionestimatewithweight2. Hence,
performing Lasso on this augmented problem yields a modied elastic net
solution. Since
is symmetric and positive denite, p
always exists. For
examples of various penalty matrices
and their applications we refer to
Hastieetal.(1995a).
TheSDAalgorithmusingoptimalscoresandmodiedelasticnetisdescribed
inAlgorithm1.
1. Initialize =(kPkj=1D;fjjg)1I1:k1.
2. For j=1;:::;q solvethemodiedelasticnetproblemwithxed 
j =argminj n1(kYjXjk2
(19)
3. Forxed  and YT^Y =USVT computetheoptimalscoresfrom(15).
4. Repeatstep2and3untilconvergence.
5. Update  forxed  using (19),thesparsediscriminantdirectionsare
now ordered according to the singular values and thereby degree of
discrimination.
The sparse discriminant analysis algorithm has a computational eort
similar to that of sparse principal component analysis (Zou et al., 2006). It
likewiseperformsanelasticnetstepandanSVDineachiteration. Theelastic
netstepfor pnhasthehighestcomputationalcostwhichisintheorder

2+2Tj
j+1kjk1)

 Xp2
T Xp2
^ =  Xp2
T y0p

XT X+2
^ = XTy

:

,

7

2.4 Sparse mixture of Gaussians

ofqO(pnm+m3)wheremisthenumberofnonzerocoecients. Thiscanbe
massiveifpandmarelarge. However,ingeneralfewnonzerocoordinatesare
desired in the mentioned applications, and the algorithm therefore becomes
very eective. Additionally, the number of iterations needed is generally
small.
Insteadofrepresentingeachclassbyasingleprototypewenowrepresenteach
classbyamixtureofGaussians. WedivideeachclassjintoRjsubclassesand
denethetotalnumberofsubclasses R=Pkj=1Rj. Tolimitthenumberof
parametersweconsideraGaussianmixturemodelwhereeachsubclasshasits
ownmean jr andcommoncovariancematrix. Sincethesingleprototype
problem is formulated as an optimal scoring problem it is straight forward
to extend it to mixtures of Gaussians in line with Hastie and Tibshrani
(1996). Instead of using an indicator response matrix Y we use a blurred
response matrix ZnR which consists of the subclass probabilities, zjr for
each observation. Let jr be the mixing probability within the rth subclass
within the jth class, and PRjr=1jr =1. Recall the EM steps of using Bayes
theorem to model Gaussian mixtures. The estimation steps of the subclass
probabilities, zjr andthemixingprobabilities, jr are
jrexpf(Xjr)1(Xjr)
g
(20)
zir =
PRjr=1jrexpf(Xjr)1(Xjr)
jr = X
(21)
i2gizir;
withthe maximization steps
jr = Pi2gixizir
Pi2gizir
 = n1 kX
j=1X
i2gi

(22)
(23)
WenowwritetheSMDAalgorithmbycomputingQR1sparsedirections
forthesubclassesinthemixtureofGaussiansmodelasdescribedinalgorithm
2.

RjX
r=1zir(xijr)(xijr)T

RjX
r=1jr =1

2

2

g

:

8

Algorithm 2 Sparse Mixture Discriminant Analysis:

1. Initialize the blurred response matrix Z with the subclass proba-
bilities. As in Hastie and Tibshrani (1996) the subclass probabili-
ties can be derived from Learning Vector Quantization or K-means
preprocessing, or from a priori knowledge of data.
Initialize  =
(RPkj=1PRjr=1jr)1I1:R1.
2. Forj=1;:::;Q,QR1solvethemodiedelasticnetproblemwith
xed 
(24)

j =argminj n1(kZjXjk2
=D12p UVT

2+2Tj
j+1kjk1)
3. Forxed  and YT^Y =USVT computetheoptimalscores
(25)
where Dp isadiagonalmatrixofsubclassprobabilities, jr. jr isthe
sum of the elements in the rth column in Z divided by the number of
samples n.
5. Updatethesubclassprobabilitiesin Z andthemixingprobabilitiesin
Dp usingtheestimationsteps(20)and(21).
6. Repeatstep2-5untilconvergence.
7. RemovethelastRmtrivialdirections,wherethe(m+1)th singular
value Sm+1 <(issomesmallthresholdvalue):
(26)
For j = 1;:::;m solve the modied elastic net problem with xed 
using (24)toobtainthe mnontrivialdiscriminantdirections.

=D12p UVT1:m ;

;

9

3 Experimental results
This section illustrates results on a small data set of shapes from female
and male silhouettes and on three dierent high-dimensional data sets: A
benchmark high-dimensional microarraydata set, a data set basedon spec-
tralimagingof Penicillium fungiforclassicationtothespecieslevel,anda
datasetwith1DNMRsofthreefungalgeneraforclassicationtothegenus
level. Thenumberofiterationsthealgorithmsusedinthefollowingapplica-
tions were less than 30 in all cases. The parameters for the elastic net were
chosen using leave-one-out cross validation on the training data. Data was
normalizedandthepenaltymatrix
=I unlessotherwisementioned.
To illustrate the sparse representation of the discriminant directions from
SDAweconsideredashapebaseddatasetconsistingof20maleand19female
silhouettes from adults. A minimum description length (MDL) approach to
annotate the silhouettes were used as in Thodberg and Olafsdottir (2003),
andProcrustesalignmentwasperformedontheresulting65MDLmarksof
(x;y)-coordinates. Fortrainingthemodelwe22ofthesilhouetteswereused
(11 female and 11 male), which left 17 silhouettes for testing (8 female and
9male). Figure1illustratesthetwoclassesofsilhouettes.

3.1 Female and male silhouettes

(a) Female

(b) Male

Figure 1: The silhouettes and the 65 markers for the two groups: Female
andmalesubjects.

10

Performingleave-one-outcrossvalidationonthetrainingdataweselected
10nonzerofeaturesand 2=102 asparametersforSDA.TheSDAresults
areillustratedingure2. Note,howthefewmarkersincludedinthemodel
were placed near high curvature points in the silhouettes. The training and
testclassicationrateswereboth82%. Intheoriginalpaper(Thodbergand
Olafsdottir, 2003) a logistic regression was performed on a subset of PCA
scores, where the subset was determined by backwards elimination using a
classical statistical test for signicance. Results were only stated for leave-
one-outcrossvalidationontheentiredatasetwhichgavea85%classication
rate,seeThodbergand Olafsdottir(2003). TheSDAmodelingure2iseasy
to interpret compared to a model based on 2-4 principal components each
with contributions from all 65 MDL marks. The SDA model points out
exactlywherethedierencesbetweenthetwogendersare.

(a) Model

(b) SD

Figure 2: Results from SDA on the silhouette data. (a) The mean shape
of the silhouettes and the model with the 10 nonzero loadings illustrating
which markers dier from female to male subjects. The arrows illustrate
thedirectionsofthedierences. (b)Thesparsedirectiondiscriminatingthe
classes. The crosses illustrate the observations, the solid curves illustrate
theestimatedgaussiandistributionsoftheclassesfromthetrainingset,and
the dashed curves illustrate the estimated gaussian of the classes from the
trainingandthetestset.

11

3.2 Leukemia-subtype microarray

Thissectionconsidersahigh-dimensionalbenchmarkdatasetfromtheKent
RidgeBiomedicalDataSetRepository 2,namelytheleukemia-subtypedata
set published in Yeoh and et. al (2002). The study aimed at classifying
subtypes of pediatric acute lymphoblastic leukemia (ALL). Cancer diseases
requirefastandcorrectdiagnosisandonewaytofacilitatethisisbymicroar-
ray analysis. The microarray data set considered here consisted of 12558
genes, 6 subtypes of cancer, 163 training samples and 85 test samples. The
sixdiagnosticgroupsindatawere: BCR-ABL,E2A-PBX1,Hyperdiploid>50
chromosomes, MLLrearrrangement, T-ALLandTEL-AML1. Originally, in
Yeoh and et. al (2002), data was analyzed in two steps: A feature selection
step and a classication step. Furthermore, data was analyzed in a deci-
siontreestructuresuchthatonegroupwasseparatedusinganSVMateach
tree node. Here, we illustrate the strengths of SDA which performs feature
selection, dimension reduction and classication in one step. With only 25
nonzerofeatures,comparedto40inYeohandet. al(2002),ineachofthe5
discriminant directions good classication rates were obtained. The results
are summarized in table 1 and are on non-normalized data for comparison
withtheoriginalanalysisofdata. Therewere2misclassiedobservationsin
thetrainingsetand3misclassiedobservationsinthetestset. Inthelatter
case all the misclassied observations belonged to the BCR ABL group but
wereclassiedasHyperdiploid>50.
Figure 3 illustrates scatter plots of the six groups projected onto the
sparse directions obtained by SDA. Note, that each sparse direction sepa-
rates dierent groups. This leads to knowledge not only of the separation
of all groups, but also of which genes have a dierent expression level for
one subtype of cancer compared to the others, similar to the decision tree
structure in the original analysis. Expression proles of the selected genes
foreachsparsedirectioncanbefoundinappendix.
This section analyzes another high-dimensional data set which considers
multi-spectralimagingforobjectiveclassicationoffungi. Fewoftheworld's
fungal species are known today (Hawksworth, 2001) and due to the various
usefulandtoxicmycotoxinstheycanproduceitisofgreatinteresttoquickly

3.3 Spectral id of fungal species

2http://sdmc.i2r.a-star.edu.sg/rp/

12

Table 1: Subgroup predictions using SDA with 25 nonzero features in each
of the 5 discriminant directions. The ridge weight, 2 =101 as well as the
numberofnonzeroloadingswerechosenusingleave-one-outcrossvalidation
onthetrainingset.Group
Allgroups
BCR-ABL
E2A-PBX1
Hyperdiploid>50
T-ALL
TEL-AML1
MLL

Trainingset
99%
89%
100%
98%
100%
100%
100%

Testset
96%
50%
100%
100%
100%
100%
100%

Figure3: SparsediscriminantvariablesinSDAoftheLeukemia-subtypedata
set.

13

and accurately classify known species and identify unknown ones. Here, we
consider the three Penicillium species: Melanoconodium, polonicum, and
venetum. The three species all have green/blue conidia (the spores of the
fungi) and are therefore visually dicult to distinguish.
It is desirable to
have accurate and objective classication of the fungi species as they pro-
ducedierentmycotoxins. Someareveryusefultous,suchaspenicillinwhile
otherscanbeharmful. Avisualclassicationisbasedonthephenotypesof
the species and is in general faster than chemical or genetic methods for
classication. Usingimageanalysistoperformtheclassicationadditionally
givesanobjectiveandaccuratemethodwhichcanbereproducedinvarious
laboratories.
For each of the three species, four strains were inoculated on yeast ex-
tract sucrose (YES) agar in three replica, in total 36 samples. The data
set consisted of 3542 variables extracted from multi-spectral images (Clem-
mensenetal.,2007)with18spectralbands(10inthevisualrange,and8in
thenearinfraredrange). Thevariablesweresummarystatisticstakenfrom
histogramsofthefungalcoloniesineachspectralband,andineachpairwise
dierenceandpairwisemultiplicationbetweenspectralbands. Table2sum-
marizestheresultsfromreduced-rankPDA,forwardselection(FS)basedon
Wilk'sLambda,andSDA.Thedatawaspartitionedinto2/3whichwasthe
training data and 1/3 which was the test data where one of the three repe-
titionsof eachstrainwasleft outfor testing. Thisgave28trainingsamples
and12testsamples. Inthiscasetheclassicationrateswerenotimproved,
but the complexity of the models was reduced by both SDA and FS. Fur-
thermore, thecomputationalcostofSDAwassmallerthanforFSbasedon
Wilk's. TheCPU-timewasmorethandoubledwhichforjusttwononzero
loadingdoesn'tseemalarmingbutasthenumberofnonzeroloadingsgrow,
the computational eort likewise grows. On top of that, the two methods:
FSandSDAhadoneoftheselectedvariablesincommon. Figure4illustrates
thesparsediscriminantdirectionsinSDA.Itisnotsurprisingthatthethree
groups are completely discriminated as they dier in their conidium color
whichrangefromgreentoblue,seeClemmensenetal.(2007). Theselected
features are thus also percentiles in dierences of blue and green spectral
bands.

14

Table 2: Classication rates from PDA, SDA and forward selection based
on Wilk's  (FS) combined with LDA on the Penicillium data. The Ridge
penaltyweightwas106forPDAandSDA,chosenusingleave-one-outcross-
validationonthetrainingset. Likewisethenumberofnonzeroloadingswas
chosen using cross-validation. The covariance matrix in the reduced-rank
PDA was ridge regularized since p >> n. Note, that the computational
complexityforforwardselectionwasmuchlargerthanforSDA.
Test Nonzeroloadings CPU-time
384.3s
0.4s
0.1s

Method Train
PDA
FS
SDA

100% 100%
100% 100%
100% 100%

7084
2
2

Figure 4: The Penicillium data set projected onto the sparse discriminant
directionsinSDA.

15

3.4 Chemical id of fungal genera

In the previous section we used visual information to classify fungi to the
species level. Here we will use chemical information in form of 1D NMR of
fungiforclassicationtothegenuslevel(Rasmussen,2006). Threegeneraof
fungi were considered: Aspergillus, Neosartorya, and Penicillium. For each
genustherewere5,2,and5species,respectively. Therewere71observations
with4-8samplesofeachspecies. Informationfromthe950highestpeaksin
the NMR data were used as features. Data were logarithmicly transformed
asdierencesinpeakswithlowerintensitiesseemedtohavein
uence. Asthe
biologygaveahierarchyofsubgroupswithineachgenusitseemedreasonable
tomodeleachgenusasamixtureofGaussians,i.e.amixtureofspeciesand
thereforewetestedtheSMDAonthisdata. Table3summarizestheresults
usingPDA,SDAandSMDAonthe1DNMRdata. Inadditiontoimproved
classicationratesthesparsemethodsprovidedinsightinwhichchemicalfea-
tures that distinguish the fungal genera. Furthermore, the sparse methods
gave models with smaller complexity and thereby smaller variance. Conse-
quently, the sparse methods tended to overt less than the more complex
PDA model. Figure 5 and 6 illustrate the (sparse) discriminative directions
for PDA, SDA, and SMDA. Note, that due to the underlying mixture of
Gaussiansmodel,thesparsedirectionsintheSMDAprovidedknowledgeof
the separation between genera not only at the genus level but also at the
specieslevel.
Table3: ErrorsfromPDA,SDAandSMDAonthe1DNMRdata. Withfew
nonzeroloadingsinSDAandSMDAthetestclassicationratesareimproved.
The Ridge penalty weight is in [103;101] for the three methods and was
as well as the number of nonzero loadings chosen using leave-one-out cross
validation on the training set. The covariance matrix in the reduced-rank
PDAeasridgeregularizedsince p>>n.
Method Train Test Nonzeroloadings
100% 76%
PDA
1900
SDA
97% 91%
10
SMDA 100% 94%
44

16

(a) PDA

(b) SDA

Figure5: DiscriminantdirectionsinPDAandSDAofthe1DNMRdataset.
In particular for Aspergillus and Neosartorya there seem to be subclusters
withinthegenera.
4 Discussion
LineardiscriminantanalysisandclassicationbymixturesofGaussiansare
widelyusedmethodsfordealingwithsupervisedclassication. Inthispaper
wehaveproposedalgorithmsforcomputingsparseversionsoflineardiscrimi-
nantanalysisandmixturediscriminantanalysis. Themethodsareespecially
usefulwhenthenumberofobservationsissmallinrelationtothenumberof
variables(np),andingeneralwhenitisimportanttogainknowledgeof
asubsetoffeatureswhichseparatestwoormoregroupsinhigh-dimensional
problems. Sparsediscriminantanalysishasbeenillustratedonasmallshape
baseddatasetoffemaleandmalesilhouettes,abenchmarkmicroarraydata
set for classication of leukemia subtypes and on visual and chemical data
forclassicationofthefungitothespeciesorthegenuslevel. Sparsemixture
discriminantanalysiswasillustratedonthechemicaldataforclassicationof
fungitothegenuslevel. Themethodsarefasterthanmethodsrstperform-
ing feature selection and then subsequently classication. Furthermore, the
classicationresultsarecomparableorbetterthanforsuchmethods. Finally,
themixtureofGaussiansmodelsareusefulformodellingdatawherebiolog-
ical subgroups exist such as classication of biological data to the species
or the genus level. Matlab and R versions of SDA and SMDA are available
from: www.imm.dtu.dk/~lhc.

17

Figure6: SparsediscriminantdirectionsinSMDAofthe1DNMRdataset.
Note how the distribution of each group has changed due to the underlying
mixtureofGaussiansmodel. Here, eachsparsedirectionaimsatseparating
onesubgroupfromtheremaining.

18

Acknowledgements
TheauthorswouldliketothankGrittRasmussen,ThomasOstenfeldLarsen,
CharlotteHeldGotfredsenandMichaelE.HansenatBioCentrum,TheTech-
nical University of Denmark for making the 1D NMR data available. Also
thanks to Hildur Olafsdottir for making the silhouette data available, and
KarlSjostrandforvaluablecomments.
