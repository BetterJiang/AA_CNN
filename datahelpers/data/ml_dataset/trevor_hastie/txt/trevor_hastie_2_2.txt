Very Sparse Random Projections

Ping Li

Department of Statistics

Stanford University

Stanford CA 94305, USA

pingli@stat.stanford.edu

Trevor J. Hastie

Department of Statistics

Stanford University

Stanford CA 94305, USA
hastie@stanford.edu

Kenneth W. Church

Microsoft Research
Microsoft Corporation

Redmond WA 98052, USA
church@microsoft.com

ABSTRACT
There has been considerable interest in random projections,
an approximate algorithm for estimating distances between
pairs of points in a high-dimensional vector space. Let
A 2 Rn(cid:2)D be our n points in D dimensions. The method
multiplies A by a random matrix R 2 RD(cid:2)k, reducing the
D dimensions down to just k for speeding up the compu-
tation. R typically consists of entries of standard normal
N (0; 1). It is well known that random projections preserve
pairwise distances (in the expectation). Achlioptas proposed
sparse random projections by replacing the N (0; 1) entries
in R with entries in f(cid:0)1; 0; 1g with probabilities f 1
6g,
achieving a threefold speedup in processing time.
We recommend using R of entries in f(cid:0)1; 0; 1g with prob-
2pDg for achieving a signi(cid:12)cant pD-

abilities f 1
2pD
fold speedup, with little loss in accuracy.

; 1(cid:0) 1pD

3 ; 1

6 ; 2

;

1

Categories and Subject Descriptors
H.2.8 [Database Applications]: Data Mining

General Terms
Algorithms, Performance, Theory

Keywords
Random projections, Sampling, Rates of convergence

1.

INTRODUCTION

Random projections [1, 43] have been used in Machine
Learning [2, 4, 5, 13, 14, 22], VLSI layout [42], analysis of La-
tent Semantic Indexing (LSI) [35], set intersections [7, 36],
(cid:12)nding motifs in bio-sequences [6, 27], face recognition [16],
privacy preserving distributed data mining [31], to name a
few. The AMS sketching algorithm [3] is also one form of
random projections.

We de(cid:12)ne a data matrix A of size n(cid:2) D to be a collection
i=1 2 RD. All pairwise distances can

of n data points fuign

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
KDD’06, August 20–23, 2006, Philadelphia, Pennsylvania, USA.
Copyright 2006 ACM 1-59593-339-5/06/0008 ...$5.00.

be computed as AAT, at the cost of time O(n2D), which is
often prohibitive for large n and D, in modern data mining
and information retrieval applications.

To speed up the computations, one can generate a ran-
dom projection matrix R 2 RD(cid:2)k and multiply it with the
original matrix A 2 Rn(cid:2)D to obtain a projected data matrix

B =

1
pk

AR 2 Rn(cid:2)k;

k (cid:28) min(n; D):

(1)

The (much smaller) matrix B preserves all pairwise dis-
tances of A in expectations, provided that R consists of
i.i.d. entries with zero mean and constant variance. Thus,
we can achieve a substantial cost reduction for computing
AAT, from O(n2D) to O(nDk + n2k).

In information retrieval, we often do not have to materi-
alize AAT. Instead, databases and search engines are inter-
ested in storing the projected data B in main memory for
e(cid:14)ciently responding to input queries. While the original
data matrix A is often too large, the projected data matrix
B can be small enough to reside in the main memory.

j=1

The entries of R (denoted by frjigD

k
i=1) should be i.i.d.
with zero mean. In fact, this is the only necessary condi-
tion for preserving pairwise distances [4]. However, di(cid:11)er-
ent choices of rji can change the variances (average errors)
and error tail bounds. It is often convenient to let rji follow
a symmetric distribution about zero with unit variance. A
\simple" distribution is the standard normal1, i.e.,

rji (cid:24) N (0; 1); E (rji) = 0; E(cid:0)r2

ji(cid:1) = 1; E(cid:0)r4

ji(cid:1) = 3:

It is \simple" in terms of theoretical analysis, but not in
terms of random number generation. For example, a uni-
form distribution is easier to generate than normals, but
the analysis is more di(cid:14)cult.

In this paper, when R consists of normal entries, we call
this special case as the conventional random projections,
about which many theoretical results are known. See the
monograph by Vempala [43] for further references.

We derive some theoretical results when R is not restricted
to normals. In particular, our results lead to signi(cid:12)cant im-
provements over the so-called sparse random projections.

1.1 Sparse Random Projections

In his novel work, Achlioptas [1] proposed using the pro-

1The normal distribution is 2-stable.
stable distributions that have closed-form density [19].

It is one of the few

jection matrix R with i.i.d entries in

rji = ps8<
:

1 with prob. 1
2s
0 with prob. 1 (cid:0) 1
(cid:0)1 with prob. 1

2s

s

;

(2)

where Achlioptas used s = 1 or s = 3. With s = 3, one can
achieve a threefold speedup because only 1
3 of the data need
to be processed (hence the name sparse random projections).
Since the multiplications with ps can be delayed, no (cid:13)oating
point arithmetic is needed and all computation amounts to
highly optimized database aggregation operations.

This method of sparse random projections has gained its
popularity. It was (cid:12)rst experimentally tested on image and
text data by [5] in SIGKDD 2001. Later, many more publi-
cations also adopted this method, e.g., [14, 29, 38, 41].
1.2 Very Sparse Random Projections
We show that one can use s (cid:29) 3 (e.g., s = pD, or even

log D ) to signi(cid:12)cantly speed up the computation.

s = D

Examining (2), we can see that sparse random projec-
tions are random sampling at a rate of 1
s , i.e., when s = 3,
one-third of the data are sampled. Statistical results tell
us that one does not have to sample one-third (D=3) of
the data to obtain good estimates. In fact, when the data
are approximately normal, log D of the data probably suf-
(cid:12)ce (i.e., s = D
log D ), because of the exponential error tail
bounds, common in normal-like distributions, such as bino-
mial, gamma, etc. For better robustness, we recommend
choosing s less aggressively (e.g., s = pD).

To better understand sparse and very sparse random pro-
jections, we (cid:12)rst give a summary of relevant results on con-
ventional random projections, in the next section.

2. CONVENTIONAL RANDOM

PROJECTIONS: R (cid:24) N (0; 1)

Conventional random projections multiply the original data
matrix A 2 Rn(cid:2)D with a random matrix R 2 RD(cid:2)k, con-
sisting of i.i.d. N (0; 1) entries. Denote by fuign
i=1 2 RD the
rows in A and by fvign
i=1 2 Rk the rows of the projected
RTui. We focus on the leading two rows:
data, i.e., vi = 1pk
u1, u2 and v1, v2. For convenience, we denote

D

D

m1 = ku1k2 =

1;j ; m2 = ku2k2 =
u2

Xj=1
Xj=1
u1;j u2;j ; d = ku1 (cid:0) u2k2 = m1 + m2 (cid:0) 2a:
2.1 Moments

Xj=1

a = uT

1 u2 =

u2
2;j ;

D

It is easy to show that (e.g., Lemma 1.3 of [43])

2
k

m2
1;

(3)

d2; (4)

2
k

E(cid:0)kv1k2(cid:1) = ku1k2 = m1; Var(cid:0)kv1k2(cid:1)N =
E(cid:0)kv1 (cid:0) v2k2(cid:1) = d;

Var(cid:0)kv1 (cid:0) v2k2(cid:1)N =

where the subscript \N " indicates that a \normal" projec-
tion matrix is used.

From our later results in Lemma 3 (or [28, Lemma 1]) we

can derive

E(cid:16)vT

1 v2(cid:17) = a;

Var(cid:16)vT

1 v2(cid:17)N

=

1

k (cid:0)m1m2 + a2(cid:1) :

(5)

Therefore, one can compute both pairwise 2-norm dis-
tances and inner products in k (instead of D) dimensions,
achieving a huge cost reduction when k (cid:28) min(n; D).
2.2 Distributions

It is easy to show that (e.g. Lemma 1.3 of [43])

(6)

(7)

v1;i

d=k

kv1k2
m1=k (cid:24) (cid:31)2
k;
kv1 (cid:0) v2k2
k (cid:20) m1

pm1=k (cid:24) N (0; 1);
v1;i (cid:0) v2;i
pd=k (cid:24) N (0; 1);
(cid:20) v1;i
v2;i (cid:21) (cid:24) N(cid:18)(cid:20) 0

0 (cid:21) ; (cid:6) =

k denotes a chi-squared random variable with k de-

(cid:24) (cid:31)2
k;
a m2 (cid:21)(cid:19) :
a
where (cid:31)2
grees of freedom. v1;i i.i.d. is any entry in v1 2 Rk.
Knowing the distributions of the projected data enables
us to derive (sharp) error tail bounds. For example, various
Johnson and Lindenstrauss (JL) embedding theorems [1,4,9,
15,20,21] have been proved for precisely determining k given
some speci(cid:12)ed level of accuracy, for estimating the 2-norm
distances. According to the best known result [1]:

(8)

1

If k (cid:21) k0 = 4+2(cid:13)

(cid:15)2 =2(cid:0)(cid:15)3=3 log n, then with probability at least

1 (cid:0) n(cid:0)(cid:13) , for any two rows ui, uj , we have

(1 (cid:0) (cid:15))kui (cid:0) ujk2 (cid:20) kvi (cid:0) vjk2 (cid:20) (1 + (cid:15))kui (cid:0) ujk2:

(9)

Remark:
(a) The JL lemma is conservative in many ap-
plications because it was derived based on Bonferroni cor-
rection for multiple comparisons. (b) It is only for the l2
distance, while many applications care more about the in-
ner product. As shown in (5), the variance of the inner

product estimator, Var(cid:0)vT

gins (i.e., m1m2) even when the data are uncorrelated. This
is probably the weakness of random projections.
2.3 Sign Random Projections

1 v2(cid:1)N , is dominated by the mar-

A popular variant of conventional random projections is
to store only the signs of the projected data, from which one

can estimate the vector cosine angles, (cid:18) = cos(cid:0)1(cid:16)

by the following result [7, 17]:

a

pm1m2(cid:17),

Pr (sign(v1;i) = sign(v2;i)) = 1 (cid:0)

(cid:18)
(cid:25)

;

(10)

One can also estimate a by assuming that m1, m2 are known,
from a = cos((cid:18))pm1m2, at the cost of some bias.

The advantage of sign random projections is the saving
in storing the projected data because only one bit is needed
for the sign. With sign random projections, we can com-
pare vectors using hamming distances for which e(cid:14)cient al-
gorithms are available [7,20,36]. See [28] for more comments
on sign random projections.

3. OUR CONTRIBUTIONS
We propose very sparse random projections to speed up
the (processing) computations by a factor of pD or more.
(cid:15) We derive exact variance formulas for kv1k2, kv1(cid:0)v2k2,
and vT
1 v2 as functions of s.2 Under reasonable regular-
ity conditions, they converge to the corresponding vari-
ances when rji (cid:24) N (0; 1) is used, as long as s = o(D)
2 [1] proved the upper bounds for the variances of kv1k2 and
kv1 (cid:0) v2k2 for s = 1 and s = 3.

log D ). When s = pD, the
(e.g., s = pD, or even s = D
D1=4(cid:17), which is fast since
rate of convergence is O(cid:16) 1
D has to be large otherwise there would be no need
of seeking approximate answers. This means we can
achieve a pD-fold speedup with little loss in accuracy.
(cid:15) We show that v1;i, v1;i (cid:0) v2;i and (v1;i; v2;i) converge
D1=4(cid:17) when s = pD. This
to normals at the rate O(cid:16) 1

allows us to apply, with a high level of accuracy, re-
sults of conventional random projections, e.g., the JL-
embedding theorem in (9) and the sign random pro-
jections in (10). In particular, we suggest using a max-
imum likelihood estimator of the asymptotic (normal)
distribution to estimate the inner product a = uT
1 u2,
taking advantage of the marginal norms m1, m2.

(cid:15) Our results essentially hold for any other distributions
of rji. When rji is chosen to have negative kurtosis,
we can achieve strictly smaller variances (errors) than
conventional random projections.

4. MAIN RESULTS

1;j u2

Main results of our work are presented in this section with
detailed proofs in Appendix A. For convenience, we always
let s = o(D) (e.g., s = pD) and assume all fourth mo-
ments are bounded, e.g., E(u4
2;j ) < 1 and
E(u2
2;j ) < 1. In fact, analyzing the rate of convergence
of asymptotic normality only requires bounded third mo-
ments and an even much weaker assumption is needed for
ensuring asymptotic normality. Later we will discuss the
possibility of relaxing this assumption of bounded moments.
4.1 Moments

1;j ) < 1, E(u4

The (cid:12)rst three lemmas concern the moments (means and

1 v2, respectively.

(11)

(12)

u4

1;j! :

D

Xj=1

E (u1;j )4
E2 (u1;j )2 ! 0; (13)

(14)

1

1 + (s (cid:0) 3)

Lemma 1.

As D ! 1,

variances) of v1, v1 (cid:0) v2 and vT
E(cid:0)kv1k2(cid:1) = ku1k2 = m1;
k  2m2
Var(cid:0)kv1k2(cid:1) =
(s (cid:0) 3)PD
Var(cid:0)kv1k2(cid:1) D(cid:24)
1 =(cid:16)PD

j=1 (u1;j )4
m2
1

i.e.,

s (cid:0) 3
D

!
k (cid:0)2m2
1(cid:1) :
1;j(cid:17)2
=PD

1

1;j 0 ,

2

j=1 u2

D(cid:24) denotes \asymptotically equivalent" for large D.
Note that m2

1;j +Pj6=j 0 u2

j=1 u4
1;j u2
with D diagonal terms and D(D(cid:0)1)
cross-terms. When all
dimensions of u1 are roughly equally important, the cross-
terms dominate. Since D is very large, the diagonal terms
are negligible. However, if a few entries are extremely large
compared to the majority of the entries, the cross-terms
may be of the same order as the diagonal terms. Assum-
ing bounded fourth moment prevents this from happening.
The next Lemma is strictly analogous to Lemma 1. We
present them separately because Lemma 1 is more conve-
nient to present and analyze, while Lemma 2 contains the
results on the 2-norm distances, which we will use.

Lemma 2.

D

1

E(cid:0)kv1 (cid:0) v2k2(cid:1) = ku1 (cid:0) u2k2 = d;
Var(cid:0)kv1 (cid:0) v2k2(cid:1)
(u1;j (cid:0) u2;j )4!
k  2d2 + (s (cid:0) 3)
k (cid:0)2d2(cid:1) :

Xj=1

1

=

D(cid:24)

The third lemma concerns the inner product.

Lemma 3.

1

1 u2 = a;

1 v2(cid:17) = uT
1 v2(cid:17)

E(cid:16)vT
Var(cid:16)vT
k  m1m2 + a2 + (s (cid:0) 3)
k (cid:0)m1m2 + a2(cid:1) :

1

=

D(cid:24)

u2
1;j u2

2;j! :

D

Xj=1

(15)

(16)

(17)

(18)

(19)

(20)

Therefore, very sparse random projections preserve pair-
wise distances in expectations with variances as functions
of s. Compared with Var(kv1k2)N , Var(kv1 (cid:0) v2k2)N , and
Var(vT
1 v2)N in (3), (4), and (5), respectively, the extra terms
all involve (s(cid:0)3) and are asymptotically negligible. The rate
of convergence is O(cid:16)q s(cid:0)3
D (cid:17), in terms of the standard er-
ror (square root of variance). When s = pD, the rate of
D1=4(cid:17).
convergence is O(cid:16) 1
achieve slightly smaller variances.
4.2 Asymptotic Distributions

When s < 3, \sparse" random projections can actually

The asymptotic analysis provides a feasible method to

study distributions of the projected data.

The task of analyzing the distributions is easy when a nor-
mal random matrix R is used. The analysis for other types
of random projection distributions is much more di(cid:14)cult (in
fact, intractable). To see this, each entry v1;i = 1pk
i u1 =
j=1 rjiu1;j . Other than the case rji (cid:24) N (0; 1), ana-
lyzing v1;i and v1 exactly is basically impossible, although
in some simple cases [1] we can study the bounds of the
moments and moment generating functions.

1pk PD

RT

Lemma 4 and Lemma 5 present the asymptotic distribu-
tions of v1 and v1 (cid:0) v2, respectively. Again, Lemma 5 is
strictly analogous to Lemma 4.

Lemma 4. As D ! 1,

L=) N (0; 1);

kv1k2
m1=k

L=) (cid:31)2
k;

(21)

v1;i

pm1=k

with the rate of convergence

jFv1;i (y) (cid:0) (cid:8)(y)j (cid:20) 0:8psPD
! 0:8r s

D

1

j=1 ju1;jj3
m3=2
Eju1;jj3
(cid:0)E(cid:0)u2

1;j(cid:1)(cid:1)3=2 ! 0;

where L=) denotes \convergence in distribution;" Fv1;i (y) is
the empirical cumulative density function (CDF) of v1;i and
(cid:8)(y) is the standard normal N (0; 1) CDF.

(22)

Lemma 5. As D ! 1,
L=) N (0; 1);

v1;i (cid:0) v2;i
pd=k

with the rate of convergence

kv1 (cid:0) v2k2

d=k

L=) (cid:31)2
k;

(23)

jFv1;i(cid:0)v2;i (y) (cid:0) (cid:8)(y)j (cid:20) 0:8psPD

j=1 ju1;j (cid:0) u2;jj3

d3=2

! 0:

(24)

The above two lemmas show that both v1;i and v1;i (cid:0)
v2;i are approximately normal, with the rate of convergence
determined by ps=D, which is O(cid:16) 1
Lemma 6. As D ! 1,

D1=4(cid:17) when s = pD.

The next lemma concerns the joint distribution of (v1;i; v2;i).

(cid:6)(cid:0) 1

2 (cid:20) v1;i

v2;i (cid:21) L=) N(cid:18)(cid:20) 0

0 (cid:21) ;(cid:20) 1 0

0 1 (cid:21)(cid:19) ;

and

where

(cid:6) =

Pr (sign(v1;i) = sign(v2;i)) ! 1 (cid:0)

(cid:18)
(cid:25)

:

1

a

k (cid:20) m1

a m2 (cid:21) ;

(cid:18) = cos(cid:0)1(cid:18)

a

pm1m2(cid:19) :

The asymptotic normality shows that we can use other
random projections matrix R to achieve asymptotically the
same performance as conventional random projections, which
are the easiest to analyze. Since the convergence rate is so
fast, we can simply apply results on conventional random
projections such as the JL lemma and sign random projec-
tions when a non-normal projection matrix is used.3
4.3 A Margin-free Estimator
1 v2) = uT

Recall that, because E(vT

1 u2, one can estimate

a = uT

1 u2 without bias as ^aM F = vT

1 v2, with the variance

Var (^aM F ) =

Var (^aM F )

1

1

k  m1m2 + a2 + (s (cid:0) 3)
k (cid:0)m1m2 + a2(cid:1) ;

1

=

1;j u2
u2

2;j! ; (27)

D

Xj=1

(28)

where the subscript \MF" indicates \Margin-free," i.e., an
estimator of a without using margins. Var (^aM F ) is the vari-
ance of vT
1 v2 in (19). Ignoring the asymptotically negligible
part involving s (cid:0) 3 leads to Var (^aM F )
lihood estimator based on the asymptotic normality.
4.4 An Asymptotic MLE Using Margins

We will compare ^aM F with an asymptotic maximum like-

1

.

The tractable asymptotic distributions of the projected
data allow us to derive more accurate estimators using max-
imum likelihood.

j=1 u2

In many situations, we can assume that the marginal
norms m1 = PD
2;j are known,
3In the proof of the asymptotic normality, we used E(jrjij3)
and E(jrjij2+(cid:14)). They should be replaced by the correspond-
ing moments when other projection distributions are used.

1;j and m2 = PD

j=1 u2

as m1 and m2 can often be easily either exactly calculated
or accurately estimated.4

The authors’ very recent work [28] on conventional ran-
dom projections shows that if we know the margins m1 and
m2, we can estimate a = uT
1 u2 often more accurately using
a maximum likelihood estimator (MLE).
The following lemma estimates a = uT

1 u2, taking advan-

tage of knowing the margins.

Lemma 7. When the margins, m1 and m2 are known, we
can use a maximum likelihood estimator (MLE) to estimate
a by maximizing the joint density function of (v1; v2). Since
(v1;i; v2;i) converges to a bivariate normal, an asymptotic
MLE is the solution to a cubic equation

a3 (cid:0) a2(cid:16)vT

1 v2(cid:17) + a(cid:0)(cid:0)m1m2 + m1kv2k2 + m2kv1k2(cid:1)
(cid:0) m1m2vT

1 v2 = 0:

(29)

is

(25)

(26)

The asymptotic variance of this estimator, denoted by ^aM LE,

Var (^aM LE)

=

1

1

k(cid:0)m1m2 (cid:0) a2(cid:1)2
m1m2 + a2 (cid:20) Var (^aM F )

:

1

(30)

=

The ratio Var(^aM LE )1
Var(^aM F )1

(m1m2(cid:0)a2)2
(m1m2 +a2)2 = (1(cid:0)cos2((cid:18)))2

(1+cos2((cid:18)))2 ranges
from 0 to 1, indicating possibly substantial improvements.
For example, when cos((cid:18)) (cid:25) 1 (i.e., a2 (cid:25) m1m2), the im-
provement will be huge. When cos((cid:18)) (cid:25) 0 (i.e., a (cid:25) 0), we
do not bene(cid:12)t from ^aM LE. Note that some studies (e.g., du-
plicate detection) are mainly interested in data points that
are quite similar (i.e., cos((cid:18)) close to 1).

4.5 The Kurtosis of rji : (s (cid:0) 3)

We have seen that the parameter s plays an important
role in the performance of very sparse random projections.
It is interesting that s (cid:0) 3 is exactly the kurtosis of rji:

(cid:13)2(rji) =

E((rji (cid:0) E(rji))4)
E2((rji (cid:0) E(rji))2) (cid:0) 3 = s (cid:0) 3;

(31)

as rji has zero mean and unit variance.5

The kurtosis for rji (cid:24) N (0; 1) is zero. If one is only inter-
ested in smaller estimation variances (ignoring the bene(cid:12)t of
sparsity), one may choose the distribution of rji with nega-
tive kurtosis. A couple of examples are

(cid:15) A continuous uniform distribution in [(cid:0)l; l] for any l >

0. It’s kurtosis = (cid:0) 6
5 .

(cid:15) A discrete uniform distribution symmetric about zero,
N 2(cid:0)1 , ranging be-
5 (when N ! 1). The

with N points. Its kurtosis = (cid:0) 6
tween -2 (when N = 2) and (cid:0) 6
case with N = 2 is the same as (2) with s = 1.

N 2 +1

5

(cid:15) Discrete and continuous U-shaped distributions.

4Computing all marginal norms of A costs O(nD), which
is often negligible. As important summary statistics, the
marginal norms may be already computed during various
stage of processing, e.g., normalization and term weighting.
5Note that the kurtosis can not be smaller than (cid:0)2 because
of the Cauchy-Schwarz inequality: E2(r2
ji). One
may consult http://en.wikipedia.org/wiki/Kurtosis for refer-
ences to kurtosis of various distributions.

ji) (cid:20) E(r4

5. HEAVY-TAIL AND TERM WEIGHTING
The very sparse random projections are useful even for

heavy-tailed data, mainly because of term weighting.

We have seen that bounded forth and third moments are
needed for analyzing the convergence of moments (variance)
and the convergence to normality, respectively. The proof
of asymptotic normality in Appendix A suggests that we
only need stronger than bounded second moments to ensure
asymptotic normality. In heavy-tailed data, however, even
the second moment may not exist.

Heavy-tailed data are ubiquitous in large-scale data min-
ing applications (especially Internet data) [25,34]. The pair-
wise distances computed from heavy-tailed data are usually
dominated by \outliers," i.e., exceptionally large entries.

Pairwise vector distances are meaningful only when all
dimensions of the data are more or less equally important.
For heavy-tailed data, such as the (unweighted) term-by-
document matrix, pairwise distances may be misleading.
Therefore, in practice, various term weighting schemes are
proposed e.g., [33, Chapter 15.2] [10, 30, 39, 45], to weight
the entries instead of using the original data.

It is well-known that choosing an appropriate term weight-
ing method is vital. For example, as shown in [23, 26], in
text categorization using support vector machine (SVM),
choosing an appropriate term weighting scheme is far more
important than tuning kernel functions of SVM. See similar
comments in [37] for the work on Naive Bayes text classi(cid:12)er.
We list two popular and simple weighting schemes. One
variant of the logarithmic weighting keeps zero entries and
replaces any non-zero count with 1+log(original count). An-
other scheme is the square root weighting. In the same spirit
of the Box-Cox transformation [44, Chapter 6.8], these vari-
ous weighting schemes signi(cid:12)cantly reduce the kurtosis (and
skewness) of the data and make the data resemble normal.
Therefore, it is fair to say that assuming (cid:12)nite moments
(third or fourth) is reasonable whenever the computed dis-
tances are meaningful.

However, there are also applications in which pairwise dis-
tances do not have to bear any clear meaning. For example,
using random projections to estimate the joint sizes (set
intersections).
If we expect the original data are severely
heavy-tailed and no term weighting will be applied, we rec-
ommend using s = O(1).

1;j

j=1 u4
j=1 u2

Finally, we shall point out that very sparse random pro-
jections can be fairly robust against heavy-tailed data when
s = pD. For example, instead of assuming (cid:12)nite fourth mo-
1;j)2 grows slower than O(pD),
ments, as long as D PD
(PD
we can still achieve the convergence of variances if s = pD,
in Lemma 1. Similarly, analyzing the rate of converge to
normality only requires that pD PD
1;j)3=2 grows slower
(PD
than O(D1=4). An even weaker condition is needed to only
ensure asymptotic normality. We provide some additional
analysis on heavy-tailed data in Appendix B.

j=1 ju1;jj3
j=1 u2

(Web page), j = 1 to D. Some summary statistics are listed
in Table 1.

The data are certainly heavy-tailed as the kurtoses for
u1;j and u2;j are 195 and 215, respectively, far above zero.
Therefore we do not expect that very sparse random projec-
tions with s = D
log D (cid:25) 6000 work well, though the results
are actually not disastrous as shown in Figure 1(d).

E(u2

1;j )E(u2

2;j )

E(u2

kurtosis.

(cid:17)(u1;j ; u2;j ) =

Table 1: Some summary statistics of the word pair,
(cid:13)2 denotes the
\THIS" (u1) and \HAVE" (u2).
1;j u2
2;j )+E2(u1;j u2;j ) , af-
fects the convergence of Var(cid:0)vT
1 v2(cid:1) (see the proof of
Lemma 3). These expectations are computed empir-
ically from the data. Two popular term weighting
schemes are applied. The \square root weighting"
replaces u1;j with pu1;j and the \logarithmic weight-
ing" replaces any non-zero u1;j with 1 + log u1;j .

(cid:13)2(u1;j )
(cid:13)2(u2;j )
E(u4
1;j )
E2(u2
1;j )
E(u4
2;j )
E2(u2
2;j )

(cid:17)(u1;j ; u2;j )
cos((cid:18)(u1; u2))

Unweighted
195.1
214.7

Square root Logarithmic
1.58
4.15

13.03
17.05

180.2

205.4

78.0
0.794

12.97

18.43

7.62
0.782

5.31

8.21

3.34
0.754

We (cid:12)rst test random projections on the original (unweighted,

heavy-tailed) data, for s = 1; 3; 256 = pD and 6000 (cid:25) D

log D ,
presented in Figure 1. We then apply square root weighting
and logarithmic weighting before random projections. The
results are presented in Figure 2, for s = 256 and s = 6000.
These results are consistent with what we would expect:

(cid:15) When s is small, i.e., O(1), sparse random projections
perform very similarly to conventional random projec-
tions as shown in panels (a) and (b) of Figure 1 .

(cid:15) With increasing s, the variances of sparse random pro-
jections increase. With s = D
log D , the errors are large
(but not disastrous), because the data are heavy-tailed.
With s = pD, sparse random projections are robust.

(cid:15) Since cos((cid:18)(u1; u2)) (cid:25) 0:7 (cid:24) 0:8 in this case, marginal
information can improve the estimation accuracy quite
substantially. The asymptotic variances of ^aM LE match
the empirical variances of the asymptotic MLE estima-
tor quite well, even for s = pD.

(cid:15) After applying term weighting on the original data,
sparse random projections are almost as accurate as
conventional random projections, even for s (cid:25) D
log D ,
as shown in Figure 2.

6. EXPERIMENTAL RESULTS

Some experimental results are presented as a sanity check,
using one pair of words, \THIS" and \HAVE," from two
rows of a term-by-document matrix provided by MSN. D =
216 = 65536. That is, u1;j (u2;j ) is the number of occur-
rences of word \THIS" (word \HAVE") in the jth document

7. CONCLUSION

We provide some new theoretical results on random pro-
jections, a randomized approximate algorithm widely used
in machine learning and data mining.
In particular, our
theoretical results suggest that we can achieve a signi(cid:12)cant
pD-fold speedup in processing time with little loss in accu-
racy, where D is the original data dimension. When the data

s
r
o
r
r
e

 

d
r
a
d
n
a
S

t

s
r
o
r
r
e

 

d
r
a
d
n
a
S

t

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10

 

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10

 

 

MF
MLE
Theor. MF
Theor. ¥

k

100

s
r
o
r
r
e

 

d
r
a
d
n
a
S

t

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10

 

 

MF
MLE
Theor. MF
Theor. ¥

k

100

s
r
o
r
r
e

 

d
r
a
d
n
a
S

t

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10

 

 

MF
MLE
Theor. MF
Theor. ¥

k

100

s
r
o
r
r
e

 

d
r
a
d
n
a
S

t

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10

 

 

MF
MLE
Theor. MF
Theor. ¥

k

100

(a) s = 1

(b) s = 3

(a) Square root (s = 256)

(b) Logarithmic (s = 256)

MF
MLE
Theor. MF
Theor. ¥

 

1.5

s
r
o
r
r
e

 

d
r
a
d
n
a
S

t

1

0.5

 

MF
MLE
Theor. MF
Theor. ¥

100

 

0
10

k

k

100

s
r
o
r
r
e

 

d
r
a
d
n
a
S

t

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10

 

 

MF
MLE
Theor. MF
Theor. ¥

k

100

s
r
o
r
r
e

 

d
r
a
d
n
a
S

t

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10

 

 

MF
MLE
Theor. MF
Theor. ¥

k

100

(c) s = 256

(d) s = 6000

(c) Square root (s = 6000)

(d) Logarithmic (s = 6000)

a

.

pVar(^a)

Figure 1: Two words \THIS" (u1) and \HAVE" (u2)
from the MSN Web crawl data are tested. D = 216.
Sparse random projections are applied to estimated
1 u2, with four values of s: 1, 3, 256 = pD
a = uT
and 6000 (cid:25) D
log D , in panels (a), (b), (c) and (d),
respectively, presented in terms of the normalized
104 simulations are con-
standard error,
ducted for each k, ranging from 10 to 100. There
are (cid:12)ve curves in each panel. The two labeled as
\MF" and \Theor." overlap. \MF" stands for the
empirical variance of the \Margin-free" estimator
^aM F ; while \Theor. MF" for the theoretical vari-
ance of ^aM F , i.e., (27). The solid curve, labeled as
\MLE," presents the empirical variance of ^aM LE, the
estimator using margins as formulated in Lemma 7.
There are two curves both labeled as \Theor. 1,"
for the asymptotic theoretical variances of ^aM F (the
higher curve, (28)) and ^aM LE (the lower curve, (30)).

are free of \outliers" (e.g., after careful term weighting), a
cost reduction by a factor of D

log D is also possible.

Our proof of the asymptotic normality justi(cid:12)es the use of
an asymptotic maximum likelihood estimator for improving
the estimates when the marginal information is available.

8. ACKNOWLEDGMENT

We thank Dimitris Achlioptas for very insightful com-
ments. We thank Xavier Gabaix and David Mason for point-
ers to useful references. Ping Li thanks the enjoyable and
helpful conversations with Tze Leung Lai, Joseph P. Ro-
mano, and Yiyuan She. Finally, we thank the four anony-
mous reviewers for constructive suggestions.

9. REFERENCES
[1] Dimitris Achlioptas. Database-friendly random projections:

Johnson-Lindenstrauss with binary coins. Journal of
Computer and System Sciences, 66(4):671{687, 2003.

Figure 2: After applying term weighting on the orig-
inal data, sparse random projections are almost as
accurate as conventional random projections, even
for s = 6000 (cid:25) D
log D . Note that the legends are the
same as in Figure 1.

[2] Dimitris Achlioptas, Frank McSherry, and Bernhard

Sch(cid:127)olkopf. Sampling techniques for kernel methods. In Proc.
of NIPS, pages 335{342, Vancouver, BC, Canada, 2001.
[3] Noga Alon, Yossi Matias, and Mario Szegedy. The space
complexity of approximating the frequency moments. In
Proc. of STOC, pages 20{29, Philadelphia,PA, 1996.

[4] Rosa Arriaga and Santosh Vempala. An algorithmic theory

of learning: Robust concepts and random projection. In
Proc. of FOCS (Also to appear in Machine Learning),
pages 616{623, New York, 1999.

[5] Ella Bingham and Heikki Mannila. Random projection in
dimensionality reduction: Applications to image and text
data. In Proc. of KDD, pages 245{250, San Francisco, CA,
2001.

[6] Jeremy Buhler and Martin Tompa. Finding motifs using
random projections. Journal of Computational Biology,
9(2):225{242, 2002.

[7] Moses S. Charikar. Similarity estimation techniques from

rounding algorithms. In Proc. of STOC, pages 380{388,
Montreal, Quebec, Canada, 2002.

[8] G. P. Chistyakov and F. G(cid:127)otze. Limit distributions of

studentized means. The Annals of Probability,
32(1A):28{77, 2004.

[9] Sanjoy Dasgupta and Anupam Gupta. An elementary proof

of a theorem of Johnson and Lindenstrauss. Random
Structures and Algorithms, 22(1):60 { 65, 2003.

[10] Susan T. Dumais. Improving the retrieval of information

from external sources. Behavior Research Methods,
Instruments and Computers, 23(2):229{236, 1991.

[11] Richard Durrett. Probability: Theory and Examples.
Duxbury Press, Belmont, CA, second edition, 1995.

[12] William Feller. An Introduction to Probability Theory and

Its Applications (Volume II). John Wiley & Sons, New
York, NY, second edition, 1971.

[13] Xiaoli Zhang Fern and Carla E. Brodley. Random

projection for high dimensional data clustering: A cluster
ensemble approach. In Proc. of ICML, pages 186{193,
Washington, DC, 2003.

[14] Dmitriy Fradkin and David Madigan. Experiments with

random projections for machine learning. In Proc. of KDD,
pages 517{522, Washington, DC, 2003.

A probabilistic analysis. In Proc. of PODS, pages 159{168,
Seattle,WA, 1998.

[36] Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
Randomized algorithms and NLP: Using locality sensitive
hash function for high speed noun clustering. In Proc. of
ACL, pages 622{629, Ann Arbor, MI, 2005.

[15] P. Frankl and H. Maehara. The Johnson-Lindenstrauss

[37] Jason D. Rennie, Lawrence Shih, Jaime Teevan, and

lemma and the sphericity of some graphs. Journal of
Combinatorial Theory A, 44(3):355{362, 1987.

[16] Navin Goel, George Bebis, and Ara Ne(cid:12)an. Face

recognition experiments with random projection. In Proc.
of SPIE, pages 426{437, Bellingham, WA, 2005.

[17] Michel X. Goemans and David P. Williamson. Improved

approximation algorithms for maximum cut and
satis(cid:12)ability problems using semide(cid:12)nite programming.
Journal of ACM, 42(6):1115{1145, 1995.

[18] F. G(cid:127)otze. On the rate of convergence in the multivariate

CLT. The Annals of Probability, 19(2):724{739, 1991.

[19] Piotr Indyk. Stable distributions, pseudorandom

generators, embeddings and data stream computation. In
FOCS, pages 189{197, Redondo Beach,CA, 2000.

[20] Piotr Indyk and Rajeev Motwani. Approximate nearest

neighbors: Towards removing the curse of dimensionality.
In Proc. of STOC, pages 604{613, Dallas, TX, 1998.

[21] W. B. Johnson and J. Lindenstrauss. Extensions of

Lipschitz mapping into Hilbert space. Contemporary
Mathematics, 26:189{206, 1984.

[22] Samuel Kaski. Dimensionality reduction by random

mapping: Fast similarity computation for clustering. In
Proc. of IJCNN, pages 413{418, Piscataway, NJ, 1998.

[23] Man Lan, Chew Lim Tan, Hwee-Boon Low, and Sam Yuan

Sung. A comprehensive comparative study on term
weighting schemes for text categorization with support
vector machines. In Proc. of WWW, pages 1032{1033,
Chiba, Japan, 2005.

[24] Erich L. Lehmann and George Casella. Theory of Point

Estimation. Springer, New York, NY, second edition, 1998.

[25] Will E. Leland, Murad S. Taqqu, Walter Willinger, and
Daniel V. Wilson. On the self-similar nature of Ethernet
tra(cid:14)c. IEEE/ACM Trans. Networking, 2(1):1{15, 1994.

[26] Edda Leopold and Jorg Kindermann. Text categorization

with support vector machines. how to represent texts in
input space? Machine Learning, 46(1-3):423{444, 2002.
[27] Henry C.M. Leung, Francis Y.L. Chin, S.M. Yiu, Roni

Rosenfeld, and W.W. Tsang. Finding motifs with
insu(cid:14)cient number of strong binding sites. Journal of
Computational Biology, 12(6):686{701, 2005.

[28] Ping Li, Trevor J. Hastie, and Kenneth W. Church.

Improving random projections using marginal information.
In Proc. of COLT, Pittsburgh, PA, 2006.

[29] Jessica Lin and Dimitrios Gunopulos. Dimensionality
reduction by random projection and latent semantic
indexing. In Proc. of SDM, San Francisco, CA, 2003.
[30] Bing Liu, Yiming Ma, and Philip S. Yu. Discovering

unexpected information from your competitors’ web sites.
In Proc. of KDD, pages 144{153, San Francisco, CA, 2001.

[31] Kun Liu, Hillol Kargupta, and Jessica Ryan. Random
projection-based multiplicative data perturbation for
privacy preserving distributed data mining. IEEE
Transactions on Knowledge and Data Engineering,
18(1):92{106, 2006.

[32] B. F. Logan, C. L. Mallows, S. O. Rice, and L. A. Shepp.

Limit distributions of self-normalized sums. The Annals of
Probability, 1(5):788{809, 1973.

[33] Chris D. Manning and Hinrich Schutze. Foundations of

Statistical Natural Language Processing. The MIT Press,
Cambridge, MA, 1999.

[34] M. E. J. Newman. Power laws, pareto distributions and
zipf’s law. Contemporary Physics, 46(5):232{351, 2005.
[35] Christos H. Papadimitriou, Prabhakar Raghavan, Hisao

Tamaki, and Santosh Vempala. Latent semantic indexing:

David R. Karger. Tackling the poor assumptions of naive
Bayes text classi(cid:12)ers. In Proc. of ICML, pages 616{623,
Washington, DC, 2003.

[38] Ozgur D. Sahin, Aziz Gulbeden, Fatih Emek(cid:24)ci, Divyakant

Agrawal, and Amr El Abbadi. Prism: indexing
multi-dimensional data in p2p networks using reference
vectors. In Proc. of ACM Multimedia, pages 946{955,
Singapore, 2005.

[39] Gerard Salton and Chris Buckley. Term-weighting

approaches in automatic text retrieval. Inf. Process.
Manage., 24(5):513{523, 1988.

[40] I. S. Shiganov. Re(cid:12)nement of the upper bound of the

constant in the central limit theorem. Journal of
Mathematical Sciences, 35(3):2545{2550, 1986.

[41] Chunqiang Tang, Sandhya Dwarkadas, and Zhichen Xu. On

scaling latent semantic indexing for large peer-to-peer
systems. In Proc. of SIGIR, pages 112{121, She(cid:14)eld, UK,
2004.

[42] Santosh Vempala. Random projection: A new approach to
VLSI layout. In Proc. of FOCS, pages 389{395, Palo Alto,
CA, 1998.

[43] Santosh Vempala. The Random Projection Method.

American Mathematical Society, Providence, RI, 2004.

[44] William N. Venables and Brian D. Ripley. Modern Applied

Statistics with S. Springer-Verlag, New York, NY, fourth
edition, 2002.

[45] Clement T. Yu, K. Lam, and Gerard Salton. Term

weighting in information retrieval using the term precision
model. Journal of ACM, 29(1):152{170, 1982.

APPENDIX
A. PROOFS

Let fuign
i=1 denote the rows of the data matrix A 2 Rn(cid:2)D.
A projection matrix R 2 RD(cid:2)k consists of i.i.d. entries rji:
Pr(rji = ps) = Pr(rji = (cid:0)ps) =
1
; Pr(rji = 0) = 1 (cid:0)
s
ji) = 1; E(r4
E(rji) = 0; E(r2

1
2s

;

ji) = s; E(jr3

jij) = ps;

E (rji rj 0i0 ) = 0;

We denote the projected data vectors by vi = 1pk

E(cid:0)r2

ji rj 0i0(cid:1) = 0 when i 6= i0; or j 6= j0:

RTui.

For convenience, we denote

D

D

m1 = ku1k2 =

a = uT

1 u2 =

D

u2
2;j ;

1;j ; m2 = ku2k2 =
u2

Xj=1
Xj=1
u1;j u2;j ; d = ku1 (cid:0) u2k2 = m1 + m2 (cid:0) 2a:

Xj=1

We will always assume

s = o(D); E(u4

2;j ) < 1;
By the strong law of large numbers

1;j ) < 1; E(u4

() E(u2

1;j u2

2;j ) < 1):

1;j

j=1 uI
D
j=1 (u1;j u2;j )J

! E(cid:16)uI

j=1 (u1;j (cid:0) u2;j )I

1;j(cid:17) ; PD
! E (u1;j u2;j )J ; a:s:

D

D

PD
PD

! E (u1;j (cid:0) u2;j )I ;

I = 2; 4;

J = 1; 2:

A.1 Moments

The following expansions are useful for proving the next

three lemmas.

D

D

D

D

u2

m2

u2
1;j

m1m2 =

Xj=1
Xj=1
1;j!2
1 =  D
Xj=1
u1;j u2;j!2
a2 =  D
Xj=1

u2
2;j =

u4

u2
1;j u2

Xj=1
1;j + 2Xj<j 0
Xj=1
Xj=1

u2
1;j u2

=

=

D

Lemma 1.

2;j +

u2
1;j u2

2;j 0 ;

D

Xj6=j 0

u2
1;j u2

1;j 0 ;

Lemma 2.

2;j + 2Xj<j 0

u1;j u2;j u1;j 0 u2;j 0 :

As D ! 1,

(s (cid:0) 3)PD

j=1 (u1;j )4
m2
1

=

D PD
s (cid:0) 3

j=1 (u1;j )4 =D

m2

1=D2

o(D)

D

!

E (u1;j )4
E2 (u1;j )2 ! 0:

1

E(cid:0)kv1k2(cid:1) = ku1k2 = m1;
k  2m2
Var(cid:0)kv1k2(cid:1) =
(s (cid:0) 3)PD

j=1 (u1;j )4
m2
1

!

1 + (s (cid:0) 3)

u4

1;j! :

D

Xj=1

s (cid:0) 3
D

E (u1;j )4
E2 (u1;j )2 ! 0:

As D ! 1,

RT

Proof of Lemma 1 . v1 = 1pk

RTu1, Let Ri be the ith
column of R, 1 (cid:20) i (cid:20) k. We can write the ith element of v1
to be v1;i = 1pk
k 0
@

i u1 = 1pk PD
Xj=1(cid:0)r2
1;j + 2Xj<j 0
ji(cid:1) u2

(rji) u1;j (rj 0 i) u1;j 01
A ;

j=1 (rji) u1;j . Therefore,

v2
1;i =

1

D

from which it follows that

from which it follows that

E(cid:0)v2

v4
1;i =

=

1

k2 0
BB@

E(cid:0)v4
1;i(cid:1) =
Var(cid:0)v2
1;i(cid:1) =

;

1
CCA

D

D

1
k

D

D

1

1

1;j 0

u2

Xj=1

Xj=1

u2
1;j = m1:

1;j(cid:0)r2

j 0i(cid:1) u2

1;j(cid:17)(cid:16)Pj<j 0 (rji) u1;j (rj 0 i) u1;j 0(cid:17)

1;j ; E(cid:0)kv1k2(cid:1) =

1;i(cid:1) =
(rji) u1;j (rj 0 i) u1;j 01
k2 0
1;j + 2Xj<j 0
Xj=1(cid:0)r2
ji(cid:1) u2
A
@
PD
j=1(cid:0)r4
ji(cid:1) u4
1;j + 2Pj<j 0(cid:0)r2
ji(cid:1) u2
+4(cid:16)Pj<j 0 (rji) u1;j (rj 0i) u1;j 0(cid:17)2
+4(cid:16)PD
j=1(cid:0)r2
ji(cid:1) u2
k2 0
1;j + 6Xj<j 0
Xj=1
@s
k2 0
1;j + 6Xj<j 0
Xj=1
@s
k2 0
Xj=1
@(s (cid:0) 1)
k2  2m2
1 + (s (cid:0) 3)
k  2m2

1;j + 4Xj<j 0
1;j! ;
Xj=1
1;j! :
Xj=1
1 + (s (cid:0) 3)

1;j 01
A ;
1;j 0 (cid:0)  D
Xj=1
1;j 01
A

1;j u2
u2

1;j u2
u2

1;j u2
u2

u4

u4

u4

u4

u2

u4

=

=

1

1

1

1

D

D

D

D

1;j!21
A

Var(cid:0)kv1k2(cid:1) =

1

E(cid:0)kv1 (cid:0) v2k2(cid:1) = ku1 (cid:0) u2k2 = d;
k  2d2 + (s (cid:0) 3)
Var(cid:0)kv1 (cid:0) v2k2(cid:1) =
As D ! 1,
(s (cid:0) 3)PD

j=1 (u1;j (cid:0) u2;j )4

s (cid:0) 3
D

!

d2

(u1;j (cid:0) u2;j )4! :

D

Xj=1

E (u1;j (cid:0) u2;j )4
E2 (u1;j (cid:0) u2;j )2 ! 0

Proof of Lemma 2. The proof is analogous to the proof

of Lemma 1.

Lemma 3.

1 u2 = a;

1 v2(cid:17) = uT
1 v2(cid:17) =

E(cid:16)vT
Var(cid:16)vT
As D ! 1,

1

k  m1m2 + a2 + (s (cid:0) 3)

1;j u2
u2

2;j! :

D

Xj=1

2

!

j=1 u2
m1m2 + a2

1;j u2
2;j

(s (cid:0) 3)PD
s (cid:0) 3
E(cid:0)u2
D

1;j u2

E(cid:0)u2
2;j(cid:1) + E2 (u1;j u2;j ) ! 0:
1;j(cid:1) E(cid:0)u2

2;j(cid:1)

Proof of Lemma 3.

v1;iv2;i =

1

k 0
@

D

Xj=1(cid:0)r2

ji(cid:1) u1;j u2;j + Xj6=j 0

(rji) u1;j (rj 0 i) u2;j 01
A ;

=) E (v1;iv2;i) =

1
k

D

Xj=1

u1;j u2;j ; E(cid:16)vT

1 v2(cid:17) = a:

1;iv2
v2
2;i

=

=

1

k2 0
@
0
BBBB@

1
k2

2

D

1;j u2

(rji) u1;j (rj 0i) u2;j 01
A

ji(cid:1) u1;j u2;j + Xj6=j 0
Xj=1(cid:0)r2
PD
j=1(cid:0)r4
ji(cid:1) u2
2Pj<j 0(cid:0)r2
ji(cid:1) u1;j u2;j(cid:0)r2
(cid:16)Pj6=j 0 (rji) u1;j (rj 0 i) u2;j 0(cid:17)2
(cid:16)PD
ji(cid:1) u1;j u2;j(cid:17)(cid:16)Pj6=j 0 (rji) u1;j (rj 0i) u2;j 0(cid:17)
j=1(cid:0)r2

j 0i(cid:1) u1;j 0 u2;j 0 +

2;j +

+

;

1
CCCCA

which immediately leads to

v2
1;i
m1=k

L=) (cid:31)2
1;

kv1k2
m1=k

=

We need to go back and check the Lindeberg condition.

k

1;i

Xi=1(cid:18) v2

m1=k(cid:19) L=) (cid:31)2

k:

D

E(cid:18)jzjj2+(cid:14)
((cid:15)sD)(cid:14)(cid:19)

D

(cid:14)
2 1

1
s2
D

Xj=1
=(cid:16) s
D(cid:17)
D (cid:19)
!(cid:18) o(D)

1
Xj=1
E(cid:0)z2
j ; jzjj (cid:21) (cid:15)sD(cid:1) (cid:20)
s2
D
(cid:15)(cid:14) PD
j=1 ju1;jj2+(cid:14)=D
1;j =D(cid:17)(2+(cid:14))=2
(cid:16)PD
j=1 u2
Eju1;jj2+(cid:14)
1;j )(cid:1)(2+(cid:14))=2 ! 0;
(cid:0)E(u2

(cid:14)
2 1
(cid:15)(cid:14)

kv1k2
m1=k

L=) (cid:31)2
k;

Lemma 5. As D ! 1,

D

1

=

1;iv2

u2
1;j u2

=)
E(cid:0)v2
2;i(cid:1)
k2 0
Xj=1
@s
k2 0
@(s (cid:0) 2)
k2  m1m2 + (s (cid:0) 3)

2;j + 4Xj<j 0
Xj=1

u2
1;j u2

=

=

1

1

D

u1;j u2;j u1;j 0 u2;j 0 + Xj6=j 0
2;j 0 + 2a21
A

u2
1;j u2

2;j + 2a2! ;

2;j + Xj6=j 0
Xj=1

u2
1;j u2

D

u2
1;j u2

2;j 01
A

Var (v1;iv2;i) =

1

k2  m1m2 + a2 + (s (cid:0) 3)

u2
1;j u2

2;j! ;

D

Xj=1

Var(cid:16)vT

1 v2(cid:17) =

1

k  m1m2 + a2 + (s (cid:0) 3)

u2
1;j u2

2;j! :

D

Xj=1

A.2 Asymptotic Distributions

Lemma 4. As D ! 1,

with the rate of convergence

v1;i

L=) N (0; 1);

pm1=k
jFv1;i (y) (cid:0) (cid:8)(y)j (cid:20) 0:8psPD
! 0:8r s

D

1

j=1 ju1;jj3
m3=2
Eju1;jj3
(cid:0)E(cid:0)u2

1;j(cid:1)(cid:1)3=2 ! 0;

where L=) denotes \convergence in distribution," Fv1;i (y) is
the empirical cumulative density function (CDF) of v1;i and
(cid:8)(y) is the standard normal N (0; 1) CDF.

Proof of Lemma 4. The Lindeberg central

limit theo-
rem (CLT) and the Berry-Esseen theorem are needed for
the proof [12, Theorems VIII.4.3 and XVI.5.2].6

Write v1;i = 1pk

RT

i u1 = PD

j=1

1pk

(rji) u1;j . Then

with zj = 1pk

E(zj) = 0; Var(zj) =

u2
1;j
k

Let s2
Lindeberg condition

j=1 Var(zj) = PD

k

; E(jzjj2+(cid:14)) = s
= m1

j=1 u2

1;j

j=1 zj,

(rji) u1;j = PD
2 ju1;jj2+(cid:14)
k(2+(cid:14))=2 ; 8(cid:14) > 0:

(cid:14)

k . Assume the

E(cid:0)z2

j ; jzjj (cid:21) (cid:15)sD(cid:1) ! 0; for any (cid:15) > 0:

Then

PD

j=1 zj
sD

=

L=) N (0; 1);

v1;i

pm1=k

6The best Berry-Esseen constant 0:7915 ((cid:25) 0:8) is from [40].

D = PD
Xj=1

1
s2
D

D

provided Eju1;jj2+(cid:14) < 1, for some (cid:14) > 0, which is much
weaker than our assumption that E(u4
It remains to show the rate of convergence using the Berry-
j=1 ju1;jj3

Esseen theorem. Let (cid:26)D =PD
jFv1;i (y) (cid:0) (cid:8)(y)j (cid:20) 0:8

(cid:26)D
s3
D

k3=2 PD
j=1 ju1;jj3
m3=2

1

1;j ) < 1.
j=1 Ejzjj3 = s1=2
= 0:8psPD
Eju1;jj3
(cid:0)E(cid:0)u2

D

1;j(cid:1)(cid:1)3=2 ! 0:

! 0:8r s

v1;i (cid:0) v2;i
pd=k

with the rate of convergence

L=) N (0; 1);

kv1 (cid:0) v2k2

d=k

L=) (cid:31)2
k;

jFv1;i(cid:0)v2;i (y) (cid:0) (cid:8)(y)j (cid:20) 0:8psPD
! 0:8r s

D

j=1 ju1;j (cid:0) u2;jj3

d3=2

Eju1;j (cid:0) u2;jj3
2 (u1;j (cid:0) u2;j )2 ! 0:
E

3

Proof of Lemma 5. The proof is analogous to the proof

of Lemma 4.

The next lemma concerns the joint distribution of (v1;i; v2;i).

Lemma 6. As D ! 1,
v2;i (cid:21) L=) N(cid:18)(cid:20) 0
2 (cid:20) v1;i

(cid:6)(cid:0) 1

0 (cid:21) ;(cid:20) 1 0

0 1 (cid:21)(cid:19) ; (cid:6) =

1

a

k (cid:20) m1

a m2 (cid:21)

and

Pr (sign(v1;i) = sign(v2;i)) ! 1 (cid:0)

(cid:18)
(cid:25)

; (cid:18) = cos(cid:0)1(cid:18)

a

pm1m2(cid:19) :

Proof of Lemma 6. We have seen that Var (v1;i) = m1
k ,

Var (v2;i) = m2

k , E (v1;iv2;i) = a

k , i.e.,

cov(cid:18)(cid:20) v1;i

v2;i (cid:21)(cid:19) =

1

a

k (cid:20) m1

a m2 (cid:21) = (cid:6):

The Lindeberg multivariate central limit theorem [18] says

(cid:6)(cid:0) 1

2 (cid:20) v1;i

v2;j (cid:21) L=) N(cid:18)(cid:20) 0

0 (cid:21) ;(cid:20) 1 0

0 1 (cid:21)(cid:19) :

The multivariate Lindeberg condition is automatically satis-
(cid:12)ed by assuming bounded third moments of u1;j and u2;j . A
trivial consequence of the asymptotic normality yields

Pr (sign(v1;i) = sign(v2;i)) ! 1 (cid:0)

(cid:18)
(cid:25)

:

Strictly speaking, we should write (cid:18) = cos(cid:0)1(cid:18) E(u1;j u2;j)
A.3 An Asymptotic MLE Using Margins

1;j)E(u2

qE(u2

2;j)(cid:19) :

Lemma 7. Assuming that the margins, m1 and m2 are
known and using the asymptotic normality of (v1;i; v2;i), we
can derive an asymptotic maximum likelihood estimator (MLE),
which is the solution to a cubic equation

a3 (cid:0) a2(cid:16)vT

1 v2(cid:17) + a(cid:0)(cid:0)m1m2 + m1kv2k2 + m2kv1k2(cid:1)
(cid:0) m1m2vT

1 v2 = 0;

Denoted by ^aM LE, the asymptotic variance of this estima-

tor is

Var (^aM LE)

=

1

1

k(cid:0)m1m2 (cid:0) a2(cid:1)2

m1m2 + a2

:

Proof of Lemma 7. For notational convenience, we treat
(v1;i; v2;i) as exactly normally distributed so that we do not
need to keep track of the \convergence" notation.
i=1 is then

i=1(cid:17) = (2(cid:25))(cid:0) k
exp (cid:0)

The likelihood function of fv1;i; v2;igk
lik(cid:16)fv1;i; v2;igk
2 j(cid:6)j(cid:0) k
2 (cid:2)
Xi=1(cid:2) v1;i
k (cid:20) m1

a m2 (cid:21) :

(cid:6) =

1
2

a

1

k

where

v2;i (cid:3) (cid:6)(cid:0)1(cid:20) v1;i

v2;i (cid:21)! :

We can then express the log likelihood function, l(a), as

log lik(cid:16)fv1;i; v2;igk
m1m2 (cid:0) a2

k
2

k
2

k

1

Xi=1(cid:0)v2

i=1(cid:17) / l(a) = (cid:0)

1;im2 (cid:0) 2v1;iv2;ia + v2

log(cid:0)m1m2 (cid:0) a2(cid:1) (cid:0)
2;im1(cid:1) ;
1 v2(cid:17) + a(cid:0)(cid:0)m1m2 + m1kv2k2 + m2kv1k2(cid:1)
(cid:0) m1m2vT

1 v2 = 0

The MLE equation is the solution to l0(a) = 0, which is

a3 (cid:0) a2(cid:16)vT

The large sample theory [24, Theorem 6.3.10] says that
^aM LE is asymptotically unbiased and converges in distribu-

tion to a normal random variable N(cid:16)a;

the expected Fisher Information, is

1

I(a)(cid:17), where I(a),

I(a) = (cid:0)E(cid:0)l00(a)(cid:1) = k

after some algebra.

m1m2 + a2

(m1m2 (cid:0) a2)2 ;

Therefore, the asymptotic variance of ^aM LE would be

Var (^aM LE)

=

1

1

k(cid:0)m1m2 (cid:0) a2(cid:1)2

m1m2 + a2

:

(32)

B. HEAVY-TAILED DATA

We illustrate that very sparse random projections are fairly
robust against heavy-tailed data, by a Pareto distribution.
The assumption of (cid:12)nite moments has simpli(cid:12)ed the anal-
ysis of convergence a great deal. For example, assuming
((cid:14) + 2)th moment, 0 < (cid:14) (cid:20) 2 and s = o(D), we have
(s)(cid:14)=2 PD
(cid:16)PD

j=1 ju1;jj2+(cid:14)
j=1(u2

D(cid:17)(cid:14)=2 PD
j=1 ju1;jj2+(cid:14)=D
1;j )=D(cid:17)1+(cid:14)=2
(cid:16)PD
j=1(u2
E(cid:0)u2+(cid:14)
1;j (cid:1)
1;j(cid:1)(cid:1)1+(cid:14)=2 ! 0: (33)
(cid:0)E(cid:0)u2

1;j )(cid:17)1+(cid:14)=2 =(cid:16) s
!(cid:16) s
D(cid:17)(cid:14)=2

Note that (cid:14) = 2 corresponds to the rate of convergence
for the variance in Lemma 1, and (cid:14) = 1 corresponds to the
rate of convergence for asymptotic normality in Lemma 4.
From the proof of Lemma 4 in Appendix A, we can see that
the convergence of (33) (to zero) with any (cid:14) > 0 su(cid:14)ces for
achieving asymptotic normality.

For heavy-tailed data, the fourth moment (or even the
second moment) may not exist. The most common model for
heavy-tailed data is the Pareto distribution with the density
function7 f (x; (cid:11)) = (cid:11)
(cid:11)(cid:0)m , only
de(cid:12)ned if (cid:11) > m. The measurements of (cid:11) for many types of
data are available in [34]. For example, (cid:11) = 1:2 for the word
frequency, (cid:11) = 2:04 for the citations to papers, (cid:11) = 2:51 for
the copies of books sold in the US, etc.

x(cid:11)+1 , whose mth moment = (cid:11)

For simplicity, we assume that 2 < (cid:11) (cid:20) 2 + (cid:14) (cid:20) 4. Un-
der this assumption, the asymptotic normality is guaranteed
and it remains to show the rate of convergence of moments

s(cid:14)=2

j=1 ju1;jj2+(cid:14)
j=1(u2

shown in [11, Example 2.7.4].8 Thus, we can write

and distributions. In this case, the second moment E(cid:0)u2
1;j(cid:1)
j=1 ju1;jj2+(cid:14) grows as O(cid:16)D(2+(cid:14))=(cid:11)(cid:17) as
exists. The sum PD
s(cid:14)=2 PD
1;j )(cid:17)1+(cid:14)=2 = O(cid:18)
(cid:16)PD
D2(cid:0)4=(cid:11)(cid:17)
O(cid:16)
=8<
D3(cid:0)6=(cid:11)(cid:17)1=2
O(cid:16)
:

from which we can choose s using prior knowledge of (cid:11).

For example, suppose (cid:11) = 3 and s = pD. (34) indicates
that the rate of convergence for variances would be O(D1=12)
in terms of the standard error. (34) also veri(cid:12)es that the rate
of convergence to normality is O(D1=4), as expected.

D1+(cid:14)=2(cid:0) 2+(cid:14)

(cid:11) (cid:19)

(cid:14) = 2

(cid:14) = 1

(34)

;

s

s

Of course, we could always choose s more conservatively,
e.g., s = D1=4, if we know the data are severely heavy-tailed.
Since D is large, a factor of D1=4 is still considerable.

What if (cid:11) < 2? The second moment no longer exists.
The analysis will involve the so-called self-normalizing sums
[8, 32]; but we will not delve into this topic. In fact, it is
not really meaningful to compute the l2 distances when the
data do not even have bounded second moment.

7Note that in general, a Pareto distribution has an addition
parameter xmin, and f (x; (cid:11); xmin) = (cid:11)xmin
x(cid:11)+1 with x (cid:21) xmin.
Since we are only interested in the relative ratio of moments,
we can without loss of generality assume xmin = 1. Also
note that in [34], their \(cid:11)" is equal to our (cid:11) + 1.
8Note that if x (cid:24) Pareto((cid:11)), then xt (cid:24) Pareto((cid:11)=t).

