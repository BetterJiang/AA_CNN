Sparse Principal Component Analysis

Hui Zou⁄, Trevor Hastiey, Robert Tibshiraniz

April 26, 2004

Abstract

Principal component analysis (PCA) is widely used in data processing and dimensionality

reduction. However, PCA suﬁers from the fact that each principal component is a linear combi-

nation of all the original variables, thus it is often di–cult to interpret the results. We introduce

a new method called sparse principal component analysis (SPCA) using the lasso (elastic net)

to produce modiﬂed principal components with sparse loadings. We show that PCA can be

formulated as a regression-type optimization problem, then sparse loadings are obtained by im-

posing the lasso (elastic net) constraint on the regression coe–cients. E–cient algorithms are

proposed to realize SPCA for both regular multivariate data and gene expression arrays. We

also give a new formula to compute the total variance of modiﬂed principal components. As

illustrations, SPCA is applied to real and simulated data, and the results are encouraging.

Keywords: multivariate analysis, gene expression arrays, elastic net, lasso, singular value de-

composition, thresholding

⁄Hui Zou is a Ph.D student in the Department of Statistics at Stanford University, Stanford, CA 94305. Email:

hzou@stat.stanford.edu.

yTrevor Hastie is Professor, Department of Statistics and Department of Health Research & Policy, Stanford

University, Stanford, CA 94305. Email: hastie@stat.stanford.edu.

zRobert Tibshirani is Professor, Department of Health Research & Policy and Department of Statistics, Stanford

University, Stanford, CA 94305. Email: tibs@stat.stanford.edu.

1

1

Introduction

Principal component analysis (PCA) (Jolliﬁe 1986) is a popular data processing and dimension

reduction technique . As an un-supervised learning method, PCA has numerous applications such

as handwritten zip code classiﬂcation (Hastie et al. 2001) and human face recognition (Hancock

et al. 1996). Recently PCA has been used in gene expression data analysis (Misra et al. 2002).

Hastie et al. (2000) propose the so-called Gene Shaving techniques using PCA to cluster high

variable and coherent genes in microarray data.

PCA seeks the linear combinations of the original variables such that the derived variables

capture maximal variance. PCA can be done via the singular value decomposition (SVD) of the

data matrix.

In detail, let the data X be a n £ p matrix, where n and p are the number of
observations and the number of variables, respectively. Without loss of generality, assume the

column means of X are all 0. Suppose we have the SVD of X as

X = UDVT

(1)

where T means transpose. U are the principal components (PCs) of unit length, and the columns

of V are the corresponding loadings of the principal components. The variance of the ith PC is

D2

i;i. In gene expression data the PCs U are called the eigen-arrays and V are the eigen-genes

(Alter et al. 2000). Usually the ﬂrst q (q ¿ p) PCs are chosen to represent the data, thus a great
dimensionality reduction is achieved.

The success of PCA is due to the following two important optimal properties:

1. principal components sequentially capture the maximum variability among X, thus guaran-

teeing minimal information loss;

2. principal components are uncorrelated, so we can talk about one principal component without

2

referring to others.

However, PCA also has an obvious drawback, i.e., each PC is a linear combination of all p variables

and the loadings are typically nonzero. This makes it often di–cult to interpret the derived PCs.

Rotation techniques are commonly used to help practitioners to interpret principal components

(Jolliﬁe 1995). Vines (2000) considered simple principal components by restricting the loadings to

take values from a small set of allowable integers such as 0, 1 and -1.

We feel it is desirable not only to achieve the dimensionality reduction but also to reduce the

size of explicitly used variables. An ad hoc way is to artiﬂcially set the loadings with absolute

values smaller than a threshold to zero. This informal thresholding approach is frequently used in

practice but can be potentially misleading in various respects (Cadima & Jolliﬁe 1995). McCabe

(1984) presented an alternative to PCA which found a subset of principal variables. Jolliﬁe & Uddin

(2003) introduced SCoTLASS to get modiﬂed principal components with possible zero loadings.

Recall the same interpretation issue arising in multiple linear regression, where the response is

predicted by a linear combination of the predictors. Interpretable models are obtained via variable

selection. The lasso (Tibshirani 1996) is a promising variable selection technique, simultaneously

producing accurate and sparse models. Zou & Hastie (2003) propose the elastic net, a generalization

of the lasso, to further improve upon the lasso. In this paper we introduce a new approach to get

modiﬂed PCs with sparse loadings, which we call sparse principal component analysis (SPCA).

SPCA is built on the fact that PCA can be written as a regression-type optimization problem, thus

the lasso (elastic net) can be directly integrated into the regression criterion such that the resulting

modiﬂed PCA produces sparse loadings.

In the next section we brie(cid:176)y review the lasso and the elastic net. The method details of SPCA

are presented in Section 3. We ﬂrst discuss a direct sparse approximation approach via the elas-

tic net, which is a useful exploratory tool. We then show that ﬂnding the loadings of principal

3

components can be reformulated as estimating coe–cients in a regression-type optimization prob-

lem. Thus by imposing the lasso (elastic net) constraint on the coe–cients, we derive the modiﬂed

principal components with sparse loadings. An e–cient algorithm is proposed to realize SPCA.

We also give a new formula, which justiﬂes the correlation eﬁects, to calculate the total variance

of modiﬂed principal components. In Section 4 we consider a special case of the SPCA algorithm

to e–ciently handle gene expression arrays. The proposed methodology is illustrated by using real

data and simulation examples in Section 5. Discussions are in Section 6. The paper ends up with

an appendix summarizing technical details.

2 The Lasso and The Elastic Net

Consider the linear regression model. Suppose the data set has n observations with p predictors.

Let Y = (y1; : : : ; yn)T be the response and Xj = (x1j; : : : ; xnj)T ; i = 1; : : : ; p are the predictors.

After a location transformation we can assume all Xj and Y are centered.

The lasso is a penalized least squares method, imposing a constraint on the L1 norm of the

regression coe–cients. Thus the lasso estimates ^ﬂlasso are obtained by minimizing the lasso criterion

^ﬂlasso = arg min

Y ¡

pXj=1

ﬂ ﬂﬂﬂﬂﬂﬂ

2

+ ‚

pXj=1

jﬂjj ;

(2)

Xjﬂjﬂﬂﬂﬂﬂﬂ

where ‚ is a non-negative value. The lasso was originally solved by quadratic programming

(Tibshirani 1996). Efron et al. (2004) proved that the lasso estimates as a function of ‚ are piece-

wise linear, and proposed an algorithm called LARS to e–ciently solve the whole lasso solution

path in the same order of computations as a single least squares ﬂt.

The lasso continuously shrinks the coe–cients toward zero, thus gaining its prediction accuracy

via the bias variance trade-oﬁ. Moreover, due to the nature of the L1 penalty, some coe–cients

4

will be shrunk to exact zero if ‚1 is large enough. Therefore the lasso simultaneously produces an

accurate and sparse model, which makes it a favorable variable selection method. However, the

lasso has several limitations as pointed out in Zou & Hastie (2003). The most relevant one to this

work is that the number of selected variables by the lasso is limited by the number of observations.

For example, if applied to the microarray data where there are thousands of predictors (genes)

(p > 1000) with less than 100 samples (n < 100), the lasso can only select at most n genes, which

is clearly unsatisfactory.

The elastic net (Zou & Hastie 2003) generalizes the lasso to overcome its drawbacks, while

enjoying the similar optimal properties. For any non-negative ‚1 and ‚2, the elastic net estimates

^ﬂen are given as follows

^ﬂen = (1 + ‚2) arg min

Y ¡

pXj=1

ﬂ ﬂﬂﬂﬂﬂﬂ

2

+ ‚2

pXj=1

jﬂjj2 + ‚1

pXj=1

jﬂjj :

(3)

Xjﬂjﬂﬂﬂﬂﬂﬂ

Hence the elastic net penalty is a convex combination of ridge penalty and the lasso penalty .

Obviously, the lasso is a special case of the elastic net with ‚2 = 0. Given a ﬂxed ‚2, the LARS-

EN algorithm (Zou & Hastie 2003) e–ciently solves the elastic net problem for all ‚1 with the

computation cost as a single least squares ﬂt. When p > n, we choose some ‚2 > 0. Then the

elastic net can potentially include all variables in the ﬂtted model, so the limitation of the lasso is

removed. An additional beneﬂt oﬁered by the elastic net is its grouping eﬁect, that is, the elastic

net tends to select a group of highly correlated variables once one variable among them is selected.

In contrast, the lasso tends to select only one out of the grouped variables and does not care which

one is in the ﬂnal model. Zou & Hastie (2003) compare the elastic net with the lasso and discuss

the application of the elastic net as a gene selection method in microarray analysis.

5

3 Motivation and Method Details

In both lasso and elastic net, the sparse coe–cients are a direct consequence of the L1 penalty,

not depending on the squared error loss function. Jolliﬁe & Uddin (2003) proposed SCoTLASS

by directly putting the L1 constraint in PCA to get sparse loadings. SCoTLASS successively

maximizes the variance

subject to

aT
k (XT X)ak

aT
k ak = 1

and (for k ‚ 2) aT

h ak = 0;

h < k;

and the extra constraints

pXj=1

jak;jj • t

(4)

(5)

(6)

for some tuning parameter t. Although su–ciently small t yields some exact zero loadings, SCoT-

LASS seems to lack of a guidance to choose an appropriate t value. One might try several t values,

but the high computational cost of SCoTLASS makes it an impractical solution. The high compu-

tational cost is due to the fact that SCoTLASS is not a convex optimization problem. Moreover,

the examples in Jolliﬁe & Uddin (2003) show that the obtained loadings by SCoTLASS are not

sparse enough when requiring a high percentage of explained variance.

We consider a diﬁerent approach to modify PCA, which can more directly make good use of

the lasso. In light of the success of the lasso (elastic net) in regression, we state our strategy

We seek a regression optimization framework in which PCA is done exactly. In addition,

the regression framework should allow a direct modiﬂcation by using the lasso (elastic

net) penalty such that the derived loadings are sparse.

6

3.1 Direct sparse approximations

We ﬂrst discuss a simple regression approach to PCA. Observe that each PC is a linear combination

of the p variables, thus its loadings can be recovered by regressing the PC on the p variables.

Theorem 1 8i, denote Yi = UiDi. Yi is the i-th principal component. 8 ‚ > 0, suppose ^ﬂridge is
the ridge estimates given by

^ﬂridge = arg min

ﬂ jYi ¡ Xﬂj2 + ‚jﬂj2 :

(7)

Let ^v =

^ﬂridge
j ^ﬂridgej

, then ^v = Vi:

The theme of this simple theorem is to show the connection between PCA and a regression

method is possible. Regressing PCs on variables was discussed in Cadima & Jolliﬁe (1995), where

they focused on approximating PCs by a subset of k variables. We extend it to a more general

ridge regression in order to handle all kinds of data, especially the gene expression data. Obviously

when n > p and X is a full rank matrix, the theorem does not require a positive ‚. Note that if

p > n and ‚ = 0, ordinary multiple regression has no unique solution that is exactly Vi. The same

story happens when n > p and X is not a full rank matrix. However, PCA always gives a unique

solution in all situations. As shown in theorem 1, this discrepancy is eliminated by the positive

ridge penalty (‚jﬂj2). Note that after normalization the coe–cients are independent of ‚, therefore
the ridge penalty is not used to penalize the regression coe–cients but to ensure the reconstruction

of principal components. Hence we keep the ridge penalty term throughout this paper.

Now let us add the L1 penalty to (7) and consider the following optimization problem

^ﬂ = arg min

ﬂ jYi ¡ Xﬂj2 + ‚jﬂj2 + ‚1 jﬂj1 :

(8)

7

We call ^Vi =

^ﬂ
j ^ﬂj

an approximation to Vi, and X ^Vi the ith approximated principal component. (8)

is called naive elastic net (Zou & Hastie 2003) which diﬁers from the elastic net by a scaling factor

(1 + ‚). Since we are using the normalized ﬂtted coe–cients, the scaling factor does not aﬁect ^Vi.

Clearly, large enough ‚1 gives a sparse ^ﬂ, hence a sparse ^Vi. Given a ﬂxed ‚, (8) is e–ciently solved

for all ‚1 by using the LARS-EN algorithm (Zou & Hastie 2003). Thus we can (cid:176)exibly choose a

sparse approximation to the ith principal component.

3.2 Sparse principal components based on SPCA criterion

Theorem 1 depends on the results of PCA, so it is not a genuine alternative. However, it can be

used in a two-stage exploratory analysis: ﬂrst perform PCA, then use (8) to ﬂnd suitable sparse

approximations.

We now present a \self-contained" regression-type criterion to derive PCs. We ﬂrst consider

the leading principal component.

Theorem 2 Let Xi denote the ith row vector of the matrix X. For any ‚ > 0, let

(^ﬁ; ^ﬂ) = arg min
ﬁ;ﬂ

subject to

nXi=1ﬂﬂXi ¡ ﬁﬂT Xiﬂﬂ2

jﬁj2 = 1:

+ ‚jﬂj2

(9)

Then ^ﬂ / V1:

The next theorem extends theorem 2 to derive the whole sequence of PCs.

Theorem 3 Suppose we are considering the ﬂrst k principal components. Let ﬁ and ﬂ be p £ k
matrices. Xi denote the i-th row vector of the matrix X. For any ‚ > 0, let

(^ﬁ; ^ﬂ) = arg min
ﬁ;ﬂ

nXi=1ﬂﬂXi ¡ ﬁﬂT Xiﬂﬂ2

+ ‚

kXj=1

jﬂjj2

8

(10)

subject to ﬁT ﬁ = Ik:

Then ^ﬂi / Vi for i = 1; 2; : : : ; k.

Theorem 3 eﬁectively transforms the PCA problem to a regression-type problem. The critical el-

ement is the object functionPn
i=1ﬂﬂXi ¡ ﬁﬁT Xiﬂﬂ2
Pn

i=1ﬂﬂXi ¡ ﬁﬂT Xiﬂﬂ2

. If we restrict ﬂ = ﬁ, thenPn

i=1ﬂﬂXi ¡ ﬁﬂT Xiﬂﬂ2

=

, whose minimizer under the orthonormal constraint on ﬁ is exactly the ﬂrst k

loading vectors of ordinary PCA. This is actually an alternative derivation of PCA other than the

maximizing variance approach, e.g. Hastie et al. (2001). Theorem 3 shows that we can still have

exact PCA while relaxing the restriction ﬂ = ﬁ and adding the ridge penalty term. As can be seen

later, these generalizations enable us to (cid:176)exibly modify PCA.

To obtain sparse loadings, we add the lasso penalty into the criterion (10) and consider the

following optimization problem

(^ﬁ; ^ﬂ) = arg min
ﬁ;ﬂ

nXi=1ﬂﬂXi ¡ ﬁﬂT Xiﬂﬂ2

+ ‚

kXj=1

jﬂjj2 +

kXj=1

‚1;j jﬂjj1

(11)

subject to ﬁT ﬁ = Ik:

Whereas the same ‚ is used for all k components, diﬁerent ‚1;js are allowed for penalizing the

loadings of diﬁerent principal components. Again, if p > n, a positive ‚ is required in order to get

exact PCA when the sparsity constraint (the lasso penalty) vanishes (‚1;j = 0). (11) is called the

SPCA criterion hereafter.

9

3.3 Numerical solution

We propose an alternatively minimization algorithm to minimize the SPCA criterion. From the

proof of theorem 3 (see appendix for details) we get

i=1ﬂﬂXi ¡ ﬁﬂT Xiﬂﬂ2
Pn
j=1‡ﬂT

= TrXT X +Pk

+ ‚Pk

j=1 jﬂjj2 +Pk

j (XT X + ‚)ﬂj ¡ 2ﬁT

j=1 ‚1;j jﬂjj1

j XT Xﬂj + ‚1;j jﬂjj1· :

(12)

Hence if given ﬁ, it amounts to solve k independent elastic net problems to get ^ﬂj for j = 1; 2; : : : ; k.

On the other hand, we also have (details in appendix)

i=1ﬂﬂXi ¡ ﬁﬂT Xiﬂﬂ2
Pn

j=1 jﬂjj2 +Pk
= TrXT X ¡ 2TrﬁT XT Xﬂ + TrﬂT (XT X + ‚)ﬂ +Pk

+ ‚Pk

j=1 ‚1;j jﬂjj1

j=1 ‚1;j jﬂjj1 :

(13)

Thus if ﬂ is ﬂxed, we should maximize TrﬁT (XT X)ﬂ subject to ﬁT ﬁ = Ik, whose solution is given

by the following theorem.

Theorem 4 Let ﬁ and ﬂ be m £ k matrices and ﬂ has rank k. Consider the constrained maxi-
mization problem

^ﬁ = arg max

ﬁ

Tr¡ﬁT ﬂ¢

subject to ﬁT ﬁ = Ik:

(14)

Suppose the SVD of ﬂ is ﬂ = U DV T , then ^ﬁ = U V T .

Here are the steps of our numerical algorithm to derive the ﬂrst k sparse PCs.

General SPCA Algorithm

1. Let ﬁ start at V[; 1 : k], the loadings of ﬂrst k ordinary principal components.

10

2. Given ﬂxed ﬁ, solve the following naive elastic net problem for j = 1; 2; : : : ; k

ﬂj = arg min
ﬂ⁄

ﬂ⁄T (XT X + ‚)ﬂ⁄ ¡ 2ﬁT

j XT Xﬂ⁄ + ‚1;j jﬂ⁄j1 :

(15)

3. For each ﬂxed ﬂ, do the SVD of XT Xﬂ = U DV T , then update ﬁ = U V T .

4. Repeat steps 2-3, until ﬂ converges.

5. Normalization: ^Vj = ﬂj
jﬂjj

, j = 1; : : : ; k.

Some remarks:

1. Empirical evidence indicates that the outputs of the above algorithm vary slowly as ‚ changes.

For n > p data, the default choice of ‚ can be zero. Practically ‚ is a small positive number

to overcome potential collinearity problems of X. Section 4 discusses the default choice of ‚

for the data with thousands of variables, such as gene expression arrays.

2. In principle, we can try several combinations of f‚1;jg to ﬂgure out a good choice of the
tunning parameters, since the above algorithm converges quite fast. There is a shortcut

provided by the direct sparse approximation (8). The LARS-EN algorithm e–ciently deliver

a whole sequence of sparse approximations for each PC and the corresponding values of ‚1;j.

Hence we can pick a ‚1;j which gives a good compromise between variance and sparsity. In

this selection, variance has a higher priority than sparsity, thus we tend to be conservative in

pursuing sparsity.

3. Both PCA and SPCA depend on X only through XT X. Note that XT X

n

is actually the

sample covariance matrix of variables (Xi). Therefore if §, the covariance matrix of (Xi), is

known, we can replace XT X with § and have a population version of PCA or SPCA. If X is

11

standardized beforehand, then PCA or SPCA uses the (sample) correlation matrix, which is

preferred when the scales of the variables are diﬁerent.

3.4 Adjusted total variance

The ordinary principal components are uncorrelated and their loadings are orthogonal. Let ^§ =

XT X, then VT V = Ik and VT ^§V is diagonal.

It is easy to check that only the loadings of

ordinary principal components can satisfy both conditions. In Jolliﬁe & Uddin (2003) the loadings

were forced to be orthogonal, so the uncorrelated property was sacriﬂced. SPCA does not explicitly

impose the uncorrelated components condition too.

Let ^U be the modiﬂed PCs. Usually the total variance explained by ^U is calculated by

trace( ^UT ^U). This is unquestionable when ^U are uncorrelated. However, if they are correlated, the

computed total variance is too optimistic. Here we propose a new formula to compute the total

variance explained by ^U, which takes into account the correlations among ^U.

Suppose ( ^Ui; i = 1; 2; : : : ; k) are the ﬂrst k modiﬂed PCs by any method. Denote ^Uj¢1;:::;j¡1 the

reminder of ^Uj after adjusting the eﬁects of ^U1; : : : ; ^Uj¡1, that is

^Uj¢1;:::;j¡1 = ^Uj ¡ H1;:::;j¡1 ^Uj;

(16)

where H1;:::;j¡1 is the projection matrix on ^Ui i = 1; 2; : : : ; j ¡ 1. Then the adjusted variance
. When the

2

, and the total explained variance is given by Pk

of ^Uj isﬂﬂﬂ ^Uj¢1;:::;j¡1ﬂﬂﬂ

2

j=1ﬂﬂﬂ ^Uj¢1;:::;j¡1ﬂﬂﬂ

modiﬂed PCs ^U are uncorrelated, then the new formula agrees with trace( ^UT ^U). Note that the

above computations depend on the order of ^Ui. However, since we have a natural order in PCA,

ordering is not an issue here.

Using the QR decomposition, we can easily compute the adjusted variance. Suppose ^U = QR,

12

where Q is orthonormal and R is upper triangular. Then it is straightforward to see that

ﬂﬂﬂ ^Uj¢1;:::;j¡1ﬂﬂﬂ
Hence the explained total variance is equal toPk

2

= R2

j;j:

(17)

j=1 R2

j;j.

3.5 Computation complexity

PCA is computationally e–cient for both n > p or p (cid:192) n data. We separately discuss the
computational cost of the general SPCA algorithm for n > p and p (cid:192) n.

1. n > p. Traditional multivariate data ﬂt in this category. Note that although the SPCA

criterion is deﬂned using X, it only depends on X via XT X. A trick is to ﬂrst compute the

p £ p matrix ^§ = XT X once for all, which requires np2 operations. Then the same ^§ is
used at each step within the loop. Computing XT Xﬂ costs p2k and the SVD of XT Xﬂ is of

order O(pk2). Each elastic net solution requires at most O(p3) operations. Since k • p, the
total computation cost is at most np2 + mO(p3), where m is the number of iterations before

convergence. Therefore the SPCA algorithm is able to e–ciently handle data with huge n, as

long as p is small (say p < 100).

2. p (cid:192) n. Gene expression arrays are typical examples of this p (cid:192) n category. The trick of ^§
is no longer applicable, because ^§ is a huge matrix (p £ p) in this case. The most consuming
step is solving each elastic net, whose cost is of order O(pJ 2) for a positive ﬂnite ‚, where J is

the number of nonzero coe–cients. Generally speaking the total cost is of order mO(pJ 2k),

which is expensive for a large J. Fortunately, as shown in Section 4, there exits a special

SPCA algorithm for e–ciently dealing with p (cid:192) n data.

13

4 SPCA for p (cid:192) n and Gene Expression Arrays

Gene expression arrays are a new type of data where the number of variables (genes) are much

bigger than the number of samples. Our general SPCA algorithm still ﬂts this situation using a

positive ‚. However the computation cost is expensive when requiring a large number of nonzero

loadings. It is desirable to simplify the general SPCA algorithm to boost the computation.

Observe that theorem 3 is valid for all ‚ > 0, so in principle we can use any positive ‚. It turns

out that a thrifty solution emerges if ‚ ! 1. Precisely, we have the following theorem.

Theorem 5 Let ^Vi(‚) =

^ﬂi
j ^ﬂij

be the loadings derived from criterion (11). Deﬂne (^ﬁ⁄; ^ﬂ⁄) as the

solution of the optimization problem

(^ﬁ⁄; ^ﬂ⁄) = arg min

ﬁ;ﬂ ¡2TrﬁT XT Xﬂ +

subject to ﬁT ﬁ = Ik:

ﬂ2
j +

kXj=1

kXj=1

‚1;j jﬂjj1

(18)

When ‚ ! 1, ^Vi(‚) !

:

^ﬂ⁄i
j ^ﬂ⁄i j

By the same statements in Section 3.3, criterion (18) is solved by the following algorithm, which

is a special case of the general SPCA algorithm with ‚ = 1:

Gene Expression Arrays SPCA Algorithm

Replacing step 2 in the general SPCA algorithm with

Step 2⁄: Given ﬂxed ﬁ, for j = 1; 2; : : : ; k

ﬂj =(cid:181)ﬂﬂﬁT

j XT Xﬂﬂ ¡

‚1;j

2 ¶+

Sign(ﬁT

j XT X):

(19)

The operation in (19) is called soft-thresholding. Figure 1 gives an illustration of how the

soft-thresholding rule operates. Recently soft-thresholding has become increasingly popular in

14

y

(0,0)

x

Figure 1: An illstration of soft-thresholding rule y = (jxj ¡ ¢)+Sign(x) with ¢ = 1.

the literature. For example, nearest shrunken centroids (Tibshirani et al. 2002) adopts the soft-

thresholding rule to simultaneously classify samples and select important genes in microarrays.

5 Examples

5.1 Pitprops data

The pitprops data ﬂrst introduced in Jeﬁers (1967) has 180 observations and 13 measured variables.

It is the classic example showing the di–culty of interpreting principal components. Jeﬁers (1967)

tried to interpret the ﬂrst 6 PCs. Jolliﬁe & Uddin (2003) used their SCoTLASS to ﬂnd the modiﬂed

PCs. Table 1 presents the results of PCA, while Table 2 presents the modiﬂed PCs loadings by

SCoTLASS and the adjusted variance computed using (17).

As a demonstration, we also considered the ﬂrst 6 principal components. Since this is a usual

15

D
n (cid:192) p data set, we set ‚ = 0. ‚1 = (0:06; 0:16; 0:1; 0:5; 0:5; 0:5) were chosen according to Figure 2
such that each sparse approximation explained almost the same amount of variance as the ordinary

PC did. Table 3 shows the obtained sparse loadings and the corresponding adjusted variance.

Compared with the modiﬂed PCs by SCoTLASS, PCs by SPCA account for nearly the same

amount of variance (75.8% vs. 78.2%) but with a much sparser loading structure. The important

variables associated with the 6 PCs do not overlap, which further makes the interpretations easier

and clearer. It is interesting to note that in Table 3 even though the variance does not strictly

monotonously decrease, the adjusted variance follows the right order. However, Table 2 shows this

is not true in SCoTLASS. It is also worthy to mention that the whole computation of SPCA was

done in seconds in R, while the implementation of SCoTLASS for each t was expensive (Jolliﬁe &

Uddin 2003). Optimizing SCoTLASS over several values of t is even a more di–cult computational

challenge.

Although the informal thresholding method, which is referred to as simple thresholding hence-

forth, has various drawbacks, it may serve as the benchmark for testing sparse PCs methods. An

variant of simple thresholding is soft-thresholding. We found that used in PCA, soft-thresholding

performs very similarly to simple thresholding. Thus we omitted the results of soft-thresholding

in this paper. Both SCoTLASS and SPCA were compared with simple thresholding. Table 4

presents the loadings and the corresponding explained variance by simple thresholding. To make

fair comparisons, we let the numbers of nonzero loadings by simple thresholding match the results

of SCoTLASS and SPCA. In terms of variance, it seems that simple thresholding is better than

SCoTLASS and worse than SPCA. Moreover, the variables with non-zero loadings by SPCA are

very diﬁerent to that chosen by simple thresholding for the ﬂrst three PCs; while SCoTLASS seems

to create a similar sparseness pattern as simple thresholding does, especially in the leading PC.

16

PC 1

PC 2

V
E
P

5
1

.

0

0
1

.

0

5
0

.

0

0
0

.

0

0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

0.0

0.5

1.0

1.5

2.0

2.5

l 1

PC 3

l 1

PC 4

0.0

0.5

1.5

1.0

l 1

PC 5

0.0

0.5

1.0

1.5

l 1

PC 6

V
E
P

8
0

.

0

6
0
0

.

4
0

.

0

2
0
0

.

0
0

.

0

8
0
0

.

6
0
.
0

4
0
.
0

V
E
P

2
0
.
0

0
0
.
0

V
E
P

V
E
P

0
3

.

0

5
2

.

0

0
2

.

0

5
1

.

0

0
1

.

0

5
0

.

0

0
0

.

0

5
1

.

0

0
1

.

0

5
0

.

0

0
0

.

0

8
0
0

.

6
0
.
0

4
0
.
0

V
E
P

2
0
.
0

0
0
.
0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

l 1

l 1

Figure 2: Pitprops data: The sequences of sparse approximations to the ﬂrst 6 principal components.
Plots show the percentage of explained variance (PEV) as a function of ‚1.

17

Table 1: Pitprops data: loadings of the ﬂrst 6 principal components

Variable
topdiam
length
moist
testsg
ovensg
ringtop
ringbut
bowmax
bowdist
whorls
clear
knots
diaknot
Variance (%)
Cumulative Variance (%)

PC1
-0.404
-0.406
-0.124
-0.173
-0.057
-0.284
-0.400
-0.294
-0.357
-0.379
0.011
0.115
0.113
32.4
32.4

PC2
0.218
0.186
0.541
0.456
-0.170
-0.014
-0.190
-0.189
0.017
-0.248
0.205
0.343
0.309
18.3
50.7

PC3
-0.207
-0.235
0.141
0.352
0.481
0.475
0.253
-0.243
-0.208
-0.119
-0.070
0.092
-0.326
14.4
65.1

PC4
0.091
0.103
-0.078
-0.055
-0.049
0.063
0.065
-0.286
-0.097
0.205
-0.804
0.301
0.303
8.5
73.6

PC5
-0.083
-0.113
0.350
0.356
0.176
-0.316
-0.215
0.185
-0.106
0.156
-0.343
-0.600
0.080
7.0
80.6

PC6
0.120
0.163
-0.276
-0.054
0.626
0.052
0.003
-0.055
0.034
-0.173
0.175
-0.170
0.626
6.3
86.9

Table 2: Pitprops data: loadings of the ﬂrst 6 modiﬂed PCs by SCoTLASS

t = 1:75
Variable
topdiam
length
moist
testsg
ovensg
ringtop
ringbut
bowmax
bowdist
whorls
clear
knots
diaknot
Number of nonzero loadings
Variance (%)
Adjusted Variance (%)
Cumulative Adjusted Variance (%)

PC2
0.047
0.000
0.641
0.641
0.000
0.356
0.000
-0.007
0.000
-0.065
0.000
0.206
0.000
7
16.4
15.3
42.5

PC3
-0.087
-0.076
-0.187
0.000
0.457
0.348
0.325
0.000
0.000
0.000
0.000
0.000
-0.718
7
14.8
14.4
56.9

PC4
0.066
0.117
-0.127
-0.139
0.000
0.000
0.000
-0.589
0.000
-0.067
0.000
0.771
0.013
8
9.4
7.1
64.0

PC5
-0.046
-0.081
0.009
0.000
-0.614
0.000
0.000
0.000
0.000
0.189
-0.659
0.040
-0.379
8
7.1
6.7
70.7

PC6
0.000
0.000
0.017
0.000
-0.562
-0.045
0.000
0.000
0.065
-0.065
0.725
0.003
-0.384
8
7.9
7.5
78.2

PC1
0.546
0.568
0.000
0.000
0.000
0.000
0.279
0.132
0.376
0.376
0.000
0.000
0.000
6
27.2
27.2
27.2

18

Table 3: Pitprops data: loadings of the ﬂrst 6 sparse PCs by SPCA

PC4 PC5 PC6
0
0
0
0
0
0
0
0
0
0
-1
0
0
1
7.7
7.4
62.7

0
0
0
0
0
0
0
0
0
0
0
-1
0
1
7.7
6.8
69.5

0
0
0
0
0
0
0
0
0
0
0
0
1
1
7.7
6.2
75.8

Variable
topdiam
length
moist
testsg
ovensg
ringtop
ringbut
bowmax
bowdist
whorls
clear
knots
diaknot
Number of nonzero loadings
Variance (%)
Adjusted Variance (%)
Cumulative Adjusted Variance (%)

PC1
-0.477
-0.476
0.000
0.000
0.177
0.000
-0.250
-0.344
-0.416
-0.400
0.000
0.000
0.000
7
28.0
28.0
28.0

PC2
0.000
0.000
0.785
0.620
0.000
0.000
0.000
-0.021
0.000
0.000
0.000
0.013
0.000
4
14.4
14.0
42.0

PC3
0.000
0.000
0.000
0.000
0.640
0.589
0.492
0.000
0.000
0.000
0.000
0.000
-0.015
4
15.0
13.3
55.3

5.2 A simulation example

We ﬂrst created three hidden factors

V1 » N (0; 290);

V2 » N (0; 300)

V3 = ¡0:3V1 + 0:925V2 + †;

† » N (0; 1)

V1; V2 and †

are independent:

Then 10 observed variables were generated as the follows

Xi = V1 + †1
i ;

Xi = V2 + †2
i ;

Xi = V3 + †3
i ;

†1
i » N (0; 1);

†2
i » N (0; 1);

†3
i » N (0; 1);

i = 1; 2; 3; 4;

i = 5; 6; 7; 8;

i = 9; 10;

f†j
ig are independent;

j = 1; 2; 3

i = 1;¢¢¢ ; 10:

19

Table 4: Pitprops data: loadings of the ﬂrst 6 modiﬂed PCs by simple thresholding
PC6
0.120
0.164
-0.277
0.000
0.629
0.000
0.000
0.000
0.000
-0.174
0.176
-0.171
0.629
8
6.3
6.2
80.8
PC6
0
0
0
0
0
0
0
0
0
0
0
0
1
1
7.7
3.6
71.9

Variable
topdiam
length
moist
testsg
ovensg
ringtop
ringbut
bowmax
bowdist
whorls
clear
knots
diaknot
Number of nonzero loadings
Variance (%)
Adjusted Variance (%)
Cumulative Adjusted Variance (%)
Variable
topdiam
length
moist
testsg
ovensg
ringtop
ringbut
bowmax
bowdist
whorls
clear
knots
diaknot
Number of nonzero loadings
Variance (%)
Adjusted Variance (%)
Cumulative Adjusted Variance (%)

PC2
0.234
0.000
0.582
0.490
0.000
0.000
0.000
0.000
0.000
-0.267
0.221
0.369
0.332
7
16.6
16.5
45.4
PC2
0.000
0.000
0.640
0.540
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.406
0.365
4
14.8
14.7
45.4

PC5
0.000
0.000
0.361
0.367
0.182
-0.326
-0.222
0.191
0.000
0.000
-0.354
-0.620
0.000
8
6.9
6.7
74.6
PC5
0
0
0
0
0
0
0
0
0
0
0
-1
0
1
7.7
5.2
68.3

PC3
0.000
-0.253
0.000
0.379
0.517
0.511
0.272
-0.261
0.000
0.000
0.000
0.000
-0.350
7
14.2
14.0
59.4
PC3
0.000
0.000
0.000
0.425
0.580
0.573
0.000
0.000
0.000
0.000
0.000
0.000
-0.393
4
13.6
11.1
56.5

PC4
0.092
0.104
0.000
0.000
0.000
0.000
0.000
-0.288
-0.098
0.207
-0.812
0.304
0.306
8
8.6
8.5
67.9
PC4
0
0
0
0
0
0
0
0
0
0
-1
0
0
1
7.7
7.6
64.1

PC1
-0.439
-0.441
0.000
0.000
0.000
0.000
-0.435
-0.319
-0.388
-0.412
0.000
0.000
0.000
6
28.9
28.9
28.9
PC1
-0.420
-0.422
0.000
0.000
0.000
-0.296
-0.416
-0.305
-0.370
-0.394
0.000
0.000
0.000
7
30.7
30.7
30.7

20

To avoid the simulation randomness, we used the exact covariance matrix of (X1; : : : ; X10) to

perform PCA, SPCA and simple thresholding. In other words, we compared their performances

using an inﬂnity amount of data generated from the above model.

The variance of the three underlying factors is 290, 300 and 283.8, respectively. The numbers of

variables associated with the three factors are 4, 4 and 2. Therefore V2 and V1 are almost equally

important, and they are much more important than V3. The ﬂrst two PCs together explain 99:6%

of the total variance. These facts suggest that we only need to consider two derived variables with

‘right’ sparse representations. Ideally, the ﬂrst derived variable should recover the factor V2 only

using (X5; X6; X7; X8), and the second derived variable should recover the factor V1 only using

(X1; X2; X3; X4). In fact, if we sequentially maximize the variance of the ﬂrst two derived variables

under the orthonormal constraint, while restricting the numbers of nonzero loadings to four, then

the ﬂrst derived variable uniformly assigns nonzero loadings on (X5; X6; X7; X8); and the second

derived variable uniformly assigns nonzero loadings on (X1; X2; X3; X4).

Both SPCA (‚ = 0) and simple thresholding were carried out by using the oracle information

that the ideal sparse representations use only four variables. Table 5 summarizes the comparison

results. Clearly, SPCA correctly identiﬂes the sets of important variables. As a matter of fact,

SPCA delivers the ideal sparse representations of the ﬂrst two principal components. Mathemat-

ically, it is easy to show that if t = 2 is used, SCoTLASS is also able to ﬂnd the same sparse

solution. In this example, both SPCA and SCoTLASS produce the ideal sparse PCs, which may

be explained by the fact that both methods explicitly use the lasso penalty.

In contrast, simple thresholding wrongly includes X9; X10 in the most important variables. The

explained variance by simple thresholding is also lower than that by SPCA, although the relative

diﬁerence is small (less than 5%). Due to the high correlation between V2 and V3, variables X9; X10

gain loadings which are even higher than that of the true important varaibles (X5; X6; X7; X8). Thus

21

Table 5: Results of the simulation example: loadings and variance

PCA
PC1
0.116
0.116
0.116
0.116
-0.395
-0.395
-0.395
-0.395
-0.401
-0.401

PC2
-0.478
-0.478
-0.478
-0.478
-0.145
-0.145
-0.145
-0.145
0.010
0.010

PC3
-0.087
-0.087
-0.087
-0.087
0.270
0.270
0.270
0.270
-0.582
-0.582

X1
X2
X3
X4
X5
X6
X7
X8
X9
X10
Adjusted
Variance (%)

SPCA (‚ = 0) Simple Thresholding
PC1
0.0
0.0
0.0
0.0
0.5
0.5
0.5
0.5
0.0
0.0

PC1
0.000
0.000
0.000
0.000
0.000
0.000
-0.497
-0.497
-0.503
-0.503

PC2
-0.5
-0.5
-0.5
-0.5
0.0
0.0
0.0
0.0
0.0
0.0

PC2
0.5
0.5
0.5
0.5
0.0
0.0
0.0
0.0
0.0
0.0

60.0

39.6

0.08

40.9

39.5

38.8

38.6

the truth is disguised by the high correlation. On the other hand, simple thresholding correctly

discovers the second factor, because V1 has a low correlation with V3.

5.3 Ramaswamy data

Ramaswamy data (Ramaswamy et al. 2001) has 16063 (p = 16063) genes and 144 (n = 144) samples.

Its ﬂrst principal component explains 46% of the total variance. In a typical microarray data like

this, it appears that SCoTLASS cannot be practically useful. We applied SPCA (‚ = 1) to ﬂnd
the sparse leading PC. A sequence of ‚1 were used such that the number of nonzero loadings varied

in a rather wide range. As displayed in Figure 3, the percentage of explained variance decreases at

a slow rate, as the sparsity increase. As few as 2.5% of these 16063 genes can su–ciently construct

the leading principal component with little loss of explained variance (from 46% to 40%). Simple

thresholding was also applied to this data. It seems that when using the same number of genes,

simple thresholding always explains slightly higher variance than SPCA does. Among the same

number of selected genes by SPCA and simple thresholding, there are about 2% diﬁerent genes,

and this diﬁerence rate is quite consistent.

22

6
4
0

.

5
4
0

.

4
4
0

.

3
4
0

.

2
4
0

.

1
4
0

.

0
4
0

.

7
3
.
0

V
E
P

Ramaswamy data

SPCA
simple thresholding

200

1600

4000

8000

16063

number of nonzero loadings

Figure 3: The sparse leading principal component: percentage of explained variance versus sparsity.
Simple thresholding and SPCA have similar performances. However, there still exists consistent
diﬁerence in the selected genes (the ones with nonzero loadings).

6 Discussion

It has been a long standing interest to have a formal approach to derive principal components with

sparse loadings. From a practical point of view, a good method to achieve the sparseness goal

should (at least) possess the following properties.

† Without any sparsity constraint, the method should reduce to PCA.

† It should be computationally e–cient for both small p and big p data.

† It should avoid mis-identifying the important variables.

The frequently used simple thresholding is not criterion based. However, this informal ad hoc

method seems to have the ﬂrst two of the good properties listed above. If the explained variance

and sparsity are the only concerns, simple thresholding is not such a bad choice, and it is extremely

convenient. We have shown that simple thresholding can work pretty well in gene expression

23

arrays. The serious problem with simple thresholding is that it can mis-identify the real important

variables. Nevertheless, simple thresholding is regarded as a benchmark for any potentially better

method.

Using the lasso constraint in PCA, SCoTLASS successfully derives sparse loadings. However,

SCoTLASS is not computationally e–cient, and it lacks a good rule to pick its tunning parameter.

In addition, it is not feasible to apply SCoTLASS to gene expression arrays, while in which PCA

is a quite popular tool.

In this work we have developed SPCA using the SPCA criterion. The new SPCA criterion

gives exact PCA results when its sparsity (lasso) penalty term vanishes. SPCA allows a quite

(cid:176)exible control on the sparse structure of the resulting loadings. Uniﬂed e–cient algorithms have

been proposed to realize SPCA for both regular multivariate data and gene expression arrays.

As a principled procedure, SPCA enjoys advantages in several aspects, including computational

e–ciency, high explained variance and ability of identifying important variables.

7 Appendix: proofs

Theorem 1 proof: Using XT X = VD2VT and VT V = I, we have

^ﬂridge = ¡XT X + ‚I¢¡1

= V(cid:181) D2

D2 + ‚I¶ VT Vi

XT (XVi)

= Vi

D2
i
i + ‚

D2

:

(20)

⁄

24

Theorem 2 proof: Note that

nXi=1ﬂﬂXi ¡ ﬁﬂT Xiﬂﬂ2

= Pn
= Pn

=

i=1 TrXT

i (I ¡ ﬂﬁT )(I ¡ ﬁﬂT )Xi
i=1 Tr(I ¡ ﬂﬁT )(I ¡ ﬁﬂT )XiXT
Tr(I ¡ ﬂﬁT )(I ¡ ﬁﬂT )(Pn
i=1 XiXT
i )

i

= Tr(I ¡ ﬂﬁT ¡ ﬁﬂT + ﬂﬁT ﬁﬂT )XT X
= TrXT X + TrﬂT XT Xﬂ ¡ 2TrﬁT XT Xﬂ:

Since ﬁT XT Xﬂ and ﬂT XT Xﬂ are both scalars, we get

i=1ﬂﬂXi ¡ ﬁﬂT Xiﬂﬂ2
Pn

+ ‚jﬂj2

= TrXT X ¡ 2ﬁT XT Xﬂ + ﬂT (XT X + ‚)ﬂ:

For a ﬂxed ﬁ, the above quantity is minimized at

ﬂ =¡XT X + ‚¢¡1

XT Xﬁ:

Substituting (23) into (22) gives

i=1ﬂﬂXi ¡ ﬁﬂT Xiﬂﬂ2
Pn

+ ‚jﬂj2

= TrXT X ¡ 2ﬁT XT X(XT X + ‚)¡1XT Xﬁ:

Therefore

^ﬁ = arg max

ﬁ

ﬁT XT X(XT X + ‚)¡1XT Xﬁ

25

(21)

(22)

(23)

(24)

(25)

subject to ﬁT ﬁ = 1:

XT X^ﬁ.

And ^ﬂ =¡XT X + ‚¢¡1

By X = UDVT , we have

XT X(XT X + ‚)¡1XT X = V

D4

D2 + ‚

VT :

Hence ^ﬁ = sV1 with s=1 or -1. Then ^ﬂ = s D2
1+‚ V1:
D2

1

(26)

⁄

Theorem 3 proof: By the same steps in the proof of theorem 2 we derive (22) as long as ﬁT ﬁ = Ik.

Hence we have

=

j=1 jﬂjj2

+ ‚Pk

i=1ﬂﬂXi ¡ ﬁﬂT Xiﬂﬂ2
Pn
TrXT X ¡ 2TrﬁT XT Xﬂ + TrﬂT (XT X + ‚)ﬂ
j XT Xﬂj·
j=1‡ﬂT
= TrXT X +Pk

j (XT X + ‚)ﬂj ¡ 2ﬁT

(27)

(28)

Thus given a ﬂxed ﬁ, the above quantity is minimized at ﬂj = ¡XT X + ‚¢¡1

j = 1; 2; : : : ; k; or equivalently

XT Xﬁj for

ﬂ =¡XT X + ‚¢¡1

XT Xﬁ:

Therefore

^ﬁ = arg max

ﬁ

TrﬁT XT X(XT X + ‚)¡1XT Xﬁ

subject to ﬁT ﬁ = Ik:

(29)

(30)

This is an eigen-analysis problem whose solution is ^ﬁj = sjVj with sj=1 or -1 for j = 1; 2; : : : ; k,

26

because the eigenvectors of XT X(XT X + ‚)¡1XT X are V. Hence (29) gives ^ﬂj = sj

j = 1; 2; : : : ; k.

D2
j
j +‚ Vj for
D2

⁄

Theorem 4 proof: By assumption ﬂ = U DV T with U T U = Ik and V V T = V T V = Ik. The

constraint ﬁT ﬁ = Ik is equivalent to k(k+1)

2

constraints

ﬁT

i ﬁi = 1;

i = 1; 2 : : : ; k

ﬁT

i ﬁj = 0;

j > i:

(31)

(32)

Using Lagrangian multipliers method, we deﬂne

L = ¡

kXi=1

ﬂT
i ﬁi +

1
2

kXi=1

‚i;i(ﬁT

i ﬁi ¡ 1) +

kXj>i

‚i;j(ﬁT

i ﬁj):

(33)

Setting @L
@ﬁi

= 0 gives ﬂi = ‚i;i ^ﬁi + ‚i;j ^ﬁj; or in a matrix form ﬂ = ^ﬁ⁄, where ⁄i;j = ‚j;i. Both ﬂ

and ﬁ are full rank, so ⁄ is invertible and ﬁ = ﬂ⁄¡1. We have

Tr^ﬁT ﬂ = Tr⁄¡1ﬂT ﬂ = Tr¡⁄¡1;T V D2V T¢ ;

Ik = ^ﬁT ^ﬁ = ⁄¡1;T ﬂT ﬂ⁄¡1 = ⁄¡1;T V D2V T ⁄¡1:

Let A = V T ⁄¡1V , observe

Tr¡⁄¡1V D2V T¢ = Tr¡V T ⁄¡1V D2¢ = TrAT D2 =

kXj=1

AjjD2
jj;

AT D2A = Ik:

27

(34)

(35)

(36)

(37)

Since A2

jjD2

jj • 1,

kXj=1

AjjD2

jj •

Djj:

kXj=1

(38)

The \=" is taken if only if A is diagonal and Ajj = D¡1

jj . Therefore ⁄¡1 = V AV T = V D¡1V T ,

and ^ﬁ = ﬂ⁄ = U DV T V D¡1V T = U V T :

Theorem 5 proof: Let ^ﬂ⁄ = (1 + ‚) ^ﬂ, then we observe ^Vi(‚) =

: On the other hand, ^ﬂ =

^ﬂ⁄i
j ^ﬂ⁄i j

means

(^ﬁ; ^ﬂ⁄) = arg min
ﬁ;ﬂ

nXi=1ﬂﬂﬂﬂXi ¡ ﬁ

ﬂT
1 + ‚

2

Xiﬂﬂﬂﬂ

+ ‚

2

+

kXj=1ﬂﬂﬂﬂ

ﬂj

1 + ‚ﬂﬂﬂﬂ

kXj=1

‚1;jﬂﬂﬂﬂ

ﬂj

1 + ‚ﬂﬂﬂﬂ1

subject to ﬁT ﬁ = Ik:

Then by (12), we have

2

i=1ﬂﬂﬂXi ¡ ﬁ ﬂT
1+‚ Xiﬂﬂﬂ
Pn
1+‚‡Pk
j=1‡ﬂT
j=1‡ﬂT
1+‚‡Pk

j

j

2

XT X+‚

j=1ﬂﬂﬂ ﬂj
j=1 ‚1;jﬂﬂﬂ ﬂj
1+‚ﬂﬂﬂ1
1+‚ﬂﬂﬂ
+Pk
+ ‚Pk
j XT Xﬂj + ‚1;j jﬂjj1··
1+‚ ﬂj ¡ 2ﬁT
1+‚ ﬂj + ‚1;j jﬂjj1· ¡ 2TrﬁT XT Xﬂ· :

XT X+‚

= TrXT X + 1

= TrXT X + 1

(^ﬁ; ^ﬂ⁄) = arg min

ﬁ;ﬂ ¡2TrﬁT XT Xﬂ +

subject to ﬁT ﬁ = Ik:

ﬂT
j

kXj=1

XT X + ‚

1 + ‚

ﬂj +

kXj=1

‚1;j jﬂjj1

As ‚ ! 1, (41) approaches (18). Thus the conclusion follows.

28

⁄

^ﬂ⁄
1+‚

(39)

(40)

(41)

⁄

References

Alter, O., Brown, P. & Botstein, D. (2000), ‘Singular value decomposition for genome-wide ex-

pression data processing and modeling’, Proceedings of the National Academy of Sciences

97, 10101{10106.

Cadima, J. & Jolliﬁe, I. (1995), ‘Loadings and correlations in the interpretation of principal com-

ponents’, Journal of Applied Statistics 22, 203{214.

Efron, B., Hastie, T., Johnstone, I. & Tibshirani, R. (2004), ‘Least angle regression’, The Annals

of Statistics 32, In press.

Hancock, P., Burton, A. & Bruce, V. (1996), ‘Face processing: human perception and principal

components analysis’, Memory and Cognition 24, 26{40.

Hastie, T., Tibshirani, R., Eisen, M., Brown, P., Ross, D., Scherf, U., Weinstein, J., Alizadeh, A.,

Staudt, L. & Botstein, D. (2000), ‘’gene shaving’ as a method for identifying distinct sets of

genes with similar expression patterns’, Genome Biology 1, 1{21.

Hastie, T., Tibshirani, R. & Friedman, J. (2001), The Elements of Statistical Learning; Data

mining, Inference and Prediction, Springer Verlag, New York.

Jeﬁers, J. (1967), ‘Two case studies in the application of principal component’, Applied Statistics

16, 225{236.

Jolliﬁe, I. (1986), Principal component analysis, Springer Verlag, New York.

Jolliﬁe, I. (1995), ‘Rotation of principal components: choice of normalization constraints’, Journal

of Applied Statistics 22, 29{35.

29

Jolliﬁe, I. T. & Uddin, M. (2003), ‘A modiﬂed principal component technique based on the lasso’,

Journal of Computational and Graphical Statistics 12, 531{547.

McCabe, G. (1984), ‘Principal variables’, Technometrics 26, 137{144.

Misra, J., Schmitt, W., Hwang, D., Hsiao, L., Gullans, S., Stephanopoulos, G. & Stephanopou-

los, G. (2002), ‘Interactive exploration of microarray gene expression patterns in a reduced

dimensional space’, Genome Research 12, 1112{1120.

Ramaswamy, S., Tamayo, P., Rifkin, R., Mukheriee, S., Yeang, C., Angelo, M., Ladd, C., Reich,

M., Latulippe, E., Mesirov, J., Poggio, T., Gerald, W., Loda, M., Lander, E. & Golub, T.

(2001), ‘Multiclass cancer diagnosis using tumor gene expression signature’, Proceedings of the

National Academy of Sciences 98, 15149{15154.

Tibshirani, R. (1996), ‘Regression shrinkage and selection via the lasso’, Journal of the Royal

Statistical Society, Series B 58, 267{288.

Tibshirani, R., Hastie, T., Narasimhan, B. & Chu, G. (2002), ‘Diagnosis of multiple cancer types by

shrunken centroids of gene’, Proceedings of the National Academy of Sciences 99, 6567{6572.

Vines, S. (2000), ‘Simple principal components’, Applied Statistics 49, 441{451.

Zou, H. & Hastie, T. (2003), Regression shrinkage and selection via the elastic net, with applications

to microarrays, Technical report, Department of Statistics, Stanford University. Available at

http://www-stat.stanford.edu/~hastie/pub.htm.

30

