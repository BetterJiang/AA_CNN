The Annals of Statistics
1998, Vol. 26, No. 2, 451–471

CLASSIFICATION BY PAIRWISE COUPLING

By Trevor Hastie1 and Robert Tibshirani2

Stanford University and University of Toronto

We discuss a strategy for polychotomous classiﬁcation that involves
estimating class probabilities for each pair of classes, and then coupling
the estimates together. The coupling model is similar to the Bradley–Terry
method for paired comparisons. We study the nature of the class proba-
bility estimates that arise, and examine the performance of the procedure
in real and simulated data sets. Classiﬁers used include linear discrim-
inants, nearest neighbors, adaptive nonlinear methods and the support
vector machine.

1. Introduction. We consider the discrimination problem with K classes
and N training observations. The training observations consist of predictor
measurements x = (cid:2)x1(cid:4) x2(cid:4) (cid:5) (cid:5) (cid:5) (cid:4) xp(cid:3) on p predictors and the known class mem-
berships. Our goal is to predict the class membership of an observation with
predictor vector x0.
Typically, K-class classiﬁcation rules tend to be easier to learn for K = 2
than for K > 2—only one decision boundary requires attention. Friedman
(1996a) suggested the following approach for the K-class problem: solve each
of the two-class problems and then, for a test observation, combine all the
pairwise decisions to form a K-class decision. Friedman’s combination rule is
quite intuitive: assign to the class that wins the most pairwise comparisons.
Friedman points out that this rule is equivalent to the Bayes rule when the

class posterior probabilities pi (at the test point) are known:

argmaxi(cid:4)pi(cid:5) = argmaxi

(cid:1)(cid:2)

I

j(cid:6)=i

(cid:3)

pi/(cid:2)pi + pj(cid:3) > pj/(cid:2)pi + pj(cid:3)(cid:4)(cid:5)

(cid:5)

We call Friedman’s procedure the “max–wins” rule. Note that Friedman’s rule
requires only an estimate of each pairwise decision. Many (pairwise) classiﬁers
provide not only a rule, but estimated class probabilities as well. In this paper,
we argue that one can improve on Friedman’s procedure by combining the
pairwise class probability estimates into a joint probability estimate for all K
classes.

This leads us to consider the following problem. Given a set of mutually
exclusive events A1(cid:4) A2(cid:4) (cid:5) (cid:5) (cid:5) AK, some experts give us pairwise probabilities
rij = Prob(cid:2)Ai(cid:8)Ai or Aj(cid:3). Is there a set of probabilities pi = Prob(cid:2)Ai(cid:3) that are
compatible with the rij?

Received November 1996; revised September 1997.
1Supported in part by NSF Grant DMS-95-04495 and NIH Grant ROI-CA-72028-01.
2Supported by the Natural Sciences and Engineering Research Council of Canada and the

IRIS Centre of Excellence.

AMS 1991 subject classiﬁcations. Primary 62H30, 68T10; secondary 62J15.
Key words and phrases. Pairwise, Bradley–Terry model.

451

452

T. HASTIE AND R. TIBSHIRANI

In general, a solution satisfying these constraints may not exist. Since
Prob(cid:2)Ai(cid:8)Ai or Aj(cid:3) = pj/(cid:2)pi + pj(cid:3) and
pi = 1, we are requiring that K − 1
free parameters satisfy K(cid:2)K − 1(cid:3)/2 constraints, and this will not have a so-
lution in general. For example, if the rij are the ijth entries in the matrix

(cid:6)

(1.1)




·
0(cid:5)9 0(cid:5)4
·
0(cid:5)7
0(cid:5)1
·
0(cid:5)6 0(cid:5)3


(cid:4)

then they are not compatible with any pi’s. This is clear since r12 > 0(cid:5)5 and
r23 > 0(cid:5)5, but also r31 > 0(cid:5)5.
The model Prob(cid:2)Ai(cid:8)Ai or Aj(cid:3) = pj/(cid:2)pi + pj(cid:3) forms the basis for the
Bradley–Terry model for paired comparisons [Bradley and Terry (1952)]. In
this paper, we ﬁt this model by maximizing a (negative) Kullback–Leibler
distance criterion to ﬁnd the best approximation ˆrij = ˆpi/(cid:2) ˆpi + ˆpj(cid:3) to a given
set of rij’s. We carry this out at each predictor value x, and use the estimated
probabilities to predict class membership at x.
In the example above, the solution is ˆp = (cid:2)0(cid:5)47(cid:4) 0(cid:5)25(cid:4) 0(cid:5)28(cid:3). This solution
makes qualitative sense since event A1 “beats” A2 by a larger margin than
the winner of any of the other pairwise matches.

Figure 1 shows an example of these procedures in action. There are 600
data points in three classes, each class generated from a mixture of Gaus-
sians. A linear discriminant model was ﬁt to each pair of classes, giving pair-
wise probability estimates rij at each x. The ﬁrst panel shows Friedman’s
procedure applied to the pairwise rules. The shaded regions are areas of in-
decision, where each class wins one vote. The coupling procedure described in
the next section was then applied, giving class probability estimates ˆp(cid:2)x(cid:3) at
each x. The decision boundaries resulting from these probabilities are shown
in the second panel. The procedure has done a reasonable job of resolving the

Pairwise LDA + Max (0.132)

Pairwise LDA + Coupling (0.136)

3-Class LDA (0.213)

2

2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

2

2

2
2
2
2

3

2

2
2
2
2
2
2
2
2
2
2
1
2
2
2
2
2
1
11
1
1
2
22
2
2
2
2
2
2
2
2
1
1
2
2
1
2
1
2
1
1
1
2
1
2
2
1
1
2
2
2
1
1
1
1
22
1
1
2
2
1
1
1
1
1
1
1
1
1
1
2
1
2
1
1
1
1
1
1
1
1
1
2
1
1
1
1
1
1
1
1
1
1
1
1
2
1
3
2
2
1
1
1
1
1
1
1
1
1
3
1
1
1
1
2
1
3
1
3
3
3
1
3
3
3
1
1
1
3
3
1
3
1
1
1
1
1
3
1
1
3
3
3
1
3
2
1
1
3
1
1
3
1
1
1
3
3
1
3
3
3
1
2
3
1
3
2
1
1
1 33
1
3
33
1
1
3
3
2
3
1
2
1
1
1
3
3
3
3
1
3
3
1
3
1
33
1
2
1
1 3
3
3
3
3
1
3
2
1
1
3
3
3
3
1
2
1
3
1
333
3
2
1
3
3
1
3
1
1
1
1
1
2
1
2
3
1
1
1
1
3
2
1
2
1
2
3
1
3
3
1
3
3
3
3
2
1
3
2
2
3
2
2
1
2
1
1
2
1
2
3
2
1
2
3
2
3
1
1 1
22
2
3
2
2
1
2
2
1 1
1
2
2
1
1
2
2
3
3
2
2
1
2
3
2
2
1
3
3
2
2
2
2
3
3
3
1
3
3
2
2
2
2
2
1
2
2
2
3
1
2
2
1
3
1
2
1
1
2
1
1
2
3
22
1
2
1
1
3
2
3
1
3
3
3
3
3
1
2
1
2
2 2
3
2
3
2
2
1
3
2
2
1
3
1
2
3
1
2
1
2
3
1
3
3
1
3
13
2
1
2
2
2
3
11
3
1
2
1
1
2
1
3
1
3
3
3
1
1
3
1
2
2
2
2
3
1
2
2
2
3
3
3
3
3
3
2
3
3
2
3
1
1
2
1
1
3
3
2
1
1
3
2
1
1
3
1
1
3
1
3
1
3
2
1
2
1
2
1
3
2
1
2
2
2
2
2
1
2
2
2

3

2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

2

2

3

2

2
2
2
2

2

3

2
2
2
2
2
2
2
2
2
2 2
1
2
2
2
2
2
1
1
1
1
2
2
2
2
2
2
2
1
2
2
1
2
1
2
2
1
2
1
1
2
11
2
1
1
22
2
1
1
22
2
2
1
1
1
1
1
2
2
1
1
2
1
1
1
1
1
1
1
1
1
2
1
1
1
1
1
1
1
1
1
2
1
1
1
1
1
1
1
1
1
1
1
2
1
3
1
2
2
1
1
1
1
1
1
1
1
1
1
3
1
1
1
2
1
3
1
3
3
3
1
3
3
1
1
3
3
1
3
1
1
1
3
11
1
3
1
1
3
3
3
1
3
2
1
1
1
3
1
3
1
11
1
3
1
3
3
3
1
3
2
3
1
3
2
1
3
1
1
1
3
3
1
3
2
3
1
2
1
1
11
3
3
3
3
3
3
1
3
3
1
3
1
3
2
31
1
3
1
3
3
3
3
2
3
1
1
3
3
3
3
3
2
1
1
3
1
3
3
2
1
1
3
3
3
31
1
1
1
1
2
3
2
1
1
1
1
1
3
2
2
1
1
2
3
1
1
3
3
3
3
3
3
3
2
1
3
2
2
22
2
3
1
2
1
1
2
1
3
2
1
2
13
2
3
1
22
1
2
3
2
2
2
1
1
2
1
2
2
1
2
1
2
3
2
3
2
1
2
3
1 1
2
2
3
1
2
2
3
2
2
3
3
1
3
3
3
2
2
2
2
2
1
2
2
2
3
2
3
1
2
1
1
2
1
1
1
2
22
2
3
1
1
2
1
3
3
2
1
3
3
3
3
3
2
1
1
2
2
3
2
2
3
2
2
2
1
3
2
1
3
2
3
1
1
2
1
2
3
3
3
1
1
1
1
3
2
2
1
1
3
3
2
2
2
1
1
1
2
3
1
1
3
3
3
1
3
1
3
1
2
2
2
2
1
3
2
2
2
3
3
3
3
3
3
3
2
3
3
2
1
1
2
1
1
3
12
3
11
2
3
1
1
3
1
1
3
3
1
2
3
1
2
1
2
1
3
2
1
2
2
2
2
2
1
2
2
2

3

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

3

3

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3 3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

3

3

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

2
2
2 2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

2

3

2
2
2
2
2
2
2
2
2
2
2
2
1
2
22
2
2
1
1
1
1
2
2
2
2
2
22
2
11
2
2
2
2
1
2
2
1
2
1
1
1
111
11
2
2
2
1
1
2
2
1
2
1
1
2
1
1
2
2
1
1
3
2
1
1
1
1
1
1
1
2
2
1
1
1
1
1
1
1
1
1
2
1
1
1
1
1
1
1
1
1
1
1
2
1
3
1
2
2
1
1
11
1
1
1
2
1
1
3
1
1
1
1
2
2
1
3
1
33
2
3
1
3
3
1
3
1
1
3
3
1
1
1
3
1
1
1
3
1
1
3
1
3
3
3
2
2
1
1
1
3
1
3
1
1
1
3
1
3
3
3
3
1
2
3
1
3
2
1
1
3
1
1
1
3
1
3
3
2
3 3
1
1
2
1
1
3
3
3
3
3
3
1
33
3
1
1
1
2
3
3
1
1
3
3
3
3
1 3
2
3
3
1
1
3
3
3
3
2
1
1
1
3
3
3
2
3
1
1
1
3
3
1
1
1
1
2
1
3
2
1
1
1
1
3
2
1
2
1
3
2
3
1
3
1
3
3
3
3
3
2
1
3
2
2
2
2
3
1
1
2
1
2
1
2
3
2
1
2
3
2
3
1
1 1
2
22
3
2
2
1
2
1
2
2
1
2
1
2
1
2
3
3
2
1
2
2
1
3
2
2
1
3
2
2
3
2
2
3
3
3
1
3
22
3
2
2
2
31
2
2
3
2
1
2
2
1
3
2
2
1
1
1
1
1
2
2
3
1
1
1
2
2
1
3
3
3
3
33
3
2
1
1
2
2
2
3
2
2
3
2
2
2
1
3
2
3
1
31
2
1
2
1
1
2
3
3
1
1
2
2
33
1
1
2
3
2
1
3
1
2
1
1
12
3
1
3
3
3
1
1
1
3
3
2
2
2
2
1
3
2
2
2
3
3
3
2
3
3
33
3
2
3
1
1
1
2
1
3
3
2
1
1
2
1
1
3
1
1
3
1
3
1
3
2
1
2
12
2
1
3
2
1
2
2
2
2
3
1
2
2
2

3

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

3

3

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3

Fig. 1. A three-class problem, with the data in each class generated from a mixture of Gaussians.
The ﬁrst panel shows the maximum-wins procedure. The second panel shows the decision boundary
from coupling of the pairwise linear discriminant rules based on ˆd in (2.6). The third panel shows
the three-class LDA boundaries. Test-error rates are shown in parentheses.

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
CLASSIFICATION BY PAIRWISE COUPLING

453

confusion, in this case producing decision boundaries similar to the three-class
LDA boundaries shown in panel 3. The numbers in parentheses above the plots
are test-error rates based on a large test sample from the same population.
Notice that despite the indeterminacy, the max–wins procedure performs no
worse than the coupling procedure, and both perform better than LDA. Later,
we show an example where the coupling procedure does substantially better
than max–wins.

Often the pairwise approach yields a more ﬂexible class of models than a
K-class method. For example, a standard linear discriminant analysis (LDA)
assumes that all classes have the same covariance. In the pairwise application
of LDA, this assumption is used only for each pair of classes.

This paper is organized as follows. The coupling model and algorithm are
given in Section 2. Section 3 discusses the properties of the coupling solution,
and its relation to the max–wins rule. Pairwise threshold optimization, a key
advantage of the pairwise approach, is discussed in Section 4. Section 5 ex-
amines the performance of the various methods on some real and simulated
problems. In Section 6, we apply coupling to an adaptive, nonlinear classi-
ﬁer based on additive modelling. Application to the support-vector machine
is described in Section 7, while Sections 8 and 9 look at coupling applied to
nearest-neighbor rules. The ﬁnal section contains some discussion.

2. Coupling the probabilities. Let the class probabilities at feature vec-
tor x be p(cid:2)x(cid:3) = (cid:2)p1(cid:2)x(cid:3)(cid:4) (cid:5) (cid:5) (cid:5) (cid:4) pK(cid:2)x(cid:3)(cid:3). In this section, we drop the argument x,
since the calculations are done at each x separately.
We assume that, for each i (cid:6)= j, there are nij observations in the train-
ing set, and from these we have estimated conditional probabilities rij =
Prob(cid:2)i(cid:8)i or j(cid:3).
Our model is

(2.2)

or equivalently,

(2.3)

µij = E(cid:2)rij(cid:3) =

pi

pi + pj

(cid:4)

log µij = log(cid:2)pi(cid:3) − log(cid:2)pi + pj(cid:3)(cid:4)

a log-nonlinear model.
We wish to ﬁnd ˆpi’s so that the ˆµij’s are close to the rij’s. There are K − 1
independent parameters but K(cid:2)K − 1(cid:3)/2 equations, so it is not possible in
general to ﬁnd ˆpi’s so that ˆµij = rij for all i(cid:4) j.
Therefore, we must settle for ˆµij’s that are close to the observed rij’s. Our
closeness criterion is the average (negative, weighted) Kullback–Leibler dis-
tance between rij and µij:

(2.4)

and we ﬁnd p to maximize this function.

(cid:1)

(cid:18)(cid:2)p(cid:3) = (cid:2)

i<j

nij

rij log

+ (cid:2)1 − rij(cid:3) log

rij
µij

(cid:5)

(cid:4)

1 − rij
1 − µij

454

T. HASTIE AND R. TIBSHIRANI

This model and criterion is formally equivalent to the Bradley–Terry model
for preference data. One observes a proportion rij of nij preferences for item
i, and the sampling model is binomial:

nijrij ∼ Bin(cid:2)nij(cid:4) µij(cid:3)(cid:5)

If each of the rij were independent, then (cid:18)(cid:2)p(cid:3) would be equivalent to the log-
likelihood under this model. However, our rij are not independent, as they
share a common training set and were obtained from a common set of clas-
siﬁers. Furthermore, the binomial models do not apply in this case; the rij
are evaluations of functions at a point, and the randomness arises in the way
these functions are constructed from the training data. We include the nij as
weights in (2.4); this is a crude way of accounting for the different precisions
in the pairwise probability estimates.
The score (gradient) equations are

(cid:2)
j(cid:6)=i

nijµij = (cid:2)

j(cid:6)=i

nijrij(cid:4)

i = 1(cid:4) 2(cid:4) (cid:5) (cid:5) (cid:5) (cid:4) K(cid:4)

pi = 1. We use the following iterative procedure to compute the

(2.5)

subject to
ˆpi’s:

(cid:6)

Algorithm.

1. Start with some guess for the ˆpi, and corresponding ˆµij.
2. Repeat (i = 1(cid:4) 2(cid:4) (cid:5) (cid:5) (cid:5) (cid:4) K(cid:4) 1(cid:4) (cid:5) (cid:5) (cid:5)) until convergence:

(cid:6)
(cid:6)
j(cid:6)=i nijrij
j(cid:6)=i nij ˆµij

(cid:4)

ˆpi ← ˆpi

renormalize the ˆpi, and recompute the ˆµij.

3. ˆp ← ˆp/

(cid:6) ˆpi.

The algorithm also appears in Bradley and Terry (1952). The updates in step
2 attempt to modify p so that the sufﬁcient statistics match their expectation,
but go only part of the way. We prove in the Appendix that (cid:18)(cid:2)p(cid:3) increases
at each step. Since (cid:18)(cid:2)p(cid:3) is bounded above by zero, the procedure converges.
At convergence, the score equations are satisﬁed, and the ˆµij’s and ˆp are
consistent. This algorithm is similar in ﬂavor to the Iterative Proportional
Scaling (IPS) procedure used in log-linear models. IPS has a long history,
dating back to Deming and Stephan (1940). Bishop, Fienberg and Holland
(1975) give a modern treatment and many references.

The resulting classiﬁcation rule is

(2.6)

ˆd(cid:2)x(cid:3) = argmaxi(cid:4) ˆpi(cid:2)x(cid:3)(cid:5)(cid:5)

3. Properties of the solution. The weights nij in (2.4) can improve the
efﬁciency of the estimates a little, but do not have much effect unless the class
sizes are very different. For simplicity, and to facilitate comparison with other
techniques, in this section we assume equal weighting (nij = 1 for all i(cid:4) j).

CLASSIFICATION BY PAIRWISE COUPLING

455

In the examples later in the paper, we experimented with more sophisticated
weights such as nij/(cid:2)µij(cid:2)1 − µij(cid:3)(cid:3), but these made very little difference in
practice.

A simple noniterative estimate can be obtained from the row averages

(3.7)

˜pi = 2

K

(cid:6)
j(cid:6)=i rij
(cid:2)K − 1(cid:3) (cid:5)

These estimates can be derived as an approximation to the identity

(3.8)

(cid:13)

pi = (cid:2)

j(cid:6)=i

pi + pj
K − 1

(cid:14)(cid:13)

pi

pi + pj

(cid:14)

by replacing pi + pj in the ﬁrst ratio by 2/K, and each of the second ratios
by their corresponding rij. We use these estimates as starting values in the
maximum likelihood procedure. In fact, the ˜pi’s are in the same order as the
ˆpi’s, and hence are sufﬁcient if only the classiﬁcation rule is required.

Theorem 1.
Proof. The ˆpi satisfy

k(cid:6)=i rik. Now

ˆpi > ˆpj.

˜pi > ˜pj if and only if
(cid:6)
k(cid:6)=i ˆµik = (cid:6)
˜pi > ˜pj ⇔ (cid:2)
rik >
⇔ (cid:2)
ˆµik >
⇔ ˆpi > ˆpj(cid:4)

k(cid:6)=i

k(cid:6)=i

(cid:2)
(cid:2)
k(cid:6)=j
k(cid:6)=j

rjk
ˆµjk

since p/(cid:2)p + q(cid:3) is an increasing function of p for q > 0. Similarly, one can
show that ˜pi = ˜pj if and only if

ˆpi = ˆpj. ✷

Looking at this more closely, we ﬁnd that the approximate solution ˜p =
(cid:2) ˜p1(cid:4) (cid:5) (cid:5) (cid:5) (cid:4) ˜pK(cid:3) tends to underestimate differences between the ˆpi’s. Speciﬁ-
cally, the following result shows that ˜p is closer to the equiprobability vector
(cid:2)1/K(cid:4) 1/K(cid:4) (cid:5) (cid:5) (cid:5) (cid:4) 1/K(cid:3) in Kullback–Leibler distance than is ˆp.

Theorem 2. (cid:2)

(cid:2)1/K(cid:3) log(cid:4)1/K ˜pi(cid:5) ≤ (cid:2)

(cid:2)1/K(cid:3) log(cid:4)1/K ˆpi(cid:5)(cid:5)

i

i

The proof is given in the Appendix. Given the equivalence of coupling with
the Bradley–Terry model, both of these results may already be known in the
literature on paired comparisons.
We now take a closer look at Friedman’s rule of assigning to the class that
wins the most pairwise comparisons with the other classes. Let Iij = 1 if

T. HASTIE AND R. TIBSHIRANI

456
rij ≥ 0(cid:5)5 and 0 otherwise. Then we deﬁne
(cid:6)
˜pi = 2
j(cid:6)=i Iij
(cid:2)K − 1(cid:3) (cid:4)
˜d = argmaxi(cid:4) ˜pi(cid:5)(cid:5)

(3.9)

K

In general, however, some surprising things can occur. Here is a situation

Theorem 1 tells us that if we start with the Iij’s rather than the rij’s, then
the rules ˆd and ˜d assign to the same class.
A second scenario in which they agree is the case where the model rij =
pi/(cid:2)pi + pj(cid:3) holds exactly for all i(cid:4) j for some pi. For then ˆpi = pi, and
both procedures classify to the largest pi, whether or not these are the correct
probabilities.
where r1j > 1/2 for all j (cid:6)= 1, but ˆp1 is not largest:
·
0(cid:5)56 0(cid:5)51 0(cid:5)60
·
0(cid:5)96 0(cid:5)44
0(cid:5)44
·
0(cid:5)59
0(cid:5)49 0(cid:5)04
·
0(cid:5)40 0(cid:5)56 0(cid:5)41


(cid:5)

(cid:18)rij(cid:19) =

(3.10)

The solution is ˆp = c(cid:2)0(cid:5)29(cid:4) 0(cid:5)34(cid:4) 0(cid:5)16(cid:4) 0(cid:5)21(cid:3) and

(3.11)

(cid:18) ˆµij(cid:19) =

(3.12)

(cid:18)rij(cid:19) =

·
0(cid:5)46 0(cid:5)64 0(cid:5)57
·
0(cid:5)67 0(cid:5)62
0(cid:5)54
·
0(cid:5)44
0(cid:5)36 0(cid:5)33
·
0(cid:5)43 0(cid:5)38 0(cid:5)56

·
0(cid:5)51 0(cid:5)53 0(cid:5)51
·
0(cid:5)54 0(cid:5)55
0(cid:5)49
·
0(cid:5)59
0(cid:5)47 0(cid:5)46
·
0(cid:5)49 0(cid:5)45 0(cid:5)41


(cid:5)


(cid:4)










Here is an example where the classes have an ordering i > j > k > (cid:18) in the
sense that rij > 0(cid:5)5 for all i(cid:4) j with i < j,

but the solution ˆp = (cid:2)0(cid:5)262(cid:4) 0(cid:5)270(cid:4) 0(cid:5)254(cid:4) 0(cid:5)214(cid:3) does not respect this ordering.
Figure 2 shows another example similar to Figure 1, where we can com-
pare the performance of the rules ˆd and ˜d. The hatched area in the top left
panel, is an indeterminate region where there is more than one class achiev-
ing max(cid:2) ˜pi(cid:3). In the top right panel, the coupling procedure has resolved this
indeterminacy in favor of class 1 by weighing the various probabilities.
There is another interesting phenomenon occurring here—the coupling has
reversed a decision made by the max–wins rule. Notice that in the top left
panel, the region to the left of the upper shaded wedge is a class-3 region,

CLASSIFICATION BY PAIRWISE COUPLING

457

Pairwise LDA + Max (0.449)

Pairwise LDA + Coupling (0.358)

1

1

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

3

3

1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1
1
1

1

1
1
3
1
3
3
1

1

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
1
1
1
3
3
1
3
1
1
1
1
1
1
1
1

1
1
1
1
1
1
1
1
11
1
1
1
3
1
1
1
1
1
1
1
1
1
1
1
1
1 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
1
3
1
1
1
2
1
1
1
1
1
1
2
1
2
2
1
2
2
2
2
1
2
2
2
2
3
2
2
2
2
2
2
2
2
2
2
2
2
2
2
1
3
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
3
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
3
2
1
2
2
2
2
3
2
2
2
3
3
2
3
2
2
2
2
3
2
3
3
2
2 2
2
1
2
2
1 1
1
2
1
2
2
2
1
2
3
2
1
2
2
2
2
2
2
2
3
2
2
3
3
2
2
2
2
2
1
2
3
2
3
2
3
1
2
2
1
2
2
1
1
2
2
2
2
2
2
2
3
2
2
2
3
2
1
3
3
3
3
1
2
3
1
1
3
1
2
3
3
2
2
2
3
3
3
2
2
3
2
2
3
2
3
2
2
2
3
2
1
3
21
3
3
2
2
2
1
2
3
1
2
2
3
3
3
2
2
3
3
2
3
3
2
3
2
3
2
1
1
2
2
2
3
2
1
2
2
3
2
2
11
2
3
2
3
2
1
1
3
3
3
2
2
1
3
3
3
1
3
2
2
2
3
2
3
2
3
3
2
2
3
1
2
3
3
3
1
2
2
3
1
2
1
3
2
1
2
2
3
3
2
3
3
2
2
2
3
3
3
1
3
2
3 1
1
3
2
2
3
3
2
1
3
3
33
2
3
1
2
1
3
3
3
2
3
3
2
1
3
1
2
3
3
1
3
2
1
3
2
3
3
1
3
1
3
1
2
2 3
3
3
3
2
3
2
3
3
3
1
1
3
1
3
3
2
2
1
3
3
2
3
1
1
2
2
2
13
3
2
3
2
1
1
1
3

3
3

1
3

3

3

2
3

3
3
3
3
3
3
33
3
3
3
3
3
3
3
3

3

3

1
1
1
3
3
3
1

1

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
1
1
1
3
3
1
3
1
1
1
1
1
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
1
3
1
1
1
2
1
1
1
1
1
1
2
1
2
2
1
2 2
2
2
2
1
2
2
2
3
2
2
2
2
2
2
2
2
2
2
2
2
2
2
1
3
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
3
2
2
2
2
2
3 2
2
2
2
2
2
2
2
2
3
2
2
2
2
2
1
3
2
2
2
2
2
3
3
2
3
2
2
2
2
3
2
3
3
2
2
2
1
2
2
1
1
1
1
2
2
2
1
2
2
3
2
1
2
2
2
2
2
2
2
2
3
2
2
3
2
3
2
2
2
2
2
1
3
2 2
3
2
3
1
2
1
2
2
1
1
2
2
2
2
2
2
2
3
2
2
2
3
2
1
3
3
3
3
1
2
3
1
1
3
2
1
2
2
3
3
3
3
2
3
2
2
3
2
2
3
2
3
2
2
2
3
2
3
1
1
3
3
2
1
2
2
2
3
1
2
2
3
3
3
2
2
2
3
3
2
3
2
3
3
2
3
2
1
2
1
2
2
3
2
2
1
1 2
3
2
2
1
3
2
2
3
2
1
3
3
3
1
2
2
3
1
3
3
1
3
2
2
2
3
2
3
3
2
2
3
2
3
2
1
3
3
3
1
2
3
2
1
1
2
3
2
2
1
2
3
3
3
2
2
3
2
3
2
3
1
3
3
2
1
1
3
2
2
3
3
2
1
3
3
3
2
3
3
1
2
1
3
3
3
2
3
3
3
1
3
2
3
1
2
3
1
3
2
1
3
3
2
3
1
3
1
3
1
3
2
3
3
3
3
2
2
3
3
3
1
2
1
3
1
3
3
1
2
2
3
3
2
3
1
1
2
2
2
1
3
2
3
2
1
1
3
1
3

3
3

1
3

3

2
3

2

1
1
1

1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1

12 13

1

32

LDA (0.457)

1

1

QDA (0.334)

1

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

3

3

1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1
1
1

1

1
1
3
1
3
3
1

1

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
1
1
1
3
3
1
3
1
1
1
1
1
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
3
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
1
3
1
1
1
2
1
1
1
1
1
1
2
1
2
2 2
1
2
2
2
2
2
1
2
2
3
2
2
2
2
2
2
2
2
2
2
2
2
2
2
1
3
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
3
2
2
2
2
2
2
2
2
2
2
2
2
2
2
3
2
2
2
1
2
3
2
2
2
3
2
3
2
3
2
2
2
2
2
3
3
3
2
2
2
1
2
2
1
1
1
1
2
2
2
1
2
2
3
2
1
2
2
2
2
2
2
2
3
2
2
2
3
3
2
2
2
2
2
1
2
2
3
3
2
3
1
2
2
1
2
2
1
1
2
2
2
2
2
2
2
2
3
2
2
3
2
1
3
3
3
1
3
2
3
1
1
2
1
3
3
3
2
2
2
3
3
3
2
2
3
2
2
3
2
3
2
2
2
3
2
3
1
1
3
3
3 2
2
2
1
2
3
1
2
2
3
3
3
2
2
2
2
3
3
3
2
3
2
2
3
2
1
1
2
2
3
2
2
2
1
2
3
2
1
3
2
2
1
2
3
1
3
3
3
2
1
2
3
1
3
3
1
3
2
2
2
3
3
2
3
2
2
3
2
3
31
2
1
3
3
1
3
2
2
1
3
1
2
2
2
1
2
3
3
3
2
3
2
2 2
3
3
3
3
2
1
1
3
22
3
2
3
1
2
3
3
2
3
3
1
3
2
3
1
3
3
2
3
3
3
1
3
3
2
1
3
1
3
1
2
3
3
2
3
1
3
1
3
1
3
2
3
3
3
2
3
2
3
3
3
1
2
1
3
1
3
3
2
2
1
3
3
2
3
1
1
2
2
2
1
3
2
3
2
1
1
3
1
3

3
3

1
3

3

3

2
3

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

3

3

1

1
1
3
3
1
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3 1
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
1
1
1
3
3
1
3
1
1
1
1
1
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
1
3
1
1
1
2
1
1
1
1
1
2
1
1
2
2
1
2
2
2
2
2
1
2
2
2
3
2
2
2
22
2
2
2
2
2
2
2
2
2
1
3
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
3
2
2
2
2
2
2
2
2
2
2
2
2
2
3
2
2
2
2
1
2
2
3
2
2
2
2
3
3
2
2
3
2
2
2
2
3
3
3
2
2
2
1
2
2
1
1
1
2
2
1
2
1
2
2
3
1
2
2
2
2
2
2
2
2
2
3
2
3
2
3
2
2
2
2
2
2
1
32
2
2
3
3
1
2
2
1
2
2
1
1
2
2
2
2
3
2
2
2
2
2
3
2
3
1
3
1
3
3
2
3
1
1
3
2
13
3
3
2
2
2
3
3
3
2
2
2
3
2
2
3
3
2
2
2
3
2
3
1
1
3
1
2
12
2
2
3
1
2
2
3
3
3
2
2
2
2
3
3
3
2
3
3
2
2
3
1
2
2
2
3
2
2
2
1
3
2
2
1
3
2
2
3
1
2
1
3
3
3
2
1
2
1
3
3
3
1
3
2
2
2
3
3
2
2
3
2
3
2
3
1
2
3
3
3
1
2
3
2
1
1
3
2
2
1
2
2
3
3
2
3
3
2
2
2
3
3
1
3
3
1
2
3
1
2
3
2
3
2
1
3
3
2
3
3
3
1
2
1
3
3
3
2
3
3
3
2
1
3
1
2
3
3
1
33
2
1
3
2
3
1
3
1
3
1
22
3
3
3
3
3
2
2
3
3
3
1
1
3
1
3
3
2
2
1
3
3
2
3
1
1
2
2
2
1
3
2
33
2
1
1
1
3

3
3

13

3

3

2
3

2

1
1
1

1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1

2

2

1

1

Fig. 2. A three-class problem similar to that in Figure 1, with the data in each class generated
from a mixture of Gaussians. The ﬁrst panel shows the maximum-wins procedure ˜d in (3.9). The
second panel shows the decision boundary from coupling of the pairwise linear discriminant rules
based on ˆd in (2.6). The third panel shows the three-class LDA boundaries, and the fourth the
QDA boundaries. The numbers in the captions are the error rates based on a large test set from
the same population.

while in the top right panel this is a class-1 region. Picking a point within
this region, we see the matrix of rij:
·
0(cid:5)98 0(cid:5)46
·
0(cid:5)30
0(cid:5)02
·
0(cid:5)54 0(cid:5)70

(cid:18)rij(cid:19) =


(cid:5)




(3.13)

Class 3 narrowly wins against class 1, while class 1 beats class 2 far more
resoundingly than does class 3. In this example, the coupling has improved the
misclassiﬁcation rate (numbers in parentheses in plots) dramatically over both

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
458

T. HASTIE AND R. TIBSHIRANI

P(Class 1)=1.5/K

P(Class 1)=1.1/K

t
c
e
r
r
o
C
 
y
t
i
l
i

b
a
b
o
r
P

0
1

.

8
0

.

6
0

.

4

.

0

2
.
0

t
c
e
r
r
o
C
 
y
t
i
l
i

b
a
b
o
r
P

0
1

.

8
0

.

6
0

.

4

.

0

2
.
0

5

10

15

20

5

10

15

20

Number of Classes K

Number of Classes K

Fig. 3. Probability of predicting the true class for the rules ˆd (solid) and ˜d (broken). See the text
for details of the problems.

the max–wins and LDA procedures. However, QDA performs a little better in
this example.
The rule max–wins ˜d may also suffer from excess variability, compared to
the coupling rule ˆd. To investigate this, we performed a simple experiment. We
deﬁned class probabilities p1 = s/K(cid:4) pj = (cid:2)1−s/K(cid:3)/(cid:2)K−1(cid:3)(cid:4) j = 2(cid:4) 3(cid:4) (cid:5) (cid:5) (cid:5) (cid:4) K.
Then we set

(3.14)

pi

rij =
pi + pj
rji = 1 − rij(cid:4)

+ 0(cid:5)1zij(cid:4)

j > i(cid:5)

Here zij is a standard normal variate, and rij was truncated at zero below and
1 above. We tried the values s = 1(cid:5)5 and s = 1(cid:5)1. In both scenarios, class 1 has
higher probability and hence is the correct class. Figure 3 shows the average
number of times that class 1 was selected by the rules ˆd (solid) and ˜d (broken).
The averages are over 1000 simulations and have a standard error of about
0.01. The number of classes varies along the horizontal axis. We see that the
ˆd rule outperforms ˜d by about 10% when s = 1(cid:5)5 and 6% when s = 1(cid:5)1.

4. Pairwise threshold optimization. As pointed out by Friedman
(1996a), approaching the classiﬁcation problem in a pairwise fashion allows
one to optimize the classiﬁer in a way that would be computationally bur-
densome for a K-class classiﬁer. While the number of computations for a full
optimization is often proportional to K3, the total required for K(cid:2)K − 1(cid:3)/2

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
CLASSIFICATION BY PAIRWISE COUPLING

459

optimizations would be proportional to K2. Here, we discuss optimization of
the classiﬁcation threshold.
For each two-class problem, let logit pij(cid:2)x(cid:3) = dij(cid:2)x(cid:3). Normally, we would
classify to class i if dij(cid:2)x(cid:3) > 0. Suppose we ﬁnd that dij(cid:2)x(cid:3) > tij is better.
ij(cid:2)x(cid:3). We do
ij(cid:2)x(cid:3) = logit −1d(cid:20)
Then we deﬁne d(cid:20)
ij(cid:2)x(cid:3) to obtain
this for all pairs, and then apply the coupling algorithm to the p(cid:20)
i(cid:2)x(cid:3).
probabilities p(cid:20)
In this way, we can optimize over K(cid:2)K−1(cid:3)/2 parameters separately, rather
than optimize jointly over K parameters. An example of the beneﬁt of thresh-
old optimization is given in the next section.

ij(cid:2)x(cid:3) = dij(cid:2)x(cid:3) − tij, and hence p(cid:20)

5. Examples.

A three-class problem. Here we deﬁne three classes in the plane as fol-
lows: X1(cid:4) X2 were generated uniformly in the square (cid:4)−2(cid:4) 2(cid:5) × (cid:4)−2(cid:4) 2(cid:5). We
deﬁne centers (cid:2)0(cid:4) 2(cid:3), (cid:2)−√
j is the distance
from a point to the jth center, a point is assigned to the class j satisfying
argmin(cid:4)d2
j − tj(cid:5), where t1 = 2 log(cid:2)0(cid:5)05(cid:3)(cid:4) t2 = 2 log(cid:2)0(cid:5)20(cid:3)(cid:4) t3 = 2 log(cid:2)0(cid:5)75(cid:3). Each
class has 100 observations. This example is constructed so that the usual lin-
ear discriminant threshold 2 log(cid:2)1/3(cid:3) is not optimal.

2(cid:3) and (cid:2)√

2(cid:3). Then, if d2

2(cid:4)−√

2(cid:4)−√

The data are shown in Figure 4 along with the decision boundary from
pairwise coupling of LDA (solid). Threshold optimization was used in each

2

1

2
x

0

2

2
2

2
2
2
2
2
2

2
2
2

2

1
-

1
2

2
2
2

2
2
2
2

2
2
2
2
2
2
2
2
2
2
2
2

2
2
22
2
2
2

2
2

2
2

2
2
2
2
2
2

2

2

2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

2

2

2
-

-2

-1

1
1
1
1
1
1
1
1
1
1
1
1
1

1
1

1
1
1
1
1
1
1 1

1

1
1

1
1
1
1
1
1
1
1
1
1
1
1

1

1

1

1

1
1
1
2
2

3

2
2

1
1

1

1

3
3
3

3

2
2

2
2

2
2
2
2

1
1
1
1
1

1
1
2

1
1
1

1

1

2
2

2
2
2

2
2

2

1
1

1

1

1
1
1
1
1
1
1
1
1
1
33
33
3
3
3
3
3

3

3

1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
3

1
1
1

3

3
3

1
1

1

1

1
1
3
3
3

3

3
3

3
3

3

3
3
3

3

3

3

3

3
3
3
3

3
3

3
3
3

33
3
3

3
3

3
3

2

3

3

2
2

2
2
2
3
2

3
3

3

3

3

3
3
3
3

3
3

0
x1

3
3
3

3
3

3

3

3

3

3

3
3
3
3

1

3

3
3

3

3

3

3
3

3
3

3
3

3

3
3

3
3
3
3

Fig. 4. Simulated three-class problem, showing pairwise coupled linear rule with threshold op-
timization (solid) and standard three-class linear discriminant rule (broken). See text for details
of the simulation.

460

T. HASTIE AND R. TIBSHIRANI

two-class LDA. The broken line shows the boundary from standard three-class
LDA. The threshold optimization has accurately captured the boundary.

Various data sets. Table 1 shows the error rates for the three-class prob-

lem and a number of other data sets. The classiﬁers used are:

1. LDA—linear discriminant analysis;
2. QDA—quadratic discriminant analysis;
3. Max—the rule ˜d from (3.9);
4. Max/thresh—the rule ˜d with threshold optimization. The threshold was
found to minimize the training error in the two classes, over a grid of
possible values;

5. Coupled—the rule ˆd from (2.6);
6. Coupled/thresh—the rule ˆd with threshold optimization as above.

A summary of the data sets is given in Table 2. The real data sets are
available from the machine learning archive at University of California at
Irvine—ftp://ics.uci.edu, with the exception of the digits data set. This
consists of 25 constructed features for the classiﬁcation of handwritten digits
0–9, and is available from the authors.

The pairwise procedures all outperform linear discriminant analysis for
most of these problems. Threshold optimization seems to improve performance
both for Friedman’s max rule and the coupling rule. We note that quadratic
discriminant analysis does nearly as well as pairwise coupling for these
problems.

Training errors (ﬁrst line) and test errors (second line) for different examples. Values are mean

(standard errors) over ﬁve simulations

Table 1

Data set

LDA

QDA

Max

Max/thresh Coupled

Coupled/
thresh

Vowel

Vehicle

Crabs

Digits

0.296(0.020) 0.023(0.002) 0.132(0.013) 0.112(0.012) 0.128(0.013) 0.118(0.015)
0.500(0.018) 0.490(0.014) 0.479(0.019) 0.489(0.02)
0.480(0.017) 0.473(0.02)
Waveform 0.148(0.011) 0.052(0.005) 0.121(0.010) 0.115(0.008) 0.120(0.010) 0.115(0.008)
0.214(0.006) 0.220(0.009) 0.176(0.006) 0.174(0.009) 0.173(0.005) 0.172(0.007)
0.192(0.004) 0.073(0.004) 0.165(0.002) 0.157(0.002) 0.165(0.003) 0.158(0.002)
0.233(0.006) 0.158(0.008) 0.210(0.008) 0.213(0.008) 0.209(0.008) 0.213(0.010)
0.045(0.005) 0.038(0.003) 0.039(0.004) 0.027(0.005) 0.039(0.004) 0.027(0.005)
0.051(0.006) 0.060(0.007) 0.063(0.010) 0.063(0.012) 0.063(0.010) 0.063(0.012)
0.047
0.082

0.020
0.055

0.009
0.055

0.018
0.053

0.008
0.053

0.005
0.076

Three
class

0.065(0.004) 0.032(0.004) 0.069(0.004) 0.030(0.002) 0.069(0.004) 0.031(0.002)
0.063(0.004) 0.029(0.003) 0.068(0.005) 0.039(0.007) 0.068(0.005) 0.035(0.007)

CLASSIFICATION BY PAIRWISE COUPLING

461

Table 2

Summary of data sets

Data set

# Training

# Test

# Classes

# Features

Vowel
Waveform
Vehicle
Crabs
Digits
Three class

528
300
423
80
1000
300

462
500
423
120
1000
300

11
3
4
4
10
3

10
21
18
5
25
2

6. Example: adaptive nonlinear classiﬁcation. There have been many
proposals for adaptive estimation of nonlinear regression surfaces. Some pro-
posals, such as additive models [Hastie and Tibshirani (1990)] and MARS
[multivariate additive regression splines; Friedman (1991)] cannot be applied
in a straightforward way to classiﬁcation problems.

In this section, we propose a new way of generalizing adaptive regression,
via the pairwise coupling idea. We start with a simple global procedure, such as
LDA or multiple logistic regression. Here we used LDA. Then we ﬁnd the pair
of classes with the highest confusion rate, that is, if e(cid:2)j(cid:4) k(cid:3) is the proportion
of times that a class-j observation is classiﬁed as class k, we ﬁnd j and k
to maximize e(cid:2)j(cid:4) k(cid:3) + e(cid:2)k(cid:4) j(cid:3). Then, for only classes j and k (coded 0 and
1), we apply an adaptive regression procedure. All other pairs are modelled
via a linear regression. Pairwise coupling is applied to update the joint class
probabilities, and the procedure is repeated for some ﬁxed number of iterations
(we used ﬁve iterations below). At each stage, we ﬁnd the pair of linearly
modelled classes with the highest error rate, and allow a nonlinear model for
that pair.

An advantage of this approach is that complex, nonlinear functions are used
only for the classes where they are needed. This is in contrast to the methods
described above, which use the same set of basis functions for all classes.

For illustration, we applied these methods to some data on vowel recog-
nition. The nonlinear regression procedure used was adaptive backﬁtting for
additive models, using cubic splines. See Hastie (1989) or Hastie and Tibshi-
rani [(1990), Chapter 9] for details.

The vowel example is a popular benchmark for neural network algorithms,
and consists of training and test data with 10 predictors and 11 classes. We ob-
tained the data from the benchmark collection maintained by Scott Fahlman
at Carnegie Mellon University. The data were contributed by Anthony Robin-
son [see Robinson (1989)], and he provided the following (edited) description.
An ASCII approximation to the International Phonetic Association symbol
and the word in which the eleven vowel sounds were recorded is given in
Table 3. The word was uttered once by each of the ﬁfteen speakers. Four male
and four female speakers were used to train the networks, and the other four
male and three female speakers were used for testing the performance. Ten
features were derived for each utterance, from a linear ﬁltering of the speech
signal.

462

T. HASTIE AND R. TIBSHIRANI

Table 3

Words used in recording the vowels

Vowel Word

Vowel Word

i
I
E
A
a:
Y

heed
hid
head
had
hard
hud

O
C:
U
u:
3:

hod
hoard
hood
who’d
heard

In the ﬁve iterations of the adaptive nonlinear classiﬁcation technique, the
procedure built nonlinear rules for classes (cid:2)9(cid:4) 10(cid:3), (cid:2)1(cid:4) 2(cid:3), (cid:2)5(cid:4) 6(cid:3), (cid:2)5(cid:4) 7(cid:3) and
(cid:2)2(cid:4) 3(cid:3). The error rate decreased and then levelled off. The test set confusion
matrices for LDA and the nonlinear pairwise procedure are shown in (6.15)
and (6.16) below. The columns represent the true class, while the rows repre-
sent predicted class:








1

2

3

4

5

6

7

8

9 10 11

0
0
5
3
8

0
0
0
1
7

0
0
28 23
0
0
0
10 16 11
1
2
1
2 16
0
0 11 33
0
0
0
0
9
6 22 19 12
1
0
1 11
0
0
0
0
0
0
0
0
0
0
3
0
0
0
0
0
0
1
6

0
0
0
0
1
0
2
4 23
4
0
1

0
8
0
1
1
2
2
5
0
0
0
0
0
0
0
0 11
0
1
0
0
0
0
6
2
8 15
9
1
8 14 13
0
6 24

0
4
0
0
0
0
0

9
0
0
0
3

5

1

2

3

4

5

6

7

8

9 10 11

30
9
8 32
1
0
0
0
0
0
0
3
0

0
0
0
3
0 25
3
0 10 31
0
0
0
0
0
0
1

0
0
0
1
0 13
7 16 20
0
0
0
0
1

0
0
3
0
0
5
2
1
9 13
5
0 13
0
0
0
6

0
0
1
0
1
0
0
0
0
0
0
0
0
4
4 21
5
3 17 29
0
0

0
5
3
6
0
0
0
0
0
0
0 10
2
0
0
0
2
7
1
1 18
5
6 24

0
4
0
0
0
0
0

9
0
0
0
3

0
0

1
2
3
4
5
6
7
8
9
10
11

1
2
3
4
5
6
7
8
9
10
11

(6.15)

(6.16)








(cid:4)

(cid:5)

The nonlinear construction has successfully reduced the error rates, espe-
cially for classes (cid:2)9(cid:4) 10(cid:3) and (cid:2)1(cid:4) 2(cid:3). The overall training and test-error rates are

CLASSIFICATION BY PAIRWISE COUPLING

463

Table 4

Vowel recognition results

Technique

LDA
Adaptive backﬁtting/pairwise coupling

Error rates

Training

0.32
0.16

Test

0.56
0.44

shown in Table 4; the pairwise approach produces a substantial improvement
over LDA.

This problem was reasonably large (528 training cases, 10 classes, 11 fea-
tures). The computations for this example took about a minute on a Silicon
Graphics Challenge Series R10000 computer. Computations for other exam-
ples, all of which used a simple classiﬁer applied to each of the K(cid:2)K − 1(cid:3)/2
problems, took less than a minute. However, if we were to apply a nonlinear
classiﬁer (such as adaptive additive models) to every pairwise problem, the
computation would increase considerably. Hence the advantage of the adap-
tive approach of this section, in which we apply the nonlinear classiﬁer only
to the pairs that need it.

Hastie, Tibshirani and Buja (1994) discussed two other approaches for
adapting regression procedures to classiﬁcation problems. The ﬁrst is to con-
struct a multiresponse version of the regression procedure, simultaneously
modelling K outcomes in terms of a set of optimally chosen basis functions.
Applying such a procedure to an indicator matrix coding the responses, one ob-
tains a K-vector of ﬁtted values for each observation. One can then classify the
observation to the class having the largest ﬁtted value. This idea has become
known as “softmax” in the machine learning literature. In the experiments in
Hastie, Tibshirani and Buja (1994), this procedure did not work particularly
well, and there we demonstrate a kind of masking that can occur when the
classes are linearly aligned in feature space. The other approach is to apply
linear discriminant analysis in the space of ﬁtted values from the multire-
sponse regression above. This is called “ﬂexible discriminant analysis” (FDA)
in Hastie, Tibshirani and Buja (1994): the theory of this technique exploits
the connection between linear discriminant analysis and optimal scoring. The
error rates for softmax and optimal scoring on the vowel data are 50% and
44%, respectively, as given in Hastie, Tibshirani and Buja (1994).

7. The support-vector machine. Boser, Guyon and Vapnik (1992) pro-
posed a two-class classiﬁer that ﬁnds the hyperplane maximizing the min-
imum (signed) distance between the plane and the training points. Speciﬁ-
cally, the norm vector of the hyperplane w · x + b is found by minimizing the
functional

(7.17)

J(cid:2)w(cid:4) ξ(cid:3) = 1

2(cid:8)(cid:8)w(cid:8)(cid:8)2 + γ

subject to yi(cid:2)w · xi(cid:3) + b ≥ 1 − ξi(cid:5)

ξi

N(cid:2)

1

wTx + b > 0

T. HASTIE AND R. TIBSHIRANI

464
Here the outcome y is coded −1 and 1. The classiﬁer predicts class 2 if
(7.18)
and class 1 otherwise. Because of the nature of the criterion J(cid:2)w(cid:4) ξ(cid:3), the
solution vector ˆw is a linear combination of a subset of the feature vectors xi,
called the “support vectors”. See Vapnik (1996) for a complete discussion. The
intercept b is found by minimizing the training error. Normally, one would
optimize the choice of the regularization parameter γ in (7.17) for a given
problem, but for simplicity and fairness to other procedures considered here,
we used the ﬁxed value of γ = 5.

The support-vector machine has shown very promising results in some real-
world problems (Vapnik, personal communication). However, there seems to be
no simple multiclass version, so this is an attractive candidate for the pairwise
coupling procedure. To proceed, we need to obtain class probability estimates
from the support-vector machine, which we do as follows. Deﬁne z = aTx, and
let m1(cid:4) m2 be the means of z in each class. Let m = (cid:2)m1 + m2(cid:3)/2 and s be the
standard deviation of z − m. Then we deﬁne m(cid:20)
1(cid:4) s(cid:3)(cid:4)
2(cid:4) s(cid:3)(cid:4)

f1(cid:2)z(cid:3) = φ(cid:2)m(cid:20)
f2(cid:2)z(cid:3) = φ(cid:2)m(cid:20)

2 = b + m and

1 = b − m(cid:4) m(cid:20)

(7.19)

where φ(cid:2)µ(cid:4) σ(cid:3) denotes the Gaussian density with mean µ and standard devi-
ation σ. This construction satisﬁes f1(cid:2)z(cid:3) > f2(cid:2)z(cid:3) if z < b and f1(cid:2)z(cid:3) < f2(cid:2)z(cid:3) if
z > b, and so is consistent with the classiﬁcation rule (7.18).
The results of the multiclass support-vector machine are shown in Table 5.
The SV/Max used the rule ˜d to combine the pairwise classiﬁcations, while

Results for support vector machine. Figures are training and test-error rates for
a single realization, except for the three-class problem where mean (standard

error) over 10 simulations is given

Table 5

Data

Vowel

Waveform

Vehicle

Crabs

Digits

Three class

LDA

SV/Max

SV/Coupled

0.316
0.556
0.153
0.208
0.213
0.206
0.050
0.067
0.047
0.082
0.065(0.004)
0.063(0.004)

0.097
0.470
0.067
0.206
0.170
0.222
0.025
0.058
0.022
0.066
0.020(0.004)
0.026(0.004)

0.097
0.450
0.067
0.206
0.170
0.209
0.025
0.075
0.023
0.063
0.020(0.003)
0.025(0.003)

Ave. % test error

improvement vs LDA

7.3

15.5

CLASSIFICATION BY PAIRWISE COUPLING

465
SV/coupled used the coupling rule ˆd. Overall, it performs about as well as the
coupled linear discriminant method.

8. Experiments with nearest neighbors. A J-nearest-neighbor clas-
siﬁer chooses the majority class among the J closest training points to the
target point. Typically, Euclidean distance (cid:8)(cid:8)x− xi(cid:8)(cid:8) = (cid:2)x− xi(cid:3)T(cid:2)x− xi(cid:3) is used
to measure distance between the test input x and the training inputs xi.
We can view J-nearest neighbors as follows. For each class k, we construct

class probability estimates
ˆpk(cid:2)x(cid:3) = 1

N(cid:2)
i=1

J

I(cid:2)(cid:8)(cid:8)x − xi(cid:8)(cid:8) ≤ dJ(cid:2)x(cid:3)(cid:3)I(cid:2)yi = k(cid:3)(cid:4)

(8.20)
where dJ(cid:2)x(cid:3) is the Jth largest of the (cid:8)(cid:8)x − xi(cid:8)(cid:8) values. Then we classify to
the class k with highest estimated probability ˆpk(cid:2)x(cid:3). One way to potentially
improve the performance of nearest neighbors is to multiply each probabil-
ity estimate by a bias factor—that is, form the estimates ˆpk(cid:2)x(cid:3)bk with each
bk positive and less than 1, and then optimize over the biases b1(cid:4) b2(cid:4) (cid:5) (cid:5) (cid:5) (cid:4) bK
[Friedman (1996a); Rosen, Burke and Goodman (1995)]. (In fact, Friedman
uses an additive bias ˆpk(cid:2)x(cid:3) + tk; we ﬁnd a multiplicative bias more natural.)
The joint optimization of K parameters can be computationally difﬁcult, so
Friedman (1996a) suggested carrying this out in a pairwise fashion and then
combining the rules via the max–wins procedure ˜d.

Here is a simple example, due to Friedman (1996b), that illustrates how
bias factors can help. It is illustrated in Figure 5. We have 200 data points in

2
x

0
.
1

8
.
0

6
.
0

4
.
0

2
0

.

0
0

.

1
1

1
1
1
1
1
1
1
1
1
1
1

1

1
1

1
1
1
1
1
1
1
1

1

1

1
1

1

1
1

1
1

1
1
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1

1

1

1

1
1
1
1

1

1
1
1
1

1

1

1
1
1

1
1
1

1

1
1
1
1
1
1
1
1

1

1

1

1
1
1
1
1
1
1
1

1
1
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1
1
1

1
2
2
2
1
2
2
2
1
2
2
1
2
2
2
2
2
1
2
2
2
2
2
2
2
2
2
2
2
1
2
2
2
2
2
2
2
2
1
1
2
2
2
2
2
2
2
2
1
1
2
2
2
2
2
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
1
1
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
1
2
2
1
2
2
2
2
2
1
1
2
2
1
2
2
2
2
2
2
2
1
2
1
2
1
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2 2
2
2
2
2
2
2
2
2
2
2
2
2

1

2

1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1

1

1
1
1
1
1
1
1
1
1
1

1

1
1

0.0

0.2

0.4

0.6

0.8

1.0

x1

Fig. 5. Example of case where nearest-neighbor biasing is needed. There are 200 data points
uniformly distributed in each of the two rectangular regions, separated by the broken line. A large
circular neighborhood, centered in the class-1 region but near the decision boundary, will tend to
have more points in class 2 and hence will misclassify.

466

T. HASTIE AND R. TIBSHIRANI

each of two classes. The points in the ﬁrst class are uniformly distributed in
the rectangle (cid:4)0(cid:4) 3/4(cid:5)×(cid:4)0(cid:4) 1(cid:5), while those in the second class are uniformly dis-
tributed in the rectangle (cid:4)3/4(cid:4) 1(cid:5) × (cid:4)0(cid:4) 1(cid:5). Then a large circular neighborhood,
in the class-1 region but near the decision boundary, will tend to have more
points in class 2 and hence will misclassify. In order to avoid this misclassiﬁ-
cation, a standard nearest-neighbor rule must shrink the neighborhood. This,
in turn, causes an increase in variance. If we instead include a bias factor of
1/3, the two class densities can be fairly compared in the neighborhood, and
we do not have to shrink the neighborhood.

This biasing of class densities does not work for small J, because the proba-
bility estimates are too discrete. We propose instead to view nearest-neighbor
J(cid:2)x(cid:3) be the distance of the
classiﬁcation in terms of density estimation. Let dk
Jth nearest neighbor to x computed separately in each class. A natural esti-
mate of the class-k density at x is
ˆfk(cid:2)x(cid:3) ∝

(8.21)

(cid:4)

J
nk(cid:4)dk

J(cid:2)x(cid:3)(cid:5)p

where p is the dimension of the space and nk the number of training points
in class k. Assuming sample priors nk/n, the corresponding class probability
estimates at x are

(8.22)

ˆpk(cid:2)x(cid:3) ∝

(cid:5)

1
J(cid:2)x(cid:3)(cid:5)p

(cid:4)dk

Note that, when J = 1, this is identical to the usual deﬁnition of 1-nearest-
neighbor classiﬁcation, but not for J ≥ 2. These estimates do not suffer from
the discreteness problem, and can be modiﬁed by a bias factor just as before.
These (biased) pairwise probabilities are then combined using the coupling
procedure (2.6).

9. Example: ten Gaussian classes with unequal covariance.

In this
simulated example taken from Friedman (1996a), there are 10 Gaussian
classes in 20 dimensions. The mean vectors of each class were chosen as 20
independent uniform (cid:4)0(cid:4) 1(cid:5) random variables. The covariance matrices are
constructed from eigenvectors whose square roots are uniformly distributed
on the 20-dimensional unit sphere (subject to being mutually orthogonal),
and eigenvalues uniform on (cid:4)0(cid:5)01(cid:4) 1(cid:5)01(cid:5). There are 100 observations per class
in the training set, and 200 per class in the test set. The optimal decision
boundaries in this problem are quadratic, and neither linear nor nearest-
neighbor methods are well suited. Friedman states that the Bayes error rate
is less than 1%.

Figure 6 shows the test error rates for linear discriminant analysis, J-
nearest neighbor and their paired versions using threshold optimization. We
see that the coupled classiﬁers nearly halve the error rates in each case. In
addition, the coupled rule works a little better than Friedman’s max rule in
each task. Friedman (1996a) reported a median test error rate of about 16%
for his thresholded version of pairwise nearest neighbor.

CLASSIFICATION BY PAIRWISE COUPLING

467

Fig. 6. Test errors for 20 simulations of ten-class Gaussian example.

Why does the pairwise thresholding work in this example? We looked more
closely at the pairwise nearest-neighbor rules that were constructed for this
problem. The thresholding biased the pairwise distances by about 7% on aver-
age. The average number of nearest neighbors used per class was 4.47 (0.122),
while the standard J-nearest-neighbor approach used 6.70 (0.590) neighbors
for all ten classes. For all ten classes, the 4.47 translates into 44.7 neighbors.
Hence, relative to the standard J-NN rule, the pairwise rule, in using the
threshold optimization to reduce bias, is able to use about six times as many
near neighbors.

10. Discussion. Geoffrey Hinton suggested that pairwise approaches to
classiﬁcation might suffer from the following problem. Suppose, for example,
we are classifying handwritten digits (0–9), and one digit (say, 0) tends to
be closer on average in feature space to a randomly chosen digit image than
are other digits. At prediction time, a test image (say, a poorly written 9) is
presented to every pairwise classiﬁer (0–1, 0–2, etc.). Most of these classiﬁers
were not trained on 9’s and hence might give unreliable pairwise conditional
probabilities. If the 9 classiﬁer does not give high enough conditional probabil-
ities, then the 0 digit might win because it tends to receive higher probability
for most random digits. The point is that it may be bad to predict from pair-
wise classiﬁers that have not been trained on images of that type of image, so
that the prediction requires an extrapolation in feature space.

To investigate the validity of this point, we modiﬁed the experiment of
Figure 3. If the true class was 1, the class probabilities were 2/K for class 1
and (cid:2)1−2/K(cid:3)/(cid:2)K−1(cid:3) for the rest. If the true class was not 1, the probabilities
were 1(cid:5)5/K for the true class, 1(cid:5)2/K for class 1 and probabilities (cid:2)1− 1(cid:5)5/K−
1(cid:5)2/K(cid:3)/(cid:2)K−2(cid:3) for the remaining classes. Hence, the ﬁrst class always ﬁnishes
second when it is not the true class. The rij were as deﬁned in (3.14), with

468
s = 1(cid:5)5. We generated 500 realizations from this model, choosing the true
class at random from 1(cid:4) 2(cid:4) (cid:5) (cid:5) (cid:5) (cid:4) K each time.

T. HASTIE AND R. TIBSHIRANI

The left panel of Figure 7 shows the probability of correct classiﬁcation for
the coupling rule (solid) and the max rule (broken). Comparing this to the
left panel of Figure 3, we see that the existence of the popular class 1 has
increased the error rate from 4% to about 16% when K = 3, with less of an
increase for larger K. The max rule does consistently worse than the coupling
rule.

This simulation suggests that Hinton’s suggested problem may be real.
However, it is not clear whether other (non-pairwise) approaches would fare
any better. In addition, when one estimates probabilities, all is not lost. The
right panel shows boxplots of the maximum class probability from the coupled
classiﬁer with K = 5, stratiﬁed by whether the classiﬁcation is correct or not.
Not surprisingly, when the classiﬁer errs, it tends to be less sure about its pre-
diction. If one is willing to “punt,” that is, decide not to classify at all, based
on the magnitude of the maximum class probability, the results improve. For
example, if we punt whenever the maximum class probability is below its 5%
point, the error rate decreases from 16% to 12%.

Suppose the pairwise classiﬁers provide not only conditional probability es-
timates rij but also estimates of the variance of rij, say, vij. Then we can use
these variances as reciprocal weights in the coupling algorithm. Speciﬁcally,
we replace the nij by nij/vij in the algorithm. In theory, this should help the
extrapolation problem mentioned above: a point in class k that is far away
from the training data for classes i and j will have high estimated variance
for classiﬁer i(cid:4) j, and hence its r value will be downweighted. However, our

t
c
e
r
r
o
C
 
y
t
i
l
i

b
a
b
o
r
P

0
.
1

8
.
0

6
.
0

4

.

0

2

.

0

6
.
0

5
.
0

4
.
0

3

.

0

2

.

0

5

10

15

20

number of classes

correct

incorrect

Fig. 7. Left panel: probability of predicting the true class for the rules ˆd (solid) and ˜d (broken).
Right panel: maximum class probability from the coupled classiﬁer with K = 5(cid:4) stratiﬁed by
whether the classiﬁcation is correct or not. See the text for details of the problem.

(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
(cid:127)
CLASSIFICATION BY PAIRWISE COUPLING

469

experiments with this approach, using pairwise linear classiﬁers, did not im-
prove upon the results for the unweighted coupling procedure. A more reﬁned
approach would also incorporate the covariances of the rij’s into the model,
but we have not pursued this.

The pairwise procedures, both Friedman’s max–win and our coupling, are
most likely to offer improvements when additional optimization or efﬁciency
gains are possible in the simpler two-class scenarios. In some situations, they
perform exactly like the multiple-class classiﬁers. Two examples are:

1. Each of the pairwise rules is based on QDA, that is, each class is modelled
by a Gaussian distribution with separate covariances, and then the rij’s
are derived from Bayes rule.

2. A generalization of the above, where the density in each class is modelled
in some fashion, perhaps nonparametrically via density estimates or near-
neighbor methods, and then the density estimates are used in Bayes rule.

Pairwise LDA followed by coupling seems to offer a nice compromise between
LDA and QDA, although the decision boundaries are no longer linear. For this
special case, one might derive a different coupling procedure globally on the
logit scale, which would guarantee linear decision boundaries. Work of this
nature is currently in progress with Jerry Friedman.

APPENDIX

Convergence of the algorithm. The effect of the update in step 2 of the

algorithm (for a single i) is

(A.23)

(cid:4)

(cid:6)
(cid:6)
j(cid:6)=i nijrij
j(cid:6)=i nij ˆµij

α =
ˆµij → α ˆµij
α ˆµij + ˆµji
ˆµji → ˆµji
α ˆµij + ˆµji
i = αpi(cid:5)
pi → p(cid:20)

(cid:4)

(cid:4)

The resulting change in (cid:18)(cid:2)p(cid:3) is
(cid:1)(cid:2)
− (cid:2)

(cid:18)(cid:2)p(cid:20)(cid:3) − (cid:18)(cid:2)p(cid:3) =

j(cid:6)=i

(cid:14)

(cid:5)

ˆµij + 1 − ˆµij

(cid:5)

log

(cid:6)
(cid:6)
j(cid:6)=i nijrij
j(cid:6)=i ˆnijµij
(cid:13)(cid:6)
(cid:6)
j(cid:6)=i nijrij
j(cid:6)=i ˆnijµij

nijrij

nij log

j(cid:6)=i

470

For brevity, let x = (cid:6)

(cid:18)(cid:2)p(cid:20)(cid:3) − (cid:18)(cid:2)p(cid:3) = x log

(A.24)

(cid:5)

(cid:14)
ˆµij + 1

(cid:1)(cid:13)

T. HASTIE AND R. TIBSHIRANI

j(cid:6)=i nijrij, d = (cid:6)
j(cid:6)=i nij ˆµij. Then
− (cid:2)
− 1
(cid:14)
− (cid:2)
ˆµij
− (cid:2)x − d(cid:3)

nij log
(cid:13)

≥ x log

− 1

x
d

x
d

x
d

x
d

j(cid:6)=i

j(cid:6)=i

nij

x
d

= x log
≥ 0(cid:5)

In the second line above, we have used the inequality log(cid:2)1+x(cid:3) ≤ x for x > −1.
In the last line, we have used the inequality x log(cid:2)x/y(cid:3) ≥ x − y for x(cid:4) y ≥ 0,
which can be veriﬁed by noting that, at the stationary points x = y, it has
value 0 and the Hessian is positive deﬁnite. Note that equality holds when
x = d, that is,

j(cid:6)=i nijrij = (cid:6)

j(cid:6)=i nij ˆµij.

Therefore, the log-likelihood increases at each step. Since it is bounded

(cid:6)

above by 0, the algorithm converges.

Note that this algorithm differs from standard iterative proportional scal-
ing, in that it does not minimize over each pi at each iteration. Due to non-
linearity of the model, this would require a line search at each step. However,
it does increase the likelihood at each iteration and converges quite quickly
in practice.

(cid:2)1/K(cid:3) log(cid:4)1/K ˜pi(cid:5) ≤ (cid:2)
Theorem 2. (cid:2)
(cid:6)
j(cid:6)=i ˆµij = (cid:6)
˜pi =

Proof. Since

j(cid:6)=i rij, we have
2

i

i

ˆpi/(cid:2) ˆpi + ˆpj(cid:3)

(cid:2)1/K(cid:3) log(cid:4)1/K ˆpi(cid:5)(cid:5)

(cid:2)
K(cid:2)K − 1(cid:3)
j(cid:6)=i
(cid:2)
j(cid:6)=i

K − 1

1

= ˆpi

Therefore,

(cid:2)
(cid:2)log ˜pi − log ˆpi(cid:3) = (cid:2)
≥ 0(cid:5)

i

i

log

2/k

(cid:2) ˆpi + ˆpj(cid:3) (cid:5)
(cid:1)

1

K − 1

(cid:5)

2/k

(cid:2) ˆpi + ˆpj(cid:3)

(cid:2)
j(cid:6)=i

(cid:6)
j(cid:6)=i 1/(cid:2) ˆpi + ˆpj(cid:3) takes
In the second line above, we have used the fact that
its minimum when the ˆpi are equal. The minimum is K(cid:2)K − 1(cid:3)/2, and the
theorem is proved. ✷

CLASSIFICATION BY PAIRWISE COUPLING

471

Acknowledgments. We thank Jerry Friedman for sharing a preprint of
his pairwise classiﬁcation paper with us, and acknowledge helpful discussions
with Jerry, Geoff Hinton, Radford Neal and David Tritchler. Comments by an
Editor and two referees led to valuable improvements in the manuscript.

REFERENCES

Bishop, Y., Fienberg, S. and Holland, P. (1975). Discrete Multivariate Analysis. MIT Press.
Boser, B., Guyon, I. and Vapnik, I. (1992). A training algorithm for optimal margin classiﬁers.

In Proceedings of COLT II, Philadelphia, PA.

Bradley, R. and Terry, M. (1952). The rank analysis of incomplete block designs. I. The method

of paired comparisons. Biometrika 39 324–345.

Deming, W. and Stephan, F. (1940). On a least squares adjustment of a sampled frequency table

when the expected marginal totals are known. Ann. Math. Statist. 11 427–444.

Friedman, J. (1991). Multivariate adaptive regression splines (with discussion). Ann. Statist. 19

1–141.

Friedman, J. (1996a). Another approach to polychotomous classiﬁcation. Technical report, Stan-

ford Univ.

Friedman, J. (1996b). Bias, variance, 0–1 loss and the curse of dimensionality. Technical report,

Stanford Univ.

Hastie, T. (1989). Discussion of “Flexible parsimonious smoothing and additive modelling” by

Friedman and Silverman. Technometrics 31 3–39.

Hastie, T. and Tibshirani, R. (1990). Generalized Additive Models. Chapman and Hall, London.
Hastie, T., Tibshirani, R. and Buja, A. (1994). Flexible discriminant analysis by optimal scoring.

J. Amer. Statist. Assoc. 89 1255–1270.

Robinson, A. J. (1989). Dynamic error propagation networks. Ph.D. dissertation, Dept. Electrical

Engineering, Cambridge Univ.

Rosen, D., Burke, H. and Goodman, O. (1995). Local learning methods in high dimensions:
beating the bias-variance dilemma via recalibration. In NIPS Workshop: Machines
that Learn—Neural Networks for Computing.

Vapnik, V. (1996). The Nature of Statistical Learning Theory. Springer, New York.

Department of Statistics
Sequoia Hall
Stanford University
Stanford, California 94305
E-mail: trevor@playfair.stanford.edu

Departments of Public Health

Sciences and Statistics

University of Toronto
Toronto, Ontario
M5S 1A8 Canada
E-mail: tibs@utstat.toronto.edu

