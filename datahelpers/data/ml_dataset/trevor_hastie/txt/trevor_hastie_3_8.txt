Electronic Journal of Statistics
Vol. 1 (2007) 1–29
ISSN: 1935-7524
DOI: 10.1214/07-EJS004

Forward stagewise regression and the

monotone lasso

Trevor Hastie†

Departments of Statistics, and Health, Research & Policy

Sequoia Hall, Stanford University, CA 94305.

e-mail: hastie@stanford.edu

Jonathan Taylor

Department of Statistics

Sequoia Hall, Stanford University, CA 94305.

e-mail: jtaylor@stanford.edu

Robert Tibshirani‡

Departments of Health, Research & Policy, and Statistics

Sequoia Hall, Stanford University, CA 94305.

e-mail: tibs@stanford.edu

Guenther Walther§

Department of Statistics

Sequoia Hall, Stanford University, CA 94305.

e-mail: walther@stanford.edu

Abstract: We consider the least angle regression and forward stagewise al-
gorithms for solving penalized least squares regression problems. In Efron,
Hastie, Johnstone & Tibshirani (2004) it is proved that the least angle re-
gression algorithm, with a small modiﬁcation, solves the lasso regression
problem. Here we give an analogous result for incremental forward stage-
wise regression, showing that it solves a version of the lasso problem that
enforces monotonicity. One consequence of this is as follows: while lasso
makes optimal progress in terms of reducing the residual sum-of-squares
per unit increase in L1-norm of the coeﬃcient β, forward stage-wise is op-
timal per unit L1 arc-length traveled along the coeﬃcient path. We also
study a condition under which the coeﬃcient paths of the lasso are mono-
tone, and hence the diﬀerent algorithms coincide. Finally, we compare the
lasso and forward stagewise procedures in a simulation study involving a
large number of correlated predictors.

AMS 2000 subject classiﬁcations: Primary 62J99; secondary 62J07.

∗The authors thank Steven Boyd, Jerome Friedman, Saharon Rosset, Ben van Roy and Ji

Zhu for helpful discussions.

† Hastie was partially supported by grants DMS-0204612 and DMS-0505676 from the
National Science Foundation, and grant 2R01 CA 72028-07 from the National Institutes of
Health.

‡Tibshirani was partially supported by National Science Foundation Grant DMS-9971405

and National Institutes of Health Contract N01-HV-28183.

§Walther was partially supported by National Science Foundation Grant DMS-0505682

National Institutes of Health grant 5R33HL068522.

1

T. Hastie et al./The monotone lasso

2

Keywords and phrases: regression, lasso, stagewise.

Received March 2007.

Contents

1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2 Background: The LARS Algorithm . . . . . . . . . . . . . . . . . . . .
3 Forward Stagewise and the Monotone Lasso . . . . . . . . . . . . . . .
4 Forward Stagewise for General Convex Loss Functions . . . . . . . . .
5 Discussion of Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . .
6 Monotonicity of Proﬁles . . . . . . . . . . . . . . . . . . . . . . . . . .
7 Lasso versus Forward Stagewise: Which is Better? . . . . . . . . . . .
A Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
A.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . .
A.2 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . .
A.3 Simulation Details
. . . . . . . . . . . . . . . . . . . . . . . . . .
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2
4
7
15
17
19
21
23
23
26
28
28

1. Introduction

The lasso (Tibshirani 1996) is a method for regularizing a least squares re-
gression. Suppose we have predictor measurements xij , j = 1, 2, . . . , p and an
outcome measurement yi, observed for cases i = 1, 2, . . . N . The lasso ﬁts a
linear model

f (x) = β0 +

xjβj

p

Xj=1

(1)

by solving the optimization problem

minβ

N

Xi=1


yi − β0 −

2

p

Xj=1

xij βj


subject to

p

Xj=1

|βj| ≤ s

(2)

If the tuning parameter s ≥ 0 is large enough, this gives the ordinary least
squares estimates. However, smaller values of s produce shrunken estimates ˆβ,
often with many components equal to zero. Choosing s can be thought of as
choosing the number of predictors to include in a regression model. Thus the
lasso can select predictors like subset selection methods. However, since it is
a smooth optimization problem, it is less variable than subset selection, and
can be applied to much larger problems (large in p). Chen, Donoho & Saunders
(1998) developed related technology in the context of signal processing.

The criterion (2) leads to a quadratic programming problem for each s, and
thus standard numerical-analysis methods can be used to solve it. Figure 1

T. Hastie et al./The monotone lasso

3

Lasso

LAR

Forward stagewise

s
t

i

n
e
c
i
f
f

e
o
C

s
t

i

n
e
c
i
f
f

e
o
C

s
t

i

n
e
c
i
f
f

e
o
C

4

2

0

2
−

4
−

4

2

0

2
−

4
−

6
−

4

2

0

2
−

4
−

0

20

40

60

80

L1 Norm (Standardized)

Fig 1. Coeﬃcient proﬁles, simulated example. The L1 norm is computed on the standardized
variables. The coeﬃcients are given on their original scale, on which the details are more
visible. The lasso starts to diﬀer from LAR at the broken vertical line, when the gray co-
eﬃcient passes through zero. Forward stagewise starts to diﬀer from LAR and lasso at the
dotted vertical line, where the gray coeﬃcient goes ﬂat instead of turning back towards zero.

T. Hastie et al./The monotone lasso

4

shows an example, based on simulated data with 10 predictors (details of the
data generation and model are given later). The top panel shows the coeﬃcient

proﬁles of the lasso solutions, as the bound s = P |βj| is increased from 0 up

to the point where the full least squares solutions are obtained (right end of
ﬁgure).

Notice the piecewise linear nature of the lasso proﬁles. Efron et al. (2004)
exploited this fact to derive a simple algorithm — least angle regression — for
simultaneously solving the entire set of lasso problems (all values of s). This
work was motivated by an observation in Hastie, Tibshirani & Friedman (2001,
Section 10.12.2) that the lasso proﬁle bore a striking similarity to the coeﬃcient
proﬁle produced by a version of boosting for linear models, which they named
the incremental forward stagewise algorithm (hereafter FSǫ).

This FSǫ algorithm (see Algorithm 1) creates a coeﬃcient proﬁle as follows:
at each step it increments the coeﬃcient of that variable most correlated with
the current residuals by an amount ±ǫ, with the sign determined by the sign of
the correlation. Efron et al. (2004) in fact considered the limiting version of this
algorithm, with ǫ ↓ 0, which also has piecewise linear coeﬃcient paths. We refer
to this as the inﬁnitesimal forward stagewise algorithm, hereafter FS0 or simply
forward stagewise. Efron et al. (2004) showed that under certain conditions, this
FS0 path is identical to the lasso path. However, for most problems they are
diﬀerent (e.g. Figure 1), and sometimes strikingly so (Figure 7). The FS0 paths
are much smoother than the lasso paths.

The primary result in this paper is the characterization of FS0 as a monotone
version of the lasso, in a sense to be described in Section 3. As such, it is a more
restricted version of the lasso, and hence the additional smoothness. Because of
the monotonicity, the criterion cannot be deﬁned pointwise (as lasso can), but
instead deﬁnes the entire path via a diﬀerential equation.

In Section 4 we generalize this characterization to loss functions other than
squared error. Section 5 considers other candidate criteria, and Section 6 exam-
ines conditions under which the lasso and FS0 are the same. Section 7 compares
the two procedures in a simulation study. Some proofs are given in the Appendix.

2. Background: The LARS Algorithm

Hastie et al. (2001) showed that the solution path for the lasso is strikingly
similar to that of a simpliﬁed version of “boosting”. Boosting is an adap-
tive, non-linear, function-ﬁtting method that has received much attention in
the past ten years (Schapire & Freund 1997, Schapire, Freund, Bartlett & Lee
1998, Friedman, Hastie & Tibshirani 2000). In modern versions of boosting, the
set of “variables” is a large space of binary trees, which are selected, shrunk,
and added to the current model. In their simpliﬁcation, Hastie et al. (2001) re-
placed boosted trees by an incremental forward stagewise algorithm for linear
regression, reproduced here as Algorithm 1.

The Least Angle Regression algorithm (LAR, see Algorithm 2) of Efron et al.
(2004) was initially intended as the limiting version FS0 of FSǫ. As we explain

T. Hastie et al./The monotone lasso

5

Algorithm 1. Incremental Forward Stagewise Regression: FSǫ

1. Start with r = y − ¯y, β1, β2, . . . βp = 0.
2. Find the predictor xj most correlated with r.
3. Update βj ← βj + δj, where δj = ǫ · sign[corr(r, xj )];
4. Update r ← r − δj xj, and repeat steps 2 and 3 until no predictor has any

correlation with r.

below, LAR is diﬀerent from both FS0 and lasso, but both can be obtained
through simple modiﬁcations. The bottom panel of Figure 1 shows the coeﬃcient
proﬁles of FS0. Notice that they are similar to lasso and LAR, but tend to be
smoother.1

LAR is a kind of “democratic” alternative to a version of the commonly
used forward-stepwise regression algorithm. Forward-stepwise regression starts
with all coeﬃcients equal to zero, and then builds a sequence of models by
successively including one variable at a time, and updating the least-squares ﬁt.
The version we consider here enters at each stage the variable most correlated
with the residuals.2 This process is repeated until all p predictors have been
entered, or the residuals are zero.

LAR uses a similar strategy, but only enters “as much” of a predictor as
it “deserves”: the coeﬃcient of the predictor is increased only up to the point
where some other predictor has as much correlation with the current residual.
This new predictor is entered, and the process is continued. Algorithm 2 gives
more details.

Algorithm 2. LARS: Least Angle Regression

1. Standardize the predictors to have mean zero and variance 1. Start with

the residual r = y − ¯y, β1, β2, . . . βp = 0.

2. Find the predictor xj most correlated with r.
3. Move βj from 0 towards its least-squares coeﬃcient hxj, ri, until some
other competitor xk has as much correlation with the current residual as
does xj.

4. Move (βj , βk) in the direction deﬁned by their joint least squares coeﬃcient
of the current residual on (xj, xk), until some other competitor xl has as
much correlation with the current residual.

5. Continue in this way until all p predictors have been entered. After p steps,

we arrive at the full least-squares solution.

The proﬁles for LAR are shown in the middle panel of Figure 1. They look

1The name “LARS”, derived from least angle regression and lasso, is the name we have

given to our algorithm that implements LAR, lasso and FS0.

2This can diﬀer from the more traditional version, which includes the variable that leads

to the largest drop in residual sum-of-squares.

T. Hastie et al./The monotone lasso

6

similar to the lasso solutions, especially in the beginning. The ﬁrst discrepancy
is at the place marked by a vertical broken line in the lasso proﬁles. The LAR
proﬁle passes through zero at this point, while the lasso proﬁle hits zero, and
stays there. This similarity is no coincidence. It turns out that with one mod-
iﬁcation, the LAR procedure exactly produces the set of lasso solutions for all
s. The modiﬁcation needed is as follows:

Algorithm 2a LARS: lasso Modiﬁcation.

5a. If a non-zero coeﬃcient hits zero, drop it from the active set and recompute

the current joint least squares direction.

The LARS(lasso) algorithm is extremely eﬃcient, requiring the same order of
computation as that of a single least squares ﬁt using the p predictors. Least an-
gle regression always takes p steps to get to the full least squares estimates. The
lasso path can have more than p steps, although the two are often quite similar.
Algorithm 2a is an eﬃcient way of computing the solution to any lasso problem,
especially when N ≪ p (Donoho & Tsaig 2006). Osborne, Presnell & Turlach
(2000) also discovered a piecewise-linear path for computing the lasso, which
they called a homotopy algorithm.

Efron et al. (2004) showed that another variant of the LAR algorithm gives
FS0; see also Theorem 1 on page 10. Suppose we have reached a point when a
variable enters the active set A; all variables xj, j ∈ A have correlation equal in
magnitude with the current residual r. Then the new LAR direction is deﬁned
by the least-squares ﬁt of r on XA. The modiﬁcation needed to achieve FS0
replaces this by a type of non-negative least squares direction.

Algorithm 2b LARS: FS0 Modiﬁcation.

4. Find the new direction by solving the constrained least squares problem

minb||r − XAb||2

2 subject to bjsj ≥ 0, j ∈ A,

where sj is the sign of hxj , ri.

The constraints arise from the fact that in step 3 of the incremental forward
stagewise procedure, the coeﬃcient of each predictor is increased in the direction
of its correlation with the current residual. In Figure 1 LAR and FS0 start to
diﬀer (vertical dotted line) when the third variable enters A.

The top left panel of Figure 3 on page 12 shows the residual sum of squares
(RSS) for each of the three procedures, as a function of the L1 norm of the
coeﬃcient vector. As expected, the lasso curve lies below the other two, because
it decreases the RSS the fastest per unit increase in L1 norm. The right panel
plots RSS against the L1 arc-length of the coeﬃcient proﬁle; here FS0 wins —
a point we enlarge on in the next section.

In summary, we see that the forward stagewise and LAR algorithms “nearly”
solve the L1-penalized regression problem. It is natural to ask: what problems

T. Hastie et al./The monotone lasso

7

are the forward stagewise and LAR algorithms solving? Keith Knight asked that
question in the discussion of Efron et al. (2004).

We provide some answers to the ﬁrst question in this paper. We make three

main contributions:

• we characterize forward stagewise as a monotone version of the lasso, in
an extended space of variables consisting of each variable and its negative.
• we study a condition under which the proﬁles of all three methods are

monotone, and hence the three methods coincide.

• we compare the lasso and forward stagewise procedures in a simulation

study involving a large number of correlated predictors.

3. Forward Stagewise and the Monotone Lasso

In this section we consider an expanded representation of the lasso problem
which facilitates a clearer understanding of the forward stagewise procedure. For
each predictor xj, we include its negative version −xj, resulting in an expanded
data set with 2p predictors. In matrix notation we create an expanded data
matrix ˜X = [X : −X]. In this framework the lasso problem becomes

xij β+

j −

p

Xj=1

2

xij β−

j 




(3)

subject to β+

j , β−

(β+

j + β−

j ) ≤ s

= minβ0,β+

j ,β−
j

n

Xi=1

p


yi − β0 −
Xj=1

Xj=1

j ≥ 0 ∀j and

p

In what follows we will sometimes suppress the constant term β0, which can
always be removed once and for all by centering all the variables. Since each
value of the bound s characterizes a solution, we can use s to index the solution
β(s); whenever the constraint is active, the solution satisﬁes ||β(s)||1 = s, and
we say the solution proﬁle is parametrized by L1-norm. Problem (3) is equiv-
alent to a standard representation for solving the lasso problem by quadratic
programming, and the KKT conditions ensure that at most one of ˆβ+
j and ˆβ−
j
are greater than zero at the same time. Hence by augmenting the data with the
negative of the variables, the positive lasso in the enlarged space is equivalent
to the original lasso problem. Figure 2 (top pair of panels) shows the coeﬃcient
paths of the positive and negative variables for the lasso solution in Figure 1.

In the lower pair of plots, an additional constraint is imposed on this sequence
of lasso problems: the coeﬃcient paths are constrained to be monotone non-
decreasing. These monotone paths are exactly equivalent to the paths of the
forward-stagewise algorithm. By this we mean that the collapsed versions of
the paths (subtracting the coeﬃcients for the negative versions of the variables,
from the corresponding coeﬃcients for the positive versions) are exactly the
forward-stagewise paths in the lower panel in Figure 1.

T. Hastie et al./The monotone lasso

8

This leads us to characterize the forward-stagewise algorithm as a monotone
version of the lasso. These extra restrictions are an additional form of regular-
ization, leading to smoother coeﬃcient proﬁles.

This expanded space of variables creates a more natural analog of boosting,
which operates in a large dictionary of binary trees. For every tree, its negative
is also available.3 In the expanded space the equivalent of Algorithm 1 is given
in Algorithm 3. It is obvious that Algorithm 3 generates monotone coeﬃcient

Algorithm 3. Monotone Incremental Forward Stagewise Regression

1. Start with r = y − ¯y, β1, β2, . . . β2p = 0.
2. Find the predictor ˜xj most positively correlated with r.
3. Update βj ← βj + ǫ.
4. Update r ← r − ǫ˜xj, and repeat steps 2 and 3 until no predictor has any

correlation with r.

paths, indexed by the number of steps m, or the total distance stepped t = m · ǫ.
Drawing on the results of Efron et al. (2004), we show in Theorem 1 that the
limit as ǫ ↓ 0 leads exactly to the monotone representation as in Figure 2. First
we deﬁne the notion of L1 arc-length.

Deﬁnition 1. Suppose β(t) is a one-dimensional diﬀerentiable curve in t ≥ 0,
with β(0) = 0. The L1 arc-length of β(t) in [0, t] is given by

TV(β, t) =Z t

0

|| ˙β(s)||1ds,

(4)

where ˙β(s) = ∂β(s)/∂s.

We have named the arc-length “TV” for total-variation; the L1 arc-length of
β(t) up to time t is the sum of total variation measures for each of its coordinate
functions, and is a measure of roughness of the curve.

For a piecewise-diﬀerentiable continuous curve, the arc-length is the sum of
the arc-lengths of the diﬀerentiable pieces. The following lemma is easily proved:

Lemma 1. If the coordinates of β(t) are monotone and piecewise diﬀerentiable
in t, then TV(β, t) = ||β(t)||1.

Hence the arc-length and L1 norm for a monotone coeﬃcient proﬁle are the

same.

Although it is convenient to use this expanded representation ˜X, we can
always collapse to the original representation X. The coeﬃcients in the original
representation are simply the paired diﬀerences βj(t) = β+
j (t). Note
that

j (t) − β−

3We can think of a tree as a variable; the N values are obtained by passing the training

data down to the terminal nodes.

T. Hastie et al./The monotone lasso

9

Lasso

Monotone Lasso

)
e
v
i
t
i
s
o
P

(
 
s
t

i

n
e
c
i
f
f

e
o
C

)
e
v
i
t

a
g
e
N

(
 
s
t

i

n
e
c
i
f
f

e
o
C

)
e
v
i
t
i
s
o
P

(
 
s
t

i

n
e
c
i
f
f

e
o
C

)
e
v
i
t

a
g
e
N

(
 
s
t

i

n
e
c
i
f
f

e
o
C

4

3

2

1

0

4

3

2

1

0

4

3

2

1

0

4

3

2

1

0

0

20

40

60

80

L1 Norm (Standardized)

Fig 2. Expanded coeﬃcient proﬁles, simulated example. The L1 norm is computed on the
standardized variables. The coeﬃcients are given on their original scale, on which the details
are more visible.

T. Hastie et al./The monotone lasso

10

• The L1 norm for the lasso coeﬃcients is the same in either representation,
since only one coeﬃcient in each pair is non-zero at a time (see the proof
for part 1 of Theorem 1).

• The L1 arc-length for FS0 is the L1 norm in the expanded representation,
and is equal to the L1 arc-length in the original representation. This is
NOT the same as the L1 norm in the original representation.

Every point along the lasso path is the solution to a convex optimization
problem. Unfortunately, the monotonicity restriction of the forward stagewise
path appears to preclude such a succinct characterization. Alternatively, we
can show that the lasso path is the solution to a diﬀerential equation, which
characterizes the path in terms of a series of optimal moves. We then show
that the forward stagewise path is the solution to a closely related diﬀerential
equation, which restricts these optimal moves to be monotone. In the remainder
of this section:

• we characterize the forward stagewise path in terms of a sequence of mono-
tone moves, and compare these moves to the less restrictive moves of the
lasso (Theorem 1);

• this leads us to deﬁne the monotone lasso — a path deﬁned by a diﬀeren-
tial equation — with the derivatives giving the move directions from the
current position (Deﬁnitions 2–3). The lasso can also be characterized as
a solution to a related diﬀerential equation;

• we show that the monotone lasso is locally optimal in terms of arc-length—
it makes the optimal move per unit increase in arc-length of the coeﬃcient
proﬁle. The lasso makes the optimal move per unit increase in the L1 norm
of the coeﬃcients (Theorem 2);

Theorem 1. Let β0 ∈R2p be a point either on the lasso or forward stagewise

• we show that the forward stagewise algorithm computes the solution to

the monotone lasso criterion. (Proposition 1).

We then generalize these results for other loss functions in Section 4.

path in the expanded-variable space, and let A be the active set of variables
achieving the maximal correlation with the current residual r = y − ˜Xβ0.

1. The lasso coeﬃcients move in a direction given by the coeﬃcients of the

least squares ﬁt of ˜XA on r.

2. The forward stagewise coeﬃcients move in a direction given by the coeﬃ-

cients of the non-negative least squares ﬁt of ˜XA on r.

In either case only the coeﬃcients in A change, and this ﬁxed direction is pur-
sued until the ﬁrst of the following events occurs:

(a) a variable not in A attains the maximal correlation and joins A;
(b) The coeﬃcient of a variable in the active set reaches 0, at which point it

leaves A (lasso only);

(c) the residuals match those of the unrestricted least squares ﬁt.

When (a) or (b) occur, the direction is recomputed.

T. Hastie et al./The monotone lasso

11

these moves can be deﬁned starting from any value β0.

Theorem 1 is stated in terms of a point β0 on the lasso/FS0 paths. In fact

The proof of this theorem can be assembled from the results proved in
Efron et al. (2004). For convenience we give a simple proof in the appendix,
using convex optimality conditions (see also Rosset & Zhu (2004)).

Theorem 1 leads us to deﬁne the monotone lasso as the solution to a diﬀer-
ential equation, which is characterized in terms of its positive path derivatives.
The FS0 algorithm computes this solution.

Deﬁnition 2. Let β ∈R2p be any coeﬃcient for a linear model in the expanded
1. The lasso move direction ρl(β) :R2p 7→R2p is deﬁned
2. The monotone lasso move direction ρml(β) :R2p 7→R2p is deﬁned

variable set, and let r = y − ˜Xβ. Let A be the active set of variables achieving
maximal correlation with r.

with θj = 0 except for j ∈ A, where θA is the least squares coeﬃcient of r
on ˜XA.

ρl(β) = (cid:26)

0

if ˜XT r = 0
otherwise,

θ/Pj θj

(5)

(6)

ρml(β) = (cid:26)

θ/Pj θj

0 if ˜XT r = 0
otherwise,

with θj = 0 except for j ∈ A, where θA is the non-negative least squares
coeﬃcient of r on ˜XA.

venient when we parametrize the coeﬃcient paths later in this section.

The normalizations in (5) and (6) are not essential, but turn out to be con-

Theorem 2. Let β0 ∈R2p be a coeﬃcient vector in the expanded-variable space.

Figure 3 shows the residual-sum-of-squares (RSS) curves for the lasso and
forward stagewise algorithms, applied to our simulation example. It appears in
this example that lasso decreases RSS most rapidly as a function of the L1
norm of the coeﬃcients ||β(t)||1, while forward stagewise wins in terms of L1
arc-length. It turns out that this is always the case, and is a characterization of
the local optimality for each of the procedures.

Then the lasso/monotone lasso move directions deﬁned in Deﬁnition 2 are op-
timal in the sense that

1. A lasso move decreases the residual sum of squares at the optimal quadratic

rate with respect to the L1 coeﬃcient norm;

2. A monotone-lasso move decreases the residual sum of squares at the opti-

mal quadratic rate with respect to the coeﬃcient L1 arc-length.

There is some intuition in this distinction when we think of forward stagewise
as a form of boosting. There we pay a cost in terms of eﬀort for any move we
make (number of trees), which is captured by arc-length. With the lasso we get

T. Hastie et al./The monotone lasso

12

5
2

4
2

3
2

2
2

1
2

0
2

9
1

Lasso
Forward
Stagewise
LAR

s
e
r
a
u
q
S

 
f
o
 
m
u
S

 
l
a
u
d
s
e
R

i

5
2

4
2

3
2

2
2

1
2

0
2

9
1

Lasso
Forward
Stagewise
LAR

s
e
r
a
u
q
S

 
f
o
 
m
u
S

 
l
a
u
d
s
e
R

i

0

20

40

60

80

0

20

40

60

80

100

L1 Norm

L1 Arc Length

Fig 3. The RSS for our simulation example, as a function of the L1 norm (left panel) and
arc length (right panel) of the coeﬃcient paths for lasso, forward stagewise, and Least Angle
Regression.

rewarded for decreasing a coeﬃcient towards zero. The monotonicity constraint
also results in much smoother coeﬃcient proﬁles, and hence shorter arc-lengths.
Zhao & Yu (2004) propose a modiﬁcation to boosting to allow the backtracking
needed to make FS0 coincide with lasso.

Our proof follows closely the material in Section 6 of Efron et al. (2004).
Since the directions are ﬁxed while A is ﬁxed, the paths are piecewise linear,
and hence the residual-sum-of-squares curves are piecewise quadratic.

Proof of Theorem 2: lasso. Consider a move in direction d from β0 : β0 + γ · d.
Deﬁne

R(γ) = ||y − ˜X(β0 + γ · d)||2
2;
T (γ) = (β0 + γ · d)T 1,

(7)
(8)

where 1 = (1, . . . , 1)T . Assuming dj > 0 when β0
the changed coeﬃcient. We then compute the path derivative

j = 0, T (γ) is the L1 norm of

U (γ) =

∂R
∂T

=

∂R(γ)

∂γ

∂γ . ∂T (γ)
dT ˜XT (cid:16)y − ˜X(β0 + γ · d)(cid:17)

dT 1

= −2

U (γ)|γ=0 = −2

dT ˜XT r

dT 1

,

;

(9)

(10)

(11)

T. Hastie et al./The monotone lasso

13

where r = y − ˜Xβ0. Since ˜xT
j r = C for all j ∈ A, the maximal-correlation active
set, this derivative is minimized by allowing only those elements dj with j ∈ A
to be nonzero. For any such d = dA, the derivative is U (0) = −2C. Among
these, we seek the dA with smallest Hessian.

∂2R
∂T 2 =

∂U
∂T

=

∂U

∂γ. ∂T

dT
A

∂γ
˜XAdA
˜XT
A
(dT
A1)2

= 2

(12)

Minimizing this Rayleigh-quotient is equivalent to minimizing

dT
A

˜XT
A

˜XAdA

subject to dT

A1 = 1.

It is straightforward to show that the solution is given by dA ∝ ( ˜XT
A
But since ˜Xt

Ar = C · 1, this is equivalent to the lasso move.

˜XA)−11.

Hence the sequence of lasso moves result in an optimal piecewise-quadratic

RSS drop-oﬀ curve as a function of L1 norm.

Proof of Theorem 2: monotone lasso.

The increment in the L1-arc-length of the path β0 + γ · d (starting from β0)

is easily seen to be

Similar to (9), we get

L(γ) = γ||d||1.

V (γ) =

∂R
∂L

=

∂R(γ)

∂γ . ∂L(γ)

∂γ

dT ˜XT (y − ˜X(β + γ · d)

= −2

V (γ)|γ=0 = −2

||d||1

dT ˜XT y
||d||1

.

(13)

(14)

(15)

(16)

.

This is minimized by selecting dj ≥ 0 for j ∈ A, again with minimizing value
−2C. The Hessian is

∂2R
∂L2 =

∂V

∂γ. ∂L

∂γ
˜XT
A
||d||2
1

dT
A

˜XAdA

= 2

,

(17)

which we would like to minimize subject to dj ≥ 0. This is equivalent to the
optimization problem

minddT ˜XT
A

˜XAd

subject to dj ≥ 0, Pj∈A dj = 1.

(18)

It is straightforward to show via the KKT conditions for this quadratic pro-
gramming problem that the solution is identical to the solution for ρ in (53) in
the appendix, which is the direction given by a non-negative least-squares ﬁt of
r to ˜XA (the forward-stagewise move).

T. Hastie et al./The monotone lasso

14

The graphs in Figure 3 suggest that the gap is bigger as a function of arc-
length than norm. This is in fact the case, as can be seen in the proof of The-
orem 2. As a function of norm, starting from the same point, the downward
gradient is the same for both lasso and FS0, but the Hessian is smaller for lasso.
As a function of arc-length, the gradient for lasso can be larger than for FS0, if
some of the dj are negative.

Armed with the lasso and monotone lasso move directions from deﬁnition 2,

we can now characterize paths as solutions to diﬀerential equations.
Deﬁnition 3. The monotone lasso coeﬃcient path β(ℓ) for a dataset ˜X =
{X, −X} is the solution to the diﬀerential equation

∂β
∂ℓ

= ρml(β(ℓ)),

(19)

with initial condition β(0) = 0. Since (19) is piecewise continuous, this path is
continuous and piecewise diﬀerentiable.

Because the directions ρml(β) deﬁned in (6) are standardized to have unit
L1 norm, the solution curve is unit L1-speed, and hence is parametrized by L1
arc length.

In order to solve (19), we need to track the entire path; this solution is

provided by the forward-stagewise algorithm.
Proposition 1. The forward-stagewise algorithm for a dataset ˜X = {X, −X}
and square-error loss computes the monotone lasso path β(ℓ); it starts at 0, and
then increments the coeﬃcients continuously according to the monotone lasso
moves (6). Speciﬁcally

Initialize Set β(0) = 0, ℓ0 = 0, and ρ0 = ρml(0), with corresponding active set

A0.

For j = 0, 1, 2, . . .

1. Let β(ℓ) = β(ℓj) + (ℓ − ℓj) · ρj, ℓ ∈ [ℓj, ℓj+1], where ℓj+1 is the value

of ℓ > ℓj at which Aj changes to Aj+1.

2. Compute ρj+1 = ρml(β(ℓj+1)).
3. If ρj+1 = 0 exit, and β(ℓ) is deﬁned on [0, L], with L = ℓj+1.

Proposition 1 follows from Theorem 1. We can characterize the lasso path in

a similar fashion.4
Proposition 2. The lasso coeﬃcient path β(ℓ) for a dataset ˜X = {X, −X} is
the solution to the diﬀerential equation

∂β
∂ℓ

= ρl(β(ℓ)),

(20)

with initial condition β(0) = 0. Since (20) is piecewise continuous, this path is
continuous and piecewise diﬀerentiable.

4Since the lasso path is deﬁned in Tibshirani (1996) as the solution to a convex optimization
problem, this alternative characterization is a proposition (unlike Deﬁnition 3), and follows
from Theorem 1.

T. Hastie et al./The monotone lasso

15

The normalization of ρl deﬁned in (5) guarantees that the solution path is

parametrized by L1 norm (since the coeﬃcients are non-negative).

The characterizations above draw on the similarities between the lasso and
monotone lasso. The characterization of the monotone lasso falls slightly short
of that of the lasso for the following reasons.

• We can deﬁne a lasso solution explicitly at any given point on the path,
as the solution to an optimization problem (2); we are unable to do this
for the monotone lasso.

• When p < n, both the lasso and monotone-lasso paths end in the unre-
stricted least-squares solution. When p > n, any least squares solution
has zero residuals, with inﬁnitely many solution coeﬃcients β. The lasso
path leads to the unique zero-residual solution having minimum L1 norm.
By construction the monotone lasso path also produces a unique zero-
residual solution in these circumstances, but we are unable to characterize
it further.

4. Forward Stagewise for General Convex Loss Functions

Gradient boosting (Friedman 2001, Hastie et al. 2001) is often used with loss
functions other than squared error; typical candidates are the binomial log-
likelihood or the “Adaboost” loss for binary classiﬁcation problems. Our linear-
model simpliﬁcation is also applicable there. As a concrete example, consider
the linear logistic regression model in the expanded space:

log

Pr(y = 1|˜x)
Pr(y = 0|˜x)

= ˜xT β

= η(˜x).

(21)

(22)

The negative of the binomial log-likelihood is given by

L(β) = −

n

Xi=1

where

[yi log pi + (1 − yi) log(1 − pi)] ,

(23)

pi =

e˜xT
i β

1 + e˜xT
i β

.

(24)

More generally, consider the case where we have a linear model η(˜x) = ˜xT β,
and a loss function of the form

L(β) =

n

Xi=1

l(yi, η(˜xT β)).

(25)

The analog of Algorithm 3 for this general case is given in Algorithm 4.
For the binomial case, the negative gradient in step 2. is −∂L/∂βj = ˜xT

j (y −
p), where y is the 0/1 response vector, and p the vector of ﬁtted probabilities.

T. Hastie et al./The monotone lasso

16

Algorithm 4. Generalized Monotone Incremental Forward Stagewise Regression

1. Start with β1, β2, . . . β2p = 0.
2. Find the predictor xj with largest negative gradient element −∂L/∂βj,

evaluated at the current predictor η.

3. Update βj ← βj + ǫ.
4. Update the predictors η(xi) = ˜xT

i β, and repeat steps 2 and 3 many times

We can apply the same logic used in forward stagewise with squared error
loss in this situation, by using a quadratic approximation to the loss at the
current β0:

L(β) ≈ L(β0) +

∂L

∂β(cid:12)(cid:12)0(β − β0) +

1
2

(β − β0)T ∂2L

∂β∂βT(cid:12)(cid:12)0(β − β0).

The two derivatives in this case are

∂L

∂2L

∂β(cid:12)(cid:12)0 = ˜XT u0
∂β∂βT(cid:12)(cid:12)0 = ˜XT W0 ˜X,

u0
i =

∂l(yi, η)

∂η

W 0

ii =

∂2l(yi, η)

∂η2

i β0,

(cid:12)(cid:12)η=˜xT
(cid:12)(cid:12)η=˜xT

i β0.

(26)

(27)

(28)

(29)

(30)

where

and the diagonal matrix W0 has entries

In the case of logistic regression W 0
probabilities, and u0

i = −(yi − p0

ii = p0

i (1 − p0

i ), where p0

i are the current

i ). Minimizing (26) gives the Newton update

δ = β − β0

= −( ˜XT W0 ˜X)−1 ˜XT u0,

(31)

which can be expressed as the coeﬃcients from a weighted least squares ﬁt of
˜X on −W0−1
Deﬁnition 4. The monotone lasso move direction ρml(β, L) at a point β0,with
expanded data ˜X, and with loss function L is:

u0, with weights W0.

1. Compute ˜XT u0; if all elements are zero, return ρ = 0.
k u0.
2. Establish the active set A of indices for which −˜xT
3. Let ˆδ be the coeﬃcients from a weighted, positive, least squares ﬁt of ˜XA

j u0 = max2p

k=1 −˜xT

on −W0−1

u0, with weights W0.

4. Deﬁne

T. Hastie et al./The monotone lasso

ρj =(cid:26) ˆδj/Pj

ˆδj
0

if j ∈ A
otherwise.

17

(32)

It is easy to check that this deﬁnition coincides with the deﬁnition for squared-
error loss. Unlike there, it will in general not be piecewise constant. We can
expect it to be piecewise smooth, with breaks when the active sets change.
Deﬁnition 5. The monotone lasso coeﬃcient path β(ℓ) for a dataset ˜X =
{X, −X} and loss L is the solution to the diﬀerential equation

∂β
∂ℓ

= ρml(β(ℓ), L),

(33)

with initial condition β(0) = 0.

The deﬁnitions are exactly analogous for generalizations of the lasso.
Unlike for squared error loss, the solution paths are in general piecewise
smooth (but nonlinear), and so eﬃcient exact path algorithms are not available.
Rosset (2005) show that as long as the loss function is quadratic, piecewise
linear or a mixture of both, then the paths will be piecewise linear, and can be
tracked.

For these general convex-loss lasso problems, lasso solutions are always avail-
able at any point along the path. Park & Hastie (2006) develop eﬃcient algo-
rithms for obtaining the lasso path for the generalized linear model family of
loss functions (including logistic regression).

For the monotone lasso and general loss function, we have no exact algo-
rithms for tracking the path, and hence for ﬁnding solutions at any point on the
path. Friedman & Popescu (2004), however, have developed eﬃcient ǫ-stepping
algorithms for ﬁnding forward-stagewise solutions for a variety of losses.

5. Discussion of Criteria

In conducting this research, we had several interesting false starts in terms of
ﬁnding a criterion for forward stagewise. We brieﬂy discuss some of these here.
We saw in the top right panel of Figure 3 on page 12 the residual sum of

squares

RSS(ℓ) =

N

Xi=1(cid:16)yi −

p

Xj=1

xij βj(ℓ)(cid:17)2

for each of the three methods, as a function of their L1 arc-length ℓ. The curve
for the forward stagewise sequence always lies below the curves for the other
two methods. We were able to show a local optimality for forward stagewise in
Theorem 2.

We initially had thought that forward stagewise might enjoy a global opti-

mality criterion like the lasso.

T. Hastie et al./The monotone lasso

18

Candidate criterion 1: For each L1 arc-length ℓ, the forward stagewise coef-
ﬁcient β(ℓ) minimizes RSS(ℓ).

This is true if the lasso paths are monotone, because then the two procedures

coincide, as do L1 arc-length and L1 norm. But in general it is not the case.

Lemma 2. In general there does not exist a coeﬃcient proﬁle that for all ℓ
minimizes RSS(ℓ) over the set of curves having L1 arc-length at most ℓ.

Proof. For any ℓ construct the “unit speed” coeﬃcient path from the origin to
the lasso solution for that ℓ. This has L1 arc-length and L1 norm equal to ℓ, and
hence has the minimum value of RSS(ℓ) over all curves having L1 arc-length ℓ.
Thus any solution to our problem must agree with the lasso solution for all ℓ.
From the right-hand panel of Figure 3, this is not the case when the lasso and
forward stagewise proﬁles are diﬀerent.

∆=(cid:8)β : [0, L] →R2p | β(0) = 0, TV(β, ℓ) ≤ ℓ, ∀ℓ ≤ L, βj(ℓ)non-decreas.(cid:9),

Another attempt at a global formulation of the problem involve used the

integrated loss. Deﬁne the set of monotone-increasing functions

having arc-length at most ℓ up to the point ℓ, for all ℓ ≤ L. Since the class
is monotone, the arc-length is the L1 norm, so we are asking for a monotone
path that simultaneously solves a sequence of lasso problems, subject to that
constraint.

DM
L

 

S
S
R
n
a
e
M
e
v
i
t

 

l

a
u
m
u
C
n

 

i
 

e
c
n
e
r
e

f
f
i

D

0
0
0

.

.

5
0
0
−

.

0
1
0
−

0

20

40

60

80

L1 Arc Length

Fig 4. A simulation provides a counter example to candidate 2. Shown is the diﬀerence
between mean cumulative RSS for the exact solution to criterion 2 and forward stagewise (the
former computed on a discretized set of 40 values for arc-length). Initially forward stagewise
wins, only to be overtaken by the exact solution.

Candidate criterion 2: The forward stagewise algorithm minimizes the inte-

T. Hastie et al./The monotone lasso

19

grated residual sum of squares over the monotone class DM
L :

˜β(L) = argminβ∈DM

L Z L

0

N

Xi=1(cid:16)yi −

p

Xj=1

˜xij βj(ℓ)(cid:17)2

dℓ

(34)

As the integrated loss is a continuous, strictly convex functional on DM
L ,
there exists a unique optimal path that solves (34). However it turns out that
the forward stagewise solution is not always the optimal path.

We computed the exact solution to (34) for our simulation example, at a
discretized sequence of 40 values for arc-length. Figure 4 compares the results
with the forward stagewise solution at these same points. We compute for each
the cumulative mean RSS, and plot their diﬀerence (exact−FS0). In keeping
with its greedy nature, FS0 initially wins, only to be overtaken by the exact
procedure. Hence FS0 does not in general optimize criterion 2.

6. Monotonicity of Proﬁles

We have yet to say how the example of Figure 1 was generated. The data were
generated from the model

Y = sin(6X)/(1 + X) + Z/4,

(35)

with X taking 300 equally spaced values in [0, 1] and Z ∼ N (0, 1). The 10 predic-
tors were piecewise linear basis functions (x−tk)·I(x > tk), for each of the knots
{tk}10
1 = {0.0, 0.1, 0.2, . . . 0.9}. Figure 5 shows the successive approximations to
sin(6x)/(1 + x) by the diﬀerent methods, for ﬁve equally spaced solutions along
their paths. Despite the diﬀerences in their coeﬃcient proﬁles, the ﬁts appear
to be quite similar. The last column of Figure 5 uses piecewise constant basis
functions I(x > tk) in place of the piecewise linear ones (x − tk) · I(x > tk).
Figure 6 shows their coeﬃcient proﬁles. Notice that all proﬁles are monotone,
and hence the proﬁles for all three algorithms coincide.

The fact that they are the same under monotonicity is not a coincidence, and
follows from their deﬁnitions. In that case there are no zero-crossing events, and
then LAR and lasso coincide. In addition, monotonicity means that positive
coeﬃcients are never decreased and vice-versa, hence the non-negative least
squares move in the forward stagewise procedure is the same as the least squares
move in LAR.

Hence it is useful to characterize situations in which the coeﬃcients proﬁles
are monotone. Let X denote the N × p matrix of standardized predictors, and
let XA denote a subset of the columns of X, each multiplied by a set of arbitrary
signs s1, s2, . . . s|A|. Finally, let SA be a diagonal matrix of the sj values. The
results of Efron et al. (2004) show that a necessary and suﬃcient condition for
every path to be monotone is

SA(X T

A XA)−1SA1 ≥ 0 ∀A ⊆ {1, . . . , p}, SA

(36)

T. Hastie et al./The monotone lasso

20

Lasso

Forward Stagewise

LAR

LAR (Haar)

Fig 5. Successive approximations to sin(6x)/(1 + x) for ﬁve equally spaced solutions along
their paths, for the example of Figure 1. The ﬁrst three columns use piecewise linear bases;
the last column uses piecewise constant bases, and the three methods coincide.

T. Hastie et al./The monotone lasso

21

Lasso/FS/LAR (Haar)

s
t

i

n
e
c
i
f
f

e
o
C

4

.

0

2

.

0

0
0

.

2

.

0
−

4

.

0
−

0

4

8

12

16

L1 Norm (Standardized)

Fig 6. Coeﬃcient proﬁles for the same data as Figure 1, except that we have used piecewise
constant basis functions. The coeﬃcients are monotone, and lasso, FS0 and LAR coincide.

In other words, for all subsets of predictors and sign changes of those predictors,
the inverse covariance matrix must be diagonally dominant (this means that
each diagonal element is at least as big as the sum of the of the other elements
in its row).

For the piecewise-linear basis functions, it is easy to ﬁnd a violation of
(36); A = {4, 10, 9} (the ﬁrst three variables entered in Figure 1), with SA =
diag(−1, 1, 1), gives some negative entries. Condition (36) clearly holds for any
orthogonal basis, such as the Haar basis for piecewise constant ﬁts. However,
our piecewise constant basis is not orthogonal. We prove the following theorem
in the appendix.

Theorem 3. Condition (36) holds for piecewise constant bases, and hence the
lasso, FS0 and LAR solutions coincide.

7. Lasso versus Forward Stagewise: Which is Better?

As discussed in Section 2, the current interest in FSǫ is because of its connection
to least squares boosting. By understanding its properties in this simpliﬁed
setting, we hope to learn more about the regularization path of boosting.

The results of this paper show that forward stagewise behaves like a monotone
version of the lasso, and is locally optimal with regard to L1 arc-length. This is
in contrast to the lasso, which is less constrained.

This begs the question: with a large number of predictors, which algorithm is
better? The monotone lasso will tend to slow down the search, not allowing the
sudden changes of direction that can occur with the lasso. Is this a good thing?

T. Hastie et al./The monotone lasso

22

To investigate this, we carried out a simulation study. The data consists of
N = 60 observations on each of p = 1000 (Gaussian) variables, strongly corre-
lated (ρ = 0.95) in groups of 20. The true model has nonzero coeﬃcients for 50
variables, one drawn from each group, and the coeﬃcient values themselves are
drawn from a standard Gaussian. Finally Gaussian noise is added with variance
σ2 = 36, resulting in a noise-to-signal ratio of about 0.72. See Appendix A.3 on
page 28 for more details.

The grouping of the variables is intended to mimic the correlations of nearby
trees in boosting, and with the forward stagewise algorithm, this setup is in-
tended as an idealized version of gradient boosting with shrinkage.

LASSO

Forward Stagewise

s
t

i

n
e
c
i
f
f

i

 

e
o
C
d
e
z
d
r
a
d
n
a
S

t

0
2

0
1

0

0
1
−

0
2
−

0
3
−

s
t

i

n
e
c
i
f
f

i

 

e
o
C
d
e
z
d
r
a
d
n
a
S

t

0
2

0
1

0

0
1
−

0
2
−

0
3
−

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

||β(ℓ)||1/||β(L)||1

||β(ℓ)||1/||β(L)||1

Fig 7. Comparison of lasso and forward stagewise paths on simulated regression data. The
number of samples is 60 and the number of variables 1000. The forward-stagewise paths
ﬂuctuate less than those of lasso in the ﬁnal stages of the algorithms. Both paths are indexed
by L1-norm ||β(ℓ)||1, scaled as a fraction of the L1-norm at the end of the path ||β(L)||1.

Figure 7 shows the coeﬃcient paths for lasso and forward stagewise for a

single realization from this model.

Here the coeﬃcient proﬁles are similar only in the early stages of the paths.
For the later stages, the forward stagewise paths are much smoother — in fact
exactly monotone here — while those for the lasso ﬂuctuate widely. This is due
to the strong correlations among subsets of the variables.

The test-error performance of the two models is rather similar (ﬁgure 8), and
they achieve about the same minimum. In the later stages forward stagewise
takes longer to overﬁt, a likely consequence of the smoother paths. We are using
||β(ℓ)||1 to index both curves for this plot, which would look quite diﬀerent if
instead we used arc-length. Since the forward stagewise path is monotone here,

T. Hastie et al./The monotone lasso

23

ooo
ooo
o
oo
oo
oo

o
ooo
ooo
o
oo

o
oo

r
o
r
r
E
d
e
r
a
u
q
S

n
a
e

M

0
6

0
5

0
4

0
3

0
2

Lasso
Forward Stagewise

ooooooooooooooooooooooooooooooooooooooooooooooo oooooooooooooooooo ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo oooooooo oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo
ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

o
o
o
o
oooo
o
oo
oooo
o

0

20

40

60

80

||β(ℓ)||1

Fig 8. Mean squared error for lasso and forward stagewise on the simulated data. Despite
the diﬀerence in the coeﬃcient paths, the two models perform similarly over the critical part
of the regularization path. In the right tail, lasso appears to overﬁt more rapidly.

the L1 norm is arc-length, so in a sense both MSE proﬁles are measured using
their appropriate index.

On a more theoretical note, Buhlmann (2006) proves consistency of forward
stagewise for high-dimensional linear models. See also Tropp (2004) and Tropp
(2006) for comparisons of lasso and forward stagewise regression.

We conclude that for problems with large numbers of correlated predictors,
the forward stagewise procedure and its associated L1 arc-length criterion might
be preferable to the lasso and L1 norm criterion. This suggests that for general
boosting-type applications, the incremental forward stagewise algorithms which
are currently used, might be preferable to algorithms that try to solve the equiv-
alent lasso problem.

Appendix A: Appendix

A.1. Proof of Theorem 1

Part 1.. The Lagrangian corresponding to (3) is

Xi  yi − β0 −" p
Xj=1

xij β+

j −

p

Xj=1
−

xijβ−

j #!2

p

Xj=1

λ+
j β+

j −

p

p

Xj=1
Xj=1

+ λ

(β+

j + β−
j )

j β−
λ−
j ,

(37)

T. Hastie et al./The monotone lasso

with KKT conditions (for each j):
− xT
j r + λ − λ+
xT
j r + λ − λ−
j β+
λ+
λ−
j β−

j = 0,

j = 0,
j = 0,
j = 0.

24

(38)

(39)

(40)

(41)

deduce the following aspects of the solution:

j=1 xj β−

j −Pp

j i is the residual vector. From these we can
j r = 0∀j, and the solution corresponds to the unrestricted

Here r = y −hPp

j=1 xj β+

1. If λ = 0, then xT

least-squares ﬁt.

2.

j > 0, λ > 0 =⇒ λ+
β+
=⇒ xT
=⇒ λ−
=⇒ β−

j = 0

j r = λ > 0
j > 0
j = 0.

3. Likewise β−

that only one of the pair (β+

j > 0, λ > 0 =⇒ β+
j , β−

j = 0. Hence 2 and 3 give the intuitive result
j ) can be positive at any time.

4. |xT
5. If β+

j r| ≤ λ.
j > 0, then xT

j r = λ, or if β−

j > 0, then −xT

j r = λ

Since β0 is on the lasso path, there exists a λ = λ0 such that β0 = β(λ0). Deﬁne
the active set A to be the set of indices of variables in ˜X with positive coeﬃcients at
λ = λ0, and assume at λ1 = λ0 − ∆ for suitably small ∆ > 0 this set has not changed.
Deﬁne βA(λ) to be the corresponding coeﬃcients at λ. Then from deduction 5

(42)

(43)

Hence

or

˜XT

A(cid:0)y − ˜XAβA(λ)(cid:1) = λ1 for λ ∈ [λ1, λ0].

˜XT
A

˜XA(βA(λ1) − βA(λ0)) = ∆1,
˜XA)−11.

(44)
So while A remains constant, the coeﬃcients βA(λ) change linearly, according to (44).
Since r = y − ˜XAβA(λ0) and ˜XT

βA(λ1) − βA(λ0) = ∆ · ( ˜XT

Ar = λ01 from (42),

A

βA(λ1) − βA(λ0) =

∆
λ0 · ( ˜XT

A

˜XA)−1 ˜XT

Ar

(45)

as claimed.

The active set A will change if a variable “catches up” (in terms of 5), in which case
it is augmented and the direction (44) is recalculated. It will also change if a coeﬃcient
attempts to pass through zero, in which case it is removed from A.

T. Hastie et al./The monotone lasso

25

Part 2.. At each step, the monotone incremental forward stagewise algorithm (Algo-
rithm 3) selects the variable having largest correlation with the residuals, and moves
its coeﬃcient up by ǫ. There may be a set A of variables competing for this maximal
correlation, and a succession of N such moves can be divided up according to the
Nj = ρjN that augmented variable js coeﬃcient. In the limit as ǫ decreases and N
increases such that N ǫ = ε, we can expect an active set A of variables tied in terms
of the largest correlation, and a sequence of moves of total L1 arc-length ε distributed
among this set with proportions ρA. Efron et al. (2004) showed in their Lemma 11
that for suﬃciently small ε, this set would not change. Suppose ˜XT
Ar = c1 for some c,
reﬂecting the equal correlations.

The limiting sequence of moves ερA must have positive components, must main-
tain the equal correlation with the residuals, and subject to these constraints should
decrease the residual sum-of-squares as fast as possible.

Consider the optimization problem

minρ

1
2||r − ε ˜XAρ||2

2 subject to ρj ≥ 0, Pj∈A ρj = 1,

which captures the ﬁrst and third of these requirements. The Lagrangian is

with KKT conditions

L(ρ, γ, λ) =

ρj − 1),

p

1
2||r − ε ˜Xρ||2

2 −

Xj=1

γj ρj + λ(Xj
A(r − ε ˜XAρ) − γ + λ1 = 0

−ε ˜XT

(46)

(47)

(48)

(49)

γj ≥ 0
ρj ≥ 0
γj ρj = 0
Pj ρj = 1.

Here γ is a vector with components γj, j ∈ A. Note that for ρj > 0, γj = 0, and hence
(48) shows that the correlations with the residual remain equal, which was the second
requirement above.

Consider a second optimization problem (the one in the statement of the the theo-

rem):

minθ

1
2||r − ˜XAθ||2

2 subject to θj ≥ 0.

The corresponding KKT conditions are

− ˜XT

A(r − ˜XAθ) − ν = 0

νj ≥ 0
θj ≥ 0
νj θj = 0

(50)

(51)

(52)

We now show that the ˆρ = ˆθ/||ˆθ||1 solves (46) for all ε > 0, where ˆθ is the solution

to (50). From (48) we get

and from (51)

ε2 ˜XT
A

˜XAρ = (λ + εc)1 + γ,

˜XT
A

˜XA ˆθ = c1 + ν.

(53)

(54)

T. Hastie et al./The monotone lasso

ˆθj , we multiply (54) by ε2/s to get

ε2 ˜XT
A

˜XA ˆρ =

ε2
s

(c1 + ν)

= (λ∗ + εc)1 + γ∗,

With s = ||ˆθ||1 =Pj∈A

where

26

(55)

(56)

γ∗
j = νj

λ∗ =

ε2
s
ε2c
s − εc.

It is easy to check, using (52), that (ˆρ, γ∗, λ∗) satisfy (48)-(49).

Variables with ˆθj = 0 may drop out of the active set, since from (48) and (49),
if γj > 0, for ε > 0 their correlation will decrease faster than those with positive
coeﬃcients.

This directions is pursued until a variable not in A “catches up” in terms of corre-
lation, at which point the procedure stops, A is updated, and the direction is recom-
puted.

A.2. Proof of Theorem 3

We need to verify that when using piecewise constant basis functions, (36) holds for
every A and every sign matrix SA.

Suppose we use k piecewise constant basis functions with knots t1 ≤ ··· ≤ tk. Let

n

nj =

I(xi > tj)

Xi=1

be the number of observed x’s to the right of the j-th knot. Without loss of generality,
we assume that each nj > 0, otherwise that predictor contributes nothing to the model
as all observed x’s are either to the right or the left of that knot.

A simple calculation shows that, for i ≤ j, after normalizing the columns of X,

(X tX)ij =r (n − ni)

ni

·

nj

nj − n

,

which is the covariance function of a Brownian bridge (Bs)0≤s≤1, normalized to have
unit variance, at the time points 0 < s1 ≤ ··· ≤ sk < 1

nk−j+1

.

sj =

n

If we can prove that (X tX)−1 is diagonally dominant for every k and every choice of
knots, then, as every principal minor is of exactly the same form (with a smaller k and
fewer knots), we will also have proved that (X t
AXA)−1 is diagonally dominant, hence
that the lasso paths are monotone.

We prove that (X tX)−1 is diagonally dominant by computing (X tX)−1. One way

to compute the (X tX)−1 is to compute the density of

 

Bs1

ps1(1 − s1)

, . . . ,

Bsk

psk(1 − sk)!

T. Hastie et al./The monotone lasso

27

and read oﬀ the inverse from the exponent of the density. Before we turn to computing
the density, we note that

 

Bs1

ps1(1 − s1)

, . . . ,

Bsk

psk(1 − sk)! D

vj =

.

1 − sj

where W is a standard Brownian motion and
sj

=(cid:18) Wv1√v1

, . . . ,

Wsk√vk(cid:19)

It is now simple to show that, up to a constant multiple, the exponent of the density

of

evaluated at (w1, . . . , wk) is

(cid:18) Wv1√v1

, . . . ,

Wvk√vk(cid:19)

w2

s1 +

k

Xj=2

j−1wvj−1 )2

j wvj − v1/2
(v1/2
vj − vj−1

.

Therefore (X tX)−1 has elements

(X tX)−1

j,j = vj(cid:18)

1

+

1

vj+1 − vj(cid:19)

=

(vj − vj−1)(vj+1 − vj )

vj − vj−1
(vj+1 − vj−1)vj
√vj vj−1
vj − vj−1

.

(X tX)−1

j,j−1 = −

Because the oﬀ-diagonal entries of (X tX)−1 are non-positive, we only have to show
that

(X tX)−11 ≥ 0,

as multiplying on the left and right by SA will only increase the entries of the above
vector.

We must therefore prove that for all j,

(vj+1 − vj−1)vj

(vj − vj−1)(vj+1 − vj ) −

√vj vj−1
vj − vj−1 −

√vj+1vj
vj+1 − vj ≥ 0.

By scaling and combining fractions, the above is implied by the following: for every
a < 1 < b

1 − √a ·

b − 1
b − a −

√b ·

1 − a
b − a ≥ 0.

It remains therefore to prove that this inequality holds. However, this is just Jensen’s

inequality: deﬁne a two-point distribution placing mass (b− 1)/(b− a) on √a and mass
(1 − a)/(b − a) on √b. Then, if Z is distributed according to this law:
1 − a
b − a ≤ (E(X 2))1/2 = 1.

E(X) = √a ·

+ √b ·

b − 1
b − a

We note that general conditions for monotonicity can be derived. However it is not
clear how these might be veriﬁed in practice.

T. Hastie et al./The monotone lasso

28

A.3. Simulation Details

Here we give more details of the simulation in Section 7. The regression model has the
form

(57)
where X ∼ N (0, Σ). X has 1000 components, correlated in blocks of size 20. Hence
Σ is a block-diagonal covariance matrix, with 50 blocks Σm each of size 20, and each
block has the (identical) form

Y = Xβ + ǫ,

Σm = (1 − ρ)I20 + ρ11T .

(58)

We used ρ = 0.95 in each block. The 1000-vector β was chosen to be sparse, with
only 50 non-zero entries — one per block. Without loss of generality, we picked the
ﬁrst variable in each block to have a non-zero coeﬃcient, drawn at random from a
standard Gaussian distribution. The noise term ǫ was also Gaussian, with variance
σ2 = 36. N = 60 realizations were drawn from this model. For the noise-to signal
ratio we compute var(ǫ)/var(Xβ). Since X and β have mean zero, the denominator is
E(βT Σβ) = tr[ΣE(ββT )] = 50, hence the ratio is 0.72.

References

Buhlmann, P. (2006), ‘Boosting for high-dimensional linear models’, Annals of

Statistics 34(2), 559–583. MR2281878

Chen, S. S., Donoho, D. L. & Saunders, M. A. (1998), ‘Atomic decomposition

by basis pursuit’, SIAM Journal on Scientiﬁc Computing pp. 33–61.

Donoho, D. & Tsaig, Y. (2006), Fast solution of ℓ1-norm minimization problems

when the solution may be sparse, Technical report, Stanford University.

Efron, B., Hastie, T., Johnstone, I. & Tibshirani, R. (2004), ‘Least angle regres-

sion’, Annals of Statistics 32(2), 407–499. (with discussion).

Friedman, J. (2001), ‘Greedy function approximation: a gradient boosting ma-

chine’, Annals of Statistics 29, 1180. MR1873328

Friedman, J., Hastie, T. & Tibshirani, R. (2000), ‘Additive logistic regression:
a statistical view of boosting (with discussion)’, Annals of Statistics 28, 337–
307.

Friedman, J. & Popescu, B. (2004), Gradient directed regularization, Technical

report, Stanford University.

Hastie, T., Tibshirani, R. & Friedman, J. (2001), The Elements of Statistical
Learning; Data mining, Inference and Prediction, Springer Verlag, New York.
Osborne, M., Presnell, B. & Turlach, B. (2000), ‘A new approach to vari-
able selection in least squares problems’, IMA Journal of Numerical Analysis
20, 389–404.

Park, M.-Y. & Hastie, T. (2006), l1 regularization path algorithms for general-

ized linear models, Technical report, Stanford University.

Rosset, S. (2005), Tracking curved regularized optimization solution paths, in
‘Advances in Neural Information Processing Systems (NIPS*2004)’, Vol. 17,
MIT Press, Cambridge, MA. to appear.

T. Hastie et al./The monotone lasso

29

Rosset, S. & Zhu, J. (2004), ‘Piecewise linear regularized solution paths’, Annals

of Statistics. (to appear 2007).

Schapire, R. E. & Freund, Y. (1997), ‘Boosting the margin: A new explana-
tion for the eﬀectiveness of voting methods’, Proceedings of the Fourteenth
International Conference on Machine Learning .

Schapire, R., Freund, Y., Bartlett, P. & Lee, W. (1998), ‘Boosting the margin: a
new explanation for the eﬀectiveness of voting methods’, Annals of Statistics
26(5), 1651–1686.

Tibshirani, R. (1996), ‘Regression shrinkage and selection via the lasso’, J.

Royal. Statist. Soc. B. 58, 267–288. MR1379242

Tropp, J. (2006), ‘Just relax: convex programming methods for identifying
sparse signals in noise’, IEEE Transactions on Information Theory 52, 1030–
1051. MR2238069

Tropp, J. A. (2004), ‘Greed is good: algorithmic results for sparse approxima-
tion’, IEEE Transactions on Information Theory 50, 2231–2242. MR2097044
Zhao, P. & Yu, B. (2004), Boosted lasso, Technical Report 678, Statistics De-

partment, University of California, Berkeley.

