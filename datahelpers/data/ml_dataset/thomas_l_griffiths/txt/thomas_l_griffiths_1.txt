Cognition 112 (2009) 21–54

Contents lists available at ScienceDirect

Cognition

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / C O G N I T

A Bayesian framework for word segmentation: Exploring the effects
of context
Sharon Goldwater a,*, Thomas L. Grifﬁths b, Mark Johnson c
a School of Informatics, University of Edinburgh, Informatics Forum, 10 Crichton Street, Edinburgh, EH8 9AB, UK
b Department of Psychology, University of California, Berkeley, CA, United States
c Department of Cognitive and Linguistic Sciences, Brown University, United States

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 30 May 2008
Revised 11 March 2009
Accepted 13 March 2009

Keywords:
Computational modeling
Bayesian
Language acquisition
Word segmentation

Since the experiments of Saffran et al. [Saffran, J., Aslin, R., & Newport, E. (1996). Statistical
learning in 8-month-old infants. Science, 274, 1926–1928], there has been a great deal of
interest in the question of how statistical regularities in the speech stream might be used
by infants to begin to identify individual words. In this work, we use computational mod-
eling to explore the effects of different assumptions the learner might make regarding the
nature of words – in particular, how these assumptions affect the kinds of words that are
segmented from a corpus of transcribed child-directed speech. We develop several models
within a Bayesian ideal observer framework, and use them to examine the consequences of
assuming either that words are independent units, or units that help to predict other units.
We show through empirical and theoretical results that the assumption of independence
causes the learner to undersegment the corpus, with many two- and three-word sequences
(e.g. what’s that, do you, in the house) misidentiﬁed as individual words. In contrast, when
the learner assumes that words are predictive, the resulting segmentation is far more accu-
rate. These results indicate that taking context into account is important for a statistical
word segmentation strategy to be successful, and raise the possibility that even young
infants may be able to exploit more subtle statistical patterns than have usually been
considered.

Ó 2009 Elsevier B.V. All rights reserved.

1. Introduction

One of the ﬁrst problems infants must solve as they are
acquiring language is word segmentation:
identifying
word boundaries in continuous speech. About 9% of utter-
ances directed at English-learning infants consist of iso-
lated words (Brent & Siskind, 2001), but there is no
obvious way for children to know from the outset which
utterances these are. Since multi-word utterances gener-
ally have no apparent pauses between words, children
must be using other cues to identify word boundaries. In
fact, there is evidence that infants use a wide range of weak
cues for word segmentation. These cues include phonotac-

* Corresponding author. Tel.: +44 131 651 5609.

E-mail addresses: sgoldwat@inf.ed.ac.uk, sgwater@gmail.com (S. Gold-

water).

0010-0277/$ - see front matter Ó 2009 Elsevier B.V. All rights reserved.
doi:10.1016/j.cognition.2009.03.008

tics (Mattys, Jusczyk, Luce, & Morgan, 1999), allophonic
variation (Jusczyk, Hohne, & Bauman, 1999), metrical
(stress) patterns (Jusczyk, Houston, & Newsome, 1999;
Morgan, Bonamo, & Travis, 1995), effects of coarticulation
(Johnson & Jusczyk, 2001), and statistical regularities in
the sequences of syllables found in speech (Saffran, Aslin,
& Newport, 1996). This last source of information can be
used in a language-independent way, and seems to be used
by infants earlier than most other cues, by the age of
7 months (Thiessen & Saffran, 2003). These facts have
caused some researchers to propose that strategies based
on statistical sequencing information are a crucial ﬁrst step
in bootstrapping word segmentation (Thiessen & Saffran,
2003), and have provoked a great deal of interest in these
strategies (Aslin, Saffran, & Newport, 1998; Saffran, New-
port, & Aslin, 1996; Saffran et al., 1996; Toro, Sinnett, &
Soto-Faraco, 2005). In this paper, we use computational

22

S. Goldwater et al. / Cognition 112 (2009) 21–54

modeling techniques to examine some of the assumptions
underlying much of the research on statistical word
segmentation.

Most previous work on statistical word segmentation
is based on the observation that transitions from one syl-
lable or phoneme to the next tend to be less predictable
at word boundaries than within words (Harris, 1955; Saf-
fran et al., 1996). Behavioral research has shown that in-
fants are indeed sensitive to this kind of predictability, as
measured by statistics such as transitional probabilities
(Aslin et al., 1998; Saffran et al., 1996). This research,
however, is agnostic as to the mechanisms by which in-
fants use statistical patterns to perform word segmenta-
tion. A number of researchers in both cognitive science
and computer science have developed algorithms based
on transitional probabilities, mutual
information, and
similar statistics of predictability in order to clarify how
these statistics can be used procedurally to identify words
or word boundaries (Ando & Lee, 2000; Cohen & Adams,
2001; Feng, Chen, Deng, & Zheng, 2004; Swingley,
2005). Here, we take a different approach: we seek to
identify the assumptions the learner must make about
the nature of language in order to correctly segment nat-
ural language input.

Observations about predictability at word boundaries
are consistent with two different kinds of assumptions
about what constitutes a word: either a word is a unit that
is statistically independent of other units, or it is a unit that
helps to predict other units (but to a lesser degree than the
beginning of a word predicts its end). In most artiﬁcial lan-
guage experiments on word segmentation,
the ﬁrst
assumption is adopted implicitly by creating stimuli
through random (or near-random) concatenation of nonce
words. This kind of random concatenation is often neces-
sary for controlled experiments with human subjects,
and has been useful in demonstrating that humans are
sensitive to the statistical regularities in such randomly
generated sequences. However, it obviously abstracts away
from many of the complexities of natural language, where
regularities exist not only in the relationships between
sub-word units, but also in the relationships between
words themselves. We know that humans are able to use
sub-word regularities to begin to extract words; it is natu-
ral to ask whether attending to these kinds of regularities
is sufﬁcient for a statistical learner to succeed with word
segmentation in a more naturalistic setting. In this paper,
we use computer simulations to examine learning from
natural, rather than artiﬁcial, language input. We ask what
kinds of words are identiﬁed by a learner who assumes
that words are statistically independent, or (alternatively)
by a learner who assumes as well that words are predictive
of later words. We investigate this question by developing
two different Bayesian models of word segmentation
incorporating each of these two different assumptions.
These models can be seen as ideal learners: they are de-
signed to behave optimally given the available input data,
in this case a corpus of phonemically transcribed child-di-
rected speech.

Using our ideal learning approach, we ﬁnd in our ﬁrst
set of simulations that the learner who assumes that words
are statistically independent units tends to undersegment

the corpus, identifying commonly co-occurring sequences
of words as single words. These results seem to conﬂict
with those of several earlier models (Batchelder, 2002;
Brent, 1999; Venkataraman, 2001), where systematic
undersegmentation was not found even when words were
assumed to be independent. However, we argue here that
these previous results are misleading. Although each of
these learners is based on a probabilistic model that de-
ﬁnes an optimal solution to the segmentation problem,
we provide both empirical and analytical evidence that
the segmentations found by these learners are not the opti-
mal ones. Rather, they are the result of limitations imposed
by the particular learning algorithms employed. Further
mathematical analysis shows that undersegmentation is
the optimal solution to the learning problem for any rea-
sonably deﬁned model that assumes statistical indepen-
dence between words.

Moving on to our second set of simulations, we ﬁnd
that permitting the learner to gather information about
word-to-word dependencies greatly reduces the problem
of undersegmentation. The corpus is segmented in a
much more accurate, adult-like way. These results indi-
cate that, for an ideal learner to identify words based on
statistical patterns of phonemes or syllables, it is impor-
tant to take into account that frequent predictable pat-
terns may occur either within words or across words.
This kind of dual patterning is a result of the hierarchical
structure of language, where predictable patterns occur at
many different levels. A learner who considers predict-
ability at only one level (sub-word units within words)
will be less successful than a learner who considers also
the predictability of larger units (words) within their sen-
tential context. The second, more nuanced interpretation
of the statistical patterns in the input leads to better
learning.

depends

crucially

on

segmentation

Our work has important implications for the under-
standing of human word segmentation. We show that suc-
cessful
the
assumptions that the learner makes about the nature of
words. These assumptions constrain the kinds of infer-
ences that are made when the learner is presented with
naturalistic input. Our ideal learning analysis allows us to
examine the kinds of constraints that are needed to suc-
cessfully identify words, and suggests that infants or young
children may need to account for more subtle statistical ef-
fects than have typically been discussed in the literature.
To date, there is little direct evidence that very young lan-
guage learners approximate ideal learners. Nevertheless,
this suggestion is not completely unfounded, given the
accumulating evidence in favor of humans as ideal learners
in other domains or at other ages (Frank, Goldwater, Mans-
inghka, Grifﬁths, & Tenenbaum, 2007; Schulz, Bonawitz, &
Grifﬁths, 2007; Xu & Tenenbaum, 2007). In order to further
examine whether infants behave as ideal learners, or the
ways in which they depart from the ideal, it is important
to ﬁrst understand what behavior to expect from an ideal
learner. The theoretical results presented here provide a
characterization of this behavior, and we hope that they
will provide inspiration for future experimental work
investigating the relationship between human learners
and ideal learners.

S. Goldwater et al. / Cognition 112 (2009) 21–54

23

The remainder of this paper is organized as follows.
First, we brieﬂy review the idea of transitional probabilities
and how they relate to the notion of words, and provide
some background on the probabilistic modeling approach
taken here. We draw a distinction between two kinds of
probabilistic model-based systems – those based on maxi-
mum-likelihood and Bayesian estimation – and argue in fa-
vor of the Bayesian approach. We discuss in some detail
the strengths and weaknesses of the Model-Based Dy-
namic Programming (MBDP-1) system, a Bayesian learner
described by Brent (1999). Next, we introduce our own
Bayesian unigram model and learning algorithm, which
address some of the weaknesses of MBDP-1. We provide
the results of simulations using this model and compare
them to the results of previously proposed models. We
then generalize our unigram modeling results using addi-
tional empirical and theoretical arguments, revealing some
deep mathematical similarities between our unigram mod-
el and MBDP-1. Finally, we extend our model to incorpo-
rate bigram dependencies, present the results of this
bigram model, and conclude by discussing the implications
of our work.

2. Words and transitional probabilities

The question of how infants begin to segment words
from continuous speech has inspired a great deal of re-
search over the years (Jusczyk, 1999). While many differ-
ent cues have been shown to be important, here we focus
on one particular cue: statistical regularities in the se-
quences of sounds that occur in natural language. The
idea that word and morpheme boundaries may be discov-
ered through the use of statistical information is not new,
but originally these methods were seen primarily as ana-
lytic tools for linguists (Harris, 1954, 1955). More re-
cently, evidence that infants are sensitive to statistical
dependencies between syllables has lent weight to the
idea that this kind of information may actually be used
by human learners for early word segmentation (Saffran
et al., 1996; Thiessen & Saffran, 2003). In particular, re-
search on statistical word segmentation has focused on
the notion of transitional probabilities between sub-word
units (e.g., segments or syllables). The transitional proba-
bility from (say) syllable x to syllable y is simply the con-
ditional probability of y given x. In natural language, there
is a general tendency towards lower transitional probabil-
ities at word boundaries than within words (Harris, 1954;
Saffran et al., 1996), a tendency which infants seem able
to exploit in order to segment word-like units from con-
tinuous speech (Aslin et al., 1998; Saffran et al., 1996).
While other cues are also important for word segmenta-
tion, and may in fact take precedence over transitional
probabilities in older infants, transitional probabilities
seem to be one of the earliest cues that infants are able
to use for this task (Johnson & Jusczyk, 2001; Thiessen
& Saffran, 2003).

Much of the experimental work devoted to studying
word segmentation and related linguistic tasks has fo-
cused on exploring the kinds of statistical information
that human learners are or are not sensitive to, e.g., tran-
sitional probabilities vs. frequencies (Aslin et al., 1998),

syllables vs. phonemes (Newport, Weiss, Aslin, & Wonna-
cott,
in preparation), adjacent vs. non-adjacent depen-
dencies (Newport & Aslin, 2004), and the ways in
which transitional probabilities interact with other kinds
of cues (Johnson & Jusczyk, 2001; Thiessen & Saffran,
2003, 2004). In addition, many researchers have explored
the extent to which word segmentation based on transi-
tional probabilities can be viewed as a special case of
more general pattern- or sequence-learning mechanisms
that operate over a range of cognitive domains (Creel,
Newport, & Aslin, 2004; Fiser & Aslin, 2002). A question
that has received less explicit attention is how the notion
of
transitional probabilities relates to the notion of
words. Transitional probabilities are a property of the
boundaries between words (or units within words), but
ultimately it is the words themselves, rather than the
boundaries, that are of interest to the language learner/
user.
It behooves us, then, to consider what possible
properties of words (or, more accurately, word se-
quences) could give rise to the patterns of transitional
probabilities that are typically discussed in the literature,
i.e. lower probabilities at word boundaries and higher
probabilities within words.

Given a lexicon of words, one way that the standard
patterns of transitional probabilities can arise is by choos-
ing words independently at random from the lexicon and
stringing them together to form a sequence. To a ﬁrst
approximation, this is the procedure that is typically used
to generate stimuli for most of the experiments mentioned
above.1 There are many good reasons to generate experi-
mental stimuli in this way, especially when the focus of re-
search is on transitional probabilities: choosing words at
random controls for possible ordering effects and other con-
founds, and leads to simple and systematic patterns of tran-
sitional probabilities. However, there is clearly another way
we could generate a sequence of words, by choosing each
word conditioned on the previous word or words. Depend-
ing on the strength of the word-to-word dependencies, tran-
sitional probabilities between words may be low or high. In
general, if the strength of the dependencies between words
is variable, then in a non-independent sequence of words,
word boundaries will still tend to be associated with lower
transitional probabilities (since many pairs of words will
not be highly dependent). However, there will also be word
boundaries with relatively high transitional probabilities
(where two words are highly associated, as in rubber ducky
or that’s a).

The models that we develop in this paper are designed
to examine the results of making these two different
assumptions about the nature of language: that words
are statistically independent units, or that they are predic-
tive units. In thinking about the differences between learn-
ers making each of these two kinds of assumptions, we
frame the issue in terms of the space of linguistic hypoth-

1 The words in experimental stimuli are never chosen completely
independently, due to restrictions against immediate repetition of words.
When the lexicon is small, this leads to signiﬁcant deviations from
independence. However, as the lexicon size grows, sequences without
repetition will become more and more similar to truly independent
sequences.

24

S. Goldwater et al. / Cognition 112 (2009) 21–54

eses (loosely, grammars) that each learner considers. No-
tice that a learner who assumes that utterances are formed
from sequences of independently chosen words is more re-
stricted than a learner who assumes that words may pre-
dict other words. The second learner is able to learn
grammars that describe either predictive or non-predictive
sequences of words, while the ﬁrst learner can only learn
grammars for non-predictive sequences of words. If words
are truly independent, then the ﬁrst learner may have an
advantage due to the presence of the stronger constraint,
because this learner has a much smaller space of hypothe-
ses to consider. On the other hand, the second learner will
have an advantage in the case where words are not inde-
pendent, because the learner who assumes independence
will never be able to converge on the correct hypothesis.
Before describing our implementation of these two kinds
of learners, we ﬁrst outline our general approach and pro-
vide a summary of related work.

2.1. Probabilistic models for word segmentation

Behavioral work in the vein of Saffran et al. (1996) has
provided a wealth of information regarding the kinds of
statistics human learners are sensitive to, at what ages,
and to what degree relative to other kinds of segmentation
cues. Computational modeling provides a complementary
method of investigation that can be used to test speciﬁc
hypotheses about how statistical information might be
used procedurally to identify word boundaries or what
underlying computational problem is being solved. Using
the terminology of Marr (1982), these two kinds of ques-
tions can be investigated by developing models at (respec-
tively) the algorithmic level or computational level of the
acquisition system. Typically, researchers investigate algo-
rithmic-level questions by implementing algorithms that
are believed to incorporate cognitively plausible mecha-
nisms of information processing. Algorithmic-level ap-
proaches to word segmentation include a variety of
neural network models (Allen et al., 1996; Cairns & Shill-
cock, 1997; Christiansen, Allen, & Seidenberg, 1998; Elman,
1990) as well as several learning algorithms based on tran-
sitional probabilities, mutual information, and similar sta-
tistics (Ando & Lee, 2000; Cohen & Adams, 2001; Feng
et al., 2004; Swingley, 2005) (with most of the latter group
coming from the computer science literature).

In contrast to these proposals, our work provides a
computational-level analysis of the word segmentation
problem. A computational-level approach focuses on
identifying the problem facing the learner and determin-
ing the logic through which it can be solved. For problems
of induction such as those facing the language learner,
probability theory provides a natural
framework for
developing computational-level models. A probabilistic
is a set of declarative mathematical statements
model
specifying the goals of the learning process and the kinds
of information that will be used to achieve them. Of
course, these declarative statements must be paired with
some algorithm that can be used to achieve the speciﬁc
goal, but generally the algorithm is not seen as the focus
of research. Rather, computational-level investigations of-
ten take the form of ideal learner analyses, examining the

behavior of a learner who behaves optimally given the
assumptions of the model.2

Very generally, we can view the goal of a language lear-
ner as identifying some abstract representation of the ob-
served data (e.g., a grammar)
that will allow novel
linguistic input to be correctly interpreted, and novel out-
put to be correctly produced. Many different representa-
tions are logically possible, so the learner must have
some way to determine which representation is most
likely to be correct. Probabilistic models provide a natural
way to make this determination, by creating a probability
distribution over different hypothesized representations
given the observed data. A learner who is able to correctly
identify this posterior distribution over hypotheses can use
this information to process future input and output in an
optimal way (i.e., in a way that is as similar as possible
to the correct generating process – the adult grammar).
Under this view, then, the posterior distribution over
grammars is the outcome of the learning process.

How does the learner go about identifying the posterior
distribution? Bayes’ rule tells us that the probability of a
hypothesized grammar h given the observed data d can
be computed as

P
PðhjdÞ ¼ PðdjhÞPðhÞ
h0 Pðdjh0ÞPðh0Þ

where the sum in the denominator ranges over all hypoth-
eses h0 within the hypothesis space. Here, PðdjhÞ (known as
the likelihood) is the probability of the observed data given
a particular hypothesis, and tells us how well that hypoth-
esis explains the data. PðhÞ (the prior probability of h) tells
us how good a linguistic hypothesis h is, regardless of any
data. The prior can be viewed as a learning bias or measure
of linguistic naturalness: hypotheses with high prior prob-
ability may be adopted based on less evidence than
hypotheses with low prior probability. Bayes’ rule states
that the posterior probability PðhjdÞ is proportional to the
product of the likelihood and the prior, with the denomina-
tor acting as a normalizing constant to ensure that PðhjdÞ
sums to one over all hypotheses. The learner can compute
the posterior probabilities of different hypotheses by eval-
uating each one according to its explanatory power (likeli-
hood) and the learner’s prior expectations.

Bayes’ rule answers the question of how to determine
the posterior probability of a hypothesis if the prior prob-
ability and likelihood are known, but it does not tell us
how to compute those terms in the ﬁrst place. We turn ﬁrst
to the calculation of the likelihood. Typically, the likelihood
is computed by deﬁning a generative model: a probabilistic

2 A note on terminology: the word model unfortunately encompasses
two related but (importantly) distinct senses. It can be used to describe
either (1) a proposal about the nature of learning or its implementation (as
in ‘‘connectionist model”, ‘‘exemplar model”); or (2) a speciﬁc mathemat-
ical statement regarding the process generating a set of data (as in
‘‘probabilistic model”, ‘‘generative model”). A probabilistic model (second
sense) together with its learning algorithm can be viewed as an instance of
a learning model (ﬁrst sense). To avoid confusion, we will generally use the
term ‘‘model” only for the second sense, and the terms ‘‘system” or
‘‘learner” to describe the fully implemented combination of a probabilistic
model and learning algorithm.

S. Goldwater et al. / Cognition 112 (2009) 21–54

25

process for generating the observed data given the hypoth-
esis under consideration. As a simple non-linguistic exam-
ple, imagine the data d consists of the results of 10 coin
ﬂips and we wish to determine the probability of d given
hypotheses h that differ in the probability of ﬂipping heads.
We assume a generative model in which each observation
is the result of ﬂipping the same coin, and that coin always
has probability p of landing on heads independently of any
previous outcomes. The set of hypotheses under consider-
ation consists of all possible values for p. We therefore
have Pðdjh ¼ pÞ ¼ pnHð1   pÞnT , where nH and nT are the
number of heads and tails observed in a particular se-
quence of ﬂips.

2.1.1. Maximum-likelihood estimation

A standard method of using a probabilistic generative
model for learning is to perform maximum-likelihood esti-
mation, i.e., to choose the hypothesis ^h that maximizes
the value of the likelihood function. This is equivalent to
choosing the hypothesis with maximum posterior proba-
bility, assuming a uniform prior distribution over hypoth-
eses with respect to the given parameterization. In the
coin ﬂip example, it is easy to show using elementary cal-
culus that the likelihood is maximized when h ¼ nH
.
nHþnT
Thus, if we observe a sequence consisting of four tails
and six heads, the maximum-likelihood estimate for h is
^h ¼ :6.

For more complex generative models such as those typ-
ically used in language modeling, it is usually impossible to
identify the maximum-likelihood hypothesis analytically.
If the number of possible hypotheses is small, it may be
feasible to explicitly compute the likelihood for each possi-
ble hypothesis and choose the best one. However, in gen-
eral, it will be necessary to design some sort of algorithm
for searching through the space of hypotheses without
evaluating all of them. The ideal algorithm would be one

that is guaranteed to ﬁnd the globally optimal hypothesis.
In many cases, however, approximate search algorithms are
used. These algorithms generally work by seeking the opti-
mal hypothesis within some local region of the search
space. Approximate search algorithms may be used for
practical reasons (when an exact procedure is not known,
as in Anderson (1991)) or for theoretical reasons (if the re-
searcher wishes to incorporate particular assumptions
about human learning, as in Sanborn, Grifﬁths, & Navarro
(2006)). In either case, certain hypotheses are excluded
from consideration by the algorithm itself. Consequently,
the use of an approximate search procedure can make a
purely computational-level analysis difﬁcult. The kinds of
generalizations made by the learner are determined both
by the explicit constraints speciﬁed by the probabilistic
model, and the implicit constraints speciﬁed by the search
procedure. Examples of this type of learning system are de-
scribed by Venkataraman (2001) and Batchelder (2002).
The models underlying these systems are very similar;
we describe only Venkataraman’s work in detail.

Venkataraman proposes a method of word segmenta-
tion based on maximum-likelihood estimation. He dis-
cusses three different generative models of increasing
complexity; we focus our analysis on the simplest of these,
although our argument can be extended to all three. This
model is a standard unigram model, i.e., it assumes that
words are generated independently at
random. The
observed data consists of a corpus of phonemically tran-
scribed child-directed speech, where utterance boundaries
(corresponding to pauses in the input) are known, but
word boundaries are unknown (see Fig. 1). The probabilis-
tic model underlying this system describes how to gener-
ate a corpus given U, the number of utterances in the
corpus; the distinguished symbol $, which is used to mark
utterance boundaries; and R, the phonemic symbol alpha-
bet (which does not include $):

Fig. 1. An excerpt from the beginning of the corpus used as input to Venkataraman’s (2001) word segmentation system, showing (a) the actual input corpus
and (b) the corresponding standard orthographic transcription. The corpus was originally prepared by Brent and Cartwright (1996) using data from
Bernstein-Ratner (1987), and was also used as input to Brent’s (1999) MBDP-1 system.

26

S. Goldwater et al. / Cognition 112 (2009) 21–54

Repeat U times:

Repeat until $ is generated:
1. Generate the next word, w, with probability PwðwÞ.
2. Generate $ with probability p$.

where Pw is some probability distribution over Rþ, the set
of all possible words. As each word is generated, it is con-
catenated onto the previously generated sequence of
words. No boundary marker is added unless the end-of-
utterance marker, $, is generated. Under this model, the
probability of generating words w1 . . . wn as a single utter-
ance is

Pðw1 . . . wn$Þ ¼

PwðwnÞp$

"
Y

n 1

#
PwðwiÞð1   p$Þ
Y

i¼1
¼ p$
1   p$

n

i¼1

PwðwiÞð1   p$Þ

ð1Þ

and the probability of generating the unsegmented utter-
ance u is found by summing over all possible sequences
of words that could be concatenated to form u:
PðuÞ ¼

Pðw1 . . . wn$Þ

X

w1...wn¼u

The probability of the entire corpus is the product of the
probabilities of the individual utterances. The hypothesis
space for this model consists of all the possible assign-
ments of probability values to words and the utterance
i.e., possible values for p$ and the
boundary marker,
parameters of Pw.

Notice that in the hypothesis space just deﬁned, some
choices of Pw may assign non-zero probability to only a ﬁ-
nite subset of potential words. Different hypotheses will
have different sizes, i.e., different numbers of words will
have non-zero probability. Crucially, however, no prefer-
ence is given to hypotheses of any particular size – the
maximum-likelihood assumption states that we should
choose whichever hypothesis assigns the highest probabil-
ity to the observed data.

What is the maximum-likelihood hypothesis under this
model? It is straightforward to show that, in general, the
maximum-likelihood solution for a model is the probabil-
ity distribution that is closest to the empirical distribution
(relative frequencies) of observations in the corpus, where
the ‘‘distance” is computed by an information theoretic
measure called the Kullback–Leibler divergence (Bishop,
2006, p. 57, inter al.). In the above model, there is one
hypothesis that is able to match the empirical distribution
of the corpus exactly. This hypothesis treats each utterance
as a single ‘‘word”, with probability equal to the empirical
probability of that utterance in the corpus, and assumes
p$ ¼ 1. In other words, this solution memorizes the entire
data set without segmenting utterances at all, and assigns
zero probability to any unobserved utterance. Intuitively,
any solution that does hypothesize word boundaries will
require p$ < 1, which means that some unobserved utter-
ances will have non-zero probability – those that can be
created by, for example, concatenating two observed utter-
ances, or rearranging the hypothesized words into novel

orderings. Since some probability mass is allocated to
these unobserved utterances, the probability of the ob-
served data must be lower than in the case where p$ ¼ 1
and no generalization is possible.

For a maximum-likelihood learner using the model in
Eq. (1), then, only a trivial segmentation will be found un-
less some constraint is placed on the kinds of hypotheses
that are considered. Crucially, however, this argument
does not depend on the particular form of Pw used in Eq.
(1), where words are assumed to be generated indepen-
dent of context. Many other possible distributions over
words would yield the same result. Venkataraman, for
example, presents two other models in which the unigram
distribution Pw is replaced with a bigram or trigram distri-
bution: rather than generating words independent of con-
text, each word is generated conditioned on either one or
two previous words. That is, the bigram model deﬁnes

"
Y
Pðw1 . . . wn$Þ ¼ Pðw1j$Þ

n

#
Pð$jwnÞ
Pðwijwi 1Þ

i¼2

lexical

Essentially, the reason that all of these models yield the
same maximum-likelihood solution (an unsegmented cor-
pus) is that they are allowed to consider hypotheses with
arbitrary numbers of
items. When comparing
hypotheses with different levels of complexity (corre-
sponding here to the number of word types in the hypoth-
esis), a maximum-likelihood learner will generally prefer a
more complex hypothesis over a simple one. This leads to
the problem of overﬁtting, where the learner chooses a
hypothesis that ﬁts the observed data very well, but gener-
alizes very poorly to new data. In the case of word segmen-
tation, the solution of complete memorization allows the
learner to ﬁt the observed data perfectly. Since we know
that this is not the solution found by Venkataraman’s
learners, we must conclude that the algorithm he proposes
to search the space of possible hypotheses must be impos-
ing additional constraints beyond those of the models
themselves. It should be clear from the previous discussion
that this is not the approach advocated here, since it ren-
ders constraints implicit and difﬁcult to examine. Batchel-
der’s (2002) maximum-likelihood learning system uses an
explicit ‘‘external constraint” to penalize lexical items that
are too long. This approach is a step in the right direction,
but is less mathematically principled than Bayesian model-
ing,
in which a (non-uniform) prior distribution over
hypotheses is used within the model itself to constrain
learning. We now review several of the Bayesian models
that served as inspiration for our own work.

2.1.2. Bayesian models

In the previous section, we argued that unconstrained
maximum-likelihood estimation is a poor way to choose
between hypotheses with different complexities. In Bayes-
ian modeling, the effect of the likelihood can be counter-
balanced by choosing a prior distribution that favors
simpler hypotheses. Simpler hypotheses will tend not to
ﬁt the observed data as well, but will tend to generalize
more successfully to novel data. By considering both the
likelihood and prior in determining the posterior probabil-
ity of each hypothesis, Bayesian learners naturally avoid

S. Goldwater et al. / Cognition 112 (2009) 21–54

27

the kind of overﬁtting that maximum-likelihood learners
encounter. The trade-off between ﬁt and generalization
will depend on exactly how the prior is deﬁned; we now
describe several methods that have been used to deﬁne
priors in previous Bayesian models.

Perhaps the most well-known framework for deﬁning
Bayesian models is known as minimum description length
(MDL) (Rissanen, 1989), and is exempliﬁed by the work
of de Marcken (1995) and Brent and Cartwright (1996).
MDL is a particular formulation of Bayesian learning that
has been used successfully in a number of other areas of
language acquisition as well (Creutz & Lagus, 2002; Dow-
man, 2000; Ellison, 1994; Goldsmith, 2001; Goldwater &
Johnson, 2004). The basic idea behind MDL is to deﬁne
some encoding scheme that can be used to encode the cor-
pus into a more compact representation. In word segmen-
tation, for example, a code might consist of a list of lexical
items along with a binary representation for each one.
With appropriate choices for the lexical items and binary
representations (with shorter representations assigned to
more common words), the length of the corpus could be
reduced by replacing each word with its binary code. In
this framework, the learner’s hypotheses are different pos-
sible encoding schemes. The minimum description length
principle states that the optimal hypothesis is the one that
minimizes the combined length, in bits, of the hypothesis it-
self (the codebook) and the encoded corpus. Using results
from information theory, it can be shown that choosing a
hypothesis using the MDL principle is equivalent to choos-
ing the maximum a posteriori (MAP) hypothesis – the
hypothesis with the highest posterior probability – under
a Bayesian model where the prior probability of a hypoth-
esis decreases exponentially with its length.
In other
words, MDL corresponds to a particular choice of prior dis-
tribution over hypotheses, where hypotheses are preferred
if they can be described more succinctly.

Although MDL models can in principle produce good
word segmentation results, there are no standard search
algorithms for these kinds of models, and it is often difﬁ-
cult to design efﬁcient model-speciﬁc algorithms. For
example, Brent and Cartwright (1996) were forced to limit
their analysis to a very short corpus (about 170 utterances)
due to efﬁciency concerns. In later research, Brent devel-
oped another Bayesian model for word segmentation with
a more efﬁcient search algorithm (Brent, 1999). He named
this system Model-Based Dynamic Programming (MBDP-
1).3 Since we will be returning to this model at various
points throughout this paper, we now describe MBDP-1 in
more detail.

Unlike models developed within the MDL framework,
where hypotheses correspond to possible encoding meth-
ods, MBDP-1 assumes that the hypotheses under consider-
ation are actual sequences of words, where each word is a
sequence of phonemic symbols. The input corpus consists
of phonemically transcribed utterances of child-directed
speech, as in Fig. 1. Some word sequences, when concate-
nated together to remove word boundaries, will form ex-

3 The 1 in MBDP-1 was intended as a version number, although Brent

never developed any later versions of the system.

actly the string of symbols found in the corpus, while
others will not. The probability of the observed data given
a particular hypothesized sequence of words will therefore
either be equal to 1 (if the concatenated words form the
corpus) or 0 (if not). Consequently, only hypotheses that
are consistent with the corpus must be considered. For
each possible segmentation of the corpus, the posterior
probability of that segmentation will be directly propor-
tional to its prior probability. The prior probability, in turn,
is computed using a generative model. This model assumes
that the sequence of words in the corpus was created in a
sequence of four steps:4

Step 1: Generate the number of types that will be in the

lexicon.

Step 2: Generate a token frequency for each lexical type.
Step 3: Generate the phonemic representation of each
type (except for the single distinguished ‘‘utter-
ance boundary” type, $).

Step 4: Generate an ordering for the set of tokens.

Each step in this process is associated with a probability
distribution over the possible outcomes of that step, so
together these four steps deﬁne the prior probability distri-
bution over all possible segmented corpora. We discuss the
speciﬁc distributions used in each step in Appendix B; here
it is sufﬁcient to note that these distributions tend to
assign higher probability to segmentations containing
fewer and shorter lexical items, so that the learner will
prefer to split utterances into words.

To search the space of possible segmentations of the
corpus, Brent develops an efﬁcient online algorithm. The
algorithm makes a single pass through the corpus, seg-
menting one utterance at a time based on the segmenta-
tions found for all previous utterances. The online nature
of this algorithm is intended to provide a more realistic
simulation of human word segmentation than earlier batch
learning algorithms (Brent & Cartwright, 1996; de Marc-
ken, 1995), which assume that the entire corpus of data
is available to the learner at once (i.e., the learner may iter-
ate over the data many times).

In the remainder of this paper, we will describe two
new Bayesian models of word segmentation inspired, in
part, by Brent’s work. Like Brent, we use a generative mod-
el-based Bayesian framework to develop our learners.
Moreover, as we prove in Appendix B, our ﬁrst (unigram)
model is mathematically very similar to the MBDP-1 mod-
el. However, our work differs from Brent’s in two respects.
First, our models are more ﬂexible, which allows us to
more easily investigate the effects of different modeling
assumptions. In theory, each step of Brent’s model can be
individually modiﬁed, but in practice the mathematical
statement of the model and the approximations necessary
for the search procedure make it difﬁcult to modify the
model in any interesting way. In particular, the fourth step
assumes a uniform distribution over orderings, which

4 Our presentation involves a small change from Brent (1999), switching
the order of Steps 2 and 3. This change makes no difference to the model,
but provides a more natural grouping of steps for purposes of our analysis
in Appendix B.

28

S. Goldwater et al. / Cognition 112 (2009) 21–54

creates a unigram constraint that cannot easily be changed.
We do not suppose that Brent was theoretically motivated
in his choice of a unigram model, or that he would be op-
posed to introducing word-to-word dependencies, merely
that the modeling choices available to him were limited
by the statistical techniques available at the time of his
work. In this paper, we make use of more ﬂexible recent
techniques that allow us to develop both unigram and bi-
gram models of word segmentation and explore the differ-
ences in learning that result.

The second key contribution of this paper lies in our fo-
cus on analyzing the problem of word segmentation at the
computational level by ensuring, to the best of our ability,
that the only constraints on the learner are those imposed
by the model itself. We have already shown that the mod-
el-based approaches of Venkataraman (2001) and Batchel-
der (2002) are constrained by their choice of search
algorithms; in the following section we demonstrate that
the approximate search procedure used by Brent (1999)
prevents his learner, too, from identifying the optimal
solution under his model. Although in principle one could
develop a Bayesian model within the MDL or MBDP frame-
works that could account for word-to-word dependencies,
the associated search procedures would undoubtedly be
even more complex than those required for the current
unigram models, and thus even less likely to identify opti-
mal solutions. Because our own work is based on more re-
cent Bayesian techniques, we are able to develop search
procedures using a standard class of algorithms known as
Markov chain Monte Carlo methods (Gilks, Richardson, &
Spiegelhalter, 1996), which produce samples from the pos-
terior distribution over hypotheses. We provide evidence
that the solutions identiﬁed by our algorithms are indeed
optimal or near-optimal, which allows us to draw conclu-
sions using ideal observer arguments and to avoid the
obfuscating effects of ad hoc search procedures.

3. Unigram model

3.1. Generative model

Like MBDP-1, our models assume that the hypotheses
under consideration by the learner are possible segmenta-
tions of the corpus into sequences of words. Word se-
quences that are consistent with the corpus have a
likelihood of 1, while others have a likelihood of 0, so the
posterior probability of a segmentation is determined by
its prior probability. Also as in MBDP-1, we compute the
prior probability of a segmentation by assuming that the
sequence of words in the segmentation was created
according to a particular probabilistic generative process.
Let w ¼ w1 . . .w N be the words in the segmentation. Set-
ting aside the complicating factor of utterance boundaries,
our unigram model assumes that the ith word in the se-
quence, wi, is generated as follows:

(1) Decide if wi is a novel lexical item.
(2) a.
x1 . . .x M) for wi.

If so, generate a phonemic form (phonemes

b. If not, choose an existing lexical form ‘ for wi.

We assign probabilities to each possible choice as follows:

Q
, Pðwi is not novelÞ ¼ n
(1) Pðwi is novelÞ ¼ a0
nþa0
nþa0
(2) a. Pðwi ¼ x1 . . .x M j wi is novelÞ ¼
j¼1PðxjÞ
b. Pðwi ¼ ‘ j wi is not novelÞ ¼ n‘

p#ð1   p#ÞM 1

M

n

where a0 is a parameter of the model, n is the number of
previously generated words (¼ i   1), n‘ is the number of
times lexical item ‘ has occurred in those n words, and p#
is the probability of generating a word boundary. Taken
together, these deﬁnitions yield the following distribution
over wi given the previous words w i ¼ fw1 . . .w i 1g:
Pðwi ¼ ‘jw iÞ ¼

þ a0P0ðwi ¼ ‘Þ
i   1 þ a0

i   1 þ a0

ð2Þ

n‘

where we use P0 to refer to the unigram phoneme distribu-
tion in Step 2a. (The p# and 1   p# factors in this distribu-
tion result from the process used to generate a word from
constituent phonemes: after each phoneme is generated, a
word boundary is generated with probability p# and the
process ends, or else no word boundary is generated with
probability 1   p# and another phoneme is generated.)

We now provide some intuition for the assumptions
that are built into this model. First, notice that in Step 1,
when n is small, the probability of generating a novel
lexical item is relatively large. As more word tokens are
generated and n increases, the relative probability of
generating a novel item decreases, but never disappears
entirely. This part of the model means that segmentations
with too many different lexical items will have low proba-
bility, providing pressure for the learner to identify a seg-
mentation consisting of relatively few lexical items. In
Step 2a, we deﬁne the probability of a novel lexical item
as the product of the probabilities of each of its phonemes.
This ensures that very long lexical items will be strongly
dispreferred. Finally, in Step 2b, we say that the probability
of generating an instance of the lexical item ‘ is propor-
tional to the number of times ‘ has already occurred. In ef-
fect, the learner assumes that a few lexical items will tend
to occur very frequently, while most will occur only once
or twice. In particular, the distribution over word frequen-
cies produced by our model becomes a power-law distri-
bution for large corpora (Arratia, Barbour, & Tavare,
1992), the kind of distribution that is found in natural lan-
guage (Zipf, 1932).

The model we have just described is an instance of a
kind of model known in the statistical literature as a Dirich-
let process (Ferguson, 1973). The Dirichlet process is com-
monly used in Bayesian statistics as a non-parametric
prior for clustering models, and is closely related to Ander-
son’s (1991) rational model of categorization (Sanborn
et al., 2006). The Dirichlet process has two parameters:
the concentration parameter a0 and the base distribution
P0. The concentration parameter determines how many
clusters will typically be found in a data set of a particular
size (here, how many word types for a particular number
of tokens), and the base distribution determines the typical
characteristics of a cluster (here, the particular phonemes
in a word type). A more detailed mathematical treatment

S. Goldwater et al. / Cognition 112 (2009) 21–54

29

of our model and its relationship to the Dirichlet process is
provided in Appendix A, but this connection leads us to re-
fer to our unigram model of word segmentation as the
‘‘Dirichlet process” (DP) model.

So far, the model we have described assigns probabili-
ties to sequences of words where there are no utterance
boundaries. However, because the input corpus contains
utterance boundaries, we need to extend the model to ac-
count for them. In the extended model, each hypothesis
consists of a sequence of words and utterance boundaries,
and hypotheses are consistent with the input if removing
word boundaries (but not utterance boundaries) yields
the input corpus. To compute the probability of a sequence
of words and utterance boundaries, we assume that this
sequence was generated using the model above, with the
addition of an extra step: after each word is generated,
an utterance boundary marker $ is generated with proba-
bility p$ (or not, with probability 1   p$). For simplicity,
we will suppress this portion of the model in the main
body of this paper, and refer the reader to Appendix A for
full details.

3.2. Inference

We have now deﬁned a generative model that allows
us to compute the probability of any segmentation of the
input corpus. We are left with the problem of inference,
or actually identifying the highest probability segmenta-
tion from among all possibilities. We used a method
known as Gibbs sampling (Geman & Geman, 1984), a type
of Markov chain Monte Carlo algorithm (Gilks et al.,
1996) in which variables are repeatedly sampled from
their conditional posterior distribution given the current
values of all other variables in the model. Gibbs sampling
is an iterative procedure in which (after a number of
iterations used as a ‘‘burn-in” period to allow the sam-
pler to converge) each successive iteration produces a
sample from the full posterior distribution PðhjdÞ.
In
our sampler, the variables of interest are potential word
boundaries, each of which can take on two possible val-
ues, corresponding to a word boundary or no word
boundary. Boundaries may be initialized at random or
using any other method; initialization does not matter
since the sampler will eventually converge to sampling
from the posterior distribution.5 Each iteration of the
sampler consists of
stepping through every possible
boundary location and resampling its value conditioned
on all other current boundary placements. Since each set
of assignments to the boundary variables uniquely deter-
mines a segmentation, sampling boundaries is equivalent
to sampling sequences of words as our hypotheses.
Although Gibbs sampling is a batch learning algorithm,
where the entire data set is available to the learner at
once, we note that there are other sampling techniques
known as particle ﬁlters
(Doucet, Andrieu, & Godsill,
2000; Sanborn et al., 2006) that can be used to produce

5 Of course, our point that initialization does not matter is a theoretical
one; in practice, some initializations may lead to faster convergence than
others, and checking that different initializations lead to the same results is
one way of testing for convergence of the sampler, as we do in Appendix A.

approximations of the posterior distribution in an online
fashion (examining each utterance in turn exactly once).
We return in the General Discussion to the question of
how a particle ﬁlter might be developed for our own
model in the future. Full details of our Gibbs sampling
algorithm are provided in Appendix A.

3.3. Simulations

3.3.1. Data

To facilitate comparison to previous models of word
segmentation, we report results on the same corpus used
by Brent (1999) and Venkataraman (2001). The data is de-
rived from the Bernstein–Ratner corpus (Bernstein-Rat-
ner, 1987) of the CHILDES database (MacWhinney &
Snow, 1985), which contains orthographic transcriptions
of utterances directed at 13- to 23-month-olds. The data
was post-processed by Brent, who removed disﬂuencies
and non-words, discarded parental utterances not direc-
ted at the children, and converted the rest of the words
into a phonemic representation using a phonemic dictio-
nary (i.e. each orthographic form was always given the
same phonemic form). The resulting corpus contains
9790 utterances, with 33,399 word tokens and 1321 un-
ique types. The average number of words per utterance
is 3.41 and the average word length (in phonemes) is
2.87. The word boundaries in the corpus are used as the
gold standard for evaluation, but are not provided in the
input to the system (except for word boundaries that
are also utterance boundaries).

The process used to create this corpus means that it is
missing many of the complexities of real child-directed
speech. Not the least of these is the acoustic variability
with which different tokens of the same word are pro-
duced, a factor which presumably makes word segmenta-
tion more difﬁcult. On the other hand, the corpus is also
missing many cues which could aid in segmentation, such
as coarticulation information, stress, and duration. While
this idealization of child-directed speech is somewhat
unrealistic, the corpus does provide a way to investigate
the use of purely distributional cues for segmentation,
and permits direct comparison to other word segmenta-
tion systems.

3.3.2. Evaluation procedure

For quantitative evaluation, we adopt the same mea-
sures used by Brent (1999) and Venkataraman (2001): pre-
cision (number of correct items found out of all items
found) and recall (number of correct items found out of
all correct items). These measures are widespread in the
computational linguistics community; the same measures
are often known as accuracy and completeness in the cogni-
tive science community (Brent & Cartwright, 1996; Chris-
tiansen et al., 1998). We also report results in terms of
F0 (another common metric used in computational lin-
guistics, also known as F-measure or F-score). F0 is the geo-
metric average of precision and recall, deﬁned as
2  precision
, and penalizes results where precision and
recall are very different. We report the following scores
for each model we propose:

precisionþrecall

recall

30

S. Goldwater et al. / Cognition 112 (2009) 21–54

aries must be correctly identiﬁed to count as correct.

 P, R, F: precision, recall, and F0 on words: both bound-
 LP, LR, LF: precision, recall, and F0 on the lexicon, i.e.
 BP, BR, BF: precision, recall, and F0 on potentially ambig-
uous boundaries (i.e. utterance boundaries are not
included in the counts).

word types.

As an example, imagine a one-utterance corpus whose cor-
rect segmentation is look at the big dog there, where
instead we ﬁnd the segmentation look at the bigdo g
the re. There are seven words in the found segmentation,
and six in the true segmentation; three of these words
match. We report all scores as percentages, so P = 42.9%
(3/7), R = 50.0% (3/6), and F = 46.2%. Similarly, BP = 66.7%
(4/6), BR = 80.0% (4/5), BF = 72.7%,
LP = 50.0% (3/6),
LR = 50.0% (3/6), and LF = 50.0%. Note that if the learner
correctly identiﬁes all of the boundaries in the true solu-
tion, but also proposes extra boundaries (oversegmenta-
tion), then boundary recall will reach 100%, but boundary
precision and boundary F0 will be lower. Conversely, if
the learner proposes no incorrect boundaries, but fails to
identify all of the true boundaries (undersegmentation),
then boundary precision will be 100%, but boundary recall
and F0 will be lower. In either case, scores for word tokens
and lexical items will be below 100%.

For comparison, we report scores as well for Brent’s
MBDP-1 system (Brent, 1999) and Venkataraman’s n-gram
segmentation systems (Venkataraman, 2001), which we
will refer to as NGS-u and NGS-b (for the unigram and bi-
gram models). Both Brent and Venkataraman use online
search procedures (i.e., their systems make a single pass
through the data, segmenting each utterance in turn), so
in their papers they calculate precision and recall sepa-
rately on each 500-utterance block of the corpus and graph
the results to show how scores change as more data is pro-
cessed. They do not report lexicon recall or boundary pre-
cision and recall. Their results are rather noisy, but
performance seems to stabilize rapidly, after about 1500
utterances. To facilitate comparison with our own results,
we calculated scores for MBDP-1 and NGS over the whole
corpus, using Venkataraman’s implementations of these
algorithms.6

Since our algorithm produces random segmentations
sampled from the posterior distribution rather than a sin-
gle optimal solution, there are several possible ways to
evaluate its performance. For most of our simulations, we
evaluated a single sample taken after 20,000 iterations.
We used a method known as simulated annealing (Aarts &
Korst, 1989) to speed convergence of the sampler, and in
some cases (noted below) to obtain an approximation of
the MAP solution by concentrating samples around the
mode of the posterior. This allowed us to examine possible
differences between a random sample of the posterior and
a sample more closely approximating the MAP segmenta-
tion. Details of the annealing and MAP approximation pro-
cedures can be found in Appendix A.

6 The implementations are available at http://www.speech.sri.com/

people/anand/.

(a) Varying p# with α

0 = 20

0.3

0.5

value of p#

0.7

0.9

(b) Varying α

0 with p# = .5

60%

55%

50%

60%

55%

50%

LF
F
0.1

LF
F

1

2

5

10

50
20
value of α
0

100 200

500

Fig. 2. F0 for words (F) and lexical items (LF) in the DP model (a) as a
function of p#, with a0 ¼ 20 and (b) as a function of a0, with p# ¼ :5.

3.3.3. Results and discussion

The DP model we have described has two free parame-
ters: p# (the prior probability of a word boundary), and a0
(which affects the number of word types proposed).7 Fig. 2
shows the effects of varying of p# and a0. Lower values of p#
result in more long words, which tends to improve recall
(and thus F0) in the lexicon. The accompanying decrease in
token accuracy is due to an increasing tendency for the mod-
el to concatenate short words together, a phenomenon we
discuss further below. Higher values of a0 allow more novel
words, which also improves lexicon recall, but begins to de-
grade precision after a point. Due to the negative correlation
between token accuracy and lexicon accuracy, there is no
single best value for either p# or a0. In the remainder of this
section, we focus on the results for p# ¼ :5; a0 ¼ 20 (though
others are qualitatively similar – we discuss these brieﬂy
below).

In Table 1, we compare the results of our system to
those of MBDP-1 and NGS-u. Although our system has
higher lexicon accuracy than the others, its token accuracy
is much worse. Performance does not vary a great deal be-
tween different samples, since calculating the score for a
single sample already involves averaging over many ran-
dom choices – the choices of whether to place a boundary
at each location or not. Table 2 shows the mean and stan-
dard deviation in F0 scores and posterior probabilities over
samples taken from 10 independent runs of the algorithm
with different random initializations. The same statistics

7 The DP model actually contains a third free parameter, q, used as a
prior over the probability of an utterance boundary (see Appendix A). Given
the large number of known utterance boundaries, the value of q should
have little effect on results, so we simply ﬁxed q ¼ 2 for all simulations.

S. Goldwater et al. / Cognition 112 (2009) 21–54

Table 1
Word segmentation accuracy of unigram systems.

Model

Performance measure

NGS-u
MBDP-1
DP

P

67.7
67.0
61.9

R

70.2
69.4
47.6

F

68.9
68.2
53.8

BP

80.6
80.3
92.4

BR

84.8
84.3
62.2

BF

82.6
82.3
74.3

LP

52.9
53.6
57.0

LR

51.3
51.3
57.5

31

LF

52.0
52.4
57.2

Note: P, R, and F are precision, recall, and F0 for word tokens; BP, LP, etc. are the corresponding scores for ambiguous boundaries and lexical items. Best
scores are shown in bold. DP results are with p# ¼ :5 and a0 ¼ 20.

Table 2
Results of the DP model, averaged over multiple samples.

Samples (10 runs)
Samples (1 run)
MAP approx.

F

53.9 (.32)
53.5 (.07)
53.7 (.26)

LF

57.8 (.60)
57.7 (.43)
58.7 (.56)

  log PðwÞ
200,587 (192)
200,493 (25)
199,853 (228)

Note: Token F0 (F), lexicon F0 (LF), and negative log posterior probability
were averaged over 10 samples from independent runs of our Gibbs
sampler, over 10 samples from a single run, and over 10 samples from
independent runs of our MAP approximation (see text). Standard devia-
tions are shown in parentheses.

are also provided for ten samples obtained from a single
run of the sampler. Samples from a single run are not inde-
pendent, so to reduce the amount of correlation between
these samples they were taken at 100-iteration intervals
(at iterations 19100,19200, . . .,20000). Nevertheless, they
show less variability than the truly independent samples.
In both cases, lexicon accuracy is more variable than token
accuracy, probably because there are far fewer lexical
items to average over within a single sample. Finally, Table
2 provides results for the approximate MAP evaluation
procedure. This procedure is clearly imperfect, since if it
were able to identify the true MAP solution, there would
be no difference in results across multiple runs of the algo-
rithm. In fact, compared to the standard sampling proce-
dure, there is only slightly less variation in F0 scores, and
greater variation in probability.8 Nevertheless, the MAP
approximation does succeed in ﬁnding solutions with signif-
icantly higher probabilities. These solutions also have higher
lexicon accuracy, although token accuracy remains low.

The reason that token accuracy is so low with the DP
model is that it often mis-analyzes frequently occurring
words. Many instances of these words occur in common
collocations such as what’s that and do you, which the sys-
tem interprets as a single words. This pattern of errors is
apparent in the boundary scores: boundary precision is
very high, indicating that when the system proposes a
boundary, it is almost always correct. Boundary recall is
low, indicating undersegmentation.

We analyzed the behavior of the system more carefully
by examining the segmented corpus and lexicon. A full 30%
of the proposed lexicon and nearly 30% of tokens consist of

8 The large standard deviation in the probabilities of the approximate
MAP solutions is due to a single outlier. The standard deviation among the
remaining nine solutions is 160, well below the standard deviation in the
sample solutions, where there are no outliers.

undersegmentation (collocation) errors, while only 12% of
types and 5% of tokens are other non-words. (Some addi-
tional token errors, under 4%, are caused by proposing a
correct word in an incorrect location.) About 85% of collo-
cations (both types and tokens) are composed of two
words, nearly all the rest are three words. To illustrate
the phenomenon, we provide the system’s segmentation
of the ﬁrst 40 utterances in the corpus in Fig. 3, and the
35 most frequently found lexical items in Fig. 4. The 70
most frequent collocations identiﬁed as single words by
the system are shown in Fig. 5.

It is interesting to examine the collocations listed in
Fig. 5 with reference to the existing literature on children’s
early representation of words. Peters (1983), in particular,
provides a number of examples of children’s underseg-
mentation errors (using their productions as evidence).
Several of the full sentences and social conventions in
Fig. 5 (e.g., thank you, that’s right, bye bye, look at this) are
included among her examples. In addition, some of the
other collocation errors found by our unigram system
match the examples of ‘‘formulaic frames” given by Peters:
the ‘‘verb introducer” can you and ‘‘noun introducers” it’s a,
this is, those are, and see the. Phonological reductions in
adult speech also suggest that a few of the collocations
found by the system (e.g., did you, what do you) may even
be treated as single units by adults in some circumstances.
However, the extent and variety of collocations found by
the system is certainly much broader than what research-
ers have so far found evidence for in young children.

We will defer for the moment any further discussion of
whether children’s early word representations are similar
to those found by our DP model (we return to this issue
in the General Discussion), and instead turn to the question
of why these units are found. The answer seems clear:
groups of words that frequently co-occur violate the uni-
gram assumption in the model, since they exhibit strong
word-to-word dependencies. The only way the learner
can capture these dependencies is by assuming that these
collocations are in fact words themselves. As an example,
consider the word that. In our corpus, the empirical proba-
bility of the word that is 798/33399  .024. However, the
empirical probability of that following the word what’s is
far higher: 263/569  .46. Since the strong correlation be-
tween what’s and that violates the independence assump-
tion of the model, the learner concludes that what’sthat
must be a single word.

Note that by changing the values of the parameters a0
and p#, it is possible to reduce the level of undersegmenta-
tion, but only slightly, and at the cost of introducing other

32

S. Goldwater et al. / Cognition 112 (2009) 21–54

Fig. 3. The ﬁrst 40 utterances in the corpus as segmented by the DP model (represented orthographically for readability), illustrating that the model
undersegments the corpus. The stochastic nature of the Gibbs sampling procedure is apparent: some sequences, such as youwantto and getit, receive two
different analyses.

errors. For example, raising the value of p# to 0.9 strongly
increases the model’s preference for short lexical items,
but collocations still make up 24% of both types and tokens
in this case. Measures of token accuracy increase by a few
points, but are still well below those of previous systems.
The main qualitative difference between results with
p# ¼ :5 and p# ¼ :9 is that with the higher value, infre-
quent words are more likely to be oversegmented into very
short one- or two- phoneme chunks (reﬂected in a drop in
lexicon accuracy). However, frequent words still tend to be
undersegmented as before.

It is also worth noting that, although the proportion of
collocations in the lexicons found by MBDP-1 and NGS-u
is comparable to the proportion found by our own model
(24%), only 6% of tokens found by these systems are collo-
cation errors. This fact seems to contradict our analysis of
the failures of our own unigram model, and raises a num-
ber of questions. Why don’t these other unigram models
exhibit the same problems as our own? Is there some other
weakness in our model that might be causing or com-
pounding the problems with undersegmentation? Is it pos-
sible to design a successful unigram model for word
segmentation? We address these questions in the follow-
ing section.

4. Other unigram models

4.1. MBDP-1 and search

In the previous section, we showed that the optimal
segmentation under our unigram model is one that identi-
ﬁes common collocations as individual words. Our earlier

discussion of Venkataraman’s (2001) NGS models demon-
strated that the optimal solution under those models is a
completely unsegmented corpus. What about Brent
(1999) MBDP-1 model? While the deﬁnition of this uni-
gram model makes it difﬁcult to determine what the opti-
mal solution is, our main concern was whether it exhibits
the same problems with undersegmentation as our own
unigram model. The results presented by Brent do not indi-
cate undersegmentation, but it turns out that these results,
like Venkataraman’s, are inﬂuenced by the approximate
search procedure used. We determined this by calculating
the probability of various segmentations of the corpus un-
der each model, as shown in Table 3. The results indicate
that the MBDP-1 model assigns higher probability to the
solution found by our Gibbs sampler than to the solution
found by Brent’s own incremental search algorithm. In
other words, the model underlying MBDP-1 does favor
the lower-accuracy collocation solution, but Brent’s
approximate search algorithm ﬁnds a different solution
that has higher accuracy but lower probability under the
model.

We performed two simulations suggesting that our own
inference procedure does not suffer from similar problems.
First, we initialized the Gibbs sampler in three different
ways: with no utterance-internal boundaries, with a
boundary after every character, and with random bound-
aries. The results were virtually the same regardless of ini-
tialization (see Appendix A for details). Second, we created
an artiﬁcial corpus by randomly permuting all the words in
the true corpus and arranging them into utterances with
the same number of words as in the true corpus. This
manipulation creates a corpus where the unigram assump-

S. Goldwater et al. / Cognition 112 (2009) 21–54

33

responsible for the poor segmentation performance on
the natural language corpus. In particular, the unigram
assumption of the model seems to be at fault. In the fol-
lowing section we present some additional simulations de-
signed to further test this hypothesis. In these simulations,
we change the model of lexical items used in Step 2a of the
model, which has so far assumed that lexical items are cre-
ated by choosing phonemes independently at random. If
the original poor lexical model is responsible for the DP
model’s undersegmentation of the corpus, then improving
the lexical model should improve performance. However,
if the problem is that the unigram assumption fails to ac-
count for sequential dependencies in the corpus, then a
better lexical model will not make much difference.

4.2. The impact of the lexical model on word segmentation

One possible improvement to the lexical model is to re-
place the assumption of a uniform distribution over pho-
nemes with the more realistic assumption that phonemes
have different probabilities of occurrence. This assumption
is more in line with the MBDP-1 and NGS models. In NGS,
phoneme probabilities are estimated online according to
their empirical distribution in the corpus. In MBDP-1, pho-
neme probabilities are also estimated online, but according
to their empirical distribution in the current lexicon. For
models like MBDP-1 and the DP model, where the pho-
neme distribution is used to generate lexicon items rather
than word tokens, the latter approach makes more sense. It
is relatively straightforward to extend the DP model to in-
fer the phoneme distribution in the lexicon simultaneously
with inferring the lexicon itself. Before implementing this
extension, however, we tried simply ﬁxing the phoneme
distribution to the empirical distribution in the true lexi-
con. This procedure gives an upper bound on the perfor-
mance that could be expected if the distribution were
learned. We found that this change improved lexicon F0
somewhat (to 60.5, with a0 ¼ 20 and p# ¼ :5), but made al-
most no difference on token F0 (53.6). Inference of the pho-
neme distribution was therefore not implemented.

Fig. 4. The 35 most frequent items in the lexicon found by the DP model
(left) and in the correct lexicon (right). Except for the phonemes z and s,
lexical items are represented orthographically for readability. Different
possible spellings of a single phonemic form are separated by slashes. The
frequency of each lexical item is shown to its left. Items in the segmented
lexicon are indicated as correct (+) or incorrect ( ). Frequencies of correct
items in the segmented lexicon are lower than in the true lexicon because
many occurrences of these items are accounted for by collocations.

tion is correct. If our inference procedure works properly,
the unigram system should be able to correctly identify
the words in the permuted corpus. This is exactly what
we found, as shown in Table 4. The performance of the
DP model jumps dramatically, and most errors occur on
infrequent words (as evidenced by the fact that token
accuracy is much higher than lexicon accuracy). In con-
trast, MBDP-1 and NGS-u receive a much smaller beneﬁt
from the permuted corpus, again indicating the inﬂuence
of search.

These results imply that the DP model itself, rather than
the Gibbs sampling procedure we used for inference, is

Other changes could be made to the lexical model in or-
der to create a better model of word shapes. For example,
using a bigram or trigram phoneme model would allow
the learner to acquire some notion of phonotactics. Basing
the model on syllables rather than phonemes could incor-
porate constraints on the presence of vowels or syllable
weight. Rather than testing all these different possibilities,
we designed a simulation to determine an approximate
upper bound on performance in the unigram DP model.
In this simulation, we provided the model with informa-
tion that no infant would actually have access to: the set
of word types that occur in the correctly segmented cor-
pus. The lexical model is deﬁned as follows:
‘ 2 L
‘ R L

Ptrueðwi ¼ ‘Þ ¼ ð1   Þ 1

(

jLj þ P0ðwi ¼ ‘Þ

P0ðwi ¼ ‘Þ

where L is the true set of lexical items in the data, and  is
some small mixing constant. In other words, this model is a
mixture between a uniform distribution over the true lex-
ical items and the basic model P0. If  ¼ 0, the model is

34

S. Goldwater et al. / Cognition 112 (2009) 21–54

Fig. 5. The 70 most frequently occurring items in the segmented lexicon that consist of multiple words from the true lexicon. These items are all identiﬁed
as single words; the true word boundaries have been inserted for readability. The frequency of each item is shown to its left.

Table 3
Negative log probabilities of various segmentations under each unigram
model.

Table 4
Accuracy of the various systems on the permuted corpus.

Model

Performance measure

Model

Segmentation

NGS-u
MBDP-1
DP

True

204.5
208.2
222.4

None

90.9
321.7
393.6

MBDP-1

NGS-u

210.7
217.0
231.2

210.8
218.0
231.6

DP

183.0
189.8
200.6

Note: Row headings identify the models used to evaluate each segmen-
tation. Column headings identify the different segmentations: the true
segmentation, the segmentation with no utterance-internal boundaries,
and the segmentation found by each system. Actual log probabilities are
1000 those shown.

constrained so that segmentations may only contain words
from the true lexicon. If  > 0, a small amount of noise is
introduced so that new lexical items are possible, but have
much lower probability than the true lexical items. If the
model still postulates collocations when  is very small,
we have evidence that the unigram assumption, rather

P

76.6
77.0
94.2

R

85.8
86.1
97.1

F

81.0
81.3
95.6

BP

83.5
83.7
95.7

BR

97.6
97.7
99.8

BF

90.0
90.2
97.7

LP

60.0
60.8
86.5

LR

52.4
53.0
62.2

LF

55.9
56.6
72.4

NGS-u
MBDP-1
DP

Note: P, R, and F are precision, recall, and F0 for word tokens; BP, LP, etc.
are the corresponding scores for ambiguous boundaries and lexical items.
Best scores are shown in bold. DP results are with p# ¼ :5 and a0 ¼ 20.

than any failure in the lexicon model, is responsible for
the problem.

The results from this model are shown in Table 5. Not
surprisingly, the lexicon F0 scores in this model are very
high, and there is a large improvement in token F0 scores
against previous models. However,
considering the
amount of information provided to the model, its scores
are still surprisingly low, and collocations remain a prob-
lem, especially for frequent items.

S. Goldwater et al. / Cognition 112 (2009) 21–54

35

Table 5
Results of the DP model using Ptrue.

Mixing constant

Accuracy

% Collocations

 ¼ 10 2
 ¼ 10 3
 ¼ 10 4
 ¼ 10 5
 ¼ 10 6

F

60.5
62.7
64.5
65.5
68.2

LF

81.7
83.4
84.8
85.3
85.6

Tokens

Lexicon

27.7
25.8
24.6
23.7
21.4

21.6
19.1
16.9
15.5
13.1

Note: Shown, for each value of , is token F0 (F), lexicon F0 (LF), and the
percentage of tokens and lexical items that are multi-word collocations.

Considering the case where  ¼ 10 6 yields some in-
sight into the performance of these models with improved
lexical models. The solution found, with a lexicon consist-
ing of 13.1% collocations, has higher probability than the
true solution. This is despite the fact that the most proba-
ble incorrect lexical items are about ﬁve orders of magni-
items.9 These
tude less probable than the true lexical
incorrect lexical items are proposed despite their extremely
low probability because only the ﬁrst occurrence of each
word is accounted for by the lexical model. Subsequent
occurrences are accounted for by the part of the model that
generates repeated words, where probabilities are propor-
tional to the number of previous occurrences. Therefore,
low-probability lexical items incur no penalty (beyond that
of any other word) after the ﬁrst occurrence. This is why
the collocations remaining in the DP model using Ptrue are
the highest-frequency collocations: over many occurrences,
the probability mass gained by modeling these collocations
as single words outweighs the mass lost in generating the
ﬁrst occurrence.

The results of this simulation suggest that the many of
collocations found by the unigram DP model are not due
to the weakness of the lexical model. Regardless of how
good the lexical model is, it will not be able to completely
overcome the inﬂuence of the unigram assumption gov-
erning word tokens when modeling the full corpus. In or-
der to reduce the number of collocations, it is necessary
to account for sequential dependencies between words.
Before showing how to do so, however, we ﬁrst present
theoretical results regarding the generality of our conclu-
sions about unigram models.

4.3. MBDP-1, the DP model, and other unigram models

The probabilistic models used in MBDP-1 and our
Dirichlet process model appeal to quite different genera-
tive processes. To generate a corpus using MBDP, the num-
ber of word types is sampled, then the token frequencies,
then the forms of the words in the lexicon, and ﬁnally an
ordering for the set of tokens. Using the DP model, the
length of the corpus (number of word tokens) must be cho-

9 There are 1321 lexical items in the corpus, so under the lexical model,
the probability of each of these is approximately 10 3. There are 50
phonemes and p# ¼ :5, so a single-character word has probability .01 under
P0. Multiplying by the discount factor  ¼ 10 6 yields Ptrue ¼ 10 8 for one-
character words not in the true lexicon. Longer incorrect words will have
much lower probability.

sen, and then the sequence of words is generated, implic-
itly determining the number of word types and the
lexicon. Although these two approaches to generating a
corpus are very different, it is possible to show that, by
varying the speciﬁc distributions assumed at each step of
the MBDP-1 generative process, the two approaches can
result in exactly the same distribution over word se-
quences. In Appendix B we show that by changing how
the size of the lexicon and the token frequencies are cho-
sen in Steps 1 and 2 of the MBDP model, we can produce
distributions over words that are equivalent to the distri-
bution given by the DP model when conditioned on the to-
tal number of words.

This formal correspondence between MBDP-1 and the
DP model suggests that the two models might express sim-
ilar preferences about segmentations of corpora. In partic-
ular, the generative processes behind the two models share
two components – the distributions over the lexicon and
ordering of tokens – and differ only in the way that word
frequencies are assigned. We can see the consequences of
these shared components by comparing the probabilities
that MBDP-1 and the DP model assign to different segmen-
tations. We compared the probabilities that the models as-
signed to 500 samples of a full segmentation w taken from
the last 1000 iterations of a 20,000-iteration simulation
like the ones described in our unigram model simulations.
As shown in Fig. 6, these probabilities were highly corre-
lated, with the linear correlation between the values of
log PðwÞ under the two models being r ¼ :95. This correla-
tion is almost entirely due to the shared components of the
generative processes: if we remove the shared factors cor-
responding to the probability of the word forms in the lex-
icon and the ordering of the tokens, the correlation is
signiﬁcantly reduced, becoming just r ¼ :30.

This analysis indicates that MBDP-1 and the DP model
show a very close correspondence in their preferences for
different segmentations. The probability of a segmenta-
tion under the two models is highly correlated, despite
the fact that they deﬁne quite different distributions over
word frequencies. This effect is even more pronounced if
we look at the relationship between the probabilities as-
signed by the two models to segmentations ranging in
quality from very poor to very good. Using simulated
annealing, we generated 500 segmentations of varying
quality as deﬁned by the DP model (details of the method
are provided in Appendix A) and evaluated their probabil-
ity under each model. As shown in Fig. 6, the models
agree strongly about the relative quality of those results.
The dominant term in log PðwÞ for both models is that
used in the fourth step: the distribution over orderings
of word tokens.

The dominance of the distribution over the orderings of
word tokens in determining the probability of the segmen-
tation suggests a further conclusion. The distribution used
in this step in both MBDP-1 and the DP model is uniform
over all permutations of the word tokens in the corpus.
Intuitively, this uniform distribution indicates that the or-
der of the words does not matter, and expresses the uni-
gram assumption that underlies
In
Appendix B, we show that any unigram model has to make
this assumption. Thus, all unigram models can be expected

these models.

36

S. Goldwater et al. / Cognition 112 (2009) 21–54

P
D
B
M

 
,
)

w

 

(

 

P
g
o
L

x 105

All steps

−1.823

−1.824

−1.825

−1.826

−2.005

−2.004
Log P( w), DP

−2.003

−2.002
x 105

P
D
B
M

 
,
)
n

 

,

 

(

K
P
g
o
L

−6300

−6320

−6340

−6360

−6380

−6400

−2.43

Steps 1 and 2

x 105

During annealing

P
D
B
M

 
,
)

w

 

(

 

P
g
o
L

−2

−3

−4

−2.425

Log P(K, n), DP

−2.42
x 104

−4

−3

Log P( w), DP

−2

x 105

Fig. 6. Left: Probabilities of 500 segmentations sampled from the posterior distribution of the DP model, computed under both that model and MBDP-1.
Middle: Probabilities of the non-shared components of the same samples – the number of word types and their frequencies. Right: Probabilities under the
two models of 500 segmentations obtained during annealing (to exhibit a wider range of quality).

to be highly correlated with MBDP-1 and the DP model in
their preferences regarding segmentations. This suggests
that the problems that we have identiﬁed with the seg-
mentations produced by the DP model generalize not just
to MBDP-1, but also to other models that assume that
words are independent units.

5. Bigram model

5.1. The hierarchical Dirichlet process model

In the previous section, we discussed empirical and the-
oretical evidence that deﬁning words as statistically inde-
pendent units leads to undersegmentation of natural
language. We now ask whether modifying this assumption
can lead to better segmentation. We address this question
by developing a different model in which words are as-
sumed to help predict other words. Recent work has sug-
gested that children may be sensitive to statistical
dependencies that range over several words (Mintz,
2002; Gómez & Maye, 2005). As a ﬁrst step towards explor-
ing the effects of such dependencies on word segmenta-
tion, we will deﬁne a model
that considers only
dependencies between adjacent words. This model as-
sumes that the probability of a word depends on a single
previous word of context, so the unit of dependency is a
pair of words, or bigram.

Like our unigram model, our bigram model deﬁnes the
probability of a segmentation by assuming that it was gen-
erated as a sequence of words w ¼ w1 . . .w N using a prob-
abilistic process. Unlike the unigram model, wi is generated
using a process that takes into account the previous (al-
ready generated) word in the sequence, wi 1:

(1) Decide whether the pair hwi 1; wii will be a novel

bigram type.

(2) a. If so,

i. Decide whether wi will be a novel unigram

type.

ii. a. If so, generate a phonemic form (phonemes

x1 . . .x M) for wi.

b. If not, choose an existing lexical form ‘ for

wi.

b. If not, choose a lexical form ‘ for wi from among
those that have previously been generated fol-
lowing wi 1.

Notice that Step 2a, which creates the second word of a
novel bigram, uses the same steps we used in our unigram
model. The unigram process in Step 2a generates a set of
word types which the bigram process in Steps 1–2 assem-
bles into bigrams.

The probabilities associated with the bigram generative

process are

(1) Pðhwi 1; wii is a novel bigramjwi 1 ¼ ‘0Þ ¼ a1
n‘0 þa1
Pðhwi 1; wii is not a novel bigramjwi 1 ¼ ‘0Þ ¼ n‘0
n‘0 þa1
(2) a. i. Pðwi is a novel wordjhwi 1;wii is a novel bigramÞ¼

a0
bþa0
Pðwi is not a novel wordjhwi 1; wii is a
novel bigramÞ ¼ b
bþa0

P0ðx1 . . .x MÞ

ii. a. Pðwi ¼ x1 . . .x M j wi is a novel wordÞ ¼
b. Pðwi ¼ ‘ j wi is not a novel wordÞ ¼ b‘
b. Pðwi ¼ ‘jhwi 1; wii is not a novel bigram and

b

wi 1 ¼ ‘0Þ ¼ nh‘0 ;‘i
n‘0

where a0 and a1 are parameters of the model, P0 is the lex-
ical model deﬁned as part of our unigram model, ‘0 is the
lexical form of wi 1, n‘0 and nh‘0;‘i are the number of occur-
rences in the ﬁrst i   1 words of the unigram ‘0 and the
bigram h‘0; ‘i, b is the number of bigram types in the ﬁrst
i   1 words, and b‘ is the number of those types whose sec-
ond word is ‘.

The intuition behind this model is similar to that of the
unigram model. Step 1 says that the more times ‘0 has been
generated, the less likely a new word will be generated fol-
lowing it; this limits the number of bigram types. Step 2a is
like the unigram generative process, except that the prob-
abilities are deﬁned in terms of bigram types instead of
unigram tokens. The idea is that some words combine
more promiscuously into bigrams than others: If ‘ has been
generated in many different contexts already, it is more
likely to be generated in this new context. Finally, in Step
2b, the probability of generating ‘ following ‘0 is propor-
tional to the number of times this pair has been generated

S. Goldwater et al. / Cognition 112 (2009) 21–54

37

(a) Varying α

0 with α

1 = 100, p# = .2

(c) Varying α

0 with α

1 = 100, p# = .5

F
LF
100

300
1000
value of α 0
1 with α

(b) Varying α

0 = 3000, p# = .2

(d) Varying α

0 = 3000, p# = .5

3000

10000

3000

10000

300
1000
value of α 0
1 with α

80%

70%

60%

50%

80%

70%

60%

50%

80%

70%

60%

50%

F
LF
100

80%

70%

60%

50%

F
LF

F
LF

30

50

100 180 300

value of α 1

1000

30

50

100 180 300

value of α 1

1000

Fig. 7. F0 for words (F) and lexical items (LF) in the HDP model as a function of the different model parameters. Each plot varies either a0 (plots (a) and (c))
or a1 (plots (b) and (d)), while holding the other parameters ﬁxed.

already, which leads to a preference for power-law distri-
butions over the second item in each bigram.

The bigram model we have just deﬁned is known as a
hierarchical Dirichlet process (HDP) (Teh, Jordan, Beal, & Blei,
2005). The HDP is an extension of the DP, and is typically
used to model data in which there are multiple distributions
over similar sets of outcomes, and the distributions are be-
lieved to be similar. For language modeling, we can deﬁne
a bigram model by assuming that each word has a different
distribution over the words that follow it, but all these distri-
butions are linked by sharing a common set of unigrams.
Again, we will use this formal connection to name the model,
making our bigram model the ‘‘hierarchical Dirichlet pro-
cess” or HDP model. Our HDP language model is similar to
previously proposed n-gram models using hierarchical Pit-
man–Yor processes (Goldwater, Grifﬁths, & Johnson,
2006b; Teh, 2006). For further discussion and a more de-
tailed presentation of the model, including a treatment of
utterance boundaries, see Appendix A.

5.2. Simulations

5.2.1. Method

For our simulations with the bigram model, we used the
same input corpus and evaluation measures as in our uni-
gram model simulations. To identify a high-probability
solution, we implemented a Gibbs sampler that is concep-
tually similar to the unigram sampler. Details can be found
in Appendix A.10 The sampler was initialized by assigning

word boundaries at random in each utterance, although, as
in the unigram model, other initialization methods yield re-
sults similar to those presented below. We experimented
with various values for the three free parameters of the
model, a0, a1, and p#.11

5.2.2. Results and discussion

Fig. 7 plots the accuracy of our bigram model for various
values of a0, a1, and p# based on a single sample taken after
20,000 iterations. What we see from Fig. 7 is that p# (the
probability of generating the word boundary marker when
producing a novel word) has relatively little effect on re-
sults: lexicon accuracy is slightly higher for the lower value
of p#, but segmentation accuracy is the same. a0 (which
determines the probability of generating a novel word)
also primarily affects lexicon accuracy. Since higher values
of a0 lead to more novel words (i.e., lexical items), lexicon
recall increases for higher values of a0. Lexicon precision
drops slightly, but the overall effect is that F0 for lexical
items increases.

The ﬁnal parameter of the bigram model, a1, has the
greatest effect on results. Recall that this parameter deter-
mines the probability of generating a novel bigram. Since
this is the only parameter that deals with word context,
it is not surprising that it has such a strong effect on seg-
mentation accuracy. Small values of a1 lead to solutions
with fewer novel bigrams, which is achieved by overseg-
menting words into smaller units. As a1 rises, the number
of proposed boundaries falls, which lowers boundary recall

10 Our implementation is slightly different than in the original presenta-
tion of this model (Goldwater, Grifﬁths, & Johnson, 2006a), and also ﬁxes a
small bug in that implementation. Thus, the results presented here are
quantitatively (though not qualitatively) different from those presented in
previous papers.

11 In the full model including utterance boundaries described in Appendix
A, there is a fourth free parameter, p$. However, we found that this
parameter had almost no effect on results, and kept it ﬁxed at 0.5 for all
simulations reported here.

38

S. Goldwater et al. / Cognition 112 (2009) 21–54

but increases precision. The lexicon becomes both larger
and more correct. For moderate values of a1, a good bal-
ance is achieved between oversegmentation and underseg-

mentation of the corpus, and both token accuracy and
lexicon accuracy are high. Token accuracy in particular is
dramatically higher than in the unigram model, and

Table 6
Word segmentation accuracy of unigram and bigram systems.

Model

Performance measure

NGS-u
MBDP-1
DP

NGS-b
HDP

P

67.7
67.0
61.9

68.1
75.2

R

70.2
69.4
47.6

68.6
69.6

F

68.9
68.2
53.8

68.3
72.3

BP

80.6
80.3
92.4

81.7
90.3

BR

84.8
84.3
62.2

82.5
80.8

BF

82.6
82.3
74.3

82.1
85.2

LP

52.9
53.6
57.0

54.5
63.5

LR

51.3
51.3
57.5

57.0
55.2

LF

52.0
52.4
57.2

55.7
59.1

Note: Unigram scores are from Table 1, repeated here for convenience. P, R, and F are precision, recall, and F0 for word tokens; BP, LP, etc. are the
corresponding scores for ambiguous boundaries and lexical items. Best scores are shown in bold. HDP results are with p$ ¼ :5; p# ¼ :2; a0 ¼ 3000, and

Fig. 8. Results for the HDP model with p# ¼ :2, a0 ¼ 3000, and a1 ¼ 100: the ﬁrst 35 segmented utterances (left) and the 35 most frequent lexical items
(right). Fewer collocations appear than in the DP model, there are fewer errors on high-frequency words, and word frequencies match the true frequencies
(Fig. 4) more closely.

S. Goldwater et al. / Cognition 112 (2009) 21–54

39

Table 7
Error analysis for two unigram models and the HDP bigram model.

Model

# Toks

Token errors

# Types

Lexicon errors

Collocs (%)

Non-wds (%)

Placement (%)

Collocs (%)

Non-wds (%)

DP(a0; P0)
DP(a0; Ptrue)
HDP

25,677
27,503
30,914

29.3
21.4
12.0

5.3
1.4
8.0

3.5
1.3
4.8

1331
1325
1148

30.8
13.1
14.7

12.2
1.4
21.8

Note: Shown are the number of tokens and lexical items found by each system, and the percentage of those consisting of collocations, other items not in the
true lexicon, and placement errors (words belonging to the true lexicon, but proposed in the wrong location). Parameters for the DP models were p# ¼ :5,
a0 ¼ 20. The mixing constant in the DPða0; PtrueÞ model was  ¼ 10 6. Parameters for the HDP model were p# ¼ :2, a0 ¼ 3000, and a1 ¼ 100.

high-frequency words are correctly segmented far more
often.

Table 6 presents the results of the HDP model using the
(a0 ¼ 3000;
parameter settings with highest token F0
a1 ¼ 100, and p# ¼ 0:2), as well as results from the only
previously published model incorporating bigram depen-
dencies, NGS-b. Results from the three unigram models
in Table 1 are replicated here for comparison. Due to
search, the performance of the NGS-b model is not much
different from that of the NGS-u. In contrast, the HDP mod-
el performs far better than the DP model on several mea-
sures and achieves the highest segmentation accuracy of
all the models tested here.12 As illustrated in Fig. 8, the seg-
mentation found by our bigram model contains far fewer er-
rors than the segmentation found by our unigram model,
and undersegmentation is much less prevalent. Table 6
shows that our bigram model outperforms the other models
on several measures, and is very close to best performance
on the others. This improvement can be attributed to a large
increase in boundary recall relative to the DP model, with
little loss in precision. In other words, the bigram model pro-
poses more word boundaries and is almost as accurate with
those proposals.

Not only are the results of the bigram model much bet-
ter than those of the basic unigram model DP(a0; P0), they
are qualitatively different. In the unigram model, type
accuracy is higher than token accuracy, indicating many
errors on frequent words. In the bigram model, the oppo-
site is true: frequent words are much more likely to be seg-
mented correctly, so token accuracy is higher than type
accuracy. As Table 7 shows, the bigram model does make
some collocation errors, but they are far less common than
in the unigram model. Other kinds of errors make up a lar-
ger proportion of the errors in the bigram model. A partic-
ularly interesting kind of error is the segmentation of
sufﬁxes as individual words. The top 100 most frequent
lexical items proposed by the bigram model include z, s,
IN, d, and t, which correspond to plural, progressive, and
past tense endings. Together, these items account for
2.4% of the segmented tokens. Interestingly, this is the
same percentage as in the unigram model (although it is
a larger proportion of incorrect tokens, due to the reduc-
tion of other errors). This suggests that incorporating

12 At the time we developed the HDP model, it outperformed all published
results on the corpus used here. Since then, higher accuracy has been
achieved by another Bayesian model based on similar principles (Johnson,
2008). Gambell et al. (2006) also report much higher accuracy using a non-
statistical method, but these results are based on a different corpus and
input representation. See Section 6 for further details.

word-to-word dependencies may not help to account for
morphological dependencies.
Incorporating a notion of
morphology or syllable structure into the model (similar
to the models presented by Johnson (2008)) could improve
results.

Comparison of the bigram model to the DP(a0; Ptrue)
model is particularly enlightening. Access to the true word
types gives the unigram model much higher accuracy on
lexical items, but frequent items are still analyzed as collo-
cations at a much higher rate than in the bigram model.
The net result is that the bigram model scores better on to-
ken accuracy, even though it is completely unsupervised.
This difference between type accuracy and token accuracy
is not surprising: the contextual dependencies built into
the bigram model primarily encode information about
the behavior of word tokens. With even a small amount
of uncertainty in the contents of the lexicon, a model that
does not take word usage into account will have difﬁculty
segmenting natural language. On the other hand, incorpo-
rating contextual dependencies allows the model to learn
about likely sequences of words, greatly improving seg-
mentation while also building a fairly accurate lexicon.

As in the unigram model, we performed additional sim-
ulations to examine the amount of variability in the results
produced by a single sample of the bigram model and
determine whether the MAP approximation would im-
prove segmentation. Average results over ten samples are
shown in Table 8. Again we ﬁnd that taking samples from
a single run yields less variability than taking samples from
independent runs. Unlike our unigram results, the MAP
approximation does seem to reduce variability, and yields
signiﬁcantly higher lexicon F0 than the standard sampler
(p < :002 according to a Wilcoxon sum-rank test). The
average log posterior probability of the MAP approxima-
tion is also lower than that of the standard sampler
(p < :0005), although segmentation accuracy is not signif-
icantly different.

Table 8
Results of the HDP model, averaged over multiple samples.

Samples (10 runs)
Samples (1 run)
MAP approx.

F

71.7 (.56)
71.0 (.12)
71.7 (.67)

LF

57.1 (.85)
56.3 (.75)
58.8 (.87)

  log PðwÞ
199,370 (653)
199,500 (400)
182,450 (155)

Note: Token F0 (F), lexicon F0 (LF), and negative log posterior probability
were averaged over 10 samples from independent runs of our Gibbs
sampler, over 10 samples from a single run, and over 10 samples from
independent runs of our MAP approximation (see text). Standard devia-
tions are shown in parentheses.

40

S. Goldwater et al. / Cognition 112 (2009) 21–54

6. General discussion

In this paper we have developed several computational
models based on Bayesian statistics in order to explore the
effects of context on word segmentation. Unlike previous
work investigating how transitional probabilities or similar
statistics might be used to identify boundaries, our model-
ing effort focuses on the problem of learning words. Chang-
ing the focus in this way brings to light the distinction
between two possible assumptions about the behavior of
words in natural language: the assumption that words
are statistically independent units, and the assumption
that words are predictive units. Our empirical and analytic
results show that, for an ideal learner, adopting the ﬁrst
assumption will lead to undersegmentation of natural lan-
guage, with many common collocations identiﬁed as single
words. Assuming instead that words are predictive of each
other, an ideal learner can produce far more accurate
segmentations.

These results raise a number of questions about the
consequences of the assumptions that were made in deﬁn-
ing our models. In this section, we brieﬂy discuss these
assumptions, identify connections to other models, and
point out ways in which our work could be extended. We
close by considering the implications of our results for
behavioral
exploring human statistical
learning.

experiments

6.1. Ideal observer models of statistical learning

Our models indicate how an ideal learner provided with
all the information contained in a corpus and able to evalu-
ate all possible segmentations would choose to segment
child-directed speech. There are those who would argue that
human infants are in no way ideal learners – either because
they are not seeking to optimize any particular objective
function, or because they simply do not have the means to
do so (or even come close). If that is the case, then our con-
clusions may be interesting from a theoretical perspective,
but have little to say about human language acquisition.
However, we feel that the question of whether (or in what
situations) humans behave as ideal learners is still very
much unresolved, and indeed is an active and growing re-
search topic. Developing explicit ideal learner models with
testable predictions, as we have done here, provides a way
to investigate this question more fully in the future. In fact,
we are currently engaged in research comparing the predic-
tions of our Bayesian word segmentation models with the
predictions of a number of previously proposed models in
several human word segmentation experiments (Frank
et al., 2007; Frank et al., in preparation).

While the issue of whether infants are ideal learners af-
fects the extent to which the models we have presented
should be taken as making predictions about infant behav-
ior, our results are still informative as an indication of the
best a learner might be expected to do with a particular
corpus and set of assumptions about the structure of lan-
guage. In this way, the model plays a similar role to ideal
observer analyses in visual perception, which tell us what
an observer would see if they were able to optimally
extract information from a stimulus (Yuille & Kersten,

2006). Indeed, the approach that we have taken here is
complemented by a recent model of statistical learning
for visual stimuli which was directly motivated by this
kind of ideal observer analysis (Orbán, Fiser, Aslin, & Leng-
yel, 2008).

Orbán et al. (2008) developed an ideal observer model
to explain how people learn regularities in patterns of
shapes appearing in a two-dimensional visual array. The
motivation for this work was discovering how people learn
the ‘‘chunks” that should be used in encoding visual
scenes. The paradigm is a visual analogue of the statistical
learning experiments with speech sounds that have been
the focus of our analysis, with each chunk consisting of
several lower-level visual features in the same way that
words consist of speech sounds, and the goal of learning
being the identiﬁcation of these regularities through expo-
sure. The model that Orbán et al. developed assumes that
each image is generated by ﬁrst activating some number
of chunks, and then sampling the locations of the visual
features that comprise those chunks. The total number of
chunks expressed in a set of images is left free to vary,
being chosen from a prior distribution. Applying probabi-
listic inference makes it possible to identify the chunks
that were used to generate a set of images, as well as
how many such chunks are necessary.

There are two basic differences between the model pro-
posed by Orbán et al. and the model that we have pre-
sented here. The ﬁrst is that the features that comprise
chunks are allowed to appear in variable locations in
two-dimensional space, while the sounds that comprise
words need to appear in ﬁxed order in a speech stream.
The second is that an image is encoded simply through
the presence or absence of these features, while a segmen-
tation of a stream of speech sounds also carries informa-
tion about the order in which the words appear, which is
particularly important for our bigram model. These differ-
ences reﬂect the differences between the kind of stimuli
that the models are designed to process, and the kinds of
regularities they are intended to detect. As a consequence,
the two models are not directly comparable, each being
specialized to its own domain. However, the models have
several similarities, including their ability to determine
how many chunks or words are needed to explain a set
of images or a corpus, and their foundation in the princi-
ples of Bayesian statistics. In particular, both models are
based on deﬁning a procedure for generating stimuli
(either images or utterances) that is inverted by applying
Bayes’ rule to recover latent structure.

6.2. Online inference in word segmentation

When discussing ideal learning in relation to humans,
questions about learning algorithms inevitably arise. There
are two questions we are often asked with regard to the
work presented here. First, are there algorithms that could
be used to optimize the kinds of objective functions we
propose in a more cognitively plausible way? Second, what
is it about the algorithms used by Brent and Venkataraman
that allows their systems to succeed despite problems with
the underlying models? The algorithm we have used here
assumes that the entire data set is available in memory

S. Goldwater et al. / Cognition 112 (2009) 21–54

41

for the learner to iterate over during learning, while Brent’s
and Venkataraman’s algorithms operate in an online fash-
ion, observing (and learning from) each utterance in the
data set exactly once. It is worth noting that the two online
algorithms are very similar, but beyond that, we have no
special insight at present into why they perform as well
as they do. However, in future work we may begin to ad-
dress this question as we examine possible online learning
algorithms for our own models.

As we mentioned earlier, there exist online algorithms
for similar types of models which can be made to approx-
imate the optimal posterior distribution with varying lev-
els of accuracy, depending on the amount of memory
they are allowed to use. The most promising kind of algo-
rithm for online inference in our model is the particle ﬁlter
(Doucet et al., 2000; Doucet, de Freitas, & Gordon, 2001), in
which the posterior distribution is approximated by a set
of samples from that distribution. These samples are then
updated as new observations are made. In our model, each
sample would consist of a segmentation of the corpus and
samples would be updated on hearing a new utterance.
The new utterance would be segmented using the lexicon
associated with each sample, and those samples that pro-
duce good segmentations would be assigned higher
weight. This procedure allows a good segmentation and
lexicon to be inferred in an online fashion. Particle ﬁlters
are becoming increasingly widespread as a means of trans-
lating Bayesian models into process models capable of
making trial-by-trial predictions (Brown & Steyvers,
2009; Daw & Courville, 2008; Sanborn, Grifﬁths, & Navarro,
2006), along with other approximate methods for perform-
ing Bayesian inference (e.g. Kruschke, 2006).

We plan to implement a particle ﬁlter for this model in
the future and investigate how differing memory capacity,
reﬂected in the number of samples maintained, might af-
fect the results of segmentation under this regime. By com-
paring the results of our own online algorithm to those of
Brent and Venkataraman, we may also gain insight into the
success of those previous algorithms. In any case, we
emphasize that the results presented in this paper, which
assume that the learner is able to correctly identify the
posterior distribution, are an important ﬁrst step in teasing
apart how the outcome of learning is affected (on the one
hand) by the underlying goals and assumptions of the lear-
ner, and (on the other hand) by whatever procedures are
used to achieve those goals. Our analysis shows how mak-
ing different assumptions about language inﬂuences the
conclusions that an ideal learner should reach, and do
not depend on the particular learning algorithm we used,
only on the assumption that some algorithm is available
to the learner that can closely approximate the optimal
posterior distribution of the models presented.

6.3. Representational assumptions

Every computational model must make some assump-
tions about how the input data is to be represented. Fol-
lowing Brent (1999) and Venkataraman (2001), we have
used an input corpus consisting of phonemically tran-
scribed words. This choice allows us to compare our results
directly to theirs, but does present some potential prob-

lems. First, it has been argued that syllables, not phonemes,
are the basic sub-word level of representation from which
infants begin to construct words (Swingley, 2005). While
this may be the case, it is worth noting that a relatively
small proportion of errors made by our models consist of
proposing a boundary intra-syllabically, which indicates
that assuming syllable structure as given may not be cru-
cial. In fact, recent work suggests that learners may be able
to acquire the structure of both syllables and words simul-
taneously, assuming a Bayesian model similar to those pro-
posed here (Johnson, 2008). More importantly, we have
shown that the kinds of errors we are most concerned
about in this paper – collocation errors – cannot be solved
by improving the learner’s lexical model, which is essen-
tially what a syllable-based input representation might do.
Another representational issue raised in recent compu-
tational work is how stress (or other prosodic information)
might play a role in word segmentation. Gambell et al.
(2006), for example, suggest that when the input represen-
tation (in their case, syllables) is augmented with stress
marks (with a single syllable in each word labeled ‘‘strong”
and the others labeled ‘‘weak”), a simple rule-based seg-
mentation strategy is sufﬁcient. Their accuracy scores are
impressive (95% F0 on word tokens), but may reﬂect an
overly optimistic view of the information available to the
learner. In particular, because the learner postulates word
boundaries between any two strong syllables, and most
monosyllabic words in their corpus (including function
words) are labeled ‘‘strong”, and English is a heavily mono-
syllabic language, many word boundaries are available
essentially for free. It is not clear how performance would
be affected under the more realistic assumption that most
common monosyllabic words are in fact unstressed.13 If
stress is as easy to extract and use for segmentation as Gam-
bell and Yang suggest, then infants’ initial preference to seg-
ment via statistical cues (Thiessen & Saffran, 2003) remains
a puzzle. Nevertheless, we have no doubt that stress serves
as an important additional source of information for seg-
mentation, and it is worth examining how the availability
of stress cues might shed new light on the results we present
here. For example, perhaps the kind of undersegmented
solution found by our unigram model is sufﬁcient to allow
the learner to identify dominant stress patterns in the lan-
guage, which could then be used to improve later
segmentation.

While addressing such questions of cue combination is
clearly important, we believe that a thorough understand-
ing of the computational aspects of the word segmentation
problem must begin with the simplest possible input be-
fore moving on to models combining multiple cues. This

13 Stress marks in Gambell and Yang’s work were determined using the
CMU pronouncing dictionary, with the ﬁrst pronunciation used in case of
ambiguity. Examining the dictionary reveals that, of the 20 most frequent
words in our input corpus (you, the, a, that, what, is, it, this, what’s, to, do,
look, can, that’s, see, there, I, and, in, your), all except the and a would be
assigned ‘‘strong” stress marks according to this method. However, in
actual usage, the remaining words except for that, this, look, and see are
likely unstressed in nearly all circumstances. These words alone account for
24% of the tokens in our corpus, so changing their representation could
have a large impact on results. Although Gambell and Yang used a different
corpus, we imagine that the general pattern would be similar.

42

S. Goldwater et al. / Cognition 112 (2009) 21–54

progression is analogous to work in the behavioral study of
statistical word segmentation, which began with stimuli
containing only a single cue (the probabilistic relationships
between syllables) and only later expanded to include
complex stimuli containing multiple cues. The models pre-
sented here are developed within a ﬂexible Bayesian
framework that allows different components of the model
(e.g., the model of lexical items or the model of context) to
be independently modiﬁed. This ﬂexibility will enable us
to incorporate additional sources of information into our
models in the future in order to examine some of the ques-
tions of representation and cue combination mentioned
here.

A related issue of representation is the fact that we have
completely abstracted away from the acoustic and pho-
netic variability in the input that human learners receive.
While criticizing Gambell and Yang for their use of citation
stress patterns, we have permitted ourselves the luxury of
normalized phonemic
representations. However, we
emphasize two points. First, our theoretical results regard-
ing the problem of undersegmentation in any unigram
model do not depend on the particular choice of input rep-
resentation, and suggest that in order to overcome this
tendency, any additional cues extracted from the input
would have to be overwhelmingly strong. Second,
although our current system uses an idealized noise-free
input corpus, a major advantage of statistical methods is
their ability to handle noisy input in a robust way. In future
work, we plan to extend the models presented here to ac-
count for variability and noise in the input, and investigate
how this affects the resulting segmentation.

Finally, some readers might wonder about the effect of
our choice to use a corpus in which utterance boundaries
are given. While this is probably a reasonable assumption
(since such boundaries can generally be determined based
on pauses in the input), it is fair to ask how important the
utterance boundaries are to our results. Experimental evi-
dence suggests that human subjects’ segmentation accu-
racy improves as utterances become shorter (i.e., as more
utterance boundaries are provided) (Frank et al., 2007;
Frank et al., in preparation), and very short utterances con-
sisting of isolated words seem to be important in children’s
early word learning (Brent & Siskind, 2001). On the other
hand, it has also been shown that word segmentation is
possible even without the presence of utterance bound-
aries (Saffran et al., 1996, inter alia). While we have not
performed extensive tests of the effects of utterance
boundaries in our models, we did run some preliminary
simulations using the same corpus as in our reported re-
sults, but with all utterance boundaries removed. The re-
sults of these simulations revealed that for both the DP
and HDP models, segmentation accuracy was similar to
or perhaps slightly worse than the accuracy on the original
corpus. (Due to randomness in the results, more extensive
testing would be required to determine whether the re-
sults with and without utterance boundaries were signiﬁ-
cantly different.) The lack of much difference in results
with and without utterance boundaries is somewhat sur-
prising, given that utterance boundaries provide a certain
number of word boundaries for free. However, our models
do not explicitly incorporate any notion of phonotactics,

which could be where much of the beneﬁt lies in knowing
utterance boundaries (i.e., because utterance boundaries
allow the learner to identify phone sequences that com-
monly occur at word edges). It is also worth noting that
the behavior of our models on continuous input is very dif-
ferent from that of MBDP-1 (Brent, 1999): when presented
with a continuous stream of phonemes, Brent’s learner will
fail to ﬁnd any segmentation at all. As with many of the
other differences between our learners and MBDP-1, this
difference is not due to differences in the probabilistic
models underlying the systems, but rather to Brent’s online
search procedure, which requires the presence of utterance
boundaries in order to begin to identify words. Other com-
putational models, including Venkataraman (2001), the re-
cent phonotactic-based model of Fleck (2008), and the
connectionist models of Christiansen et al. (1998) and
Aslin, Woodward, LaMendola, and Bever (1996), also cru-
cially require utterance boundaries in the training corpus
in order to predict word boundaries. (We note, however,
that similar connectionist models could in principle per-
form segmentation of continuous input, given an appropri-
ate interpretation of the output.) All of these models differ
from our own in this respect, and fail to explain how in-
fants are able to segment words from the continuous
streams of input used in experiments such as those of Saf-
fran et al. (1996).

6.4. Implications for behavioral research

To date, behavioral experiments of the sort exempliﬁed
by Saffran et al. (1996) have typically used stimuli that are
constructed according to a unigram model. Thus, transi-
tions between words are random, while transitions within
words are predictable and occur with higher probability.
This assumption is, of course, a simpliﬁcation of natural
language, but has been useful for demonstrating that hu-
man learners are able to segment speech-like input on
the basis of statistical regularities alone. Continuing work
has examined how these kinds of statistical regularities
interact with other kinds of cues such as stress and pho-
netic variability. However, nearly all this work is based
on stimuli in which statistical regularities exist at only a
single level, usually the level of syllables. (One exception
is Newport et al. (in preparation), in which regularities at
both the phoneme level and syllable level are considered.)
Our simulations indicate that a learner who is only able to
track regularities at the sub-word level faces a severe
handicap when trying to segment natural language based
on purely statistical information. This is because regulari-
ties in sub-word units may occur as a result of these units
being grouped within words, or as a result of the words
themselves being grouped within utterances. Without tak-
ing into account the larger (word-level) context, the lear-
ner must assume that all regularities are a result of
groupings within words. This assumption causes under-
segmentation of the input, as word-level groupings are
analyzed as individual words.

The fact that regularities exist at many different levels
in natural language should come as no surprise to anyone,
yet our results are a reminder that it is important to con-
sider the consequences of this hierarchical structure even

S. Goldwater et al. / Cognition 112 (2009) 21–54

43

for very early language acquisition tasks. Based on existing
behavioral studies, we do not know whether humans are
able to track and use bigram or other higher-level statistics
for word segmentation; our work indicates that this would
certainly be helpful, and suggests that the question is an
important one to pursue. Unfortunately, designing experi-
mental stimuli that move beyond unigrams could be too
complex to be feasible within typical statistical learning
paradigms. However, it might be possible to investigate
this question less directly by probing adults’ sensitivity
to bigram frequencies, or by more thoroughly examining
the nature of undersegmentation errors in young children.
Peters’ (1983) work on children’s production errors pro-
vides a good starting point, but is necessarily limited to
children who have already begun to talk, which is well be-
yond the age at which word segmentation begins. Our
ﬁndings suggest an important role for experimental stud-
ies that would examine the possibility of widespread
undersegmentation errors in the perception and represen-
tation of natural speech in preverbal infants.

7. Conclusion

In this paper, we have presented two computational
models of word segmentation developed within a Bayesian
ideal learner framework. The ﬁrst model, which makes the
assumption that words are statistically independent units,
was found to severely undersegment the input corpus.
Moreover, our analytical results show that this kind of
undersegmentation is unavoidable for any ideal learner
making the same assumption of independence between
words. In contrast, our second model demonstrates that a
more sophisticated statistical learner that takes into ac-
count dependencies both at the sub-word and word level
is able to produce much more accurate (adult-like) segmen-
tations. These results do not yet provide direct evidence of
whether infants are able to take context into account during
early segmentation, but do provide speciﬁc predictions that
can be tested in future research. In particular, we envision
three competing hypotheses that these models may help
to tease apart. First, infant learners may bear no resem-
blance to ideal learners in this task. If this is the case, then
we would expect the predictions of our models to differ
from human performance even on simple stimuli where
words are independent. This hypothesis can therefore be
tested using standard statistical learning paradigms, as we
are currently engaged in doing. Second, infants may approx-
imate ideal learners, but be unable to process statistical
information at multiple levels early on. We would then ex-
pect to ﬁnd widespread undersegmentation in early speech
perception, a phenomenon which has yet to be examined. If
this hypothesis is correct, we would also need to complete a
developmental story explaining how additional cues
(including perhaps word-level dependencies) are later
incorporated to correct the initial undersegmentation. Fi-
nally, infants may be best modeled as ideal learners who
are able to process sophisticated statistical information,
including contextual cues, even very early in learning. In
this case, early word representations would be more
adult-like, and might require less modiﬁcation after stress
and other cues become more important later in infancy.

Our work helps to clarify the differences between these
three positions, and we hope that it will provide a source
of inspiration for future experimental and computational
studies examining the evidence for each.

Appendix A. Model deﬁnitions

A.1. Unigram model

Recall that our unigram model generates the ith word in

the corpus, wi, according to the following distribution:

Q
(1) Pðwi is novelÞ ¼ a0
, Pðwi is not novelÞ ¼ n
nþa0
nþa0
(2) a. Pðwi ¼ x1 . . .x M j wi is novelÞ ¼
j¼1PðxjÞ
b. Pðwi ¼ ‘ j wi is not novelÞ ¼ n‘

p#ð1   p#ÞM 1

M

n

In this section, we ﬁrst show how this model can be viewed
in terms of two models that are well-known in the non-
parametric Bayesian statistical literature: the Chinese res-
taurant process (Aldous, 1985) and the Dirichlet process
(Ferguson, 1973). We then provide full details of the model
we used to account for utterance boundaries, and describe
our Gibbs sampling algorithm.

A.1.1. The Chinese restaurant process

The Chinese restaurant process (CRP) is a stochastic
process that creates a partition of items into groups. Imag-
ine a restaurant containing an inﬁnite number of tables,
each with inﬁnite seating capacity. Customers enter the
restaurant and seat themselves. Each customer sits at an
occupied table with probability proportional to the num-
ber of people already seated there, and at an unoccupied
table with probability proportional to some constant a.
That is, if zi is the number of the table chosen by the ith
customer and z i are the tables chosen by the customers
preceding the ith customer, then

8<
:
Pðzi ¼ kjz iÞ ¼ n

ðz iÞ
i 1þa 1 6 k 6 Kðz iÞ
k
i 1þa k ¼ Kðz iÞ þ 1

a

ð3Þ

k

where nðz iÞ
is the number of customers already sitting at
table k, Kðz iÞ is the total number of occupied tables in
z i, and a P 0 is a parameter of the process determining
how ‘‘spread out” the customers become. Higher values
of a mean that more new tables will be occupied relative
to the number of customers, leading to a more uniform dis-
tribution of customers across tables. The ﬁrst customer by
deﬁnition sits at the ﬁrst table, so this distribution is well-
deﬁned even when a ¼ 0. See Fig. A1 for an illustration.



Y


k   1
nðzÞ

Under the Chinese restaurant process model, the prob-
ability of a particular sequence of table assignments for n
customers is (for a > 0)
PðzÞ ¼ CðaÞ
Cðn þ aÞ  aKðzÞ 
R
The Gamma function appearing in Eq. (4) is deﬁned as
1
CðxÞ ¼
0 ux 1e udu for x > 0, and is a generalized factorial
function: CðxÞ ¼ ðx   1Þ!
for positive integer x, and
CðxÞ ¼ ðx   1ÞCðx   1Þ for any x > 0.

ð4Þ

!

KðzÞ

k¼1

44

S. Goldwater et al. / Cognition 112 (2009) 21–54

Fig. A1. The Chinese restaurant process. Black dots indicate the seating arrangement of the ﬁrst nine customers. Below each table is Pðz10 ¼ kjz 10Þ.

Note that this distribution is the same as the distribu-
tion deﬁned by Steps 1 and 2b of our unigram model. It as-
signs each outcome to a group, but does not distinguish the
groups in any interesting way. However, we can extend the
Chinese restaurant metaphor by imagining that the ﬁrst
customer to sit at each table opens a fortune cookie con-
taining a single word, and this word then provides a label
for the table. The words in the cookies are generated by
the distribution P0, and the number of customers at each
table corresponds to the number of occurrences of that
word in the corpus. Thus, this model can be viewed as a
two-stage restaurant (Goldwater et al., 2006b; Goldwater,
2006), where P0 generates word types and the CRP gener-
ates frequencies for those types.

In the two-stage restaurant, the probability of the ith
word in a sequence, given the previous labels and table
assignments, can be found by summing over all the tables
labeled with that word:
Pðwi ¼ ‘jz i; ‘ðz iÞ; aÞ

Pðwi ¼ ‘jzi ¼ k; ‘ðz iÞÞPðzi ¼ kjz i; aÞ

Kðz iÞþ1

X
X

k¼1
Kðz iÞ

¼

¼

Pðwi ¼ ‘jzi ¼ k; ‘kÞPðzi ¼ kj z i; aÞ

k¼1
X
þ Pðwi ¼ ‘jzi ¼ Kðz iÞ þ 1ÞPððzi ¼ Kðz iÞ þ 1Þjz i; aÞ
Kðz iÞ

k

Ið‘k ¼ ‘Þ nðz iÞ
i   1 þ a
þ aP0ð‘Þ

k¼1

¼
¼ nðw iÞ

þ P0ð‘Þ

a

i   1 þ a

‘

i   1 þ a

ð5Þ
where ‘ðz iÞ are the labels of all the tables in z i (with ‘k
being the label of table k), IðÞ is an indicator function tak-
ing on the value 1 when its argument is true and 0 other-
wise, and nðw iÞ
is the number of previous occurrences of ‘
in w i (i.e. the number of assignments in z i to tables la-
beled with ‘). The probability of wi conditioned only on
the previously observed words (and the hyperparameter
a) is also given by Eq. (5), because
Pðwi ¼ ‘jw i; aÞ

‘

¼

Pðwi ¼ ‘jz i; ‘ðz iÞ; aÞPðz i; ‘ðz iÞjaÞ

X
X

fz i;‘ðz iÞg

nðw iÞ

‘

þ aP0ð‘Þ
X
i   1 þ a

fz i;‘ðz iÞg

¼
fz i;‘ðz iÞg
¼ nðw iÞ
¼ nðw iÞ

‘

‘

þ aP0ð‘Þ

i   1 þ a

þ aP0ð‘Þ

i   1 þ a

Pðz i; ‘ðz iÞjaÞ

Pðz i; ‘ðz iÞjaÞ

ð6Þ

Computing the distribution over words in this way
makes it clear that the probability of observing a particular
word is a sum over the probability of generating that word
as an old word or as a new word. That is, even words that
have been observed before may be generated again by P0
(although, in general, this probability will be low com-
pared to the probability of generating a repeated word by
sitting at a previously occupied table) (see Fig. A2).

In the models described in this paper, we deﬁne P0 as a
distribution over an inﬁnite number of outcomes (all pos-
sible strings over a ﬁxed alphabet). However, we note that
it is also possible to deﬁne P0 as a ﬁnite distribution. In the
case where P0 is a K-dimensional multinomial distribution
with parameters x, Eq. (5) reduces to a K-dimensional
Dirichlet(ax)-multinomial model. When P0 is a distribu-
tion over an inﬁnite set, as in this paper, the number of dif-
ferent word types that will be observed in a corpus is not
ﬁxed in advance. Rather, new word types can be generated
‘‘on the ﬂy” from an inﬁnite supply. In general, the number
of different word types observed in a corpus will slowly
grow as the size of the corpus grows.

A.1.2. The Dirichlet process

Models whose complexity grows with the size of the
data are referred to in the statistical literature as inﬁnite
or non-parametric, having the property that, as the data
size grows to inﬁnity, they are able to model any arbitrary
probability distribution. In this section we show that the
two-stage CRP model is equivalent to a standard non-para-
metric statistical model known as the Dirichlet process
(DP).

Rather than providing a rigorous deﬁnition of the
Dirichlet process here, we only attempt to give some intu-
ition. The interested reader may refer to one of several re-
cent papers for further exposition (Navarro, Grifﬁths,
Steyvers, & Lee, 2006; Neal, 2000; Teh et al., 2005). The
DP is a distribution over distributions: each sample G from
a DP is a distribution over a countably inﬁnite set of out-
comes. The set of outcomes over which G is deﬁned (the
support of G), and the relative probabilities of those out-
comes, are determined by the two parameters of the DP,
G0 and a. G0 (the base distribution) is a probability distribu-
tion whose support (of up to uncountably inﬁnite size) is a
superset of the support of G. For example, G0 could be a
normal distribution (over the uncountably inﬁnite set of
real numbers) or P0 (the distribution over the countably
inﬁnite set of possible strings Rþ deﬁned as part of our uni-
gram model). The probability of any particular outcome
under G0 is the probability of that outcome being in the
support of G. So, if G0 is normal, most of the possible out-
comes of G will be numbers near the mean of G0. If

S. Goldwater et al. / Cognition 112 (2009) 21–54

45

Fig. A2. The two-stage restaurant. Each label ‘k is shown on table k. Black dots indicate the number of occurrences of each label in w 10. Below each table is
Pðz10 ¼ kjw10 ¼ ‘the’; z 10; ‘ðz 10Þ; P0Þ. Under this seating arrangement, Pðw10 ¼ ‘the’Þ ¼ 6þaP0ðtheÞ
, and for any
other word ‘, Pðw10 ¼ ‘Þ ¼ aP0ð‘Þ
9þa .
G0 ¼ P0, most of the possible outcomes of G will be rela-
tively short strings. Given a set of outcomes for G deter-
mined by G0, the concentration parameter a of the DP
determines the variance of G: how uniform (or skewed)
is the distribution over its possible outcomes.

, Pðw10 ¼ ‘dog’Þ ¼ 1þaP0ðdogÞ

, Pðw10 ¼ ‘a’Þ ¼ 2þaP0ðaÞ
9þa

9þa

9þa

It is straightforward to deﬁne a unigram language mod-

el using the DP:
wijG  G
Gja; P0  DPða; P0Þ
ð7Þ
where the  should be read as ‘‘is distributed according
to”. This formulation makes the distribution G sampled
from the DP explicit. Implementing such a model is not
possible since G is an inﬁnite distribution. However, what
we
distribution
Pðwijw i; a; P0Þ, which can be found by integrating over
all possible values of G:
Pðwijw i; a; P0Þ ¼

Pðwi jGÞPðGjw i; a; P0ÞdG

really want

conditional

the

Z

is

X

i 1

j¼1

This integration results in the following conditional distri-
bution (Blackwell & MacQueen, 1973):

wijw i; a; P0 

1

i   1 þ a

dðwjÞ þ

a

i   1 þ a P0

ð8Þ

where dðwjÞ is a distribution with all its mass at wj. Rewrit-
ing this distribution as a probability mass function makes
clear the equivalence between the DP language model
and the two-stage CRP language model in (5):

X

i 1

Pðwi ¼ ‘jw i; a; P0Þ ¼

1

Iðwj

i   1 þ a
i   1 þ a P0ð‘Þ ¼ nðw iÞ

a

j¼1

‘

þ aP0ð‘Þ

i   1 þ a

¼ ‘Þ þ

ð9Þ

The use of the Dirichlet process for learning linguistic
structure, as described here, is a very recent development.
More typically, the DP has been used as a prior in inﬁnite
mixture models (Escobar & West, 1995; Lo, 1984; Neal,
2000), where each table represents a mixture component,
and the data points at each table are assumed to be gener-
ated from some parameterized distribution. Technically,
our DP language model can be viewed as a mixture model
where each table is parameterized by its label ‘k, and
Pðwij‘ziÞ ¼ Iðwi ¼ ‘ziÞ, so every data point in a single mix-
ture component is identical. Previous applications of DP
mixture models use more complex distributions to permit
variation in the data within components. Usually Gaussi-
ans are used for continuous data (Rasmussen, 2000; Wood,

Goldwater, & Black, 2006) and multinomials for discrete
data (Blei, Ng, & Jordan, 2002; Navarro et al., 2006). In
the area of language modeling, a number of researchers
have described models similar to our DP model but with
a ﬁxed ﬁnite model size (Elkan, 2006; MacKay & Peto,
1994; Madsen, Kauchak, & Elkan, 2005). Some of the earli-
est work using DPs and their extensions for language-re-
lated tasks focused on modeling semantic content rather
than linguistic structure (Blei, Grifﬁths, Jordan, & Tenen-
baum, 2004). Our own earlier publications describing the
models in this paper (Goldwater et al., 2006a, Goldwater,
Grifﬁths, & Johnson, 2007; Goldwater, 2006) are, to our
knowledge, the ﬁrst to describe the application of DPs to
the problem of structure induction. More recently, a num-
ber of papers have been published showing how to use DPs
and hierarchical DPs (see below) for learning syntax (Fin-
kel, Grenager, & Manning, 2007; Johnson, Grifﬁths, & Gold-
water, 2007; Liang, Petrov, Jordan, & Klein, 2007).

A.1.3. Modeling utterance boundaries

Our model as described so far accounts for the number
of times each word appears in the data. Since the input cor-
pus used in our experiments also includes utterance
boundaries, these must be accounted for in the model as
well. This is done by assuming that each utterance is gen-
erated as follows:

1. Decide whether the next word will end the utterance or

not.

2. Choose the identity of that word according to the DP

model.

3. If the current word is not utterance-ﬁnal, return to Step

1.

The proportion of words that are utterance-ﬁnal

is
determined by a binomial distribution with parameter p$.
Of course, we do not know in advance what the value of
p$ should be, so we assume that this parameter is drawn
from a symmetric Beta(q
2) prior (Gelman, Carlin, Stern, &
Rubin, 2004). The predictive distribution of ui (the binary
variable determining whether the ith word is utterance-ﬁ-
nal or not) can be found by integrating over p$:

Z

Pðui ¼ 1ju i;qÞ¼

Pðui ¼ 1jp$ÞPðp$ju i;qÞdp$ ¼ n$ þ q
i  1þ q

2

ð10Þ

where n$ is the number of utterance-ﬁnal words (i.e., the
number of utterances) in the ﬁrst i   1 words. We do not
show the derivation of this integral here; it is a standard

46

S. Goldwater et al. / Cognition 112 (2009) 21–54

result in Bayesian statistics (Gelman et al., 2004; Bernardo
& Smith, 1994).

A.1.4. Inference

approaches

Our inference procedure is a Gibbs sampler that repeat-
edly resamples the value of each possible word boundary
location in the corpus, conditioned on the current values
of all other boundary locations. In a single iteration, each
possible boundary is considered exactly once. It is possible
to show that as the number of iterations through the train-
ing data increases, the distribution of samples (i.e., seg-
mentations)
posterior
distribution, no matter what the initial sample was. Thus,
when we run the Gibbs sampler, we discard the ﬁrst sev-
eral thousand iterations through the training data (this is
called the ‘‘burn-in” period) to allow the sampler time to
begin producing samples that are approximately distrib-
uted according to the posterior distribution. Details of
the burn-in procedure and other practical issues are de-
scribed below; we ﬁrst provide a formal description of
the sampler.

the model’s

Our Gibbs sampler considers a single possible word
boundary location bj at a time, so each sample is from a
set of two hypotheses (sequences of words with utterance
boundaries), h1 and h2, as illustrated in Fig. A3. These
hypotheses contain all the same word boundaries except
at the one position under consideration, where h2 has a
boundary and h1 does not. We can write h1 ¼ bw1c and
h2 ¼ bw2w3c, where w1 ¼ w2:w3, and b and c are the se-

quences of words to the left and right of the area under
consideration. We sample the value of bj using the follow-
ing equations:
Pðbj ¼ 0jh 

; dÞ ¼

; dÞ

; dÞ

Pðh1jh 
; dÞ þ Pðh2jh 

; dÞ / Pðh1jh 

Pðh1jh 

ð11Þ

; dÞ

Pðbj ¼ 1jh 

; dÞ ¼

; dÞ

Pðh2jh 
; dÞ þ Pðh2jh 

Pðh1jh 

; dÞ / Pðh2jh 

ð12Þ
where d is the observed (unsegmented) data and h  con-
sists of all of the words shared by the two hypotheses
(i.e., the words in b and c). Since we only care about the rel-
ative probabilities of the two outcomes, we can ignore the
denominators and compute only the quantities in the
numerators. Note that
Pðh1jh 

; dÞ ¼ Pðdjh1; h ÞPðh1jh Þ

ð13Þ

¼ Pðh1jh Þ
Pðdjh Þ

Pðdjh Þ

for any h1 that is consistent with the observed data, and
similarly for h2. Substituting into Eqs. (11) and (12) yields
Pðbj ¼ 0jh 
ð14Þ
Pðbj ¼ 1jh 
ð15Þ

; dÞ / Pðh1jh Þ
; dÞ / Pðh2jh Þ

where the conditioning on d has disappeared. Calculating
these quantities is now straightforward. We have

Fig. A3. An example illustrating our Gibbs sampling algorithm. In each of the three steps shown, a single potential boundary location (indicated with #) is
considered. The probabilities of the words that differ (shown in bold) are computed conditioned on the remaining words in each hypothesis according to Eq.
(9). Then, one of the two hypotheses is chosen at random, with probability proportional to the ratio of these probabilities. We indicate the hypothesis
chosen at each step in this example by ðÞ. Note that in the probability computation for the hypotheses in which a boundary is proposed, the denominator
for the factor corresponding to the second word is incremented by one to take into account the fact that the ﬁrst word is now part of the conditioning
environment.

S. Goldwater et al. / Cognition 112 (2009) 21–54

47

Pðh1jh Þ ¼ Pðw1jh ÞPðuw1 jh Þ
 nðh Þ
u þ q
n  þ q

w1 þ a0P0ðw1Þ

¼ nðh Þ

n  þ a0

2

ð16Þ

$

$

u ¼ nðh Þ

where n  is the number of words in h  and nðh Þ
if
w1 is utterance-ﬁnal and n    nðh Þ
otherwise. The ﬁrst fac-
tor in the second line follows from Eq. (9) and the fact that
the DP model is exchangeable: the probability of a particu-
lar sequence of words does not depend on the order of the
words in that sequence (Aldous, 1985). In other words, all
permutations of the sequence have equal probability. We
can therefore compute the probability of any word in the
sequence conditioned on all the other words by treating
the word in question as if it were the last word to be gen-
erated, and applying Eq. (9). The second factor similarly
follows from Eq. (10), since the Beta-binomial model is also
exchangeable.

The posterior probability of h2 can be computed in a

similar fashion, as
Pðh2jh Þ ¼ Pðw2; w3jh Þ

$ þ q

 n    nðh Þ
n  þ q

¼ Pðw2jh ÞPðuw2 jh Þ Pðw3jw2; h Þ Pðuw3 j uw2 ; h Þ
¼ nðh Þ
w2 þ a0P0ðw2Þ
n  þ a0
 nðh Þ
w3 þ Iðw2 ¼ w3Þ þ a0P0ðw3Þ
 nðh Þ
u þ Iðuw2 ¼ uw3Þ þ q

n  þ 1 þ a0

2

2

n  þ 1 þ q

ð17Þ
where IðÞ is an indicator function taking on the value 1
when its argument is true, and 0 otherwise. The IðÞ terms,
and the extra 1 in the denominators of the third and fourth
factors, account for the fact that when generating w3, the
conditioning context consists of h  plus one additional
word and boundary location.

After initializing word boundaries at random (or non-
randomly; see experiments below), the Gibbs sampler iter-
ates over the entire data set multiple times. On each itera-
tion, every potential boundary point is sampled once using
Eqs. (16) and (17). After the burn-in period, these samples
will be approximately distributed according to the poster-
ior distribution PðhjdÞ.

Our Gibbs sampler has the advantage of being straight-
forward to implement, but it also has the disadvantage
that modiﬁcations to the current hypothesis are small
and local. Consequently, mobility through the hypothesis
space may be low, because movement from one hypothe-
sis to another very different one may require transitions
through many low-probability intermediate hypotheses.
Since the initial random segmentation is unlikely to be
near the high-probability part of the solution space, it
may take a very long time for the algorithm to reach that
part of the space.

To alleviate this problem and reduce convergence time,
we modiﬁed the Gibbs sampler to use simulated annealing
(Aarts & Korst, 1989). Annealing the sampler causes it to
make low-probability choices more frequently early in
search, which allows it to more rapidly explore a larger

area of the search space. Annealing is achieved by using a
temperature parameter c that starts high and is gradually
reduced to 1. Annealing with a temperature of c corre-
sponds to raising the probabilities in the distribution under
consideration (in this case, h1 and h2) to the power of 1
c
prior to sampling. Thus, when c > 1, the sampled distribu-
tion becomes more uniform, with low-probability transi-
tions becoming more probable. As the temperature is
reduced, samples become more and more concentrated in
the high-probability areas of the search space. Notice also
that if the temperature is reduced below 1, the sampled
distribution becomes even more peaked, so that in the lim-
it as c ! 0, all probability mass will be concentrated on the
mode of the distribution. This means that, by reducing the
temperature to almost zero, we can obtain an approxima-
tion to the MAP solution.

All of the results in this paper are based on one of three
possible annealing regimes. For most of our simulations,
we ran the sampler for 20,000 iterations, annealing in 10
c ¼ ð:1; :2; . . .;
increments of 2000 iterations each, with 1
:9; 1Þ. Trace plots of three simulations using different initial-
izations can be found in Fig. A4, illustrating the effects of the
annealing process on the posterior and the fact that different
initializations make no difference to the ﬁnal results. For
simulations in which we wished to evaluate using the MAP
approximation, we extended the run of the sampler for an
additional 40,000 iterations, multiplying the temperature
by 5/6 after every 2000 of these iterations.

Our third annealing regime was used to compare the
probabilities assigned by the DP model and MBDP to a
wide range of segmentations. These segmentations were
generated by running the sampler for 50,000 iterations,
slowly decrementing the temperature in 500 increments,
c ¼ ð:002; :004; . . .; : 998; 1Þ. A single sample was taken
with 1
at each temperature.

A.2. Bigram model

A.2.1. The hierarchical Dirichlet process

To extend our model to include bigram dependencies,
we use a hierarchical Dirichlet process (HDP) (Teh et al.,
2005). This approach is similar to previously proposed n-
gram models using hierarchical Pitman–Yor processes
(Goldwater et al., 2006b; Teh, 2006). The HDP is a model
that can be used in situations in which there are multiple
distributions over similar sets of outcomes, and the distri-
butions are believed to be similar. For language modeling,
we can deﬁne a bigram model by assuming each word has
a different distribution over the words that follow it, but all
these distributions are linked. The deﬁnition of the HDP bi-
gram language model (disregarding utterance boundaries
for the moment) is
wijwi 1 ¼ ‘; H‘  H‘ 8‘
H‘ja1; G  DPða1; GÞ 8‘
Gja0; P0  DPða0; P0Þ
That is, Pðwijwi 1 ¼ ‘Þ is distributed according to H‘, a DP
speciﬁc to lexical item ‘. H‘ is linked to the DPs for all other
words by the fact that they share a common base distribu-
tion G, which is generated from another DP.

48

S. Goldwater et al. / Cognition 112 (2009) 21–54

x 105

4.5

4

3.5

3

2.5

pho
ran
utt

)
d

 
|
 

w

 

(

 

P
g
o
−

l

2

0

2000

4000

6000

8000 10000 12000 14000 16000 18000

iterations

x 105

2.01

2.008

2.006

2.004

2.002

pho
ran
utt

)
d

 
|
 

w

 

(

 

P
g
o
−

l

2
1.8

1.82

1.84

1.86

1.88

1.9

iterations

1.92

1.94

1.96

1.98

x 104

Fig. A4. Trace plots of the posterior probabilities of samples from simulations for the DP model initialized with a boundary after every phoneme (‘pho’),
with random boundaries (‘ran’), and with a boundary only at the end of each utterance (‘utt’). Top: trace plot for the entire run of the algorithm, plotted
every 10 iterations. The initial probabilities of each run (circles at x ¼ 0) are very different, but within a few iterations the plots are barely distinguishable.
Steep drops in the plots occur when the temperature is lowered. Bottom: detail of the ﬁnal part of the plot, showing the overlap between the three
simulations.

As in the unigram model, H‘ and G are never repre-
sented explicitly. By integrating over them, we get a distri-
bution over bigram frequencies that can be understood in

terms of the CRP, as illustrated in Fig. A5. Each lexical item
‘ is associated with its own restaurant, which represents
the distribution over words that follow ‘. Different restau-

Fig. A5. Bigrams are modeled using a hierarchical Chinese restaurant process. Each lexical item ‘ has its own restaurant to represent the distribution of
tokens following ‘ in the data. The labels on the tables in these bigram restaurants are drawn from the distribution in the backoff or ‘‘master” restaurant
(top). Each customer (black dot) in the bigram restaurants represents a bigram token; each customer in the backoff restaurant represents a label on some
bigram table.

S. Goldwater et al. / Cognition 112 (2009) 21–54

49

rants are not completely independent, however: the labels
on the tables in the restaurants are all chosen from a com-
mon base distribution, which is represented by another
CRP. A word ‘0 that has high probability in the base distri-
bution will tend to appear in many different bigram types
(i.e. following many other word types). However, Pð‘0j‘Þ
may be very different for different ‘, since each ‘ has its
own restaurant for bigram counts.

To understand how our bigram model accounts for
utterance boundaries, it is easiest if we consider the utter-
ance boundary marker $ as a special word type, so that wi
ranges over Rþ [ f$g. To generate an utterance, each word
is chosen in sequence, conditioned on the previous word,
until an utterance boundary is generated. The predictive
probability distribution over the ith word is
P2ðwijw i; z iÞ ¼

PðwijHwi 1ÞPðHwi 1 jw i; z iÞdHwi 1

Z

¼ nhwi 1;wii þ a1P1ðwijw i; z iÞ

nwi 1 þ a1

ð18Þ
where nhwi 1; wii is the number of occurrences of the bigram
hwi 1; wii in w i (we suppress the superscript w i notation)
and P1ðwijw i; z iÞ is deﬁned as
P1ðwijw i; z iÞ ¼

PðwijGÞPðGjw i; z iÞdG

Z

¼ twi þ a0P0
t þ a0

0ðwiÞ

ð19Þ

where twi is the total number of bigram tables (across all
words) labeled with wi, t is the total number of bigram ta-
bles, and P0

0 is deﬁned to allow generation of either an
utterance boundary or a string of phonemes x1 . . .x M:
0ðwiÞ ¼ p$
P0

wi ¼ $
ð1   p$ÞP0ðwiÞ wi 2 Rþ

ð20Þ

where p$ is a parameter of the model, and P0 is the lexical
model used in the unigram model.14

In the model just deﬁned, P1 is the posterior estimate of
the base distribution shared by all bigrams, and can be
viewed as a unigram backoff. In P1, words are generated
from the DP G. Since G determines the probability that a
word type appears on a bigram table, P1 is estimated from
the number of tables on which each type appears. In other
words, when a particular bigram sequencehwi 1; wii is never
observed in w i, the probability of wi following wi 1 is esti-
mated using the number of different word types that have
been observed to precede wi. If this number is high, then
Pðwijwi 1Þ will be higher than if this number is low.15

14 Technically, allowing P0 to generate utterance boundaries regardless of
context permits our model to generate two consecutive utterance bound-
aries (i.e., an utterance with no words). An earlier version of the model
(Goldwater et al., 2006a; Goldwater, 2006) disallowed empty utterances,
but was slightly more complicated. Since we use the model for inference
only, we have adopted the simpler assumption here.
15 Many standard n-gram smoothing methods use similar kinds of
estimates based on both type and token counts. In fact, Kneser–Ney
smoothing (Kneser et al., 1995), a particularly effective smoothing
technique for n-gram models (Chen et al., 1998), has been shown to fall
out naturally as the posterior estimate in a hierarchical Bayesian language
model similar to the one described here, with the DPs replaced by Pitman–
Yor processes (Goldwater et al., 2006b; Teh, 2006).

A.2.2. Inference

Inference can be performed on the HDP bigram model
using a Gibbs sampler similar to the sampler used
for the unigram model. Our unigram sampler relied on
the fact that words in the unigram model were exchange-
able, so that (for example) Pðw1ÞPðw2jw1ÞPðw3jw1; w2Þ ¼
Pðw3ÞPðw1jw3ÞPðw2jw3; w1Þ. While our bigram model
is
clearly not exchangeable at the level of words,
it is
exchangeable at the level of bigrams. To generate a se-
quence of words such as w1w2w3, four bigrams must be
generated: h$, w1i, hw1; w2i, hw2; w3i, and hw3, $i. Under
the HDP model, the order in which those bigrams is gener-
ated does not affect their joint probability. This property
allows us to sample one boundary at a time in much the
same way we did in the unigram model. Since we
must now consider context during sampling, we write
the two possible segmentations as s1 ¼ bwlw1wrc and
s2 ¼ bwlw2w3wrc, with wl and wr being the left and right
context words for the area under consideration, and b
and c being the remaining words. A complicating factor
that did not arise in the unigram model is the fact that bi-
gram probabilities depend on the seating assignment of
the words under consideration. (The seating assignment
affects the number of tables assigned to each word, which
is used to compute the unigram probability P1.) Each
hypothesis h therefore consists of a segmentation of the
data, along with an assignment to tables of the words in
that segmentation. We use H1 to refer to the set of
hypotheses consistent with s1, H2 to refer to the set of
hypotheses consistent with s2, and h  to refer to the set
of bigrams and table assignments shared by H1 and H2
(i.e., the bigrams covering bwl and wrc, plus the table
assignments for those words). Then we have

Pðhjh Þ¼

Pðhwl;w1ijh Þ Pðhw1;wrijhwl;w1i;h Þ
Pðz1jhwl;w1i;h Þ
Pðhwl;w2ijh Þ Pðhw2;w3ijhwl;w2i;h Þ
Pðhw3;wrijhwl;w2i;hw2;w3i;h Þ
Pðz2jhwl;w2i;h Þ Pðz3jz2;hw2;w3i;h Þ h2 H2

h2 H1

8>>>>>><
>>>>>>:

8>>><
>>>:

where z1, z2, and z3 are random variables representing the
table numbers of w1, w2, and w3.
Using these equations, we could compute the probability
of every hypothesis in the set H1 [ H2, and sample from this
set. Instead, we implemented a simpler and more efﬁcient
sampler by adopting the following approximations (which
are exact provided there are no repeated unigrams or bi-
grams in either fwl; w1; wrg or fwl; w2; w3; wrg)16:

Pðhjh Þ¼

Pðhwl;w1ijh Þ Pðhw1;wrijh Þ
Pðz1jhwl;w1i;h Þ
Pðhwl;w2ijh Þ Pðhw2;w3ijh Þ Pðhw3;wrijh Þ
Pðz2jhwl;w2i;h Þ Pðz3jhw2;w3i;h Þ

h2 H1

h2 H2

16 It would be straightforward to add a Metropolis-Hastings correction
step (Metropolis, Rosenbluth, Rosenbluth, Teller, & Teller, 1953; Hastings,
1970; Neal, 1993) to correct for the slight discrepancy between our
approximation and the true distribution Pðhjh Þ. However, we expect the
number of cases in which our approximation is incorrect to be small, and
the difference between the approximate and true distributions to be slight,
so we did not implement the correction step.

50

S. Goldwater et al. / Cognition 112 (2009) 21–54

These approximations allow us to sample a hypothesis
from H1 [ H2 in two steps. First, we decide whether our
hypothesis will be from H1 or H2 (i.e., whether the seg-
mentation will be s1 or s2). Then, we sample table assign-
ments for either w1 or w2 and w3, as appropriate. By
assumption of the approximation, the assignments for w2
and w3 can be sampled independently.

More precisely, we sample a segmentation s using the

following equations:
Pðs ¼ s1jh Þ
¼ nðh Þ

hwl;w1i þ a1P1ðw1jh Þ

nðh Þ
wl þ a1

 nðh Þ
hw1;wri þ a1P1ðwrjh Þ

nðh Þ
w1 þ a1

ð21Þ

nðh Þ
w2 þ a1

nðh Þ
wl þ a1

ðwl;w2Þ þ a1P1ðw2jh Þ

 nðh Þ
ðw2;w3Þ þ a1P1ðw3jh Þ

Pðs ¼ s2jh Þ
¼ nðh Þ
 nðh Þ
ðw3;wrÞ þ a1P1ðwrjh Þ
8><
Given wi and h , zi has the following distribution:
>:
Pðzi ¼ kjwi; h Þ / nðh Þ




1 6 k 6 K wðh Þ
þ 1

a1P1ðwiÞ k ¼ K wðh Þ

nðh Þ
w3 þ a1



k



i

i

ð22Þ

ð23Þ

where Kðwðh Þ
Þ is the number of tables assigned to wi in h .
After sampling s, we use Eq. (23) to sample values for
either z1 or each of z2 and z3.

i

x 105

5

4

3

2

)
d

 
|
 
z
 

,

w

 

(

 

P
g
o
−

l

We used the same annealing regimes described above
to encourage faster convergence of our bigram sampler
and obtain an approximation to the MAP segmentation.
Fig. A6 illustrates that, as in the unigram sampler, three
different simulations with different initializations end up
producing samples with similar posterior probabilities.
We plot the joint probability Pðw; zÞ since each sample is
an assignment to both words and tables; the column la-
beled   log PðwÞ in Table 8 in the main text actually gives
  log Pðw; zÞ as well, but we omitted the z from the label
since there is no mention of table assignments in our expo-
sition of the model there.

Note that our sampler differs somewhat from the origi-
nal sampler described for this model (Goldwater et al.,
2006a), which used an approximation that did not explic-
itly track assignments to bigram tables. The difference in
performance between the approximate version and the
version presented here is minimal; results reported here
are different from previously reported results primarily
due to ﬁxing a small bug that was discovered while making
the other modiﬁcations to our sampler.

Appendix B. Proofs

This Appendix presents two proofs. First, we show that
there is a generative procedure for the DP model that is in
the same form as that of MBDP-1, allowing us to show that
these procedures make identical assumptions about two of
the distributions used in generating segmented corpora,
and then we show that one of these distributions – the

pho
ran
utt

0

0.2

0.4

0.6

0.8

1

iterations

1.2

1.4

1.6

1.8

2
x 104

x 105

pho
ran
utt

1.82

1.84

1.86

1.88

1.9
iterations

1.92

1.94

1.96

1.98

2
x 104

2.03

2.02

2.01

2

1.99

1.98

)
d

 
|
 
z
 

,

w

 

(

 

P
g
o
−

l

Fig. A6. Trace plots of the posterior probabilities of samples from samplers for the HDP model initialized with a boundary after every phoneme (‘pho’), with
random boundaries (‘ran’), and with a boundary only at the end of each utterance (‘utt’). Top: trace plot for the entire run of the algorithm, plotted every 10
iterations, with the initial probabilities of each run circled at x ¼ 0. Bottom: detail of the ﬁnal part of the plot.

S. Goldwater et al. / Cognition 112 (2009) 21–54

51

distribution on orderings – has to be used in the generative
procedure assumed by any unigram model.

B.1. Connecting MBDP-1 and the DP model

The probabilistic model behind MBDP-1 is based on a
sequence of four steps that are intended to generate a seg-
mented corpus. The steps are as follows:

Step 1: Sample the number of word types, K, from a dis-

tribution PðKÞ.

sample

Step 2: Sample a vector of frequencies n ¼ ðn1; . . .; nKÞ
Step 3: Sample the lexicon ‘ ¼ ð‘1; . . .; ‘ KÞ from a distri-
Step 4: With n ¼

for these words from a distribution PðnkjKÞ.
P
bution Pð‘jKÞ.

jnj being the total number of word
tokens,
ordering
s : f1; . . .; ng ! f1; . . .; Kg, constrained to map
exactly nj positions to word ‘j, from a distribu-
tion PðsjnÞ. The words in the corpus are
w ¼ ðw1; . . .; wnÞ ¼ ð‘sð1Þ; . . .; ‘ sðnÞÞ.
X

If we follow this procedure, the probability of a particular
segmented corpus, w, is
PðwÞ ¼

Pðwjs; n; ‘; KÞPðsjnÞPðnjKÞPð‘jKÞPðKÞ

ð24Þ

an

K;‘;n;s

where Pðwjs; n; ‘; KÞ ¼ 1 if the other variables generate w
and 0 otherwise. If all frequencies in n are non-zero, then
for any given value of w, only one set of (lexical item, fre-
quency) pairs is consistent with w, so the sum in Eq. (24)
simply counts the K! permutations of the indices of those
ð‘j; njÞ pairs, each of which has the same probability under
the generative process.

K

PðnjKÞ ¼

independent, with

MBDP-1 uses a particular distribution in each of the
Q
four steps. In Step 1, the number of word types is drawn
from PðKÞ / 1=K2. In Step 2, frequencies are assumed to
j¼1PðnjÞ where
Q
be
PðnjÞ / 1=n2
j . In Step 3, words are generated by choosing
independently, with Pð‘Þ ¼ 1
k¼1PðxkÞ,
phonemes
1 p#
where q is the number of phonemes in ‘, xk is the kth pho-
neme (which might be the end marker, #) and PðxÞ is a
probability distribution over phonemes. The words in the
lexicon are required to be unique, otherwise the possible

conﬁgurations of K, ‘, n and s that could have produced
w proliferate.17 In Step 4, PðsjnÞ is taken to be uniform over
all

valid mappings.



n

q

n1 . . .n k

The probabilistic model used in MBDP has the unusual
property of generating a corpus as a single object, without
specifying how a sequence of words might be generated
one after another. This property makes it difﬁcult to form

17 We maintain the unique lexicon requirement for the models discussed
in this appendix since it avoids having to sum over many solutions in
computing PðwÞ. However, our equivalence results hold provided the
distribution from which the lexicon is drawn, Pð‘jKÞ, is exchangeable, with
the probability of ‘ not depending on the indices of the ‘j. A relatively
complex (but exchangeable) distribution based on Pð‘Þ was used in Brent
(1999).

predictions about what the next word or utterance might
be, and to compute conditional distributions. Indeed, in
the description of MBDP-1 it is noted, when referring to
the ratio of the probability of one corpus to another that
‘‘. . .the semantics of the model are not consistent with a
conditional probability interpretation. The sequence
w1; . . .; wk is not a conjunction of events from the probabil-
ity space but rather a single event that is determined by
the joint outcomes of steps 1–4 above. Thus, w1; . . .; wk 1
and w1; . . .; wk are actually distinct, mutually exclusive
events from the probability space.” (Brent, 1999, p. 104).
In contrast, the DP model is deﬁned in terms of a series
of conditional distributions, with each word being sampled
conditioned on the previous words. In the remainder of
this section, we will show that the DP model can nonethe-
less be speciﬁed by a procedure similar to that used in
MBDP-1.

We will start by assuming that in Step 1 we ﬁx the
number of word types, K. In Step 2, we draw each nj from
a PoissonðkÞ distribution where k is generated from a
Z
K ; bÞ distribution. Integrating over k, this gives
Gammaða
1
PðnjÞ ¼

PðnjjkÞpðkÞdk

ð25Þ

1

expf kð1 þ bÞgknjþa

K 1dk

0

Cðnj þ a
KÞ
ð1 þ bÞnjþa

K

ð26Þ

0

¼ 1
nj!
¼ 1
nj!

Z

a
b
K
Cða
KÞ
a
b
K
Cða
KÞ
Y
1
 
nj!
j¼1
¼ a
K

K

PðnjKÞ ¼

where the neat result is due to conjugacy between the
Gamma and Poisson. As a result, we have

Cðnj þ a
KÞ
a


b
K
Y
Cða
KÞ
ð1 þ bÞnjþa
Kþ
b
1 þ b

a

K

1

1
nj!

fjjnj>0g

ð1 þ bÞnj

Cðnj þ a
KÞ
Cð1 þ a
KÞ
ð27Þ
where the notation on the product indicates that only
words with non-zero frequencies should be included, and
Kþ is the size of this set. We can expand out the factorial
 
and Gamma functions, to give
PðnjKÞ ¼ a
K



1 þ b

j þ a
j

Ynj 1

ð1 þ bÞnj

Y

ð28Þ

 

!

1
nj

Kþ

Kþ

1

b

K

a

j¼1

j¼1

where we simplify the constraint on j by assuming that the
ﬁrst Kþ indices over which j quantiﬁes correspond to
words that have frequencies greater than zero. Steps 3
and 4 can proceed as in MBDP.

Applying Eq. (24), the probability of a segmented corpus
w results from summing over all conﬁgurations of ‘, n, and
s that produce w. We can marginalize out the words ‘j for
which nj ¼ 0, so Pð‘Þ ¼ Pð‘þÞ, where ‘þ is the set of words
K0! conﬁgura-
with non-zero frequencies. There are then K!
tions of ‘ and n that yield the same corpus w, where
K0 ¼ K   Kþ is the number of words for which nj ¼ 0. This
ways of allocating K0 zeros
corresponds to the

 

K
K0

across the K words, multiplied by the Kþ! permutations

S. Goldwater et al. / Cognition 112 (2009) 21–54


ð‘j; njÞ pairs with nj > 0. Finally,
. Putting all of this together with

52

the indices of
n



of
PðsjnÞ ¼ 1
Eq. (28), we obtain
 
PðwÞ ¼ Pð‘þÞ K!
Y
K0!

Q

n1 . . .n Kþ



Kþ

j¼1

1
nj

Kþ
j¼1nj!
n!

1

ð1 þ bÞnj
Q

 
Ynj 1

a
K

Kþ

j¼1

a

b




!
1 þ b
j þ a
j

K

ð29Þ

tence of a generative procedure in which we ﬁrst choose
frequencies according to some distribution, PðnÞ, and then
choose an ordering of words uniformly at
random.
Exchangeability is the essence of the assumption behind
a unigram model, since it indicates that we are completely
indifferent to the ordering of the words, and must be a
property of all unigram models.

For any distribution over segmented corpora w, we can
compute PðnÞ by summing over all w in which the frequen-
cies are n. Exchangeability means that all permutations of
the indices of words in the corpus have the same value, and
permutation of indices does not affect n. There is also a un-
ique n for any sequence. Consequently, PðnÞ will be the
probability of any single sequence with frequencies corre-
sponding to n (which will be the same for all such se-
quences), multiplied by the number of such sequences.
Let mn represent the number of unique sequences yielded
by frequencies n and w
n be an arbitrary sequence with
these frequencies. We then have PðnÞ ¼ mnPðw
nÞ. Now,
we can compute PðwjnÞ, the distribution over segmented
corpora given frequencies. This will just be PðwÞ divided
by PðnÞ. Since PðwÞ is the same as Pðw
nÞ by exchangeabil-
ity, this reduces to 1=mn for any corpus w. This is equiva-
lent to simply choosing uniformly at random from all





valid permutations, as mn is just

n

n1 . . .n k

.

References

Aarts, E., & Korst, J. (1989). Simulated annealing and Boltzmann machines: A
optimization and neural

combinatorial

stochastic
computing. New York: Wiley.

approach to

Aldous, D. (1985). Exchangeability and related topics. In École d’été de

probabilités de Saint-Flour, XIII—1983 (pp. 1–198). Berlin: Springer.

Allen, J. & Christiansen, M. (1996). Integrating multiple cues in word
segmentation: A connectionist model using hints. In Proceedings of the
18th annual conference of the cognitive science society (pp. 370–375).
Mahwah, NJ: Lawrence Erlbaum.

Anderson, J. R. (1991). The adaptive nature of human categorization.

Psychological Review, 98(3), 409–429.

Ando, R. & Lee, L. (2000). Mostly-unsupervised statistical segmentation of
Japanese: Application to kanji. In Proceedings of the joint meeting of the
language processing and the North
conference on applied natural
American chapter of
linguistics
(ANLP-NAACL).

the association for computational

Arratia, R., Barbour, A. D., & Tavare, S.

(1992). Poisson process
approximations for the Ewens sampling formula. The Annals of
Applied Probability, 2, 519–535.

Aslin, R., Saffran, J., & Newport, E. (1998). Computation of conditional
probability statistics by 8-month-old infants. Psychological Science, 9,
321–324.

Aslin, R., Woodward, J., LaMendola, N., & Bever, T. (1996). Models of word
segmentation in ﬂuent maternal speech to infants. In J. Morgan & K.
Demuth (Eds.), Signal to syntax (pp. 117–134). Mahwah, NJ: Lawrence
Erlbaum.

Batchelder, E. (2002). Bootstrapping the lexicon: A computational model

of infant speech segmentation. Cognition, 83, 167–206.

Bernardo, J., & Smith, M. (1994). Bayesian theory. New York: Wiley.
Bernstein-Ratner, N. (1987). The phonology of parent–child speech. In K.
Nelson & A. van Kleeck (Eds.). Children’s language (Vol. 6). Hillsdale,
NJ: Erlbaum.

Bishop, C. (2006). Pattern recognition and machine learning. New York:

Springer.

Blackwell, D., & MacQueen, J. (1973). Ferguson distributions via Pólya urn

schemes. Annals of Statistics, 1, 353–355.

Blei, D., Ng, A., & Jordan, M. (2002). Latent Dirichlet allocation. In Advances

in neural information processing systems (Vol. 14).

Blei, D., Grifﬁths, T., Jordan, M., & Tenenbaum, J. (2004). Hierarchical topic
models and the nested Chinese restaurant process. In Advances in
neural information processing systems (Vol. 16).



We are now in a position to generalize this distribution to
the case where K ! 1, with
Y
1 þ b

PðwÞ ¼ Pð‘þÞ

ð1 þ bÞnj

Y

lim
K!1

1
nj

aKþ

j¼1

Kþ

1

b

a

Kþ
j¼1nj!
n!
¼ Pð‘þÞ aKþ
n!
Q
Q
where we use the fact that
K ðK   j   1ÞÞð
limK!1ð

Kþ
j¼1

ba

1

Kþ

ðnj   1Þ!
Q

ð1 þ bÞnþa
j¼1
Q
K0! KKþ ¼
nj 1
kþa
k Þ ¼ 1.
Kþ
K
j¼1
k¼1

Kþ
j¼1

K!

1

K ðK   j   1Þ and

ð30Þ

P

Eq. (30) gives a distribution over segmented corpora de-
rived via the MBDP-1 scheme of generating a corpus as a
single object. However, we can also ask what the distribu-
tion over word sequences would be if we conditioned on
the total number of words. To do this, we need to divide
PðwÞ by the probability of a particular value of n, PðnÞ, un-
der this model. Fortunately, PðnÞ is easy to compute. We
chose PðnjÞ to be a Poisson distribution with rate set by a
gamma distribution, and n ¼
1
j¼1nj. The sum of a set of
Poisson random variables is Poisson with rate equal to
the sum of the rates of the original variables. The gamma
distribution has a similar property, meaning that the sum
of
the rates of our Poisson distributions follows a
Gammaða; bÞ distribution. Summing out this variable as
we did for the individual frequencies, we obtain
PðnÞ ¼ 1
n!

Cðn þ aÞ

ð1 þ bÞnþa

CðaÞ

ð31Þ

ba

Dividing Eq. (30) by PðnÞ gives the conditional distribution
over corpora given their length

PðwjnÞ ¼ Pð‘þÞaKþ CðaÞ
Cðn þ aÞ

ðnj   1Þ!

ð32Þ

Y

Kþ

j¼1

which is exactly the distribution over word sequences
yielded by the DP model.

B.2. Relationship to other unigram models

The result in the previous section implies that the DP
model can be speciﬁed in a way such that it uses exactly
the same distributions as MBDP-1 in Steps 3 and 4. We
can now generalize this result, showing that any unigram
model has to use the same distribution in Step 4, taking
a uniform distribution over orderings of word tokens. We
do this by showing that exchangeability – assigning the
same probability to all sequences of words in which the
words occur with the same frequencies – implies the exis-

Brent, M. (1999). An efﬁcient, probabilistically sound algorithm for

Gilks, W., Richardson, S., & Spiegelhalter, D. J. (Eds.). (1996). Markov chain

segmentation and word discovery. Machine Learning, 34, 71–105.

Monte Carlo in practice. Suffolk: Chapman and Hall.

S. Goldwater et al. / Cognition 112 (2009) 21–54

53

Brent, M., & Cartwright, T.

regularity and
phonotactic constraints are useful for segmentation. Cognition, 61,
93–125.

(1996). Distributional

Brent, M., & Siskind, J. (2001). The role of exposure to isolated words in

early vocabulary development. Cognition, 81(2), B33–44.

Brown, S. D., & Steyvers, M. (2009). Detecting and predicting changes.

Cognitive Psychology, 58, 49–67.

Cairns, P., & Shillcock, R. (1997). Bootstrapping word boundaries: A
bottom-up corpus-based approach to speech segmentation. Cognitive
Psychology, 33, 111–153.

Chen, S. F. & Goodman, J. (1998). An empirical study of smoothing
techniques for language modeling (Technical Report No. TR-10-98).
Center for Research in Computing Technology, Harvard University.

Christiansen, M., Allen, J., & Seidenberg, M. (1998). Learning to segment
speech using multiple cues: A connectionist model. Language and
Cognitive Processes, 13, 221–268.

Cohen, P. & Adams, N. (2001). An algorithm for segmenting categorical
timeseries into meaningful episodes. In Proceedings of the fourth
symposium on intelligent data analysis.

Creel, S., Newport, E., & Aslin, R. (2004). Distant melodies: Statistical
learning of non-adjacent dependencies in tone sequences. Journal of
Experimental Psychology: Learning, Memory, and Cognition, 30(5),
1119–1130.

Creutz, M. & Lagus, K. (2002). Unsupervised discovery of morphemes. In
Proceedings of the workshop on morphological and phonological learning
at ACL’02.

Daw, N., & Courville, A. (2008). The pigeon as particle ﬁlter. In J. Platt, D.
Koller, Y. Singer, & S. Roweis (Eds.). Advances in neural information
processing systems (Vol. 20, pp. 369–376). Cambridge, MA: MIT Press.
Doucet, A., Andrieu, C., & Godsill, S. (2000). On sequential Monte Carlo
sampling methods for Bayesian ﬁltering. Statistics and Computing,
10(3), 197–208.

Doucet, A., de Freitas, N., & Gordon, N. (2001). Sequential Monte Carlo

methods in practice. New York: Springer.

Dowman, M.

(2000).

Addressing

the

(Eds.), Proceedings of

subcategorizations with Bayesian inference.
Joshi
cognitive
Associates.

society. Mahwah, NJ:

science

the 22nd annual conference of

of

learnability

verb
In L. Gleitman & A.
the
Erlbaum

Lawrence

Elkan, C. (2006). Clustering documents with an exponential-family
approximation of the Dirichlet compound multinomial distribution.
In Proceedings of the 23rd international conference on machine learning.
Ellison, T. M. (1994). The iterative learning of phonological constraints.

Computational Linguistics, 20(3).

Elman, J. (1990). Finding structure in time. Cognitive Science, 14, 179–211.
Escobar, M., & West, M. (1995). Bayesian density estimation and inference
using mixtures. Journal of the American Statistical Association, 90(430),
577–588.

Feng, H., Chen, K., Deng, X., & Zheng, W. (2004). Accessor variety criteria

for Chinese word extraction. Computational Lingustics, 30(1).

Ferguson, S. (1973). A Bayesian analysis of some nonparametric problems.

Annals of Statistics, 1, 209–230.

Finkel,

J., Grenager, T., & Manning, C. (2007). The inﬁnite tree.

In

Proceedings of the association for computational linguistics.

Fiser, J., & Aslin, R. (2002). Statistical learning of new visual feature
combinations by infants. Proceedings of the National Academy of
Sciences, 99, 15822–15826.

Fleck, M.

(2008). Lexicalized phonotactic word segmentation.

In
Proceedings of the joint meeting of the association for computational
linguistics and the human language technology conference (ACL-08: HLT)
(pp. 130–138). Columbus, Ohio: Association for Computational
Linguistics.

Frank, M., Goldwater, S., Mansinghka, V., Grifﬁths, T., & Tenenbaum, J.
(2007). Modeling human performance
on statistical word
segmentation tasks. In Proceedings of the 29th annual conference of
the cognitive science society.

Frank, M. C., Goldwater, S., Grifﬁths, T., Newport, E., Aslin, R., &
Tenenbaum, J. (in preparation). Modeling human performance in
statistical word segmentation.

Gambell, T. & Yang, C. (2006). Word segmentation: Quick but not dirty.
(Unpublished manuscript, available at <http://www.ling.upenn.edu/
ycharles/papers/quick.pdf>).

Gelman, A., Carlin, J., Stern, H., & Rubin, D. (2004). Bayesian data analysis.

New York: Chapman & Hall/CRC.

Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions
and the Bayesian restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 6, 721–741.

Goldsmith, J. (2001). Unsupervised learning of the morphology of a

natural language. Computational Linguistics, 27, 153–198.

Goldwater, S.

(2006). Nonparametric Bayesian models of

lexical

acquisition. Unpublished doctoral dissertation, Brown University.

Goldwater, S. & Johnson, M. (2004). Priors in Bayesian learning of
In Proceedings of the seventh meeting of the
interest group in

phonological rules.
association for computational
computational phonology (SIGPHON’04).

linguistics. special

Goldwater, S., Grifﬁths, T., & Johnson, M.

(2006a). Contextual
dependencies in unsupervised word segmentation. In Proceedings of
the joint meeting of the international committee on computational
linguistics and the association for computational linguistics (COLING/
ACL). Sydney.

Goldwater, S., Grifﬁths, T., & Johnson, M. (2006b). Interpolating between
types and tokens by estimating power-law generators. In Advances in
neural information processing systems (Vol. 18).

Goldwater, S., Grifﬁths, T., & Johnson, M. (2007). Distributional cues to
word segmentation: Context is important. In Proceedings of the 31st
Boston University conference on language development.

Gómez, R., & Maye, J. (2005). The developmental trajectory of nonadjacent

dependency learning. Infancy, 7, 183–206.

Harris, Z. (1954). Distributional structure. Word, 10, 146–162.
Harris, Z. (1955). From phoneme to morpheme. Language, 31, 190–222.
Hastings, W. (1970). Monte Carlo sampling methods using Markov chains

and their applications. Biometrika, 57, 97–109.

Johnson, M. (2008). Using adapter grammars to identify synergies in the
unsupervised learning of linguistic structure. In Proceedings of the
joint meeting of the association for computational linguistics and the
human language technology conference (ACL-08: HLT).

Johnson, M., Grifﬁths, T., & Goldwater, S. (2007). Adaptor grammars: A
framework for specifying compositional nonparametric Bayesian
models. In Advances in neural information processing systems (Vol. 19).
Johnson, E., & Jusczyk, P. (2001). Word segmentation by 8-month-olds:
When speech cues count more than statistics. Journal of Memory and
Language, 44, 548–567.

Jusczyk, P. (1999). How infants begin to extract words from speech. Trends

in Cognitive Sciences, 3(9).

Jusczyk, P., Hohne, E., & Bauman, A. (1999).

allophonic
for word
Psychophysics, 61(8), 1465–1476.

cues

segmentation.

Infants’ sensitivity to
and

Perception

Jusczyk, P., Houston, D., & Newsome, M. (1999). The beginnings of word
segmentation in English-learning infants. Cognitive Psychology, 39,
159–207.

Kneser, R. & Ney, H. (1995). Improved backing-off for n-gram language
In Proceedings of the IEEE international conference on

modeling.
acoustics, speech and signal processing.

Kruschke,

J. (2006). Locally Bayesian learning with applications to
retrospective revaluation and highlighting. Psychological Review,
113(4), 677–699.

Liang, P., Petrov, S., Jordan, M., & Klein, D. (2007). The inﬁnite PCFG using
hierarchical Dirichlet processes. In Proceedings of the joint meeting of
empirical methods in natural language processing and computational
natural language learning (EMNLP/CoNLL).

Lo, A. (1984). On a class of Bayesian nonparametric estimates. Annals of

Statistics, 12, 351–357.

MacKay, D., & Peto, L. B. (1994). A hierarchical Dirichlet language model.

Natural Language Engineering, 1(1).

MacWhinney, B., & Snow, C. (1985). The child language data exchange

system. Journal of Child Language, 12, 271–296.

Madsen, R., Kauchak, D., & Elkan, C. (2005). Modeling word burstiness
the 22nd

In Proceedings of

using the Dirichlet distribution.
international conference on machine learning.

de Marcken, C. (1995). The unsupervised acquisition of a lexicon from
of

speech (Tech. Rep.). Massachusetts

Institute

continuous
Technology. (A.I. Memo No. 1558).

Marr, D. (1982). Vision: A computational approach. San Francisco: Freeman

& Co..

Mattys, S., Jusczyk, P., Luce, P., & Morgan, J. (1999). Phonotactic and
prosodic effects on word segmentation in infants. Cognitive
Psychology, 38, 465–494.

Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A., & Teller, E.
(1953). Equations of state calculations by fast computing machines.
Journal of Chemical Physics, 21, 1087–1092.

Mintz, T. (2002). Category induction from distributional cues in an

artiﬁcial language. Memory and Cognition, 30, 678–686.

Morgan, J., Bonamo, K., & Travis, L. (1995). Negative evidence on negative

evidence. Developmental Psychology, 31, 180–197.

54

S. Goldwater et al. / Cognition 112 (2009) 21–54

Navarro, D. J., Grifﬁths, T. L., Steyvers, S., & Lee, M. D. (2006). Modeling
of

individual differences using Dirichlet processes.
mathematical Psychology, 50, 101–122.

Journal

Neal, R. (1993). Probabilistic inference using Markov chain Monte Carlo
methods (Tech. Rep. No. CRG-TR-93-1). University of Toronto,
Department of Computer Science.

Neal, R. (2000). Markov chain sampling methods for Dirichlet process
mixture models. Journal of Computational and Graphical Statistics, 9,
249–265.

Newport, E., & Aslin, R. (2004). Learning at a distance I. Statistical learning

of non-adjacent dependencies. Cognitive Psychology, 48, 127–162.

Newport, E., Weiss, D., Aslin, R., & Wonnacott, L. (in preparation).

Statistical learning in speech by infants: Syllables or segments?.

Orbán, G., Fiser, J., Aslin, R., & Lengyel, M. (2008). Bayesian learning of
the National

visual chunks by human observers. Proceedings of
Academy of Sciences, 105, 2745–2750.

Peters, A. (1983). The units of language acquisition. New York: Cambridge

University Press.

Schulz, L., Bonawitz, E., & Grifﬁths, T. (2007). Can being scared make
tummy ache? naive theories, ambiguous evidence and
inferences. Developmental Psychology, 43,

causal

your
preschoolers’
1124–1139.

Swingley, D. (2005). Statistical clustering and the contents of the infant

vocabulary. Cognitive Psychology, 50, 86–132.

Teh, Y. (2006). A Bayesian interpretation of interpolated Kneser–Ney
(Tech. Rep. No. TRA2/06). National University of Singapore, School of
Computing.

Teh, Y., Jordan, M., Beal, M., & Blei, D. (2005). Hierarchical Dirichlet
processes. Advances in Neural information processing systems (Vol. 17).
Cambridge, MA: MIT Press.

Thiessen, E., & Saffran, J. (2003). When cues collide: Use of stress and
statistical cues to word boundaries by 7- to 9-month-old infants.
Developmental Psychology, 39(4), 706–716.
J. (2004). Spectral
infancy

tilt as a cue to word
and

Thiessen, E., & Saffran,

adulthood.

Perception

segmentation
Psychophysics, 66(5), 779–791.

in

and

Rasmussen, C. (2000). The inﬁnite Gaussian mixture model. In Advances in

Toro,

neural information processing systems (Vol. 12).

Rissanen, J. (1989). Stochastic complexity and statistical inquiry. Singapore:

World Scientiﬁc Co..

J., Sinnett, S., & Soto-Faraco, S. (2005). Speech segmentation
learning depends on attention. Cognition, 97,

by statistical
B25–B34.

Venkataraman, A. (2001). A statistical model for word discovery in

Saffran, J., Aslin, R., & Newport, E. (1996). Statistical learning in 8-month-

transcribed speech. Computational Linguistics, 27(3), 351–372.

old infants. Science, 274, 1926–1928.

Saffran, J., Newport, E., & Aslin, R. (1996). Word segmentation: the role of

distributional cues. Journal of Memory and Language, 35, 606–621.

Sanborn, A., Grifﬁths, T., & Navarro, D. (2006). A more rational model of
categorization. In Proceedings of the 28th annual conference of the
cognitive science society.

Sanborn, A. N., Grifﬁths, T. L., & Navarro, D. J. (2006). A more rational
model of categorization. In Proceedings of the 28th annual conference of
the cognitive science society. Mahwah, NJ: Erlbaum.

Wood, F., Goldwater, S., & Black, M. (2006). A non-parametric Bayesian
the 28th IEEE

approach to spike sorting.
conference on engineering in medicine and biologicial systems.

In Proceedings of

Xu, F., & Tenenbaum, J. (2007). Word learning as Bayesian inference.

Psychological Review, 114, 245–272.

Yuille, A., & Kersten, D. (2006). Vision as Bayesian inference: Analysis by

synthesis? Trends in Cognitive Sciences, 10, 301–308.

Zipf, G. (1932). Selective studies and the principle of relative frequency in

language. Cambridge, MA: Harvard University Press.

