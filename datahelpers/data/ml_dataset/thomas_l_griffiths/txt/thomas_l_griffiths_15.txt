Cognitive Psychology 51 (2005) 334–384

www.elsevier.com/locate/cogpsych

Structure and strength in causal induction q

Thomas L. Griﬃths a,*, Joshua B. Tenenbaum b

a Department of Psychology, Stanford University, USA

b Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology,

77 Massachusetts Avenue, Cambridge, MA 02139-4307, USA

Accepted 28 September 2004

Available online 5 October 2005

Abstract

We present a framework for the rational analysis of elemental causal induction—learning
about the existence of a relationship between a single cause and eﬀect—based upon causal
graphical models. This framework makes precise the distinction between causal structure
and causal strength: the diﬀerence between asking whether a causal relationship exists and ask-
ing how strong that causal relationship might be. We show that two leading rational models of
elemental causal induction, DP and causal power, both estimate causal strength, and we
introduce a new rational model, causal support, that assesses causal structure. Causal support
predicts several key phenomena of causal induction that cannot be accounted for by other
rational models, which we explore through a series of experiments. These phenomena include
the complex interaction between DP and the base-rate probability of the eﬀect in the absence
of the cause, sample size eﬀects, inferences from incomplete contingency tables, and causal

q We thank Russ Burnett, David Lagnado, Tania Lombrozo, Brad Love, Doug Medin, Kevin Murphy,
David Shanks, Steven Sloman, and Sean Stromsten for helpful comments on previous drafts of this paper,
and Liz Baraﬀ, Onny Chatterjee, Danny Oppenheimer, and Davie Yoon for their assistance in data
collection. Klaus Melcher and David Shanks generously provided their data for our analyses. Initial
results from Experiment 1 were presented at the Neural Information Processing Systems conference,
December 2000. TLG was supported by a Hackett Studentship and a Stanford Graduate Fellowship. JBT
was supported by grants from NTT Communication Science Laboratories, Mitsubishi Electric Research
Laboratories, and the Paul E. Newton chair.

* Corresponding author. Present address: Department of Cognitive and Linguistic Sciences, Brown

University, Box 1978, Providence RI 02912, USA.

E-mail address: Tom_Griﬃths@brown.edu (T.L. Griﬃths).

0010-0285/$ - see front matter Ó 2005 Elsevier Inc. All rights reserved.
doi:10.1016/j.cogpsych.2005.05.004

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

335

learning from rates. Causal support also provides a better account of a number of existing
datasets than either DP or causal power.
Ó 2005 Elsevier Inc. All rights reserved.

Keywords: Causality; Causal induction; Computational modeling; Rational analysis; Bayesian models

The contagion spread rapidly and before its progress could be arrested, sixteen
persons were aﬀected of which two died. Of these sixteen, eight were under my
care. On this occasion I used for the ﬁrst time the aﬀusion of cold water, in the
manner described by Dr. Wright. It was ﬁrst tried in two cases. . . The eﬀects
corresponded exactly with those mentioned by him to have occurred in his
own case and thus encouraged the remedy was employed in ﬁve other cases.
It was repeated daily, and of these seven patients, the whole recovered. (James
Currie, 1798/1960, p. 430)

1. Introduction

Statistical methods for evaluating the relationships between variables were only
developed at the end of the 19th century, more than 200 years after the birth of modern
science. The foundations of physics, chemistry, biology, and medicine were all laid be-
fore formal methods for analyzing correlations or contingency tables existed. Even dif-
ﬁcult statistical problems like evaluating medical treatments were addressed by early
scientists, as the epigraph illustrates. Its author, Dr. James Currie, was an 18th century
shipÕs surgeon who later went into practice in Liverpool. After having heard a Dr. Wil-
liam Wright give an account of the eﬃcacy of being doused with cold water in treating
an extended fever, Currie conducted his own experiment, with results described above.
He was suﬃciently encouraged that he went on to use the treatment with hundreds of
other patients, publishing a detailed treatise on the matter (Currie, 1798/1960). Wash-
ing the skin of the patient is still used to ease fevers, although modern medicine cautions
against using water cold enough to induce shivering.

While the development of statistical methods for designing and analyzing exper-
iments has greatly streamlined scientiﬁc argument, science was possible before statis-
tics: in many cases, the causal relationships between variables that are critical to
understanding our world could be discovered without any need to perform explicit
calculations. Science was possible because people have a capacity for causal induc-
tion, inferring causal structure from data. This capacity is suﬃciently accurate as
to have resulted in genuine scientiﬁc discoveries, and provides the basis for the con-
struction of the intuitive theories that express our knowledge about the world. Cur-
rieÕs assessment of the water treatment is directly analogous to the kinds of causal
inferences we perform every day, such as evaluating whether taking vitamins pre-
vents us from getting sick, or whether drinking coﬀee increases our productivity.

The most basic problem of causal induction is learning that a relationship exists
between a single cause and eﬀect. This problem of elemental causal induction has been

336

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

the subject of most previous studies of human causal judgment. This simplest case is
suﬃciently constrained to allow rigorous testing of mathematical models against
peopleÕs judgments of cause–eﬀect relations. Recent accounts of elemental causal
induction have emphasized the rational basis of human learning, presenting formal
analyses of how an agent should learn about causal relationships (e.g., Anderson,
1990; Cheng, 1997; Lo´ pez, Cobos, Can˜o, & Shanks, 1998). This strategy has resulted
in several distinct mathematical models of causal judgment, none of which explains
all of the phenomena of causal induction. Consequently, there is an ongoing debate
about which model gives a better account of human judgments (e.g., Cheng, 1997;
Lober & Shanks, 2000).

In this paper, we present a framework for analyzing the computational problem of
elemental causal induction, based on causal graphical models. Causal graphical models
are a set of tools for learning and reasoning about causal relationships that has been
developed by computer scientists, statisticians, and philosophers (e.g. Pearl, 2000; Spir-
tes, Glymour, & Schienes, 1993). Our framework clariﬁes the assumptions made by
previous rational models, such as DP (Allan, 1980; Jenkins & Ward, 1965; Lo´ pez
et al., 1998) and causal power (Cheng, 1997), and results in a fundamentally diﬀerent
model of human judgments, which we call ‘‘causal support.’’ While previous models
view human judgments as reﬂecting the strength of a causal relationship, causal support
addresses the structural question of whether or not a causal relationship exists.

Causal graphical models have recently become the focus of a great deal of interest
among cognitive scientists, with several studies examining the extent to which human
causal reasoning corresponds to the qualitative assumptions of this approach (Danks
& McKenzie, submitted; Gopnik et al., 2004; Glymour, 1998, 2001; Lagnado & Slo-
man, 2002; Waldmann & Martignon, 1998). Our work extends these results by show-
ing that causal graphical models can be used to make quantitative predictions about
human behavior (e.g., Steyvers, Tenenbaum, Wagenmakers, & Blum, 2003; Tenen-
baum & Griﬃths, 2001, 2003). The framework we present here uses causal graphical
models to explicate the roles of structure and strength in the problem of causal
induction, and in deﬁning causal support, our model of human judgments.

Causal support predicts several phenomena that are problematic for other ra-
tional models. Our presentation will be organized around these phenomena. The ﬁrst
phenomenon we will consider is the interaction between covariation, measured by
DP, and the base-rate probability of the eﬀect in the absence of the cause in deter-
mining human judgments. This interaction manifests in two curious phenomena,
the ‘‘frequency illusion’’—a decrease in causal judgments as the base-rate decreases
when DP = 0 (Allan & Jenkins, 1983; Buehner, Cheng, & Cliﬀord, 2003; Shanks, Lo´ -
pez, Darby, & Dickinson, 1996)—and non-monotonic eﬀects of changes in base-rate
at other values of DP (Lober & Shanks, 2000). We will also discuss eﬀects of sample
size (White, 1998, 2002c, 2003c), inferences from incomplete contingency tables, and
causal induction from rates (c.f. Anderson & Sheu, 1995; Wasserman, 1990). No
other rational model of can explain all of these phenomena, or ﬁt as wide a range
of datasets as causal support.

The plan of the paper is as follows. First we outline the problem of elemental
causal induction in more detail, describing the experimental paradigms that are

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

337

Table 1
Contingency table representation used in elemental causal induction

Cause present (c+)
 )
Cause absent (c

Eﬀect present (e+)
N(e+, c+)
 )
N(e+, c

Eﬀect absent (e

 )

 , c+)
 )
 , c

N(e
N(e

the focus of our investigation, the two leading rational models, DP and causal power,
and some of the data that has been gathered in support of them. Then, we provide a
brief summary of causal graphical models, present our framework for analyzing the
problem of elemental causal induction, and use this to derive causal support. The
body of the paper discusses the phenomena predicted by causal support but not
by other models, explaining the statistical origins of these predictions. We close by
considering the circumstances under which we expect causal support to be most con-
sistent with human judgments, its relationship with ideas such as ‘‘reliability’’ (Bueh-
ner & Cheng, 1997; Perales & Shanks, 2003), and how this account of elemental
causal induction can be extended to shed light on other aspects of causal learning.

2. Elemental causal induction

Much psychological research on causal induction has focused upon the problem
of learning a single causal relation: given a candidate cause, C, and a candidate eﬀect,
E, people are asked to assess the relationship between C and E.1 Most studies present
information corresponding to the entries in a 2 · 2 contingency table, as in Table 1.
People are given information about the frequency with which the eﬀect occurs in the
 )
presence and absence of the cause, represented by the numbers N(e+, c+),N(e
and so forth. In a standard example, C might be injecting a chemical into a mouse,
and E the expression of a particular gene. For this case, N(e+, c+) would be the num-
 ) would be the number of
ber of injected mice expressing the gene, while N(e
uninjected mice not expressing the gene.

 , c

 , c

This contingency information is usually presented to participants in one of three
modes. Early experiments on causal induction would either explicitly provide partic-
ipants with the numbers contained in the contingency table (e.g., Jenkins & Ward,
1965), which we will refer to as a ‘‘summary’’ format, or present individual cases
one by one, with the appropriate frequencies (e.g., Ward & Jenkins, 1965), which
we will refer to as an ‘‘online’’ format. Some more recent experiments use a mode
of presentation between these two extremes, showing a list of all individual cases
simultaneously (e.g., Buehner et al., 2003; White, 2003c), which we will refer to as
a ‘‘list’’ format.

1 We will represent variables such as C, E with capital letters, and their instantiations with lowercase
  indicating that the cause or eﬀect

letters, with c+, e+ indicating that the cause or eﬀect is present, and c
is absent.

 , e

338

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

Experiments also diﬀer in the questions that are asked of the participants.
Participants can be asked to rate the strength of the causal relationship, the
probability of a causal relationship, or their conﬁdence that a causal relation-
ship exists. Understanding the eﬀects of question wording is an ongoing task
(e.g., White, 2003b), but one variable that has been shown to have a strong
eﬀect is asking counterfactual questions, such as ‘‘What is the probability that
a mouse not expressing the gene before being injected will express it after being
injected with the
et al., 2003; Collins & Shanks,
submitted).

chemical?’’

(Buehner

Causal induction tasks also vary in their treatment of the valence of the potential
cause, and the nature of the rating scale used for responses. Causes can be either
‘‘generative,’’ increasing the probability of an outcome (as in our mouse gene exam-
ple), or ‘‘preventive,’’ reducing its probability (as in the case of Dr. CurrieÕs cold
water treatment). Some experiments use exclusively generative or exclusively preven-
tive causes and ask for judgments on a nonnegative scale (e.g., 0–100), while others
mix generative and preventive causes and ask for judgments on a scale that has both
positive and negative ends (e.g.,  100 to 100).

Given the many ways in which experiments on causal judgment can diﬀer, it
is important to identify the scope of the present analysis. We will discuss exper-
iments that use all three modes of presentation, as each mode captures an aspect
of causal
induction that is important for the development of rational models:
the summary format removes memory demands and allows a deliberative infer-
ence, the online format taps intuitions about causality that are engaged by direct
interaction with data, and the list format falls between these extremes. We will
focus on experiments that require participants to make judgments about poten-
tial causes of a single kind, generative or predictive. Most of the critical datasets
in the current debate about rational models of causal induction are of this form
(e.g., Buehner & Cheng, 1997; Lober & Shanks, 2000). In Section 15, we will
consider how our framework can be extended to shed light on other issues in
causal induction, including learning about multiple potential causes, the dynam-
ics of causal judgments in online tasks, and combining generative and preventive
causes.

2.1. Rational models of elemental causal induction

In the spirit of MarrÕs (1982) computational level and AndersonÕs (1990) ra-
tional analysis, recent theories have tried to establish the task of causal induction
as a computational problem and use the optimal solution to that problem to ex-
plain human behavior. We will describe two leading rational models of causal
induction which are at the center of a debate about modeling causal judgments:
DP and causal power.

2.1.1. DP and associative strength

One common approach to modeling judgments about causal relationships is to

combine the frequencies from a contingency table in the form

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

339

DP ¼

Nðeþ; cþÞ

Nðeþ; cþÞ þ Nðe ; cþÞ  

Nðeþ; c Þ

Nðeþ; c Þ þ Nðe ; c Þ ¼ PðeþjcþÞ   Pðeþjc Þ;

ð1Þ
where P(e+|c+) is the empirical conditional probability of the eﬀect given the pres-
ence of the cause, estimated from the contingency table counts N(Æ). DP thus reﬂects
the change in the probability of the eﬀect occuring as a consequence of the occurence
of the cause. This measure was ﬁrst suggested by Jenkins and Ward (1965), subse-
quently explored by Allan (Allan, 1980, 1993; Allan & Jenkins, 1983), and has ap-
peared in various forms in both psychology and philosophy (Cheng & Holyoak,
1995; Cheng & Novick, 1990, 1992; Melz, Cheng, Holyoak, & Waldmann, 1993;
Salmon, 1980). One argument for the appropriateness of DP as a normative model
uses the fact that it is the asymptotic value of the weight given to the cause C when
the causal induction task is modeled with a linear associator trained using the Resc-
orla–Wagner (Rescorla & Wagner, 1972) learning rule (Cheng, 1997; Cheng & Holy-
oak, 1995; Chapman & Robbins, 1990; Danks, 2003; Wasserman, Elek, Chatlosh, &
Baker, 1993).

2.1.2. The power PC theory and causal power

Cheng (1997) rejected DP as a measure of causal strength because it is a mea-
sure of covariation, not causality. According to Cheng (1997) and Novick and
Cheng (2004), human judgments reﬂect a set of assumptions about causality that
diﬀer from those of purely ‘‘covariational’’ measures such as DP and conventional
statistics. ChengÕs (1997) power PC theory attempts to make these assumptions
explicit, providing an axiomatic characterization of causality and proposing that
human causal judgments correspond to ‘‘causal power,’’ the probability that C
produces E in the absence of all other causes. Causal power for a generative
cause can be estimated from contingency data, with Cheng (1997) giving the
expression:

power ¼

DP

1   Pðeþjc Þ .

ð2Þ

Causal power takes DP as a component, but predicts that DP will have a greater ef-
 ) is large. Causal power can also be evaluated for preventive causes,
fect when P(e+|c
following from a similar set of assumptions about the nature of such causes. The
causal power for a preventive cause is

power ¼   DP

Pðeþjc Þ .

ð3Þ

For preventive causes, the eﬀect of P(e+|c+) on causal power is reversed, with DP
having a greater inﬂuence when P(e+|c+) is small.

The measure of causal power in Eq. (2) can be derived from a counterfactual
treatment of ‘‘suﬃcient cause’’ (Pearl, 2000). Causal power corresponds to the
probability that, for a case in which C was not present and E did not occur,
introduced. This probability depends upon DP,
E would occur

if C was

340

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

corresponding to the raw increase in occurrences of E, but has to be normalized
by the proportion of the cases in which C could actually have inﬂuenced E. If
some of the cases already show the eﬀect, then C had no opportunity to inﬂuence
those cases and they should not be taken into account when evaluating the
introduces
strength
 ) in the denominator. Pearl (2000) derives DP from a similar
P(e
treatment of ‘‘necessary and suﬃcient cause.’’

C.
 ) = 1 P(e+|c

requirement

of

normalization

of

The

 |c

To illustrate the diﬀerence between causal power and DP, consider the problem
of establishing whether injecting chemicals into mice results in gene expression.
Two groups of 60 mice are used in two experiments evaluating the eﬀect of diﬀer-
ent chemicals on diﬀerent genes. In each experiment, one group is injected with the
chemical, and the other group receives no injection. In the ﬁrst experiment, 30 of
 ) = 0.5, and 36 of the injected mice
the uninjected mice express the gene, P(e+|c
express it, P(e+|c+) = 0.6. In the second experiment, 54 of the uninjected mice ex-
press the gene, P(e+|c
the injected mice express it,
P(e+|c+) = 1. In each case DP = 0.1, but the second set of results seem to provide
more evidence for a relationship between the chemical and gene expression. In par-
ticular, if we imagine that the frequency of gene expression among the uninjected
mice would be reproduced exactly in the other group of mice prior to injection, it
seems that the ﬁrst chemical produces gene expression in only six of the thirty mice
who would not have otherwise expressed the gene, while all of the mice not
expressing the gene in the second experiment have their fates altered by the injec-
tion. This diﬀerence is reﬂected in causal power, which is 0.2 in the ﬁrst case and 1
in the second.

 ) = 0.9, and all 60 of

2.2. The debate over rational models

DP and causal power make diﬀerent predictions about the strength of causal
relationships, and several experiments have been conducted with the aim of deter-
mining which model gives a better account of human data (e.g., Buehner &
Cheng, 1997; Collins & Shanks, submitted; Lober & Shanks, 2000; Perales &
Shanks, 2003; Shanks, 2002; Vallee-Tourangeau, Murphy, Drew, & Baker,
1998). Each model captures some of the trends identiﬁed in these experiments,
but there are several results that are predicted by only one of the models, as well
as phenomena that are predicted by neither. These negative results are almost
equally distributed between the two models, and suggest that there may be some
basic factor missing from both. The problem can be illustrated by considering two
sets of experiments: those conducted by Buehner and Cheng (1997) and Lober and
Shanks (2000).

The experiments conducted by Buehner and Cheng (1997; Buehner et al., 2003)
explored how judgments of the strength of a causal relationship vary when DP is held
constant. This was done using an experimental design adapted from Wasserman
et al. (1993), giving 15 sets of contingencies expressing all possible combinations
 ) and DP in increments of 0.25. Experiments were conducted with both
of P(e+|c
generative causes, for which C potentially increases the frequency of E as in the cases

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

341

described above, and preventive causes, for which C potentially decreases the fre-
quency of E, and with both online and summary formats. For the moment, we will
focus on the online study with generative causes (Buehner & Cheng, 1997, Experi-
ment 1B), where a total of 16 trials gave the contingency information. The results
of this experiment showed that at constant values of DP, people made judgments
 ). Furthermore, this sensitivity was consis-
that were sensitive to the value of P(e+|c
tent with the role of P(e+|c

 ) in causal power.

However, as was pointed out by Lober and Shanks (2000), the results also proved
problematic for the Power PC theory. The design used by Buehner and Cheng (1997)
provides several situations in which sets of contingencies give the same value of caus-
al power. The data are shown in Fig. 1, together with the values of DP and causal
power. DP and causal power gave r scores of .889 and .881, respectively, with scaling

P(e+|c+)
P(e+|c–)

100

8/8
8/8

6/8
6/8

4/8
4/8

2/8
2/8

0/8
0/8

8/8
6/8

6/8
4/8

4/8
2/8

2/8
0/8

8/8
4/8

6/8
2/8

4/8
0/8

8/8
2/8

6/8
0/8

8/8
0/8

Humans

∆ P

Power   

Support 

χ2  

50

0

100

50

0

100

50

0

100

50

0

100

50

0

Fig. 1. Predictions of rational models compared with the performance of human participants from
Buehner and Cheng (1997, Experiment 1B). Numbers along the top of the ﬁgure show stimulus
contingencies, error bars indicate one standard error.

342

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

parameters c = 0.98, 1.05.2 As can be seen from the ﬁgure, both DP and causal
power predict important trends in the data, but since these trends are orthogonal,
neither model provides a full account of human performance. The only sets of con-
tingencies for which the two models agree are those where DP is zero. For these
cases, both models predict negligible judgments of the strength of the causal relation-
ship. In contrast to these predictions, people give judgments that seem to decrease
 ) decreases. Similar eﬀects with DP = 0 have been observed
systematically as P(e+|c
in other studies (e.g., Allan & Jenkins, 1983; Shanks et al., 1996), where the phenom-
enon is referred to as the ‘‘frequency illusion.’’

In a further test of the two theories, Lober and Shanks (2000) conducted a series
of experiments in which either causal power or DP was held constant while the other
varied. These experiments used both online (Experiments 1–3) and summary (Exper-
iments 4–6) formats. The results showed systematic variation in judgments of the
strength of the causal relationship at constant values of causal power, in a fashion
consistent with DP. The results of Experiments 4–6 are shown in Fig. 2, together with
the values of DP and causal power. The models gave r scores of .980 and .581,
respectively, with c = 0.8, 1.1. While DP gave a good ﬁt to these data, the human
 )} of {30/30, 18/30}, {24/30, 12/
judgments for contingencies with {P(e+|c+),P(e+|c
30}, {12/30, 0/30} are not consistent with DP: they show a non-monotonic trend,
with smaller judgments for {24/30,12/30} than for either of the extreme cases. The
quadratic trend over these three sets of contingencies was statistically signiﬁcant,
but Lober and Shanks (2000) stated that ‘‘. . . because the eﬀect was non-linear, it
probably should not be given undue weight’’ (p. 209). For the purposes of Lober
and Shanks, this eﬀect was not important because it provided no basis for discrim-
ination between DP and causal power: neither of these theories can predict a non-
monotonic change in causal judgments as a function of the base-rate probability
P(e+|c

 ).

The results of Buehner and Cheng (1997) and Lober and Shanks, 2000 illustrate
that neither DP nor causal power provides a full account of peopleÕs judgments in
causal induction tasks. These are not isolated results: DP and causal power cannot
explain several other phenomena of human causal induction. One of these phenom-
ena is the eﬀect of sample size: both DP and causal power are deﬁned using the con-
ditional probabilities P(e|c), and are thus insensitive to the number of observations
expressing those probabilities. However, human judgments change as the number
of observations contributing to a contingency table varies (White, 1998, 2002c,
2003c). Another is inferences from incomplete data: people can assess causal rela-
tionships in circumstances where there is not enough information to compute the

2 In each case where we have ﬁt a computational model to empirical data, we have used a scaling
transformation to account for the possibility of non-linearities in the rating scale used by participants. This
is not typical in the literature, but we feel it is necessary to separate the quantitative predictions from a
dependency on the linearity of the judgment scale—an issue that arises in any numerical judgment task.
We use the transformation y = sign(x) abs(x)c, where y are the transformed predictions, x the raw
predictions, and c a scaling parameter selected to maximize the linear correlation between the transformed
predictions and the data. This power law transformation accommodates a range of non-linearities.

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

343

27/30
20/30

24/30
10/30

21/30
0/30

28/28
21/28

28/28
14/28

28/28
7/28

28/28
0/28

30/30
18/30

24/30
12/30

12/30
0/30

27/30
25/30

Humans

∆ P

Power   

Support 

χ2  

P(e+|c+)
P(e+|c–)
100

50

0

100

50

0

100

50

0

100

50

0

100

50

0

Fig. 2. Predictions of rational models compared with the performance of participants from Lober and
Shanks (2000, Experiments 4–6). Numbers along the top of the ﬁgure show stimulus contingencies.

conditional probabilities of the eﬀect in the presence and the absence of the cause. In
the early stages of both everyday and scientiﬁc inferences, we might be presented
with an incomplete contingency table. Neither DP nor causal power can explain
the judgments that people make from such data. Finally, people are able to learn
about causal relationships from data other than contingencies, such as the rate at
which an eﬀect is observed in the presence and absence of a cause (e.g., Anderson
& Sheu, 1995). DP and causal power are not deﬁned for rate data, even though it
is extremely similar to contingency data.

In the remainder of the paper, we will use a computational framework based on
causal graphical models to provide insight into the problems of DP and causal
power, and to derive a new model of causal induction. We will argue that the diﬃ-
culties faced by DP and causal power arise because people are often sensitive to the
structural question of whether or not a causal relationship exists, and both DP and

344

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

causal power model human judgments purely in terms of the strength of a causal
relationship. Our framework makes the distinction between structure and strength
precise, and allows us to deﬁne a new model, causal support, which addresses this
structural question. We will show that causal support accurately predicts human
judgments in all of the settings mentioned above.

3. Causal graphical models

Our framework for analyzing elemental causal induction will use causal graphical
models, a formalism for learning and reasoning about causal relationships that is a
current topic of research in computer science and statistics (e.g., Pearl, 2000; Spirtes
et al., 1993) and is beginning to be applied in cognitive science (Danks & McKenzie,
submitted; Gopnik et al., 2004; Glymour, 1998, 2001; Lagnado & Sloman, 2002;
Rehder, 2003; Steyvers et al., 2003; Tenenbaum & Griﬃths, 2001, 2003; Waldmann
& Martignon, 1998). Causal graphical models, also known as causal Bayesian net-
works or causal Bayes nets, provide a means of specifying the causal relationships
that hold among a set of variables. Our brief summary of causal graphical models
will touch on three important issues: causal structure, functional causal relation-
ships, and the diﬀerence between learning structure and estimating parameters.

3.1. Causal structure

A graphical model provides an intuitive representation for the causal structure
relating a set of variables. Nodes in the graph represent variables, and directed edges
represent causal connections between those variables (Glymour, 1998, 1999; Pearl,
2000; Spirtes et al., 1993). The result is a directed graph, with ‘‘parent’’ nodes having
arrows to their ‘‘children.’’ For example, consider the directed graphs denoted
Graph 0 and Graph 1 in Fig. 3, which we will later use in describing our framework
for elemental causal induction. Both graphs are deﬁned over three binary variables—
an eﬀect E, a potential cause C, and a background cause B, capturing the combined

Graph 0

Graph 1

B

C

B

C

E

E

Fig. 3. Directed graphs involving three variables, B, C, and E, relevant to elemental causal induction. B
represents background variables, C a potential causal variable, and E the eﬀect of interest. Graph 1, is
assumed in computing DP and causal power. Computing causal support involves comparing the structure
of Graph 1 to that of Graph 0, in which C and E are independent.

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

345

eﬀect of all other causes of E. Each graph represents a hypothesis about the causal
relations that could hold among these variables. In Graph 0, B causes E, but C has
no relationship to either B or E. In Graph 1, both B and C cause E.

3.2. Functional causal relationships

The graph structure used in a causal graphical model identiﬁes the causal relation-
ships among variables, but says nothing about the precise nature of these relation-
ships—they could be deterministic or probabilistic, and multiple causes of an
eﬀect could act independently or interact strongly. The functional form of a causal
relationship encodes all of this information, and translates a causal structure into
a probability distribution over the variables that form its nodes.

Any causal graphical model with variables {X1, . . . , Xn} implies a probability dis-
tribution of the form P(X1, . . . , Xn) = Õi P(Xi|Pa(Xi)), where Pa(Xi) is the set of par-
ents of the node associated with Xi. This factorization of the probability distribution
follows from the assumption that each variable Xi is independent of all of its non-de-
scendants in the graph when conditioned upon its causes, Pa(Xi). This assumption is
called the causal Markov condition, and the relationship between statistical depen-
dence and causation that it implies forms the basis of many algorithms for learning
causal structure (e.g., Pearl, 2000; Spirtes et al., 1993).

While causal structure determines the dependencies expressed in a distribution,
the actual probability distribution depends upon the choice of the conditional
probabilities P(Xi|Pa(Xi)). Specifying these probabilities requires choosing a param-
eterization for each node, stating how the probability distribution over that vari-
able depends upon the values of its parents. This parameterization determines
the functional form of the causal relationship. Sometimes the parameterization is
trivial—for example, in Graph 0, we need to specify only P0(E|B), where the sub-
script indicates that this probability is associated with Graph 0. This can be done
using a single numerical parameter w0 which provides the probability that the eﬀect
will be present in the presence of the background cause, P0(e+|b+; w0) = w0. How-
ever, when a node has multiple parents, there are many diﬀerent ways in which the
functional relationship between causes and eﬀects could be deﬁned. For example,
in Graph 1 we need to account for how the causes B and C interact in producing
the eﬀect E.

The conditional probability distribution associated with a node can be any prob-
abilistically sound function of its parents, including a deterministic function. Diﬀer-
ent assumptions about the kinds of mechanisms in a domain naturally lead to
diﬀerent functional forms for this dependency, so there will not be a single function-
al form that can be used to characterize all settings in which causal learning takes
place. Here, we consider three simple functional forms for the relationship between
B, C, and E: noisy-OR, noisy-AND-NOT, and linear. These functional forms char-
acterize some of the diﬀerent ways in which causes combine to inﬂuence their eﬀects,
and will be used in our analysis of previous accounts of causal induction. All three
parameterizations assume that E is a binary random variable—on each trial, E
either occurs or does not. Later in the paper, in Section 12, we will introduce a

346

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

Poisson parameterization that can be used when the available data concern the rate
at which the eﬀect occurs in an interval.

The noisy-OR parameterization (Pearl, 1988) results from a natural set of
assumptions about the relationship between cause and eﬀect. For Graph 1, these
assumptions are that B and C are both generative causes, increasing the probability
of the eﬀect; that the probability of E in the presence of just B is w0, and in the pres-
ence of just C is w1; and that, when both B and C are present, they have independent
opportunities to produce the eﬀect. This gives

  = c

P 1ðeþjb; c; w0; w1Þ ¼ 1   ð1   w0Þbð1   w1Þc.

ð4Þ
where w0, w1 are parameters associated with the strength of B, C, respectively, and
  = 0 for the purpose of arithmetic operations. This expression
b+ = c+ = 1,b
gives w0 for the probability of E in the presence of B alone, and w0 + w1 w0w1 for
the probability of E in the presence of both B and C. This parameterization is called
a noisy-OR because if w0 and w1 are both 1, Eq. (4) reduces to the logical OR function:
the eﬀect occurs if and only if B or C are present. With w0 and w1 in the range [0, 1] it
generalizes this function to allow probabilistic causal relationships.

An analogous parameterization for preventive causes can be derived from a set of
assumptions similar to those made in the noisy-OR. In the case of Graph 1, these
assumptions are that B has the opportunity to produce E with probability w0, and
C independently prevents E from occurring with probability w1. The resulting
noisy-AND-NOT generalizes the logical statement that E will occur if B occurs
and not C, allowing the inﬂuence of these factors to be probabilistic. The conditional
probability can be written as
P 1ðeþjb; c; w0; w1Þ ¼ wb

ð5Þ
which gives w0 for the probability of E in the presence of just B, and w0(1 w1) when
both B and C are present. As with the noisy-OR, both w0 and w1 are constrained to
lie in the range [0,1].

0ð1   w1Þc;

Finally, a linear parameterization of Graph 1 assumes that the probability of E
occuring is a linear function of B and C. This corresponds to assuming that the pres-
ence of a cause simply increases the probability of an eﬀect by a constant amount,
regardless of any other causes that might be present. There is no distinction between
generative and preventive causes. The result is

P 1ðeþjb; c; w0; w1Þ ¼ w0  b þ w1  c.

ð6Þ
This parameterization requires that we constrain w0 + w1 to lie between 0 and 1 to
ensure that Eq. (6) results in a legal probability distribution. Because of this depen-
dence between parameters, this parameterization is not normally used for causal
graphical models. A similar set of assumptions can be captured using the more stan-
dard logistic parameterization (e.g., Neal, 1992), in which the parameters for diﬀer-
ent causes can vary independently from strongly positive (generative) to strongly
negative (preventive). We consider the linear parameterization here because, as we
show in the next section, it implicitly underlies one of the leading psychological mod-
els of causal judgment.

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

347

3.3. Structure learning and parameter estimation

Constructing a causal graphical model from a set of observed data involves two
kinds of learning: structure learning and parameter estimation. Structure learning re-
fers to identiﬁcation of the topology of the causal graph, while parameter estimation
involves determining the parameters of the functional relationships between causes
and eﬀects for a given causal structure. Structure learning is arguably more funda-
mental than parameter estimation, since the parameters can only be estimated once
the structure is known. Learning the causal structure among many variables is a dif-
ﬁcult computational problem, as the number of possible structures is a super-expo-
nential function of the number of variables.

There are several approaches to parameter estimation in graphical models (e.g.,
Heckerman, 1998). The simplest approach is maximum-likelihood estimation. For
a particular graphical structure and parameterization, the likelihood of a set of
parameters given data D is the probability of D under the distribution speciﬁed by
that structure, parameterization, and choice of parameter values. For example, if
D is summarized by contingencies N(e, c), the likelihood for the parameters w0, w1
in Graph 1 is given by

Y

P 1ðDjw0; w1; Graph 1Þ ¼

P 1ðejc; bþ; w0; w1ÞNðe;cÞ

;

e;c

ð7Þ

where the product is taken over all pairs of e+, e
estimate is a choice of values for w0, w1 that maximizes P1(D|w0, w1, Graph 1).

  and c+, c

 . A maximum-likelihood

Structure learning attempts to identify the causal structure underlying a set of ob-
served data. There are two major approaches to structure learning: constraint-based
learning, and Bayesian inference. In constraint-based learning, constraints on possi-
ble causal structures are inferred by evaluating the statistical dependencies among
variables, and sets of causal structures that satisfy these constraints are logically de-
rived from these constraints (e.g., Pearl, 2000; Spirtes et al., 1993). In contrast, the
Bayesian approach to structure learning evaluates each causal structure in terms
of the probability it assigns to a dataset. By integrating over the values that param-
eters could assume, it is possible to compute the probability of a dataset given a
graphical structure without committing to a particular choice of parameter values
(e.g., Cooper & Herskovits, 1992). This computation provides P(D|Graph i), and
a posterior probability distribution over graphs, P(Graph i|D) can be obtained by
applying BayesÕ rule:

PðGraph ijDÞ / PðDjGraph iÞPðGraph iÞ;

ð8Þ

where P(Graph i) is a prior probability distribution over graphs. Often, priors are
either uniform (giving equal probability to all graphs), or give lower probability
to more complex structures. Bayesian structure learning proceeds by either
searching the space of structures to ﬁnd that with the highest posterior probabil-
ity (Friedman, 1997; Heckerman, 1998), or evaluating particular causal relation-
ships by integrating over the posterior distribution over graphs (Friedman &
Koller, 2000).

348

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

4. A framework for elemental causal induction

Framing the problem of causal induction in terms of causal graphical models re-
veals that it has two components: structure learning and parameter estimation. Giv-
en an always-present background variable B, a potential cause C, and a potential
eﬀect E, together with contingency information about the co-occurrence of C and
E, the structure learning component is the assessment of whether or not a causal
relationship in fact exists between C and E. This can be formalized as a decision be-
tween the structures Graph 1 and Graph 0 in Fig. 3. The parameter estimation com-
ponent is the assessment of the strength of this relationship: assuming Graph 1 is
appropriate, and determining the value of the parameter describing the relationship
between C and E.

The distinction between causal structure and causal strength is important in sci-
entiﬁc inference. When scientists investigate a phenomenon, they are typically inter-
ested in two questions: whether an eﬀect exists, and how strong this eﬀect might be.
Diﬀerent statistical tools are used for answering these questions. In the frequentist
approach to statistics commonly used in psychological experimentation, statistical
hypothesis testing is applied to the former, and measures of eﬀect size to the latter.
Statistical signiﬁcance is a function of both eﬀect size and sample size. As a conse-
quence, there is no logically necessary relationship between causal structure and
apparent causal strength: small eﬀects can be statistically signiﬁcant, and large eﬀects
in small samples may not be supported by suﬃcient data to guarantee rejection of a
null hypothesis.

In this section, we will show that the distinction between structure and strength
can be used to shed light on rational models of human causal induction. Both DP
and causal power address only the parameter estimation component of elemental
causal induction, providing measures of the strength of a causal relationship. We will
describe a rational model that addresses the structure learning component, which we
term ‘‘causal support.’’

4.1. Causal induction as parameter estimation: DP and causal power

The two rational models at the heart of the current theoretical debate about ele-
mental causal induction address the same component of the underlying computa-
tional problem in fundamentally similar ways. Both DP and causal power are
maximum-likelihood estimates of the causal strength parameter w1 in Graph 1,
but under diﬀerent parameterizations (Tenenbaum & Griﬃths, 2001). As shown in
Appendix A, DP corresponds to the linear parameterization (Eq. (6)), whereas causal
power for generative causes corresponds to the noisy-OR parameterization (Eq. (4))
and for preventive causes corresponds to the noisy-AND-NOT parameterization
(Eq. (5)). Glymour (1998) also showed that causal power corresponds to a direct esti-
mate for the strength parameter in a noisy-OR. Since they are estimates of the
parameters of a ﬁxed graphical structure, DP and causal power both measure the
strength of a causal relationship, based upon the assumption that the relationship
exists.

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

349

As point estimates of a parameter, DP and causal power share several properties.
Firstly, they do not answer the question of whether or not a causal relationship ex-
ists. Large values of DP or causal power are likely to be associated with genuine
causal relationships, but small values are non-diagnostic. Shanks (1995a, p. 260)
notes this, pointing out that while DP for the eﬀect of smoking on lung cancer is
small, around 0.00083, ‘‘no one would deny that the relationship between smoking
and lung cancer is an important one.’’ This relates to a second property of both
DP and causal power: these measures contain no information about uncertainty in
the estimates involved. This uncertainty is crucial to deciding whether a causal rela-
tionship actually exists. Even a small eﬀect can provide strong evidence for a causal
relationship, provided its value is estimated with high certainty. The insensitivity to
sample size exhibited by DP and causal power is one consequence of not incorporat-
ing uncertainty.

Identifying both DP and causal power as maximum-likelihood parameter esti-
mates also helps to illustrate how these measures diﬀer: they make diﬀerent
assumptions about the functional form of a causal relationship. The linear relation-
ship assumed by DP seems less consistent with the intuitions people express about
causality than the noisy-OR, an important insight which is embodied in ChengÕs
(1997) power PC theory. ChengÕs (1997) distinction between ‘‘causal’’ and ‘‘covari-
ational’’ measures turns on this fact: she views the noisy-OR parameterization as
resulting from the correct set of assumptions about the nature of causality, and
it is the use of this parameterization that distinguishes the Power PC theory from
covariation-based accounts. We suspect that the appropriate parameterization for
the relationship between a cause and its eﬀects will depend upon an individualÕs
beliefs about the causal mechanism by which those eﬀects are brought about.
For some causal mechanisms, other parameterizations may be more appropriate
than the noisy-OR.

4.2. Causal induction as structure learning: causal support

In terms of the graphical models in Fig. 3, DP and causal power are both con-
cerned with the problem of estimating the parameters of Graph 1. However, much
of human causal induction seems to be directed at the problem of inferring the qual-
itative causal structure responsible for a set of observations. In the case of elemental
causal induction, this problem reduces to deciding whether a set of observations were
generated by Graph 0, in which C does not cause E, or Graph 1, in which C causes E.
This binary decision is a result of the deterministic nature of the existence of causal
relationships—either a relationship exists or it does not—a property of causality that
is not captured by considering only the strength of a causal relationship (c.f. Gold-
varg & Johnson-Laird, 2001).

The structural inference as to whether the data were generated by Graph 0 or
Graph 1 can be formalized as a Bayesian decision. Making this decision requires
evaluating the evidence the data provide for a causal relationship between C and
E—determining the extent to which those data are better accounted for by Graph
1 than Graph 0. Having speciﬁed two clear hypotheses about the source of a set

350

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

of contingency data D, we can then compute the posterior probabilities of Graphs 0
and 1 by applying BayesÕ rule. The posterior probability of Graph 1 indicates the ex-
tent to which a rational learner should believe in the existence of a causal relation-
ship after observing D, but it may be more appropriate to model human judgments
using a directly comparative measure, such as the log posterior odds (c.f. Anderson,
1990; Shiﬀrin & Steyvers, 1997). In log odds form, BayesÕ rule is

PðGraph 1jDÞ
PðGraph 0jDÞ ¼ log

PðDjGraph 1Þ
PðDjGraph 0Þ þ log

PðGraph 1Þ
PðGraph 0Þ ;

log

ð9Þ

ð10Þ

where the left-hand side of the equation is the log posterior odds, and the ﬁrst and
second terms on the right-hand side are the log likelihood ratio and the log prior
odds, respectively.

BayesÕ rule stipulates how a rational learner should update his or her beliefs given
new evidence, with the log posterior odds combining prior beliefs with the implica-
tions of the evidence. The eﬀect of data D on the belief in the existence of a causal
relationship is completely determined by the value of the log likelihood ratio. The log
likelihood ratio is thus commonly used as a measure of the evidence that data pro-
vide for a hypothesis, and is also known as a Bayes factor (Kass & Raﬀerty, 1995).
We will use this measure to deﬁne ‘‘causal support,’’ the evidence that data D pro-
vide in favor of Graph 1 over Graph 0

support ¼ log

PðDjGraph 1Þ
PðDjGraph 0Þ .

To evaluate causal support, it is necessary to compute P(D|Graph 1) and P(D|Graph 0).
Using a procedure described in Appendix A, it is possible to compute these proba-
bilities without committing to particular values of the parameters w0 and w1 by inte-
grating over all possible values these parameters could assume.

In computing causal support for binary-valued eﬀects, we use the noisy-OR
parameterization (Eq. (4)) for generative causes, and the noisy-AND-NOT param-
eterization (Eq. (5)) for preventive causes. This choice of parameterizations seems
appropriate for capturing the assumptions behind problems like evaluating the
inﬂuence of chemicals on gene expression, where each cause should have an inde-
pendent opportunity to inﬂuence the eﬀect. Since causal power is a maximum-like-
lihood estimator of w1 under this parameterization, this results in a relationship
between causal support and causal power. Speaking loosely, causal support is
the Bayesian hypothesis test for which causal power is an eﬀect size measure: it
evaluates whether causal power is signiﬁcantly diﬀerent from zero. Causal support
can also be deﬁned for models with diﬀerent functional dependencies and with dif-
ferent kinds of variables, a fact that we will exploit in Section 12 when we consider
causal learning from rates.

As illustrated in Fig. 4, the major determinant of causal support is the extent to
which the posterior distribution over w1 places its mass away from zero. The contin-
gency data for the top three cases shown in the ﬁgure all result in the same estimate
of causal power (approximately the peak of the posterior distribution on w1), but
increasing the number of observations contributing to these contingencies decreases

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

351

2/4
0/4

10/20
 0/20

25/50
 0/50

0/4
0/4

 0/20
 0/20

 0/50
 0/50

30/30
18/30

24/30
12/30

12/30
 0/30

Support

y
t
i
l
i

b
a
b
o
r
p

 
r
o
i
r
e

t
s
o
p

 
l

i

a
n
g
r
a
M

0

0.5
 w1 

1

Fig. 4. Marginal posterior distributions on w1 and values of causal support for six diﬀerent sets of
contingencies. The ﬁrst three sets of contingencies result in the same estimates of DP and causal power, but
diﬀerent values of causal support. The change in causal support is due to the increase in sample size, which
reduces uncertainty about the value of w1. As it becomes clear that w1 takes on a value other than zero, the
evidence for Graph 1 increases, indicated by the increase in causal support. The second set of three
contingencies shows that increasing sample size does not always result in increased causal support, with
greater certainty that w1 is zero producing a mild decrease in causal support. The third set of three
contingencies illustrates how causal support and causal power can diﬀer. While the peak of the distribution
over w1, which will be close to the value of causal power, decreases across the three examples, causal
support changes in a non-monotonic fashion.

uncertainty about the value of w1. It thus becomes more apparent that w1 has a value
greater than zero, and causal support increases. However, higher certainty does not
always result in an increase in causal support, as shown by the next three cases in the
ﬁgure. Causal power is zero for all three cases, and once again the posterior distri-
bution shows higher certainty when the number of observations is large. Greater
conﬁdence that w1 should be zero now results in a decrease in causal support,
although the eﬀect is weaker than in the previous case. The last three cases illustrate
how causal support can diﬀer from causal power. The contingencies {30/30,18/30}
suggest a high value for w1, with relatively high certainty, and consequently strong
causal support; {24/30,12/30} suggest a lower value of w1, with less certainty, and
less causal support; and {12/30, 0/30} produces an even lower value of w1, but the
higher certainty that this value is greater than zero results in more causal support.

352

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

4.3. Approximating causal support with v2

The standard frequentist analysis of contingency tables is PearsonÕs (1904/1948)
v2 test for independence, or the related likelihood ratio test G2 (e.g., Wickens,
1989). The use of the v2 test as a model for human causal judgment was suggested
in the psychological literature (e.g., Allan, 1980), but was rejected on the grounds
that it neglects the kind of asymmetry that is inherent in causal relationships, provid-
ing information solely about the existence of statistical dependency between the two
variables (Allan, 1980; Lo´pez et al., 1998; Shanks, 1995b). v2 also makes no commit-
ment about the functional form of the relationship between cause and eﬀect: it sim-
ply detects any kind of statistical dependency between C and E. These weak
assumptions are important for v2 to be useful as a statistical test across a wide range
of settings, but they produce problems for it as a model of human causal judgments.
As a measure of the evidence for a particular dependency structure, v2 is related to
causal support. In Appendix A, we show that under certain conditions causal sup-
port can be approximated by PearsonÕs v2 test for independence. This approximation
only holds when the contingency table contains a large number of observations, and
the potential cause has a weak eﬀect. v2 should be treated with caution as a model of
causal induction, for exactly the reasons identiﬁed above: it is a symmetric test of sta-
tistical dependency, while causal support postulates a speciﬁc form and direction for
a causal relationship. The diﬀerent assumptions behind these two approaches can re-
sult in quite diﬀerent predictions.

4.4. Summary

Causal induction involves two problems: evaluating whether or not a causal rela-
tionship exists, and establishing the strength of that relationship. Causal graphical
models provide the tools for treating both of these problems formally, in terms of struc-
ture learning and parameter estimation. In the remainder of the paper, we will argue
that the focus on parameter estimation in previous models of elemental causal induc-
tion is responsible for the inability of these models to account for several trends in hu-
man judgments, and we will show that causal support fares better in explaining these
phenomena. We will examine in detail ﬁve phenomena that are problematic for existing
models, but are predicted by causal support. These phenomena were introduced in Sec-
tion 2.2: the interaction between DP and the base-rate probability of the eﬀect,
 ), as demonstrated in Buehner and Cheng (1997); non-monotonic judgments
P(e+|c
as a result of changes in base-rate, as seen by Lober and Shanks (2000); eﬀects of sample
size (e.g., White, 1998, 2002c, 2003c); inferences from contingency tables with empty
cells; and causal induction from rates (cf. Wasserman, 1990; Anderson & Sheu, 1995).

5. Interaction between DP and P(e+|c 
)

In Section 2.2, we discussed results from Experiment 1B of Buehner and Cheng
(1997; later published in Buehner et al., 2003) which are shown in Fig. 1. This

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

353

experiment used an online format, and produced trends that were independently
problematic for both DP and causal power, as well as a trend that neither model
could predict: peopleÕs judgments at DP = 0 decrease as the base-rate probability
 ), decreases. The fundamental problem in explaining the results
of the eﬀect, P(e+|c
of Buehner and Cheng (1997) is accounting for the interaction between DP and the
base-rate probability in producing human judgments. DP predicts no interaction,
and causal power predicts the simple relationship given in Eq. (2), but neither of
these predictions matches human judgments. We will show that causal support is
able to correctly predict this interaction, demonstrating that the model can capture
the trends found by Buehner and Cheng (1997).

Causal support correctly predicts the interaction between DP and P(e+|c

Fig. 1 shows the data from Buehner and Cheng (1997, Experiment 1B) together
with predictions of four models: DP, causal power, causal support, and v2. As noted
in Section 2.2, both DP and causal power capture some trends in the data, but miss
others, resulting in correlations of r = .889 and r = .881. Causal support provides the
best quantitative account of this dataset, r = .968, c = .668, and accounts for all of
the major trends in human judgments, including those at DP = 0. Due to the small
samples, v2 gives a poor approximation to causal support, and a correlation of
r = .889, c = .596.
 ) in
inﬂuencing peopleÕs judgments. In particular, it is the only model that predicts hu-
man judgments at DP = 0. The explanation for these predictions is not that there
 ) decreases, but rather that
is decreasing evidence for a causal relationship as P(e+|c
 ) = 1, and
there is little evidence for or against a causal relationship when P(e+|c
 ) decreases. This account
increasing evidence against a causal relationship as P(e+|c
depends on the assumption that the causal relationship—if it exists—is generative
(increasing the probability of the eﬀect, rather than preventing it). At one extreme,
 )} = {8/8, 8/8}, all mice expressed the gene irrespective of
when {P(e+|c+), P(e+|c
treatment, and intuitively there should be no evidence for a causal relationship.
But there can also be no evidence against a (generative) causal relationship, because
of a complete ‘‘ceiling’’ eﬀect: it is impossible for the cause to increase the probability
 ) = 1. This uncertainty in causal
of E occurring above its baseline value when P(e+|c
 ) = 1 and DP = 0 is predicted by both causal support, which
judgment when P(e+|c
is near 0, and also (as Cheng, 1997, points out) by causal power, which is undeﬁned
there.
 )
decreases. Causal support becomes increasingly negative as the ceiling eﬀect weakens
and the observation that DP = 0 provides increasing evidence against a generative
 ) = 0/8, no untreated mice
causal relationship. At the other extreme, when P(e+|c
expressed the gene, and there are eight opportunities for a causal relationship to
manifest itself in the treated mice if such a relationship in fact exists. The fact that
the eﬀect does not appear in any treated mice, P(e+|c+) = 0/8, strongly suggests that
the drug does not cause gene expression. The intermediate cases provide intermediate
evidence against a causal relationship. The contingencies {2/8, 2/8} oﬀer six chances
for the treatment to have an eﬀect, and the fact that it never does so is slightly weaker
evidence against a relationship than in the {0/8, 0/8} case, but more compelling than

Only causal support, however, predicts the gradient of judgments as P(e+|c

354

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

for {6/8, 6/8}, where the cause only has two chances to manifest itself and the obser-
vation that DP = 0 could easily be a coincidence. This gradient of uncertainty shapes
the Bayesian structural inference underlying causal support, but it does not impact
the maximum-likelihood parameter estimates underlying causal power or DP.
 ), showing
the posterior distribution on w1 for each set of contingencies. Greater certainty in the

Fig. 5 reveals why causal support is sensitive to the change in P(e+|c

y
t
i
l
i

b
a
b
o
r
p
 
r
o
i
r
e
t
s
o
p
 
l
a
n
g
r
a
M

i

8/8
8/8

6/8
6/8

4/8
4/8

2/8
2/8

0/8
0/8

8/8
6/8

6/8
4/8

4/8
2/8

2/8
0/8

8/8
4/8

6/8
2/8

4/8
0/8

8/8
2/8

6/8
0/8

8/8
0/8

0

0.5
 w1 

1

0

50

Support

100

Fig. 5. Marginal posterior distributions on w1 and values of causal support for the contingencies used in
Buehner and Cheng (1997, Experiment 1B).

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

355

value of w1 is reﬂected in a more peaked distribution, and causal support becomes
larger as it becomes more apparent that w1 is greater than zero. The top ﬁve plots
show the cases where DP = 0. Despite the fact that DP is the same in these ﬁve cases,
the posterior distributions on w1 look quite diﬀerent. Four of the distributions have a
maximum at w1 = 0, consistent with the estimate of causal power for these contin-
gencies, but they diﬀer in the certainty of the estimate, reﬂected in the breadth of
 ) increases, fewer observations contribute
the posterior about this point.3 As P(e+|c
to the estimate of w1, and the posterior becomes more diﬀuse, being almost uniform
for P(e+|c

 ) = 1.

This explanation can also be applied to a similar trend that emerges with preven-
tive causes. The results of Experiment 1A of Buehner and Cheng (1977; also pub-
lished in Buehner et al., 2003), are shown in Fig. 6. This experiment followed a
design analogous to Experiment 1B, but with preventive causes. Here, peopleÕs judg-
 ) increases. Causal support, parameterized with
ments for DP = 0 decrease as P(e+|c
a noisy-AND-NOT (Eq. (5)) rather than a noisy-OR, provides the best account of
these data, including the trend at DP = 0, with r = .922, c = 0.537. Causal power
performs similarly, r = .912, c = 1.278, while DP gives r = .800, c = 0.943 and v2
gives r = .790, c = 0.566. The explanation for the predictions of causal power is sim-
ilar to the generative case, although now the ‘‘ceiling eﬀect’’ is a ‘‘ﬂoor eﬀect’’: as
 ) increases there is more opportunity for a non-zero value of w1 to be
P(e+|c
demonstrated.

6. Non-monotonic eﬀects of P(e+|c 

)

Accounting for the interaction between DP and the base-rate probability,
 ), is fundamental to explaining the results of Buehner and Cheng (1997). It
P(e+|c
is also important in explaining other phenomena of causal induction. The second
dataset discussed in Section 2.2 was Experiments 4–6 from Lober and Shanks
(2000), shown in Fig. 2. DP accounts for these data quite well, reﬂected in the high
correlation coeﬃcient, r = .980, c = 0.797, while causal power does poorly, r = .581,
c = 1.157. However, neither of these models can predict the non-monotonic eﬀect of
 )} pairs {30/30, 18/30}, {24/30, 12/30}, {12/
P(e+|c
30, 0/30}. A similar, but weaker, trend can be seen in the online data of Buehner and
Cheng (1997, Experiment 1B), shown in Fig. 1, for the contingencies {8/8,4/8},{6/
8,2/8},{4/8,0/8}. These non-monotonic trends cannot even be predicted by models
that form linear combinations of the entries in a contingency table, such as those
of Anderson and Sheu (1995) and Schustack and Sternberg (1981), despite their
many free parameters and great ﬂexibility.

 ) seen with the {P(e+|c+),P(e+, c

3 The distribution for w1 when P(e+|c+) = P(e+|c

 ) = 1 is mostly ﬂat but has a very slight peak at w1 = 1,
despite causal power being undeﬁned for this case. This is because there is also uncertainty in the value of
w0. If w0 actually takes on any value less than 1, and the large number of occurrences of the eﬀect in the
absence of the cause is just a coincidence, then the large number of occurrences of the eﬀect in the presence
of the cause still needs to be explained, and the best explanation is that w1 is high.

356

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

P(e+|c+)
P(e+|c–)

100

8/8
8/8

6/8
6/8

4/8
4/8

2/8
2/8

0/8
0/8

6/8
8/8

4/8
6/8

2/8
4/8

0/8
2/8

4/8
8/8

2/8
6/8

0/8
4/8

2/8
8/8

0/8
6/8

0/8
8/8

Humans

∆ P

Power   

Support 

χ2  

50

0

100

50

0

100

50

0

100

50

0

100

50

0

Fig. 6. Predictions of rational models compared with the performance of human participants from
Buehner and Cheng (1997, Experiment 1A). Numbers along the top of the ﬁgure show stimulus
contingencies, error bars indicate one standard error.

Causal support gives the best quantitative ﬁt to this dataset, r = .994, c = 0.445,
with v2 performing similarly, r = .993, c = 0.502. Both causal support and v2 predict
non-monotonic trends, as shown in Fig. 2. The intuitive reasons for these predictions
were mentioned when discussing Fig. 4, which uses exactly the same set of contingen-
cies: while {24/30, 12/30} suggests a higher value of causal power than {12/30, 0/30},
such a diﬀerence in contingencies is more likely to arise by chance. As with the expla-
nation of predictions for DP = 0 given in Section 5, the high certainty in the value of
w1 for {12/30, 0/30} results partly from the low value of P(e+|c

 ).

The non-monotonic trend observed in Experiments 4–6 of Lober and Shanks
(2000) did not appear in their Experiments 1–3, despite the use of the same contin-
gencies, as shown in Fig. 7. The only diﬀerence between these two sets of experiments

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

357

27/30
20/30

24/30
10/30

21/30
0/30

28/28
21/28

28/28
14/28

28/28
7/28

28/28
0/28

30/30
18/30

24/30
12/30

12/30
0/30

27/30
25/30

Humans

∆ P

Support 

χ2

  

P(e+|c+)
P(e+|c–)
100

50

0

100

50

0

100

50

0

100

50

0

Fig. 7. Predictions of rational models compared with the performance of participants from Lober and
Shanks (2000, Experiments 1–3). Numbers along the top of the ﬁgure show stimulus contingencies, but the
results are constructed by averaging over the blocks of trials seen by individual subjects, in which
contingencies varied.

was the presentation format, with an online format being used in Experiments 1–3,
and a summary format in Experiments 4–6. This presents a challenge for the expla-
nation based on causal support given above. However, we will argue that this dis-
crepancy can be resolved through a ﬁner-grained analysis of these experiments.

Catena, Maldonado, and Ca´ndido (1998) and Collins and Shanks (2002) both
found that peopleÕs judgments in online experiments are inﬂuenced by response fre-
quency. Speciﬁcally, people seem to make judgments that are based upon the infor-
mation presented since their last judgment, meaning that ‘‘primacy’’ or ‘‘recency’’
eﬀects can be produced by using diﬀerent response schedules (Collins & Shanks,
2002). Lober and Shanks (2000) used a procedure in which participants made judg-
ments after blocks of trials, with six blocks of length 10 in Experiment 1, two blocks
of length 18, and one of length 20 in Experiment 2, and 3 blocks of length 20 in
Experiment 3. The actual trials presented in each block were selected at random.
Thus, while the overall contingencies might match those used in Experiments 4–6,
the contingencies contributing to any individual judgment varied. This may account
for the diﬀerence between the results of the two experiments. In particular, the small-
er sample sizes contributing to the contingencies may aﬀect peopleÕs judgments.

358

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

Table 2
Correlations of rational models with results from Lober and Shanks (2000)

Experiment 1
Experiment 2
Experiment 3
Overall means

DP

0.354
0.462
0.336
0.695

Support

0.350
0.465
0.382
0.895

v2

0.354
0.471
0.303
0.829

Note. Boldface indicates highest correlation in each row.

To test this hypothesis, we used the records of the individual trials seen by the par-
ticipants in these experiments to establish the contingencies each participant saw in
each block, and evaluated the performance of diﬀerent models in predicting the judg-
ments of individual participants for each block from these contingencies.4 The results
of these analyses are given in Table 2. The correlations are lower than those reported
elsewhere in the paper because they concern the responses of individual participants
rather than average scores. While all models did equally well in predicting the results
of Experiments 1 and 2, causal support gave a better account of the results of Exper-
iment 3, with a correlation of r = .382 as compared to r = .336 for DP, and r = .303
for v2. The model predictions, averaged across blocks and participants in the same
fashion as the data, are shown in Fig. 7. The mean values of causal support do
not show any predicted non-monotonicity, in accord with the data. As shown in Ta-
ble 2, the mean causal support correlates better with the mean human judgments
than the mean predictions of any other model, with r = .895 for causal support,
r = .695 for DP, and r = .829 for v2.

There are two reasons why causal support does not predict a non-monotonic
trend for Experiments 1–3: smaller samples, and variation in observed contingencies.
The eﬀect of sample size is simple: causal support predicts a non-monotonic trend for
contingencies derived from 30 trials in each condition, but this trend is almost com-
pletely gone when there are only 10 trials in each condition. Small samples provide
weaker evidence that the strength of the cause is diﬀerent from zero in all conditions,
with {12/30, 0/30} and {24/30, 12/30} giving equivalently weak support for a causal
relationship. The eﬀect of variation in observed contingencies is more complex. Since
both DP and causal power are estimated from the conditional probabilities P(e|c),
and the empirical probabilities give unbiased estimates of the true probabilities, aver-
aging across many sets of contingencies generated according to those probabilities
gives mean values that approximate the true DP and causal power. Causal support
is a more complex function of observed contingencies, and averaging causal support
across a set of samples produces diﬀerent results from computing causal support
from the average contingencies. In particular, variation in the contingencies within

4 The raw data from Lober and Shanks (2000) were supplied by Klaus Melcher. The models compared in
this section were ﬁt using the same scaling parameter for all participants within the same experiment.
Causal power was not computed for this comparison, as the presence of extreme contingencies in several
cases resulted in undeﬁned values of causal power, interfering with correlations and averaging.

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

359

blocks results in some blocks providing very weak evidence for a causal relationship
(for example, in the {12/30, 0/30} condition, one participant saw a block in which the
cause was present on eight trials, but the eﬀect occurred on only one of these trials).
Such results have the greatest eﬀect in the {12/30, 0/30} condition, and the mean
causal support in this condition consequently ends up slightly lower than causal sup-
port estimated from mean contingencies.
 ) appears in both Lober and Shanks
(2000, Experiments 4–6) and Buehner and Cheng (1997), it has not been the principal
target of any experiments. Since no existing model of elemental causal induction can
predict this phenomenon, conﬁrming its existence would provide strong evidence in
favor of causal support. Experiment 1 was designed to explore this non-monotonic
eﬀect further.

Although a non-monotonic eﬀect of P(e+|c

7. Experiment 1

7.1. Method

7.1.1. Participants

One hundred and eight Stanford undergraduates participated for course credit.

7.1.2. Stimuli

The contingencies used in the experiment are shown in Fig. 8. They included three
 ), and
sets of three contingencies at ﬁxed values of DP but diﬀerent values of P(e+|c
several distractors. The sets of contingencies with ﬁxed DP used DP = 0.40,
 ) within these sets, so
DP = 0.07 and DP = 0.02. DP predicts no eﬀect of P(e+|c
any eﬀect provides evidence against this model. Causal power predicts a monotonic
 ) increases, and causal support predicts a
increase in peopleÕs judgments as P(e+|c
non-monotonic trend in the ﬁrst two sets of contingencies, and a monotonic increase
 ) in the third. Finding non-monotonic eﬀects in the ﬁrst two sets of con-
with P(e+|c
tingencies would thus provide evidence for causal support over causal power.

7.1.3. Procedure

The experiment was conducted in survey form. The instructions placed the prob-

lem of causal induction in a medical context:

Imagine that you are working in a laboratory and you want to ﬁnd out whether
certain chemicals cause certain genes to be expressed in mice. Below, you can see
laboratory records for a number of studies. In each study, a sample of mice were
injected with a certain chemical and later examined to see if they expressed a par-
ticular gene. Each study investigated the eﬀects of a diﬀerent chemical on a diﬀer-
ent gene, so the results from diﬀerent studies bear no relation to each other.
Of course, these genes may sometimes be expressed in animals not injected with
a chemical substance. Thus, a sample of mice who were not injected with any
chemical were also checked to see if they expressed the same genes as the

360

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

100
60

70
30

40
0

100
93

53
46

7
0

100
98

51
49

2
0

10
10

100
100

74
72

90
83

7
93

Humans

∆ P

Power   

Support 

χ2  

N(e+,c+)
N(e+,c–)

20

10

0

20

10

0

20

10

0

20

10

0

20

10

0

Fig. 8. Predictions of rational models compared with results of Experiment 1. Numbers along the top of
the ﬁgure show stimulus contingencies. These numbers give the number of times the eﬀect was present out
of 100 trials, for all except the last column, where the cause was present on 7 trials and absent on 193. The
ﬁrst three groups of contingencies are organized to display non-monotonicities in judgments, the last
group contains distractor stimuli. Error bars indicate one standard error.

injected mice. Also, some chemicals may have a large eﬀect on gene expression,
some may have a small eﬀect, and others, no eﬀect.

Participants were then asked for ratings on a total of 14 diﬀerent contingency

structures. The instructions for producing the ratings were:

For each study, write down a number between 0 and 20, where 0 indicates that
the chemical DOES NOT CAUSE the gene to be expressed at all, and 20 indi-
cates that the chemical DOES CAUSE the gene to be expressed every time.

Each participant completed the survey as part of a booklet of unrelated

experiments.

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

361

7.2. Results and discussion

p<.005,

for

DP = 0.07,

The results are shown in Fig. 8. As predicted, there was a statistically signiﬁcant
 ) in all three sets of contingencies with ﬁxed DP. For DP = 0.40,
eﬀect of P(e+|c
F(2,214) = 6.61, MSE = 17.43,
F(2,214) = 3.82,
MSE = 26.93, p<.05, for DP = .02, F(2,214) = 6.06, MSE = 11.27, p < .005. Since
a quadratic trend analysis would only test deviation from linearity, and not non-
 ) in each of these sets of contingen-
monotonicity, we evaluated the eﬀect of P(e+|c
 ). The response
cies by testing each pair of means with neighboring values of P(e+|c
for {40/100, 0/100} was signiﬁcantly greater than that
for {70/100, 30/100},
t(107) = 3.00, p < .005, and {70/100,30/100} was signiﬁcantly less than {100/
100, 60/100},
trend at
DP = .40. The response for {7/100, 0/100} was signiﬁcantly greater than that for
{53/100, 46/100}, t(107) = 2.13, p < .05, and {53/100, 46/100} was signiﬁcantly less
than {100/100, 93/100}, t(107) = 2.70, p < .01, indicating a non-monotonic trend at
DP = 0.07. At DP = .02, {2/100, 0/100} was greater
than {51/100, 49/100},
t(107) = 4.05, p < .001, but there was no signiﬁcant diﬀerence between {51/100, 49/
100} and {100/100, 98/100}, t(107) = 0.55, p = .58, providing no evidence for non-
monotonicity.

indicating a non-monotonic

t(107) = 3.81, p < .001,

The results of this experiment suggest that the non-monotonicitic trend seen by
Lober and Shanks (2000) is a robust aspect of human judgments, even though it
may be a small eﬀect. Such trends can be used to assess models of causal induction.
Causal support, v2, and DP gave similarly high correlations with the experimental
results, with r = .952, c = .604, r = .965, c = .446, and r = .943, c = .568, respective-
ly. Causal power performed far worse, with r = .660, c = .305. The statistically sig-
 ) in the three sets of contingencies with ﬁxed DP are
niﬁcant eﬀects of P(e+|c
contrary to the predictions of DP. Only causal support and v2 predicted the observed
non-monotonic trends.

8. Sample size eﬀects

In explaining the results of Lober and Shanks (2000, Experiments 1–3), we
touched upon the issue of sample size. Sample size is an important factor that aﬀects
structure learning but not parameter estimation:
larger samples provide better
grounds for assessing the evidence for a causal relationship, but do not aﬀect param-
eter estimation. Both DP and causal power are computed using the conditional prob-
abilities P(e|c), rather than the number of observations contributing to these
probabilities. Consequently, they predict no variation in judgments as the number
of trials on which the cause was present or absent is varied. In contrast, both causal
support and v2 are sensitive to sample size.

There are two dimensions along which sample size might be varied: the ratio of
 )),
the number of trials on which the cause is present or absent (N(c+) and N(c
 ) has been
and the total number of trials (N). Variation of the ratio of N(c+) to N(c
explored extensively by White (1998, 2002c, 2003c). These experiments revealed

362

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

several eﬀects of sample size, inconsistent with the predictions of DP and causal
power, and were taken to support WhiteÕs (2002c) proportion of Conﬁrming Instanc-
 ). We will discuss the
es (pCI) model, which diﬀers from DP only when N(c+) „ N(c
pCI model further in Section 15. We applied all ﬁve models—DP, causal power, pCI,
causal support, and v2—to the results of these experiments, producing the correla-
tions shown in Table 3. For several of these experiments, DP was constant for all
stimuli, resulting in a correlation of 0 between DP and human judgments. We also
present the results of the ﬁve models on Experiment 1 of Anderson and Sheu
(1995), in which the entries in single cells of the contingency table were varied sys-
tematically, resulting in some sample size variation as well as other eﬀects. The ob-
served eﬀects of sample size were broadly consistent with causal support, which gave
either the best or close to the best account of 10 of the 12 datasets. There was no
systematic relationship between the format in which contingency information was
presented and the performance of the models, although causal support gave the best
correlations with the three online datasets.

Variation of the total number of trials producing a set of contingencies, N, has
been studied less extensively. White (2003c, Experiment 4), conducted an experiment
in which this quantity was varied, and found no statistically signiﬁcant eﬀect on hu-
man ratings. As shown in Fig. 4, causal support makes clear predictions about the
eﬀect of N on the evidence for a causal relationship, and the demonstration of such
an eﬀect would provide evidence for the involvement of structure learning in human
causal induction. One possible explanation for the lack of a sample size eﬀect in
WhiteÕs (2003c) experiment is the use of ratings as a dependent measure: the eﬀect
of sample size might be concealed by allowing people the possibility of giving equal
ratings to diﬀerent stimuli. Consequently, we decided to explore this phenomenon
further, using a more sensitive response measure: Experiment 2 was designed to

Table 3
Correlations of rational models with sample size experiments

Paper

White (1998)

White (2002c)

White (2003c)

Anderson and Sheu (1995)

Experiment

Format

3 (seeds)
3 (contentless)
1
2
3
1
2
3
4
5
6
1

Summary (16)
Summary (16)
List (8)
List (12)
List (6)
List (8)
Online (8)
Summary (8)
List (8)
List (8)
Online (4)
Online (80)

DP

0.907
0.922
0.956
0.772
0
0.200
0.070
0.392
0
0
0
0.884

Power

pCI

Support

v2

0.902
0.867
0.765
0.852
0.760
0.389
0.409
0.383
0.037
0.373
0
0.816

0.929
0.933
0.956
0.760
0.941
0.818
0.706
0.467
0.860
0.788
0.425
0.877

0.924
0.935
0.938
0.916
0.837
0.854
0.812
0.677
0.679
0.803
0.676
0.894

0.865
0.885
0.936
0.830
0.146
0.791
0.640
0.586
0.729
0.631
0
0.329

Note. Boldface indicates highest correlation in each row. Number in parentheses in format column
indicates number of stimuli.

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

363

examine whether sample size aﬀects peopleÕs assessment of causal relationships,
using a rank ordering task.

9. Experiment 2

9.1. Method

9.1.1. Participants

Participants were 20 members of the MIT community who took part in the exper-

iment in exchange for candy.

9.1.2. Stimuli

We used nine stimuli, each composed of diﬀerent contingency data. The two crit-
ical sets of contingencies were a set for which DP = 0, consisting of {0/4, 0/4}, {0/
20, 0/20}, and {0/50, 0/50}, and a set for which DP = 0.5, {2/4, 0/4}, {10/20, 0/20},
and {25/50, 0/50}. DP, pCI, and causal power are constant for these sets of stimuli,
and any ordering should thus be equally likely. v2 predicts an increase in judgments
with sample size for the DP = 0.5 set, but is constant for the DP = 0 set. Causal sup-
port predicts sample size should result in an increase in judgments with DP = 0.5,
and a decrease with DP = 0, as shown in Fig. 4. The experiment also included three
distractors, to conceal our manipulation: {3/4, 1/4}, {12/20, 8/20}, and {50/50, 0/50}.

9.1.3. Procedure

Participants read a description of an imaginary laboratory scenario, similar to
that used in Experiment 1, and were shown nine cards that expressed the stimulus
information described above. They were given the following instructions:

Each of the cards in front of you summarizes the results of a diﬀerent study.
Look these summaries over carefully, and then place them in order from the
study from which it seems LEAST LIKELY that the chemical causes the gene
to be expressed, to the study in which it seems MOST LIKELY that the chem-
ical causes the gene to be expressed.

The wording of the question in terms of likelihood followed the procedure report-
ed by White (2003c). If participants asked whether cards could be ranked equally,
they were told that they could order them randomly.

9.2. Results and discussion

Analysis of the orderings produced by the participants showed that 17 out of 20
ordered the stimuli with DP = 0.5 by increasing sample size (binomial test, p < .001),
while 16 out of 20 ordered the stimuli with DP = 0 by decreasing sample size (bino-
mial test, p < .001). We computed rank-order correlations with the responses of indi-
vidual participants for each of the ﬁve models. We computed the rank-order
correlations with the ﬁve models for each participant, averaging these correlations

364

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

to result in scores for each model.5 Causal support and v2 performed equivalently,
q = 0.948 and q = 0.945, respectively, followed by DP, q = 0.905, and causal power,
q = 0.859. Causal support gave the highest correlation with the responses of eleven
participants, causal power and v2 with four participants each, and DP with only one
participant.

The results indicate that people are sensitive to sample size when making causal
judgments. Speciﬁcally, increasing sample size increases judgments when eﬀects are
large, but decreases judgments for zero eﬀects. Only causal support can explain this
pattern of results. Sensitivity to sample size is a property of structure learning, not
parameter estimation, and thus provides evidence that people approach problems
of causal induction as structure learning problems.

10. Inferences from incomplete contingency tables

We began this paper by observing that everyday causal induction has several
commonalities with the reasoning of early scientists. Among these commonalities
is the need to make inferences from limited data. In many settings where people
infer causal relationships, they do not have all of the information that is typically
provided in causal
induction tasks. Speciﬁcally, without a carefully designed
experiment, we often do not know the frequency of the eﬀect in the absence of
the cause, leaving some of the cells in a contingency table empty. While our epi-
graph indicates the attention that James Currie paid to the number of patients
who recovered both with and without treatment, such reporting was the exception
rather than the rule prior to the development of modern experimentation. Many
early medical texts, such as JennerÕs (1798) famous treatise on the smallpox vac-
cine, consist of a description of a number of cases in which the treatment proved
successful, providing only N(e+, c+). To make an inference from such data, his
readers had to use their expectations about the frequency of infections in the ab-
sence of treatment.

DP and causal power are both undeﬁned when there are no trials on which the
 ) cannot be computed. This is a problem, as people
cause was absent, since P(e+|c
readily make causal judgments under such circumstances. For example, suppose that
a doctor claims to have invented a treatment that will cure a rare illness, Hopkins–
Francis syndrome. He tells you that he has given this treatment to one patient with
Hopkins–Francis syndrome, and after one month, all the patientÕs symptoms are
gone. How much evidence does this provide for the treatmentÕs eﬀectiveness? It
may provide some evidence, but not strong evidence, since we do not know how
many patients would recover spontaneously in this interval.

5 Correlations were averaged using the Fisher z transformation. These results include only 19 of the 20
participants, since causal support perfectly predicted the ordering given by one participant, resulting in an
inﬁnite z score. The reported mean correlation is thus an underestimate for causal support. While the other
models do not predict an ordering for the two critical sets, they do predict an ordering among the full set of
nine stimuli, hence q > 0.

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

365

A few months later, the doctor tells you that he has now given the treatment
to three patients, and after one month all of their symptoms are gone. These
data provide stronger evidence, but not that much stronger. The evidence is
strengthened once more when, a few months later, the doctor tells you that
he has given the treatment to twenty patients, and after one month all of their
symptoms are gone. Finally, the doctor tells you that he has also seen twenty
patients over the same time period who received a placebo instead of the new
treatment, and all people in this group still had symptoms after a month of
observation. Moreover, the people who received the treatment or the placebo
were chosen at random. Now this provides very strong evidence for the treat-
mentÕs eﬀectiveness.

We can identify ﬁve stages in the accumulation of evidence in this example.
The ﬁrst stage is the baseline, with no information, and the contingencies {0/
0, 0/0}. After a single observation, we have {1/1, 0/0}. Two more observations
provide {3/3, 0/0}, and 17 more successful treatments give {20/20, 0/0}. Finally,
the control condition provides {20/20,0/20}. DP and causal power can only be
computed for this last case, where they both indicate strong evidence for a caus-
al relationship, DP = power = 1.00. They are undeﬁned for the other contingency
tables, and thus cannot capture the weak but growing evidence these tables pro-
vide. Causal support is 0 for {0/0, 0/0}, reﬂecting the lack of evidence for or
against a causal relationship (negative values of causal support indicate evidence
for Graph 0, while positive values indicate evidence for Graph 1). Causal sup-
port then gradually increases as the observations accumulate, taking values of
0.41, 0.73, and 1.29, before jumping dramatically to 23.32 for when the control
condition is added. Unlike DP or causal power, causal support thus predicts our
intuitive ordering of the strength of evidence provided by these ﬁve stimuli. In
Experiment 3, we examined whether this ordering matched the judgments of na-
ive participants.

11. Experiment 3

11.1. Method

11.1.1. Participants

Participants were 20 members of the MIT community who took part in the exper-

iment in exchange for candy.

11.1.2. Stimuli

We used the ﬁve stimuli described above: {0/0, 0/0}, {1/1, 0/0}, {3/3, 0/0}, {20/

20, 0/0}, and {20/20, 0/20}.

11.1.3. Procedure

The procedure was identical to that of Experiment 2, with the stimuli being pre-

sented on cards and participants being asked to provide an ordering.

366

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

11.2. Results and discussion

Analysis of the orderings produced by the participants showed that 15 out of 20
perfectly reproduced the ordering predicted by causal support (binomial test,
p < .001). The other ﬁve participants still showed some conformity to the predictions
of causal support, with a mean correlation of q = 0.51. DP and causal power are
undeﬁned for all but one of these stimuli, preventing the computation of any
correlations.

Causal support is the only model we have considered that is capable of capturing
peopleÕs inferences from incomplete contingency tables. The ability to infer causal
relationships from limited data is an extremely important part of causal induction
in both everyday and scientiﬁc settings. Even today, many medical treatments are
initially tested with small samples and incomplete contingency tables. Many doctors
say they do not believe in a drugÕs eﬀectiveness until it passes large-scale studies with
appropriate controls, in part because standard statistical practice does not provide a
rigorous way to evaluate treatment eﬀectiveness with such limited data. However,
the researchers who are actually coming up with and testing new treatments need
to have some way of evaluating which treatments are promising and which are
not, or they would never make any progress. Causal support provides an account
of the rational basis of these intuitions.

12. Learning from rates

Several studies of elemental causal induction have gone beyond inferences from
contingency table data, examining how people learn about causal relationships from
rate data (e.g., Anderson & Sheu, 1995; Wasserman, 1990). Rates are closely related
to contingencies, being the number of times the eﬀect occurs in a continuous interval
rather than the number of times the eﬀect occurs on a set of discrete trials. Despite
this relationship, previous models of causal induction such as DP and causal power
have not been assessed using rate data. In this section, we will show how these mod-
els can be extended to allow inferences from rate data, and evaluate their predictions
about human judgments. This novel setting provides an opportunity to test the gen-
erality of our framework, as the distinction between structure and strength holds
whether the observed data are rates or contingencies.

Anderson and Sheu (1995, Experiment 2) conducted an experiment in which par-
ticipants learned whether clicking on a ﬂute icon caused a change in the rate of mu-
sical notes produced by the ﬂute. They found that their results were poorly predicted
by the diﬀerence in rates, deﬁned as

DR ¼ NðcþÞ   Nðc Þ;

ð11Þ
where N(c+) is the number of events in the interval when the cause, in this case click-
 ) is the number of events when the cause was
ing on the ﬂute, was present, and N(c
absent. Anderson and Sheu (1995) found that performance could be better predicted
by ‘‘grating contrast,’’ which they deﬁned as

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

contrast ¼ NðcþÞ   Nðc Þ
NðcþÞ þ Nðc Þ

367

ð12Þ

and justiﬁed by its use as a measure of contrast in psychophysical research. They
gave no theoretical motivation for using this measure.

In the remainder of this section, we will develop a rational account of causal induc-
tion from rates. This account makes explicit the relationship between DR, DP, and
causal power, revealing that DR is the maximum-likelihood parameter estimate of
the strength of a causal relationship, and allows us to deﬁne causal support for rate
data. The ﬁrst step in this analysis is deﬁning a parameterization for Graph 0 and
Graph 1 that is appropriate for outcomes that are rates rather than discrete trials.

The Poisson distribution is commonly used in statistics for modeling the number
of events that occur within a ﬁxed interval. Under this distribution, the number of
events N occurring in a single unit of time will have probability

;

PðNÞ ¼ e k kN
N !

ð13Þ
where k is the rate parameter. We can use the Poisson distribution to deﬁne a param-
eterization for Graph 0 and Graph 1 that is an extension of the linear and noisy-OR
parameterizations to continuous time. Under a noisy-OR parameterization where
the event E has parents B and C, the probability of E if just B is present is w0, and
the probability of E with both B and C present is w0 + w1 w0w1, where the last term
corrects for double-counting the cases in which both B and C would have produced
E. Extending this model to the case where events are emitted over a continuous interval,
the probability of an event at any point in time is simply the sum of the probabilities for
each of the parents, as in the linear parameterization, since the probability of two events
from a Poisson process occurring simultaneously is zero. The resulting process is the
sum of the Poisson processes associated with the parents, and the sum of two indepen-
dent Poisson processes is a Poisson process with a rate equal to the sum of the rates of
the original processes. This gives us the parameterization

PðNjb; c; k0; k1Þ ¼ e ðbk0þck1Þ ðbk0 þ ck1ÞN

N !

.

ð14Þ

where k0 is the rate associated with B, and k1 is the rate associated with C.

Under the model speciﬁed by Eq. (14), DR is the maximum likelihood parameter
estimate for k1. The correspondence to DP and causal power can be seen by taking
the rate information as just the positive events in a contingency table where the total
 ) for unknown
sample size is unknown, so N(c+) = NP(e+|c+) and N(c
N. If we assume that N is ﬁxed across diﬀerent experiments, we can obtain estimates
consistent with the ordering and magnitude implied by DP using DR = N(c+)   N(c
 )
= NDP. If we make the further assumption that N is very large, DR will also correspond
to causal power, since P(e

 ) will tend to 1.

 ) = NP(e+|c

 |c

Using the parameterization given in Eq. (14), we can deﬁne causal support as in
Eq. (10), where Graph 0 is the model in which B is the only parent of E, and Graph 1
is the model in which both B and C are parents of E. The details of this model are
provided in Appendix A, where we also justify the approximation

368

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

r ¼ ðNðcþÞ   Nðc ÞÞ2

v2

Nðc Þ

.

ð15Þ

This approximation bears the same relationship to causal support for rates as the
Pearson v2 test for independence does for contingency data: it is a frequentist inde-
pendence test that will be asymptotically equivalent to causal support. Computing v2
r
involves dividing the squared diﬀerence between the observed rates by the variance
of the rate in the absence of the cause, comparing the magnitude of the eﬀect of
introducing the cause to the variation that should arise by chance. This may account
for the eﬃcacy of the grating contrast model used by Anderson and Sheu (1995),
which involves a similar ratio.

Our analysis provides a set of models that generalize DP, causal power, causal sup-
port, and v2 to allow causal induction from rates. Unfortunately, the procedure used in
previous experiments exploring causal induction from rates (Anderson & Sheu, 1995;
Wasserman, 1990) prevents modeling of their data without detailed information about
the performance of individual participants. These experiments used a procedure in
which participants interacted with objects, and then observed whether there was an
alteration in the rate of the eﬀect after their interaction. The models described in this
section cannot be applied to these data without a record of the number of periods of
interaction and non-interaction. To address this issue we conducted our own experi-
ments, in which participants were provided with information about the rate of occur-
rence of the eﬀect in the presence and absence of the cause directly, using both summary
(Experiment 4) and online (Experiment 5) formats.

13. Experiment 4

13.1. Method

13.1.1. Participants

Eighty-two Stanford University undergraduates took part in the study.

13.1.2. Stimuli

A questionnaire presented a summary of nine experiments involving diﬀerent
chemical compounds and electrical ﬁelds, giving the number of particle emissions in-
side and outside the electrical ﬁeld. The number of particle emissions in each example
 )} pairs):
was selected to give three critical sets of rates (expressed as {N(c+),N(c
{52, 2}, {60, 10}, {100, 50}, for which DR = 50, {12, 2}, {20, 10}, {60, 50} for which
DR = 10, and {4, 2}, {12, 10}, {52, 50}, for which DR = 2.

13.1.3. Procedure

The instructions outlined a hypothetical laboratory scenario:

Imagine that you are working in a laboratory and you want to ﬁnd out whether
electrical ﬁelds inﬂuence the radioactive decay of certain chemical compounds.
Below, you can see laboratory records for a number of studies. In each study, a

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

369

sample of some particular compound was placed inside a particular kind of
electrical ﬁeld for one minute, and the rate of radioactive decay was measured
(in number of particles emitted per minute). Each study investigated the eﬀects
of a diﬀerent kind of ﬁeld on a diﬀerent kind of chemical compound, so the
results from diﬀerent studies bear no relation to each other.
Of course, the chemical compounds can emit particles even when not in an elec-
trical ﬁeld, and they do so at diﬀerent rates. Some compounds naturally decay
at a fast rate, while others naturally decay at a slow rate. Thus, the decay rate
of each compound was also measured for one minute in the absence of any
electrical ﬁeld. For each study below, you can see how many particles were
emitted during one minute inside the electrical ﬁeld, and during one minute
outside of the electrical ﬁeld. What you must decide is whether the electrical
ﬁeld increases the rate of particle emissions for each chemical compound.

Participants were instructed to provide ratings in response to a question like that of
Experiment 2. Ratings were made on a scale from 0 (the ﬁeld deﬁnitely does not cause
the compound to decay) to 100 (the ﬁeld deﬁnitely does cause the compound to decay).
Each participant completed the survey as part of a booklet of unrelated experiments.

13.2. Results and discussion

The results are shown in Fig. 9, together with the model predictions. There was a
 ) at DR = 50 (F(2,162) = 12.17, MSE = 257.27,
statistically signiﬁcant eﬀect of N(c
p < .001), DR = 10 (F(2,162) = 42.07, MSE = 468.50, p < .001), and DR = 2
(F(2,162) = 29.87, MSE = 321.76, p < .001). Causal support and v2
r gave equivalent
quantitative ﬁts, r = .978, c = 0.35, and r = .980, c = .01, respectively, followed by
grating contrast, r = .924, c = 0.43, and DR, r = .899, c = 0.05. Causal power as-
sumes the wrong statistical model for this kind of data, but might be applied if we
assumed that participants were comparing the rates to some hypothetical maximum
number of particles that might be emitted, N. As N approaches inﬁnity, causal power
converges to DR. The trends predicted by causal power do not vary with the choice
of N, so the value N = 150 was chosen to allow these trends to be illustrated. The
predicted trends are clearly at odds with those observed in the data, reﬂected in
the correlation r = .845, c = 0.06.

Since DR predicts that responses within each of the critical sets should be con-
 ) is inconsistent with this model. This
stant, the statistically signiﬁcant eﬀect of N(c
trend is, however, predicted by causal support and v2
r . These predictions reﬂect the
 ) increases: if the eﬀect oc-
fact that the certainty in the value of k1 decreases as N(c
curs at a high rate in the absence of the cause, it becomes more diﬃcult to determine
if an increase in the number of times the eﬀect is observed when the cause is present
 ) is thus a sign that people
actually reﬂects a causal relationship. The eﬀect of N(c
are attempting to determine the causal structure underlying their observations. To
demonstrate that these results generalize to cases where rates are supplied perceptu-
ally rather than in summary format, we replicated this experiment using online pre-
sentation in Experiment 5.

370

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

N (c+)
N (c–)
100

100
50

60
10

52
2

60
50

20
10

12
2

50

0

100

50

0

100

50

0

100

50

0

100

50

0

100

50

0

52
50

12
10

4
2

Humans

∆ R

Power (N = 150)

Contrast

Support

χ2
r

Fig. 9. Predictions of rational models compared with results of Experiment 4. Numbers along the top of
the ﬁgure show stimulus rates, error bars indicate one standard error.

14. Experiment 5

14.1. Method

14.1.1. Participants

Participants were 40 members of the MIT Brain and Cognitive Sciences Depart-

ment subject pool.

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

371

14.1.2. Stimuli

The stimuli were the same as those for Experiment 4, with the addition of three
 )} pairs

stimuli used for an initial practice phase. These stimuli used the {N(c+), N(c
{20, 28}, {10, 70}, and {16, 16}.

14.1.3. Procedure

The experiment was administered by computer. Participants were provided with
instructions similar to those for Experiment 4, establishing the same laboratory cover
story. The experiment consisted of three practice trials, followed by nine trials that used
the same rate information as Experiment 3. In each trial, participants saw a picture of a
novel mineral substance, and observed it for 30 s while it emitted particles, indicated by
a beep and the temporary appearance of a mark on a ‘‘particle detector’’ shown on
screen. They then clicked a button to turn on a magnetic ﬁeld for another 30 s, and ob-
served the particle emissions in the presence of the magnetic ﬁeld. After having ob-
served the mineral both in and out of the magnetic ﬁeld, they rated the causal
relationship between the magnetic ﬁeld and particle emissions using a slider, with the
same instructions as used in Experiment 4. The actual times of the particle emissions
on each trial were generated from a uniform distribution over 30 s, with a minimum
of 250 ms between emissions to ensure that they were perceptually distinct.

14.2. Results and discussion

The results are shown in Fig. 10, together with the model predictions. The results
reproduced the trends found in Experiment 3, correlating with the previous data at
 ) at DR = 50
r = .975. There was a statistically signiﬁcant eﬀect of N(c
(F(2,78) = 5.82, MSE = 116.81,
(F(2,78) = 23.18,
MSE = 503.18, p < .001), but not at DR = 2 (F(2,78) = .97, MSE = 725.56,
r , and DR all showed similar quantitative ﬁt, r = .951,
p = .385). Causal support, v2
c = 0.416, r = .944, c = 0.018, and r = .948, c = 0.085, respectively, followed by grat-
ing contrast, r = .832, c = 0.561. Causal power, computed with N = 150, fared better
on these data than on Experiment 4, r = .912, c = 0.047.

and DR = 10

p < .005)

The statistically signiﬁcant diﬀerences among sets of rates for which DR is constant
provides evidence against peopleÕs responses being driven by parameter estimation.
The trends predicted by causal support are slightly less pronounced in these data than
in Experiment 4, which we suspect may be a consequence of the more perceptual pre-
sentation format. In particular, the results for DR = 2 may be a result of greater per-
 ) increases: for {4, 2} and {12, 10}, it is quite easy to count the
ceptual noise as N(c
number of particle emissions in the presence and absence of the cause, and to make
an inference informed by these counts. For {52, 50}, it is easy to lose count, and people
may be less certain that the diﬀerence is small. Despite these small discrepancies, the
results generally reproduce the trends observed in Experiment 4, and support the same
conclusion: that people are making a structural inference when asked to assess a caus-
al relationship, regardless of whether the data are discrete contingencies or dynamic
rates. Our framework makes it possible to explain these results, and only causal sup-
port predicts human judgments for both of these kinds of data.

372

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

N (c+)
N (c–)
100

100
50

60
10

52
2

60
50

20
10

12
2

50

0

100

50

0

100

50

0

100

50

0

100

50

0

100

50

0

52
50

12
10

4
2

Humans

∆ R

Power (N = 150)

Contrast

Support

χ2
r

Fig. 10. Predictions of rational models compared with results of Experiment 5. Numbers along the top of
the ﬁgure show stimulus rates, error bars indicate one standard error.

15. General discussion

We have presented a rational computational framework for analyzing the prob-
lem of elemental causal induction, using causal graphical models to emphasize the
two components of this problem: parameter estimation—estimation of the strength
of a causal relationship—and structure learning—evaluation of whether a causal
relationship actually exists. Two leading rational models of elemental causal

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

373

induction, DP and causal power, both address the problem of parameter estimation.
We have described ﬁve phenomena that suggest that, in many circumstances, people
are approaching causal induction as structure learning. Causal support, a rational
solution to the problem of learning causal structure, provides the best account of
the two datasets at the center of the current debate about rational models—Buehner
and Cheng (1997) and Lober and Shanks (2000)—and gives either the best or close to
the best account of 10 out of the 12 other experiments we have analyzed (Anderson
& Sheu, 1995; White, 1998, 2002c, 2003c). It also predicts several eﬀects that no exist-
ing models can account for, in online, summary, and list formats, and in contingency
and rate data, which we have validated through our own experiments.

Our results provide strong evidence that human judgments in causal induction
tasks are often driven by the problem of structure learning, and that causal support
provides an appropriate model of these judgments. We will close by discussing some
issues raised by our analysis. First, we will describe some other models that address
the problem of structure learning. We will then consider the circumstances under
which we expect accounts based upon structure learning and parameter estimation
to be most consistent with human judgments, before going on to clarify the relation-
ship between causal support and ideas such as ‘‘reliability’’ (Buehner & Cheng, 1997;
Perales & Shanks, 2003). Finally, we will sketch some ways in which our framework
for elemental causal induction can be extended to shed light on other aspects of caus-
al learning.

15.1. Structure learning in other models of causal induction

Our framework can be used to show that the leading models of elemental causal
induction are both based upon parameter estimation. It also makes it possible to
reinterpret several other psychological models in terms of structure learning. These
models diﬀer from causal support in terms of how they approach the issue of infer-
ring causal structure, and in their assumptions about the functional form of the caus-
al relationship between cause and eﬀect. We will brieﬂy discuss two models:
AndersonÕs (1990; Anderson and Sheu, 1995) rational model, and WhiteÕs (2002c,
2002b) proportion of Conﬁrming Instances model.

15.1.1. Anderson’s rational model

Anderson (1990) presented a Bayesian analysis of the problem of elemental causal
induction. The model was quite complex, with seven free parameters, and a simpler
model, with four free parameters, was presented by Anderson and Sheu (1995). The
central idea of both models is that causal induction requires evaluating the hypoth-
esis that the cause inﬂuences the eﬀect. In our framework, this is a decision between
Graph 0 and Graph 1, just as in causal support. Under AndersonÕs approach, these
structures have a diﬀerent parameterization from that used in causal support. In-
stead of the noisy-OR parameterization, a separate parameter is used to represent
the probability of the eﬀect for each set of values its causes take on. Either these
parameters (Anderson & Sheu, 1995) or the prior on these parameters Anderson
(1990) are chosen to provide the best ﬁt to human data. As noted by Anderson

374

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

and Sheu (1995), this procedure results in predictions that are only slightly more
restricted than linear regression using the cell counts from the contingency table
(cf. Schustack & Sternberg, 1981). In contrast, causal support involves no free
parameters, with the model predictions being computed directly from contingencies,
as with DP and causal power.

15.1.2. The proportion of conﬁrming instances

White (2000, 2002b, 2002a, 2002c, 2003a, 2003c) has also argued that people make
judgments in causal induction tasks by assessing the evidence that a causal relation-
ship exists. White (2002c) deﬁned a measure of evidence termed the proportion of
Conﬁrming Instances (pCI), subsequently modiﬁed by White (2003a) to give the
expression

pCI ¼ Nðeþ; cþÞ þ Nðe ; c Þ   Nðeþ; c Þ   Nðe ; cþÞ
Nðeþ; cþÞ þ Nðe ; c Þ þ Nðeþ; c Þ þ Nðe ; cþÞ .

ð16Þ

This equation also appears in other models of elemental causal induction (e.g., Cate-
na et al., 1998), was originally proposed by Inhelder and Piaget (1958, p. 234), and is
 ). The numerator of Eq. (16) has
monotonically related to DP when N(c+) = N(c
been explored as a model of causal induction in itself, being referred to as DD in
the literature (e.g., Allan, 1980; Allan & Jenkins, 1983; Ward & Jenkins, 1965),
and we show in Appendix A that it can be motivated from the perspective of struc-
ture learning. pCI produces the same predictions as DP for the majority of our exper-
iments, and suﬀers many of the same problems. The exception is its sensitivity to
sample size: as shown in Section 8, it does reasonably well in predicting the eﬀect
 ), although it cannot account for the results of our Exper-
of varying N(c+) and N(c
iments 2 or 3. When learning from rates, the grating contrast measure deﬁned by
Anderson and Sheu (1995), given in Eq. (12), corresponds exactly to pCI.

15.2. Parameter estimation and structure learning in human judgments

Having distinguished between parameter estimation and structure learning as
components of causal induction, it is natural to ask when we might expect one or
the other to dominate human judgments. We have shown that across many experi-
ments, causal support gives a better account of peopleÕs judgments than the maxi-
mum likelihood parameter estimate for the same model, causal power. These
experiments all used tasks for which a structure-learning interpretation is reasonable.
However, several recent studies have begun to use a diﬀerent question format, asking
people to give a counterfactual conditional probability, for which a strength estimate
is appropriate (Buehner et al., 2003; Collins & Shanks, submitted). For example,
rather than asking ‘‘Do injections cause gene expression?,’’ we could ask ‘‘What is
the probability that a mouse not expressing the gene before being injected will ex-
press it after being injected with the chemical?.’’ Such questions elicit responses that
are more consistent with causal power. These results can be explained using the
framework we have established in this paper: causal power gives the maximum like-
lihood estimate of parameter w1 in Eq. (4). The counterfactual questions used in

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

375

these experiments ask for the probability that, for a case in which C was not present
and E did not occur, E would occur if C was introduced. Using the axiomatic treat-
ment of counterfactuals developed by Pearl (2000), this probability is just w1. Con-
sequently, the appropriate way to answer counterfactual questions is via parameter
estimation, and responses should correspond more closely to causal power than to
causal support.

15.3. Causal support and reliability

Buehner and Cheng (1997) appealed to the notion of ‘‘reliability’’ in trying to ex-
plain aspects of their results that deviated from the predictions of the Power PC theory.
To account for some of the data discussed above, they claimed that people sometimes
conﬂate conﬁdence in their estimates of causal power with the estimates themselves.
Shanks and colleagues (Collins & Shanks, submitted; Perales & Shanks, 2003) have fur-
ther explored this ‘‘reliability hypothesis.’’ Our framework can be used to make the no-
tion of reliability precise, and to explain why it might inﬂuence peopleÕs judgments.

Buehner and Cheng seem to consider reliability of secondary importance in eval-
uating causal relationships, acting something like the error bars on the estimate of
causal power. Perales and Shanks (2003) convey a similar impression, equating reli-
ability with conﬁdence in assessments of the strength of a causal relationship. This
notion of reliability thus seems to correspond to the certainty associated with a
parameter estimate. Our framework provides a formal means of distinguishing be-
tween an estimate and its certainty, based upon the posterior distribution on w1,
as shown in Figs. 4 and 5. The location of the peak of this distribution indicates
the strength of a relationship, and the width of this peak indicates the certainty in
that estimate. Viewing causal induction as a structural inference makes it apparent
that neither strength nor reliability should be considered primary: rational causal
inferences should combine both factors. Causal support evaluates the evidence that
w1 diﬀers from zero. This evidence generally increases as the peak of the posterior on
w1 moves away from zero (increasing strength), and as that peak becomes narrower
(increasing reliability).

Our framework explains human judgments as a rational combination of strength
and certainty, rather than the result of strength estimates being confounded by reli-
ability. It also provides some suggestions about the circumstances in which the cer-
tainty in an estimate should be relevant. The results of Buehner et al. (2003) and
Collins and Shanks (submitted) with counterfactual questions, summarized in Sec-
tion 15.2, illustrate that certainty seems to inﬂuence judgments less when a task
explicitly involves parameter estimation. Without the rational explanation provided
by our framework, it is diﬃcult to explain why judgments should be confounded less
by reliability when people are asked counterfactual questions.

15.4. Limitations and extensions

The framework we have described in this paper, and the model of human judg-
ments that we derived from it, causal support, address only the simplest cases of

376

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

causal induction. We have not discussed the dynamics of causal judgments in online
studies, or more complex inductive tasks such as those involving multiple causes. We
will brieﬂy consider these limitations, and some potential directions for extending the
framework.

15.4.1. The dynamics of causal judgments

The online presentation format makes it possible to ask participants to provide
responses at several diﬀerent points in the presentation of contingency information,
providing the opportunity to measure the dynamics of causal judgments. Shanks
et al. (1996) and Lo´ pez et al., 1998 discuss several phenomena involving changes
in judgments over time, and trial order eﬀects have been reported in several other
studies (e.g.,Catena et al., 1998; Collins & Shanks, 2002). Specifying how structure
learning and parameter estimation interact in peopleÕs judgments over time presents
a particularly interesting problem for future research. We have conducted some pre-
liminary work in this direction (Danks, Griﬃths, & Tenenbaum, 2003), using a sim-
ple Bayesian formalism to provide a structure-sensitive strength estimate. This model
also deals naturally with the distinction between generative and preventive causes,
simultaneously updating a probability distribution over underlying models (genera-
tive, preventive, and no relationship) and distributions over the parameters of those
models. This model provides an account of some of the qualitative features of learn-
ing curves from online experiments presented in Shanks et al. (1996).

15.4.2. Multiple causes

Many important phenomena in causal induction, such as backwards blocking
Shanks (1985) and other retrospective revaluation eﬀects (Shanks & Dickinson,
1987; Shanks et al., 1996) involve simultaneously learning about multiple causes.
The framework for elemental causal induction that we have outlined in this paper
can naturally be extended to accommodate multiple causes. The connection with
causal graphical models clariﬁes how this extension should be made, as well as iden-
tifying some of the options for diﬀerent modeling approaches. Steyvers et al. (2003)
have shown that this kind of approach can be applied to situations involving multi-
ple causes. We have also demonstrated that an approach based on causal graphical
models can provide an explanation for backwards blocking phenomena in particular
causal induction tasks (Tenenbaum & Griﬃths, 2003).

15.4.3. Non-probabilistic causes

One of the consequences of distinguishing between parameter estimation and
structure learning is a better understanding of causal induction with non-probabilis-
tic causes. Approaches to causal induction based upon strength estimation only pro-
duce meaningful predictions when causes are probabilistic: if a causal relationship is
deterministic, with the cause always producing the eﬀect, the strength of the cause is
1.00. As a consequence, parameter estimation cannot be used to explain how people
draw causal inferences about deterministic systems. Studies of causal induction from
contingency data almost exclusively employ probabilistic causes, so this has not been
identiﬁed as an issue for DP or causal power. However, in many studies in the

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

377

broader literature on causal induction people make graded inferences about non-
probabilistic causes (e.g., Gopnik et al., 2004; Tenenbaum, Sobel, Griﬃths, & Gopnik,
submitted). Such inferences can be addressed within our framework: the decision be-
tween causal structures can be made regardless of causal strength, and the degree of
evidence for a causal relationship can vary even with deterministic causes. Our ap-
proach to causal induction produces predictions consistent with peopleÕs judgments
in several settings involving deterministic causes (e.g., Tenenbaum & Griﬃths, 2003).

15.5. Conclusion

We have used causal graphical models to identify two components of the problem
of causal induction: structure learning and parameter estimation. We have shown
that previous rational models of causal induction only address the problem of
parameter estimation. By emphasizing the role of structure learning in human causal
induction, we have been able to explain a variety of phenomena that were problem-
atic for previous models, and to understand the inference that underlies the discovery
that a causal relationship actually exists. Our approach explains not only lay peopleÕs
fundamental intuitions about cause and eﬀect, but also the intuitions that drove dis-
covery for early scientists, such as Dr. James Currie of the epigraph, and that con-
tinue to be important in the early stages of much contemporary scientiﬁc research.
Looking beyond the simple setting of elemental causal induction, our Bayesian
approach provides a method for explaining how prior knowledge and statistical
inference might be combined in causal learning. Human judgments about causality
draw upon a rich body of knowledge about the mechanisms mediating between caus-
es and eﬀects, which relationships are plausible, and what functional forms particu-
lar relationships might take. The simple Bayesian inference that underlies our
account of elemental causal induction can be augmented to capture the role of this
knowledge, via constraints on the prior probabilities of particular causal relation-
ships, and constraints on the functional form of the probability distributions deﬁned
on those structures. This capacity to capture the interaction between top-down
knowledge and bottom-up statistical cues is one of the greatest strengths of our
framework, and we are currently exploring how it might explain a broader range
of causal inferences, including those that involve sparse data, hidden causes, and
dynamical physical systems.

Appendix A.

A.1. DP and causal power are maximum likelihood parameter estimates

Both DP and causal power are maximum likelihood estimates of the causal
strength parameter for C in Graph 1 of Fig. 3(A), but under diﬀerent parameteriza-
tions. For any parameterization of Graph 1, the log likelihood of the data is given by
ð17Þ

Nðe; cÞ log P 1ðejcÞ;

log PðDjw0; w1Þ ¼

X

e;c

P

378

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

where D is contingency data N(e, c), P1(e|c) is the probability distribution implied by
the model, suppressing its dependence on w0, w1, and
e;c denotes a sum over all
 . Eq. (17) is maximized whenever w0 and w1 can be chosen
pairs of e+, e
to make the model probabilities equal to the empirical probabilities:

  and c+, c

P 1ðeþjcþ; bþ; w0; w1Þ ¼PðeþjcþÞ;
P 1ðeþjc ; bþ; w0; w1Þ ¼Pðeþjc Þ.

 ). Eq. (6) then reduces to P(e+|c+) for the case c = c+ and to P(e+|c

ð18Þ
ð19Þ
To show that DP corresponds to a maximum likelihood estimate of w1 under a linear
parameterization of Graph 1, we identify w1 in Eq. (6) with DP (Eq. (1)), and w0 with
 ) for the
P(e+|c
 , thus satisfying the suﬃcient conditions in Eqs. (18) and (19) for w0 and
case c = c
w1 to be maximum likelihood estimates. To show that causal power corresponds to a
maximum likelihood estimate of w1 under a noisy-OR parameterization, we follow
the analogous procedure: identify w1 in Eq. (4) with causal power (Eq. (2)), and
 ) for
w0 with P(e+|c
 , again satisfying the conditions for w0 and w1 to be maximum likelihood
c = c
estimates.

 ). Then Eq. (4) reduces to P(e+|c+) for c = c+ and to P(e+|c

A.2. DD, pCI, and structure learning

DD is approximately proportional to the log likelihood ratio in favor of Graph 1
be
2  for small e, and the parameterization of

Graph

the

of

to

1

2. This choice of parameters gives

we

deﬁne

if
parameterization
P 1ðeþjcþ; bþÞ ¼ P 1ðe jc ; bþÞ ¼ 1
2 þ 1
Graph 0 to be P 0ðeþjbþÞ ¼ 1
P 1ðeþjcþ; bþÞ
P 0ðeþjbþÞ ¼ P 1ðe jc ; bþÞ
P 1ðe jcþ; bþÞ
P 0ðe jbþÞ ¼ P 1ðeþjc ; bþÞ

P 0ðe jbþÞ ¼ 1 þ ;
P 0ðeþjbþÞ ¼ 1   

and since logð1 þ xÞ  x, for values of x near 0, it follows that:

PðDjGraph 1Þ
PðDjGraph 0Þ   Nðeþ; cþÞ þ Nðe ; c Þ   Nðeþ; c Þ   Nðe ; cþÞ
.

½

log

However, the assumptions under which this unweighted combination of counts is an
appropriate measure of the evidence for a causal relationship are overly restrictive:
the background probability of the eﬀect has to be 0.5, and the cause must be asymp-
totically weak. The deﬁnition of pCI, which normalizes this quantity by the sum of
all entries in the contingency table, cannot be motivated from the perspective of
structure learning, since it removes the eﬀect of overall sample size.

A.3. Evaluating causal support

Causal support is deﬁned as the log likelihood ratio in favor of Graph 1 over

Graph 0:

ð20Þ

ð21Þ

ð22Þ

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

support ¼ log

PðDjGraph 1Þ
PðDjGraph 0Þ .

379

ð23Þ

We obtain the likelihoods P(D|Graph 1),P(D|Graph 0) by integrating out the param-
eters w0, w1. This means that each value of the parameters is assigned a prior prob-
ability, and this probability is combined with the likelihood of the data given the
structure and the parameters to give a joint distribution over data and parameters
given the structure. We can then sum over all values that the parameters can take
on, to result in the probability of the data given the structure. Thus, if we want to
compute the probability of the observed data for the structure depicted by Graph
1, we have

Z

Z

PðDjGraph 1Þ ¼

1

1

0

0

P 1ðDjw0; w1; Graph 1ÞPðw0; w1jGraph 1Þdw0 dw1

and the equivalent value for Graph 0 is given by

Z

PðDjGraph 0Þ ¼

1

0

P 0ðDjw0; Graph 0Þ Pðw0jGraph 0Þ dw0;

ð24Þ

ð25Þ

where the likelihoods P(D|w0, w1, Graph 1), P(D|w0, Graph 0) are speciﬁed by the
parameterization of the graph, and the prior probabilities P(w0, w1|Graph 1),
P(w0|Graph 0) are set a priori. Integrating over all values of the parameters penalizes
structures that require more parameters, simply because the increase in the dimen-
sionality of the space over which the integrals are taken is usually disproportionate
to the size of the region for which the likelihood is improved.

For generative causes, P(D|Graph 1) is computed using the noisy-OR parameter-
ization, and for preventive causes, it is computed using the noisy-AND-NOT. We
also need to deﬁne prior probabilities P(w0, w1|Graph 1) and P(w0|Graph 0), to which
we assign a uniform density. Because causal support depends on the full likelihood
functions for both Graph 1 and Graph 0, we may expect causal support to be mod-
ulated by causal power, but only in interaction with other factors that determine how
much of the posterior probability mass for w1 in Graph 1 is bounded away from zero
(where it is pinned in Graph 0). In the model for rate data, k0 and k1 are both positive
real numbers, and priors for these parameters require diﬀerent treatment. We take a
joint prior distribution in which Pðk0Þ / 1
is an uninformative prior, and P(k1|k0) is
Gamma(1,k0).

k0

A.4. An algorithm for computing causal support

Eq. (25) can be evaluated analytically. If w0 denotes the probability of the eﬀect
occurring regardless of the presence or absence of the cause and we take a uniform
prior on this quantity, we have

Z

PðDjGraph 0Þ ¼

1

wNðeþÞ

0

0

ð1   w0ÞNðe Þ

dw0 ¼ BðNðeþÞ þ 1; Nðe Þ þ 1Þ;

ð26Þ

380

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

where B(r, s) is the beta function, and N(e+) is the marginal frequency of the eﬀect.
For integers r and s, B(r, s) can be expressed as a function of factorials, being
ðr 1Þ!ðs 1Þ!
ðrþs 1Þ! . In general Eq. (24) cannot be evaluated analytically, but it can be approx-
imated simply and eﬃciently by Monte Carlo simulation. Since we have uniform pri-
ors on w0 and w1, we can obtain a good approximation to P(D|Graph 1) by drawing
m samples of w0 and w1 from a uniform distribution on [0,1] and computing

X

m

PðDjGraph 1Þ  1
m

i¼1

P 1ðDjw0i; w1i; Graph 1Þ;

ð27Þ

where w0i and w1i are the ith sampled values of w0 and w1. We thus need only com-
pute the probability of the observed scores D under this model for each sample,
which can be done eﬃciently using the counts from the contingency table. This prob-
ability can be written as

Y

P 1ðDjw0i; w1i; Graph 1Þ ¼

P 1ðejc; bþ; w0i; w1iÞNðe;cÞ

;

ð28Þ

e;c

  and c+, c

 , and P1(e|c;w0i, w1i) reﬂects the cho-
where the product ranges over e+, e
sen parameterization—noisy-OR for generative causes, and noisy-AND-NOT for
preventive. As with all Monte Carlo simulations, the accuracy of the results im-
proves as m becomes large. For the examples presented in this paper, we used
m = 100,000. Matlab code for evaluating causal support via Monte Carlo simulation
can be obtained by writing to the corresponding author, or from the authorsÕ
webpages.

A.5. The v2 approximation

For large samples and weak causes we can approximate the value of causal sup-
port with the familiar v2 test for independence. There are both intuitive and formal
reasons for the validity of the v2 approximation. Intuitively, the relationship holds
because the v2 statistic is used to test for the existence of statistical dependency be-
tween two variables, and C and E are dependent in Graph 1 but not in Graph 0. A
large value of v2 indicates that the null hypothesis of independence should be reject-
ed, and that Graph 1 is favored. However, v2 assumes a diﬀerent parameterization of
Graph 1 from causal support, and the two will only be similar for large samples and
weak causes.

The formal demonstration of the relationship between v2 and causal support is as
follows. When the likelihood P(D|w0, w1) is extremely peaked (e.g., in the limit
N ﬁ 1), we may replace the integrals in Eq. (24) with supremums over w0, w1. That
is, the marginal likelihood essentially becomes the maximum of the likelihood, and
causal support reduces to the ratio of likelihood maxima—or equivalently, the dif-
ference in log likelihood maxima—for Graph 1 and Graph 0. Under these circum-
stances causal support reduces to the frequentist likelihood ratio statistic, equal to
half of the G2 statistic (e.g., Wickens, 1989). Correspondingly, PearsonÕs v2 for
independence

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

381

ð29Þ

P

X

v2 ¼ N

e;c

ð

Pðe; cÞ   PðeÞPðcÞ

Þ2

;

PðeÞPðcÞ
P
ipi log pi
qi

can be shown to approximate twice causal support by a Taylor series argument: the
second-order Taylor series of
(Cov-
er & Thomas, 1991). The v2 approximation only holds when DP is small and N is
large.

, expanded around p = q, is 1
2

ðpi qiÞ2

qi

i

For learning with rates, the likelihood ratio statistic for comparing Graphs 0 and

1 under the parameterization given in Eq. (14) is

r NðcþÞ log NðcþÞ þ Nðc Þ log Nðc Þ   ðNðcþÞ þ Nðc ÞÞ log
v2

;
ð30Þ
which, by essentially the same argument as that given above for G2, will approximate
the value of causal support in the large sample limit. Using the Taylor series argu-
ment employed in the contingency case, we obtain the v2 approximation given in
Eq. (15), which holds only when the diﬀerence in rates is small.

2

NðcþÞ þ Nðc Þ

References

Allan, L. G. (1980). A note on measurement of contingency between two binary variables in judgment

tasks. Bulletin of the Psychonomic Society, 15, 147–149.

Allan, L. G. (1993). Human contingency judgments: Rule based or associative? Psychological Bulletin, 114,

435–448.

Allan, L. G., & Jenkins, H. M. (1983). The eﬀect of representations of binary variables on judgment of

inﬂuence. Learning and Motivation, 14, 381–405.

Anderson, J. R. (1990). The adaptive character of thought. Hillsdale, NJ: Erlbaum.
Anderson, J. R., & Sheu, C.-F. (1995). Causal inferences as perceptual judgments. Memory and Cognition,

23, 510–524.

Buehner, M., & Cheng, P. W. (1997). Causal induction: The Power PC theory versus the Rescorla–Wagner
theory. In M. Shafto & P. Langley (Eds.), Proceedings of the19th annual conference of the cognitive
science society (pp. 55–61). Hillsdale, NJ: Lawrence Erlbaum Associates.

Buehner, M. J., Cheng, P. W., & Cliﬀord, D. (2003). From covariation to causation: A test of the
assumption of causal power. Journal of Experimental Psychology: Learning, Memory, and Cognition,
29, 1119–1140.

Catena, A., Maldonado, A., & Ca´ndido, A. (1998). The eﬀect of the frequency of judgment and the type of
trials on covariation learning. Journal of Experimental Psychology: Human Perception and
Performance, 24, 481–495.

Chapman, G. B., & Robbins, S. J. (1990). Cue interaction in human contingency judgment. Memory and

Cognition, 18, 537–545.

Cheng, P. (1997). From covariation to causation: A causal power theory. Psychological Review, 104,

367–405.

Cheng, P. W., & Holyoak, K. J. (1995). Complex adaptive systems as intuitive statisticians: Causality,
contingency, and prediction. In H. L. Roitblat & J.-A. Meyer (Eds.), Comparative approaches to
cognitive science (pp. 271–302). Cambridge, MA: MIT Press.

Cheng, P. W., & Novick, L. R. (1990). A probabilistic contrast model of causal induction. Journal of

Personality and Social Psychology, 58, 545–567.

Cheng, P. W., & Novick, L. R. (1992). Covariation in natural causal induction. Psychological Review, 99,

365–382.

382

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

Collins, D. J., & Shanks, D. R. (2002). Momentary and integrative response strategies in causal judgment.

Memory and Cognition, 30, 1138–1147.

Collins, D. J. & Shanks, D. R. (submitted). Conformity to the power PC theory of causal induction

depends on type of probe question. Memory and Cognition.

Cooper, G., & Herskovits, E. (1992). A Bayesian method for the induction of probabilistic networks from

data. Machine Learning, 9, 308–347.

Cover, T., & Thomas, J. (1991). Elements of information theory. New York: Wiley.
Currie, J. (1798/1960). Medical reports on the eﬀects of water, cold and warm, as a remedy in fever and
other diseases, whether applied to the surface of the body or used internally, including an inquiry into
the circumstances that render cold drink, or the cold bath dangerous in health. To which are added
observations on the nature of fever and on the eﬀect of opium, alcohol and inanition. In L. Clendening
(Ed.), Source book of medical history (pp. 428–433). New York: Dover.

Danks, D. (2003). Equilibria of the Rescorla–Wagner model. Journal of Mathematical Psychology, 47,

109–121.

Danks, D., Griﬃths, T. L., & Tenenbaum, J. B. (2003). Dynamical causal learning. In S. Becker, S. Thrun,
information processing systems (Vol. 15, pp. 67–74).

& K. Obermayer (Eds.). Advances neural
Cambridge, MA: MIT Press.

Danks, D. & McKenzie, C. R. M. (submitted). Learning complex causal structures.
Friedman, N. (1997). Learning belief networks in the presence of missing values and hidden variables. In
D. Fisher (Ed.), Fourteenth international conference on machine learning (pp. 125–133). San Francisco,
CA: Morgan Kaufmann.

Friedman, N., & Koller, D. (2000). Being Bayesian about network structure. In Proceedings of the 16th

annual conference on uncertainty in AI, Stanford, CA (pp. 201–210).

Glymour, C. (1998). Learning causes: Psychological explanations of causal explanation. Minds and

Machines, 8, 39–60.

Glymour, C. (2001). The mindÕs arrows: Bayes nets and graphical causal models in psychology. Cambridge,

MA: MIT Press.

Glymour, C., & Cooper, G. (1999). Computation, causation, and discovery. Cambridge, MA: MIT Press.
Goldvarg, E., & Johnson-Laird, P. N. (2001). Naive causality: A mental model theory of causal meaning

and reasoning. Cognitive Science, 25, 565–610.

Gopnik, A., Glymour, C., Sobel, D., Schulz, L., Kushnir, T., & Danks, D. (2004). A theory of causal

learning in children: Causal maps and Bayes nets. Psychological Review, 111, 1–31.

Heckerman, D. (1998). A tutorial on learning with Bayesian networks. In M. I. Jordan (Ed.), Learning in

graphical models (pp. 301–354). Cambridge, MA: MIT Press.

Inhelder, B., & Piaget, J. (1958). The growth of logical thinking from childhood to adolescence. London:

Routledge and Kegan Paul.

Jenkins, H. M., & Ward, W. C. (1965). Judgment of contingency between responses and outcomes.

Psychological Monographs, 79.

Jenner, E. (1798). An inquiry into the causes and eﬀects of the variolae vaccinae.
Kass, R. E., & Raﬀerty, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90,

773–795.

Lagnado, D. A., & Sloman, S. (2002). Learning causal structure. In Proceedings of the twenty-fourth

annual meeting of the Cognitive Science Society. London: Erlbaum.

Lober, K., & Shanks, D. (2000). Is causal induction based on causal power? Critique of Cheng (1997).

Psychological Review, 107, 195–212.

Lo´pez, F. J., Cobos, P. L., Can˜ o, A., & Shanks, D. R. (1998). The rational analysis of human causal and
probability judgment. In M. Oaksford & N. Chater (Eds.), Rational models of cognition (pp. 314–352).
Oxford: Oxford University Press.

Marr, D. (1982). Vision. San Francisco, CA: W.H. Freeman.
Melz, E. R., Cheng, P. W., Holyoak, K. J., & Waldmann, M. R. (1993). Cue competition in human
categorization: Contingency or the Rescorla-Wagner rule. comments on Shanks (1991). Journal of
Experimental Psychology: Learning, Memory, and Cognition, 19, 1398–1410.

Neal, R. M. (1992). Connectionist learning of belief networks. Artiﬁcial Intelligence, 56, 71–113.

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

383

Novick, L. R., & Cheng, P. W. (2004). Assessing interactive causal inference. Psychological Review, 111,

455–485.

Pearl, J. (1988). Probabilistic reasoning in intelligent systems. San Francisco, CA: Morgan Kaufmann.
Pearl, J. (2000). Causality: Models, reasoning and inference. Cambridge, UK: Cambridge University Press.
Pearson, K. (1904/1948). On the theory of contingency and its relation to association and normal correlation.

In Karl PearsonÕs early statistical papers (pp. 443–475). Cambridge: Cambridge University Press.

Perales, J. C., & Shanks, D. R. (2003). Normative and descriptive accounts of the inﬂuence of power and

contingency on causal judgment. Quarterly Journal of Experimental Psychology, 56A, 977–1007.

Rehder, B. (2003). A causal-model theory of conceptual representation and categorization. Journal of

Experimental Psychology: Learning, Memory, and Cognition, 29, 1141–1159.

Rescorla, R. A., & Wagner, A. R. (1972). A theory of Pavlovian conditioning: Variations on the
eﬀectiveness of reinforcement and non-reinforcement. In A. H. Black & W. F. Prokasy (Eds.), Classical
conditioning II: Current research and theory (pp. 64–99). New York: Appleton-Century-Crofts.

Salmon, W. C. (1980). Scientiﬁc explanation and the causal structure of the world. Princeton, NJ: Princeton

University Press.

Schustack, M. W., & Sternberg, R. J. (1981). Evaluation of evidence in causal inference. Journal of

Experimental Psychology: General, 110, 101–120.

Shanks, D. R. (1985). Forward and backward blocking in human contingency judgment. Quarterly

Journal of Experimental Psychology, 97B, 1–21.

Shanks, D. R. (1995a). Is human learning rational. Quarterly Journal of Experimental Psychology, 48A,

257–279.

Shanks, D. R. (1995b). The psychology of associative learning. Cambridge University Press.
Shanks, D. R. (2002). Tests of the power PC theory of cauasl induction with negative contingencies.

Experimental Psychology, 49, 1–8.

Shanks, D. R., & Dickinson, A. (1987). Associative accounts of causality judgment. In G. H. Bower (Ed.).

The psychology of learning and motivation (Vol. 21, pp. 229–261). San Diego, CA: Academic Press.

Shanks, D. R., Lo´ pez, F. J., Darby, R. J., & Dickinson, A. (1996). Distinguishing associative and
probabilsitic contrast theories of human contingency judgment. The psychology of learning and
motivation (Vol. 34, pp. 265–312). San Diego, CA: Academic Press.

Shiﬀrin, R. M., & Steyvers, M. (1997). A model for recognition memory: REM: Retrieving eﬀectively from

memory. Psychonomic Bulletin & Review, 4, 145–166.

Spirtes, P., Glymour, C., & Schienes, R. (1993). Causation prediction and search. New York: Springer-Verlag.
Steyvers, M., Tenenbaum, J. B., Wagenmakers, E. J., & Blum, B. (2003). Inferring causal networks from

observations and interventions. Cognitive Science, 27, 453–489.

Tenenbaum, J. B., & Griﬃths, T. L. (2001). Structure learning in human causal induction. In T. Leen, T.
Dietterich, & V. Tresp (Eds.). Advances neural information processing systems (Vol. 13, pp. 59–65).
Cambridge, MA: MIT Press.

Tenenbaum, J. B., & Griﬃths, T. L. (2003). Theory-based causal induction. In S. Becker, S. Thrun, & K.
Obermayer (Eds.). Advances in neural information processing systems (Vol. 15, pp. 35–42). Cambridge,
MA: MIT Press.

Tenenbaum, J. B., Sobel, D. M., Griﬃths, T. L., & Gopnik, A. (submitted). Bayesian inference in causal

learning from ambiguous data: Evidence from adults and children.

Vallee-Tourangeau, F., Murphy, R. A., Drew, S., & Baker, A. G. (1998). Judging the importance of
constant and variable candidate causes: A test of the power PC theory. Quarterly Journal of
Experimental Psychology, 51A, 65–84.

Waldmann, M. R., & Martignon, L. (1998). A Bayesian network model of causal learning. In M. A.
Gernsbacher & S. J. Derry (Eds.), Proceedings of the twentieth annual conference of the cognitive science
society (pp. 1102–1107). Mahwah, NJ: Erlbaum.

Ward, W. C., & Jenkins, H. M. (1965). The display of information and the judgment of contingency.

Canadian Journal of Psychology, 19, 231–241.

Wasserman, E. A. (1990). Detecting response-outcome relations: Toward an understanding of the causal
texture of the encironment. In G. H. Bower (Ed.). The psychology of learning and motivation (Vol. 26,
pp. 27–82). San Diego, CA: Academic Press.

384

T.L. Griﬃths, J.B. Tenenbaum / Cognitive Psychology 51 (2005) 334–384

Wasserman, E. A., Elek, S. M., Chatlosh, D. C., & Baker, A. G. (1993). Rating causal relations: Role of
proabbility in judgments of response-outcome contingency. Journal of Experimental Psychology:
Learning, Memory and Cognition, 19, 174–188.

White, P. A. (1998). Causal judgement: Use of diﬀerent types of contingency information as conﬁrmatory

and disconﬁrmatory. European Journal of Cognitive Psychology, 10, 131–170.

White, P. A. (2000). Causal judgment from contingency information: The interpretation of factors
common to all instances. Journal of Experimental Psychology: Learning Memory and Cognition, 26,
1083–1102.

White, P. A. (2002a). Causal attribution from covariation information: the evidential evaluation model.

European Journal of Social Psychology, 32, 667–684.

White, P. A. (2002b). Causal judgement from contingency information: Judging interactions between two

causal candidates. Quarterly Journal of Experimental Psychology, 55A, 819–838.

White, P. A. (2002c). Perceiving a strong causal relation in a weak contingency: Further investigation of
the evidential evaluation model of causal judgement. Quarterly Journal of Experimental Psychology,
55A, 97–114.

White, P. A. (2003a). Causal

judgement as evaluation of evidence: The use of conﬁrmatory and

disconﬁrmatory information. Quarterly Journal of Experimental Psychology, 56A, 491–513.

White, P. A. (2003b). Eﬀects of wording and stimulus format on the use of contingency information in

causal judgment. Memory and Cognition, 31, 231–242.

White, P. A. (2003c). Making causal judgments from the proportion of conﬁrming instances: the pCI rule.

Journal of Experimental Psychology: Learning, Memory, and Cognition, 29, 710–727.

Wickens, T. D. (1989). Multiway contingency tables analysis for the social sciences. Hillsdale, NJ: Erlbaum.

