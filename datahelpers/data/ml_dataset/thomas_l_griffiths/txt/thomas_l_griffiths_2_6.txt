Contextual Dependencies in Unsupervised Word Segmentation

Citation for published version:
Goldwater, S, Griffiths, TL & Johnson, M 2006, 'Contextual Dependencies in Unsupervised Word
Segmentation'. in Proceedings of the 21st International Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,
Sydney, Australia, pp. 673-680., 10.3115/1220175.1220260

Digital Object Identifier (DOI):
10.3115/1220175.1220260

Link:
Link to publication record in Edinburgh Research Explorer

Document Version:
Publisher's PDF, also known as Version of record

Published In:
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics

General rights
Copyright for the publications made accessible via the Edinburgh Research Explorer is retained by the author(s)
and / or other copyright owners and it is a condition of accessing these publications that users recognise and
abide by the legal requirements associated with these rights.

Take down policy
The University of Edinburgh has made every reasonable effort to ensure that Edinburgh Research Explorer
content complies with UK legislation. If you believe that the public display of this file breaches copyright please
contact openaccess@ed.ac.uk providing details, and we will remove access to the work immediately and
investigate your claim.

Download date: 03. Oct. 2016

     Edinburgh Research Explorer                                      Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 673–680,

Sydney, July 2006. c(cid:13)2006 Association for Computational Linguistics

673

ContextualDependenciesinUnsupervisedWordSegmentation∗SharonGoldwaterandThomasL.GrifﬁthsandMarkJohnsonDepartmentofCognitiveandLinguisticSciencesBrownUniversityProvidence,RI02912{SharonGoldwater,TomGriffiths,MarkJohnson}@brown.eduAbstractDevelopingbettermethodsforsegment-ingcontinuoustextintowordsisimpor-tantforimprovingtheprocessingofAsianlanguages,andmayshedlightonhowhu-manslearntosegmentspeech.Wepro-posetwonewBayesianwordsegmenta-tionmethodsthatassumeunigramandbi-grammodelsofworddependenciesre-spectively.Thebigrammodelgreatlyout-performstheunigrammodel(andpreviousprobabilisticmodels),demonstratingtheimportanceofsuchdependenciesforwordsegmentation.Wealsoshowthatpreviousprobabilisticmodelsrelycruciallyonsub-optimalsearchprocedures.1IntroductionWordsegmentation,i.e.,discoveringwordbound-ariesincontinuoustextorspeech,isofinterestforbothpracticalandtheoreticalreasons.Itistheﬁrststepofprocessingorthographieswithoutexplicitwordboundaries,suchasChinese.Itisalsooneofthekeyproblemsthathumanlanguagelearnersmustsolveastheyarelearninglanguage.Manypreviousmethodsforunsupervisedwordsegmentationarebasedontheobservationthattransitionsbetweenunits(characters,phonemes,orsyllables)withinwordsaregenerallymorepre-dictablethantransitionsacrosswordboundaries.Statisticsthathavebeenproposedformeasuringthesedifferencesinclude“successorfrequency”(Harris,1954),“transitionalprobabilities”(Saf-franetal.,1996),mutualinformation(Sunetal.,∗Thisworkwaspartiallysupportedbythefollowinggrants:NIH1R01-MH60922,NIHRO1-DC000314,NSFIGERT-DGE-9870676,andtheDARPACALOproject.1998),“accessorvariety”(Fengetal.,2004),andboundaryentropy(CohenandAdams,2001).Whilemethodsbasedonlocalstatisticsarequitesuccessful,herewefocusonapproachesbasedonexplicitprobabilisticmodels.Formulat-inganexplicitprobabilisticmodelpermitsustocleanlyseparateassumptionsabouttheinputandpropertiesoflikelysegmentationsfromdetailsofalgorithmsusedtoﬁndsuchsolutions.Speciﬁ-cally,thispaperdemonstratestheimportanceofcontextualdependenciesforwordsegmentationbycomparingtwoprobabilisticmodelsthatdif-feronlyinthattheﬁrstassumesthattheproba-bilityofawordisindependentofitslocalcontext,whilethesecondincorporatesbigramdependen-ciesbetweenadjacentwords.Thealgorithmsweusetosearchforlikelysegmentationsdodiffer,butsolongasthesegmentationstheyproduceareclosetooptimalwecanbeconﬁdentthatanydif-ferencesinthesegmentationsreﬂectdifferencesintheprobabilisticmodels,i.e.,inthekindsofde-pendenciesbetweenwords.Wearenottheﬁrsttoproposeexplicitprob-abilisticmodelsofwordsegmentation.TwosuccessfulwordsegmentationsystemsbasedonexplicitprobabilisticmodelsarethoseofBrent(1999)andVenkataraman(2001).Brent’sModel-BasedDynamicProgramming(MBDP)systemas-sumesaunigramworddistribution.Venkatara-manusesstandardunigram,bigram,andtrigramlanguagemodelsinthreeversionsofhissystem,whichwerefertoasn-gramSegmentation(NGS).Despitetheirratherdifferentgenerativestructure,theMBDPandNGSsegmentationaccuraciesareverysimilar.Moreover,thesegmentationaccuracyoftheNGSunigram,bigram,andtrigrammod-elshardlydiffer,suggestingthatcontextualdepen-denciesareirrelevanttowordsegmentation.How-674

ever,thesegmentationsproducedbyboththesemethodsdependcruciallyonpropertiesofthesearchprocedurestheyemploy.Weshowthisbyexhibitingforeachmodelasegmentationthatislessaccuratebutmoreprobableunderthatmodel.Inthispaper,wepresentanalternativeframe-workforwordsegmentationbasedontheDirich-letprocess,adistributionusedinnonparametricBayesianstatistics.Thisframeworkallowsustodevelopextensiblemodelsthatareamenabletostandardinferenceprocedures.Wepresenttwosuchmodelsincorporatingunigramandbigramworddependencies,respectively.WeuseGibbssamplingtosamplefromtheposteriordistributionofpossiblesegmentationsunderthesemodels.Theplanofthepaperisasfollows.Inthenextsection,wedescribeMBDPandNGSindetail.InSection3wepresenttheunigramversionofourownmodel,theGibbssamplingprocedureweuseforinference,andexperimentalresults.Section4extendsthatmodeltoincorporatebigramdepen-dencies,andSection5concludesthepaper.2NGSandMBDPTheNGSandMBDPsystemsaresimilarinsomeways:botharedesignedtoﬁndutterancebound-ariesinacorpusofphonemicallytranscribedut-terances,withknownutteranceboundaries.Bothalsouseapproximateonlinesearchprocedures,choosingandﬁxingasegmentationforeachutter-ancebeforemovingontothenext.Inthissection,wefocusontheverydifferentprobabilisticmod-elsunderlyingthetwosystems.WeshowthattheoptimalsolutionundertheNGSmodelistheun-segmentedcorpus,andsuggestthatthisproblemstemsfromthefactthatthemodelassumesauni-formprioroverhypotheses.WethenpresenttheMBDPmodel,whichusesanon-uniformpriorbutisdifﬁculttoextendbeyondtheunigramcase.2.1NGSNGSassumesthateachutteranceisgeneratedin-dependentlyviaastandardn-grammodel.Forsimplicity,wewilldiscusstheunigramversionofthemodelhere,althoughourargumentisequallyapplicabletothebigramandtrigramversions.TheunigrammodelgeneratesanutteranceuaccordingtothegrammarinFigure1,soP(u)=p$(1−p$)n−1nYj=1P(wj)(1)1−p$U→WUp$U→WP(w)W→w∀w∈Σ∗Figure1:TheunigramNGSgrammar.whereuconsistsofthewordsw1...wnandp$istheprobabilityoftheutteranceboundarymarker$.Thismodelcanbeusedtoﬁndthehighestprob-abilitysegmentationhypothesishgiventhedatadbyusingBayes’rule:P(h|d)∝P(d|h)P(h)NGSassumesauniformpriorP(h)overhypothe-ses,soitsgoalistoﬁndthesolutionthatmaxi-mizesthelikelihoodP(d|h).Usingthismodel,NGS’sapproximatesearchtechniquedeliverscompetitiveresults.However,thetruemaximumlikelihoodsolutionisnotcom-petitive,sinceitcontainsnoutterance-internalwordboundaries.Toseewhynot,considerthesolutioninwhichp$=1andeachutteranceisasingle‘word’,withprobabilityequaltotheempir-icalprobabilityofthatutterance.Anyotherso-lutionwillmatchtheempiricaldistributionofthedatalesswell.Inparticular,asolutionwithad-ditionalwordboundariesmusthave1−p$>0,whichmeansitwastesprobabilitymassmodelingunseendata(whichcannowbegeneratedbycon-catenatingobservedutterancestogether).Intuitively,theNGSmodelconsiderstheunseg-mentedsolutiontobeoptimalbecauseitranksallhypothesesequallyprobableapriori.Weknow,however,thathypothesesthatmemorizetheinputdataareunlikelytogeneralizetounseendata,andarethereforepoorsolutions.Topreventmemo-rization,wecouldrestrictourhypothesisspacetomodelswithfewerparametersthanthenumberofutterancesinthedata.Amoregeneralandmathe-maticallysatisfactorysolutionistoassumeanon-uniformprior,assigninghigherprobabilitytohy-potheseswithfewerparameters.ThisisinfacttheroutetakenbyBrentinhisMBDPmodel,asweshallseeinthefollowingsection.2.2MBDPMBDPassumesacorpusofutterancesisgener-atedasasingleprobabilisticeventwithfoursteps:1.GenerateL,thenumberoflexicaltypes.2.Generateaphonemicrepresentationforeachtype(excepttheutteranceboundarytype,$).675

3.Generateatokenfrequencyforeachtype.4.Generateanorderingforthesetoftokens.Inaﬁnaldeterministicstep,theorderedtokensareconcatenatedtocreateanunsegmentedcor-pus.Thismeansthatcertainsegmentedcorporawillproducetheobserveddatawithprobability1,andallotherswillproduceitwithprobability0.Theposteriorprobabilityofasegmentationgiventhedataisthusproportionaltoitspriorprobabilityunderthegenerativemodel,andthebestsegmen-tationisthatwiththehighestpriorprobability.TherearetwoimportantpointstonoteabouttheMBDPmodel.First,thedistributionoverLassignshigherprobabilitytomodelswithfewerlexicalitems.Wehavearguedthatthisisneces-sarytoavoidmemorization,andindeedtheunseg-mentedcorpusisnottheoptimalsolutionunderthismodel,aswewillshowinSection3.Second,thefactorizationintofourseparatestepsmakesittheoreticallypossibletomodifyeachstepin-dependentlyinordertoinvestigatetheeffectsofthevariousmodelingassumptions.However,themathematicalstatementofthemodelandtheap-proximationsnecessaryforthesearchproceduremakeitunclearhowtomodifythemodelinanyinterestingway.Inparticular,thefourthstepusesauniformdistribution,whichcreatesaunigramconstraintthatcannoteasilybechanged.Sinceourresearchaimstoinvestigatetheeffectsofdifferentmodelingassumptionsonlexicalacquisition,wedevelopinthefollowingsectionsafarmoreﬂex-iblemodelthatalsoincorporatesapreferenceforsparsesolutions.3UnigramModel3.1TheDirichletProcessModelOurgoalisamodeloflanguagethatpreferssparsesolutions,allowsindependentmodiﬁcationofcomponents,andisamenabletostandardsearchprocedures.WeachievethisgoalbybasingourmodelontheDirichletprocess(DP),adistributionusedinnonparametricBayesianstatistics.Ourun-igrammodelofwordfrequenciesisdeﬁnedaswi|G∼GG|α0,P0∼DP(α0,P0)wheretheconcentrationparameterα0andthebasedistributionP0areparametersofthemodel.EachwordwiinthecorpusisdrawnfromadistributionG,whichconsistsofasetofpos-siblewords(thelexicon)andprobabilitiesasso-ciatedwiththosewords.GisgeneratedfromaDP(α0,P0)distribution,withtheitemsinthelexiconbeingsampledfromP0andtheirproba-bilitiesbeingdeterminedbyα0,whichactsliketheparameterofaninﬁnite-dimensionalsymmet-ricDirichletdistribution.Weprovidesomeintu-itionfortherolesofα0andP0below.AlthoughtheDPmodelmakesthedistributionGexplicit,weneverdealwithGdirectly.WetakeaBayesianapproachandintegrateoverallpossiblevaluesofG.Theconditionalprobabil-ityofchoosingtogenerateawordfromaparticu-larlexicalentryisthengivenbyasimplestochas-ticprocessknownastheChineserestaurantpro-cess(CRP)(Aldous,1985).Imaginearestaurantwithaninﬁnitenumberoftables,eachwithinﬁniteseatingcapacity.Customersentertherestaurantandseatthemselves.Letzibethetablechosenbytheithcustomer.ThenP(zi|z−i)=n(z−i)ki−1+α00≤k<K(z−i)α0i−1+α0k=K(z−i)(2)wherez−i=z1...zi−1,n(z−i)kisthenumberofcustomersalreadysittingattablek,andK(z−i)isthetotalnumberofoccupiedtables.Inourmodel,thetablescorrespondto(possiblyrepeated)lexicalentries,havinglabelsgeneratedfromthedistribu-tionP0.Theseatingarrangementthusspeciﬁesadistributionoverwordtokens,witheachcus-tomerrepresentingonetoken.Thismodelisaninstanceofthetwo-stagemodelingframeworkde-scribedbyGoldwateretal.(2006),withP0asthegeneratorandtheCRPastheadaptor.Ourmodelcanbeviewedintuitivelyasacachemodel:eachwordinthecorpusiseitherretrievedfromacacheorgeneratedanew.Summingoverallthetableslabeledwiththesamewordyieldstheprobabilitydistributionfortheithwordgivenpreviouslyobservedwordsw−i:P(wi|w−i)=n(w−i)wii−1+α0+α0P0(wi)i−1+α0(3)wheren(w−i)wisthenumberofinstancesofwob-servedinw−i.Theﬁrsttermistheprobabilityofgeneratingwfromthecache(i.e.,sittingatanoccupiedtable),andthesecondtermistheproba-bilityofgeneratingitanew(sittingatanunoccu-piedtable).Theactualtableassignmentsz−ionlybecomeimportantlater,inthebigrammodel.676

Thereareseveralimportantpointstonoteaboutthismodel.First,theprobabilityofgeneratingaparticularwordfromthecacheincreasesasmoreinstancesofthatwordareobserved.Thisrich-get-richerprocesscreatesapower-lawdistributiononwordfrequencies(Goldwateretal.,2006),thesamesortofdistributionfoundempiricallyinnat-urallanguage.Second,theparameterα0canbeusedtocontrolhowsparsethesolutionsfoundbythemodelare.Thisparameterdeterminesthetotalprobabilityofgeneratinganynovelword,aproba-bilitythatdecreasesasmoredataisobserved,butneverdisappears.Finally,theparameterP0canbeusedtoencodeexpectationsaboutthenatureofthelexicon,sinceitdeﬁnesaprobabilitydistri-butionacrossdifferentnovelwords.Thefactthatthisdistributionisdeﬁnedseparatelyfromthedis-tributiononwordfrequenciesgivesthemodelad-ditionalﬂexibility,sinceeitherdistributioncanbemodiﬁedindependentlyoftheother.Sincethegoalofthispaperistoinvestigatetheroleofcontextinwordsegmentation,wechosethesimplestpossiblemodelforP0,i.e.aunigramphonemedistribution:P0(w)=p#(1−p#)n−1nYi=1P(mi)(4)wherewordwconsistsofthephonemesm1...mn,andp#istheprobabilityofthewordboundary#.Forsimplicityweusedauniformdistributionoverphonemes,andexperimentedwithdifferentﬁxedvaluesofp#.1Aﬁnaldetailofourmodelisthedistributiononutterancelengths,whichisgeometric.Thatis,weassumeagrammarsimilartotheoneshowninFigure1,withtheadditionofasymmetricBeta(τ2)priorovertheprobabilityoftheUproductions,2andthesubstitutionoftheDPforthestandardmultinomialdistributionovertheWproductions.3.2GibbsSamplingHavingdeﬁnedourgenerativemodel,weareleftwiththeproblemofinference:wemustdeterminetheposteriordistributionofhypothesesgivenourinputcorpus.Todoso,weuseGibbssampling,astandardMarkovchainMonteCarlomethod(Gilksetal.,1996).Gibbssamplingisanitera-tiveprocedureinwhichvariablesarerepeatedly1Note,however,thatourmodelcouldbeextendedtolearnbothp#andthedistributionoverphonemes.2TheBetadistributionisaDirichletdistributionovertwooutcomes.WUw1=w2.w3UWUWw3w2h1:h2:Figure2:Thetwohypothesesconsideredbytheunigramsampler.Dashedlinesindicatepossibleadditionalstructure.Allrulesexceptthoseinboldarepartofh−.sampledfromtheirconditionalposteriordistribu-tiongiventhecurrentvaluesofallothervariablesinthemodel.ThesamplerdeﬁnesaMarkovchainwhosestationarydistributionisP(h|d),soafterconvergencesamplesarefromthisdistribution.OurGibbssamplerconsidersasinglepossibleboundarypointatatime,soeachsampleisfromasetoftwohypotheses,h1andh2.Thesehy-pothesescontainallthesameboundariesexceptattheonepositionunderconsideration,whereh2hasaboundaryandh1doesnot.ThestructuresareshowninFigure2.Inordertosampleahypothe-sis,weneedonlycalculatetherelativeprobabili-tiesofh1andh2.Sinceh1andh2arethesameex-ceptforafewrules,thisisstraightforward.Leth−beallofthestructuresharedbythetwohypothe-ses,includingn−words,andletdbetheobserveddata.ThenP(h1|h−,d)=P(w1|h−,d)=n(h−)w1+α0P0(w1)n−+α0(5)wherethesecondlinefollowsfromEquation3andthepropertiesoftheCRP(inparticular,thatitisexchangeable,withtheprobabilityofaseatingconﬁgurationnotdependingontheorderinwhichcustomersarrive(Aldous,1985)).Also,P(h2|h−,d)=P(r,w2,w3|h−,d)=P(r|h−,d)P(w2|h−,d)P(w3|w2,h−,d)=nr+τ2n−+1+τ·n(h−)w2+α0P0(w2)n−+α0·n(h−)w3+I(w2=w3)+α0P0(w3)n−+1+α0(6)wherenristhenumberofbranchingrulesr=U→WUinh−,andI(.)isanindicatorfunc-tiontakingonthevalue1whenitsargumentis677

true,and0otherwise.Thenrtermisderivedbyintegratingoverallpossiblevaluesofp$,andnot-ingthatthetotalnumberofUproductionsinh−isn−+1.Usingtheseequationswecansimplyproceedthroughthedata,samplingeachpotentialbound-arypointinturn.OncetheGibbssamplercon-verges,thesesampleswillbedrawnfromthepos-teriordistributionP(h|d).3.3ExperimentsInourexperiments,weusedthesamecorpusthatNGSandMBDPweretestedon.Thecor-pus,suppliedtousbyBrent,consistsof9790transcribedutterances(33399words)ofchild-directedspeechfromtheBernstein-Ratnercor-pus(Bernstein-Ratner,1987)intheCHILDESdatabase(MacWhinneyandSnow,1985).Theut-teranceshavebeenconvertedtoaphonemicrep-resentationusingaphonemicdictionary,sothateachoccurrenceofawordhasthesamephonemictranscription.Utteranceboundariesaregivenintheinputtothesystem;otherwordboundariesarenot.BecauseourGibbssamplerisslowtoconverge,weusedannealingtospeedinference.Webeganwithatemperatureofγ=10anddecreasedγin10incrementstoaﬁnalvalueof1.Atemperatureofγcorrespondstoraisingtheprobabilitiesofh1andh2tothepowerof1γpriortosampling.WeranourGibbssamplerfor20,000iterationsthroughthecorpus(withγ=1fortheﬁnal2000)andevaluatedourresultsonasinglesampleatthatpoint.Wecalculatedprecision(P),recall(R),andF-score(F)onthewordtokensinthecorpus,wherebothboundariesofawordmustbecorrecttocountthewordascorrect.Theinducedlexiconwasalsoscoredforaccuracyusingthesemetrics(LP,LR,LF).RecallthatourDPmodelhasthreeparameters:τ,p#,andα0.Giventhelargenumberofknownutteranceboundaries,weexpectthevalueofτtohavelittleeffectonourresults,sowesimplyﬁxedτ=2forallexperiments.Figure3showstheef-fectsofvaryingofp#andα0.3Lowervaluesofp#causelongerwords,whichtendstoimprovere-call(andthusF-score)inthelexicon,butdecreasetokenaccuracy.Highervaluesofα0allowmorenovelwords,whichalsoimproveslexiconrecall,3Itisworthnotingthatalltheseparameterscouldbein-ferred.Weleavethisforfuturework.0.10.30.50.70.9505560(a) Varying P(#)  125102050100200500505560(b) Varying α0  LFFLFFFigure3:Word(F)andlexicon(LF)F-score(a)asafunctionofp#,withα0=20and(b)asafunctionofα0,withp#=.5.butbeginstodegradeprecisionafterapoint.Duetothenegativecorrelationbetweentokenaccuracyandlexiconaccuracy,thereisnosinglebestvalueforeitherp#orα0;furtherdiscussionreferstothesolutionforp#=.5,α0=20(thoughothersarequalitativelysimilar).InTable1(a),wecomparetheresultsofoursys-temtothoseofMBDPandNGS.4Althoughoursystemhashigherlexiconaccuracythantheoth-ers,itstokenaccuracyismuchworse.Thisresultoccursbecauseoursystemoftenmis-analyzesfre-quentlyoccurringwords.Inparticular,manyofthesewordsoccurincommoncollocationssuchaswhat’sthatanddoyou,whichthesysteminter-pretsasasinglewords.Itturnsoutthatafull31%oftheproposedlexiconandnearly30%oftokensconsistofthesekindsoferrors.Uponreﬂection,itisnotsurprisingthatauni-gramlanguagemodelwouldsegmentwordsinthisway.Collocationsviolatetheunigramassumptioninthemodel,sincetheyexhibitstrongword-to-worddependencies.Theonlywaythemodelcancapturethesedependenciesisbyassumingthatthesecollocationsareinfactwordsthemselves.Whydon’ttheMBDPandNGSunigrammod-elsexhibittheseproblems?WehavealreadyshownthatNGS’sresultsareduetoitssearchpro-cedureratherthanitsmodel.ThesameturnsouttobetrueforMBDP.Table2showstheprobabili-4WeusedtheimplementationsofMBDPandNGSavail-ableathttp://www.speech.sri.com/people/anand/toobtainre-sultsforthosesystems.678

(a)PRFLPLRLFNGS67.770.268.952.951.352.0MBDP67.069.468.253.651.352.4DP61.947.653.857.057.557.2(b)PRFLPLRLFNGS76.685.881.060.052.455.9MBDP77.086.181.360.853.056.6DP94.297.195.686.562.272.4Table1:Accuracyofthevarioussystems,withbestscoresinbold.TheunigramversionofNGSisshown.DPresultsarewithp#=.5andα0=20.(a)Resultsonthetruecorpus.(b)Resultsonthepermutedcorpus.Seg:TrueNoneMBDPNGSDPNGS204.590.9210.7210.8183.0MBDP208.2321.7217.0218.0189.8DP222.4393.6231.2231.6200.6Table2:Negativelogprobabilities(x1000)un-dereachmodelofthetruesolution,thesolutionwithnoutterance-internalboundaries,andtheso-lutionsfoundbyeachalgorithm.Bestsolutionsundereachmodelarebold.tiesundereachmodelofvarioussegmentationsofthecorpus.Fromtheseﬁgures,wecanseethattheMBDPmodelassignshigherprobabilitytothesolutionfoundbyourGibbssamplerthantothesolutionfoundbyBrent’sownincrementalsearchalgorithm.Inotherwords,Brent’smodeldoespre-ferthelower-accuracycollocationsolution,buthissearchalgorithminsteadﬁndsahigher-accuracybutlower-probabilitysolution.Weperformedtwoexperimentssuggestingthatourowninferenceproceduredoesnotsufferfromsimilarproblems.First,weinitializedourGibbssamplerinthreedifferentways:withnoutterance-internalboundaries,withaboundaryaftereverycharacter,andwithrandomboundaries.Ourre-sultswerevirtuallythesameregardlessofinitial-ization.Second,wecreatedanartiﬁcialcorpusbyrandomlypermutingthewordsinthetruecorpus,leavingtheutterancelengthsthesame.Thear-tiﬁcialcorpusadherestotheunigramassumptionofourmodel,soifourinferenceprocedureworkscorrectly,weshouldbeabletocorrectlyidentifythewordsinthepermutedcorpus.Thisisexactlywhatwefound,asshowninTable1(b).Whileallthreemodelsperformbetterontheartiﬁcialcor-pus,theimprovementsoftheDPmodelarebyfarthemoststriking.4BigramModel4.1TheHierarchicalDirichletProcessModelTheresultsofourunigramexperimentssuggestedthatwordsegmentationcouldbeimprovedbytakingintoaccountdependenciesbetweenwords.Totestthishypothesis,weextendedourmodeltoincorporatebigramdependenciesusingahi-erarchicalDirichletprocess(HDP)(Tehetal.,2005).Ourapproachissimilartopreviousn-grammodelsusinghierarchicalPitman-Yorprocesses(Goldwateretal.,2006;Teh,2006).TheHDPisappropriateforsituationsinwhichtherearemulti-pledistributionsoversimilarsetsofoutcomes,andthedistributionsarebelievedtobesimilar.Inourcase,wedeﬁneabigrammodelbyassumingeachwordhasadifferentdistributionoverthewordsthatfollowit,butallthesedistributionsarelinked.ThedeﬁnitionofourbigramlanguagemodelasanHDPiswi|wi−1=w,Hw∼Hw∀wHw|α1,G∼DP(α1,G)∀wG|α0,P0∼DP(α0,P0)Thatis,P(wi|wi−1=w)isdistributedaccord-ingtoHw,aDPspeciﬁctowordw.HwislinkedtotheDPsforallotherwordsbythefactthattheyshareacommonbasedistributionG,whichisgen-eratedfromanotherDP.5Asintheunigrammodel,weneverdealwithHworGdirectly.Byintegratingoverthem,wegetadistributionoverbigramfrequenciesthatcanbeunderstoodintermsoftheCRP.Now,eachwordtypewisassociatedwithitsownrestaurant,whichrepresentsthedistributionoverwordsthatfolloww.Differentrestaurantsarenotcompletelyinde-pendent,however:thelabelsonthetablesintherestaurantsareallchosenfromacommonbasedistribution,whichisanotherCRP.TounderstandtheHDPmodelintermsofagrammar,weconsider$asaspecialwordtype,sothatwirangesoverΣ∗∪{$}.Afterobservingw−i,theHDPgrammarisasshowninFigure4,5ThisHDPformulationisanoversimpliﬁcation,sinceitdoesnotaccountforutteranceboundariesproperly.Thegrammarformulation(seebelow)does.679

P2(wi|w−i,z−i)Uwi−1→WwiUwi∀wi∈Σ∗,wi−1∈Σ∗∪{$}P2($|w−i,z−i)Uwi−1→$∀wi−1∈Σ∗1Wwi→wi∀wi∈Σ∗Figure4:TheHDPgrammarafterobservingw−i.withP2(wi|h−i)=n(wi−1,wi)+α1P1(wi|h−i)nwi−1+α1(7)P1(wi|h−i)=tΣ∗+τ2t+τ·twi+α0P0(wi)tΣ∗+α0wi∈Σ∗t$+τ2t+τwi=$whereh−i=(w−i,z−i);t$,tΣ∗,andtwiarethetotalnumberoftables(acrossallwords)labeledwith$,non-$,andwi,respectively;t=t$+tΣ∗isthetotalnumberoftables;andn(wi−1,wi)isthenumberofoccurrencesofthebigram(wi−1,wi).Wehavesuppressedthesuperscript(w−i)nota-tioninallcases.ThebasedistributionsharedbyallbigramsisgivenbyP1,whichcanbeviewedasaunigrambackoffwheretheunigramprobabilitiesarelearnedfromthebigramtablelabels.WecanperforminferenceonthisHDPbigrammodelusingaGibbssamplersimilartoouruni-gramsampler.DetailsappearintheAppendix.4.2ExperimentsWeusedthesamebasicsetupforourexperimentswiththeHDPmodelasweusedfortheDPmodel.Weexperimentedwithdifferentvaluesofα0andα1,keepingp#=.5throughout.SomeresultsoftheseexperimentsareplottedinFigure5.Withappropriateparametersettings,bothlexiconandtokenaccuracyarehigherthanintheunigrammodel(dramaticallyso,fortokens),andthereisnolongeranegativecorrelationbetweenthetwo.Onlyafewcollocationsremaininthelexicon,andmostlexiconerrorsareonlow-frequencywords.Thebestvaluesofα0aremuchlargerthanintheunigrammodel,presumablybecausealluniquewordtypesmustbegeneratedviaP0,butinthebigrammodelthereisanadditionallevelofdis-counting(theunigramprocess)beforereachingP0.Smallervaluesofα0leadtofewerwordtypeswithfewercharactersonaverage.Table3comparestheoptimalresultsoftheHDPmodeltotheonlypreviousmodelincorpo-ratingbigramdependencies,NGS.Duetosearch,theperformanceofthebigramNGSmodelisnotmuchdifferentfromthatoftheunigrammodel.In10020050010002000406080(a) Varying α0  FLF5102050100200500406080(b) Varying α1  FLFFigure5:Word(F)andlexicon(LF)F-score(a)asafunctionofα0,withα1=10and(b)asafunctionofα1,withα0=1000.PRFLPLRLFNGS68.168.668.354.557.055.7HDP79.474.076.667.958.963.1Table3:Bigramsystemaccuracy,withbestscoresinbold.HDPresultsarewithp#=.5,α0=1000,andα1=10.contrast,ourHDPmodelperformsfarbetterthanourDPmodel,leadingtothehighestpublishedac-curacyforthiscorpusonbothtokensandlexicalitems.Overall,theseresultsstronglysupportourhypothesisthatmodelingbigramdependenciesisimportantforaccuratewordsegmentation.5ConclusionInthispaper,wehaveintroducedanewmodel-basedapproachtowordsegmentationthatdrawsontechniquesfromBayesianstatistics,andwehavedevelopedmodelsincorporatingunigramandbigramdependencies.TheuseoftheDirichletprocessasthebasisofourapproachyieldssparsesolutionsandallowsustheﬂexibilitytomodifyindividualcomponentsofthemodels.WehavepresentedamethodofinferenceusingGibbssam-pling,whichisguaranteedtoconvergetothepos-teriordistributionoverpossiblesegmentationsofacorpus.Ourapproachtowordsegmentationallowsustoinvestigatequestionsthatcouldnotbeaddressedsatisfactorilyinearlierwork.Wehaveshownthatthesearchalgorithmsusedwithpreviousmodelsofwordsegmentationdonotachievetheirob-680

P(h1|h−,d)=n(wl,w1)+α1P1(w1|h−,d)nwl+α1·n(w1,wr)+I(wl=w1=wr)+α1P1(wr|h−,d)nw1+1+α1P(h2|h−,d)=n(wl,w2)+α1P1(w2|h−,d)nwl+α1·n(w2,w3)+I(wl=w2=w3)+α1P1(w3|h−,d)nw2+1+α1·n(w3,wr)+I(wl=w3,w2=wr)+I(w2=w3=wr)+α1P1(wr|h−,d)nw3+1+I(w2=w4)+α1Figure6:Gibbssamplingequationsforthebigrammodel.Allcountsarewithrespecttoh−.jectives,whichhasledtomisleadingresults.Inparticular,previousworksuggestedthattheuseofword-to-worddependencieshaslittleeffectonwordsegmentation.Ourexperimentsindicatein-steadthatbigramdependenciescanbecrucialforavoidingunder-segmentationoffrequentcolloca-tions.Incorporatingthesedependenciesintoourmodelgreatlyimprovedsegmentationaccuracy,andledtobetterperformancethanpreviousap-proachesonallmeasures.ReferencesD.Aldous.1985.Exchangeabilityandrelatedtopics.In´Ecoled’´et´edeprobabilit´esdeSaint-Flour,XIII—1983,pages1–198.Springer,Berlin.C.Antoniak.1974.MixturesofDirichletprocesseswithap-plicationstoBayesiannonparametricproblems.TheAn-nalsofStatistics,2:1152–1174.N.Bernstein-Ratner.1987.Thephonologyofparent-childspeech.InK.NelsonandA.vanKleeck,editors,Chil-dren’sLanguage,volume6.Erlbaum,Hillsdale,NJ.M.Brent.1999.Anefﬁcient,probabilisticallysoundal-gorithmforsegmentationandworddiscovery.MachineLearning,34:71–105.P.CohenandN.Adams.2001.Analgorithmforsegment-ingcategoricaltimeseriesintomeaningfulepisodes.InProceedingsoftheFourthSymposiumonIntelligentDataAnalysis.H.Feng,K.Chen,X.Deng,andW.Zheng.2004.Acces-sorvarietycriteriaforchinesewordextraction.Computa-tionalLingustics,30(1).W.R.Gilks,S.Richardson,andD.J.Spiegelhalter,editors.1996.MarkovChainMonteCarloinPractice.ChapmanandHall,Suffolk.S.Goldwater,T.Grifﬁths,andM.Johnson.2006.Interpo-latingbetweentypesandtokensbyestimatingpower-lawgenerators.InAdvancesinNeuralInformationProcess-ingSystems18,Cambridge,MA.MITPress.Z.Harris.1954.Distributionalstructure.Word,10:146–162.B.MacWhinneyandC.Snow.1985.Thechildlanguagedataexchangesystem.JournalofChildLanguage,12:271–296.J.Saffran,E.Newport,andR.Aslin.1996.Wordsegmenta-tion:Theroleofdistributionalcues.JournalofMemoryandLanguage,35:606–621.M.Sun,D.Shen,andB.Tsou.1998.Chinesewordseg-mentationwithoutusinglexiconandhand-craftedtrainingdata.InProceedingsofCOLING-ACL.Y.Teh,M.Jordan,M.Beal,andD.Blei.2005.HierarchicalDirichletprocesses.InAdvancesinNeuralInformationProcessingSystems17.MITPress,Cambridge,MA.Y.Teh.2006.ABayesianinterpretationofinterpolatedkneser-ney.TechnicalReportTRA2/06,NationalUniver-sityofSingapore,SchoolofComputing.A.Venkataraman.2001.Astatisticalmodelforworddis-coveryintranscribedspeech.ComputationalLinguistics,27(3):351–372.AppendixTosamplefromtheposteriordistributionoverseg-mentationsinthebigrammodel,wedeﬁneh1andh2aswedidintheunigramsamplersothatforthecorpussubstrings,h1hasasingleword(s=w1)whereh2hastwo(s=w2.w3).Letwlandwrbethewords(or$)precedingandfollowings.Thentheposteriorprobabilitiesofh1andh2aregiveninFigure6.P1(.)canbecalculatedexactlyusingtheequationinSection4.1,butthisrequiresex-plicitlytrackingandsamplingtheassignmentofwordstotables.Foreasierandmoreefﬁcientim-plementation,weuseanapproximation,replacingeachtablecounttwibyitsexpectedvalueE[twi].InaDP(α,P),theexpectednumberofCRPtablesforanitemoccurringntimesisαlogn+αα(Anto-niak,1974),soE[twi]=α1Xjlogn(wj,wi)+α1α1Thisapproximationrequiresonlythebigramcounts,whichwemusttrackanyway.