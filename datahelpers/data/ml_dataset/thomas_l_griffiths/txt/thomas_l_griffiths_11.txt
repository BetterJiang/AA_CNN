Learning Systems of Concepts with an Inﬁnite Relational Model

Charles Kemp and Joshua B. Tenenbaum

Department of Brain and Cognitive Science

Massachusetts Institute of Technology

Thomas L. Grifﬁths

Department of Cognitive and Linguistic Sciences

Brown University

{ckemp,jbt}@mit.edu

{tom griffiths}@brown.edu

Takeshi Yamada and Naonori Ueda
NTT Communication Science Laboratories

{yamada,ueda}@cslab.kecl.ntt.co.jp

Abstract

Relationships between concepts account for a large pro-
portion of semantic knowledge. We present a nonpara-
metric Bayesian model that discovers systems of related
concepts. Given data involving several sets of entities,
our model discovers the kinds of entities in each set and
the relations between kinds that are possible or likely.
We apply our approach to four problems: clustering ob-
jects and features, learning ontologies, discovering kin-
ship systems, and discovering structure in political data.

Philosophers, psychologists and computer scientists have
proposed that semantic knowledge is best understood as
a system of relations. Two questions immediately arise:
how can these systems be represented, and how are these
representations acquired? Researchers who start with the
ﬁrst question often devise complex representational schemes
(e.g. Minsky’s (1975) classic work on frames), but explain-
ing how these representations are learned is a challenging
problem. We take the opposite approach. We consider only
simple relational systems, but show how these systems can
be acquired by unsupervised learning.

The systems we wish to discover are simple versions of
the “domain theories” discussed by cognitive scientists and
AI researchers (Davis 1990). Suppose that a domain in-
cludes several types, or sets of entities. One role of a domain
theory is to specify the kinds of entities that exist in each set,
and the possible or likely relationships between those kinds.
Consider the domain of medicine, and a single type deﬁned
as the set of terms that might appear on a medical chart. A
theory of this domain might specify that cancer and diabetes
are both disorders, asbestos and arsenic are both chemicals,
and that chemicals can cause disorders. Our model assumes
that each entity belongs to exactly one kind, or cluster, and
simultaneously discovers the clusters and the relationships
between clusters that are best supported by the data.

A key feature of our approach is that it does not require
the number of clusters to be ﬁxed in advance. The number of
clusters used by a theory should be able to grow as more and
more data are encountered, but a theory-learner should intro-
duce no more clusters than are necessary to explain the data.
Our approach automatically chooses an appropriate number
Copyright c(cid:2) 2006, American Association for Artiﬁcial Intelli-
gence (www.aaai.org). All rights reserved.

of clusters using a prior that favors small numbers of clus-
ters, but has access to a countably inﬁnite collection of clus-
ters. We therefore call our approach the inﬁnite relational
model (IRM). Previous inﬁnite models (Rasmussen 2000;
Antoniak 1974) have focused on feature data, and the IRM
extends these approaches to work with arbitrary systems of
relational data.

Our framework can discover structure in relational data
sets that appear quite different on the surface. We demon-
strate its range by applying it to four problems. First we
suggest that object-feature data can be proﬁtably viewed as
a relation between two sets of entities — the objects and
the features — and show how the IRM simultaneously clus-
ters both. We then use the IRM to learn a biomedical on-
tology. Ontologies are classic examples of the theories we
have described, since they group entities into higher-level
concepts and specify how these high-level concepts relate to
each other. Next we show that the IRM discovers aspects of
the kinship structure of an Australian tribe. Our ﬁnal exam-
ple considers a political data set, and we discover a system
with clusters of countries, clusters of interactions between
countries, and clusters of country features.

The Inﬁnite Relational Model

Suppose we are given one or more relations involving one
or more types. The goal of the IRM is to partition each type
into clusters, where a good set of partitions allows relation-
ships between entities to be predicted by their cluster assign-
ments. For example, we may have a single type people and
a single relation likes(i, j) which indicates whether person
i likes person j. Our goal is to organize the entities into
clusters that relate to each other in predictable ways (Fig-
ure 1a). We also allow predicate types: if there are multiple
relations deﬁned over the same domain, we will group them
into a type and refer to them as predicates. For instance, we
may have several social predicates deﬁned over the domain
people × people: likes(·,·), admires(·,·), respects(·,·), and
hates(·,·). We can introduce a type for these social pred-
icates, and deﬁne a ternary relation applies(i, j, p) which
is true if predicate p applies to the pair (i, j). Our goal
is now to simultaneously cluster the people and the predi-
cates (Figure 1c). The IRM can handle arbitrarily complex
systems of attributes, entities and relations:
if we include
demographic attributes for the people, for example, we can

381Output

1 6 4 8 2 3 5 9 7

b)

1 6
4

8 2
3
5
9 7

z

Input
T 1

1 2 3 4 5 6 7 8 9

a)

T 1

1
2
3
4
5
6
7
8
9

1
6
4
8
2
3
5
9
7

0.1
0.1
0.9

0.9
0.1
0.1

0.1
0.9
0.1

1 6 4 8 2 3 5 9 7

c)

T 1

η

R

1
6
4
8
2
3
5
9
7

d)

T 1

T 2

T 1

T 2

T 3

T 1

Figure 1: (a) Input and output when the IRM is applied to a binary relation R : T 1 × T 1 → {0, 1}. The IRM discovers a
partition of the entities, and the input matrix takes on a relatively clean block structure when sorted according to this partition.
(b) The IRM assumes that relation R is generated from two latent structures: a partition z and a parameter matrix η. Entry
R(i, j) is generated by tossing a coin with bias η(zi, zj), where zi and zj are the cluster assignments of entities i and j. The
IRM inverts this generative model to discover the z and the η that best explain relation R. (c) Clustering a three place relation
R : T 1 × T 1 × T 2 → {0, 1}. T 1 might be a set of people, T 2 a set of social predicates, and R might specify whether each
predicate applies to each pair of people. The IRM looks for solutions where each three dimensional sub-block includes mostly
1s or mostly 0s. (d) Clustering three relations simultaneously. T 1 might be a set of people, T 2 a set of demographic features,
and T 3 a set of questions on a personality test. Note that the partition for T 1 is the same wherever this type appears.

simultaneously cluster people, social predicates, and demo-
graphic attributes.

Formally, suppose that the observed data are m rela-
tions involving n types. Let Ri be the ith relation, T j
be the jth type, and zj be a vector of cluster assignments
for T j. Our task is to infer the cluster assignments, and
we are ultimately interested in the posterior distribution
P (z1, . . . , zn|R1, . . . , Rm). We specify this distribution by
deﬁning a generative model for the relations and the cluster
assignments:

P (R1, . . . , Rm, z1, . . . , zn) =

m(cid:2)i=1

P (Ri|z1, . . . , zn)

P (zj)

n(cid:2)j=1

where we assume that the relations are conditionally inde-
pendent given the cluster assignments, and that the cluster
assignments for each type are independent. To complete the
generative model we ﬁrst describe the prior on the cluster
assignment vectors, P (zj), then show how the relations are
generated given a set of these vectors.

Generating clusters
To allow the IRM the ability to discover the number of clus-
ters in type T , we use a prior that assigns some probabil-
ity mass to all possible partitions of the type. A reasonable
prior should encourage the model to introduce only as many
clusters as are warranted by the data. Following previous
work on nonparametric Bayesian models (Rasmussen 2000;
Antoniak 1974), we use a distribution over partitions in-
duced by a Chinese Restaurant Process (CRP, Pitman 2002).
Imagine building a partition from the ground up: starting
with a single cluster containing a single object, and adding
objects until all the objects belong to clusters. Under the
CRP, each cluster attracts new members in proportion to its
size. The distribution over clusters for object i, conditioned

on the cluster assignments of objects 1, . . . , i − 1 is
P (zi = a|z1, . . . , zi−1) =(cid:3) na
na > 0

i−1+γ
i−1+γ

γ

a is a new cluster

where na is the number of objects already assigned to clus-
ter a, and γ is a parameter. The distribution on z induced
by the CRP is exchangeable: the order in which objects are
assigned to clusters can be permuted without changing the
probability of the resulting partition. P (z) can therefore be
computed by choosing an arbitrary ordering and multiplying
conditional probabilities as speciﬁed above. Since new ob-
jects can always be assigned to new clusters, the IRM effec-
tively has access to a countably inﬁnite collection of clusters,
hence the ﬁrst part of its name.

A CRP prior on partitions is mathematically convenient,
and consistent with the intuition that the prior should favor
partitions with small numbers of clusters. Yet it is not a uni-
versal solution to the problem of choosing the right number
of clusters. In some settings we may have prior knowledge
that is not captured by the CRP: for instance, we may expect
that the clusters will be roughly equal in size. Even so, the
CRP provides a useful starting point for structure discovery
in novel domains.

Generating relations from clusters
We assume that relations are binary-valued functions, al-
though extensions to frequency data and continuous data are
straightforward. Consider ﬁrst a problem with a single type
T and a single two-place relation R : T × T → {0, 1}.
Type T , for example, could be a collection of people, and
R(i, j) might indicate whether person i likes person j. The
complete generative model for this problem is:

z | γ ∼ CRP(γ)

η(a, b)| β ∼ Beta(β, β)
R(i, j)| z, η ∼ Bernoulli(η(zi, zj)),

(1)

382where a, b ∈ N . The model is represented graphically in
Figure 1b.
Here we assume that an entity’s tendency to participate
in relations is determined entirely by its cluster assignment.
The parameter η(a, b) speciﬁes the probability that a link ex-
ists between any given pair (i, j) where i belongs to cluster
a and j belongs to cluster b. We place symmetric conjugate
priors (with hyperparameter β) on each entry in the η matrix.
To specify the most general version of the IRM, we extend
Equation 1 to relations of arbitrary arity. Consider an m di-
mensional relation R involving n different types. Let dk be
the label of the type that occupies dimension k: for exam-
ple, the three place relation R : T 1 × T 1 × T 2 → {0, 1} has
d1 = d2 = 1, and d3 = 2. As before, the probability that
the relation holds between a group of entities depends only
on the clusters of those entities:
R(i1, . . . , im)|z1, . . . , zn, η ∼ Bernoulli(η(zd1
In settings with multiple relations, we introduce a parameter
matrix ηi for each relation Ri.

i1 , . . . , zdm
im

)).

Inference
Consider again a binary relation R over a single type T .
Since we use conjugate priors on the entries in η, it is simple

to compute P (R|z) =(cid:4) P (R|η, z)p(η)dη:

Beta(m(a, b) + β, ¯m(a, b) + β)

Beta(β, β)

P (R|z) = (cid:2)a,b∈N

where m(a, b) is the number of pairs (i, j) where i ∈ a and
j ∈ b and R(i, j) = 1, ¯m(a, b) is the number of pairs where
R(i, j) = 0, and Beta(·,·) is the Beta function. If some en-
tries in R are missing at random, we can ignore them and
maintain counts m(a, b) and ¯m(a, b) over only the observed
values. Even though η is integrated out, it is simple to re-
cover the relationships between clusters given z. The maxi-
mum a posteriori value of η(a, b) given z is:

m(a, b) + β

¯m(a, b) + m(a, b) + 2β

.

Since we integrate out η,

inference can be carried
out using Markov chain Monte Carlo methods to sam-
ple from the posterior on cluster assignments P (z|R) ∝
P (R|z)P (z) (Jain & Neal 2004), or by searching for the
mode of this distribution. We are interested in discover-
ing the single best representation for each data set men-
tioned in this paper, and we search for the best partition z
by repeatedly running hill climbing from an initial conﬁg-
uration where a single cluster is used for each type. We
also search for the best values of the parameters γ and β us-
ing an exponential prior p(γ) ∝ e−γ and an improper prior
p(β) ∝ β −5
2 .
The search uses proposals that move an object from one
cluster to another, split a cluster, or merge two clusters. The
goal of the IRM can be understood intuitively by represent-
ing the relation R as an adjacency matrix. Our search pro-
cedure tries to shufﬂe the rows and columns of this matrix
so that it assumes a clean block structure like the matrix in

Figure 1a. The same idea applies to relations with more than
two dimensions: Figure 1c shows a ternary relation, and here
the aim is to shufﬂe the dimensions so that the matrix takes
on a 3-dimensional block structure. Figure 1d shows three
relations involving three types. The goal is again to create
matrices with clean block structures, but now the partition
for T 1 must be the same wherever this type appears.

Related work

Statisticians and sociologists have used the stochastic block-
model to discover social roles in network data. This model
relies on a generative process identical to Equation 1, ex-
cept that the zi are drawn from a multinomial distribution
over a ﬁxed, ﬁnite number of clusters (Nowicki & Snijders
2001). Several alternative approaches to relational learning
(e.g. Kubica et al. (2002)) focus on clique structures, where
relational links are expected primarily between members of
the same cluster. An advantage of the blockmodel is that
it also handles other kinds of relational structures — hier-
archies, for example, where members of one cluster tend to
send links to individuals from higher-status clusters.

Recent work in machine learning has extended the intu-
ition behind the blockmodel in several directions. There
are approaches that learn overlapping clusters for a single
type (Wolfe & Jensen 2004) and approaches that handle
multiple relations and types using Probabilistic Relational
Models (Taskar, Segal, & Koller 2001; Getoor et al. 2002).
Existing models often focus on data sets with some speciﬁc
form:
for example, the Group-Topic model (Wang, Mo-
hanty, & McCallum 2005) simultaneously clusters entities
(e.g. people) and attributes associated with links between
those entities (e.g. words). Compared to much of this work,
a distinctive feature of the IRM is its ability to automatically
handle arbitrary collections of relations, each of which may
take any number of arguments. The IRM is a lightweight
framework that can be applied to data sets with qualitatively
different forms: note that a single generic piece of code was
used to analyze all of the data sets in this paper.

Another distinctive feature of the IRM is its ability to
learn increasingly complex representations as more data are
encountered. This ability allows the model to choose a size
vector specifying the number of clusters in each type, and
is particularly important for structure discovery in novel do-
mains, where we may have little or no prior knowledge about
the number of clusters in each type. Other approaches to
choosing the number of clusters are also possible: for ex-
ample, we could learn systems of many different sizes and
choose the best using cross-validation or Bayesian model se-
lection. These alternatives may be practical when there is
only one type, but scale badly as the number of types in-
creases: if there are n types, each point in an n-dimensional
space of size vectors must be separately considered.

Finally, most previous approaches to relational clustering
discover only clusters of objects, and our emphasis on clus-
tering predicates is somewhat unusual. An intelligent system
should attempt to ﬁnd patterns at all levels, and clustering
entities, features and relations is one step towards this goal.

383a)

RI = 0.97

RI = 0.98

RI = 1

β = 0.1

10
8
6
4
2

β = 1

10
8
6
4
2

RI = 0.62

RI = 0.84

RI = 1

RI = 0.91

RI = 0.86

RI = 1

RI = 0.55

RI = 0.61

RI = 0.94

b)

10
8
6
4
2

10
8
6
4
2

2 4 6 8 10

2 4 6 8 10

2 4 6 8 10

2 4 8 16  32

2 4 8 16  32

2 4 8 16  32

S1

S2

S3

S1

S2

S3

Figure 2: Each sub-plot shows the number of clusters recovered by the IRM (y axis) against (a) the true number of clusters or
(b) the number of entities per cluster. In (a), the total number of entities is ﬁxed at 40 and the true dimensionality varies between
2 and 10. In (b), the number of clusters is ﬁxed at 5 and the total number of entities varies between 10 (2 per cluster) and 160
(32 per cluster). The columns represent results for three relational systems described in the text (S1, S2, S3) and the rows show
performance for clean data (top) and noisy data (bottom). Each datapoint is an average across 10 randomly generated data sets
and across all the types in each system, and error bars show standard deviations. RI is the mean adjusted Rand index, which
measures the quality of the clusters discovered.

Synthetic data

We generated synthetic data to explore the IRM’s ability to
infer the number of clusters in each type. We considered
data sets with three different forms. System S1 has two
types T 1 and T 2 and a single binary relation R : T 1×T 2 →
{0, 1}. System S2 uses four types and three binary rela-
tions with domains T 1 × T 2, T 1 × T 3 and T 2 × T 4, and
tests the IRM’s ability to work with multiple types and re-
lations. System S3 is a single ternary relation with domain
T 1 × T 2 × T 3, and tests the IRM’s ability to work with
higher-order relations. For the ﬁrst set of simulations, each
type included 40 objects, and we generated data sets where
the dimensionality d — the true number of clusters in each
type — varied between 2 and 10. For each setting of d the
clusters were approximately equal in size. For each system
and each setting of d, we varied the β parameter (see Equa-
tion 1) used to generate the data. When β is small, each
relation has sub-blocks that are relatively clean, but the data
become noisier as β increases.

The top row of Figure 2a shows that the IRM accurately
recovers the true number of clusters in each type when the
data are clean (β = 0.1). The results are averaged across
all the types in each system: note that the true dimension-
ality is always the same for all the types in any given data
set. The second row suggests that performance remains sta-
ble when β = 1 and the data are noisier. Performance for
the ternary relation (system S3) is still perfect when β = 1,
and experiments with d = 10 showed that performance only
begins to suffer once β reaches 6 and the data are extremely
noisy. This result suggests that nonparametric Bayesian ap-
proaches may be particularly useful for relational problems:
the more that is known about relationships between types,
the easier it should be to discover the number of clusters in
each type.

To assess the quality of the clusters found by the IRM,

we used the adjusted Rand index (Hubert & Arabie 1985).
Compared to a ground-truth partition, a randomly generated
partition has an expected score of zero, and the correct par-
tition has a score of 1. Mean scores are shown above each
plot in Figure 2a, and we see that the IRM accurately recov-
ers both the true number of clusters and the composition of
these clusters.

As more and more entities are observed, the expected
number of clusters in a data set should probably increase.
The IRM has this property: under the CRP prior, the ex-
pected number of clusters grows as O(log(n)). Yet this as-
pect of the prior should not exert too strong an inﬂuence:
even if the number of entities is large, a successful method
should be able to recognize when the true dimensionality
is small. We tested this aspect of our model by generat-
ing data sets where the true dimensionality was always 5,
and the number of entities in each type varied between 10
and 160. Figure 2b shows that when the data are clean, the
IRM successfully recovers the dimensionality regardless of
the number of entities used. For noisier data sets, there is
little statistical evidence for the true clusters when there are
only a handful of entities per cluster, but the model reaches
the right dimensionality and stays there as the number of
entities increases.

Clustering objects and features

Even though the IRM is primarily intended for relational
data, it can also discover structure in object-feature data.
Any object-feature matrix can be viewed as a relation R :
T 1 × T 2 → {0, 1} between a set of objects (T 1) and a
set of features (T 2), and the IRM provides a strategy for
co-clustering, or simultaneously clustering both sets. Since
features can be viewed as unary predicates, co-clustering is
a simple instance of predicate clustering. Of the many ex-
isting approaches to co-clustering, the IRM is closest to the

384killer whale, blue whale, humpback, seal, walrus, dolphin
antelope, horse, giraffe, zebra, deer

O1
O2
O3 monkey, gorilla, chimp
O4
hippo, elephant, rhino
O5
grizzly bear, polar bear

ﬂippers, strain teeth, swims, arctic, coastal, ocean, water
hooves, long neck, horns
hands, bipedal, jungle, tree
bulbous body shape, slow, inactive

F1
F2
F3
F4
F5 meat teeth, eats meat, hunter, ﬁerce
F6

walks, quadrapedal, ground

F1 2

43

5

6

O1

O2
O3
O4
O5

Figure 3: Animal clusters, feature clusters, and a sorted matrix showing the relationships between them. The matrix includes
seven of the twelve animal clusters and all of the feature clusters. Some features refer to habitat (jungle, tree, coastal), and
others are anatomical (bulbous body shape, has teeth for straining food from the water) or behavioral (swims, slow).

Animals (11.5) Medicine (15) Alyawarra (16)
(15)
(5)

0.53
0.47

0.59
0.38

(14)
(9)

IRM 0.50
IMM 0.41

(12)
(5)

Table 1: Adjusted Rand indices comparing the best IRM so-
lution and the best IMM solution to ground truth partitions.
In parentheses are the true number of clusters (top row) and
the number of clusters found by each model (bottom rows).

work of Hofmann & Puzicha (1999).

Figure 3 shows that coherent clusters emerge when the
IRM is applied to a 50 by 85 animal-feature matrix collected
in a psychological experiment (Osherson et al. 1991). Fea-
ture ratings were collected on a scale from 0 to 100, and
we created a binary matrix by thresholding at the global
mean. The feature clusters capture the coherent covariation
of features across the objects in the data set. Importantly,
the model also discovers relationships between feature and
object clusters: for example, aquatic mammals tend to have
aquatic features.

The IRM reduces to the inﬁnite mixture model (IMM,
Rasmussen 2000) if we choose not to cluster the features, as-
suming instead that each feature is generated independently
over the animal partition. Applied to the Osherson data, the
IMM ﬁnds 5 animal clusters and the IRM ﬁnds 12. Any sin-
gle feature may provide weak evidence for the additional
structure discovered by the IRM, but grouping several of
these features allows the IRM to discover a ﬁner-grained
partition.

We asked two human subjects to sort the animals into
groups. One used 13 groups and the other used 10, and
we compared the model solutions to these partitions using
the adjusted Rand index. The IRM matched both human so-
lutions better than the IMM, and Table 1 reports the mean
values achieved.

Learning ontologies

Although there are many kinds of domain theories, ontolo-
gies have played a particularly important role in the devel-
opment of AI. Many researchers have developed ontolo-
gies and used them to support learning and inference, but
the acquisition of ontological knowledge itself has received

less attention. We demonstrate here that the IRM discovers
a simple biomedical ontology given data from the Uniﬁed
Medical Language System (UMLS, McCray 2003).

The UMLS includes a semantic network with 135 con-
cepts and 49 binary predicates. The concepts are high-level
concepts like ‘Disease or Syndrome’, ‘Diagnostic Proce-
dure’, and ‘Mammal.’ The predicates include verbs like
complicates, affects and causes. We applied the IRM to the
ternary relation R : T 1 × T 1 × T 2 → {0, 1}, where T 1
is the set of concepts and T 2 is the set of binary predicates
(see Figure 1c). We have already seen that features (unary
predicates) can be clustered, and here we see that predicates
of higher orders can also be clustered. Our general philos-
ophy is that every type is potentially a candidate for clus-
tering, although there may be problem-speciﬁc reasons why
we choose not to cluster some of them.

Figure 4 shows some of the clusters that emerge when we
cluster both concepts and predicates. 14 concept clusters
and 21 predicate clusters are found in total. We assessed
the quality of the concept clusters using a 15 cluster parti-
tion created by domain experts (McCray et al. 2001). The
expert-designed partition includes clusters labeled ‘Living
Things’, ‘Chemicals and Drugs’ and ‘Disorders’ that match
some of the clusters shown in Figure 4. Again, the IRM
discovers not just clusters, but relationships between these
clusters. By computing maximum a posteriori values of
η(a, b), we identify the pairs of clusters (a, b) that are most
strongly linked, and the predicates that link them. Some of
the strongest relationships tell us that biological functions
affect organisms, that chemicals cause diseases, and that bi-
ologically active substances complicate diseases.

If we are interested only in discovering concept clusters,
the IMM can be applied to a ﬂattened version of the rela-
tional data. Suppose that a is an element of T 1, and we wish
to ﬂatten the ternary relation R : T 1 × T 1 × T 2 → {0, 1}.
The features of a correspond to all values of R(a, x1, x2)
where x1 ∈ T 1 and x2 ∈ T 2 and all values of R(x1, a, x2).
Any relational system can be similarly converted into an ob-
ject feature matrix involving just one of its component di-
mensions. Table 1 suggests that the IRM solution for the
relational data matches the expert partition somewhat better
than the best solution for the IMM on the ﬂattened data.

385a)

Concept clusters

1.Organisms 2.Chemicals 3.Biological functions 4.Bio-active substances

Predicate clusters

affects

diagnoses
indicates
prevents

treats

analyzes

assesses effect of

measures

carries out

exhibits
performs

Alga

Amino Acid
Amphibian Carbohydrate

Animal

Archaeon
Bacterium

Bird

Chemical
Eicosanoid

Isotope
Steroid

Biological function

Cell function

Genetic function
Mental process

Antibiotic
Enzyme

Poisonous substance

Hormone

Molecular function

Pharmacologic substance

5.Diseases

Cell dysfunction

Disease

Mental dysfunction
Neoplastic process
Pathologic function

Physiological function

Vitamin

Expt. model of disease

affects

1 2

interacts with

causes

complicates

analyzes

assesses effect of

b)

1
2
3
4
5

Figure 4: (a) Predicate and concept clusters found using the UMLS data. We have labeled the concept clusters and shown
only six members of each. (b) Adjacency matrices for six predicates, where the rows and columns are sorted according to the
14 cluster concept partition. The ﬁrst ﬁve clusters are the clusters shown in (a): we see, for instance, that chemicals affect
biological functions and that organisms interact with organisms.

Learning kinship systems

Australian tribes are renowned among anthropologists for
the complex relational structure of their kinship systems.
We focus here on the Alyawarra, a tribe from Central Aus-
tralia (Denham 1973). To a ﬁrst approximation, Alyawarra
kinship is captured by the Kariera system shown in Fig-
ure 5a. The tribe has four kinship sections, and Figure 5
shows how the sections of individuals are related to the kin-
ship sections of their parents. For example, every member of
section 1 has a mother in section 4 and a father in section 3.
We show here that the IRM discovers some of the properties
of this system.

Denham asked 104 tribe members to provide kinship
terms for each other. Figure 5c shows six of the 26 differ-
ent kinship terms recorded: for each term, the (i, j) cell in
the corresponding matrix indicates whether person i used
that term to refer to person j. The four kinship sections are
clearly visible in the ﬁrst two matrices. Adiadya refers to a
classiﬁcatory younger brother or sister: that is, to a younger
person in one’s own section, even if he or she is not a bi-
ological sibling. Umbaidya is used by female speakers to
refer to a classiﬁcatory son or daughter, and by male speak-
ers to refer to the child of a classiﬁcatory sister. We see
from the matrix that women in section 1 have children in
section 4, and vice versa. Anowadya refers to a preferred
marriage partner. The eight rough blocks indicate that men
must marry women, that members of section 1 are expected
to marry members of section 2, and that members of section
3 are expected to marry members of section 4.

We applied the IRM to the ternary relation R : T 1 × T 1 ×
T 2 → {0, 1} where T 1 is the set of 104 people and T 2 is
the set of kinship terms (see Figure 1c). Denham recorded
demographic information for each of his informants, and we
created a “ground truth” partition by assigning each person
to one of 16 clusters depending on gender, kinship section,

and a binary age feature (older than 45). The best solution
for the IRM uses 15 clusters, and Figure 5b shows that these
clusters are relatively clean with respect to the dimensions
of age, gender, and kinship section.

As for the biomedical data, we can apply the IMM to a
ﬂattened version of the data if we are interested only in clus-
tering the people. Table 1 suggests that the IRM solution
captures the true structure substantially better than the IMM.

Clustering with multiple types and relations

The problem of theory discovery is especially interesting
when there are multiple types and relations. Our ﬁnal ex-
ample shows that the IRM discovers structure in a political
data set including 14 countries, 54 binary predicates repre-
senting interactions between countries, and 90 features of
the countries (Rummel 1999). To create a binary data set,
we thresholded each continuous variable at its mean and
used one-of-n coding for the categorical variables. The re-
sulting data set has three types: countries (T 1), interaction
predicates (T 2) and country features (T 3), and two relations:
R1 : T 1 × T 1 × T 2 → {0, 1}, and R2 : T 1 × T 3 → {0, 1}.
The IRM analyzes R1 and R2 simultaneously and discovers
partitions of all three types.

The model partitions the 14 countries into the ﬁve groups
shown in Figure 6a. The data come from 1965 and there are
two groups from the Western bloc, a group from the commu-
nist bloc, and two groups from the so-called “neutral bloc.”
The model discovers 18 clusters of interaction predicates,
and Figures 6b through 6i represent some of the clusters that
emerge. Note that the countries in each matrix are sorted
according to the order in 6a. The clusters in 6b and 6e repre-
sent positive and negative interactions, and the cluster in 6i
conﬁrms that the country partition is well explained by bloc
membership.

The IRM divides the country features into the ﬁve groups

386a)

b)

mother

mother

4

2

father

1

3

father

c)

n
o

i
t
c
e
S

x
e
S

e
g
A

1
2
3
4
M
F
0−7
8−14
15−29
30−44
45−59
60−99

Adiaya

Umbaidya

Anowadya

Aleriya

Abmarliya

Amburniya

Figure 5: (a) The Kariera kinship system. Each person belongs to one of four kinship sections, and the section of any person
predicts the sections of his or her parents. (b) Composition of the 15 clusters found by the IRM. The six age categories were
chosen by Denham, and are based in part on Alyawarra terms for age groupings (Denham 1973). (c) Data for six Alyawarra
kinship terms. The 104 individuals are sorted by the clusters shown in (b).

shown in Figure 6a. The ﬁrst group includes a single fea-
ture — ‘non-communist’ — which captures one of the most
important aspects of this Cold-War data set. The second and
third clusters include features that are characteristic of West-
ern and communist countries respectively, and the fourth
cluster includes features that are often true of developing
countries but never true of the UK and the USA. 1

Conclusion

We presented the Inﬁnite Relational Model, a framework for
simultaneously clustering one or more sets of entities and
discovering the relationships between clusters that are pos-
sible or likely. Our framework supports the discovery of
simple theories that specify the kinds of entities in a domain
and the relations that hold between them. These theories
capture important aspects of semantic knowledge, but we are
ultimately interested in more expressive representations that
can capture a greater proportion of human knowledge. Log-
ical representations, for example, will probably be needed to
fully capture knowledge about kinship.

There are many ways to formalize the notion of a rela-
tional system, and it is useful to arrange these formalizations
along a spectrum from simple to complex. We considered
relatively simple systems, which allowed us to give a princi-
pled account of how these systems might be learned. Meth-
ods for learning logical theories (Muggleton & De Raedt
1994; Kok & Domingos 2005) consider relational systems
at the more complex end of the spectrum, and it may be
worth thinking about hybrid approaches where the clusters
discovered by the IRM serve as primitives in more complex

1Figure 6 reﬂects some inconsistencies in the original data: for
instance, 6i suggests that Israel is part of the neutral bloc, but the
second labeled feature in 6a suggests that Israel is part of the West-
ern bloc.

theories. Adding representational power while preserving
learnability is an imposing challenge, but we hope that ap-
proaches like ours will help bring algorithms for relational
learning closer to theories of the organization and develop-
ment of human semantic knowledge.
Acknowledgments Supported in part by AFOSR MURI
contract FA9550-05-1-0321, the William Albert Asbjornsen
memorial fellowship (CK) and the Paul E. Newton Chair
(JBT). We thank Steven Sloman for providing the animal-
feature data and Woodrow Denham for providing the
Alyawarra data.

References

Antoniak, C. 1974. Mixtures of Dirichlet processes with
applications to Bayesian nonparametric problems. The An-
nals of Statistics 2:1152–1174.
Davis, E. 1990. Representations of commonsense knowl-
edge.
Denham, W. 1973. The detection of patterns in Alyawarra
nonverbal behavior.
Ph.D. Dissertation, University of
Washington.
Getoor, L.; Friedman, N.; Koller, D.; and Taskar, B. 2002.
Learning probabilistic models of link structure. Journal of
Machine Learning Research 3:679–707.
Hofmann, T., and Puzicha, J. 1999. Latent class models
for collaborative ﬁltering. In Proc. 16th International Joint
Conference on Artiﬁcial Intelligence.
Hubert, L., and Arabie, P. 1985. Comparing partitions.
Journal of Classiﬁcation 2:193–218.
Jain, S., and Neal, R. M. 2004. A split-merge Markov chain
Monte Carlo procedure for the Dirichlet Process mixture
model. Journal of Computational and Graphical Statistics
13:158–182.

387a)

Brazil
Netherlands
UK
USA
Burma
Indonesia
Jordan
Egypt
India
Israel
China
Cuba
Poland
USSR

b)

military
alliance

t
v
o
g

c
o
b

l

i

t
s
n
u
m
m
o
c
n
o
n

 
l

 

c
o
b

l

a
n
o

i
t

u

t
i
t
s
n
o
c

n
r
e

t
s
e
w

s
n
o

i
t
c
e
e

l

 

e
e
r
f

i

i

 
t
s
n
u
m
m
o
c

s
t
s
n
u
m
m
o
c

n
a
i
r
a

t
i
l

a

t

o

t

t
s
i
t
i
l

e

s
n
o

i
t
c
e
e

l

i

p
h
s
r
o
s
n
e
c
 

h
g
h

i

 

e
e
r
f
 

y
c
a
r
e

o
n

t
i
l
l
i

l

i

e
c
n
e
o
v
 
c
i
t
s
e
m
o
d

$

 

s
k
o
o
b

 
s
u
o
g

i

i
l

e
r

n
o

i
t

a
c
u
d
e

 
t
v
o
g

P
N
G
/
s
t
r
o
p
x
e

l

e
n
n
o
s
r
e
p

 
y
r
a

t
i
l
i

m

s
d
o
o
g

i

 

e
n
r
o
b
a
e
s

s
s
i
r
c
 
t
v
o
g

t

n
e
u
q
n

c
o
b

l

i
l

 
l

 

e
d
N
U

a
r
t

u
e
n

s
n
o

n
o

i
t

i
t

l

u
o
v
e
r
 
t
v
o
g

i

a
n
s
s
a
s
s
a

s
n
o
g

i

i
l

e
r
 

m
u
n

y
r
a

t
i
l
i

i

 

m
g
n
n
e
v
r
e

t

n

i

d
e
m
u
s
n
o
c
 
y
g
r
e
n
e

i

p
h
s
r
o
s
n
e
c
 

e
m
o
s

 

S
U
m
o
r
f
 
r
a

f

s
e
g
r
u
p

l
l

a

f

i

n
a
r

s
t

h

t

g
n
e

l
 

n
e
d
u

t
s
 

d
a
o
r
l
i

a
r

i

n
g
e
r
o

f

y
r
t

n
u
o
c
 
f

o

 

e
g
a

s
O
G
N
w
a

 

l

s
e
g
a
u
g
n
a

l
 

m
u
n

n
e
k
a

t
 

$

 

d
a

i

t

n
e
s
 
l
i

 

a
m
n
g
e
r
o

i

s
r
e
k
r
o
w
e
a
m
e

 

l

f

f

t

e
d

i

 

n

i
 

n
e

i

t

o
r
p

n
e
k
a

t
 

y
t
i
s
n
e
d

i

 

d
a
S
U

 
.

n
p
o
p

a
e
r
a

 

d
n
a

l

s
t

n
e
m
t
s
e
v
n

s
O
G
N
 
s
t
r
a

y
h
c
r
a
n
o
m

i

h

t

g
n
e

l
 

d
a
o
r

s
t

n
a
r
g
m
e

i

l

e
b
a
r
a

t

i

e
d

 

n

i
 
s
e
i
r
o
a
c

l

l

d
e
y
o
p
m
e
n
u

e
n
o
h
p
e
e

l

t

n
o

i
t

l

a
u
p
o
p

$

 

e
s
n
e

f

e
d

s
c

i
l

o
h

s
t
s
e

t

t

o
r
p

a
C

s
t

a
e
r
h

t

P
N
G

sends

tourists to

exports
books to

exports to

treaties

conferences

membership

of IGOs

c)

d)

joint

joint

membership

of NGOs

e)

negative
behavior

negative

communications accusations

protests

f)

book

translations

g)

h)

economic

aid

emigration

i)

common bloc
membership

Figure 6: (a) Country clusters and feature clusters for the political data. Every second feature is labelled, and gray entries
indicate missing data. (b) – (i) Representatives of eight predicate clusters found by the IRM. The countries in each matrix are
ordered according to the partition in (a).

Kok, S., and Domingos, P. 2005. Learning the structure of
Markov logic networks. In Proc. 22nd International Con-
ference on Machine Learning.
Kubica, J.; Moore, A.; Schneider, J.; and Yang, Y. 2002.
Stochastic link and group detection. In Proc. 17th National
Conference on Artiﬁcial Intelligence.
McCray, A. T.; Burgun, A.; and Bodenreider, O. 2001.
Aggregating UMLS semantic types for reducing concep-
tual complexity. In Medinfo, volume 10, 216–20.
McCray, A. T. 2003. An upper level ontology for the
biomedical domain. Comparative and Functional Ge-
nomics 4:80–84.
Minsky, M. 1975. A framework for representing knowl-
edge. In Winston, P., ed., The psychology of computer vi-
sion.
Inductive logic
Muggleton, S., and De Raedt, L. 1994.
programming: theory and methods. Journal of Logic Pro-
gramming 19,20:629–679.
Nowicki, K., and Snijders, T. A. B. 2001. Estimation and
prediction for stochastic blockstructures. Journal of the
American Statistical Association 96:1077–1087.

Osherson, D. N.; Stern, J.; Wilkie, O.; Stob, M.; and Smith,
E. E. 1991. Default probability. Cognitive Science 15:251–
269.
Pitman, J. 2002. Combinatorial stochastic processes. Notes
for Saint Flour Summer School.
Rasmussen, C. 2000. The inﬁnite Gaussian mixture model.
In Advances in Neural Information Processing Systems 12.
Rummel, R. J. 1999. Dimensionality of Nations project:
attributes of nations and behavior of nation dyads, 1950–
1965. ICPSR data ﬁle.
Taskar, B.; Segal, E.; and Koller, D. 2001. Probabilistic
classiﬁcation and clustering in relational data. In Proc. 18th
International Joint Conference on Artiﬁcial Intelligence,
volume 15.
Wang, X.; Mohanty, N.; and McCallum, A. 2005. Group
and topic discovery from relations and text. In Proc. KDD
workshop on link discovery.
Wolfe, A. P., and Jensen, D. 2004. Playing multiple roles:
discovering overlapping roles in social networks. In Proc.
ICML workshop on statistical relational learning and its
connections to other ﬁelds.

388