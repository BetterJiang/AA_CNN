Bayesian Inference for PCFGs via Markov Chain Monte Carlo

Citation for published version:
Johnson, M, Griffiths, T & Goldwater, S 2007, 'Bayesian Inference for PCFGs via Markov Chain Monte
Carlo'. in Human Language Technologies 2007: The Conference of the North American Chapter of the
Association for Computational Linguistics; Proceedings of the Main Conference. Association for
Computational Linguistics, Rochester, New York, pp. 139-146.

Link:
Link to publication record in Edinburgh Research Explorer

Document Version:
Publisher's PDF, also known as Version of record

Published In:
Human Language Technologies 2007: The Conference of the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main Conference

General rights
Copyright for the publications made accessible via the Edinburgh Research Explorer is retained by the author(s)
and / or other copyright owners and it is a condition of accessing these publications that users recognise and
abide by the legal requirements associated with these rights.

Take down policy
The University of Edinburgh has made every reasonable effort to ensure that Edinburgh Research Explorer
content complies with UK legislation. If you believe that the public display of this file breaches copyright please
contact openaccess@ed.ac.uk providing details, and we will remove access to the work immediately and
investigate your claim.

Download date: 03. Oct. 2016

     Edinburgh Research Explorer                                      Rochester, NY, April 2007. c(cid:13)2007 Association for Computational Linguistics

Proceedings of NAACL HLT 2007, pages 139–146,

139

BayesianInferenceforPCFGsviaMarkovchainMonteCarloMarkJohnsonCognitiveandLinguisticSciencesBrownUniversityMarkJohnson@brown.eduThomasL.GrifﬁthsDepartmentofPsychologyUniversityofCalifornia,BerkeleyTomGriffiths@berkeley.eduSharonGoldwaterDepartmentofLinguisticsStanfordUniversitysgwater@stanford.eduAbstractThispaperpresentstwoMarkovchainMonteCarlo(MCMC)algorithmsforBayesianinferenceofprobabilisticcon-textfreegrammars(PCFGs)fromter-minalstrings,providinganalternativetomaximum-likelihoodestimationusingtheInside-Outsidealgorithm.Weillus-tratethesemethodsbyestimatingasparsegrammardescribingthemorphologyoftheBantulanguageSesotho,demonstrat-ingthatwithsuitablepriorsBayesiantechniquescaninferlinguisticstructureinsituationswheremaximumlikelihoodmethodssuchastheInside-Outsidealgo-rithmonlyproduceatrivialgrammar.1IntroductionThestandardmethodsforinferringtheparametersofprobabilisticmodelsincomputationallinguisticsarebasedontheprincipleofmaximum-likelihoodesti-mation;forexample,theparametersofProbabilisticContext-FreeGrammars(PCFGs)aretypicallyes-timatedfromstringsofterminalsusingtheInside-Outside(IO)algorithm,aninstanceoftheEx-pectationMaximization(EM)procedure(LariandYoung,1990).However,muchrecentworkinma-chinelearningandstatisticshasturnedawayfrommaximum-likelihoodinfavorofBayesianmethods,andthereisincreasinginterestinBayesianmethodsincomputationallinguisticsaswell(Finkeletal.,2006).ThispaperpresentstwoMarkovchainMonteCarlo(MCMC)algorithmsforinferringPCFGsandtheirparsesfromstringsalone.ThesecanbeviewedasBayesianalternativestotheIOalgorithm.ThegoalofBayesianinferenceistocomputeadistributionoverplausibleparametervalues.This“posterior”distributionisobtainedbycombiningthelikelihoodwitha“prior”distributionP(θ)overpa-rametervaluesθ.InthecaseofPCFGinferenceθisthevectorofruleprobabilities,andthepriormightassertapreferenceforasparsegrammar(seebe-low).TheposteriorprobabilityofeachvalueofθisgivenbyBayes’rule:P(θ|D)∝P(D|θ)P(θ).(1)InprincipleEquation1deﬁnestheposteriorprob-abilityofanyvalueofθ,butcomputingthismaynotbetractableanalyticallyornumerically.ForthisreasonavarietyofmethodshavebeendevelopedtosupportapproximateBayesianinference.OneofthemostpopularmethodsisMarkovchainMonteCarlo(MCMC),inwhichaMarkovchainisusedtosam-plefromtheposteriordistribution.ThispaperpresentstwonewMCMCalgorithmsforinferringtheposteriordistributionoverparsesandruleprobabilitiesgivenacorpusofstrings.Theﬁrstalgorithmisacomponent-wiseGibbssamplerwhichisverysimilarinspirittotheEMalgo-rithm,drawingparsetreesconditionedonthecur-rentparametervaluesandthensamplingtheparam-etersconditionedonthecurrentsetofparsetrees.Thesecondalgorithmisacomponent-wiseHastingssamplerthat“collapses”theprobabilisticmodel,in-tegratingovertheruleprobabilitiesofthePCFG,withthegoalofspeedingconvergence.Bothalgo-140

rithmsuseanefﬁcientdynamicprogrammingtech-niquetosampleparsetrees.Giventheirusefulnessinotherdisciplines,webelievethatBayesianmethodslikethesearelikelytobeofgeneralutilityincomputationallinguis-ticsaswell.Asasimpleillustrativeexample,weusethesemethodstoinfermorphologicalparsesforverbsfromSesotho,asouthernBantulanguagewithagglutinatingmorphology.OurresultsillustratethatBayesianinferenceusingapriorthatfavorssparsitycanproducelinguisticallyreasonableanalysesinsit-uationsinwhichEMdoesnot.Therestofthispaperisstructuredasfollows.Thenextsectionintroducesthebackgroundforourpaper,summarizingthekeyideasbehindPCFGs,Bayesianinference,andMCMC.Section3intro-ducesourﬁrstMCMCalgorithm,aGibbssamplerforPCFGs.Section4describesanalgorithmforsamplingtreesfromthedistributionovertreesde-ﬁnedbyaPCFG.Section5showshowtointegrateouttheruleweightparametersθinaPCFG,allow-ingustosampledirectlyfromtheposteriordistribu-tionoverparsesforacorpusofstrings.Finally,Sec-tion6illustratesthesemethodsinlearningSesothomorphology.2Background2.1Probabilisticcontext-freegrammarsLetG=(T,N,S,R)beaContext-FreeGrammarinChomskynormalformwithnouselessproduc-tions,whereTisaﬁnitesetofterminalsymbols,Nisaﬁnitesetofnonterminalsymbols(disjointfromT),S∈Nisadistinguishednonterminalcalledthestartsymbol,andRisaﬁnitesetofproductionsoftheformA→BCorA→w,whereA,B,C∈Nandw∈T.Inwhatfollowsweuseβasavariablerangingover(N×N)∪T.AProbabilisticContext-FreeGrammar(G,θ)isapairconsistingofacontext-freegrammarGandareal-valuedvectorθoflength|R|indexedbypro-ductions,whereθA→βistheproductionprobabilityassociatedwiththeproductionA→β∈R.WerequirethatθA→β≥0andthatforallnonterminalsA∈N,PA→β∈RθA→β=1.APCFG(G,θ)deﬁnesaprobabilitydistributionovertreestasfollows:PG(t|θ)=Yr∈Rθfr(t)rwheretisgeneratedbyGandfr(t)isthenumberoftimestheproductionr=A→β∈Risusedinthederivationoft.IfGdoesnotgeneratetletPG(t|θ)=0.Theyieldy(t)ofaparsetreetisthesequenceofterminalslabelingitsleaves.Theprobabilityofastringw∈T+ofterminalsisthesumoftheprobabilityofalltreeswithyieldw,i.e.:PG(w|θ)=Xt:y(t)=wPG(t|θ).2.2BayesianinferenceforPCFGsGivenacorpusofstringsw=(w1,...,wn),whereeachwiisastringofterminalsgeneratedbyaknownCFGG,wewouldliketobeabletoinferthepro-ductionprobabilitiesθthatbestdescribethatcorpus.Takingwtobeourdata,wecanapplyBayes’rule(Equation1)toobtain:P(θ|w)∝PG(w|θ)P(θ),wherePG(w|θ)=nYi=1PG(wi|θ).Usingttodenoteasequenceofparsetreesforw,wecancomputethejointposteriordistributionovertandθ,andthenmarginalizeovert,withP(θ|w)=PtP(t,θ|w).Thejointposteriordistributionontandθisgivenby:P(t,θ|w)∝P(w|t)P(t|θ)P(θ)= nYi=1P(wi|ti)P(ti|θ)!P(θ)withP(wi|ti)=1ify(ti)=wi,and0otherwise.2.3DirichletpriorsTheﬁrststeptowardscomputingtheposteriordis-tributionistodeﬁneaprioronθ.WetakeP(θ)tobeaproductofDirichletdistributions,withonedis-tributionforeachnon-terminalA∈N.ThepriorisparameterizedbyapositiverealvaluedvectorαindexedbyproductionsR,soeachproductionprob-abilityθA→βhasacorrespondingDirichletparam-eterαA→β.LetRAbethesetofproductionsinR141

withleft-handsideA,andletθAandαArefertothecomponentsubvectorsofθandαrespectivelyindexedbyproductionsinRA.TheDirichletpriorPD(θ|α)is:PD(θ|α)=YA∈NPD(θA|αA),wherePD(θA|αA)=1C(αA)Yr∈RAθαr−1randC(αA)=Qr∈RAΓ(αr)Γ(Pr∈RAαr)(2)whereΓisthegeneralizedfactorialfunctionandC(α)isanormalizationconstantthatdoesnotde-pendonθA.Dirichletpriorsareusefulbecausetheyarecon-jugatetothedistributionovertreesdeﬁnedbyaPCFG.Thismeansthattheposteriordistributiononθgivenasetofparsetrees,P(θ|t,α),isalsoaDirichletdistribution.ApplyingBayes’rule,PG(θ|t,α)∝PG(t|θ)PD(θ|α)∝ Yr∈Rθfr(t)r! Yr∈Rθαr−1r!=Yr∈Rθfr(t)+αr−1rwhichisaDirichletdistributionwithparametersf(t)+α,wheref(t)isthevectorofproductioncountsintindexedbyr∈R.Wecanthuswrite:PG(θ|t,α)=PD(θ|f(t)+α)whichmakesitclearthattheproductioncountscom-binedirectlywiththeparametersoftheprior.2.4MarkovchainMonteCarloHavingdeﬁnedaprioronθ,theposteriordistribu-tionovertandθisfullydeterminedbyacorpusw.Unfortunately,computingtheposteriorprobabil-ityofevenasinglechoiceoftandθisintractable,asevaluatingthenormalizingconstantforthisdis-tributionrequiressummingoverallpossibleparsesfortheentirecorpusandallsetsofproductionprob-abilities.Nonetheless,itispossibletodeﬁneal-gorithmsthatsamplefromthisdistributionusingMarkovchainMonteCarlo(MCMC).MCMCalgorithmsconstructaMarkovchainwhosestatess∈Saretheobjectswewishtosam-ple.ThestatespaceSistypicallyastronomicallylarge—inourcase,thestatespaceincludesallpos-sibleparsesoftheentiretrainingcorpusw—andthetransitionprobabilitiesP(s′|s)arespeciﬁedviaaschemeguaranteedtoconvergetothedesireddistri-butionπ(s)(inourcase,theposteriordistribution).We“run”theMarkovchain(i.e.,startingininitialstates0,sampleastates1fromP(s′|s0),thensam-plestates2fromP(s′|s1),andsoon),withtheprob-abilitythattheMarkovchainisinaparticularstate,P(si),convergingtoπ(si)asi→∞.Afterthechainhasrunlongenoughforittoap-proachitsstationarydistribution,theexpectationEπ[f]ofanyfunctionf(s)ofthestateswillbeapproximatedbytheaverageofthatfunctionoverthesetofsamplestatesproducedbythealgorithm.Forexample,inourcase,givensamples(ti,θi)fori=1,...,ℓproducedbyanMCMCalgorithm,wecanestimateθasEπ[θ]≈1ℓℓXi=1θiTheremainderofthispaperpresentstwoMCMCalgorithmsforPCFGs.BothalgorithmsproceedbysettingtheinitialstateoftheMarkovchaintoaguessfor(t,θ)andthensamplingsuccessivestatesusingaparticulartransitionmatrix.Thekeydifferencebe-twenthetwoalgorithmsistheformofthetransitionmatrixtheyassume.3AGibbssamplerforP(t,θ|w,α)TheGibbssampler(GemanandGeman,1984)isoneofthesimplestMCMCmethods,inwhichtran-sitionsbetweenstatesoftheMarkovchainresultfromsamplingeachcomponentofthestatecondi-tionedonthecurrentvalueofallothervariables.Inourcase,thismeansalternatingbetweensamplingfromtwodistributions:P(t|θ,w,α)=nYi=1P(ti|wi,θ),andP(θ|t,w,α)=PD(θ|f(t)+α)=YA∈NPD(θA|fA(t)+αA).Thuseverytwostepswegenerateanewsampleoftandθ.Thisalternationbetweenparsingandup-datingθisreminiscentoftheEMalgorithm,with142

tit1tnw1wiwnθAj...θA1...θA|N|αA1......αAjαA|N|............Figure1:ABayesnetrepresentationofdependen-ciesamongthevariablesinaPCFG.theExpectationstepreplacedbysamplingtandtheMaximizationstepreplacedbysamplingθ.ThedependenciesamongvariablesinaPCFGaredepictedgraphicallyinFigure1,whichmakesclearthattheGibbssamplerishighlyparallelizable(justliketheEMalgorithm).Speciﬁcally,theparsestiareindependentgivenθandsocanbesampledinparallelfromthefollowingdistributionasdescribedinthenextsection.PG(ti|wi,θ)=PG(ti|θ)PG(wi|θ)WemakeuseofthefactthattheposteriorisaproductofindependentDirichletdistributionsinor-dertosampleθfromPD(θ|t,α).TheproductionprobabilitiesθAforeachnonterminalA∈NaresampledfromaDirchletdistibutionwithparametersα′A=fA(t)+αA.Thereareseveralmethodsforsamplingθ=(θ1,...,θm)fromaDirichletdistri-butionwithparametersα=(α1,...,αm),withthesimplestbeingsamplingxjfromaGamma(αj)dis-tributionforj=1,...,mandthensettingθj=xj/Pmk=1xk(Gentle,2003).4EfﬁcientlysamplingfromP(t|w,θ)ThissectioncompletesthedescriptionoftheGibbssamplerfor(t,θ)bydescribingadynamicprogram-mingalgorithmforsamplingtreesfromthesetofparsesforastringgeneratedbyaPCFG.Thisal-gorithmappearsfairlywidelyknown:itwasde-scribedbyGoodman(1998)andFinkeletal(2006)andusedbyDingetal(2005),andisverysimi-lartootherdynamicprogrammingalgorithmsforCFGs,soweonlysummarizeithere.Thealgo-rithmconsistsoftwosteps.Theﬁrststepcon-structsastandard“inside”tableorchart,asusedintheInside-OutsidealgorithmforPCFGs(LariandYoung,1990).Thesecondstepinvolvesarecursionfromlargertosmallerstrings,samplingfromtheproductionsthatexpandeachstringandconstruct-ingthecorrespondingtreeinatop-downfashion.Inthissectionwetakewtobeastringofterminalsymbolsw=(w1,...,wn)whereeachwi∈T,anddeﬁnewi,k=(wi+1,...,wk)(i.e.,thesub-stringfromwi+1uptowk).Further,letGA=(T,N,A,R),i.e.,aCFGjustlikeGexceptthatthestartsymbolhasbeenreplacedwithA,so,PGA(t|θ)istheprobabilityofatreetwhoserootnodeisla-beledAandPGA(w|θ)isthesumoftheprobabili-tiesofalltreeswhoserootnodesarelabeledAwithyieldw.TheInsidealgorithmtakesasinputaPCFG(G,θ)andastringw=w0,nandconstructsata-blewithentriespA,i,kforeachA∈Nand0≤i<k≤n,wherepA,i,k=PGA(wi,k|θ),i.e.,theprobabilityofArewritingtowi,k.Thetableentriesarerecursivelydeﬁnedbelow,andcomputedbyenu-meratingallfeasiblei,kandAinanyordersuchthatallsmallervaluesofk−iareenumeratedbeforeanylargervalues.pA,k−1,k=θA→wkpA,i,k=XA→BC∈RXi<j<kθA→BCpB,i,jpC,j,kforallA,B,C∈Nand0≤i<j<k≤n.AttheendoftheInsidealgorithm,PG(w|θ)=pS,0,n.ThesecondstepofthesamplingalgorithmusesthefunctionSAMPLE,whichreturnsasamplefromPG(t|w,θ)giventhePCFG(G,θ)andtheinsidetablepA,i,k.SAMPLEtakesasargumentsanon-terminalA∈Nandapairofstringpositions0≤i<k≤nandreturnsatreedrawnfromPGA(t|wi,k,θ).Itfunctionsinatop-downfashion,selectingtheproductionA→BCtoexpandtheA,andthenrecursivelycallingitselftoexpandBandCrespectively.functionSAMPLE(A,i,k):ifk−i=1thenreturnTREE(A,wk)(j,B,C)=MULTI(A,i,k)returnTREE(A,SAMPLE(B,i,j),SAMPLE(C,j,k))Inthispseudo-code,TREEisafunctionthatcon-structsunaryorbinarytreenodesrespectively,and143

MULTIisafunctionthatproducessamplesfromamultinomialdistributionoverthepossible“split”positionsjandnonterminalchildrenBandC,where:P(j,B,C)=θA→BCPGB(wi,j|θ)PGC(wj,k|θ)PGA(wi,k|θ)5AHastingssamplerforP(t|w,α)TheGibbssamplerdescribedinSection3hasthedisadvantagethateachsampleofθre-quiresreparsingthetrainingcorpusw.Inthissection,wedescribeacomponent-wiseHastingsalgorithmforsamplingdirectlyfromP(t|w,α),marginalizingovertheproduc-tionprobabilitiesθ.TransitionsbetweenstatesareproducedbysamplingparsestifromP(ti|wi,t−i,α)foreachstringwiinturn,wheret−i=(t1,...,ti−1,ti+1,...,tn)isthecurrentsetofparsesforw−i=(w1,...,wi−1,wi+1,...,wn).Marginalizingoverθeffectivelymeansthattheproductionprobabilitiesareupdatedaftereachsentenceisparsed,soitisreasonabletoexpectthatthisalgorithmwillconvergefasterthantheGibbssamplerdescribedearlier.Whilethesamplerdoesnotexplicitlyprovidesamplesofθ,theresultsoutlinedinSections2.3and3canbeusedtosampletheposteriordistributionoverθforeachsampleoftifrequired.LetPD(θ|α)beaDirichletproductprior,andlet∆betheprobabilitysimplexforθ.Thenbyinte-gratingovertheposteriorDirichletdistributionswehave:P(t|α)=Z∆PG(t|θ)PD(θ|α)dθ=YA∈NC(αA+fA(t))C(αA)(3)whereCwasdeﬁnedinEquation2.Becausewearemarginalizingoverθ,thetreestibecomedepen-dentupononeanother.Intuitively,thisisbecausewimayprovideinformationaboutθthatinﬂuenceshowsomeotherstringwjshouldbeparsed.WecanuseEquation3tocomputetheconditionalprobabilityP(ti|t−i,α)asfollows:P(ti|t−i,α)=P(t|α)P(t−i|α)=YA∈NC(αA+fA(t))C(αA+fA(t−i))Now,ifwecouldsamplefromP(ti|wi,t−i,α)=P(wi|ti)P(ti|t−i,α)P(wi|t−i,α)wecouldconstructaGibbssamplerwhosestatesweretheparsetreest.Unfortunately,wedon’tevenknowifthereisanefﬁcientalgorithmforcalculat-ingP(wi|t−i,α),letaloneanefﬁcientsamplingal-gorithmforthisdistribution.Fortunately,thisdifﬁcultyisnotfatal.AHast-ingssamplerforaprobabilitydistributionπ(s)isanMCMCalgorithmthatmakesuseofaproposaldistributionQ(s′|s)fromwhichitdrawssamples,andusesanacceptance/rejectionschemetodeﬁneatransitionkernelwiththedesireddistributionπ(s).Speciﬁcally,giventhecurrentstates,asamples′6=sdrawnfromQ(s′|s)isacceptedasthenextstatewithprobabilityA(s,s′)=min(cid:26)1,π(s′)Q(s|s′)π(s)Q(s′|s)(cid:27)andwithprobability1−A(s,s′)theproposalisre-jectedandthenextstateisthecurrentstates.Weuseacomponent-wiseproposaldistribution,generatingnewproposedvaluesforti,whereiischosenatrandom.OurproposaldistributionistheposteriordistributionoverparsetreesgeneratedbythePCFGwithgrammarGandproductionproba-bilitiesθ′,whereθ′ischosenbasedonthecurrentt−iasdescribedbelow.EachstepofourHastingssamplerisasfollows.First,wecomputeθ′fromt−iasdescribedbelow.Thenwesamplet′ifromP(ti|wi,θ′)usingthealgorithmdescribedinSec-tion4.Finally,weaccepttheproposalt′igiventheoldparsetiforwiwithprobability:A(ti,t′i)=min(1,P(t′i|wi,t−i,α)P(ti|wi,θ′)P(ti|wi,t−i,α)P(t′i|wi,θ′))=min(1,P(t′i|t−i,α)P(ti|wi,θ′)P(ti|t−i,α)P(t′i|wi,θ′))ThekeyadvantageoftheHastingssamplerovertheGibbssamplerhereisthatbecausetheacceptanceprobabilityisaratioofprobabilities,thedifﬁcultto144

computeP(wi|t−i,α)isacommonfactorofboththenumeratoranddenominator,andhenceisnotre-quired.TheP(wi|ti)termalsodisappears,being1forboththenumeratorandthedenominatorsinceourproposaldistributioncanonlygeneratetreesforwhichwiistheyield.Allthatremainsistospecifytheproductionprob-abilitiesθ′oftheproposaldistributionP(t′i|wi,θ′).WhiletheacceptanceruleusedintheHastingsalgorithmensuresthatitproducessamplesfromP(ti|wi,t−i,α)withanyproposalgrammarθ′inwhichallproductionshavenonzeroprobability,thealgorithmismoreefﬁcient(i.e.,fewerproposalsarerejected)iftheproposaldistributionisclosetothedistributiontobesampled.Giventheobservationsaboveaboutthecorre-spondencebetweentermsinP(ti|t−i,α)andtherelativefrequencyofthecorrespondingproductionsint−i,wesetθ′totheexpectedvalueE[θ|t−i,α]ofθgivent−iandαasfollows:θ′r=fr(t−i)+αrPr′∈RAfr′(t−i)+αr′6InferringsparsegrammarsAsstatedintheintroduction,theprimarycontribu-tionofthispaperisintroducingMCMCmethodsforBayesianinferencetocomputationallinguistics.BayesianinferenceusingMCMCisatechniqueofgenericutility,muchlikeExpectation-Maximizationandothergeneralinferencetechniques,andwebe-lievethatitbelongsineverycomputationallinguist’stoolboxalongsidetheseothertechniques.InferringaPCFGtodescribethesyntac-ticstructureofanaturallanguageisanobvi-ousapplicationofgrammarinferencetechniques,anditiswell-knownthatPCFGinferenceus-ingmaximum-likelihoodtechniquessuchastheInside-Outside(IO)algorithm,adynamicprogram-mingExpectation-Maximization(EM)algorithmforPCFGs,performsextremelypoorlyonsuchtasks.WehaveappliedtheBayesianMCMCmethodsde-scribedheretosuchproblemsandobtainresultsverysimilartothoseproducedusingIO.Webe-lievethattheprimaryreasonwhybothIOandtheBayesianmethodsperformsopoorlyonthistaskisthatsimplePCFGsarenotaccuratemodelsofEnglishsyntacticstructure.WeknowthatPCFGsα=(0.1,1.0)α=(0.5,1.0)α=(1.0,1.0)Binomialparameterθ1P(θ1|α)10.80.60.40.20543210Figure2:ADirichletpriorαonabinomialparame-terθ1.Asα1→0,P(θ1|α)isincreasinglyconcen-tratedaround0.thatrepresentonlymajorphrasalcategoriesignoreawidevarietyoflexicalandsyntacticdependen-ciesinnaturallanguage.State-of-the-artsystemsforunsupervisedsyntacticstructureinductionsys-temusesmodelsthatareverydifferenttothesekindsofPCFGs(KleinandManning,2004;SmithandEisner,2006).1Ourgoalinthissectionismodest:weaimmerelytoprovideanillustrativeexampleofBayesianinfer-enceusingMCMC.AsFigure2shows,whentheDirichletpriorparameterαrapproaches0thepriorprobabilityPD(θr|α)becomesincreasinglyconcen-tratedaround0.Thisabilitytobiasthesamplertowardsparsegrammars(i.e.,grammarsinwhichmanyproductionshaveprobabilitiescloseto0)isusefulwhenweattempttoidentifyrelevantproduc-tionsfromamuchlargersetofpossibleproductionsviaparameterestimation.TheBantulanguageSesothoisarichlyagglutina-tivelanguage,inwhichverbsconsistofasequenceofmorphemes,includingoptionalSubjectMarkers(SM),Tense(T),ObjectMarkers(OM),Mood(M)andderivationalafﬁxesaswellastheobligatoryVerbstem(V),asshowninthefollowingexample:reSM-aT-diOM-bonV-aM“Weseethem”1ItiseasytodemonstratethatthepoorqualityofthePCFGmodelsisthecauseoftheseproblemsratherthansearchorotheralgorithmicissues.IfoneinitializeseithertheIOorBayesianestimationprocedureswithtreebankparsesandthenrunstheprocedureusingtheyieldsalone,theaccuracyoftheparsesuni-formlydecreaseswhilethe(posterior)likelihooduniformlyin-creaseswitheachiteration,demonstratingthatimprovingthe(posterior)likelihoodofsuchmodelsdoesnotimproveparseaccuracy.145

WeusedanimplementationoftheHastingssamplerdescribedinSection5toinfermorphologicalparsestforacorpuswof2,283unsegmentedSesothoverbtypesextractedfromtheSesothocorpusavail-ablefromCHILDES(MacWhinneyandSnow,1985;Demuth,1992).Wechosethiscorpusbecausethewordshavebeenmorphologicallysegmentedmanu-ally,makingitpossibleforustoevaluatethemor-phologicalparsesproducedbyoursystem.Wecon-structedaCFGGcontainingthefollowingproduc-tionsWord→VWord→VMWord→SMVMWord→SMTVMWord→SMTOMVMtogetherwithproductionsexpandingthepretermi-nalsSM,T,OM,VandMtoeachofthe16,350dis-tinctsubstringsoccuringanywhereinthecorpus,productingagrammarwith81,755productionsinall.Ineffect,Gencodesthebasicmorphologi-calstructureoftheSesothoverb(ignoringfactorssuchasderivationmorphologyandirregularforms),butprovidesnoinformationaboutthephonologicalidentityofthemorphemes.NotethatGactuallygeneratesaﬁnitelanguage.However,Gparameterizestheprobabilitydistribu-tionoverthestringsitgeneratesinamannerthatwouldbedifﬁculttosuccintlycharacterizeexceptintermsoftheproductionsgivenabove.Moreover,withapproximately20timesmoreproductionsthantrainingstrings,eachstringishighlyambiguousandestimationishighlyunderconstrained,soitprovidesanexcellenttest-bedforsparsepriors.Weestimatedthemorphologicalparsestintwoways.First,werantheIOalgorithminitializedwithauniforminitialestimateθ0forθtoproduceanestimateoftheMLEˆθ,andthencomputedtheViterbiparsesˆtofthetrainingcorpuswwithrespecttothePCFG(G,ˆθ).Second,werantheHastingssamplerinitializedwithtreessampledfrom(G,θ0)withseveraldifferentvaluesfortheparametersoftheprior.Weexperimentedwithanumberoftech-niquesforspeedingconvergenceofboththeIOandHastingsalgorithms,andtwoofthesewereparticu-larlyeffectiveonthisproblem.Annealing,i.e.,us-ingP(t|w)1/τinplaceofP(t|w)whereτisa“tem-perature”parameterstartingaround5andslowlyad-justedtoward1,spedtheconvergenceofbothalgo-rithms.Weranbothalgorithmsforseveralthousanditerationsoverthecorpus,andbothseemedtocon-vergefairlyquicklyonceτwassetto1.“Jittering”theinitialestimateofθusedintheIOalgorithmalsospeditsconvergence.TheIOalgorithmconvergestoasolutionwhereθWord→V=1,andeverystringw∈wisanalysedasasinglemorphemeV.(Infact,inthisgrammarP(wi|θ)istheempiricalprobabilityofwi,anditiseasytoprovethatthisθistheMLE).ThesamplestproducedbytheHastingsalgo-rithmdependontheparametersoftheDirichletprior.Wesetαrtoasinglevalueαforallpro-ductionsr.Wefoundthatforα>10−2thesam-plesproducedbytheHastingsalgorithmwerethesametrivialanalysesasthoseproducedbytheIOalgorithm,butasαwasreducedbelowthistbe-gantoexhibitnontrivialstructure.Weevaluatedthequalityofthesegmentationsinthemorpholog-icalanalysestintermsofunlabeledprecision,re-call,f-scoreandexactmatch(thefractionofwordscorrectlysegmentedintomorphemes;weignoredmorphemelabelsbecausethemanualmorphologicalanalysescontainmanymorphemelabelsthatwedidnotincludeinG).Figure3containsaplotofhowthesequantitiesvarywithα;obtaininganf-scoreof0.75andanexactwordmatchaccuracyof0.54atα=10−5(thecorrespondingvaluesfortheMLEˆθareboth0).Notethatweobtainedgoodresultsasαwasvariedoverseveralordersofmagnitude,sotheactualvalueofαisnotcritical.Thusinthisappli-cationtheabilitytoprefersparsegrammarsenablesustoﬁndlinguisticallymeaningfulanalyses.ThisabilitytoﬁndlinguisticallymeaningfulstructureisrelativelyrareinourexperiencewithunsupervisedPCFGinduction.WealsoexperimentedwithaversionofIOmodi-ﬁedtoperformBayesianMAPestimation,wheretheMaximizationstepoftheIOprocedureisreplacedwithBayesianinferenceusingaDirichletprior,i.e.,wheretheruleprobabilitiesθ(k)atiterationkarees-timatedusing:θ(k)r∝max(0,E[fr|w,θ(k−1)]+α−1).ClearlysuchanapproachisverycloselyrelatedtotheBayesianprocedurespresentedinthisarticle,146

ExactRecallPrecisionF-scoreDirichletpriorparameterαr10.011e-041e-061e-081e-1010.750.50.250Figure3:AccuracyofmorphologicalsegmentationsofSesothoverbsproposedbytheHastingsalgo-rithmsasafunctionofDirichletpriorparameterα.F-score,precisionandrecallareunlabeledmor-phemescores,whileExactisthefractionofwordscorrectlysegmented.andinsomecircumstancesthismaybeausefulestimator.However,inourexperimentswiththeSesothodataabovewefoundthatforthesmallval-uesofαnecessarytoobtainasparsesolution,theexpectedrulecountE[fr]formanyrulesrwaslessthan1−α.Thusonthenextiterationθr=0,result-ingintherebeingnoparsewhatsoeverformanyofthestringsinthetrainingdata.VariationalBayesiantechniquesofferasystematicwayofdealingwiththeseproblems,butweleavethisforfurtherwork.7ConclusionThispaperhasdescribedbasicalgorithmsforper-formingBayesianinferenceoverPCFGsgiventer-minalstrings.WepresentedtwoMarkovchainMonteCarloalgorithms(aGibbsandaHastingssamplingalgorithm)forsamplingfromtheposteriordistributionoverparsetreesgivenacorpusoftheiryieldsandaDirichletproductpriorovertheproduc-tionprobabilities.Asacomponentofthesealgo-rithmswedescribedanefﬁcientdynamicprogram-mingalgorithmforsamplingtreesfromaPCFGwhichisusefulinitsownright.Weusedthesesamplingalgorithmstoinfermorphologicalanaly-sesofSesothoverbsgiventheirstrings(ataskonwhichthestandardMaximumLikelihoodestimatorreturnsatrivialandlinguisticallyuninterestingso-lution),achieving0.75unlabeledmorphemef-scoreand0.54exactwordmatchaccuracy.ThusthisisoneofthefewcasesweareawareofinwhichaPCFGestimationprocedurereturnslinguisticallymeaningfulstructure.WeattributethistotheabilityoftheBayesianpriortoprefersparsegrammars.Weexpectthatthesealgorithmswillbeofinter-esttothecomputationallinguisticscommunitybothbecauseaBayesianapproachtoPCFGestimationismoreﬂexiblethantheMaximumLikelihoodmeth-odsthatcurrentlydominatetheﬁeld(c.f.,theuseofapriorasabiastowardssparsesolutions),andbecausethesetechniquesprovideessentialbuildingblocksformorecomplexmodels.ReferencesKatherineDemuth.1992.AcquisitionofSesotho.InDanSlobin,editor,TheCross-LinguisticStudyofLanguageAc-quisition,volume3,pages557–638.LawrenceErlbaumAs-sociates,Hillsdale,N.J.YeDing,ChiYuChan,andCharlesE.Lawrence.2005.RNAsecondarystructurepredictionbycentroidsinaBoltzmannweightedensemble.RNA,11:1157–1166.JennyRoseFinkel,ChristopherD.Manning,andAndrewY.Ng.2006.Solvingtheproblemofcascadingerrors:ApproximateBayesianinferenceforlinguisticannotationpipelines.InProceedingsofthe2006ConferenceonEmpir-icalMethodsinNaturalLanguageProcessing,pages618–626,Sydney,Australia.AssociationforComputationalLin-guistics.StuartGemanandDonaldGeman.1984.Stochasticrelaxation,Gibbsdistributions,andtheBayesianrestorationofimages.IEEETransactionsonPatternAnalysisandMachineIntelli-gence,6:721–741.JamesE.Gentle.2003.RandomnumbergenerationandMonteCarlomethods.Springer,NewYork,2ndedition.JoshuaGoodman.1998.Parsinginside-out.Ph.D.thesis,HarvardUniversity.availablefromhttp://research.microsoft.com/˜joshuago/.DanKleinandChrisManning.2004.Corpus-basedinduc-tionofsyntacticstructure:Modelsofdependencyandcon-stituency.InProceedingsofthe42ndAnnualMeetingoftheAssociationforComputationalLinguistics,pages478–485.K.LariandS.J.Young.1990.TheestimationofStochasticContext-FreeGrammarsusingtheInside-Outsidealgorithm.ComputerSpeechandLanguage,4(35-56).BrianMacWhinneyandCatherineSnow.1985.Thechildlan-guagedataexchangesystem.JournalofChildLanguage,12:271–296.NoahA.SmithandJasonEisner.2006.Annealingstructuralbiasinmultilingualweightedgrammarinduction.InPro-ceedingsofthe21stInternationalConferenceonComputa-tionalLinguisticsand44thAnnualMeetingoftheAssocia-tionforComputationalLinguistics,pages569–576,Sydney,Australia.AssociationforComputationalLinguistics.