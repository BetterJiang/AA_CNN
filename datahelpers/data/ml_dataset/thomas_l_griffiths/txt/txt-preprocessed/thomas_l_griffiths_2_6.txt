ContextualDependenciesinUnsupervisedWordSegmentationSharonGoldwaterandThomasL.GrifthsandMarkJohnsonDepartmentofCognitiveandLinguisticSciencesBrownUniversityProvidence,RI02912{SharonGoldwater,TomGriffiths,MarkJohnson}@brown.eduAbstractDevelopingbettermethodsforsegment-ingcontinuoustextintowordsisimpor-tantforimprovingtheprocessingofAsianlanguages,andmayshedlightonhowhu-manslearntosegmentspeech.Wepro-posetwonewBayesianwordsegmenta-tionmethodsthatassumeunigramandbi-grammodelsofworddependenciesre-spectively.Thebigrammodelgreatlyout-performstheunigrammodel(andpreviousprobabilisticmodels),demonstratingtheimportanceofsuchdependenciesforwordsegmentation.Wealsoshowthatpreviousprobabilisticmodelsrelycruciallyonsub-optimalsearchprocedures.1IntroductionWordsegmentation,i.e.,discoveringwordbound-ariesincontinuoustextorspeech,isofinterestforbothpracticalandtheoreticalreasons.Itistherststepofprocessingorthographieswithoutexplicitwordboundaries,suchasChinese.Itisalsooneofthekeyproblemsthathumanlanguagelearnersmustsolveastheyarelearninglanguage.Manypreviousmethodsforunsupervisedwordsegmentationarebasedontheobservationthattransitionsbetweenunits(characters,phonemes,orsyllables)withinwordsaregenerallymorepre-dictablethantransitionsacrosswordboundaries.Statisticsthathavebeenproposedformeasuringthesedifferencesincludesuccessorfrequency(Harris,1954),transitionalprobabilities(Saf-franetal.,1996),mutualinformation(Sunetal.,Thisworkwaspartiallysupportedbythefollowinggrants:NIH1R01-MH60922,NIHRO1-DC000314,NSFIGERT-DGE-9870676,andtheDARPACALOproject.1998),accessorvariety(Fengetal.,2004),andboundaryentropy(CohenandAdams,2001).Whilemethodsbasedonlocalstatisticsarequitesuccessful,herewefocusonapproachesbasedonexplicitprobabilisticmodels.Formulat-inganexplicitprobabilisticmodelpermitsustocleanlyseparateassumptionsabouttheinputandpropertiesoflikelysegmentationsfromdetailsofalgorithmsusedtondsuchsolutions.Speci-cally,thispaperdemonstratestheimportanceofcontextualdependenciesforwordsegmentationbycomparingtwoprobabilisticmodelsthatdif-feronlyinthattherstassumesthattheproba-bilityofawordisindependentofitslocalcontext,whilethesecondincorporatesbigramdependen-ciesbetweenadjacentwords.Thealgorithmsweusetosearchforlikelysegmentationsdodiffer,butsolongasthesegmentationstheyproduceareclosetooptimalwecanbecondentthatanydif-ferencesinthesegmentationsreectdifferencesintheprobabilisticmodels,i.e.,inthekindsofde-pendenciesbetweenwords.Wearenotthersttoproposeexplicitprob-abilisticmodelsofwordsegmentation.TwosuccessfulwordsegmentationsystemsbasedonexplicitprobabilisticmodelsarethoseofBrent(1999)andVenkataraman(2001).BrentsModel-BasedDynamicProgramming(MBDP)systemas-sumesaunigramworddistribution.Venkatara-manusesstandardunigram,bigram,andtrigramlanguagemodelsinthreeversionsofhissystem,whichwerefertoasn-gramSegmentation(NGS).Despitetheirratherdifferentgenerativestructure,theMBDPandNGSsegmentationaccuraciesareverysimilar.Moreover,thesegmentationaccuracyoftheNGSunigram,bigram,andtrigrammod-elshardlydiffer,suggestingthatcontextualdepen-denciesareirrelevanttowordsegmentation.How-674

ever,thesegmentationsproducedbyboththesemethodsdependcruciallyonpropertiesofthesearchprocedurestheyemploy.Weshowthisbyexhibitingforeachmodelasegmentationthatislessaccuratebutmoreprobableunderthatmodel.Inthispaper,wepresentanalternativeframe-workforwordsegmentationbasedontheDirich-letprocess,adistributionusedinnonparametricBayesianstatistics.Thisframeworkallowsustodevelopextensiblemodelsthatareamenabletostandardinferenceprocedures.Wepresenttwosuchmodelsincorporatingunigramandbigramworddependencies,respectively.WeuseGibbssamplingtosamplefromtheposteriordistributionofpossiblesegmentationsunderthesemodels.Theplanofthepaperisasfollows.Inthenextsection,wedescribeMBDPandNGSindetail.InSection3wepresenttheunigramversionofourownmodel,theGibbssamplingprocedureweuseforinference,andexperimentalresults.Section4extendsthatmodeltoincorporatebigramdepen-dencies,andSection5concludesthepaper.2NGSandMBDPTheNGSandMBDPsystemsaresimilarinsomeways:botharedesignedtondutterancebound-ariesinacorpusofphonemicallytranscribedut-terances,withknownutteranceboundaries.Bothalsouseapproximateonlinesearchprocedures,choosingandxingasegmentationforeachutter-ancebeforemovingontothenext.Inthissection,wefocusontheverydifferentprobabilisticmod-elsunderlyingthetwosystems.WeshowthattheoptimalsolutionundertheNGSmodelistheun-segmentedcorpus,andsuggestthatthisproblemstemsfromthefactthatthemodelassumesauni-formprioroverhypotheses.WethenpresenttheMBDPmodel,whichusesanon-uniformpriorbutisdifculttoextendbeyondtheunigramcase.2.1NGSNGSassumesthateachutteranceisgeneratedin-dependentlyviaastandardn-grammodel.Forsimplicity,wewilldiscusstheunigramversionofthemodelhere,althoughourargumentisequallyapplicabletothebigramandtrigramversions.TheunigrammodelgeneratesanutteranceuaccordingtothegrammarinFigure1,soP(u)=p$(1p$)n1nYj=1P(wj)(1)1p$UWUp$UWP(w)WwwFigure1:TheunigramNGSgrammar.whereuconsistsofthewordsw1...wnandp$istheprobabilityoftheutteranceboundarymarker$.Thismodelcanbeusedtondthehighestprob-abilitysegmentationhypothesishgiventhedatadbyusingBayesrule:P(h|d)P(d|h)P(h)NGSassumesauniformpriorP(h)overhypothe-ses,soitsgoalistondthesolutionthatmaxi-mizesthelikelihoodP(d|h).Usingthismodel,NGSsapproximatesearchtechniquedeliverscompetitiveresults.However,thetruemaximumlikelihoodsolutionisnotcom-petitive,sinceitcontainsnoutterance-internalwordboundaries.Toseewhynot,considerthesolutioninwhichp$=1andeachutteranceisasingleword,withprobabilityequaltotheempir-icalprobabilityofthatutterance.Anyotherso-lutionwillmatchtheempiricaldistributionofthedatalesswell.Inparticular,asolutionwithad-ditionalwordboundariesmusthave1p$>0,whichmeansitwastesprobabilitymassmodelingunseendata(whichcannowbegeneratedbycon-catenatingobservedutterancestogether).Intuitively,theNGSmodelconsiderstheunseg-mentedsolutiontobeoptimalbecauseitranksallhypothesesequallyprobableapriori.Weknow,however,thathypothesesthatmemorizetheinputdataareunlikelytogeneralizetounseendata,andarethereforepoorsolutions.Topreventmemo-rization,wecouldrestrictourhypothesisspacetomodelswithfewerparametersthanthenumberofutterancesinthedata.Amoregeneralandmathe-maticallysatisfactorysolutionistoassumeanon-uniformprior,assigninghigherprobabilitytohy-potheseswithfewerparameters.ThisisinfacttheroutetakenbyBrentinhisMBDPmodel,asweshallseeinthefollowingsection.2.2MBDPMBDPassumesacorpusofutterancesisgener-atedasasingleprobabilisticeventwithfoursteps:1.GenerateL,thenumberoflexicaltypes.2.Generateaphonemicrepresentationforeachtype(excepttheutteranceboundarytype,$).675

3.Generateatokenfrequencyforeachtype.4.Generateanorderingforthesetoftokens.Inanaldeterministicstep,theorderedtokensareconcatenatedtocreateanunsegmentedcor-pus.Thismeansthatcertainsegmentedcorporawillproducetheobserveddatawithprobability1,andallotherswillproduceitwithprobability0.Theposteriorprobabilityofasegmentationgiventhedataisthusproportionaltoitspriorprobabilityunderthegenerativemodel,andthebestsegmen-tationisthatwiththehighestpriorprobability.TherearetwoimportantpointstonoteabouttheMBDPmodel.First,thedistributionoverLassignshigherprobabilitytomodelswithfewerlexicalitems.Wehavearguedthatthisisneces-sarytoavoidmemorization,andindeedtheunseg-mentedcorpusisnottheoptimalsolutionunderthismodel,aswewillshowinSection3.Second,thefactorizationintofourseparatestepsmakesittheoreticallypossibletomodifyeachstepin-dependentlyinordertoinvestigatetheeffectsofthevariousmodelingassumptions.However,themathematicalstatementofthemodelandtheap-proximationsnecessaryforthesearchproceduremakeitunclearhowtomodifythemodelinanyinterestingway.Inparticular,thefourthstepusesauniformdistribution,whichcreatesaunigramconstraintthatcannoteasilybechanged.Sinceourresearchaimstoinvestigatetheeffectsofdifferentmodelingassumptionsonlexicalacquisition,wedevelopinthefollowingsectionsafarmoreex-iblemodelthatalsoincorporatesapreferenceforsparsesolutions.3UnigramModel3.1TheDirichletProcessModelOurgoalisamodeloflanguagethatpreferssparsesolutions,allowsindependentmodicationofcomponents,andisamenabletostandardsearchprocedures.WeachievethisgoalbybasingourmodelontheDirichletprocess(DP),adistributionusedinnonparametricBayesianstatistics.Ourun-igrammodelofwordfrequenciesisdenedaswi|GGG|0,P0DP(0,P0)wheretheconcentrationparameter0andthebasedistributionP0areparametersofthemodel.EachwordwiinthecorpusisdrawnfromadistributionG,whichconsistsofasetofpos-siblewords(thelexicon)andprobabilitiesasso-ciatedwiththosewords.GisgeneratedfromaDP(0,P0)distribution,withtheitemsinthelexiconbeingsampledfromP0andtheirproba-bilitiesbeingdeterminedby0,whichactsliketheparameterofaninnite-dimensionalsymmet-ricDirichletdistribution.Weprovidesomeintu-itionfortherolesof0andP0below.AlthoughtheDPmodelmakesthedistributionGexplicit,weneverdealwithGdirectly.WetakeaBayesianapproachandintegrateoverallpossiblevaluesofG.Theconditionalprobabil-ityofchoosingtogenerateawordfromaparticu-larlexicalentryisthengivenbyasimplestochas-ticprocessknownastheChineserestaurantpro-cess(CRP)(Aldous,1985).Imaginearestaurantwithaninnitenumberoftables,eachwithinniteseatingcapacity.Customersentertherestaurantandseatthemselves.Letzibethetablechosenbytheithcustomer.ThenP(zi|zi)=n(zi)ki1+00k<K(zi)0i1+0k=K(zi)(2)wherezi=z1...zi1,n(zi)kisthenumberofcustomersalreadysittingattablek,andK(zi)isthetotalnumberofoccupiedtables.Inourmodel,thetablescorrespondto(possiblyrepeated)lexicalentries,havinglabelsgeneratedfromthedistribu-tionP0.Theseatingarrangementthusspeciesadistributionoverwordtokens,witheachcus-tomerrepresentingonetoken.Thismodelisaninstanceofthetwo-stagemodelingframeworkde-scribedbyGoldwateretal.(2006),withP0asthegeneratorandtheCRPastheadaptor.Ourmodelcanbeviewedintuitivelyasacachemodel:eachwordinthecorpusiseitherretrievedfromacacheorgeneratedanew.Summingoverallthetableslabeledwiththesamewordyieldstheprobabilitydistributionfortheithwordgivenpreviouslyobservedwordswi:P(wi|wi)=n(wi)wii1+0+0P0(wi)i1+0(3)wheren(wi)wisthenumberofinstancesofwob-servedinwi.Thersttermistheprobabilityofgeneratingwfromthecache(i.e.,sittingatanoccupiedtable),andthesecondtermistheproba-bilityofgeneratingitanew(sittingatanunoccu-piedtable).Theactualtableassignmentszionlybecomeimportantlater,inthebigrammodel.676

Thereareseveralimportantpointstonoteaboutthismodel.First,theprobabilityofgeneratingaparticularwordfromthecacheincreasesasmoreinstancesofthatwordareobserved.Thisrich-get-richerprocesscreatesapower-lawdistributiononwordfrequencies(Goldwateretal.,2006),thesamesortofdistributionfoundempiricallyinnat-urallanguage.Second,theparameter0canbeusedtocontrolhowsparsethesolutionsfoundbythemodelare.Thisparameterdeterminesthetotalprobabilityofgeneratinganynovelword,aproba-bilitythatdecreasesasmoredataisobserved,butneverdisappears.Finally,theparameterP0canbeusedtoencodeexpectationsaboutthenatureofthelexicon,sinceitdenesaprobabilitydistri-butionacrossdifferentnovelwords.Thefactthatthisdistributionisdenedseparatelyfromthedis-tributiononwordfrequenciesgivesthemodelad-ditionalexibility,sinceeitherdistributioncanbemodiedindependentlyoftheother.Sincethegoalofthispaperistoinvestigatetheroleofcontextinwordsegmentation,wechosethesimplestpossiblemodelforP0,i.e.aunigramphonemedistribution:P0(w)=p#(1p#)n1nYi=1P(mi)(4)wherewordwconsistsofthephonemesm1...mn,andp#istheprobabilityofthewordboundary#.Forsimplicityweusedauniformdistributionoverphonemes,andexperimentedwithdifferentxedvaluesofp#.1Analdetailofourmodelisthedistributiononutterancelengths,whichisgeometric.Thatis,weassumeagrammarsimilartotheoneshowninFigure1,withtheadditionofasymmetricBeta(2)priorovertheprobabilityoftheUproductions,2andthesubstitutionoftheDPforthestandardmultinomialdistributionovertheWproductions.3.2GibbsSamplingHavingdenedourgenerativemodel,weareleftwiththeproblemofinference:wemustdeterminetheposteriordistributionofhypothesesgivenourinputcorpus.Todoso,weuseGibbssampling,astandardMarkovchainMonteCarlomethod(Gilksetal.,1996).Gibbssamplingisanitera-tiveprocedureinwhichvariablesarerepeatedly1Note,however,thatourmodelcouldbeextendedtolearnbothp#andthedistributionoverphonemes.2TheBetadistributionisaDirichletdistributionovertwooutcomes.WUw1=w2.w3UWUWw3w2h1:h2:Figure2:Thetwohypothesesconsideredbytheunigramsampler.Dashedlinesindicatepossibleadditionalstructure.Allrulesexceptthoseinboldarepartofh.sampledfromtheirconditionalposteriordistribu-tiongiventhecurrentvaluesofallothervariablesinthemodel.ThesamplerdenesaMarkovchainwhosestationarydistributionisP(h|d),soafterconvergencesamplesarefromthisdistribution.OurGibbssamplerconsidersasinglepossibleboundarypointatatime,soeachsampleisfromasetoftwohypotheses,h1andh2.Thesehy-pothesescontainallthesameboundariesexceptattheonepositionunderconsideration,whereh2hasaboundaryandh1doesnot.ThestructuresareshowninFigure2.Inordertosampleahypothe-sis,weneedonlycalculatetherelativeprobabili-tiesofh1andh2.Sinceh1andh2arethesameex-ceptforafewrules,thisisstraightforward.Lethbeallofthestructuresharedbythetwohypothe-ses,includingnwords,andletdbetheobserveddata.ThenP(h1|h,d)=P(w1|h,d)=n(h)w1+0P0(w1)n+0(5)wherethesecondlinefollowsfromEquation3andthepropertiesoftheCRP(inparticular,thatitisexchangeable,withtheprobabilityofaseatingcongurationnotdependingontheorderinwhichcustomersarrive(Aldous,1985)).Also,P(h2|h,d)=P(r,w2,w3|h,d)=P(r|h,d)P(w2|h,d)P(w3|w2,h,d)=nr+2n+1+n(h)w2+0P0(w2)n+0n(h)w3+I(w2=w3)+0P0(w3)n+1+0(6)wherenristhenumberofbranchingrulesr=UWUinh,andI(.)isanindicatorfunc-tiontakingonthevalue1whenitsargumentis677

true,and0otherwise.Thenrtermisderivedbyintegratingoverallpossiblevaluesofp$,andnot-ingthatthetotalnumberofUproductionsinhisn+1.Usingtheseequationswecansimplyproceedthroughthedata,samplingeachpotentialbound-arypointinturn.OncetheGibbssamplercon-verges,thesesampleswillbedrawnfromthepos-teriordistributionP(h|d).3.3ExperimentsInourexperiments,weusedthesamecorpusthatNGSandMBDPweretestedon.Thecor-pus,suppliedtousbyBrent,consistsof9790transcribedutterances(33399words)ofchild-directedspeechfromtheBernstein-Ratnercor-pus(Bernstein-Ratner,1987)intheCHILDESdatabase(MacWhinneyandSnow,1985).Theut-teranceshavebeenconvertedtoaphonemicrep-resentationusingaphonemicdictionary,sothateachoccurrenceofawordhasthesamephonemictranscription.Utteranceboundariesaregivenintheinputtothesystem;otherwordboundariesarenot.BecauseourGibbssamplerisslowtoconverge,weusedannealingtospeedinference.Webeganwithatemperatureof=10anddecreasedin10incrementstoanalvalueof1.Atemperatureofcorrespondstoraisingtheprobabilitiesofh1andh2tothepowerof1priortosampling.WeranourGibbssamplerfor20,000iterationsthroughthecorpus(with=1forthenal2000)andevaluatedourresultsonasinglesampleatthatpoint.Wecalculatedprecision(P),recall(R),andF-score(F)onthewordtokensinthecorpus,wherebothboundariesofawordmustbecorrecttocountthewordascorrect.Theinducedlexiconwasalsoscoredforaccuracyusingthesemetrics(LP,LR,LF).RecallthatourDPmodelhasthreeparameters:,p#,and0.Giventhelargenumberofknownutteranceboundaries,weexpectthevalueoftohavelittleeffectonourresults,sowesimplyxed=2forallexperiments.Figure3showstheef-fectsofvaryingofp#and0.3Lowervaluesofp#causelongerwords,whichtendstoimprovere-call(andthusF-score)inthelexicon,butdecreasetokenaccuracy.Highervaluesof0allowmorenovelwords,whichalsoimproveslexiconrecall,3Itisworthnotingthatalltheseparameterscouldbein-ferred.Weleavethisforfuturework.0.10.30.50.70.9505560(a) Varying P(#)  125102050100200500505560(b) Varying 0  LFFLFFFigure3:Word(F)andlexicon(LF)F-score(a)asafunctionofp#,with0=20and(b)asafunctionof0,withp#=.5.butbeginstodegradeprecisionafterapoint.Duetothenegativecorrelationbetweentokenaccuracyandlexiconaccuracy,thereisnosinglebestvalueforeitherp#or0;furtherdiscussionreferstothesolutionforp#=.5,0=20(thoughothersarequalitativelysimilar).InTable1(a),wecomparetheresultsofoursys-temtothoseofMBDPandNGS.4Althoughoursystemhashigherlexiconaccuracythantheoth-ers,itstokenaccuracyismuchworse.Thisresultoccursbecauseoursystemoftenmis-analyzesfre-quentlyoccurringwords.Inparticular,manyofthesewordsoccurincommoncollocationssuchaswhatsthatanddoyou,whichthesysteminter-pretsasasinglewords.Itturnsoutthatafull31%oftheproposedlexiconandnearly30%oftokensconsistofthesekindsoferrors.Uponreection,itisnotsurprisingthatauni-gramlanguagemodelwouldsegmentwordsinthisway.Collocationsviolatetheunigramassumptioninthemodel,sincetheyexhibitstrongword-to-worddependencies.Theonlywaythemodelcancapturethesedependenciesisbyassumingthatthesecollocationsareinfactwordsthemselves.WhydonttheMBDPandNGSunigrammod-elsexhibittheseproblems?WehavealreadyshownthatNGSsresultsareduetoitssearchpro-cedureratherthanitsmodel.ThesameturnsouttobetrueforMBDP.Table2showstheprobabili-4WeusedtheimplementationsofMBDPandNGSavail-ableathttp://www.speech.sri.com/people/anand/toobtainre-sultsforthosesystems.678

(a)PRFLPLRLFNGS67.770.268.952.951.352.0MBDP67.069.468.253.651.352.4DP61.947.653.857.057.557.2(b)PRFLPLRLFNGS76.685.881.060.052.455.9MBDP77.086.181.360.853.056.6DP94.297.195.686.562.272.4Table1:Accuracyofthevarioussystems,withbestscoresinbold.TheunigramversionofNGSisshown.DPresultsarewithp#=.5and0=20.(a)Resultsonthetruecorpus.(b)Resultsonthepermutedcorpus.Seg:TrueNoneMBDPNGSDPNGS204.590.9210.7210.8183.0MBDP208.2321.7217.0218.0189.8DP222.4393.6231.2231.6200.6Table2:Negativelogprobabilities(x1000)un-dereachmodelofthetruesolution,thesolutionwithnoutterance-internalboundaries,andtheso-lutionsfoundbyeachalgorithm.Bestsolutionsundereachmodelarebold.tiesundereachmodelofvarioussegmentationsofthecorpus.Fromthesegures,wecanseethattheMBDPmodelassignshigherprobabilitytothesolutionfoundbyourGibbssamplerthantothesolutionfoundbyBrentsownincrementalsearchalgorithm.Inotherwords,Brentsmodeldoespre-ferthelower-accuracycollocationsolution,buthissearchalgorithminsteadndsahigher-accuracybutlower-probabilitysolution.Weperformedtwoexperimentssuggestingthatourowninferenceproceduredoesnotsufferfromsimilarproblems.First,weinitializedourGibbssamplerinthreedifferentways:withnoutterance-internalboundaries,withaboundaryaftereverycharacter,andwithrandomboundaries.Ourre-sultswerevirtuallythesameregardlessofinitial-ization.Second,wecreatedanarticialcorpusbyrandomlypermutingthewordsinthetruecorpus,leavingtheutterancelengthsthesame.Thear-ticialcorpusadherestotheunigramassumptionofourmodel,soifourinferenceprocedureworkscorrectly,weshouldbeabletocorrectlyidentifythewordsinthepermutedcorpus.Thisisexactlywhatwefound,asshowninTable1(b).Whileallthreemodelsperformbetteronthearticialcor-pus,theimprovementsoftheDPmodelarebyfarthemoststriking.4BigramModel4.1TheHierarchicalDirichletProcessModelTheresultsofourunigramexperimentssuggestedthatwordsegmentationcouldbeimprovedbytakingintoaccountdependenciesbetweenwords.Totestthishypothesis,weextendedourmodeltoincorporatebigramdependenciesusingahi-erarchicalDirichletprocess(HDP)(Tehetal.,2005).Ourapproachissimilartopreviousn-grammodelsusinghierarchicalPitman-Yorprocesses(Goldwateretal.,2006;Teh,2006).TheHDPisappropriateforsituationsinwhichtherearemulti-pledistributionsoversimilarsetsofoutcomes,andthedistributionsarebelievedtobesimilar.Inourcase,wedeneabigrammodelbyassumingeachwordhasadifferentdistributionoverthewordsthatfollowit,butallthesedistributionsarelinked.ThedenitionofourbigramlanguagemodelasanHDPiswi|wi1=w,HwHwwHw|1,GDP(1,G)wG|0,P0DP(0,P0)Thatis,P(wi|wi1=w)isdistributedaccord-ingtoHw,aDPspecictowordw.HwislinkedtotheDPsforallotherwordsbythefactthattheyshareacommonbasedistributionG,whichisgen-eratedfromanotherDP.5Asintheunigrammodel,weneverdealwithHworGdirectly.Byintegratingoverthem,wegetadistributionoverbigramfrequenciesthatcanbeunderstoodintermsoftheCRP.Now,eachwordtypewisassociatedwithitsownrestaurant,whichrepresentsthedistributionoverwordsthatfolloww.Differentrestaurantsarenotcompletelyinde-pendent,however:thelabelsonthetablesintherestaurantsareallchosenfromacommonbasedistribution,whichisanotherCRP.TounderstandtheHDPmodelintermsofagrammar,weconsider$asaspecialwordtype,sothatwirangesover{$}.Afterobservingwi,theHDPgrammarisasshowninFigure4,5ThisHDPformulationisanoversimplication,sinceitdoesnotaccountforutteranceboundariesproperly.Thegrammarformulation(seebelow)does.679

P2(wi|wi,zi)Uwi1WwiUwiwi,wi1{$}P2($|wi,zi)Uwi1$wi11WwiwiwiFigure4:TheHDPgrammarafterobservingwi.withP2(wi|hi)=n(wi1,wi)+1P1(wi|hi)nwi1+1(7)P1(wi|hi)=t+2t+twi+0P0(wi)t+0wit$+2t+wi=$wherehi=(wi,zi);t$,t,andtwiarethetotalnumberoftables(acrossallwords)labeledwith$,non-$,andwi,respectively;t=t$+tisthetotalnumberoftables;andn(wi1,wi)isthenumberofoccurrencesofthebigram(wi1,wi).Wehavesuppressedthesuperscript(wi)nota-tioninallcases.ThebasedistributionsharedbyallbigramsisgivenbyP1,whichcanbeviewedasaunigrambackoffwheretheunigramprobabilitiesarelearnedfromthebigramtablelabels.WecanperforminferenceonthisHDPbigrammodelusingaGibbssamplersimilartoouruni-gramsampler.DetailsappearintheAppendix.4.2ExperimentsWeusedthesamebasicsetupforourexperimentswiththeHDPmodelasweusedfortheDPmodel.Weexperimentedwithdifferentvaluesof0and1,keepingp#=.5throughout.SomeresultsoftheseexperimentsareplottedinFigure5.Withappropriateparametersettings,bothlexiconandtokenaccuracyarehigherthanintheunigrammodel(dramaticallyso,fortokens),andthereisnolongeranegativecorrelationbetweenthetwo.Onlyafewcollocationsremaininthelexicon,andmostlexiconerrorsareonlow-frequencywords.Thebestvaluesof0aremuchlargerthanintheunigrammodel,presumablybecausealluniquewordtypesmustbegeneratedviaP0,butinthebigrammodelthereisanadditionallevelofdis-counting(theunigramprocess)beforereachingP0.Smallervaluesof0leadtofewerwordtypeswithfewercharactersonaverage.Table3comparestheoptimalresultsoftheHDPmodeltotheonlypreviousmodelincorpo-ratingbigramdependencies,NGS.Duetosearch,theperformanceofthebigramNGSmodelisnotmuchdifferentfromthatoftheunigrammodel.In10020050010002000406080(a) Varying 0  FLF5102050100200500406080(b) Varying 1  FLFFigure5:Word(F)andlexicon(LF)F-score(a)asafunctionof0,with1=10and(b)asafunctionof1,with0=1000.PRFLPLRLFNGS68.168.668.354.557.055.7HDP79.474.076.667.958.963.1Table3:Bigramsystemaccuracy,withbestscoresinbold.HDPresultsarewithp#=.5,0=1000,and1=10.contrast,ourHDPmodelperformsfarbetterthanourDPmodel,leadingtothehighestpublishedac-curacyforthiscorpusonbothtokensandlexicalitems.Overall,theseresultsstronglysupportourhypothesisthatmodelingbigramdependenciesisimportantforaccuratewordsegmentation.5ConclusionInthispaper,wehaveintroducedanewmodel-basedapproachtowordsegmentationthatdrawsontechniquesfromBayesianstatistics,andwehavedevelopedmodelsincorporatingunigramandbigramdependencies.TheuseoftheDirichletprocessasthebasisofourapproachyieldssparsesolutionsandallowsustheexibilitytomodifyindividualcomponentsofthemodels.WehavepresentedamethodofinferenceusingGibbssam-pling,whichisguaranteedtoconvergetothepos-teriordistributionoverpossiblesegmentationsofacorpus.Ourapproachtowordsegmentationallowsustoinvestigatequestionsthatcouldnotbeaddressedsatisfactorilyinearlierwork.Wehaveshownthatthesearchalgorithmsusedwithpreviousmodelsofwordsegmentationdonotachievetheirob-680

