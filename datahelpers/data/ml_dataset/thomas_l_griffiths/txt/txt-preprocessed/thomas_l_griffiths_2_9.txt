BayesianInferenceforPCFGsviaMarkovchainMonteCarloMarkJohnsonCognitiveandLinguisticSciencesBrownUniversityMarkJohnson@brown.eduThomasL.GrifthsDepartmentofPsychologyUniversityofCalifornia,BerkeleyTomGriffiths@berkeley.eduSharonGoldwaterDepartmentofLinguisticsStanfordUniversitysgwater@stanford.eduAbstractThispaperpresentstwoMarkovchainMonteCarlo(MCMC)algorithmsforBayesianinferenceofprobabilisticcon-textfreegrammars(PCFGs)fromter-minalstrings,providinganalternativetomaximum-likelihoodestimationusingtheInside-Outsidealgorithm.Weillus-tratethesemethodsbyestimatingasparsegrammardescribingthemorphologyoftheBantulanguageSesotho,demonstrat-ingthatwithsuitablepriorsBayesiantechniquescaninferlinguisticstructureinsituationswheremaximumlikelihoodmethodssuchastheInside-Outsidealgo-rithmonlyproduceatrivialgrammar.1IntroductionThestandardmethodsforinferringtheparametersofprobabilisticmodelsincomputationallinguisticsarebasedontheprincipleofmaximum-likelihoodesti-mation;forexample,theparametersofProbabilisticContext-FreeGrammars(PCFGs)aretypicallyes-timatedfromstringsofterminalsusingtheInside-Outside(IO)algorithm,aninstanceoftheEx-pectationMaximization(EM)procedure(LariandYoung,1990).However,muchrecentworkinma-chinelearningandstatisticshasturnedawayfrommaximum-likelihoodinfavorofBayesianmethods,andthereisincreasinginterestinBayesianmethodsincomputationallinguisticsaswell(Finkeletal.,2006).ThispaperpresentstwoMarkovchainMonteCarlo(MCMC)algorithmsforinferringPCFGsandtheirparsesfromstringsalone.ThesecanbeviewedasBayesianalternativestotheIOalgorithm.ThegoalofBayesianinferenceistocomputeadistributionoverplausibleparametervalues.ThisposteriordistributionisobtainedbycombiningthelikelihoodwithapriordistributionP()overpa-rametervalues.InthecaseofPCFGinferenceisthevectorofruleprobabilities,andthepriormightassertapreferenceforasparsegrammar(seebe-low).TheposteriorprobabilityofeachvalueofisgivenbyBayesrule:P(|D)P(D|)P().(1)InprincipleEquation1denestheposteriorprob-abilityofanyvalueof,butcomputingthismaynotbetractableanalyticallyornumerically.ForthisreasonavarietyofmethodshavebeendevelopedtosupportapproximateBayesianinference.OneofthemostpopularmethodsisMarkovchainMonteCarlo(MCMC),inwhichaMarkovchainisusedtosam-plefromtheposteriordistribution.ThispaperpresentstwonewMCMCalgorithmsforinferringtheposteriordistributionoverparsesandruleprobabilitiesgivenacorpusofstrings.Therstalgorithmisacomponent-wiseGibbssamplerwhichisverysimilarinspirittotheEMalgo-rithm,drawingparsetreesconditionedonthecur-rentparametervaluesandthensamplingtheparam-etersconditionedonthecurrentsetofparsetrees.Thesecondalgorithmisacomponent-wiseHastingssamplerthatcollapsestheprobabilisticmodel,in-tegratingovertheruleprobabilitiesofthePCFG,withthegoalofspeedingconvergence.Bothalgo-140

rithmsuseanefcientdynamicprogrammingtech-niquetosampleparsetrees.Giventheirusefulnessinotherdisciplines,webelievethatBayesianmethodslikethesearelikelytobeofgeneralutilityincomputationallinguis-ticsaswell.Asasimpleillustrativeexample,weusethesemethodstoinfermorphologicalparsesforverbsfromSesotho,asouthernBantulanguagewithagglutinatingmorphology.OurresultsillustratethatBayesianinferenceusingapriorthatfavorssparsitycanproducelinguisticallyreasonableanalysesinsit-uationsinwhichEMdoesnot.Therestofthispaperisstructuredasfollows.Thenextsectionintroducesthebackgroundforourpaper,summarizingthekeyideasbehindPCFGs,Bayesianinference,andMCMC.Section3intro-ducesourrstMCMCalgorithm,aGibbssamplerforPCFGs.Section4describesanalgorithmforsamplingtreesfromthedistributionovertreesde-nedbyaPCFG.Section5showshowtointegrateouttheruleweightparametersinaPCFG,allow-ingustosampledirectlyfromtheposteriordistribu-tionoverparsesforacorpusofstrings.Finally,Sec-tion6illustratesthesemethodsinlearningSesothomorphology.2Background2.1Probabilisticcontext-freegrammarsLetG=(T,N,S,R)beaContext-FreeGrammarinChomskynormalformwithnouselessproduc-tions,whereTisanitesetofterminalsymbols,Nisanitesetofnonterminalsymbols(disjointfromT),SNisadistinguishednonterminalcalledthestartsymbol,andRisanitesetofproductionsoftheformABCorAw,whereA,B,CNandwT.Inwhatfollowsweuseasavariablerangingover(NN)T.AProbabilisticContext-FreeGrammar(G,)isapairconsistingofacontext-freegrammarGandareal-valuedvectoroflength|R|indexedbypro-ductions,whereAistheproductionprobabilityassociatedwiththeproductionAR.WerequirethatA0andthatforallnonterminalsAN,PARA=1.APCFG(G,)denesaprobabilitydistributionovertreestasfollows:PG(t|)=YrRfr(t)rwheretisgeneratedbyGandfr(t)isthenumberoftimestheproductionr=ARisusedinthederivationoft.IfGdoesnotgeneratetletPG(t|)=0.Theyieldy(t)ofaparsetreetisthesequenceofterminalslabelingitsleaves.TheprobabilityofastringwT+ofterminalsisthesumoftheprobabilityofalltreeswithyieldw,i.e.:PG(w|)=Xt:y(t)=wPG(t|).2.2BayesianinferenceforPCFGsGivenacorpusofstringsw=(w1,...,wn),whereeachwiisastringofterminalsgeneratedbyaknownCFGG,wewouldliketobeabletoinferthepro-ductionprobabilitiesthatbestdescribethatcorpus.Takingwtobeourdata,wecanapplyBayesrule(Equation1)toobtain:P(|w)PG(w|)P(),wherePG(w|)=nYi=1PG(wi|).Usingttodenoteasequenceofparsetreesforw,wecancomputethejointposteriordistributionovertand,andthenmarginalizeovert,withP(|w)=PtP(t,|w).Thejointposteriordistributionontandisgivenby:P(t,|w)P(w|t)P(t|)P()= nYi=1P(wi|ti)P(ti|)!P()withP(wi|ti)=1ify(ti)=wi,and0otherwise.2.3DirichletpriorsTherststeptowardscomputingtheposteriordis-tributionistodeneaprioron.WetakeP()tobeaproductofDirichletdistributions,withonedis-tributionforeachnon-terminalAN.ThepriorisparameterizedbyapositiverealvaluedvectorindexedbyproductionsR,soeachproductionprob-abilityAhasacorrespondingDirichletparam-eterA.LetRAbethesetofproductionsinR141

withleft-handsideA,andletAandArefertothecomponentsubvectorsofandrespectivelyindexedbyproductionsinRA.TheDirichletpriorPD(|)is:PD(|)=YANPD(A|A),wherePD(A|A)=1C(A)YrRAr1randC(A)=QrRA(r)(PrRAr)(2)whereisthegeneralizedfactorialfunctionandC()isanormalizationconstantthatdoesnotde-pendonA.Dirichletpriorsareusefulbecausetheyarecon-jugatetothedistributionovertreesdenedbyaPCFG.Thismeansthattheposteriordistributionongivenasetofparsetrees,P(|t,),isalsoaDirichletdistribution.ApplyingBayesrule,PG(|t,)PG(t|)PD(|) YrRfr(t)r! YrRr1r!=YrRfr(t)+r1rwhichisaDirichletdistributionwithparametersf(t)+,wheref(t)isthevectorofproductioncountsintindexedbyrR.Wecanthuswrite:PG(|t,)=PD(|f(t)+)whichmakesitclearthattheproductioncountscom-binedirectlywiththeparametersoftheprior.2.4MarkovchainMonteCarloHavingdenedaprioron,theposteriordistribu-tionovertandisfullydeterminedbyacorpusw.Unfortunately,computingtheposteriorprobabil-ityofevenasinglechoiceoftandisintractable,asevaluatingthenormalizingconstantforthisdis-tributionrequiressummingoverallpossibleparsesfortheentirecorpusandallsetsofproductionprob-abilities.Nonetheless,itispossibletodeneal-gorithmsthatsamplefromthisdistributionusingMarkovchainMonteCarlo(MCMC).MCMCalgorithmsconstructaMarkovchainwhosestatessSaretheobjectswewishtosam-ple.ThestatespaceSistypicallyastronomicallylargeinourcase,thestatespaceincludesallpos-sibleparsesoftheentiretrainingcorpuswandthetransitionprobabilitiesP(s|s)arespeciedviaaschemeguaranteedtoconvergetothedesireddistri-bution(s)(inourcase,theposteriordistribution).WeruntheMarkovchain(i.e.,startingininitialstates0,sampleastates1fromP(s|s0),thensam-plestates2fromP(s|s1),andsoon),withtheprob-abilitythattheMarkovchainisinaparticularstate,P(si),convergingto(si)asi.Afterthechainhasrunlongenoughforittoap-proachitsstationarydistribution,theexpectationE[f]ofanyfunctionf(s)ofthestateswillbeapproximatedbytheaverageofthatfunctionoverthesetofsamplestatesproducedbythealgorithm.Forexample,inourcase,givensamples(ti,i)fori=1,...,producedbyanMCMCalgorithm,wecanestimateasE[]1Xi=1iTheremainderofthispaperpresentstwoMCMCalgorithmsforPCFGs.BothalgorithmsproceedbysettingtheinitialstateoftheMarkovchaintoaguessfor(t,)andthensamplingsuccessivestatesusingaparticulartransitionmatrix.Thekeydifferencebe-twenthetwoalgorithmsistheformofthetransitionmatrixtheyassume.3AGibbssamplerforP(t,|w,)TheGibbssampler(GemanandGeman,1984)isoneofthesimplestMCMCmethods,inwhichtran-sitionsbetweenstatesoftheMarkovchainresultfromsamplingeachcomponentofthestatecondi-tionedonthecurrentvalueofallothervariables.Inourcase,thismeansalternatingbetweensamplingfromtwodistributions:P(t|,w,)=nYi=1P(ti|wi,),andP(|t,w,)=PD(|f(t)+)=YANPD(A|fA(t)+A).Thuseverytwostepswegenerateanewsampleoftand.Thisalternationbetweenparsingandup-datingisreminiscentoftheEMalgorithm,with142

tit1tnw1wiwnAj...A1...A|N|A1......AjA|N|............Figure1:ABayesnetrepresentationofdependen-ciesamongthevariablesinaPCFG.theExpectationstepreplacedbysamplingtandtheMaximizationstepreplacedbysampling.ThedependenciesamongvariablesinaPCFGaredepictedgraphicallyinFigure1,whichmakesclearthattheGibbssamplerishighlyparallelizable(justliketheEMalgorithm).Specically,theparsestiareindependentgivenandsocanbesampledinparallelfromthefollowingdistributionasdescribedinthenextsection.PG(ti|wi,)=PG(ti|)PG(wi|)WemakeuseofthefactthattheposteriorisaproductofindependentDirichletdistributionsinor-dertosamplefromPD(|t,).TheproductionprobabilitiesAforeachnonterminalANaresampledfromaDirchletdistibutionwithparametersA=fA(t)+A.Thereareseveralmethodsforsampling=(1,...,m)fromaDirichletdistri-butionwithparameters=(1,...,m),withthesimplestbeingsamplingxjfromaGamma(j)dis-tributionforj=1,...,mandthensettingj=xj/Pmk=1xk(Gentle,2003).4EfcientlysamplingfromP(t|w,)ThissectioncompletesthedescriptionoftheGibbssamplerfor(t,)bydescribingadynamicprogram-mingalgorithmforsamplingtreesfromthesetofparsesforastringgeneratedbyaPCFG.Thisal-gorithmappearsfairlywidelyknown:itwasde-scribedbyGoodman(1998)andFinkeletal(2006)andusedbyDingetal(2005),andisverysimi-lartootherdynamicprogrammingalgorithmsforCFGs,soweonlysummarizeithere.Thealgo-rithmconsistsoftwosteps.Therststepcon-structsastandardinsidetableorchart,asusedintheInside-OutsidealgorithmforPCFGs(LariandYoung,1990).Thesecondstepinvolvesarecursionfromlargertosmallerstrings,samplingfromtheproductionsthatexpandeachstringandconstruct-ingthecorrespondingtreeinatop-downfashion.Inthissectionwetakewtobeastringofterminalsymbolsw=(w1,...,wn)whereeachwiT,anddenewi,k=(wi+1,...,wk)(i.e.,thesub-stringfromwi+1uptowk).Further,letGA=(T,N,A,R),i.e.,aCFGjustlikeGexceptthatthestartsymbolhasbeenreplacedwithA,so,PGA(t|)istheprobabilityofatreetwhoserootnodeisla-beledAandPGA(w|)isthesumoftheprobabili-tiesofalltreeswhoserootnodesarelabeledAwithyieldw.TheInsidealgorithmtakesasinputaPCFG(G,)andastringw=w0,nandconstructsata-blewithentriespA,i,kforeachANand0i<kn,wherepA,i,k=PGA(wi,k|),i.e.,theprobabilityofArewritingtowi,k.Thetableentriesarerecursivelydenedbelow,andcomputedbyenu-meratingallfeasiblei,kandAinanyordersuchthatallsmallervaluesofkiareenumeratedbeforeanylargervalues.pA,k1,k=AwkpA,i,k=XABCRXi<j<kABCpB,i,jpC,j,kforallA,B,CNand0i<j<kn.AttheendoftheInsidealgorithm,PG(w|)=pS,0,n.ThesecondstepofthesamplingalgorithmusesthefunctionSAMPLE,whichreturnsasamplefromPG(t|w,)giventhePCFG(G,)andtheinsidetablepA,i,k.SAMPLEtakesasargumentsanon-terminalANandapairofstringpositions0i<knandreturnsatreedrawnfromPGA(t|wi,k,).Itfunctionsinatop-downfashion,selectingtheproductionABCtoexpandtheA,andthenrecursivelycallingitselftoexpandBandCrespectively.functionSAMPLE(A,i,k):ifki=1thenreturnTREE(A,wk)(j,B,C)=MULTI(A,i,k)returnTREE(A,SAMPLE(B,i,j),SAMPLE(C,j,k))Inthispseudo-code,TREEisafunctionthatcon-structsunaryorbinarytreenodesrespectively,and143

MULTIisafunctionthatproducessamplesfromamultinomialdistributionoverthepossiblesplitpositionsjandnonterminalchildrenBandC,where:P(j,B,C)=ABCPGB(wi,j|)PGC(wj,k|)PGA(wi,k|)5AHastingssamplerforP(t|w,)TheGibbssamplerdescribedinSection3hasthedisadvantagethateachsampleofre-quiresreparsingthetrainingcorpusw.Inthissection,wedescribeacomponent-wiseHastingsalgorithmforsamplingdirectlyfromP(t|w,),marginalizingovertheproduc-tionprobabilities.TransitionsbetweenstatesareproducedbysamplingparsestifromP(ti|wi,ti,)foreachstringwiinturn,whereti=(t1,...,ti1,ti+1,...,tn)isthecurrentsetofparsesforwi=(w1,...,wi1,wi+1,...,wn).Marginalizingovereffectivelymeansthattheproductionprobabilitiesareupdatedaftereachsentenceisparsed,soitisreasonabletoexpectthatthisalgorithmwillconvergefasterthantheGibbssamplerdescribedearlier.Whilethesamplerdoesnotexplicitlyprovidesamplesof,theresultsoutlinedinSections2.3and3canbeusedtosampletheposteriordistributionoverforeachsampleoftifrequired.LetPD(|)beaDirichletproductprior,andletbetheprobabilitysimplexfor.Thenbyinte-gratingovertheposteriorDirichletdistributionswehave:P(t|)=ZPG(t|)PD(|)d=YANC(A+fA(t))C(A)(3)whereCwasdenedinEquation2.Becausewearemarginalizingover,thetreestibecomedepen-dentupononeanother.Intuitively,thisisbecausewimayprovideinformationaboutthatinuenceshowsomeotherstringwjshouldbeparsed.WecanuseEquation3tocomputetheconditionalprobabilityP(ti|ti,)asfollows:P(ti|ti,)=P(t|)P(ti|)=YANC(A+fA(t))C(A+fA(ti))Now,ifwecouldsamplefromP(ti|wi,ti,)=P(wi|ti)P(ti|ti,)P(wi|ti,)wecouldconstructaGibbssamplerwhosestatesweretheparsetreest.Unfortunately,wedontevenknowifthereisanefcientalgorithmforcalculat-ingP(wi|ti,),letaloneanefcientsamplingal-gorithmforthisdistribution.Fortunately,thisdifcultyisnotfatal.AHast-ingssamplerforaprobabilitydistribution(s)isanMCMCalgorithmthatmakesuseofaproposaldistributionQ(s|s)fromwhichitdrawssamples,andusesanacceptance/rejectionschemetodeneatransitionkernelwiththedesireddistribution(s).Specically,giventhecurrentstates,asamples6=sdrawnfromQ(s|s)isacceptedasthenextstatewithprobabilityA(s,s)=min(cid:26)1,(s)Q(s|s)(s)Q(s|s)(cid:27)andwithprobability1A(s,s)theproposalisre-jectedandthenextstateisthecurrentstates.Weuseacomponent-wiseproposaldistribution,generatingnewproposedvaluesforti,whereiischosenatrandom.OurproposaldistributionistheposteriordistributionoverparsetreesgeneratedbythePCFGwithgrammarGandproductionproba-bilities,whereischosenbasedonthecurrenttiasdescribedbelow.EachstepofourHastingssamplerisasfollows.First,wecomputefromtiasdescribedbelow.ThenwesampletifromP(ti|wi,)usingthealgorithmdescribedinSec-tion4.Finally,weaccepttheproposaltigiventheoldparsetiforwiwithprobability:A(ti,ti)=min(1,P(ti|wi,ti,)P(ti|wi,)P(ti|wi,ti,)P(ti|wi,))=min(1,P(ti|ti,)P(ti|wi,)P(ti|ti,)P(ti|wi,))ThekeyadvantageoftheHastingssamplerovertheGibbssamplerhereisthatbecausetheacceptanceprobabilityisaratioofprobabilities,thedifcultto144

computeP(wi|ti,)isacommonfactorofboththenumeratoranddenominator,andhenceisnotre-quired.TheP(wi|ti)termalsodisappears,being1forboththenumeratorandthedenominatorsinceourproposaldistributioncanonlygeneratetreesforwhichwiistheyield.Allthatremainsistospecifytheproductionprob-abilitiesoftheproposaldistributionP(ti|wi,).WhiletheacceptanceruleusedintheHastingsalgorithmensuresthatitproducessamplesfromP(ti|wi,ti,)withanyproposalgrammarinwhichallproductionshavenonzeroprobability,thealgorithmismoreefcient(i.e.,fewerproposalsarerejected)iftheproposaldistributionisclosetothedistributiontobesampled.Giventheobservationsaboveaboutthecorre-spondencebetweentermsinP(ti|ti,)andtherelativefrequencyofthecorrespondingproductionsinti,wesettotheexpectedvalueE[|ti,]ofgiventiandasfollows:r=fr(ti)+rPrRAfr(ti)+r6InferringsparsegrammarsAsstatedintheintroduction,theprimarycontribu-tionofthispaperisintroducingMCMCmethodsforBayesianinferencetocomputationallinguistics.BayesianinferenceusingMCMCisatechniqueofgenericutility,muchlikeExpectation-Maximizationandothergeneralinferencetechniques,andwebe-lievethatitbelongsineverycomputationallinguiststoolboxalongsidetheseothertechniques.InferringaPCFGtodescribethesyntac-ticstructureofanaturallanguageisanobvi-ousapplicationofgrammarinferencetechniques,anditiswell-knownthatPCFGinferenceus-ingmaximum-likelihoodtechniquessuchastheInside-Outside(IO)algorithm,adynamicprogram-mingExpectation-Maximization(EM)algorithmforPCFGs,performsextremelypoorlyonsuchtasks.WehaveappliedtheBayesianMCMCmethodsde-scribedheretosuchproblemsandobtainresultsverysimilartothoseproducedusingIO.Webe-lievethattheprimaryreasonwhybothIOandtheBayesianmethodsperformsopoorlyonthistaskisthatsimplePCFGsarenotaccuratemodelsofEnglishsyntacticstructure.WeknowthatPCFGs=(0.1,1.0)=(0.5,1.0)=(1.0,1.0)Binomialparameter1P(1|)10.80.60.40.20543210Figure2:ADirichletprioronabinomialparame-ter1.As10,P(1|)isincreasinglyconcen-tratedaround0.thatrepresentonlymajorphrasalcategoriesignoreawidevarietyoflexicalandsyntacticdependen-ciesinnaturallanguage.State-of-the-artsystemsforunsupervisedsyntacticstructureinductionsys-temusesmodelsthatareverydifferenttothesekindsofPCFGs(KleinandManning,2004;SmithandEisner,2006).1Ourgoalinthissectionismodest:weaimmerelytoprovideanillustrativeexampleofBayesianinfer-enceusingMCMC.AsFigure2shows,whentheDirichletpriorparameterrapproaches0thepriorprobabilityPD(r|)becomesincreasinglyconcen-tratedaround0.Thisabilitytobiasthesamplertowardsparsegrammars(i.e.,grammarsinwhichmanyproductionshaveprobabilitiescloseto0)isusefulwhenweattempttoidentifyrelevantproduc-tionsfromamuchlargersetofpossibleproductionsviaparameterestimation.TheBantulanguageSesothoisarichlyagglutina-tivelanguage,inwhichverbsconsistofasequenceofmorphemes,includingoptionalSubjectMarkers(SM),Tense(T),ObjectMarkers(OM),Mood(M)andderivationalafxesaswellastheobligatoryVerbstem(V),asshowninthefollowingexample:reSM-aT-diOM-bonV-aMWeseethem1ItiseasytodemonstratethatthepoorqualityofthePCFGmodelsisthecauseoftheseproblemsratherthansearchorotheralgorithmicissues.IfoneinitializeseithertheIOorBayesianestimationprocedureswithtreebankparsesandthenrunstheprocedureusingtheyieldsalone,theaccuracyoftheparsesuni-formlydecreaseswhilethe(posterior)likelihooduniformlyin-creaseswitheachiteration,demonstratingthatimprovingthe(posterior)likelihoodofsuchmodelsdoesnotimproveparseaccuracy.145

WeusedanimplementationoftheHastingssamplerdescribedinSection5toinfermorphologicalparsestforacorpuswof2,283unsegmentedSesothoverbtypesextractedfromtheSesothocorpusavail-ablefromCHILDES(MacWhinneyandSnow,1985;Demuth,1992).Wechosethiscorpusbecausethewordshavebeenmorphologicallysegmentedmanu-ally,makingitpossibleforustoevaluatethemor-phologicalparsesproducedbyoursystem.Wecon-structedaCFGGcontainingthefollowingproduc-tionsWordVWordVMWordSMVMWordSMTVMWordSMTOMVMtogetherwithproductionsexpandingthepretermi-nalsSM,T,OM,VandMtoeachofthe16,350dis-tinctsubstringsoccuringanywhereinthecorpus,productingagrammarwith81,755productionsinall.Ineffect,Gencodesthebasicmorphologi-calstructureoftheSesothoverb(ignoringfactorssuchasderivationmorphologyandirregularforms),butprovidesnoinformationaboutthephonologicalidentityofthemorphemes.NotethatGactuallygeneratesanitelanguage.However,Gparameterizestheprobabilitydistribu-tionoverthestringsitgeneratesinamannerthatwouldbedifculttosuccintlycharacterizeexceptintermsoftheproductionsgivenabove.Moreover,withapproximately20timesmoreproductionsthantrainingstrings,eachstringishighlyambiguousandestimationishighlyunderconstrained,soitprovidesanexcellenttest-bedforsparsepriors.Weestimatedthemorphologicalparsestintwoways.First,werantheIOalgorithminitializedwithauniforminitialestimate0fortoproduceanestimateoftheMLE,andthencomputedtheViterbiparsestofthetrainingcorpuswwithrespecttothePCFG(G,).Second,werantheHastingssamplerinitializedwithtreessampledfrom(G,0)withseveraldifferentvaluesfortheparametersoftheprior.Weexperimentedwithanumberoftech-niquesforspeedingconvergenceofboththeIOandHastingsalgorithms,andtwoofthesewereparticu-larlyeffectiveonthisproblem.Annealing,i.e.,us-ingP(t|w)1/inplaceofP(t|w)whereisatem-peratureparameterstartingaround5andslowlyad-justedtoward1,spedtheconvergenceofbothalgo-rithms.Weranbothalgorithmsforseveralthousanditerationsoverthecorpus,andbothseemedtocon-vergefairlyquicklyoncewassetto1.JitteringtheinitialestimateofusedintheIOalgorithmalsospeditsconvergence.TheIOalgorithmconvergestoasolutionwhereWordV=1,andeverystringwwisanalysedasasinglemorphemeV.(Infact,inthisgrammarP(wi|)istheempiricalprobabilityofwi,anditiseasytoprovethatthisistheMLE).ThesamplestproducedbytheHastingsalgo-rithmdependontheparametersoftheDirichletprior.Wesetrtoasinglevalueforallpro-ductionsr.Wefoundthatfor>102thesam-plesproducedbytheHastingsalgorithmwerethesametrivialanalysesasthoseproducedbytheIOalgorithm,butaswasreducedbelowthistbe-gantoexhibitnontrivialstructure.Weevaluatedthequalityofthesegmentationsinthemorpholog-icalanalysestintermsofunlabeledprecision,re-call,f-scoreandexactmatch(thefractionofwordscorrectlysegmentedintomorphemes;weignoredmorphemelabelsbecausethemanualmorphologicalanalysescontainmanymorphemelabelsthatwedidnotincludeinG).Figure3containsaplotofhowthesequantitiesvarywith;obtaininganf-scoreof0.75andanexactwordmatchaccuracyof0.54at=105(thecorrespondingvaluesfortheMLEareboth0).Notethatweobtainedgoodresultsaswasvariedoverseveralordersofmagnitude,sotheactualvalueofisnotcritical.Thusinthisappli-cationtheabilitytoprefersparsegrammarsenablesustondlinguisticallymeaningfulanalyses.ThisabilitytondlinguisticallymeaningfulstructureisrelativelyrareinourexperiencewithunsupervisedPCFGinduction.WealsoexperimentedwithaversionofIOmodi-edtoperformBayesianMAPestimation,wheretheMaximizationstepoftheIOprocedureisreplacedwithBayesianinferenceusingaDirichletprior,i.e.,wheretheruleprobabilities(k)atiterationkarees-timatedusing:(k)rmax(0,E[fr|w,(k1)]+1).ClearlysuchanapproachisverycloselyrelatedtotheBayesianprocedurespresentedinthisarticle,146

