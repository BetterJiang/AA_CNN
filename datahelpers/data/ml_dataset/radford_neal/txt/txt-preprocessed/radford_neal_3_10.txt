Abstract

1  INTRODUCTION

belief

exactly

network

probabilities

Conditional
can be calculated
be transformed
number of states
Cowell,
clique
tions grows exponentially

into a "junction
for each clique
et al 1999). For densely

for a Bayesian
only when the network
can
tree" in which the
is manageable
connected

and the time for exact calcula

with the size of the network.

sizes are large,

(see

networks,

methods,

simulation

in particular
Stochastic
such as Gibbs sampling,
chain Monte Carlo methods
of edges in the network,
are not limited
A Gibbs sampling
but suffer from other problems.
to the desired
simulation
invariant
distribu
but the distribution
tion asymptotically,

by the density

converges

after a finite

Markov

lack this monotonicity

through

states,

from the past

networks

when the state space

so some other way of keeping

Propp and Wilson showed how coupling
can be implemented  efficiently
can be partially
with unique maximal and
ordered,
minimal
in a way that is preserved
transitions.
Belief
property,
possible
ble to represent
chain, whose states
chain.
are of interest
ate stages
being imperfect.
state that represents

are sets of states
may contain

a set of chains by a single

of the simulation

chains is needed.

The set of chains

When the summary chain reaches
chain,

state of the original

We show that it is possi
summary

and spurious

track of all

a single

of the original
both those that

a

added at intermedi

chains
due to this representation

to the right distribution

from the past"  samples  from

by ( conceptu

from the correct
unknown error,

for unob

networks

that differs

dependent

. The method of

by a (usually)

from every possible

using Gibbs
a distribution

ex
distribution

Inference
for belief
sampling
produces
served
variables
distribution
since convergence
occurs only asymptotically
"coupling
actly the correct
Gibbs sampling
ally) running
ulations
starting
from a time far enough in the past that all
runs reach the same state at time t = 0. Ex
state is in
plicitly
We
tractable
propose
net
works that uses a compact,
cise, summary of a set of states.
samples
the correct
and requires
step as ordinary
require
needed if chains

only about twice the time per
but it may
steps than would be

for large networks,
a method for layered

however.
noisy-or
but often impre

more simulation

Gibbs sampling,

from exactly

were tracked

exactly.

sim
state

uncertain

tolerance.

be estimated

from that desired

by a generally
in the early part of
thrown away in a burn
the error has dropped
time must

time differs
amount. This error is greatest
the chain,  which
is generally
in phase until it is felt  that
The burn-in
within the desired
usually
the rate of convergence
of the Markov chain is not known theoretically.
possible
or underestimate
from the desired
might greatly
to minimize

and waste computing
that are too far
user
time,

to overestimate
and include
distribution.

states
The conservative

overestimate

It is
time,

the risk of getting

the wrong answer.

the required

because

burn-in

to

a single

are started

this problem

distribution.
Instead

of initialization

from exactly
chain

the method of "cou
the

bias, Propp
exact sampling,
also

of starting
at time t = 0, dependent

To overcome
and Wilson (1997) introduced
known as perfect simulation,  using
pling from the past" to obtain states
desired
in some arbitrary
chains
T < 0, far enough back that all the chains coalesce
a single
comes from the correct
pling can begin at that time with zero bias (ie, with
are still
no systematic
imate due to ordinary  sampling
but this error is
easily

initial state
in every  possible

state by time t =  0. The state at t = 0 then

distribution,
and useful
sam

The final results

controlled.

state at some time

error).

error,

approx

to

considering  every  possible

This method
distribution,

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

257

of all the true chains (as well as the spu

coalescence
rious chains)
will then exactly
whose state at t =  0 comes from exactly
distribution.

will have occurred.
the single

represent

The summary chain

coalesced

chain,

the desired

3  GIBBS SAMPLING

simulation

Stochastic
duced as a way of sampling
tions for belief
networks
method is now commonly

using Markov chains was intro
from conditional

distribu

by Pearl (1987, 1988). The
known as Gibbs sampling.

will depend on the coalescence

. The computa

for conditional
distri

efficiently

in layered

networks,
con

noisy-or
that are siblings

For these networks,
can be performed

belief
in which
are not also directly
the summary chain tran

We apply this idea to inference
butions
variables
nected.
sitions
tion time required
time of the true chains,
any spurious
this overhead
two-layer
on randomly-generated
eases"
show that the summary chain
method works well enough to be of practical
Interestingly,
Gibbs sampling
appears
ficult problems,
for which uncertainty
necessary
burn-in

caused by
showing
that

time would be greatest.

and "symptoms"

plus the overhead

We give examples

can sometimes

to be smallest

networks

chains.

be large. Other tests

for the most dif
the
concerning

of "dis

the relative overhead compared

interest.
to simple

2  NOISY-OR BELIEF NETWORKS

and edges describe

network

acyclic

graph in which

A variable

for these variables

is a directed
random variables

is ex
with an edge pointing
to an
For

A belief
nodes represent
how the joint distribution
pressed.
other variable
each variable
tional
are specified.
P(A1 = a1,,Am = am), are expressed
of the product
conditional

is a "parent"
A with parents

P(A = a I B1 = b1,   

Joint probabilities

of that variable.
B1,    , Bn, the condi

probabilities

of all forward

, Bn = bn)

in terms

for all variables,

probabilities:

II P(A; = a; I values,

Aj, of A;)
aj, for parents,

proba

take on

It applies

influences

distributions
without
for every combination

values
the child vari
= 1) when it is turned on.

explicitly
of values
when variables

scheme is a way of specifying
these con
listing
for the parent

The noisy-OR
ditional
bilities
variables.
and 1. Each parent variable
able to be turned on (value
The degree of influence
on the link from parent
ability
is turned on. (If these weights
is a deterministic
to be on for some other reason with probability
The conditional

the prob
on the child given that the parent

are all one, the scheme
W is also caused

is determined
X; to child

OR-gate.) Variable

probability

by a weight,

of turning

W, giving

pw.

for W to be on is therefore

X;

of 0

c;,

3.1 Sampling using Markov chains

probabilities

Markov chains

by a sequence

probabilities

of
upon making the

Po, for the initial

that x<tl is conditionally
indepen

I x(t)). The joint distribution

of dis
x<o), X(l),   , a marginal
dis
state X(o), and tran
for state x<t+I) to follow state

A Markov chain is specified
crete random variables
tribution,
sition
x<t): P(x<t+I)
x<o), X(l),    is then determined
Markov assumption
dent of x<t-k) for k  >  1 given x(t-l).
Stationary
that do not depend on time, which can be represented
matrix M. The value in the i1th row
with a transition
and /th column of M is the probability
tion to state j given that the system is in state i. The
P(X(t) =  j)) can
state probabilities
Pt, with Pt = p0Mt.
be represented
if 7f = 7f M. An er
A distribution
godic Markov chain has an invariant
that
is reached
distribution

distribution
no matter what the initial

as a row vector
7f  is invariant

asymptotically
Po is, i.e. limt-+oo Pt = 7f.

at time t (i.e.,

have transition

of a transi

The error in the distribution
time t can be measured
between
the invariant
x, this is

of the Markov chain at
by the total variation
at that time, Pt, and

the state distribution

distribution,

7f. For a finite state space

distance

IIPt- 1rll = 2 L IPt(x)-1r(x)l.

1

xEx

Asymptotically,
error = ae-tfc

the error decays exponentially

as

where a and c are constants
chain. (See Rosenthal
If the user has an error tolerance
for Gibbs sampling  should  be

(1995) for further

.)
E, the burn-in
time

specific

to the Markov

discussion

burn in time= -cln(Eja)
the convergence

However,
chain is usually  unknown
c are not known). The conservative
choose as large a burn-in
the chances

a and
the constants
user will try to

behaviour of the Markov
(i.e.,

time as is practical to lessen
of obtaining  erroneous  results.

P(W = 1 I values,

x;, for parents,

X;)
1  -(1 - Pw) II (1 - c;)

i : Xi=l

3.2 Gibbs sampling for belief networks

When the state, X, consists
X1, ... , Xn, a transition

of several
matrix that leaves

variables,
a desired

258

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

variable.

1r invariant

The transition

an update that changes

can be built from a sequence

that change only
matrix M, on the
of values
for

combinations
as M =  B1BzBn,
with Bk
only xk (i.e., en

distribution
of matrices representing transitions
a single
state space of all possible
X1, . . .  , Xn, is written
representing
tries in Bk for transitions
than Xk are zero). In the Gibbs sampling
entries
are the conditional
able Xk taking
values
the distribution

values,
Each such Bk will  leave
and hence so will M.

of the other variables.
1r invariant,

scheme,
the
in Xk alone

in Bk corresponding

that change variables
other

probabilities

given the current

under 1r of the vari

on its various

to changes

transition proba

When other variables

are

network.

for a belief

Gibbs sampling

probabilities

Pearl (1987) derives
bilities
fixed, conditional
depend only on its parents,
dren's
P(VkjV1,,   , Vk-1, Vk+l,   , Vn)
ex:  P(VkjV1,   , vk-d II P(Y;jV1,   , vk,   , V;-d

its children,
of belief

From the definition

for a particular
and its chil

parents.

variable

networks,

j>k

ber of computations.
be used as the starting
that will be free of bias.

The state found in this way may
run

state for a Gibbs sampling

4.1 The idea of coupling from the past

between

These chains

are "coupled",

idea is to run many chains from

from every pos
by introducing
in an attempt

Propp and Wilson's
some time, T < 0, in the past, starting
sible state.
dependencies
to make them coalesce
time t =  0 they  have
it can be said that no matter what state was started
from at time t = T, the same state at time t =  0 results.
If coalescence
cur by time t =  0, then the procedure
further

their transitions,
to the same state by t =  0. If by
into one chain,

at t =  T does not oc

of chains started

all coalesced

back in the past.

then

is repeated from

de

use

states

between

of all chains to

as long as tran

the results
times remain independent.

to the extent that

from the various

made at different

by introducing

numbers for all their subse

state can be encouraged

at the same state henceforth

This causes chains to stay together

beyond this. Such dependencies

the chains started
must be dependent

At a minimum,
initial
two chains arriving
the same pseudo-random
quent transitions.
once they first coalesce.  Coalescence
a single
pendencies
chains do not invalidate
sitions
In this paper, we will consider
sampling,
updated
required
ing updated  according
A single
cient for making this random choice
Dependencies  between
the same such random number for all chains.
chains
same pseudo-random
those  times  that  were

be
to its conditional
number is suffi

are introduced
If the
back in the past, the

numbers as before are used at
previously

in some sequence.
only for setting

at each time step.
by using

the value of the variable

in which the variables

pseudo-random

only systematic
Gibbs

real-valued

are restarted

of the network
are

is therefore

Randomness

from further

visited.

chains

distribution.

from the past procedure

1, lead to coalescence

Propp and Wilson show that if the chain is ergodic,
this coupling
ability
are started
the unique state of the coalesced
tributed
distribution.

at t =  0 once the chains
far in the past, and that

from sufficiently

to the chain's

at t =  0 is dis

according

will, with prob

exactly

chains

equilibrium

does not occur,

it would be ineffi
just one time
back, since the new chain must be run all
to start runs

When coalescence
cient to try again with a chain started
step further
the way to time t =  0. It is more efficient
at times t =  - 1, -2, -4, -8, - 16, until coalescence
finally occurs.
this scheme is not far from optimal,
than four times the total number of simulation
steps

Propp and Wilson (1997) show that
requiring
no more

of Vk, and
parents
of vk' since v is

Here V1, V2,   , Vk_1 are possible
Vj for j >  k are possible
children
ordered  with
parents
can be evaluated
tributions
is not actually
can be omitted.

for a variable

using the specified

When Vj
given its parents.

before  children.

This expression
conditional

dis

a child of Vk, the corresponding
factor

from the distribution

The effect of sampling
known variables
variables
the known variables fixed while
by Gibbs sampling,

by simply keeping
the others

the values
are updated

conditional

on known values

using the above probabilities.

is achieved

for other

of un

of

networks,

if a child of the variable

belief

For noisy-or
being updated
that child can be ignored
variables
tion between

parents.

instantiated

has the value 0, the other parents
- child
in the calculation
informa

to 0 cannot transmit

of

4  EXACT SAMPLING

runs. An er

us
the

simulation

exact sampling

from the past as a way to eliminate

Propp and Wilson (1997) proposed
ing coupling
error from using finite-length
godic Markov chain will reach its equilibrium
tion if it is run for an infinite
amount of time. There
fore, if one were willing
sure that the correct
had been reached.
not necessary
however- that, at least when the state space is finite,
there is a way to find the exact result

Propp and Wilson show that it is

distribution

to wait forever,

of the Markov chain

to wait forever

with a finite num-

to arrive

one could be

at this result,

distribu

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

259

as would be needed if the actual
somehow guessed.

coalescence
time were

4.3 Efficiently tracking chains

distribution.

to time

time t =  0,

before

reaching

By always selecting

chain must be continued

that favour coalescence
the state at time t = 0,

Note that a coalesced
t = 0 to obtain a state from the correct
Usually,  coalescence occurs
but if the state at that time is used, there is a bias
introduced  toward  conditions
of chains.
there is no dependence
occurs.
coalesce,
and start new chains from further
old chains
must be extended
the old pseudo-random
that were already
a bias is introduced
numbers

Also note that when the chains do not all
it is not valid to just throw away these chains
back. Instead,

in time- i.e.,
numbers generated
must be re-used.

on the time that coalescence

allow coalescence.

by a preference

for pseudo-random

that more easily

Otherwise,

backwards

visited

at times

the

track of every chain for every possible

starting

and Wilson (1997) show that coupling

since the size of the state
in the number of variables

in the

infeasible,

can be given a partial

Keeping
state is generally
space is exponential
network.  Propp
from the past can be implemented
states
through
two chains,
states.
tonic chains,
convergence
the past quite attractive

coalescence
of the chain.  This

Furthermore,

from the minimal

Markov chain transitions,

started

they show that for such mono
cannot be much slower

than
makes coupling
from

for such problems.

efficiently
when

by simulating
and maximal

just

order that is preserved

states

network

usually

in a
An al

way of keeping

Belief
cannot be ordered
way that makes the Markov chain monotonic.
track of every chain is there
ternative
that
fore needed.
There is also no known guarantee
time will not be much greater
than
the coalescence
the time required
to
close to the desired
though we know of
no examples

of this occurring.

distribution,

for Gibbs sampling

to converge

of the

chain.

issue,

the chains

networks,

the efficiency

the tracking

of all the chains.  For

The amount of work for each transition

to summarize
are sets of states

a scheme is needed to
noisy-or

To address
simplify
belief
we attempt
with one chain whose states
original
of the summary chain is the same as for two transitions
of the  original  chain,
ables in the network
our method does not track the set of chains
which may slow detection
nal result
Similar  techniques,  applied
random  fields,  have
Huber (1998) and by Haggstrom

to simulation
been independently
by
and Nelander

though the fi
of coalescence,
distribution.

However,
precisely,

are directly  connected.

from the exactly

that no sibling
vari

developed

provided

of Markov

correct

is still

(1999).

of coalescing  further  back

4.2 Using states obtained by exact sampling

from the past procedure

just described
each time with

thus obtaining

the desired

multiple
distribu

is run, it must search
the chains to coalesce.

a number of times,

states

typically

time that allows

time to complete

The coupling
can be performed
variables,
new pseudo-random
from exactly
independent
tion. Each time the procedure
for a starting
The procedure
running
ing amounts
coalesce).
tempts result
all such attempts
period,
in the past are independent
later on. However,
reasonable
minate,
using Gibbs sampling

time, one must declare

which is superior

since the chances

In practice,

in coalescence,

to succeed

requires
amounts of
(i.e., one must go back vary
of time in the past to cause the chains

varying

if any of these independent

it is reasonable
within

to expect
a reasonable

time

to
at

of the failure

to coalesce

if none of the runs coalesce

in a
the results

indeter

to getting

a wrong answer
with the same number of steps.

an or

found using

However,

the correct

continues

distribution

distribution.

run,  which
at times t  2:  0 will all have

from the past may be used to initialize
forward

it is desir
from the past procedure

sev
in order to find a number of initial

A state from the invariant
coupling
dinary Gibbs sampling
from t =  0. The states
exactly
able to run the coupling
eral times,
from the invariant
from each of the chains that follow
tial states
them, since they are completely
other,
from the past. At the same time, the following
on prior
are less valuable
states,
at the much lower
cost of one Markov chain transition.

of each
independent
chains
states

states
and to take samples

but they come at the cost of coupling

but they can be produced

of their dependence

distribution,

are more valuable

than the states

them. These ini

that follow

because

5  EXACT  SAMPLING FOR

NOISY-OR BELIEF NETWORKS

We now show how coalescence
of Markov chains for a layered
work can be determined
chain,
started

of a large number
noisy-or
by simulating
sets of states

approximate
states.

one summary
of chains

in all possible

whose states

initial

belief
net

5.1 Approximating a set of states

belief

The state space S of noisy-or
ables that take the values
set of states
S(?), in which variables
mapping,

0, 1, or ?. The
(3, from such a state to a set of states
of the

networks
a

in S by a single

take the values

state in a state space

has vari

0 or 1. We approximate

space is

original
j3(V(?)) = { V E S  : for all i, 11;,(?) = V;, or 11;,(?) = ?}
That is, j3 selects
able matches the corresponding
? matching either

all the states in S where every vari
in S(?), with

0 or 1. For example,

variable

Not every set of states inS can be represented
by a state in S(?). For instance,
representation

of { 100010 }

there is no exact

exactly

100011
110010

Hence, to avoid losing true states,
will sometimes

have to include

spurious

the approximation

states as well.

delaying

of changing

may be introduced,

so as to approximate

5.2 Approximating a set of chains
We now show how a single summary chain on S(?) can
be simulated
a set of chains on
S, with none of the true chains being lost, though spu
coalescence.
rious chains
transitions
Gibbs sampling chain,
in
Like the original
at a time. For
the summary chain change one variable
being  updated
has some
each state in S, the variable
to a 1, given the
conditional probability
other variables
in that state. Over the set of states that
the state of the summary chain maps to, this condi
will have some maximum and some
tional probability
minimum value, which can be used to determine
the
transition
in s(?), using
the fact that the chains are coupled by using the same
U. We that assume U is uni
pseudo-random
formly distributed
in
the vari
the original
able being updated to 1 if U is less than the conditional
probability
the summary chain can then be determined

as follows:
 If U is less than the minimum probability,

over [0, 1)), and that transitions

of a 1, and to 0 otherwise.

chain are determined

probabilities

of the variable

Transitions

variable,

by setting

in

all the original
Set the variable

then
chains would set the variable
to 1.
in the summary chain to 1 also.

 Similarly,

if U is greater
imum probability,
chain to 0.

then or equal to the max

set the variable

in the summary

 If U is between the minimum and maximum prob

then some of the original

chains  would

but spurious

chains may be intro

done this way do not lose track of any

Transitions
of the true chains,
duced. For example, consider
a transition
mary chain that changes the last variable
(which j3 maps to {100010, 110010}),
that in the original
two states are to states with different
last variable  (e.g.,
The transition
110011).
have to be to the state 1?001?,
states of S, rather than the previous
two spurious

values for the
100010 --t 100010 and 110010 --t
in the summary chain will
which maps to four
Hence

two states.
chains have been introduced.

in the sum
of 1 ?0010
and suppose
from these

the transitions

chains,

just two judiciously
conditional

5.3 Efficient simulation of the summary chain
It is possible
the minimum and maxi
to determine
mum of the conditional
probability  used
in updating
vk, which is P(Vk = 1  I vl, . .. , Vk-1, vk+l, ... , Vn),
by
over all V in j3(S(7)), without exhaustive
search,
chosen two states in
examining
j3(S(?)), for which this
probability
on its minimum and maximum value. Let pa(V;) be
parents of V;, and c(V;) be the children
of V;. The
required
or max
conditional
imized by minimizing
the ratio
P(Vk = 1[pa(Vk)) x  TI  P(l'J [pa(l'J), Vk = 1)
P(Vk=O[pa(Vk))x TI  P(Vj[pa(Vj),Vk=O)

probability
or maximizing

is minimized

VjEc(Vk)

VjEc(Vk)

will take

the appropriate

states are

in detail by Harvey

below, and justified

The rules for selecting
summarized
(1999).
For the minimum probability
V E j3(V(7l) for which:

that Vk = 1, look at the

If l'J is a child or parent of Vk and l'i(?) = ? ,
then l'J = 0.
If Vp is a parent of
l'i(?) = 1, and vPl = ?, then Vp = 1.

some child, l'J, of Vk, and

For the maximum probability
V E j3(V(?)) for which:

that Vk = 1, look at the

If l'J is a child or parent of Vk and l'i(?) = ? ,
then l'J = 1.
If Vp is a parent of some child, l'J, of Vk, and
l'i(?) E {1, ?}, and V?) = ?,then Vp = 0.

260

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

(3(1 ?001 ?)

{ 
} 110010

110011

abilities,
set the variable
To represent
chain to?.

to 0 and some would set it to 1.
in the summary

this, set the variable

not mentioned

above are irrelevant
the conditional  probability

Note that variables
to computing
These rules assume that sibling
rectly connected.

for vk.
variables
are not di
That is, a child does not share a

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

261

effects on the conditional

parent with its parent vk 0 If this were so, the two pos
for the value of that parent could have
sible settings
opposite
probability
ratios
for vk being 0 or 1 given its parents,
and for the child
of vk having its present value given that vk is 0 or
1. For layered
go only from
variables
below, this problem does not arise.

in one layer to those in the layer immediately

in which  edges

networks,

used, the true set of chains will

chains,

at an earlier

at some time T < 0 in

zero probability,

the summary chain may need to be

are set to ? . Since no chains are lost

when this happens.  However,
because

are set to ? , representing
If none of the states of
it is not hard to
then by time t =  0, the

5.4 Time required
The summary chain is started
all variables
the state  where
the set of all possible
states.
the network  have
show that if T is early  enough,
summary chain will have reached a state representing
a single network state - i.e., one in which none of
the variables
in the approximation
have coalesced
of spurious
started
chains were tracked explicitly.
The coalescence
time in the past sufficient
t =  0. If simulation
- T = 1, 2, 4, 8, ... , Propp and Wilson (1997) show that
the expected
is
around 2.89 times the coalescence
simulation
tional probability
the minimum and the maximum. Therefore,
the ex
pected computational
in terms of com
putations similar
is 5. 78 times the expected

two calculations
of condi
at each time step, in order to obtain

total number of time steps simulated

time than would be needed if all

to produce coalescence

time for a simulation

time. Our noisy-or

scheme requires

Gibbs sampling

coalescence
time.

work, measured

is the minimum

to ordinary

updates,

by

in the initial distribu

other than the eigenvalue  associated

corresponding

n

.

of

by Rosen

will have a mag

eigenvalues

There is a (left)

at time n will be Pn = p0M

chain, with the rate
by the magnitude
(i.e.,
is second-largest
with

distribution,
equal to 1. The magnitudes
of the
complex)
are less than 1.

sition matrix M, as reviewed, for example,
eigenvector
thal (1995).
to the invariant
1r, of the Markov chain,
with eigenvalue
other (possibly
If the Markov chain is started
tion Po, the distribution
As n  oo, Pn  1r for an ergodic
being determined
of convergence
the eigenvalue
whose magnitude
the largest
1r, whose value is 1). This eigenvalue
nitude less than one, but the closer it is to one, the
slower will be the convergence
of the Markov chain.
The transition
matrix of a summary chain will have
all the eigenvectors
and eigenvalue
corre
chain, including
to which both
sponding
The
chain is ergodic).
chains converge
and
summary chain will also have some eigenvectors
eigenvalues
with states where some vari
ables have ? as their value, and some of these eigen
values may be larger in magnitude
than the second
chain. Therefore,
largest
the
of the original
summary chain cannot converge
any faster than the
chains it summarizes,
Below, we examine  these
for some simple diagnostic
layer of variables
various "symptoms",
bottom layer. Interest
diseases

networks
"diseases",

by variables
in the
for what

and eigenvalues
in which a top
which can cause

and eigenvalues

the eigenvector

distribution,

and may converge

to the invariant

eigenvectors

of the original

(if the original

represented

on inference

associated

eigenvalue

given certain

are present,

more slowly.

represent

focuses

observed symptoms.

is done  using  start  times  of

6  COALESCENCE TIMES

the convergence

Here we look at the relationship

When the summary chain method is used, the coales
than if chains were tracked
cence time may be greater
explicitly.
between
these two coalescence
times for some small problems
by calculating
summary chains from the eigenvalues
tion matrices.
chain can be bounded in terms of its expected  coa
lescence
general,
indicative

time, and though the reverse
we will here take the convergence
of the chain's

is not true in
rate to be
coalescence
time.

rates of the original
of their transi

rate of the original

The convergence

expected

and

6.1 Transition matrix eigenvalues
The convergence
lated to the magnitudes

rate of an ergodic

of the eigenvalues

of the tran-

Markov chain is re

6.2 Perfectly summarized networks

in certain

A trivial

summarized

of a set of two states.

The transitions
of the chains being coupled from the
types of
past can be perfectly
networks.
example is a one-disease
network
with one or more symptoms that are known. When
there is just one ? variable
in the summary state, it is
an exact representation
Any network with two unknowns can also be perfectly
summarized.
Although it is possible
of the summary chain does not exactly
set of states of the true chains,
summary chain summarizes
variable
variables
tion, starting
are possible,
When the true chains all coalesce,
will therefore
no? values).

correctly,
are missed. (This can be shown by induc

the
each variable
of the
the set of values for that

with the initial
and all summary  variables

the summary chain
it will have

where all values
are set to ? . )

that the state
represent

even if some constraints

show coalescence

as well (i.e.,

situation

between

262

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE

PROCEEDINGS 2000

that are its potential

between
causes. Sections

to be absent does not produce interactions
the diseases
of
be independent,
the network can effectively
with sub
with those of
states whose transitions
other sections.
only of inde
pendent single-disease
sub-networks,
act sampling  will  converge
uations are discovered
at run-time,
the symptoms are instantiated.

ex
for example,
in one iteration.
Such sit
on how
depending

If a large network consists

do not interact

Symptom  apriori  probability:

0.1
Disease apriori probability:
0.0
Noisy-or weight: 1.0

Figure 1: A network with two diseases.

eigenvalues

eigenvectors

eigenvector,

cannot be perfectly

which has non-zero  com

of0/1/?
for
chain. A

less than 1. Its magnitude of

matrix of the original chain.

has an
is greater than that of

and this can lead to worse convergence

to the next largest,
moderately

network transitions
by a chain in which states consist

v'0.8512 + 0.0752
worse convergence
for the
with this network show

example of this is shown in Figure 2.
and associated

6.3 Imperfectly summarized networks
In general,
summarized
variables,
such a summary chain than for the original
moderate
The non-zero
of the summary chain transition
matrix for this net
work are shown in Table 2. Some of these are the
same as for the transition
The additional
ponents for states  with  ?-valued  variables,
eigenvalue
whose magnitude
all the other eigenvalues
.97, compared
0.85, indicates
summary chain. Experiments
that detecting
chain requires
the past, compared to only 17.6 time steps if every
state is tracked
If the probabilities
work are more extreme,
worse than
tracking
work that coalesces
explicitly,
ordinary
long time is needed (on average)
detected
mary chain's
pared to 0.352 for the original
apriori
symptom probability
set of chains to coalesce
quickly
explanations
the diseases
of the
network- an effect that is lost when sets of states are
approximately

summarized
net
the summary chain is much
every state. Figure 3 shows a net
quickly

to be
when using the summary chain. This sum
is .996, com

and hence must also converge  quickly
Gibbs sampling

when
is done, but for which a very
for coalescence

coalescence
starting

by t =  0 using the summary

when every state is tracked

on average 53.9 time steps in

chain. The non-zero

second-largest

eigenvalue

in the imperfectly

in this network helps the

of the evidence

besides

by allowing

for other

summarized.

explicitly.



7  EMPIRICAL TESTS

Although the above results
the past using a summary chain can perform poorly,
empirical
testing
well on simulated

shows that it performs
two-level

show that coupling
from

diagnostic

networks.

reasonably

These

Eigenvalues

and eigenvectors

of the original chain

states

00
01
10
11

1

0

.81

0

0.47368
0.47368
0.05263

0.49809
-0.44828
-0.04981

Eigenvalues

and eigenvectors

of the summary chain

states

00
0?
01
?0
??
?1
10
1?
11

1

0
0

.81

.81

0
0

0
0

0.47368 0.49809 -0.44178

0

0
0
0

0
0
0

0.49391
0.05488
0.47368 -0.44828 -0.09631

0

0

0

0.05263 -0.04981 -0.01070

Table 1: Eigenvectors
matrices

for the two-disease

network.

and eigenvalues

of transition

eigenvalue

eigenvalues

eigenvectors

(as column vectors)

network is shown in Figure 1.
Markov chain has 22 =  4 states for the
variables

An example two-disease
The original
two disease
(00, 01, 10, 11). The summary
chain has 32 =  9 states (00, 0?, 01, ?0, ??, ?1, 10, 1?,
11). The non-zero
of the transition
matri
ces, with associated
are shown in Table 1.
in the summary chain apart
The largest
of 1 is 0.81, the same as the second
from the eigenvalue
of the original
largest
eigenvalue
using the summary chain will be just
that coalescence
chains from all
as quick as when explicitly
possible
and indeed experimental
initial states,
show that both methods perform identically.
Simple networks
such as these are not of great inter
est by themselves,
but they may exist as sub-networks
within a larger network. In a noisy-or
symptoms and diseases,

network of
a symptom that is known

chain. This suggests

tracking

results

UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000

263

Disease apriori probability: 0.1
Symptom apriori probability: 0.0
Noisy-or weight: 1.0

Disease  apriori

probability:
Symptom apriori probability:

Noisy-or weight:

0.001
0.001
1.000

Figure 2: An imperfectly-summarized

network.

Figure 3: Extreme

imperfectly-summarized

network

0.973  0.851 + 0.075i  0.851 - 0.075i

0.321 -0.193  0.338 + Oi

0.338 + Oi

states

000
00?
001
0?0
0??
0?1
010
01?
011
?00
?0?
?01
??0
???
??1
?10
?1?
?11
100
10?
101
1 ?0
1 ??
1 ?1
110
11?
111

0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0

0
0
0

0

0
0
0
0
0
0
0
0

0
0
0
0

0
0

0

0

0.441
0.049

0

0.049
0.005

0.045
0.005

0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0

0
0
0

0

0.321 -0.191 -0.152 - 0.283i -0.152 + 0.283i

0.321 -0.189  -0.167 + 0.254i -0.167- 0.254i

0.036 -0.021 -0.019 + 0.028i -0.019 - 0.028i

0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0

0
0
0

0

would also converge

not guaranteed)
slowly. Neverthe

that

of the chain for these problems

show that if the convergence
were somehow

prop

(though

it is likely

explicitly,
Gibbs sampling
less, calculations
erties
known, use of coupling
putationally
error tolerance.
ties are not known, however,
period
"burn-in"
eliminates
all uncertainty
has been reached,
tribution
tive whenever

favourable

In practice,

problematic.

the convergence
proper
which makes relying
from the past
the right dis

Coupling
about whether

on a

which may make it attrac

from the past would be com
only for users with quite a low

it is computationally

feasible.

