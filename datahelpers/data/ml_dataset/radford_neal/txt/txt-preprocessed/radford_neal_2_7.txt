Abstract.Connectionistlearningproceduresarepresentedfor\sigmoid"and\noisy-OR"
varietiesofstochasticfeedforwardnetwork.Thesenetworksareinthesameclassasthe\belief
networks"usedinexpertsystems.Theyrepresentaprobabilitydistributionoverasetofvisible
variablesusinghiddenvariablestoexpresscorrelations.Conditionalprobabilitydistributions
canbeexhibitedbystochasticsimulationforuseintaskssuchasclassi(cid:12)cation.Learningfrom
empiricaldataisdoneviaagradient-ascentmethodanalogoustothatusedinBoltzmannma-
chines,butduetothefeedforwardnatureoftheconnections,thenegativephaseofBoltzmann
machinelearningisunnecessary.Experimentalresultsshowthat,asaresult,learningina
sigmoidfeedforwardnetworkcanbefasterthaninaBoltzmannmachine.Thesenetworks
haveotheradvantagesoverBoltzmannmachinesinpatternclassi(cid:12)cationanddecisionmak-
ingapplications,andprovidealinkbetweenworkonconnectionistlearningandworkonthe
representationofexpertknowledge.
Introduction
Theworkreportedherebeganwiththedesireto(cid:12)ndanetworkarchitecturethatsharedwith
Boltzmannmachines[,,]thecapacitytolearnarbitraryprobabilitydistributionsover
binaryvectors,butthatdidnotrequirethenegativephaseofBoltzmannmachinelearning.It
washypothesizedthateliminatingthenegativephasewouldimprovelearningperformance.
ThisgoalwasachievedbyreplacingtheBoltzmannmachine'ssymmetricconnectionswith
feedforwardconnections.InanalogywithBoltzmannmachines,thesigmoidfunctionwasused
tocomputetheconditionalprobabilityofaunitbeingonfromtheweightedinputfromother
units.StochasticsimulationofsuchanetworkissomewhatmorecomplexthanforaBoltzmann
machine,butisstillpossibleusinglocalcommunication.Maximumlikelihood,gradient-ascent
learningcanbedonewithalocalHebb-typerule.
Thesenetworksturnouttofallwithinthegeneralclassof\beliefnetworks"studiedbyPearl
[]andothersasameansofrepresentingprobabilisticknowledgeinexpertsystems.However,
thespeci(cid:12)cnetworkarchitecturesconsideredbyPearldonotuseasigmoidprobabilityfunction.
Rather,theyemploya\noisy-OR"modelfortheprobabilityofaunitbeingon,basedonthe
statesofunitsfeedingintoit.Itisnaturaltoaskwhetheralearningprocedurecanbedeveloped
forthismodelaswell.Alocallearningrulewasindeedfoundforageneralizationofthenoisy-
ORmodel,thoughthistimethegradient-ascentproceduremustbeconstrainedtoavoidan
invalidregionofweight-space.


Therepresentationalpowerofthetwotypesoffeedforwardnetworkwereinvestigated,and
comparedtothatoftheBoltzmannmachine.Itturnsoutthateachofthesethreenetworks
canrepresentprobabilitydistributionsoverthefullsetofunitsthattheothertwonetworks
cannot.Withthehelpof\hidden"units,allthesenetworkscanrepresentarbitrarydistributions
overasetof\visible"units.
Thelearningperformanceofthesenetworkswasevaluatedusingasimplemixturedistribution
andanassociatedclassi(cid:12)cationtask.Thesigmoidfeedforwardnetworkwasfoundtobecapable
oflearningatasigni(cid:12)cantlyhigherratethantheBoltzmannmachine.Thenoisy-ORnetwork
performedlesswell,however,andinothertasksitshowedastrongtendencytogetstuckat
alocalmaximum.Additionalexperimentsestablishedthatthesigmoidfeedforwardnetwork's
advantageinlearningspeedovertheBoltzmannmachineisduetotheeliminationofthe
negativephase.
ThispaperbeginswithareviewofBoltzmannmachinesandbeliefnetworks.Ithende(cid:12)nethe
sigmoidandnoisy-ORvarietiesofstochasticfeedforwardnetwork,derivegradient-ascentlearn-
ingrulesforthem,andinvestigatetheirrepresentationalpower.Theexperimentcomparingthe
learningperformanceofthesenetworkswithBoltzmannmachinesisthendescribed.Finally,I
showhowstochasticfeedforwardnetworksrelatetootherconnectionistapproachestostatistical
modelingandtoworkontherepresentationofprobabilisticknowledgeinexpertsystems,andI
discusshowthesenetworksopenupnewpossibilitiesfordecisionmaking,alternativelearning
procedures,andbiologicalmodeling.
AreviewofBoltzmannmachines
TheBoltzmannmachine[,,]ismostnaturallyviewedasadeviceformodelingaprob-
abilitydistribution,fromwhichconditionaldistributionsforuseinpatterncompletionand
classi(cid:12)cationmaybederived.Inthelimitasprobabilitiesapproachzeroandone,determin-
isticinput-outputmappingscanberepresentedaswell.Thesecapabilitieswouldmakethe
Boltzmannmachineattractiveinmanyapplications,wereitnotthatitslearningprocedureis
generallyseenasbeingpainfullyslow.
De(cid:12)nitionofBoltzmannmachines.ABoltzmannmachineconsistsofsome(cid:12)xednumber
oftwo-valuedunitslinkedbysymmetricalconnections.Insomeformulations,thetwopossible
valuesofaunitareand;inotherformulationsthetwovaluesare(cid:0)and+.These
alternateformulationsarerepresentationallyequivalent,butcandi(cid:11)ersomewhatinlearning
performance.
Thestatesoftheunitswillbedenotedbythevectors,withthestateofunitibeingsi.This
statevectorwilloftenberegardedasarealizationofacorrespondingrandomvariableS.The
weightontheconnectionbetweenunitiandunitjwillbedenotedbywij.Sinceconnections
aresymmetrical,wij=wji.Unitsdonotconnecttothemselves.\Bias"weights,wi,froma
(cid:12)ctitiousunitwhosevalueisawaysarealsoassumedtobepresent.
The\energy"ofanetworkwithstatesisde(cid:12)nedasfollows:
E(s)=(cid:0)(cid:12)Xj<isisjwij
Generalizationstounitswithcontinuousvaluesandtonetworkswithhigher-orderinteractionsarepossible,
butwillnotbeconsideredinthispaper.


where(cid:12)istheconstantifunitstakeonvaluesofandortheconstantifunitstakeon
valuesof(cid:0)and+.
ThisenergyfunctioninducesaBoltzmanndistributionoverstates,inwhichlow-energystates
aremoreprobablethanhigh-energystates.Speci(cid:12)cally,
P(S=s)=exp((cid:0)E(s))=Z
whereZisanormalizationfactorneededtomakethedistributionsumtoone:
Z=Xsexp((cid:0)E(s))
Typically,someoftheunitsinthenetworkare\hidden",andweareinterestedonlyinthe
marginaldistributionoftheother\visible"units.Wethenconsiderthestatevectorstobe
splitintothepairhh;vi,andsimilarlytherandomvariableSbecomeshH;Vi.Thedistribution
overthevisibleunitsisthen
P(V=v)=XhP(S=hh;vi)
StochasticsimulationofBoltzmannmachines.SinceZisthesumofanexponentially
largenumberofterms,directlycomputingtheprobabilityofagivenstatevectorisinfeasiblefor
networksofsigni(cid:12)cantsize.Evenifthiscalculationcouldbeperformede(cid:14)ciently,wewouldstill
needtimeexponentialinthenumberofhiddenunitstocalculatethemarginalprobabilityofa
visiblevector,ortheprobabilitydistributionforasubsetofvisibleunitsconditionalongiven
valuesfortheothervisibleunits.Thesedistributionscan,however,beexhibitedviastochastic
simulation,aprocesswhichisfundamentaltotheoperationofallthenetworksconsideredin
thispaper.
Thesimulationstartswiththenetworkinanarbitrarystate.Unitsarethenrepeatedlyvisited
inturn,withanewvaluebeingselectedoneachvisitaccordingtotheunit'sprobabilitydistri-
butionconditionalonthevaluesofallotherunits.ForBoltzmannmachines,thisconditional
distributionforunitiisasfollows:
P(Si=xjSj=sj:j=i)=(cid:27)(x(cid:3)Pj=isjwij)
For(cid:0)=+valuedunits,x(cid:3)=x,whilefor=valuedunitsx(cid:3)=x(cid:0).The\sigmoid"
function,(cid:27)(t),isde(cid:12)nedas=(+exp((cid:0)t)).Notethat(cid:27)((cid:0)t)=(cid:0)(cid:27)(t).
Toproduceasamplefromthedistributionoverstatevectors,thesimulationisallowedtorun
foralengthoftimesu(cid:14)cientforittosettleto\equilibrium".Acollectionofstatevectors
takenatsu(cid:14)cientlywidelyseparatedtimesasthesimulationcontinuestorunwillthenform
asamplefromthedistributionforS.Conditionaldistributionscanbeexhibitedbyclamping
certainunitsto(cid:12)xedvaluesduringthesimulationandupdatingonlythevaluesoftheremaining
units.Thisallowsthenetworktoperformpatterncompletionandclassi(cid:12)cationtasks.
Unfortunately,itisdi(cid:14)culttosayhowmuchtimeshouldbeallowedforthesimulationto
reachequilibrium,oratwhatintervalstatevectorsshouldsubsequentlybetakentoformthe
sample.Thetechniqueof\simulatedannealing"isoftenusedtoreachequilibriumfaster.In
thismethod,theprobabilitydistributionusedinthestochasticsimulationismademoreuniform
byraisingtheprobabilityofeachstatetothepower=T(andthenrenormalizing).HereTisa


\temperature"parameter,whichisinitiallysethighinordertomakeequilibriumeasytoreach,
andisthengraduallyreducedto,atwhichpointonehopesthattheequilibriumdistribution
fortheoriginalprobabilitieswillhavebeenreached.
LearninginBoltzmannmachines.ThelearningproblemforBoltzmannmachinesisto
adjusttheweightssoastomakethedistributionovervisibleunitsmatchascloselyaspossible
thedistributionofsomereal-worldattributes,asevidencedbyasetoftrainingcases.
Adoptingthemaximum-likelihoodapproachtosuchestimation,weattempttomaximizethe
log-likelihoodgiventhetrainingcases,de(cid:12)nedas
L=logYvTP(V=v)=XvTlogP(V=v)
whereTisthecollectionoftrainingcases(whichmaycontainrepetitions).Thepartialderiva-
tiveofLwithrespecttoaparticularweightcanbeexpressedasfollows:
@L@wij=(cid:12)XvT(cid:16)XsP(S=sjV=v)sisj(cid:0)XsP(S=s)sisj(cid:17)
Theaboveformulaprovidesthebasisforagradient-ascentlearningprocedureinvolvingtwo
parallelstochasticsimulationsforeachtrainingcase.Inthe\positivephase"simulation,the
visibleunitsareclampedtothevaluestheytakeinthetrainingcase,withtheresultthat
thesimulationproducesasamplefromtheconditionaldistributionofSgivenV=v.Inthe
\negativephase"simulation,nounitsareclamped,producingasamplefromtheunconditional
distributionforS.Foreachstatevectors+inthepositivephasesample,theweightwijis
incrementedbyasmallamountproportionaltos+is+j.Foreachstatevectors(cid:0)inthenegative
phasesample,wijisdecrementedbyanamountinthesameproportiontos(cid:0)is(cid:0)j.Thisprocedure
isrepeateduntilconvergenceisreached.
Needforthenegativephase.
Intuitively,theneedforanegativeaswellasapositive
phaseinBoltzmannmachinelearningarisesfromthepresenceofthenormalizingfactor,Z,in
theexpressionfortheprobabilityofastatevector.Becauseofthis,thedirectionofsteepest
descentinenergyisnotthesameasthatofsteepestascentinprobability.Thenegativephase
ofthelearningprocedureisneededtoaccountforthise(cid:11)ect.
Lookedatanotherway,thenegativephaseprovidesthemechanismbywhichthelearningcomes
toastop|oncethecorrectdistributionovervisibleunitshasbeenlearned,thisdistribution
isexhibitedinthenegativephase,justasitisforcedinthepositivephase.Thepositivephase
incrementsandnegativephasedecrementsthenbalance,andtheweightsbecomestable.
Thepresenceofthenegativephasehasseveraldisadvantages:
)Itdirectlyincreasescomputationbyafactorofmorethantwo.
)Itmaymakethelearningproceduremoresensitivetostatisticalerrors.
)Itmayreduceanybiologicalplausibilitytheschemepossesses.
Regardingpoint(),sincethenegativephasesimulationshavemoreunclampedunits,they
takelongertorunthanthepositivephasesimulations.Regardingpoint(),thepresenceof
anegativephasemaymakeitnecessarytocollectalargersampleofstatevectorsfromthe
simulationsinordertoreducethevarianceintheestimateofthegradientofL,whichwillbe


thesumofthevariancesofthepositiveandthenegativephasestatistics.Takingthedi(cid:11)erence
ofthestatisticsfromtwophasesmayalsoexacerbatetheill-e(cid:11)ectsofnotreachingequilibrium
inthesimulations.
Ontheotherhand,thenegativephasecanbeexploitedtocontrolhownetworkresourcesare
utilized.Inparticular,thenetworkcanbeforcedtolearnaninput-outputmappingratherthan
thedistributionoftheinputitselfbyclampinginputsinthenegativeaswellasthepositive
phase.Itwillturnoutthatinstochasticfeedforwardnetworks,wherethenegativephasehas
beeneliminated,controloverwhatthenetworklearnscanbeexercisedbyothermeans.
Alookatbeliefnetworks
Beliefnetworks,alsoknownas\Bayesiannetworks",\causalnetworks",\in(cid:13)uencediagrams",
and\relevancediagrams",aredesigned,likeBoltzmannmachines,torepresentaprobability
distributionoverasetofattributes.StudyofthesenetworksbyPearl[]andothers[	]has
beenmotivatedprincipallybythedesiretorepresentknowledgeobtainedfromhumanexperts,
however.Accordingly,hard-to-interpretparameterssuchastheweightsinaBoltzmannmachine
havebeenavoidedinfavourofmoreintuitiverepresentationsofconditionalprobabilities.
De(cid:12)nitionofbeliefnetworks.Stickingascloselyaspossibletotheterminologyofthe
previoussections,wecanviewthestateofabeliefnetworkasavector,s,withsibeingthe
stateofuniti.Inthispaper,theunitswillalwaysbetwo-valued.
Theprobabilityofastatevectorisde(cid:12)nedintermsofwhatIwillcall\forwardcondition
probabilities"|theprobabilityofaunithavingaparticularvalueconditionalonthevalues
oftheunitsthatprecedeit:P(S=s)=YiP(Si=sijSj=sj:j<i)
Theconditionalprobabilitiesaboveareassumedtohavebeengivenbyanexpert.Typically,
onlyasubsetoftheunitsprecedingunitiwillbe\connected"toit,andonlythesewillbe
relevantinspecifyingitsforwardconditionalprobabilities.Notethattheorderingofunitsin
thestatevectoriscrucial,sinceitdetermineswhichconditionalprobabilitiesmustbespeci(cid:12)ed.
Stochasticsimulationofbeliefnetworks.
IncontrastwithBoltzmannmachines,comput-
ingtheprobabilityofaparticularstatevectorforabeliefnetworkisstraightforward.Onecan
alsoeasilygenerateasamplefromthedistributionforS.However,computingconditionalprob-
abilitiesandsamplingfromconditionaldistributionsareingeneraldi(cid:14)cultproblems.Various
methodsforcomputingexactconditionalprobabilitiesinbeliefnetworkshavebeenproposed
[,,],butallareeitherrestrictedtospecialformsofnetworkorhaveexponentialtime
complexityintheworstcase.
Itappearsthattheonlyplausiblemethodofsamplingfromconditionaldistributionsinbelief
networkswithhighconnectivityisstochasticsimulation,describedbyPearl[,].Aswith
Boltzmannmachines,astepinthesimulationrequiresselectinganewvalueforunitifromits
distributionconditionalonthevaluesoftheotherunits.Forabeliefnetwork,thisdistribution
isgivenbytheproportionality


P(Si=xjSj=sj:j=i)/P(Si=xjSj=sj:j<i)
Yj>iP(Sj=sjjSi=x&Sk=sk:k<j;k=i)
Forthisproceduretobeguaranteedtowork(inthelimitasthenumberofsimulationpasses
grows),theforwardconditionalprobabilitiesshouldbenon-zero.Thetimetoreachequilibrium
inthesimulationcanbereducedbyusingsimulatedannealing,asdescribedforBoltzmann
machines.
Thenoisy-ORmodelofconditionalprobabilities.Sofar,forwardconditionalprobabil-
itieshavebeenassumedtobegivenexplicitly.Infact,thiswillgenerallynotbefeasible,since
explicitlyspecifyingtheconditionaldistributionforSigiventhevaluesoftheprecedingunits
requiresi(cid:0)parameters.Evenifsomeoftheprecedingunitsarenotconnectedtouniti,more
compactspeci(cid:12)cationswillgenerallybenecessary.
Onemethod,termedthe\noisy-OR"model[,],viewstheunitsas=valuedOR-gates
withtheprecedingunitsasinputs.Aninputofdoesnotinvariablyforceaunittotakeon
thevalue,however.Rather,thereisacertainprobability,qij,thateventhoughunitjhas
thevalue,itwillfailtoforceaunitithatitfeedsintotogoto.Underthismodel,the
forwardconditionalprobabilitiescanbeexpressedintermsoftheqijasfollows:
P(Si=jSj=sj:j<i)=(cid:0)Yj<i;sj=qij
Onceagain,a(cid:12)ctitiousunitwhosevalueisalwayshasbeenassumed,alongwithassociated
parametersqi.Notethatifqijisone,unitjise(cid:11)ectivelynotconnectedtouniti.
Twovarietiesofstochasticfeedforwardnetwork
Wearenowinapositiontodescribethetwotypesofstochasticfeedforwardnetworkthat
areinvestigatedinthispaper.The(cid:12)rst,\sigmoid",varietywasdesignedinanalogywith
Boltzmannmachines.Whentheconnectionwithbeliefnetworkswasrealized,thesecond
varietywasdevelopedasageneralizationofthe\noisy-OR"modelforspecifyingconditional
probabilities.
De(cid:12)nitionofsigmoidnetworks.Twoformulationsofsigmoidfeedforwardnetworkswill
beconsidered.Inone,unitstakeonthevaluesand,intheother,theytakeonthevalues
(cid:0)and+.Theweightonthedirectedconnectioninthefeedforwardnetworkfromunitjto
unitiwillbedenotedbywij.Abiasunit,,setpermanentlytoisassumed,withassociated
weights,wi.Theforwardconditionalprobabilitiesforsigmoidnetworkscanthenbede(cid:12)ned
asfollows:
P(Si=sijSj=sj:j<i)=(cid:27)(s(cid:3)iPj<isjwij)
Hereagain,for(cid:0)=+valuedunits,s(cid:3)i=si,whilefor=valuedunits,s(cid:3)i=si(cid:0).
Onecaneasilyverifythatanetworkof=valuedunitswithforwardconditionalprobabilities
de(cid:12)nedasabovecanbeconvertedtoanequivalentnetworkof(cid:0)=+valuedunitswithweights
wijbythetransformation:


wi=wi+X<j<iwij=
wij=wij=;
for<j<i
Thistransformationiseasilyinverted.Thetwoformulationsthushaveequalrepresentational
power,thoughtheirperformancewithgradient-ascentlearningmaydi(cid:11)er.Asimilarequivalence
appliestoBoltzmannmachines.
Theprobabilityofastatevector,s,isde(cid:12)nedintermsoftheforwardconditionalprobabilities:
P(S=s)=YiP(Si=sijSj=sj:j<i)
=Yi(cid:27)(s(cid:3)iPj<isjwij)
AswithBoltzmannmachines,weareofteninterestedinthemarginaldistributionoverasubset
of\visible"units,givenby
P(V=v)=XhP(S=hh;vi)
whereShasbeensplitintohH;Vi.Wearealsointerestedinconditionaldistributionsinvolving
subsetsofvisibleunits,astheseallowonetoperformtaskssuchaspatterncompletionand
classi(cid:12)cation.
Stochasticsimulationofsigmoidnetworks.Toexhibitthesemarginalandconditional
distributionsviastochasticsimulation,wemustrepeatedlyselectanewvalueforeachunit
fromitsdistributionconditionalontherestofthenetwork.Thisdistributionisgivenbythe
proportionality
P(Si=xjSj=sj:j=i)/(cid:27)(x(cid:3)Xj<isjwij)Yj>i(cid:27)(s(cid:3)j(xwji+Xk<j;k=iskwjk))
Toselectanewvaluefromtheabovedistribution,unitimusthaveavailablebothitsown
totalinput:Pj<isjwij,andtheinputtoeachunit,j,thatitfeedsinto,exclusiveofitsown
contribution:Pk<j;k=iskwjk.Theprocedureisthussomewhatmorecomplexthanthatfor
aBoltzmannmachine,buttheinformationrequiredcanstillbemadeavailablethroughlocal
networkcommunication,provideddatacanpassbothwaysalongthedirectedconnections.
A\short-cut"simulationmethodispossiblewhenwewishtosamplefromthedistribution
conditionalonvaluesforsomesubsetofvisibleunits,andtheseclampedunitshappentobe
the(cid:12)rstonesinthestatevector.Inthiscase,ratherthanemploythefullstochasticsimulation
procedure,wecansimplyselectnewvaluesforeachunclampedunitinasingleforwardpass,
usingtheforwardconditionalprobabilities.Theselectionforeachunitdependsonlyonthe
valuesforprecedingunits,andthevaluesinthepreviousstatevectorhavenoe(cid:11)ectontheresult.
Accordingly,nosettlingtoequilibriumisrequired,andthestatevectorsobtainedinsuccessive
passesareallindependent.Thisshort-cutcanbeexploitedwhenfeedforwardnetworksareused
forpatternclassi(cid:12)cationorforothertasksthathavetheformofaninput-outputmapping.



Noisy-ORnetworks.
Inthe\noisy-OR"formofbeliefnetworkthatwasdescribedprevi-
ously,theprobabilities,qij,thataninputoffromunitjintounitiwillbeine(cid:11)ectiveinforcing
unititocanbereplacedwithweightsde(cid:12)nedbywij=(cid:0)logqij.Theforwardconditional
probabilitiescanthenbewrittenasfollows:
P(Si=jSj=sj:j<i)=(cid:0)exp((cid:0)Pj<isjwij)
Here,unitstakeonvaluesofand,andaunitsetpermanentlytoexists.
Intheaboveformulation,allweightsarenon-negative.However,theconditionalprobability
speci(cid:12)cationwillbevalidevenwhensomeweightsarenegative,providedthattheweighted
inputtoaunitcannotbenegative,nomatterwhatstatestheprecedingunitshave.Fora
networkwith=valuedunits,thisisequivalenttotheconstraintthat,foralli:
wi+Xj<i;wij<wij(cid:21)
Withthisgeneralization,unitscanbehavenotonlyasORgates,butalsoasORgateswith
someorallinputsnegated.Forexample,ifunithasinputweightsofw=+,w=(cid:0),
andw=(cid:0),itwillbehaveasaslightlynoisyORgatewithnegatedinputsfromunits
and(i.e.asaNANDgate).
Noisy-ORnetworkscanalsobeformulatedwith(cid:0)=+valuedunits.Forwardconditional
probabilitiesarede(cid:12)nedasabove,withtheconstraintthattobevalid,theweightsmustsatisfy
thefollowing,foralli:
wi(cid:0)X<j<ijwijj(cid:21)
Thesameequivalencebetween=and(cid:0)=+formulationsthatappliedtosigmoidnetworks
appliestonoisy-ORnetworksaswell.
Conditionalprobabilitydistributionsfornoisy-ORnetworkscanbeexhibitedviaastochastic
simulationprocessentirelyanalogoustothatdescribedaboveforsigmoidnetworks.
Learningstochasticfeedforwardnetworks
Thefeedforwardnetworksthathavebeendescribedareofinterestprincipallybecausethey
canbelearnedfromempiricaldata.Exceptforthelackofanegativephase,theirlearning
proceduresaresimilartothatusedfortheBoltzmannmachine.
Theselearningproceduresareallbasedonthewidely-usedmethodofmaximum-likelihood
estimation.Oneshouldrealizethatthismethodisproneto\over(cid:12)tting"thedatawhenthe
amountoftrainingdataissmallinrelationtothenumberoffreenetworkparameters,with
theresultthatthenetworkgeneralizespoorlytofuturecases.Also,alltheseproceduresuse
gradient-ascenttotryto(cid:12)ndasetofweightsmaximizingthelikelihood.Thismayleadtothe
learninggettingstuckatapointthatisalocalbutnotglobalmaximumofthelikelihood.
Sincethegradientsareevaluatedstochastically,thereisinfactsomenon-zeroprobabilityoftheweightsbeing
driventoanygivensetofvalues.Consequently,inthelongrun,thelearningprocedurewillspendmostofits
timeinthevicinityoftheglobalmaximum.The\longrun"inthiscasecanbeverylongindeed,however,
andinpracticethelearningproceduresoccasionallyendupinlocalmaximathataree(cid:11)ectivelystable.


Learninginsigmoidnetworks.
Inthelearningscenarioassumedhere,wehaveacollection,
T,oftrainingcasesdrawnfromthedistributionofinterest.Eachtrainingcaseconsistsofthe
valuesforcertainattributes,assumedheretobetwo-valued.Exactrepetitionsarepossible,
indeedexpected,inproportiontohowcommonaparticularcombinationofattributesis.
Wewishtomodelthedistributionfromwhichthetrainingsamplewasdrawn.Todothis,we
decideonsomesizeforastatevector,S,forournetwork,andthenselectsomesubset,V,
ofunitsinthestatevectortorepresenttheattributesinthetrainingcases.Theremaining,
\hidden",unitswillconstitutethesetH.Notethatsincetheorderingofunitsinthestate
vectorissigni(cid:12)cantforfeedforwardnets,di(cid:11)erentselectionsforthesubsetofvisibleunitsmay
givedi(cid:11)erentresults.Thisisdiscussedfurtherbelow.
Wenowwishto(cid:12)ndvaluesforthenetworkweightsthatmaximizethelikelihoodgiventhe
trainingcases.However,toavoidover(cid:12)ttingortoreducecomputationalexpense,wemight
decideto(cid:12)xcertainweightsatzero,basedonouraprioriknowledgeofthetask.Other
weightswillbesettozero(ortosmallrandomvaluesifwewishtobreaksymmetryfaster)and
thenadjustedbygradient-ascentsoastomaximizethelog-likelihood:
L=logYvTP(V=v)=XvTlogP(V=v)
Forasigmoidfeedforwardnetwork,thepartialderivativesofthelog-likelihoodwithrespectto
theweightsmaybefoundasfollows:
@L@wij=XvT
P(V=v)@P(V=v)
@wij
=XvT
P(V=v)Xh@P(S=hh;vi)
@wij
=XvTXhP(S=hh;vijV=v)

P(S=hh;vi)@P(S=hh;vi)
@wij
=XvTXsP(S=sjV=v)
P(S=s)@P(S=s)
@wij
(cid:27)(s(cid:3)iPk<iskwik)@(cid:27)(s(cid:3)iPk<iskwik)
=XvTXsP(S=sjV=v)

@wij
=XvTXsP(S=sjV=v)s(cid:3)isj(cid:27)((cid:0)s(cid:3)iPk<iskwik)
Thelaststepusesthefactthat(cid:27)(t)=(cid:27)(t)(cid:27)((cid:0)t).
Thesepartialderivativescanbeevaluatedbyrunningaseparatestochasticsimulationofthe
networkforeachtrainingcase,clampingthevisibleunitstothevaluestheytakeinthat
trainingcaseandobservingthestatevectorsthatariseasaresult.Ifthesimulationisrun
\longenough",theseobservationswillformasamplefromtheconditionaldistributionforS
giventhevaluesinthecurrenttrainingcase.Incrementingeachweight,wij,byasmallamount
proportionaltotheaveragevalueofs(cid:3)isj(cid:27)((cid:0)s(cid:3)iPk<iskwik)overthesamplesforalltraining
caseswillthenmovetheweightsalongthegradienttowardalocalmaximumofthelikelihood.


Variousdetailedimplementationsofthisprocedurearepossible,aswillbediscussedinthe
sectiononexperimentalresults.
Intuitively,onlyasinglephaseisneededtolearnasigmoidfeedforwardnetworkbecausenor-
malizationoftheprobabilitydistributionoverstatevectorsisaccomplishedlocallyateachunit
viathesigmoidfunction,ratherthangloballyviathehard-to-computenormalizationconstant,
Z.TheroleoftheBoltzmannmachine'snegativephaseinstoppinglearningoncethedistribu-
tionhasbeencorrectlymodeledistakenoverbythefactor(cid:27)((cid:0)s(cid:3)iPk<iskwik)usedtoweight
thelearningincrements.Inthelimitingcasewhereunitiislearningadeterministicfunction
oftheprecedingunits,forexample,thisfactoristheprobabilitythatunitiwouldbesetto
thewrongvalueinanunclampednetwork.Asthecorrectfunctionisapproached,thisfactor
becomeszero,andthelearningstops.
Learninginnoisy-ORnetworks.Learninginnoisy-ORnetworksisanalogoustothatin
sigmoidnetworks,withtheaddedcomplicationthatthegradient-ascentproceduremustbe
constrainedtotheregionofweight-spacethatproducesvalidprobabilitiesforstatevectors.
Thepartialderivativesofthelog-likelihoodwithrespecttotheweightsinanoisy-ORnetwork
canbeexpressedasfollows:
@L@wij=XvTXsP(S=sjV=v)
P(S=s)@P(S=s)
@wij
ifsi=)
=XvTXsP(S=sjV=v)((cid:0)sj+sj=((cid:0)exp((cid:0)Pk<iskwik))
ifsi=
(cid:0)sj
Thisformulaisvalidbothfornetworkswith=valuedunitsandforthosewith(cid:0)=+valued
units.Thesederivativesarecomputedviastochasticsimulationandusedtoperformgradient-ascent
learningasdescribedaboveforsigmoidnetworks.Fornoisy-ORnetworks,however,wemust
alsoensurethattheweightsalwaysde(cid:12)neavalidprobabilitydistribution.Infact,inorderfor
thesimulationstoreachequilibriuminareasonableamountoftime,itisdesirabletofurther
constraintheweightssothattheconditionalprobabilityofunitibeinggiventhevaluesof
theprecedingunitsisatleastsomeminimum.Foranoisy-ORnetworkwith=valuedunits,
thiswillbesoprovidedthat
wi+Xj<i;wij<wij(cid:21)(cid:17)
Where(cid:17)issomesmallpositiveconstant.Thiscanbeensuredbyapplyingtheprocedurein
Figuretotheweightsforunitiaftereachmovementalongthegradient.Onecanshowthat
thisproceduremovestheweightstothesetofvalidvaluesthatisclosestinEuclideandistance
tothepreviousset.Sincethevalidregionofweightspaceisconvex,itfollowsthatifonestarts
withavalidinitialsetofweights,movesinthedirectionofthegradient,andthenappliesthe
aboveprocedure,theresultingtotalmovementwillhaveapositiveprojectioninthedirection
ofthegradientwheneverthisispossible.
Ananalogousconstraintprocedureexistsfornoisy-ORnetworkswith(cid:0)=+valuedunits.
Seeclaimintheappendix.


Loop:C fg[fj:<j<i&wij<g
t PjCwij
Ift(cid:21)(cid:17)thenexitloop
d ((cid:17)(cid:0)t)=jCj
ForeachjC(cid:0)fg:
ifjwijj<dthend jwijj
ForeachjC:wij wij+d
Endloop
Figure:Proceduretomovetheweightsintounititothevalidregion.
Controllingwhatislearned.WhentrainingaBoltzmannmachine,onecancontrolwhat
thenetworklearnsbyclampingcertainunitsinthenegativeaswellasthepositivephase.This
iscommonlydonewhenonlythemappingfromasetof\input"attributestoasetof\output"
attributesisofinterest.Clampingtheinputunitsinbothphasesforcesthehiddenunitsto
modeltheconditionaldistributionoftheoutputgiventheinput,ratherthanthedistribution
oftheinputitself.
Thesametechniquecouldbeusedwithstochasticfeedforwardnetworks,butthiswouldnatu-
rallyrequirere-introductionofanegativelearningphase,theeliminationofwhichwasthemain
motivationforadoptingafeedforwardstructure.Fortunately,onecanachievesimilarcontrol
viajudiciousplacementofinput,output,andhiddenunitswithinafeedforwardnetwork.
ThreenetworkarchitecturesthatillustratethecontrolpossibleareshowninFigure,usinga
medicaldiagnosisproblemasanexample.Inallcases,asetofvisible\symptom"unitsare
usedtorepresentvariousattributesofapatient,andasetofvisible\disease"unitsareused
toencodeadiagnosis.Thereareassumedtobenoconnectionsamongtheunitswithineach
visibleset.
Innetwork(a),thehiddenunitsfeedintobothsetsofvisibleunits.Asaresultoftraining,these
unitsmaycometomodelcorrelationsamongsymptomunits,amongdiseaseunits,orbetween
thesymptomsandthedisease.
Ifthenetworksucceedsinmodelingthetotaldistribution
perfectly,itwillbecapableofperforminganysortofpatterncompletiontask.Forexample,
onecouldclampasetofsymptomsandthenobservethemostlikelydiseasesasthenetworkis
stochasticallysimulated,or,conversely,onecouldclampadiseaseandobservethemostlikely
symptoms.However,ifthenumberofhiddenunitsisinsu(cid:14)cienttomodelthetotaldistribution,
thenetwillendupmodelingwhichevercorrelationsarestrongest,andthesemightnotbethe
onesthataremostimportantfordiagnosis.
Innetwork(b),thehiddenunitsareplacedbetweenthesymptomunitsandthediseaseunits.
Thisforcesthehiddenunitstolearntomodeltheconditionaldistributionofthediseaseunits
giventhesymptoms.Onecouldthenclampasetofsymptomsandobservethemostlikely
diseases.Infact,thiscanbedoneusingtheshort-cutsimulationproceduredescribedpreviously,
sincetheclampedsymptomunitsprecedealltheunclampedunits(thefullsimulationprocedure
isstillrequiredduringlearning).Theconverseoperationofclampingadiseaseandobserving
likelysymptomsnolongerworkswell,however,sincetherearenohiddenunitsinapositionto
modelcorrelationsamongsymptoms.
Network(c)addsasetofhiddenunitspriortothesymptomunitsinordertocapturesuch


iiiiHiddenunits
iiiiSymptoms
Hiddenunits
???
iiiiSymptoms
iiii
??
(cid:17)(cid:17)(cid:17)+QQQs
iiiiHiddenunits
iiiiHiddenunits
iiiiiiii
Symptoms
Disease
iiiiDisease
iiiiDisease
(a)
(b)
(c)
Figure:Threenetworkarchitecturesforamedicaldiagnosisproblem.
correlations.Thisnetworkhascapabilitiescomparabletothoseofnetwork(a),withthedi(cid:11)er-
encethatthenumberofhiddenunitsdevotedtomodelingeachtypeofcorrelationisunderthe
controlofthenetworkdesigner.Network(c)mightbeappropriateforadiagnosisapplication
inwhichknowledgeofcorrelationsamongsymptomsissometimesneededinorderto(cid:12)llin
missingsymptomvalues.
Representationalpoweroffeedforwardnetworks
InthissectionIwillinvestigatehowpowerfulthevariousformsofstochasticfeedforwardnet-
workareatrepresentingprobabilitydistributions.
Iwill(cid:12)rstconsidersigmoidfeedforward
networks,andthendiscussthenoisy-ORvariety.Recallthatasshownearlier,thereisnodif-
ferenceintherepresentationalpowerofnetworkswith=valuedunitsandthosewith(cid:0)=+
valuedunits.
PowerofsigmoidfeedforwardnetsandBoltzmannmachines.Letusconsider(cid:12)rst
therelativecapacityofsigmoidfeedforwardnetworksandBoltzmannmachinestorepresent
probabilitydistributionsoverthefullstatevector,S.Theabilityofthesenetworkstoproduce
marginaldistributionsoverasubset,V,ofvisibleunitswillbediscussedlater.
Onecanshowthatanydistributionoveroneortwounitscanbeapproximatedarbitrarily
closelybyeitheraBoltzmannmachineorasigmoidfeedforwardnetwork.Fornetworksof
threeunits,therestrictedsetofpossibleprobabilitydistributionsturnsouttobethesamefor
thetwotypesofnetwork.Withfourormoreunits,however,bothBoltzmannmachinesand
sigmoidfeedforwardnetworkscanrepresentprobabilitydistributionsthattheothercannot.
ThisisillustratedinFigure,whichshowsaBoltzmannmachinethancannotbetranslatedinto
asigmoidfeedforwardnetwork,andasigmoidfeedforwardnetworkthatcannotbetranslated
intoaBoltzmannmachine.Inbothcases,=valuedunitsareused,andabsentconnection
weightsareassumedtobezero.Anunattachedconnectionisfromthebiasunit.
ToshowthatBoltzmannmachineofFigure(a)cannotbetranslatedtoanequivalentsigmoid
feedforwardnetwork,onecanproceedasfollows.First,sincetheBoltzmannnetworkissym-
metrical,thereisnosigni(cid:12)cantchoiceoforderingintryingtoconstructanequivalentsigmoid
Seeclaimintheappendix.
Seeclaimintheappendix


(cid:18)(cid:17)(cid:19)(cid:16)(cid:18)(cid:17)(cid:19)(cid:16)(cid:18)(cid:17)(cid:19)(cid:16)
(cid:18)(cid:17)(cid:19)(cid:16)
(cid:18)(cid:17)(cid:19)(cid:16)
++
(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)@@@@@@@@@
AAAAAAAAU
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:11)(cid:27)(cid:0)
++
+
+
+
+
+
?
(b)(cid:18)(cid:17)(cid:19)(cid:16)
(cid:18)(cid:17)(cid:19)(cid:16)
(cid:18)(cid:17)(cid:19)(cid:16)
(a)
Figure:AnuntranslatableBoltzmannmachine(a)andsigmoidnetwork(b).
net.Next,notethattheweightsintothelastunitinthesigmoidnetmustbeidenticalto
theweightsintheBoltzmannmachineinordertoproducethecorrectconditionalprobability
distributionsforthelastunit'svaluegiventhevaluesoftheotherunits.Onecanthenshow
thatthereisnowaytosettheweightsintothesecond-to-lastunitsoastoproducetherequired
conditionaldistributionsgiventhevarioussettingsoftheotherunits.
TurningtothesigmoidfeedforwardnetworkofFigure(b),notethatthelowerunitis(almost)
theORofthethreeupperunits.ThismappingcanbeduplicatedinaBoltzmannmachine
bysimplyusingthesameconnectionweights,butonecanverifythatitisthenimpossibleto
arrangefortheeightpossiblestatesoftheupperthreeunitstobeequallyprobable,asthey
areintheoriginalfeedforwardnetwork.
Representingmixturedistributions.Supposeweareinterestedintheprobabilitydistri-
butionoveravectorof\visible"units,V.Asseenabove,ingeneral,notallsuchdistributions
willberepresentableinanetconsistingofthesevisibleunitsalone.Thisproblemcanbe
overcomebyincludingadditional\hidden"unitsinanetwork.
Inparticular,sigmoidfeedforwardnetworks(aswellasBoltzmannmachines)canusehidden
unitstorepresentvisibledistributionsthatare\mixtures"ofseveralotherdistributions.Such
amixturedistributioncanbewrittenasfollows:
P(V=v)=XmP(V=vjM=m)P(M=m)
ThehiddenvariableMidenti(cid:12)esa\component"ofthemixture.Eachcomponentproducesits
owndistributionforV,andthesecomponentdistributionsarethencombinedintheproportions
P(M).Mixturedistributionsarecommonlyencounteredandmuchstudied[].
Torepresentamixturedistributioninanetwork,weneed(cid:12)rsttorepresentthemixturevariable,
M.Foramixtureofncomponents,onewaytodothisisviaaclusterofnunits,exactlyoneof
whichisonatanytime.Figureshowshowathree-unitclusterofthissortcanbeconstructed
forbothaBoltzmannmachineandasigmoidfeedforwardnetwork(with=valuedunits).In
bothcases,thethreestatevectorswithexactlyoneunitonhavenearlyequalprobabilities,and
Seeclaimintheappendix.
Seeclaimintheappendix.


?(cid:0):	=(cid:27)(cid:0)()
+
(cid:18)(cid:17)(cid:19)(cid:16)(cid:18)(cid:17)(cid:19)(cid:16)(cid:0)(cid:0)(cid:0)(cid:9)PPPPPqBBBBBN
(a)(cid:18)(cid:17)(cid:19)(cid:16) (cid:18)(cid:17)(cid:19)(cid:16)(cid:18)(cid:17)(cid:19)(cid:16)(cid:20)(cid:20)(cid:20)(cid:20)(cid:20)
(cid:0)
TTTTT
@@R
(cid:0)
(cid:0)
(cid:0)

(cid:0)(cid:0)(cid:9)
+
(b)(cid:18)(cid:17)(cid:19)(cid:16)
(cid:0)
(cid:0)
+
+
(cid:0)(cid:0)
@@
Figure:-in-clustersinaBoltzmannmachine(a)andasigmoidnetwork(b).
allotherstatevectorshaveverysmallprobability.Theconstructionsgeneralizetoclustersof
anysize,andtoclustersinwhichthepossiblestatevectorshaveunequalprobabilities.
A-in-nclustercandirectlyimplementamixtureinwhichthecomponentdistributionsassign
independentprobabilitiestothevariousvisibleunits.Forfeedforwardnetworks,allthatis
requiredistoconnecteachclusterunittothevisibleunitsusingweightsthatproducethe
requiredconditionalprobabilities.ForBoltzmannmachines,aftermakingtheseconnectionsto
visibleunits,onemustalsoadjustthebiasweightstotheclusterunitsinordertore-createthe
correctmixtureproportions.
Withmorecomplexmixtures,eachcomponentdistributionwouldberepresentedbyasubnet-
work.Theresultsofonesubnetworkwouldthenbeselectedforroutingtothevisibleunits
accordingtowhichoftheclusterunitsison,usingsimulationsofANDandORgates.
Notethatanydistributionoverkvisibleunitscanberepresentedasamixtureofkcomponent
distributions,eachofwhichgeneratesbutasinglevector.Itfollowsthatsigmoidfeedforward
networks,andBoltzmannmachines,canapproximateanydistributionoverkvisibleunits
arbitrarilyclosely,providedoneispreparedtoemploykhiddenunits.
Powerofnoisy-ORnetworks.Unlikesigmoidfeedforwardunits,theabilityofanoisy-OR
networktorepresentadistributionissensitivetonegationoftheunitvalues.Forexample,
thereisnowaytomakeanoisy-ORunitbehaveasanANDgate,butonecanmakeonebehave
asaNANDgate.Thissensitivityisofsigni(cid:12)canceonlyforvisibleunits,sincetheoutputof
ahiddenunitcanalwaysbeimplicitlynegatedbynegatingtheweightsonallitsout-going
connections.
Inviewoftheabove,therearecertainlydistributionsoverSthatbothaBoltzmannmachine
andasigmoidfeedforwardnetworkcanimplementbutwhichanoisy-ORnetworkcannot.
Conversely,onecanshowthatthedistributionproducedbyathree-unitnoisy-ORnetwork
with=valuedunitsinwhichw=w=andallotherweightsarezerocannotbe
duplicatedbyeitheraBoltzmannmachineorasigmoidfeedforwardnetworkwithonlythree
units.
Seeclaimintheappendix


A-in-nclustersimilartothatofFigure(b)butwiththeunitvaluesnegatedcanbecon-
structedfromnoisy-ORunits.Suchaclustercanbeusedtorepresentamixturedistribution
overvisibleunitsusinganoisy-ORnetwork,justaswithsigmoidfeedforwardnetworksand
Boltzmannmachines.
EmpiricalcomparisonwithBoltzmannmachines
Inthissection,Iwilldescribeanexperimentinwhichthelearningproceduresforstochastic
feedforwardnetworksandforBoltzmannmachineswerecomparedonthetaskoflearninga
simplemixturedistributionandclassifyingitemsderivedfromit.
Objectivesoftheexperiment.Thisexperimentisintendedtoanswerthefollowingques-
tions:)Arethelearningproceduresforstochasticfeedforwardnetworkscapableinpracticeof
learninganapproximationtoanon-trivialdistribution,asevidencedbyasetoftraining
cases?
)Ifso,howdoesthespeedoflearninginsigmoidfeedforwardnetworkscomparetothe
speedoflearninginaBoltzmannmachine?
)Candi(cid:11)erencesinlearningspeedbetweensigmoidfeedforwardnetworksandtheBoltz-
mannmachinebeattributedtothelackofanegativephaseinthelearningprocedure
forthefeedforwardnetworks?
)Aretheredi(cid:11)erencesinthelearningperformancesofnetworkswith=valuedunits
andthosewith(cid:0)=+valuedunits?
)Howdoeslearninginnoisy-ORnetworkscomparetolearninginsigmoidfeedforward
networks?
)Howwelldothesolutionslearnedbythevariousnetworksonthebasisoftrainingdata
generalizetothetruedistribution?
Regardingpoints()and(),theexpectationisthatthenegativephaseaddsadditionalnoise
totheestimationofthegradient,andthatthisnoiseisdetrimentaltothelearningprocessin
Boltzmannmachines.Themagnitudeofthise(cid:11)ectishardtojudge,however.Theaddednoise
couldevenbebene(cid:12)cial,ifitallowsthenetworktoescapelocalmaximaduringlearning.
Concerningpoint(),thereisreasontothinkthatlearningperformancemightbeworsefor
networkswith=valuedunitsthanfornetworkswith(cid:0)=+units.With=valuedunits,
weightwijismodi(cid:12)edonlyattimeswhenunitjhasthevalue.Super(cid:12)cially,atleast,this
seemsine(cid:14)cient.Also,sincethetransformationbetween=valuedand(cid:0)=+valuednet-
worksa(cid:11)ectsthebiasweightsdi(cid:11)erentlyfromtheothers,gradientascentwilloperatedi(cid:11)erently
inthetwoformulations.
Regardingpoint(),onewouldexpectanyproblemswithover(cid:12)ttingtobesimilarforthe
variousnetworks,sincetheyallhavethesamenumberoffreeparameters.
Thelearningprocedureused.NumerousvariationsoftheBoltzmannmachinelearning
procedurehavebeentried[],eachofwhichrequires(cid:12)xinganumberofparameters,suchas
thelearningrate,andthetemperaturesinanannealingschedule.Thispresentsaproblemin


comparinglearninginBoltzmannmachinestolearninginstochasticfeedforwardnetworks|
avalidcomparisonwouldrequiresearchingfortheoptimalparametersettingsforeachtypeof
network,whichwouldbearatherlargeundertaking.
TheapproachIhaveadoptedistotrainbothtypesofnetworkusingasimplemethodthathas
onlyoneadjustableparameter|thelearningrate,(cid:15).Acompletepictureoftheperformanceof
eachtypeofnetworkforvariousvaluesof(cid:15)canbeobtainedwithareasonablenumberofruns,
andtherelativeperformanceofthedi(cid:11)erentnetworkswiththeirbest(cid:15)canthenbecompared.
Theprocedureusedcanbecharacterizedasfollows:
)Learningwasdonein\batch"mode|i.e.eachchangetotheweightswasmadeon
thebasisoftheentiresetoftrainingcases.
)Eachtrainingcasewasclampedintoaseparatecopyofthenetwork,whereaseparate
stochasticsimulationwasrun.	ForBoltzmannmachines,therewasalsoanunclamped
negativephasecopyofthenetworkassociatedwitheachtrainingcase.
)Noannealingwasdone.
)Thestateofeachcopyofthenetworkwasretainedaftereachchangetotheweights,on
theassumptionthatiftheweightchangesare\small",theseexistingsimulationstates
willbeclosetoequilibrium,andthatasinglepassovertheunitsineachsimulation
willthusproducestatevectorsthatarereasonableforthenewweights.
)Changestotheweightsweremadeaftereachsimulationpass,basedonthesample
consistingofthecurrentstatevectorsfromthesimulationsforalltrainingcases(plus
thestatevectorsfromthenegativephasesimulations,forBoltzmannmachines).
)Weightchangeswerescaledbyalearningrateparameter,(cid:15).
)Weightsweresettozeroinitially.Symmetrywasbrokenbythestochasticnatureof
thesimulation.
Indetail,theweightsintheBoltzmannmachineswerechangedby
(cid:1)wij=(cid:12)(cid:15)N(cid:16)Xs+T+s+is+j(cid:0)Xs(cid:0)T(cid:0)s(cid:0)is(cid:0)j(cid:17)
Here,T+isthesetofcurrentstatevectorsfromthepositivephasesimulations(onepertraining
case),andT(cid:0)isthesetofstatevectorsfromthecorrespondingnegativephasesimulations.N
isthenumberoftrainingcases.
Similarly,theweightsinthesigmoidfeedforwardnetworkswerechangedby
(cid:1)wij=(cid:15)NXsTs(cid:3)isj(cid:27)((cid:0)s(cid:3)iPk<iskwik)
andthoseinthenoisy-ORnetworksby
(cid:1)wij=(cid:15)NXsT((cid:0)sj+sj=((cid:0)exp((cid:0)Pk<iskwik))
ifsi=)
ifsi=
(cid:0)sj
Thisaspectofthelearningprocedureappearstobeadvantageousfromanengineeringpointofview,butis
quiteimplausibleinabiologicalcontext.


mP(M=m)
P(Vi=vijM=m);i=::

.
.........

.
.........
.
.........


.
.........
Table:Themixturedistributiontobelearned.
Weightchangesinnoisy-ORnetworkswerelimitedtoamagnitudeofnomorethan,toavoid
thepossibilityofhugeweightchangesresultingfromthedivisionabove.Afterallchangeswere
made,theconstraintprocedureofFigurewith(cid:17)=(cid:0)wasapplied.
Thelackofannealinginthisprocedureisunconventional,asisthechangingofweightsbased
onasinglestatevectorfromeachtrainingcase.Therationalebehindthesechoicesisthat
as(cid:15)approacheszero,thesimulationswillnecessarilyapproachequilibrium,astheywillrun
formanypasseswiththeweightsessentiallyunchanged.Furthermore,thecumulativee(cid:11)ectof
manychangeswithasmall(cid:15)thatarebasedonasinglestatevectorfromeachtrainingcase
willbeequivalenttoasinglechangewithalarger(cid:15)thatisbasedonalargersample.As(cid:15)
approacheszero,thelearningprocedureusedwillthus\dotherightthing".
Whetherthisprocedureisbetterorworsethanpreviousmethodsisnotimportantforthis
experiment,however,providedonlythatanydi(cid:11)erencesinlearningperformancebetweenthe
variousnetworksseenusingthisprocedurewillshowupinsomeguiseinanyotherimple-
mentation.Itisdi(cid:14)culttoseehowthiscanbeguaranteed,butsomeassurancewassoughtby
runningadditionalexperimentstoseewhetherthedi(cid:11)erencesobservedweresensitivetoadding
annealingtotheprocedureortocollectingalargersamplebeforechangingtheweights.
Thetasklearned.Thenetworkswereevaluatedonthetaskoflearningthemixturedistri-
butionshowninTable.Therearefourequally-probablemixturecomponents,eachofwhich
producesadistributionoverninevisibleattributesinwhicheachattributeisindependentof
theothers(givenknowledgeofthemixturecomponent).
Allthenetworkstestedhadasimilarstructure.Sixinterconnectedhiddenunitswereprovided
toallowthenetworktomodelthemixturevariable,usingaclustersuchasinFigure.(Four
hiddenunitswouldhavesu(cid:14)ced;sixwereprovidedtohelpavoidproblemswithlocalmaxima.)
Thesehiddenunitswereconnectedtoasetofninevisibleunits.Forthefeedforwardnetworks,
theseconnectionsweredirectedfromthehiddenunits,tothevisibleunits.Thevisibleunits
werenotconnectedtoeachother.Allunitshadabiasconnection.
Sincethetaskistomodeltheentiredistribution,thenegativephaseinBoltzmannmachines
wasleftcompletelyunclamped.
Theentropyofthetargetdistributionis.bits.Forthisexperiment,thenumberoftraining
Furtherimplementationdetails:Weightswerekeptas(cid:12)xed-pointvalueswithaprecisionof(cid:0)andlimited
inmagnitudetonomorethan.Thisallowedcomputationbytablelook-up,andpreventedround-o(cid:11)
accumulationduringincrementalmaintenanceofthetotalinputtoeachunit.Weightswereroundedtoa
precisionof(cid:0)byaddingarandomnumberintherangeto(cid:0)andthentakingthe(cid:13)oor.Verysmall
gradientscanthenstillhaveane(cid:11)ect.


casesused,N,was.Theparticularsetofcasesgeneratedatrandomandusedinthese
runshadanaveragevalueof(cid:0)logP(V=v)of.bits,whereP((cid:1))isthetrueprobability
distribution.Thisisclosetotheentropy,asexpected.Thisvalueisthetargetfor(cid:0)L=Nin
networktraining,butduetoover(cid:12)tting,thetrainingproceduresmightwellreachvalueseven
lowerthanthis.
Thenetworkswerealsoevaluatedonthetaskofguessingthelastattributegiventhevaluesof
theothereightattributes.Withknowledgeoftherealdistribution,theoptimalerrorrateon
thisclassi(cid:12)cationtaskis.%.Notethatperformanceonthistaskisnottheformallearning
objective,andneednot,infact,bemonotonicallyrelatedtotheactualobjectiveofmaximizing
thelikelihood.
Evaluationmethod.Typically,stochasticsimulationisusedwhenapplyingnetworkssuch
asthesetoaprobleminstance,aswellaswhentrainingthem.Forexample,theclassi(cid:12)cation
taskwouldbeperformedbyclampingthevaluesoftheeightknownattributesandobserving
whichvaluefortheunknownninthattributeshowsupmostoftenasthenetworkissimulated.
Thismethodwasnotusedformostoftheevaluationsinthisexperiment,however.Instead,
theexactprobabilitiesofallstatesofthetrainednetworkwerecomputed,andfromthese,
thelog-likelihoodgiventhetrainingdata,thecross-entropywiththerealdistribution,and
performanceontheclassi(cid:12)cationtaskforthetrainingdataandforitemsdrawnfromthereal
distributionwereallcalculated.
Ofcourse,thismethodisinfeasiblefornetworksthatareevenslightlylargerthantheones
usedhere.
Itisconvenientforthisexperimentsinceiteliminatesstatisticalnoisefromthe
evaluations.Inthetestsofgeneralizationperformance,theclassi(cid:12)cationtaskwasperformed
usingstochasticsimulationaswellaswithexactprobabilities,andresultsweresimilar,as
reportedbelow.
ComparingsigmoidfeedforwardnetsandBoltzmannmachines.Thetraining
casesdrawnfromthemixturedistributionwereusedtotrainbothsigmoidfeedforwardnetworks
andBoltzmannmachines,usingvaluesfor(cid:15)of,,,,andsoonuntilnetworkbehaviour
becameunstable.Networkswith=valuedunitsandthosewith(cid:0)=+valuedunitswere
bothtried.
IllustrativeresultsareshowninFigure,for(cid:0)=+valuedunitsand(cid:15)ofand.Threeruns
areshown,inwhichdi(cid:11)erentrandomseedswereusedinthesimulations.Duringeachrun,the
log-likelihood,L,wascomputedexactlyafter,,,,,andsimulationpasses.
(Recallthateachpassconsistsofa(potential)changetoeachunitvalueineachsimulation,
andthatweightsarechangedaftereachpass.)Thevalueof(cid:0)L=Ninbits(i.e.usingbase
logarithms)isplotted.Itis	bitsinitially,sincewithzeroweightsallofthe	-elementvisible
vectorsareequallyprobable.
With(cid:15)=,theBoltzmannmachineandthesigmoidfeedforwardnetworkbehavedsimilarly.
As(cid:15)wasincreased,however,theBoltzmannmachinebecameunstable.Thisisseeninthe
(cid:12)gurefor(cid:15)=,wheretheBoltzmannmachinereachedthe.bitperformancelevel,but
thereafterfailedtoimproveconsistently.Incontrast,thesigmoidfeedforwardnetworkwith
(cid:15)=simplylearnedatfourtimestheratethatitdidwith(cid:15)=.Forlarger(cid:15),theBoltzmann
machinebecameevenmoreunstable,whilethesigmoidfeedforwardnetworktoleratedlearning
ratesupto(cid:15)=beforebecomingunstableat(cid:15)=andabove.


SigmoidFFnetwork
Boltzmannmachine
(cid:15)=
(cid:15)=










Numberofpasses
Numberofpasses
Figure:LearningperformanceofBoltzmannmachinesandsigmoidfeedforwardnetworkswith
(cid:0)=+valuedunitsfor(cid:15)ofand.Threerunswithdi(cid:11)erentrandomnumberseedsare
shown.





...	....	.
....	....
(cid:0)L=N








Typeofnetwork
Time/passBest(cid:15)Valuesof(cid:0)L=N
Errorrates

Boltzmannmachine
.	s
...
%%%
(=units)

Boltzmannmachine
.s
.	..
	%%%
((cid:0)=+units)

	%	%%
.s
SigmoidFFnetwork
...
(=units)
%%%
.s
SigmoidFFnetwork

...
((cid:0)=+units)
Table:Bestperformancesafterlearningpasses(threerunseach).
TheinstabilityoftheBoltzmannmachinewith(cid:15)=wasexaminedata(cid:12)nertimescaleby
evaluatingthenetworkaftereverylearningpassforoneoftheruns,asperformancewentfrom
.	bitsfor(cid:0)L=Natpassto.bitsatpass.Changesin(cid:0)L=Nofasmuchas.
bitswereseenaftersinglelearningpasses,and(cid:0)L=Nrangedinvaluefrom.bitsto.
bitsduringthisinterval.Examinationatthistimescaleoflearninginasigmoidfeedforward
networksimplyshowssteadyimprovement.
Resultsusing=valuedunitsweresimilar,exceptthatlearningwasslowerforagivenvalue
of(cid:15)inbothtypesofnetwork.Thiswaslargelycompensatedforwithsigmoidfeedforward
networksbythefactthatalarger(cid:15)couldbeusedbeforeinstabilitysetin.WithBoltzmann
machines,however,thereappearedtobesomenetadvantageforthe(cid:0)=+formulation.
TherelativeperformanceofthesenetworksisshowninTable,undertheassumptionthat
learningmustbestoppedafterpasses.Thiswouldproduceafaircomparisonifthecompu-
tationtimeperpasswasthesameforallnetworks.Infact,Boltzmannmachinepassesrequire
somewhatmoretime,aswouldbeexpectedfromtheneedtosimulatenegativephasecases,
sothecomparisonissomewhatbiasedinfavourofBoltzmannmachines.(Thetimesshown,
measuredonamachineratedatapproximatelyMIPS,shouldnotbetakentooseriously,
sincetheyarea(cid:11)ectedbymanyimplementationfactorsthatmaynotbeofgeneralsigni(cid:12)cance.)
TheentriesinTablewereproducedbyselectingthevalueof(cid:15)thatgavethebestvalueof
(cid:0)L=Nafterpasses,averagedoverthethreerunsthatweredone.Thesevaluesareshown
alongwiththecorrespondingerrorrateswhenguessingthelastattributeofthetrainingcases
fromthe(cid:12)rsteightattributes.Thesuperiorityofthesigmoidfeedforwardnetworksisevident.
ThehigherrorratesfortheBoltzmannmachine(especiallyusing=valuedunits)isdueto
thefactthatallthesenetworksinitiallylearncorrelationsamongthe(cid:12)rsteightattributes,and
onlylaterdiscoverhowtheninthattributerelatestothese.FourofthesixBoltzmannmachine
runshadnotprogressedfarintothesecondstageafterpasses.
Cleardi(cid:11)erencesbetweenthe=and(cid:0)=+formulationsareseeninotherproblems.Forexample,withboth
Boltzmannmachinesandsigmoidfeedforwardnetworks,learningtoassignhighprobabilitytoonlythose-bit
visiblevectorswithoddparity,usingfourhiddenunitstoexpresscorrelations,ismucheasierwith(cid:0)=+
unitsthanwith=units.


Furtherexperiments.Additionalexperimentsweredonetocheckwhetherthesuperiority
seenforsigmoidfeedforwardnetworkswasrelatedtotheparticularlearningprocedureused.
Oneunconventionalaspectofthelearningprocedureisthatweightswerechangedbasedon
asinglestatevectorfromeachsimulation.Anexperimentwasdonetocomparethismethod
withoneofequalcostinwhichchangeswherebasedonmorethanonestatevector,underthe
assumptionthatthesimulationpassesdominatethecomputationalcost.Inthisexperiment,
weightchangeswerecalculatedasbefore,butactualupdateswereperformedonlyeveryten
passes,atwhichtimethepreviouslycalculated,butdeferred,changeswhereallmadesimulta-
neously.Thechangestoweightswerethusbasedonasampleoftenstatevectorsfromeach
trainingcasesimulation,ratherthanonasinglestatevector.
ThismethodresultedinuniformlypoorerresultsforbothBoltzmannmachinesandsigmoid
feedforwardnetworks.Forsmallvaluesof(cid:15),learningwasslowerthanwhenchangeswere
madeimmediately.Thismightbeexpected,sinceinformationisnotbeingexploitedasearly.
Furthermore,learningbecameunstableatasmallervalueof(cid:15).Thismightalsobeexpected,
sincethemaximumamountbywhichaweightcanchangewithoutfeedbackisgreater.
ExperimentswerealsoperformedtodeterminewhethertheBoltzmannmachine'sinstabilityat
valuesof(cid:15)forwhichthesigmoidfeedforwardnetworkwasstablewasduetothelackofannealing
inthelearningprocedure.Whenthesimulationswereannealedbeforeusingthestatevectorsfor
learning,thestabilityoftheBoltzmannmachineappearedtobeslightlyimproved.However,
using(cid:0)=+valuedunits,therewerestillsignsofinstabilityat(cid:15)=,andthisbecamesevere
at(cid:15)=.Trainingasigmoidfeedforwardnetworkusingthesameannealingprocedureproduced
resultsthatwerenotappreciablydi(cid:11)erentfromthoseseenwithoutannealing(for(cid:15)=and
(cid:15)=).
Interpretationoftheresults.Theseexperimentsshowthatasigmoidfeedforwardnetwork
canlearnthetargetmixturedistributionfasterthanaBoltzmannmachine.Thisdi(cid:11)erenceis
duetothefeedforwardnetwork'stoleranceofahighlearningratethatcausesinstabilityin
theBoltzmannmachine.Sincethisinstabilityisapparentatthetime-scaleofasinglelearning
pass,andsinceitislittlea(cid:11)ectedbytheintroductionofannealing,itappearsthatitisdue
simplytothesamplingnoiseinthecalculationofthegradientfromtheresultsofpositiveand
negativephasestochasticsimulations.
Oneadvantageofafeedforwardnetworkinthisrespectmaybeseenclearlywhenthereareno
hiddenunits.Inthiscase,thepositive-phase,clampedsimulationsarecompletelydeterministic,
whilethenegative-phase,unclampedsimulationsremainstochastic.Learninginafeedforward
network,forwhichonlythepositivephasesimulationisnecessary,willthentakeplacewithno
noisedisturbingthemeasurementofthegradient.LearninginaBoltzmannmachine,which
requiresanegativephase,willstillbesubjecttonoise.Whenhiddenunitsarepresent,the
estimateofthegradientinthefeedforwardnetworkwillhavesomenoise,butstillnotasmuch
asintheBoltzmannmachine.
Thisisnotthefullexplanationofthedi(cid:11)erence,however,aswasseeninexperimentswhere
thesigmoidfeedforwardnetworkwastrainedwitharedundantunclampedphase,usingthe
\short-cut"simulationmethodtoensurethatstatevectorscamefromthetrueequilibrium
Theannealingschedulewasasfollows:Onepassatin(cid:12)nitetemperature,	passesattemperaturesdeclining
geometricallyfromto,and(cid:12)nallypassesatatemperatureof.Thetrainingtimewithannealingwas,
ofcourse,manytimesthatrequiredwithoutannealing.

distribution.Regardlessofwhetherthisredundantphasewasnegative(asinBoltzmannma-
chines)orpositive(equivalenttoasecondsetofunclampedtrainingcases)itsinclusiondid
notinduceinstability,butmerelyintroducedabitmorevariabilityintheprogressoflearning
(seeFigure,below).
Thedi(cid:11)erenceappearstoresultfromthewayweightsarechangedinthetwonetworks.Ina
feedforwardnetwork,eachchangetowijisweightedbytheforwardconditionalprobabilityof
Sihavingavaluedi(cid:11)erentfromthatitpresentlyhas.Aslearningprogresses,theseweighting
factorstendtodecrease,leadingtostability.InBoltzmannmachines,themagnitudeofeach
changeremainsconstant;itisonlythebalancebetweenpositivephaseincrementsandnegative
phasedecrementsthat,intheory,bringslearningtoastablehalt,butthisbalanceissensitive
tonoiseinthesamples.
E(cid:11)ectoffailuretoreachequilibrium.Althoughitwasnotamajorfactorintheexperi-
mentdescribedhere,failuretoreachequilibriuminthesimulationsisnotedasaproblemin[],
whereitisobservedthatafteraperiodofgoodprogresslearningcan\gosour",asweightsare
builtuptovalueswheretheyformlargeenergybarriersthatinhibitsettlingtotheequilibrium
distribution.Theauthorsprescribe\weight-decay"asapartialsolution.
Onewouldthinkthatlearningcould\gosour"instochasticfeedforwardnetworksaswellas
inBoltzmannmachines,butsuchproblemshavenotbeenobserved.However,itispossible
tomakethesigmoidfeedforwardnetworkgosourinthemixturedistributionexperimentby
addingaredundant,unclampednegativephase,simulatedinthenormalmanner(i.e.withno
short-cut).ThisisseeninFigure.Using(cid:0)=+valuedunitswith(cid:15)=,learningwith
theredundantnegativephasecloselymatchesthatwithoutanegativephaseforaboutthe(cid:12)rst
passes,butthenbecomesunstable.Interestingly,addingaredundant,unclampedpositive
phasedoesnotcausethelearningtogosour.
Theseresultscanbeunderstoodbypicturingthee(cid:11)ectsoffailingtosamplefromthetrue
equilibriumdistributioninthevariousphases.
Inaclampedpositivephase,thee(cid:11)ectwill
betocon(cid:12)nethestatevectorsseentoasubsetofthosehigh-probabilitystatevectorsthat
arecompatiblewiththeclampedvisibleunits.Thelearningincrementsthatresultfromthis
samplewillstilltendtoincreasetheprobabilityoftheclampedtrainingdata,albeitatalesser
ratethanwouldbethecaseifallthecompatiblehigh-probabilitystatevectorshadbeenseen.
Inanunclampednegativephase,failuretosamplefromtheequilibriumdistributionwillproduce
statevectorsthatdonotrepresentallthoseofhighprobability.Oncelearninghasmadesome
progress,therewillbeagroupofhigh-probabilitystatevectorscompatiblewitheachtraining
case.Inanon-representativesample,someofthesegroupsmaynotbesampledfromatall,while
othergroupswillcontributemorethantheirshareofstatevectorstothesample.Thelearning
decrementsthatoccurinthenegativephasewillthenunfairlydecreasetheprobabilityofthe
trainingcasescompatiblewiththeover-sampledgroups,morethano(cid:11)settingtheincrements
inthepositivephaseandproducinginstability.
Similarly,anextraunclampedpositivephaseresultsinsometrainingcasesbeingover-sampled,
andthusweightedmoreheavilyinthelearning.Thismayproducesub-optimalprogress,but
notinstability.Infact,runsdonewithanextraunclampedpositivephase(simulatedwithout
useoftheshort-cut)werenotableforahighvariability|somerunsdidsigni(cid:12)cantlybetter
thanthosewithouttheextraphase,whileothersdidratherworse.Nevertheless,eventheless
successfulrunsshowednearlysteadyimprovementasthesimulationsprogressed,incontrast


Withredundantnegativephase
Withredundantnegativephase
(noshort-cut)
(short-cutsimulation)
Withredundantpositivephase
Withredundantpositivephase
(noshort-cut)
(short-cutsimulation)













Numberofpasses
Numberofpasses
Figure:E(cid:11)ectofredundantphasesonlearningperformanceinsigmoidfeedforwardnetworks
((cid:0)=+valuedunits,(cid:15)=).Fiverunswithdi(cid:11)erentrandomnumberseedsareshown.






...	....	....	....	.

(cid:0)L=N




Noisy-ORFeedforwardNetwork
(cid:0)L=N...	....	.
(cid:15)=











Numberofpasses
Figure:Learningperformanceofanoisy-ORfeedforwardnetworkwith(cid:0)=+valuedunits
for(cid:15)of.Threerunswithdi(cid:11)erentrandomnumberseedsareshown.
tothedrasticworseningseenattimeswithanextranegativephase.
Thus,itappearsthattheconsequencesoffailuretoreachequilibriumaremoreseriousina
negativephasethaninapositivephase.Thisgivesstochasticfeedforwardnetworksaqualitative
advantageincircumstanceswhereequilibriumishardtoreach|learningmaybeadversely
a(cid:11)ected,buttheinstabilitythatcanoccurwithBoltzmannmachinesdoesnotarise.
Performanceofnoisy-ORnetworks.Noisy-ORfeedforwardnetworkswerealsoapplied
tothetaskoflearningthemixturedistribution,withratherdisappointingresults.Performance
wasbothpoorerandmoreerraticthanforsigmoidfeedforwardnetworks.
Figureshowstheprogressofthreerunsusing(cid:0)=+valuedunits,with(cid:15)=.Thenetworks
appeartohavedi(cid:14)cultylearningtoreduce(cid:0)L=Ntolessthanbits.Increasing(cid:15)sometimes
improvedlearningspeed,butnotreliablyso.Performanceofnoisy-ORnetworkswith=
valuedunitswasessentiallysimilar,exceptthatahighervalueof(cid:15)wasdesirable.
Inotherexperiments,noisy-ORnetworkssometimesshowedastrongtendencytogetstuck
atalocalmaximum(oratapointwherethegradientwassosmallthatlearningessentially
stopped).Forexample,attemptstotrainanoisy-ORnetworkwith=valuedunitstocompute
XORusingtwohiddenunitsbetweeninputsandoutput(theminimumrequired)succeededin
onlyoutoftries.Somewhatbetterresultswereobtainedusing(cid:0)=+valuedunits|
successinoutofattempts.Sigmoidnetworksalmostnevergetstuckwhensolvingthis
problem,evenwithonlyonehiddenunit(theminimumneededwithsigmoidnetworks).
Itseemspossiblethatsomere-parameterizationofnoisy-ORnetworkswouldimprovegradient-
ascentlearning,perhapsbyeliminatingtheneedforconstraints.Attemptsalongtheselines
havesofarfailedtoproducebetterresults,however.
Somedetails:Thetwohiddenunitswereconnectedtotheinputs,butnottoeachother.Theoutputunitwas
connectedtotheinputsandtothehiddenunits.Trainingwasdoneforpasseswith(cid:15)=.


Generalizationtotherealdistribution.Alltheresultsshownsofarconcerntheperfor-
manceofthenetworksonthetrainingcases.Generally,thetrueobjectiveisgoodperformance
onitemsdrawnfromtherealdistributionofwhichthetrainingcasesareasample.
Tableshowstheperformanceofallthenetworktypesonboththetrainingdataandonitems
fromtherealdistribution.Eachtypeofnetworkwastrainedwithareasonablevalueof(cid:15)until
performanceonthetrainingdataapproachedconvergence.(Thechoiceof(cid:15)andthepointof
near-convergencewerebothsubjectivelydetermined.)Thevalueof(cid:0)L=Nandtheclassi(cid:12)cation
errorratesforthetrainingdataareshown,alongwiththecorresponding(cid:12)guresforthereal
distribution.(Theanalogueof(cid:0)L=Nfortherealdistributionisthecross-entropy.)
Classi(cid:12)cationerrorratesshowninthetablewerecalculatedintwoways.The(cid:12)rstcalculation
usestheexact,realdistribution,andassumesthatclassi(cid:12)cationisbasedontheexactprobabil-
itiesde(cid:12)nedbythenetworks.Thesecondcalculationusesasampleoftestitemsdrawn
fromtherealdistribution,andusesclassi(cid:12)cationsbasedonsimulationresultsforeachtest
item,inwhichthe(cid:12)rsteightattributeswereclamped,andtheresultingvaluesfortheninth
attributeobserved.Resultsweresimilarforallexceptonerunofthenoisy-ORnetworkwith
=valuedunits,showingthatclassi(cid:12)cationperformanceisgenerallynotdependentonvery
smalldi(cid:11)erencesinprobabilitiesthatwouldbeswampedbynoiseinthesimulations.
Forcomparison,resultsfromamaximum-likelihood(cid:12)tofamixturemodelwithsixcomponents
tothetrainingdatausingtheEMalgorithm[,]aregivenaswell,evaluatedonatestsample
ofitems.
Thesigmoidfeedforwardnetworks,theBoltzmannmachinewith(cid:0)=+valuedunits,and
themixturemodelallshowsignsofover(cid:12)ttingthedata,sincetheirvaluesfor(cid:0)L=Nonthe
trainingdataarelessthanthevalueof:bitsthatthetruemodelwouldgive.Accordingly,
itisnotsurprisingthattheirperformanceontherealdistributionisnotquiteasgoodason
thetrainingdata.Themixturemodelappearstohaveover(cid:12)ttedtoalesserextendthanthe
networks.Thisisexpected,sinceitisamorerestrictedmodelthatneverthelesscanexactly
modelthisparticulardistribution.Thepenaltyinover(cid:12)ttingpaidforthegeneralityofthe
networkmodelsdoesnotseemlarge,however.
Thenoisy-ORfeedforwardnetworksandtheBoltzmannmachinewith=valuedunitsdonot
showsuchde(cid:12)nitesignsofover(cid:12)ttingthetrainingdata.Thisappearstobeare(cid:13)ectionof
theirgenerallyinferiorlearningperformance,notofanyintrinsicsuperiorityingeneralization.
Nevertheless,thebestvalueforcross-entropywiththerealdistributionseeninanyofthese
runsis:foronerunofthenoisy-ORnetworkwith=valuedunits,thoughtheothertwo
runsofthisnetworkwereratherpoor.
Generalizationperformanceforallthesenetworksmightwellbeimprovedbystoppinglearning
beforeconvergenceusingacross-validationcriterion[].
Asanaside,itisinterestingthattheweightsfoundduringnetworktraininggenerallybearonly
avagueresemblancetothosethatwouldresultfrommanuallysolvingtheproblemusingthe
clustersofFiguretorepresentmixturecomponents.
Datawascollectedfromsimulationpasses,withtheannealingscheduleofnote()beingappliedbefore
eachgroupoftenpasses.
TheEMalgorithmdoestakeconsiderablylesstimethananyofthenetworktrainingprocedures.


Typeofnetwork
Passes
(cid:15)
Valuesof(cid:0)L=NErrorrates(exact(cid:24)simulated)

Boltzmannmachine

.	.	.	
%%%(cid:24)%%%
(=units)
.	.	.	
	%%%(cid:24)	%%%


...
	%%%(cid:24)%%%
Boltzmannmachine
((cid:0)=+units)
.	..
	%	%	%(cid:24)%%%
...


SigmoidFFnetwork
%	%%(cid:24)%%%
.	..
(=units)
%	%	%(cid:24)%	%	%
SigmoidFFnetwork
...


%%%(cid:24)%%%
.	.	.
((cid:0)=+units)
	%%	%(cid:24)	%	%%

%%%(cid:24)%%%
.	..	

Noisy-ORFFnetwork
...	
(=units)
	%	%%(cid:24)	%%%

	%	%%(cid:24)%	%%
...

Noisy-ORFFnetwork
...
((cid:0)=+units)
	%	%%(cid:24)%%%
...
{

Mixturemodel
%%%
...
(EMalgorithm)
	%%	%
Table:Performanceontrainingdata((cid:12)rstline)andonitemsfromtherealdistribution
(secondline)forvariousnetworkstrainedtonear-convergence.Resultsfromthreerunswith
di(cid:11)erentrandomnumberseedsareshown.


Discussion
Iconcludebydiscussinghowstochasticfeedforwardnetworksrelatetootherconnectionist
approachestostatisticalmodelingandtoworkontherepresentationofexpertknowledge.
Ialsooutlinesomeareasinwhichstochasticfeedforwardnetworksappeartoopenupnew
possibilities.
Relationtootherconnectioniststatisticalmethods.Problemssuchasspeechorhand-
writingrecognitionarefundamentallystatisticalinnature.Althoughsomeaprioriknowledge
ofthetaskmaybeavailable,muchoftheinformationrequiredtosolvetheproblemmust
comefromtrainingdata.Thepreferredoutputforsuchclassi(cid:12)cationproblemsisaprobability
distributionoverpossibleclasses,conditionalontheattributespresentedasinput.
Adeterministicfeedforwardnetwork,trainedbyamethodsuchasbackpropagation[],can
representadistributionovertwoclassesbysimplyproducingtheprobabilityofoneofthe
classesasitsoutput.Suchanetworkthatusesthesigmoidfunctiontocomputetheoutput
ofaunitfromitsweightedinputappearsverysimilartoasigmoidstochasticfeedforward
networkwiththesamestructure.Indeed,thetwonetworksareessentiallyequivalentifthere
arenohiddenunits.However,inthegeneralcase,thisisnotso,sincethehiddenunitsin
thedeterministicnetworktakeon(cid:12)xednumericvalues,whilethoseinthestochasticnetwork
representadistributionoverbinaryvectors.
Distributionsovermorethantwoclassescanberepresentedinadeterministicnetworkusinga
clusterofoutputunits,oneforeachpossibleclassi(cid:12)cation.Theoutputofunitc,representing
P(Class=cjInput),issettoexp(Xc)=Piexp(Xi),whereXiisthetotalinputofuniti[].
Distributionsoveravectorofoutputattributescanberepresentedusingseveralsuchclusters,
undertheassumptionthattheprobabilitiesforthevariousattributesareindependent.
Stochasticnetworkshavethemoregeneralcapacitytoexhibitdistributionsoveralargeoutput
vectorinwhichtherearearbitrarydependenciesamongattributes.Theimprovedlearning
speedofstochasticfeedforwardnetworksoverBoltzmannmachinesmaymakethisapproach
feasibleinpractice.Furthermore,inastochasticfeedforwardnetworkwheretheinputunits
precedeallthehiddenandoutputunits,asinFigure(b),theshort-cutsimulationmethod
canbeusedtoproducepossibleclassi(cid:12)cationsforagiveninputwithouttheneedtosettleto
equilibrium,ataspeedcomparabletothatofadeterministicfeedforwardnetwork.Settlingto
equilibriumisstillnecessaryduringlearning,whentheoutputunitsareclamped.
Afurtheradvantageofstochasticoverdeterministicnetworksistheirsuperiorabilitytocope
withmissingdata,especiallywhenarchitecturessuchasthoseofFigure(a)and(c)areused.
Relationtoexpertsystems.
Inapplicationssuchasmedicaldiagnosis,expertshaveexten-
siveknowledgerelevanttothetask.Empiricaldata,whilevaluable,maybeoflimitedextent,
ormayhavebeenacquiredundercircumstancesdi(cid:11)erentfromthosecurrentlyprevailing.The
needinsuchapplicationstointegrateknowledgederivedfromexpertswiththatderivedfrom
empiricaldatahasbeenrecognizedbyworkersinthearea(seethediscussionin[],forexample).
Thelearningproceduresdescribedinthispapermaycontributetosolvingthisproblem.
Onepossibleapproachwouldbefortheexperttospecifythestructureofabeliefnetwork(i.e.
thecausalconnections),whileleavingthenumericvaluesoftheforwardconditionalprobabilities
tobeestimatedempirically.Iftrainingdataisavailableinwhichallattributesareknown,
thiswillbestraightforward.Itislikely,however,thatthebeliefnetworkwillcontainunits


whosevalueswerenotalwaysmeasured,orwhicharenotdirectlyobservable(suchasthetrue
underlyingdiseaseapatientsu(cid:11)eredfrom).Inthiscase,thegradient-ascentlearningprocedures
ofthispapercouldbeapplied,perhapsstartingwithweightvaluesderivedfromanexpert's
tentativeassessmentoftheprobabilities.Theexpertmightalsoconstrainprobabilitiestosome
intervalinordertoguardagainsttrainingdatathatisnotextensiveenough,orthatisnot
representativeofallpossiblecontextsinwhichthesystemmightbeused.
Moreambitiously,inpartsofthenetworkwherecausalconnectionsarenotcleartotheexpert
apoolofhiddenunitscouldbeincludedandtheirweightstrainedfromempiricaldata.A
problemwiththisapproachisthattheresultingnetworksmaybehardtointerpret.Using
\weightdecay"[]toencouragesomeweightstogotozeromighthelp.
Thedesiretokeepthenetwork'soperationintelligibletotheexpertsmightalsoleadonetouse
thenoisy-ORmodelforconditionalprobabilities,despitethesuperiorlearningperformanceseen
usingsigmoidunits.Theparticularpropertiesofthenoisy-ORmodelmightalsobedesirable
fortechnicalreasons;theyareexploitedintheheuristicdiagnosticsearchalgorithmof[],for
example.Noisy-ORandsigmoidunitscanalsobemixedinthesamenetwork,andforthat
matter,encorporatingaBoltzmannmachineasasub-networkisnotimpossible.
Makingdecisions.Stochasticfeedforwardnetworksarecompatiblewiththe\in(cid:13)uencedia-
grams"usedtoformulatedecisionproblems.AnalgorithmofShachter[]exploitsthestruc-
tureofthesediagramsto(cid:12)nddecisionsthatmaximizeexpectedutility.Unfortunately,this
algorithmcansometimestakeexponentialtime.Iwilldescribehereamethodofmakingsim-
pledecisionsusingstochasticsimulationthatalsoexploitsthefeedforwardstructure.
Consideranetworkwiththreesetsofvisibleunits|a\context"set,C,a\action"setA,and
a\result"set,R.Usingempiricaldata,wecantrainthisnetworktorepresenttheconditional
probabilitiesthatRwillresultgiventhatweperformactionAincontextC.Supposenowthat
wewishtobringaboutsome\goal",g,atatimewhenthecontextisc.Ourbestbetisto
performanactionathatmaximizesP(R=gjA=a&C=c).
Wecould(cid:12)ndtheactionthatmaximizestheprobabilityofourgoalbyrunningaseparate
stochasticsimulationforeverypossibleaction.Ineachsimulation,theactionandcontextunits
wouldbeclamped,andwewouldobservehowoftenthegoalshowsupintheresultunits.We
wouldthenchosetheactionthatleadstothegoalshowingupmostoften.However,thismethod
isinfeasibleiftherearemanyactions,representedbyalargenumberofunits.(Considerthe
numberofpossiblemedicaltreatmentswhentwentydrugscanbegivenincombination,for
example.)
However,wecantransformtheproblembyrewritingtheprobabilitytobemaximizedusing
Bayes'rule:P(R=gjA=a&C=c)=P(A=ajR=g&C=c)P(R=gjC=c)
P(A=ajC=c)
Now,providedthatP(A=ajC=c)isthesameforalla,wecanchoosethebestactionby
runningasinglestochasticsimulationinwhichweclampthecontextunitstocandtheresult
unitstog,andthenobservewhichvalueofaturnsupmostoftenintheactionunits.
EnsuringthatP(A=ajC=c)isthesameforallaiseasyinastochasticfeedforwardnetwork
|wesimplysetupthenetworksothatunitsinAhavenoincomingconnections,ensuring


equalprobabilitiesforeachainanunclampednetwork,andwefurtherarrangethatthereis
nodirectedpathfromaunitinAtoaunitinC,whichensuresthatclampingCwillnot
changetheseprobabilities.ProducingtheseequalprobabilitiesinaBoltzmannmachineisnot
soeasy.WecouldtrytotraintheBoltzmannmachinetosatisfytheconstraint,butthereis
noguaranteethatwewillsucceedverywell,andtheattemptmayinterferewithlearningthe
distributionofRgivenAandC.
Unfortunately,thetransformedmethoddoesnotcompletelysolvethisdecisionproblem.Itis
possiblethatwithaparticularcontextandgoalclamped,adi(cid:11)erentactionwillshowupafter
everysimulationpass,evenduringalongsimulation.Wewillthenhavenobasisfordeciding
which(ifany)oftheseactionsisbest.Themethodismostapplicableinsituationswhereonly
asmall,butunknown,subsetofactionshaveasigni(cid:12)cantprobabilityofproducingthegoal.
Potentialfornewlearningprocedures.Gradient-ascentlearninghastheadvantagethat
itissimple,andthatitcanbeperformedinan\on-line"mannerifdesired.However,itcanbe
ratherslow,andcangetstuckatalocalmaximum.ForBoltzmannmachines,thereappearsto
benoreasonablealternativetogradient-ascent,butforstochasticfeedforwardnetworksthefact
thattheprobabilityofafullstatevectorcanbeexplicitlycalculatedallowsonetocontemplate
otherlearningprocedures.
Fornoisy-ORnetworks,onepossibilityistoapplyastochasticversionoftheEMalgorithm
[].Thisseemsfeasibleprovidedthatthee(cid:14)cacyofeachinputtoaunitinforcingtheunitto
takeonthevalueismadeexplicitinasetofauxiliaryunitsthatarestochasticallysimulated
alongwiththemainunits.Probabilitiescanthenbeiterativelyestimatedfromco-occurrence
counts.
ItalsoseemsfeasibletoimplementtrueBayesianlearningforthesenetworks,inwhichthe
outputforatestcaseisobtainedbyintegratingtheoutputsofthenetworkoverallpossible
setsofconnectionweights,modi(cid:12)edbytheposteriorprobabilityofeachweightsetgiventhe
trainingcases.Forstochasticfeedforwardnetworks,thisintegrationcanbedonebyapplying
stochasticsimulationtothelearningprocessasawhole.Thismethodmayavoidboththe
problemofover(cid:12)ttingthetrainingdataandthepossibilityofgettingstuckinalocalmaximum.
Itmayworkespeciallywellfornoisy-ORnetworkswiththeauxiliaryunitsdescribedabove,
sinceitturnsoutthatonecanthenavoidhavingtosimulatedistributionsovercontinuous
parameters.
Biologicalmodeling.Stochasticfeedforwardnetworksalsoprovideadditionaloptionsfor
modelingofrealneuralprocesses.AlthoughanalogiesbetweenthenegativephaseofBoltzmann
machinelearninganddreamsleeparespeculateduponin[],itmaywellturnoutthatthe
negativephaseisbiologicallyimplausible.Theworkhereshowsthatthiswouldnotnecessarily
befataltotheideathatstochasticsimulationplayssomeroleinlearninginthebrain.The
somewhatcomplexsimulationprocedureforfeedforwardnetworksmaybeabarriertotheir
encorporationinmodelsofneuralprocessing,however.
Itistemptingtotrytosolvethisproblemusingsimulatedannealingby\cooling"thesimulationdowntoa
temperatureofzeroinorderto(cid:12)ndthemostprobablestatevectorcompatiblewiththeclampedcontextand
goal.However,theactionpartofthemostprobablestatevectorisguaranteedtorepresentthebestdecision
onlyiftherearenohiddenunitsinthenetwork.


Appendix|Proofsofclaims
ClaimTheprocedureofFiguremovestheweightsintounititothepointclosestinEu-
clideandistancethatsatis(cid:12)estheconstraint
wi+Xj<i;wij<wij(cid:21)(cid:17)
Proof.Letwijbetheoriginalsetofweights,andletwij=wij+(cid:14)jbethesetofweights
satisfyingtheconstraintforwhich(cid:1)=Pj(cid:14)jisminimal.Wecanproveanumberofproperties
ofthe(cid:14)j.
First,allthe(cid:14)jarenon-negative,sincedecreasingaweightwillcertainlynothelpsatisfythe
constraint.Also,forj>,(cid:14)j=ifwij(cid:21)and(cid:14)j(cid:20)jwijjifwij<,sinceonce(cid:14)jislarge
enoughtomakewijzero,makingitanylargerdoesnothelpsatisfytheconstraint.
Next,ifwij(cid:20)wik<,then(cid:14)j(cid:21)(cid:14)k.Otherwisereplacingboth(cid:14)jand(cid:14)kby((cid:14)j+(cid:14)k)=would
reduce(cid:1)whilekeepingtheconstraintsatis(cid:12)ed.Similarly,(cid:14)(cid:21)(cid:14)jforallj,sinceotherwise
therewouldbeanadvantageinreplacingthembothby((cid:14)+(cid:14)j)=.
Wecanthereforerenumbertheunitsbeforeiinsuchawaythatforsomen:
(cid:14)(cid:21)(cid:14)(cid:21)(cid:1)(cid:1)(cid:1)(cid:21)(cid:14)n>
wi(cid:20)(cid:1)(cid:1)(cid:1)(cid:20)win<
andwij(cid:21)and(cid:14)j=forj>n.Onecannowshow,byargumentssimilartothoseabove,
thatthereisanmsuchthatforalljlessthanm,(cid:14)j=(cid:14)and(cid:14)j<jwijj,whileform<j(cid:20)n,
(cid:14)j=jwijj.
Theentiresetofoptimalchanges,(cid:14)j,isthereforedeterminedbythevalueof(cid:14).Theother(cid:14)j
areeitherequalto(cid:14),orareless,ifalesservaluesu(cid:14)cestomakewijnon-negative.
TheprocedureofFigureisnoweasilyseentobeasearchfortheappropriatevalueof(cid:14).
ClaimTheweightsinbothaBoltzmannmachineandinasigmoidfeedforwardnetwork
consistingofonlyoneortwounitscanbesetsoastoproduceanyprobabilitydistributionover
statevectors.(Exceptthatdistributionsinwhichsomestatevectorshavezeroprobabilitycan
onlybeapproachedastheweightsgotoin(cid:12)nity.)
Proof.Weneedonlyconsidernetworkswith=valuedunits.Clearly,anydistributionover
anetworkofoneunitcanbeproducedbysimplyadjustingthesinglebiasweight.
Toproduceagivendistributionoverasigmoidfeedforwardnetworkwithtwounits,startby
settingthebiasweightforthe(cid:12)rstunittoproducetherequiredmarginalprobabilitydistri-
butionforthatunit.Thensetthebiasweightforthesecondunittoproducetherequired
conditionalprobabilitydistributiongiventhatthe(cid:12)rstunithasvalue.Lastly,settheweight
ontheconnectionfromthe(cid:12)rsttosecondunittoproducethecorrectconditionalprobability
distributiongiventhatthe(cid:12)rstunithasvalue,takingintoaccountthevalueofbiasweight
determinedearlier.
Foratwo-unitBoltzmannmachine,wemust(cid:12)ndweightsthatgiveenergiestothefourpossible
statesthatproducetherequireddistribution.Theenergyofstateh;iiszeroirrespectiveof
theweightvalues.Wecanarrangeforstatesh;iandh;itohavetheappropriateenergies


relativetothatofh;ibyadjustingtheirbiasweights.Theenergyofstateh;icanthenbe
madewhateverwewishbysettingtheweightontheconnectionbetweenthetwounits,taking
accountofthebiasweights.
ClaimThesetofprobabilitydistributionsthatcanbeproducedoveranetworkofthreeunits
isthesameforBoltzmannmachinesandsigmoidfeedforwardnetworks.
Proof.Totranslateathree-unitBoltzmannmachinetoasigmoidfeedforwardnetwork,start
bysettingupthe(cid:12)rsttwounitsofthesigmoidnetworksoastoduplicatethemarginaldistribu-
tionover(any)twounitsoftheBoltzmannmachine.Claimguaranteesthatthisispossible.
Nowaddathirdunitafterthesetwo,connectedtothe(cid:12)rsttwousingthesameweightsasin
theBoltzmannmachine.Thisduplicatestherequiredconditionalprobabilitiesforthethird
unit,withoutdisturbingthedistributionoverthe(cid:12)rsttwounits.
Totranslateathree-unitsigmoidfeedforwardnettoaBoltzmannmachine,startbysettingthe
weightstothethirdunitintheBoltzmannmachinetobethesameasthoseintothelastunit
inthesigmoidnetwork.Thisduplicatestheconditionalprobabilitiesforthisunitgiventhe
valuesoftheothertwounits.Nowweneedtosetuptheweightsbetweentheremainingtwo
unitssoastoproducethesamemarginaldistributionasinthesigmoidnetwork,takinginto
accountthebiasinge(cid:11)ectsofthethirdunit.Thiscanbedonebecause,again,allthingsare
possiblewithonlytwounits.
ClaimTheprobabilitydistributionproducedbythe=valuedBoltzmannmachineofFig-
ure(a)cannotbeduplicatedbyasigmoidfeedforwardnetworkwiththesamenumberofunits.
Proof.Wecanassumethatthefeedforwardnetworkalsouses=valuedunits.Duetothe
symmetryoftheBoltzmannmachine,thereisnochoiceinorderingtheunitswhentryingto
(cid:12)ndanequivalentsigmoidfeedforwardnetwork.Thelastunitinthefeedforwardnet(unit)
musthavethesameweightsasintheBoltzmannmachineinordertoreproducetheconditional
probabilitiesforthatunit'svaluegiventhevaluesintherestofthenetwork.
Nowconsiderhowwemustsettheweightsintothesecond-to-lastunitinthefeedforward
network(unit).Bysymmetry,thetwoweightsfromtheearlierunitsmustbeequal;call
theirvaluew.Thereisalsoabiasweight,b.Considertheoddsinfavourofunithavingthe
valuewhenunithasthevalueandtherearezero,one,ortwounitswithvaluebefore
unit.EquatingtheseoddsinthefeedforwardnetworktotheoddsintheBoltzmannmachine
producestheconstraints,respectively:exp(b)(cid:27)()(cid:27)()=exp()
exp(b+w)(cid:27)()(cid:27)()=exp()
exp(b+w)(cid:27)()(cid:27)()=exp()
Bytakinglogarithms,oneobtainsasystemoflinearequationsinwandbwhichnumerical
calculationshowstobeinconsistent.
ClaimTheprobabilitydistributionproducedbythe=valuedsigmoidfeedforwardnetwork
ofFigure(b)cannotbeduplicatedbyaBoltzmannmachinewiththesamenumberofunits.


Proof.WecanrestrictconsiderationtoBoltzmannmachineswith=valuedunits.Consider
theunitinacandidateBoltzmannmachinecorrespondingtothebottomunitinthesigmoid
network.Theweightsintothisunitmustbethesameasthoseinthesigmoidnetwork,inorder
toreproducetheconditionalprobabilitiesforthisunitgiventhevariouscombinationsofother
unitvalues.Notethatthevalueofthebottomunitise(cid:11)ectivelyadeterministicfunctionof
thevaluesoftheupperthreeunits|i.e.thereareonlyeightstatevectorswithsigni(cid:12)cant
probability.
NowconsidertheconstraintsplacedontheweightsintheBoltzmannmachinebytherequire-
mentthatallcombinationsofvaluesfortheupperthreeunitsbeequallyprobable,astheyare
inthesigmoidnetwork.IntheBoltzmannmachine,thistranslatestotherequirementthatthe
energyofthenetworkbethesameforalleightpossiblestatevectors.Inparticular,sincethe
energyofthestatevectorh;;;iiszero,theenergyoftheothersevenstatevectorsmust
bezeroaswell.Applyingthisconstrainttothethreestatevectorsh;;;i,h;;;i,and
h;;;i,we(cid:12)ndthatthebiasweightsintothethreeupperunitsmustbe(cid:0).Applying
ittothethethreestatevectorsh;;;i,h;;;i,andh;;;i,wegetthattheweights
betweentheupperunitsmustallbe(cid:0)aswell.Theenergyofthestateh;;;iisnow
determinedtobe(cid:0),showingthatapropersetofweightsisimpossible.
ClaimTheprobabilitydistributionproducedbyathree-unitnoisy-ORnetworkwith=val-
uedunitsinwhichw=w=andallotherweightszerocannotbeduplicatedbyeithera
Boltzmannmachineorasigmoidfeedforwardnetworkwithonlythreeunits.
Proof.
InviewofClaim,itsu(cid:14)cestoshowthatthenoisy-ORnetcannotbeduplicatedby
aBoltzmannmachinewith=valuedunits.
ConsidertheunitinacandidateBoltzmannmachinecorrespondingtounitinthenoisy-OR
net.
Inthenoisy-ORnet,thisunitisalwayszerowhentheotherunitsarebothzero.To
approximatethisintheBoltzmannmachine,thebiasweightforthisunitmustbeverylarge
andnegative.Theprobabilityofthisunitbeingzerowhenoneoftheothertwounitsisone
isonlye(cid:0),however.Theweightsfromtheothertwounitsmustthereforebeverylargeand
positive,inordertonearlycanceloutthelargenegativebiasinthiscase.Now,however,the
probabilityoftheunitbeingonewhenbothotherunitsareoneisnearlyintheBoltzmann
machine,butonly(cid:0)e(cid:0)inthenoisy-ORnet.ThusnosetofweightsfortheBoltzmann
machinecanproduce(orevencloselyapproximate)therequireddistribution.
Acknowledgements
IthankGeo(cid:11)HintonandtheothermembersoftheconnectionistresearchgroupattheUni-
versityofTorontoformanyhelpfuldiscussions.ThisresearchwassupportedbytheNatural
SciencesandEngineeringResearchCouncilofCanadaandtheOntarioInformationTechnology
ResearchCentre.
