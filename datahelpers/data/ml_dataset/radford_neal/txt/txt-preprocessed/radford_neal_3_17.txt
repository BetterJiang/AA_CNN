Abstract. I describe a new Markov chain method for sampling from the distribution of the
state sequences in a non-linear state space model, given the observation sequence. This method
updates all states in the sequence simultaneously using an embedded Hidden Markov model
(HMM). An update begins with the creation of a pool of K states at each time, by applying
some Markov chain update to the current state. These pools dene an embedded HMM
whose states are indexes within this pool. Using the forward-backward dynamic programming
algorithm, we can then eciently choose a state sequence at random with the appropriate
probabilities from the exponentially large number of state sequences that pass through states
in these pools. I show empirically that when states at nearby times are strongly dependent,
embedded HMM sampling can perform better than Metropolis methods that update one state
at a time.

1 Introduction

Consider a state space model with observations y0, . . . , yn1, each in some set Y, and hidden
states x0, . . . , xn1, each in some set X . Suppose we know the dynamics of hidden states and
the observation process for this model. Our task is to sample from the distribution for the
hidden state sequence given the observations.

If the state space, X , is nite, of size K, so that this is a Hidden Markov Model (HMM), a
hidden state sequence can be sampled by a well-known forward-backwards dynamic program-
ming procedure in time proportional to nK 2. Scott (2002) reviews this algorithm and related
methods. If X = p and the dynamics and observation process are linear, with Gaussian noise,

1

an analogous adaptation of the Kalman lter can be used. For more general models, one might
use Markov chain sampling. For instance, one could perform Gibbs sampling or Metropolis
updates for each xt in turn. Such simple Markov chain updates may be very slow to converge,
however, if the states at nearby times are highly dependent.

In this note, I describe how Markov chain sampling for these models can be facilitated by
using updates that are based on temporarily embedding an HMM whose nite state space is a
subset of X , and then applying the ecient HMM sampling procedure.

2 The Embedded HMM Algorithm

In describing the algorithm, model probabilities will be denoted by P (which will denote
probabilities or probability densities without distinction, as appropriate for the state space,
X , and observation space, Y). The initial state distribution is given by P (x0), transition
probabilities are given by P (xt | xt1), and observation probabilities are given by P (yt | xt).
Our goal is to sample from the conditional distribution P (x0, . . . , xn1 | y0, . . . , yn1), which
we will abbreviate to (x0, . . . , xn1)

0 , . . . , x(i)

To accomplish this, we will simulated a Markov chain with state space X n whose equilibrium
distribution is (x0, . . . , xn1). The state at iteration i of this chain will be written as x(i) =
(x(i)
n1). The transition probabilities for this Markov chain will be denoted using Q.
In particular, we will use some initial distribution for the state, Q(x(0)), and will simulate
the chain according to the transition probabilities Q(x(i) | x(i1)). For validity of the sampling
method, we need these transitions to leave  invariant:

(x) = X

x

(x)Q(x | x),

for all x in X n

(1)

(If X is continuous, the sum is replaced by an integral.) This is implied by the detailed balance
condition:

(x)Q(x | x) = (x)Q(x | x),

for all x and x in X n

(2)

The transition Q(x(i) | x(i1)) is dened using a set of auxiliary Markov chains, one for each
time step, whose state spaces are X , and whose transition probabilities, written as Rt( | ),
leave a specied pool distribution, t, invariant. The transitions for the reversal of this chain
with respect to t will be denoted by R( | ). These transitions satisfy the following condition:

t(x)Rt(x | x) = t(x) Rt(x | x),

for all x and x in X

(3)

Note that if the transitions Rt satisfy detailed balance with respect to t, Rt will be the same
as Rt.

For each time, t, the transitions Rt and Rt are used to produce a pool of K candidate states,
Ct, one of which is the current state, x(i1)
. The new sequence, x(i), is randomly selected from
among all sequences whose states at each time t are in Ct, using a form of the forward-backward
procedure.

t

In detail, the pool of candidate states for time t is found as follows:

2

1) Pick an integer Jt uniformly from {0, . . . , K  1}.

2) Let x[0]

t = x(i1)

t

.

3) For j from 1 to Jt, randomly pick x[j]

t

Rt(x[j]

t

| x[j1]

t

).

according to the transition probabilities

4) For j from 1 down to K +Jt+1, randomly pick x[j]

t according to the reversed transition

probabilities, Rt(x[j]

t

| x[j+1]

t

).

5) Let Ct be the pool consisting of x[j]

t , for j  {K + Jt + 1, . . . , 0, . . . , Jt}. If some of the

x[j]
t are the same, they will be present in the pool more than once.

Once the pools of candidate states have been found, a new state sequence, x(i), is picked from
among all sequences, x, for which every xt is in Ct. The probability of picking x is proportional
to (x)/Q t(xt), which is proportional to

P (x0)Qn1

t=1 P (xt | xt1)Qn1

t=0 P (yt | xt)

Qn1
t=0 t(xt)

(4)

If duplicate states occur in some of the pools, they are treated as if they were distinct when
picking a sequence in this way. In eect, we pick indexes of states in these pools, with proba-
bilities as above, rather than states themselves. The distribution of these sequences of indexes
can be regarded as the posterior distribution for a hidden Markov model, with the transition
probability from state j at time t  1 to state k at time t being proportional to P (x[k]
t1),
and the probabilities of the hypothetical observed symbols being proportional to the remaining
factors above, P (yt | x[k]
t ). Crucially, it is possible, using the forward-backward tech-
nique, to randomly pick a new state from this distribution in time growing linearly with n,
even though the number of possible sequences grows as K n.

t )/t(x[k]

| x[j]

t

3 Proof of Correctness

To show that a Markov chain with these transitions will converge to , we need to show that it
leaves  invariant, and that the chain is ergodic. Ergodicity need not always hold, and proving
that it does hold may require considering the particulars of the model. However, it is easy to see
that the chain will be ergodic if all possible state sequences have non-zero probability density
under , the pool distributions, t, have non-zero density everywhere, and the transitions Rt
are ergodic. This probably covers most problems that arise in practice.

To show that the transitions Q( | ) leave  invariant, it suces to show that they satisfy
detailed balance with respect to . This will follow from the stronger condition that the
probability of moving from x to x (starting from a state picked from ) with given values for
the Jt and given pools of candidate states, Ct, is the same as the corresponding probability of
moving from x to x with the same pools of candidate states and with values J 
t dened by
t = Jt  ht, where ht is the index (from K + Jt + 1 to Jt) of x
J 

t in the candidate pool.

The probability of such a move from x to x is the product of several factors. First, there is
the probability of starting from x under , which is (x). Then, for each time t, there is the

3

probability of picking Jt, which is 1/K, and of then producing the states in the candidate pool
using the transitions Rt and Rt, which is

Jt

Y

j=1

Rt(x[j]

t

| x[j1]

t

) 

Rt(x[j]

t

| x[j+1]

t

)

1

Y

j=K+Jt+1

=

=

Jt1

Y

j=0

Rt(x[j+1]

t

| x[j]

t ) 

Rt(x[j+1]

t

| x[j]
t )

1

Y

j=K+Jt+1

t(x[K+Jt+1]

t

)

t(x[0]
t )

Jt1

Y

j=K+Jt+1

Rt(x[j+1]

t

| x[j]
t )

t(x[j]
t )
t(x[j+1]

t

)

(5)

(6)

Finally, there is the probability of picking x from among the sequences with states from the
pools, Ct, which is proportional to (x)/Q t(x

t). The product of all these factors is

(x)  (1/K)n 

n1

Y

t=0




t(x[K+Jt+1]

t

)

t(x[0]
t )

Jt1

Y

j=K+Jt+1

Rt(x[j+1]

t


| x[j]
t )




(x)
Qn1
t=0 t(x
t)

= (1/K)n

(x)(x)
Qn1
t=0 (xt)(x
t)

n1

Y

t=0


t(x[K+Jt+1]

t

Jt1

Y

)
j=K+Jt+1

Rt(x[j+1]

t


| x[j]
t )


(7)

The corresponding expression for a move from x to x is identical, apart from a relabelling of
candidate state x[j]

t as x[jht]

.

t

4 An Example Class of Models

As a simple concrete example, consider a model in which the state space X and the observation
space, Y, are both . Let each observation be simply the state plus Gaussian noise of standard
deviation   ie, P (yt | xt) = N (yt | xt, 2)  and let the state transitions be dened by
P (xt | xt1) = N (xt | tanh(xt1),  2), for some constant expansion factor  and transition
noise standard deviation  .

Let us choose the pool distributions, t, to be normal, with some means t and standard
deviations t, which may depend on y0, . . . , yn1, but not on x0, . . . , xn1. For example, we
might x t = 0 and t = 1 for all t, or we might let t be the posterior distribution for xt
given yt, based on an improper at prior, so that t = yt and t = , or we might let t be
some more elaborate approximation to the marginal distribution of xt given y0, . . . , yn1.

Of the many transitions that would leave t invariant, we might choose Rt to be of the

following form:

Rt(x | x) = N (x | t + (xt), (12)2
t )

(8)

where  is an adjustable parameter in (1, +1). When  = 0, the states in the pool (other
than the current state) are drawn independently from t. These transitions satisfy detailed
balance with respect to t, so Rt is the same as Rt.

The forward-backward algorithm will pick a state sequence from among those that can be
constructed using states from the candidate pools, with probabilities given by equation (4). In

4

the particular case when t = yt and 2
these probabilities simplify to being proportional to

t = 2, for which t(xt) is proportional to P (yt | xt),

P (x0)

n1

Y

t=1

P (xt | xt1)

(9)

Note that despite appearances, this distribution cannot be sampled from using a forward pass
alone, since P (xt | xt1) need not sum to one for xt in Ct.

5 Demonstration

The characteristics of the state and observation sequences produced using the models of the
previous section vary considerably with the choice of , , and  . For some choices, simple
forms of the Metropolis algorithm that update each xt separately can perform better than
the embedded HMM method, since these simple methods have lower overhead. Here I will
demonstrate that the embedded HMM can perform better than such single-state updating
methods when the states are highly dependent.

Figure 1 shows a sequence, x0, . . . , xn1, and observation sequence, y0, . . . , yn1, produced
using  = 2.5,  = 2.5, and  = 0.4, with n = 1000. The state sequence stays in the
vicinity of +1 or 1 for long periods, with rare switches between these regions. Because of the
large observation noise, there is considerable uncertainty regarding the state sequence given
the observation sequence, with the posterior distribution assigning fairly high probability to
sequences that contain short-term region switches that are not present in the actual state
sequence, or that lack some of the short-term switches that are actually present. It is dicult
for a method that updates only one state at a time to explore such a posterior distribution,
because it must move through low-probability intermediate states in which a switch to the
opposite region is followed immediately by a switch back.

Figure 2 shows that embedded HMM sampling works well for this problem, using K = 10
states and the simple choice of t = 0 and t = 1 for the pool distributions, and Rt as in
equation (8), with  = 0. We can see that only two updates produce a state sequence with
roughly the correct characteristics.

Figure 3 demonstrates how a single embedded HMM update can make a large change to the
state sequence. It shows a portion of the state sequence after 99 updates, the pools of states
produced for the next update, and the state sequence found by the embedded HMM using
these pools. A large change is made to the state sequence in the region from time 840 to 870,
with states in this region switching from the vicinity of 1 to the vicinity of +1.

In Figure 4, the state at two time points is plotted over the course of 99 embedded HMM
updates. Both points correspond to short-term switches in the actual state sequence. In the
posterior distribution, there is uncertainty about the true state at these points, with non-
negligible probability for values near 1 and for values near +1. We see in both plots that the
embedded HMM moves between these two regions.

In contrast, simple Metropolis methods that update one state at a time do much less well
for this problem. Figure 5 shows the state sequences produced after 50 and 100 iterations of a
Metropolis method in which each iteration updates each state in turn, using a N (0, 1) proposal

5

5

0

5


0

200

400

600

800

1000

Figure 1: A state sequence (black dots) and observation sequence (gray dots) of length 1000
produced by the model with  = 2.5,  = 2.5, and  = 0.4.

5

0

5


5

0

5


0

200

400

600

800

1000

0

200

400

600

800

1000

Figure 2: State sequences (black dots) produced after one embedded HMM update (top) and
two updates (bottom), starting with the states set equal to the data points (gray dots), for the
same model and data as Figure 1. The embedded HMM used K = 10, t = 0, t = 1, and
 = 0.

6

6

4

2

0

2


4


6


820

840

860

880

900

920

940

Figure 3: Closeup of an embedded HMM update. The true state sequence is shown by black
dots and the observation sequence by gray dots. The current state sequence is shown by the
dark line. The pools of states used for the update are shown as small dots, and the new state
sequence picked by the embedded HMM by the light line.

0

.

2

5

.

1

0

.

1

5
0

.

0

.

0

.

5
0


0

.

1


2

1

0

1


5
7
6
e
m



i
t

t

a



t

e
a
S

t

0
0
2
e
m



i
t

t

a



t

e
a
S

t

0

20

40

60

80

100

0

20

40

60

80

100

Iteration

Iteration

Figure 4: Traces of states during an embedded HMM run. The left plot shows the state at
time 200 after each of the rst 99 updates; the right plot shows the same for the state at time
675.

7

5

0

5


5

0

5


0

200

400

600

800

1000

0

200

400

600

800

1000

Figure 5: State sequences (black) produced after 50 single-state Metropolis updates (top) and
after 100 updates (bottom), starting with the states set equal to the data points (gray).

0
0
2
e
m



i
t

t

a
e



t

t

a
S

4

3

2

1

0

5
7
6
e
m



i
t

t

a
e



t

t

a
S

1

0

1


2


0

200

400

600

800

1000

0

200

400

600

800

1000

Iteration

Iteration

Figure 6: Traces of states during a Metropolis run. The left plot shows the state at time 200
after each of the rst 999 updates; the right plot shows the same for the state at time 675.

8

distribution. Even after 100 iterations, the state sequence does not closely resemble the actual
state sequence (in particular, it contains too many short-term switches). The traces of states
at times 200 and 675 in Figure 6 conrm that these Metropolis updates do not move around
the posterior distribution eciently. The state at time 200 never reaches the vicinity of 1
during these 999 iterations. The state at time 675 does visit the vicinity of both 1 and +1,
but the values show very high autocorrelations. Simple Metropolis updates with various other
proposal distributions performed similarly, or worse.

On the other hand, one iteration of these simple Metropolis methods is approximately 30
times faster than one of the embedded HMM updates (with K = 10), when both methods are
implemented in the interpretive R language. In this example, however, the greater eciency
of the embedded HMM updates more than outweighs this.

With other settings of the , , and  parameters, dierent pool distributions are preferable
to the simple N (0, 1) distribution used for this demonstration. In particular, letting the pool
distribution for xt depend on yt or on a window of observations in its vicinity is sometimes
better. I have not found setting  to a non-zero value to be benecial for this model, but I
expect that setting  close to one in order to produce a pool of states in the vicinity of the
current state will be useful in higher-dimensional problems.

Acknowledgement

I thank Sam Roweis and Matthew Beal for helpful discussions. This research was supported
by the Natural Sciences and Engineering Research Council of Canada.

Reference

Scott, S. L. (2002) Bayesian methods for hidden Markov models: Recursive computing in the

21st century, Journal of the American Statistical Association, vol. 97, pp. 337351.

9


