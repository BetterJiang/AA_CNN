Abstract. Consider a Markov chain dened on a nite state space, X , that leaves invariant the uniform
distribution on X , and whose transition probabilities are integer multiples of 1/Q, for some integer Q. I
show how a simulation of n transitions of this chain starting at x0 can be viewed as applying a random
permutation on the space X  U , where U = {0, 1, . . . , Q1}, to the start state (x0, u0), with u0 drawn
uniformly from U . This result can be applied to a non-uniform distribution with probabilities that
are integer multiples of 1/P , for some integer P , by representing it as the marginal distribution for
X from the uniform distribution on a suitably-dened subset of X  Y, where Y = {0, 1, . . . , P  1}.
By letting Q, P , and the cardinality of X go to innity, this result can be generalized to non-rational
probabilities and to continuous state spaces, with permutations on a nite space replaced by volume-
preserving one-to-one maps from a continuous space to itself. These constructions can be eciently
implemented for chains commonly used in Markov chain Monte Carlo (MCMC) simulations. I present
two applications in this context  simulation of K realizations of a chain from K initial states, but with
transitions dened by a single stream of random numbers, as may be ecient with a vector processor or
multiple processors, and use of MCMC to improve an importance sampling distribution that already has
substantial overlap with the distribution of interest. I also discuss the implications of this permutation
MCMC method regarding the role of randomness in MCMC simulation, and the potential use of
non-random and quasi-random numbers.

1 Introduction

Markov chain Monte Carlo (MCMC) simulation might seem to be a fundamentally contractive process.
A simulation started from a broad initial distribution must, after many transitions, be concentrated
in the possibly much-smaller region that has high probability under the equilibrium distribution being
sampled. This implies that the random map determined by the random numbers underlying the Markov
chain transitions must be contractive  for a nite state space, it must map a large set of states to
a smaller set of states, and for a continuous state space, it must map a set of large volume to a set
of smaller volume. This contractive property underlies the ability to couple a set of chains so that
eventually they all coalesce to the same state, as is exploited by methods such as coupling from the
past (Propp and Wilson, 1996) and circular coupling (Neal, 1999/2002).

1

In this paper, I show that with a simple extension of the state space this contractive behaviour can be
converted to a non-contractive map  which is a permutation when this extended state space is nite,
or a one-to-one map that preserves volume when the extended state space is continuous. This result
was suggested by the volume-preserving property of Hamiltonian dynamics (see Neal, 2010), which
can be used to dene an importance sampling procedure based on annealing (Neal, 2005).
I expect
that the result in this paper can be applied to produce similar procedures that combine annealing and
importance sampling using other MCMC techniques. However, I will leave that for future work, and
instead present two simpler applications of what I will call permutation MCMC.

One application is to parallel simulation from many initial states using a single stream of random
numbers. Using a single random number stream for all parallel chains may reduce the computational
cost, perhaps especially if the parallelism takes the form of vector operations. (At worst, it costs the
same as using multiple streams, if due to high communication cost it is fastest to compute the same
stream separately in each processor.) Using a single stream also avoids the issue of how to set up multiple
streams that are unrelated, a problem that is discussed, for example, by Wu and Huang (2006).

However, some ways of using a single random number stream to dene transitions in parallel chains
lead to the chains coalescing to the same state, or to states that approach each other increasingly closely,
eliminating the benet of multiple chains in producing better estimates. I will demonstrate that this is
avoided when transitions are dened as random permutations, or as random volume-preserving maps.

A second application is to improving importance sampling, in which expectations with respect to
some distribution of interest are found using points drawn from some approximating distribution that
is easier to sample from. This method produces good results only when the importance sampling
distribution is a suciently good approximation, and most crucially does not give very low probability
to regions that have signicant probability under the distribution of interest. This can be hard to
guarantee in high-dimensional problems. We might improve an importance sampling distribution that
is close to being adequate  in the sense that it at least has substantial overlap with the distribution of
interest  by performing some number of transitions of a Markov chain that leaves the distribution of
interest invariant starting from a point drawn from the original importance sampling distribution. This
will improve the approximation even if the random numbers used to simulate this Markov chain are
xed, provided we choose the number of transitions randomly, so that the nal importance sampling
distribution is a mixture of distributions after varying numbers of transitions. With standard methods
of simulation, however, computing the importance sampling probabilities (or densities), as needed to
nd appropriate weights, will often be infeasible, because the same nal point might be produced from
several initial points (or for a continuous distribution, an unknown change in volume may alter the
densities).
I will show how this problem can be bypassed by viewing the transitions as applying a
random permutation (or volume-preserving map), for which the probability (or density) of the nal
point is the same as that of the initial point.

Recently, Murray and Elliott (2012) independently devised MCMC simulation methods equivalent or
similar to some of the methods I present below, though without additional variables needed to produce
a volume-preserving map. Their aim was to nd a simulation method that is insensitive to dependence
in the stream of random numbers used, or even to whether they are actually random. I conclude this
paper by also discussing what permutation MCMC says about the role of randomness, and how MCMC
eciency might be improved by using permutation MCMC with non-random or quasi-random numbers.

The programs and scripts used for the experiments in this paper are available from my web page.

2

2 Viewing MCMC for a uniform distribution as a random permutation

I will begin with the simple case of a Markov chain that samples from the uniform distribution on some
nite state space. In the following sections, I generalize to other discrete and continuous distributions.
Consider a Markov chain on some nite state space, X , which we can take to be {0, . . . , M1}. Let
the probability of this chain transitioning to state x when the current state is x be T (x, x), and for
the moment assume these transition probabilities are integer multiples of 1/Q, for some integer Q. We
wish to use this chain to sample from the uniform distribution on X , so T will be chosen to leave this
uniform distribution invariant  that is,

(1/M ) T (x, x) = 1/M

(1)

XxX

If we view T as a matrix, this condition is equivalent to all its columns (as well as all its rows) summing
to one.

A standard way to simulate a realization, x0, x1, x2, . . . of this chain, starting from some state x0, is
to draw u0, u1, u2, . . . independently from the uniform distribution on U = {0, 1, . . . , Q1} and then
set

xi+1 = maxnx : Q

x1Xx=0

T (xi, x)  uio

(2)

The rst step in converting this simulation to a random permutation is to extend the state space
to X  U . We then draw a value for u0 uniformly from U . Subsequent transitions from (x0, u0) are
dened using s0, s1, s2, . . ., which are independently drawn uniformly from U . (We will see below that
s0, . . . , sn specify a random permutation mapping (x0, u0) to (xn, un).) From the state (xi, ui), xi+1 is
derived from xi and ui as in equation (2) above, and ui+1 is derived from xi, ui, si, and xi+1 as follows:

ui+1 = si + ui  Q

T (xi, x) + Q

xi+11Xx=0

xi1Xx=0 eT (xi+1, x) (mod Q)

(3)

To see informally the rationale for this, note that the terms on the right other than si dene a value

where eT (x, x) = T (x, x) are the transition probabilities for the reversed chain.
for u that would lead back to xi if an equation analogous to (2) were applied with T replaced by eT .

This part of the map from (xi, ui) to (xi+1, ui+1) is therefore a permutation. Adding si modulo Q is a
random circular permutation, so the full map from (xi, ui) to (xi+1, ui+1) is a random permutation as
well. For any n > 0, the map from (x0, u0) to (xn, un), being a composition of random permutations, is
also a random permutation.

Furthermore, if we look at only a single realization of the chain, setting ui+1 to an independent
random si plus anything (mod Q) has the same eect as setting ui+1 independently at random, so the
joint distribution of (x1, u1), (x2, u2), . . . is the same as for the standard method of simulation.

Appendix A shows in detail that the map (xi, ui)  (xi+1, ui+1) dened by equations (2) and (3) is

a permutation, by explicitly exhibiting the inverse map.

3

Here is a matrix of transition probabilities for a simple example with M = 4 states:

T = 

2/3 1/3

0

1/3 1/3 1/3

0

0

0

0

1/3 1/3 1/3

0

1/3 2/3



(4)

All the columns above sum to one, so these transitions leave the uniform distribution on X =
{0, 1, 2, 3, 4} invariant. Since all the transition probabilities are multiples of 1/3, we can set Q = 3, and

hence U = {0, 1, 2}. Note that these transitions are reversible  that is, eT (x, x) = T (x, x) = T (x, x).

The permutation maps from (xi, ui) to (xi+1, ui+1) when si has each of its possible values are shown

here:

x

x

x

0

1

2

3

0

1

2

3

0

1

2

3

0

u

1

2

0

u

1

2

0

u

1

2

si = 0

si = 1

si = 2

In these diagrams, the array of circles represents all possible (x, u) pairs, and the arrows show how such a
pair for (xi, ui) is mapped to (xi+1, ui+1). For example, the arrow out of the state with xi = 1 and ui = 2
goes to a state with xi+1 = 2, regardless of the value of si, since Q (T (1, 0)+T (1, 1)) = 3(2/3) = 2  2,
so the maximum in equation (2) will be x = 2. When si = 0, equation (3) will nd a value for ui+1
that would lead back to the state xi = 1 starting from xi+1 = 2, which requires that ui+1 be at least
Q T (2, 0) = 0, to which must be added the amount by which ui was greater than the minimum needed
for the transition to xi+1 = 2 to be taken (essential to avoid two states mapping to the same new state),
which in this case is 0. The result is that the diagram for si = 0 has the transition (1, 2)  (2, 0).

The maps for si 6= 0 can be obtained from the map for si = 0 by circularly shifting the ui+1. Note
that when, as here, the transitions are reversible, the diagram for si = 0 will consist entirely of single
states with arrows pointing to themselves and pairs of states connected by arrows both ways.

For comparison, here are the maps produced by the T dened in equation (4) when equation (3) is

replaced by ui+1 = si + ui (mod Q):

x

x

x

0

1

2

3

0

1

2

3

0

1

2

3

0

u

1

2

0

u

1

2

0

u

1

2

si = 0

si = 1

si = 2

Some states have zero or two incoming arrows, so these are clearly not permutations.

4

Below, are the transition probabilities, T , for a non-reversible Markov chain with M = 4 states
that leaves the uniform distribution on X = {0, 1, 2, 3, 4} invariant, along with the reverse transition

probabilities, eT , found by transposing T :

1/2 1/2

0

0

T = 

1/4 1/4 1/4 1/4

0

0

1/2 1/2

1/4 1/4 1/4 1/4

,



eT = 

1/2 1/4

1/2 1/4

0

0

1/4

1/4

0

0

1/4 1/2 1/4

1/4 1/2 1/4



(5)

Since all transition probabilities are multiples of 1/4, we can set Q = 4, so that U = {0, 1, 2, 3}. The
permutation maps for this example from (xi, ui) to (xi+1, ui+1) for each si are as follows:

x

0

1

2

3

x

0

1

2

3

si = 0 :

u

si = 2 :

u

0

1

2

3

0

1

2

3

x

0

1

2

3

si = 1 :

u

si = 3 :

u

0

1

2

3

0

1

2

3

x

0

1

2

3

In this example, the value of xi+1 that follows (xi, ui) is determined using equation (2) in the same
way as for a reversible chain, but the value of ui+1 when si = 0 is not one that would lead back to
xi if T were applied starting from xi+1, but is rather a value that would lead back to xi if the reverse

transition, eT , were applied.

In real MCMC applications, unlike these examples, the state space is enormous, and transition
probabilities are dened algorithmically, rather than via an explicit table. One may then ask whether
the computation of xi+1 and ui+1 from xi and ui according to equations (2) and (3) is feasible.
I
will defer consideration of this issue to the following sections, in which the method is generalized to
non-uniform distributions and to continuous state spaces.

5

3 Generalization to non-uniform discrete distributions

As a rst step in generalizing the result in the previous section, let us consider a distribution on
X = {0, . . . , M  1} with probabilities proportional to a function (x) whose values are all integer
(We may not know the constant of proportionality,
multiples of 1/P , for some positive integer P .

1/Px (x).) This distribution can be obtained as the marginal distribution on X obtained from a

uniform joint distribution on the following subset of X  Y, where Y = {0, . . . , P 1}:

Z = { (x, y) : 0  y < P (x)}

(6)

The cardinality of Z is M + = PPx

(x).

This construction is analogous to what is done for slice sampling MCMC methods (Neal, 2003), in
which Markov transitions are dened on this extended state space. Here, I will assume that our MCMC
method is dened in terms of transitions on X , with the introduction of the extended space X Y being
only a device to allow these transitions to be expressed as permutations. (However, transitions dened
on X  Y could be accommodated if desired.)

Suppose that we have dened a Markov chain on X , with transition probabilities T (x, x), all integer
multiples of 1/Q, that leaves the distribution (x) invariant. We can dene a Markov chain on Z that
leaves the uniform distribution on Z invariant, with transition probabilities as follows:

T (x, x)
P (x)
0

if 0  y < P (x)
otherwise

(7)

(8)

1

M +

T +((x, y), (x, y)) = 
M +Xx

1

These transition probabilities are all integer multiples of 1/Q+, where Q+ = (max

P (x))! Q.
To conrm that T + leaves the uniform distribution invariant, note that for any (x, y) in Z,
X(x,y)Z
(x)T (x, x) =

1
M + T +((x, y), (x, y)) =

(x)Xx

T (x, x)
P (x)

1

M +

1

P (x)

=

x

The eect of applying the original transitions, T , starting from some initial state, x0, can be du-
plicated by drawing y0 uniformly from {0, . . . , P (x0)} and then using the transitions T + to simulate
states (x1, y1), (x2, y2), . . . The resulting distribution for x1, x2, . . . is the same as if T were applied
starting with x0  the transition from (xi, yi) to (xi+1, yi+1) dened by T + ignores yi, and gives equal
probabilities of T (xi, xi+1)/P (xi+1) to P (xi+1) values of y, so the total probability for a value xi+1
to follow xi is T (xi, xi+1).

An MCMC simulation using T that samples from X with probabilities given by  can therefore be
replaced by a simulation using T + that samples from Z with uniform probabilities. This simulation on
the extended state space Z can be expressed as a random permutation, as described in Section 2. To do
this, we must decide on an ordering of states in Z. In this paper, I will use a lexicographical order (rst
on x, then on y), in which an (x, y) pair in Z is associated with a label, x+, in X + = {0, 1, . . . , M +1},
according to the following map:

X +(x, y) = y + P

(x)

x1Xx=0

6

(9)

The inverse of this map takes x+ to (X(x+), Y (x+)), where

X(x+) = maxnx : P

x1Xx=0

(x)  x+o,

Y (x+) = x+  P

X(x+)1Xx=0

(x)

(10)

