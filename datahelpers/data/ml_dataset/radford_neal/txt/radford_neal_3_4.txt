2
1
0
2

 
c
e
D
6
2

 

 
 
]
L
M

.
t
a
t
s
[
 
 

1
v
6
4
2
6

.

2
1
2
1
:
v
i
X
r
a

Gaussian Process Regression with

Heteroscedastic or Non-Gaussian Residuals

Chunyi Wang

Department of Statistical Sciences

University of Toronto

chunyi@utstat.toronto.edu

Radford M. Neal

Department of Statistical Sciences and

Department of Computer Science

University of Toronto

radford@utstat.toronto.edu

26 December 2012

Abstract Gaussian Process (GP) regression models typically assume that residuals are Gaussian
and have the same variance for all observations. However, applications with input-dependent noise
(heteroscedastic residuals) frequently arise in practice, as do applications in which the residuals
do not have a Gaussian distribution. In this paper, we propose a GP Regression model with a
latent variable that serves as an additional unobserved covariate for the regression. This model
(which we call GPLC) allows for heteroscedasticity since it allows the function to have a changing
partial derivative with respect to this unobserved covariate. With a suitable covariance function,
our GPLC model can handle (a) Gaussian residuals with input-dependent variance, or (b) non-
Gaussian residuals with input-dependent variance, or (c) Gaussian residuals with constant variance.
We compare our model, using synthetic datasets, with a model proposed by Goldberg, Williams
and Bishop (1998), which we refer to as GPLV, which only deals with case (a), as well as a standard
GP model which can handle only case (c). Markov Chain Monte Carlo methods are developed for
both modelsl. Experiments show that when the data is heteroscedastic, both GPLC and GPLV
give better results (smaller mean squared error and negative log-probability density) than standard
GP regression. In addition, when the residual are Gaussian, our GPLC model is generally nearly as
good as GPLV, while when the residuals are non-Gaussian, our GPLC model is better than GPLV.

1

Introduction

Gaussian Process (GP) regression models are popular in the machine learning community (see,
for example, the text by Rasmussen and Williams (2006)), perhaps mainly because these models
are very ﬂexible — one can choose from many covariance functions to achieve diﬀerent degrees of
smoothness or diﬀerent degrees of additive structure, the parameters of such a covariance function
can be automatically determined from the data. However, standard GP regression models typi-
cally assume that the residuals have i.i.d. Gaussian distributions that do not depend on the input
covariates, though in many applications, the variances of the residuals do depend on the inputs,
and the distributions of the residuals are not necessarily Gaussian. In this paper, we present a
GP regression model which can deal with input-dependent residuals. This model includes a latent
variable with a ﬁxed distribution as an unobserved input covariate. When the partial derivative of

1

the response with respect to this unobserved covariate changes across observations, the variance of
the residuals will change. When the latent variable is transformed non-linearly, the residuals will
be non-Gaussian. We call this the “Gaussian Process with a Latent Covariate” (GPLC) regression
model.

In Section 2 below, we give the details of this model as well as the standard GP model (“STD”)
and a model due to Goldberg, Williams and Bishop (1998), which we call the “Gaussian Process
regression with a Latent Variance” (GPLV) model, and we discuss the relationships/equivalencies
between these models. We describe computational methods in Section 3, and present the results of
these models on various synthetic datasets in Section 4.

2 The models

We look at non-linear regression problems, where the aim is to ﬁnd the association between a
vector of covariates x and a response y using n observed pairs (x1, y1), ..., (xn, yn), and then make
predictions for yn+1, yn+2, ... corresponding to xn+1, xn+2...:

yi = f (xi) + i

(1)

The covariate xi is a vector of length p, and the correspoding response yi is a scalar.

2.1 The standard GP regression model

In the standard Gaussian process regression model The random residuals, i’s, are assumed to have
i.i.d. Gaussian distributions with mean 0 and constant variance σ2.

Bayesian GP models assume that the noise-free function f comes from a Gaussian Process which
has prior mean function zero and some speciﬁed covariance function. Note that a zero mean prior
is not a requirement — we could specify a non-zero prior mean function m(x) if we have a priori
knowledge of the mean structure. Using a zero mean prior just reﬂects our prior knowledge that the
function is equally likely to be positive or negative. It doesn’t mean we believe the actual function
will have an average over its domain of zero.

The covariance functions can be fully-speciﬁed functions, but common practice is to specify a
covariance function with unknown hyperparameters, and then estimate the hyperparameters from
the data. Given the values of the hyperparameters, the responses, y, in a set of cases have a
multivariate Gaussian distribution with zero mean and a covariance matrix given by

(2)
where δij = 0 when i (cid:54)= j and δii = 1, and K(xi, xj) is the covariance function of f . Any function
that always leads to a positive semi-deﬁnite covariance matrix can be used as a covariance function.
One example is the squared exponential covariance function with isotropic length-scale:

Cov(yi, yj) = K(xi, xj) + δijσ2

(cid:18)

−(cid:107)xi − xj(cid:107)2

ρ2

(cid:19)

K(xi, xj) = c2 + η2 exp

(3)

Where c is a suitably chosen constant, and η, ρ and σ are hyperparameters. η2 is sometimes
referred to as the “signal variance”, which controls the magnitude of variation of f ; σ2 is the

2

residual variance; ρ is the length scale parameter for the input covariates. We can also assign a
diﬀerent length scale parameter to each covariate, which leads to the squared exponential covariance
function with automatic relevance determination (ARD):

K(xi, xj) = c2 + η2 exp

(cid:32)
− p(cid:88)

k=1

(xik − xjk)2

ρ2
k

(cid:33)

We will use squared exponential forms of covariance function from (3) or (4) in most of this paper.
If the values of the hyperparameters are known, then the predictive distribution of y∗ for a test
case x∗ based on observed values x = (x1, ..., xn) and y = (y1, ..., yn) is Gaussian with the following
mean and variance:

E(y∗|x, y, x∗) = kT C−1y

Var(y∗|x, y, x∗) = v − kT C−1k

(4)

(5)

(6)

(7)

In the equations above, k is the vector of covariances between y∗ and each of y1, . . . , yn, C is the
covariance matrix of the observed y, and v is the prior variance of y∗, which is Cov(y∗, y∗) from (2).

When the values of the hyperparameters (denoted as θ) are unknown and therefore have to be
estimated from the data, we put priors on them (typically independent Gaussian priors on the loga-
rithm of each hyperparameter), and obtain the posterior distribution p(θ|x, y) ∝ N (y|0, C(θ))p(θ).
The predictive mean of y can then be computed by integrating over the posterior distribution of
the hyperparameters:

E(y∗|x, y, x∗) =

k(θ)T C(θ)−1y · p(θ|x, y)dθ

Letting E = E(y∗|x, y, x∗), the predictive variance is given by
Var(y∗|x, y, x∗) = Eθ[Var(y∗|x, y, x∗, θ)] + Varθ[E(y∗|x, y, x∗, θ)]

(cid:2)v(θ) − k(θ)T C(θ)−1k(θ)(cid:3) p(θ|x, y)dθ +

(cid:90)

(cid:90)

=

(cid:2)k(θ)T C(θ)−1y − E(cid:3)2

(8)
p(θ|x, y)dθ

Θ

Θ

2.2 A GP regression model with a latent covariate

In this paper, we consider adding a latent variable w into the model as an unobserved input. The
regression equation then becomes

yi = g(xi, wi) + ζi.

(9)

In this setting, the latent value wi has some known random distribution, the same for all i. If we
view g(xi, wi) as a function of xi only, its value is random, due to the randomness of wi. So g is
not the regression function giving the expected value of y for a given value of x — that is given by
the averge value of g over all w, which we write as f (x) = E(y|x):

(cid:90)

Θ

(cid:90)

f (x) =

g(x, w)p(w)dw

(10)

where p(w) is the probability density of w. Note that (10) implies that the term ζi, which we assume
has i.i.d. Gaussian distribution with constant variance, is not the real residual of the regression,
since

ζi = yi − g(xi, wi) (cid:54)= yi − f (xi) = i

3

where i is the true residual. We put ζi in the regression for two reasons. First, the covariance
function for g can sometimes produce nearly singular covariance matrices, that are computationally
non-invertible because of round-oﬀ error. Adding a small diagonal term can avoid the computational
issue without signiﬁcantly changing the properties of the covariance matrix. Secondly, the function g
will produce a probability density function for  that has singularities at points where the derivative
of g with respect to w is zero, which is probably not desired in most applications. Adding a jitter
term ζi smooths away such singularities.

We will model g(x, w) using a GP with a squared exponential covariance function with ARD,

for which the covariance between training cases i and j, with latent values wi and wj, is

Cov(yi, yj) = K((xi, wi), (xj, wj)) + σ2δij

= c2 + η2 exp

(xik − xjk)2

ρ2
k

− (wi − wj)2

ρ2
p+1

(cid:32)
− p(cid:88)

k=1

(cid:33)

(11)

+ σ2δij

We choose independent standard normals as the distributions for w1, ..., wn. The mean for the
wi is chosen to be zero because the squared exponential function is stationary, and hence only the
diﬀerence between wi and wj matters. The variance of the wi is ﬁxed at 1 because the eﬀect of a
change of scale of wi can be achieved instead by a change in the length scale parameter lp+1.

We write p(w) for the density for the vector of latent variables w, and p(θ) for the prior density of
all the hyperparameters (denoted as a vector θ). The posterior joint density for the latent variables
and the hyperparameters is

p(w, θ|x, y) = N (y|0, C(θ, w))p(w)p(θ)

(12)
where N (a|µ, Σ) denotes the probability density of a multivariate Gaussian distribution with mean
µ and covariance matrix Σ, evaluated at a. C(θ, w) is the covariance matrix of y, which depends
on θ and w.

The prediction formulas for GPLC models are similar to (7) and (8). In addition to averaging
over the hyperparameters, we also have to average over the posterior distribution of the latent
variables w = (w1, ..., wn):

E(y∗|x, y, x∗) =

k(θ, w, w∗)T C(θ, w)−1y p(θ, w|x, y)dθdw

(cid:90)

(cid:90)

W

Θ

(13)

(14)

Var(y∗|x, y, x∗) = Eθ,w[Var(y∗|x, y, x∗, θ, w)] + Varθ,w[E(y∗|x, y, x∗, θ, w)]

(cid:2)v(θ, w∗) − k(θ, w, w∗)T C(θ, w)−1k(θ, w, w∗)(cid:3) p(w, θ|x, y)dθdwdw∗
(cid:90)
(cid:2)k(θ, w, w∗)T C(θ, w)−1y − E(cid:3)2

p(w, θ|x, y)dθdwdw∗

(cid:90)

=

(cid:90)
(cid:90)

Θ

W

+

W

Θ

where E = E(y∗|x, y, x∗)

Note that the vector of covariances of the response in a test case with the responses in training
cases, written as k(θ, w) in (13) and (14), depends on, w∗, the latent value for the test case. Since
we do not observe w∗, we randomly draw values from the prior distribution of w∗, compute the

4

Figure 1: How GPLC can produce a non-Gaussian distribution of residuals.

corresponding expectation or variance and take the average. Similarly, the prior variance for the
response in a test case, written v(θ, w∗) above, depends in general on w∗ (though not for the squared
exponential covariance function that we use in this paper).

To see that this model allows residual variances to depend on x, and that the residuals can have

non-Gaussian distributions, we compute the Taylor-expansion of g(x, w) at w = 0:

g(x, w) = g(x, 0) + g(cid:48)

2(x, 0)w +

w2
2

g(cid:48)(cid:48)
2 (x, 0) + ...

(15)

2 and g(cid:48)(cid:48)

where g(cid:48)
2 denotes the ﬁrst and second order partial derivatives of g with respect to its second
argument (w). If we can ignore the second and higher order terms, i.e. the linear approximation is
good enough, then the response given x is Gaussian, and

Var[g(x, w)] ≈ 0 + [g(cid:48)

2(x, 0)]2Var(w) = [g(cid:48)

2(x, 0)]2

(16)

which depends on x when g(cid:48)
2(x, 0) depends on x (which usually is the case when g is drawn from
a GP prior). Thus in this case, the model produces Gaussian residuals with input-dependent
variances.

If the high-order terms in (15) cannot be ignored, then the model will have non-Gaussian, input-
dependent residuals. For example, consider g(x, w) = (x + w)2, where the second order term in w
clearly cannot be ignored. Conditional on x, g(x, w) follows a non-central Chi-Squared distribution.
Figure 1 illustrates that at x = 2, an unobserved normally distributed input w translates into a
non-Gaussian output y. Note that for demonstration purposes the density curves of w and y are
not to scale (since the scales on the x-axis and y-axis are diﬀerent).

Figure 2 illustrates how an unobserved covariate can produce heteroscedasticity. The data in the
left plot are generated from a GP, with xi drawn uniformly from [0,5] and wi drawn from N (0, 1).
The hyperparameters of the squared exponential covariance function were set to η = 3, ρx = 0.8,
and ρw = 3. Supposing we only observe (x, y), the data is clearly heteroscedastistic, since the
spread of y against x changes when x changes. For instance, the spread of y looks much bigger

5

012345051015202530xyy=(x+w)2  E(Y|X=x)Density of w+2Density of yFigure 2: Heteroscedasticity produced by an unobserved covariate. The left plot shows a sample of
x and y from the GP prior, with w not shown. The right plot shows 19 dashed curves of g(xi, wi)
(for the same g as on the left) where the wi are ﬁxed to the same value, equal to the 5ith percentile
of the standard normal for the ith curve (i.e. 1 to 19).

when x is around 1.8 than it is when x is near 3.5. We also notice that the distribution of the
residuals can’t be Gaussian, as, for instance, we see strong skewness near x = 5. These plots show
that if an important input quantity is not observed, the function values based only on the observed
inputs will in general be heteroscedastic, and non-Gaussian (even if the noise term ζi is Gaussian).

Note that although an unobserved input quantity will create heteroscedasticity, our model can
work well even if no such quantity really exists. The model can be seen as just using the latent
variable as a mathematical trick, to produce changing residual variances. Whether or not there
really exists an unobserved input quantity doesn’t matter (though in practice, unobserved quantities
often do exist).

2.3 A GP regression model with a latent variance

Goldberg, Williams and Bishop (1998) proposed a GP treatment of regression with input-dependent
residuals. In their scheme, a “main” GP models the mean of the response just like a standard GP
regression model, except the residuals are not assumed to have constant variance — a secondary
GP is used to model the logarithm of the standard deviation of the noise, which depends on the
input. The regression equation looks the same as in (1):

yi = f (xi) + i

(17)

but the residuals 1, ...n are do not have the same variance — instead, the logarithm of the standard
deviation zi = log SD[(xi)] depends on xi through:

zi = r(xi) + Ji

(18)

6

012345−4−3−2−10123456xyy=f(x,w)  random ww=0012345−3−2−10123456xyy=f(x,w)  w=0f (x) and r(x) are both given (independent) GP priors, with zero mean and covariance functions
Cy and Cz, which have diﬀerent hyperparameters (e.g. (ηy, ρy) and (ηz, ρz)). Ji is a Gaussian
“jitter” term (see Neal, 1997) which has i.i.d. Gaussian distribution with zero mean and standard
deviation σJ (a preset constant, usually a very small number, e.g. 10−3). Writing x = (x1, ..., xn),
y = (y1, ..., yn), θy = (ηy, ρy), θz = (ηz, ρz), and z = (z1, ..., zn), the posterior density function of
the latent values and the hyperparameters is
p(θy, θz, z|x, y) ∝ p(y|x, z, θy)p(z|x, θz)p(θy, θz) ∝ N (y|0, Cy(θy, z))N (z|0, Cz(θz))p(θy, θz)
where Cy is the covariance matrix for y (for the “main” GP), Cz is the covariance matrix for
z (for the “secondary” GP) and p(θy, θz) represents the prior density for the hyperparameters
(typically independent Gaussian priors for their logarithms). The predictive mean can be com-
puted in a similar fashion as the prediction of GPLC, but instead of averaging over w, we average
over z. To compute the covariance vector k, we need the value of zn+1, which we can sample from
p(zn+1|z1, ..., zn).

(19)

Alternatively, instead of using a GP to model the logarithm of the residuals standard deviations,
we can set the standard deviations to the absolute values of a function modeled by a GP. That is,
we let SD(i) = |zi|, with zi = r(xi). So the regression model can be written as

yi = f (xi) + r(xi)ui

(20)

where ui

iid∼ N (0, 1).

This is similar to modeling the log of the standard deviation with a GP, but it does allow the
standard devaition, |zi|, to be zero, whereas exp(zi) is always positive, and it is less likely to produce
extremely large values for the standard deviation of a residual. A more general approach is taken
by Wilson and Ghahramani (2010), who use a parameterized function to map values modeled by a
GP to residual variances, estimating the parameters from the data.

In the original paper by Goldberg et. al., a toy example was given where the hyperparameters
are all ﬁxed, with only the latent values sampled using MCMC. In this paper, we will take a full
Bayesian approach, where both the hyperparameters and the z values are sampled from (19). In
addition, we will discuss fast computation methods for this model in Section 3.

2.4 Relationships between GPLC and other models

We will show in this section that GPLC can be equivalent to the a standard GP regression model
or a GPLV model, when the covariance function is suitably speciﬁed.

Suppose the function g(x, w) in (9) has the form

g(xi, wi) = h(xi) + σwi

(21)

where wi ∼ N (0, 1).
If we only observe xi but not wi, then (21) is a regression problem with
unknown i.i.d. Gaussian residuals with mean 0 and variance σ2, which is equivalent to the standard
GP regression model (1), if we give a GP prior to h. If we specify a covariance function that produces
such a g(x, w), then if we set ζi = 0 (or equivalently, the hyperparameter σ2 = Var(ζi) = 0), our
GPLC model will be equivalent to the standard GP model. Below, we compute the covariance
between training cases i and j (with latent values wi and wj) to ﬁnd out the form of the appropriate
covariance function.

7

Let’s put a GP prior with zero mean and covariance function K1(xi, xj) on h(x). As usual, the wi
have independent N (0, 1) priors. Since the values of g(x, w) are a linear combination of independent
Gaussians, they will have a Gaussian process distribution, conditional on the hyperparameters. Now
the covariance between cases i and j is

Cov[g(xi, wi), g(xj, wj)] = E[(h(xi) + σwi)(h(xj) + σwj)]

= E[h(xi)h(xj)] + σ2wiwj
= K1(xi, xj) + σ2wiwj

Therefore, if we put a GP prior on g(x, w) with zero mean and covariance function

K[(xi, wi), (xj, wj)] = K1(xi, xj) + σ2wiwj

(22)

(23)

the results given by GPLC will be equivalent to standard GP regression with covariance function
K1. In practice, if we are willing to make the assumption that the residuals have equal variances (or
know this as a fact), this modiﬁed GPLC model is not useful, since the complexity of handling latent
variables computationally is unnecessary. However, consider a more general covariance function

where K1[(xi, wi), (xj, wj)] = exp(cid:0)−(cid:80)p
nential covariance function with ARD, and K2[(xi, wi), (xj, wj)] = (cid:80)p

k=1(xik − xjk)2/ρ2

k − (wi − wj)2/ρ2
p+1
k=1 γ2

K[(xi, wi), (xj, wj)] = K1[(xi, wi), (xj, wj)] + K2[(xi, wi), (xj, wj)]

(cid:1) is a squared expo-

(24)

p+1wiwj is
a linear covariance function with ARD. Then the covariance function (23) can be obtained as a
limiting case of (24), when ρp+1 goes to inﬁnity in K1 and γ1, ..., γp all go to zero. Therefore,
we could use this more general model, and let the data choose whether to (nearly) ﬁt the simpler
standard GP model.

kxikxjk + γ2

Similarly, if we believe that the function g(x, w) is of the form

g(x, w) = h1(x) + wh2(x)

(25)

with h1 and h2 independently having Gaussian Process priors with covariance functions K1 and
K2, we can use a GPLC model with a covariance function of the form

K[(xi, wi), (xj, wj)] = K1(xi, xj) + wiwjK2(xi, xj)

(26)

Now consider the alternative GPLV model (20): if we put independent GP priors on f (xi) and
r(xi), each with zero mean, and covariance functions K1 and K2, respectively, then model (20) is
equivalent to the modiﬁed GPLC model above with covariance function (26). The hyperparameters
of K1 of both models should have the same posterior distribution, as would the hyperparameter of
K2. Notice that the two models have diﬀerent latent variables: the latent variable in GPLC, wi, is
the value of the ith (normalized) residual; the latent variable in GPLV is zi = r(xi), which is plus
or minus the standard deviation of the ith residual.

3 Computation

Bayesian inference for GP models is based on the posterior distribution of the hyperparameters and
the latent variables. Unfortunately this distribution is seldom analytically tractable. We usually
use Markov Chain Monte Carlo to sample the hyperparameters and the latent values from their
posterior distribution.

8

3.1 Overview of methods

Common choices of MCMC method include the classic Metropolis algorithm (Metropolis et. al,
1953) and slice sampling (Neal, 2003). The Metropolis sampler is easy to implement, but for
high-dimensional distributions, it is generally hard to tune. We can update all the parameters at
each iteration using a multivariate proposal distribution (e.g. N (0, D) where D is diagonal), or
we can update one parameter at a time based on a univariate proposal distribution. Either way,
to contruct an eﬃcient MCMC method we have to assign an appropriate value for the proposal
standard deviation (a “tuning parameter”) for each hyperparameter or latent variable so that the
acceptance rate on each variable is neither too big nor too small (generally, between 20% and 80%
is acceptable, though the optimal value is typically unknown). There is generally no good way to
ﬁnd out what tuning parameter value is the best for each variable other than trial and error. For
high-dimensional problems, tuning the chain is very diﬃcult. Using squared exponential covariance
function, our model GPLC has D = n + p + 3 variables (including hyperparameters and latent
variables), and the GPLV model has D = n + 2p + 2 variables.

Slice sampling, on the other hand, although slightly more diﬃcult to implement, is relatively
easier to tune. It does also have tuning parameters (one can control the step-out size and the number
of step-outs), but the performance of the chain is not very sensitive to the tuning parameters.
Figure 2.7 of Thompson (2011) demonstrates that step-out sizes from 1 to 1000 all lead to similar
computation time, while a change in proposal standard deviation from 1 to 1000 for a Metropolis
sampler can result in a MCMC which is 10 to 100 times slower. In this paper, we use univariate
step-out slice sampling for regular GP regression models and GPLC models. For GPLV, since the
latent values are highly correlated, regular Metropolis and slice samplers do not work well. We will
give a modiﬁed Metropolis sampler than works better than both of these simpler samplers.

3.2 Major computations for GP models

Evaluating the log of the posterior probability density of a GP model is typically dominated by com-
putating the covariance matrix, C, and ﬁnding the Cholesky decomposition of C, with complexities
pn2 and n3, respectively.

For both standard GP models and GPLV models, updates of most of the hyperparameters require
that the covariance matrix C be recomputed, and hence also the Cholesky decomposition (denoted
as chol(C)). For GPLC, when the ith latent variable is updated, most of C is unchanged, except for
the ith row and ith column. This change requires a rank-n update on the Cholesky decomposition,
which is almost as costly as as ﬁnding the Cholesky decomposition for a new C.

Things are slightly more complicated for GPLV, since the model consists of two GPs, with two
covariance matrices. When one of the hyperparameters for the main GP (denoted as θy) is changed,
the covariance matrix for the main GP, Cy, is changed, and thus chol(Cy) has to be recomputed.
However, Cz, the covariance matrix for the secondary GP, remain unchanged. When one of θz, the
hyperparameters of the secondary GP is changed, Cy (and chol(Cy)) remain unchanged, but Cz and
chol(Cz) must be recomputed. When one of the latent values, say the ith, is changed, Cz remains
unchanged as it only depends on x and θz, but the ith entry on the diagonal of Cy is changed. This
minor change to Cy requires only a rank-1 update (Sherman et. al, 1950), with complexity n2.

We list the major operations for the GP models discussed in this paper in Table 1.

9

one hyperparameter

one latent variable

STD

GPLC

GPLV

Operation
Complexity

# of such operations

Operation
Complexity

# of such operations

Operation
Complexity

# of such operations

C, chol(C)

pn2, n3
p + 2

C, chol(C)

pn2, n3
p + 3

Cy, chol(Cy) or Cz, chol(Cz)

pn2, n3
2p + 2

-
-
-

1/n of C, all of chol(C)

pn, n3

n

rank-1 update Cy

n2
n

Table 1: Major operations needed when hyperparameters and latent variables change in GP models.

3.3 A modiﬁed Metropolis sampler for GPLV

Neal (1998) describes a method for updating latent variables in a GP model that uses a proposal
distribution that takes into account the correlation information. This method proposes to change
the current latent values, z, to a z(cid:48) obtained by

z(cid:48) = (1 − a2)1/2z + aLu

(27)

where a is a small constant (a tuning parameter, typically slightly greater than zero), L is the
lower triangular Cholesky decomposition for Cz, the covariance matrix for the N (0, Cz(θz)) prior
for z, and u is a random vector of i.i.d. standard normal values. The transition from z to z(cid:48) is
reversible, and leaves the prior for z invariant. Because of this, the Metropolis-Hastings acceptance
probability for these proposals depends only on the ratio of likelihoods for z(cid:48) and z.

We will use this method to develop a sampling strategy for GPLV. Recall the unnormalized

posterior distribution for the hyperparameters and latent values is given by
p(θy, θz, z|x, y) = N (y|0, Cy(θy, z))N (z|0, Cz(θz))p(θy, θz)

y, θ(cid:48)

To obtain new values θ(cid:48)

z and z(cid:48) based on current values θy, θz and z, we can do the following:
1. For each of the hyperparameters in θy (i.e. those associated with the “main” GP), do an
update of this hyperparameter (for instance a Metropolis or slice sampling update). Notice
that for each of these updates we need to recompute chol(Cy), but not chol(Cz), since Cz
does not depend on θy.

2. For each of the hyperparameters in θz (i.e. those for the “secondary” GP):

(a) Do an update of this hyperparameter (e.g. with Metropolis or slice sampling). We need

to recompute chol(Cz) for this, but not chol(Cy), since Cy does not depend on θz.

(b) Update all of z with the proposal described in (27). We need to recompute chol(Cy) to
do this, but not chol(Cz), since Cz depends only on θz but not z. We repeat this step
for m times (a tuning parameter) before moving to the next hyperparameter in θz.

In this scheme, the hyperparameters θy and θz are not highly correlated and hence are relatively
easy to sample using the Metropolis algorithm. The latent variables z are highly correlated. Because
updating the z-values is hard, we try to update them as much as possible. Notice that Cz depends
only on x and θz, so a change of z will not result in a change of Cz. Hence once we update a
component of θz (and obtain a new Cz), it makes sense to do m > 1 updates on z before updating
another z-hyperparameter.

10

4 Experiments

We will compare our GPLC model with Goldberg et. al.’s GPLV model, and with a standard GP
regression model having Gaussian residuals of constant variance.

4.1 Datasets

We use four synthetic datasets, with one or three covariates, and Gaussian or non-Gaussian resid-
uals, as summarized below:

Dataset

U1
U2
M1
M2

p True function Residual SD Residual distribution
1
1
3
3

f (x)
f (x)
g(x)
g(x)

r(x)
r(x)
s(x)
s(x)

non-Gaussian

Gaussian

non-Gaussian

Gaussian

Datasets U1 and U2 both have one covariate, which is uniformly drawn from [0,1], and the true

function is

f (xi) = [1 + sin(4xi)]1.1

For U1, the response yi = f (xi) + i is contaminated with a Gaussian noise, i, with input-

dependent standard deviation

SD(i) = r(xi) = 0.2 + 0.3 exp[−30(xi − 0.5)2]

tion, EV (µ, σ) (see Leadbetter et. al., 2011), with probability density (1/σ)e(ω−µ)/σ exp(cid:0)−e(ω−µ)/σ(cid:1).

For U2, the response has a non-Gaussian residual, ω, with a location-scale extreme value distribu-

The mean of ω is E(ω) = µ + σγ, where γ = 0.5772 . . . is the Euler’s constant. The variance of ω
is Var(ω) = π2σ2/6. We translate and scale the EV residuals so that their mean is zero and their
standard deviation is r(x) (same as those of  in U1). The density curve of a EV residual with
mean 0 and variance 1 is shown in Figure 3.

Figure 3: Density curve of extreme value residual with mean 0 and variance 1.

11

−8−6−4−202400.050.10.150.20.250.30.350.40.450.5xf(x)Extreme Value DistributionDatasets M1 and M2 both have three independent, standard normal covariates, denoted as

x = (x1, x2, x3). The true function is

g(x) = [1 + sin(x1/1.5 + 2)]0.9 − [1 + sin(x2/2 + x3/3 − 2)]1.5

As we did for U1 and U2, we add Gaussian residuals to M1 and extreme value residuals to M2.
For both M1 and M2, the standard deviations of these residuals depend on the input covariates as
follows:

s(x) = 0.1 + 0.4 exp[−0.2(x1 − 1)2 − 0.3(x2 − 2)2] + 0.3 exp[−0.3(x3 + 2)2]

4.2 Predictive performance of the methods

For each dataset (U1, U2, M1, and M2), we randomly generated 10 diﬀerent training sets (using
the same program but diﬀerent random seeds), each with n = 100 observations, and a test dataset
with N = 5000 observations. We obtained MCMC samples using the methods described in the
previous section, dropping the initial 1/4 samples as burn-in, and used them to make predictions
for the test cases.

In order to evaluate how well each model does in terms of the mean of its predictive distribution,
we computed the mean squared error (MSE) with respect to the true function values f (xi) as
follows

where ˆy1, ..., ˆyN ) are the predicted responses for test cases. We also computed the average negative
log-probability density (NLPD) of the responses in the test cases, as follows

(28)

(29)

MSE(ˆy) =

1
N

(ˆyi − f (xi))2

i=1

N(cid:88)
 1

M

N(cid:88)

i=1

log

M(cid:88)

j=1

NLPD(ˆy) = − 1
N

ψ(y(i)|ˆµij, ˆσ2
ij)



where ψ(·|µ, σ2) denotes the probability density for N (µ, σ2), ˆµij, ˆσ2
ij is the predictive mean and
variance for test case y(i) using the hyperparameters and latent variables from the jth MCMC
iteration, and M is the number of MCMC samples used for prediction.

We give pairwise comparison of the MSE and the NLPD in Figures 4, 5, 6, and 7. These plots
show that both GPLC and GPLV give smaller NLPD values than the standard GP model for all
datasets. At least for the multivariate datasets, GPLC and GPLV also give smaller MSEs than the
standard GP model. This shows that both GPLC and GPLV can be eﬀective for heteroscedastic
regression problems.

Comparing GPLC and GPLV, we notice that for datasets with Gaussian residuals, GPLC is
almost as good as GPLV (except for the NLPDs for U1, where GPLV gives smaller values 8 out of
10 times), while for non-Gaussian residuals, GPLC is the clear winner, giving MSEs and NLPDs
that are smaller than for GPLV most of the time. The numerical MSE and NLPD values are listed
in the Appendix.

12

Negative Log Probability Density

Mean Squared Error

Figure 4: Dataset U1: Pairwise comparison of methods using NLPD(Left) and MSE(right)

Negative Log Probability Density

Mean Squared Error

Figure 5: Dataset U2: Pairwise comparison of methods using NLPD(Left) and MSE(right)

13

STD0.20.30.20.250.30.35STDGPLC0.20.30.20.250.30.35GPLCSTD0.20.30.20.250.30.35STDGPLV0.20.30.20.250.30.35GPLVSTDGPLC0.20.250.30.20.250.3GPLCGPLV0.20.250.30.20.250.3GPLVGPLCGPLVSTD51015x 10−351015x 10−3STDGPLC51015x 10−351015x 10−3GPLCSTD2468101214x 10−32468101214x 10−3STDGPLV2468101214x 10−32468101214x 10−3GPLVSTDGPLC51015x 10−351015x 10−3GPLCGPLV51015x 10−351015x 10−3GPLVGPLCGPLVREG0.10.20.30.10.20.3REGGPLC0.10.20.30.10.20.3GPLCREG0.20.30.150.20.250.3REGGPLV0.20.30.150.20.250.3GPLVREGGPLC0.10.20.10.150.20.25GPLCGPLV0.10.20.10.150.20.25GPLVGPLCGPLVREG246x 10−3246x 10−3REGGPLC246x 10−3246x 10−3GPLCREG246x 10−3246x 10−3REGGPLV246x 10−3246x 10−3GPLVREGGPLC24x 10−312345x 10−3GPLCGPLV24x 10−312345x 10−3GPLVGPLCGPLVNegative Log Probability Density

Mean Squared Error

Figure 6: Dataset M1: Pairwise comparison of methods using NLPD(Left) and MSE(right)

Negative Log Probability Density

Mean Squared Error

Figure 7: Dataset M2: Pairwise comparison of methods using NLPD(Left) and MSE(right)

14

STD0.30.40.50.30.40.5STDGPLC0.30.40.50.30.40.5GPLCSTD0.30.40.50.30.40.5STDGPLV0.30.40.50.30.40.5GPLVSTDGPLC0.30.40.30.350.40.45GPLCGPLV0.30.40.30.350.40.45GPLVGPLCGPLVSTD0.020.030.040.020.030.04STDGPLC0.020.030.040.020.030.04GPLCSTD0.020.040.010.020.030.04STDGPLV0.020.040.010.020.030.04GPLVSTDGPLC0.020.040.010.020.030.04GPLCGPLV0.020.040.010.020.030.04GPLVGPLCGPLVSTD0.30.40.50.30.40.5STDGPLC0.30.40.50.30.40.5GPLCSTD0.30.40.50.30.40.5STDGPLV0.30.40.50.30.40.5GPLVSTDGPLC0.250.30.350.40.250.30.350.4GPLCGPLV0.250.30.350.40.250.30.350.4GPLVGPLCGPLVSTD0.020.040.010.020.030.04STDGPLC0.020.040.010.020.030.04GPLCSTD0.020.040.010.020.030.040.05STDGPLV0.020.040.010.020.030.040.05GPLVSTDGPLC0.020.040.010.020.030.040.05GPLCGPLV0.020.040.010.020.030.040.05GPLVGPLCGPLV4.3 Comparison of MCMC methods for GPLV models

To test whether or not the modiﬁed Metropolis sampler described in Section 3.3 is eﬀective, we
compare it to two standard samplers, which update the latent values one by one using either the
Metropolis algorithm or the univariate step-out slice sampler. The Metropolis algorithm is used to
update the hyperparameters in all of the three samplers. The signiﬁcant computations are listed
in Table 2.

We adjust the tuning parameters so that the above samplers are reasonably eﬃcient. For the slice
sampler, we use a slice width of 1, and allow indeﬁite stepping-out. For the univariate Metropolis
sampler, we adjust the standard deviations of the Gaussian proposals so that the acceptance rate
of each parameter variable is around 50%. For the modiﬁed Metropolis sampler, we set a = 0.3
and m = 40. The acceptance rate for zi is around 1.4%, but since we sample zi 40 times for each
iteration, we have about 60% of chance to get a new value of zi while all hyperparameters are
updated once.

The eﬃciency of an MCMC method is usually measured by the autocorrelation time, τ , of values

from the chain it produces (see Neal, 1993):

τ = 1 + 2

∞(cid:88)

k(cid:88)

γi

(30)

where γi is the lag-i autocorrelation. Roughly speaking, the autocorrelation time is the number of
iterations a sampler needs to obtain another nearly uncorrelated sample.

i=1

With an MCMC sample of size M , we can only estimate sample autocorrelations ˆγi up to
i = M − 1, and since these estimates are all noisy, we usually estimate τ from a more limited set
of autocorrelations, as follows:

ˆτ = 1 + 2

ˆγi

(31)

where k is a cut-oﬀ point where ˆγi seems to be nearly 0 for all i > k,

i=1

In our experiment, we will consider the autocorrelation time of both the hyperparameters and
the latent variables. The model has four hyperparameters (ηy, ρy and ηz, ρz), so it is not too diﬃcult
to look at all of them. But there are n = 100 latent variables, each with its own autocorrelation
time. Instead of comparing all of them one by one, we will compare the autocorrelation time of the
sum of the latent variables as well as the sum of the squares of the latent variables.

Another measure of sampler eﬃciency is the time it takes for a sampler to reach equilibrium,
i.e. the stationary distribution of the Markov chain. This is often referred to as the Markov chain
“mixing time” (denoted as TM ). Theoretical bound of mixing rate can be found for some classical

Modiﬁed Metropolis

Operation

Cy, chol(Cy) Cz, chol(Cz)

# of such operations

p + 1

p + 1

θy

θz

z

Cy, chol(Cy)

m(p + 1)

Metropolis/Slice

Operation

Cy, chol(Cy) Cz, chol(Cz)

rank-1 up chol(Cy)

# of such operations

p + 1

p + 1

n

Table 2: Major operations of the MCMC methods for GPLV

15

Modiﬁed Metropolis
Metropolis
Slice

˜τ

ηy
3.06
2.81
13.37

ρy
4.92
11.21
16.71

ηz
3.09
2.73
11.85

ρz

10.63
27.26
23.65

(cid:80)

i zi
0.32
13.78
37.81

(cid:80)

i z2
i
0.46
13.92
53.81

Table 3: Autocorrelation times (adjusted for computation time) of MCMC methods for GPLV.

algorithms, however, in practice, the mixing time of a sophisticated sampler is usually very diﬃcult
to determine.
It is common practice to look at trace plots of log-probability density values to
decide whether or not a chain has reached equilibrium (this is usually how the number of “burn-in”
iterations is decided).

The mixing time and the autocorrelation time usually agree in the sense that a sampler that
takes less time to get a new uncorrelated sample can usually achieve equilibrium faster, and vice
versa, though this is not always the case.

Note both autocorrelation time τ and mixing time TM take the number of iterations of the a
Markov chain as their unit of measurement. However, the CPU time required for an iteration
diﬀers between samplers. For a fair comparison, we adjust τ by multiplying it with the average
CPU time per iteration. The result, which we denote as ˜τ , measures the CPU time a sampler needs
to obtain an uncorrelated sample. To fairly compare mixing times using trace plots, we will adjust
the number of iterations in the plots so that each entire trace takes the same amount of time.

We run the three samplers ﬁve times, starting from the same point (the prior mean) but with
diﬀerent random seeds. The average adjusted autocorrelation times are listed in Table 3. The
modiﬁed Metropolis sampler signiﬁcantly outperforms the others at sampling the latent variables:
it is about 50 to 100 times faster than the regular Metropolis sampler and slice sampler. For the
hyperparameters, however, the modiﬁed Metropolis sampler gives roughly the same autocorrelation
times as the regular Metropolis sampler does. Both of them seems to work better than slice sampler,
but the diﬀerence is much smaller than the diﬀerence in sampling latent variables. Figure 8 shows
selected autocorrelation plots from one of the ﬁve runs (adjusted for computation time).

Figure 9 gives the trace plots of the three methods for one of the ﬁve runs (other runs are similar).
The bottom plot is the trace of log-probability density values of the initial 400 iterations of the
slice sampler. The middle plot shows the initial iterations of the regular Metropolis sampler, with
the number adjusted to take the same time as the 400 slice sampler iterations. The top plot shows
the initial iterations of the modiﬁed Metropolis sampler, again taking the same computation time.

It is clear that the modiﬁed Metropolis takes the least time to mix. Starting from the prior
mean (which seem to be a reasonable initial point), with log-probability density (LPD) value of
approximately −13, the modiﬁed Metropolis method immediately pushes the LPD to 70 at the
second step, and then soon declines slightly to what appears to be the equilibrium distribution.
The other two methods move take much more time to reach this equilibrium distribution, with the
simple Metropolis and slice sampling methods taking roughly the same amount of time.

We conclude that the modiﬁed Metropolis is the best of these MCMC method — the fastest to
reach equilibrium, the best at sampling latent values thereafter, and at least as good at sampling
hyperparameters.

16

Figure 8: Selected autocorrelation plots for MCMC methods for GPLV (with horizontal scales that
adjust for computation time).

Figure 9: Trace plots of log posterior density for MCMC methods for GPLV.

17

02040−0.500.51LagSample ACF for Σ zModified Metropolis0100−0.500.51LagSample ACF for Σ zMetropolis0102030−0.500.51LagSample ACF for ΣzSlice0100200−0.500.51LagSample ACF for ηzModified Metropolis0500−0.500.51LagSample ACF for ηzMetropolis050100150−0.500.51LagSample ACF for ηzSlice050100150200250300350400−20020406080log−densitySlice0200400600800100012001400160018002000−20020406080log−densityMetropolis0200400600800100012001400−20020406080log−densityModified MetropolisIn a simpler context — ﬁnancial time series where no “main” GP is needed, since the mean
response can be taken to be always zero — Wilson and Ghahramani (2010) use the “elliptical slice
sampling” method of Murray, et. al. (2010) to sample latent values that determine the variances
of observations. Elliptical slice sampling is related to the modiﬁed Metropolis method above. It
would be interesting to see how they compare in a general regression context.

References

Goldberg, P. W. and Williams, C. K. I. and Bishop, C. M. (1998) “Regression with Input-dependent
Noise: A Gaussian Process Treatment”, Advances in Neural Information Processing Systems 10.,
the MIT Press.

Leadbetter, M.R., Lindgren, G. and Rootz´en, H. (1983). “Extremes and related properties of

random sequences and processes.” Springer-Verlag. ISBN 0-387-90731-9.

Metropolis, N. and Rosenbluth, A.W. and Rosenbluth, M.N. and Teller, A.H. and Teller, E. (1953)
“Equations of State Calculations by Fast Computing Machines”, Journal of Chemical Physics
vol. 21, pp. 1087-1092.

Murray, I., Adams, R. P., and MacKay, D. J. C. (2010) “Elliptical slice sampling”, AISTATS 2010,

in JMLR W&CP, 9:541-548.

Neal, R. M. (1993), “Probabilistic Inference Using Markov Chain Monte Carlo Methods”, Tech-
nical Report, Dept. of Computer Science, University of Toronto, CRG-TR-93-1, Available from
http://www.utstat.utoronto.ca/~radford

Neal, R. M. (1997), “Monte Carlo implementation of Gaussian process models for Bayesian regres-
sion and classiﬁcation”, Technical Report, Dept. of Statistics, University of Toronto, no. 9702,
Available from http://www.utstat.utoronto.ca/~radford

Neal, R. M. (1998), “Regression and Classiﬁcation Using Gaussian Process Priors”, Bernardo, J.

M. (editor) Bayesian Statistics 6, Oxford University Press, pp. 475-501

Neal, R. M. (2003), “Slice sampling”, Annals of Statistics, vol. 11, pp. 125-139

Rasmussen, C. E. and Williams, C. K. I. (2006), Gaussian Processes for Machine Learning, the

MIT Press, ISBN 026218253X,

Sherman, J. and Morrison, W. J. (1950). “Adjustment of an Inverse Matrix Corresponding to a
Change in One Element of a Given Matrix”, Annals of Mathematical Statistics 21 (1) pp. 124-127.

Thompson, M., (2011). “Slice sampling with multivariate steps”, Doctoral Thesis, Department of

Statistics, University of Toronto

Wilson, A. G. and Ghahramani, Z. (2010) “Copula processes”, Advances in Neural Information

Processing Systems (NIPS 2010)

18

Appendix: MSE and NLPD for each method and training set

Dataset U1:

Dataset U2:

Dataset M1:

Dataset M2:

Training Set

REG

1
2
3
4
5
6
7
8
9
10

1
2
3
4
5
6
7
8
9
10

1
2
3
4
5
6
7
8
9
10

1
2
3
4
5
6
7
8
9
10

0.3364
0.3259
0.3142
0.3198
0.3231
0.3596
0.3404
0.3683
0.3177
0.2961

0.2878
0.2616
0.2527
0.2697
0.2695
0.2599
0.2544
0.2708
0.2863
0.2727

0.4726
0.3852
0.4560
0.4300
0.4817
0.4668
0.4282
0.4184
0.4161
0.4253

0.3736
0.3934
0.4015
0.4819
0.4311
0.4348
0.3892
0.3709
0.4043
0.4718

GPLV

REG

MSE
GPLC

GPLV

0.2480
0.2076
0.2661
0.2237
0.2185
0.2321
0.2374
0.3305
0.2172
0.2107

0.2408
0.1349
0.1488
0.1998
0.1985
0.1769
0.1948
0.1283
0.1353
0.1670

0.3407
0.2860
0.3410
0.3751
0.3906
0.3024
0.3799
0.4022
0.3817
0.3201

0.298
0.2965
0.2832
0.4202
0.3548
0.3140
0.2770
0.2580
0.2899
0.3923

0.0119
0.0077
0.0060
0.0077
0.0076
0.0137
0.0040
0.0081
0.0061
0.0017

0.0070
0.0026
0.0013
0.0042
0.0030
0.0025
0.0010
0.0046
0.0049
0.0033

0.0186
0.0121
0.0305
0.0204
0.0426
0.0364
0.0195
0.0197
0.0202
0.0216

0.0099
0.0136
0.0167
0.0391
0.0269
0.0216
0.0102
0.0058
0.0156
0.0403

0.0088 0.0059
0.0039 0.0058
0.0144 0.0139
0.0065 0.0058
0.0071 0.0073
0.0105 0.0110
0.0030 0.0030
0.0079 0.0080
0.0057 0.0055
0.0018 0.0020

0.0035 0.0034
0.0009 0.0007
0.0025 0.0035
0.0036 0.0037
0.0036 0.0030
0.0052 0.0032
0.0015 0.0017
0.0019 0.0017
0.0020 0.0027
0.0023 0.0021

0.0120 0.0201
0.0113 0.0098
0.0247 0.0277
0.0188 0.0201
0.0399 0.0351
0.0247 0.0163
0.0172 0.0171
0.0148 0.0217
0.0204 0.0297
0.0190 0.0197

0.0092 0.0093
0.0099 0.0122
0.0105 0.0128
0.0174 0.0489
0.0234 0.0230
0.0165 0.0176
0.0065 0.0083
0.0052 0.0067
0.0124 0.0146
0.0223 0.0281

NLPD
GPLC

0.2459
0.2489
0.2774
0.2273
0.2296
0.2397
0.2696
0.3143
0.2023
0.3050

0.1976
0.1099
0.1123
0.1148
0.1275
0.1396
0.1130
0.0811
0.0833
0.1245

0.3202
0.3046
0.3390
0.3860
0.4321
0.3603
0.3656
0.3198
0.3281
0.3269

0.2413
0.2458
0.2695
0.3636
0.3257
0.2678
0.2479
0.3147
0.2441
0.3355

19

