MCMC Methods for Gaussian Process Models

Using Fast Approximations for the Likelihood

Chunyi Wang

Radford M. Neal

Department of Statistical Sciences

Department of Statistical Sciences and

University of Toronto

Department of Computer Science

chunyi@utstat.toronto.edu

University of Toronto

radford@utstat.toronto.edu

9 May 2013

Gaussian Process (GP) models are a powerful and ﬂexible tool for non-parametric

regression and classiﬁcation. Computation for GP models is intensive, since computing

the posterior density, π, for covariance function parameters requires computation of
the covariance matrix, C, a pn2 operation, where p is the number of covariates and
n is the number of training cases, and then inversion of C, an n3 operation. We

introduce MCMC methods based on the “temporary mapping and caching” framework,
using a fast approximation, π∗, as the distribution needed to construct the temporary
space. We propose two implementations under this scheme: “mapping to a discretizing

chain”, and “mapping with tempered transitions”, both of which are exactly correct

MCMC methods for sampling π, even though their transitions are constructed using

an approximation. These methods are equivalent when their tuning parameters are set

at the simplest values, but diﬀer in general. We compare how well these methods work
when using several approximations, ﬁnding on synthetic datasets that a π∗ based on the
“Subset of Data” (SOD) method is almost always more eﬃcient than standard MCMC
using only π. On some datasets, a more sophisticated π∗ based on the “Nystr¨om-
Cholesky” method works better than SOD.

3
1
0
2

 

y
a
M
0
1

 

 
 
]

O
C

.
t
a
t
s
[
 
 

1
v
5
3
2
2

.

5
0
3
1
:
v
i
X
r
a

1

Introduction

Evaluating the posterior probability density function is the most costly operation when Markov

Chain Monte Carlo (MCMC) is applied to many Bayesian inference problems. One example is

the Gaussian Process regression model (see Section 5 for a brief introduction), for which the

time required to evaluate the posterior probability density increases with the cube of the sample

size. However, several fast but approximate methods for Gaussian Process models have been

1

developed. We show in this paper how such an approximation to the posterior distribution for

parameters of the covariance function in a Gaussian process model can be used to speed up

sampling, using either of two schemes, based on “mapping to a discretizing chain” or “mapping

with tempered transitions”. Both schemes produce an exactly correct MCMC method, despite

using an approximation to the posterior density for some operations.

In the next section, we describe a general scheme for contructing eﬃcient MCMC methods

using temporary mapping and caching techniques, ﬁrst introduced by Neal (2006), which is the

basis for both of the schemes for using approximations that are introduced in this paper.

One possibility for a space to temporarily map to is the space of Markov chain realizations
that leave a distribution π∗ invariant. Our hope is that if we use such a space with a π∗ that is
a good approximation to π, but faster to compute, then MCMC with temporary mapping and

caching will be faster than MCMC methods using only π.

We then consider how the tempered transiton method due to Neal (1996) can also be viewed

as mapping temporary to another space. Using this view, we give a diﬀerent proof that de-

tailed balance holds for tempered transitions. We then discuss how the sequence of transitions
ˆT1, ˆT2, ..., ˇT2, ˇT1 (which collectively form the tempered transition) should be chosen when they are
deﬁned using fast approximations, rather than (as in the original context for tempered transtions)

by modifying the original distribution, π, in a way that does not reduce computation time.

We apply these two proposed schemes to Gaussian process regression models that have a

covariance function with unknown hyperparameters, whose posterior distribution must be sampled

using MCMC. We discuss several fast GP approximation methods that can be used to contruct
an approximate π∗. We conclude by presenting experiments on synthetic datasets using the new
methods that show that these methods are indeed faster than standard methods using only π.

2 MCMC with temporary mapping and caching

To start, we present two general ideas for improving MCMC — temporarily mapping to a diﬀerent

state space, and caching the results of posterior density computations for possible later use.

2.1 Creating Markov transitions using temporary mappings
To obtain samples of a target distribution π from space X using MCMC, we need to ﬁnd a

transition probability T (x(cid:48)|x), for which(cid:90)

π(x)T (x(cid:48)|x)dx = π(x(cid:48))

(1)

2

i.e., T (x(cid:48)|x) leaves the target distribution π invariant. There are many ways to form such a tran-
sition. In the famous Metropolis algorithm (Metropolis et. al, 1953), from a current state x, we
propose to move to a candidate state x∗ according to a proposal distribution S(x(cid:48)|x) that is sym-
metric (i.e., S(x(cid:48)|x) = S(x|x(cid:48))), and then accept this proposal with probability min(1, π(x∗)/π(x)).
If this proposal is accepted, the new state is x(cid:48) = x∗, otherwise x(cid:48) = x. It’s easy to show that
these transitions leave π invariant (in fact they satisfy the stronger “detailed balance” condition
that π(x)T (x(cid:48)|x) = π(x(cid:48))T (x|x(cid:48))).

The temporary mapping technique (Neal, 2006) deﬁnes such a transition via three other

stochastic mappings, ˆT , ¯T and ˇT , as follows:
ˆT−→ y

x

¯T−→ y(cid:48)

ˇT−→ x(cid:48)

(2)
where x, x(cid:48) ∈ X and y, y(cid:48) ∈ Y. Starting from x, we obtain a value y in the temporary space Y by
ˆT (y|x). The target distribution for y has probability mass/density function ρ(y). We require that

We then obtain another sample y(cid:48) using ¯T (y(cid:48)|y), which leaves ρ invariant:

π(x) ˆT (y|x)dx = ρ(y)

ρ(y) ¯T (y(cid:48)|y)dy = ρ(y(cid:48))

(cid:90)
(cid:90)
(cid:90)

(3)

(4)

(6)

(7)

(8)

(9)

Finally, we map back to x(cid:48) ∈ X using ˇT (x(cid:48)|y), which we require to satisfy

It’s easy to see that the combined transition T (x(cid:48)|x) =(cid:82)(cid:82) ˆT (y|x) ¯T (y(cid:48)|y) ˇT (x(cid:48)|y(cid:48))dydy(cid:48) leaves

(5)

ρ(y(cid:48)) ˇT (x(cid:48)|y(cid:48))dy(cid:48) = π(x(cid:48))

π invariant:

(cid:90)

π(x)T (x(cid:48)|x)dx =

=

π(x) ˆT (y|x) ¯T (y(cid:48)|y) ˇT (x(cid:48)|y(cid:48))dydy(cid:48)dx

ρ(y) ¯T (y(cid:48)|y) ˇT (x(cid:48)|y(cid:48))dydy(cid:48)

(cid:90) (cid:90) (cid:90)
(cid:90) (cid:90)
(cid:90)

ρ(y(cid:48)) ˇT (x(cid:48)|y(cid:48))dy(cid:48)

=
= π(x(cid:48))

Quite a few existing methods can be viewed as mapping to temporary spaces. For instance,

the technique of temporarily introducing auxiliary variables can be considered as mapping from x

to y = (x, z), where z is a set of auxiliary variables.

3

2.2 Caching values for future re-use

Many MCMC transitions require evalulating the probability density of π, up to a possibly un-

known normalizing constant. For example, each iteration of the Metropolis algorithm needs the
probability density values of both the current state x and the candidate state x∗. Since these eval-
uations typically dominate the MCMC computation time, it may be desirable to save (‘cache’)

computed values of π(x) so they can be re-used when the same state x appears in the chain again.
Caching is always useful for the Metropolis algorithm, since if we reject a proposal x∗, we will
need π(x) for the next transition, and if we instead accept x∗ then it becomes the current state
and we will need π(x∗) for the next transition.

When the proposal distribution is discrete (as it will always be when the state space is dis-
crete), the probability of proposing an x∗ that was previously proposed can be positive, so saving
the computed value of π(x∗) may be beneﬁcial even if x∗ is rejected. When the state space is
continuous, however, the proposal distributions commonly used are also continuous, and we will
have zero probability of proposing the same x∗ again. But in this case, as we will see next, caching
can still be beneﬁcial if we ﬁrst map to another space with a “discretizing chain”.

3 Mapping to a discretizing chain

To take full advantage of both mapping and caching, we propose a temporary mapping scheme
where the temporary space is continuous, but is eﬀectively discrete with regard to transitions ¯T .
Let R(x(cid:48)|x) be the transition probabilities for a Markov Chain which leaves π∗ invariant. Let
˜R(x|x(cid:48)) = R(x(cid:48)|x)π∗(x)/π∗(x(cid:48)) be the reverse transition probabilities, which clearly also leave π∗
invariant.

We map from X to Y, a space of realizations of this Markov Chain of length K, where one
time step of this chain is “marked”. To map x ∈ X to y ∈ Y, we use a ˆT that operates as follows:

• Choose k uniformly from 0, ..., K − 1.
• Simulate K − 1 − k forward transition steps using R starting at xk = x, producing states

xk+1, ..., xK−1.

• Simulate k reverse transitions using ˜R, starting at xk = x, producing states xk−1, ..., x0.
• Set the “marked” time step to k.
The transition ¯T moves the mark along the chain from k to another time step k(cid:48) ∈ {0, . . . , K−1},
while keeping the current chain realization, (x0, . . . , xK−1), ﬁxed. The transition ˇT just takes the

4

X

x

ˆT

x(cid:48)

y has a mark here

R

ˇT

¯T

Y

y(cid:48) has a mark here

Figure 1: Mapping to a discretizing chain and back.

marked state, so x(cid:48) = xk(cid:48). The actual implementation will not necessarily simulate all K − 1 steps
of the discretizing chain — a new step is simulated only when it is needed. We can then let K go
to inﬁnity, so that ¯T can move the mark any ﬁnite number of steps forward or backward.

Figure 1 illustrates this scheme. Note that an element y ∈ Y is a chain realization with a mark
placed on the time step k. We write y = (k; x0, ..., xK−1). When we say we “move the mark from
k to k(cid:48)”, we actually use a transition ¯T to move from y = (k; x0, ..., xK−1) to y(cid:48) = (k(cid:48); x0, ..., xK−1),
where y and y(cid:48) share the same chain realization and diﬀer only on the marked position. We are
free to choose the way ¯T moves the mark in any way that leaves ρ invariance — for instance, we
can pick a number s and propose to move mark from k to k + s or k − s with equal probabilities.
We can make r such moves within each mapping. The discretizing chain makes the state space
eﬀectively discrete, even though the space Y is continuous, and consequently, when we move the
mark around the chain realization, there is a positive probability of hitting a location that has

been visited before.

The transition ¯T has to leave ρ(y) invariant. We compute the ratio of ρ(y(cid:48)) and ρ(y) to see how
we can construct a such a ¯T . ρ has been implicitly deﬁned in (3) as the distribution resulting from
applying ˆT to x drawn from π. The probability to sample y is given by the simulation process
described above (i.e. start from x, simulate K − 1− k forward steps using R and k backward steps
using ˜R), namely, if y = (k; x0, ..., xK−1),

ρ(y) = π(xk)

=

π(xk)
π∗(xk)

1
K
1
K

(cid:124)

R(xk+1|xk)··· R(xK−1|xK−2) × ˜R(xk−1|xk)··· ˜R(x0|x1)
π∗(xk)R(xk+1|xk)··· R(xK−1|xK−2) × ˜R(xk−1|xk)··· ˜R(x0|x1)

(cid:123)(cid:122)

:=A

5

(cid:125)

(10)

An expression for ρ(y(cid:48)) can be similarly obtained for y(cid:48) = (k(cid:48); x0, ..., xK−1):

(cid:123)(cid:122)

:=A(cid:48)

(cid:125)

ρ(y(cid:48)) =

π(xk(cid:48))
π∗(xk(cid:48))

1
K

(cid:124)

π∗(xk(cid:48))R(xk(cid:48)+1|x(cid:48))··· R(xK−1|xK−2) × ˜R(xk(cid:48)−1|xk(cid:48))··· ˜R(x0|x1)

(11)

We take out a factor of the ratio of densities π/π∗ from both (10) and (11), and write the remaining
term as A or A(cid:48), as indicated in the respective equation. Since R and ˜R are reverse transitions
with respect to π∗, if k(cid:48) > k, then

π∗(xk)R(xk+1|xk)··· R(xk(cid:48)|xk(cid:48)−1)

= ˜R(xk|xk+1)π∗(xk+1)R(xk+2|xk+1)··· R(xk(cid:48)|xk(cid:48)−1)
...
= ˜R(xk|xk+1)... ˜R(xk(cid:48)−1|xk(cid:48))π∗(xk(cid:48))

(12)
It therefore follows that A = A(cid:48). A similar argument shows that A = A(cid:48) when k(cid:48) ≤ k. Thus the
ratio of ρ(y(cid:48)) and ρ(y) is

ρ(y(cid:48))
ρ(y)

=

π(xk(cid:48))/π∗(xk(cid:48))
π(xk)/π∗(xk)

(13)

Equation (13) implies that to leave ρ invariant we can use a Metropolis type transition, ¯T , that
proposes to move the mark from k to k(cid:48) and accepts the move with probability

(cid:18)

min

1,

π(xk(cid:48))/π∗(xk(cid:48))
π(xk)/π∗(xk)

(cid:19)

Note that if π = π∗, then the transition ¯T will accept a move of the mark to any other time step
on the discretizing chain, since the discretizing chain actually leaves the target distribution π∗
invariant and therefore every time step of this chain is a valid sample of π. If π∗ (cid:54)= π, but is very
similar to π, we can hope the acceptance rate will be high. In addition, if the evaluation of π∗(x)
takes much less time than that of π(x), mapping to the discretizing chain and then proposing

large moves of the mark can save computation time, since it eﬀectively replaces evaluations of π
with evaluations of π∗, except for the acceptance decisions.. On the other hand, if π∗ is completely
arbitrary, the acceptance rate will be low, and if the evalution of π∗ is not much faster than π(x),
we will not save computation time. These π∗’s are not useful. We need π∗ to be a fast but good
approximation to π. We will discuss this in the context of GP models in a later section.

Every time we map into a temporary space, we can make multiple ¯T updates (move the “mark”
several times). This way we can take advantage of the “caching” idea, since sometimes the mark

will be moved to a state where π has already been computed, and therefore no new computation

6

is needed. The number of updates is a tuning parameter, which we denote as “r”. Another tuning

parameter, which we denote as “s”, is the number of steps of transition R to “jump” when we try
to move the mark. Note that although we only “bring back” (using ˇT ) the last updated sample as
x(cid:48), all of the marked states are valid samples of π(x), and can be used for computing expectations
with respect to π if desired.

4 Tempered transitions

The “tempered transitions” method of Neal (1996) can also be viewed as mapping to a temporary

space. This method aims to sample from π using a sequence of distributions π = π0, π1, . . . , πn.
For i = 0, . . . , n, let ˆTi (called the “up” transition) and ˇTi (the “down” transition) be mutually

reversible transitions with respect to the density πi — i.e. for any pair of states xi and x(cid:48)
i,

πi(xi) ˆTi(x(cid:48)

i|xi) = ˇTi(xi|x(cid:48)

i)πi(x(cid:48)
i)

(14)

This condition implies that both ˆTi and ˇTi have πi as their invariant distribution. If ˆTi = ˇTi then
(14) reduces to the detailed balance condition. If ˆTi = S1S2...Sk with all of Si being reversible
transitions, then ˇTi = SkSk−1...S1 would satisfy condition (14).

We map from x ∈ X to y ∈ Y, a space of realizations of tempered transitions, using a ˆT that

operates as follows:

Generate ˆx1 from x using ˆT1;
Generate ˆx2 from ˆx1 using ˆT2;

...

Generate ¯xn from ˆxn−1 using ˆTn.
Generate ˇxn−1 from ¯xn using ˇTn;
Generate ˇxn−2 from ˇxn−1 using ˇTn−1;

...

Generate x∗ from ˇx1 using ˇT1.

An element y ∈ Y can be written as y = (x, ˆx1, ..., ¯xn, ..., ˇx1, x∗).

¯T attempts to ﬂip the order of y, accepting the ﬂip with probability

(cid:18)

min

1,

π1(ˆx0)
π0(ˆx0)

··· πn(ˆxn−1)
πn−1(ˆxn−1)

· πn−1(ˇxn−1)
πn(ˇxn−1)

··· π0(ˇx0)
π1(ˇx0)

(cid:19)

(15)

where ˆx0 and ˇx0 are synonyms for x and x∗, respectively, to keep notations consistent. In other
words, with this probability, we set y(cid:48) to y∗ = (x∗, ˇx1, ..., ¯xn, ..., ˆx1, x) (the order is reversed);
otherwise we sset y(cid:48) = y (the order is preserved).

7

Finally, ˇT maps back to x(cid:48) ∈ X by taking the ﬁrst coordinate of y(cid:48) (either the original x or x∗,

depending on whether or not the ﬂip was accepted).

Using the temporary mapping perspective, we can show that tempered transitions are valid
updates, leaving π invariant, by deﬁning ρ to be the result of applying ˆT to a point drawn from
π, and then showing that ¯T leaves ρ invariant, and that ˇT produces a point distributed as π from
a point distributed as ρ.

The ˆT mapping from x = ˆx0 to y = (ˆx0, ˆx1, ..., ¯xn, ..., ˇx1, ˇx0) involves a sequence of transitions:

ˆT1−→ ˆx1

ˆT2−→ ˆx2 −→ ··· −→ ˆxn−1

ˆTn−→ ¯xn

ˇTn−→ ˇxn−1

ˇTn−1−→ ˇxn−2 −→ ··· −→ ˇx1

ˇT1−→ ˇx0

ˆx0

The probability density, ρ, for y can be computed from this as

ρ(y) = π0(ˆx0) ˆT1(ˆx1|ˆx0)··· ˆTn(¯xn|ˆxn−1) ˇTn(ˇxn−1|¯xn)··· ˇT1(ˇx0|ˇx1)

Similarly,

ρ(y∗) = π0(ˇx0) ˆT1(ˇx1|ˇx0)··· ˆTn(¯xn|ˇxn−1) ˇTn(ˆxn−1|¯xn)··· ˇT1(ˆx0|ˆx1)

Now we compute the ratio of probability densities of y∗ and y:

ρ(y∗)
ρ(y)

=

π0(ˇx0) ˆT1(ˇx1|ˇx0)··· ˆTn(¯xn|ˇxn−1) ˇTn(ˆxn−1|¯xn)··· ˇT1(ˆx0|ˆx1)
π0(ˆx0) ˆT1(ˆx1|ˆx0)··· ˆTn(¯xn|ˆxn−1) ˇTn(ˇxn−1|¯xn)··· ˇT1(ˇx0|ˇx1)
= π0(ˇx0) · ˆT1(ˇx1|ˇx0)
··· ˇT1(ˆx0|ˆx1)
ˇT1(ˇx0|ˇx1)
ˆT1(ˆx1|ˆx0)
= π0(ˇx0) · π1(ˇx1)
·
π1(ˇx0)
··· πn(ˆxn−1)
πn−1(ˆxn−1)

··· ˆTn(¯xn|ˇxn−1) ˇTn(ˆxn−1|¯xn)
ˇTn(ˇxn−1|¯xn) ˆTn(¯xn|ˆxn−1)

··· πn(¯xn)
πn(ˇxn−1)

· πn−1(ˇxn−1)
πn(ˇxn−1)

··· π0(ˇx0)
π1(ˇx0)

· πn(ˆxn−1)
πn(¯xn)

··· π1(ˆx0)
π1(ˆx1)

=

π1(ˆx0)
π0(ˆx0)

1

π0(ˆx0)

·

1

π0(ˆx0)

(16)

(17)

(18)

(19)

(20)

We obtain (19) from the mutual reversibility property of the transitions ˆTi and ˇTi, and (18) and
(20) simply by reordering terms.

From (20), we see that the probability of accepting the ﬂip from y to y∗ given by (15) is equal
to min(1, ρ(y∗)/ρ(y)), and thus ¯T satisﬁes detailed balance with respect to ρ. It is also clear from
(16) that the marginal distribution under ρ of the ﬁrst component of y is π0 = π, and thus ˇT maps
from ρ to π.

The original motivation of the tempered transition method described by Neal (2006) is to move

between isolated modes of multimodal distributions. The distributions π1, ..., πn are typically
of the same class as π, but broader, making it easier to move between modes of π (typically,

as i gets larger, the distribution πi gets broader, thus making it more likely that modes have
substantial overlap). Evaluating the densities for π1, ..., πn typically takes similar computation

8

time as evaluating the density for π. Our mapping-caching scheme, on the other hand, is designed

to reduce computation. Ideally, in our scheme the bigger i is, the faster is the evaluation of πi(x).
One possibility for this is that each πi is an approximation of π, and as i increases the computation
of πi becomes cheaper (but worse).

The two methods we propose in this paper are equivalent if the following are all true:
• For mapping to a discretizing chain:

1. The transition R which leaves π∗ invariant is reversible.
2. s = 2k, i.e. ¯T always attempts to move the mark over an even number of R updates.

3. r = 1, i.e. ¯T attempts to move the mark only once within each mapping.

• For mapping by tempered transitions:

1. n = 1, i.e., there is only one additional distribution.
2. ˆT1 = ˇT1 = Rk, i.e. these transitions consist of k updates using R (and hence π1 = π∗).

When all above are true except that n > 1, so more than one additional distribution is used

in the tempered transitions, we might expect tempered transitions to perform better, as they

propose a new point through the guidance of these additional distributions, and computations for

these additional distributions should be negligible, if they are faster and faster approximations.

On the other hand, we might think that r > 1 will improve the performance when mapping to

a discretizing chain, since then caching could be exploited. So each method may have its own

advantages.

5 Application to Gaussian process models

We now show how these MCMC methods can be applied to Bayesian inference for Gaussian process

models.

5.1 Introduction to Gaussian process models

We start with a brief introduction to Gaussian process (GP) models to establish notation. The

problem is to model the association between covariates x and a response y using n observed pairs

(x1, y1), ..., (xn, yn), and then make predictions for the y in future items once their covariates, x,
have been observed. We can write such a model as

yi = f (xi) + i

9

(21)

where xi is a covariate vector of length p, and yi is the correspoding scalar response. The i are
random residuals, assumed to have Gaussian distributions with mean 0 and constant variance σ2.

Bayesian GP models assume that the noise-free function f comes from a Gaussian Process

which has prior mean function zero and some speciﬁed covariance function. Note that a zero

mean prior is not a requirement — we could specify a non-zero prior mean function m(x) if

we have a priori knowledge of the mean structure. Using a zero mean prior just reﬂects prior

knowledge that the function is equally likely to be positive or negative; the posterior mean of the

function is typically not zero.

The covariance function could be ﬁxed a priori, but more commonly is speciﬁed in terms of

unknown hyperparameters, θ, which are then estimated from the data. Given the values of the

hyperparameters, the response y follows a multivariate Gaussian distribution with zero mean and

a covariance matrix given by

Cov(yi, yj) = K(xi, xj) + Cov(i, j) = K(xi, xj) + δijσ2

(22)
where δii = 1 and δij = 0 when i (cid:54)= j, and K is the covariance function of f . Any covariance
function that always leads to a positive semi-deﬁnite covariance matrix can be used. One example

is the squared exponential covariance function with isotropic length-scale (to which we add a

constant allowing the overall level of the function to be shifted from zero):

K(xi, xj) = c2 + η2 exp

(cid:18)
−(cid:107)xi − xj(cid:107)2

(cid:19)

ρ2

(23)

(24)

Here, c is a fairly large constant (not excessively large, to avoid numerical singularity), and η, σ,

and ρ are hyperparameters — η controls the magnitude of variation of f , σ is the residual standard

deviation, and ρ is a length scale parameter for the covariates. We can instead assign a diﬀerent

length scale to each covariate, which leads to the squared exponential covariance function with

automatic relevance determination (ARD):

K(xi, xj) = c2 + η2 exp

(cid:32)
− p(cid:88)

k=1

(cid:33)

(xik − xjk)2

ρ2
k

Unless noted otherwise, we will use the squared exponential covariance functions (23) or (24)

thoughout this paper.

When the values of the hyperparameters are known, the predictive distribution for the response,
y∗, a test case with covariates x∗, based on observed values x = (x1, ..., xn) and (y1, ..., yn), is
Gaussian with the following mean and variance:

E(y∗|x, y, x∗, θ) = kT C(θ)−1y

(25)

10

Var(y∗|x, y, x∗, θ) = v − kT C(θ)−1k

(26)

In the equations above, k is the vector of covariances between y∗ and each of yi, C(θ) is the
covariance matrix of the observed y, based on the known hyperparameters θ, and v is the prior
variance of y∗, which is Cov(y∗, y∗) from (22).

When the values of the hyperparameters are unknown, and therefore must be estimated from

the data, we put a prior, p(θ), on them (typically an independent Gaussian prior on the logarithm
of each hyper-parameter), and obtain the posterior distribution p(θ|x, y) ∝ N (y|0, C(θ)) p(θ).
The predictive mean of y is then computed by integrating over the posterior distribution of the

hyperparameters:

E(y∗|x, y, x∗) =

kT C(θ)−1y · p(θ|x, y) dθ

The predicted variance is given by

Θ

Var(y∗|x, y, x∗) = E[Var(y∗|x, y, x∗, θ)| x, y] + Var[E(y∗|x, y, x∗, θ)| x, y]

(27)

(28)

(cid:90)

Finding C−1 directly takes time proportional to n3, but we do not have to ﬁnd the inverse of
C explicitly. Instead we ﬁnd the Cholesky decomposition of C, denoted as R = chol(C), for which
RT R = C and R is an “upper” triangular matrix (also called a “right” triangular matrix). This
also takes time proportional to n3, but with a much smaller constant. We then solve RT u = y for
u using a series of forward subsititutions (taking time proportional to n2). From R and u, we can

compute the likelihood for θ, which is needed to compute the posterior density, by making use of

the expressions

and

yT C−1y = yT (RT R)−1y = yT R−1(cid:0)RT(cid:1)−1

n(cid:89)

y = uT u

(29)

(30)

det(C) = det(R)2 =

R2
ii

Similarly, equations (25) and (26) and be reformulated to use R rather than C−1.

i=1

5.2 Approximating π for GP models
As discussed in Section 3, using a poor π∗ for the discretizing chains on Y, or poor πi for tempered
transitions, can lead to a poor MCMC method which is not useful. We would like to choose

approximations to π that are good, but that can nevertheless be computated much faster than π.

For GP regression models, π will be the posterior distribution of the hyperparameters, θ.

Quite a few eﬃcient approximation methods for GP models have been discussed from a diﬀerent

perspective. For example, Qui˜nonero-Candela (2007) categorizes these approximations in terms

11

of “eﬀective prior”. Most of these methods are used for approximate training and prediction; not
all of them are suitable for forming a posterior approximation, π∗. For example, we cannot take
advantage of an eﬃcient approximated prediction.

5.2.1 Subset of data (SOD)

The most obvious approximation is to simply take a subset of size m from the n observed pairs
(xi, yi) and use the posterior distribution given only these observations as π∗:

π∗(θ) = N (y|0, ˆC(m)(θ)) p(θ)

(31)
where p(θ) is the prior for θ, the vector of hyperparameters, and N (a|µ, Σ) denotes the probability
density of a multivariate normal distribution N (µ, Σ) evaluated at a. ˆC(m)(θ) is computed based
on hyperparameters θ and the m observations in the subset.

Even though the SOD method seems quite naive, it does speed up computation of the Cholesky
decomposition of C from time proportional to n3 to time proportional to m3. If a small subset (say
10% of the full dataset) is used to form π∗, we can aﬀord to do a lot of Markov chain updates for
π∗, since the time it takes to make these updates will be quite small compared to a computation
of π. So a π∗ formed by this method might still be useful.

To form a π∗ using SOD, we need the following major computations, if there are p covariates:

Operation

Complexity

Compute ˆC(m)
Find chol( ˆC(m))

pm2
m3

5.2.2 Using low-rank plus diagonal matrices

A covariance matrix in a GP model typically has the form C = K + σ2I, where K is the noise-free
covariance matrix, and σ2 is the residual variance. More generally, if the residual variance diﬀers

for diﬀerent observations, the covariance matrix will be K plus a diagonal matrix giving these
residual variances. If we approximate K by a matrix ˆK with rank m < n, and let ˆC = ˆK + σ2I,
then after writing ˆK = BSBT , where B is n by m, we can quickly ﬁnd ˆC−1 by taking advantage
of the matrix inversion lemma, which states that

(BSBT + D)−1 = D−1 − D−1B(S−1 + BT D−1B)−1BT D−1

(32)

This can be simpliﬁed as follows when D = dI, where d is a scalar, B has orthonormal columns
(so that BT B = I), and S is a diagonal matrix with diagonal elements given by the vector s,

12

denoted by diag(s):

(B diag(s) BT + dI)−1 = d−1I − d−1IB(diag(s−1) + BT d−1IB)−1BT d−1I

= d−1I − d−2B(diag(1/s) + BT B/d)−1BT
= d−1I − d−1B(diag(d/s) + I)−1BT
= d−1I − d−1B(diag((s + d)/s)))−1B
= d−1I − B diag(s/(d(s + d))) BT

(33)

(34)

(35)

(36)

(37)

Expressions above such as 1/s denote element-by-element arithmetic on the vector operands.

We can use the matrix determinant lemma to compute the determinant of ˆC.

det(BSBT + D) = det(S−1 + BT D−1B) det(D) det(S)

(38)

When D = dI with d being a scalar, det(D) = dn is trivial, and det(S−1 + BT D−1B) can be found
from the Cholesky decomposition of S−1 + BT D−1B.

Once we obtain ˆC−1 and det( ˆC), we can easily establish our π∗:

π∗(θ) = N (y|0, ˆC)p(θ)

(39)

5.2.3 The Eigen-exact approximation

(cid:80)n

i λieieT

Since the noise-free covariance matrix, K, is non-negative deﬁnite, we can write it as K = EΛET =
i , where E has columns e1, e2, ..., en, the eigenvectors of K, and the diagonal matrix Λ has
the eigenvalues of K, λ1 ≥ λ2 ≥ ... ≥ λn on its diagonal. This is known as the eigendecomposition.
A natural choice of low-rank plus diagonal approximation would be ˆC = ˆK+σ2I where ˆK = BSBT
where B is an n × m matrix with columns e1, ..., em, and S is a diagonal matrix with diagonal
entries λ1, ..., λm. We expect this to be a good approximation if λm+1 is close to zero.

With this approximation, ˆC−1 can be computed rapidly from B and S using (37). However, the
time needed to ﬁnd the ﬁrst m eigenvalues and eigenvectors (and hence B and S) is proportional
to mn2, with a much larger constant factor than for the n3 computation of all eigenvalues and

eigenvectors. In practice, depending on the values of m and n and the software implementation, a
π∗ formed by this method could even be slower than the original π. Since our experiments conﬁrm
this, we mention it here only because it is a natural reference point.

5.2.4 The Nytr¨om-Cholesky approximation

In the Nystr¨om method, we take a random m by m submatrix of the noise-free covariance matrix,

K, which is equivalent to looking at the noise-free covariance for a subset of the data of size m, and

13

then ﬁnd its eigenvalues and eigenvectors. This takes time proportional to m3. We will denote the
, ..., e(m)
submatrix chosen by K (m,m), and its eigenvalues and eigenvectors by λ(m)
m .
We can then approximate the ﬁrst m eigenvalues and eigenvectors of the full noise-free covariance

m and e(m)

, ..., λ(m)

1

1

matrix by

ˆλi = (n/m)λ(m)

i

(cid:112)m/n

λ(m)
i

ˆei =

K (n,m)e(m)

i

(40)

(41)

where K (n,m) is the n by m submatrix of K with only the columns corresponding to the m cases

in the random subset.

The covariance matrix C can then be approximated in the same fashion as Eigen-exact, with
the exact eigenvalues and eigenvectors replaced by the approximated eigenvalues ˆλ1, ..., ˆλm and
eigenvectors ˆe1, ...ˆem. However, a more eﬃcient computational method for this approximation,
requiring no eigenvalue/eigenvector computations, is available as follows:

ˆK = K (n,m)[K (m,m)]−1K (m,n)

(42)

where K (m,n) = [K (n,m)]T ). We can ﬁnd the Cholesky decomposition of K (m,m) as RT R, in
time proportional to m3, with a much smaller constant factor than ﬁnding the eigenvalues and
eigenvectors. Equation (42) can then be put in the form of BSBT by letting B = K (n,m)R−1 and
S = I. In practice, the noise free submatrix K (m,m) often has some very small positive eigenvalues,

which can appear to be negative due to round-oﬀ error, making the Cholesky decomposition fail,

a problem that can be avoided by adding a small jitter to the diagonal (Neal, 1993).

An alternative way of justifying the approximation in (42) is by considering the covariance

matrix for the predictive distribution of all n noise-free observations from the random subset of
m noise-free observations, which (from a generalization of (26)) is K − K (n,m)[K (m,m)]−1K (m,n).
When this is close to zero (so these m noise-free observations are enough to almost determine the
function), ˆK will be almost the same as K.

More sophisticated schemes for Nystr¨om-Cholesky have been proposed. For instance, Drineas
and Mahoney (2005) randomly select the m columns to construct ˆC according to some “judiciously-
chosen’’ and data-dependent probability distribution rather than uniformly choose the m columns.

To form a π∗ using Nystr¨om-Cholesky, we need the following major computations:

Operation

Complexity

Compute K (n,m)
Find chol(K (m,m))

pmn
m3

14

6 Experiments

Here we report tests of the performance of the methods described in this paper using synthetic

datasets.

6.1 Experimental setup

The datasets we used in these experiments were randomly generated, with all covariates drawn

independently from uniform distributions on the interval [0, 1], and responses then generated

according to a Gaussian process with speciﬁed hyperparameters.

We generated ten types of datasets in this way, with diﬀerent combinations of the following:
• Number of observations: n = 300 or n = 900.
• Number of covariates: p=1 or p = 5.
• Type of covariance function: squared exponential covariance function with a single length
scale (isotropic), or with multiple length scales (Automatic Relevance Determination, ARD).

Note that these are identical when p = 1.

• Size of length scales: “short” indicates that a dataset has small length scales,“long” that it

has large length scales.

The speciﬁc hyperparameter values that were used for each combination of covariance function

and length scale are shown in Table 1.

The eﬃciency of an MCMC method is usually measured by the autocorrelation time, τ , for

the sequence of values produced by the chain (see Neal, 1993):

∞(cid:88)

τ = 1 + 2

ρi

(43)

where ρi is the lag-i autocorrelation for some function of interest. In practice, with an MCMC
sample of size M , we can only ﬁnd estimates, ˆρi, of autocorrelations up to lag i = M − 1. To

i=1

Length scale size Length scale type

short

short

long

long

isotropic

ARD

isotropic

ARD

η

5

5

5

5

l

l = 0.1

li = 0.1i

l = 2

li = 2i

Table 1: Hyperparameter values used to generate the synthetic datasets.

15

avoid excessive variance from summing many noisy estimates, we typically estimate τ by

k(cid:88)

ˆτ = 1 + 2

ˆρi

(44)

i=1

where k is a point where for all i > k, ˆρi is not signiﬁcantly diﬀerent from 0.

Below, we will compare methods with respect to autocorrelation time of the log likelihood. For

a fair comparison, we multiply the estimate of each method’s autocorrelation times by the average

CPU time it needs to obtain a new sample point.

6.2 Experiments with mapping to a discretizing chain
For each dataset, we tried the method of mapping to a discretizing chain using both a π∗ formed
with SOD and a π∗ formed with Nystr¨om-Cholesky. For comparison, we also ran a standard
MCMC model. All the Markov chains were started from the hyperparameter values that were

used to generate them, so these tests assess only autocorrelation time once the high-probability

region of the posterior has been reached, not time needed for convergence when starting at a

low-probability initial state. The adjustable parameters of each method were chosen to give

good performance. All chains were run for 2000 iterations, and autocorrelation times were then

computed based on the last two-thirds of the chain.

The standard MCMC method we used is a slice sampler (Neal, 2003), speciﬁcally a univariate

slice sampler with stepping-out and shrinkage, updating parameters in sequence. For the discretiz-
ing Markov chain, the transition R(x(cid:48)|x) uses the same slice sampler. Although slice sampling
has tuning parameters (the stepsize, w, and the upper limit on number of steps, M ), satisfactory

results can be obtained without extensive tuning (that is, the autocorrelation time of a moderately-

well-tuned chain will not be much bigger than for an optimally-tuned chain). Because ﬁnding an

optimal set of tuning parameters is generally hard (requiring much time for trial runs), we will

accept the results using moderately-well-tuned chains.

We found that r = s = 1 gives the best performance for the method of mapping to a discretizing
chain when the slice sampler is used for R(x(cid:48)|x), at least if only fairly small values of r and s are
considered. Recall that r is the number of ¯T updates to do in each temporary mapping, and s is
the number of steps of R(x(cid:48)|x) to propose to move the mark for each ¯T update. Note that a single
slice sampling update will usually evaluate π or π∗ more than once, since an evaluation is needed
for each outward step and each time a point is sampled from the interval found by stepping out.

Therefore if we didn’t use a mapping method we would have to compute π(x) several times for

each slice sampling update. When a mapping method is used, π(x) only needs to be evaluated

16

once each update, for the new state (its value at the previous state having been saved), while
meanwhile, π∗(x) will be evaluated several times.

We tuned the remaining parameter m, the subset size for SOD, or the number of random

columns for Nystr¨om-Cholesky, by trial and error. Generally speaking, m should be between 10%

and 50% of n, depending on the problem. For Nystr¨om-Cholesky, quite good results are obtained
if such a value for m makes π∗ be very close to π(x).

The results are in Table 2, which shows CPU time per iteration times autocorrelation time for

the standard MCMC method, and for other methods the ratio of this with the standard method.

Table 3 shows actual autocorrelation time and CPU time per iteration for each experimental run.

From these results, we see that Subset of Data is overall the most reliable method for forming
a π∗. We can almost always ﬁnd a SOD type of π∗ that leads to more eﬃcient MCMC than
the standard method. Depending on the problem, mapping to a discretizing chain using such a
π∗ can be two to four times faster than standard MCMC, for the Gaussian Process regression
problems we tested. The computational savings go up when the size of the dataset increases. This

is likely because when n is small, evaluation of π is fast, so overhead operations (especially those
not related to n) are not trivial in comparison. The computational saving of π∗ compared to π
will be then less than the m3 to n3 ratio we expect from SOD for large n. Also when n is small,
time to compute C (proportional to pn2) may be signiﬁcant, which also reduces the computational
savings from a π∗ based on SOD.

For some datasets, we can ﬁnd a Nystr¨om-Cholesky π∗ with a small m that can approximate π
well, in which case this method works very nicely. However, for datasets with small length scales
with p = 5, in order to ﬁnd a working π∗ we have to set m to be around 95% of n or greater,
making π∗ as slow as, or even slower than π. This is due to the fact that when the length scale
parameters for the GP are small, the covariance declines rapidly as the input variable changes, so
x and x(cid:48) that are even moderately far apart have low covariance. As a result, we were not able to
ﬁnd eﬃcient mapping method using Nystr¨om-Cholesky with performance even close to standard

MCMC (so no result is shown in the table). On the other hand, when the length scale is large, a

good approximation can be had with a small m (as small as 10% of n). For n = 900 and p = 5

with ARD covariance, Nystr¨om-Cholesky substantially outperforms SOD.

6.3 Experiments with tempered transitions

We have seen in the previous section that the method of mapping to a discretizing chain has a

lot of tuning parameters, and ﬁnding the optimal combination of these tuning parameters is not

easy. The method of tempered transitions actually has more tuning parameters. To start with,

17

#

Length scale

size

type

1

2

3

4

5

6

7

8

9

10

small

isotropic

small

isotropic

small

ARD

long

long

isotropic

ARD

small

isotropic

small

isotropic

small

ARD

long

long

isotropic

ARD

p

1

5

5

5

5

1

5

5

5

5

n

300

300

300

300

300

900

900

900

900

900

m

SOD NYS

Autocorrelation time × CPU time per iteration
TMP TSTD TSOD/TSTD TNYS/TSTD TTMP/TSTD

40

150

100

150

90

60

300

100

100

300

30

-

-

40, 20

100, 50

90, 45

120

130, 65

80

90

-

-

110

90

100, 50

60, 30

-

-

-

-

0.76

1.62

3.39

2.05

5.23

9.06

18.17

25.47

16.86

47.46

0.45

0.81

0.83

0.81

0.66

0.27

0.51

0.43

0.34

0.67

0.51

-

-

0.97

0.85

0.23

-

-

0.40

0.34

1.05

0.14

0.36

0.69

0.51

0.28

-

-

-

-

Table 2: Results of experiments on the ten datasets.

CPU time (s) per iteration

Autocorrelation time

STD SOD NYS TMP

STD SOD NYS TMP

0.26

0.28

0.56

0.13

0.49

3.10

3.76

7.21

1.81

5.66

0.078

0.11

0.14

0.23

0.072

0.19

0.53

0.82

1.48

0.69

1.95

-

-

0.15

0.41

0.83

-

-

0.91

1.75

0.15

0.13

0.14

0.09

0.13

0.61

-

-

-

-

2.90

5.77

6.09

4.32

9.32

11.98

3.53

-

-

5.40

1.67

8.63

15.62

23.04

12.88

16.56

11.16

18.07

10.89

20.37

2.92

4.83

3.53

9.33

8.39

4.63

2.48

4.21

11.24

7.38

8.27

16.18

-

-

7.40

9.14

-

-

-

-

#

1

2

3

4

5

6

7

8

9

10

Table 3: CPU time per iteration and autocorrelation time for each run in Table 2.

18

Mapping to a discretizing chain

Tempered transitions

Figure 2: Comparison of autocorrelation times of the log likelihood for MCMC runs using mapping

to a discretizing chain and using tempered transitions. Dataset #2 is used (with ﬁve covariates,

small length scales, an isotropic covariance function, and 300 observations).

we have to decide the number of “layers” (we call each of ˆTi or ˇTi a “layer”). For each layer,
ˆTi+1−→ ˆxi+1), we have to decide how many MCMC updates to simulate. This reduces the
(e.g. ˆxi
attraction of tempered transitions, but in some situations it does improve sampling eﬃciency.

In the experiments for the method of mapping to a discretizing chain, the results given by

both SOD and Nystr¨om-Cholesky for datasets with n = 300, p = 5 are less satisfatory compared

to others. We tried tempered transitions with these datasets. For simplicity, we used just two

layers, each of which uses SOD to form the transition. The number of observations in each subset
(denoted as mi for transition ˆTi and ˇTi) is listed in Table 2 under the column “TMP” and the time
ratio results are under the column “TTMP/TSTD”. We can see that for all these datasets, tempered
transitions outperform the method of mapping to a discretizing chain, sometimes substantially.

The advantage of tempered transitons is further illustrated n Figure 2, which shows the sample

autocorrelation plots of the log likelihood for both methods, on dataset #2.

7 Discussion and future work

We have introduced two classes of MCMC methods using the “mapping and caching” framework:

the method of mapping to a discretizing chain, and the tempered transition method. Our ex-
periments indicate that for method of mapping to a discretizing chain, when an appropriate π∗

19

05101520253035404550−0.200.20.40.60.8LagSample AutocorrelationMapping to Chain, S,n=300,p=5,ISO05101520253035404550−0.200.20.40.60.8LagSample AutocorrelationMapping by Transition, S,n=300,p=5,ISOis chosen (e.g. SOD approximation of π with an appropriate m), an eﬃcient MCMC can be

constructed by making “local” jumps (e.g. setting r = s = 1). A good MCMC method can also
be constructed using the tempered transitions, with a small number of πi, where each ˆTi and ˇTi
makes only a small update.

These results are understandable. Though π∗ and πi, are broader than π, making small ad-
justments a small number of times will have a good chance to still stay in a high probability area

of π. However, even though the acceptance rate is high, this strategy of making small adjustments

cannot bring us very far from the previous state. On the other hand, if we make large jumps, for

instance, by using large values for r and s in the method of mapping to a discretizing chain, the

acceptance rate will be low, but when a proposal is accepted, it will be much further away from

the previous state, which is favourable for a MCMC method. We haven’t had much success using

this strategy so far, perhaps due to diﬃculty of parameter tuning, but we believe this direction is

worth pursuing. The tempered transition method may be more suitable for this direction, because

moving from one state to another state further away is somewhat similar to moving among modes
— the sequence of ˆTi and ˇTi should be able to “guide” the transition back to a region with high
probability under π.

Acknowledgements

This research was supported by the Natural Sciences and Engineering Research Council of Canada.

R. N. holds a Canada Research Chair in Statistics and Machine Learning.

References

Drineas, P. and Mahoney, M. (2005) “On the Nystrom Method for Approximating a Gram Matrix

for Improved Kernel-Based Learning’’ Journal of Machine Learning Research vol.6 pp.2153-2175

Metropolis, N. and Rosenbluth, A.W. and Rosenbluth, M.N. and Teller, A.H. and Teller, E. (1953)

“Equations of State Calculations by Fast Computing Machines”, Journal of Chemical Physics

vol. 21, pp. 1087-1092.

Neal, R. M. (1993), “Probabilistic Inference Using Markov Chain Monte Carlo Methods”, Tech-

nical Report, Dept. of Computer Science, University of Toronto, CRG-TR-93-1, Available from

http://www.utstat.utoronto.ca/~radford

20

Neal, R. M. (1996) “Sampling from multimodal distributions using tempered transitions”, Statis-

tics and Computing, vol. 6, pp. 353-366

Neal, R. M. (1997), “Monte Carlo implementation of Gaussian process models for Bayesian regres-

sion and classiﬁcation”, Technical Report, Dept. of Statistics, University of Toronto, no. 9702,

Available from http://www.utstat.utoronto.ca/~radford

Neal, R. M. (1998), “Regression and Classiﬁcation Using Gaussian Process Priors”, Bernardo, J.

M. (editor) Bayesian Statistics, vol. 6, Oxford University Press, pp. 475-501

Neal, R. M. (2003), “Slice sampling”, Annals of Statistics, vol. 11, pp. 125-139

Neal, R. M.

(2006), “Constructing Eﬃcient MCMC Methods Using Temporary Map-

ping and Caching”, Talk at Columbia University, December 2006 Available

from

http://www.utstat.utoronto.ca/~radford

Qui˜nonero-Candela, J., Rasmussen, C. E., and Williams, C. K. I. (2007), “Approximation Methods

for Gaussian Process Regression”, Technical Report MSR-TR-2007-124, Microsoft Research

Rasmussen, C. E. and Williams, C. K. I. (2006), Gaussian Process for Machine Learning, the MIT

Press, ISBN 026218253X,

Williams, C.K.I. and Seeger, M. (2001), “Using the Nystr¨om Method to Speed up Kernel Ma-

chines”, Advances in Neural Information Processing Systems 13, pp. 682-688

Woodbury, M.A., (1950) “Inverting modiﬁed matrices”, Memorandum Rept. 42, Statistical Re-

search Group, Princeton University, Princeton, NJ, 1950, 4pp

21

