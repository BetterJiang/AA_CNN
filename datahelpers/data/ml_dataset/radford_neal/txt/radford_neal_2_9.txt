Technical Report No. 0406, Department of Statistics, University of Toronto

4
0
0
2

 
l
u
J
 

5
1

 
 
]

.

R
P
h
t
a
m

[
 
 

1
v
1
8
2
7
0
4
0
/
h
t
a
m
:
v
i
X
r
a

Improving Asymptotic Variance of MCMC Estimators:

Non-reversible Chains are Better

Radford M. Neal

Department of Statistics and Department of Computer Science

University of Toronto, Toronto, Ontario, Canada

http://www.cs.utoronto.ca/∼radford/

radford@stat.utoronto.ca

15 July 2004

Abstract. I show how any reversible Markov chain on a ﬁnite state space that is irreducible, and
hence suitable for estimating expectations with respect to its invariant distribution, can be used to
construct a non-reversible Markov chain on a related state space that can also be used to estimate
these expectations, with asymptotic variance at least as small as that using the reversible chain
(typically smaller). The non-reversible chain achieves this improvement by avoiding (to the extent
possible) transitions that backtrack to the state from which the chain just came. The proof that this
modiﬁcation cannot increase the asymptotic variance of an MCMC estimator uses a new technique
that can also be used to prove Peskun’s (1973) theorem that modifying a reversible chain to reduce
the probability of staying in the same state cannot increase asymptotic variance. A non-reversible
chain that avoids backtracking will often take little or no more computation time per transition
than the original reversible chain, and can sometime produce a large reduction in asymptotic
variance, though for other chains the improvement is slight. In addition to being of some practical
interest, this construction demonstrates that non-reversible chains have a fundamental advantage
over reversible chains for MCMC estimation. Research into better MCMC methods may therefore
best be focused on non-reversible chains.

1 Introduction

Markov chain Monte Carlo (MCMC) is widely used to estimate expectations of functions with
respect to complex, high-dimensional probability distributions, particularly in Bayesian statistics
and statistical physics (see, for example, Liu 2001). An MCMC estimator can be based on any
Markov chain that is irreducible and that has the distribution of interest as its invariant distribution.
However, the choice of Markov chain will aﬀect the eﬃciency with which estimates of expectations
with a given accuracy can be obtained. In this paper, I show that an MCMC estimator based on
a reversible Markov chain on a ﬁnite state space can be improved in terms of asymptotic variance
(or in degenerate cases, not made worse) by transforming it to a Markov chain on a related space

1

that will be non-reversible (except when the state space has only one or two states).

The non-reversible chains produced by this construction avoid, when possible, transitions that
backtrack by returning to the state from which the chain just came. This is done by expanding the
state space to pairs of states of the original chain — representing, roughly speaking, the previous
and current states — and then updating this pair using two operations in sequence, one a swap, and
the other a modiﬁed Gibbs sampling update of the second component that tries to avoid leaving
the state unchanged. Many such modiﬁcations are possible; one that is generally applicable was
introduced by Liu (1996). Though both the swap and the modiﬁed Gibbs sampling update are
reversible, their application in sequence is not reversible.

Simulation of this non-reversible chain will often require little or no more computation time
than simulation of the original reversible chain. The advantage of the non-reversible chain can be
dramatic when the original chain is such that suppressing backtracking has the eﬀect of forcing
movement in the same direction for many steps, thereby suppressing the slow random-walk motion
that reversible chains are subject to. In other cases, however, the improvement may be slight. Aside
from possible practical applications of the particular construction I present, the results indicate that
non-reversible chains are fundamentally superior to reversible chains for MCMC estimation, and
hence research into improved MCMC methods may be best directed toward methods based on
non-reversible chains.

My proof that asymptotic variance will not increase as a result of modifying the chain to avoid
backtracking uses a new technique based on dividing the chains into blocks delimited by transitions
that are aﬀected by the modiﬁcation, and then showing that the only eﬀect of the modiﬁcation is
to partially stratify the sampling for these blocks. This stratiﬁcation can only decrease asymptotic
variance, or leave it unchanged. As an introduction to this technique, I start by giving a new proof
of Peskun’s (1973) theorem that asymptotic variance will not be increased by modifying a reversible
chain to reduce the probability of staying in the same state, while keeping the probability of other
transitions at least as large as before. This proof gives some insight into why the hypothesis of
reversibility is necessary for Peskun’s theorem. This new proof technique holds promise for proving
that other transformations of both reversible and non-reversible chains are also beneﬁcial.

2 Preliminaries

Suppose we wish to estimate the expectation of some function, f (x), with respect to a distribution
with probabilities π(x), where x is in some ﬁnite space X . (Generalizations to inﬁnite spaces will
not be dealt with in this paper.) The MCMC approach to this problem is to simulate a Markov
chain X1, X2, . . . that has π as an invariant distribution — that is, for which

π(y) = Xx∈X

π(x)T (x, y),

for all y ∈ X

(1)

where T (x, y) = P (Xt+1 = y | Xt = x) are the transition probabilities of the Markov chain (assumed
to be the same for all t). If the Markov chain is also irreducible (a series of transitions with non-zero
probability connects any two states), it will have only one invariant distribution, and the estimator

ˆµn =

f (Xt)

1
n

n

Xt=1

2

(2)

will converge to µ = Eπ[f (X)] as n goes to inﬁnity. Furthermore, a Central Limit Theorem
applies, showing that the distribution of ˆµn is asymptotically normal (possibly a degenerate normal
distribution with variance zero). These fundamentals aspects of MCMC are discussed, for example,
by Tierney (1994) and Liu (2001). Some statements of the results mentioned above in these
references make a further assumption that the chain is aperiodic, but this is is not essential (Hoel,
Port, and Stone 1972, Chapter 2, Theorems 3, 5, and 7; Romanovsky 1970, Section 43).

The asymptotic variance of the estimator (2) is deﬁned to be

V∞(ˆµ) = lim
n→∞

n Var(ˆµn)

(3)

Note that this does not depend on the initial distribution for X1. The bias of the estimator will
be of order 1/n, so its asymptotic mean squared error will be equal to its asymptotic variance. In
practice, rather than ˆµn from (2), we would use an estimator based only on Xt with t greater than
some time past which we believe the chain has reached a distribution close to π, but this reﬁnement
(which reduces bias) does not aﬀect the asymptotic variance.

Asymptotic variance can be used as a criterion for which of two Markov chains with the same
invariant distribution is better, on the assumption that the squared error of a practical estimator
based on k consecutive states of the Markov chain will be approximately k V∞. This is not guaran-
teed to be true. For example, if π is uniform over X , a Markov chain that deterministically cycles
through all states in some order will have asymptotic variance of zero, but if the number of states in
X is enormous, an estimator based on simulating a practical number of transitions of this Markov
chain may have large squared error. Nevertheless, in many contexts, V∞ will be a good guide to
practical utility, and it will be used in this paper as the criterion for comparing Markov chains.
Asymptotic variance has previously been used to compare Markov chains by Peskun (1973) and by
Mira and Geyer (2000), as well as by many others.

A Markov chain is said to be “reversible” if its transition probabilities satisfy the following

“detailed balance” condition with respect to π:

π(x)T (x, y) = π(y)T (y, x),

for all x, y ∈ X

(4)

As a consequence, a sequence X1, . . . , Xn from a reversible Markov chain with X1 having initial
distribution π will have the same distribution as the reversed sequence of states, Xn, . . . , X1. De-
tailed balance implies that π is an invariant distribution of the Markov chain, but the converse
need not hold.

Many MCMC methods use reversible Markov chains, notably the widely-used Metropolis-
Hastings algorithm (Hastings 1970). In this algorithm, a transition from state x is performed by
ﬁrst randomly drawing a state, x∗, from some “proposal distribution”, with probabilities S(x, x∗),
and then accepting x∗ as the next state of the chain with probability

a(x, x∗) = min(cid:20) 1,

π(x∗) S(x∗, x)

π(x) S(x, x∗) (cid:21)

(5)

If x∗ is not accepted, the next state of the chain is the same as the current state. The result is
that for y 6= x, the transition probability is T (x, y) = S(x, y)a(x, y). One can easily show that
these transitions satisfy detailed balance with respect to π, and hence leave π invariant. As a

3

special case of the Metropolis-Hastings algorithm, when x consists of several components, x∗ might
diﬀer from x in only a single component, with the value for that component in x∗ being drawn
from its conditional distribution under π given the values of the other components. The acceptance
probability of (5) will then alway be one. This is called a “Gibbs sampling” (or “heatbath”) update
of the component.

Not all Markov chains used for MCMC are reversible, however.

In particular, non-reversible
Markov chains often arise as a result of applying two or more reversible transitions in sequence. If
T1 and T2 are matrices of transition probabilities that satisfy detailed balance with respect to π
(and hence leave π invariant), their product, T1T2, will also leave π invariant, but will typically not
satisfy detailed balance. A common example of this is when Gibbs sampling updates are applied
to each component of state in some deterministic sequence.

There is no reason to avoid non-reversible chains in practical applications of MCMC — what
is essential is that the chain leave π invariant, not that it be reversible with respect to π. The
non-reversibility of deterministic-scan Gibbs sampling is thought to be of little signiﬁcance, but
other non-reversible MCMC methods are designed to exploit non-reversibility to avoid the slow,
diﬀusive movement via a random walk that is typical of reversible Markov chains. Examples include
“overrelaxation” methods (Adler 1981; Neal 1998, 2003) and the “guided Monte Carlo” methods
of Horowitz (1991) and Gustafson (1998).

However, non-reversible Markov chains have often been avoided in theoretical discussions, since
they are harder to analyse than reversible chains. In contrast, Diaconis, Holmes, and Neal (2000)
analysed a particular non-reversible chain and showed that it converges to its invariant distribution
much faster than a related reversible chain. Mira and Geyer (2000) explored whether non-reversible
chains can be transformed to reversible chains with the same asymptotic variance, and found a
method that sometimes does this, but not always, again showing that non-reversible chains might
be superior to reversible chains. These results, and the practical usefulness of some non-reversible
chains, lead one to ask whether any reversible chain can be transformed to a non-reversible chain
that is better. With some caveats — notably, a restriction to ﬁnite state spaces — this paper
provides an aﬃrmative answer to this question.

3 Peskun’s theorem on modifying a reversible chain to avoid

staying in the same state

Before showing how to construct a non-reversible chain that is better than a given reversible chain,
I will present Peskun’s theorem, which shows that modifying a reversible chain to decrease the
probability of staying in the same state, while keeping the probabilities of other transitions at
least as large, cannot increase asymptotic variance. This theorem is relevant to the non-reversible
construction that follows. I also introduce the new proof techniques I use by proving this theorem
in Section 5.

Theorem 1 (Peskun 1973): Let X1, X2, . . . and X ′
2, . . . be two irreducible Markov chains on
the ﬁnite state space X , both of which are reversible with respect to the distribution π, and hence
have π as their unique invariant distribution. Let the transition probabilities for these chains be

1, X ′

T (x, y) = P (Xt+1 = y | Xt = x),

T ′(x, y) = P (X ′

t+1 = y | X ′

t = x)

(6)

4

Let f (x) be some function of state, whose expectation with respect to π is µ. Consider the following
two estimators for µ based on these two chains:

ˆµn =

1
n

n

Xt=1

f (Xt),

ˆµ′

n =

1
n

n

Xt=1

f (X ′
t)

If T and T ′ satisfy the following condition,
T ′(x, y) ≥ T (x, y),

for all x, y ∈ X with x 6= y

then the asymptotic variance of ˆµ′ will be no greater than that of ˆµ.

(7)

(8)

Since it seems ineﬃcient to stay in the same place, Peskun’s theorem might seem obvious.
Two facts show that the situation is more subtle than this. First, only the asymptotic variance is
guaranteed not to increase if oﬀ-diagonal entries in the transition matrix are increased. The variance
of an estimator based on ﬁnite number of transitions, started from π, may increase (Tierney, 1998).
Second, Peskun’s theorem does not hold if the condition that the chains be reversible is omitted.
Here is a counterexample using a non-reversible chain with four states:

1/2

0

1/2

1

+1

1/2

1/2

−1

1

1/2

0

1/2

π(x) = 


µ = 0

1/3 where f (x) = 0
1/6 where f (x) 6= 0

Values of f (x) are shown in the circles. Solid arrows show values of both T (x, y) and T ′(x, y);
dotted arrows are for T (x, y) only; dashed arrows are for T ′(x, y) only. The asymptotic variance
is zero when using T — the chain proceeds clockwise through the four states, never backtracking
(though sometimes staying put in a state where f (x) = µ), with the result that |ˆµn− µ| ≤ 1/n. The
modiﬁcation that produces T ′ disturbs this cyclic behaviour, with the result that the asymptotic
variance becomes greater than zero.

One application of Peskun’s theorem is to motivate a modiﬁed form of Gibbs sampling due
to Liu (1996), which tries to avoid setting a component to the same value it had previously.
Suppose, for example, that the state consists of two components, and the current state is (x, y). As
mentioned above, a Gibbs sampling update for y can be seen as a Metropolis-Hastings update with
a proposal distribution that keeps the ﬁrst component unchanged and draws a new value for the
second component, y∗, from its conditional distribution, π(y|x). In Liu’s modiﬁcation, the proposal
distribution is conﬁned to values for the second component other than the current value, y, with
the probability for proposing y∗ being π(y∗|x) / (1−π(y|x)). The acceptance probability (from (5))
then becomes

a((x, y), (x, y∗)) = min(cid:20) 1,

π(x, y∗) π(y|x) / (1−π(y∗|x))
π(x, y) π(y∗|x) / (1−π(y|x)) (cid:21) = min(cid:20) 1,

1 − π(y∗|x) (cid:21)
1 − π(y|x)

(9)

In the special case that π(y|x) = 1, we never accept the proposal (which is undeﬁned).

5

One can easily verify that this modiﬁcation increases the probability of a transition to all values
except for the current value. However, Peskun’s theorem will not apply if these modiﬁed Gibbs
sampling updates are applied in sequence (producing a non-reversible chain). Peskun’s theorem
does apply if we select a component to update at random, although this is not how Gibbs sampling is
commonly done in practice. Liu’s modiﬁcation of Gibbs sampling will play a role in the construction
of a non-reversible chain that avoids backtracking, which is presented next.

4 Constructing a non-reversible chain from a reversible chain

so as to avoid backtracking

As above, suppose we have an irreducible Markov chain on a ﬁnite state space, X , with transi-
tion probabilities given by T (x, y). Suppose also that this chain is reversible, so these transition
probabilities satisfy the detailed balance condition (4) with respect to some invariant distribution,
π.
In this section, I show how to construct from T a non-reversible Markov chain that avoids
backtracking. The state space for this chain will be ¨X = {(x, y) : T (x, y) > 0}, and it will leave
invariant the distribution ¨π with probabilities deﬁned as follows:

¨π(x, y) = π(x)T (x, y) = π(y)T (y, x)

(10)

One can view ¨π as the distribution for a pair of consecutive states from the original chain, with the
ﬁrst state in the pair drawn from π. The second formula above follows from the reversibility of the
original chain. Note that under ¨π, the marginal distributions of the ﬁrst and second components
are both π. We can therefore estimate the expectation with respect to π of any function deﬁned
on X by averaging the values of either component in pairs distributed according to ¨π.

I will ﬁrst show how to construct a chain with state space ¨X and invariant distribution ¨π that is
essentially the original chain in disguise, but which can later be modiﬁed to prevent backtracking.
This construction is called “expanding” the chain by Kemeny and Snell (1960, Section 6.5). A
transition of this chain consists of the following two operations, applied in sequence:

1) Swap the two components of the state.

2) Replace the second component of this swapped state with a new value sampled from its

conditional distribution (under ¨π) given the current value of the ﬁrst component.

From (10), the conditional probability for the second component to be y, given that the ﬁrst
component is x, is T (x, y). The transition probabilities, ¨T , for this chain can therefore be written
as follows:

¨T ((x0, y0), (x1, y1)) = δ(x1, y0) T (x1, y1)

(11)

where δ(x, y) is one if x = y and zero otherwise.

The ﬁrst operation above leaves ¨π invariant, since ¨π(x, y) = ¨π(y, x), due to the reversibility of
the original chain. The second operation above leaves ¨π invariant as well, since it is simply a Gibbs
sampling update of the second component. Applying these two operations in sequence therefore
also leaves ¨π invariant.

Although both operations above are reversible, applying them in sequence produces a non-
reversible chain (except in degenerate situations). This non-reversibility is of no consequence,

6

however, since the above chain on ¨X essentially replicates the operation of the original chain on X .
Starting from state (x0, x1), the chain will proceed to states (x1, x2), (x2, x3), (x3, x4) etc., with
each xt being drawn according to the probabilities T (xt−1, xt), just as in the original chain. If we
estimate the expectation of f with respect to π by the average value of f applied to the second
components of these states, the result will be exactly the same as an estimate based on states of
the original chain.

To obtain a more interesting non-reversible chain, we can change the second operation above to
use the modiﬁed Gibbs sampling update of Liu (1996), discussed above in Section 3. More generally,
we might modify the second operation in any way that reduces the probability of staying in the same
state, while keeping the probabilities of transitions to other states at least as large as before, and
maintaining reversibility with respect to ¨π. Such a modiﬁed chain, whose transition probabilities we
will write as ¨T ′, will diﬀer substantively from the original reversible chain, since it will reduce the
probability of “backtracking” to the state preceding the current state. For example, starting from
state (x0, x1), a chain with transition probabilities ¨T , equivalent to the original reversible chain,
might proceed to state (x1, x2) and then to (x2, x1) — corresponding to the original reversible
chain moving from x1 to x2 and then back to x1. A modiﬁed chain with transitions ¨T ′ might also
proceed from (x0, x1) to (x1, x2), but after the swap operation of the next transition, the state
(x2, x1) would be updated by a modiﬁed Gibbs sampling operation that has a reduced probability
of leaving the second component equal to x1.

That this avoidance of backtracking cannot increase asymptotic variance is the central result of

this paper. This is stated in the theorem below, which is proved in Section 6.

Theorem 2: Let X1, X2, . . . be an irreducible Markov chain on the ﬁnite state space X having
transition probabilities T (x, y) = P (Xt+1 = y | Xt = x) that satisfy detailed balance with respect to
the distribution with probabilities π(x). Deﬁne a Markov chain (X1, Y1), (X2, Y2), . . . on the state
space ¨X = {(x, y) : T (x, y) > 0} with transition probabilities

¨T ′((x0, x1), (y0, y1)) = δ(x1, y0) U ′

x1(x0, y1)

(12)

where δ(x, y) is one if x = y and zero otherwise, and U ′
for any values of x, y ∈ X , satisfying the following two conditions for all x, y, z ∈ X with y 6= z:

x(y, z) deﬁnes a set of probabilities for z ∈ X

T (x, y) U ′

x(y, z) = T (x, z) U ′
U ′
x(y, z) ≥ T (x, z)

x(z, y)

(13)

(14)

Let f (x) be some function of state, whose expectation with respect to π is µ. Deﬁne the following
two estimators for µ based on these two chains:

ˆµn =

1
n

n

Xt=1

f (Xt),

ˆµ′
n =

1
n

n

Xt=1

f (Y ′
t )

(15)

Then the following properties of the ¨T ′ and the estimators above hold: (a) the chain with transition
probabilities ¨T ′ is irreducible; (b) the transition probabilities ¨T ′
leave invariant the distribution
with probabilities ¨π(x, y) = π(x)T (x, y); (c) if X contains at least three elements, the chain with
transition probabilities ¨T ′ is not reversible with respect to ¨π; (d) the bias of the estimator ˆµ′
n is of
order 1/n; (e) the asymptotic variance of ˆµ′ is no greater than the asymptotic variance of ˆµ.

7

The transition probabilities ¨T (from (11)), which essentially mimic T , can also be written as
¨T ((x0, x1), (y0, y1)) = δ(x1, y0) Ux1(x0, y1), with Ux1(x0, y1) = T (x1, y1). The change from T to ¨T ′
in this theorem can therefore also be seen as a change from ¨T to ¨T ′ or from U to U ′.

The new probabilities U ′

x(y, z) are modiﬁed update probabilities for the second component of
state, with x being the ﬁrst component of state, y the current value of the second component, and
z a new value for the second component. These updates must satisfy detailed balance with respect
to ¨π. In the case of Liu’s modiﬁcation, we ﬁnd using (9) that for all x, y, z ∈ X with z 6= y,
1 − T (x, z) (cid:21)

1 − T (x, z) (cid:21) = min(cid:20) T (x, z)
1 − T (x, y)
1 − T (x, y)

U ′
x(y, y) is determined from the above by the requirement that probabilities sum to one. Note that
if T (x, y) ≥ 1/2, this expression simpliﬁes to T (x, z) / (1−T (x, z)).

min(cid:20) 1,

T (x, z)

1 − T (x, y)

U ′

x(y, z) =

T (x, z)

,

(16)

As a ﬁrst example of such a modiﬁed chain, consider a Markov chain on X = {1, 2, . . . , N} with
transition probabilities of T (x, y) = 1/2 when y = x + 1 or y = x − 1 or x = y = 0 or x = y = N ,
and T (x, y) = 0 otherwise. This chain is irreducible, has the uniform distribution as its invariant
distribution, and is reversible. From (16), we can see that for given (x, y) ∈ ¨X , U ′
x(y, z) = 1 for
some z. The transitions, ¨T ′, of the modiﬁed chain are therefore deterministic. For N = 5, these
transitions follow the arrows in the the diagram below:

(1,2)

(2,3)

(3,4)

(4,5)

(1,1)

(5,5)

(2,1)

(3,2)

(4,3)

(5,4)

The periodic nature of the modiﬁed chain results in the asymptotic variance being zero for any
function of state, whereas for the original chain, the asymptotic variance is of order N 2, due to its
random walk behaviour. This example parallels the chain analysed by Diaconis, Holmes, and Neal
(2000), with c set to zero in the deﬁnition (their equation 4.1) of their chain. Note, however, that
the more general scheme they describe (in their section 5.1) does not correspond to the result of
modifying a random walk Metropolis algorithm to avoid backtracking in the way described here.

As another illustration, consider a chain on X = {1, 2, . . . , N} × {1, 2, . . . , M}, which may be
visualized as dots arranged in an N by M rectangle, in which transitions go up, down, left, or
right, with equal probabilities, except that if such a movement would leave the rectangle, the chain
instead stays in the current state. This chain leaves the uniform distribution invariant. These
transitions are shown below, for N = 6 and M = 3 (the unmarked transition probabilities are 1/4):

1/2

1/2

8

1/2

1/2

The state space, ¨X , of the modiﬁed chain consists of the arrows in the diagram above. The
transitions probabilities, ¨T ′, for the modiﬁed chain, based on the modiﬁed updates of (16) are
illustrated below, for two possible current states:

1/3

1/3

1/3

1/3

2/3

The current states in these diagrams are shown by sold arrows, and possible successor states by
dotted arrows, labeled with their probabilities.

The diagrams below show two paths within the rectangle, produced using the original chain (on

the left) and the modiﬁed chain (on the right):

Note that in two places the original chain backtracks to the preceding state. The new chain never
backtracks in this way, but it is still possible for it to revisit states that were visited two or more
time steps earlier. As a result, the improvement in asymptotic variance is not as dramatic as for
the previous example. Asymptotic variance is improved only by a constant factor, which does not
increase with N and M .

To simulate a chain that has been modiﬁed to avoid backtracking, with transition probabilities
¨T ′, we need to be able to draw a value from X according to the probabilities U ′
x(y, · ). If we use
U ′
x(y, z) deﬁned by (16), and if T (x, z) is non-zero for only a small, known set of z values, we can
do this by explicitly computing the probabilities using (16). This will often be about as eﬃcient as
simulating the original chain.

When T (x, z) is non-zero for many value of z, the following procedure for drawing a z value
from U ′
x(y, · ) as deﬁned by (16) may be useful. If T (x, y) ≥ 1/2, draw a value z∗ according to the
probabilities T (x, z∗). If z∗ = y, let z = y. Otherwise, accept z∗ as the value z with probability
1/(1− T (x, z∗)). If z∗ is not accepted, let z equal y. If instead T (x, y) < 1/2, repeatedly draw
z∗ according to the probabilities T (x, z∗), until a z∗ not equal to y is obtained (which won’t take
long). Accept this z∗ as the value z with probability min[ 1, (1−T (x, y)) / (1−T (x, z) ]. If z∗ is not
accepted, let z equal y.

In some cases, neither of the two procedures described above for simulating from U ′

x(y, · ) may

9

be easy to implement eﬃciently — for instance, T (x, y) for a Metropolis-Hastings transition may
be hard to compute when y = x, since this requires summing the probabilities of rejection for
all possible proposals.
In any case, a rigorous demonstration that a modiﬁed chain that avoids
backtracking can be simulated as quickly as the original chain is too much to expect, because it
might be possible to simulate the original chain especially easily using some special trick that is
not applicable to the modiﬁed chain.

Nevertheless, I think it is fair to say that avoiding backtracking, either using Liu’s modiﬁed Gibbs
sampling update or some other form for U ′
x(y, z), is not the sort of modiﬁcation that inherently
involve a large increase in computation time per transition. That this modiﬁcation decreases asymp-
totic variance (or in degenerate cases, does not increase it) is therefore an important indication that
non-reversible chains have an advantage over reversible chains.

5 A new proof of Peskun’s theorem

As an introduction to the techniques that will be used to prove that the no-backtracking construc-
tion of the previous section does not increase asymptotic variance, I will here use these techniques
to prove Peskun’s theorem, stated as Theorem 1 in Section 3.

In this proof, the “old chain” will refer to the original chain with transition probabilities T , and
the “new chain” will refer to the chain with transition probabilities T ′, which may be smaller than
those of the old chain for self transitions, but are at least as large for transitions between distinct
states. The proof that the estimator for the expectation of any function of state using the new
chain has asymptotic variance at least as small as the corresponding estimator using the old chain
will proceed as follows:

1) We reduce the problem to comparing asymptotic variances when T and T ′ diﬀer only for

transitions involving two states, A and B.

2) We can view simulations of the old and new chains as diﬀering only for certain “delta”

transitions involving states A and B.

3) These delta transitions divide the Markov chain simulation into blocks of states, which start
and end in either state A or state B. We can rewrite the old and new estimators, ˆµ and ˆµ′,
in terms of the lengths of these blocks and the sums of the function values for states in these
blocks.

4) We see that blocks starting and ending with A and blocks starting and ending with B are
equally likely, but may have diﬀerent distributions for their contents. In contrast, blocks that
start with A and end with B have essentially the same distribution of content as blocks that
start with B and end with A.

5) The only diﬀerence between the old and new chains is that in the new chain the sampling
for “homogeneous” blocks (starting and ending in the same state) is stratiﬁed — there are
the same number of blocks starting and ending with A as blocks starting and ending with
B, whereas the split between these types is random in the old chain (albeit with equal
probabilities for the two types of homogeneous blocks).

6) Finally, this stratiﬁcation will lower (or at least not increase) the asymptotic variance.

10

Step 1: Looking at one pair of states is enough
Whenever T ′(x, y) ≥ T (x, y) for all x 6= y, we can get from T to T ′ by a series of steps that each
change transition probabilities for only a single pair of states. For example, consider the following
steps from T to T ′ that both satisfy detailed balance with respect to π = [ 0.4 0.4 0.2 ]:

T = 


0.4 0.4 0.2
0.4 0.4 0.2
0.4 0.4 0.2

 ⇒ 



0.3 0.5 0.2
0.5 0.3 0.2
0.4 0.4 0.2

 ⇒ 



0.3 0.5 0.2
0.5 0.2 0.3
0.4 0.6 0.0




= T ′

(17)

Furthermore, if the detailed balance condition (4) holds for T and for T ′, it will hold also for all
the intermediate transition probabilities (such as those in the middle matrix above), since the pair
of transition probabilities for any x and y (with x 6= y) at any intermediate point will be either the
same as for T or the same as for T ′.

It is therefore enough to prove Peskun’s Theorem when T and T ′ diﬀer for only two states, say

A and B. The transition probabilities for the old and new chain will then be related as follows:

T ′(x, y) = T (x, y), when x /∈ {A, B} or y /∈ {A, B}
T ′(A, B) = T (A, B) + δA
T ′(A, A) = T (A, A) − δA,
T ′(B, B) = T (B, B) − δB
T ′(B, A) = T (B, A) + δB,

(18)

where δA and δB are positive.

Step 2: Marking “delta” transitions

Transitions T and T ′ diﬀer only if the current state is A or B, and then only with respect to how a
probability mass of δA or δB is assigned to new states A or B. We can mark such “delta” transitions
while simulating the Markov chain.

The standard way to simulate a Markov chain is as follows: For each state, x, partition the
interval [0, 1) into intervals [ℓ(x, y), h(x, y)) such that h(x, y) − ℓ(x, y) = T (x, y); to simulate a
transition out of state x, generate a random variate, U , that is uniformly distributed on [0, 1), and
move to the state, y, for which ℓ(x, y) ≤ U < h(x, y). We can choose to simulate the old transitions,
T , using partitions in which ℓ(A, A) = ℓ(B, B) = 0. With such a choice, we can write the algorithm
for simulating a transition of the old chain in the manner on the left below, in which a slight change
yields the simulation algorithm for the new chain shown below on the right:

Old chain:

New chain:

U ∼ Uniform(0, 1)
if Xt = A and U < δA then

Xt+1 = A, mark this transition

else if Xt = B and U < δB then

Xt+1 = B, mark this transition

else

U ∼ Uniform(0, 1)
if Xt = A and U < δA then

Xt+1 = B, mark this transition

else if Xt = B and U < δB then

Xt+1 = A, mark this transition

else

Xt+1 = y such that U ∈ [ℓ(Xt, y), h(Xt, y))

Xt+1 = y such that U ∈ [ℓ(Xt, y), h(Xt, y))

Clearly, T and T ′ diﬀer only for the “delta” transitions marked above.

11

Step 3: Using delta transitions to deﬁne blocks

We can use the markings of delta transitions to divide a simulation of one of these Markov chains
into “blocks” of consecutive states, that both start and end with either state A or state B. Note
that states A and B may also occur at places other than the start and end of a block. It is possible
for a blocks to consist of only a single A or a single B.

Since asymptotic variance does not depend on the initial state distribution, let’s suppose that

P (X1 = A) = P (X1 = B) = 1/2, so that the chains will begin at the start of a block.

For the old chain, with transitions T , we might see blocks like this:

A B B

B B B B

B

B

A A A

A A A

B B A A A A

For the new chain, with transitions T ′, the blocks might look like this:

A B A

A B

B A

BA

A B A

B

B A

A B

BA

B A

The diﬀerence is that in the old chain, the state stays the same when crossing a block boundary,
whereas for the new chain, it changes from A to B or from B to A.

We can view the simulation in terms of these blocks, and write the estimates ˆµ and ˆµ′ in terms

of the lengths of the blocks and the sums of f for states in these blocks. For the old chain,

ˆµn ≈

k

Xi=1

Hi.

k

Xi=1

Li

(19)

where Hi is the sum for f (Xt) for states Xt in block i, Li is the length of block i, and k is the
number of blocks in the n iterations of the chain. The equality is only approximate because there
may be a partial block after block k. Estimation in terms of blocks is discussed further in Step 6
of the proof.

Step 4: Probabilities of the four types of blocks and their contents

Blocks come in four types — AA, BB, AB, BA — based on start and end states. For both the
old and new chains, the probabilities of these types (ie, their frequencies of occurrence in a long
realization of the chain) satisfy

P (AA) = P (BB) and P (AB) = P (BA)

(20)

We can show this using the fact that both T and T ′ leave π invariant. In particular, for the old

chain,

π(B) = π(A)T (A, B) + π(B)T (B, B) + Xx /∈{A,B}

π(x)T (x, B)

while for the new chain, using the relationships in (18),

π(B) = π(A)(T (A, B) + δA) + π(B)(T (B, B) − δB) + Xx /∈{A,B}

π(x)T (x, B)

from which it follows that π(A)δA = π(B)δB.

12

(21)

(22)

This lets us show that for a state, Xt, from the old chain (with t being large),

P (Xt starts block with A) = P (Xt−1 = A) P (delta transition at t−1| Xt−1 = A) = π(A) δA (23)
P (Xt starts block with B) = P (Xt−1 = B) P (delta transition at t−1| Xt−1 = B) = π(B) δB (24)
and hence P (Xt starts block with A) = P (Xt starts block with B). In the same way, we see that
P (Xt ends block with A) = P (Xt ends block with B). It follows that

P (AA) + P (AB) = P (BB) + P (BA) and P (AA) + P (BA) = P (BB) + P (AB)

(25)

so P (AA) = P (BB) and P (AB) = P (BA).

Similarly, for a state, Xt, from the new chain (with t being large),

P (Xt starts block with A) = P (Xt−1 = B) P (delta transition at t−1| Xt−1 = B) = π(B) δB (26)
P (Xt starts block with B) = P (Xt−1 = A) P (delta transition at t−1| Xt−1 = A) = π(A) δA (27)
and hence P (Xt starts block with A) = P (Xt starts block with B) for the new chain as well, and
similarly P (Xt ends block with A) = P (Xt ends block with B), from which it again follows that
P (AA) = P (BB) and P (AB) = P (BA).

Although blocks of type AA and blocks of type BB are equally common, the distributions for
their contents — and hence for their length and for the sum of values of f (x) over states in the
block — will generally be diﬀerent. In contrast, blocks of type AB and blocks of type BA have
the same distribution of content — except that the BA blocks are the reversals of the AB blocks,
which has no eﬀect on the sum of f (x) for states in the block. This equivalence of AB and BA
blocks is a consequence of the chains being reversible, and holds for both the old and new chains.

To illustrate: The probability of block AQB occurring at some large time t in the old chain is

P (Xt = A & block starts) P (Xt+1 = Q| Xt = A)P (Xt+2 = B & block ends| Xt+1 = Q)

= π(A) δA T (A, Q) T (Q, B) δB = δAδB π(A) T (A, Q) T (Q, B)
= δAδB T (Q, A) π(Q) T (Q, B) = δAδB T (Q, A) T (B, Q) π(B)
= π(B) δB T (B, Q) T (Q, A) δA

(28)

(29)

(30)

which is also the probability of block BQA occurring at time t. For the new chain, the probability
of block AQB occurring at time t is

P (Xt = A & block starts) P (Xt+1 = Q| Xt = A)P (Xt+2 = B & block ends| Xt+1 = Q)

= π(B) δB T (A, Q) T (Q, B) δB = π(A) δA T (A, Q) T (Q, B) δB

(31)

which is the same as for the old chain, and the same as for block BQA.

Step 5: In the new chain, sampling for homogeneous blocks is stratiﬁed

Rather than simulate the chains one state at a time, let’s imagine simulating the chain block by
block. To show the relationship between the old and the new chains, I’ll show how this simulation
can be done in a coupled fashion.

13

To do this, we will need the probability that a block is “homogeneous” — that it ends with the

same state it begins with — which is

P (ends with A| starts with A) =

=

P (AA)

P (AA) + P (AB)

P (BB)

P (BB) + P (BA)

= P (ends with B | starts with B)

(32)

Call this probability h, and note that it is the same for the old chain and the new chain, since the
transitions within a block, and the marking of its end, are the same for both chains. Note as well
that the distribution of the contents of a block, given its type, is the same for the old chain and
the new chain.

We can now simulate block transitions for the “old” and “new” chains as follows. We’ll assume
H below is sampled the same for both chains, but that the simulation of the contents of blocks is
not coupled between the old and new chains.

Old chain:

H ∼ Bernoulli(h)
if H = 1 then

if previous block ended with A

simulate an AA block

else

simulate a BB block

else

if previous block ended with A

simulate an AB block

else

New chain:

H ∼ Bernoulli(h)
if H = 1 then

if previous block ended with A

simulate a BB block

else

simulate an AA block

else

if previous block ended with A

simulated an AB block, then reverse it

else

simulate an AB block, then reverse it

simulate an AB block

Comparing the simulations for the old and new chains, we see that they produce the same
sequence of homogeneous/non-homogeneous blocks. However, for the new chain, the homogeneous
blocks alternate between AA blocks and BB blocks. This is true both when one homogeneous
block follows another, and when any number of non-homogeneous blocks intervene.
In the old
chain, the type of homogeneous block changes only when an odd number of non-homogeneous
blocks intervene. This is illustrated below:

Old chain:

A B B

B B B BB

BBB

A A A

A

A A A A

B B A A

A A A

New chain: A B A A

BB

A A

B B

A B

A A

B B

A A

B

BA

A

B B

A A

Because AA blocks alternate with BB blocks in the new chain, sampling within the new chain
is stratiﬁed in this respect — that is, the number of AA blocks will be equal to the number of BB
blocks (plus or minus one). We can also see this by noting that in the new chain, every block ending
in A (except the last) is paired with a following block beginning with B. Letting NAA, NAB, NBA,
and NBB be the numbers of blocks of each type, it follows that

| (NAA +NBA) − (NBB + NBA)| ≤ 1

(33)

and hence |NAA − NBB| ≤ 1.

14

Step 6: Stratiﬁcation of homogeneous blocks won’t increase asymptotic variance

The intuition behind the proof is now complete: Sampling with the new chain is stratiﬁed with
respect to blocks of type AA and BB. Furthermore, this is the only diﬀerence between the old and
new chains, since the sum of the values of f for states in a block has the same distribution for AB
blocks and BA blocks. Since we expect that stratiﬁcation will not increase asymptotic variance
(and will typically decrease it), the asymptotic variance for the new chain should be no larger than
for the old chain.

To justify this formally, we need two lemmas whose detailed statements, proofs, and applications
to this proof are found in the Appendix. The ﬁrst lemma says that the asymptotic variance
(appropriately deﬁned) of an estimator based on a simulation that continues for some speciﬁed
number of blocks is the same as that of an estimator based on a simulation that continues for some
speciﬁed number of Markov chain transitions. This is true because the Central Limit Theorem
implies that asymptotically there is very little diﬀerence between simulating for a speciﬁed number
of blocks and simulating for the number of transitions equal to the expected number for that many
blocks.

Accordingly, we can compare the old and new chains in the context of simulations that continue
for a speciﬁed number of blocks. The second lemma justiﬁes the idea that the stratiﬁcation of AA
and BB blocks will not increase the asymptotic variance of estimators based on such simulations.
Stratiﬁcation is of course well-known to be beneﬁcial (or at least not harmful) in the context of
independent sampling from two populations. The lemma shows that this continues to be true when,
as here, the stratiﬁcation is only partial (the ratio of homogeneous to non-homogeneous blocks is
not ﬁxed), sampling has a Markov chain aspect rather than being independent, and the estimator
takes the form of a ratio rather than a linear function of the sampled variables.

This proof provides some insight into why Peskun’s theorem needs the premise that the chains
are reversible with respect to π, rather than the weaker premise that they leave π invariant. This
premise is used at two points in the proof. In Step 1, the reduction to old and new chains that diﬀer
only for transitions involving two states would not be possible for non-reversible chains, since there
would be no guarantee that the intermediate chains linking old and new chains diﬀering for several
pairs of states would leave π invariant. This is not relevant to the counterexample in Section 3,
however, since it involves two chains that already diﬀer only with regard to transitions between
two states (state A on the left and B on the right).

The reason the new chain in the counterexample has higher asymptotic variance relates to the
second use of reversibility, in Step 4, where the contents of blocks of type BA are seen to have
essentially the same distribution as the contents of blocks of type AB, apart from a reversal that
does not aﬀect the sum of f (x), which is what matters for the estimates. For the counterexample,
the sum of f (x) for blocks of type AB will always be +1, whereas this sum will always be −1 for
blocks of of type BA. In contrast, the sum of f (x) for blocks of type AA or type BB will always
be 0. Examining Step 5 of the proof, one can see that while the new chain stratiﬁes sampling
for blocks of type AA and BB, the old chain stratiﬁes sampling for blocks of type AB and BA.
For a non-reversible chain, stratifying between types AB and BA may be more important, so it is
possible for the old chain to have lower asymptotic variance than the new chain.

15

6 Proof that modifying a reversible chain to avoid backtracking

doesn’t increase asymptotic variance

We are now in a position to prove the main result of this paper, Theorem 2 in Section 4. I will
address the ﬁve claims in the theorem in order. The proof of the ﬁnal and principal claim (e), that
the modiﬁed chain has asymptotic variance at least as small as the original chain, will follow closely
the proof of Peskun’s theorem presented in the previous section.

Claim (a): The modiﬁed chain, with transition probabilities ¨T ′ is irreducible
Let (a, b) and (c, d) be distinct states in ¨X . We need to show that (a, b) and (c, d) are linked by
transitions with non-zero probability under ¨T ′. From the deﬁnition of ¨X , T (c, d) > 0. If b = c,
then ¨T ′((a, b), (c, d)) = U ′
c(a, d) ≥ T (c, d) > 0. Otherwise, from the irreducibility of T , there
exist states x1, . . . , xk in X with T (b, x1) > 0, T (xk, c) > 0, and T (xi, xi+1) > 0 for i = 1, . . . , k−1,
and hence (b, x1), (x1, x2), . . . , (xk, c) ∈ ¨X . Furthermore,

¨T ′((a, b), (b, x1)) = U ′
¨T ′((b, x1), (x1, x2)) = U ′

···
¨T ′((xk, c), (c, d)) = U ′

b(a, x1) ≥ T (b, x1) > 0

x1(b, x2) ≥ T (x1, x2) > 0

c(xk, d) ≥ T (c, d) > 0

(34)

(35)

(36)

(37)

(38)

(39)

(40)

(41)

(42)

Claim (b): The transition probabilities ¨T ′ leave ¨π invariant
This is implied by the way ¨T ′ was constructed in Section 4. It can also be shown directly as follows:

X(x0,y0)∈ ¨X

¨π(x0, y0) ¨T ′((x0, y0), (x1, y1)) = X(x0,y0)∈ ¨X

π(x0) T (x0, y0) δ(y0, x1) U ′

x1(x0, y1)

π(x0) T (x0, x1) U ′

x1(x0, y1)

π(x1) T (x1, x0) U ′

x1(x0, y1)

= Xx0∈X
= Xx0∈X
= Xx0∈X
= π(x1) T (x1, y1) Xx0∈X

π(x1) T (x1, y1) U ′

x1(y1, x0)

U ′

x1(y1, x0)

= π(x1) T (x1, y1) = ¨π(x1, y1)

Claim (c): If X contains at least three elements, ¨T ′ is not reversible
Let a, b, and c be three distinct elements of X . Since the original chain is irreducible, either
T (a, b) > 0 or there exist distinct x1, . . . , xn such that T (a, x1) > 0, T (xn, b) > 0 and T (xi, xi+1) > 0
for i = 1, . . . , n−1. Similarly, either T (b, c) > 0 or there exist distinct y1, . . . , ym linking b to c. One
way or another, we can ﬁnd distinct x, y, z such that T (x, y) > 0 and T (y, z) > 0 — if T (a, b) = 0,

16

take three consecutive states from a, x1, . . . , xn, c; if T (b, c) = 0, take three consecutive states from
b, y1, . . . , ym, c; and if T (a, b) > 0 and T (b, c) > 0, use a, b, c. The states (x, y) and (y, z) are
in ¨X , and have positive probability under ¨π. (Note that all states in X have positive probability
under π, since the original chain is irreducible.) It follows that ¨T ′((x, y), (y, z)) is positive. However,
¨T ′((y, z), (x, y)) is zero, since z 6= x. The modiﬁed chain with transition probabilities ¨T ′ is therefore
non-reversible.

Claim (d): The bias of the estimator ˆµ′

n is of order 1/n

The modiﬁed chain leaves invariant the distribution ¨π(x, y) = π(x)T (x, y) = π(y)T (y, x). The

marginal distribution for the second component of state under ¨π is Px ¨π(x, y) = π(y). An MCMC

estimator that looks at a function of the second component of state will therefore converge to the
correct expectation of this function with respect to π, with bias of order 1/n, in accordance with
the standard properties of MCMC estimators, as discussed in Section 2.

Claim (e): The asymptotic variance of ˆµ′ is no greater than that of ˆµ

This is the principal claim. Its proof will follow the same steps as the proof of Peskun’s theorem
in Section 5.

Step 1: Looking at one pair of states is enough
In Section 4, a chain on ¨X was deﬁned that was essentially equivalent to the original chain on
X , with transitions T . The transition probabilities for this chain were deﬁned in equation (11) to
have the form ¨T ((x0, y0), (x1, y1)) = δ(x1, y0) T (x1, y1), which can be seen as an instance of the
deﬁnition of ¨T ′ in equation (12), with Ux1(x0, y1) = T (x1, y1). We can therefore view Theorem 2
as claiming that changing from this U to some other U ′ that satisﬁes conditions (13) and (14)
will not increase asymptotic variance. More generally, we will see that a change from transitions ¨T
based on any U satisfying condition (13) to transitions ¨T ′ based on some other U ′ that also satisﬁes
this condition and for which

U ′
x(y, z) ≥ Ux(y, z),

for all x, y, z ∈ X with y 6= z

(43)

will not increase asymptotic variance.

Any such change from U to U ′ can be expressed as a sequences of changes, each of which aﬀects
Ux(y, z) only when x is some particular state O, and y and z are both either state A or state B.
In order for condition (13) to be satisﬁed, such U and U ′ must be related as follows:

U ′
x(y, z) = Ux(y, z), when x 6= O or x /∈ {A, B} or y /∈ {A, B}
U ′
U ′
O(A, B) = UO(A, B) + δA
O(A, A) = UO(A, A) − δA,
U ′
U ′
O(B, B) = UO(B, B) − δB
O(B, A) = UO(B, A) + δB,

(44)

The resulting transition probabilities, ¨T and ¨T ′, will be identical except for the following states:
¨T ′((A, O), (O, A)) = ¨T ((A, O), (O, A)) − δA,
¨T ′((B, O), (O, A)) = ¨T ((B, O), (O, A)) + δB,

¨T ′((A, O), (O, B)) = ¨T ((A, O), (O, B)) + δA
¨T ′((B, O), (O, B)) = ¨T ((B, O), (O, B)) − δB

(45)

17

The rest of the proof will assume that ¨T and ¨T ′ diﬀer only as above. The chain with transition
probabilities ¨T will be referred to as the “old” chain, while that with transitions probabilities ¨T ′
will be called the “new” chain.

Steps 2 & 3: Deﬁning blocks delimited by “delta” transitions

We can mark the transitions in a realization of either the old or the new the chain that would have
been diﬀerent in the other chain — ie, those transitions where the addition or subtraction or δA or
δB in (46) would have made a diﬀerence. This can be done using a simulation procedure entirely
analogous to that described in Step 2 of the proof of Peskun’s theorem.

As in Step 3 of that proof, we can use these “delta” transitions to deﬁne the boundaries between
“blocks” of states in a realization of the old or new chain. Such blocks will always begin with state
(O, A) or (O, B) and end with state (A, O) or (B, O). If we assume that we start the chain in state
(O, A) or (O, B), a typical sequence of blocks for the old chain might look like

(O,A)

(B,O) (O,B) (A,O) (O,A)

(A,O) (O,A)

(A,O)

(O,A)

(B,O) (O,B)

whereas a typical block sequence for the new chain might look like

(O,A)

(B,O) (O,A)

(A,O) (O,B) (B,O) (O,A)

(B,O)

(O,A)

(B,O) (O,A)

Note that states (O, A), (A, O), (O, B), and (B, O) may occur within a block, as well as at the
beginning and end. The diﬀerence between the two chains is that states in the old chain on each
side of a block boundary are simply reversals of each other, whereas in the new chain, one of the
states on the two sides of a boundary will contain A and the other B.

Step 4: Probabilities of the four types of blocks and their contents

Blocks starting with (O, A) and ending with (A, O) will be called AA blocks, those starting with
(O, A) and ending with (B, O) will be called AB blocks, and similarly for blocks of types BB and
BA. The way these blocks are produced is illustrated in the diagram below:

AA block

A

AB block

BA block

O

18

B

BB block

An AA block starts with state (O, A), represented in the diagram by an arrow from O to A.
Transitions from this state lead to an arrow from A to some other state, then arrows between other
states, and eventually to an arrow from some state to A, followed by an arrow from A to O, which
represents the the state (A, O). For the block to end here, a delta transition must occur at this
point, which happens with probability δA. An AB block also starts with an arrow from O to A,
but ends with and arrow pointing to state B, and then an arrow from B to O that is followed by
a delta transition.

To ﬁnd how the probabilities of the four types of blocks, P (AA), P (AB), P (BA), and P (BB)
are related, we can start by noting that since condition (13) applies to both U and U ′, which are
related by (44), it follows that

T (O, A) δA = T (O, A) (U ′

= T (O, B) (U ′

O(A, B) − UO(A, B))
O(B, A) − UO(B, A)) = T (O, B) δB

(46)

Next, note that the probability that a block beginning with (O, A) starts at time t (with t being

large) in the old chain is

¨π(A, O)) δA = π(A) T (A, O) δA = π(O) T (O, A) δA

(47)

Using (46), we see that this is equal to the probability that a block beginning with (O, B) starts
at time t, which is

¨π(B, O) δB = π(B) T (B, O) δB = π(O) T (O, B) δB

Similarly, the probability that a block ends with state (A, O) at time t,

¨π(A, O) δA = π(A) T (A, O) δA = π(O) T (O, A) δA

is equal to the probability that a block ends with state (B, O) at time t,

¨π(B, O) δB = π(B) T (B, O) δB = π(O) T (O, B) δB

(48)

(49)

(50)

It follows that P (AA) + P (AB) = P (BB) + P (BA) and P (AA) + P (BA) = P (BB) + P (AB) for
the old chain, from which we can conclude that P (AA) = P (BB) and P (AB) = P (BA). A similar
argument shows this for the new chain as well.

The distribution of the contents of blocks of type AA may diﬀer from that for the contents of
blocks of type BB, but blocks of type AB and blocks of type BA can be viewed as reversals of
each other. Consider, for example, a block consisting of the following states:

(O, A), (A, X), (X, Y ), (Y, B), (B, O)

(51)

As seen above, the probability of a block starting with (O, A) at some given time is π(A) T (A, O) δA
(in both the old and the new chain, as seen from (46) and the reversibility of T ). Multiplying by the
probabilities of the subsequent transitions, and the probability of a delta transition from (B, O),
the probability of the AB block above occurring at a given time in the old chain is

π(A) T (A, O) δA UA(O, X) UX (A, Y ) UY (X, B) UB(Y, O) δB

(52)

19

This is also the probability in the new chain, since U and U ′ are the same for all except delta
transitions. Now consider the reversal of the block in (51):

(O, B), (B, Y ), (Y, X), (X, A), (A, O)

The probability of this block occurring at a given time is

π(B) T (B, O) δB UB(O, Y ) UY (B, X) UX (Y, A) UA(X, O) δA

(53)

(54)

We can see that (52) and (54) are equal as follows, using the reversibility of T with respect to π
and the fact that U satisﬁes condition (13):

π(A) T (A, O) UA(O, X) UX (A, Y ) UY (X, B) UB(Y, O)

= π(A) T (A, X) UA(X, O) · UX(A, Y ) UY (X, B) UB(Y, O)
= UA(X, O) · π(X) T (X, A) UX (A, Y ) · UY (X, B) UB(Y, O)
= UA(X, O) · π(X) T (X, Y ) UX (Y, A) · UY (X, B) UB(Y, O)
= UX(Y, A) UA(X, O) · π(Y ) T (Y, X) UY (X, B) · UB(Y, O)
= UX(Y, A) UA(X, O) · π(Y ) T (Y, B) UY (B, X) · UB(Y, O)
= UY (B, X) UX (Y, A) UA(X, O) · π(B) T (B, Y ) UB(Y, O)
= UY (B, X) UX (Y, A) UA(X, O) · π(B) T (B, O) UB(O, Y )
= π(B) T (B, O) UB(O, Y ) UY (B, X) UX (Y, A) UA(X, O)

(55)

(56)

(57)

(58)

(59)

(60)

(61)

(62)

Step 5: Sampling in the new chain is stratiﬁed

As was done in the proof of Peskun’s theorem, we can now imagine simulating both the old chain
and the new chain one block at a time. For each block, we decide whether it should be homogeneous
(of type AA or BB) or non-homogeneous (of type AB or BA). The probability of a block being
homogeneous is the same regardless of whether the block starts with A or B, since the results in
Step 4 imply that P (AA) / (P (AA) + P (AB)) = P (BB) / (P (BB) + P (BA)). The probability
that a block is homogeneous is also the same for the old chain and the new chain. If we make the
same random decisions as to whether or not blocks are homogeneous in the old and new chains,
the sequence of homogeneous versus non-homogeneous blocks will be the same for the two chains.

The only signiﬁcant diﬀerence between the old and new chains is that in the new chain the
sequence of homogeneous blocks alternates between AA blocks and BB blocks, and hence the
number of AA blocks is equal to the number of BB blocks (or diﬀers by only one). This arises
for exactly the same reasons as in the proof of Peskun’s theorem — if NAA, NAB, NBA, and NBB
are the numbers of blocks of each type, | (NAA +NBA) − (NBB + NBA)| ≤ 1 in the new chain,
and hence |NAA − NBB| ≤ 1. The sampling for AA and BB blocks is therefore stratiﬁed in the
new chain, but not in the old chain. The old chain stratiﬁes sampling of AB and BA blocks, but
since the distributions for the contents of blocks of types AB and BA are the same (apart from a
reversal, which doesn’t aﬀect sums of function values), this stratiﬁcation in the old chain has no
eﬀect.

20

Steps 6: Stratiﬁcation will not increase asymptotic variance

Finally, as in the proof of Peskun’s theorem, we can apply Lemma 1 in the Appendix to show that
the asymptotic variance using a simulation that continues for a speciﬁed number of blocks is the
same as that using a simulation for a speciﬁed number of transitions. We can then apply Lemma 2
to show that the block-by-block simulation of the new chain, which is stratiﬁed with respect to AA
and BB blocks, will have asymptotic variance at least as small as for the old chain.

7 Conclusion

This paper shows how any reversible Markov chain can be transformed into a non-reversible chain
that tries to avoid backtracking to the state visited immediately before. This transformation
never increases the asymptotic variance of an MCMC estimator using the chain, and will usually
decrease it. Sometimes, the decrease in asymptotic variance is dramatic, but other times it is small.
In general, one would expect the decrease in asymptotic variance to be small when a state of the
original Markov chain has many possible successor states (of roughly similar probability), since
in this situation, even the original chain will rarely backtrack. In many circumstances, the chain
that avoids backtracking will require little or no more time per transition than the original chain,
though this cannot be guaranteed in all cases.

The particular transformation described in this paper may sometimes be of practical use. For
many problems, however, the gains may be slight or non-existent.
In particular, for problems
with continuous state spaces, and continuous transition distributions, exact backtracking has zero
probability of occurring anyway. There seems to be scope for generalizing the idea of trying to
avoid backtracking, however. Possibilities include trying to avoid backtracking to any of the past
several states, and trying to avoid backtracking not just to the exact previous state, but also to
anywhere in its vicinity.

More generally, the results in this paper indicate that non-reversible Markov chains have a
fundamental advantage over reversible chains, and that the search for better MCMC methods may
therefore be best focused on non-reversible chains. The proof techniques used in this paper may
be useful in analysing such methods.

Acknowledgements

I thank Longhai Li and Jeﬀrey Rosenthal for helpful discussions. This research was supported by
the Natural Sciences and Engineering Research Council of Canada. The author holds a Canada
Research Chair in Statistics and Machine Learning.

Appendix: Statements and proofs of lemmas

The following lemmas were used in the proofs of Sections 5 and 6.

The ﬁrst lemma justiﬁes looking at the asymptotic variance of simulations continuing for a
speciﬁed number of blocks, instead of a speciﬁed number of transitions. To apply it to blocks
deﬁned by “delta” transitions, we can extend the state space of the Markov chain to include an

21

indicator of whether the current state is the last in a block (essentially moving the decision whether
a transition from state A or B is to be “marked” back to the previous transition into state A or
B). With this extension, the set S below can consist of states A or B at the end of a block.
Lemma 1: Let X1, X2, . . . be an irreducible Markov chain on a ﬁnite state space X , with invariant
distribution π(x). Let S be some non-empty subset of X , and let f (x) be some function of state,
whose expectation with respect to π is µ. Deﬁne

N (k) = min nn :

IS (Xt) = ko

n

Xt=1

(63)

(64)

(65)

where IS is the indicator function for S. Consider the following two families of estimators:

n

ˆµn =

1
n

f (Xt),

˜µk =

Xt=1

1

N (k)

N (k)

f (Xt)

Xt=1

The asymptotic variances of these estimators are the same:

lim
n→∞

n Var(ˆµn) = lim
n→∞

n Var(˜µ⌈nπ(S)⌉)

Proof: Without loss of generality, suppose µ = 0. We will see that as n increases, nVar(ˆµn) and
nVar(˜µ⌈nπ(S)⌉) both approach (n + n1/2+ǫ)Var(ˆµn+n1/2+ǫ), where ǫ is a positive constant to be set
below. The diagram below may help to visualize the proof:
n − n1/2+ǫ

n + n1/2+ǫ

n

0

N (⌈nπ(S)⌉)

First, we note that (n + n1/2+ǫ) ˆµn+n1/2+ǫ = nˆµn + n1/2+ǫZ, where Z is the average of f (Xi) for
i from n + 1 to n + n1/2+ǫ. Dividing by pn + n1/2+ǫ, we get

pn + n1/2+ǫ ˆµn+n1/2+ǫ = qn/(n + n1/2+ǫ)h√nˆµn + nǫZi

(66)

As n increases, the ﬁrst factor on the right will go to one. By the Central Limit Theorem for
Markov chains, |Z| will be less than (n1/2+ǫ)−1/2+ǫ = n−1/4+ǫ2 with probability approaching one
exponentially fast, so if ǫ is in (0, (√2−1)/2), the term nǫZ will go to zero. It follows that nVar(ˆµn)
will approach (n+n1/2+ǫ)Var(ˆµn+n1/2+ǫ). (Since f (x) is bounded, an exponentially small probability
of a large value for |Z| cannot aﬀect this limit.)

From the Central Limit Theorem, we can also conclude that N (⌈nπ(S)⌉) will be in the interval

(n−n1/2+ǫ, n+n1/2+ǫ) with probability approaching one exponentially fast. If so, we can write

(n+n1/2+ǫ) ˆµn+n1/2+ǫ = N (⌈nπ(S)⌉) ˜µ⌈nπ(S)⌉ + (n+n1/2+ǫ−N (⌈nπ(S)⌉)) Y

(67)

where Y is the average of f (Xi) for i from N (⌈nπ(S)⌉) + 1 to n + n1/2+ǫ. Dividing by pn + n1/2+ǫ,

we get

pn + n1/2+ǫ ˆµn+n1/2+ǫ =

N (⌈nπ(S)⌉)

np1 + n−1/2+ǫh√n˜µ⌈nπ(S)⌉ + (√n/N (⌈nπ(S)⌉)) KY i

where K = n+n1/2+ǫ−N (⌈nπ(S)⌉) will be in (0, 2n1/2+ǫ) if N (⌈nπ(S)⌉) is in (n−n1/2+ǫ, n+n1/2+ǫ).

(68)

22

|KY | will be less than (2n1/2+ǫ)1/2+ǫ =
By the Central Limit Theorem for Markov chains,
21/2+ǫn1/4+ǫ+ǫ2 with a probability that approaches one exponentially fast. Since N (⌈nπ(S)⌉) will
approach n, we can see that nVar(˜µ⌈nπ(S)⌉) will approach (n + n1/2+ǫ)Var(ˆµn+n1/2+ǫ).

Since nVar(ˆµn) and nVar(˜µ⌈nπ(S)⌉) both approach the same value as n goes to inﬁnity, they must

also have the same limit, as the lemma states.

The second lemma justiﬁes the claim that partial stratiﬁcation of sampling for blocks cannot
increase asymptotic variance. In applying this lemma to the proofs in Sections 5 and 6, Z1, Z2, . . .
are identiﬁers for the type of each block (we can use 0 = AA, 1 = BB, and 2 = AB or BA), which
form a Markov chain, since the distribution for the type of the next block depends only on the
type of the previous block. The types of blocks for the modiﬁed chain are Z ′
2, . . ., which are
stratiﬁed with respect to 0 and 1. In the applications of this lemma, H corresponds to the sum of
the values of f for all states in a block, and L corresponds to the number of states in this block.

1, Z ′

Lemma 2: Let Z1, Z2, . . . be an irreducible Markov chain with state space {0, 1, 2}, whose invariant
distribution, ρ, satisﬁes ρ(0) = ρ(1). Let Qz for z = 0, 1, 2 be distributions for pairs (H, L) ∈ R×R+
having ﬁnite second moments. Conditional on Z1, Z2, . . ., let (Hi, Li) be drawn independently from
QZi. Deﬁne

Z ′

i = 


Zi

Zk +

i−1

Pj=1

I{0,1}(Zj) (modulo 2)

if Zi = 2

if Zi 6= 2

(69)

where k = min{i : Zi 6= 2}. (In other words, the Z ′
i are the same as the Zi except that the positions
where 0 or 1 occurs have their values changed to a sequence of alternating 0s and 1s.) Conditional
on Z1, Z2, . . ., let (H ′
i. Deﬁne two families of estimators as
follows:

i) be drawn independently from QZ ′

i, L′

n

n

n

n

Rn =

Xi=1

Hi.

Li,

Xi=1

R′

n =

Xi=1

i .
H ′

L′
i

Xi=1

Then the asymptotic variance of R′ is no greater than that of R. In other words,

lim
n→∞

nVar(R′

n) ≤ lim

n→∞

nVar(Rn)

(70)

(71)

n,1−N ′

n,m) and |N ′

i=1 I{m}(Zi) and N ′

Proof: Let Nn,m = (1/n)Pn

n,0| ≤ 1/n, so the proportions of pairs from Q0 and Q1 are stratiﬁed in R′

i). Note that E(Nn,m) =
E(N ′
n. By
the Central Limit Theorem for Markov chains, Nn = (Nn,0, Nn,1, Nn,2) and N ′
n,2)
asymptotically have (degenerate) multivariate normal distributions, with the same mean vectors,
2, . . . is not a Markov chain, a
though diﬀerent covariance matrices.
Markov chain can be deﬁned on an extended state space that includes Z ′

n,m = (1/n)Pn

(Note that although Z ′

i=1 I{m}(Z ′

1, Z ′

n = (N ′

n,0, N ′

n,1, N ′

i as a component.)

The asymptotoic variances of Rn and R′

n can be decomposed as follows:

nVar(Rn) = nVar(E(Rn|Nn)) + E(nVar(Rn|Nn))
nVar(R′
n))

n)) + E(nVar(R′

n) = nVar(E(R′

n|N ′

n|N ′

(72)

(73)

Note that the distribution of Rn given Nn = N is the same as the distribution of R′
Writing Rn = (1/n)P Hi / (1/n)P Li, we can apply the Central Limit Theorem to the numerator

n given N ′

n = N .

23

and denominator, then use the delta rule to conclude that Rn given Nn is asymptotically normal,
with asymptotic variance that depends only on Nn (not on n). Since Nn and N ′
n have the same
means and both are asymptotically normal, we can apply the delta rule again to conclude that the
second term on the right in (72) is equal to the second term on the right in (73).

Looking at the ﬁrst terms in (72) and (73), we can rewrite nVar(E(Rn|Nn)) and nVar(E(R′

n|N ′

n))

as follows:

nVar(E(Rn|Nn)) = nVar(E(E(Rn|Nn)|Nn,2)) + E(nVar(E(Rn|Nn)|Nn,2))
nVar(E(R′
n,2))

n)) = nVar(E(E(R′

n,2)) + E(nVar(E(R′

(74)

(75)

n|N ′

n|N ′

n)|N ′

n|N ′

n)|N ′

n,2 given N ′
n)|N ′

Since the expectations of Nn,1 and Nn,2 given Nn,2 = N are the same as the expectations of N ′
and N ′
n,2 = N , we can conclude that E(E(Rn|Nn)|Nn,2 = N ) is asymptotically equal
n,2 = N ). The distributions of Nn,2 and N ′
to E(E(R′
n,2 are the same, so it follows that the
ﬁrst terms on the right in (74) and (75) are asymptotically equal. Due to stratiﬁcation, N ′
n,0 and
N ′
n,2) = 0. It follows that the second terms on
the right in (74) and (75) are related by

n,1 are ﬁxed given N ′

n,2, so that Var(E(R′

n)|N ′

n|N ′

n|N ′

n,1

E(nVar(E(Rn|Nn)|Nn,2)) ≥ E(nVar(E(R′

n|N ′

n)|N ′

n,2)) = 0

(76)

Combining these results, we can conclude that asymptotically Var(E(Rn|Nn)) ≥ Var(E(R′

and ﬁnally, that Var(Rn) is asymptotically at least as large as Var(R′

n).

n|N ′

n)),

References

Adler, S. L. (1981) “Over-relaxation method for the Monte Carlo evaluation of the partition function

for multiquadratic actions”, Physical Review D, vol. 23, pp. 2901-2904.

Diaconis, P., Holmes, S., and Neal, R. M. (2000) “Analysis of a non-reversible Markov chain

sampler”, Annals of Applied Probability, vol. 10, pp. 726-752.

Gustafson, P. (1998) “A guided walk Metropolis algorithm”, Statistics and Computing, vol. 8,

pp. 357-364.

Hastings, W. K. (1970) “Monte Carlo sampling methods using Markov chains and their applica-

tions”, Biometrika, vol. 57, pp. 97-109.

Hoel, P. G., Port, S. C., and Stone, C. J. (1972) Introduction to Stochastic Processes, Waveland

Press.

Horowitz, A. M. (1991) “A generalized guided Monte Carlo algorithm”, Physics Letters B, vol. 268,

pp. 247-252.

Kemeny, J. G. and Snell, J. L. (1960) Finite Markov Chains, Van Nostrand.

Peskun, P. H. (1973) “Optimum Monte-Carlo sampling using Markov chains”, Biometrika, vol. 60,

pp. 607-612.

Liu, J. S. (1996) “Peskun’s theorem and a modiﬁed discrete-state Gibbs sampler”, Biometrika,

vol. 83, pp. 681-682.

Liu, J. S. (2001) Monte Carlo Strategies in Scientiﬁc Computing, Springer-Verlag.

24

Mira, A. and Geyer, C. J. (2000) “On non-reversible Markov chains”, in N. Madras (editor) Monte

Carlo Methods, Fields Institute / AMS, pp. 95-110.

Neal, R. M. (1998) “Suppressing random walks in Markov chain Monte Carlo using ordered overre-
laxation”, in M. I. Jordan (editor) Learning in Graphical Models, Dordrecht: Kluwer Academic
Publishers.

Neal, R. M. (2003) “Slice sampling” (with discussion), Annals of Statistics, vol. 31, pp. 705-767.

Romanovsky, V. I. (1970) Discrete Markov Chains, translated from the Russian by E. Seneta,

Wolters-Noordhoﬀ.

Tierney, L. (1994) “Markov chains for exploring posterior distributions” (with discussion), Annals

of Statistics, vol. 22, pp. 1701-1762.

Tierney, L. (1998) “A Note on Metropolis-Hastings kernels for general state spaces”, Annals of

Applied Probability, vol. 8, pp. 1-9.

25

