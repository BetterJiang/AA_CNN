5
0
0
2

 

v
o
N
8

 

.

 
 
]
T
S
h
t
a
m

[
 
 

1
v
6
1
2
1
1
5
0
/
h
t
a
m
:
v
i
X
r
a

Technical Report No. 0511, Department of Statistics, University of Toronto

Estimating Ratios of Normalizing Constants Using

Linked Importance Sampling

Radford M. Neal

Department of Statistics and Department of Computer Science

University of Toronto, Toronto, Ontario, Canada

http://www.cs.utoronto.ca/∼radford/

radford@stat.utoronto.ca

8 November 2005

Abstract. Ratios of normalizing constants for two distributions are needed in both Bayesian statistics,
where they are used to compare models, and in statistical physics, where they correspond to diﬀerences
in free energy. Two approaches have long been used to estimate ratios of normalizing constants.
The ‘simple importance sampling’ (SIS) or ‘free energy perturbation’ method uses a sample drawn
from just one of the two distributions. The ‘bridge sampling’ or ‘acceptance ratio’ estimate can be
viewed as the ratio of two SIS estimates involving a bridge distribution. For both methods, diﬃcult
problems must be handled by introducing a sequence of intermediate distributions linking the two
distributions of interest, with the ﬁnal ratio of normalizing constants being estimated by the product
of estimates of ratios for adjacent distributions in this sequence. Recently, work by Jarzynski, and
independently by Neal, has shown how one can view such a product of estimates, each based on
simple importance sampling using a single point, as an SIS estimate on an extended state space. This
‘Annealed Importance Sampling’ (AIS) method produces an exactly unbiased estimate for the ratio
of normalizing constants even when the Markov transitions used do not reach equilibrium. In this
paper, I show how a corresponding ‘Linked Importance Sampling’ (LIS) method can be constructed
in which the estimates for individual ratios are similar to bridge sampling estimates. As a further
elaboration, bridge sampling rather than simple importance sampling can be employed at the top
level for both AIS and LIS, which sometimes produces further improvement. I show empirically that
for some problems, LIS estimates are much more accurate than AIS estimates found using the same
computation time, although for other problems the two methods have similar performance. Like AIS,
LIS can also produce estimates for expectations, even when the distribution contains multiple isolated
modes. AIS is related to the ‘tempered transition’ method for handling isolated modes, and to a
method for ‘dragging’ fast variables. Linked sampling methods similar to LIS can be constructed
that are analogous to tempered transitions and to this method for dragging fast variables, which may
sometimes work better than those analogous to AIS.

1

1 Introduction

Consider two distributions on the same space, with probability mass or density functions π0(x) =
p0(x)/Z0 and π1(x) = p1(x)/Z1. Suppose that we are not able to directly compute π0 and π1, but only
p0 and p1, since we do not know the normalizing constants, Z0 and Z1. We wish to ﬁnd a Monte Carlo
estimate for the ratio of these normalizing constants, Z1/Z0, which we sometimes denote by r, using
samples of values drawn (at least approximately) from π0 and from π1. Sometimes, we may know Z0,
in which case we can arrange for it to be one, so that estimation of this ratio will give the numerical
value of Z1. Other times, we will be able to obtain only the ratio of normalizing constants, but this
may be suﬃcient for our purposes.

In statistical physics, x represents the state of some physical system, and the distributions are

typically ‘canonical’ distributions having the following form (for j = 0, 1):

pj(x) = exp(−βjU (x, λj ))

(1)

where U (x, λj) is an ‘energy’ function, which may depend on the parameter λj, and βj is the inverse
temperature of system j. Many interesting properties of the systems are related to the ‘free energy’,
deﬁned as − log(Zj) / βj . Often, only the diﬀerence in free energy between systems 0 and 1 is relevant,
and this is determined by the ratio Z1/Z0.

In Bayesian statistics, x comprises the parameters and latent variables for some statistical model,
π0 is the prior distribution for these quantities (for which the normalizing constant is usually known),
and π1 is the posterior distribution given the observed data. We can compute p1(x) as the product
of the prior density for x and the probability of the data given x, but the normalizing constant, Z1,
is diﬃcult to compute. We can interpret Z1 as the ‘marginal likelihood’ — the probability of the
observed data under this model, integrating over possible values of the model’s parameters and latent
variables. The marginal likelihood for a model indicates how well it is supported by the data.

Although I will use simple distributions as illustrations in this paper, in real applications, x is
usually high dimensional, and at least one of π0 and π1 is usually quite complex. Accordingly, sam-
pling from these distributions generally requires use of Markov chain methods, such as the venerable
Metropolis algorithm (Metropolis, et al 1953). See (Neal 1993) for a review of Markov chain sampling
methods. Sometimes, however, π0 will be relatively simple, and independent points drawn from it can
be generated eﬃciently, as would often be the case with the prior distribution for a Bayesian model,
or for a physical system at inﬁnite temperature (β0 = 0).

Many methods for estimating ratios of normalizing constants from Monte Carlo data have been
investigated in the physics literature (for a review, see (Neal 1993, Section 6.2)), and later rediscov-
ered in the statistics literature (Gelman and Meng 1998). A logical method to start with is ‘simple
importance sampling’ (SIS), also called ‘free energy perturbation’, based on the following identity,
which can easily be proved on the assumption that no region having zero probability under π0 has

2

non-zero probability under π1:

Z1
Z0

= Eπ0(cid:20) p1(X)

p0(X)(cid:21) ≈

1
N

N

Xi=1

p1(x(i))
p0(x(i))

=

1
N

N

Xi=1

ˆr(i)
SIS = ˆrSIS

(2)

In the above equation, Eπ0 denotes an expectation with respect to the distribution π0, which is
estimated by a Monte Carlo average over points x(i), . . . , x(N ) drawn from π0 (either independently,
or using a Markov chain sampler). Here and later, ˆrM will denote an estimate of r = Z1/Z0, found by
method M. If this estimate is an average of unbiased estimates based on a number of samples, these
individual estimates will be denoted by ˆr(i)
M .

The simple importance sampling estimate, ˆrSIS, will be poor if π0 and π1 are not close enough —
in particular, if any region with non-negligible probability under π1 has very small probability under
π0. Such a region would have an important eﬀect on the value of r, but very little information about
it would be contained in the sample from π0. In such a situation, it may be possible to obtain a good
estimate by introducing intermediate distributions. Parameterizing these distributions in some way
using η, we can deﬁne a sequence of distributions, πη0, . . . , πηn, with η0 = 0 and ηn = 1 so that the ﬁrst
and last distributions in the sequence are π0 and π1, with the intermediate distributions interpolating
between them. We can then write

Z1
Z0

=

Zηj+1
Zηj

n−1

Yj=0

(3)

Provided that πηj+1 and πηj are close enough, we can estimate each of the factors Zηj+1 /Zηj using
simple importance sampling, and from these estimates obtain an estimate for Z1/Z0.

We can obtain good estimates in a wider range of situations, or using fewer intermediate distributions
(sometimes none), by applying a technique introduced by Bennett (1976), who called it the ‘acceptance
ratio’ method. This method was later rediscovered by Meng and Wong (1996), who called it ‘bridge
sampling’. Lu, Singh, and Kofke (2003) provide a recent review and assessment. One way of viewing
this method is that it replaces the simple importance sampling estimate for Z1/Z0 by a ratio of
estimates for Z∗/Z0 and Z∗/Z1, where Z∗ is the normalizing constant for a ‘bridge distribution’,
π∗(x) = p∗(x)/Z∗, which is chosen so that it is overlapped by both π0 and π1. Using simple importance
sampling estimates for Z∗/Z0 and Z∗/Z1, we can obtain the estimate

Z1
Z0

= Eπ0(cid:20) p∗(X)

p0(X)(cid:21) . Eπ1(cid:20) p∗(X)

p1(X)(cid:21) ≈

1
N0

N0

Xk=1

p∗(x0,k)

p0(x0,k) . 1

N1

p∗(x1,k)
p1(x1,k)

N1

Xk=1

= ˆrbridge

(4)

where x0,1, . . . , x0,N0 are drawn from π0 and x1,1, . . . , x1,N1 are drawn from π1.

One simple choice for the bridge distribution is the ‘geometric’ bridge:

pgeo

∗ (x) = pp0(x)p1(x)

3

(5)

which is in a sense half-way between π0 and π1. As discussed by Bennett (1976) and by Meng and
Wong (1996), the asymptotically optimal choice of bridge distribution is

popt
∗ (x) =

p0(x)p1(x)

r(N0/N1)p0(x) + p1(x)

(6)

where r = Z1/Z0. Of course, we cannot use this bridge distribution in practice, since we do not know
r. We can use a preliminary guess at r to deﬁne an initial bridge distribution, however, which will give
us a bridge sampling estimate for Z1/Z0. Using this estimate as the new value of r, we can reﬁne our
bridge distribution, iterating this process as many times as desired. The result of this iteration can
also be viewed as a maximum likelihood estimate for r, as discussed by Shirts, et al (2003), who argues
on this basis that it is asymptotically as good as any estimate for r. I have found that estimates with
r set iteratively are often better than those found with the true value of r (which does not contradict
optimality of the true value for a ﬁxed choice of bridge distribution).

If π0 and π1 do not overlap suﬃciently, no bridge distribution will produce good estimates, and
we will have to introduce intermediate distributions as in equation (3). Note, however, that the
bridge sampling estimate with either of the above bridge distributions converges to the correct ratio
asymptotically as long there is some region that has non-zero probability under both π0 and π1, a
much weaker requirement than that for simple importance sampling.

This advantage of bridge sampling over SIS can be seen in a simple example involving distributions
that are uniform over an interval of the reals. Let p0(x) = I(0,3)(x) and p1(x) = I(2,4)(x), so that
Z0 = 3 and Z1 = 2. The simple importance sampling estimate of equation (2) does not work, as it
converges to 1/3 rather than 2/3. However, using a bridge distribution with p∗(x) = I(2,3), which is
eﬀectively what both popt
∗ will be in this example, the bridge sampling estimate of equation (4)
converges to the correct value, since the numerator converges to 1/3 and the denominator to 1/2.

∗ and pgeo

Although both simple importance sampling and bridge sampling have been successfully used in many
applications, they have some deﬁciencies. One issue is that although the SIS estimate of equation (2)
is unbiased for Z1/Z0, the bridge sampling estimate of equation (4) is not, and the same would
appear to be the case for an estimate using intermediate distributions (via equation (3)). This is of
no direct importance, particularly since we are often more interested in log(Z1/Z0) than in Z1/Z0
itself. However, it does preclude averaging independent replications of the bridge sampling estimate
to obtain a better estimate, since the bias would prevent convergence to the correct value as the
number of replications increases. A more vexing diﬃculty is that, except sometimes for π0, sampling
from the distributions πη must usually be done by Markov chain methods, which approach the desired
distribution only asymptotically. To speed convergence, the Markov chain for sampling πηj is often
started from the last state sampled for πηj−1 , but it is unclear how many iterations should then be
discarded before an adequate approximation to the correct distribution is reached.

Surprisingly, these diﬃculties can be completely overcome when using simple importance sampling
with a single point. As shown by Jarzynski (1997, 2001), and later independently by myself (Neal 2001),
an estimate for Z1/Z0 using intermediate distributions as in equation (3) will be exactly unbiased if

4

each of the ratios Zηj+1/Zηj is estimated using the simple importance sampling estimate of equation (2)
with N = 1, sampling each distribution with a Markov chain update starting with the point for the
previous distribution. Averaging the estimates obtained from M independent replications of this
process (called ‘runs’) produces the following estimate:

Z1
Z0 ≈

1
M

M

Xi=1

n−1

Yj=0

pηj+1 (x(i)
j )
pηj (x(i)
j )

=

1
M

M

Xi=1

ˆr(i)
AIS = ˆrAIS

(7)

0 , . . . , x(M )

0

invariant to x(i)

are drawn independently from π0, and each x(i)
j

Here, x(1)
for j > 0 is generated by applying
a Markov chain transition that leaves πηj
j−1. This single Markov transition (which
could, however, consist of several Metropolis or other updates if we so choose), will usually not be
enough to reach equilibrium, but the estimate ˆrAIS is nevertheless exactly unbiased, and will converge
to the true value as M increases, provided that no region having zero probability under πηj has non-
zero probability under πηj+1. This can be proved by showing how the estimate above can be seen as
a simple importance sampling estimate on an extended state space that includes the values sampled
for the intermediate distributions.

I call this method ‘Annealed Importance Sampling’ (AIS), since the sequence of distributions used
often corresponds to an ‘annealing’ procedure, in which the temperature is gradually decreased. As I
discuss in (Neal 2001), this allows the procedure to sample diﬀerent isolated modes of the distribution
on diﬀerent runs, properly weighting the points obtained from each of these runs to produce the correct
probability for each mode. AIS is related to an earlier method for moving between isolated modes
that I call ‘tempered transitions’ (Neal 1996). In a recent paper (Neal 2004), I show how tempered
transitions can be modiﬁed to produce a method for eﬃcient Markov chain sampling when some of
the state variables are ‘fast’ — ie, when it is possible to more quickly recompute the probability of a
state when only these fast variables change than when the other ‘slow’ variables change as well. In this
method, the fast variables are ‘dragged’ through intermediate distributions in order to produce more
appropriate values to go with a proposed change to the slow variables. Deciding whether to accept
the ﬁnal proposal involves what is in eﬀect an estimate of the ratio of normalizing constants for the
conditional distributions of the fast variables.

In this paper, I show how the ideas behind Annealed Importance Sampling and bridge sampling
can be combined.
I call the resulting method ‘Linked Importance Sampling’ (LIS), since the two
samples needed for bridge sampling are linked by a single state that is used in both. Intermediate
distributions can be used, with each distribution being linked by a single state to the next distribution.
In contrast to bridge sampling, LIS estimates are unbiased, and as is the case for AIS, they remain
exactly unbiased even when intermediate distributions are used, and when sampling is done using
Markov chain transitions that have not converged to their equilibrium distributions.

Crooks (2000) mentions a diﬀerent way of combining AIS with bridge sampling — since AIS esti-
mates are simple importance sampling estimates on an extended state space, we can combine ‘forward’
and ‘reverse’ estimates to produce a bridge sampling estimate that may be superior. I will call this

5

method ‘bridged AIS’. Similarly, such a top-level application of bridge sampling can be combined with
the low-level application of bridge sampling in LIS, giving what I call ‘bridged LIS’.

Using tests on sequences of one-dimensional distributions, I demonstrate that for some problems
LIS is much more eﬃcient than AIS — a result that should be expected, since in extreme cases, such
as for the uniform distributions discussed above, the simple importance sampling estimates underlying
AIS do not converge to the correct answer even asymptotically, whereas bridge sampling estimates do.
For some other problems, however, AIS and LIS perform about equally well. The bridged version of
AIS sometimes performs much better than the unbridged version, but still performs less well than LIS
and its bridged version on some problems. I also analyse the asymptotic properties of AIS and LIS
for some types of distribution, providing additional insight into their behaviour.

Variants of tempered transitions and of my method for dragging fast variables can be constructed
that are analogous to LIS rather than to AIS. I discuss the ‘linked’ variant of tempered transitions
brieﬂy, and include a more detailed description of a linked version of dragging, which may sometimes be
better than the version related to AIS. I conclude by discussing some possibilities for future research.

2 The Linked Importance Sampling procedure

Assume that we can evaluate the unnormalized probability or density functions pη(x), for any value
of the parameter η, with the normalized form of such a distribution being denoted by πη. The values
η = 0 and η = 1 deﬁne the two distributions we are interested in, for which the normalizing constants
are Z0 and Z1. A sequence of n−1 intermediate values for η deﬁne distributions that will assist in
estimating the ratio of these normalizing constants, r = Z1/Z0. We denote the values of η for the
distributions used by η0, . . . , ηn, with η0 = 0 and ηn = 1. Typically, ηj < ηj+1 for all j.

For problems in statistical physics, η might be proportional to the inverse temperature, β, of
equation (1), or might map to a value for λ. For a Bayesian inference problem, η might be a power
that the likelihood is raised to, so that η = 0 causes the data to be ignored, and η = 1 gives full
weight to the data; the ratio Z1/Z0 will then be the marginal likelihood. In both of these examples,
progressing in small steps from η = 0 to η = 1 is not only useful in estimating Z1/Z0, but also often
has an ‘annealing’ eﬀect, which helps avoid being trapped in a local mode of the distribution.

2.1 Details of the LIS procedure

For each distribution, πη, assume we have a pair of Markov chain transition probability (or density)

functions, denoted by Tη(x, x′) and T η(x, x′), satisfying R Tη(x, x′)dx′ = 1 and R T η(x, x′)dx′ = 1, for

which the following mutual reversibility relationship holds:

πη(x) Tη(x, x′) = πη(x′) T η(x′, x),

for all x and x′

(8)

From this relationship, one can easily show that both Tη and T η leave πη invariant — ie, that

R πη(x)Tη(x, x′)dx = πη(x′), and the same for T η. If Tη is reversible (ie, satisﬁes ‘detailed balance’),

6

then T η will be the same as Tη. Non-reversible transitions often arise when components of state are
updated in some predetermined order, in which case the reverse transition simply updates components
in the opposite order. As a special case, Tη might draw the next state from πη independently of the
current state. Such independent sampling may often be possible for T0.

These Markov chain transitions are used to obtain samples that are approximately drawn from each
of the n+1 distributions, πη0, . . . , πηn . We assume that we can begin sampling from π0 by drawing
a single point independently from π0. For j > 0, we begin sampling from πηj by selecting a link
state, xj−1∗j, from the sample associated with πηj−1. For all j, we produce a sample of Kj +1 states
from this starting point by applying a total of Kj forward (Tηj ) or reversed (T ηj ) Markov transitions.
Link states are selected using bridge distributions, pj ∗j+1, which are deﬁned in terms of pηj and pηj+1,
perhaps using the form of equation (5) or (6), with p0 replaced by pηj and p1 by pηj+1.
In detail, the Linked Importance Sampling procedure produces M estimates, ˆr(1)

LIS, . . . , ˆr(M )

LIS , that

are averaged to produce the ﬁnal estimate, ˆrLIS. Each ˆr(i)

LIS is obtained by performing the following:

The LIS Procedure

1) Pick an integer ν0 uniformly at random from {0, . . . , K0}, and then set x0,ν0 to a value drawn

from πη0.

2) For j = 0, . . . , n, sample Kj +1 states drawn (at least approximately) from πηj as follows:

a) If j > 0: Pick an integer νj uniformly at random from {0, . . . , Kj}, and then set xj,νj to

xj−1∗j.

b) For k = νj + 1, . . . , Kj, draw xj,k according to the forward Markov chain transition prob-

abilities Tηj (xj,k−1, xj,k). (If νj = Kj, do nothing in this step.)

c) For k = νj − 1, . . . , 0, draw xj,k according to the reverse Markov chain transition probabil-

ities T ηj (xj,k+1, xj,k). (If νj = 0, do nothing in this step.)

d) If j < n: Pick a value for µj from {0, . . . , Kj} according to the following probabilities:

Π0(µj | xj) =

pj ∗j+1(xj,µj )

pηj (xj,µj ) .

and then set xj ∗j+1 to xj,µj .

Kj

Xk=0

pj ∗j+1(xj,k)

pηj (xj,k)

(9)

3) Set µn to a value chosen uniformly at random from {0, . . . , Kn}. (This selection has no eﬀect on

the estimate, but is used in the proof of correctness.)

4) Compute the estimate from this run as follows:

ˆr(i)
LIS =

n−1

Yj=0

1

Kj + 1

Kj

Xk=0

pj ∗j+1(xj,k)

pηj (xj,k) .

1

Kj+1 + 1

Kj+1

Xk=0




pj ∗j+1(xj+1,k)

pηj+1(xj+1,k) 


(10)

(Note that most of the factors of 1/(Kj + 1) and 1/(Kj+1 + 1) cancel, giving a ﬁnal result of
(Kn +1) / (K0 +1), but the redundant factors are retained above for clarity of meaning.)

7

π
1/2

π
0

π
1

Figure 1: An illustration of Linked Importance Sampling. One intermediate distribution is used, with
η1 = 1/2. The distributions π0, π1/2, and π1 are represented by ovals enclosing the regions of high
probability under each distribution. Nine Markov chain transitions are performed at each stage. The
two link states are shown as black dots. The initial and ﬁnal states (indexed by ν0 and µn) are shown
as gray dots. Other states generated by the forward and reverse Markov chain transitions are shown
as empty dots. For this run, ν0 = 4, µ0 = 9, ν1 = 1, µ1 = 8, ν2 = 3, and µ2 = 7.

The result of performing steps (1) through (3) is illustrated in Figure 1. After M runs of this procedure,
the ﬁnal estimate is computed as

ˆrLIS =

ˆr(i)

LIS

1
M

M

Xi=1

(11)

The crucial aspect of Linked Importance Sampling is that when moving from distribution πηj to
πηj+1, a link state, xj ∗j+1, is randomly selected from among the sample of points xj,1, . . . , xj,Kj+1 that
are associated with πηj . We can view the link state as part of the sample associated with πηj+1 as well
as that associated with πηj . Accordingly, when using the ‘optimal’ bridge of equation (6), I will set
N0/N1 to (Kj +1)/(Kj+1 +1), though the proof of optimality for bridge sampling does not guarantee
that this is an optimal choice when using this bridge distribution for LIS.

2.2 Proof that LIS estimates are unbiased

In order to prove that ˆr(i)
LIS is an unbiased estimate of r = Z1/Z0, we can regard steps (1) through (3)
above as deﬁning a distribution, Π0, over all the quantities involved in the procedure — namely, xj,
µj, and νj, for j = 0, . . . , n, with xj representing xj,0, . . . , xj,Kj . We then consider the procedure for
generating these same quantities in reverse, which operates as follows:

8

The Reverse LIS Procedure

1) Pick an integer µn uniformly at random from {0, . . . , Kn}, and then set xn,µn to a value drawn

from πηn.

2) For j = n, . . . , 0, sample Kj +1 states drawn (at least approximately) from πηj as follows:

a) If j < n: Pick an integer µj uniformly at random from {0, . . . , Kj}, and then set xj,µj to

xj ∗j+1.

b) For k = µj + 1, . . . , Kj, draw xj,k according to the forward Markov chain transition prob-

abilities Tηj (xj,k−1, xj,k). (If µj = Kj, do nothing in this step.)

c) For k = µj − 1, . . . , 0, draw xj,k according to the reverse Markov chain transition proba-

bilities T ηj (xj,k+1, xj,k). (If µj = 0, do nothing in this step.)

d) If j > 0: Pick a value for νj from {0, . . . , Kj} according to the following probabilities:

Π1(νj | xj) =

pj−1∗j(xj,νj )

pηj (xj,νj ) .

Kj

Xk=0

pj−1∗j(xj,k)

pηj (xj,k)

(12)

and then set xj−1∗j to xj,νj .

3) Set ν0 to a value chosen uniformly at random from {0, . . . , K0}.

This reverse procedure also deﬁnes a distribution over all the quantities generated (xj, µj, and νj for
j = 0, . . . , n), which will be denoted by Π1.

We now deﬁne the unnormalized probability (density) functions P0(x, µ, ν) = Z0Π0(x, µ, ν) and
P1(x, µ, ν) = Z1Π1(x, µ, ν). The ratio of normalizing constants for these distributions is obviously
r = Z1/Z0. We can estimate this ratio by simple importance sampling, using the ratios

P1(x, µ, ν)
P0(x, µ, ν)

=

Z1 Π1(µn) πηn(xn,µn)

Z0 Π0(ν0) πη0(x0,ν0)

n−1

n

Qj=0
Qj=1

n

Π1(µj)

Π0(νj)

n

n

Π1(νj | xj) Π1(ν0)

Qj=0
Qj=0
Π0(xj | νj, xj,νj )

Qj=1
Π1(xj | µj, xj,µj )
Qj=0
Π0(µj | xj) Π0(µn)

n−1

From Steps (2b) and (2c) of the forward and reverse procedures, along with the mutual reversibility

(13)

(14)

(15)

(16)

relationship of equation (8), we see that

n

Π0(xj | νj, xj,νj ) =

=

=

Yk=νj+1
Yk=νj+1

n

Tηj (xj,k−1, xj,k) ·

Tηj (xj,k−1, xj,k) ·

νj −1

Yk=0
Yk=0

νj −1

T ηj (xj,k+1, xj,k)

Tηj (xj,k, xj,k+1)

πηj (xj,k)
πηj (xj,k+1)

πηj (xj,0)
πηj (xj,νj )

n

Yk=1

Tηj (xj,k−1, xj,k)

9

and similarly,

Π1(xj | µj, xj,µj ) =

πηj (xj,0)
πηj (xj,µj )

n

Yk=1

Tηj (xj,k−1, xj,k)

(17)

From this, we see that parts of the ratio in equation (13) can be written as

n

n

Qj=0
Qj=0

Z1 πηn(xn,µn)

Z0 πη0(x0,ν0)

Π1(xj | µj, xj,µj )

Π0(xj | νj, xj,νj )

=

pηn(xn,µn)
pη0(x0,ν0)

πηj (xj,νj )
πηj (xj,µj )

=

n

Yj=0

n−1

Yj=0

pηj+1(xj,µj )
pηj (xj,µj )

(18)

The last step uses the fact that for j = 1, . . . , n, xj,νj = xj−1∗j = xj−1,µj−1.

From Steps (1) and (2a), we see that Π0(νj) = 1 / (Kj +1) and Π1(µj) = 1 / (Kj +1). Using this,

and again using xj,νj = xj−1,µj−1, we get that

n−1

n

Qj=0
Qj=1

Π1(µj)

Π0(νj)

n

n−1

Qj=1
Qj=0

n−1

Π0(µj | xj)

Π1(νj | xj)

=

n−1

Qj=0

Π1(νj+1 | xj+1) (Kj+1 +1)
Qj=0
pηj+1(xj+1,νj+1) .

Π0(µj | xj) (Kj +1)

pj ∗j+1(xj+1,νj+1)

Kj+1+1

Kj+1

1

=

=

n−1

Yj=0

n−1

Yj=0

pj ∗j+1(xj,µj )

Kj +1

pηj (xj,µj ) . 1
Xk=0

Yj=0

Kj +1

n−1

Kj

1




pηj (xj,µj )
pηj+1(xj,µj )

pj ∗j+1(xj+1,k)
pηj+1 (xj+1,k)

pj ∗j+1(xj,k)

Kj

Xk=0
Xk=0
pηj (xj,k) .

pηj (xj,k)

pj ∗j+1(xj,k)

1

Kj+1+1

Kj+1

Xk=0

pj ∗j+1(xj+1,k)

pηj+1(xj+1,k) 


(19)

(20)

(21)

From Steps (1) and (3), we see that Π0(ν0) = Π1(ν0) = 1 / (K0 + 1) and Π1(µn) = Π0(µn) =
1 / (Kn+1), so these factors cancel in equation (13). The factors in equation (18) cancel with the ﬁrst
part of equation (21). The ﬁnal result is that the simple importance sampling estimate based on a
single LIS run is as shown in equation (10), demonstrating that ˆrLIS is indeed an unbiased estimate of
r = Z1/Z0.

2.3 Bridged LIS estimates

Since the LIS estimate can be viewed as a simple importance sampling estimate on an extended space,
we can consider a ‘bridged LIS’ estimate in which this top-level SIS estimate is replaced by a bridge
sampling estimate. This will require that we actually perform the reverse LIS procedure described
above, from which an LIS estimate for the reverse ratio, r = Z0/Z1, can be computed:

ˆr(i)
LIS =

n

Yj=1




1

Kj + 1

Kj

Xk=0

pj−1∗j(xj,k)

pηj (xj,k) .

1

Kj−1 + 1

Kj−1

Xk=0

pj−1∗j(xj−1,k)

pηj−1 (xj−1,k) 


(22)

10

The reversed procedure requires independent sampling from π1. This will usually not be possible
directly, but well-separated states from a Markov chain sampler with π1 as its invariant distribution
will provide a good approximation, provided that this sampler moves around the whole distribution,
without being trapped in an isolated mode. Indeed, the entire sample of Kn+1 states from π1 that is
needed at the start of the reverse procedure can be obtained by taking consecutive states from such a
Markov chain sampler.

For the bridged form of LIS, we also need a suitable bridge distribution, P∗, for which we must be
able to evaluate the ratios P∗/P0 and P∗/P1. (Note that this choice of a ‘top-level’ bridge distribution
is separate from the choices of ‘low-level’ bridge distributions, pj ∗j+1, though we might use the same
form for both.) With the optimal bridge of equation (6), these ratios can be written as follows, if the
forward procedure is performed M times and the reverse procedure M times:

P opt
∗ (x, µ, ν)
P0(x, µ, ν)

P opt
∗ (x, µ, ν)
P1(x, µ, ν)

+ 1#−1
= " r (M/M ) (cid:18) P1(x, µ, ν)
P0(x, µ, ν)(cid:19)−1
P1(x, µ, ν)(cid:19)−1#−1
= " r (M/M ) + (cid:18)P0(x, µ, ν)

The geometric bridge of equation (5) results in

P geo
∗ (x, µ, ν)
P0(x, µ, ν)

P geo
∗ (x, µ, ν)
P1(x, µ, ν)

P0(x, µ, ν)

= s P1(x, µ, ν)
= s P0(x, µ, ν)

P1(x, µ, ν)

(23)

(24)

(25)

(26)

These expressions allow us to express bridged LIS estimates in terms of the simple LIS estimate of
equation (10), and its reverse version of equation (22). For the optimal bridge, we get

ˆropt
LIS-bridged =

1
M

M

Xi=1

r (M/M ) / ˆr(i)

1

LIS + 1 . 1

M

M

Xi=1

1

r (M/M ) + 1/ˆr(i)

LIS

Similarly, for the geometric bridge, we get

ˆrgeo
LIS-bridged =

1
M

M

Xi=1 qˆr(i)

LIS . 1

M

M

Xi=1 qˆr(i)

LIS

(27)

(28)

2.4 LIS estimates with independent sampling with no intermediate distributions

It is interesting to look at the special case of Linked Importance Sampling with n = 1 — ie, in which
the are no intermediate distributions between π0 and π1 — in which the points from both π0 and
π1 are sampled independently. The LIS procedure can then be simpliﬁed somewhat, and it is also
possible to improve the LIS estimate by averaging over the choice of link state. Such averaging is not

11

feasible when Markov chain sampling is used, since choosing a diﬀerent link state would require a new
simulation of the Markov transitions.

Since we will sample points independently, there is no need to decide how many points will be
sampled by the forward transitions and how many by the reverse transitions in Steps (2a) and (2b)
of the LIS procedure. We simply obtain a pair of samples consisting of points x0,0, . . . , x0,K0 drawn
independently from π0, and points x1,1, . . . , x1,K1 drawn independently from π1. We then randomly
select a link state, indexed by µ, from among x0,0, . . . , x0,K0 according to the following probabilities,
which depend on the choice of a single bridge distribution, denoted by p∗(x):

Π0(µ| x0) =

p∗(x0,µ)

p0(x0,µ) .

K0

Xk=0

p∗(x0,k)
p0(x0,k)

The LIS estimate for r = Z1/Z0 based on this pair of samples from π0 and π1 is

ˆr(i)
LIS =

1

K0 +1

K0

Xk=0

p∗(x0,k)

p0(x0,k) . 1

K1+1" p∗(x0,µ)

p1(x0,µ)

+

K1

Xk=1

p∗(x1,k)

p1(x1,k)#

(29)

(30)

The superscript i is used here to indicate that this estimate is based on the i’th pair of samples. We
can see that it is very similar to the bridge sampling estimate of equation (4), except that the link
state is included in both samples. Since these LIS estimates are unbiased, we can average M of them
to obtain a ﬁnal LIS estimate.

We can also average the estimate of equation (30) over the random choice of link state, which is
guaranteed to produce an estimate (also unbiased) with smaller mean-squared-error (see Schervish
1995, Section 3.2). The result is

ˆr(i)
LIS-ave =

Π0(µ| x0)

1

K0 +1

K0

Xµ=0

K0

Xk=0

=

K1+1
K0+1

K0

Xµ=0

p∗(x0,µ)

p0(x0,µ) . " p∗(x0,µ)

p∗(x0,k)

p1(x0,µ)

p0(x0,k) . 1
Xk=1

K1 +1" p∗(x0,µ)
p1(x1,k)#

p1(x0,µ)

p∗(x1,k)

K1

+

+

K1

Xk=1

p∗(x1,k)

p1(x1,k)#

(31)

(32)

Averaging these estimates over M pairs of samples produces a ﬁnal estimate denoted by ˆrLIS-ave.

To use bridged LIS in this context, we need to ﬁnd reverse estimates as well, but these reverse
estimates needn’t be independent of the forward estimates, since the asymptotic validity of the bridge
sampling estimate of equation (4) does not depend on the samples x0 and x1 being independent.
Accordingly, we can use the same samples from π0 and π1 for the forward and the reverse operations.
However, to perform reverse sampling, we need to have a sample of K1 +1 points drawn from π1, the
ﬁrst of which is ignored when performing forward sampling. Conversely, the ﬁrst of the K0 +1 points
drawn from π0 is ignored when performing the reverse sampling.

We can improve the bridged LIS estimates by averaging the numerator and the denominator of
equation (27) or (28) with respect to the random choice of link state. We can also average with

12

respect to the omission of one of the points from one of the samples — ie, rather than omitting
the ﬁrst of K1 + 1 points in the sample from π1 when computing a forward estimate, we average
with respect to a random choice of point to omit, and similarly for reverse estimates. Note that the
averaging should be done over the sums in the numerator and denominator, not with respect to the
entire estimate, nor with respect to the values of ˆr(i)
LIS appearing inside the summands. The
eﬀective sample size after this additional averaging of dependent points is unclear, so it is not obvious
what the ratio of sample sizes in equation (6) should be, but using (K0 + 1)/(K1 + 1) is probably
adequate.

LIS and ˆr(i)

3 Analytical comparisons of AIS and LIS

In this section, I analyse (somewhat informally) the performance of AIS and LIS asymptotically, and
in other situations where analytical results are possible.

3.1 Asymptotic properties of AIS and LIS estimates

I begin by analysing the asymptotic performance of AIS and LIS when the sequence of distributions
is deﬁned by an unnormalized density function of the following form:

pη(x) = p0(x) exp(−ηU (x))

(33)

This class includes sequences of canonical distributions deﬁned by equation (1) in which the inverse
temperature varies, as well as sequences that can be used for Bayesian analysis, in which p0 deﬁnes the
prior and η is a power that the likelihood (expressed as exp(−U (x))) is raised to, with η = 1 giving the
posterior distribution. For these distributions, we can express r using the well-known ‘thermodynamic
integration’ formula as follows:

r = log(Z1/Z0) = −Z 1

0

Eπη (U ) dη

(34)

The analysis here is asymptotic, as the number of intermediate distributions used, given by n−1,
goes to inﬁnity. I will assume the ηj deﬁning these distributions are chosen according to a scheme in
which for any a ∈ (0, 1), the spacing ηj+1 − ηj when j = ⌊a n⌋ is asymptotically proportional to 1/n
— in other words, the relative density of intermediate distributions in the neighborhood of diﬀerent
values of η stays the same as the overall density increases. The simplest such scheme is to let ηj = j/n,
though other schemes may sometimes be better.

With the above form for pη, the AIS estimate from a single run (from equation (7)) can be written

as follows:

log ˆr(i)

AIS =

n−1

Xj=0

log(cid:16)pηj+1 (x(i)

j ). pηj (x(i)

j )(cid:17) =

n−1

Xj=0

j (cid:17)
−(ηj+1 − ηj) U(cid:16)x(i)

(35)

13

When ηj = j/n, this can be seen as a stochastic form of Riemann’s Rule for numerically integrating
equation (34), though one diﬀerence is that log ˆrAIS converges to the correct value as M goes to inﬁnity
even if n stays ﬁxed.

Provided that there is some ﬁnite bound on the variance of U under all the distributions πη, and
that the Markov transitions used mix well, a Central Limit Theorem will apply, allowing us to conclude
that the distribution of ℓn = log ˆr(i)
AIS becomes Gaussian as n goes to inﬁnity. Let the mean of ℓn be
µn, and let the variance of ℓn asymptotically be σ2/n, where σ is determined by details of the spacing
of intermediate distributions and of the degree of autocorrelation in the Markov transitions. Note
that E[Y q] = exp(qµ + q2ς 2/2) when Y = exp(X) and X is Gaussian with mean µ and variance ς 2.
Using this, the mean of exp(ℓn) is exp(µn + σ2/2n). This must equal r, since ˆrAIS is unbiased, so
µn = log(r) − σ2/2n. Using this, we can see that the variance of ˆr(i)
AIS = exp(ℓn) is r [exp(σ2/2n) − 1],
which for large n will be approximately rσ2/2n. The variance of ˆrAIS will therefore be rσ2/2nM .
Asymptotically, the total computational eﬀort, which will generally be proportional to nM , can be
divided in any way between more intermediate distributions (n) or more runs (M ) without aﬀecting
the accuracy of estimation of r, provided that n is kept large enough that these asymptotic results
apply — a fact noted by Hendrix and Jarzynski (2001). We can therefore use a value of M greater
than one without penalty, in order to obtain an error estimate from the degree of variation over the
M runs.

For LIS, we can write the log of the estimate from one run (equation (10)) as follows:

n−1

Kj

1

Kj+1

1

log



pj ∗j+1(xj,k)

pηj (xj,k) 

log ˆr(i)

LIS =

Kj + 1

Xj=0

Xk=0
Suppose that we let Kj = ⌈mK 0
j ⌉ for all j and some set of K 0
j , and that we then let m go to
inﬁnity. Assuming that the variances of the ratios of probabilities are ﬁnite, and that the Markov
chain transitions used mix suﬃciently well, a Central Limit Theorem will again apply, and we can
conclude that all of the n terms in the sum above, and therefore also the sum itself, will approach
Gaussian distributions, with variances proportional to 1/m.

Xk=0

Kj+1 + 1

pj ∗j+1(xj+1,k)

pηj+1 (xj+1,k) 



 (36)

 − log


To analyse the LIS estimate in more detail, we need to assume a form of bridge distribution, as well
as a form for pη. If pη has the form of equation (33) and we use the geometric bridge of equation (5),
we can write

log ˆr(i)

LIS =

n−1

Xj=0

 log


log


1

Kj + 1

1

Kj+1 + 1

Kj

Xk=0
Xk=0

exp(−(ηj+1−ηj) U (xj,k) / 2)
 −
exp(−(ηj−ηj+1) U (xj+1,k) / 2)


Kj




(37)

Since exp(z) ≈ 1 + z and log(1 + z) ≈ z when z is small, we can rewrite this when n is large (and

14

hence ηj+1−ηj is small) as
Xj=0

LIS ≈

log ˆr(i)

n−1

n−1

 log

1 −
log
1 +



ηj+1−ηj

2

ηj+1−ηj

2

1

Kj + 1

ηj+1−ηj

2

1

Kj+1 + 1

U (xj,k) +

Kj

Xk=0

1

Kj + 1

K0

Kj

Xk=0
Xk=0

Kj+1

U (xj,k)
 −

U (xj+1,k)


Xk=0
Xk=0

Kj+1 + 1

Kn + 1

Kj+1

Kn

1

1

≈

−

Xj=0
η1−η0

2

= −

−

n−1

Xj=1

U (x0,k) −

ηn−ηn−1

2

1

K0 + 1

Xk=0
ηj+1−ηj−1

1

2

Kj + 1

Kj

Xk=0

U (xj,k)

U (xj+1,k)


U (xn,k)

(38)

(39)

(40)

When ηj = j/n, this looks like a stochastic form of the Trapezoidal Rule for numerically integrating
equation (34). Since the Trapezoidal Rule converges faster than Reimann’s Rule, one might expect
LIS to perform better than AIS asymptotically, but this is not so in this stochastic situation. Suppose
for simplicity that we set all Kj = m. The variance of log ˆr(i)
LIS will be dominated by the variance of the
last sum above, which will be proportional to 1/nm, assuming that m is large, so that the dependence
between terms (from sharing link states) is negligible. Using the same argument as for AIS above,
the variance of log ˆrLIS will be proportional to 1/nmM . Considering that the computation time for an
LIS run will be proportional to nm, versus n for AIS, we see that the variances of the AIS and LIS
estimates go down the same way in proportion to computation time, asymptotically as n and m go to
inﬁnity.

Furthermore, the proportionality constant should be the same for AIS and LIS, assuming that the
overhead of the two procedures is negligible compared to the time spent performing Markov transitions,
so that the proportionality constants for computation time are the same for AIS (multiplying n) and
for LIS (multiplying nm). The proportionality constants for variance for AIS (multiplying 1/nM )
and for LIS (multiplying 1/nmM ) depend in a complex way on the form of the density of ηj values
and on the mixing properties of the Markov transitions, but the result should be the same for AIS
and LIS, provided the same scheme is used for choosing ηj values, and the same Markov transitions
are used, parameterized smoothly in terms of η. A diﬀerence that might appear signiﬁcant is that
for AIS only one Markov transition is done for each ηj, whereas for LIS, m such transitions are
done. However, as n goes to inﬁnity, nearby distributions become more similar, so transitions for m
consecutive distributions become similar to m transitions for one of these distributions.

The apparently pessimistic conclusion from this is that when both n and m (and hence the Kj)
are large, the performance of LIS should be about the same as that of AIS (with n for AIS chosen to

15

equalize the computation time), assuming that the distributions used have the form of equation (33),
that the variance of U is ﬁnite under all of the distributions πη, and that the Markov transitions used
mix well enough. Fortunately, however, there is no reason to make both m and n large with LIS. For
good performance, n must be large enough that πηj and πηj+1 overlap signiﬁcantly, but there is no
reason to make n much larger than this. The accuracy of the estimates can be improved as desired
by increasing m and/or M while keeping n ﬁxed. The results below show that LIS estimates with n
ﬁxed are sometimes much better than AIS estimates.

Finally, let us consider the asymptotic performance of the bridged versions of AIS and LIS, assuming
that the variance of U is ﬁnite, so that the distribution of the estimates from individual runs becomes
Gaussian as n (for AIS) or m (for LIS) goes to inﬁnity. Looking at equations (27) and (28), which
also are applicable to bridged AIS estimates, we see that the log of ˆr(i)
LIS-bridged can for both optimal and
geometric bridges be expressed as the diﬀerence of the log of the numerator, which is the mean of a
function of the forward estimates, ˆr(i)
LIS, and the log of the denominator, which is the mean of a function
of the reverse estimates, ˆr(i)
LIS. If these forward and reverse estimates have Gaussian distributions with
small variances, σ2 and σ2, then ˆr(i)
LIS-bridged will also be Gaussian, with a variance that can be computed
in terms of the derivatives of the summands in the numerator and the denominator, with respect to
ˆr(i)
LIS and ˆr(i)
LIS, evaluated at the true values of r and 1/r. I will assume that r = 1 below, as can be
done without loss of generality.

LIS = r = 1 and ˆr(i)

For the geometric bridge, these derivatives are both 1/2, from which it follows that the variance of
the numerator in equation (28) is σ2/4M and that of the denominator is σ2/4M . Since the numerator
and denominator evaluate to one for ˆr(i)
LIS = 1/r = 1, the sum of the variances of
the logs of the numerator and denominator is σ2/4M + σ2/4M . If σ2 = σ2 and M = M , this reduces
to σ2/2M . The variance of an unbridged LIS estimate will be σ2/M . However, the bridged estimate
requires time proportional to M + M , compared to just M for the unbridged estimate. The value of
M for the unbridged method can therefore be twice as large as for the bridged method, with the result
that bridged and unbridged estimates perform equally well asymptotically (assuming the variance of
U is ﬁnite).

For the optimal bridge, the derivatives of the summands in the numerator and denominator are
both 1/4, when evaluated at ˆr(i)
LIS = 1/r = 1, and assuming that M = M . The
numerator and denominator both evaluate to 1/2, with the result that asymptotically the variance of
the bridged estimate, assuming σ2 = σ2, is σ2/2M , the same as for the geometric bridge.

LIS = r = 1 and ˆr(i)

In conclusion, bridged AIS and LIS estimates asymptotically have the same performance as the
corresponding unbridged estimates (with twice the value of M ), for both the optimal and geometric
bridges, assuming U has ﬁnite variance. This conclusion applies more generally, as long as a Central
Limit Theorem holds for the individual estimates, ˆr(i)
LIS. However, the bridged methods may
be much better when the variance of U is inﬁnite, or for classes of distributions other than that of
equation (33). The bridged methods may also provide improvement when the values of n or m are
not large enough for the asymptotic results to apply.

LIS and ˆr(i)

16

3.2 Properties of AIS and LIS when sampling from uniform distributions

In this section, I will demonstrate that when n is kept suitably small, LIS can perform much better
than AIS when these methods are applied to sequences of uniform distributions.

As a ﬁrst example, consider the class of nested uniform distributions with unnormalized densities

given by

pη(x) = ( 1 if −sη < x < sη

0 otherwise

(41)

for which the normalizing constants are Zη = 2sη, so that r = Z1/Z0 = s. The results concerning
this class of distributions can easily be extended to any class of uniform distributions, in any number
of dimensions, that have nested regions of support. For both AIS and LIS, I will assume that the
intermediate distributions are deﬁned by ηj = j/n. With this choice, the probability that a point, x,
randomly sampled from πj will have pj+1(x) = 1 is s1/n, for any j.

During an AIS run, only a single point is sampled from each distribution. An AIS run will produce
an estimate for r of zero if any of the ratios pηj+1(x(i)
j ) in equation (7) are zero, which
happens with probability 1 − (s1/n)n = 1 − s, and will otherwise produce an estimate of one. Note
that the distribution of estimates is independent of n. AIS is therefore not a useful technique for
nested uniform distributions — simple importance sampling (ie, AIS with n = 1) would work just as
well (or just as poorly, if s is very small). Bridged AIS produces no improvement in this context.

j ) / pηj (x(i)

Suppose instead we use LIS with all Kj = m, and suppose that the Markov transitions, Tj, produce
points that are almost independent of the previous point. For this problem, both the geometric and
optimal forms of the bridge distribution result in pj ∗j+1(x) = pηj+1(x). If m + 1 points are sampled
independently from πηj , the fraction of these points for which pηj+1(x) is one will have variance s1/n (1−
s1/n) / (m+1). For suﬃciently large m, the variance of the log of this fraction will be approximately
(s1/n (1−s1/n) / (m+1)) / s2/n, which simpliﬁes to (s−1/n−1) / (m+1). For this approximation to be
useful, the probability that none of the m + 1 points sampled from πηj lie in the region where pηj+1 is
one, equal to (1 − s1/n)m+1, must be negligible. This probability must be fairly small anyway, if LIS
is to perform well.

Suppose that the computational cost of an LIS run is proportional to the sum of the number of
If we ﬁx this cost, the
points sampled from π0 and the number of Markov transitions performed.
number of intermediate distributions, n, and the number of transitions for each distribution, m, will
be related by m(n + 1) = C, for some constant C. Assume for the moment that both n and m
are large. The probability of a run producing a zero estimate will then be negligible, and we can
assess the accuracy of the estimate for one run by the variance of log ˆr(i)
LIS (modiﬁed in some way to
eliminate the inﬁnity resulting from the negligible, but non-zero, probability that ˆr(i)
LIS is zero). Looking
at equation (36), we see that for these nested uniform distributions, the second log term vanishes —
pj ∗j+1(xj+1,k) / pηj+1 (xj+1,k) is always one, since pj ∗j+1 is the same as pηj+1 . When m is large, the
dependence between terms with diﬀerent values of j will be negligible, so we can add the variances of

17

the terms to get the variance of the estimate, obtaining the result that

Var(cid:16) log ˆr(i)

LIS(cid:17) ≈ n (s−1/n−1) / (m+1)

(42)

(43)

When n is large, s−1/n = exp(log(1/s)/n) is approximately 1 + log(1/s)/n, and hence the variance
above is approximately log(1/s) / (m+1). So it seems that the larger the value of m, the better —
until we reach a value of m for which the corresponding value of n, equal to C/m − 1, is small enough
that this result no longer applies.

Best performance will therefore come using a fairly small value of n, but a large value of m.

Substituting m = C/(n+1) into equation (42), and assuming m/(m+1) ≈ 1, we get
LIS(cid:17) ≈ n (s−1/n−1) / (C/(n+1)) = n(n+1) (s−1/n−1) / C

Var(cid:16) log ˆr(i)

The value of n that minimizes this depends only on s, not on C. The optimal choice of n increases
slowly as s gets smaller: s = 0.1 gives n = 2, s = 0.05 gives n = 3, s = 0.01 gives n = 4, and
s = 0.0001 gives n = 7.

As a second example, consider the class of non-nested uniform distributions with unnormalized

densities given by

pη(x) = ( 1 if ηt − 1 < x < ηt + 1

0 otherwise

(44)

For this class, Zη = 2 for all η, so r = Z1/Z0 = 1.
I will again assume that the intermediate
distributions are deﬁned by ηj = j/n, and that all Kj = m. Assuming that n is greater than t/2, the
probability that a point, x, randomly sampled from πηj will have pηj+1(x) = 1 is 1 − t/2n, for any j.
For this example, AIS estimates do not converge to the true value of r as M increases, regardless
of the value of n. To see this, note that the ratios in equation (7) will all be either zero or one, and
that the estimate from one run, ˆr(i)
AIS, will be one if all of these ratios are one, and zero otherwise. The
probability of a particular ratio being one is 1− t/2n, so the probability that all are one (assuming the
Tη produce points independent of the current point) is (1 − t/2n)n, which approaches exp(−t/2) as n
goes to inﬁnity. The AIS estimate, averaging over M runs, will have mean exp(−t/2), rather than the
correct value of one.

In contrast, bridged AIS estimates will converge to the true value as M increases, as long as n is at
least t/2, so that there is overlap between successive distributions in the sequence. However, when t
is large, the overlap between the distributions over paths produced by forward and reverse AIS runs,
given by exp(−t/2), will be very small, and the procedure will be very ineﬃcient.
To see how well LIS performs, recall the formula for log ˆrLIS from equation (36):

log ˆr(i)

LIS =

n−1

Xj=0


log


1

Kj + 1

Kj

Xk=0

pj ∗j+1(xj,k)

pηj (xj,k) 

 − log


1

Kj+1 + 1

Kj+1

Xk=0

pj ∗j+1(xj+1,k)

pηj+1 (xj+1,k) 



 (45)

18

Due to symmetry, the two log terms above have the same distribution, for all j. The variance of
one of these log terms (for large m) is ((t/2n) (1− t/2n) / (m + 1)) / (1− t/2n)2, which simpliﬁes to
1 / ((2n/t−1) (m+1)). The second log term in equation (36) for one j will involve the same points, xj+1,k,
as the ﬁrst log term for the next j. The eﬀect of this is that these terms will be negatively correlated,
with correlation of −1 if n = t. However, since the two terms occur with opposite signs, the eﬀect on the
ﬁnal sum is that n−1 pairs of terms (out of 2n terms total) are positively correlated. Straightforward
calculations show that this correlation is 2n/t − 1 for t/2 < n ≤ t and 1 / (2n/t − 1) for n ≥ t. Using
the fact that when X and Y have the same distribution, Var(X + Y ) = 2 Var(X) [1 + Cor(X, Y )], we
obtain the result that, for large m,

Var(cid:16) log ˆr(i)

LIS(cid:17) ≈

2

(2n/t−1) (m+1)( n + (n−1) (2n/t − 1)
n + (n−1) / (2n/t − 1)

if t/2 < n ≤ t
if n ≥ t

)

Setting m = C/(n+1), and assuming m/(m+1) ≈ 1, gives

Var(cid:16) log ˆr(i)

LIS(cid:17) ≈

2(n+1)

C(2n/t−1)( n + (n−1) (2n/t − 1)
n + (n−1) / (2n/t − 1)

if t/2 < n ≤ t
if n ≥ t

)

(46)

(47)

Numerical investigation shows that the global minimum of the variance occurs where n is near (3/2) t.
A second local minimum where n is near (3/4) t also exists. The two minima are nearly equally good
when t is large. There is a local maximum where n is near t, with the variance there being about
19% greater than at the global minimum. The variance is much larger for very large and very small
values of n. We therefore see that for this example too, the best results are obtained by ﬁxing n to a
moderate value; any desired level of accuracy can then be obtained by increasing m and/or M .

4 Empirical comparisons of AIS and LIS

The analytical results of the previous section indicate that LIS can sometimes perform much better
than AIS, but that the beneﬁts of LIS may only be seen when the number of intermediate distributions
used is kept suitably small (but not so small that they do not overlap). In this section, I investigate
the performance of AIS and LIS (and their bridged versions) empirically. The programs used for these
tests (written in R) are available from my web page.

These tests were done using sequences of one-dimensional distributions having unnormalized density

functions of the following form:

pη(x) = exp(cid:16)−(cid:12)(cid:12)(cid:12)

(x−ηt) / sη(cid:12)(cid:12)(cid:12)

where s, t, and q are ﬁxed constants. As η moves from 0 to 1, the centre of this distribution shifts
by t, and changes width by the factor s. The power q controls how thick the tails of the distributions
are. When q = 2, the distributions are Gaussian; a larger value produces lighter tails. Note that Zη
is proportional to sη, and hence r = Z1/Z0 is equal to s.

q(cid:17)

(48)

19

.

8
0

4

.

0

0
0

.

8
0

.

4
0

.

0
.
0

.

8
0

4

.

0

0
0

.

.

8
0

4

.

0

0
0

.

−2

0

2

4

6

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

q = 2,  t = 4,  s = 1

q = 2,  t = 0,  s = 0.05

q = 2,  t = 2,  s = 0.3

8
0

.

4
0

.

0
.
0

8
0

.

4
0

.

0
.
0

−2

0

2

4

6

−3

−2

−1

0

1

2

3

−3

−2

−1

0

1

2

3

q = 10,  t = 4,  s = 1

q = 10,  t = 0,  s = 0.05

q = 10,  t = 2,  s = 0.3

Figure 2: The sequences of unnormalized density functions used for the tests. The plots show the
unnormalized density functions for η = 0, 1/4, 2/4, 3/4, 1, for six combinations of s, t, and q.

If t = 0, the distributions can be written in the form of equation (33), after reparameterizing in
terms of η′ = 1/sηq, so that pη′(x) = exp(−η′|x|q). In this case, we expect the asymptotic behaviour
to be as discussed in Section 3.1, but the behaviour with samples of practical size may be diﬀerent.
As q goes to inﬁnity, the distributions converge to uniform distributions over (ηt−sη, ηt+sη), and the
results of Section 3.2 become relevant.

I did an initial set of tests using six sequences of distributions. Three of these sequences were of
Gaussian distributions, with q = 2. The ﬁrst of these used s = 1 and t = 4, producing a shift with no
change in scale as η increases from 0 to 1. The second used s = 0.05 and t = 0, producing a contraction
with no shift. The last used s = 0.3 and t = 2, combining a shift with a contraction. A second set of
three sequences used the same values of s and t, but with q = 10, which produces more ‘rectangular’
distributions with lighter tails. The six sequences are shown in Figure 2. Each sequence in these plots
consists of ﬁve distributions, corresponding to η = 0, 1/4, 2/4, 3/4, 1. These were the sequences used
for the LIS runs (hence n = 4 for these runs). The AIS runs used more distributions, spaced more ﬁnely
with respect to η, so as to produce the same number of Markov transitions and sampling operations
as in the LIS runs.

These distributions (for any η) can easily be sampled from using rejection sampling. Samples from
π0 and π1 were used to initialize forward and reverse runs of AIS and LIS. For this test, we pretend
that sampling for other πη must be done using Markov chain methods. The transition used for πη, Tη,
was a random-walk Metropolis update, using a Gaussian proposal distribution with mean equal to the

20

current point and standard deviation sη. Since Metropolis updates are reversible, T η was the same.

Two sets of forward and reverse LIS runs were done with n = 4, all Kj = 50, and M = 20, one
set using the geometric bridge, the other using the optimal bridge with the true value of r. The
forward estimates were computed from equation (10); the reverse estimates from equation (22), which
is equivalent to using the forward procedure with the reverse sequence of distributions. Bridged LIS
estimates were also found using equation (27), with the value of r found by iteration. To make the
comparison with forward and reverse estimates fair, the bridged LIS estimates used M = 10 — ie, only
half of the forward and half of the reverse runs were used, for a total of 20 runs.

A corresponding set of forward, reverse, and bridged AIS runs were also done, with n = 250 and
M = 20 (M = 10 for the bridged estimates). If sampling a point from π0 or π1 takes about the same
computation time as a Metropolis update, these AIS runs will take about the same time as the LIS
runs. (This assumes that sampling and Markov transitions dominate the time, which is typically true
for real problems but perhaps not for this simple test problem.)

Sets of longer LIS and AIS runs were also done, which were the same as the sets above except that

for LIS, Kj = 200 for all j, and for AIS, n = 1000, which again equalizes the computation time.

Experience, together with the asymptotic results of Section 3.1, shows that estimates produced
using a small value of M are better than, or at least as good as, those produced with larger M . I
chose M = 20 (M = 10 for bridged estimates) since this is about the smallest value that allows reliable
estimation of standard errors, which would usually be needed in practice.

The standard errors for AIS and LIS estimates of ˆr were estimated by the sample standard deviation
of the ˆr(i) divided by √M . When comparing the methods, I looked primarily at the mean squared
error when estimating log(r) (rather than when estimating r). The estimate I used was log(ˆr), and the
standard error for this estimate was estimated by the standard error for ˆr divided by ˆr. For the reverse
runs, log(r) was estimated by − log(ˆr). For bridged AIS and LIS, the standard errors for the log of
the numerator and the log of the denominator of equation (27) were found, and the overall standard
error was computed as the square root of the sum of the squares of these two standard errors. This
method of converting estimates and standard errors for r to those for log(r) is valid asymptotically.
It might be improved upon for ﬁnite samples, but such improvements would probably not aﬀect the
relative merits of the methods compared here.

Figures 3 through 8 plot the mean squared errors of estimates for log(r) for the six sets of runs.
Results are shown for AIS, for LIS using the geometric bridge, and for LIS using the optimal bridge,
with the true value of r. Results for both the forward and reverse versions of each method are shown,
together with the bridged version, using the optimal bridge, with r obtained by iteration. Results
for the short runs (n = 4, Kj = 50 for LIS, n = 250 for AIS) are on the left, and for the long runs
(n = 4, Kj = 200 for LIS, n = 2000 for AIS) on the right. The mean squared error for each method was
estimated by simulating each method 2000 times, and comparing the estimates with the true value of
log(r). The bars in the plots are dark up to the estimated mean squared error minus twice its standard

21

AIS: n=250,  LIS: n=4 m=50

AIS: n=1000,  LIS: n=4 m=200

AIS

LIS−geometric

LIS−optimal

AIS

LIS−geometric

LIS−optimal

7
0
0

.

6
0
0

.

5
0
0

.

4
0

.

0

3
0
0

.

2
0
.
0

1
0
.
0

0
0
.
0

for

rev bri

for

rev bri

for

rev bri

2
1
0
0

.

0
1
0
0

.

8
0
0
0

.

6
0
0

.

0

4
0
0
0

.

2
0
0
.
0

0
0
0
.
0

for

rev bri

for

rev bri

for

rev bri

r
o
r
r

 

E
d
e
r
a
u
q
S
n
a
e
M

 

Figure 3: Results of short and long runs on the distribution sequence with s = 1, t = 4, and q = 2.

AIS: n=250,  LIS: n=4 m=50

AIS: n=1000,  LIS: n=4 m=200

AIS

LIS−geometric

LIS−optimal

AIS

LIS−geometric

LIS−optimal

r
o
r
r

E
 
d
e
r
a
u
q
S
n
a
e
M

 

5
1
.
0

0
1
.
0

5
0

.

0

0
0

.

0

0.26 0.29

for

rev bri

for

rev bri

for

rev bri

5
3
0
.
0

0
3
0
.
0

5
2
0
.
0

0
2
0

.

0

5
1
0

.

0

0
1
0

.

0

5
0
0

.

0

0
0
0

.

0

0.04 0.04

for

rev bri

for

rev bri

for

rev bri

Figure 4: Results of short and long runs on the distribution sequence with s = 1, t = 4, and q = 10.

22

AIS: n=250,  LIS: n=4 m=50

AIS: n=1000,  LIS: n=4 m=200

AIS

LIS−geometric

LIS−optimal

AIS

LIS−geometric

LIS−optimal

5
2
0
0

.

0
2
0
0

.

5
1
0

.

0

0
1
0

.

0

5
0
0
.
0

0
0
0
.
0

for

rev bri

for

rev bri

for

rev bri

5
0
0
0

.

4
0
0
0

.

3
0
0

.

0

2
0
0
0

.

1
0
0
.
0

0
0
0
.
0

for

rev bri

for

rev bri

for

rev bri

r
o
r
r

 

E
d
e
r
a
u
q
S
n
a
e
M

 

Figure 5: Results of short and long runs on the distribution sequence with s = 0.05, t = 0, and q = 2.

AIS: n=250,  LIS: n=4 m=50

AIS: n=1000,  LIS: n=4 m=200

LIS−geometric

LIS−optimal

AIS

LIS−geometric

LIS−optimal

AIS

0.13

for

rev bri

for

rev bri
Short Runs

for

rev bri

Figure 6: Results of short and long runs on the distribution sequence with s = 0.05, t = 0, and q = 10.

23

0
1
.
0

8
0
.
0

6
0
.
0

4
0

.

0

2
0

.

0

0
0

.

0

r
o
r
r

E
 
d
e
r
a
u
q
S
n
a
e
M

 

5

0
2
0
.
0

5
1
0
.
0

0
1
0

.

0

5
0
0

.

0

0
0
0

.

0

for

rev bri

for

rev bri
Long Runs

for

rev bri

AIS: n=250,  LIS: n=4 m=50

AIS: n=1000,  LIS: n=4 m=200

AIS

LIS−geometric

LIS−optimal

AIS

LIS−geometric

LIS−optimal

7
0

.

0

6
0

.

0

5
0

.

0

4
0
0

.

3
0
0

.

2
0
.
0

1
0
.
0

0
0
.
0

for

rev bri

for

rev bri

for

rev bri

2
1
0
0

.

0
1
0
0

.

8
0
0
0

.

6
0
0

.

0

4
0
0
0

.

2
0
0
.
0

0
0
0
.
0

for

rev bri

for

rev bri

for

rev bri

r
o
r
r

 

E
d
e
r
a
u
q
S
n
a
e
M

 

Figure 7: Results of short and long runs on the distribution sequence with s = 0.3, t = 2, and q = 2.

AIS: n=250,  LIS: n=4 m=50

AIS: n=1000,  LIS: n=4 m=200

AIS

LIS−geometric

LIS−optimal

0.24 0.37

for

rev bri

for

rev bri

for

rev bri

r
o
r
r

E
 
d
e
r
a
u
q
S
n
a
e
M

 

0
2
.
0

5
1
.
0

0
1

.

0

5
0

.

0

0
0

.

0

6
0
.
0

5
0
.
0

4
0
.
0

3
0

.

0

2
0

.

0

1
0

.

0

0
0

.

0

AIS

LIS−geometric

LIS−optimal

for

rev bri

for

rev bri

for

rev bri

Figure 8: Results of short and long runs on the distribution sequence with s = 0.3, t = 2, and q = 10.

24

error, and are then light up to the estimated mean squared error plus twice its standard error. For
bars that extend above the plot the estimated mean squared error is shown at the top of the bar.

The results for translated sequences of distributions (t = 4 and s = 1) are shown in Figures 3 and 4.
When the distributions are Gaussian (q = 2), no advantage is seen for LIS — if anything, LIS performs
slightly worse than AIS, particularly when the geometric bridge is used. The forward and reverse forms
of AIS and LIS should have identical performance for these distribution sequences, due to symmetry;
any diﬀerences seen result from random variation. The bridged forms of both AIS and LIS perform
better than the unbridged forward and reverse forms. The advantage of bridging is less for the longer
runs, however, as expected from the analysis at the end of Section 3.1.

When q = 10, the distributions have much lighter tails than the Gaussian, more closely resembling
the uniform distributions analysed in Section 3.2. For these sequences of distributions, LIS performs
substantially better than AIS. The unbridged version of AIS does particularly badly. The mean
squared error for the bridged version of AIS is about 2.5 times greater than for the bridged version of
LIS. It makes little diﬀerence whether the geometric or optimal bridge is used for LIS.

Figures 5 and 6 show the results for sequences of distributions with the same mean (t = 0) but
decreasing width (s = 0.05). For these sequences, a modest advantage of LIS over AIS is apparent
for the sequence of Gaussian distributions (q = 2), with the variance for AIS estimates being about a
factor of 1.3 greater than for LIS estimates with the geometric bridge, and about a factor of 1.7 greater
than for LIS estimates with the optimal bridge. The reversed AIS and LIS estimates are somewhat
worse than the forward estimates for this sequence of distributions. No advantage is seen for bridged
AIS or LIS estimates.

The results for the sequence of distributions with q = 10 is similar, except that the advantage of LIS

over AIS is much greater — about a factor of 6.

Results for the last type of sequence, with s = 0.3 and t = 2, are shown in Figures 7 and 8. This
problem is a hybrid of the previous two, with both translation and change in width, producing results
intermediate between those for the previous two problems. No diﬀerence in performance between AIS
and LIS is apparent for the Gaussian distributions (q = 2), but the bridged forms of both perform
slightly better. For the sequence of distributions with q = 10, a clear advantage of LIS over AIS can
be seen, but this advantage is not as great as for the sequence with t = 0 and s = 0.05. The bridged
forms of both AIS and LIS are again better, more so for the short runs than for the long runs.

In addition to looking at the mean squared error of estimates found with these methods, I also
looked at the fraction of times that the estimate for log(r) diﬀered from the true value by more
than twice the standard error estimated using the M runs. This should be approximately 5% if the
distribution of estimates is Gaussian, and the standard errors are accurate. For the longer runs, this
fraction was indeed near or only slightly above 5% for all methods, except for the unbridged AIS runs
when these performed very poorly. For the shorter runs, however, the unbridged AIS and LIS methods
produced estimates more than two standard errors from the mean around 10% of the time (sometimes

25

much more often, when unbridged AIS performed poorly). Both the bridged AIS and the bridged LIS
methods gave more reliable standard errors. However, it is possible that better standard errors for
the unbridged methods might be obtained with a more sophisticated approach than I used.

I performed additional runs to verify and extend some of the analytic results from Section 3.
Figures 9 and 10 show results obtained using LIS with increasing numbers of intermediate distributions,
starting with the value of n = 4 used for the tests above, and continuing to n = 9, n = 19, and
n = 39, while keeping the computation time constant by decreasing m in proportion to n+1. The two
distribution sequences with s = 1 and t = 4 and with s = 0.05 and t = 0 were used, in both cases with
q = 10. The sequence with t = 0 and s = 0.05 has the form of equation (33), so in accordance with the
analysis of Section 3.1, we expect that asymptotically, as n increases, LIS and AIS should have the
same performance. This is indeed what we see in Figure 9. We also see the same behaviour for the
sequence with t = 4 and s = 1 in Figure 10.

As q increases, the distributions become close to uniform, and the results of Section 3.2 should
apply. To test this, I tried values of q = 2, q = 10, q = 20, and q = 30 for the distribution sequence with
s = 1 and t = 4 and the sequence with s = 0.05 and t = 0. Results are shown in Figures 11 and 12. (The
results for q = 2 and q = 10 are the same as on the left in Figures 3 to 6, though the scale diﬀers.)

For the sequences with s = 1 and t = 4, the limiting uniform distributions have the form of the
second example in Section 3.2. As noted there, AIS estimates do not converge to the correct value of
r for this distribution sequence; bridged AIS estimates do converge, but may be rather ineﬃcient. We
see analogous behaviour in Figure 11 when q is large. The mean squared error of the AIS estimates
increases approximately linearly with q over the range q = 10 to q = 30. The bridged AIS estimates also
get worse as q increases, but more slowly. In contrast, the mean squared error of the LIS estimates
changes hardly at all as q increases.

The story is similar for sequences with s = 0.05 and t = 1, for which the limiting uniform distributions
correspond to those in the ﬁrst example of Section 3.2. The LIS estimates perform about equally well
for all values of q, but the AIS estimates are dramatically worse for large values of q. For this sequence,
reverse AIS estimates are much worse than forward AIS estimates, and bridging does not help.

According to the analysis of Section 3.1, the choice of choice of n = 4 for LIS used above is not
optimal for either of these distribution sequences when q is large. For the sequence with s = 1 and
t = 4, using n = 6 should be better by a factor of 1.176. However, in LIS runs with q = 30, the mean
squared error using n == 4 and m = 200 is indistinguishable from that using n = 6 and m = 143,
given the standard errors (a factor of 1.09 or more should have been detectable). Of course, q = 30
does not give exactly uniform distributions, and these values of m may not be large enough for the
asymptotic results to apply, especially since the Markov transitions do not sample independently. For
the sequence with s = 0.05 and t = 0, the results in Section 3.1 indicate that using n = 3 should be
better by a factor of 1.084. In this case, LIS runs with q = 30 using n = 3 and m = 250 are better than
runs using n = 4 and m = 200 by a factor of 1.16, signiﬁcantly greater than one given the standard
errors, but not signiﬁcantly diﬀerent from the expected ratio of 1.084.

26

AIS: n=1000,  LIS: n=4 m=200
LIS−optimal

LIS−geometric

AIS

4
0

.

0

3
0

.

0

r
o
r
r

 

E
d
e
r
a
u
q
S
n
a
e
M

 

2
0

.

0

1
0

.

0

0
0

.

0

4
0

.

0

for

rev

bri

for

rev

bri

for

rev

bri

AIS: n=1000,  LIS: n=19 m=50
LIS−optimal

LIS−geometric

AIS

3
0

.

0

r
o
r
r

 

E
d
e
r
a
u
q
S
n
a
e
M

 

2
0

.

0

1
0
0

.

0
0
0

.

for

rev

bri

for

rev

bri

for

rev

bri

AIS: n=1000,  LIS: n=9 m=100
LIS−optimal

LIS−geometric

AIS

for

rev

bri

for

rev

bri

for

rev

bri

AIS: n=1000,  LIS: n=39 m=25
LIS−optimal

LIS−geometric

AIS

for

rev

bri

for

rev

bri

for

rev

bri

4
0

.

0

3
0

.

0

2
0

.

0

1
0

.

0

0
0

.

0

4
0

.

0

3
0

.

0

2
0

.

0

1
0
0

.

0
0
0

.

Figure 9: Results using increasing values of n for LIS, while keeping computation time constant, for
the distribution sequence with s = 1, t = 4, and q = 10. The same AIS procedure was used for all plots,
but results vary randomly.

27

AIS: n=1000,  LIS: n=4 m=200
LIS−optimal

LIS−geometric

AIS

for

rev

bri

for

rev

bri

for

rev

bri

AIS: n=1000,  LIS: n=19 m=50
LIS−optimal

LIS−geometric

AIS

for

rev

bri

for

rev

bri

for

rev

bri

0
2
0

.

0

5
1
0

.

0

0
1
0

.

0

5
0
0

.

0

0
0
0

.

0

0
2
0

.

0

5
1
0

.

0

0
1
0

.

0

5
0
0
0

.

0
0
0
0

.

0
2
0

.

0

5
1
0

.

0

0
1
0

.

0

5
0
0

.

0

0
0
0

.

0

0
2
0

.

0

5
1
0

.

0

0
1
0

.

0

5
0
0
0

.

0
0
0
0

.

AIS: n=1000,  LIS: n=9 m=100
LIS−optimal

LIS−geometric

AIS

for

rev

bri

for

rev

bri

for

rev

bri

AIS: n=1000,  LIS: n=39 m=25
LIS−optimal

LIS−geometric

AIS

for

rev

bri

for

rev

bri

for

rev

bri

r
o
r
r

 

E
d
e
r
a
u
q
S
n
a
e
M

 

r
o
r
r

 

E
d
e
r
a
u
q
S
n
a
e
M

 

Figure 10: Results using increasing values of n for LIS, while keeping computation time constant, for
the distribution sequence with s = 0.05, t = 0, and q = 10. The same AIS procedure was used for all
plots, but results vary randomly.

28

q=2

q=10

AIS

LIS−geometric

LIS−optimal

AIS

LIS−geometric

LIS−optimal

7

.

0

6

.

0

5

.

0

4

.

0

3
.

0

2

.

0

1

.

0

0

.

0

7

.

0

6

.

0

5

.

0

4

.

0

3

.

0

2

.

0

1
0

.

0
0

.

for

rev

bri

for

rev

bri

for

rev

bri

q=20

AIS

LIS−geometric

LIS−optimal

for

rev

bri

for

rev

bri

for

rev

bri

7

.

0

6

.

0

5

.

0

4

.

0

3
.

0

2

.

0

1

.

0

0

.

0

7

.

0

6

.

0

5

.

0

4

.

0

3

.

0

2

.

0

1
0

.

0
0

.

for

rev

bri

for

rev

bri

for

rev

bri

q=30

AIS

LIS−geometric

LIS−optimal

0.94 0.95

for

rev

bri

for

rev

bri

for

rev

bri

r
o
r
r

 

E
d
e
r
a
u
q
S
n
a
e
M

 

r
o
r
r

 

E
d
e
r
a
u
q
S
n
a
e
M

 

Figure 11: Results with increasing values of q, for sequences of distributions with s = 1 and t = 4. The
AIS runs used n = 250; the LIS runs used n = 4 and m = 50, requiring the same amount of computation.

29

q=2

q=10

AIS

LIS−geometric

LIS−optimal

AIS

LIS−geometric

LIS−optimal

5
2

.

0

0
2

.

0

5
1

.

0

0
1

.

0

5
0
0

.

0
0

.

0

5
2

.

0

0
2

.

0

5
1

.

0

0
1

.

0

5
0

.

0

0
0
0

.

for

rev

bri

for

rev

bri

for

rev

bri

q=20

LIS−geometric

LIS−optimal

AIS

0.34

for

rev

bri

for

rev

bri

for

rev

bri

5
2

.

0

0
2

.

0

5
1

.

0

0
1

.

0

5
0
0

.

0
0

.

0

5
2

.

0

0
2

.

0

5
1

.

0

0
1

.

0

5
0

.

0

0
0
0

.

for

rev

bri

for

rev

bri

for

rev

bri

q=30

LIS−geometric

LIS−optimal

AIS

0.59

for

rev

bri

for

rev

bri

for

rev

bri

r
o
r
r

 

E
d
e
r
a
u
q
S
n
a
e
M

 

r
o
r
r

 

E
d
e
r
a
u
q
S
n
a
e
M

 

Figure 12: Results with increasing values of q, for sequences of distributions with s = 0.05 and t = 1.
The AIS runs used n = 250; the LIS runs used n = 4 and m = 50, requiring the same amount of
computation.

30

5 Other applications of linked sampling

So far in this paper, I have focused on how Linked Importance Sampling can be used to estimate ratios
of normalizing constants. LIS can also be used to estimate expectations with respect to π1, however,
and in some applications, this may be its most important use. Linked sampling methods related to
LIS can also be applied in other ways. I brieﬂy described these other applications here, outlining the
use of linked sampling for ‘dragging’ fast variables in some detail.

5.1 Estimating expectations

The expectation of some function, a(x), with respect to π1 can be estimated using simple importance
sampling, with points drawn from π0, as follows:

Eπ1[a(X)] = Eπ0(cid:20)a(X)

p1(X)

p0(X)(cid:21) . Z1

Z0 ≈

1
N

N

Xi=1

a(x(i))

p1(x(i))

p0(x(i)) . 1

N

p1(x(i))
p0(x(i))

N

Xi=1

(49)

where x(i), . . . , x(N ) are drawn from π0. Like equation (2), this estimate is valid only if no region
having zero probability under π0 has non-zero probability under π1. The two factors of 1/N of course
cancel, but are included to emphasize the connection with the estimate for r = Z1/Z0, which is simply
the denominator of the estimate above.

Since LIS can be viewed as simple importance sampling on an extended state space, with distribu-
tions Π0 and Π1 deﬁned by the forward and reverse procedures of Section 2, we can use equation (49)
to estimate any quantity that can be expressed as an expectation with respect ot Π1. Step (1) of the
reverse procedure deﬁning Π1 sets xn,µn to a value randomly chosen from πηn = π1. Step (2) then
sets the other xn,k to values obtained from xn,µn by applying Markov chain transitions that leave π1
invariant. It follows that under Π1, all the points xn,k have marginal distribution π1 (though they
may not be independent). Accordingly,

Eπ1[a(X)] = E Π1"

1

Kn +1

a(Xn,k)#

Kn

Xk=0

(50)

Estimating the right side as in equation (49), and using the fact that the ratio of probabilities under
Π1 over those under Π0 is given by ˆr(i)

LIS in equation (10), we get the estimate

Eπ1[a(X)] ≈

LIS

ˆr(i)
Kn +1

M

Xi=1

Kn

Xk=0

a(x(i)

n,k) .

M

Xi=1

ˆr(i)

LIS

(51)

If the M runs of LIS are started by sampling independently from π0 (as will often be possible), the
standard error of this estimate can be assessed in the usual fashion for importance sampling, as I have
discussed for the analogous AIS estimates in (Neal 2001). This error assessment can be diﬃcult, since
when some ˆr(i)
LIS is hard to estimate. Note, however,
that the degree to which the Markov chain transitions used have converged need not be assessed, a

LIS are much larger than others, the variance of ˆr(i)

31

possible advantage compared with simple MCMC estimates. The estimate of equation (51) will be
asymptotically correct (as M → ∞) regardless of how far these Markov chain transitions are from
convergence.

The primary reason one might wish to use LIS to estimate expectations is that going through
the sequence of distributions parameterized by η0, . . . , ηn may produce an ‘annealing’ eﬀect, which
prevents the Markov chain sampler from being trapped in a local mode of the distribution. Compared
with the analogous AIS procedure, LIS may perform better for some forms of distributions, for the
same reasons as were discussed in Sections 3 and 4. One should also note that LIS estimates for
expectations with respect to πηj for all j can easily be obtained from a single set of runs, by simply
considering the results of each LIS run up to the point where the sample for πηj is obtained.

5.2 A linked form of tempered transitions

My ‘tempered transition’ method (Neal 1996) is another approach to sampling from distributions
with isolated modes, between which movement is diﬃcult for Markov chain transitions such as simple
Metropolis updates.
In this approach, such simple Markov chain transitions are supplemented by
occasional complex ‘tempered transitions’, composed of many simple Markov chain transitions. A
tempered transition consists of several stages, which proceed through a sequence of distributions, from
the distribution being sampled, to a ‘higher temperature’ distribution in which movement between
modes is easier, and then back down to the distribution being sampled. At each stage of a tempered
transition, we generate a single new state by applying a Markov chain transition to the current state,
after which we switch to the next distribution in the sequence. The second half of a tempered transition
is similar to an Annealed Importance Sampling run, while the ﬁrst half is similar to an AIS run with
the reversed sequence of distributions.

A similar ‘linked’ procedure can be deﬁned, in which at each stage we generate a chain of states by
applying a Markov chain transition. We then select a ‘link state’ from this sequence (using a suitable
bridge distribution) which serves as the starting point for the chain of states generated in the next
stage. In the ﬁnal stage, a chain of states is produced using a Markov chain transition that leaves the
distribution being sampled invariant, and a candidate state is selected uniformly at random from this
chain. The appropriate probability for accepting this candidate state is computed using ratios similar
to those going into the LIS estimate of equation (10).

As discussed in Section 4, for AIS to work well, all distributions in the sequence must assign
reasonably high probability to regions of the space that have non-negligible probability under the next
distribution in the sequence. One would expect tempered transitions to work well only when this
holds for both the sequence and its reversal.
In contrast, one would expect the ‘linked’ version of
tempered transitions to work well as long as the sequence satisﬁes the weaker condition that there be
some ‘overlap’ between adjacent distributions (assuming a suitable bridge distribution is used).

32

5.3 Dragging fast variables using linked chains

A slight modiﬁcation of the tempered transition method can be applied to problems in which the state
is composed of both ‘fast’ and ‘slow’ variables. We will write the distribution of interest for such a
problem as

π(x, y) = (1/Z) exp(−U (x, y))

(52)

where x denotes the ‘fast’ variables and y the ‘slow’ variables. We assume that the computation
is dominated by the time required to evaluate U (x, y), but that once U (x, y) has been evaluated,
with relevant intermediate quantities saved, evaluating U (x′, y) for any new x′ is much faster than
evaluating U (x′, y′) for some y′ not previously encountered. One example of such a problem is inference
for Gaussian process classiﬁcation models (Neal 1999), in which y consists of the hyperparameters
deﬁning the covariance function used, and x consists of the latent variables associated with the n
observations. After a change to y, we must recompute the Cholesky decomposition of an n × n
covariance matrix, which takes time proportional to n3, whereas after a change to x only, U (x, y) can
be re-computed in time proportional to n2, assuming the Cholesky decomposition for this value of y
has been saved.

In my method for ‘dragging’ fast variables (Neal 2004), the ability to quickly re-evaluate U (x, y)
when only x changes is exploited to allow larger changes to be made to y than would be possible
if x were kept ﬁxed, or were given a new value from some simple proposal distribution. From the
state (x0, y0), a dragging update proposes a new value y1, drawn from some symmetrical proposal
distribution, in conjunction with a new value x1 that is found by applying a succession of Markov
chain updates that leave invariant distributions in the series, πηj (x), for j = 1, . . . , n− 1, with 0 <
ηj < ηj+1 < 1. The proposed state, (x1, y1), is then accepted or rejected in a fashion analogous to
tempered transitions.

The distributions in the sequence used are deﬁned by the following unnormalized probability or

density function, which depends on the current and proposed values for y:

pη(x) = exp (− ((1−η) U (x, y0) + η U (x, y1)))

(53)

The corresponding normalized probability or density function will be written as πη. Note that π0(x) =
π(x|y0) and π1(x) = π(x|y1). Crucially, after U (x, y0) and U (x, y1) have been evaluated once (for
any x), we can evaluate pη(x) for any η and any x without any further ‘slow’ computations. Indeed,
since U (x0, y0) will usually have already been evaluated as part of the previous Markov chain transition,
only one slow computation will be required to evaluate pη(x) for any number of values of η and x.

A ‘linked’ dragging update can be deﬁned as follows. Given the sequence of distributions deﬁned
by η0, . . . , ηn, with η0 = 0 and ηn = 1, the numbers of transitions (T or T ) to perform for each
distribution over x, denoted by K0, . . . , Kn, and a set of bridge distributions, denoted by pj ∗j+1, for
j = 0, . . . , n−1, an update from the current state (x0, y0) is done as follows:

33

The Linked Dragging Procedure

1) Propose a new value, y1, from some proposal distribution S(y1|y0), which satisﬁes the symmetry

condition that S(y1|y0) = S(y0|y1).

2) Pick an integer ν0 uniformly at random from {0, . . . , K0}, and then set x0,ν0 to the current values

of the fast variables, x0.

3) For j = 0, . . . , n, create a chain of values for x associated with πηj as follows:

a) If j > 0: Pick an integer νj uniformly at random from {0, . . . , Kj}, and then set xj,νj to

xj−1∗j.

b) For k = νj + 1, . . . , Kj, draw xj,k according to the forward Markov chain transition prob-

abilities Tηj (xj,k−1, xj,k). (If νj = Kj, do nothing in this step.)

c) For k = νj − 1, . . . , 0, draw xj,k according to the reverse Markov chain transition probabil-

ities T ηj (xj,k+1, xj,k). (If νj = 0, do nothing in this step.)

d) If j < n: Pick a value for µj from {0, . . . , Kj} according to the following probabilities

Π0(µj | xj) =

pj ∗j+1(xj,µj )

pηj (xj,µj ) .

Kj

Xk=0

pj ∗j+1(xj,k)

pηj (xj,k)

(54)

and then set xj ∗j+1 to xj,µj .

3) Set µn to a value chosen uniformly at random from {0, . . . , Kn}, and let the proposed new values

for the fast variables, x1, be equal to xn,µn.

4) Accept (x1, y1) as the new state with probability

min


1,

n−1

Yj=0




1

Kj + 1

Kj

Xk=0

pj ∗j+1(xj,k)

pηj (xj,k) .

1

Kj+1 + 1

Kj+1

Xk=0

pj ∗j+1(xj+1,k)

pηj+1(xj+1,k) 


(55)




If (x1, y1) is not accepted, the new state is the same as the old state, (x0, y0).

One can show that this update leaves π(x, y) invariant by showing that it satisﬁes detailed balance,
which in turns follows from the stronger property that the probability of starting at (x0, y0), assuming
this start state comes from π(x, y), then generating the various quantities produced by the above
procedure, and ﬁnally accepting (x1, y1) as the new state, is the same as the probability of starting
this procedure at (x1, y1), generating the same quantities in reverse, and ﬁnally accepting (x0, y0).
The proof of this is analogous to the derivation of LIS in Section 2.

To use the linked dragging procedure, we need to select suitable bridge distributions. Since the
characteristics of πη(x) will depend on y0 and y1, and of course η, we may not know enough to select
good estimates for the values of r needed to use the optimal bridge of equation (6), though we might

34

try just setting r to one. This is not a problem for the geometric bridge of equation (5), for which the
acceptance probability above can be written as

1

Kj + 1

Kj

Xk=0 s pηj+1(xj,k)
pηj (xj,k) .

1

Kj+1 + 1

Kj+1

pηj+1 (xj+1,k)
Xk=0 s pηj (xj+1,k)





From equation (53), we see that

min


1,

n−1

Yj=0




pηj+1(xj,k)
pηj (xj,k)

pηj (xj+1,k)
pηj+1(xj+1,k)

= exp (− (ηj+1−ηj) (U (xj,k, y1)−U (xj,k, y0)))

= exp (− (ηj+1−ηj) (U (xj+1,k, y0)−U (xj+1,k, y1)))

(56)

(57)

(58)

For the simplest case with no intermediate distributions (ie, with n = 1), the acceptance probability
simpliﬁes to

min

1,




1

K0 + 1

1

K1 + 1

K0

K1

Xk=0
Xk=0

exp (− (U (xj,k, y1)−U (xj,k, y0)) / 2)

exp (− (U (xj,k, y0)−U (xj,k, y1)) / 2)




(59)

6 Conclusions and Future work

In this paper, I have demonstrated that in some situations Linked Importance Sampling is substan-
tially more eﬃcient than Annealed Importance Sampling, provided a suitable number of intermediate
distributions are used. However, in other situations, where the tails of the distributions involved are
suﬃciently heavy, the two methods are about equally eﬃcient. More research is therefore needed to
determine for which problems of practical interest LIS, and related linked sampling methods, will be
useful.

In tests on multivariate Gaussian distributions, I have not seen an advantage for LIS over AIS. Both
perform about equally well on a sequence of 100-dimensional spherical Gaussian distributions with
variances changing by a factor of two, so that log(r) = −100. This is in accord with the results in
Section 4, where LIS had little or no advantage over AIS when the distributions were Gaussian. LIS
is more likely to be useful for problems involving continuous distributions with lighter tails.

One problem that may beneﬁt from LIS is that of computing the probability of a very rare event,
which can be cast as computing the normalizing constant for a distribution with the constraint that
the state be in the set corresponding to this event. Intermediate distributions might use looser forms
of this constraint. If, in all these distributions, states violating the constraints have zero probability,
AIS will tend to have the same bad behaviour seen with uniform distributions in Section 3.2, while
LIS may work much better.

35

Another context where LIS may outperform AIS is when only a ﬁxed number of intermediate
distributions are available — ie, only a ﬁnite number of values are allowed for η. This is the situation
for the ‘sequential importance sampler’ of MacEachern, Clyde, and Liu (1999), which can be seen as
an instance of AIS (Neal 2001). Here, the intermediate distributions use only a fraction of the n items
in the data set; such a fraction can only have the form j/n with j an integer. The distance between
successive distributions for this problem may sometimes be too great for AIS to work well, but their
overlap might nevertheless be suﬃcient for LIS.

It may be possible to improve LIS by reducing the variance in how well it samples at each stage.
Instead of performing a predetermined number, Kj, of Markov transitions at stage j, we might instead
perform as many transitions as are necessary to obtain a good sample. Deﬁne a ‘tour’ to be a
sequence of transitions that moves from a high value of some key quantity (eg, U (x) for the canonical
distributions of equation (1)) to a low value of this quantity, or vice versa. Good sampling might
be ensured by performing some predetermined number of tours, with the number of these tours that
occur before and after the link state being chosen at random. Suitable ‘high’ and ‘low’ values would
probably need to be found using preliminary runs.

More speculatively, it seems as if there should be some method that has the advantages of LIS
over AIS, but that like AIS uses many intermediate distributions, performing only a single Markov
transition for each. Intuitively, it seems that such a ‘smooth’ method that does not abruptly change
η should be more eﬃcient. One can use LIS with all Kj set to one, but this will produce good results
only if n is large, which we saw in the analysis of Section 3.1 does not lead to an advantage over
AIS. Perhaps some way could be found of using states associated with all values of η when estimating
each of the ratios Zηj+1 /Zηj , while still producing an estimate that is exactly unbiased even when the
Markov transitions do not reach equilibrium.

Acknowledgements

This research was supported by the Natural Sciences and Engineering Research Council of Canada. I
hold a Canada Research Chair in Statistics and Machine Learning.

References

Bennett, C. H. (1976) “Eﬃcient estimation of free energy diﬀerences from Monte Carlo data”, Journal

of Computational Physics, vol. 22, pp. 245-268.

Crooks, G. E. (2000) “Path-ensemble averages in systems driven far from equilibrium”, Physical Review

E, vol. 61, pp. 2361-2366.

Gelman, A. and Meng, X.-L. (1998) “Simulating normalizing constants: From importance sampling

to bridge sampling to path sampling”, Statistical Science, vol. 13, pp. 163-185.

36

Hendrix, D. A. and Jarzynski, C. (2001) “A “fast growth” method of computing free energy diﬀer-

ences”, Journal of Chemical Physics, vol. 114, pp. 5974-5981.

Jarzynski, C. (1997) “Nonequilibrium equality for free energy diﬀerences”, Physical Review Letters,

vol. 78, pp. 2690-2693.

Jarzynski, C. (2001) “A “fast growth” method of computing free energy diﬀerences”, Journal of

Chemical Physics, vol. 114, pp. 5974-5981.

Lu, N., Singh, J. K., and Kofke, D. A. (2003) “Appropriate methods to combine forward and reverse

free-energy perturbation averages”, Journal of Chemical Physics, vol. 118, pp. 2977-2984.

MacEachern, S. N., Clyde, M., and Liu, J. S. (1999) “Sequential importance sampling for nonpara-

metric Bayes models: The next generation”, Canadian Journal of Statistics, vol. 27, pp. 251-267.

Meng, X.-L. and Wong, H. W. (1996) “Simulating ratios of normalizing constants via a simple identity:

A theoretical exploration”, Statistica Sinica, vol. 6, pp. 831-860.

Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. (1953) “Equation
of state calculations by fast computing machines”, Journal of Chemical Physics, vol. 21, pp. 1087-
1092.

Neal, R. M. (1993) Probabilistic Inference Using Markov Chain Monte Carlo Methods, Technical Re-
port CRG-TR-93-1, Dept. of Computer Science, University of Toronto, 140 pages. Obtainable from
http://www.cs.utoronto.ca/∼radford/.

Neal, R. M. (1996) “Sampling from multimodal distributions using tempered transitions”, Statistics

and Computing, vol. 6, pp. 353-366.

Neal, R. M. (1999) “Regression and classiﬁcation using Gaussian process priors” (with discussion), in

J. M. Bernardo, et al (editors) Bayesian Statistics 6, Oxford University Press, pp. 475-501.

Neal, R. M. (2001) “Annealed importance sampling”, Statistics and Computing, vol. 11, pp. 125-139.

Neal, R. M. (2004) “Taking bigger Metropolis steps by dragging fast variables”, Technical Report

No. 0411, Dept. of Statistics, University of Toronto, 9 pages.

Schervish, M. J. (1995) Theory of Statistics, Springer.

Shirts, M. R., Bair, E., Hooker, G., and Pande, V. S.‘ (2003) “Equilibrium free energies from nonequi-
librium measurements using maximum-likelihood methods”, Physical Review Letters, vol. 91,
p. 140601.

37

