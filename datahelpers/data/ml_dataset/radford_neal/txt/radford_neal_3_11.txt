2
1
0
2

 

y
a
M
1

 

 
 
]

O
C

.
t
a
t
s
[
 
 

1
v
0
7
0
0

.

5
0
2
1
:
v
i
X
r
a

Technical Report No. 1201, Department of Statistics, University of Toronto

How to View an MCMC Simulation as a Permutation,

with Applications to Parallel Simulation and

Improved Importance Sampling

Radford M. Neal

Dept. of Statistics and Dept. of Computer Science

University of Toronto

http://www.cs.utoronto.ca/∼radford/

radford@stat.utoronto.ca

30 April 2012

Abstract. Consider a Markov chain deﬁned on a ﬁnite state space, X , that leaves invariant the uniform
distribution on X , and whose transition probabilities are integer multiples of 1/Q, for some integer Q. I
show how a simulation of n transitions of this chain starting at x0 can be viewed as applying a random
permutation on the space X × U , where U = {0, 1, . . . , Q−1}, to the start state (x0, u0), with u0 drawn
uniformly from U . This result can be applied to a non-uniform distribution with probabilities that
are integer multiples of 1/P , for some integer P , by representing it as the marginal distribution for
X from the uniform distribution on a suitably-deﬁned subset of X × Y, where Y = {0, 1, . . . , P − 1}.
By letting Q, P , and the cardinality of X go to inﬁnity, this result can be generalized to non-rational
probabilities and to continuous state spaces, with permutations on a ﬁnite space replaced by volume-
preserving one-to-one maps from a continuous space to itself. These constructions can be eﬃciently
implemented for chains commonly used in Markov chain Monte Carlo (MCMC) simulations. I present
two applications in this context — simulation of K realizations of a chain from K initial states, but with
transitions deﬁned by a single stream of random numbers, as may be eﬃcient with a vector processor or
multiple processors, and use of MCMC to improve an importance sampling distribution that already has
substantial overlap with the distribution of interest. I also discuss the implications of this “permutation
MCMC” method regarding the role of randomness in MCMC simulation, and the potential use of
non-random and quasi-random numbers.

1 Introduction

Markov chain Monte Carlo (MCMC) simulation might seem to be a fundamentally contractive process.
A simulation started from a broad initial distribution must, after many transitions, be concentrated
in the possibly much-smaller region that has high probability under the equilibrium distribution being
sampled. This implies that the random map determined by the random numbers underlying the Markov
chain transitions must be contractive — for a ﬁnite state space, it must map a large set of states to
a smaller set of states, and for a continuous state space, it must map a set of large volume to a set
of smaller volume. This contractive property underlies the ability to “couple” a set of chains so that
eventually they all “coalesce” to the same state, as is exploited by methods such as coupling from the
past (Propp and Wilson, 1996) and circular coupling (Neal, 1999/2002).

1

In this paper, I show that with a simple extension of the state space this contractive behaviour can be
converted to a non-contractive map — which is a permutation when this extended state space is ﬁnite,
or a one-to-one map that preserves volume when the extended state space is continuous. This result
was suggested by the volume-preserving property of Hamiltonian dynamics (see Neal, 2010), which
can be used to deﬁne an importance sampling procedure based on annealing (Neal, 2005).
I expect
that the result in this paper can be applied to produce similar procedures that combine annealing and
importance sampling using other MCMC techniques. However, I will leave that for future work, and
instead present two simpler applications of what I will call “permutation MCMC”.

One application is to parallel simulation from many initial states using a single stream of random
numbers. Using a single random number stream for all parallel chains may reduce the computational
cost, perhaps especially if the parallelism takes the form of vector operations. (At worst, it costs the
same as using multiple streams, if due to high communication cost it is fastest to compute the same
stream separately in each processor.) Using a single stream also avoids the issue of how to set up multiple
streams that are unrelated, a problem that is discussed, for example, by Wu and Huang (2006).

However, some ways of using a single random number stream to deﬁne transitions in parallel chains
lead to the chains coalescing to the same state, or to states that approach each other increasingly closely,
eliminating the beneﬁt of multiple chains in producing better estimates. I will demonstrate that this is
avoided when transitions are deﬁned as random permutations, or as random volume-preserving maps.

A second application is to improving importance sampling, in which expectations with respect to
some distribution of interest are found using points drawn from some approximating distribution that
is easier to sample from. This method produces good results only when the importance sampling
distribution is a suﬃciently good approximation, and most crucially does not give very low probability
to regions that have signiﬁcant probability under the distribution of interest. This can be hard to
guarantee in high-dimensional problems. We might improve an importance sampling distribution that
is close to being adequate — in the sense that it at least has substantial overlap with the distribution of
interest — by performing some number of transitions of a Markov chain that leaves the distribution of
interest invariant starting from a point drawn from the original importance sampling distribution. This
will improve the approximation even if the random numbers used to simulate this Markov chain are
ﬁxed, provided we choose the number of transitions randomly, so that the ﬁnal importance sampling
distribution is a mixture of distributions after varying numbers of transitions. With standard methods
of simulation, however, computing the importance sampling probabilities (or densities), as needed to
ﬁnd appropriate weights, will often be infeasible, because the same ﬁnal point might be produced from
several initial points (or for a continuous distribution, an unknown change in volume may alter the
densities).
I will show how this problem can be bypassed by viewing the transitions as applying a
random permutation (or volume-preserving map), for which the probability (or density) of the ﬁnal
point is the same as that of the initial point.

Recently, Murray and Elliott (2012) independently devised MCMC simulation methods equivalent or
similar to some of the methods I present below, though without additional variables needed to produce
a volume-preserving map. Their aim was to ﬁnd a simulation method that is insensitive to dependence
in the stream of random numbers used, or even to whether they are actually random. I conclude this
paper by also discussing what permutation MCMC says about the role of randomness, and how MCMC
eﬃciency might be improved by using permutation MCMC with non-random or quasi-random numbers.

The programs and scripts used for the experiments in this paper are available from my web page.

2

2 Viewing MCMC for a uniform distribution as a random permutation

I will begin with the simple case of a Markov chain that samples from the uniform distribution on some
ﬁnite state space. In the following sections, I generalize to other discrete and continuous distributions.
Consider a Markov chain on some ﬁnite state space, X , which we can take to be {0, . . . , M−1}. Let
the probability of this chain transitioning to state x′ when the current state is x be T (x, x′), and for
the moment assume these transition probabilities are integer multiples of 1/Q, for some integer Q. We
wish to use this chain to sample from the uniform distribution on X , so T will be chosen to leave this
uniform distribution invariant — that is,

(1/M ) T (x, x′) = 1/M

(1)

Xx∈X

If we view T as a matrix, this condition is equivalent to all its columns (as well as all its rows) summing
to one.

A standard way to simulate a realization, x0, x1, x2, . . . of this chain, starting from some state x0, is
to draw u0, u1, u2, . . . independently from the uniform distribution on U = {0, 1, . . . , Q−1} and then
set

xi+1 = maxnx′ : Q

x′−1Xx=0

T (xi, x) ≤ uio

(2)

The ﬁrst step in converting this simulation to a random permutation is to extend the state space
to X × U . We then draw a value for u0 uniformly from U . Subsequent transitions from (x0, u0) are
deﬁned using s0, s1, s2, . . ., which are independently drawn uniformly from U . (We will see below that
s0, . . . , sn specify a random permutation mapping (x0, u0) to (xn, un).) From the state (xi, ui), xi+1 is
derived from xi and ui as in equation (2) above, and ui+1 is derived from xi, ui, si, and xi+1 as follows:

ui+1 = si + ui − Q

T (xi, x) + Q

xi+1−1Xx=0

xi−1Xx=0 eT (xi+1, x) (mod Q)

(3)

To see informally the rationale for this, note that the terms on the right other than si deﬁne a value

where eT (x, x′) = T (x′, x) are the transition probabilities for the reversed chain.
for u that would lead back to xi if an equation analogous to (2) were applied with T replaced by eT .

This part of the map from (xi, ui) to (xi+1, ui+1) is therefore a permutation. Adding si modulo Q is a
random circular permutation, so the full map from (xi, ui) to (xi+1, ui+1) is a random permutation as
well. For any n > 0, the map from (x0, u0) to (xn, un), being a composition of random permutations, is
also a random permutation.

Furthermore, if we look at only a single realization of the chain, setting ui+1 to an independent
random si plus anything (mod Q) has the same eﬀect as setting ui+1 independently at random, so the
joint distribution of (x1, u1), (x2, u2), . . . is the same as for the standard method of simulation.

Appendix A shows in detail that the map (xi, ui) → (xi+1, ui+1) deﬁned by equations (2) and (3) is

a permutation, by explicitly exhibiting the inverse map.

3

Here is a matrix of transition probabilities for a simple example with M = 4 states:

T = 

2/3 1/3

0

1/3 1/3 1/3

0

0

0

0

1/3 1/3 1/3

0

1/3 2/3



(4)

All the columns above sum to one, so these transitions leave the uniform distribution on X =
{0, 1, 2, 3, 4} invariant. Since all the transition probabilities are multiples of 1/3, we can set Q = 3, and

hence U = {0, 1, 2}. Note that these transitions are reversible — that is, eT (x, x′) = T (x′, x) = T (x, x′).

The permutation maps from (xi, ui) to (xi+1, ui+1) when si has each of its possible values are shown

here:

x

x

x

0

1

2

3

0

1

2

3

0

1

2

3

0

u

1

2

0

u

1

2

0

u

1

2

si = 0

si = 1

si = 2

In these diagrams, the array of circles represents all possible (x, u) pairs, and the arrows show how such a
pair for (xi, ui) is mapped to (xi+1, ui+1). For example, the arrow out of the state with xi = 1 and ui = 2
goes to a state with xi+1 = 2, regardless of the value of si, since Q (T (1, 0)+T (1, 1)) = 3(2/3) = 2 ≤ 2,
so the maximum in equation (2) will be x′ = 2. When si = 0, equation (3) will ﬁnd a value for ui+1
that would lead back to the state xi = 1 starting from xi+1 = 2, which requires that ui+1 be at least
Q T (2, 0) = 0, to which must be added the amount by which ui was greater than the minimum needed
for the transition to xi+1 = 2 to be taken (essential to avoid two states mapping to the same new state),
which in this case is 0. The result is that the diagram for si = 0 has the transition (1, 2) → (2, 0).

The maps for si 6= 0 can be obtained from the map for si = 0 by circularly shifting the ui+1. Note
that when, as here, the transitions are reversible, the diagram for si = 0 will consist entirely of single
states with arrows pointing to themselves and pairs of states connected by arrows both ways.

For comparison, here are the maps produced by the T deﬁned in equation (4) when equation (3) is

replaced by ui+1 = si + ui (mod Q):

x

x

x

0

1

2

3

0

1

2

3

0

1

2

3

0

u

1

2

0

u

1

2

0

u

1

2

si = 0

si = 1

si = 2

Some states have zero or two incoming arrows, so these are clearly not permutations.

4

Below, are the transition probabilities, T , for a non-reversible Markov chain with M = 4 states
that leaves the uniform distribution on X = {0, 1, 2, 3, 4} invariant, along with the reverse transition

probabilities, eT , found by transposing T :

1/2 1/2

0

0

T = 

1/4 1/4 1/4 1/4

0

0

1/2 1/2

1/4 1/4 1/4 1/4

,



eT = 

1/2 1/4

1/2 1/4

0

0

1/4

1/4

0

0

1/4 1/2 1/4

1/4 1/2 1/4



(5)

Since all transition probabilities are multiples of 1/4, we can set Q = 4, so that U = {0, 1, 2, 3}. The
permutation maps for this example from (xi, ui) to (xi+1, ui+1) for each si are as follows:

x

0

1

2

3

x

0

1

2

3

si = 0 :

u

si = 2 :

u

0

1

2

3

0

1

2

3

x

0

1

2

3

si = 1 :

u

si = 3 :

u

0

1

2

3

0

1

2

3

x

0

1

2

3

In this example, the value of xi+1 that follows (xi, ui) is determined using equation (2) in the same
way as for a reversible chain, but the value of ui+1 when si = 0 is not one that would lead back to
xi if T were applied starting from xi+1, but is rather a value that would lead back to xi if the reverse

transition, eT , were applied.

In real MCMC applications, unlike these examples, the state space is enormous, and transition
probabilities are deﬁned algorithmically, rather than via an explicit table. One may then ask whether
the computation of xi+1 and ui+1 from xi and ui according to equations (2) and (3) is feasible.
I
will defer consideration of this issue to the following sections, in which the method is generalized to
non-uniform distributions and to continuous state spaces.

5

3 Generalization to non-uniform discrete distributions

As a ﬁrst step in generalizing the result in the previous section, let us consider a distribution on
X = {0, . . . , M − 1} with probabilities proportional to a function π(x) whose values are all integer
(We may not know the constant of proportionality,
multiples of 1/P , for some positive integer P .

1/Px π(x).) This distribution can be obtained as the marginal distribution on X obtained from a

uniform joint distribution on the following subset of X × Y, where Y = {0, . . . , P −1}:

Z = { (x, y) : 0 ≤ y < P π(x)}

(6)

The cardinality of Z is M + = PPx

π(x).

This construction is analogous to what is done for “slice sampling” MCMC methods (Neal, 2003), in
which Markov transitions are deﬁned on this extended state space. Here, I will assume that our MCMC
method is deﬁned in terms of transitions on X , with the introduction of the extended space X ×Y being
only a device to allow these transitions to be expressed as permutations. (However, transitions deﬁned
on X × Y could be accommodated if desired.)

Suppose that we have deﬁned a Markov chain on X , with transition probabilities T (x, x′), all integer
multiples of 1/Q, that leaves the distribution π(x) invariant. We can deﬁne a Markov chain on Z that
leaves the uniform distribution on Z invariant, with transition probabilities as follows:

T (x, x′)
P π(x′)
0

if 0 ≤ y′ < P π(x′)
otherwise

(7)

(8)

1

M +

T +((x, y), (x′, y′)) = 
M +Xx

1

These transition probabilities are all integer multiples of 1/Q+, where Q+ = (max

P π(x))! Q.
To conﬁrm that T + leaves the uniform distribution invariant, note that for any (x′, y′) in Z,
X(x,y)∈Z
π(x)T (x, x′) =

1
M + T +((x, y), (x′, y′)) =

π(x′)Xx

T (x, x′)
P π(x′)

1

M +

1

P π(x)

=

x

The eﬀect of applying the original transitions, T , starting from some initial state, x0, can be du-
plicated by drawing y0 uniformly from {0, . . . , P π(x0)} and then using the transitions T + to simulate
states (x1, y1), (x2, y2), . . . The resulting distribution for x1, x2, . . . is the same as if T were applied
starting with x0 — the transition from (xi, yi) to (xi+1, yi+1) deﬁned by T + ignores yi, and gives equal
probabilities of T (xi, xi+1)/P π(xi+1) to P π(xi+1) values of y′, so the total probability for a value xi+1
to follow xi is T (xi, xi+1).

An MCMC simulation using T that samples from X with probabilities given by π can therefore be
replaced by a simulation using T + that samples from Z with uniform probabilities. This simulation on
the extended state space Z can be expressed as a random permutation, as described in Section 2. To do
this, we must decide on an ordering of states in Z. In this paper, I will use a lexicographical order (ﬁrst
on x, then on y), in which an (x, y) pair in Z is associated with a label, x+, in X + = {0, 1, . . . , M +−1},
according to the following map:

X +(x, y) = y + P

π(¨x)

x−1X¨x=0

6

(9)

The inverse of this map takes x+ to (X(x+), Y (x+)), where

X(x+) = maxnx : P

x−1X¨x=0

π(¨x) ≤ x+o,

Y (x+) = x+ − P

X(x+)−1X¨x=0

π(¨x)

(10)

We can now apply the method of Section 2, replacing references there to X , M , T , and Q with
references to X +, M +, T +, and Q+, after redeﬁning T + to apply to states in X + rather than the
associated pairs in Z. In terms of T + as redeﬁned in this way, the transitions deﬁned by equation (2)
and equation (3) become

T +(x+

i , x+) ≤ uio

x+

i+1 = maxnx+′ : Q+
ui+1 = si + ui − Q+

x+′−1Xx+=0
i+1−1Xx+=0

x+

x+

i −1Xx+=0eT +(x+

T +(x+

i , x+) + Q+

i+1, x+) (mod Q+)

where eT +(x, x′) = T +(x′, x).
It is convenient to re-express these transitions in terms of (x, y) and the original transitions T . Noting
that X +(x, y) ≥ X +(x′, y) when x ≥ x′ and X(x+) ≥ X(x+′) when x+ ≥ x+′, we can write one of the
sums appearing above as follows, with xi = X(x+

i+1), and yi+1 = Y (x+

i ), xi+1 = X(x+

i+1):

(11)

(12)

(13)

(14)

(16)

(17)

Q+

T +(x+

x+

i+1−1Xx+=0

P π(x)Xy=0

i , x+) = Q+hxi+1−1Xx=0
= Q+hxi+1−1Xx=0

T (xi, x) +

yi+1

P π(xi+1)

yi+1−1Xy=0
T (xi, xi+1)i

T +(x+

i , X +(x, y)) +

T +(x+

i , X +(xi+1, y))i

To rewrite the other sum above, we let yi = Y (x+

i ), and deﬁne the reverse transition probabilities for

the original chain as eT (x, x′) = T (x′, x)π(x′)/π(x). We then have
i+1, x+) = Q+h xi−1Xx=0eT (xi+1, x) +

i −1Xx+=0eT +(x+

Also, note that yi/P π(xi) and yi+1/P π(xi+1) are less than one.

Q+

x+

yi

P π(xi) eT (xi+1, xi)i

(15)

The transitions of equations (11) and (12) can now be rewritten using expressions (14) and (15) for

the sums they contain, as follows:

T (xi, x) ≤ uio
xi+1−1Xx=0

xi+1 = maxnx′ : Q+
x′−1Xx=0
yi+1 = j P π(xi+1)(cid:16)ui − Q+
xi+1−1Xx=0
ui+1 = si + ui − Q+
xi−1Xx=0 eT (xi+1, x) + Q+ eT (xi+1, xi)

T (xi, x) − Q+ T (xi, xi+1)

+ Q+

T (xi, x)(cid:17). (Q+T (xi, xi+1))k

yi+1

P π(xi+1)

7

yi

P π(xi)

(mod Q+)

(18)

Deﬁning y∗ = y/P , so that y∗ is in [0, π(x)), and letting u∗ = u/Q+ and s∗ = s/Q+, both in [0, 1), we
can rewrite the above equations as follows:

i −

T (xi, x)(cid:17). T (xi, xi+1)k. P

y∗

xi+1−1Xx=0

io
T (xi, x) ≤ u∗

x′−1Xx=0
xi+1 = maxnx′ :
i+1 = jP π(xi+1)(cid:16)u∗
xi+1−1Xx=0
xi−1Xx=0 eT (xi+1, x) + eT (xi+1, xi)

T (xi, x) − T (xi, xi+1)

u∗
i+1 = s∗

i + u∗

i −

+

y∗
i+1

π(xi+1)

y∗
i

π(xi)

(mod 1)

(21)

where U (mod 1) means U − ⌊U⌋, the fractional part of U .

If we now let P (and hence also M + and Q+) go to inﬁnity, which corresponds to letting the probabil-
i+1, which when substituted

ities π(x) take on any real values in [0, 1], we get a simpler expression for y∗
into equation (21) gives a simpler expression for u∗

i+1. The ﬁnal result is the following transition:

(19)

(20)

(22)

(23)

(24)

xi+1 = maxnx′ :
x′−1Xx=0
i+1 = π(xi+1)(cid:16)u∗

i −

y∗

io
T (xi, x) ≤ u∗

xi+1−1Xx=0

T (xi, x)(cid:17). T (xi, xi+1)

u∗
i+1 = s∗

i +

xi−1Xx=0 eT (xi+1, x) + eT (xi+1, xi)

y∗
i

π(xi)

(mod 1)

In this limit, the values s∗
from the uniform distribution on [0, 1). Note that this transition preserves the property that y∗
[0, π(xi+1)). In a program, it may be more eﬃcient to maintain the quantity y∗/π(x) rather than y∗.

2, . . . that determine the random transitions are drawn independently
i+1 is in

0, s∗

1, s∗

Since the map deﬁned by equations (22) to (24) is a limit of discrete permutation maps, for an
increasingly ﬁne uniform grid, it should not only be one-to-one, but also preserve volume. This is shown
in Appendix B, by explicitly exhibiting the inverse of the map, and showing that its continuous part
has a Jacobian matrix whose determinant has absolute value one. Note, however, that a computer
implementation of this map that uses ﬂoating-point representations of y∗ and u∗ may not be exactly
reversible, or may not exactly preserve volume, due to round-oﬀ error.

This map is similar to one deﬁned by Murray and Elliott (2012), in their equations (9), (10), and (17).
However, they have no equivalent of y∗, and their update for u∗ is what would be obtained by replacing
y∗
i in equation (24) above by the deﬁnition of y∗
i+1 from equation (23). Because of this diﬀerence, Murray
and Elliott’s map does not preserve volume. This may not matter for their purpose of producing an
MCMC method that works with dependent random number streams.

As an example of the map deﬁned by equations (22) to (24), consider a state space of X = {0, 1, 2}
with probabilities π(0) = 3/10, π(1) = 1/10, and π(2) = 6/10. Let the transition probabilities, T , and

8

their reversal, eT , be given by the following matrices:
,

1/3 1/3 1/3
1
0
1/3
2/3

T = 

0
0

eT = 

The diagram below shows how (xi, y∗

0
0

2/3
1/3
0
1
1/6 1/6 2/3



i , u∗

i ) is mapped to (xi+1, y∗

i+1, u∗

i+1) when si = 0:

0

0

A

*

u

B

E

F

I

x=0

x=0

x=1

C
M
O
Q

(x, y  )*

x=2

H K

x=1

x=2

D

G

R

U

1

J

L
N
P
V

0

A

B
E
F
I

x=0

C

Q

D

R

K

M O

U

*u

x=1

x=2

x=2

x=0

x=2

x=0

x=1

x=2

(25)

1

S

T
G
H
W

1

S

T

W

X

J

L

N

P

V

X

The horizontally dimension of each of the two squares above represents the range of [0, 1) for u∗. The
vertical dimension represents the range of (x, y∗), with [0, 1) divided into a section for each value of x of
size π(x), with the value of y∗ going from 0 to π(x) within each such section. Rectangles in the square
on the left correspond to transition probabilities, T (xi, xi+1). Points in such a rectangle are mapped to
points in a rectangle of equal area in the square on the right, as identiﬁed by the letters labelling the

If π(x) is only proportional to the probabilities, not equal, the the vertical scale in the diagram above

corners. Such rectangles on the right correspond to reverse transition probabilities, eT (xi+1, xi).
would be stretched or compressed by the factorPx

Often, the update deﬁned by equations (22) to (24) will be implementable nearly as eﬃciently as a
standard update for the same transition probabilities, in which just equation (22) is used with a value
for u∗
i drawn uniformly from [0, 1). If the search over x′ involving the sum in equation (22) can be done
eﬃciently, it will usually deliver the value of this sum for xi+1 at little or no extra cost, as needed for
equation (23). The sum in equation (24) involving the reversed transition probabilities will typically be
feasible if the analogous sum for the original transition probabilities is feasible, certainly so if the chain
is reversible, so that the original and reverse transition probabilities are the same.

π(x).

In one common class of problems, X = X1 × ··· × Xd, so that a state x is composed of values for
d discrete variables, with the number of possible values for each of these variables (ie, the cardinality
of each Xk) being small. One popular approach to sampling a distribution on such a state space is to
simulate a Markov chain that applies d updates in succession, each of which changes only one component
of x. Examples of such problems are the Ising and Potts models of statistical physics, and Bayesian
mixture models with conjugate priors in which the continuous parameters have been integrated out,
leaving only discrete class indicators for each data point (as in Algorithm 3 in (Neal, 2000)).

9

Denoting the transition probabilities when only component k of the state is updated by Tk, we see
that for problems of this sort Tk(x, x′) will be non-zero for only a small number of values for x′ (at
most the cardinality of Xk), even when the cardinality of X is enormous. Furthermore, the values of x′
for which Tk(x, x′) may be non-zero are easily identiﬁed, so the sums in equations (22) to (24) can be
eﬃciently computed by explicit summation.

The Metropolis-Hastings algorithm (Metropolis, et al, 1953; Hastings, 1970) is another popular way
to deﬁne a Markov chain on X that converges to the distribution deﬁned by π(x). This method draws a
proposal, ˆxi, for the state to follow xi according to some proposal distribution with probabilities given
by S(xi, ˆxi), and then accepts this proposal as xi+1 with probability a(xi, ˆxi), where

a(x, ˆx) = min(cid:20)1,

π(ˆx) S(ˆx, x)

π(x) S(x, ˆx)(cid:21)

(26)

If the proposal is not accepted, xi+1 = xi. The resulting transition probabilities are reversible, and are
given by

T (x, x′) = 

S(x, x′) a(x, x′)

S(x, x) + X¨x∈X

S(x, ¨x) (1−a(x, ¨x))

if x 6= x′
if x = x′

(27)

Standard simulation of such a chain is feasible if one can sample from S(x, ˆx) and compute π(x) and
S(x, ˆx) (with the latter not required if S(x, ˆx) = S(ˆx, x) so that their ratio in the acceptance probability
is always one). Simulation using equations (22) to (24) might be more diﬃcult, however, because
computation of T (x, x) involves the rejection probabilities (whose computation requires evaluating π)
for all possible proposals.

is used to determine xi+1.

This problem can be bypassed by changing how u∗
i

In equation (22),
the full range of u∗
is partitioned into contiguous sub-intervals associated with each value for xi+1.
i
Instead, we can partition the range of u∗
i into subintervals corresponding to proposals, with sizes given
by S(xi, ˆxi), and then further subdivide each of these subintervals into a part associated with acceptance
of the proposal and a part (possibly empty) associated with rejection. A self-transition, with probability
T (x, x), is then represented by the union of an interval of size S(x, x), corresponding to proposing (and
accepting) the same state as the current state, and zero or more intervals associated with rejection of
proposals to move to a diﬀerent state.

In detail, a Metropolis-Hastings transition can be performed as follows:

i −

io
ˆxi+1 = maxnx′ :
x′−1Xx=0
S(xi, x) ≤ u∗
ˆxi+1−1Xx=0
S(xi, x)(cid:17). S(xi, ˆxi+1)
ai+1 = (cid:16)u∗
xi+1 = ( ˆxi+1
i+1 = ( π(xi+1) ai+1 / a(xi, ˆxi+1)

if ai+1 < a(xi, ˆxi+1)
otherwise

xi

y∗
i

y∗

if ai+1 < a(xi, ˆxi+1)
otherwise

10

(28)

(29)

(30)

(31)

u∗

i+1 = 

s∗
i +

xi−1Xx=0

s∗
i + u∗

i

S(xi+1, x) + S(xi+1, xi) a(xi+1, xi)

y∗
i

π(xi)

(mod 1)

if ai+1 < a(xi, ˆxi+1)

(32)

(mod 1)

otherwise

Appendix C shows that this map is indeed one-to-one and volume preserving.

As an example, consider Metropolis-Hastings transitions that leave invariant the distribution on

X = {0, 1, 2, 3} that has probabilities

π(0) = 1/3, π(1) = 1/3, π(2) = 2/9, π(3) = 1/9

(33)

using the matrix of proposal probabilities, S(x, ˆx), on the left below, which produces the transition
probabilities, T (x, x′), shown on the right:

S =



1/2 1/2

0

1/3 1/3 1/3

0

0

0

0

1/3 1/3 1/3

0

1/2 1/2



,

T =



1/2 + 1/6

1/3

1/3

1/3 + 1/9

0

2/9

0

0

0

0

1/3

0

1/3 + 1/12 1/4

1/2

1/2



(34)

The resulting map from (xi, y∗

i , u∗

i ) to (xi+1, y∗

i+1, u∗

i+1) deﬁned by equations (29) to (32), when si = 0,

is pictured below:

0

0

x=0

x=0

A
B

u*

A B

D

D

F

G

*
(x, y  )

x=1

x=0

x=1

C
G

H
L
M

x=2

x=3

1

F

E
I

J

K

x=1

x=2

x=2

N
O

P

K

I
L

N

1

C

x=1

E

H

x=2

J

M

x=3

O

P

x=3

Since Metropolis-Hastings transitions are reversible, this one diagram shows both the transition and
its reversal. A corner of a rectangle marked by a black dot is not moved by the map; a corner labelled
by a letter is mapped to the other corner with the same label. The shaded rectangles are associated
with rejected proposals.

Implementing a Metropolis-Hastings transition in this fashion will often be feasible, since the sums in
equations (28), (29), and (32) involve only the proposal probabilities, which will often have a tractable

11

form. However, if such an implementation is still costly, an alternative is to express the proposal
distribution as a mixture of simpler proposal distributions, as follows:

S(x, ˆx) = Xδ

p(δ) Sδ(x, ˆx)

(35)

Here, p(δ) are the probabilities for some set of simpler proposal distributions, Sδ. Rather than use S
itself as a proposal distribution, we perform transition i by randomly selecting a value δi according to
p(δ) and then performing a Metropolis-Hastings transition with Sδi as the proposal distribution. (Note
that it is generally valid to choose from a set of transition probabilities randomly at each transition
of the chain, as long as each transition leaves π invariant, and the choice is made independently of
other choices.) We consider the sequence δ0, δ1, δ2, . . . to be part of the speciﬁcation of the random
volume-preserving map (along with s∗

0, s∗

1, s∗

2, . . .).

For example, a “random walk” Metropolis transition can be deﬁned when X has a group structure,

with operations ⊕ and ⊖. We can then let δ be an element of this group, and deﬁne

Sδ(x, ˆx) = 

1/2 if ˆx = x ⊕ δ
1/2 if ˆx = x ⊖ δ
0

otherwise

(36)

This is symmetric, with Sδ(x, ˆx) = Sδ(ˆx, x), so the acceptance probability of equation (26) simpliﬁes
to min[1, π(ˆx)/π(x)]. It is easy to implement equations (28) to (32) when S is set to such an Sδ. The
probabilities p(δ) will control the sizes and directions of jumps that are proposed.

4 Generalization to continuous distributions

I will begin generalizing to continuous distributions by considering a Markov chain on a state space that
is an interval of real numbers, which will be denoted as X ∗ = (a, b). I will assume that the chain leaves
invariant a distribution that has a density function proportional to π∗(x∗), and that the transitions of
the chain can be expressed as conditional density functions, with T ∗(x∗, x∗′) the conditional density
for the state x∗′ to follow the state x∗. I also assume that the total variation distance between the
transition density functions T ∗(x∗, ·) and T ∗(x∗ + ǫ, ·) goes to zero as ǫ goes to zero — that is, a slight
perturbation in the current state has only a small probability of altering what state follows — except
perhaps at a ﬁnite set of values for x∗.

A chain with a ﬁnite state space X = {0, . . . , M −1} can approximate the above chain arbitrarily
well as M goes to inﬁnity. We associate an x∗ ∈ X ∗ with each x ∈ X according to x∗ = a + (x + 1/2)h,
where h = (b−a)/M , and deﬁne the transition probabilities for the approximating chain by

T (x, x′) =

a+h(x′+1)

Za+hx′

T ∗(a + (x + 1/2)h, x∗) dx∗

(37)

This corresponds more-or-less to how simulations of Markov chains for continuous state spaces are
actually done on computers, which can represent numbers only to some ﬁnite precision.

We can deﬁne a distribution on X with probabilities proportional to π(x) = π∗(a + (x + 1/2)h). For
any ﬁnite M , the transitions of equation (37) will in general not leave this distribution exactly invariant.

12

T ∗(x∗

i , x∗) ≤ u∗

io = F −1

∗

(x∗

i , u∗
i )

x∗

i+1 = maxnx∗′ : Z x∗′
eT ∗(x∗

i + Z x∗

u∗
i+1 = s∗

a

a

i

(38)

(39)

However, we can nevertheless use π(x) to deﬁne transitions on an extended space using equations (22)
to (24). In the limit as M increases and hence h approaches zero, we can write the transition probabilities
as T (x, x′) ≈ h T ∗(a + (x + 1/2)h, a + (x′ + 1/2)h) = T ∗(x∗, x∗′). Equations (22) and (24) can then
be written as follows:

i+1, x∗) dx∗ (mod 1) = s∗

i+1, x∗

i ) (mod 1)

i + eF∗(x∗

∗

a T ∗(x, x′) dx′ gives the cumulative distribution functions for the transition
distributions, and F −1
is the inverse of F∗ with respect to its second argument (which may be deﬁned
as in equation (38) for those u that correspond to more than one x′). The chain’s reverse transition

Here, F∗(x, x′) = R x′
probabilities are denoted by eT ∗(x∗, x∗′) = T ∗(x∗′, x∗)π(x∗′)/π(x∗), and eF∗(x, x′) = R x′

gives the cumulative distribution functions for these reversed transitions.

a eT ∗(x, x′) dx′

Note that y∗ does not appear in these transition equations.

It appears in a term on the right of
equation (24), but this term disappears as h goes to zero. The update for y∗ in equation (23) becomes
undeﬁned as h goes to zero, as it depends on low-order parts of u∗ that disappear in this limit.

However, with y∗ eliminated, the transition on the space of x∗ and u∗ deﬁned by equations (38) and
(39) does not preserve volume. To ﬁx this, we need to retain the part of u∗ that disappears in the limit
as an additional part of the extended state, denoted as v∗, a value in [0, 1). We also introduce values
t∗
0, t∗
2, . . . are independently drawn from the uniform distribution on [0, 1),
and which form part of the speciﬁcation of the random one-to-one volume-preserving map that we can
view the transitions as being. We can then deﬁne the missing part of the transition as follows:

2, . . ., which like s∗

1, s∗

0, s∗

1, t∗

i+1 = π∗(x∗
y∗
v∗
i+1 = t∗

i+1) v∗

i

i + y∗

i /π∗(x∗

i ) (mod 1)

(40)

(41)

Appendix D shows that equations (38) to (41) deﬁne a map on the state space { (x∗, u∗, y∗, v∗) : x∗ ∈
(a, b), u∗ ∈ [0, 1), y∗ ∈ [0, π∗(x∗)), v∗ ∈ [0, 1)} that is one-to-one and volume preserving, with s∗
i and
t∗
i considered ﬁxed. It follows from this that the transition leaves invariant the distribution in which
x∗ has marginal probabilities given by π∗, y∗ given x∗ is uniform over [0, π∗(x∗)), and u∗ and v∗ are
independent of the other parts of the state and uniformly distributed over [0, 1). Note that it may be
more eﬃcient in a program to maintain y∗/π(x∗) rather than y∗, since equations (40) and (41) then
become simply (y∗
i )) (mod 1), eliminating a multiply, a
divide, and the need to evaluate π. In some contexts, such as parallel simulation as discussed below, it
may not be necessary to maintain y∗ and v∗ at all, since they are not needed when updating x∗ and u∗.
This map can easily be adapted to a state space of all real numbers by letting a go to −∞ and b go

i+1)) = v∗

i and v∗

i+1/π(x∗

i+1 = t∗

i + (y∗

i /π(x∗

to ∞.

The map deﬁned by equations (38) and (39) is the same as that deﬁned by Murray and Elliott
(2012) in their equations (9), (10), and (11). They do not include variables equivalent to y∗ and v∗,
however, and so do not produce a volume-preserving map. For their application, and others such as
parallel simulation with a single random number stream, this may not matter, but volume-preservation
is needed for the importance sampling application discussed below.

13

MCMC is rarely used to sample one-dimensional distributions, since other methods of sampling are
generally preferable, but the map described above can also be used to express an update of a single
real component of a multi-dimensional state. Gibbs sampling (Gelfand and Smith, 1990) is one popular
method of this sort, in which the transition updating component j of a multi-dimensional state, w, sets
it to a value drawn independently from the conditional distribution of component j given the current
values of other components. The map deﬁned by equations (38) to (41) simpliﬁes in this case, since the
transitions are reversible, and ignore the previous value of component j.

Let π(w) be proportional to the probability density for the multi-dimensional state w, and deﬁne
w |j x∗ to be state w with component j replaced by the value x∗. We can then deﬁne π∗ and F∗ for use
in the transition from state wi as follows:

π∗(x∗) = π(wi |j x∗)
F∗(x∗) = Z x∗

−∞

π∗(x∗′) dx∗′ . Z ∞

−∞

π∗(x∗′) dx∗′

The update for component j based on the map of equations (38) to (41) can then be expressed as

x∗
i+1 = F −1

∗

(u∗
i )

u∗
i+1 = s∗

i + F∗(x∗

i ) (mod 1)

y∗
i+1 = π(x∗

i+1) v∗

i

v∗
i+1 = t∗

i + y∗

i /π∗(x∗

i ) (mod 1)

(42)

(43)

(44)

(45)

(46)

(47)

The multi-dimensional state after this update is wi+1 = wi |j x∗

i+1.

The diagram below shows such a transition (when si = 0 and ti = 0):
*
i+1v

*
x
i

u

*
i

F
*

*
yi+1

*
u i+1

F
*

x

*
i+1

*
yi

v*
i

The left plot shows how, in this example, a square region of values for x∗
i+1 and u∗
rectangular region for x∗
rectangular region of values for y∗
to π(x∗

i maps to a smaller
i+1, according to the F∗ that is plotted. The right plot shows how a
i and v∗
i+1, according
i+1), represented by the slopes of the lines shown, which in this example match the
−∞π∗(x∗′) dx∗′ = 1; in general they diﬀer

slopes of the corresponding parts of F∗ (as happens when R ∞

i maps to a larger square region for y∗

i ) and π(x∗

i+1 and v∗

i and u∗

14

by a constant factor that cancels). Note that the area on the left and the area on the right change by
factors that cancel, so the transition preserves volume.

Transitions deﬁned by equations (44) to (47) can be eﬃciently implemented when the conditional
In comparison, a
cumulative distribution function, F∗, and its inverse can be computed eﬃciently.
standard implementation of Gibbs sampling will be eﬃcient whenever it is possible to eﬃciently sample
from this conditional distribution. For some distributions, such as the exponential and normal distri-
butions, computing F −1
(u) for a u drawn from the uniform distribution on [0, 1) is close to being the
most eﬃcient way of generating a random variate, in which case equation (44) has the same cost as
a standard Gibbs sampling update. The cost of doing a volume-preserving Gibbs sampling update is
then the extra time for equations (45) to (47), or only the time for equation (45) in a context, such
as discussed below for parallel simulation, where we do not actually need to evaluate equations (46)
and (47).

∗

Metropolis updates are another common way of deﬁning transitions for multi-dimensional states. The
proposal distributions used may change only a single variable, or a subset of variables, or all variables
simultaneously. If we regard such a proposal distribution as a mixture of proposal distributions Sδ, as
in equation (35), with the states proposed by each Sδ coming from a small set, then a volume-preserving
form of the transition can be obtained in the same way as described there for a discrete state space. In
particular, random-walk proposals as in equation (36) can be used, with the operation ⊕ being ordinary
scalar or vector addition, or some other group operation such as addition modulo 1.

5 Application to parallel or vectorized simulation

As a ﬁrst application of permutation MCMC, I will show how it can be used in parallel Markov chain
simulations from multiple initial states, done with a single stream of random numbers. Using a single
stream of random numbers may be faster than using separate random number streams, especially if
the parallelization takes the form of vector operations, and it also avoids the problem of ensuring that
multiple random streams are independent (see the discussion by Wu and Huang (2006)). However, if a
single random stream is used for MCMC simulation in the standard fashion, the chains will often coalesce
to the same state at some time, and thereafter track each other exactly, or they may approach each other
and then move together, with states that are similar, but not identical. This would eliminate much or all
of the beneﬁt of simulating multiple chains. Such behaviour is not possible with permutation MCMC,
as it would require multiple states to map to the same state, or for the mapping to be contractive, and
hence not preserve volume.

To illustrate parallel simulation using permutation MCMC for a discrete state, I will use an Ising
model — which has seen extensive use in statistical physics as a model of magnetization, and which is
also used in image restoration (Geman and Geman, 1984).

In the Ising model, the state variables — called “spins” in the physics application — can take the
values −1 and +1, and are arranged in a two-dimensional array with r row and c columns. The joint
distribution over the n = r×c spins, x(1), . . . , x(n), is deﬁned in terms of the following “energy” function:
(48)

x(a)x(b)

E(x) = − X(a,b) ∈ N

where N is a set of unordered pairs of spins that are “neighbors” — adjacent either horizontally or

15

vertically. A spin in the leftmost column is adjacent to the spin in the rightmost column in the same
row, and similarly for spins in the topmost and bottommost rows, so that all spins have four neighbors.

The probability of a state, x, in the Ising model is deﬁned using this energy function as follows:

P (x) = π(x) / Zβ ,

with π(x) = exp(−βE(x))

Here, β is some constant, and Zβ =Px π(x) is the normalizing factor for the un-normalized probabili-

ties, π(x).

Gibbs sampling is one commonly-used MCMC method for Ising systems. Each spin is updated in
some order, with the new value for the spin being randomly drawn from its conditional distribution
given the values for the other spins, which is as follows:

(49)

(50)

P (x(a) = +1 | x(b) for b 6= a) = h 1 + exp(cid:16)− 2β Xb s.t.

x(b)(cid:17)i−1

(a,b)∈N

Since for each update there are only two possible next states, both standard Gibbs sampling and
permutation Gibbs sampling using equations (22) to (24) are easily implemented. Appendix E contains
an R program for implementing vectorized Gibbs sampling for the Ising model, both in the standard
way, with as many random number streams as chains (all subsets of one stream in this implementation),
or in the standard way, but with the same random stream for all chains, or using a single random stream
with permutation MCMC. Note that the current value of the spin being updated is ignored, and that
each update is reversible (though the sequential combination of such updates is not), which lead to
some simpliﬁcation of the general permutation MCMC transitions of equations (22) to (24).

For this demonstration, I used a 4×5 array of spins and set β to 0.4. Figure 1 shows six Gibbs sampling
simulations (distinguished by colour) done using standard MCMC (with diﬀerent random numbers for
each simulation), coupled MCMC (same random numbers with standard method), and permutation
MCMC (same random numbers using permutation updates). All simulations were started in a state
where each spin was set independently, with equal probabilities for −1 and +1. For permutation MCMC,
initial values for u∗ and for y∗/π(x) were drawn from the uniform distribution on (0, 1).

The left plots in Figure 1 show the total energy, the right plots the magnetization (the sum of spins),

after each of 250 iterations, where each iteration consists of an update of each spin in turn.

The set of standard simulations with multiple random number streams (top) is visually indistinguish-
able from the set of permutation MCMC simulations with a single random number stream (bottom).
In contrast, the coupled chains simulated in the standard way with a single stream (middle) quickly
coalesce to the same state, and thereafter provide no more information than a single chain.

I did a larger experiment with 100 parallel chains, each running for 1000 iterations. From the runs
done using each method, I estimated the expectation of the energy, the expectation of the magnetization,
and the expectation of the absolute value of the magnetization, along with standard errors for these
estimates, computed assuming that the 100 parallel chains are independent. The results were as follows:

Standard MCMC
Coupled MCMC
Permutation MCMC
Symmetry / Long run −26.944 ± 0.020

Energy

Magnetization
−27.003 ± 0.071 +0.006 ± 0.339
−27.725 ± 0.002 +4.591 ± 0.027
−26.914 ± 0.070 −0.389 ± 0.336

0

|Magnetization|
14.769 ± 0.040
15.186 ± 0.001
14.720 ± 0.040
14.746 ± 0.012

16

Energy

Magnetization

0
2

0
1

0

0
1
−

0
2
−

0

50

100

150

200

250

0

50

100

150

200

250

Iteration

Iteration

0
2

0
1

0

0
1
−

0
2
−

0

50

100

150

200

250

0

50

100

150

200

250

Iteration

Iteration

0
2

0
1

0

0
1
−

0
2
−

0
4

0
2

0

0
2
−

0
4
−

0
4

0
2

0

0
2
−

0
4
−

0
4

0
2

0

0
2
−

0
4
−

 

C
M
C
M
d
r
a
d
n
a
S

t

 

C
M
C
M
d
e
p
u
o
C

l

C
M
C
M
n
o

 

i
t

a
u
m
r
e
P

0

50

100

150

200

250

0

50

100

150

200

250

Iteration

Iteration

Figure 1: Six Gibbs sampling simulations (in diﬀerent colours) for a 4 × 5 Ising model with β = 0.4,
using standard MCMC, coupled MCMC, and permutation MCMC. The trace plots show energy (left)
and magnetization (right) after each iteration.

17

The last line above gives more precise values. From symmetry, the true expectation of the magnetiza-
tion is zero. For the energy and the absolute value of the magnetization, a more precise estimate of
expectation was obtained by using a standard MCMC run that simulated 200 chains for 5000 iterations.
Within their standard errors, the results above from standard MCMC and permutation MCMC are
consistent with these more precise values (and with each other). The standard errors from standard
MCMC and permutation MCMC are very close. As one would expect, the results from coupled MCMC
are much less accurate, and the standard errors based on assuming the chains are independent are much
too small.

I will illustrate parallel simulation using permutation MCMC on a continuous distribution using the
bivariate normal with means of zero, standard deviations of one, and correlation 0.95, truncated to the
interval (−1, 2.5) for the ﬁrst coordinate and the interval (−1.5, 2) for the second coordinate. I tried
both Gibbs sampling and single-variable Metropolis updates, using the program in Appendix F.

Figure 2 shows Gibbs sampling simulations for this truncated normal distribution. The permutation
MCMC method for this simulation uses equations (38) and (39) only — values for y∗ and v∗ are not
needed in this application, and hence neither are equations (40) and (41).

Coupled Gibbs sampling for this distribution leads to all chains approaching each other very closely
within a few iterations. In contrast, the simulation done with permutation Gibbs sampling is visually
indistinguishable from standard Gibbs sampling with separate random number streams.

Figure 3 shows single-variable Metropolis sampling simulations for the truncated normal distribution.
In the permutation MCMC version, each coordinate update is done using equations (28) to (32), using
a random-walk proposal as in equation (36), with the same δ values used for all chains, chosen randomly
from the normal distribution with mean zero and standard deviation 4.

The coupled Metropolis simulation again shows a strong dependence between the chains, although
unlike Gibbs sampling, the states for diﬀerent chains remain far enough apart to distinguish in the plot.
The permutation Metropolis simulation with a single random number stream appears visually similar
to the standard Metropolis simulation with multiple streams.

In a larger experiment, I ran each method with 100 parallel chains, for 1000 iterations. The expec-
tations of the two coordinates, and of their squares, were estimated from iterations after the ﬁrst ten,
and standard errors for these estimates were found on the assumption that the chains are independent.
The results were as follows:

1st coord

2nd coord

1st squared

Standard Gibbs sampling
Coupled Gibbs sampling
Permutation Gibbs sampling

2nd squared
0.5909 ± 0.0067
0.5102 ± 0.0002
0.6013 ± 0.0066
0.6144 ± 0.0120
0.6883 ± 0.0089
0.6354 ± 0.0143
0.5962 ± 0.0010
The more precise estimates in the last line above are from a standard Gibbs sampling run that simulated
200 chains for 20000 iterations.

0.2274 ± 0.0083
0.1249 ± 0.0002
0.2333 ± 0.0070
0.2511 ± 0.0144
0.4259 ± 0.0074
0.2658 ± 0.0162
0.2329 ± 0.0012

0.2109 ± 0.0083
0.1011 ± 0.0002
0.2156 ± 0.0072
0.2355 ± 0.0149
0.4016 ± 0.0076
0.2468 ± 0.0164
0.2162 ± 0.0012

0.5775 ± 0.0073
0.4955 ± 0.0003
0.5874 ± 0.0072
0.5997 ± 0.0125
0.7105 ± 0.0092
0.6243 ± 0.0155
0.5821 ± 0.0011

Standard Metropolis
Coupled Metropolis
Permutation Metropolis

Long simulation run

18

Plot of x1 vs. x2

Trace plot of x1

−1.0

−0.5

0.0

0.5

1.0

1.5

2.0

2.5

x1

−1.0

−0.5

0.0

0.5

1.0

1.5

2.0

2.5

x1

0
2

.

5
1

.

0
1

.

5
0

.

0
0

.

5

.

0
−

.

0
1
−

5

.

1
−

0
.
2

5
.
1

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

5
.
1
−

0
.
2

5
.
1

.

0
1

5

.

0

0

.

0

5

.

0
−

0

.

1
−

5

.

1
−

1
x

1
x

1
x

5
2

.

0
2

.

5
1

.

0
1

.

5
0

.

0

.

0

.

5
0
−

0

.

1
−

5
.
2

0
.
2

5
.
1

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

5
.
2

0
.
2

.

5
1

0

.

1

5

.

0

0

.

0

5

.

0
−

0

.

1
−

0

50

100

150

200

250

Iteration

0

50

100

150

200

250

Iteration

g
n

i
l

2
x

i

p
m
a
S
 
s
b
b
G
d
r
a
d
n
a
S

t

 

g
n

i
l

2
x

i

p
m
a
S
 
s
b
b
G
d
e
p
u
o
C

 

l

g
n

i
l

p
m
a
S
 
s
b
b
G
n
o

 

i

2
x

i
t

a

t

u
m
r
e
P

−1.0

−0.5

0.0

0.5

1.0

1.5

2.0

2.5

0

50

100

150

200

250

x1

Iteration

Figure 2: Six Gibbs sampling simulations for a truncated bivariate normal distribution, using standard
MCMC, coupled MCMC, and permutation MCMC.

19

Plot of x1 vs. x2

Trace plot of x1

−1.0

−0.5

0.0

0.5

1.0

1.5

2.0

2.5

x1

−1.0

−0.5

0.0

0.5

1.0

1.5

2.0

2.5

x1

0
2

.

5
1

.

0
1

.

5
0

.

0
0

.

5

.

0
−

.

0
1
−

5

.

1
−

0
.
2

5
.
1

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

5
.
1
−

0
.
2

5
.
1

.

0
1

5

.

0

0

.

0

5

.

0
−

0

.

1
−

5

.

1
−

1
x

1
x

1
x

5
2

.

0
2

.

5
1

.

0
1

.

5
0

.

0

.

0

.

5
0
−

0

.

1
−

5
.
2

0
.
2

5
.
1

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

5
.
2

0
.
2

.

5
1

0

.

1

5

.

0

0

.

0

5

.

0
−

0

.

1
−

0

50

100

150

200

250

Iteration

0

50

100

150

200

250

Iteration

s

i
l

o
p
o
r
t

2
x

 

e
M
d
r
a
d
n
a
S

t

s

i
l

o
p
o
r
t

2
x

 

e
M
d
e
p
u
o
C

l

s

i
l

o
p
o
r
t

e
M
n
o

 

2
x

i
t

a

t

u
m
r
e
P

−1.0

−0.5

0.0

0.5

1.0

1.5

2.0

2.5

0

50

100

150

200

250

x1

Iteration

Figure 3: Six single-variable Metropolis simulations for a truncated bivariate normal distribution, using
standard MCMC, coupled MCMC, and permutation MCMC.

20

From this table, we see that, as for the Ising model, coupled MCMC does not work well for a parallel
simulation with a single stream, but the accuracy of permutation Gibbs sampling and permutation
Metropolis with a single random stream is close to that of standard simulation with multiple random
streams. (In the table, there appears to be a slight advantage for permutation Gibbs sampling, and
a slight disadvantage for permutation Metropolis, but neither is seen consistently in replications with
diﬀerent random seeds.)

Will parallel simulation using permutation MCMC with a single random number stream always
behave similarly to standard MCMC with multiple streams? For simplicity, consider the case of just two
chains. With standard MCMC with two random streams, the states of these chains will be independent
(assuming independent initial states). The map deﬁned by permutation MCMC on its extended state
space (consisting of x or x∗, u or u∗, and possible y or y∗, etc.) will be a permutation (or a volume-
preserving map). The map on the joint space for two chains will also be a permutation (or will be
volume-preserving), and hence will leave the uniform distribution on the pair of (extended) states for
the two chains invariant. In this uniform joint distribution, the states of the two chains are independent.

However, this does not imply that parallel simulation with permutation MCMC will produce chains
with independent states, because the map on the joint space produced by permutation MCMC may
not be ergodic. Indeed, if the extended state space is ﬁnite (as in Section 2), it cannot be ergodic —
if the two chains are in the same state, they will remain in the same state, so there are at least two
disconnected regions of the joint state space that both have non-zero probability.

This particular non-ergodicity will actually make permutation MCMC perform better than standard
MCMC, provided the initial states of the chains are chosen to be diﬀerent, since it will induce negative
correlations that improve the accuracy of estimates (though the eﬀect is probably negligible in real
problems with huge state spaces). However, one may wonder whether there might sometimes be other
non-ergodic aspects of the permutation MCMC map as applied to multiple states, which might lead to
worse performance compared to standard parallel MCMC. Of course, as long as the MCMC method is
ergodic for one chain, the results found using permutation MCMC will be valid — only the degree of
improvement in accuracy from using multiple chains is at issue.

6 Application to importance sampling

Another application of permutation MCMC is to improving a deﬁcient, but not too deﬁcient, importance
sampling distribution.

Recall that the expectation of some function a(x) with respect to some distribution of interest, with
probability density (or mass) function proportional to π(x), can be estimated from points x1, . . . , xN
drawn independently from some importance sampling distribution, whose probability density (or mass)
function is proportional to ρ(x), as follows:

Eπ[a(x)] ≈

π(xi)/ρ(xi)

(51)

Methods for assessing the accuracy of such estimates are discussed by Geweke (1989) and Neal (2001,
Section 3).

a(xi)π(xi)/ρ(xi)

NPi=1
NPi=1

21

Suppose that we have found some tractable importance sampling distribution — for which it is
feasible to sample independent points, and to compute ρ(x) for these sampled points — but that this
importance sampling distribution gives very low probability to some region that has non-negligible
probability under the distribution of interest. Such an importance sampling distribution will not work
well, since it provides little or no information about this region, even though it cannot be ignored.

I will demonstrate here that permutation MCMC can be used to improve such a deﬁcient importance
sampling distribution, as long as there is substantial overlap between the original importance sampling
distribution and the distribution of interest (that is, their total variation distance is not close to one).

The improved importance sampling distribution is deﬁned as the mixture of distributions obtained by
sampling from the original importance sampling distribution and then applying a permutation MCMC
map for some number of iterations between 0 to M , chosen randomly, here with equal probabilities
(though unequal probabilities could be used). Since the permutation map is on an extended state
space, the value for x or x∗ sampled from the importance sampling distribution must be supplemented
with a value for u or u∗, plus for non-uniform distributions, a value for y or y∗, and for continuous
transitions, a value for v∗. These additional parts of the extended state are drawn uniformly from their
allowed range, which is [0, 1) except for y or y∗, whose range is [0, π(x)) or [0, π(x∗)). These additional
variables are drawn independently of each other and of x or x∗ (except for the dependence of the range
of y or y∗ on x or x∗).

Before drawing points from the improved importance sampling distribution, we must ﬁx partic-
ular permutation MCMC maps, for up to M iterations, by sampling s0, s1, . . . , sM −1, as well as
t0, t1, . . . , tM −1 and δ0, δ1, . . . , δM −1 if required. We can then sample points as follows:

1) Sample k uniformly from {0, . . . , M}.
2) Sample xk or x∗

k according to ρ.

3) Sample other variables in the extended state (eg, u∗

k) uniformly from their allowed range.

4) Simulate M − k permutation MCMC transitions, using sk, . . . , sM −1 (and also tk, . . . , tM −1 and
δk, . . . , δM −1 if required), producing extended states indexed by k + 1, . . . , M . (If k = M , no
transitions are simulated.)

5) Let the extended state indexed by M be the point drawn from the improved importance sampling

distribution.

Supposing that it is feasible to compute ρ(x), the probability density (or mass) of the point sampled
above under the improved improved importance sampling distribution can also be computed, with the
following additional steps:

6) Do a reverse permutation MCMC simulation for k iterations, starting with the originally sampled
xk or x∗
k, along with the other variables in the extended state indexed by k, using sk−1, . . . , s0 (and
also tk−1, . . . , t0 and δk−1, . . . , δ0 if required), producing extended states indexed by k−1, . . . , 0.
(If k = 0, no reverse transitions are simulated.)

7) For j = 0, . . . , M , compute ρ(xj) or ρ(x∗

j ), and then, for a uniform distribution, compute

¨ρ =

1

M +1

MXj=0

ρ(xj)

22

(52)

and for a non-uniform distribution, compute

or similarly for a continuous state, x∗.

¨ρ =

1

M +1

ρ(xj)
π(xj)

MXj=0

(53)

The extended states indexed by 0, . . . , M are the possible start states that map to the state indexed
by M , when between M and 0 permutation MCMC iterations are done. Since each map is a permutation,
or preserves volume, the probability of obtaining the extended state indexed by M with some number
of iterations is simply the probability of sampling the xj in the corresponding start state from the
original importance sampling distribution, times the probability of sampling the additional variables in
the extended state.

The total (possibly unnormalized) probability mass or density for obtaining the sampled point from
the improved importance sampling distribution is ¨ρ, which simply adds the probabilities of obtaining
the point from all possible start states. The division by π(xj) in equation (53) results from the range
of y∗

j being [0, π(xj )), so that the density of y∗

j is 1/π(xj).

If we perform the above procedure N times, we will obtain N points from step (5), which we can
label xi for i = 1, . . . , N , along with corresponding quantities ¨ρi from step (7). The desired distribution
on the extended state is uniform (π being accounted for in the range of y∗), so the improved importance
sampling estimate for the expectation of a(x) is

Eπ[a(x)] ≈

1/¨ρi

(54)

a(xi)/¨ρi

NPi=1
NPi=1

When M = 0, this reduces to the original importance sampling estimate of equation (51).

I will demonstrate how this method can sometimes improve an importance sampling distribution using
a two-dimensional test distribution, in which the ﬁrst coordinate, x(1), is normally-distributed with mean
zero and standard deviation one, and given a value for the ﬁrst coordinate, the second coordinate, x(2),
is normally-distributed with mean [x(1)]2 − 1 and standard deviation one.
I will consider using the
method to improve on four importance sampling distributions, all of which are bivariate normal with
zero correlation and equal standard deviations for the two coordinates, but which diﬀer in the means
for the two coordinates and in the standard deviation of the coordinates. Figure 4 shows samples of
500 points from the four importance sampling distributions (in red), each shown along with a sample
of points from the test distribution (in black).

For MCMC sampling from this test distribution, I use random-walk Metropolis transitions that
update both coordinates simultaneously, with the proposal oﬀset drawn from the bivariate normal
distribution with zero correlation, zero mean, and standard deviation of 4 for both coordinates. The
permutation MCMC implementation of this transition is done by ﬁxing δ0, δ1, . . . , δM −1 to values drawn
from this proposal oﬀset distribution, and using them as described at the end of Section 3. The program
for this permutation MCMC method and the importance sampling procedure above that uses it is shown
in Appendix G.

Figure 5 shows distributions obtained by drawing points from two of the importance sampling dis-
tributions and then applying between zero and 10 iterations of this permutation MCMC map. The

23

mean 1, 0 & std dev 0.3, 0.3

mean 1, 0 & std dev 1, 1

mean 1, 0 & std dev 3, 3

mean 0, 4 & std dev 0.3, 0.3

6

4

2

0

2
−

6

4

2

0

2
−

6

4

2

0

2
−

6

4

2

0

2
−

−2

0

2

4

−2

0

2

4

−2

0

2

4

−2

0

2

4

Figure 4: Four initial importance sampling distributions for the test distribution. Each plot shows
500 black dots from the test distribution and 500 red dots from initial independent normal importance
sampling distributions with means and standard deviations as shown.

mean (1,0), sd 0.3, seed 1

mean (1,0), sd 0.3, seed 2

mean (1,0), sd 0.3, seed 3

mean (1,0), sd 0.3, seed 4

6

4

2

0

2
−

6

4

2

0

2
−

6

4

2

0

2
−

6

4

2

0

2
−

6

4

2

0

2
−

−2

0

2

4

−2

0

2

4

−2

0

2

4

−2

0

2

4

mean (0,4), sd 0.3, seed 1

mean (0,4), sd 0.3, seed 2

mean (0,4), sd 0.3, seed 3

mean (0,4), sd 0.3, seed 4

6

4

2

0

2
−

6

4

2

0

2
−

6

4

2

0

2
−

−2

0

2

4

−2

0

2

4

−2

0

2

4

−2

0

2

4

Figure 5: Importance sampling distributions obtained by applying from 0 to 10 permutation MCMC
iterations starting from the leftmost and rightmost importance sampling distributions in Figure 4. The
four plots in each row are for diﬀerent random selections of s0, . . . , s9 and δ0, . . . , δ9.

24

top plots are for an importance sampling distribution (leftmost in Figure 4) that is quite concentrated
(standard deviation 0.3) and located in an area of the test distribution of fairly high probability. The
bottom plots are for an importance sampling distribution (rightmost in Figure 4) with the same stan-
dard deviation, but located in an area of very low probability under the test distribution. The four plots
in each row show the importance sampling distributions obtained with four diﬀerent random selections
of s0, . . . , s9 and δ0, . . . , δ9.

As can be seen in these plots, up to 10 iterations is not enough for the points sampled from these
overly-concentrated importance sampling distributions to be transformed to have a distribution that
is close to the test distribution. As the number of iterations is increased, the distribution for x(1)
and x(2) will approach the test distribution, but this alone is not enough to ensure that importance
sampling will work well — the distribution of the entire extended state must be suitable. One way to
visualize the adequacy of the importance sampling is to imagine drawing values for x(1) and x(2) from
the test distribution, along with a value for u∗ uniformly from [0, 1) and a value for y∗ uniformly from
[0, π(x(1), x(2))), and then applying the reverse permutation MCMC map for M iterations from this
state. For the improved importance sampler to be adequate, it must almost always be the case that at
least one of the states from the reverse simulation has values for x(1) and x(2) that have reasonably high
probability under the original importance sampling distribution.

Figure 6 shows results on the test distribution using the four original importance sampling distribu-
tions of Figure 4, focusing on estimating the expectation of [x(2)]2, whose true value is exactly 3. For
each original distribution, four values of M were tried, with M = 0 being equivalent to just using the
original importance sampling distribution. For each value of M , results using three random seeds are

mean 1, 0 & std dev 0.3, 0.3

mean 1, 0 & std dev 1, 1

mean 1, 0 & std dev 3, 3

mean 0, 4 & std dev 0.3, 0.3

0
1

8

6

4

2

0

0
1

8

6

4

2

0

0
1

8

6

4

2

0

0
1

8

6

4

2

0

0

20

100 500

0

20

100 500

0

20

100 500

0

20

100 500

Figure 6: Estimates for the expectation of [x(2)]2 from improved importance sampling using the four
original importance sampling distributions shown in Figure 4, and with M set to 0 (equivalent to the
original distribution), 20, 100, and 500. Estimates with N = 2000 are shown by dots, with vertical lines
extending up and down by twice the standard error. For each value of M , estimates from three random
settings of s0, . . . , s9 and δ0, . . . , δ9 are shown. The true expection of 3 is marked with a dotted line.

25

shown, which diﬀer both in the points sampled from the original importance sampling distribution and
in the random choice of s0, . . . , sM −1 and δ0, . . . , δM −1. The estimates were based on N = 2000 points.
Standard errors for the estimates were calculated with the procedure at the end of Appendix G.

From the results with M = 0, we see that of the original importance sampling distributions, only
the third, with standard deviations of 3, produces adequate results. The more concentrated importance
sampling distributions give very low probability to some high-probability regions of the test distribution,
with the result that the estimate obtained is far from the truth, and moreover the estimated standard
error is much too small.

The ﬁrst and second importance sampling distributions are improved by applying permutation
MCMC. For the ﬁrst importance sampling distribution, using M = 20 produces estimates that are
rather inaccurate, but at least the standard errors properly reﬂect this. Good estimates are obtained
when M = 100 or M = 500. For the second importance sampling distribution, even M = 20 gives rea-
sonably good estimates, and realistic standard errors, and the estimates improve (with some variation)
with larger M . The improvement seen is as expected based on the discussion above, since 20 or more
Metropolis iterations started at a point drawn randomly from the test distribution are indeed likely
to produce at least one state from the region with high probability under either of these two original
importance sampling distributions, since they have substantial overlap with the test distribution.

In contrast, for the fourth importance sampling distribution, only a slight improvement is seen from
using permutation MCMC. Even with M = 500, the estimates are very inaccurate, though perhaps the
standard errors are more realistic than for M = 0. This is also as expected given the discussion above
— the fourth importance sampling distribution is concentrated almost entirely in a low-probability area
of the test distribution, so even 500 Metropolis iterations from a point randomly sampled from the
test distribution are likely to all remain in regions with low probability under the original importance
sampling distribution.

Permutation MCMC also produces only a slight improvement for the the third importance sampling
distribution. This distribution is diﬀuse enough to cover the entire test distribution. Such a simple
diﬀuse importance sampling distribution will usually be ineﬀective for a high-dimensional problem,
because too many points drawn from it will land in regions of very low probability under the distribution
of interest. But for this two-dimensional test problem the eﬀective sample size is reduced by only a factor
about 5.5, so acceptable results are obtained. We might hope, nevertheless, that applying permutation
MCMC would improve performance, but even for M = 500, the ineﬃciency factor is reduced only to
about 3.0.

This might seem puzzling, since a point drawn from this diﬀuse importance sampling distribution is
likely to map to a point that has high probability density under the test distribution within a fairly small
number of Metropolis updates. However, points derived from original points with low density under the
test distribution have very high density under the improved importance sampling distribution, because
the value of y∗ for such points is drawn from the narrow range [0, π(x(1), x(2))). Such points therefore
receive a very low weight in the estimate of equation (54), reducing the eﬀective sample size. All sampled
points will receive nearly equal weight only when M is so large that M reverse transitions started from
a point drawn randomly drawn from the test distribution are likely to contain several points with low
density under the test distribution, comparable to the low density of many points drawn from the
original importance sampling distribution.

26

This method for improving importance samplers using permutation MCMC could itself be improved
in several respects. First, once random values for s0, . . . , sM −1 and δ0, . . . , δM −1 have been chosen, N
points from the improved importance sampling distribution could be simulated in parallel. This is most
easily done if the random choice of k from {0, . . . , M} is replaced by stratiﬁed sampling (as is desirable
in any case), so that exactly N/(M +1) points are generated with each value of k. The simulations for
each value of k could then be done with the vectorized permutation MCMC procedure.

Secondly, the procedure described above actually produces results from two improved importance
sampling distributions, only one of which is used above. The second distribution would use the state
indexed by 0 after step (7) of the procedure above. More generally, several importance sampling
distributions that use overlapping sequences s0, . . . and δ0, . . . could be deﬁned, and points drawn from
all these distributions could be obtained with much of the computation being in common. The results
from all these importance sampling distributions could then be combined.

Even with these improvements, the method described above will be restricted to improving impor-
tance sampling distributions that already have substantial overlap with the desired distribution. For
complex, high-dimensional problems, ﬁnding such a distribution may be diﬃcult. However, I expect
that permutation MCMC can be used in an annealing framework, as I have previously developed for
Hamiltonian importance sampling (Neal, 2005), and applied to a wide range of complex distributions,
including those with multiple isolated modes. As for any importance sampling procedure, it would also
be possible to estimate the ratio of the normalizing constants for the importance sampling distribu-
tion and the distribution of interest. These ratios are of great interest in both statistical physics and
Bayesian inference.

7 The role of randomness in MCMC simulation

The initial work on Markov chain Monte Carlo by Metropolis, et al (1953), who used it to sample from
the “canonical” distribution for a system of molecules, was soon followed by work on an alternative
deterministic approach to molecular simulation (Alder and Wainwright, 1959). In the context of sta-
tistical physics, these stochastic and deterministic approaches to simulation become equivalent in the
limit as the size of the system increases. It is also possible to extend the state space of a deterministic
dynamical simulation so that even for small systems it visits states according to the same distribution
that a stochastic simulation would (Nos´e, 1984).

Permutation MCMC provides another way of reducing, or even eliminating, the role of randomness

in MCMC simulation.

In a standard MCMC simulation, the initial states of the chains may be chosen randomly (though
usually not from the desired distribution), or by using some non-random heuristic (such as starting at
the mode), or more-or-less arbitrarily. Subsequent transitions are always random (in practice, pseudo-
random) rather than deterministic. As long as the transitions are ergodic, the choice of initial states
aﬀects only the number of iterations needed to reach a good approximation of the equilibrium distri-
bution, not the asymptotic results from averaging functions of state over subsequent iterations of the
chains.

With permutation MCMC, any choice of s0, s1, . . . (along with any other variables that deﬁne the
permutation or volume-preserving map) will leave the desired distribution invariant — as has also been

27

noted by Murray and Elliott (2012) for their related MCMC method. It follows that if a set of initial
states are drawn from the desired distribution (which will be uniform, if we extend the state as in this
paper), estimates based on the sequences of states found by applying permutation MCMC from these
initial states will be unbiased even if s0, s1, . . . are non-random.

In most applications, however, we are not able to choose initial states from the desired distribution.
If we use some other initial state distribution, correct results will be obtained by averaging over the
sequence of states obtained using permutation MCMC only if the particular sequence s0, s1, . . . that
we use has a suitable ergodic property. Note that if s0, s1, . . . is considered non-random, it will not
be the case that the distribution of the state at time t will converge to the desired state as t goes to
inﬁnity — these state distributions will remain just as non-uniform as the initial state distribution.
Instead, we can hope that time averages of functions of state will converge to their correct expectations.
Characterizing when this will occur is an interesting topic for further research, which may relate to
work on quasi-Monte Carlo methods for MCMC (eg, Chen, Dick, and Owen, 2011).

Some empirical insight into this question can be obtained using the permutation MCMC procedures

for the Ising model and the truncated normal distribution that were used in Section 5.

Here are the results of 100 chains, run for 1000 iterations each, using permutation Gibbs sampling

for the Ising model:

With s set randomly

Energy

Magnetization
−26.914 ± 0.070 −0.389 ± 0.336
−24.201 ± 0.041 +0.127 ± 0.249
With s repeating 0.2, 0.6
−25.512 ± 0.048 −0.025 ± 0.268
With s repeating 0.3
With s repeating 0.213, 0.631 −26.876 ± 0.061 +0.055 ± 0.303
−26.893 ± 0.054 −0.266 ± 0.271
With s repeating 0.292
−26.877 ± 0.050 −0.069 ± 0.270
With s repeating 0.237
−26.944 ± 0.020

Symmetry / Long run

0

|Magnetization|
14.720 ± 0.040
13.633 ± 0.027
14.178 ± 0.027
14.705 ± 0.035
14.721 ± 0.032
14.733 ± 0.029
14.746 ± 0.012

The ﬁrst line above is the same as was reported for permutation MCMC in Section 5. The other lines
are for runs in which s0, s1, . . . are set deterministically, by repeating a pair of values, or a single value.
We see that incorrect results are obtained when repeating the pair 0.2, 0.6, or the single value 0.3, but
results are consistent with the answer from symmetry or a long run when repeating the pair 0.213,
0.631, or the single values 0.292 or 0.237. This diﬀerence is perhaps related to 0.2, 0.3, and 0.6 being
rational with small numerator and denominator, but further research is needed to understand these
results.

Figure 7 shows six runs of permutation Gibbs sampling on the truncated normal distribution used in
Section 5. The top plot is for permutation MCMC with s0, s1, . . . set randomly, the same as the bottom
plot of Figure 2. The middle plot is for s0, s1, . . . alternately set to 0.231 and 0.452. In the bottom
plot, all the si are set to 0.211. Visually, the two deterministic sequences appear to produce correct
results. Figure 8 shows more results with all sj set to a single value — to zero for the top plot, to 0.017
for the middle plot, and to 0.211 for the bottom plot (as for Figure 7). Setting all sj to zero produces
interesting results, but it clearly does not result in the correct distribution. Setting all sj to 0.017 does
appear to produce correct results.

28

Plot of x1 vs. x2

Trace plot of x1

−1.0

−0.5

0.0

0.5

1.0

1.5

2.0

2.5

x1

−1.0

−0.5

0.0

0.5

1.0

1.5

2.0

2.5

x1

0
2

.

5
1

.

0
1

.

5
0

.

0
0

.

5

.

0
−

.

0
1
−

5

.

1
−

0
.
2

5
.
1

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

5
.
1
−

0
.
2

5
.
1

.

0
1

5

.

0

0

.

0

5

.

0
−

0

.

1
−

5

.

1
−

1
x

1
x

1
x

5
2

.

0
2

.

5
1

.

0
1

.

5
0

.

0

.

0

.

5
0
−

0

.

1
−

5
.
2

0
.
2

5
.
1

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

5
.
2

0
.
2

.

5
1

0

.

1

5

.

0

0

.

0

5

.

0
−

0

.

1
−

0

50

100

150

200

250

Iteration

0

50

100

150

200

250

Iteration

m
a
e
r
t
s
 
m
o
d
n
a
R

 
,

g
n

2
x

i
l

p
m
a
S
 
s
b
b
G

i

2
5
4
0

.

 
,

1
3
2

.

0

 

g
n

i
t

a
e
p
e
R

2
x

 
,

g
n

i
l

p
m
a
S
 
s
b
b
G

i

1
1
2

.

0

 

g
n

i
t

a
e
p
e
R

 
,

2
x

g
n

i
l

p
m
a
S
 
s
b
b
G

i

−1.0

−0.5

0.0

0.5

1.0

1.5

2.0

2.5

0

50

100

150

200

250

x1

Iteration

Figure 7: Six permutation Gibbs sampling simulations for a truncated bivariate normal distribution,
with s set randomly (as in the bottom plot of Figure 2), set to alternating values of 0.231 and 0.452,
and set always to 0.211.

29

Plot of x1 vs. x2

Trace plot of x1

−1.0

−0.5

0.0

0.5

1.0

1.5

2.0

2.5

x1

−1.0

−0.5

0.0

0.5

1.0

1.5

2.0

2.5

x1

0
2

.

5
1

.

0
1

.

5
0

.

0
0

.

.

5
0
−

.

0
1
−

5

.

1
−

0
.
2

5
.
1

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

5
.
1
−

0
.
2

5
.
1

0
1

.

5

.

0

0

.

0

5

.

0
−

0

.

1
−

5

.

1
−

1
x

1
x

1
x

5
2

.

0
2

.

5
1

.

0
1

.

5
0

.

0

.

0

.

5
0
−

0

.

1
−

5
.
2

0
.
2

5
.
1

0
.
1

5
.
0

0
.
0

5
.
0
−

0
.
1
−

5
.
2

0
.
2

5
1

.

0

.

1

5

.

0

0

.

0

5

.

0
−

0

.

1
−

0

50

100

150

200

250

Iteration

0

50

100

150

200

250

Iteration

 

0
=
 
s
 

g
n

i
t

a
e
p
e
R

 
,

g
n

2
x

i
l

p
m
a
S
 
s
b
b
G

i

7
1
0

.

0

 

g
n

i
t

a
e
p
e
R

 
,

2
x

g
n

i
l

p
m
a
S
 
s
b
b
G

i

1
1
2
0

.

 

g
n

i
t

a
e
p
e
R

 
,

2
x

g
n

i
l

p
m
a
S
 
s
b
b
G

i

−1.0

−0.5

0.0

0.5

1.0

1.5

2.0

2.5

0

50

100

150

200

250

x1

Iteration

Figure 8: Six permutation Gibbs sampling simulations for a truncated bivariate normal distribution,
with s set always to 0, to 0.017, and to 0.211 (as in the bottom plot of Figure 7).

30

To conﬁrm these visual impressions, I did 100 runs of 1000 iterations with these deterministic settings

for s0, s1, . . .. The estimates obtained, along with their standard errors, are shown below:

1st coord

2nd coord

1st squared

With s set randomly

Long simulation run

0.2156 ± 0.0072
0.2245 ± 0.0122
0.2216 ± 0.0023
0.2087 ± 0.0052
0.4001 ± 0.0770
0.2162 ± 0.0012

0.2333 ± 0.0070
0.2408 ± 0.0119
0.2388 ± 0.0023
0.2267 ± 0.0055
0.4431 ± 0.0742
0.2329 ± 0.0012

With s repeating 0.231, 0.452
With s always 0.211
With s always 0.017
With s always 0

2nd squared
0.6013 ± 0.0066
0.5944 ± 0.0090
0.6018 ± 0.0045
0.6110 ± 0.0109
1.1092 ± 0.0828
0.5962 ± 0.0010
All the runs except the one with s always set to zero produce results that are consistent with the long
simulation run. However, the runs vary in eﬃciency, as seen from the standard errors of the estimates.
Estimates with s repeating 0.231, 0.452 are less eﬃcient than when s is set randomly. When s is ﬁxed at
0.211, however, the standard errors for the two coordinates are a factor of three smaller (corresponding
to a factor of nine better eﬃciency), and the standard errors for the squares of the coordinates are a
factor of 1.5 smaller (a factor of 2.25 more eﬃcient). When s is ﬁxed at 0.017, the standard errors for
the coordinates are smaller than for random s, but the standard errors for the squares of the coordinates
are larger.

0.5874 ± 0.0072
0.5792 ± 0.0094
0.5893 ± 0.0046
0.5969 ± 0.0112
1.0609 ± 0.0974
0.5821 ± 0.0011

The source of the greater eﬃciency when s is ﬁxed at 0.017 or 0.211 can be seen from an examination
of the middle and bottom plots in Figure 8. In the middle plot, for s = 0.017, the two coordinates
move upward in a slow but consistent fashion for many iterations, and then quickly move downward,
before resuming their upward motion. The same behaviour is seen in the bottom plot, for s = 0.211,
except that the upward motion is quicker, making the pattern less obvious. Such consistent upward
motion moves around the distribution more eﬃciently than the random walk motion characteristic of
standard Gibbs sampling — n steps that are taken consistently in one direction lead to a point about
n steps away from the start point, whereas n steps taken in a random walk lead to point that is likely
only about √n steps away. In this respect the behaviour with s ﬁxed to a small value resembles that
of “overrelaxation” methods (Adler, 1981; Neal, 1998), though the dynamics of overrelaxation is quite
diﬀerent.

Exploring permutation MCMC methods with non-random values for s0, s1, . . . (and δ0, δ1, . . . where
needed) seems to be a promising avenue to ﬁnding more eﬃcient MCMC methods, but better theory
regarding when such non-random methods will be ergodic would be helpful. However, as noted by
Murray and Elliott (2012), one can ensure ergodicity (assuming the chain is ergodic when simulated in
the standard manner) by combining non-random selections of sj with occasional sequences of randomly
chosen sj that are long enough to move anywhere in the state space.

Acknowledgements

This research was supported by Natural Sciences and Engineering Research Council of Canada. The
author holds a Canada Research Chair in Statistics and Machine Learning.

31

Appendix A: Proof that the map for a simulation of a uniform

discrete distribution is a permutation

To show that the map (xi, ui) → (xi+1, ui+1) deﬁned by equations (2) and (3) is a permutation, it
suﬃces to show that the following map is its inverse:

x†

i = maxnx′ : Q

u†
i = (ui+1 − si) − Q

x′−1Xx=0 eT (xi+1, x) ≤ ui+1 − si (mod Q)o
i −1Xx=0eT (xi+1, x) + Q

xi+1−1Xx=0

T (x†

x†

i , x) (mod Q)

That is, we need to show that equations (2), (3), (55), and (56) imply that x†
Equation (2) implies that

i = xi and u†

i = ui.

(55)

(56)

(57)

and hence

0 ≤ Q

0 ≤ ui − Q

xi+1−1Xx=0
T (xi, x) < Q T (xi, xi+1) = QeT (xi+1, xi)
xi+1−1Xx=0
xi−1Xx=0 eT (xi+1, x) < Q
xi−1Xx=0 eT (xi+1, x) ≤ ui − Q
xi+1−1Xx=0

ui+1 − si (mod Q) = ui − Q

T (xi, x) + Q

T (xi, x) + Q

It follows from this and equation (3) that

Hence we can rewrite equation (58) as

0 ≤ Q

xi−1Xx=0 eT (xi+1, x) ≤ ui+1 − si+1 (mod Q) < Q

which, from equation (55), implies that x†
replace ui+1 with the right side of equation (3), leading to the conclusion that u†

i = xi. In equation (56), we can now replace x†

i = ui.

xiXx=0eT (xi+1, x) ≤ Q (58)
xi−1Xx=0 eT (xi+1, x)
xiXx=0eT (xi+1, x)

(60)

(59)

i with xi and

Appendix B: Proof that the map for a simulation of a discrete
distribution is one-to-one and volume preserving

i ) to (xi+1, y∗
We can show that the map from (xi, y∗
one-to-one by seeing that the following map is its inverse:

i , u∗

i+1, u∗

i+1) given by equations (22) to (24) is

x†

i = π(x†
y∗†

i = maxnx′ :
i )(cid:16)(u∗
xi+1−1Xx=0

u∗†
i =

x′−1Xx=0 eT (xi+1, x) ≤ u∗

i+1 − s∗

i (mod 1)o

i+1 − s∗

i (mod 1)) −

x†

i −1Xx=0eT (xi+1, x)(cid:17).eT (xi+1, x†

i )

T (x†

i , x) + T (x†

i , xi+1)

(mod 1)

y∗
i+1

π(xi+1)

32

(61)

(62)

(63)

That is, we will show that x†
in [0, π(xi)) we see that

i = xi, y∗†

i = yi, and u∗†

i = u∗

i . From equation (24) and the fact that y∗

i is

since the right side will be in the interval [0, 1). Substituting this in equation (61), we get

u∗
i+1 − s∗

i

(mod 1) =

x†

i = maxn x′ :

x′−1Xx=0 eT (xi+1, x) ≤

y∗
i

π(xi)

xi−1Xx=0 eT (xi+1, x) + eT (xi+1, xi)
xi−1Xx=0 eT (xi+1, x) + eT (xi+1, xi)

y∗
i

π(xi)o = xi

(64)

(65)

i = u∗
i .

i with xi in equations (62) and (63), and using equations (64) and (23), we ﬁnd that

After replacing x†
y∗†
i = yi and u∗†
To see that volume is preserved by the map deﬁned by equations (22) to (24) we can look at the
i+1), over a region where

Jacobian matrix for the continuous part of this map, from (y∗
xi+1 is constant:

i ) to (y∗

i+1, u∗

i , u∗

∂y∗
i+1
∂y∗
i
∂y∗
i+1
∂u∗
i

∂u∗
i+1
∂y∗
i
∂u∗
i+1
∂u∗
i





= 

0

π(xi+1)

T (xi, xi+1)

eT (xi+1, xi)

π(xi)

0



= 

0

T (xi, xi+1)

π(xi+1)

π(xi+1)

T (xi, xi+1)

0



(66)

The absolute value of the determinant of this Jacobian matrix is one, so the map preserves volume.

Appendix C: Proof that the map for a Metropolis-Hastings

simulation of a discrete distribution is one-to-one
and volume preserving

The map from (xi, y∗
s∗
i = 0, and in general its inverse is given by

i ) to (xi+1, y∗

i+1, u∗

i , u∗

i+1) deﬁned by equations (28) to (32) is its own inverse when

S(xi+1, x) ≤ u∗

i

(mod 1)o
i+1− s∗
S(xi+1, x)(cid:17). S(xi+1, ˆx†

i )

ˆx†

i −1Xx=0

(mod 1)) −

i < a(xi+1, ˆx†
i )

if a†
otherwise

i / a(xi+1, ˆx†
i )

i < a(xi+1, ˆx†
i )

if a†
otherwise

x†

ˆx†

a†

i

i

i = maxnx′ :
x′−1Xx=0
i = (cid:16)(u∗
i+1− s∗
i = ( ˆx†
i = ( π(x†
xi+1−1Xx=0

i ) a†

S(x†

y∗
i+1

xi+1

i = 

u∗
i+1− s∗

i

y∗†

u∗†

i , x) + S(x†

i , xi+1) a(x†

i , xi+1)

y∗
i+1

π(xi+1)

(mod 1)

if a†

i < a(xi+1, ˆx†
i )

(mod 1)

otherwise

33

(67)

(68)

(69)

(70)

(71)

We can show that x†
i = xi, y∗†
I will do this here assuming s∗
one-to-one and volume-preserving.

i , and u∗†

i = y∗
i = 0; the general case follows from this, since adding s∗

i , separately for rejected and accepted proposals.
is itself

i = u∗

i to u∗

i

If the proposal ˆxi+1 is rejected — that is, if ai+1 ≥ a(xi, ˆxi+1) — then x∗
i+1 = u∗
u∗
If the proposal ˆxi+1 is accepted, when ai+1 < a(xi, ˆxi+1), one can see that the way that u∗

i , and it is easy to see that the inverse map above will also leave x∗, y∗, and u∗ unchanged.

i+1 is set
i = xi — that is, the proposal will be the the previous state. This proposal will be

i+1 = x∗

i+1 = y∗

i , and

i , y∗

ensures that ˆx†
accepted, since

a†

i = (cid:16)u∗

i+1 −

ˆx†

i −1Xx=0
S(xi+1, x)(cid:17). S(xi+1, ˆx†

i ) = (cid:16)u∗

i+1 −

< a(xi+1, xi) = a(xi+1, ˆx†
i )

xi−1Xx=0

S(xi+1, x)(cid:17). S(xi+1, xi) = a(xi+1, xi)

y∗
i

π(xi)

(72)

Since the proposal is accepted, x†
From equation (30), we see that

i = xi. From a†

i = a(xi+1, xi) y∗

i / π(xi), one can also see that y∗†

i = y∗
i .

u∗
i =

ˆxi+1−1Xx=0

S(xi, x) + ai+1 S(xi, xi+1)

and from equation (31) we see that

from which it follows that u∗†

i = u∗
i .

ai+1 = a(xi, xi+1) y∗

i+1 / π(xi+1)

(73)

(74)

To conﬁrm that the map deﬁned by equations (28) to (32) preserves volume, we look at the Jacobian
matrix for the continuous part of the map, from (y∗
i+1). For a rejected proposal, when
ai ≥ a(xi, ˆxi), this is the identity matrix. For an accepted proposal, the Jacobian matrix is as follows
(noting that ˆxi = xi+1):

i ) to (y∗

i+1, u∗

i , u∗

∂y∗
i+1
∂y∗
i
∂y∗
i+1
∂u∗
i

∂u∗
i+1
∂y∗
i
∂u∗
i+1
∂u∗
i





0

S(xi+1, xi) a(xi+1, xi)

π(xi)

0

= 


=

Since the absolute value of the determinant of this Jacobian matrix is one, the map must preserve
volume.

S(xi, xi+1) a(xi, xi+1)

π(xi+1)

0


min(cid:20) S(xi, xi+1)

π(xi+1)

1

0

,

S(xi+1, xi)

π(xi)

(cid:21)

(75)

(76)



min(cid:20) S(xi+1, xi)

π(xi)

,

S(xi, xi+1)

π(xi+1) (cid:21)

34

Appendix D: Proof that the map for a simulation of a continuous

distribution on an interval (a, b) is one-to-one and
volume preserving

The map from (x∗
i ) to (x∗
because it has the following inverse:

i , u∗

i , v∗

i , y∗

i+1, u∗

i+1, y∗

i+1, v∗

i+1) deﬁned by equations (38) to (41) is one-to-one

(x∗

i+1, u∗

i+1− s∗

i (mod 1))

x∗†

∗

i = eF −1

i = F∗(x∗†
u∗†
y∗†
i = π∗(x∗†
v∗†
i = y∗

i , x∗
i ) (v∗
i+1 / π∗(x∗

i+1)
i+1− t∗
i+1)

i

(mod 1))

(77)

(78)

(79)

(80)

Showing the x∗†

i = x∗

i , u∗†

i = u∗

i , y∗†

i = y∗

i , and v∗†

i = v∗

i

is straightforward except for issues involving

π∗(x∗) or eT ∗(x∗, x∗†) being zero. Division by zero in equation (80) will not occur because the state
space excludes points where π∗(x∗) = 0, as no y∗ value is allowable with such an x∗. When eT ∗(x∗, x∗†)
may be zero, we deﬁne eF −1
invert eF∗ only where the reverse transition density is zero, but this cannot happen for a transition that

(x∗, u∗) to be the largest x∗′ for which eF∗(x∗, x∗′) = u∗. eF −1

was taken, for which the forward transition density must have been non-zero.

To see that the map deﬁned by equations (38) to (41) preserves volume, we look at the determinant

∗ will fail to

∗

of its Jacobian matrix, which is

∂x∗
i+1
∂x∗
i
∂x∗
i+1
∂u∗
i
∂x∗
i+1
∂y∗
i
∂x∗
i+1
∂v∗
i

∂u∗
i+1
∂x∗
i
∂u∗
i+1
∂u∗
i
∂u∗
i+1
∂y∗
i
∂u∗
i+1
∂v∗
i

∂y∗
i+1
∂x∗
i
∂y∗
i+1
∂u∗
i
∂y∗
i+1
∂y∗
i
∂y∗
i+1
∂v∗
i

∂v∗
i+1
∂x∗
i
∂v∗
i+1
∂u∗
i
∂v∗
i+1
∂y∗
i
∂v∗
i+1
∂v∗
i



∂x∗
i+1
∂x∗
i

1
T ∗(x∗
i , x∗

i+1)

0

0

=





heT ∗(x∗

i+1, x∗

i ) + D

D
T ∗(x∗
i , x∗

i+1)

∂x∗
i+1
∂x∗

i i ×

0

0

×

×
1

π∗(x∗
i )

×

0

π∗(x∗

i+1)

0

(81)



Here, D = (∂/∂x∗
elements elsewhere. The determinant of this matrix has two non-zero terms, whose sum is

i ) and × marks elements of the matrix that are irrelevant given the zero

i+1, x∗

i+1, x∗

i ) + D

heT ∗(x∗

∂x∗
i+1
∂x∗

i+1)eF∗(x∗
i i
= eT ∗(x∗

1
T ∗(x∗
i , x∗

i+1)

1

π∗(x∗
i )

π∗(x∗

i+1) −

∂x∗
i+1
∂x∗
i

D
T ∗(x∗
i , x∗

i+1)

1

π∗(x∗
i )

π∗(x∗

i+1)

i+1, x∗
i )

1
T ∗(x∗
i , x∗

i+1)

1

π∗(x∗
i )

π∗(x∗

i+1) =

π∗(x∗
π∗(x∗

i+1)eT ∗(x∗

i ) T ∗(x∗

i+1, x∗
i )
i , x∗
i+1)

= 1 (82)

Since the determinant is one, the map preserves volume.

35

Appendix E: R Program for Ising Model Simulations

# SETUP FOR ISING MODEL. Stores information in global variables for later use.

ising_setup <- function (n_rows, n_cols)
{ n_rows <<- n_rows; n_cols <<- n_cols; n_spins <<- n_rows * n_cols; n_neighbors <<- 4

neighbor <<- matrix (NA, n_spins, n_neighbors)
for (j in 1:n_spins)
{ neighbor[j,1] <<- if (j%%n_rows==0) j-(n_rows-1) else j+1
neighbor[j,2] <<- if (j%%n_rows==1) j+(n_rows-1) else j-1
neighbor[j,3] <<- if ((j-1)%/%n_rows==n_cols-1) j-n_rows*(n_cols-1) else j+n_rows
neighbor[j,4] <<- if ((j-1)%/%n_rows==0) j+n_rows*(n_cols-1) else j-n_rows

}

}

# VECTORIZED MCMC FOR ISING MODEL. Method is "s" for standard simulation with multiple
# random streams, "c" for coupled simulation with a single stream, and "p" for permuation
# MCMC with a single stream. Initial values for states, u, and yf (y divided by pi) can be
# specified, or left to be generated randomly. The "random" numbers can be supplied via the
# s argument (of dimensions n_iters by n_spins), and are otherwise set here using runif.
# If the rev argument is TRUE, the reverse map is applied. The value returned is an array
# of states at all iterations, with attributes giving end values of states, u, and yf.

vec_ising <- function (beta, method, n_chains, n_iters, states=NULL, u=NULL, yf=NULL,

{ if (is.null(states))

s=NULL, rev=FALSE)

states <- matrix (sample (c(-1,1), n_chains*n_spins, replace=TRUE), n_spins, n_chains)

if (is.null(u) && method=="p") u <- runif (n_chains)
if (is.null(yf) && method=="p") yf <- runif (n_chains)
if (is.null(s) && (method=="p" || method=="c"))

s <- matrix(runif(n_iters*n_spins),n_iters,n_spins)

if (rev) s <- s[n_iters:1,,drop=FALSE]

results <- array(dim=c(n_spins,n_chains,n_iters))

for (i in 1:n_iters)
{ for (j in { if (rev) n_spins:1 else 1:n_spins })

{

if (method=="p" && rev) u <- (u - s[i,j]) %% 1

# Take s from u first for reverse map

neighbor_sum <- rep(0,n_chains)
for (k in 1:n_neighbors) neighbor_sum <- neighbor_sum + states[neighbor[j,k],]

if (method=="s") u <- runif(n_chains)
if (method=="c") u <- s[i,j]
if (method=="p") old_spin01 <- states[j,] == 1

# Old values of spins in 0/1 form

p0 <- 1 / (1 + exp(2*beta*neighbor_sum))
spin01 <- p0 <= u
states[j,] <- 2*spin01 - 1

# Probabilities of setting spin to 0
# New values of spins in 0/1 form
# New values of spins in -1/+1 form

if (method=="p")
{ t <- spin01*(1-p0) + (1-spin01)*p0

# Probabilities of new spin values

36

old_t <- old_spin01*(1-p0) + (1-old_spin01)*p0 # Probabilities of old spin values
old_yf <- yf
yf <- (u - p0*spin01) / t
u <- p0*old_spin01 + old_t*old_yf
if (!rev) u <- (u + s[i,j]) %% 1

# New yf (equal to y/pi) from eq (23)
# New u (except for + s) from eq (24)
# Add s to u last for non-reverse map

}

}
results[,,i] <- states

}

structure (results, end_states=states, end_u=u, end_yf=yf)

}

# COMPUTE THE ENERGY OF A SPIN CONFIGURATION.

ising_energy <- function (spins)
{ energy <- 0

for (j in 1:n_spins) energy <- energy - spins[j] * sum(spins[neighbor[j,]])
energy / 2

# Divide by two because each term in the energy was added twice above

}

# PLOT THE ENERGY ALONG ALL CHAINS.

energy_plot <- function (res)
{ n_chains <- dim(res)[2]; n_iters <- dim(res)[3]

E <- matrix(NA,n_iters,n_chains)
for (i in 1:n_iters) for (k in 1:n_chains) E[i,k] <- ising_energy(res[,k,i])
E_bar <- colMeans(E)

cat ("Energy: mean ",round(mean(E_bar),3), " +- ",round(sd(E_bar)/sqrt(n_chains),3),

"\n",sep="")

plot (c(1,n_iters), c(-2*n_spins, 2*n_spins), type="n", xlab="Iteration", ylab="")
for (k in 1:n_chains) points (E[,k], col=k, pch=20)

}

# PLOT THE MAGNETIZATION (SUM OF SPINS) ALONG ALL CHAINS.

magnetization_plot <- function (res)
{ n_chains <- dim(res)[2]; n_iters <- dim(res)[3]

M <- matrix(NA,n_iters,n_chains)
for (i in 1:n_iters) for (k in 1:n_chains) M[i,k] <- sum(res[,k,i])
M_bar <- colMeans(M); M_abs_bar <- colMeans(abs(M))

cat ("Magnetization, mean ",round(mean(M_bar),3), " +- ",round(sd(M_bar)/sqrt(n_chains),3),

", mean abs ",round(mean(M_abs_bar),3), " +- ",round(sd(M_abs_bar)/sqrt(n_chains),3),
"\n",sep="")

plot (c(1,n_iters), c(-n_spins, n_spins), type="n", xlab="Iteration", ylab="")
for (k in 1:n_chains) points (apply(res[,k,],2,sum), col=k, pch=20)

}

37

Appendix F: R Program for Truncated Normal Simulations

# VECTORIZED GIBSS SAMPLING FOR TRUNCATED NORMAL MODEL. Method is "s" for standard simulation
# "c" for coupled simulation, and "p" for permuation MCMC. Initial values and the random
# stream can be specified, or left to be generated randomly. If the rev argument is TRUE,
# the map is reversed. The value returned is an array of states at all iterations, with
# attributes giving end values of states and u.

Uses global rho, lower, and upper variables.

vec_trunc_normal_gibbs <- function (method, n_chains, n_iters, states=NULL, u=NULL,

{ if (is.null(states))

{ states <- matrix (NA, 2, n_chains)

s=NULL, rev=FALSE)

for (j in 1:2) states[j,] <- runif(n_chains, lower[j], upper[j])

}
if (is.null(u) && method=="p") u <- runif (n_chains)
if (is.null(s) && (method=="p" || method=="c")) s <- matrix(runif(2*n_iters),n_iters,2)
if (rev) s <- s[n_iters:1,,drop=FALSE]

results <- array(dim=c(2,n_chains,n_iters))

for (i in 1:n_iters)
{ for (j in { if (rev) 2:1 else 1:2 })

{ if (method=="p" && rev) u <- (u - s[i,j]) %% 1

if (method=="s") u <- runif(n_chains)
if (method=="c") u <- s[i,j]
if (method=="p") old <- states[j,]
cond_sd <- sqrt(1-rho^2); cond_mean <- rho * states[3-j,]
ql <- pnorm (lower[j], cond_mean, cond_sd); qu <- pnorm (upper[j], cond_mean, cond_sd)
states[j,] <- # max & min below not usually needed - just in case of numerical issue

pmax (lower[j], pmin (upper[j], qnorm (ql + (qu-ql)*u, cond_mean, cond_sd)))

if (method=="p")
{ u <- (pnorm(old,cond_mean,cond_sd) - ql) / (qu-ql)

if (!rev) u <- (u + s[i,j]) %% 1

}

}
results[,,i] <- states

}
structure (results, end_states=states, end_u=u)

}

# VECTORIZED METROPOLIS SAMPLING FOR TRUNCATED NORMAL MODEL. Arguments are as above, but
# with no "rev" argument, an extra optional "yf" argument, and an extra optional "delta"
# argument that specifies random-walk proposal offsets for each variable update.

vec_trunc_normal_met <- function (method, n_chains, n_iters, states=NULL, u=NULL, yf=NULL,

{ if (is.null(states))

{ states <- matrix (NA, 2, n_chains)

s=NULL, delta=NULL)

for (j in 1:2) states[j,] <- runif(n_chains, lower[j], upper[j])

}
if (is.null(u) && method=="p") u <- runif (n_chains)
if (is.null(yf) && method=="p") yf <- runif(n_chains)
if (is.null(s) && (method=="p" || method=="c")) s <- matrix(runif(2*n_iters),n_iters,2)
if (is.null(delta)) delta <- matrix(runif(2*n_iters,0,1),n_iters,2)

38

results <- array(dim=c(2,n_chains,n_iters))

for (i in 1:n_iters)
{ for (j in 1:2)

{ if (method=="s") u <- runif(n_chains)

if (method=="c") u <- s[i,j]
old <- states[j,]
cond_sd <- sqrt(1-rho^2); cond_mean <- rho * states[3-j,]
proposal <- (states[j,]+delta[i,j]) * (u<1/2) + (states[j,]-delta[i,j]) * (u>=1/2)
a <- (2*u) %% 1
ar <- dnorm(proposal,cond_mean,cond_sd) / dnorm(old,cond_mean,cond_sd)
ap <- (proposal >= lower[j]) * (proposal <= upper[j]) * pmin(1,ar)
accept <- (a < ap)
states[j,] <- accept*proposal + (1-accept)*old
if (method=="p")
{ u[accept] <- ((u<1/2)/2 + pmin(1,1/ar)*yf/2)[accept]; yf[accept] <- (a/ap)[accept]

u <- (u + s[i,j]) %% 1

}

}
results[,,i] <- states

}
results

}

# PLOT X1 VERSUS X2 FOR SIMULATIONS.

bi_plot <- function (res, from=11, xlim=c(lower[1],upper[1]), ylim=c(lower[2],upper[2]))
{ n_chains <- dim(res)[2]; n_iters <= dim(res)[3]

plot (c(), type="n", xlab="x1", ylab="x2", xlim=xlim, ylim=ylim)
for (k in 1:n_chains) points (res[1,k,from:n_iters], res[2,k,from:n_iters], pch=20, col=k)

}

# PLOT X1 OR X2 VERSUS ITERATION NUMBER FOR SIMULATIONS.

uni_plot <- function (res, j, ylim=c(lower[j],upper[j]))
{ n_chains <- dim(res)[2]; n_iters <- dim(res)[3]

plot (c(),type="n",xlab="Iteration",ylab=paste("x",j,sep=""),xlim=c(1,n_iters),ylim=ylim)
for (k in 1:n_chains) points (res[j,k,], pch=20, col=k)

}

# FIND MEAN ESTIMATES AND STANDARD ERRORS FROM SIMULATIONS.

estimates <- function (res, from=11)
{ n_chains <- dim(res)[2]; n_iters <- dim(res)[3]

ave <- ave_sq <- matrix(NA,2,n_chains)
for (k in 1:n_chains)
{ ave[,k] <- rowMeans(res[,k,from:n_iters]);

ave_sq[,k] <- rowMeans(res[,k,from:n_iters]^2)

}
round (cbind (ave=apply(ave,1,mean),

se=apply(ave,1,sd)/sqrt(n_chains),

ave_sq=apply(ave_sq,1,mean), se_sq=apply(ave_sq,1,sd)/sqrt(n_chains)), 4)

}

39

Appendix G: R Program for Importance Sampling Test Simulations

# LOG PROBABILITY DENSITY FUNCTION FOR TEST DISTRIBUTION.

istest_log_density <- function (x) dnorm(x[1],0,1,log=TRUE) + dnorm(x[2],x[1]^2-1,1,log=TRUE)

# VECTORIZED PERMUTATION METROPOLIS FOR TEST DISTRIBUTION. Does random-walk Metropolis updates
# for both variables at once. If delta is not supplied, it’s normal with given proposal_sd.

vec_istest_met <- function (n_chains, n_iters, states=NULL, u=NULL, yf=NULL,

s=NULL, delta=NULL, rev=FALSE, proposal_sd)

{

}

if (is.null(states)) states <- matrix (rnorm(n_chains*2,0,1), 2, n_chains)
if (is.null(u))
if (is.null(yf))
if (is.null(s))
if (is.null(delta)) delta <- matrix(rnorm(n_iters*2,0,proposal_sd),n_iters,2)
if (rev) { s <- s[n_iters:1]; delta <- delta[n_iters:1,,drop=FALSE] }

u <- runif(n_chains)
yf <- runif(n_chains)
s <- runif(n_iters)

results <- array(dim=c(2,n_chains,n_iters))

for (i in 1:n_iters)
{

if (rev) u <- (u - s[i]) %% 1

proposal <- t (t(states+delta[i,]) * (u<1/2) + t(states-delta[i,]) * (u>=1/2))
a <- (2*u) %% 1
ar <- exp (apply(proposal,2,istest_log_density) - apply(states,2,istest_log_density))
ap <- pmin(1,ar)
accept <- (a < ap)
states <- t (accept*t(proposal) + (1-accept)*t(states))
u[accept] <- ((u<1/2)/2 + pmin(1,1/ar)*yf/2)[accept]
yf[accept] <- (a/ap)[accept]

if (!rev) u <- (u + s[i]) %% 1

results[,,i] <- states

}

structure (results, end_states=states, end_u=u, end_yf=yf)

# METROPOLIS IMPORTANCE SAMPLING FOR TEST DISTRIBUTION. N is number of points, M is maximum
# number of transitons, mu and sd are for normal start state distribution, and proposal_sd
# is for the normal random-walk Metropolis proposals.

istest_met_is <- function (N, M, mu, sd, proposal_sd)
{

pr <- rep(NA,M+1); w <- rep(NA,N); beta <- matrix (NA,N,2)
states <- matrix(NA,2,M+1); s <- runif(M); delta <- matrix (rnorm(M*2,0,proposal_sd), M, 2)

for (i in 1:N)
{

k <- if (M==0) 0 else sample(0:M,1) # Pick number of transitions from start to end state

40

states[,k+1] <- rnorm(2,mu,sd)
u <- runif(1); yf <- runif(1)

# Random start state
# Random start values for u and yf

if (k<M)

states[,(k+2):(M+1)] <-

# Do a forward simulation from the start state on

vec_istest_met (proposal_sd, 1, M-k, cbind(states[,k+1]), u, yf,

s=s[(k+1):M], delta=delta[(k+1):M,,drop=FALSE], rev=FALSE) [,1,]

if (k>0)

states[,1:k] <-

# Do a reverse simulation before the start state

vec_istest_met (proposal_sd, 1, k, cbind(states[,k+1]), u, yf,

s=s[1:k], delta=delta[1:k,,drop=FALSE], rev=TRUE) [,1,]

beta[i,] <- states[,M+1]

# Importance sampling point is the end state

for (k in 0:M)

pr[k+1] <- sum(dnorm(states[,k+1],mu,sd,log=TRUE))
- istest_log_density(states[,k+1])

# Compute log probabilities
#

of possible start states

w[i] <- - (max(pr) + log(sum(exp(pr-max(pr)))) - log(M+1)) # Importance weight: minus the
log of average probability

#

}

list (w=w, beta=beta)

}

# ANALYSE IMPORTANCE SAMPLING OUTPUT. Takes as arguments a vector of log weights and a
# matrix of beta values. Prints various estimates with standard errors.

is_analysis <- function (w, beta)
{

N <- length(w)
mx <- max(w)
w <- exp (w-mx)

cat ("\nEstimated log normalizing constant:", round(mx+log(mean(w)),3),

"+-", round(sd(w)/sqrt(N)/mean(w),3), "\n\n")

w <- w / mean(w)

z <- sum(w)
ave <- colSums(w*beta) / z
ave_sq <- colSums(w*beta^2) / z

cat ("Adjusted sample size:", N, "/", 1+var(w), "=", N/(1+var(w)), "\n\n")

beta_offset <- t (t(beta) - ave)
ave_se <- sqrt (colSums((w*beta_offset)^2) / N^2)
beta_sq_offset <- t (t(beta^2) - ave_sq)
ave_sq_se <- sqrt (colSums((w*beta_sq_offset)^2) / N^2)

print(cbind (ave=ave, se=ave_se, ave_sq=ave_sq, sq_se=ave_sq_se))

}

41

References

Adler, S. L. (1981) “Over-relaxation method for the Monte Carlo evaluation of the partition function for multi-

quadratic actions”, Physical Review D, vol. 23, pp. 2901-2904.

Alder, B. J. and Wainwright, T. E. (1959) “Studies in molecular dynamics.

I. General method”, Journal of

Chemical Physics, vol. 31, pp. 459-466.

Chen, S., Dick, J., and Owen, A. B. (2011) “Consistency of Markov chain quasi-Monte Carlo on continuous state

spaces”, Annals of Statistics, vol. 39, pp. 673-701.

Gelfand, A. E. and Smith, A. F. M. (1990) “Sampling-based approaches to calculating marginal densities”, Journal

of the American Statistical Association, vol. 85, pp. 398-409.

Geweke, J. (1989) “Bayesian inference in econometric models using Monte Carlo integration”, Econometrica,

vol. 57, pp. 1317-1339.

Geman, S. and Geman, D. (1984). “Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of

images”. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 6, pp. 721-741.

Hastings, W. K. (1970) “Monte Carlo sampling methods using Markov chains and their applications”, Biometrika,

vol. 57, pp. 97–109.

Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. (1953) “Equation of state

calculations by fast computing machines”, Journal of Chemical Physics, vol. 21, pp. 1087–1092.

Neal, R. M.‘ (1998) “Suppressing random walks in Markov chain Monte Carlo using ordered overrelaxation”, in

M. I. Jordan (editor) Learning in Graphical Models, pp. 205-225, Dordrecht: Kluwer.

Neal, R. M. (1999/2002) “Circularly-coupled Markov chain sampling”, Technical Report No. 9910 (revised), Dept.

of Statistics (November 1999 / February 2002), 49 pages.

Neal, R. M. (2000) “Markov chain sampling methods for Dirichlet process mixture models”, Journal of Compu-

tational and Graphical Statistics, vol. 9, pp. 249-265.

Neal, R. M. (2001) “Annealed importance sampling”, Statistics and Computing, vol. 11, pp. 125-139.

Neal, R. M. (2003) “Slice sampling” (with discussion), Annals of Statistics, vol. 31, pp. 705-767.

Neal, R. M. (2010) “MCMC using Hamiltonian dynamics”, in the Handbook of Markov Chain Monte Carlo,
S. Brooks, A. Gelman, G. Jones, and X.-L. Meng (editors), Chapman & Hall / CRC Press, pp. 113-162. Also
at http://www.cs.toronto.edu/∼radford/ham-mcmc.abstract.html.

Neal, R. M. (2005) “Hamiltonian importance sampling”, talk presented at the Banﬀ International Research
Station (BIRS) workshop on Mathematical Issues in Molecular Dynamics, June 2005. Slides available at
http://www.cs.utoronto.ca/∼radford/ftp/his-talk.pdf.

Nos´e, S. (1984) “A molecular dynamics method for simulations in the canonical ensemble”, Molecular Physics,

vol. 52, pp. 255-368.

Murray, I. and Elliott, L. T. (2012) “Driving Markov chain Monte Carlo with a dependent random stream”,

preprint, arXiv:1204.3187v1.

Propp, J. G. and Wilson, D. B. (1996) “Exact sampling with coupled Markov chains and applications to statistical

mechanics”, Random Structures and Algorithms, vol. 9, pp. 223-252.

Wu, P.-C. and Huang, K.-C. (2006) “Parallel use of multiplicative congruential random number generators”,

Computer Physics Communications, vol. 175, pp. 25-29.

42

