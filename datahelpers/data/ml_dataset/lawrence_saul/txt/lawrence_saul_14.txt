Fast Solvers and Eﬃcient Implementations

for Distance Metric Learning

Kilian Q. Weinberger
Yahoo! Research, 2821 Mission College Blvd, Santa Clara, CA 9505

kilian@yahoo-inc.com

Lawrence K. Saul
CSE Department, University of California, San Diego 9500 Gilman Drive, La Jolla, CA 92093-0404

saul@cs.ucsd.edu

Abstract

In this paper we study how to improve near-
est neighbor classiﬁcation by learning a Ma-
halanobis distance metric. We build on a re-
cently proposed framework for distance met-
ric learning known as large margin nearest
neighbor (LMNN) classiﬁcation. Our paper
makes three contributions. First, we describe
a highly eﬃcient solver for the particular
instance of semideﬁnite programming that
arises in LMNN classiﬁcation; our solver can
handle problems with billions of large margin
constraints in a few hours. Second, we show
how to reduce both training and testing times
using metric ball trees; the speedups from
ball trees are further magniﬁed by learning
low dimensional representations of the input
space. Third, we show how to learn diﬀer-
ent Mahalanobis distance metrics in diﬀerent
parts of the input space. For large data sets,
the use of locally adaptive distance metrics
leads to even lower error rates.

1. Introduction

Many algorithms for pattern classiﬁcation and ma-
chine learning depend on computing distances in a
multidimensional input space. Often, these distances
are computed using a Euclidean distance metric—a
choice which has both the advantages of simplicity and
generality. Notwithstanding these advantages, though,
the Euclidean distance metric is not very well adapted
to most problems in pattern classiﬁcation.
Viewing the Euclidean distance metric as overly sim-

Preliminary work. Under review by the International Con-
ference on Machine Learning (ICML). Do not distribute.

plistic, many researchers have begun to ask how to
learn or adapt the distance metric itself in order to
achieve better results (Xing et al., 2002; Chopra et al.,
2005; Frome et al., 2007). Distance metric learning
is an emerging area of statistical learning in which
the goal is to induce a more powerful distance met-
ric from labeled examples. The simplest instance of
this problem arises in the context of k-nearest neigh-
bor (kNN) classiﬁcation using Mahalanobis distances.
Mahalanobis distances are computed by linearly trans-
forming the input space, then computing Euclidean
distances in the transformed space. A well-chosen lin-
ear transformation can improve kNN classiﬁcation by
decorrelating and reweighting elements of the feature
vector.
In fact, signiﬁcant improvements have been
observed within several diﬀerent frameworks for this
problem, including neighborhood components analy-
sis (Goldberger et al., 2005), large margin kNN clas-
siﬁcation (Weinberger et al., 2006), and information-
theoretic metric learning (Davis et al., 2007).
These studies have established the general utility of
distance metric learning for kNN classiﬁcation. How-
ever, further work is required to explore its promise
in more diﬃcult regimes.
In particular, larger data
sets raise new and important challenges in scalability.
They also present the opportunity to learn more adap-
tive and sophisticated distance metrics.
In this paper, we study these issues as they arise in
the recently proposed framework of large margin near-
est neighbor (LMNN) classiﬁcation (Weinberger et al.,
2006). In this framework, a Mahalanobis distance met-
ric is trained with the goal that the k-nearest neigh-
bors of each example belong to the same class while
examples from diﬀerent classes are separated by a large
margin. Simple in concept, useful in practice, the
ideas behind LMNN classiﬁcation have also inspired
other related work in machine learning and computer
vision (Torresani & Lee, 2007; Frome et al., 2007).

Fast Solvers and Eﬃcient Implementations for Distance Metric Learning

The role of the margin in LMNN classiﬁcation is in-
spired by its role in support vector machines (SVMs).
Not surprisingly, given these roots, LMNN classiﬁca-
tion also inherits various strengths and weaknesses of
SVMs (Sch¨olkopf & Smola, 2002). For example, as in
SVMs, the training procedure in LMNN classiﬁcation
reduces to a convex optimization based on the hinge
loss. However, as described in section 2, na¨ıve imple-
mentations of this optimization do not scale well to
larger data sets.
Addressing the challenges and opportunities raised by
larger data sets, this paper makes three contributions.
First, we describe how to optimize the training pro-
cedure for LMNN classiﬁcation so that it can readily
handle data sets with tens of thousands of training
examples.
In order to scale to this regime, we have
implemented a special-purpose solver for the particu-
lar instance of semideﬁnite programming that arises
in LMNN classiﬁcation. In section 3, we describe the
details of this solver, which we have used to tackle
problems involving billions of large margin constraints.
To our knowledge, problems of this size have yet to
be tackled by other recently proposed methods (Gold-
berger et al., 2005; Davis et al., 2007) for learning
Mahalanobis distance metrics.
As the second contribution of this paper, we explore
the use of metric ball trees (Liu et al., 2005) for LMNN
classiﬁcation. These data structures have been widely
used to accelerate nearest neighbor search.
In sec-
tion 4, we show how similar data structures can be
used for faster training and testing in LMNN classi-
ﬁcation. Ball trees are known to work best in input
spaces of low to moderate dimensionality. Mindful of
this regime, we also show how to modify the optimiza-
tion in LMNN so that it learns a low-rank Mahalanobis
distance metric. With this modiﬁcation, the metric
can be viewed as projecting the original inputs into a
lower dimensional space, yielding further speedups.
As the third contribution of this paper, we describe
an important extension to the original framework for
LMNN classiﬁcation. Speciﬁcally,
in section 5, we
show how to learn diﬀerent Mahalanobis distance met-
rics for diﬀerent parts of the input space. The novelty
of our approach lies in learning a collection of diﬀerent
local metrics to maximize the margin of correct kNN
classiﬁcation. The promise of this approach is sug-
gested by recent, related work in computer vision that
has achieved state-of-the-art results on image classiﬁ-
cation (Frome et al., 2007). Our particular approach
begins by partitioning the training data into disjoint
clusters using class labels or unsupervised methods.
We then learn a Mahalanobis distance metric for each

cluster. While the training procedure couples the dis-
tance metrics in diﬀerent clusters, the optimization re-
mains a convex problem in semideﬁnite programming.
The globally coupled training of these metrics also
distinguishes our approach from earlier work in adap-
tive distance metrics for kNN classiﬁcation (Hastie &
Tibshirani, 1996). To our knowledge, our approach
yields the best kNN test error rate on the extensively
benchmarked MNIST data set of handwritten digits
that does not incorporate domain-speciﬁc prior knowl-
edge (LeCun et al., 1998; Simard et al., 1993). Thus,
our results show that we can exploit large data sets to
learn more powerful and adaptive distance metrics for
kNN classiﬁcation.

2. Background

Of the many settings for distance metric learning, the
simplest instance of the problem arises in the con-
text of kNN classiﬁcation using Mahalanobis distances.
A Mahalanobis distance metric computes the squared
distances between two points (cid:126)xi and (cid:126)xj as:
M((cid:126)xi, (cid:126)xj) = ((cid:126)xi − (cid:126)xj)(cid:62)M((cid:126)xi − (cid:126)xj),
d2

(1)
where M (cid:23) 0 is a positive semideﬁnite matrix. When
M is equal to the identity matrix, eq. (1) reduces to the
Euclidean distance metric. In distance metric learning,
the goal is to discover a matrix M that leads to lower
kNN error rates than the Euclidean distance metric.
Here we brieﬂy review how Mahalanobis distance met-
rics are learned for LMNN classiﬁcation (Weinberger
et al., 2006). Let the training data consist of n la-
beled examples {((cid:126)xi, yi)}n
i=1 where (cid:126)xi ∈ Rd and yi ∈
{1, . . . , c}, where c is the number of classes. For LMNN
classiﬁcation, the training procedure has two steps.
The ﬁrst step identiﬁes a set of k similarly labeled
target neighbors for each input (cid:126)xi. Target neighbors
are selected by using prior knowledge (if available) or
by simply computing the k nearest (similarly labeled)
neighbors using Euclidean distance. We use the nota-
tion j (cid:32) i to indicate that (cid:126)xj is a target neighbor of (cid:126)xi.
The second step adapts the Mahalanobis distance met-
ric so that these target neighbors are closer to (cid:126)xi than
all other diﬀerently labeled inputs. The Mahalanobis
distance metric is estimated by solving a problem in
semideﬁnite programming. Distance metrics obtained
in this way were observed to yield consistent and sig-
niﬁcant improvements in kNN error rates.
The semideﬁnite program in LMNN classiﬁcation
arises from an objective function which balances two
terms. The ﬁrst term penalizes large distances be-
tween inputs and their target neighbors. The second
term penalizes small distances between diﬀerently la-

Fast Solvers and Eﬃcient Implementations for Distance Metric Learning

beled inputs; speciﬁcally, a penalty is incurred if these
distances do not exceed (by a ﬁnite margin) the dis-
tances to the target neighbors of these inputs. The
terms in the objective function can be made precise
with further notation. Let yij ∈{0, 1} indicate whether
the inputs (cid:126)xi and (cid:126)xj have the same class label. Also,
let ξijl ≥ 0 denote the amount by which a diﬀerently
labeled input (cid:126)xl invades the “perimeter” around input
(cid:126)xi deﬁned by its target neighbor (cid:126)xj. The Mahalanobis
distance metric M is obtained by solving the following
semideﬁnite program:

Minimize (cid:80)

subject to:

j(cid:32)i
M((cid:126)xi, (cid:126)xl) − d2

(a) d2
(b) ξijl ≥ 0
(c) M (cid:23) 0.

(cid:2)d2
M((cid:126)xi, (cid:126)xj) + µ(cid:80)

M((cid:126)xi, (cid:126)xj) ≥ 1 − ξijl

(cid:3)

l(1 − yil)ξijl

The constant µ deﬁnes the trade-oﬀ between the two
terms in the objective function; in our experiments, we
set µ = 1. The constraints of type (a) encourage in-
puts ((cid:126)xi) to be at least one unit closer to their k target
neighbors ((cid:126)xj) than to any other diﬀerently labeled in-
put ((cid:126)xl). When diﬀerently labeled inputs (cid:126)xl invade the
local neighborhood of (cid:126)xi, we refer to them as impos-
tors. Impostors generate positive slack variables ξijl
which are penalized in the second term of the objective
function. The constraints of type (b) enforce nonneg-
ativity of the slack variables, and the constraint (c)
enforces that M is positive semideﬁnite, thus deﬁning
a valid (pseudo)metric. Noting that the squared Ma-
halanobis distances d2
M((cid:126)xi, (cid:126)xj) are linear in the matrix
M, the above optimization is easily recognized as an
semideﬁnite program.

3. Solver

The semideﬁnite program in the previous section grows
in complexity with the number of training examples
(n), the number of target neighbors (k), and the di-
mensionality of the input space (d).
In particular,
the objective function is optimized with respect to
O(kn2) large margin constraints of type (a) and (b),
while the Mahalanobis distance metric M itself is a
d × d matrix. Thus, for even moderately large and/or
high dimensional data sets, the required optimization
(though convex) cannot be solved by standard oﬀ-the-
shelf packages (Borchers, 1999).
In order to tackle larger problems in LMNN classiﬁca-
tion, we implemented our own special-purpose solver.
Our solver was designed to exploit the particular struc-
ture of the semideﬁnite program in the previous sec-
tion. The solver iteratively re-estimates the Maha-

lanobis distance metric M to minimize the objective
function for LMNN classiﬁcation. The amount of com-
putation is minimized by careful book-keeping from
one iteration to the next. The speed-ups from these
optimizations enabled us to work comfortably on data
sets with up to n=60, 000 training examples.
Our solver works by eliminating the slack variables ξijl
from the semideﬁnite program for LMNN classiﬁca-
tion, then minimizing the resulting objective function
by sub-gradient methods. The slack variables are elim-
inated by folding the constraints (a) and (b) into the
objective function as a sum of “hinge” losses. The
hinge function is deﬁned as [z]+ = z if z > 0 and
[z]+ = 0 if z < 0. In terms of this hinge function, we
can express ξijl as a function of the matrix M:

M((cid:126)xi, (cid:126)xl)(cid:3)

+

ξijl(M) = (cid:2)1 + d2
ε(M) =(cid:88)

(cid:34)

j(cid:32)i

M((cid:126)xi, (cid:126)xj) − d2
(cid:88)

l

Finally, writing the objective function only in terms of
the matrix M, we obtain:

(2)

(cid:35)

M((cid:126)xi, (cid:126)xj) + µ
d2

(1 − yil)ξijl(M)

.

(3)
This objective function is not diﬀerentiable due to the
hinge losses that appear in eq. (2). Nevertheless, be-
cause it is convex, we can compute its sub-gradient and
use standard descent algorithms to ﬁnd its minimum.
At each iteration of our solver, the optimization takes
a step along the sub-gradient to reduce the objective
function, then projects the matrix M back onto the
cone of positive semideﬁnite matrices. Iterative meth-
ods of this form are known to converge to the correct
solution, provided that the gradient step-size is suﬃ-
ciently small (Boyd & Vandenberghe, 2004).
The gradient computation can be done most eﬃciently
by careful book-keeping from one iteration to the next.
As simplifying notation, let Cij =((cid:126)xi − (cid:126)xj)((cid:126)xi − (cid:126)xj)(cid:62).
In terms of this notation, we can express the squared
Mahalanobis distances in eq. (8) as:

dM((cid:126)xi, (cid:126)xj) = tr(CijM).

(4)

To evaluate the gradient, we denote the matrix M at
the tth iteration as Mt. At each iteration, we also
deﬁne a set N t of triplet indices such that (i, j, l) ∈ N t
if and only if the triplet’s corresponding slack variable
exceeds zero: ξijl(Mt) > 0. With this notation, we can
write the gradient Gt = ∂ε
∂M

(cid:12)(cid:12)Mt at the tth iteration as:
(cid:88)

Gt = (cid:88)

(Cij − Cil) .

Cij + µ

(5)

j(cid:32)i

(i,j,l)∈N t

Computing the gradient requires computing the outer
products in Cij;
it thus scales quadratically in the

Fast Solvers and Eﬃcient Implementations for Distance Metric Learning

input dimensionality. As the set N t is potentially
large, a na¨ıve computation of the gradient would be
extremely expensive. However, we can exploit the fact
that the gradient contribution from each active triplet
(i, j, l) does not depend on the degree of its margin
violation. Thus, the changes in the gradient from one
iteration to the next are determined entirely by the
diﬀerences between the sets N t and N t+1. Using this
fact, we can derive an extremely eﬃcient update that
relates the gradient at one iteration to the gradient at
the previous one. The update subtracts the contribu-
tions from triples that are no longer active and adds
the contributions from those that just became active:

(cid:88)

(cid:88)

Gt+1 = Gt − µ

(Cij − Cil) + µ

(Cij − Cil)

(i,j,l)∈N t−N t+1

(i,j,l)∈N t+1−N t

(6)
For small gradient step sizes, the set N t changes very
little from one iteration to the next. In this case, the
right hand side of eq. (6) can be computed very fast.
To further accelerate the solver, we adopt an active
set method. This method is used to monitor the large
margin constraints that are actually violated. Note
that computing the set N t at each iteration requires
checking every triplet (i, j, l) with j (cid:32) i for a po-
tential margin violation. This computation scales as
O(nd2 + kn2d), making it impractical for large data
sets. To avoid this computational burden, we observe
that the great majority of triples do not incur mar-
gin violations:
in particular, for each training exam-
ple, only a very small fraction of diﬀerently labeled
examples typically lie nearby in the input space. Con-
sequently, a useful approximation is to check only a
subset of likely triples for margin violations per gra-
dient computation and only occasionally perform the
(cid:83)t−1
full computation. We set this active subset to the list
of all triples that have ever violated the margin, ie
i=1 N i. When the optimization converges, we verify
that the working set N t does contain all active triples
that incur margin violations. This ﬁnal check is needed
to ensure convergence to the correct minimum. If the
check is not satisﬁed, the optimization continues with
the newly expanded active set.
Table 1 shows how quickly the solver works on prob-
lems of diﬀerent sizes. The results in this table were
generated by learning a Mahalanobis distance metric
on the MNIST data set of 28×28 grayscale handwrit-
ten digits (LeCun et al., 1998). The digits were pre-
processed by principal component analysis (PCA) to
reduce their dimensionality from d = 784 to d = 169.
We experimented by learning a distance metric from
diﬀerent subsets of the training examples. The experi-
ments were performed on a standard desktop machine

Table 1. Statistics of the solver on subsets of the data set
of MNIST handwritten digits. See text for details.
with a 2.0 GHz dual core 2 processor. For each ex-
periment, the table shows the number of training ex-
amples, the CPU time to converge, the number of ac-
tive constraints, the total number of constraints, and
the kNN test error (with k = 3). Note that for the
full MNIST training set, the semideﬁnite program has
over three billion large margin constraints. Neverthe-
less, the active set method converges in less than four
hours—from a Euclidean distance metric with 2.33%
test error to a Mahalanobis distance metric with 1.72%
test error.

4. Tree-Based Search

Nearest neighbor search can be accelerated by storing
training examples in hierarchical data structures (Liu
et al., 2005). These data structures can also be used to
reduce the training and test times for LMNN classiﬁ-
cation. In this section, we describe how these speedups
are obtained using metric ball trees.

4.1. Ball trees

We begin by reviewing the use of ball trees (Liu et al.,
2005) for fast kNN search. Ball trees recursively par-
tition the training inputs by projecting them onto di-
rections of large variance, then splitting the data on
the mean or median of these projected values. Each
subset of data obtained in this way deﬁnes a hyper-
sphere (or “ball”) in the multidimensional input space
that encloses its training inputs. The distance to such
a hypersphere can be easily computed for any test in-
put; moreover, this distance provides a lower bound on
the test input’s distance to any of the enclosed train-
ing inputs. This bound is illustrated in Fig. 1. Let S
be the set of training inputs inside a speciﬁc ball with
radius r. The distance from a test input (cid:126)xt to any
training input (cid:126)xi ∈ S is bounded from below by:
∀(cid:126)xi ∈ S (cid:107)(cid:126)xt − (cid:126)xi(cid:107) ≥ max((cid:107)(cid:126)xt − (cid:126)c(cid:107)2 − r, 0).

(7)

These bounds can be exploited in a tree-based search
for nearest neighbors.
In particular, if the distance
to the currently kth closest input (cid:126)xj is smaller than
the bound from eq. (7), then all inputs inside the ball
S can be pruned away. This pruning of unexplored
subtrees can signiﬁcantly accelerate kNN search. The
same basic strategy can also be applied to kNN search
under a Mahalanobis distance metric.

Ntime|active set||total set|train errortest error609s8443.2K0%29.37%60037s6169323K0%10.79%60004m5034532M0.48%3.13%600003h25m5400373.2B0%1.72%Fast Solvers and Eﬃcient Implementations for Distance Metric Learning

4.3. Dimensionality reduction

Across all our experiments, we observed that the gains
from ball trees diminished rapidly with the dimen-
sionality of the input space. This observation is con-
sistent with previous studies of ball trees and NN
search. When the data is high dimensional, NN search
is plagued by the so-called “curse of dimensionality”.
In particular, distances in high dimensions tend to be
more uniform, thereby reducing the opportunities for
pruning large subtrees.
The curse of dimensionality is often addressed in ball
trees by projecting the stored training inputs into a
lower dimensional space. The most commonly used
methods for dimensionality reduction are random pro-
jections and PCA. Despite their widespread use, how-
ever, neither of these methods is especially geared to
preserve (or improve) the accuracy of kNN classiﬁca-
tion.
We experimented with two methods for dimensionality
reduction in the particular context of LMNN classiﬁca-
tion. Both methods were based on learning a low-rank
Mahalanobis distance metric. Such a metric can be
viewed as projecting the original inputs into a lower di-
mensional space. In our ﬁrst approach, we performed
a singular value decomposition (SVD) on the full rank
solution to the semideﬁnite program in section 2. The
full rank solution for the distance metric was then re-
placed by a low rank approximation based on its lead-
ing eigenvectors. We call this approach LMNN-SVD.
In our second approach, we followed a suggestion from
previous work on LMNN classiﬁcation (Torresani &
Lee, 2007).
In this approach, we explicitly parame-
terized the Mahalanobis distance metric as a low-rank
matrix, writing M = L(cid:62)L, where L is a rectangular
matrix. To obtain the distance metric, we optimized
the same objective function as before, but now in terms
of the explicitly low-rank linear transformation L. The
optimization over L is not convex unlike the original
optimization over M, but a (possibly local) minimum
can be computed by standard gradient-based methods.
We call this approach LMNN-RECT.
Fig. 2 shows the results of kNN classiﬁcation from both
these methods on the MNIST data set of handwritten
digits. For these experiments, the raw MNIST im-
ages (of size 28× 28) were ﬁrst projected onto their
350 leading principal components. The training pro-
cedure for LMNN-SVD optimized a full-rank distance
metric in this 350 dimensional space, then extracted a
low-rank distance metric from its leading eigenvectors.
The training procedures for LMNN-RECT optimized
a low-rank rectangular matrix of size r × 350, where r
varied from 15 to 40. Also shown in the ﬁgure are

Figure 1. How ball trees work: for any input (cid:126)xi ∈ S, the
distance (cid:107)(cid:126)xt − (cid:126)xi(cid:107) is bounded from below by eq. (7) . If a
training example (cid:126)xj is known to be closer to (cid:126)xt, then the
inputs inside the ball can be ruled out as nearest neighbors.

4.2. Speedups

We ﬁrst experimented with ball trees to reduce the test
times for LMNN classiﬁcation. In our experiments, we
observed a factor of 3x speed-up for 40-dimensional
data and a factor of 15x speedup for 15-dimensional
data. Note that these speedups were measured rel-
ative to a highly optimized baseline implementation
of kNN search. In particular, our baseline implemen-
tation rotated the input space to align its coordinate
axes with the principal components of the data; the
coordinate axes were also sorted in decreasing order of
variance. In this rotated space, distance computations
were terminated as soon as any partially accumulated
results (from leading principal components) exceeded
the currently smallest k distances from the kNN search
in progress.
We also experimented with ball trees to reduce the
training times for LMNN classiﬁcation. To reduce
training times, we integrated ball trees into our
special-purpose solver. Speciﬁcally, ball trees were
used to accelerate the search for so-called “impostors”.
Recall that for each training example (cid:126)xi and for each
of its similarly labeled target neighbors (cid:126)xj, the im-
postors consist of all diﬀerently labeled examples (cid:126)xl
with dM ((cid:126)xi, (cid:126)xl)2 ≤ dM ((cid:126)xi, (cid:126)xj)2 +1. The search for im-
postors dominates the computation time in the train-
ing procedure for LMNN classiﬁcation. To reduce the
amount of computation, the solver described in sec-
tion 3 maintains an active list of previous margin viola-
tions. Nevertheless, the overall computation still scales
as O(n2d), which can be quite expensive. Note that
we only need to search for impostors among training
examples with diﬀerent class labels. To speed up train-
ing, we built one ball tree for the training examples in
each class and used them to search for impostors (as
the ball-tree creation time is negligible in comparison
with the impostor search, we re-built it in every iter-
ation). We observed the ball trees to yield speedups
ranging from a factor of 1.9x with 10-dimensional data
to a factor of 1.2x with 100 dimensional data.

!xt!cr!!xt−!c!−r!!xt−!xi!!xi!xj!!xt−!xj!SFast Solvers and Eﬃcient Implementations for Distance Metric Learning

we compute its squared distance to a training input (cid:126)xi
in partition αi as:

((cid:126)xt, (cid:126)xi) = ((cid:126)xt − (cid:126)xi)(cid:62)Mαi((cid:126)xt − (cid:126)xi).

d2
Mαi

(8)

These distances are then sorted as usual to determine
nearest neighbors and label the test input. Note, how-
ever, how diﬀerent distance metrics are used for train-
ing inputs in diﬀerent partitions.
We can also use these metrics to compute distances
between training inputs, with one important caveat.
Note that for inputs belonging to diﬀerent partitions,
the distance between them will depend on the par-
ticular metric used to compute it. This asymmetry
does not present any inherent diﬃculty since, in fact,
the dissimilarity measure in kNN classiﬁcation is not
required to be symmetric. Thus, even on the train-
ing set, we can use multiple metrics to measure dis-
tances and compute meaningful leave-one-out kNN er-
ror rates.

5.1. Learning algorithm

In this section we describe how to learn multiple Ma-
halanobis distance metrics for LMNN classiﬁcation.
Each of these metrics is associated with a particular
cluster of training examples. To derive these clusters,
we experimented with both unsupervised methods,
such as the k-means algorithm, and fully supervised
methods, in which each cluster contains the training
examples belonging to a particular class.
Before providing details of the learning algorithm, we
make the following important observation. Multiple
Mahalanobis distance metrics for LMNN classiﬁcation
cannot be learned in a decoupled fashion—that is, by
solving a collection of simpler, independent problems
of the type already considered (e.g., one within each
partition of training examples). Rather, the metrics
must be learned in a coordinated fashion so that the
distances from diﬀerent metrics can be meaningfully
compared for kNN classiﬁcation.
In our framework,
such comparisons arise whenever an unlabeled test ex-
ample has potential nearest neighbors in two or more
diﬀerent clusters of training examples.
Our learning algorithm for multiple local distance met-
rics {Mα}p
α=1 generalizes the semideﬁnite program for
ordinary LMNN classiﬁcation in section 2. First, we
modify the objective function so that the distances
to target neighbors (cid:126)xj are measured under the met-
ric Mαj . Next, we modify the large margin constraints
in (a) so that the distances to potential impostors (cid:126)xl
are measured under the metric Mαl. Finally, we re-
place the single positive semideﬁnite constraint in (c)
by multiple such constraints, one for each local met-

Figure 2. Graph of kNN error rate (with k = 3) on diﬀerent
low dimensional representations of the MNIST data set.

the results from further dimensionality reduction us-
ing PCA, as well as the baseline kNN error rate in
the original (high dimensional) space of raw images.
The speedup from ball trees is shown at the top of the
graph. The amount of speedup depends signiﬁcantly
on the amount of dimensionality reduction, but very
little on the particular method of dimensionality re-
duction. Of the three methods compared in the ﬁgure,
LMNN-RECT is the most eﬀective, improving signif-
icantly over baseline kNN classiﬁcation while operat-
ing in a much lower dimensional space. Overall, these
results show that aggressive dimensionality reduction
can be combined with distance metric learning.

5. Multiple Metrics

The originally proposed framework for LMNN clas-
siﬁcation has one clear limitation: the same Maha-
lanobis distance metric is used to compute distances
everywhere in the input space. Writing the metric
as M = L(cid:62)L, we see that Mahalanobis distances are
equivalent to Euclidean distances after a global lin-
ear transformation (cid:126)x → L(cid:126)x of the input space. Such a
transformation cannot adapt to nonlinear variabilities
in the training data.
In this section, we describe how to learn diﬀerent Ma-
halanobis distance metrics in diﬀerent parts of the in-
put space. We begin by simply describing how such a
collection of local distance metrics is used at test time.
Assume that the data set is divided into p disjoint par-
titions {Pα}p
α=1, such that Pα∩ Pβ = {} for any α (cid:54)= β
i=1. Also assume that each parti-
tion Pα has its own Mahalanobis distance metric Mα
for use in kNN classiﬁcation. Given a test vector (cid:126)xt,

α Pα = {(cid:126)xi}n

and (cid:83)

1.801.791.821.762.092.381.522.533.54152025303540PCALDALMNN-SVDLMNN-RECTBASELINEInput DimensionalityClassification Error in %9x6x5x4x3x(2.33)3-NN classification after dimensionality reductionball tree speedup15xFast Solvers and Eﬃcient Implementations for Distance Metric Learning

Figure 3. Visualization of multiple local distance metrics
for MNIST handwritten digits. See text for details.

ric Mα. Taken together, these steps lead to the new
semideﬁnite program:

Minimize (cid:80)

subject to:

Mαl

(a) d2
(b) ξijl ≥ 0
(c) Mα (cid:23) 0.

j(cid:32)i

d2
Mαj
((cid:126)xi, (cid:126)xl) − d2

Mαj

(cid:104)

((cid:126)xi, (cid:126)xj) + µ(cid:80)

(cid:105)

l(1 − yil)ξijl

((cid:126)xi, (cid:126)xj) ≥ 1 − ξijl

Note how the new constraints in (a) couple the dif-
ferent Mahalanobis distance metrics. By jointly op-
timizing these metrics to minimize a single objective
function, we ensure that the distances they compute
can be meaningfully compared for kNN classiﬁcation.

5.2. Results

We evaluated the performance of this approach on ﬁve
publicly available data sets:
the MNIST data set1
of handwritten digits (n = 60000, c = 10), the 20-
Newsgroups data set2 of text messages (n = 18827,
c = 20), the Letters data set3 of distorted computer
fonts (n=14000, c=26), the Isolet data set4 of spoken
letters (n = 6238, c = 26), and the YaleFaces5 data set
of face images (n = 1690, c = 38). The data sets were
preprocessed by PCA to reduce their dimensionality.
The amount of dimensionality reduction varied with
each experiment, as discussed below.
To start, we sought to visualize the multiple metrics
learned in a simple experiment on MNIST handwritten
digits of zeros, ones, twos, and fours. For ease of visu-

1http://yann.lecun.com/exdb/mnist/
2http://people.csail.mit.edu/jrennie/20Newsgroups
3http://www.ics.uci.edu/∼mlearn/databases/letter-

recognition/letter-recognition.names

4http://archive.ics.uci.edu/ml/
5http://cvc.yale.edu/projects/yalefacesB/yalefacesB.html

Figure 4. Test kNN error rates on the Isolet and MNIST
data sets as a function of the number of distance metrics.

alization, we worked with only the leading two princi-
pal components of the MNIST data set. Fig. 3 shows
these two dimensional inputs, color-coded by class la-
bel. With these easily visualized inputs, we minimized
the objective function in section 5.1 to learn a special-
ized distance metric for each type of handwritten digit.
The ellipsoids in the plot reveal the directions ampli-
ﬁed by the local distance metric of each digit class.
Notably, each distance metric learns to amplify the di-
rection perpendicular to the decision boundary for the
nearest, competing class of digits.
Our next experiments examined the performance of
LMNN classiﬁcation as a function of the number of dis-
tance metrics. In these experiments, we used PCA to
reduce the input dimensionality to d=50; we also only
worked with a subset of n = 10000 training examples
of MNIST handwritten digits. To avoid overﬁtting,
we used an “early stopping” approach while monitor-
ing the kNN error rates on a held-out validation set
consisting of 30% of the training data.
Fig. 4 shows the test kNN error rates on the Isolet
and MNIST data sets as a function of the number of
distance metrics.
In these experiments, we explored
both unsupervised and supervised methods for parti-
tioning the training inputs as a precursor to learning
local distance metrics. In the unsupervised setting, the
training examples were partitioned by k-means cluster-
ing, with the number of clusters ranging from 1 to 30
(just 1 cluster is identical to single-matrix LMNN). As
k-means clustering is prone to local minima, we aver-
aged these results over 100 runs. The ﬁgure shows the
average test error rates in red, as well as their stan-
dard deviations (via error bars). In the supervised set-
ting, the training examples were partitioned by their
class labels, resulting in the same number of clusters
as classes. The test error rates in these experiments
are shown as blue crosses. In both the unsupervised
and supervised settings, the test error rates decreased
with the use of multiple metrics. However, the im-
provements were far greater in the supervised setting.

1510152025305.566.571510152025302.22.42.62.833.23.4% classiﬁcation errorNumber of clustersNumber of clusters51015203456789ISOLET  510152022.22.42.62.833.2MNISTmeanstdstdstd1 metric/class1 global metric51015203456789ISOLET  510152022.22.42.62.833.2MNISTmeanstdstdstd1 metric/class1 global metric51015203456789ISOLET  510152022.22.42.62.833.2MNISTmeanst. deviationstdstd1 metric/class1 global metric1510152025305.566.57ISOLET1510152025302.22.42.62.833.23.4MNISTMNISTISOLETFast Solvers and Eﬃcient Implementations for Distance Metric Learning

Figure 5. The classiﬁcation train- and testerror rates with
one metric (LMNN) and multiple metrics. The value of k
was set by cross validation.

Finally, our last experiments explored the improve-
ment in kNN error rates when one distance metric
was learned for the training examples in each class. In
these experiments, we used the full number of train-
ing examples for each data set. In addition, we used
PCA to project the training inputs into a lower di-
mensional subspace accounting for at least 95% of the
data’s total variance. Fig. 5 shows generally consis-
tent improvement in training and test kNN error rates,
though overﬁtting is an issue, especially on the 20-
NewsGroups and YaleFaces data sets. This overﬁtting
is to be expected from the relatively large number of
classes and high input dimensionality of these data
sets: the number of model parameters in these exper-
iments grows linearly in the former and quadratically
in the latter. On these data sets, only the use of a
validation set prevents the training error from vanish-
ing completely while the test error skyrockets. On the
other hand, a signiﬁcant improvement in the test er-
ror rate is observed on the largest data set, that of
MNIST handwritten digits. On this data set, multiple
distance metrics yield a 1.18% test error rate—a highly
competitive result for a method that does not take into
account prior domain knowledge (LeCun et al., 1998).

6. Discussion

In this paper, we have extended the original framework
for LMNN classiﬁcation in several important ways: by
describing a solver that scales well to larger data sets,
by integrating metric ball trees into the training and
testing procedures, by exploring the use of dimension-
ality reduction for further speedups, and by showing
how to train diﬀerent Mahalanobis distance metrics
in diﬀerent parts of the input space. These exten-
sions should prove useful in many applications of kNN
classiﬁcation. More generally, we also hope they spur
further work on problems in distance metric learning
and large-scale semideﬁnite programming, both areas
of great interest in the larger ﬁeld of machine learning.

Acknowledgments

This research is based upon work supported by the Na-
tional Science Foundation under Grant No. 0238323.

References

Borchers, B. (1999). CSDP, a C library for semideﬁ-
nite programming. Optimization Methods and Software
11(1):613-623.

Boyd, S., & Vandenberghe, L. (2004). Convex optimization.

Cambridge University Press.

Chopra, S., Hadsell, R., & LeCun, Y. (2005). Learning
a similiarty metric discriminatively, with application to
face veriﬁcation. Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR-05).
San Diego, CA.

Davis, J., Kulis, B., Jain, P., Sra, S., & Dhillon, I. (2007).
Information-theoretic metric learning. Proceedings of
the 24th International Conference on Machine Learning
(ICML-07). Corvallis, OR.

Frome, A., Singer, Y., Sha, F., & Malik, J. (2007). Learn-
ing globally-consistent local distance functions for shape-
based image retrieval and classiﬁcation. Proceedings of
the Eleventh IEEE International Conference on Com-
puter Vision (ICCV-07). Rio de Janeiro, Brazil.

Goldberger, J., Roweis, S., Hinton, G., & Salakhutdi-
nov, R. (2005). Neighbourhood components analysis.
Advances in Neural Information Processing Systems 17
(pp. 513–520). Cambridge, MA: MIT Press.

Hastie, T., & Tibshirani, R. (1996). Discriminant adaptive
nearest neighbor classiﬁcation. IEEE Transactions on
Pattern Analysis and Machine Intelligence (PAMI), 18,
607–616.

LeCun, Y., Bottou, L., Bengio, Y., & Haﬀner, P. (1998).
Gradient-based learning applied to document recogni-
tion. Proceedings of the IEEE, 86(11), 2278–2324.

Liu, T., Moore, A. W., Gray, A., & Yang, K. (2005). An
investigation of practical approximate nearest neighbor
algorithms. In L. K. Saul, Y. Weiss and L. Bottou (Eds.),
Advances in neural information processing systems 17,
825–832. Cambridge, MA: MIT Press.

Sch¨olkopf, B., & Smola, A. J. (2002). Learning with ker-
nels: Support vector machines, regularization, optimiza-
tion, and beyond. Cambridge, MA: MIT Press.

Simard, P. Y., LeCun, Y., & Decker, J. (1993). Eﬃcient
pattern recognition using a new transformation distance.
Advances in Neural Information Processing Systems (pp.
50–58). San Mateo, CA: Morgan Kaufman.

Torresani, L., & Lee, K. (2007). Large margin compo-
nent analysis. In B. Sch¨olkopf, J. Platt and T. Hofmann
(Eds.), Advances in neural information processing sys-
tems 19. Cambridge, MA: MIT Press.

Weinberger, K., Blitzer, J., & Saul, L. (2006). Distance
metric learning for large margin nearest neighbor classi-
ﬁcation. In Y. Weiss, B. Sch¨olkopf and J. Platt (Eds.),
Advances in neural information processing systems 18.
Cambridge, MA: MIT Press.

Xing, E. P., Ng, A. Y., Jordan, M. I., & Russell, S. (2002).
Distance metric learning, with application to clustering
with side-information. Advances in Neural Information
Processing Systems 14. Cambridge, MA: MIT Press.

All in-text references underlined in blue are linked to publications on ResearchGate, letting you access and read them immediately.

traintestError in %mnist20newslettersisoletyalefacesLMNN1.7214.913.623.596.48Multiple Metrics1.1813.663.23.086.4LMNN1.199.733.540.73.54Multiple Metrics0.047.081.5503.57