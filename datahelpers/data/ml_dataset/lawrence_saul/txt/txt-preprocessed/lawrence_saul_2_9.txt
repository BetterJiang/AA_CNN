Abstract

We introduce a new family of positive-denite kernel functions that mimic the
computation in large, multilayer neural nets. These kernel functions can be used
in shallow architectures, such as support vector machines (SVMs), or in deep
kernel-based architectures that we call multilayer kernel machines (MKMs). We
evaluate SVMs and MKMs with these kernel functions on problems designed to
illustrate the advantages of deep architectures. On several problems, we obtain
better results than previous, leading benchmarks from both SVMs with Gaussian
kernels as well as deep belief nets.

1

Introduction

Recent work in machine learning has highlighted the circumstances that appear to favor deep archi-
tectures, such as multilayer neural nets, over shallow architectures, such as support vector machines
(SVMs) [1]. Deep architectures learn complex mappings by transforming their inputs through mul-
tiple layers of nonlinear processing [2]. Researchers have advanced several motivations for deep
architectures: the wide range of functions that can be parameterized by composing weakly non-
linear transformations, the appeal of hierarchical distributed representations, and the potential for
combining unsupervised and supervised methods. Experiments have also shown the benets of
deep learning in several interesting applications [3, 4, 5].
Many issues surround the ongoing debate over deep versus shallow architectures [1, 6]. Deep ar-
chitectures are generally more difcult to train than shallow ones. They involve difcult nonlinear
optimizations and many heuristics. The challenges of deep learning explain the early and continued
appeal of SVMs, which learn nonlinear classiers via the kernel trick. Unlike deep architectures,
SVMs are trained by solving a simple problem in quadratic programming. However, SVMs cannot
seemingly benet from the advantages of deep learning.
Like many, we are intrigued by the successes of deep architectures yet drawn to the elegance of ker-
nel methods. In this paper, we explore the possibility of deep learning in kernel machines. Though
we share a similar motivation as previous authors [7], our approach is very different. Our paper
makes two main contributions. First, we develop a new family of kernel functions that mimic the
computation in large neural nets. Second, using these kernel functions, we show how to train multi-
layer kernel machines (MKMs) that benet from many advantages of deep learning.
The organization of this paper is as follows.
In section 2, we describe a new family of kernel
functions and experiment with their use in SVMs. Our results on SVMs are interesting in their own
right; they also foreshadow certain trends that we observe (and certain choices that we make) for the
MKMs introduced in section 3. In this section, we describe a kernel-based architecture with multiple
layers of nonlinear transformation. The different layers are trained using a simple combination of
supervised and unsupervised methods. Finally, we conclude in section 4 by evaluating the strengths
and weaknesses of our approach.

1

2 Arc-cosine kernels

In this section, we develop a new family of kernel functions for computing the similarity of vector
inputs x, y  (cid:60)d. As shorthand, let (z) = 1
2(1 + sign(z)) denote the Heaviside step function. We
dene the nth order arc-cosine kernel function via the integral representation:

kn(x, y) = 2

dw e (cid:107)w(cid:107)2
(2)d/2

2

(w  x) (w  y) (w  x)n (w  y)n

(1)

(cid:90)

The integral representation makes it straightforward to show that these kernel functions are positive-
semidenite. The kernel function in eq. (1) has interesting connections to neural computation [8]
that we explore further in sections 2.22.3. However, we begin by elucidating its basic properties.

2.1 Basic properties

We show how to evaluate the integral in eq. (1) analytically in the appendix. The nal result is most
easily expressed in terms of the angle  between the inputs:

(cid:19)

(cid:18) x  y

(cid:107)x(cid:107)(cid:107)y(cid:107)

 = cos1

.

(2)

The integral in eq. (1) has a simple, trivial dependence on the magnitudes of the inputs x and y, but
a complex, interesting dependence on the angle between them. In particular, we can write:

kn(x, y) =

(cid:107)x(cid:107)n(cid:107)y(cid:107)nJn()

1


(3)

where all the angular dependence is captured by the family of functions Jn(). Evaluating the
integral in the appendix, we show that this angular dependence is given by:

Jn() = (1)n(sin )2n+1

.

(4)

(cid:18) 1




sin 

(cid:19)n(cid:18)   

(cid:19)

sin 

For n = 0, this expression reduces to the supplement of the angle between the inputs. However, for
n >0, the angular dependence is more complicated. The rst few expressions are:

J0() =   
J1() = sin  + (  ) cos 
J2() = 3 sin  cos  + (  )(1 + 2 cos2 )

(5)
(6)
(7)

 cos1 xy

We describe eq. (3) as an arc-cosine kernel because for n = 0,
it takes the simple form
k0(x, y) = 1 1
(cid:107)x(cid:107)(cid:107)y(cid:107). In fact, the zeroth and rst order kernels in this family are strongly
motivated by previous work in neural computation. We explore these connections in the next section.
Arc-cosine kernels have other intriguing properties. From the magnitude dependence in eq. (3),
we observe the following: (i) the n = 0 arc-cosine kernel maps inputs x to the unit hypersphere
in feature space, with k0(x, x) = 1; (ii) the n = 1 arc-cosine kernel preserves the norm of inputs,
with k1(x, x) = (cid:107)x(cid:107)2; (iii) higher order (n >1) arc-cosine kernels expand the dynamic range of the
inputs, with kn(x, x)  (cid:107)x(cid:107)2n. Properties (i)(iii) are shared respectively by radial basis function
(RBF), linear, and polynomial kernels. Interestingly, though, the n = 1 arc-cosine kernel is highly
nonlinear, also satisfying k1(x,x) = 0 for all inputs x. As a practical matter, we note that arc-
cosine kernels do not have any continuous tuning parameters (such as the kernel width in RBF
kernels), which can be laborious to set by cross-validation.

2.2 Computation in single-layer threshold networks

Consider the single-layer network shown in Fig. 1 (left) whose weights Wij connect the jth input
unit to the ith output unit. The network maps inputs x to outputs f(x) by applying an elementwise
nonlinearity to the matrix-vector product of the inputs and the weight matrix: f(x) = g(Wx). The
nonlinearity is described by the networks so-called activation function. Here we consider the family
of one-sided polynomial activation functions gn(z) = (z)zn illustrated in the right panel of Fig. 1.

2

Figure 1: Single layer network and activation functions

m(cid:88)

For n = 0, the activation function is a step function, and the network is an array of perceptrons. For
n = 1, the activation function is a ramp function (or rectication nonlinearity [9]), and the mapping
f(x) is piecewise linear. More generally, the nonlinear (non-polynomial) behavior of these networks
is induced by thresholding on weighted sums. We refer to networks with these activation functions
as single-layer threshold networks of degree n.
Computation in these networks is closely connected to computation with the arc-cosine kernel func-
tion in eq. (1). To see the connection, consider how inner products are transformed by the mapping
in single-layer threshold networks. As notation, let the vector wi denote ith row of the weight
matrix W. Then we can express the inner product between different outputs of the network as:

f(x)  f(y) =

(wi  x)(wi  y)(wi  x)n(wi  y)n,

(8)

i=1

where m is the number of output units. The connection with the arc-cosine kernel function emerges
in the limit of very large networks [10, 8].
Imagine that the network has an innite number of
output units, and that the weights Wij are Gaussian distributed with zero mean and unit vari-
In this limit, we see that eq. (8) reduces to eq. (1) up to a trivial multiplicative factor:
ance.
m f(x)  f(y) = kn(x, y). Thus the arc-cosine kernel function in eq. (1) can be viewed
limm 2
as the inner product between feature vectors derived from the mapping of an innite single-layer
threshold network [8].
Many researchers have noted the general connection between kernel machines and neural networks
with one layer of hidden units [1]. The n = 0 arc-cosine kernel in eq. (1) can also be derived from
an earlier result obtained in the context of Gaussian processes [8]. However, we are unaware of any
previous theoretical or empirical work on the general family of these kernels for degrees n0.
Arc-cosine kernels differ from polynomial and RBF kernels in one especially interesting respect.
As highlighted by the integral representation in eq. (1), arc-cosine kernels induce feature spaces
that mimic the sparse, nonnegative, distributed representations of single-layer threshold networks.
Polynomial and RBF kernels do not encode their inputs in this way. In particular, the feature vector
induced by polynomial kernels is neither sparse nor nonnegative, while the feature vector induced
by RBF kernels resembles the localized output of a soft vector quantizer. Further implications of
this difference are explored in the next section.

2.3 Computation in multilayer threshold networks

A kernel function can be viewed as inducing a nonlinear mapping from inputs x to fea-
ture vectors (x).
in the induced feature space:
k(x, y) = (x)(y). In this section, we consider how to compose the nonlinear mappings in-
duced by kernel functions. Specically, we show how to derive new kernel functions

The kernel computes the inner product

k((cid:96))(x, y) = ((...

(y)))

(9)

(cid:124)

(cid:123)(cid:122)

(cid:125)

(cid:96) times

(cid:123)(cid:122)

(x)))  ((...
(cid:96) times

(cid:125)

(cid:124)

which compute the inner product after (cid:96) successive applications of the nonlinear mapping (). Our
motivation is the following: intuitively, if the base kernel function k(x, y) = (x)  (y) mimics
the computation in a single-layer network, then the iterated mapping in eq. (9) should mimic the
computation in a multilayer network.

3

f2f3fix1x2xj. . . . . . f1fmxdW. . . . . . 10100.51Step (n=0)10100.51Ramp (n=1)10100.51Quarterpipe (n=2)Figure 2: Left: examples from the rectangles-image data set. Right: classication error rates on the
test set. SVMs with arc-cosine kernels have error rates from 22.3625.64%. Results are shown for
kernels of varying degree (n) and levels of recursion ((cid:96)). The best previous results are 24.04% for
SVMs with RBF kernels and 22.50% for deep belief nets [11]. See text for details.

We rst examine the results of this procedure for widely used kernels. Here we nd that the iterated
mapping in eq. (9) does not yield particularly interesting results. Consider the two-fold composition
that maps x to ((x)). For linear kernels k(x, y) = x  y, the composition is trivial: we obtain
the identity map ((x)) = (x) = x. For homogeneous polynomial kernels k(x, y) = (x  y)d,
the composition yields:

((x))  ((y)) = ((x)  (y))d = ((x  y)d)d = (x  y)d2

.

(10)

The above result is not especially interesting: the kernel implied by this composition is also polyno-
mial, just of higher degree (d2 versus d) than the one from which it was constructed. Likewise, for
RBF kernels k(x, y) = e(cid:107)xy(cid:107)2, the composition yields:

((x))  ((y)) = e(cid:107)(x)(y)(cid:107)2 = e2(1k(x,y)).

(11)

Though non-trivial, eq. (11) does not represent a particularly interesting computation. Recall that
RBF kernels mimic the computation of soft vector quantizers, with k(x, y) (cid:28) 1 when (cid:107)xy(cid:107) is
large compared to the kernel width. It is hard to see how the iterated mapping ((x)) would
generate a qualitatively different representation than the original mapping (x).
Next we consider the (cid:96)-fold composition in eq. (9) for arc-cosine kernel functions. We state the
result in the form of a recursion. The base case is given by eq. (3) for kernels of depth (cid:96) = 1 and
degree n. The inductive step is given by:

k(l+1)
n

(x, y) =

1


n (x, x) k(l)
k(l)

n (y, y)

Jn

((cid:96))
n

,

(12)

where ((cid:96))
composition. In particular, we can write:

n is the angle between the images of x and y in the feature space induced by the (cid:96)-fold

n = cos1
((cid:96))

n (x, y)
k((cid:96))

n (x, x) k((cid:96))
k((cid:96))

n (y, y)

.

(13)

The recursion in eq. (12) is simple to compute in practice. The resulting kernels mimic the com-
putations in large multilayer threshold networks. Above, for simplicity, we have assumed that the
arc-cosine kernels have the same degree n at every level (or layer) (cid:96) of the recursion. We can also
use kernels of different degrees at different layers. In the next section, we experiment with SVMs
whose kernel functions are constructed in this way.

2.4 Experiments on binary classication
We evaluated SVMs with arc-cosine kernels on two challenging data sets of 28  28 grayscale pixel
images. These data sets were specically constructed to compare deep architectures and kernel
machines [11]. In the rst data set, known as rectangles-image, each image contains an occluding
rectangle, and the task is to determine whether the width of the rectangle exceeds its height; ex-
amples are shown in Fig. 2 (left). In the second data set, known as convex, each image contains a
white region, and the task is to determine whether the white region is convex; examples are shown

4

(cid:104)

(cid:18)

(cid:104)

(cid:105)n/2

(cid:16)

(cid:17)

(cid:105)1/2(cid:19)

