Abstract
Weproposeastatisticalmechanicalframeworkforthemodeling
ofdiscretetimeseries.Maximumlikelihoodestimationisdonevia
Boltzmannlearninginone-dimensionalnetworkswithtiedweights.
WecallthesenetworksBoltzmannchainsandshowthatthey
containhiddenMarkovmodels(HMMs)asaspecialcase.Our
frameworkalsomotivatesnewarchitecturesthataddresspartic-
ularshortcomingsofHMMs.Welookattwosucharchitectures:
parallelchainsthatmodelfeaturesetswithdisparatetimescales,
andloopednetworksthatmodellong-termdependenciesbetween
hiddenstates.Forthesenetworks,weshowhowtoimplement
theBoltzmannlearningruleexactly,inpolynomialtime,without
resorttosimulatedormean-(cid:12)eldannealing.Thenecessarycom-
putationsaredonebyexactdecimationproceduresfromstatistical
mechanics.
INTRODUCTIONANDSUMMARY
Statisticalmodelsofdiscretetimeserieshaveawiderangeofapplications,most
notablytoproblemsinspeechrecognition(Juang&Rabiner,		)andmolecular
biology(Baldi,Chauvin,Hunkapiller,&McClure,		).Acommonproblemin
these(cid:12)eldsisto(cid:12)ndaprobabilisticmodel,andasetofmodelparameters,that

accountforsequencesofobserveddata.HiddenMarkovmodels(HMMs)havebeen
particularlysuccessfulatmodelingdiscretetimeseries.Onereasonforthisisthe
powerfullearningrule(Baum,	),aspecialcaseoftheExpectation-Maximization
(EM)procedureformaximumlikelihoodestimation(Dempster,Laird,&Rubin,
	).
Inthiswork,wedevelopastatisticalmechanicalframeworkforthemodelingof
discretetimeseries.TheframeworkenablesustorelateHMMstoalargefamily
ofexactlysolvablemodelsinstatisticalmechanics.Theconnectiontostatistical
mechanicswas(cid:12)rstnoticedbySourlas(		),whostudiedspinglassmodelsof
error-correctingcodes.WeviewtheestimationprocedureforHMMsasaspecial
(andparticularlytractable)caseoftheBoltzmannlearningrule(Ackley,Hinton,&
Sejnowski,	;Byrne,		).
Therestofthispaperisorganizedasfollows.InSection,wereviewthemodeling
problemfordiscretetimeseriesandestablishtheconnectionbetweenHMMsand
Boltzmannmachines.
InSection,weshowhowtoquicklydeterminewhether
ornotaparticularBoltzmannmachineistractable,andifso,howtoe(cid:14)ciently
computethecorrelationsintheBoltzmannlearningrule.Finally,inSection,
welookattwoarchitecturesthataddressparticularweaknessesofHMMs:the
modellingofdisparatetimescalesandlong-termdependencies.
MODELINGDISCRETETIMESERIES
Adiscretetimeseriesisasequenceofsymbolsfj`gL`=inwhicheachsymbolbelongs
toa(cid:12)nitecountableset,i.e.j`f;;:::;mg.Givenonelongsequence,orperhaps
manyshorterones,themodelingtaskistocharacterizetheprobabilitydistribution
fromwhichthetimeseriesaregenerated.
.HIDDENMARKOVMODELS
A(cid:12)rst-orderHiddenMarkovModel(HMM)ischaracterizedbyasetofnhidden
states,analphabetofmsymbols,atransmissionmatrixaii,anemissionmatrix
bij,andapriordistribution(cid:25)iovertheinitialhiddenstate.Thesequenceofstates
fi`gL`=andsymbolsfj`gL`=ismodeledtooccurwithprobability
P(fi`;j`g)=(cid:25)iaiiaii:::aiL(cid:0)iLbijbij:::biLjL:
()
Themodelingproblemisto(cid:12)ndtheparametervalues(aii;bij;(cid:25)i)thatmaximize
thelikelihoodofobservedsequencesoftrainingdata.Wewillelaborateonthe
learningruleinsection.,but(cid:12)rstletusmaketheconnectiontoawell-known
familyofstochasticneuralnetworks,namelyBoltzmannmachines.
.BOLTZMANNMACHINES
ConsideraBoltzmannmachinewithm-statevisibleunits,n-statehiddenunits,tied
weights,andthelineararchitectureshowninFigure.Thisexamplerepresentsthe
simplestpossibleBoltzmann\chain",onethatisessentiallyequivalenttoa(cid:12)rst-
orderHMMunfoldedintime(MacKay,		).ThetransitionweightsAiiconnect
adjacenthiddenunits,whiletheemissionweightsBijconnecteachhiddenunitto

i

...

Aii Aii

hidden
units

Bij Bij

visible

units

Bij Bij Bij

Aii Aii Aii

Figure:Boltzmannchainwithn-statehiddenunits,m-statevisibleunits,transi-
tionweightsAii,emissionweightsBij,andboundaryweights(cid:5)i.
itsvisiblecounterpart.Inaddition,boundaryweights(cid:5)imodelanextrabiason
the(cid:12)rsthiddenunit.Eachcon(cid:12)gurationofunitsrepresentsastateofenergy
H[fi`;j`g]=(cid:0)(cid:5)i(cid:0)L(cid:0)X`=Ai`i`+(cid:0)LX`=Bi`j`;
()
wherefi`gLl=(fj`gLl=)isthesequenceofstatesoverthehidden(visible)units.The
probabilityto(cid:12)ndthenetworkinaparticularcon(cid:12)gurationisgivenby
P(fi`;j`g)=Ze(cid:0)(cid:12)H;
()
where(cid:12)==Tistheinversetemperature,andthepartitionfunction
Z=Xfi`;j`ge(cid:0)(cid:12)H
()
isthesumoverstatesthatnormalizestheBoltzmanndistribution,eq.().
ComparingthistotheHMMdistribution,eq.(),itisclearthatany(cid:12)rst-order
HMMcanberepresentedbytheBoltzmannchainof(cid:12)gure,providedwetake
Aii=Tlnaii;Bij=Tlnbij;(cid:5)i=Tln(cid:25)i:
()
Later,inSection,wewillconsidermorecomplicatedchainswhosearchitectures
addressparticularshortcomingsofHMMs.Fornow,however,letuscontinueto
developtheexampleof(cid:12)gure,makingexplicittheconnectiontoHMMs.
.LEARNINGRULES
IntheframeworkofBoltzmannlearning(Williams&Hinton,		),thedatafor
ourproblemconsistofsequencesofstatesoverthevisibleunits;thegoalisto(cid:12)nd
theweights(Aii;Bij;(cid:5)i)thatmaximizethelikelihoodoftheobserveddata.The
likelihoodofasequencefj`gisgivenbytheratio
P(fi`gjfj`g)=e(cid:0)(cid:12)H=Z
P(fj`g)=P(fi`;j`g)
e(cid:0)(cid:12)H=Zc=ZcZ;
()
Note,however,thatthereversestatement|thatforanysetofparameters,thisBoltz-
mannchaincanberepresentedasanHMM|isnottrue.TheweightsintheBoltzmann
chainrepresentarbitraryenergiesbetween(cid:6),whereastheHMMparametersrepresent
probabilitiesthatareconstrainedtoobeysumrules,suchasPiaii=.TheBoltzmann
chainof(cid:12)gurethereforehasslightlymoredegreesoffreedomthana(cid:12)rst-orderHMM.
AninterpretationoftheseextradegreesoffreedomisgivenbyMacKay(		).

(cid:213)
whereZcistheclampedpartitionfunction
Zc=Xfi`ge(cid:0)(cid:12)H:
()
NotethatthesuminZcisonlyoverthehiddenstatesinthenetwork,whilethe
visiblestatesareclampedtotheobservedvaluesfj`g.
TheBoltzmannlearningruleadjuststheweightsofthenetworkbygradient-ascent
onthelog-likelihood.Fortheexampleof(cid:12)gure,thisleadstoweightupdates
(cid:1)Aii=(cid:17)(cid:12)L(cid:0)X`=(cid:2)h(cid:14)ii`(cid:14)ii`+ic(cid:0)h(cid:14)ii`(cid:14)ii`+i(cid:3);
()
(cid:1)Bij=(cid:17)(cid:12)LX`=[h(cid:14)ii`(cid:14)jj`ic(cid:0)h(cid:14)ii`(cid:14)jj`i];
(	)
()
(cid:1)(cid:5)i=(cid:17)(cid:12)[h(cid:14)iiic(cid:0)h(cid:14)iii];
where(cid:14)ijstandsfortheKroneckerdeltafunction,(cid:17)isalearningrate,andh(cid:1)iand
h(cid:1)icdenoteexpectationsoverthefreeandclampedBoltzmanndistributions.
TheBoltzmannlearningrulemayalsobederivedasanExpectation{Maximization
(EM)algorithm.TheEMprocedureisanalternatingtwo-stepmethodformax-
imumlikelihoodestimationinprobabilitymodelswithhiddenandobservedvari-
ables.ForBoltzmannmachinesingeneral,neithertheE-stepnortheM-stepcan
bedoneexactly;onemustestimatethenecessarystatisticsbyMonteCarlosim-
ulation(Ackleyetal.,	)ormean-(cid:12)eldtheory(Peterson&Anderson,	).
Incertainspecialcases(e.g.treesandchains),however,thenecessarystatistics
canbecomputedtoperformanexactE-step(asshownbelow).WhiletheM-
stepintheseBoltzmannmachinescannotbedoneexactly,theweightupdatescan
beapproximatedbygradientdescent.Thisleadstolearningrulesintheformof
eqs.({).
HMMsmaybeviewedasaspecialcaseofBoltzmannchainsforwhichboththe
E-stepandtheM-stepareanalyticallytractable.Inthiscase,themaximizationin
theM-stepisperformedsubjecttotheconstraintsPie(cid:12)(cid:5)i=,Pie(cid:12)Aii=,and
Pje(cid:12)Bij=.TheseconstraintsimplyZ=andleadtoclosed-formequations
fortheweightupdatesinHMMs.
EXACTMETHODSFORBOLTZMANNLEARNING
ThekeytechniquetocomputepartitionfunctionsandcorrelationsinBoltzmann
chainsisknownasdecimation.Theideabehinddecimationisthefollowing.Con-
siderthreeunitsconnectedinseries,asshowninFigurea.Thoughnotdirectly
connected,theendunitshaveane(cid:11)ectiveinteractionthatismediatedbythemiddle
one.Infact,thetwoweightsinseriesexertthesamein(cid:13)uenceasasinglee(cid:11)ective
weight,givenby
e(cid:12)Aii=Xie(cid:12)A()ii+(cid:12)A()ii+(cid:12)Bi:
()
Arelatedmethod,thetransfermatrix,isdescribedbyStolorz(		).

Bij

(a)

(b)

=

(2)
Aii

(c)

(1)
Aii

=

Bi

(1)
Aii

Bi
(2)
Aii

= Aii

(1)
Aii
+
(2)
Aii

Figure:Decimation,pruning,andjoininginBoltzmannmachines.
Replacingtheweightsinthiswayamountstointegratingout,ordecimating,the
degreeoffreedomrepresentedbythemiddleunit.Ananalogousrulemaybederived
forthesituationshowninFigureb.Summingoverthedegreesoffreedomofthe
danglingunitgeneratesane(cid:11)ectivebiasonitsparent,givenby
e(cid:12)Bi=Xje(cid:12)Bij:
()
Wecallthisthepruningrule.AnothertypeofequivalenceisshowninFigurec.
Thetwoweightsinparallelhavethesamee(cid:11)ectasthesumtotalweight
Aii=A()ii+A()ii:
()
Wecallthisthejoiningrule.Itholdstriviallyforbiasesaswellasweights.
Therulesfordecimating,pruning,andjoininghavesimpleanalogsinothertypes
ofnetworks(e.g.thelawforcombiningresistorsinelectriccircuits),andthestrat-
egyforexploitingthemisafamiliarone.Startingwithacomplicatednetwork,
weiteratetherulesuntilwehaveasimplenetworkwhosepropertiesareeasily
computed.AnetworkistractableforBoltzmannlearningifitcanbereducedto
anypairofconnectedunits.Inthiscase,wemayusetherulestocomputeallthe
correlationsrequiredforBoltzmannlearning.Clearly,therulesdonotmakeallnet-
workstractable;certainnetworks(e.g.treesandchains),however,lendthemselves
naturallytothesetypesofoperations.
DESIGNERNETS
Therulesinsectioncanbeusedtoquicklyassesswhetherornotanetworkis
tractableforBoltzmannlearning.Conversely,theycanbeusedtodesignnetworks
thatarecomputationallytractable.Thissectionlooksattwonetworksdesignedto
addressparticularshortcomingsofHMMs.
.PARALLELCHAINSANDDISPARATETIMESCALES
Animportantprobleminspeechrecognition(Juangetal.,		)ishowto\combine
featuresetswithfundamentallydi(cid:11)erenttimescales."Spectralparameters,such

fast

features

slow

features

coupled
hidden
units

Figure:Coupledparallelchainsforfeatureswithdi(cid:11)erenttimescales.
asthecepstrumanddelta-cepstrum,varyonatimescaleofmsec;ontheother
hand,prosodicparameters,suchasthesignalenergyandpitch,varyonatimescale
ofmsec.Amodelthattakesintoaccountthisdisparityshouldavoidtwothings.
The(cid:12)rstisredundancy|inparticular,theratherlamesolutionofoversamplingthe
nonspectralfeatures.Thesecondisover(cid:12)tting.Howmightthisarise?Supposewe
havetrainedtwoseparateHMMsonsequencesofspectralandprosodicfeatures,
knowingthatthedi(cid:11)erentfeatures\maynotwarrantasingle,uni(cid:12)edMarkovchain"
(Juangetal.,		).Toexploitthecorrelationbetweenfeaturesets,wemustnow
couplethetwoHMMs.AnaivesolutionistoformtheCartesianproductoftheir
hiddenstatespacesandresumetraining.Unfortunately,thisresultsinanexplosion
inthenumberofparametersthatmustbe(cid:12)tfromthetrainingdata.Thelikely
consequencesareover(cid:12)ttingandpoorgeneralization.
Figureshowsanetworkformodelingfeaturesetswithdisparatetimescales|in
thiscase,a:disparity.TwoparallelBoltzmannchainsarecoupledbyweights
thatconnecttheirhiddenunits.Likethetransitionandemissionweightswithin
eachchain,thecouplingweightsaretiedacrossthelengthofthenetwork.Note
thatcouplingthetimescalesinthiswayintroducesfarfewerparametersthan
formingtheCartesianproductofthehiddenstatespaces.Moreover,thenetworkis
tractablebytherulesofsection.Suppose,forexample,thatwewishtocompute
thecorrelationbetweentwoneighboringhiddenunitsinthemiddleofthenetwork.
Thisisdoneby(cid:12)rstpruningallthevisibleunits,thenrepeatedlydecimatinghidden
unitsfrombothendsofthenetwork.
Figureshowstypicalresultsonasimplebenchmarkproblem,withdatagenerated
byanarti(cid:12)ciallyconstructedHMM.Wetestedtheparallelchainsmodelon
trainingsets,withvaryinglevelsofbuilt-incorrelationbetweenfeatures.Atwo-
stepmethodwasusedtotraintheparallelchains.First,wesetthecouplingweights
tozeroandtrainedeachchainbyaseparateBaum-Welchprocedure.Then,after
learninginthisphasewascomplete,weliftedthezeroconstraintsandresumed
trainingwiththefullBoltzmannlearningrule.Thepercentgaininthissecond
phasewasdirectlyrelatedtothedegreeofcorrelationbuiltintothetrainingdata,
suggestingthatthecouplingweightswereindeedcapturingthecorrelationbetween
featuresets.WealsocomparedtheperformanceofthisBoltzmannmachineversus
thatofasimpleCartesian-productHMMtrainedbyanadditionalBaum-Welch
procedure.Whileinbothcasesthesecondphaseoflearningledtoreducedtraining
error,theCartesianproductHMMsweredecidedlymorepronetoover(cid:12)tting.

0

600

800

0.2

0.4

0.6

0.8

1

400

200

-1500

0

0

30

20

10

-1600

d
o
o
h

i
l

e
k

i
l
-
g
o

l

-1700

(b)

i

n
a
g
%



feature correlation

epoch
(a)

training
cross-validation

training
cross-validation

Figure:(a)Log-likelihoodversusepochforparallelchainswith-statehidden
units,-statevisibleunits,andhidden{visibleunitpairs(perchain).The
secondjumpinlog-likelihoodoccurredattheonsetofBoltzmannlearning(see
text).(b)Percentgaininlog-likelihoodversusbuilt-incorrelationbetweenfeature
sets..LOOPSANDLONG-TERMDEPENDENCIES
Anothershortcomingof(cid:12)rst-orderHMMsisthattheycannotexhibitlong-term
dependenciesbetweenthehiddenstates(Juangetal.,		).Higher-orderand
duration-basedHMMshavebeenusedinthisregardwithvaryingdegreesofsuc-
cess.Therulesofsectionsuggestanotherapproach|namely,designingtractable
networkswithlimitedlong-rangeconnectivity.Asanexample,Figureashowsa
Boltzmannchainwithaninternalloopandalong-rangeconnectionbetweenthe
(cid:12)rstandlasthiddenunits.Theseextrafeaturescouldbeusedtoenforceknown
periodicitiesinthetimeseries.ThoughtractableforBoltzmannlearning,theloops
inthisnetworkdonot(cid:12)tnaturallyintotheframeworkofHMMs.Figurebshows
learningcurvesforatoyproblem,withdatageneratedbyanotherloopednetwork.
Carefullychosenloopsandlong-rangeconnectionsprovideadditional(cid:13)exibilityin
thedesignofprobabilisticmodelsfortimeseries.Cannetworkswiththeseextra
featurescapturethelong-termdependenciesexhibitedbyrealdata?Thisremains
animportantissueforfutureresearch.
Acknowledgements
WethankG.Hinton,D.MacKay,P.Stolorz,andC.Williamsforusefuldiscus-
sions.ThisworkwasfundedbyATRHumanInformationProcessingLaboratories,
SiemensCorporateResearch,andNSFgrantCDA-		.
