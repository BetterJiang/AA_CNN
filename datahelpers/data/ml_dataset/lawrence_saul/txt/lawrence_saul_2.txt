Departmental Papers (ESE)

Department of Electrical & Systems Engineering

University of Pennsylvania
ScholarlyCommons

2007

Multiplicative Updates for Nonnegative Quadratic
Programming
Fei Sha
University of California - Berkeley

Yuanqing Lin
University of Pennsylvania, linyuang@seas.upenn.edu

Lawrence K. Saul
University of California - San Diego

Daniel D. Lee
University of Pennsylvania, ddlee@seas.upenn.edu

Follow this and additional works at: http://repository.upenn.edu/ese_papers

Part of the Medicine and Health Sciences Commons

Recommended Citation
Fei Sha, Yuanqing Lin, Lawrence K. Saul, and Daniel D. Lee, "Multiplicative Updates for Nonnegative Quadratic Programming", .
January 2007.

Suggested Citation:
Sha, F., Lin, Y., Saul, L.K. and Lee, D.D. (2007). Multiplicative Updates for Nonnegative Quadratic Programming. Neural Computation. 19, 2004-2031.

© 2007 MIT Press
http://www.mitpressjournals.org/loi/neco

This paper is posted at ScholarlyCommons. http://repository.upenn.edu/ese_papers/597
For more information, please contact repository@pobox.upenn.edu.

Multiplicative Updates for Nonnegative Quadratic Programming

Abstract
Many problems in neural computation and statistical learning involve optimizations with nonnegativity
constraints. In this article, we study convex problems in quadratic programming where the optimization is
confined to an axis-aligned region in the nonnegative orthant. For these problems, we derive multiplicative
updates that improve the value of the objective function at each iteration and converge monotonically to the
global minimum. The updates have a simple closed form and do not involve any heuristics or free parameters
that must be tuned to ensure convergence. Despite their simplicity, they differ strikingly in form from other
multiplicative updates used in machine learning.We provide complete proofs of convergence for these updates
and describe their application to problems in signal processing and pattern recognition.

Disciplines
Medicine and Health Sciences

Comments
Suggested Citation:
Sha, F., Lin, Y., Saul, L.K. and Lee, D.D. (2007). Multiplicative Updates for Nonnegative Quadratic
Programming. Neural Computation. 19, 2004-2031.

© 2007 MIT Press
http://www.mitpressjournals.org/loi/neco

This journal article is available at ScholarlyCommons: http://repository.upenn.edu/ese_papers/597

ARTICLE

Communicated by Sebastian Seung

Multiplicative Updates for Nonnegative Quadratic
Programming

Fei Sha
feisha@cs.berkeley.edu
Computer Science Division, University of California, Berkeley,
Berkeley, CA 94720, U.S.A.

Yuanqing Lin
linyuanq@seas.upenn.edu
Department of Electrical and Systems Engineering, University of Pennsylvania,
Philadelphia, PA 19104, U.S.A.

Lawrence K. Saul
saul@cs.ucsd.edu
Department of Computer Science and Engineering, University of California,
San Diego, La Jolla, CA 92093, U.S.A.

Daniel D. Lee
ddlee@seas.upenn.edu
Department of Electrical and Systems Engineering, University of Pennsylvania,
Philadelphia, PA 19104, U.S.A.

Many problems in neural computation and statistical learning involve
optimizations with nonnegativity constraints. In this article, we study
convex problems in quadratic programming where the optimization is
conﬁned to an axis-aligned region in the nonnegative orthant. For these
problems, we derive multiplicative updates that improve the value of
the objective function at each iteration and converge monotonically to
the global minimum. The updates have a simple closed form and do
not involve any heuristics or free parameters that must be tuned to en-
sure convergence. Despite their simplicity, they differ strikingly in form
from other multiplicative updates used in machine learning. We provide
complete proofs of convergence for these updates and describe their ap-
plication to problems in signal processing and pattern recognition.

1 Introduction

Many problems in neural computation and statistical learning involve op-
timizations with nonnegativity constraints. Examples include large margin
classiﬁcation by support vector machines (Vapnik, 1998), density estimation

Neural Computation 19, 2004–2031 (2007)

C(cid:1) 2007 Massachusetts Institute of Technology

Multiplicative Updates for Nonnegative Quadratic Programming

2005

in Bayesian networks (Bauer, Koller, & Singer, 1997), dimensionality reduc-
tion by nonnegative matrix factorization (Lee & Seung, 1999), and acoustic
echo cancellation (Lin, Lee, & Saul, 2004). The optimizations for these prob-
lems cannot be solved in closed form; thus, iterative learning rules are
required that converge in the limit to actual solutions.

The simplest such learning rule is gradient descent. Minimizing an ob-

jective function F (v) by gradient descent involves the additive update,

vi ← vi − η(∂ F /∂vi),

(1.1)

where η >0 is a positive learning rate and all the elements of the parameter
vector v = (v1, v2, . . . , vN) are updated in parallel. Gradient descent is not
particularly well suited to constrained optimizations, however, because the
additive update in equation 1.1 can lead to violations of the constraints. A
simple extension enforces the nonnegativity constraints:

vi ← max(vi − η(∂ F /∂vi), 0).

(1.2)

The update rule in equation 1.2 is a special instance of gradient projection
methods (Bertsekas, 1999; Seraﬁni, Zanghirati, & Zanni, 2005). The nonneg-
ativity constraints are enforced by projecting the gradient-based updates
in equation 1.1 onto the convex feasible set—namely, the nonnegative or-
thant vi ≥ 0. The projected gradient updates also depend on a learning rate
parameter η.

For optimizations with nonnegativity constraints, an equally simple but
more appropriate learning rule involves the so-called exponentiated gradi-
ent (EG) (Kivinen & Warmuth, 1997):

vi ← vi e

−η(∂ F /∂vi ).

(1.3)

Equation 1.3 is an example of a multiplicative update. Because the elements
of the exponentiated gradient are always positive, this update naturally
enforces the nonnegativity constraints on vi. By taking the logarithm of both
sides of equation 1.3, we can view the EG update as an additive update1 in
the log domain:

log vi ← log vi − η(∂ F /∂vi).

(1.4)

Multiplicative updates such as EG typically lead to faster convergence
of the
than additive updates (Kivinen & Warmuth, 1997) if the solution v

∗

1 This update differs slightly from gradient descent in the variable ui =log vi , which
would involve the partial derivative ∂ F /∂ui = vi (∂ F /∂vi ) as opposed to what appears in
equation 1.4.

2006

F. Sha, Y. Lin, L. Saul, and D. Lee

i

optimization problem is sparse, containing a large number of zero ele-
ments. Note, moreover, that sparse solutions are more likely to arise in
problems with nonnegativity constraints because in these problems, min-
=0 without the precise vanishing of the partial deriva-
ima can emerge at v∗
tive (∂ F /∂vi)|v∗ (as would be required in an unconstrained optimization).
The EG update in equation 1.3, like gradient descent in equation 1.1
and projected gradient descent in equation 1.2, depends on the explicit
introduction of a learning rate η >0. The size of the learning rate must be
chosen to avoid divergent oscillations (if η is too large) and unacceptably
slow convergence (if η is too small). The necessity of choosing a learning
rate can be viewed as a consequence of the generality of these learning
rules; they do not assume or exploit any structure in the objective function
F (v) beyond the fact that it is differentiable.

Not surprisingly, many objective functions in machine learning have
structure that can be exploited in their optimizations—and in particular,
by multiplicative updates. Such updates need not involve learning rates,
and they may also involve intuitions rather different from the connection
between EG and gradient descent in equations 1.3 and 1.4. For example,
the expectation-maximization (EM) algorithm for latent variable models
(Dempster, Laird, & Rubin, 1977) and the generalized iterative scaling (GIS)
algorithm for logistic regression (Darroch & Ratcliff, 1972) can be viewed as
multiplicative updates (Saul, Sha, & Lee, 2003), but unlike the EG update,
they cannot be cast as simple variants of gradient descent in the log domain.
In this article, we derive multiplicative updates for convex problems
in quadratic programming where the optimization is conﬁned to an axis-
aligned region in the nonnegative orthant. Our multiplicative updates have
the property that they improve the value of the objective function at each
iteration and converge monotonically to the global minimum. Despite their
simplicity, they differ strikingly in form from other multiplicative updates
used in statistical learning, including EG, EM, and GIS. This article provides
a complete derivation and proof of convergence for the multiplicative up-
dates, originally described in previous work (Sha, Saul, & Lee, 2003a, 2003b).
The proof techniques should be of general interest to researchers in neural
computation and statistical learning faced with problems in constrained
optimization.

The basic problem that we study in this article is quadratic programming

with nonnegativity constraints:

minimize
subject to

2 vTAv + bTv

F (v) = 1
v ≥ 0.

(1.5)

The constraint indicates that the variable v is conﬁned to the nonnegative
orthant. We assume that the matrix A is symmetric and strictly positive
deﬁnite, so that the objective function F (v) in equation 1.5 is bounded below,

Multiplicative Updates for Nonnegative Quadratic Programming

2007

and its optimization is convex. In particular, it has one global minimum and
no local minima.

Monotonically convergent multiplicative updates for minimizing equa-
tion 1.5 were previously developed for the special case of nonnegative
matrix factorization (NMF) (Lee & Seung, 2001). In this setting, the matrix
elements of A are nonnegative, and the vector elements of b are negative.
The updates for NMF are derived from an auxiliary function similar to the
one used in EM algorithms. They take the simple, elementwise multiplica-
tive form,
vi ←−

(cid:1) |bi|

(1.6)

(cid:2)

vi ,

(Av)i

which is guaranteed to preserve the nonnegativity constraints on v. The
validity of these updates for NMF hinges on the assumption that the matrix
elements of A are nonnegative: otherwise, the denominator in equation 1.6
could become negative, leading to a violation of the nonnegativity con-
straints on v.

In this article, we generalize the multiplicative updates in equation 1.6 to
a wider range of problems in nonnegative quadratic programming (NQP).
Our updates assume only that the matrix A is positive semideﬁnite: in par-
ticular, it may have negative elements off the diagonal, and the vector b
may have both positive and negative elements. Despite the greater gener-
ality of our updates, they retain a simple, elementwise multiplicative form.
The multiplicative factors in the updates involve only two matrix-vector
multiplications and reduce to equation 1.6 for the special case of NMF. The
updates can also be extended in a straightforward way to the more general
problem of NQP with upper-bound constraints on the variable v ≤ (cid:1). Under
these additional constraints, the variable v is restricted to an axis-aligned
“box” in the nonnegative orthant with opposing vertices at the origin and
the nonnegative vector (cid:1).

We prove that our multiplicative updates converge monotonically to
the global minimum of the objective function for NQP. The proof relies on
constructing an auxiliary function, as in earlier proofs for EM and NMF al-
gorithms (Dempster et al., 1977; Lee & Seung, 2001). In general, monotonic
improvement in an auxiliary function sufﬁces only to establish convergence
to a local stationary point, not necessarily a global minimum. For our up-
dates, however, we are able to prove global convergence by exploiting the
particular structure of their ﬁxed points as well as the convexity of the
objective function.

The rest of this article is organized as follows. In section 2, we present
the multiplicative updates and develop some simple intuitions behind their
form. The updates are then derived more formally and their convergence
properties established in section 3, which completes the proofs sketched
in earlier work (Sha et al., 2003a, 2003b). In section 4, we brieﬂy describe

2008

F. Sha, Y. Lin, L. Saul, and D. Lee

some applications to problems in signal processing (Lin et al., 2004) and
pattern recognition (Cristianini & Shawe-Taylor, 2000). Finally, in section 5,
we conclude by summarizing the main advantages of our approach.

2 Algorithm

We begin by presenting the multiplicative updates for the basic problem
of NQP in equation 1.5. Some simple intuitions behind the updates are
developed by analyzing the Karush-Kuhn-Tucker (KKT) conditions for this
problem. We then extend the multiplicative updates to handle the more
general problem of NQP with additional upper-bound constraints v ≤ (cid:1).

2.1 Updates for NQP. The multiplicative updates for NQP are ex-
pressed in terms of the positive and negative components of the matrix A.
+
In particular, let A
denote the nonnegative matrices with elements:

−
and A

(cid:3)

−
and A
ij

=

|Aij| if Aij < 0,
otherwise.
0

(2.1)

(cid:3)

=

+
A
ij

Aij if Aij > 0,
0 otherwise,
+−A
−

It follows that A = A
. In terms of these nonnegative matrices, the
objective function in equation 1.5 can be decomposed as the combination
of three terms, which we write as
F (v) = Fa (v) + Fb(v) − Fc(v)

(2.2)

for reasons that will become clear shortly. We use the ﬁrst and third terms
in equation 2.2 to “split” the quadratic piece of F (v) and the second term to
capture the linear piece:

Fa (v) = 1
+
2 vTA
Fb(v) = bTv,
Fc(v) = 1
−
2 vTA

v,

v.

(2.3)

The decomposition (see equation 2.2) follows trivially from the deﬁnitions
in equations 2.1 and 2.3. The gradient of F (v) can be similarly decomposed
in terms of contributions from these three pieces. We have chosen our
notation in equation 2.3 so that bi = ∂ Fb /∂vi; for the quadratic terms in the
objective function, we deﬁne the corresponding derivatives:

ai = ∂ Fa
∂vi

= (A
+

v)i ,

(2.4)

Multiplicative Updates for Nonnegative Quadratic Programming

2009

ci = ∂ Fc
∂vi

= (A
−

v)i .

(2.5)

Note that the partial derivatives in equations 2.4 and 2.5 are guaranteed to
be nonnegative when evaluated at vectors v in the nonnegative orthant. The
multiplicative updates are expressed in terms of these partial derivatives as

(cid:6)


−bi +

vi ←−


 vi .

+ 4ai ci

b2
i
2ai

(2.6)

Note that these updates reduce to the special case of equation 1.6 for NMF
when the matrix A has no negative elements.

The updates in equation 2.6 are meant to be applied in parallel to all the
elements of v. They are remarkably simple to implement and notable for
their absence of free parameters or heuristic criteria that must be tuned to
ensure convergence. Since ai ≥0 and ci ≥0, it follows that the multiplicative
prefactor in equation 2.6 is always nonnegative; thus, the optimization
remains conﬁned to the feasible region for NQP. As we show in section 3,
moreover, these updates are guaranteed to decrease the value of F (v) at
each iteration.

There is a close link between the sign of the partial derivative ∂ F /∂vi
and the effect of the multiplicative update on vi . In particular, using the fact
that ∂ F /∂vi = ai +bi −ci, it is easy to show that the update decreases vi if
∂ F /∂vi >0 and increases vi if ∂ F /∂vi <0. Thus, the multiplicative update in
equation 2.6 moves each element vi in an opposite direction to its partial
derivative.

2.2 Fixed Points. Further intuition for the updates in equation 2.6
can be gained by examining their ﬁxed points. Let mi denote the mul-
tiplicative prefactor inside the brackets on the right-hand side of equa-
tion 2.6. Fixed points of the updates occur when either (1) vi =0 or
(2) mi =1. What does the latter condition imply? Note that the expression
for mi is simply the quadratic formula for the larger root of the polynomial
p(m) = ai m2+bi m−ci . Thus, mi =1 implies that ai +bi −ci = 0. From the def-
initions in equations 2.2 to 2.5, moreover, it follows that ∂ F /∂vi = 0. Thus,
the two criteria for ﬁxed points can be restated as (1) vi =0 or (2) ∂ F /∂vi =0.
These are consistent with the Karush-Kuhn-Tucker (KKT) conditions for
the NQP problem in equation 1.5, as we now show.

Let λi denote the Lagrange multiplier used to enforce the nonnegativity

constraint on vi. The KKT conditions are given by
λ ◦ v = 0,

Av + b = λ,

λ ≥ 0,

v ≥ 0,

(2.7)

2010
F. Sha, Y. Lin, L. Saul, and D. Lee
in which ◦ stands for elementwise vector multiplication. A necessary and
sufﬁcient condition for v to solve equation 1.5 is that there exists a vec-
tor λ such that v and λ satisfy this system. It follows from equation 2.7
that the gradient of F (v) at its minimum is nonnegative: ∇F = Av + b ≥ 0.
Moreover, for inactive constraints (corresponding to elements of the mini-
mizer that are strictly positive), the corresponding partial derivatives of the
objective function must vanish: ∂ F /∂vi = 0 if vi > 0. Thus, the KKT condi-
tions imply that (1) vi = 0 or (2) ∂ F /∂vi = 0, and any solution satisfying the
KKT conditions corresponds to a ﬁxed point of the multiplicative updates,
though not vice versa.

2.3 Upper-Bound Constraints. The multiplicative updates in equation
2.6 can also be extended to incorporate upper-bound constraints of the form
v≤ (cid:1). A simple way of enforcing such constraints is to clip the output of the
updates in equation 2.6:

(cid:6)


−bi +


(cid:4)i ,

vi ←− min


 vi


 .

+ 4ai ci

b2
i
2ai

(2.8)

As we show in the next section, this clipped update is also guaranteed to
decrease the objective function F (v) in equation 1.5 if it results in a change
of vi.

3 Convergence Analysis

In this section, we prove that the multiplicative updates in equation 2.6 con-
verge monotonically to the global minimum of the objective function F (v).
Our proof is based on the derivation of an auxiliary function that provides
an upper bound on the objective function. Similar techniques have been
used to establish the convergence of many algorithms in statistical learn-
ing (e.g., the EM algorithm, Dempster et al., 1977, for maximum likelihood
estimation) and nonnegative matrix factorization (Lee & Seung, 2001). The
proof is composed of two parts. We ﬁrst show that the multiplicative up-
dates monotonically decrease the objective function F (v). Then we show
that the updates converge to the global minimum. We assume throughout
the article that the matrix A is positive deﬁnite such that the objective func-
tion is convex. (Though theorem 2 does not depend on this assumption,
convexity is used to establish the stronger convergence results that follow.)

3.1 Monotonic Convergence. An auxiliary function G(u, v) for the ob-
jective function in equation 1.5 has two crucial properties: (1) F (u)≤ G(u, v)
and (2) F (v)= G(v, v) for all positive vectors u and v. From such an auxiliary

Multiplicative Updates for Nonnegative Quadratic Programming

2011

Figure 1: Using an auxiliary function G(u, v) to minimize an objective function
F (v). The auxiliary function is constructed around the current estimate of the
minimizer; the next estimate is found by minimizing the auxiliary function,
which provides an upper bound on the objective function. The procedure is
iterated until it converges to a stationary point (generally a local minimum) of
the objective function.

function, we can derive the update rule v
increases (and generally decreases) the objective function F (v):

(cid:8) = arg minuG(u, v), which never

(cid:8)

) ≤ G(v

(cid:8), v) ≤ G(v, v) = F (v).

F (v

(3.1)

By iterating this update, we obtain a series of values of v that improve
the objective function. Figure 1 graphically illustrates how the auxiliary
function G(u, v) is used to compute a minimum of the objective function
F (v) at v = v

∗

.

To derive an auxiliary function for NQP, we ﬁrst decompose the objective
function F (v) in equation 1.5 into three terms as in equations 2.2 and 2.3 and
then derive the upper bounds for each of them separately. The following
two lemmas establish the bounds relevant to the quadratic terms Fa (u)
and Fc(u).

+
Lemma 1. Let A
denote the matrix composed of the positive elements of the
matrix A, as deﬁned in equation 2.1. Then for all positive vectors u and v, the
quadratic form Fa (u) = 1
(cid:15)

u satisﬁes the following inequality:

+
2 uT A

.

(3.2)

Fa (u) ≤ 1
2

i

+
(A
v)i
vi

u2
i

2012

F. Sha, Y. Lin, L. Saul, and D. Lee

Proof. Let δij denote the Kronecker delta function, and let K be the diagonal
matrix with elements

Kij = δij

+
(A
v)i
vi

.

(3.3)

Since Fa (u)= 1
statement that the matrix (K − A
+
matrix M whose elements

+
2 uTA

u, the inequality in equation 3.2 is equivalent to the
) is positive semideﬁnite. Consider the

Mij = vi (Kij − A
+
ij )v j

(3.4)

). Thus,
) is positive semideﬁnite if M is positive semideﬁnite. We note that

are obtained by rescaling componentwise the elements of (K − A
+
(K − A
+
for all vectors u,
uTMu=

ui vi(Kij − A
+
ij )v j u j

(3.5)

(3.6)

(3.7)

(3.8)

ij

(cid:15)
(cid:15)
(cid:15)
(cid:15)

ij

ij
= 1
2

ij

=

=

(cid:15)

+
A
ij

vi v j ui u j

+
δij(A

v)i ui u j v j −
(cid:15)

+
A
ij

vi v j u2
i

−

ij

vi v j ui u j

+
A
ij

ij

+
A
ij

vi v j (ui − u j )2 ≥ 0.

Thus, (K − A
+
An alternative proof that (K − A
+
by appealing to the Frobenius-Perron Theorem (Lee & Seung, 2001).

) is positive semideﬁnite, proving the bound in equation 3.2.
) is semideﬁnite positive can also be made

For the terms related to the negative elements in the matrix A, we have

following result:

−
Lemma 2. Let A
denote the matrix composed of the negative elements of the
matrix A, as deﬁned in equation 2.1. Then for all positive vectors u and v, the
quadratic form Fc(u) = 1
(cid:15)

u satisﬁes the following inequality:

−
2 uT A

(cid:16)

(cid:17)

.

(3.9)

−Fc(u) ≤ − 1

2

ij

−
A
ij

vi v j

1 + log

uiu j
vi v j

Multiplicative Updates for Nonnegative Quadratic Programming
2013
Proof. To prove this bound, we use the simple inequality: z ≥ 1 + log z.
Substituting z = ui u j /(vi v j ) into this inequality gives

(cid:17)

.

ui u j ≥ vi v j

1 + log

ui u j
vi v j

(cid:16)

(cid:15)

i

(cid:18)

(3.10)

Substituting the above inequality into Fc(u) = 1
−
ij ui u j and noting the
ij A
negative sign, we arrive at the bound in equation 3.9.

Combining lemmas 1 and 2 and noting that Fb(u) =(cid:18)

i bi ui, we have

2

proved the following theorem:

Theorem 1. Deﬁne a function G(u, v) on positive vectors u and v by

G(u, v) = 1

2

+
(A
v)i
vi

u2
i

− 1
2

−
A
ij

vi v j

(cid:15)

ij

(cid:16)
1 + log

ui u j
vi v j

(cid:17)

(cid:15)

+

bi ui .

i

Then G(u, v) is an auxiliary function for the function F (v) = 1
satisfying F (u) ≤ G(u, v) and F (v) = G(v, v).

(3.11)
2 vTAv + bTv,

As explained previously, a new estimate that improves the objective
function F (v) at its current estimate v is obtained by minimizing the aux-
iliary function G(u, v) with respect to its ﬁrst argument u, as shown by
following theorem:

Theorem 2. Given a positive vector v and a mapping v
(cid:8) = arg minu G(u, v), we have
v

(cid:8) = M(v) such that

(cid:8)

) ≤ F (v).

F (v

(3.12)
(cid:8) (cid:9)= v, then the inequality holds strictly. Therefore, the objective
Moreover, if v
function is strictly decreased unless at the ﬁxed point of the mapping M(v), where
v = M(v). The mapping M(v) takes the form of equation 2.6 if v is constrained
only to be nonnegative and takes the form of equation 2.8 if v is box-constrained.

Proof. The inequality in equation 3.12 is a direct result from the deﬁni-
tion of the auxiliary function and its relation to the objective function. The
derivation in equation 3.1 is reproduced here for easy reference:

(cid:8)

) ≤ G(v

(cid:8), v) ≤ G(v, v) = F (v).

F (v

(3.13)

2014

F. Sha, Y. Lin, L. Saul, and D. Lee

(cid:8)

To show that the objective function is strictly decreased if the new esti-
mate v
is not the same as the old estimate v, we must also show that the
(cid:8), v) < G(v, v).
auxiliary function is strictly decreased: if v
This can be proved by further examining the properties of the auxiliary
function.

(cid:8) (cid:9)= v, then G(v

We begin by showing that G(u, v) is the sum of strictly convex functions
of u. For a strictly convex function, the minimizer is unique, and the min-
imum is strictly less than any other values of the function. We reorganize
the expression of the auxiliary function G(u, v) given by equation 3.11 such
that there are no interaction terms among the variables ui:

(cid:15)

ij

bi ui − 1
2

i

−
A
ij

vi v j .

(cid:15)

(cid:15)

G(u, v) = 1
2

(cid:15)
We identify the auxiliary function with G(u, v) =(cid:18)

+
(A
v)i
vi

v)i vi log

−
(A

ui
vi

u2
i

+

−

i

i

where Gi(ui) is a single-variable function of ui:

Gi(ui) = 1
2

+
(A
v)i
vi

− (A
−

u2
i

v)i vi log

+ bi ui .

ui
vi

i Gi(ui) − 1

−
2 vTA

v,

(3.14)

Note that the minimizer of G(u, v) can be easily found by minimizing each
= arg minui Gi(ui). Moreover, we will show that Gi(ui)
Gi(ui) separately: v(cid:8)
is strictly convex in ui. To see this, we examine its second derivative with
respect to ui:

i

+
i (ui) = (A
(cid:8)(cid:8)
v)i
vi

G

v)i

−
+ (A
u2
i

vi .

(3.15)

−
v)i and (A

+
For a positive vector v, (A
v)i cannot be simultaneously equal to
zero. Otherwise, the ith row of A is all-zero, contradicting our assumption
(cid:8)(cid:8)
that A is strictly convex. This implies that G
i (ui) is strictly positive and
Gi(ui) is strictly convex in ui.
Theorem 2 follows directly from the above observation. In particular, if
vi is not a minimizer of Gi(ui), then v(cid:8)
i ) < Gi(ui). Since the
auxiliary function G(u, v) is the sum of all the individual terms Gi(ui) plus
(cid:8), v) is strictly less than
a term independent of u, we have shown that G(v
G(v, v) if v

(cid:8) (cid:9)= v. This leads to F (v

(cid:9)= vi and Gi(v(cid:8)

) < F (v).

(cid:8)

i

As explained previously, the minimizer v

can be computed by ﬁnding
the minimizer of each individual term Gi(ui). Computing the derivative
of Gi(ui) with respect to ui, setting it to zero, and solving for ui lead to
the multiplicative updates in equation 2.6. Minimizing Gi(ui) subject to
box constraints ui ∈[0, (cid:4)i ] leads to the clipped multiplicative updates in
equation 2.8.

(cid:8)

Multiplicative Updates for Nonnegative Quadratic Programming

2015

(cid:8)

3.2 Global Convergence. The multiplicative updates deﬁne a mapping
M from the current estimate v of the minimizer to a new estimate v
. By it-
eration, the updates generate a sequence of estimates {v1, v2, . . .}, satisfying
vk+1=M(vk). The sequence monotonically improves the objective function
F (v). Since the sequence {F (v1), F (v2), . . . ,} is monotonically decreasing
and is bounded below by the global minimum value of F (v), the sequence
converges to some value when k is taken to the limit of inﬁnity. While
establishing monotonic convergence of the sequence, however, the above
observation does not rule out the possibility that the sequence converges
to spurious ﬁxed points of the iterative procedure vk+1=M(vk) that are not
the global minimizer of the objective function. In this section, we prove
that the multiplicative updates do indeed converge to the global minimizer
and attain the global minimum of the objective function. (The technical
details of this section are not necessary for understanding how to derive or
implement the multiplicative updates.)

3.2.1 Outline of the Proof. Our proof relies on a detailed investigation of
the ﬁxed points of the mapping M deﬁned by the multiplicative updates. In
what follows, we distinguish between the “spurious” ﬁxed points of M that
violate the KKT conditions versus the unique ﬁxed point of M that satisﬁes
the KKT conditions and attains the global minimum value of F (v). The basic
idea of the proof is to rule out both the possibility that the multiplicative
updates converge to a spurious ﬁxed point, as well as the possibility that
they lead to oscillations among two or more ﬁxed points.
Our proof consists of three stages. First, we show that any accumulation
point of the sequence {v1, v2, . . .} must be a ﬁxed point of the multiplica-
tive updates—either a spurious ﬁxed point or the global minimizer. Such
a result is considerably weaker than global convergence to the minimizer.
Second, we show that there do not exist convergent subsequences S of the
mapping M with spurious ﬁxed points as accumulation points. In par-
ticular, we show that if such a sequence S converges to a spurious ﬁxed
point, then it must have a subsequence converging to a different ﬁxed point,
yielding a contradiction. Therefore, the accumulation point of any conver-
gent subsequence must be the global minimizer. Third, we strengthen the
result on subsequence convergence and show that the sequence {v1, v2, . . .}
converges to the global minimizer.

Our proof starts from Zangwill’s convergence theorem (Zangwill, 1969),
a well-known convergence result for general iterative methods, but our
ﬁnal result does not follow simply from this general framework. We re-
view Zangwill’s convergence theorem in appendix A. The application of
this theorem in our setting yields the weaker result in the ﬁrst step of our
proof: convergence to a ﬁxed point of the multiplicative updates. As ex-
plained in the appendix, however, Zangwill’s convergence theorem does
not exclude the possibility of convergence to spurious ﬁxed points. We de-
rive our stronger result of global convergence by exploiting the particular

2016

F. Sha, Y. Lin, L. Saul, and D. Lee

structure of the objective function and the multiplicative update rules for
NQP. A key step (see lemma 4) in our proof is to analyze the mapping M
on sequences that are in the vicinity of spurious ﬁxed points. Our analysis
appeals repeatedly to the speciﬁc properties of the objective function and
the mapping induced by the multiplicative updates.
The following notation and preliminary observations will be useful. We
use {vk}∞
1 to denote
the corresponding sequence {F (v1), F (v2), . . . , F (vk), . . .}. We assume that
the matrix A in equation 1.5 is strictly positive deﬁnite so that the objective
function has a unique global minimum. From this, it also follows that the
+
matrix A
in equation 1.7 has strictly positive elements along the diagonal.

1 to denote the sequence {v1, v2, . . . , vk , . . .} and {F (vk)}∞

i

3.2.2 Positivity. Our proof will repeatedly invoke the observation that
for a strictly positive vector v, the multiplicative updates in equation 2.6
(cid:8) = M(v). There is one exception to this
yield a strictly positive vector v
rule, which we address here. Starting from a strictly positive vector v, the
= 0 directly to zero in the case
multiplicative updates will set an element v(cid:8)
that bi ≥ 0 and the ith row of the matrix A has no negative elements. It
is easy to verify in this case, however, that the global minimizer v
of the
= 0. Once an element
∗
objective function does have a zero element at v
i
in zeroed by the multiplicative updates, it remains zero under successive
updates. In effect, when this happens, the original problem in NQP reduces
to a smaller problem—of dimensionality equal to the number of nontrivial
modes in the original system. Without loss of generality, therefore, we will
assume in what follows that any trivial degrees of freedom have already
been removed from the problem. More speciﬁcally, we will assume that the
ith row of the matrix A has one or more negative elements whenever bi ≥ 0
and that consequently, a strictly positive vector v is always mapped to a
strictly positive vector v

∗

(cid:8) = M(v).
3.2.3 Accumulation Points of {vk}∞

1 . The following lemma is a direct result
of Zangwill’s convergence theorem, as reviewed in appendix A. It estab-
lishes the link between the accumulation points of{vk}∞
1 and the ﬁxed points
of M.

Lemma 3. Given a point v1, suppose the update rule in equation 2.6 generates a
sequence {vk}∞
1 . Then either the algorithm terminates at a ﬁxed point of M or the
accumulation point of any convergent subsequence in {vk}∞
1 is a ﬁxed point of M.
Proof. If there is a k ≥ 1 such that vk is a ﬁxed point of M, then the update
rule terminates. Therefore, we consider the case that an inﬁnite sequence is
generated and show how to apply Zangwill’s convergence theorem.
Let M be the update procedure in Zangwill’s convergence theorem. We
ﬁrst verify that the sequence {vk}∞
1 generated by M is in a compact set.

Multiplicative Updates for Nonnegative Quadratic Programming
Because {F (vk)}∞
for all k,

1

is a monotonically decreasing sequence, it follows that

2017

vk ∈  = {v|F (v) ≤ F (v1)}.

(3.16)

Note that the set  is compact because it deﬁnes an ellipsoid conﬁned to
the positive orthant.
We deﬁne the desired set S to be the collection of all the ﬁxed points
of M. If v /∈ S, then from theorem 2, we have that F (M(v)) < F (v). On the
other hand, if v ∈ S, then we have that F (M(v)) = F (v). This shows that the
mapping M maintains strict monotonicity of the objective function outside
the desired set.
The last condition to verify is that M is closed at v if v is not in the desired
set. Note that M is continuous if v (cid:9)= 0. Therefore, if the origin v = 0 is a
ﬁxed point of M, then M is closed outside the desired set.
If the origin is not a ﬁxed point of M, then it cannot be the global
minimizer. Moreover, we can choose the initial estimate v1 such that F (v1) <
F (0). With this choice, it follows from the monotonicity of M that the origin
is not contained in  and that M is continuous on .
Either way, we have shown that M is closed on a proper domain. There-
fore, we can apply Zangwill’s convergence theorem to the mapping M
restricted on : the limit of any convergent subsequence in {vk}∞
1 is in the
desired set or equivalently, a ﬁxed point of M.

Remark. It is easy to check whether the global minimizer occurs at the
origin with value F (0)=0. In particular, if all the elements of b are nonneg-
ative, then the origin is the global minimizer. On the other hand, if there
is a nonnegative element of b, then we can choose the initial estimate v1
such that F (v1) < F (0). For example, suppose bk < 0. Then we can choose
v1 such that its kth element is σ and all other elements are τ. A positive σ
and τ can be found such that F (v1) < 0 by noting

(cid:15)
(cid:15)

i, j(cid:9)=k

Aijτ 2 +

|Aij|τ 2 +

F (v1)= 1
2

≤ 1
2

(cid:15)
Aik τ σ +
(cid:19)(cid:15)

i(cid:9)=k

i(cid:9)=k
|Aik|σ +

(cid:15)
bi τ + 1
(cid:20)
(cid:15)
2

|bi|

τ +

Akk σ 2 + bk σ
(cid:16)

Akk σ + bk

1
2

(cid:17)

σ.

ij

i

i

(3.17)
Note that if we choose a positive σ < −2bk /Akk, we can always ﬁnd a
positive τ such that F (v1) < 0 because the right-most term of the inequality
in equation 3.17 is negative and the left and middle terms vanish as τ → 0
+
.

2018

F. Sha, Y. Lin, L. Saul, and D. Lee

,
Figure 2: Fixed points of the multiplicative updates: the global minimizer v
, indicated by squares.
indicated by a star and spurious ﬁxed points ˆv and ˆv
Contour lines of the objective function are shown as ellipses. A hypothetical
sequence {vk}∞
1 with a subsequence converging to the spurious ﬁxed point ˆv is
represented by solid lines connecting small black circles. The δ-ball around the
spurious ﬁxed point ˆv does not intersect the δ(cid:8)
-ball around the other spurious
ﬁxed point ˆv

.

(cid:8)

(cid:8)

∗

Lemma 3 states that any convergent subsequence in {vk}∞

3.2.4 Properties of the Fixed Points. As stated in section 2.2, the minimizer
of F (v) satisﬁes the KKT conditions and corresponds to a ﬁxed point of the
mapping M deﬁned by the multiplicative update rule in equation 2.6. The
mapping M, however, also has ﬁxed points that do not satisfy the KKT
conditions. We refer to these as spurious ﬁxed points.
1 must have a
ﬁxed point of M as its accumulation point. To prove that the multiplicative
updates converge to the global minimizer, we will show that spurious ﬁxed
points cannot be accumulation points. Our strategy is to demonstrate that
any subsequence S converging to a spurious ﬁxed point must itself have a
subsequence “running away” from the ﬁxed point. The idea of the proof is
shown schematically in Figure 2. The star in the center of the ﬁgure denotes
(cid:8)
the global minimizer v
. Black squares denote spurious ﬁxed points ˆv and ˆv
.
The ﬁgure also shows a hypothetical subsequence that converges to the
spurious ﬁxed point ˆv.

∗

Multiplicative Updates for Nonnegative Quadratic Programming

2019

At a high level, the proof (by contradiction) is as follows. Suppose that
there exists a convergent subsequence as shown in Figure 2. Then we can
draw a very small -ball around the spurious ﬁxed point ˆv containing
an inﬁnite number of elements of the subsequence. We will show that
under the mapping M, the subsequence must have an inﬁnite number
of successors that are outside the -ball yet inside a δ-ball where δ > .
This bounded successor sequence must have a subsequence converging
to an accumulation point, which by lemma 3 must also be a ﬁxed point.
However, we can choose the δ-ball to be sufﬁciently small such that the
annulus between the -ball and δ-ball contains no other ﬁxed points. This
yields a contradiction.

More formally, we begin by proving the following lemma:

Lemma 4. Let v1 denote a positive initial vector satisfying F (v1) < F (0). Sup-
pose that the sequence vk+1 = M(vk) generated by the iterative update in equation
2.6 has a subsequence that converges to the spurious ﬁxed point ˆv. Then there
exists an  > 0 and a δ > 0 such that for every v (cid:9)= ˆv such that (cid:13)v − ˆv(cid:13) < , there
exists an integer p ≥ 1 such that  < (cid:13)Mp(v) − ˆv(cid:13) < δ, where Mp(v) is p times
(cid:24)
composition of M applied to v: M ◦ M··· ◦ M

(cid:22)(cid:23)

(v).

(cid:21)

p

(cid:9)= 0 or ˆvi = 0 for any i. If the former
Proof. If ˆv is a ﬁxed point, then either ˆvi
is true, as shown in section 2.2, it follows that (∂ F /∂vi)|ˆv=0. When ˆvi = 0,
then either (∂ F /∂vi)|ˆv≥0 or (∂ F /∂vi)|ˆv <0. If ˆv is a spurious ﬁxed point that
violates the KKT conditions, then there exists at least one i such that

(cid:25)(cid:25)(cid:25)(cid:25)

ˆvi = 0 and

∂ F
∂vi

<0.

ˆv

Let ˆv be a small ball centered at ˆv with radius of : ˆv = {v|(cid:13)v − ˆv(cid:13) < }. By
continuity, there exists an  such that (∂ F /∂vi) <0 for all v ∈ ˆv.
Let  be the image of ˆv under the mapping M. Since M is a continuous
mapping, we can ﬁnd a minimum ball δˆv = {v|(cid:13)v − ˆv(cid:13) < δ} to encircle .
We claim that the  and δ satisfy the lemma.
As observed in section 2.2, the multiplicative update increases vi if ∂ F /∂vi
is negative. Consider the sequence {M(v),M ◦ M(v), . . . ,Mk(v), . . .}. The
ith component of the sequence is monotonically increasing until the con-
dition (∂ F /∂vi) becomes nonnegative. This happens only if the element of
the sequence is outside the ˆv ball. Thus, for every v ∈ ˆv, the update will
push vi to larger and larger values until it escapes from the ball. Let p be
the smallest integer such that

Mp−1(v) ∈ ˆv and Mp(v) /∈ ˆv

2020
F. Sha, Y. Lin, L. Saul, and D. Lee
By construction of the δˆv ball, Mp is inside the ball since  is the image of
the ˆv under the mapping M and Mp−1(v) ∈ ˆv. Therefore, Mp(v) ∈  ⊂ δˆv.

The size of the δ-ball depends on the spurious ﬁxed point ˆv and . Can δˆv
contain another spurious ﬁxed point ˆv
? The following lemma shows that
we can choose  sufﬁciently small such that the ball δˆv contains no other
ﬁxed points.

(cid:8)

(cid:8)

and δ(cid:8)

(cid:8)

If ˆv and ˆv

be the radii for ˆv

are two different spurious ﬁxed points, let  and δ be the
. There exists an
) ≤ 0 and δˆv ∩ δˆv(cid:8) is empty.

Lemma 5.
radii for ˆv such that lemma 4 holds and (cid:8)
0 > 0 such that max(, (cid:8)
Proof. It sufﬁces to show that M(v) becomes arbitrarily close to ˆv as v
approaches ˆv. Since M is a bounded and continuous mapping, the image
 of ˆv under M becomes arbitrarily small as  → 0. Note that ˆv is a ﬁxed
point of M, and we have ˆv ∈ . Therefore, the δ-ball centered at ˆv can be
made arbitrarily small as v approaches ˆv.
and choose (cid:8)

Let us choose  sufﬁciently small such that δ is less than the half of the
likewise. Then the intersection of

distance between ˆv and ˆv
δˆv and δˆv(cid:8) is empty. This is illustrated in Figure 2.

(cid:8)

Because the spurious ﬁxed points are separated by their δ-balls, we
will show that the existence of a subsequence converging to a spurious
ﬁxed point leads to a contradiction. This observation leads to the following
theorem.

Theorem 3.
subsequence of {vk}∞
as an accumulation point.

If the matrix A is strictly positive deﬁnite, then no convergent
1 can have a spurious ﬁxed point of the multiplicative updates

Suppose there is a convergent subsequence{vk} ⊂ {vk}∞

Proof. The number of spurious ﬁxed points is ﬁnite and bounded above
by the number of ways of choosing zero elements of v. Let  > 0 be the
minimum pairwise distance between these ﬁxed points. By lemma 4, we
can choose a small  such that the radius of the δ-ball for any spurious ﬁxed
point is less than /2. With this choice, the δ-balls at different spurious
ﬁxed points are nonoverlapping.
1 such that vk −→
ˆv with k ∈ K, where K is an index subset and ˆv is a spurious ﬁxed point.
Without loss of generality, we assume the whole subsequence is contained
in the -ball of ˆv.
For each element vk of the subsequence, by lemma 4, there exists an
integer pk such that Mpk (vk) is outside the -ball yet inside the δ-ball. Con-
sider the “successor” sequence Mpk (vk) with k ∈ K, schematically shown
in Figure 2 as the black circles between the -ball and the δ-ball. The inﬁnite
successor sequence is bounded between the -ball and δ-ball and therefore

Multiplicative Updates for Nonnegative Quadratic Programming

2021

must have a convergent subsequence. By lemma 3, the accumulation point
of this subsequence must be a ﬁxed point of M. However, this leads to
contradiction. On one hand, the subsequence is outside the -ball of ˆv so ˆv
is not the accumulation point. On the other hand, the subsequence is inside
the δ-ball of ˆv: therefore, it cannot have any other spurious ﬁxed point ˆv
as
its accumulation point, because we have shown that all pairs of ﬁxed points
are separated by their respective δ-balls. Therefore, the accumulation point
of the subsequence cannot be a ﬁxed point. Thus, we arrive at a contradic-
tion, showing that spurious ﬁxed points cannot be accumulation points of
any convergent subsequence.

(cid:8)

3.2.5 Convergence to the Global Minimizer. We have shown that the only
possible accumulation point of{vk}∞
1 is the global minimizer: the ﬁxed point
of M that satisﬁes the KKT conditions. We now show that the sequence
{vk}∞

1 itself does indeed converge to the global minimizer.

1

∗

∗

).

∗ = F (v

, and {F (vk)}∞

Theorem 4. Suppose that the origin is not the global minimizer and that we
choose a positive initial vector v1 such that F (v1) < 0. Then the sequence {vk}∞
1
converges to the global minimizer v
converges to the optimal
value F
Proof. As shown in equation 3.16, the inﬁnite sequence {vk}∞
1 is a bounded
set; therefore, it must have an accumulation point. By the preceding theo-
rem, the accumulation point of any convergent subsequence of{vk}∞
1 cannot
be a spurious ﬁxed point; thus, any convergent subsequence must converge
to the ﬁxed point that satisﬁes the KKT conditions: the global minimizer. By
monotonicity, it immediately follows that{F (vk)}∞
1 converges to the optimal
value F
of the objective function.
∗ < F (ˆv) for all spurious ﬁxed points ˆv, we can ﬁnd an ∗ > 0

Since F

∗

such that the set

∗ = {v|F (v) ≤ F

∗ + ∗}

contains no spurious ﬁxed points ofM. Moreover, since{F (vk)}∞
tonically decreasing sequence converging to F
vk ∈ ∗

We now prove the theorem by contradiction. Suppose {vk}∞

for all k ≥ k0.

1 is a mono-
, there exists a k0 such that

1 does not
. Then there exists an  > 0 such that the set

∗

∗

converge to the minimizer v
∗(cid:13) > }

 = {vk : (cid:13)vk − v

has an inﬁnite number of elements. In other words, there must be a sub-
sequence of {vk}∞
in which every element has distance at least  from the
minimizer. Moreover, the intersection of  and ∗
must have an inﬁnite
number of elements. Note that by construction, ∗
contains no ﬁxed points

1

2022

F. Sha, Y. Lin, L. Saul, and D. Lee

Figure 3: Estimating time delay from signals and their echos in reverberant
environments.

other than the global minimizer, and  does not contain the global min-
imizer. Thus, there are no ﬁxed points in  ∩ ∗
. The inﬁnite set  ∩ ∗
,
however, is bounded, and therefore must have an accumulation point; by
lemma 3, this accumulation point must be a ﬁxed point. This yields a con-
tradiction. Hence, the set  cannot have an inﬁnite number of elements,
and the sequence {vk}∞

1 must converge to the global minimizer.

4 Applications

In this section, we sketch two real-world applications of the multiplicative
updates to problems in signal processing and pattern recognition. Recently,
the updates have also been applied by astrophysicists to estimate the mass
distribution of a gravitational lens and the positions of the sources from
combined strong and weak lensing data (Diego, Tegmark, Protopapas, &
Sandvik, 2007).

4.1 Acoustic Time Delay Estimation. In a reverberant acoustic envi-
ronment, microphone recordings capture echos reﬂected from objects such
as walls and furniture in addition to the signals that propagate directly from
a sound source. As illustrated in Figure 3, the signal at the microphone x(t)
can be modeled as a linear combination of the source signal s(t) at different
delay times: s(t − t), s(t − t
). Given a received signal x(t)

) and s(t − t

(cid:8)

(cid:8)(cid:8)

Multiplicative Updates for Nonnegative Quadratic Programming

2023

and its source signal s(t), how can we identify in x(t) all the time-delayed
components of s(t)? To this end, we consider the model (Lin et al., 2004)

αi s(t − ti ) with αi ≥ 0,

(4.1)

x(t) = N(cid:15)

i

in which {ti}N
i=1 are all possible time delays (discretized to some ﬁnite
resolution) and {αi} are the relative amplitudes (or attenuations) of the
time-delayed components. The nonnegativity constraints in equation 4.1
incorporate the assumption that only the amplitudes of acoustic waves are
affected by reﬂections while the phases are retained (Allen & Berkley, 1979).
Within this model, the time-delayed components of s(t) can be identiﬁed by
computing the amplitudes αi that best reconstruct x(t). The reconstruction
with least-squares error is obtained from the nonnegative deconvolution:

i

αi s(t − ti )|2.

α∗ = arg min
αi≥0

1
2
Nonzero weights α∗
in the least-squares reconstruction are interpreted as
indicating time-delayed components in the received signal with delays ti .
It is convenient to rewrite this optimization in the frequency domain.
Let ˜x( f ) and ˜s( f ) denote the Fourier transforms of x(t) and s(t), respec-
tively. Also, deﬁne the positive semideﬁnite matrix Kij and the real-valued
coefﬁcients ci by

(4.2)

i

|x(t) − N(cid:15)

|˜s( f )|2e j2π f (tj−ti ),

∗

˜s

( f ) ˜x( f )e j2π f ti ,

(4.3)

(4.4)

(cid:15)
(cid:15)

f

f

Kij =

ci =

where the sums are over positive and negative frequencies. In terms of
the matrix Kij and coefﬁcients ci, the optimization in equation 4.2 can be
rewritten as

(cid:18)
minimize
subject to αi ≥ 0.

1
2

ij

αi Kijα j −(cid:18)

i ci αi

(4.5)

This has the same form as the NQP problem in equation 1.5 and can be
solved by the multiplicative updates in equation 2.6. Note that Kij deﬁnes
a Toeplitz matrix if the possible time delays ti are linearly spaced. Using
fast Fourier transforms, the Toeplitz structure of Kij can be exploited for
much faster matrix-vector operations per multiplicative update.

2024

F. Sha, Y. Lin, L. Saul, and D. Lee

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

iteration: 0

iteration: 256

0

5
)
time delay (T
s

10

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

iteration: 512

iteration: 1024

0

time delay (T
s

5
)

10

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

iteration: 2048

iteration: 4196

0

5
)
time delay (T
s

10

Figure 4: Convergence of the multiplicative updates for acoustic time delay
estimation.

Figure 4 shows the convergence of the multiplicative updates for a prob-
lem in acoustic time delay estimation. The source signal s(t) in this example
was a 30 ms window of speech, and the received signal x(t) was given by

x(t) = s(t − Ts) + 0.5 s(t − 8.5Ts),

where Ts was the sampling period. The vertical axes measure the estimated
amplitudes αi after different numbers of iterations of the multiplicative
updates; the horizontal axes measure the time delays ti in the units of
Ts. The vertical dashed lines indicate the delays at Ts and 8.5Ts. The ﬁgure
shows that as the number of iterations is increased, the time delays are
accurately predicted by the peaks in the estimated amplitudes αi .

4.2 Large Margin Classiﬁcation. Large margin classiﬁers have been
applied successfully to many problems in machine learning and statistical
pattern recognition (Cristianini & Shawe-Taylor, 2000; Vapnik, 1998). These
classiﬁers use hyperplanes as decision boundaries to separate positively
and negatively labeled examples represented by multidimensional vectors.
Generally the hyperplanes are chosen to maximize the minimum distance
(known as the margin) from any labeled example to the decision boundary
(see Figure 5).
Let {(xi , yi)}N
i=1 denote a data set of N labeled “training” examples with
binary class labels yi = ±1. The simplest case, shown in Figure 5 (left),

Multiplicative Updates for Nonnegative Quadratic Programming

2025

H

H

H

H

H

H

Figure 5: Large margin classiﬁers. Positively and negatively labeled examples
are indicated by ﬁlled and hollowed circles, respectively. (Left) A linearly sep-
arable data set. The support vectors for the maximum margin hyperplane H
parallel to the decision boundary. Large
lie on two hyperplanes H
margin classiﬁers maximize the distance between these hyperplanes. (Right)
A linearly inseparable data set. The support vectors in this case also include
examples lying between H

that cannot be classiﬁed correctly.

and H

and H

+

−

+

−

∗

is that the two classes are linearly separable by a hyperplane that passes
through the origin. Let w
denote the hyperplane’s normal vector; the clas-
siﬁcation rule, given by y = sgn(w
∗Tx), labels examples based on whether
they lie above or below the hyperplane. The maximum margin hyperplane
is computed by solving the constrained optimization:

minimize
subject to

1
2 wTw
yi wTxi ≥ 1, i = 1, 2, . . . , N.

(4.6)

The constraints in this optimization ensure that all the training examples are
correctly labeled by the classiﬁer’s decision rule. While a potentially inﬁnite
number of hyperplanes satisfy these constraints, the classiﬁer with mini-
mal||w|| (and thus maximal margin) has provably small error rates (Vapnik,
1998) on unseen examples. The optimization problem in equation 4.6 is a
convex quadratic programming problem in the vector w. Its dual formula-
tion is

(cid:18)
αi ≥ 0, i = 1, 2, . . . , N.

i x j −(cid:18)

αi α j yi yj xT

N
i, j=1

1
2

minimize
subject to

i

αi

(4.7)

Let A denote the positive semideﬁnite matrix with elements yi yj xT
i x j, and
let e denote the column vector of all ones. The objective function for the
dual in equation 4.7 can then be written as the NQP problem:

L(α) = 1
2

αTAα − eTα,

(4.8)

2026

support vectors

5

iteration: 0

iteration: 100

iteration: 200

iteration: 300

200 400 600 800 1000 1200

0
5

0
4
2
0
2
1
0
0

F. Sha, Y. Lin, L. Saul, and D. Lee

support vectors

2
1
0
1
0.5
0
1
0.5
0
1
0.5
0
0

iteration: 400

iteration: 500

iteration: 600

iteration: 700

200 400 600 800 1000 1200

Figure 6: Convergence of the multiplicative updates in equation 2.6 for a large
margin classiﬁer distinguishing handwritten digits (2s versus 3s). The coefﬁ-
cients corresponding to nonsupport vectors are quickly attenuated to zero.

and the multiplicative updates in equation 2.6 can be used to ﬁnd the
optimal α∗
. Labeled examples that correspond to active constraints in equa-
tion 4.6 are called support vectors. The normal vector w
is completely
determined by support vectors since the solution to the primal problem,
equation 4.6, is given by

∗

∗ =

w

yi α∗

i xi .

(4.9)

(cid:15)

i

i

For nonsupport vectors, the inequalities are strictly satisﬁed in equation 4.6,
and their corresponding Lagrange multipliers vanish (that is, α∗

= 0).

Figure 6 illustrates the convergence of the multiplicative updates for
large margin classiﬁcation of handwritten digits (Sch¨olkopf et al., 1997).
The plots show the estimated support vector coefﬁcients αi after different
numbers of iterations of the multiplicative updates. The horizontal axes in
these plots index the coefﬁcients αi of the N = 1389 training examples, and
the vertical axes show their values. For ease of visualization, the training
examples were ordered so that support vectors appear to the left and non-
support vectors to the right. The coefﬁcients were uniformly initialized as
αi = 1. Note that the nonsupport vector coefﬁcients are quickly attenuated
to zero.

Multiplicative updates can also be used to train large margin classiﬁers
when the labeled examples are not linearly separable, as shown in Figure 5
(right). In this case, the constraints in equation 4.6 cannot all be simultane-
ously satisﬁed, and some of them must be relaxed. One simple relaxation

Multiplicative Updates for Nonnegative Quadratic Programming

2027

is to permit some slack in the constraints but to penalize the degree of slack
as measured by the (cid:4)1-norm:
minimize wTw + C
subject to

(4.10)

(cid:18)
ξi
yi wTxi ≥ 1 − ξi
ξi ≥ 0.

i

The parameter C balances the slackness penalty versus the large margin
criterion; the resulting classiﬁers are known as soft margin classiﬁers. The
dual of this optimization is an NQP problem with the same quadratic form
as the linearly separable case but with box constraints:

αTAα − eTα

minimize
subject to 0 ≤ αi ≤ C, i = 1, 2, . . . , N.

1
2

(4.11)

The clipped multiplicative updates in equation 2.8 can be used to perform
this optimization for soft margin classiﬁers.

Another way of handling linear inseparability is to embed the data in a
high-dimensional nonlinear feature space and then to construct the maxi-
mum margin hyperplane in feature space. The nonlinear mapping is per-
formed implicitly by specifying a kernel function that computes the inner
product in feature space. The optimization for the maximum margin hy-
perplane in feature space has the same form as equation 4.7 except that
the original Gram matrix with elements xT
i x j is replaced by the kernel ma-
trix of inner products in feature space (Cristianini & Shawe-Taylor, 2000;
Vapnik, 1998). The use of multiplicative updates for large margin classiﬁ-
cation of linearly inseparable data sets is discussed further in (Sha et al.,
2003a, 2003b).

A large number of algorithms have been investigated for nonnegative
quadratic programming in SVMs. Among them, there are many criteria that
could be compared, such as speed of convergence, memory requirements,
and ease of implementation. The main utility of the multiplicative updates
appears to lie in their ease of implementation. The updates are very well
suited for applications involving small to moderately sized data sets, where
computation time is not a primary concern and where the simple, parallel
form of the updates makes them easy to implement in high-level languages
such as Matlab.

The most popular methods for training SVMs—so-called subset
methods—take a fundamentally different approach to NQP. In contrast
to the parallel form of the multiplicative updates, subset methods split the
variables at each iteration into two sets: a ﬁxed set in which the variables
are held constant and a working set in which the variables are optimized

2028

F. Sha, Y. Lin, L. Saul, and D. Lee

by an internal subroutine. At the end of each iteration, a heuristic is used to
transfer variables between the two sets and improve the objective function.
Two subset methods have been widely used for training SVMs. The ﬁrst
is the method of sequential minimal optimization (SMO) (Platt, 1999), which
updates only two coefﬁcients of the weight vector per iteration. In this case,
there exists an analytical solution for the updates, so that one avoids the
expense of an iterative optimization within each iteration of the main loop.
SMO enforces the sum and box constraints for soft margin classiﬁers. If the
sum constraint is lifted, then it is possible to update the coefﬁcients of the
weight vector sequentially, one at a time, with an adaptive learning rate that
ensures monotonic convergence. This coordinate descent approach is also
known as the kernel Adatron (Friess, Cristianini, & Campbell, 1998). SMO
and kernel Adatron are among the most viable methods for training SVMs
on large data sets, and experiments have shown that they converge much
faster than the multiplicative updates (Sha et al., 2003b). Nevertheless, for
simplicity and ease of implementation, we believe that the multiplicative
updates provide an attractive starting point for experimenting with large
margin classiﬁers.

5 Summary and Discussion

In this article, we have described multiplicative updates for solving convex
problems in NQP. The updates are distinguished by their simplicity in both
form and computation. We showed that the updates lead to monotonic
improvement in the objective function for NQP and converge to global
minima. The updates can be viewed as generalizations of the iterative rules
previously developed for nonnegative matrix factorization (Lee & Seung,
2001). They have a strikingly different form from other additive and multi-
plicative updates used in statistical learning.

We can also compare the multiplicative updates to interior point methods
(Wright, 1997) that have been studied for NQP. These methods start from an
interior point of the feasible region and then iteratively update the current
estimate of the minimizer along particular search directions. There are many
ways to determine the search directions—for example, using Newton’s
method to solve the equations characterizing primal and dual optimality, or
approximating the original optimization problem by a simpler subproblem
inside the trust region of the current estimate. The resulting updates take
an elementwise additive form, stepping along a particular search direction
in the nonnegative orthant. The step size is chosen to ensure that the search
remains in the feasible region while making progress toward the minimizer.
If the search direction corresponds to the negative gradient of the objective
function F (v), then the updates reduce to steepest descent. Most of the
computational effort in interior point methods is devoted to deriving search
directions and maintaining the feasibility of the updated estimates.

Multiplicative Updates for Nonnegative Quadratic Programming

2029

The multiplicative updates are similar to trust region methods in spirit.
Instead of constructing an ellipsoidal trust region centered at the current
estimate of the minimizer, however, we have derived the updates from
a nonlinear yet analytically tractable auxiliary function. Optimizing the
auxiliary function guarantees the improvement of the objective function in
the nonnegative orthant, which can be viewed as the trust region for each
update. The search direction derived from the auxiliary function is very
simple to compute, as opposed to that of many interior point methods. It
remains an open question to quantify more precisely the rate of convergence
of the multiplicative updates.

Though not as well theoretically characterized as traditional methods for
NQP, the multiplicative updates have nevertheless proven extremely useful
in practice. In this article, we have described our own use of the updates
for acoustic echo cancellation and large margin classiﬁcation. Meanwhile,
others have applied the updates to NQP problems that arise in the analysis
of astrophysical data (Diego et al., 2007). We are hopeful that more appli-
cations will continue to emerge in other areas of neural computation and
statistical learning.

Appendix: Zangwill’s Convergence Theorem

Zangwill’s convergence theorem enumerates the conditions for global con-
vergence of general iterative procedures. The theorem is presented in its
most general form in Zangwill (1969). For our purposes here, we speciﬁ-
cally state the theorem in the context of optimization. Let F (v) denote the
objective function to be minimized by an iterative update rule vk+1 = M(vk),
where the domain and range of the mapping M : V → V lie in the feasible
set. Suppose that we apply the mapping M to generate a sequence of pa-
rameters {vk}∞
k=1. Our goal is to examine whether this sequence converges
to a point in a “desired” set V∗ ⊂ V. Assume that the objective function F (v)
and the mapping M are continuous and that the following conditions are
met:

1. All points vk are in a compact set that is a subset of V.
2. If v /∈ V∗
3. If v ∈ V∗

, then the update leads to a strict reduction in the objective
, then either M(v) = v or F (M(v)) ≤ F (v).

function. That is, F (M(v)) < F (v).

Zangwill’s convergence theorem states that under these conditions, either
the sequence {vk}∞
, or all accumulation points
of the sequence are in the set V∗

k=1 stops at a point in the set V∗

.

The theorem can be used to analyze the convergence of iterative up-
date rules by verifying that these three conditions hold for particular “de-
sired” sets. For the multiplicative updates in equation 2.6, the theorem
implies convergence to a ﬁxed point v = M(v). It does not, however, imply

2030

F. Sha, Y. Lin, L. Saul, and D. Lee

convergence to the unique ﬁxed point that is the global minimizer of the
objective function. In particular, if we constrain V∗
to contain only the global
minimizer, then condition (2), which stipulates that F (v) strictly decreases
under the mapping M for all V /∈ V∗
, is clearly violated due to the existence
of “spurious” ﬁxed points. The proof of global convergence thus requires
the extra machinery of section 3.2.

Acknowledgments

This work was supported by NSF Award 0238323.

References

Allen, J. B., & Berkley, D. A. (1979). Image method for efﬁciently simulating small-

room acoustics. Journal of the Acoustical Society of America, 65, 943–950.

Bauer, E., Koller, D., & Singer, Y. (1997). Update rules for parameter estimation in
Bayesian networks. In Proceedings of the Thirteenth Annual Conference on Uncer-
tainty in AI (pp. 3–13). San Francisco: Morgan Kaufmann.

Bertsekas, D. P. (1999). Nonlinear programming (2nd ed.). Belmont, MA: Athena Sci-

entiﬁc.

Cristianini, N., & Shawe-Taylor, J. (2000). An introduction to support vector machines.

Cambridge: Cambridge University Press.

Darroch, J. N., & Ratcliff, D. (1972). Generalized iterative scaling for log-linear mod-

els. Annals of Mathematical Statistics, 43, 1470–1480.

Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39,
1–38.

Diego, J. M., Tegmark, M., Protopapas, P., & Sandvik, H. B. (2007). Combined re-
construction of weak and strong lensing data with WSLAP. Monthly Notices of the
Royal Astronomical Society, 375, 958–970.

Friess, T., Cristianini, N., & Campbell, C. (1998). The Kernel-Adatron algorithm: A
fast and simple learning procedure for support vector machines. In Proceedings
of the Fifteenth International Conference on Machine Learning (pp. 188–196). San
Francisco: Morgan Kaufmann.

Kivinen, J., & Warmuth, M. (1997). Exponentiated gradient versus gradient descent

for linear predictors. Information and Computation, 132, 1–63.

Lee, D. D., & Seung, H. S. (1999). Learning the parts of objects with nonnegative

matrix factorization. Nature, 401, 788–791.

Lee, D. D., & Seung, H. S. (2001). Algorithms for non-negative matrix factorization.
In T. K. Leen, T. G. Dietterich, & V. Tresp (Eds.), Advances in neural information
processing systems, 13 (pp. 556–562). Cambridge, MA: MIT Press.

Lin, Y., Lee, D. D., & Saul, L. K. (2004). Nonnegative deconvolution for time of
arrival estimation. In Proceedings of the International Conference of Speech, Acoustics,
and Signal Processing (ICASSP-2004) (Vol. 2, pp. 377–380). Piscataway, NJ: IEEE.

Platt, J. (1999). Fast training of support vector machines using sequential minimal
optimization. In B. Sch¨olkopf, C. J. C. Burges, & A. J. Smola (Eds.), Advances

Multiplicative Updates for Nonnegative Quadratic Programming

2031

in kernel methods—Support vector learning (pp. 185–208). Cambridge, MA: MIT
Press.

Saul, L. K., Sha, F., & Lee, D. D. (2003). Statistical signal processing with nonnegativity
constraints. In Proceedings of the Eighth European Conference on Speech Communica-
tion and Technology (Vol. 2, pp. 1001–1004). Geneva, Switzerland.

Sch¨olkopf, B., Sung, K., Burges, C., Girosi, F., Niyogi, P., Poggio, T., & Vapnik, V.
(1997). Comparing support vector machines with gaussian kernels to radial basis
function classiers. IEEE Transactions on Signal Processing, 45, 2758–2765.

Seraﬁni, T., Zanghirati, G., & Zanni, L. (2005). Gradient projection methods for
quadratic programs and applications in training support vector machines. Opti-
mization Methods and Software, 20, 353–378.

Sha, F., Saul, L. K., & Lee, D. D. (2003a). Multiplicative updates for nonnegative
quadratic programming in support vector machines. In S. Becker, S. Thrun, &
K. Obermayer (Eds.), Advances in neural information processing systems, 15 (pp. 897–
904). Cambridge, MA: MIT Press.

Sha, F., Saul, L. K., & Lee, D. D. (2003b). Multiplicative updates for large margin clas-
siﬁers. In Proceedings of the Sixteenth Annual Conference on Computational Learning
Theory (COLT-03) (pp. 188–202). Berlin: Springer.

Vapnik, V. (1998). Statistical learning theory. New York: Wiley.
Wright, S. J. (1997). Primal-dual interior point methods. Philadelphia, PA: SIAM.
Zangwill, W. J. (1969). Nonlinear programming: A uniﬁed approach. Englewood Cliffs,

NJ: Prentice Hall.

Received April 10, 2006; accepted August 10, 2006.

