A Generalized Linear Model for Principal

Component Analysis of Binary Data

January 6, 2003

Andrew I. Schein, Lyle H. Ungar and Lawrence K. Saul.
Department of Computer and Information Science
The University Of Pennsylvania. Philadelphia, PA.

Ninth International Workshop on AI and Statistics

PCA and LPCA

Principal Component Analysis is commonly called PCA.

PCA is a widely-used dimensionality reduction technique.

PCA handles real-valued data through a Gaussian assumption.

Today we will explore a different assumption for binary data.

Logistic PCA is to (Linear) PCA

as

Logistic Regression is to Linear Regression

Schein et al.

AI & Statistics 2003, p.1

Talk Outline

1. Linear PCA of Real-Valued Data (review)

2. Logistic PCA of Binary Data

Our Contributions Consist of:

3. Model Fitting by Alternating Least Squares

4. Experimental Results on 4 Natural Data Sets

Schein et al.

AI & Statistics 2003, p.2

Multivariate Binary Data

Person/Movie Co−Occurrence: Who Rated What

i

s
e
v
o
M

200

400

600

800

1000

1200

1400

1600

200

400

People

600

800

Schein et al.

AI & Statistics 2003, p.3

Visualizing PCA

1

.5

0

−.5

i

s
x
A
Y

 

−1
−1

−.5

0

X Axis

.5

1

Schein et al.

AI & Statistics 2003, p.4

Applications of PCA

(cid:15) Noise Removal

(cid:15) Dimensionality Reduction

(cid:15) Data Compression

(cid:15) Visualization

(cid:15) Exploratory Data Analysis

(cid:15) Feature Extraction

Schein et al.

AI & Statistics 2003, p.5

PCA as Least-Squares Decomposition

Error(V L; U ) =

N

Xn=1

jjXn (cid:0) UnV Ljj2

X = The Data: N (cid:2) D

U = Latent Coordinates: N (cid:2) L

V = Orthogonal Latent Axes: L (cid:2) D

L << D, L is the dimensionality of the latent space.

Schein et al.

AI & Statistics 2003, p.6

Gaussian Interpretation of PCA

When (cid:27) is known there is an equivalent model:

Xnd (cid:24) N ((U V L)nd; (cid:27)2)

Maximum Likelihood Objective = Least Squares Loss

So PCA assumes a Gaussian distribution on X.

Schein et al.

AI & Statistics 2003, p.7

Generalized Principal Component Analysis (GPCA)

Collins et al. (2001) propose a generalized scheme for PCA.

Deﬁne a constrained decomposition of the natural parameter

(cid:2)nd = (U V )nd; dim(U ) = N (cid:2) L; dim(V ) = L (cid:2) D:

N = Number of Observations
D = Dimensionality of Data
L = Dimensionality of Latent Space

L(V; U ) = (cid:0)Xn Xd

log P(Xndj(cid:2)nd)

Insert your favorite exponential family distribution to instantiate P.

Schein et al.

AI & Statistics 2003, p.8

The Logistic Function

(cid:27)((cid:18)) =

1

1 + exp((cid:0)(cid:18))

1

0.8

0.6

)

(q

0.4

0.2

0
−4

−3

−2

−1

0

1

2

3

4

Schein et al.

AI & Statistics 2003, p.9

q
s
Logistic PCA Model

Inserting Bernoulli Distribution We Get Log-Likelihood:

L = Xn;d

[Xnd log (cid:27)((cid:2)nd) + (1 (cid:0) Xnd) log (cid:27)((cid:0)(cid:2)nd)]

subject to constraint

(cid:2)nd = Xl

UnlVld

N = Number of Observations
D = Dimensionality of Data
L = Dimensionality of Latent Space

Schein et al.

AI & Statistics 2003, p.10

How to Fit LPCA?

Collins et al. (2001) propose a general strategy for ﬁtting GPCA.

Applying this framework to the LPCA case looks hard if not intractable.

We take the approach of ﬁtting LPCA through specialized strategies.

Our methods exploit bounds on the logistic function.

Schein et al.

AI & Statistics 2003, p.11

Deﬁning the Auxiliary Function for LPCA

A useful fact:

log (cid:27)((cid:18)) = (cid:0) log 2 + (cid:18)=2 (cid:0) log cosh((cid:18)=2)

We exploit a bound:

log cosh((cid:18)=2) (cid:20) log cosh((cid:18)0=2) + ((cid:18)2 (cid:0) (cid:18)2

0)(cid:20)tanh((cid:18)0=2)

4(cid:18)0

(cid:21)

[Jaakkola and Jordan, 1997. Tipping, 1999.]

The bound is concave and quadratic in the parameter (cid:18).

Schein et al.

AI & Statistics 2003, p.12

Visualizing the Approximation of (cid:27)
1

s (q )
approximation

0 = 2

)

(q

0.5

0
−4

−2

0

2

4

Schein et al.

AI & Statistics 2003, p.13

q
s
q
Model Fitting by Alternating Least Squares

We develop a model ﬁtting strategy that alternates between two steps:

(cid:15) Fix V , ﬁnd the least squares solution for U rows.

(cid:15) Fix U, ﬁnd the least squares solution for V columns.

Each iteration guarantees an improvement in log-likelihood.

The likelihood structure: global vs. local maxima is unknown.

Schein et al.

AI & Statistics 2003, p.14

A Related Use of the Bound

Normal Linear Factor Analysis (NLFA) is a generative cousin of PCA.

[Tipping, 1999], uses the bound to ﬁt a logit/normit factor analysis.

Logit/normit factor analysis is a type of factor analysis for binary data.

LPCA is binary PCA

while

logit/normit factor analysis is binary factor analysis

We follow Tipping’s factor analysis strategy in ﬁtting LPCA.

Schein et al.

AI & Statistics 2003, p.15

Logit/Normit Factor Analysis

Maximize:

L = Xn;d

Xnd log (cid:27)((cid:2)nd) + (1 (cid:0) Xnd) log (cid:27)((cid:0)(cid:2)nd)

subject to constraint

(cid:2)nd = Xl

UnlVld; where l is a latent space dimension

and Un (cid:24) N (0; I)

Schein et al.

AI & Statistics 2003, p.16

Logit/Normit Factor Analysis

Un (cid:24) N (0; I)

Fitting the Un in logit/normit factor is harder than in LPCA.

It requires an additional variational approximation and iterative process.

Model ﬁtting improves the lower bound on the log-likelihood.

...not necessarily the log-likelihood itself.

In contrast, ALS for LPCA guarantees an increase in the log-likelihood.

Schein et al.

AI & Statistics 2003, p.17

Example in 3 Dimensions

6

1

2

0.75

0.5

0.25

0
1

0.75

0.5

0.25

Y Axis

6
0

0

0.5

0.25

X Axis

1

1

0.75

i

s
x
A
Z

 

Schein et al.

AI & Statistics 2003, p.18

Example in 3 Dimensions

6

1

2

0.75

0.5

0.25

0
1

0.75

0.5

0.25

Y Axis

6
0

0

0.5

0.25

X Axis

1

1

0.75

i

s
x
A
Z

 

Schein et al.

AI & Statistics 2003, p.19

Example in 3 Dimensions

6

1

2

0.75

0.5

0.25

0
1

0.75

0.5

0.25

Y Axis

6
0

0

0.5

0.25

X Axis

1

1

0.75

i

s
x
A
Z

 

Schein et al.

AI & Statistics 2003, p.20

Empirical Evaluation: Data Reconstruction

Original
Data

Compressed

Data

Reconstructed

Data

X = Original Data
R = Reconstructed Data

N = Number of Observations
D = Dimensionality of the Data

Error = PN

n PD

d jXnd (cid:0) Rndj
N (cid:3) D

Schein et al.

AI & Statistics 2003, p.21

Microsoft Web Log Reconstruction Results

Web log shows URL visitation by anonymized users.

Data Set: a matrix of users and URLS clicked on

N = 32711
D = 285
Density = 0:011

Observations are a session
URLS Clicked

Data Set Task: Build a recommender system of URLS.

Our Task: Data Reconstruction

Schein et al.

AI & Statistics 2003, p.22

Microsoft Web Log Reconstruction Results

Error Rates (%)

L
1
2
4
8

Linear PCA Logistic PCA
1.52
1.41
1.36
1.11

1.28
1.15
0.760
0.355

1 LPCA dimension ’ 6 PCA dimensions

Schein et al.

AI & Statistics 2003, p.23

Advertising Data Reconstruction Results

A UC Irvine data set of web linked images and surrounding features.

N = 3279 image links
D = 1555 context features
density = 0:072

Data Set Task: Predict whether an image is an advertisement

Our Task: Data Reconstruction

Features include phrases in the anchor text and around image:

microsoft.com, toyotaofroswell.com, home+page

Schein et al.

AI & Statistics 2003, p.24

Advertising Data Reconstruction Results

Error Rates (%)

L
1
2
4
8

Linear PCA Logistic PCA
2.68
2.39
2.17
1.76

1.97
1.20
0.626
0.268

1 LPCA dimension ’ 7 PCA dimensions

Schein et al.

AI & Statistics 2003, p.25

Other Data Sets (in paper)

(cid:15) Microarray Gene Expression Data

– Observations are genes
– Attributes are environmental conditions
– Binary values indicate whether genes are expressed or not

(cid:15) MovieLens Movie Ratings Data

– Observations are users
– Attributes are movies
– Binary values indicate whether a user rated a movie or not

Schein et al.

AI & Statistics 2003, p.26

Related Models

Both of these models share a decomposition:

(cid:2)nd = (U V )nd

(cid:15) Factor Analysis: A generative relative of PCA

(cid:15) Multinomial PCA (MPCA): A multinomial, generative variant of PCA

MPCA is represented in the proceedings: [Buntine and Perttu, 2003].

Schein et al.

AI & Statistics 2003, p.27

Summary

(cid:15) We derive and implement the ALS algorithm for ﬁtting LPCA.

(cid:15) In data reconstruction experiments, LPCA outperforms PCA.

(cid:15) LPCA is well suited for smoothed probability models of binary data:

– People and URLs they click
– Phrase features surrounding image links

(cid:15) Future work will explore LPCA in other traditional PCA tasks.

– Feature extraction
– Machine learning

Schein et al.

AI & Statistics 2003, p.28

