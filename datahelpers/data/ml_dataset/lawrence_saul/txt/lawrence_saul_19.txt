LARGE MARGIN GAUSSIAN MIXTURE MODELING FOR

PHONETIC CLASSIFICATION AND RECOGNITION

Fei Sha and Lawrence K. Saul

University of Pennsylvania

Department of Computer and Information Science

3330 Walnut Street, Levine Hall
Philadelphia, PA 19104, USA

ABSTRACT

We develop a framework for large margin classiﬁcation by
Gaussian mixture models (GMMs). Large margin GMMs
have many parallels to support vector machines (SVMs), but
with classes modeled by ellipsoids instead of half-spaces.
Model parameters are trained discriminatively to maximize
the margin of correct classiﬁcation, as measured in terms of
Mahalanobis distances. The required optimization is convex
over the model’s parameter space of positive semideﬁnite ma-
trices and can be performed efﬁciently. Large margin GMMs
are naturally suited to large problems in multiway classiﬁca-
tion; we apply them to phonetic classiﬁcation and recogni-
tion on the TIMIT database. On both tasks, we obtain signiﬁ-
cant improvement over baseline systems trained by maximum
likelihood estimation. For the problem of phonetic classiﬁ-
cation, our results are competitive with other state-of-the-art
classiﬁers, such as hidden conditional random ﬁelds.

1. INTRODUCTION

Much of the acoustic-phonetic modeling in automatic speech
recognition (ASR) is handled by Gaussian mixture models
(GMMs) [1]. It is widely recognized that maximum likeli-
hood (ML) estimation of GMMs does not directly optimize
the performance of these models as classiﬁers. It is therefore
of interest to develop alternative learning paradigms that op-
timize discriminative measures of performance [2, 3, 4].

Support vector machines (SVMs) currently provide state-
of-the-art performance for many problems in pattern recog-
nition [5]. The simplest setting for SVMs is binary classiﬁ-
cation. If the positively and negatively labeled examples are
linearly separable, SVMs compute the linear decision bound-
ary that maximizes the “margin” of correct classiﬁcation—
that is, the distance of the closest example(s) to the separating
hyperplane. If the labeled examples are not linearly separa-
ble, the “kernel trick” can be used to map the examples into a
nonlinear feature space and to compute the maximum margin
hyperplane in this space. Alternately, or in conjunction with
the kernel trick, the optimization for SVMs can be relaxed to

permit margin violations (i.e., incorrectly labeled examples)
in the training data.

For various reasons, it can be challenging to apply SVMs
to large problems in multiway classiﬁcation. First, to ap-
ply the kernel trick (which is required for nonlinear decision
boundaries), one must construct a large kernel matrix with
as many rows and columns as training examples. Second,
the training complexity increases with the number of classes,
depending to some extent on the way that binary SVMs are
generalized to multiway classiﬁcation.

In this paper, we develop a framework for large margin
classiﬁcation by GMMs. As in SVMs, our approach is based
on the idea of margin maximization. Intuitively, we show how
to train “large margin” GMMs that maximize the Mahanalo-
bis distance of labeled examples from the decision boundaries
that deﬁne competing classes. As in SVMs, the parameters
of large margin GMMs are trained by a convex optimization
that focuses on examples near the decision boundaries. After
developing the basic approach in section 2, we discuss exten-
sions for segmental training and outlier handling in section 3
and report experimental results on phonetic classiﬁcation and
recognition in section 4.

Our approach has certain advantages over SVMs for large
problems in multiway classiﬁcation. For example, the classes
in large margin GMMs are modeled by ellipsoids—which in-
duce nonlinear decision boundaries in the input space—as op-
posed to the half-spaces and hyperplanes in SVMs. Because
the kernel trick is not necessary to induce nonlinear decision
boundaries, large margin GMMs are more readily trained on
very large and difﬁcult data sets, as arise in ASR.

2. LARGE MARGIN MIXTURE MODELS

We begin by describing large margin GMMs in the simple
setting where each class is modeled by a single ellipsoid. We
then extend this framework to the case where each class is
modeled by one or more ellipsoids. Finally, we relate our
framework to other discriminative paradigms that have been
proposed for training GMMs.

−Ψc µc
c Ψcµc + θc

(2)

(cid:21)
(cid:20) x

.

(cid:21)

2.1. Large margin classiﬁcation

The simplest large margin GMM represents each class of la-
beled examples by a single ellipsoid. Each ellipsoid is pa-
rameterized by a vector “centroid” µ ∈ <d and a positive
semideﬁnite “orientation” matrix Ψ ∈ <d×d. These param-
eters are analogous to the means and inverse covariance ma-
trices of multivariate Gaussians, but they are not estimated in
the same way. In addition, a nonnegative scalar offset θ ≥ 0
for each class is used in the scoring procedure.

Let (µc,Ψc,θc) denote the centroid, orientation matrix,
and scalar offset representing examples in class c. We label
an example x ∈ <d by whichever ellipsoid has the smallest
Mahanalobis distance (plus offset) to its centroid:

(cid:8)(x−µc)TΨc(x−µc) + θc

(cid:9) .

(1)

y = argmin

c

The goal of learning is to estimate the parameters (µc,Ψc,θc)
for each class of labeled examples that optimize the perfor-
mance of this decision rule.

It is useful to collect the ellipsoid parameters of each class

in a single “enlarged” positive semideﬁnite matrix:

Φc =

−µT

(cid:20) Ψc
(cid:8)zTΦc z(cid:9) where

c Ψc

µT

We can then rewrite the decision rule in eq. (1) as simply:

c

.

1

z =

y = argmin

(3)
Here, z ∈ <d+1 is the vector created by appending a unit
element to x ∈ <d.
In this transformed representation,
the goal of learning is simply to estimate the single matrix
Φc ∈ <(d+1)×(d+1) for each class of labeled examples.
We now consider the problem of learning in more detail.
Let {(xn, yn)}N
n=1 denote a set of N labeled examples drawn
from C classes, where xn ∈ <d and yn ∈ {1, 2, . . . , C}.
In large margin GMMs, we seek matrices Φc such that all
the examples in the training set are correctly classiﬁed by a
large margin—i.e., situated far from the decision boundaries
that deﬁne competing classes. For the nth example with class
label yn, this condition can be written as:
n Φczn ≥ 1 + zT
zT

(4)
Eq. (4) states that for each competing class c 6= yn, the Maha-
lanobis distance (plus offset) to the cth centroid exceeds the
Mahalanobis distance (plus offset) to the target centroid by a
margin of at least one unit.

∀c 6= yn,

n Φynzn.

We adopt a convex loss function for training large margin
GMMs. Analogous to SVMs, the loss function has two terms,
one that penalizes margin violations of eq. (4) and one that
regularizes the matrices Φc. Letting [f]+ =max(0,f) denote
the so-called “hinge” function, we can write the loss function
for large margin GMMs as:
L = γ

(cid:2)1+zT

n (Φyn− Φc)zn

++X
(cid:3)

trace(Ψc). (5)

X

X

c6=yn

n

c

The second term regularizes the orientation matrices Ψc
which appear in the d × d upper left blocks of Φc. In real-
izable settings, where all the examples can be correctly clas-
siﬁed, the second term favors the “minimum trace” Maha-
lanobis metrics consistent with the unit margin constraints in
eq. (4). The relative weight of the two terms is controlled by
a hyperparameter γ >0 set by cross-validation.

The loss function in eq. (5) is a piecewise linear, convex
function of the matrices Φc, which are further constrained to
be positive semideﬁnite. Its optimization can thus be formu-
lated as a problem in semideﬁnite programming [6]. Such
problems can be generically solved by interior point algo-
rithms with polynomial time guarantees (though we imple-
mented a special-purpose solver using gradient-based meth-
ods for the results in this paper). Most importantly, eq. (5)
has the desirable property that its optimization is not plagued
by spurious local minima.

2.2. Mixture models

We now extend the previous model to represent each class by
multiple ellipsoids. This is analogous to modeling each class
by its own GMM, as opposed to a single Gaussian. Let Φcm
denote the matrix for the mth ellipsoid in class c. The most
straightforward extension is to imagine that each example xn
has not only a class label yn, but also a mixture component
label mn. The latter labels are not provided a priori, but we
can generate “proxy” labels by ﬁtting a GMM to the exam-
ples in each class by ML estimation, then for each example,
computing the mixture component with the highest posterior
probability under this GMM. Given joint labels (yn, mn), we
rewrite the large margin criterion in eq. (4) as:
n Φcmzn ≥ 1 + zT

e−zT

n Φynmnzn.

(6)

m

∀c 6= yn, − logX
follows from the fact that − logP
n Φynmnzn + logP

Eq. (6) states that for each competing class c 6= yn, the match
to any centroid in class c is worse than the match to the target
centroid by a margin of at least one unit. This interpretation

m e−am ≤ minm am.

m e−zT

classes and mixture components: P

The loss function for mixture models is a simple exten-
sion of eq. (5). We replace the hinge loss in the ﬁrst term
by [1 + zT
n Φcmzn]+, which penal-
izes violations of the margin inequalities in eq. (6). The regu-
larizer in the second term changes only to sum over different
cm trace(Ψcm). Due to
the “softmin” operation over mixture components, the result-
ing loss function is no longer piecewise linear in the matri-
ces Φcm; however, it is easy to verify that it remains convex.
Thus, even the optimization of this more general loss function
for large margin GMMs is quite tractable.

2.3. Relation to previous work

Our framework differs in important aspects from previous
frameworks for discriminative training of GMMs [2]. Sup-

pose the class-conditional densities p(x|y) are modeled by
GMMs. One common approach to discriminative training es-
P
timates the means, covariance matrices, and mixture weights
of these models that maximize the conditional log-likelihood
n log p(yn|xn). Such models generally outperform GMMs
P
that are estimated by maximizing the joint log-likelihood
n log p(xn, yn). In contrast to our framework, however, the
optimization of GMM parameters in this way is not convex.
Moreover, as a loss function, the conditional log-likelihood
does not focus on examples near the decision boundaries nor
incorporate the idea of a large margin.

The main difference between large margin GMMs and
SVMs is that classes are modeled by ellipsoids, rather than
half-spaces, and that the kernel trick (which involves main-
taining a large kernel matrix) is not required for nonlinear de-
cision boundaries. Of course, one can generate quadratic de-
cision boundaries in SVMs by choosing a polynomial kernel
of degree two, or by “expanding” the vectors xn to include
pairwise products of their original elements. Large margin
GMMs differ from such SVMs by restricting their quadratic
forms to be positive semideﬁnite and by imagining the sup-
port of each class as some bounded region in input space.
In addition, such SVMs cannot represent the large margin
GMMs in section 2.2, with multiple ellipsoids per class.

3. EXTENSIONS

Two further extensions of large margin GMMs are impor-
tant for problems in ASR: handling of outliers, and segmental
training. We describe each extension in isolation, assuming
for simplicity that each class is modeled by a single ellip-
soid, as in section 2.1. The generalization to the large margin
GMMs described in section 2.2 is straightforward, as is the
handling of outliers in combination with segmental training.

3.1. Handling of outliers

Many discriminative learning algorithms are sensitive to out-
liers. Margin-based loss functions,
in particular, do not
closely “track” the classiﬁcation error rate when the training
data has many outliers. We adopt a simple strategy to detect
outliers and reduce their malicious effect on learning.

Outliers are detected using ML estimates of the mean and
covariance matrix of each class. These estimates are used to
initialize matrices ΦML
of the form in eq. (2). Then, for each
example xn, we compute the accumulated hinge loss incurred
by violations of the large margin constraints in eq. (4):

c

n = X

hML

(cid:2)1+zT

c6=yn

n (ΦML
yn

− ΦML

c

)zn

(7)

(cid:3)

+

n ≥ 0 measures the decrease in the loss func-
Note that hML
tion when an initially misclassiﬁed example xn is “corrected”
during the course of learning. We associate outliers with large
values of hML
n .

Outliers distort the learning process by diverting its fo-
cus away from misclassiﬁed examples that could otherwise be
easily corrected. In particular, correcting one badly misclas-
siﬁed outlier decreases the cost function proposed in eq. (5)
more than correcting multiple examples that lie just barely
on the wrong side of a decision boundary. To ﬁx this sit-
uation, we reweight the hinge loss terms in eq. (5) involv-
ing example xn by a multiplicative factor of min(1, 1/hML
n ).
This reweighting equalizes the losses incurred by all initially
misclassiﬁed examples, thus reducing the malicious effect of
outliers. We compute the weighting factors once from the ML
estimates and hold them ﬁxed during discriminative training.
In practice, this scheme appears to work very satisfactorily.

3.2. Segmental training

The margin constraints in eq. (4) apply to individually labeled
examples. We can also relax them to apply, collectively, to
multiple examples known to share the same class label. This
is useful for ASR, where we can train on variable-length “seg-
ments”, consisting of multiple consecutive analysis frames,
all of which belong to the same phoneme. Speciﬁcally, let p
index the ‘ frames in the nth phonetic segment {xnp}‘
p=1. For
segmental training, we rewrite the constraints in eq. (4) as:
∀c 6= yn,

npΦcznp ≥ 1 +
zT

zT
npΦynznp, (8)

X

X

1
‘

p

1
‘

p

where the scores on both sides have been normalized by the
segment length. The segment-based constraint in eq. (8) is
especially well motivated if a segment-based decision rule is
used for classiﬁcation (e.g., y = argminc
p Φczp) as
opposed to the frame-based rule in eq. (1).

P

p zT

4. EXPERIMENTAL RESULTS

We applied large margin GMMs to well-benchmarked prob-
lems in phonetic classiﬁcation and recognition on the TIMIT
database [7, 8, 9, 4]. We used the standard partition of train-
ing and test data and the same development set as in earlier
work [9, 4]. All sa sentences were excluded. We mapped
the 61 phonetic labels in TIMIT to 48 classes and trained ML
and large margin GMMs for each classes. Results were eval-
uated by mapping these 48 classes to 39 “phones” to remove
further confusions, as in previous benchmarks. Our front
end computed mel-frequency cepstral coefﬁcients (MFCCs)
with 25 ms windows at a 10 ms frame rate. We retained the
ﬁrst 13 MFCC coefﬁcients of each frame, along with their
ﬁrst and second time derivatives. GMMs modeled these 39-
dimensional feature vectors after they were whitened by PCA.

4.1. Phonetic classiﬁcation

Phonetic classiﬁcation is an artiﬁcial but instructive prob-
lem in ASR. One assumes in this case that the speech has

# of mixture
components

1
2
4
8
16

classiﬁcation

recognition

baseline margin
baseline margin
40.1% 34.7%
32.1% 24.3%
36.5% 33.5%
30.1% 23.4%
34.7% 32.7%
27.8% 22.3%
25.9% 21.1% 32.7% 31.1%
31.7% 30.1%
26.0% 21.4%

Table 1. Error rates for phonetic classiﬁcation and recognition on
the TIMIT database. Large margin GMMs are compared to baseline
GMMs trained by ML estimation. See text for details.

been correctly segmented into phonetic units, but that the
phonetic class label of each segment is unknown. The in-
put to the classiﬁer is the “segment” of consecutive analysis
frames that spans precisely one phoneme. We trained large
margin GMMs using the segment-based margin criteria in
section 3.2 and compared them to baseline (full covariance)
GMMs trained by ML estimation. The baseline GMMs were
also used to determine the “proxy” labels for mixture com-
ponents in eq. (6) and to detect and reweight outliers, using
eq. (7). We used the development data set to choose the hyper-
parameter γ > 0 in eq. (5), to tune a unigram language model,
and to perform early stopping of the optimization procedure.
The training time on 1.1M frames (roughly, 140K segments)
ranged from 2-9 hours depending on the model size.

Table 1 shows the percentage of incorrectly classiﬁed pho-
netic segments on the TIMIT test set. Large margin GMMs
consistently and signiﬁcantly outperform baseline GMMs
with equal numbers of mixture components. The best large
margin GMM also yields a slightly lower classiﬁcation error
rate than state-of-the-art results (21.7%) obtained by hidden
conditional random ﬁelds [4].

4.2. Phonetic Recognition

The same baseline and large margin GMMs were used to
build phonetic recognizers.
The recognizers were ﬁrst-
order hidden Markov models (HMMs) with one context-
independent state per phonetic class. Baseline or large mar-
gin GMMs were used in these HMMs to compute the log
probabilities (or scores) of observed frames. In all HMMs,
we used the development set to optimize the weighting of
log transition probabilities. Table 1 compares the phone er-
ror rates of these HMMs, obtained by aligning the results
of Viterbi decoding with the ground-truth phonetic transcrip-
tions [7]. Again, the large margin GMMs lead to consis-
tently lower error rates, here computed as the sum of sub-
stitution, deletion, and insertion error rates. We also ob-
serve, as in previous work [3], that discriminatively-trained
context-independent phone models achieve lower error rates
than context-dependent models trained by ML estimation.

5. CONCLUSION

We have shown how to learn GMMs for multiway classiﬁ-
cation based on similar principles as large margin classiﬁca-
tion in SVMs. Classes are represented by ellipsoids whose
location, shape, and size are discriminatively trained to maxi-
mize the margin of correct classiﬁcation, as measured in terms
of Mahalanobis distances. The required optimization is con-
vex over the model’s parameter space of positive semideﬁ-
nite matrices. Applied to problems in phonetic classiﬁcation
and recognition, we showed that large margin GMMs led to
signiﬁcant improvement over baseline GMMs.
In ongoing
work, we are investigating phonetic recognizers with context-
dependent phone models, which are known to reduce phone
error rates [7, 8]. We are also studying schemes for integrat-
ing the large margin training of GMMs with sequence models
such as HMMs and/or conditional random ﬁelds [4].

Acknowledgments
This work was supported by NSF award 0238323. We thank
A. Gunawardana (Microsoft Research) and K. Crammer (Uni-
versity of Pennsylvania) for many useful discussions and
helpful correspondence.

6. REFERENCES

[1] S. Young, “Acoustic modelling for large vocabulary continuous
speech recognition,” in Computational Models of Speech Pat-
tern Processing, Keith Ponting, Ed., pp. 18–39. Springer, 1999.
[2] A. Nadas, “A decision-theoretic formulation of a training prob-
lem in speech recognition and a comparison of training by un-
IEEE
conditional versus conditional maximum likelihood,”
Transactions on Acoustics, Speech and Signal Processing, vol.
31, no. 4, pp. 814–817, 1983.

[3] S. Kapadia, V. Valtchev, and S.J. Young, “MMI training for
continuous phoneme recognition on the TIMIT database,” in
Proc. of ICASSP 93, 1993, vol. 2, pp. 491–494.

[4] A. Gunawardana, M. Mahajan, A. Acero, and J. C. Platt, “Hid-
den conditional random ﬁelds for phone classiﬁcation,” in Pro-
ceedings of Eurospeech 2005, Lisbon, 2005.

[5] B. Sch¨olkopf and A. J. Smola, Learning with Kernels, MIT

Press, Cambridge, MA, 2002.

[6] L. Vandenberghe and S. P. Boyd, “Semideﬁnite programming,”

SIAM Review, vol. 38(1), pp. 49–95, March 1996.

[7] K. F. Lee and H. W. Hon, “Speaker-independent phone recog-
nition using hidden Markov models,” IEEE Transactions on
Acoustics, Speech, and Signal Processing, vol. 37, no. 11, pp.
1641–1648, 1988.

[8] T. Robinson, “An application of recurrent nets to phone proba-
bility estimation,” IEEE Transactions on Neural Networks, vol.
5, no. 2, pp. 298–305, 1994.

[9] A. K. Halberstadt and J. R. Glass,

“Heterogeneous acoustic
measurements for phonetic classiﬁcation,” in Proceedings of
Eurospeech 97, Greece, 1997, pp. 401–404.

