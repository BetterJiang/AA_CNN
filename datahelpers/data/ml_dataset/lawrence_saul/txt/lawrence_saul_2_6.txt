Analysis and Extension of Spectral Methods

for Nonlinear Dimensionality Reduction

Fei Sha
FEISHA@CIS.UPENN.EDU
Lawrence K. Saul
LSAUL@CIS.UPENN.EDU
Department of Computer & Information Science, University of Pennsylvania, 3330 Walnut Street, Philadelphia, PA 19104

Abstract

Many unsupervised algorithms for nonlinear di-
mensionality reduction, such as locally linear
embedding (LLE) and Laplacian eigenmaps, are
derived from the spectral decompositions of
sparse matrices. While these algorithms aim to
preserve certain proximity relations on average,
their embeddings are not explicitly designed to
preserve local features such as distances or an-
gles. In this paper, we show how to construct a
low dimensional embedding that maximally pre-
serves angles between nearby data points. The
embedding is derived from the bottom eigenvec-
tors of LLE and/or Laplacian eigenmaps by solv-
ing an additional (but small) problem in semidef-
inite programming, whose size is independent of
the number of data points. The solution obtained
by semideﬁnite programming also yields an esti-
mate of the data’s intrinsic dimensionality. Ex-
perimental results on several data sets demon-
strate the merits of our approach.

1. Introduction

The problem of discovering low dimensional structure in
high dimensional data arises in many areas of information
processing (Burges, 2005). Much recent work has focused
on the setting in which such data is assumed to have been
sampled from a low dimensional submanifold. Many al-
gorithms, based on variety of geometric intuitions, have
been proposed to compute low dimensional embeddings in
this setting (Roweis & Saul, 2000; Tenenbaum et al., 2000;
Belkin & Niyogi, 2003; Donoho & Grimes, 2003; Wein-
berger & Saul, 2004). In contrast to linear methods such
as principal component analysis (PCA), these “manifold
learning” algorithms are capable of discovering highly non-
linear structure. Nevertheless, their main optimizations are

Appearing in Proceedings of the 22 nd International Conference
on Machine Learning, Bonn, Germany, 2005. Copyright by the
authors.

quite tractable—involving (for example) nearest neighbor
searches, least squares ﬁts, dynamic programming, eigen-
value problems, and semideﬁnite programming.

One large family of algorithms for manifold learning con-
sists of approaches based on the spectral decomposition of
sparse matrices (Chung, 1997). Algorithms in this family
include locally linear embedding (LLE) (Roweis & Saul,
2000) and Laplacian eigenmaps (Belkin & Niyogi, 2003).
The matrices in these algorithms are derived from sparse
weighted graphs whose nodes represent high dimensional
inputs and whose edges indicate neighborhood relations.
Low dimensional embeddings are computed from the bot-
tom eigenvectors of these matrices. This general approach
to manifold learning is attractive for computational reasons
because it reduces the main problem to solving a sparse
eigensystem. In addition, the resulting embeddings tend to
preserve proximity relations without imposing the poten-
tially rigid constraints of isometric (distance-preserving)
embeddings (Tenenbaum et al., 2000; Weinberger & Saul,
2004). On the other hand, this general approach also has
several shortcomings: (i) the solutions do not yield an es-
timate of the underlying manifold’s dimensionality; (ii) the
geometric properties preserved by these embedding are dif-
ﬁcult to characterize; (iii) the resulting embeddings some-
times exhibit an unpredictable dependence on data sam-
pling rates and boundary conditions.

In the ﬁrst part of this paper, we review LLE and Lapla-
cian eigenmaps and provide an extended analysis of these
shortcomings. As part of this analysis, we derive a theoret-
ical result relating the distribution of smallest eigenvalues
in these algorithms to a data set’s intrinsic dimensionality.

In the second part of the paper, we propose a framework to
remedy the key deﬁciencies of LLE and Laplacian eigen-
maps.
In particular, we show how to construct a more
robust, angle-preserving embedding from the spectral de-
compositions of these algorithms (one of which must be
run as a ﬁrst step). The key aspects of our framework
are the following: (i) a d-dimensional embedding is com-
puted from the m bottom eigenvectors of LLE or Lapla-
cian eigenmaps with m > d, thus incorporating informa-

Analysis and Extension of Spectral Methods for Nonlinear Dimensionality Reduction

tion that the original algorithm would have discarded for a
similar result; (ii) the new embeddings explicitly optimize
the degree of neighborhood similarity—that is, equivalence
up to rotation, translation, and scaling—with the aim of
discovering conformal (angle-preserving) mappings; (iii)
the required optimization is performed by solving an addi-
tional (but small) semideﬁnite program (Vandenberghe &
Boyd, 1996), whose size is independent of the number of
data points; (iv) the solution of the semideﬁnite program
yields an estimate of the underlying manifold’s dimension-
ality. Finally, we present experimental results on several
data sets, including comparisons with other algorithms.

2. Analysis of Existing Methods

The problem of manifold learning is simply stated. Assume
that high dimensional inputs have been sampled from a low
dimensional submanifold. Denoting the inputs by {(cid:1)x i}n
where (cid:1)xi∈Rp, the goal is to compute outputs {(cid:1)y i}n
i=1
i=1 that
provide a faithful embedding in d(cid:2) p dimensions.
LLE and Laplacian eigenmaps adopt the same general
framework for solving this problem.
In their simplest
forms, both algorithms consist of three steps: (i) construct
a graph whose nodes represents inputs and whose edges in-
dicate k-nearest neighbors; (ii) assign weights to the edges
in the graph and use them to construct a sparse positive
semideﬁnite matrix; (iii) output a low dimensional embed-
ding from the bottom eigenvectors of this matrix. The main
practical difference between the algorithms lies in the sec-
ond step of choosing weights and constructing a cost func-
tion. We brieﬂy review each algorithm below, then provide
an analysis of their particular shortcomings.

2.1. Locally linear embedding

LLE appeals to the intuition that each high dimensional in-
put and its k-nearest neighbors can be viewed as samples
from a small linear “patch” on a low dimensional subman-
ifold. Weights Wij are computed by reconstructing each
input (cid:1)xi from its k-nearest neighbors. Speciﬁcally, they are
chosen to minimize the reconstruction error:

E(W ) =

(cid:1)

(cid:2)(cid:2)(cid:2)(cid:1)xi −

(cid:1)

i

(cid:2)(cid:2)(cid:2)2

Wij(cid:1)xj

j

.

(1)

(cid:3)

The minimization is performed subject to two constraints:
(i) Wij =0 if (cid:1)xj is not among the k-nearest neighbors of (cid:1)x i;
j Wij = 1 for all i. The weights thus constitute a
(ii)
sparse matrix W that encodes local geometric properties
of the data set by specifying the relation of each input (cid:1)x i to
its k-nearest neighbors.

LLE constructs a low dimensional embedding by comput-
ing outputs (cid:1)yi ∈ Rd that respect these same relations to
their k-nearest neighbors. Speciﬁcally, the outputs are cho-

sen to minimize the cost function:

(cid:1)

(cid:2)(cid:2)(cid:2)(cid:1)yi −

(cid:1)

(cid:2)(cid:2)(cid:2)2

.

Wij(cid:1)yj

j

(2)

Φ(Y ) =

i

(cid:3)

The minimization is performed subject to two constraints
that prevent degenerate solutions: (i) the outputs are cen-
i (cid:1)yi = 0, and (ii) the outputs have unit covari-
tered,
ance matrix. The d-dimensional embedding that mini-
mizes eq. (2) subject to these constraints is obtained by
computing the bottom d + 1 eigenvectors of the matrix
Φ = (I−W)T (I−W). The bottom (constant) eigenvec-
tor is discarded, and the remaining d eigenvectors (each of
size n) yield the embedding (cid:1)yi∈Rd for i∈{1, 2, . . . , n}.

2.2. Laplacian eigenmaps

Laplacian eigenmaps also appeal to a simple geometric in-
tuition: namely, that nearby high dimensional inputs should
be mapped to nearby low dimensional outputs. To this end,
a positive weight Wij is associated with inputs (cid:1)xi and (cid:1)xj
if either input is among the other’s k-nearest neighbors.
Typically, the values of the weights are either chosen to
be constant, say Wij = 1/k, or exponentially decaying,
as Wij = exp(−(cid:3)(cid:1)xi − (cid:1)xj(cid:3)2/σ2) where σ2 is a scale pa-
rameter. Let D denote the diagonal matrix with elements
Dii =
j Wij. The outputs (cid:1)yi can be chosen to minimize
the cost function:

(cid:3)

Ψ(Y ) =

(cid:1)

ij

(cid:4)
Wij (cid:3)(cid:1)yi − (cid:1)yj(cid:3)2

DiiDjj

.

(3)

As in LLE, the minimization is performed subject to con-
straints that the outputs are centered and have unit covari-
ance. The embedding is computed from the bottom eigen-
vectors of the matrix Ψ = I−D− 1
2 . The matrix Ψ
is a symmetrized, normalized form of the graph Laplacian,
given by D−W. Again, the optimization is a sparse eigen-
value problem that scales well to large data sets.

2 WD− 1

2.3. Shortcomings for manifold learning

Both LLE and Laplacian eigenmaps can be viewed as spec-
tral decompositions of weighted graphs (Belkin & Niyogi,
2003; Chung, 1997). The complete set of eigenvectors of
the matrix Φ (in LLE) and Ψ (in Laplacian eigenmaps)
yields an orthonormal basis for functions deﬁned on the
graph whose nodes represent data points. The eigenvec-
tors of LLE are ordered by the degree to which they reﬂect
the local linear reconstructions of nearby inputs; those of
Laplacian eigenmaps are ordered by the degree of smooth-
ness, as measured by the discrete graph Laplacian. The
bottom eigenvectors from these algorithms often produce
reasonable embeddings. The orderings of these eigenvec-
tors, however, do not map precisely onto notions of local

Analysis and Extension of Spectral Methods for Nonlinear Dimensionality Reduction

(cid:2)(cid:1)(cid:1)(cid:3)

(cid:2)(cid:1)(cid:1)(cid:4)

(cid:2)(cid:1)(cid:1)(cid:5)

(cid:2)(cid:1)(cid:1)(cid:6)

(cid:2)(cid:1)(cid:1)(cid:7)

(cid:2)(cid:1)(cid:1)(cid:2)(cid:1)

Figure1.Top. Data set of n = 1000 inputs randomly sampled
from a “Swiss roll”. Bottom. Two dimensional embedding and
ten smallest nonzero eigenvalues computed by LLE.

distance or angle preservation, nor do their smallest eigen-
values have a telltale gap that yields an estimate of the un-
derlying manifold’s dimensionality. Evidence and implica-
tions of these shortcomings are addressed in the next sec-
tions.

2.4. Empirical results
Fig. 1 shows the results of LLE applied to n = 1000 inputs
sampled from a “Swiss roll” (using k = 6 nearest neigh-
bors). The ten smallest nonzero eigenvalues of the ma-
trix Φ are also plotted on a log scale. While LLE does
unravel this data set, the aspect ratio and general shape of
its solution do not reﬂect the underlying manifold. There is
also no apparent structure in its bottom eigenvalues (such
as a prominent gap) to suggest that the inputs were sampled
from a two dimensional surface. Such results are fairly typ-
ical: while the algorithm often yields embeddings that pre-
serve proximity relations on average, it is difﬁcult to char-
acterize their geometric properties more precisely. This
behavior is also manifested by a somewhat unpredictable
dependence on the data sampling rate and boundary condi-
tions. Finally, in practice, the number of nearly zero eigen-
values has not been observed to provide a robust estimate of
the underlying manifold’s dimensionality (Saul & Roweis,
2003).

2.5. Theoretical analysis

How do the smallest eigenvalues of LLE and Laplacian
eigenmaps reﬂect the underlying manifold’s dimensional-
ity, if at all? In this section, we analyze a simple setting
in which the distribution of smallest eigenvalues can be
precisely characterized. Our analysis does in fact reveal
a mathematical relationship between these methods’ eigen-
spectra and the intrinsic dimensionality of the data set. We

suspect, however, that this relationship is not likely to be of
much practical use for estimating dimensionality.
Consider inputs (cid:1)xi ∈ Rd that lie on the sites of an inﬁnite
d-dimensional hypercubic lattice. Each input has 2d neigh-
bors separated by precisely one lattice spacing. The two
dimensional case is illustrated in the left panel of Fig. 2.
Choosing k = 2d nearest neighbors to construct a sparse
graph and assigning constant weights to the edges, we ob-
tain an (inﬁnite) weight matrix W for Laplacian eigenmaps
given by:

(cid:5)

Wij =

1
2d
0

if (cid:3)(cid:1)xi − (cid:1)xj(cid:3) = 1
otherwise

(4)

A simple calculation shows that for this example, Lapla-
cian eigenmaps are based on the spectral decomposition of
the matrix Ψ = I−W. Note that LLE would use the same
weight matrix W for these inputs; it would thus perform
a spectral decomposition on the matrix Φ=Ψ 2. In what
follows, we analyze the distribution of smallest eigenval-
ues of Ψ; the corresponding result for LLE follows from a
simple change of variables.
The matrix Ψ is diagonalized by a Fourier basis: namely,
for each (cid:1)q ∈ [−π, π]d,
the
form {ei(cid:1)q·(cid:1)xi} with eigenvalue:
λ((cid:1)q) = 1 − 1
d

it has an eigenvector of

cos qα.

d(cid:1)

(5)

α=1

(cid:6)

Thus, Ψ has a continuous eigenspectrum; in particular, it
has no gaps. We can compute the distribution of its eigen-
values from the integral:

1

δ(λ − λ((cid:1)q))

ρd(λ) =

Ω

(cid:4)

(2π)d

(6)
where Ω = [−π, π]d and δ(·) denotes the Dirac delta
function. For d = 1, the integral gives the simple result:
λ(2 − λ). For d > 1, the integral cannot
ρ1(λ)=(1/π)/
be evaluated analytically, but we can compute its asymp-
totic behavior as λ→0, which characterizes the distribution
of smallest eigenvalues. (This is the regime of interest for
understanding LLE and Laplacian eigenmaps.) The asymp-
totic behavior may be derived by approximating eq. (5)
by its second-order Taylor expansion λ((cid:1)q) ≈ (cid:3)(cid:1)q(cid:3) 2/(2d),
since λ (cid:2) 1 implies (cid:3)(cid:1)q(cid:3) (cid:2) 1. With this substitution,
eq. (6) reduces to an integral over a hypersphere of radius
(cid:3)(cid:1)q(cid:3)=

√
2dλ, yielding the asymptotic result:

(cid:7)

(cid:8)

λd/2−1 as λ → 0,

ρd(λ) ∼

(d/2π)d/2
Γ(d/2)

(7)
where Γ(·) is the Gamma function. Our result thus relates
the dimensionality of the input lattice to the power law that
characterizes the distribution of smallest eigenvalues.

Analysis and Extension of Spectral Methods for Nonlinear Dimensionality Reduction

Figure2.Left: inputs from an inﬁnite square lattice, for which one
can calculate the distribution of smallest eigenvalues from LLE
and Laplacian eigenmaps. Right: inputs from a regularly sampled
submanifold which give rise to the same results.

The power laws in eq. (7) were calculated from the weight
matrix in eq. (4); thus they are valid for any inputs that
give rise to matrices (or equivalently, graphs) of this form.
The right panel of Fig. 2 shows how inputs that were reg-
ularly sampled from a two dimensional submanifold could
give rise to the same graph as inputs from a square lattice.
Could these power laws be used to estimate the intrinsic
dimensionality of a data set? While in principle such an
approach seems possible, in practice it seems unlikely to
succeed. Fitting a power law to the distribution of small-
est eigenvalues would require computing many more than
the d smallest ones. The ﬁt would also be confounded by
the effects of ﬁnite sample sizes, boundary conditions, and
random (irregular) sampling. A more robust way to esti-
mate dimensionality from the results of LLE and Laplacian
eigenmaps is therefore needed.

3. Conformal Eigenmaps

In this section, we describe a framework to remedy the
previously mentioned shortcomings of LLE and Laplacian
eigenmaps. Recall that we can view the eigenvectors from
these algorithms as an orthonormal basis for functions de-
ﬁned on the n points of the data set. The embeddings
from these algorithms are derived simply from the d bottom
(non-constant) eigenvectors; as such, they are not explicitly
designed to preserve local features such as distances or an-
gles. Our approach is motivated by the following question:
from the m bottom eigenvectors of these algorithms, where
m > d but m(cid:2) n, can we construct a more faithful embed-
ding of dimensionality d?

3.1. Motivation

To proceed, we must describe more precisely what we
mean by “faithful”. A conformal mapping f between two
manifoldsM and M(cid:1)
is a smooth, one-to-one mapping that
looks locally like a rotation, translation, and scaling, thus
preserving local angles (though not local distances). This
property is shown schematically in Fig. 3. Let C 1 and C2
denote two curves intersecting at a point x∈M, and let C 3
denote another curve that intersects C 1 and C2 in the neigh-

C3

C1

f (C1)

f (C2)

f (C3)

x

C2

f (x)

Figure3.Schematic illustration of conformal mapping f between
two manifolds, which preserves the angles (but not areas) of in-
ﬁnitessimally small triangles.

borhood of x. Let Γ be the inﬁnitessimally small triangle
deﬁned by these curves in the limit that C 3 approaches x,
and let Γ(cid:1)
be its image under the mapping f . For a confor-
mal mapping, these triangles will be similar (having equal
angles) though not necessarily congruent (with equal sides
and areas). All isometric (distance-preserving) mappings
are conformal mappings, but not vice versa.

We can now more precisely state the motivation behind our
approach. Assume that the inputs (cid:1)xi ∈ Rp were sampled
from a submanifold that can be conformally mapped to
d-dimensional Euclidean space. Can we (approximately)
construct the images of (cid:1)xi under such a mapping from the
bottom m eigenvectors of LLE or Laplacian eigenmaps,
where m > d but m (cid:2) n? We will answer this question
in three steps: ﬁrst, by deﬁning a measure of local dissim-
ilarity between discrete point sets; second, by minimizing
this measure over an m-dimensional function space derived
from the spectral decompositions of LLE and Laplacian
eigenmaps; third, by casting the required optimization as
a problem in semideﬁnite programming and showing that
its solution also provides an estimate of the data’s intrinsic
dimensionality d.

3.2. Cost function
Let (cid:1)zi = f((cid:1)xi) deﬁne a mapping between two discrete point
sets for i ∈ {1, 2, . . . , n}. Suppose that the points (cid:1)xi were
densely sampled from a low dimensional submanifold: to
what extent does this mapping look locally like a rotation,
translation, and scaling? Consider any triangle ((cid:1)x i, (cid:1)xj, (cid:1)xj(cid:1))
where j and j(cid:1)
are among the k-nearest neighbors of (cid:1)x i,
as well as the image of this triangle ((cid:1)zi, (cid:1)zj, (cid:1)zj(cid:1)). These
triangles are similar if and only if:
(cid:3)(cid:1)zi − (cid:1)zj(cid:1)(cid:3)2
(cid:3)(cid:1)xi − (cid:1)xj(cid:1)(cid:3)2

(cid:3)(cid:1)zj − (cid:1)zj(cid:1)(cid:3)2
(cid:3)(cid:1)xj − (cid:1)xj(cid:1)(cid:3)2

=

(cid:3)(cid:1)zi − (cid:1)zj(cid:3)2
(cid:3)(cid:1)xi − (cid:1)xj(cid:3)2

=

The constant of proportionality in this equation indicates
the change in scale of the similarity transformation (i.e.,
the ratio of the areas of these triangles). Let η ij = 1 if (cid:1)xj
is one of the k-nearest neighbors of (cid:1)x i or if i = j. Also,
let si denote the hypothesized change of scale induced by

Analysis and Extension of Spectral Methods for Nonlinear Dimensionality Reduction

bottom m eigenvectors

LLE or Laplacian

eigenmaps

(cid:1)

(cid:2)

i

(cid:1)yi ∈ Rm
m (cid:2) n

(cid:2)

(cid:1)yi = 0

i

yiαyiβ = n δαβ

Conformal
eigenmaps

(cid:2)

high dimensional inputs

(cid:1)xi ∈ Rp

i ∈ {1, 2, . . . , n}

maximally

angle-preserving

low dimensional embedding

(cid:1)zi = L(cid:1)yi ∈ Rm

sparse singular values of L may
suggest dimensionality d < m

Figure4.Steps of the algorithm for computing maximally angle-preserving embeddings from the bottom eigenvectors of LLE or Lapla-
cian eigenmaps, as described in section 3.

the mapping at (cid:1)xi. With this notation, we can measure the
degree to which the mapping f takes triangles at (cid:1)x i into
similar triangles at (cid:1)zi by the cost function:

(cid:9)(cid:3)(cid:1)zj − (cid:1)zj(cid:1)(cid:3)2 − si(cid:3)(cid:1)xj − (cid:1)xj(cid:1)(cid:3)2

(cid:10)2

Di(si) =

ηij ηij(cid:1)

(cid:1)

jj(cid:1)

(8)
A global measure of local dissimilarity is obtained by sum-
ming over all points:

(cid:1)

D(s) =

Di(si).

(9)

i

i=1 and {(cid:1)zi}n

Note that given ﬁxed point sets {(cid:1)xi}n
i=1, it is
straightforward to compute the scale parameters that mini-
mize this cost function, since each si in eq. (8) can be op-
timized by a least squares ﬁt. Is it possible, however, that
given only {(cid:1)xi}n
i=1, one could also minimize this cost func-
tion with respect to {(cid:1)zi}n
i=1, yielding a nontrivial solution
where the latter lives in a space of much lower dimension-
ality than the former? Such a solution (assuming it had low
cost) would be suggestive of a conformal map for nonlinear
dimensionality reduction.

3.3. Eigenvector expansion

We obtain a well-posed optimization for the above prob-
lem by constraining the points (cid:1)z i to be spanned by the bot-
tom m eigenvectors returned by LLE or Laplacian eigen-
maps.
In particular, denoting the outputs of these algo-
rithms by (cid:1)yi∈Rm, we look for solutions of the form:

(cid:1)zi = L(cid:1)yi,

(10)
where L ∈ Rm×m is a general linear transformation and
i∈{1, 2, . . . , n}. Thus our goal is to compute the scale pa-
rameters si and the linear transformation L that minimize
the “dissimilarity” cost function in eqs. (8–9) with the sub-
stitution (cid:1)zi = L(cid:1)yi. We also impose the further constraint

trace(LT L) = 1

(11)

in order to rule out the trivial solution L = 0, which zeroes
the cost function by placing all (cid:1)z i at the origin.

Before showing how to optimize eqs. (8–9) subject to
the constraints in eq. (10–11), we make two observations.
First, by equating L to a multiple of the identity matrix,
we recover the original m-dimensional embedding (up to
a global change of scale) obtained from LLE or Laplacian
eigenmaps. In general, however, we shall see that this set-
ting of L does not lead to maximally angle-preserving em-
beddings. Second, if we are allowed to express (cid:1)z i in terms
of the complete basis of eigenvectors (taking m = n), then
we can reconstruct the original inputs (cid:1)x i up to a global ro-
tation and change of scale, which does not yield any form
of dimensionality reduction. By choosing m (cid:2) min(n, p)
where (cid:1)xi ∈ Rp, however, we can force a solution that
achieves some form of dimensionality reduction.

3.4. Semideﬁnite programming

Minimizing the cost function in eqs. (8–9) in terms of the
matrix L and the scale parameters si can be cast as a prob-
lem in semideﬁnite programming (SDP) (Vandenberghe &
Boyd, 1996). Details are omitted due to lack of space,
but the derivation is easy to understand at a high level.
The optimal scaling parameters can be computed in closed
form as a function of the matrix L and eliminated from
eqs. (8–9). The resulting form of the cost function only
depends on the matrix L through the positive semideﬁnite
matrix P = LT L. Let P = vec(P) denote the vector ob-
tained by concatenating the columns of P. Then, using
Schur complements, the optimization can be written as:

minimize
such that

t
P (cid:8) 0,
(cid:11)
trace(P) = 1,
SP
t

I
(SP)T

(cid:12)

(cid:8) 0,

(12)
(13)
(14)

(15)

where eq. (13) indicates that the matrix P is constrained
to be positive semideﬁnite and eq. (14) enforces the earlier

Analysis and Extension of Spectral Methods for Nonlinear Dimensionality Reduction

constraint from eq. (11). In eq. (15), I and S are m 2×m2
matrices; I denotes the identity matrix, while S depends
on {(cid:1)xi, (cid:1)yi}n
i=1 but is independent of the optimization vari-
ables P and t. The optimization is an SDP over the m 2
unknown elements of P (or equivalently P). Thus its size
is independent of the number of inputs, n, as well as their
extrinsic dimensionality, p. For small problems with (say)
m = 10, these SDPs can be solved in under a few minutes
on a typical desktop machine.
After solving the SDP, the linear map L is recovered via
L=P1/2, and the maximally angle-preserving embedding
is computed from eq. (10). The square root operation is
well deﬁned since P is positive semideﬁnite. The solution
computed from (cid:1)zi =L(cid:1)yi deﬁnes an m-dimensional embed-
ding. Note, however, that if the matrix L has one or more
small singular values, then the variance of this embedding
will be concentrated in fewer than m dimensions. Thus,
by examining the singular values of L (or equivalently, the
eigenvalues of P = LT L), we can obtain an estimate of
the data’s intrinsic dimensionality, d. We can also output a
d-dimensional embedding by simply projecting the points
(cid:1)zi ∈ Rm onto the ﬁrst d principal components of their co-
variance matrix. The overall algorithm, which we call con-
formal eigenmaps, is summarized in Fig. 4.

4. Experimental Results

We experimented with the algorithm in Fig. 4 to com-
pute maximally angle-preserving embeddings of several
data sets. We used the algorithm to visualize the low di-
mensional structure of each data set, as well as to esti-
mate their intrinsic dimensionalities. We also compared
the eigenvalue spectra from this algorithm to those from
PCA, Isomap (Tenenbaum et al., 2000), and Semideﬁnite
Embedding (SDE) (Weinberger & Saul, 2004).

4.1. Swiss roll

The top panel of Fig. 5 shows the angle-preserving embed-
ding computed by conformal eigenmaps on the Swiss roll
data set from section 2.4. The embedding was constructed
from the bottom m=10 eigenvectors of LLE. Compared to
the result in Fig. 1, the angle-preserving embedding more
faithfully preserves the shape of the underlying manifold’s
boundary. The eigenvalues of the matrix P = L T L, nor-
malized by their sum, are shown in the middle panel of
Fig. 5, along with similarly normalized eigenvalues from
PCA, Isomap, and SDE. The relative magnitudes of indi-
vidual eigenvalues are indicated by the widths of differently
colored regions in each bar plot.

The two leading eigenvalues in these graphs reveal the ex-
tent to which each algorithm’s embedding is conﬁned to
two dimensions. All the manifold learning algorithms (but

PCA

Isomap

SDE

CE

L1
L2
L3
L4

eigenvalues (normalized by trace)

0

top four rows of transformation matrix L

Figure5.Top: two dimensional embedding of “Swiss roll” inputs
in Fig. 1 by conformal eigenmaps (CE). Middle: comparison of
eigenspectra from PCA, Isomap, SDE, and CE. The relative mag-
nitudes of individual eigenvalues are indicated by the widths of
differently colored regions in each bar plot. Bottom: top four rows
of the transformation matrix L in eq. (10).

not PCA) correctly identify the underlying dimensionality
of the Swiss roll as d = 2. Finally, the bottom panel in
Fig. 5 shows the top four rows of the transformation ma-
trix L from eq. (10). Only the ﬁrst two rows have matrix
elements of appreciable magnitude, reﬂecting the underly-
ing two dimensional structure of this data set. Note, how-
ever, that sizable matrix elements appear in all the columns
of L. This shows that the maximally angle-preserving em-
bedding exploits structure in the bottom m = 10 eigenvec-
tors of LLE, not just in the bottom d=2 eigenvectors.

4.2. Images of edges
Fig. 6 shows examples from a synthetic data set of n=2016
grayscale images of edges. Each image has 24×24 resolu-
tion. The edges were generated by sampling pairs of points
at regularly spaced intervals along the image boundary. The
images lie on a two dimensional submanifold, but this sub-
manifold has a periodic structure that cannot be unraveled
in the same way as the Swiss roll from the previous section.
We synthesized this data set with the aim of understanding
how various manifold learning algorithms might eventually
perform when applied to patches of natural images.

The top panel of Fig. 7 shows the ﬁrst two dimensions of
the maximally angle-preserving embedding of this data set.
The embedding was constructed from the bottom m = 10
eigenvectors of Laplacian eigenmaps using k = 10 near-
est neighbors. The embedding has four distinct quadrants
in which edges with similar orientations are mapped to
nearby points in the plane. The middle panel in Fig. 7 com-
pares the eigenspectrum from the angle-preserving embed-

Analysis and Extension of Spectral Methods for Nonlinear Dimensionality Reduction

Figure6.Examples from a synthetic data set of n = 2016 gray-
scale images of edges. Each image has 24×24 resolution.

ding to those from PCA, Isomap, and SDE. Isomap does
not detect the low dimensional structure of this data set,
possibly foiled by the cyclic structure of the submanifold.
The distance-preserving embedding of SDE has variance
in more dimensions than the angle-preserving embedding,
suggesting that the latter has exploited the extra ﬂexibility
of conformal versus isometric maps. The bottom panel in
Fig. 7 displays the matrix L from eq. (10). The result shows
that the semideﬁnite program in eq. (12) mixes all of the
bottom m = 10 eigenvectors from Laplacian eigenmaps to
obtain the maximally angle-preserving embedding.

PCA

Isomap

SDE

CE

L1
L2
L3
L4

eigenvalues (normalized by trace)

top four rows of transformation matrix L

0

three dimensional embedding of n = 983 face
Figure8.Top:
images by conformal eigenmaps (CE). Middle: comparison of
eigenspectra from PCA, Isomap, SDE and CE. Bottom: top four
rows of the transformation matrix L in eq. (10).

PCA

Isomap

SDE

CE

L1
L2
L3
L4

eigenvalues (normalized by trace)

top four rows of transformation matrix L

0

Figure7.Top: two dimensional embedding of n = 2016 edge
images by conformal eigenmaps (CE). Middle: comparison of
eigenspectra from PCA, Isomap, SDE, and CE. Bottom: top four
rows of the transformation matrix L in eq. (10).

4.3. Images of faces
Fig. 8 shows results on a data set of n=983 images of
faces. The faces were initially processed by LLE using
k =8 nearest neighbors. The maximally angle-preserving
embedding was computed from the bottom m=10 eigen-
vectors of LLE. Though the intrinsic dimensionality of this
data set is not known a priori, several methods have re-
ported similar ﬁndings (Brand, 2003; Weinberger & Saul,
2004). As shown in the middle panel, the embedding from
conformal eigenmaps concentrates most of its variance in
three dimensions, yielding a somewhat lower estimate of
the dimensionality than Isomap or SDE. The top panel of
Fig. 8 visualizes the three dimensional embedding from
conformal eigenmaps; the faces are arranged in an intuitive
manner according to their expression and pose.

Analysis and Extension of Spectral Methods for Nonlinear Dimensionality Reduction

5. Discussion

In this work, we have appealed to conformal transforma-
tions as a basis for nonlinear dimensionality reduction. Our
approach casts a new light on older algorithms, such as
LLE and Laplacian eigenmaps. While the bottom eigen-
vectors from these algorithms have been used to derive low
dimensional embeddings, these solutions do not generally
preserve local features such as distances or angles. View-
ing these bottom eigenvectors as a partial basis for func-
tions on the data set, we have shown how to compute a
maximally angle-preserving embedding by solving an ad-
ditional (but small) problem in semideﬁnite programming.
At little extra computational cost, this framework signiﬁ-
cantly extends the utility of LLE and Laplacian eigenmaps,
yielding more faithful embeddings as well as a global esti-
mate of the data’s intrinsic dimensionality.

Previous studies in dimensionality reduction have shared
similar motivations as this work. An extension of Isomap
was proposed to learn conformal transformations (de Silva
& Tenenbaum, 2003); like Isomap, however, it relies on the
estimation of geodesic distances, which can lead to spuri-
ous results when the underlying manifold is not isomorphic
to a convex region of Euclidean space (Donoho & Grimes,
2002). Hessian LLE is a variant of LLE that learns isome-
tries, or distance-preserving embeddings, with theoretical
guarantees of asymptotic convergence (Donoho & Grimes,
2003). Like LLE, however, it does not yield an estimate
of the underlying manifold’s dimensionality. Finally, there
has been work on angle-preserving linear projections (Ma-
gen, 2002). This work, however, focuses on random linear
projections (Johnson & Lindenstrauss, 1984) that are not
suitable for manifold learning.

Our approach in this paper builds on LLE and Laplacian
eigenmaps, thus inheriting their strengths as well as their
weaknesses. Obviously, when these algorithms return spu-
rious results (due to, say, outliers or noise), the angle-
preserving embeddings computed from their spectral de-
compositions are also likely to be of poor quality. In gen-
eral, though, we have found that our approach comple-
ments and extends these earlier approaches to nonlinear di-
mensionality reduction at very modest extra cost.

Acknowledgments

This work was supported by the National Science Founda-
tion under award number 0238323.

References
Belkin, M., & Niyogi, P. (2003). Laplacian eigenmaps for
dimensionality reduction and data representation. Neu-
ral Computation, 15(6), 1373–1396.

Brand, M. (2003). Charting a manifold. Advances in Neu-
ral Information Processing Systems 15 (pp. 985–992).
Cambridge, MA: MIT Press.

Burges, C. J. C. (2005). Geometric methods for feature ex-
In L. Rokach and
traction and dimensional reduction.
O. Maimon (Eds.), Data mining and knowledge discov-
ery handbook: A complete guide for practitioners and
researchers. Kluwer Academic Publishers.

Chung, F. R. K. (1997). Spectral graph theory. American

Mathematical Society.

de Silva, V., & Tenenbaum, J. B. (2003). Global ver-
sus local methods in nonlinear dimensionality reduction.
Advances in Neural Information Processing Systems 15
(pp. 721–728). Cambridge, MA: MIT Press.

Donoho, D. L., & Grimes, C. E. (2002). When does Isomap
recover the natural parameterization of families of artic-
ulated images? (Technical Report 2002-27). Department
of Statistics, Stanford University.

Donoho, D. L., & Grimes, C. E. (2003). Hessian eigen-
maps:
locally linear embedding techniques for high-
dimensional data. Proceedings of the National Academy
of Arts and Sciences, 100, 5591–5596.

Johnson, W., & Lindenstrauss, J. (1984). Extensions of
lipschitz maps into a hilbert space. Contemporary Math-
ematics, 189–206.

Magen, A. (2002). Dimensionality reductions that preserve
volumes and distance to afﬁne spaces, and their algo-
rithmic applications. In J. Rolim and S. Vadhan (Eds.),
Randomization and approximation techniques: Sixth in-
ternational workshop, RANDOM 2002. Springer-Verlag.

Roweis, S. T., & Saul, L. K. (2000). Nonlinear dimen-
sionality reduction by locally linear embedding. Science,
290, 2323–2326.

Saul, L. K., & Roweis, S. T. (2003). Think globally, ﬁt
locally: unsupervised learning of low dimensional man-
ifolds. Journal of Machine Learning Research, 4, 119–
155.

Tenenbaum, J. B., de Silva, V., & Langford, J. C. (2000).
A global geometric framework for nonlinear dimension-
ality reduction. Science, 290, 2319–2323.

Vandenberghe, L., & Boyd, S. P. (1996). Semideﬁnite pro-

gramming. SIAM Review, 38(1), 49–95.

Weinberger, K. Q., & Saul, L. K. (2004). Unsupervised
learning of image manifolds by semideﬁnite program-
ming. Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR-04) (pp.
988–995). Washington D.C.

