Abstract

In this paper we investigate the use of Langevin Monte Carlo methods on the
probability simplex and propose a new method, Stochastic gradient Riemannian
Langevin dynamics, which is simple to implement and can be applied to large
scale data. We apply this method to latent Dirichlet allocation in an online mini-
batch setting, and demonstrate that it achieves substantial performance improve-
ments over the state of the art online variational Bayesian methods.

1

Introduction

(cid:88)

In recent years there has been increasing interest in probabilistic models where the latent variables
or parameters of interest are discrete probability distributions over K items, i.e. vectors lying in the
probability simplex

K = {(1, . . . , K) : k  0,

k = 1}  RK

(1)

k

Important examples include topic models like latent Dirichlet allocation (LDA) [BNJ03], admixture
models in genetics like Structure [PSD00], and discrete directed graphical models with a Bayesian
prior over the conditional probability tables [Hec99].
Standard approaches to inference over the probability simplex include variational inference [Bea03,
WJ08] and Markov chain Monte Carlo methods (MCMC) like Gibbs sampling [GRS96]. In the
context of LDA, many methods have been developed, e.g. variational inference [BNJ03], collapsed
variational inference [TNW07, AWST09] and collapsed Gibbs sampling [GS04]. With the increas-
ingly large scale document corpora to which LDA and other topic models are applied, there has
also been developments of specialised and highly scalable algorithms [NASW09]. Most proposed
algorithms are based on a batch learning framework, where the whole document corpus needs to be
stored and accessed for every iteration. For very large corpora, this framework can be impractical.
Most recently, [Sat01, HBB10, MHB12] proposed online Bayesian variational inference algorithms
(OVB), where on each iteration only a small subset (a mini-batch) of the documents is processed
to give a noisy estimate of the gradient, and a stochastic gradient descent algorithm [RM51] is
employed to update the parameters of interest. These algorithms have shown impressive results on
very large corpora like Wikipedia articles, where it is not even feasible to store the whole dataset in
memory. This is achieved by simply fetching the mini-batch articles in an online manner, processing,
and then discarding them after the mini-batch.
In this paper, we are interested in developing scalable MCMC algorithms for models dened over
the probability simplex. In some scenarios, and particularly in LDA, MCMC algorithms have been
shown to work extremely well, and in fact achieve better results faster than variational inference
on small to medium corpora [GS04, TNW07, AWST09]. However current MCMC methodology

1

have mostly been in the batch framework which, as argued above, cannot scale to the very large
corpora of interest. We will make use of a recently developed MCMC method called stochastic
gradient Langevin dynamics (SGLD) [WT11, ABW12] which operates in a similar online mini-
batch framework as OVB. Unlike OVB and other stochastic gradient descent algorithms, SGLD
is not a gradient descent algorithm. Rather, it is a Hamiltonian MCMC [Nea10] algorithm which
will asymptotically produce samples from the posterior distribution. It achieves this by updating
parameters according to both the stochastic gradients as well as additional noise which forces it to
explore the full posterior instead of simply converging to a MAP conguration.
There are three difculties that have to be addressed, however, to successfully apply SGLD to LDA
and other models dened on probability simplices. Firstly, the probability simplex (1) is compact
and has boundaries that has to be accounted for when an update proposes a step that brings the
vector outside the simplex. Secondly, the typical Dirichlet priors over the probability simplex place
most of its mass close to the boundaries and corners of the simplex. This is particularly the case for
LDA and other linguistic models, where probability vectors parameterise distributions over a larger
number of words, and it is often desirable to use distributions that place signicant mass on only
a few words, i.e. we want distributions over K which place most of its mass near the boundaries
and corners. This also causes a problem as depending on the parameterisation used, the gradient
required for Langevin dynamics is inversely proportional to entries in  and hence can blow up
when components of  are close to zero. Finally, again for LDA and other linguistic models, we
would like algorithms that work well in high-dimensional simplices.
These considerations lead us to the rst contribution of this paper in Section 3, which is an inves-
tigation into different ways to parameterise the probability simplex. This section shows that the
choice of a good parameterisation is not obvious, and that the use of the Riemannian geometry of
the simplex [Ama95, GC11] is important in designing Langevin MCMC algorithms. In particular,
we show that an unnormalized parameterisation, using a mirroring trick to remove boundaries, cou-
pled with a natural gradient update, achieves the best mixing performance. In Section 4, we then
show that the SGLD algorithm, using this parameterisation and natural gradient updates, performs
signicantly better than OVB algorithms [HBB10, MHB12]. Section 2 reviews Langevin dynamics,
natural gradients and SGLD to setup the framework used in the paper, and Section 6 concludes.

2 Review

2.1 Langevin dynamics

Suppose we model a data set x = x1, . . . , xN , with a generative model p(x | ) = (cid:81)N

i=1 p(xi |
) parameterized by   RD with prior p() and that our aim is to compute the posterior p( |
x). Langevin dynamics [Ken90, Nea10] is an MCMC scheme which produces samples from the
posterior by means of gradient updates plus Gaussian noise, resulting in a proposal distribution
q( | ) as described by Equation 2.

 =  +


2

log p() +

log p(xi|)

+ ,

  N (0, I)

(2)

The mean of the proposal distribution is in the direction of increasing log posterior due to the gra-
dient, while the added noise will prevent the samples from collapsing to a single (local) maximum.
A Metropolis-Hastings correction step is required to correct for discretisation error, with proposals
[RS02]. As  tends to zero, the acceptance ratio
accepted with probability min
tends to one as the Markov chain tends to a stochastic differential equation which has p( | x) as its
stationary distribution [Ken78].

1, p(|x)
p(|x)

q(|)
q(|)

(cid:16)

(cid:17)

2.2 Riemannian Langevin dynamics

Langevin dynamics has an isotropic proposal distribution leading to slow mixing if the components
of  have very different scales or if they are highly correlated. Preconditioning can help with this. A
recent approach, the Riemann manifold Metropolis adjusted Langevin algorithm [GC11] uses a user
chosen matrix G() to precondition in a locally adaptive manner. We will refer to their algorithm

2

(cid:32)

N(cid:88)

i=1

(cid:33)

as Riemannian Langevin dynamics (RLD) in this paper. The Riemannian manifold in question is
the family of probability distributions p(x | ) parameterised by , for which the expected Fisher
information matrix I denes a natural Riemannian metric tensor. In fact any positive denite matrix
G() denes a valid Riemannian manifold and hence we are not restricted to using G() = I. This
is important in practice as for many models of interest the expected Fisher information is intractable.
As in Langevin dynamics, RLD consists of a Gaussian proposal q( | ), along with a Metropolis-
Hastings correction step. The proposal distribution can be written as

 =  +

() + G 1

2 (),

  N (0, I)

where the jth component of () is given by

()j =

G1()

log p() +

log p(xi|)

(cid:33)(cid:33)

(cid:18)

G1()

D(cid:88)

k=1

 2

G()

k

G1()

j

(cid:32)

D(cid:88)

k=1

+

(cid:32)
(cid:0)G1()(cid:1)


2

(cid:18)

N(cid:88)

i=1

(cid:19)

jk Tr

G1()

G()

k

(3)

(cid:19)

jk

(4)

The rst term in Equation 4 is now the natural gradient of the log posterior. Whereas the standard
gradient gives the direction of steepest ascent in Euclidean space, the natural gradient gives the
direction of steepest descent taking into account the geometry implied by G(). The remaining
terms in Equation 4 describe how the curvature of the manifold dened by G() changes for small
changes in . The Gaussian noise in Equation 3 also takes the geometry of the manifold into account,
having scale dened by G 1

2 ().

2.3 Stochastic gradient Riemannian Langevin dynamics

In the Langevin dynamics and RLD algorithms, the proposal distribution requires calculation of the
gradient of the log likelihood w.r.t. , which means processing all N items in the data set. For
large data sets this is infeasible, and even for small data sets it may not be the most efcient use of
computation. The stochastic gradient Langevin dynamics (SGLD) algorithm [WT11] replaces the
calculation of the gradient over the full data set, with a stochastic approximation based on a subset
of data. Specically at iteration t we sample n data items indexed by Dt, uniformly from the full
data set and replace the exact gradient in Equation 2 with the approximation

logp(x | )  N
|Dt|

log p(xi|)

(5)

(cid:88)

iDt

Also, SGLD does not use a Metropolis-Hastings correction step, as calculating the acceptance prob-
ability would require use of the full data set, hence defeating the purpose of the stochastic gradient
approximation. Convergence to the posterior is still guaranteed as long as decaying step sizes satis-

fying(cid:80)

t=1 t = ,(cid:80)

t <  are used.

t=1 2

In this paper we combine the use of a preconditioning matrix G() as in RLD with this stochastic
gradient approximation, by replacing the exact gradient in Equation 4 with the approximation from
Equation 5. The resulting algorithm, stochastic gradient Riemannian Langevin dynamics (SGRLD),
avoids the slow mixing problems of Langevin dynamics, while still being applicable in a large scale
online setting due to its use of stochastic gradients and lack of Metropolis-Hastings correction steps.

3 Riemannian Langevin dynamics on the probability simplex

In this section, we investigate the issues which arise when applying Langevin Monte Carlo meth-
ods, specically the Langevin dynamics and Riemannian Langevin dynamics algorithms, to models
whose parameters lie on the probability simplex. In these experiments, a Metropolis-Hastings cor-
rection step was used. Consider the simplest possible model: a K dimensional probability vector
, and data x = x1, . . . , xN with p(xi = k | ) = k.
i=1 (xi = k). In

 with Dirichlet prior p()  (cid:81)K
This results in a Dirchlet posterior p( | x)  (cid:81)K

, where nk = (cid:80)N

k nk+k1

k k1

k

k

3

Parameterisation



log p(|x)

(cid:16)
(cid:80)D
(cid:0)G1()(cid:1)

G()
G1()
G1 G
k
jk Tr

k=1

G1(cid:17)
(cid:16)

(cid:80)D

k=1

jk

G1() G

k

Reduced-Mean

(cid:16)

n

k = k

n+

diag()1 +

  1 nK +1
1(cid:80)
(cid:0)diag()  T(cid:1)

K
1

k k

1
n

(cid:17)

Kj  1
Kj  1

11T(cid:17)

(cid:16)

n

k

k

k

1
n

Reduced-Natural

1(cid:80)K1
k = log
(cid:0)diag()  T(cid:1)
n +   (n + K) 
1(cid:80)
diag()1 +
(1(cid:80)
(1(cid:80)

1
k k




K1

K1

k k)2

1
2
j
1
2
j

k k)2

11T(cid:17)

Expanded-Mean
|k|(cid:80)
k =
k=1 |k|
  n
  1
n+1
1
diag ()
diag ()

Expanded-Natural

k = ek(cid:80)
diag(cid:0)e(cid:1)
k=1 ek
n +   n  e
diag(cid:0)e(cid:1)

1
1

ej
ej

Table 1: Parameterisation Details

k

k k

our experiments we use a sparse, symmetric prior with k = 0.1k, and sparse count data, setting
K = 10 and n1 = 90, n2 = n3 = 5 and the remaining nk to zero. This is to replicate the sparse
nature of the posterior in many models of interest. The qualitative conclusions we draw are not
sensitive to the precise choice of hyperparameters and data here.
There are various possible ways to parameterise the probability simplex, and the performance of
Langevin Monte Carlo depends strongly on the choice of parameterisation. We consider both the
mean and natural parameter spaces, and in each of these we try both a reduced (K  1 dimensional)
and expanded (K dimensional) parameterisation, with details as follows.
Reduced-Mean: in the mean parameter space, the most obvious approach is to set  =  directly,
but there are two problems with this. Though  has K components, it must lie on the simplex, a
K  1 dimensional space. Running Langevin dynamics or RLD on the full K dimensional param-
eterisation will result in proposals that are off the simplex with probability one. We can incorporate
k=1 k = 1 by using the rst K  1 components as the parameter , and set-
k=1 k. Note however that the proposals can still violate the boundary constraint
0 < k < 1, and this is particularly problematic when the posterior has mass close to the boundaries.
Expanded-Mean: we can simplify boundary considerations using a redundant parameterisation.
We take as our parameter   RK
+ with prior a product of independent Gamma(k, 1) distributions,
and so the prior on  is still Dirichlet().
The boundary conditions 0 < k can be handled by simply taking the absolute value of the proposed
. This is equivalent to letting  take values in the whole of RK, with prior given by Gammas
|k|(cid:80)
k |k|, which again results in a Dirichlet()

the constraint that(cid:80)K
ting K = 1 (cid:80)K1
p() (cid:81)K
mirrored at 0, p() (cid:81)K

ek.  is then given by k = k(cid:80)

k=1 |k|k1e|k|, and k =

k=1 k1

ek
k=1 ek

ek(cid:80)K

1+(cid:80)K1

prior on . This approach allows us to bypass boundary issues altogether.
Reduced-Natural: in the natural parameter space, the reduced parameterisation takes the form
for k = 1, . . . , K  1. The prior on  can be obtained from the Dirichlet() prior
k =
on  using a change of variables. There are no boundary constraints as the range of k is R.
Expanded-Natural: nally the expanded-natural parameterisation takes the form k =
k=1 ek
for k = 1, . . . , K. As in the expanded-mean parameterisation, we use a product of Gamma priors,
in this case for ek, so that the prior for  remains Dirichlet().
For all parameterisations, we run both Langevin dynamics and RLD. When applying RLD, we
must choose a metric G(). For the reduced parameterisations, we can use the expected Fisher
information matrix, but the redundancy in the full parameterisations means that this matrix has rank
K1 and hence is not invertible. For these parameterisations we use the expected Fisher information
matrix for a Gamma/Poisson model, which is equivalent to the Dirichlet/Multinomial apart from the
fact that the total number of data items is considered to be random as well.
The details for each parameterisation are summarised in Table 1.
In all cases we are interested
in sampling from the posterior distribution on , while  is the specic parameterisation being
used. For the mean parameterisations, the 1 term in the gradient of the log-posterior means
that for components of  which are close to zero, the proposal distribution for Langevin dynamics
(Equation 2) has a large mean, resulting in unstable proposals with a small acceptance probability.
Due to the form of G()1, the same argument holds for the RLD proposal distribution for the
natural parameterisations. This leaves us with three possible combinations, RLD on the expanded-
mean parameterisation and Langevin dynamics on each of the natural parameterisations.

4

(a) Effective sample size

(b) Samples

Figure 1: Effective sample size and samples. Burn-in iterations is 10,000; thinning factor 100.

To investigate their relative performances we run a small experiment, producing 110,000 samples
from each of the three remaining parameterisations, discarding 10,000 burn-in samples and thinning
the remaining samples by a factor of 100. For the resulting 1000 thinned samples of , we calculate
the corresponding samples of , and compute the effective sample size for each component of .
This was done for a range of step sizes , and the mean and median effective sample sizes for the
components of  is shown in Figure 1(a).
Figure 1(b) shows the samples from each sampler at their optimal step size of 0.1. The samples
from Langevin dynamics on both natural parameterisations display higher auto-correlation than the
RLD samples produced using the expanded-mean parameterisation, as would be expected from their
lower effective sample sizes. In addition to the increased effective sample size, the expanded-mean
parameterisation RLD sampler has the advantage that it is computationally efcient as G() is a
diagonal matrix. Hence it is this algorithm that we use when applying these techniques to latent
Dirichlet allocation in Section 4.

4 Applying Riemannian Langevin dynamics to latent Dirichlet allocation

Latent Dirichlet Allocation (LDA) [BNJ03] is a hierarchical Bayesian model, most frequently used
to model topics arising in collections of text documents. The model consists of K topics k, which
are distributions over the words in the collection, drawn from a symmetric Dirichlet prior with
hyper-parameter . A document d is then modelled by a mixture of topics, with mixing proportion
d, drawn from a symmetric Dirichlet prior with hyper-parameter . The model corresponds to a
generative process where documents are produced by drawing a topic assignment zdi i.i.d. from d
for each word wdi in document d, and then drawing the word wdi from the corresponding topic zdi.
We integrate out  analytically, resulting in the semi-collapsed distribution:

D(cid:89)

K(cid:89)

K(cid:89)

W(cid:89)

p(w, z,  | , ) =

where as in [TNW07], ndkw = (cid:80)Nd

d=1

 (K)

 (K + nd)

(6)
i=1 (wdi = w, zdi = k) and  denotes summation over the

 ()

w=1

k=1

k=1

corresponding index. Conditional on , the documents are i.i.d., and we can factorise Equation 6

 ( + ndk)

(W )
()W

+nkw1

kw

D(cid:89)

p(w, z,  | , ) = p( | )

p(wd, zd | , )

where

p(wd, zd,| , ) =

d=1

 ( + ndk)

 ()

K(cid:89)

k=1

W(cid:89)

w=1

ndkw
kw

5

(7)

(8)

10^510^410^310^210^110^001002003004005006007008009001000Step sizeESS  ExpandedMean RLD medianExpandedMean RLD meanReducedNatural LD medianReducedNatural LD meanExpandedNatural LD medianExpandedNatural LD mean01002003004005006007008009001000108106104102100010020030040050060070080090010001028102110141071000100200300400500600700800900100010161012108104100Thinned sample number4.1 Stochastic gradient Riemannian Langevin dynamics for LDA

dent Gamma prior p(k)  (cid:81)W

As we would like to apply these techniques to large document collections, we use the stochas-
tic gradient version of the Riemannian Langevin dynamics algorithm, as detailed in Section 2.3.
Following the investigation in Section 3 we use the expanded-mean parameterisation. For each
of the K topics k, we introduce a W -dimensional unnormalised parameter k with an indepen-
, for w = 1, . . . , W .
We use the mirroring idea as well. The metric G() is then the diagonal matrix G() =
diag (11, . . . , 1W , . . . , K1, . . . , KW )
The algorithm runs on mini-batches of documents: at time t it receives a mini-batch of documents
indexed by Dt, drawn at random from the full corpus D. The stochastic gradient of the log posterior
of  on Dt is shown in Equation 9.

kw ekw and set kw = kw(cid:80)
w=1 w1
1.

w kw

 1 +

|D|
|Dt|

(cid:88)

dDt

Ezd|wd,,

(cid:21)

 ndk
k

(cid:20) ndkw
(cid:33)

kw

log p( | w, , )

kw

(cid:32)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)kw +


2

kw

   1
(cid:88)

|D|
|Dt|

dDt

For this choice of  and G(), we use Equations 3, 4 to give the SGRLD update for ,


kw =

  kw +

Ezd|wd,, [ndkw  kwndk]

+ (kw)

1

2 kw

(9)

(10)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

where kw  N (0, ). Note that the 1 term in Equation 9 has been replaced with  in Equation 10
as the 1 cancels with the curvature terms as detailed in Table 1. As discussed in Section 3, we
reect moves across the boundary 0 < kw by taking the absolute value of the proposed update.
Comparing Equation 9 to the gradient for the simple model from Section 3, the observed counts
nk for the simple model have been replaced with the expectation of the latent topic assignment
counts ndkw. To calculate this expectation we use Gibbs sampling on the topic assignments in each
document separately, using the conditional distributions

p(zdi = k | wd, , ) =

(11)

(cid:16)
(cid:80)

k

(cid:16)

(cid:17)

(cid:17)

\i
dk
\i
dk

 + n

kwdi

 + n

kwdi

where \i represents a count excluding the topic assignment variable we are updating.

5 Experiments

d,i q(zdi)(cid:81)

We investigate the performance of SGRLD, with no Metropolis-Hastings correction step, on two
real-world data sets. We compare it to two online variational Bayesian algorithms developed
for latent Dirichlet allocation: online variational Bayes (OVB) [HBB10] and hybrid stochastic
variational-Gibbs (HSVG) [MHB12]. The difference between these two methods is the form of vari-
ational assumption made. OVB assumes a mean-eld variational posterior, q(1:D, z1:D, 1:K) =
k q(k), in particular this means topic assignment variables within the same
document are assumed to be independent, when in reality they will be strongly coupled. In con-
trast HSVG collapses d analytically and uses a variational posterior of the form q(z1:D, 1:K) =
k q(k), which allows dependence within the components of zd. This more complicated
posterior requires Gibbs sampling in the variational update step for zd, and we combined the code
for OVB [HBB10], with the Gibbs sampling routine from our SGRLD code to implement HSVG.

(cid:81)
d q(d)(cid:81)
(cid:81)
d q(zd)(cid:81)

5.1 Evaluation Method

The predictive performance of the algorithms can be measured by looking at the probability they
assign to unseen data. A metric frequently used for this purpose is perplexity, the exponentiated
cross entropy between the trained model probability distribution and the empirical distribution of
the test data. For a held-out document wd and a training set W, the perplexity is given by

(cid:26)

(cid:80)nd
i=1 log p(wdi | W, , )

(cid:27)

perp(wd | W, , ) = exp



.

(12)

nd

6

This requires calculating p(wdi
d, 1, . . . , K and topic assignments zd, to give

| W, , ), which is done by marginalising out the parameters

p(wdi | W, , ) = Ed,

dkkwdi

(13)

(cid:35)

(cid:34)(cid:88)

k

We use a document completion approach [WMSM09], partitioning the test document wd into two
sets of words, wtrain
to estimate d for the test document, then calculating the
d
perplexity on wtest
To calculate the perplexity for SGRLD, we integrate  analytically, so Equation 13 is replaced by

d using this estimate.

and using wtrain

, wtest

d

d

(cid:34)

(cid:34)(cid:88)

(cid:35)(cid:35)

p(wdi | wtrain

d

,W, , ) = E|W,

E
ztrain
d

|,

dkkwdi

where

dk := p(ztest

di = k | ztrain

d

k

, ) =

ntrain
dk + 
ntrain
d + K

.

(14)

(15)

(cid:34)(cid:88)

(cid:35)

(cid:88)

We estimate these expectations using the samples we obtain for  from the Markov chain produced
by SGRLD, and samples for ztrain
For OVB and HSVG, we estimate Equation 13 by replacing the true posterior p(, ) with q(, ).

produced by Gibbs sampling the topic assignments on wtrain

.

d

d

p(wdi | W, , ) = Ep(d,|W,,)

dkkwdi

Eq(d) [dk] Eq(k) [kwdi ]

(16)

We estimate the perplexity directly rather than use a variational bound [HBB10] so that we can
compare results of the variational algorithms to those of SGRLD.

k

k

5.2 Results on NIPS corpus

The rst experiment was carried out on the collection of NIPS papers from 1988-2003 [GCPT07].
This corpus contains 2483 documents, which is small enough to run all three algorithms in batch
mode and compare their performance to that of collapsed Gibbs sampling on the full collection.
Each document was split 80/20 into training and test sets, the training portion of all 2483 documents
were used in each update step, and the perplexity was calculated on the test portion of all docu-
ments. Hyper-parameters  and  were both xed to 0.01, and 50 topics were used. A step-size
schedule of the form t = (a  (1 + t
b ))c was used. Perplexities were estimated for a range of step
size parameters, and for 1, 5 and 10 document updates per topic parameter update. For OVB the
document updates are xed point iterations of q(zd) while for HSVG and SGRLD they are Gibbs
updates of zd, the rst half of which were discarded as burn-in. These numbers of document updates
were chosen as previous investigation of the performance of HSVG for varying numbers of Gibbs
updates has shown that 6-10 updates are sufcient [MHB12] to achieve good performance.
Figure 2(a) shows the lowest perplexities achieved along with the corresponding parameter settings.
As expected, CGS achieves the lowest perplexities. It is surprising that HSVG performs slightly
worse than OVB on this data set. As it uses a less restricted variational distribution it should perform
at least as well. SGRLD improves on the performance of OVB and HSVG, but does not match the
performance of Gibbs sampling.

5.3 Results on Wikipedia corpus

The algorithms performances in an online scenario was assessed on a set of articles downloaded
at random from Wikipedia, as in [HBB10]. The vocabulary used is again as per [HBB10]; it is
not created from the Wikipedia data set, instead it is taken from the top 10,000 words in Project
Gutenburg texts, excluding all words of less than three characters. This results in vocabulary size
W of approximately 8000 words. 150,000 documents from Wikipedia were used in total, in mini-
batches of 50 documents each. The perplexities were estimated using the methods discussed in

7

(a) NIPS corpus

(b) Wikipedia corpus

Figure 2: Test-set perplexities on NIPS and Wikipedia corpora.

Section 5.1 on a separate holdout set of 1000 documents, split 90/10 training/test. As the corpus size
is large, collapsed Gibbs sampling was not run on this data set.
For each algorithm a grid-search was run on the hyper-parameters, step-size parameters, and num-
ber of Gibbs sampling sweeps / variational xed point iterations per  update. The lowest three
perplexities attained for each algorithm are shown in Figure 2(b). Corresponding parameters are
given in the supplementary material. HSVG achieves better performance than OVB, as expected.
The performance of SGRLD is a substantial improvement on both the variational algorithms.

6 Discussion

We have explored the issues involved in applying Langevin Monte Carlo techniques to a constrained
parameter space such as the probability simplex, and developed a novel online sampling algorithm
which addresses those issues. Using an expanded parametrisation with a reection trick for negative
proposals removed the need to deal with boundary constraints, and using the Riemannian geometry
of the parameter space dealt with the problem of parameters with differing scales.
Applying the method to Latent Dirichlet Allocation on two data sets produced state of the art pre-
dictive performance for the same computational budget as competing methods, demonstrating that
full Bayesian inference using MCMC can be practically applied to models of interest, even when
the data set is large. Python code for our method is available at http://www.stats.ox.ac.
uk/teh/sgrld.html.
Due to the widespread use of models dened on the probability simplex, we believe the methods
developed here for Langevin dynamics on the probability simplex will nd further uses beyond latent
Dirichlet allocation and stochastic gradient Monte Carlo methods. A drawback of SGLD algorithms
is the need for decreasing step sizes; it would be interesting to investigate adaptive step sizes and the
approximation entailed when using xed step sizes (but see [AKW12] for a recent development).

Acknowledgements

We thank the Gatsby Charitable Foundation and EPSRC (grant EP/K009362/1) for generous fund-
ing, reviewers and area chair for feedback and support, and [HBB10] for use of their excellent
publicly available source code.

8

