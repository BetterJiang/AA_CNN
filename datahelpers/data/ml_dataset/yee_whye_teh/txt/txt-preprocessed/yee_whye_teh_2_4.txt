Abstract. This paper concerns the use of Markov chain Monte Carlo
methods for posterior sampling in Bayesian nonparametric mixture
models with normalized random measure priors. Making use of some
recent posterior characterizations for the class of normalized random
measures, we propose novel Markov chain Monte Carlo methods of both
marginal type and conditional type. The proposed marginal samplers
are generalizations of Neals well-regarded Algorithm 8 for Dirichlet
process mixture models, whereas the conditional sampler is a variation
of those recently introduced in the literature. For both the marginal
and conditional methods, we consider as a running example a mix-
ture model with an underlying normalized generalized Gamma process
prior, and describe comparative simulation results demonstrating the
ecacies of the proposed methods.

Key words and phrases: Bayesian nonparametrics, hierarchical mix-
ture model, completely random measure, normalized random measure,
Dirichlet process, normalized generalized Gamma process, MCMC pos-
terior sampling method, marginalized sampler, Algorithm 8, condi-
tional sampler, slice sampling.

1. INTRODUCTION

Mixture models provide a statistical framework
for modeling data where each observation is assumed
to have arisen from one of k groups, with k possibly
unknown, and each group being suitably modeled by
a distribution function from some parametric fam-
ily. The distribution function of each group is re-

Stefano Favaro is Assistant Professor of Statistics,
Department of Economics and Statistics, University of
Torino, C.so Unione Sovietica 218/bis, 10134 Torino,
Italy e-mail: stefano.favaro@unito.it. Yee Whye Teh is
Professor of Statistical Machine Learning, Department
of Statistics, University of Oxford, 1 South Parks Road,
Oxford OX13TG, United Kingdom e-mail:
y.w.teh@stats.ox.ac.uk.

1Also aliated with Collegio Carlo Alberto, Moncalieri,

Italy.

This is an electronic reprint of the original article
published by the Institute of Mathematical Statistics in
Statistical Science, 2013, Vol. 28, No. 3, 335359. This
reprint diers from the original in pagination and
typographic detail.

ferred to as a component of the mixture model and
is weighted by the relative frequency of the group in
the population. Specically, assuming k being xed,
a collection of observations (Y1, . . . , Yn) is modeled
as independent draws from a mixture distribution
function with k components, that is,

(1.1)

Yi

ind

k

Xj=1

Jjf (| Xj),

where f (| X) is a given parametric family of dis-
tribution functions indexed by a parameter X and
( J1, . . . , Jk) are the mixture proportions constrained
to be nonnegative and sum to unity. A convenient
formulation of the mixture model (1.1) can be stated
in terms of latent allocation random variables, namely,
each observation Yi is assumed to arise from a spe-
cic but unknown component Zi of the mixture model.
Accordingly, an augmented version of (1.1) can be
written in terms of a collection of latent random
variables (Z1, . . . , Zn), independent and identically
distributed with probability mass function P[Zi =

1

2

S. FAVARO AND Y. W. TEH

j] = Jj , such that the observations are modeled as

(1.2)

Yi|Zi

ind f (| XZi).

Integrating out the random variables (Z1, . . . , Zn)
then yields (1.1). In a Bayesian setting the formula-
tion of the mixture model (1.2) is completed by spec-
ifying suitable prior distributions for the unknown
quantities that are objects of the inferential analysis:
the parameter ( X1, . . . , Xk) and the vector of pro-
portions ( J1, . . . , Jk). We refer to the monographs
by Titterington et al. [83] and McLachlan and Bas-
ford [55] for accounts on mixture models with a xed
number of components. Markov chain Monte Carlo
(MCMC) methods for Bayesian analysis of mixture
models with a xed number of components was pre-
sented in Dielbot and Robert [10].

As regards the general case where the number
of components is unknown, a direct approach has
been considered in Richardson and Green [79], who
modeled the unknown k by mixing over the xed
k case, and made a fully Bayesian inference using
the reversible jump MCMC methods proposed in
