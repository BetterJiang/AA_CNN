Bayesian Learning via Stochastic Gradient Langevin Dynamics

Max Welling
welling@ics.uci.edu
D. Bren School of Information and Computer Science, University of California, Irvine, CA 92697-3425, USA

Yee Whye Teh
Gatsby Computational Neuroscience Unit, UCL, 17 Queen Square, London WC1N 3AR, UK

ywteh@gatsby.ucl.ac.uk

Abstract

In this paper we propose a new framework
for learning from large scale datasets based
on iterative learning from small mini-batches.
By adding the right amount of noise to a
standard stochastic gradient optimization al-
gorithm we show that the iterates will con-
verge to samples from the true posterior dis-
tribution as we anneal the stepsize. This
seamless transition between optimization and
Bayesian posterior sampling provides an in-
built protection against overﬁtting. We also
propose a practical method for Monte Carlo
estimates of posterior statistics which moni-
tors a “sampling threshold” and collects sam-
ples after it has been surpassed. We apply
the method to three models: a mixture of
Gaussians, logistic regression and ICA with
natural gradients.

1. Introduction

In recent years there has been an increasing amount
of very large scale machine learning datasets, ranging
from internet traﬃc and network data, computer vi-
sion, natural language processing, to bioinformatics.
More and more advances in machine learning are now
driven by these large scale data, which oﬀers the op-
portunity to learn large and complex models for solv-
ing many useful applied problems. Recent successes
in large scale machine learning have mostly been opti-
mization based approaches. While there are sophisti-
cated algorithms designed speciﬁcally for certain types
of models, one of the most successful class of algo-
rithms are stochastic optimization, or Robbins-Monro,
algorithms. These algorithms process small (mini-

Appearing in Proceedings of the 28 th International Con-
ference on Machine Learning, Bellevue, WA, USA, 2011.
Copyright 2011 by the author(s)/owner(s).

)batches of data at each iteration, updating model
parameters by taking small gradient steps in a cost
function. Often these algorithms are run in an on-
line setting, where the data batches are discarded af-
ter processing and only one pass through the data is
performed, reducing memory requirements drastically.
One class of methods “left-behind” by the recent ad-
vances in large scale machine learning are the Bayesian
methods. This has partially to do with the negative
results in Bayesian online parameter estimation (An-
drieu et al., 1999), but also the fact that each iteration
of typical Markov chain Monte Carlo (MCMC) algo-
rithms requires computations over the whole dataset.
Nevertheless, Bayesian methods are appealing in their
ability to capture uncertainty in learned parameters
and avoid overﬁtting. Arguably with large datasets
there will be little overﬁtting. Alternatively, as we
have access to larger datasets and more computational
resources, we become interested in building more com-
plex models, so that there will always be a need to
quantify the amount of parameter uncertainty.
In this paper, we propose a method for Bayesian learn-
ing from large scale datasets. Our method combines
Robbins-Monro type algorithms which stochastically
optimize a likelihood, with Langevin dynamics which
injects noise into the parameter updates in such a way
that the trajectory of the parameters will converge
to the full posterior distribution rather than just the
maximum a posteriori mode. The resulting algorithm
starts oﬀ being similar to stochastic optimization, then
automatically transitions to one that simulates sam-
ples from the posterior using Langevin dynamics.
In Section 2 we introduce the two ingredients of our
method:
stochastic optimization and Langevin dy-
namics. Section 3 describes our algorithm and how
it converges to the posterior distribution. Section 4
describes a practical method of estimating when our
algorithm will transition from stochastic optimization
to Langevin dynamics. Section 5 demonstrates our al-

Stochastic Gradient Langevin Dynamics

gorithm on a few models and Section 6 concludes.

2. Preliminaries

N

∏

i=1 p(xi|θ).

Let θ denote a parameter vector, with p(θ) a prior
distribution, and p(x|θ) the probability of data item
x given our model parameterized by θ. The posterior
distribution of a set of N data items X = {xi}N
is: p(θ|X) ∝ p(θ)
i=1
In the optimization
literature the prior regularizes the parameters while
the likelihood terms constitute the cost function to
be optimized, and the task is to ﬁnd the maximum
∗. A popular class
a posteriori (MAP) parameters θ
of methods called stochastic optimization (Robbins &
Monro, 1951) operates as follows. At each iteration t,
a subset of n data items Xt = {xt1, . . . , xtn} is given,
and the parameters are updated as follows:

∆θt = ϵt
2

∇ log p(θt) + N

n

∇ log p(xti|θt)

(1)

(

)

n∑

i=1

where ϵt is a sequence of step sizes. The general
idea is that the gradient computed on the subset is
used to approximate the true gradient over the whole
dataset. Over multiple iterations the whole dataset
is used and the noise in the gradient caused by using
subsets rather than the whole dataset averages out.
For large datasets where the subset gradient approx-
imation is accurate enough, this can give signiﬁcant
computational savings over using the whole dataset to
compute gradients at each iteration.
To ensure convergence to a local maximum, in addition
to other technical assumptions, a major requirement
is for the step sizes to satisfy the property
t < ∞
ϵ2

ϵt = ∞

∞∑

∞∑

(2)

t=1

t=1

Intuitively, the ﬁrst constraint ensures that parameters
will reach the high probability regions no matter how
far away it was initialized to, while the second ensures
that the parameters will converge to the mode instead
of just bouncing around it. Typically, step sizes ϵt =
a(b + t)−γ are decayed polynomially with γ ∈ (0.5, 1].
The issue with ML or MAP estimation, as stochas-
tic optimization aims to do, is that they do not cap-
ture parameter uncertainty and can potentially overﬁt
data. The typical way in which Bayesian approaches
capture parameter uncertainty is via Markov chain
Monte Carlo (MCMC) techniques (Robert & Casella,
2004). In this paper we will consider a class of MCMC
techniques called Langevin dynamics (Neal, 2010). As
before, these take gradient steps, but also injects Gaus-
sian noise into the parameter updates so that they do

not collapse to just the MAP solution:

(

∆θt = ϵ
2
ηt ∼ N(0, ϵ)

N∑

i=1

∇ log p(θt) +

∇ log p(xi|θt)

)

+ ηt

(3)

The gradient step sizes and the variances of the in-
jected noise are balanced so that the variance of the
samples matches that of the posterior. Langevin dy-
namics is motivated and originally derived as a dis-
cretization of a stochastic diﬀerential equation whose
equilibrium distribution is the posterior distribution.
To correct for discretization error, one can take (3)
to just be a proposal distribution and correct using
Metropolis-Hastings.
Interestingly, as we decrease ϵ
the discretization error decreases as well so that the re-
jection rate approaches zero. However typical MCMC
practice is to allow an initial adaptation phase where
the step sizes are adjusted, followed by ﬁxing the step
sizes to ensure a stationary Markov chain thereafter.
More sophisticated techniques use Hamiltonian dy-
namics with momentum variables to allow parameters
to move over larger distances without the ineﬃcient
random walk behaviour of Langevin dynamics (Neal,
2010). However, to the extent of our knowledge all
MCMC methods proposed thus far require computa-
tions over the whole dataset at every iteration, result-
ing in very high computational costs for large datasets.

3. Stochastic Gradient Langevin

Dynamics

Given the similarities between stochastic gradient al-
gorithms (1) and Langevin dynamics (3), it is nat-
ural to consider combining ideas from the two ap-
proaches. This allows eﬃcient use of large datasets
while allowing for parameter uncertainty to be cap-
tured in a Bayesian manner. The approach is straight-
forward: use Robbins-Monro stochastic gradients, add
an amount of Gaussian noise balanced with the step
size used, and allow step sizes to go to zero. The pro-
posed update is simply:

∇ log p(θt) + N

∇ log p(xti|θt)

(

∆θt = ϵt
2
ηt ∼ N(0, ϵt)

n∑

n

i=1

)

+ ηt

(4)

where the step sizes decrease towards zero at rates sat-
isfying (2). This allows averaging out of the stochastic-
ity in the gradients, as well as MH rejection rates that
go to zero asymptotically, so that we can simply ignore
the MH acceptance steps, which require evaluation of
probabilities over the whole dataset, all together.

Stochastic Gradient Langevin Dynamics

In the rest of this section we will give an intuitive argu-
ment for why θt will approach samples from the pos-
terior distribution as t → ∞.
In particular, we will
show that for large t, the updates (4) will approach
Langevin dynamics (3), which converges to the poste-
N∑
rior distribution. Let

∇ log p(xi|θ)

(5)

g(θ) = ∇ log p(θ) +
n∑

i=1

n

i=1

be the true gradient of the log probability at θ and

ht(θ) = ∇ log p(θ) + N

∇ log p(xti|θ) − g(θ) (6)

The stochastic gradient is then g(θ)+ht(θ), with ht(θ)
a zero mean random variable (due to the stochasticity
of the data items chosen at step t) with ﬁnite variance
V (θ), and (4) is,

∆θt = ϵt
2

(g(θt) + ht(θt)) + ηt,

ηt ∼ N(0, ϵt)

(7)

There are two sources of stochasticity in (7): the in-
jected Gaussian noise with variance ϵt, and the noise in
the stochastic gradient, which has variance ( ϵt
2 )2V (θt).
The ﬁrst observation is that for large t, ϵt → 0, and
the injected noise will dominate the stochastic gradient
noise, so that (7) will be eﬀectively Langevin dynam-
ics (3). The second observation is that as ϵt → 0,
the discretization error of Langevin dynamics will be
negligible so that the MH rejection probability will ap-
proach 0 and we may simply ignore this step.
In other words, (4), (7) eﬀectively deﬁne a non-
stationary Markov chain such that the tth step tran-
sition operator, for all large t, will have as its equilib-
rium distribution the posterior over θ. The next ques-
tion we address is whether the sequence of parameters
θ1, θ2, . . . will converge to the posterior distribution.
Because the Markov chain is not stationary and the
step sizes reduce to 0, it is not immediately clear that
this is the case. To see that this is indeed true, we
will show that a subsequence θt1, θt2, . . . will converge
to the posterior as intended so the whole sequence will
also converge.
∑
First ﬁx an ϵ0 such that 0 < ϵ0 ≪ 1. Since {ϵt} satisfy
the step size property (2), we can ﬁnd a subsequence
t=ts+1 ϵt → ϵ0 as s → ∞.
t1 < t2 < ··· such that
Since the injected noise at each step is independent, for
t=ts+1 ηt∥2,
√
ϵ0). We now
between steps ts and ts+1 will be O(
show that the total noise due to the stochasticity of
the gradients among these steps will be dominated by
the total injected noise. Since ϵ0 ≪ 1, we may take

large enough s the total injected noise, ∥∑

ts+1

ts+1

∥2 ≪ 1 for t between ts and ts+1. Making
∥θt − θts
the assumption that the gradient g(·) vary smoothly
(e.g. they are Lipschitz continuous in the models in
ts+1∑
Section 5), the total stochastic gradient is:

(g(θt) + ht(θt))

(8)

ϵt
2

t=ts+1

= ϵ0

2 g(θts) + O(ϵ0) +

ϵt
2 ht(θt)

ts+1∑

t=ts+1

Since the parameters did not vary much between ts
and ts+1, the stochasticity in ht(θt) will be dominated
by the randomness in the choice of the mini-batches.
Assuming that these are chosen randomly and inde-
pendently, ht(θt) for each t will be basically iid (if
mini-batches were chosen by random partitioning of
the whole dataset, ht(θt) will be negatively correlated
instead and will not change the results here). Thus
the variance of

∑

∑

ts+1
t=ts+1

2 ht(θt) is O(
ϵt

ϵ2
4 ) and
t

t

(√∑

)

ts+1
t=ts+1

ϵ2
t
4

= ϵ0
= ϵ0

2 g(θts) + O(ϵ0) + O
2 g(θts) + O(ϵ0)

√

The last equation says that the total stochastic gra-
dient step is approximately the exact gradient step at
θts with a step size of ϵ0, with a deviation dominated
by O(ϵ0). Since this is in turn dominated by the total
injected noise which is O(
ϵ0), this means that the se-
quence θt1, θt2 , . . . will approach a sequence generated
by Langevin dynamics with a ﬁxed step size ϵ0, so it
will converge to the posterior distribution. Note also
that it will have inﬁnite eﬀective sample size.
The implication of this argument is that we can use
stochastic gradient Langevin dynamics as an “any-
time” and general-purpose algorithm.
In the initial
phase the stochastic gradient noise will dominate and
the algorithm will imitate an eﬃcient stochastic gra-
dient ascent algorithm. In the later phase the injected
noise will dominate, so the algorithm will imitate a
Langevin dynamics MH algorithm, and the algorithm
will transition smoothly between the two. However
a disadvantage is that to guarantee the algorithm to
work it is important for the step sizes to decrease to
zero, so that the mixing rate of the algorithm will
slow down with increasing number of iterations. To
address this, we can keep the step size constant once
it has decreased below a critical level where the MH
rejection rate is considered negligible, or use this al-
gorithm for burn-in, but switch to a diﬀerent MCMC
algorithm that makes more eﬃcient use of the whole
dataset later. These alternatives can perform better

Stochastic Gradient Langevin Dynamics

but will require further hand-tuning and are beyond
the scope of this paper. The point of this paper is
to demonstrate a practical algorithm that can achieve
proper Bayesian learning using only mini-batch data.

4. Posterior Sampling

In this section we consider the use of our stochastic
gradient Langevin dynamics algorithm as one which
produces samples from the posterior distribution. We
ﬁrst derive an estimate of when the algorithm will
transition from stochastic optimization to Langevin
dynamics. The idea is that we should only start col-
lecting samples after it has entered its posterior sam-
pling phase, which will not happen until after it be-
comes Langevin dynamics. Then we discuss how the
algorithm scales with the dataset size N and give a
rough estimate of the number of iterations required for
the algorithm to traverse the whole posterior. Finally
we discuss how the obtained samples can be used to
form Monte Carlo estimates of posterior expectations.

4.1. Transition into Langevin dynamics phase

We ﬁrst generalize our method to allow for precon-
ditioning, which can lead to signiﬁcant speed ups by
better adapting the step sizes to the local structure of
the posterior (Roberts & Stramer, 2002; Girolami &
Calderhead, 2011). For instance, certain dimensions
may have a vastly larger curvature leading to much
bigger gradients. In this case a symmetric precondi-
tioning matrix M can transform all dimensions to the
same scale. The preconditioned stochastic gradient
Langevin dynamics is simply,

∆θt = ϵt

2 M

g(θt) + ht(θt)

+ ηt,

ηt ∼ N(0, ϵtM)

(

)

As noted previously, whether the algorithm is in the
stochastic optimization phase or Langevin dynamics
phase depends on the variance of the injected noise,
which is simply ϵtM, versus that of the stochastic gra-
dient. Since the stochastic gradient is a sum over the
current mini-batch, if its size n is large enough the
central limit theorem will kick in and the variations
ht(θt) around the true gradient g(θt) will become nor-
mally distributed. Its covariance matrix can then be
estimated from the empirical covariance:
V (θt) ≡ V [ht(θt)] ≈ N 2
n2

(sti − st)(sti − st)⊤ (9)

n∑

i=1

where sti = ∇ log p(xti|θt) + 1
of data item i at iteration t and st = 1
n
the empirical mean. Note that V (θt) = N 2

∇ log p(θt) is the score
i=1 sti is
n Vs, where

N

n

∑

Vs is the empirical covariance of the scores {sti}, so
scales as N 2
n . From this we see that the variance of
the stochastic gradient step is ϵ2
t N 2
4n M VsM, so that to
get the injected noise to dominate in all directions, we
need the condition

ϵtN 2
4n

λmax(M

1
2 VsM

1

2 ) = α ≪ 1

(10)

where λmax(A) is the largest eigenvalue of A. In other
words, if we choose a stepsize such that the sample
threshold α ≪ 1, the algorithm will be in its Langevin
dynamics phase and will be sampling approximately
from the posterior.
We can now relate the step size at the sampling thresh-
old to the posterior variance via the Fisher informa-
tion, which is related to Vs as IF ≈ N Vs, and to the
posterior variance Σθ ≈ I
−1
F . Using these relationships
as well as (10), we see that the step size at the sam-
pling threshold is ϵt ≈ 4αn
N λmin(Σθ). Since Langevin
dynamics explores the posterior via a random walk,
using this step size implies that we need on the order
of N/n steps to traverse the posterior, i.e. we process
the whole dataset. So we see this method is not a
silver bullet. However, the advantage of the method
is its convenience: stochastic optimization smoothly
and automatically transitions into posterior sampling
without changing the update equation. Even without
measuring the sampling threshold one will enjoy the
beneﬁt of protection against overﬁtting and the ability
to perform Bayesian learning. Measuring the sampling
threshold will only be important if one needs to faith-
fully represent the posterior distribution with a ﬁnite
collection of samples.

T

∑

4.2. Estimating Posterior Expectations
Since θ1, θ2, . . . converges to the posterior distribution,
we can estimate the posterior expectation E[f(θ)] of
some function f(θ) by simply taking the sample av-
erage 1
t=1 f(θt) (as typically in MCMC, we may
T
remove the initial burn-in phase, say estimated using
the sampling threshold). Since f(θt) is an asymptoti-
cally unbiased estimator for E[f(θ)], this sample aver-
age will be consistent. Observe however that because
the step size decreases, the mixing rate of the Markov
chain decreases as well, and the simple sample aver-
age will over-emphasize the tail end of the sequence
where there is higher correlation among the samples,
resulting in higher variance in the estimator. Instead
we propose to use the step sizes to weight the samples:

∑

∑

T

E[f(θ)] ≈

t=1 ϵtf(θt)

T
t=1 ϵt

(11)

Stochastic Gradient Langevin Dynamics

Figure 1. True and estimated posterior distribution.

Figure 2. Left: variances of stochastic gradient noise and
injected noise. Right: rejection probability versus step size.
We report the average rejection probability per iteration in
each sweep through the dataset.

∑∞
t=1 ϵt = ∞, this estimator will be consistent
Since
as well. The intuition is that the rate at which the
Markov chain mixes is proportional to the step size, so
that we expect the eﬀective sample size of {θ1, . . . , θT}
to be proportional to
t=1 ϵt, and that each θt will
contribute an eﬀective sample size proportional to ϵt.

∑

T

5. Experiments

5.1. Simple Demonstration

We ﬁrst demonstrate the workings of our stochastic
gradient Langevin algorithm on a simple example in-
volving only two parameters. To make the posterior
multimodal and a little more interesting, we use a mix-
ture of Gaussians with tied means:

x) + 1

1) ;
2 N(θ1, σ2

θ1 ∼ N(0, σ2
xi ∼ 1
1 = 10, σ2

θ2 ∼ N(0, σ2
2)
2 N(θ1 + θ2, σ2
x)
where σ2
x = 2. 100 data points
are drawn from the model with θ1 = 0 and θ2 = 1.
There is a mode at this parameter setting, but also a
secondary mode at θ1 = 1, θ2 = −1, with strong neg-
ative correlation between the parameters. We ran the
stochastic gradient Langevin algorithm with a batch-

2 = 1 and σ2

Figure 3. Average log joint probability per data item (left)
and accuracy on test set (right) as functions of the num-
ber of sweeps through the whole dataset. Red dashed line
represents accuracy after 10 iterations. Results are aver-
aged over 50 runs; blue dotted lines indicate 1 standard
deviation.

size of 1 and using 10000 sweeps through the whole
dataset. The step sizes are ϵt = a(b + t)−γ where
γ = .55 and a and b are set such that ϵt decreases
from .01 to .0001 over the duration of the run. We see
from Figure 1 that the estimated posterior distribu-
tion is very accurate. In Figure 2 we see that there are
indeed two phases to the stochastic gradient Langevin
algorithm: a ﬁrst phase where the stochastic gradient
noise dominates the injected noise, and a second phase
where the converse occurs. To explore the scaling of
the rejection rate as a function of step sizes, we reran
the experiment with step sizes exponentially decreas-
ing from 10−2 to 10−8. In the original experiment the
dynamic range of the step sizes is not wide enough for
visual inspection. Figure 2(right) shows the rejection
probability decreasing to zero as step size decreases.

5.2. Logistic Regression

We applied our stochastic gradient Langevin algorithm
to a Bayesian logistic regression model. The probabil-
ity of the ith output yi ∈ {−1, +1} given the corre-
sponding input vector xi is modelled as:

p(yi|xi) = σ(yiβ

⊤

xi)

(12)

1+exp(−z).
where β are the parameters, and σ(z) =
The bias parameter is absorbed into β by including 1
as an entry in xi. We use a Laplace prior for β with a
scale of 1. The gradient of the log likelihood is:

1

⊤

∂
∂β

log p(yi|xi) = σ(−yiβ

xi)yixi

(13)
while the gradient of the prior is simply −sign(β),
which is applied elementwise.
We applied our inference algorithm to the a9a dataset
derived by (Lin et al., 2008) from the UCI adult
dataset. It consists of 32561 observations and 123 fea-
tures, and we used batch sizes of 10. Results from 50

−1012−3−2−10123−1012−3−2−1012310010210410610−610−410−2100iterationnoise variance  ∇θ1 noise∇θ2 noiseinjected noise10−810−610−410−210−310−210−1100step sizeaverage rejection rate0246810−7−6−5−4−3−2−10Number of iterations through whole datasetLog joint probability per datum02684-6-4-5-310-2-10-700.511.520.650.70.750.80.85Number of iterations through whole datasetAccuracy on test data  Accuracy after 10 iterationsAccuracy00.51.5210.70.80.750.850.65Stochastic Gradient Langevin Dynamics

runs are shown in Figure 3, with the model trained
on a random 80% of the dataset and tested on the
other 20% in each run. We see that both the joint
probability and the accuracy increase rapidly, with the
joint probability converging after at most 10 iterations,
while the accuracy converging after less than 1 itera-
tion through the dataset, demonstrating the eﬃciency
of the stochastic gradient Langevin dynamics.

5.3. Independent Components Analysis

In the following we will brieﬂy review a popular ICA
algorithm based on stochastic (natural) gradient opti-
mization (Amari et al., 1996). We start from a proba-
bilistic model that assumes independent, heavy tailed
marginal distributions,

[∏

]∏

i

ij

i x)

pi(wT

p(x, W ) = | det(W )|

N (Wij; 0, λ)
(14)
where we have used a Gaussian prior over the weights.
It has been found that the eﬃciency of gradient descent
can be signiﬁcantly improved if we use a natural gradi-
ent. This is implemented by post-multiplication of the
gradient with the term W T W (Amari et al., 1996). If
we choose pi(yi) =
i x, we get

2 yi) with yi = wT

4 cosh2( 1

1

.= ∇W log[p(X, W )] W T W =

)

tanh(

1
2

yn)yT
n

W − λW W T W (15)

(
NI − N∑
DW

n=1

The term W T W acts like a preconditioning matrix (see
section 4.1), Mij,kl = δik(W T W )jl which is symmetric
under the exchange (i ↔ k, j ↔ l). It can be shown
−1 = (cid:14)(W T W )−1,
that the inverse of M is given by M
√
and the matrix square root as
W T W with

√

√

M = (cid:14)
2 U T if W T W = UΛU T .

W T W = UΛ 1

The update equation for Langevin dynamics thus be-
comes,

Wt+1 = Wt +

1

2 εtDW + (cid:17)t

W T W

(16)

√

where every element of (cid:17)t is normally distributed with
variance εt: ηij,t ∼ N [0, εt]. Our stochastic version
simply approximates the part of the gradient that
sums over data-cases with a sum over a small mini-
batch of size n and multiplies the result with N/n to
bring it back to the correct scale. We also anneal the
stepsizes according to εt ∝ a(b + t)−γ.
To assess how well our stochastic Langevin approach
compares against a standard Bayesian method we im-
plemented the ”corrected Langevin” MCMC sampler.

[

[

]

]

∗, as in Eqn.16.
This sampler, proposes a new state W
Note however that we sum over all data-cases and that
we do not anneal the stepsize. Secondly, we need to
accept or reject the proposed step based on all the
data-cases in order to guarantee detailed balance. The
proposal distribution is given by (suppressing depen-
dence on t),

q(W → W∗) = N

W

∗; W +

1

2 εDW ; εM

(17)

where the quadratic function in the exponent is con-
veniently computed as,

−1
2ε

tr[(δW − 1

2 εDW )T ]
2 εDW )(W T W )−1(δW − 1
(18)
∗ − W and the normalization constant
with δW = W
requires the quantity det M = det(W T W )D. The ac-
cept/reject step is then given by the usual Metropolis
Hastings rule:

p(accept) = min

1,

∗)q(W

∗ → W )
p(W
p(W )q(W → W ∗)

(19)

Finally, to compute the sampling threshold of Eqn.10,
we can use
[(
2V(s)M
1
N

2 =
∇ log p(W ) + ∇ log p(xi|W )

(W T W ) 1

)

(20)

covn

]

M

1

1

2

with covn the sample covariance over the mini-batch
of n data-cases.
To show the utility of our Bayesian variant of ICA we
deﬁne the following “instability” metric for indepen-
dent components:
Ii =

var(Wij)var(xj)

∑

(21)

j

where var(Wij) is computed over the posterior sam-
ples and var(xj) is computed over the data-cases.
The reason that we scale the variance of the weight
entry Wij with the variance of xj is that the vari-
ance of the sources yi =
j Wijxj is approximately
equal for all i because they are ﬁt to the distribution
pi(yi) =

∑

1

4 cosh2( 1

2 yi).

5.3.1. Artificial Data

In the ﬁrst experiment we generated 1000 data-cases
IID in six channels. Three channels had high kurtosis
distributions while three others where normally dis-
tributed. We ran stochastic Langevin dynamics with

Stochastic Gradient Langevin Dynamics

Figure 4. Left two (cid:12)gures: Amari distance over time for stochastic Langevin dynamics and corrected Langevin dynamics.
Thick line represents the online average. First few hundred iterations were removed to show the scale of the (cid:13)uctuations.
Right two (cid:12)gures: Instability index for the 6 independent components computed in section 5.3.1 for stochastic Langevin
dynamics and corrected Langevin dynamics.

Figure 5. Posterior density estimates for arti(cid:12)cial dataset for stochastic Langevin and corrected Langevin dynamics mea-
sured across the W11 (cid:0) W12 and W1;1 (cid:0) W2;1 axes.

∑

a batch-size of 100 for a total of 500,000 iterations
−0.55.
and a polynomial annealing schedule εt = 4
N t
After around 10,000 iterations the sampling threshold
at α = 0.1 was met. At that point we recorded the
“mixing distance” as D0 = εt and collected samples
t εt from the last sample time
only when the sum
exceeded D0 (in other words, as εt decreases we col-
lect fewer samples per unit time). We note that simply
collecting all samples had no noticeable impact on the
ﬁnal results. The last estimate of W was used to ini-
tialize corrected Langevin dynamics (this was done to
force the samplers into the same local maximum) after
which we also collected 500, 000 samples. For corrected
Langevin we used a constant stepsize of ε = 0.1
N .
The two left ﬁgures of Figure 4 show the Amari dis-
tance (Amari et al., 1996) over time for stochastic
and corrected Langevin dynamics respectively. The
right two ﬁgures show the sorted values of our pro-
posed instability index. Figures 5 show two dimen-
sional marginal density estimates of the posterior dis-
tribution of W . ICA cannot determine the Gaussian
components and this fact is veriﬁed by looking at the
posterior distribution. In fact, the stochastic Langevin
algorithm has mixed over a number of modes that pre-
sumably correspond to diﬀerent linear combinations of
the Gaussian components. To a lesser degree the cor-
rected Langevin has also explored two modes. Due
to the complicated structure of the posterior distri-

bution the stability index varies strongly between the
two sampling algorithms for the Gaussian components
(and in fact also varies across diﬀerent runs). We veri-
ﬁed that the last three components correspond to sta-
ble, high kurtosis components.

5.3.2. MEG Data

downloaded

dataset

the MEG

We
from
http://www.cis.hut.(cid:12)/projects/ica/eegmeg/MEG data.html.
There are 122 channels and 17730 time-points, from
which we extracted the ﬁrst 10 channels for our
experiment. To initialize the sampling algorithms,
we ﬁrst ran fastICA (Hyvarinen, 1999) to ﬁnd an
initial estimate of the de-mixing matrix W . We
then ran stochastic Langevin and corrected Langevin
dynamics to sample from the posterior. The settings
were very similar to the previous experiment with a
−0.55 for stochastic Langevin and
schedule of εt = 0.1
N t
a constant stepsize of 1/N for corrected Langevin.
We obtained 500, 000 samples for stochastic Langevin
in 800 seconds and 100,000 samples for corrected
Langevin in 9000 seconds. We visually veriﬁed
that the two dimensional marginal distributions of
stochastic Langevin and corrected Langevin dynamics
were very similar. The instability values are shown in
ﬁgure 6. Due to the absence of Gaussian components
we see that the stability indices are very similar across
the two sampling algorithms.
It was veriﬁed that

2468x 104246810iterationAmari distanceAmari Distance Stoc. Lan.00.511.52x 104246810iterationAmari distanceAmari Distance Corr. Lan.123456050100150200Sorted Component IDInstability MetricInstability Metric Stoc. Lan.123456020406080100Sorted Component IDInstability MetricInstability Metric Corr. Lan.W(1,1)W(1,2)PDF W(1,1) vs W(1,2) Stoc. Lan.−505−6−4−20246W(1,1)W(1,2)PDF W(1,1) vs W(1,2) Corr. Lan.−4−2024−6−4−2024W(1,1)W(2,1)PDF W(1,1) vs W(2,1) Stoc. Lan.−505−505W(1,1)W(2,1)PDF W(1,1) vs W(2,1) Corr. Lan.−4−2024012345Stochastic Gradient Langevin Dynamics

We believe that this work represents only a tentative
ﬁrst step to further work on eﬃcient MCMC sampling
based on stochastic gradients. Interesting directions of
research include stronger theory providing a solid proof
of convergence, deriving a MH rejection step based
on mini-batch data, extending the algorithm to the
online estimation of dynamical systems, and deriving
algorithms based on more sophisticated Hamiltonian
Monte Carlo approaches which do not suﬀer from ran-
dom walk behaviour.

Acknowledgements

This material is based upon work supported by the Na-
tional Science Foundation under Grant No.
0447903,
1018433 (MW) and the Gatsby Charitable Foundation
(YWT).

References

Amari, S., Cichocki, A., and Yang, H.H. A new algorithm
for blind signal separation. In Neural Information Pro-
cessing Systems, volume 8, pp. 757{763, 1996.

Andrieu, C., de Freitas, N., and Doucet, A. Sequential
MCMC for Bayesian model selection. In Proceedings of
the IEEE Signal Processing Workshop on Higher-Order
Statistics, pp. 130{134, 1999.

Bottou, L. and Bousquet, O. The tradeo(cid:11)s of large scale
learning. In Advances in Neural Information Processing
Systems, volume 20, pp. 161{168, 2008.

Girolami, M. and Calderhead, B. Riemann manifold
Langevin and Hamiltonian Monte Carlo methods. Jour-
nal of the Royal Statistical Society B, 73:1{37, 2011.

Hyvarinen, A. Fast and robust (cid:12)xed-point algorithms for
independent component analysis. IEEE Transactions on
Neural Networks, 10(3):626{634, 1999.

Lin, C.-J., Weng, R. C., and Keerthi, S. S. Trust region
Newton method for large-scale logistic regression. Jour-
nal of Machine Learning Research, 9:627{650, 2008.

Neal, R. M. MCMC using Hamiltonian dynamics.

In
Brooks, S., Gelman, A., Jones, G., and Meng, X.-L.
(eds.), Handbook of Markov Chain Monte Carlo. Chap-
man & Hall / CRC Press, 2010.

Robbins, H. and Monro, S. A stochastic approximation
method. Annals of Mathematical Statistics, 22(3):400{
407, 1951.

Robert, C. P. and Casella, G. Monte Carlo statistical meth-

ods. Springer Verlag, 2004.

Roberts, G. O. and Stramer, O. Langevin di(cid:11)usions and
metropolis-hastings algorithms. Methodology and Com-
puting in Applied Probability, 4:337{357, 2002.

Figure 6.
dataset
Langevin (right) respectively.

Instability indices of 10 components for MEG
stochastic Langevin (left) and corrected

for

the most stable component corresponded to a highly
kurtotic source (kurtosus = 15.4), while the most
unstable component was closer to Gaussian noise
with a kurtosis of 3.4 (2 corresponds to Gaussian).
These ﬁndings verify that the stochastic Langevin
procedure produces accurate posterior distributions
that are in full agreement with a well established
MCMC procedure.

6. Discussion

Stochastic gradient optimization is among the most ef-
fective algorithms if we measure “predictive accuracy
obtained per unit of computation” (Bottou & Bous-
quet, 2008). Due to subsampling noise, the parame-
ter estimates ﬂuctuate around their MAP values. The
common wisdom is that one must anneal these step-
sizes to zero to reach the ﬁxed point. However, we
argue that one should not optimize beyond the scale
of the posterior distribution. The posterior represents
the intrinsic statistical scale of precision and trying to
determine parameter values with more precision runs
the risk of overﬁtting at additional computational cost.
MCMC sampling from the posterior distribution does
of course address the overﬁtting issue. However, gen-
eral MCMC algorithms need to see all the data at ev-
ery iteration, and thus lose the beneﬁts of the stochas-
tic approximation approaches. This paper oﬀers for
the ﬁrst time a surprisingly simple solution that rep-
resents the best of both worlds: stick with stochastic
gradients but sample from the posterior nevertheless.
But perhaps the biggest advantage of stochastic gra-
dient Langevin dynamics is the fact that stochastic
optimization seamlessly transitions into posterior sam-
pling. By simply adding Gaussian noise with the cor-
rect variance our method performs “early stopping”
automatically without ever having to worry about it.
In fact, we have shown that with a polynomial anneal-
ing schedule the obtained samples will asymptotically
represent the posterior distribution faithfully.

1234567891000.010.020.030.040.050.060.07Sorted ComponentsInstability IndexInstability Index Stoc. Lan.1234567891000.010.020.030.040.050.060.07Sorted ComponentsInstability IndexIntability Index Corr. Lan.