A Stochastic Memoizer for Sequence Data

Frank Wood!
C´edric Archambeau†
Jan Gasthaus!
Lancelot James‡
Yee Whye Teh!
!Gatsby Computational Neuroscience Unit
University College London, 17 Queen Square, London, WC1N 3AR, UK
†Centre for Computational Statistics and Machine Learning
University College London, Gower Street, London, WC1E 6BT, UK
‡Department of Information and Systems Management
Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong

lancelot@ust.hk

ywteh@gatsby.ucl.ac.uk

fwood@gatsby.ucl.ac.uk

c.archambeau@cs.ucl.ac.uk

j.gasthaus@gatsby.ucl.ac.uk

Abstract

We propose an unbounded-depth, hierarchi-
cal, Bayesian nonparametric model for dis-
crete sequence data. This model can be
estimated from a single training sequence,
yet shares statistical strength between subse-
quent symbol predictive distributions in such
a way that predictive performance general-
izes well. The model builds on a speciﬁc pa-
rameterization of an unbounded-depth hier-
archical Pitman-Yor process. We introduce
analytic marginalization steps (using coagu-
lation operators) to reduce this model to one
that can be represented in time and space
linear in the length of the training sequence.
We show how to perform inference in such
a model without truncation approximation
and introduce fragmentation operators nec-
essary to do predictive inference. We demon-
strate the sequence memoizer by using it as
a language model, achieving state-of-the-art
results.

1. Introduction

A Markov assumption is often made when modeling
sequence data. This assumption stipulates that condi-
tioned on the present value of the sequence, the past
and the future are independent. Making this assump-
tion allows one to fully characterize a sequential pro-

Appearing in Proceedings of the 26 th International Confer-
ence on Machine Learning, Montreal, Canada, 2009. Copy-
right 2009 by the author(s)/owner(s).

cess in terms of a set of conditional distributions that
describe the dependence of future values on a ﬁnite his-
tory (or context) of values. The length of this context
is called the order of the Markov model.

The literature provides ample evidence of the fact that
making such an assumption is often reasonable in a
practical sense. Even data that is clearly not Markov
in nature (for instance natural language) is often well-
enough described by Markov models for them to be
of signiﬁcant practical utility. Increasing the order of
the Markov model often improves application perfor-
mance. Unfortunately it is often diﬃcult to increase
the order in practice because increasing the order re-
quires either vastly greater amounts of training data or
signiﬁcantly more complicated smoothing procedures.

In this work we propose a non-Markov model for sta-
tionary discrete sequence data. The model is non-
Markov in the sense that the next value in a sequence is
modelled as being conditionally dependent on all pre-
vious values in the sequence. It is immediately clear
that such a model must have a very large number of
latent variables. To constrain the learning of these la-
tent variables, we employ a hierarchical Bayesian prior
based on Pitman-Yor processes which promotes shar-
ing of statistical strength between subsequent symbol
predictive distributions for equivalent contexts of dif-
ferent lengths (Teh, 2006). We ﬁnd that we can analyt-
ically marginalize out most latent variables, leaving a
number that is linear in the size of the input sequence.
We demonstrate that inference in the resulting col-
lapsed model is tractable and eﬃcient.

Posterior inference in the model can be understood
as stochastically “memoizing” (Michie, 1968) con-

text/observation pairs. While memoization refers to
deterministic caching of function outputs given inputs,
what we mean by stochastic memoization is exactly
that used by (Goodman et al., 2008): calling a func-
tion multiple times with the same arguments may re-
turn an instance from a set of previous return values,
but also may return a new value. We call our contribu-
tion a stochastic memoizer for sequence data (sequence
memoizer (SM) for short) because given a context (the
argument) it will either return a symbol that was al-
ready generated in that full context, a symbol that was
returned given a context that is shorter by one sym-
bol, or, at the recursion base, potentially something
entirely novel. The stochastic memoizer for sequence
data consists of a model and eﬃcient algorithms for
model construction and inference.

In the next section we formalize what we mean by
non-Markov model and deﬁne the prior we use in the
sequence memoizer. In Section 3 we explain how a pos-
terior sampler for the sequence memoizer given a ﬁnite
sequence of observations can be constructed and repre-
sented in linear space and time. In Section 4 we explain
the marginalization operations necessary to achieve
such a representation. In Section 5 we discuss sequence
memoizer inference, particularly the novel steps nec-
essary to perform predictive inference in contexts that
do not occur in the training data. Finally, in Section 6
we use the sequence memoizer for language modelling
and demonstrate promising empirical results.

2. Non-Markov Model

Consider a sequence of discrete random variables
x1:T = (x1x2 · · · xT ) of arbitrary length T , each taking
values in a symbol set Σ. The joint distribution over
the sequence is

P (x1:T ) =

T
!
i=1

P (xi|x1:i−1),

(1)

where each factor on the right hand side is the pre-
dictive probability of xi given a context consisting
of all preceding variables x1:i−1. When one makes
a nth order Markov approximation to (1) it is as-
sumed that only the values taken by at most the pre-
ceding n variables matter for predicting the value of
the next variable in the sequence, i.e. P (xi|x1:i−1) =
P (xi|xi−n:i−1) for all i.
If the context is not trun-
cated to some ﬁxed context length, we say the model
is non-Markovian.

When learning such a model from data, a vector of
predictive probabilities for the next symbol given each
possible context must be learned. Let s ∈ Σ∗ be a

ﬁnite sequence of symbols (of arbitrary length). Let
G[s](v) be the probability of the following variable tak-
ing value v given the context s. Denote by G[s] the
vector of probabilities (parameters) with one element
for each v ∈ Σ. Estimating parameters that generalize
well to unseen contexts given a single training sequence
might seem a priori unreasonable. For example, if our
training sequence were x1:T = s, it is easy to see that
there is only a single observation xi = si in the context
x1:i−1 = s1:i−1 for every preﬁx s1:i−1.
In most cases
this single observation clearly will not be suﬃcient to
estimate a whole parameter vector G[s1:i−1] that gen-
eralizes in any reasonable way.
In the following we
describe a prior that hierarchically ties together the
vector of predictive probabilities in a particular con-
text to vectors of probabilities in related, shorter con-
texts. By doing this we are able to use observations
that occur in very long contexts to recursively inform
the estimation of the predictive probabilities for re-
lated shorter contexts and vice versa.

The way we do this is to place a hierarchical Bayesian
prior over the set of probability vectors {G[s]}s∈Σ∗. On
the root node we place a Pitman-Yor prior (Pitman &
Yor, 1997; Ishwaran & James, 2001) on the probability
vector G[] corresponding to the empty context []:

G[]|d0, c0, H ∼ PY(d0, c0, H),

(2)

where d0 is the discount parameter, c0 the concen-
tration parameter and H the base distribution.1 For
simplicity we take H to be the uniform distribution
over the (assumed) ﬁnite symbol set Σ. At the ﬁrst
level, the random measures {G[s]}s∈Σ are condition-
ally independent given G[], with distributions given
by Pitman-Yor processes with discount parameter d1,
concentration parameter c1 and base distribution G[]:

G[s]|d1, c1, G[] ∼ PY(d1, c1, G[]).

(3)

The hierarchy is deﬁned recursively for any number of
levels. For each non-empty ﬁnite sequence of symbols
s, we have

G[s]|d|s|, c|s|, G[s

#] ∼ PY(d|s|, c|s|, G[s

#]),

(4)

where [s] = [ss$] for some symbol s ∈ Σ, that is, s$ is
s with the ﬁrst contextual symbol removed and |r| is
the length of string r. The resulting graphical model
can be inﬁnitely deep and is tree-structured, with a
random probability vector on each node. The number

1In the statistics literature the discount parameter is
typically denoted by α and the concentration parameter
by θ.
In the machine learning literature α is often used
to denote the concentration parameter instead. We use
diﬀerent symbols here to avoid confusion.

H

G[ ]

G[c]

c

o

a

G[ac]

a

G[oa]

o

G[cac]

c

o

G[acac]

a

G[oacac]

o

c

G[oac]

a

o

G[a]

c

G[o]

G[ca]

a

a

G[aca]

o

G[oaca]

oac

c

G[oacac]

H

G[ ]

ac

o

a

o

G[a]

G[ac]

G[oa]

o

o

c

G[oac]

a

G[o]

a

oac

G[ ]

d0:0

1

1

1

a

c

o

G[a]

11

c

d1:1

G[oa]

d2:2

1

c

G[oaca]

c

G[oaca]

d2:4

1

c

(a) Preﬁx trie for oacac.

(b) Preﬁx tree for oacac.

(c) Initialisation.

Figure 1. (a) preﬁx trie and (b) corresponding preﬁx tree for the string oacac. Note that (a) and (b) correspond to the
suﬃx trie and the suﬃx tree of cacao. (c) Chinese restaurant franchise sampler representation of subtree highlighted in
(b).

of branches descending from each node is given by the
number of elements in Σ.

The hierarchical Pitman-Yor process (HPYP) with
ﬁnite depth has been applied to language models
(Teh, 2006), producing state-of-the-art results. It has
also been applied to unsupervised image segmentation
(Sudderth & Jordan, 2009). Deﬁning an HPYP of un-
bounded depth is straightforward given the recursive
nature of the HPYP formulation. One contribution of
this paper to make inference in such a model tractable
and eﬃcient.

A well known special case of the HPYP is the hierar-
chical Dirichlet process (Teh et al., 2006), which arises
from setting dn = 0 for n ≥ 0. Here, we will use a less-
well-known special case where cn = 0 for n ≥ 0. In this
parameter setting the Pitman-Yor process specializes
to a normalized stable process (Perman, 1990). We use
this particular prior because, as we shall see, it makes
it possible to construct representations of the posterior
of this model in time and space linear in the length
of a training observation sequence. The trade-oﬀ be-
tween this particular parameterization of the Pitman-
Yor process and one in which non-zero concentrations
are allowed is studied in Section 6 and shown to be in-
consequential in the language modelling domain. This
is largely due to the fact that the discount parameter
and the concentration both add mass to the base distri-
bution in the Pitman-Yor process. This notwithstand-
ing, the potential detriment of using a less expressive
prior is often outweighed when gains in computational
eﬃciency mean that more data can be modelled albeit
using a slightly less expressive prior.

3. Representing the Inﬁnite Model

Given a sequence of observations x1:T we are interested
in the posterior distribution over {G[s]}s∈Σ∗, and ulti-

mately in the predictive distribution for a continuation
of the original sequence (or a new sequence of obser-
vations y1:τ ), conditioned on having already observed
x1:T . Inference in the sequence memoizer as described
is computationally intractable because it contains an
inﬁnite number of latent variables {G[s]}s∈Σ∗. In this
section we describe two steps that can be taken to re-
duce the number of these variables such that inference
becomes feasible (and eﬃcient).

First, consider a single, ﬁnite training sequence s con-
sisting of T symbols. The only variables that will
have observations associated with them are the ones
that correspond to contexts that are preﬁxes of s,
i.e. {G[π]}π∈{s1:i|0≤i<T }. These nodes depend only
on their ancestors in the graphical model, which cor-
respond to the suﬃxes of the contexts π. Thus, the
only variables that we need perform inference on are
precisely all those corresponding to contexts which are
contiguous subsequences of s, i.e. {G[sj:i]}1≤j≤i<T .

This reduces the eﬀective number of variables to
O(T 2). The structure of the remaining graphical
model for the sequence s = oacac is given in Fig-
ure 1(a). This structure corresponds to what is known
as a preﬁx trie, which can be constructed from an input
string in O(T 2) time and space (Ukkonen, 1995).

The second marginalization step is more involved and
requires a two step explanation. We start by high-
lighting a marginalization transformation of this preﬁx
trie graphical model that results in a graphical model
with fewer nodes. In the next section we describe how
such analytic marginalization operations can be done
for the Pitman-Yor parameterization (cn = 0 ∀ n) we
have chosen.

Consider a transformation of the branch of the graph-
ical model trie in Figure 1(a) that starts with a. The
transformation of interest will involve marginalizing

out variables like G[ca] and G[aca]. In general we are
interested in marginalizing out all variables that cor-
respond to non-branching interior nodes in the trie.
Assume for now that we can in fact marginalize out
such variables. What remains is to eﬃciently identify
those variables that can be marginalized out. However,
just building a preﬁx trie is of O(T 2) time and space
complexity so using the trie to identify such nodes is
infeasible for long observation sequences.

Interestingly, the collapsed graphical model in Fig-
ure 1(b) has a structure called a preﬁx tree that can
be built directly from an input string in O(T ) time
and space complexity (Weiner, 1973; Ukkonen, 1995).2
The resulting preﬁx tree retains precisely the nodes
(variables) of interest, eliminating all non-branching
nodes in the trie by allowing each edge label to be a
sequence of symbols (or meta-symbol), rather than a
single symbol. The marginalization results of the next
section are used to determine the correct Pitman-Yor
conditional distributions for remaining nodes.

4. Marginalization

Now that we can identify the variables that can be
marginalized out it remains to show how to do this.
When we perform these marginalizations we would
like to ensure that the required marginalization op-
erations result in a model whose conditional distribu-
tions remain tractable and preferably stay in the same
Pitman-Yor family. In this section we show that this
is the case. We establish this fact by reviewing coagu-
lation and fragmentation operators (Pitman, 1999; Ho
et al., 2006).

For the rest of this section we shall consider a single
path in the graphical model, say G1 → G2 → G3,
with G2 having no children other than G3. Recall that
many marginalizations of this type will be performed
during the construction of the tree. Marginalizing out
G2 leaves G1 → G3, and the following result shows
that the conditional distribution of G3 given G1 stays
within the same Pitman-Yor family:

Theorem 1. If G2|G1 ∼ PY(d1, 0, G1) and G3|G2 ∼
PY(d2, 0, G2) then G3|G1 ∼ PY(d1d2, 0, G1) with G2
marginalized out.

Clearly, Theorem 1 can be applied recursively if G1 or

2For purposes of clarity it should be pointed out that
the literature for constructing these data structures is fo-
cused entirely on suﬃx rather than preﬁx tree construction.
Conveniently, however, the preﬁxes of a string are the suf-
ﬁxes of its reverse so any algorithm for building a suﬃx
tree can be used to construct a preﬁx tree by simply giving
it the reverse of the input sequence as input.

Figure 2. Depiction of coagulation and fragmentation.

G3 have just one child as well. The resulting preﬁx tree
graphical model has conditional distributions that are
also Pitman-Yor distributed, with discounts obtained
by multiplying the discounts along the paths of the un-
collapsed preﬁx trie model. This is the key operation
we use to build the preﬁx tree graphical model.

Theorem 1 follows from results on coagulation opera-
tors. In the following we shall outline coagulation op-
erators as well as their inverses, fragmentation opera-
tors. This will set the stage for Theorem 2 from which
Theorem 1 follows. Consider the stick-breaking con-
struction of G2|G1 and G3|G2. The weights or “sticks”
are distributed according to two-parameter GEM dis-
tributions3 with concentration parameters equal to 0:

G2 =

G3 =

∞
"
i=1
∞
"
j=1

πiδφi,φ

iid∼ G1, π ∼ GEM(d1, 0),

i

κjδψj ,ψ j

iid∼ G2,

κ ∼ GEM(d2, 0),

where δθ is a point mass located at θ. The child mea-
sure G3 necessarily has the same support as its parent
G2. Hence, we can sum up (i.e. coagulate) the sticks
associated with each subset of the point masses of G3
that correspond to a point mass of G2, such that

G3 =

∞
"
i=1

τiδφi,τ

i =

∞
"
j=1

κjI(zj = i).

(5)

Here, I(·) is the indicator function and zj = i if ψj = φi.
Note that zj ∼iid π. The above describes a coagula-
tion of κ by GEM(d1, 0).
In general, we shall write
(τ , π, z) ∼ COAGGEM(d,c)(κ) if π ∼ GEM(d, c), each
zj ∼iid π and τ is as given in Eq. (5).
Intuitively,
the zj’s deﬁne a partitioning of κ, and the elements of
each partition are subsequently summed up to obtain
the coagulated sticks τ . The coagulation operator is
the downward operation shown in Figure 2.

The reverse operation is called fragmentation. It takes
each stick τi, breaks it into an inﬁnite number of sticks,
and reorders the resulting shorter sticks by size-biased

3We say that ρ = (ρk)∞

k=1 is jointly GEM(d, c) if ρk =

bk Qk−1

l=1 (1 − bl) and bk ∼ Beta(1 − d, c + kd) for all k.

permutation. The size-biased permutation of a set
of positive numbers is obtained by iteratively picking
(without replacement) entries with probabilities pro-
portional to their sizes.

j=1 be the size-biased permutation of (˜κik)∞

To be more precise, we deﬁne a fragmentation of τ
by GEM(d, c) as follows. For each stick τi, draw
ρi ∼ GEM(d, c), deﬁne ˜κik = τiρik for all k and let κ =
(κj)∞
ik=1.
The fragmentation operation corresponds to the up-
ward operation shown in Figure 2. We also require
the fragmentation operation to return π = (πi)∞
i=1.
These sticks are directly extracted from the reversal
of the size-biased permutation, which maps each κj to
some τi. We set zj = i in this case and deﬁne πi as the
asymptotic proportion of zj’s that take the value i:

πi = lim
j→∞

1
j

j
"
l=1

I(zl = i).

(6)

We write (κ, π, z) ∼ FRAGGEM(d,c)(τ ) if κ, π and z
are as constructed above.

Theorem 2. The following statements are equivalent:
(1) κ ∼ GEM(d2, c), (τ , π, z)∼ COAGGEM(d1,c/d2)(κ);
(2) τ ∼ GEM(d1d2, c), (κ, π, z)∼ FRAGGEM(d2,−d1d2)(τ ).

The above theorem was proven by (Pitman, 1999; Ho
et al., 2006). While coagulation is important for con-
structing the collapsed model, fragmentation is impor-
tant for reinstantiating nodes in the graphical model
corresponding to contexts in test sequences that did
not occur in the training data. For instance, we might
need to perform inference in the context correspond-
ing to node G2 given only G1 and G3 in the collapsed
representation (see Section 5). This is formalized in
the following:
Corollary 1. Suppose G3|G1 ∼ PY(d1d2, 0, G1), with
stick-breaking representation G3 = #∞
i=1 τiδφi where
τ ∼ GEM(d1d2, 0) and φi ∼iid G1. Let (κ, π, z) ∼
FRAGGEM(d2,−d1d2)(τ ). Then G2 = #∞
i=1 πiδφi is a
draw from G2|G1, G3 and we can equivalently write
G3 = #∞

j=1 κjδψj where ψj = φzj .

5. Inference and Prediction

Once the collapsed graphical model representation has
been built, inference proceeds as it would for any other
hierarchical Pitman-Yor process model. In this work
we used Gibbs sampling in the Chinese restaurant
franchise representation, and refer the reader to (Teh,
2006) for further details. Figure 1(c) depicts a valid
seating arrangement for the restaurants in the Chinese
restaurant franchise representation corresponding to
each bold node in Figure 1(b).

At test time we may need to compute the predictive
probability of a symbol v given a context s that is
not in the training set. It is easy to see that the pre-
dictive probability is simply E[G[s](v)] = E[G[s
#](v)],
where s$ is the longest suﬃx of s that occurs in the
preﬁx trie and the expectations are taken over the
posterior. E[G[s
#](v)] can be estimated by averaging
over the seating arrangements of the restaurant corre-
sponding to s$. However s$ itself may not appear in the
preﬁx tree, in which case we will have to reinstantiate
the corresponding restaurant.

For concreteness, in the rest of this section we consider
s = [oca] in Figure 1(b). The longest suﬃx in the pre-
ﬁx trie is s$ = [ca], but this does not appear in the
preﬁx tree as G[ca] has been marginalized out. Thus
for predictive inference we need to reinstantiate G[ca]
(or rather, its Chinese restaurant representation). We
do this by fragmenting G[oaca]|G[a] into G[ca]|G[a] and
G[oaca]|G[ca]. Using the equivalence between Chinese
restaurant processes and stick-breaking constructions
for Pitman-Yor processes, we can translate the frag-
mentation operation in Corollary 1 into a fragmenta-
tion operation on the Chinese restaurant representa-
tion of G[oaca]|G[a] instead. This results in the pro-
cedure given in the next paragraph for reinstantiating
the G[ca]|G[a] restaurant.

Suppose there are K tables in the G[oaca]|G[a] restau-
rant, table k having nk customers. Independently for
each table k, sample a partition of nk customers in
a restaurant corresponding to a Pitman-Yor process
with discount parameter d3d4 and concentration pa-
rameter −d2d3d4. Say this results in Jk tables, with
numbers of customers being nkj, with #Jk
j=1 nkj = nk.
The nk customers in the original table are now seated
at Jk tables in the G[oaca]|G[ca] restaurant with table
j having nkj customers. Each of these tables sends a
customer to the G[ca]|G[a] restaurant; these customers
are all seated at the same table. There was one cus-
tomer in the G[a]|G[] restaurant corresponding to the
original table in G[oaca]|G[a] with nk customers. There
is still one customer in G[a]|G[] corresponding to the
new table in G[ca]|G[a], thus this restaurant’s seating
arrangement needs not be altered.

6. Experiments

We are interested in understanding the potential im-
pact of using a sequence memoizer in place of a Markov
model in general modelling contexts. To start we ex-
plore two issues: ﬁrst, whether using preﬁx trees in-
stead of preﬁx tries empirically gives the computa-
tional savings that is expected under worst-case anal-
ysis; and second, whether the predictive performance

of the sequence memoizer compares favorably to a
Markov model with similar complexity.

To provide concrete answers to these questions we turn
to n-gram language modeling. Applying the sequence
memoizer in this application domain is equivalent to
letting n → ∞ in an n-gram HPYP language model.
For this reason we will refer to the sequence memoizer
as an ∞-gram HPYP in language modeling contexts.
For comparison, we used n-gram HPYPs with ﬁnite n
as state-of-the-art baselines (Teh, 2006). The sequence
of observations used as training data will be referred
to as the training corpus and the predictive power of
the models will be measured in terms of test corpus
perplexity.

The datasets used in our experiments were an excerpt
from the New York Times (NYT) corpus and the en-
tire Associated Press (AP) corpus. The latter cor-
pus is exactly the same as that used in (Bengio et al.,
2003; Teh, 2006; Mnih & Hinton, 2009), allowing us to
compare perplexity scores against other state-of-the-
art models. The AP training corpus (with 1 million
word validation set folded in) consisted of a total of
15 million words while the AP test corpus consisted of
1 million words. The AP dataset was preprocessed
to replace low frequency words (< 5 appearances)
with a single “unknown word” symbol, resulting in
17964 unique word types. This preprocessing is semi-
adversarial for the ∞-gram model because the num-
ber of unique preﬁxes in the data is lowered, resulting
in less computational savings for using the preﬁx tree
relative to the trie. The NYT training corpus con-
sisted of approximately 13 million words and had a
150,000 word vocabulary. The NYT test corpus con-
sisted of approximately 200,000 words. In this more
realistic dataset no preprocessing was done to replace
low frequency words. For this reason we used the NYT
dataset to characterize the computational savings of
using the preﬁx tree.

We used the CRF sampler outlined in Section 5 with
the addition of Metropolis-Hastings updates for the
discount parameters (Wood & Teh, 2009). The dis-
counts in the collapsed node restaurants are products
of subsets of discount parameters making other ap-
proaches diﬃcult. We use distinct discount parame-
ters for each of the ﬁrst four levels of the trie, while
levels below use a single shared discount parameter.
Theoretically the model can use diﬀerent discounts for
every depth or node in the trie. Our choice in this
regard was somewhat arbitrary and warrants more
experimentation. The discounts were initialized to
d[0,1,2,...] = (.62, .69, .74, .80, .95, .95, . . .). We used ex-
tremely short burn-in (10 iterations) and collected only

Figure 3. Total number of nodes in the tree and number
of nodes that have to be sampled as a function of num-
ber of NYT observations. The number of nodes in the
corresponding trie scales quadratically in the number of
observations and is not shown. For reference the number
of nodes in the trie corresponding to the rightmost data
point is 8.2 × 1013.

5 samples. We found that this produced the same per-
plexity scores as a sampler using 125 burn-in iterations
and 175 samples (Teh, 2006), which indicates that the
posterior structure is simple and eﬃciently traversed
by our sampler. The CRF states were initialized such
that all customers of the same type in each restaurant
were seated at a single table. This initial conﬁguration
corresponds to interpolated Kneser-Ney (Teh, 2006).

Figure 3 shows the number of nodes in preﬁx trees
growing linearly in the corpus size. We found that
the total number of nodes in the trie indeed grows
quadratically in the corpus size. We do not show this
quadratic growth in the ﬁgure because its scale is so
extreme. Instead, the ﬁgure also shows the number of
nodes that have to be sampled in the ∞-gram model,
which also grows linearly in the corpus size, albeit at
a lower rate. In the tree CRF representation none of
the leaf nodes need to be sampled because they all
will contain a single customer sitting at a single table,
thus the number of nodes that have to be sampled
is approximately the number of internal nodes in the
preﬁx tree.

While the growth rate of the trie graphical model is
quadratic, n-gram HPYP models do not instantiate
more nodes than are necessary to represent the unique
contexts in the training corpora. Figure 4 explores
the numbers of nodes created in n-gram and ∞-gram
HPYP language models, for diﬀerently sized NYT cor-
pora and for diﬀerent values of n. The ﬁgure uncovers
two interesting fact. First, in all n-gram models the
growth in the number of nodes is intially quadratic and
then becomes linear. If the plot is extended to the right

Figure 4. Nodes in preﬁx tries and trees as a function of
n (of n-gram) and for diﬀerent NYT corpora sizes. Hori-
zontal lines are preﬁx tree node counts. Curved lines are
preﬁx trie node counts. Sizes of corpora are given in the
legend.

Figure 5. NYT test perplexity for n-gram and ∞-gram
HPYP language models given a 4 million word subset of
the NYT training corpus. The dotted line indicates the
ﬁrst n-gram model that has more nodes than the ∞-gram
model.

signiﬁcantly beyond n = 6 we observe that this linear
growth continues for a long time. This transition be-
tween quadratic and linear growth can be explained
by observing that virtually all branch points of the
trie occur above a certain depth, and below this depth
only linear paths remain. Also, at n = 5 the num-
ber of nodes in the n-gram trie is roughly the same
(greater in all but one case) as the number of nodes in
the ∞-gram.

Questions of model size automatically lead to ques-
tions of the expressive power of the models. Figure 5
compares the expressive power of the n-gram HPYP
language model against the ∞-gram model, using the
test set perplexity as a proxy. We see that the predic-
tive performance of the n-gram HPYP asymptotically
approaching that of the ∞-gram. While the perfor-
mance gain over the n-gram model is modest, and cer-
tainly goes to zero as n increases, remember that the

Figure 6. AP test perplexity vs. AP training corpus size
for the 5-gram HPYP language model vs. the ∞-gram

Table 1. Reported AP test perplexities.

Source
(Mnih & Hinton, 2009)
(Bengio et al., 2003)
4-gram Modiﬁed Kneser-Ney (Teh, 2006)
4-gram HPYP (Teh, 2006)
∞-gram (Sequence Memoizer)

Perplexity

112.1
109.0
102.4
101.9
96.9

computational cost associated with the n-gram sur-
passes the ∞-gram after n = 5. This indicates that
there is no reason, computational or statistical, for pre-
ferring n-gram models over the ∞-gram when n ≥ 5.

In the next set of experiments we switch to using the
AP corpus instead. Figure 6 shows the test perplexi-
ties of both the 5-gram HPYP and the ∞-gram ﬁt to
AP corpora of increasing size. For small training cor-
pora its performance is indistinguishable from that of
the ∞-gram. Furthermore, as the corpus size grows,
enough evidence about meaningful long contexts be-
gins to accrue to give the ∞-gram a slight relative
advantage. It bears repeating here that the AP cor-
pus is preprocessed in a way that will minimize this
advantage.

despite

the AP corpus being

semi-
Finally,
adversarially preprocessed,
the ∞-gram achieves
the best test perplexity of any language model we
know of that has been applied to the AP corpus.
Comparative results are given in Table 1. This
is somewhat surprising and worth further study.
Remember the trade-oﬀ between using the ∞-gram
vs. the n-gram HPYP: the n-gram HPYP allows for
non-zero concentrations at all
levels, whereas the
∞-gram requires all concentrations to be set to zero.
Conversely, the advantage of the ∞-gram is that
it can utilize arbitrarily long contexts whereas the

n-gram model cannot. That the ∞-gram produces
better results than the HPYP can thus be explained
by the fact that at least some long contexts in (En-
glish) language must sharpen predictive performance.
Further, the advantage of having free concentration
parameters must be minimal.

7. Discussion

The sequence memoizer is a model for sequence data
that allows one to model extremely long contextual
dependencies. Using hierarchical Bayesian sharing of
priors over conditional distributions with similar con-
texts we are able to learn such a model from a single
long sequence and still produce good generalization re-
sults. We show that the model can be eﬃciently con-
structed using suﬃx trees and coagulation operators.
We provide a sampler for the model and introduce the
fragmentation operations necessary to perform predic-
tive inference in novel contexts.

The sequence memoizer (or ∞-gram) performs well as
a language model. That it achieves the best known test
perplexity on a well studied corpus may be of interest
to a large community of natural language researchers.
Diving deeper to uncover the precise reason for the im-
provement (deeper than acknowledging that language
is non-Markov and long contexts do matter at least in
some instances) is worthy of additional research.
In
a similar respect, a direct comparison of the sequence
memoizer as a language model to existing variable or-
der Markov models might be of interest (Mochihashi
& Sumita, 2008).

In the authors’ view the language modelling success is
merely a validating consequence of the primary contri-
bution of the paper: the sequence memoizer itself. We
emphasize that there are other potential applications
for our model for instance text compression using char-
acter level sequential models (Cleary & Teahan, 1997).

References

Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C.
(2003). A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3, 1137–1155.

Cleary, J. G. & Teahan, W. J. (1997). Unbounded
length contexts for PPM. The Computer Journal,
40, 67–75.

Goodman, N. D., Mansinghka, V. K., Roy, D.,
Bonawitz, K., & Tenenbaum, J. B. (2008). Church:
a language for generative models.
In Uncertainty
and Artiﬁcial Intelligence. to appear.

Ho, M. W., James, L. F., & Lau, J. W. (2006). Coag-

ulation fragmentation laws induced by general co-
agulations of two-parameter Poisson-Dirichlet pro-
cesses. http://arxiv.org/abs/math.PR/0601608.

Ishwaran, H. & James, L. F. (2001). Gibbs sampling
methods for stick-breaking priors. Journal of Amer-
ican Statistical Association, 96 (453), 161–173.

Michie, D. (1968). Memo functions and machine learn-

ing. Nature, 218, 19–22.

Mnih, A. & Hinton, G. (2009). A scalable hierarchical
distributed language model. In Neural Information
Processing Systems 22. to appear.

Mochihashi, D. & Sumita, E. (2008). The inﬁnite
Markov model. In Advances in Neural Information
Processing Systems 20, (pp. 1017–1024).

Perman, M. (1990). Random Discrete Distributions
Derived from Subordinators. PhD thesis, Depart-
ment of Statistics, University of California at Berke-
ley.

Pitman, J. (1999). Coalescents with multiple colli-

sions. Annals of Probability, 27, 1870–1902.

Pitman, J. & Yor, M. (1997). The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25, 855–900.

Sudderth, E. B. & Jordan, M. I. (2009).

Shared
segmentation of natural scenes using dependent
pitman-yor processes. In Neural Information Pro-
cessing Systems 22. to appear.

Teh, Y. W. (2006). A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the Association for Computational Linguis-
tics, (pp. 985–992).

Teh, Y. W., Jordan, M. I., Beal, M. J., & Blei, D. M.
(2006). Hierarchical Dirichlet processes. Journal
of the American Statistical Association, 101 (476),
1566–1581.

Ukkonen, E. (1995). On-line construction of suﬃx

trees. Algorithmica, 14, 249–260.

Weiner, P. (1973). Linear pattern matching algo-
rithms. In IEEE 14th Annual Symposium on Switch-
ing and Automata Theory, (pp. 1–11).

Wood, F. & Teh, Y. W. (2009). A hierarchical
nonparametric Bayesian approach to statistical lan-
guage model domain adaptation. In Journal of Ma-
chine Learning, Workshop and Conference Proceed-
ings: Artiﬁcial Intelligence in Statistics 2009, vol-
ume 5, (pp. 607–614).

