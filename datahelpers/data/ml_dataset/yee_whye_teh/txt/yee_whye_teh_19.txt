Beam Sampling for the Inﬁnite Hidden Markov Model

Jurgen Van Gael
Yunus Saatci
Department of Engineering, University of Cambridge, Cambridge CB2 1PZ, UK

jv279@cam.ac.uk
ys267@cam.ac.uk

Yee Whye Teh
Gatsby Computational Neuroscience Unit, University College London, WC1N 3AR, UK

ywteh@gatsby.ucl.ac.uk

Zoubin Ghahramani
Department of Engineering, University of Cambridge, Cambridge CB2 1PZ, UK

zoubin@eng.cam.ac.uk

Abstract

The inﬁnite hidden Markov model is a non-
parametric extension of the widely used hid-
den Markov model. Our paper introduces
a new inference algorithm for the inﬁnite
Hidden Markov model called beam sam-
pling. Beam sampling combines slice sam-
pling, which limits the number of states con-
sidered at each time step to a ﬁnite number,
with dynamic programming, which samples
whole state trajectories eﬃciently. Our algo-
rithm typically outperforms the Gibbs sam-
pler and is more robust. We present appli-
cations of iHMM inference using the beam
sampler on changepoint detection and text
prediction problems.

1. Introduction

The hidden Markov model (HMM) (Rabiner, 1989) is
one of the most widely used models in machine learn-
ing and statistics for sequential or time series data.
The HMM consists of a hidden state sequence with
Markov dynamics, and independent observations at
each time given the corresponding state. There are
three learning related tasks associated with the HMM:
inference of the hidden state sequence, learning of the
parameters, and selection of the right model size.
Inference for the hidden state trajectory can be
performed exactly using the forward-backward algo-
rithm (Rabiner, 1989), a dynamic programming algo-
rithm with O(T K 2) computational costs where T is
the number of time steps and K number of states.

Appearing in Proceedings of the 25 th International Confer-
ence on Machine Learning, Helsinki, Finland, 2008. Copy-
right 2008 by the author(s)/owner(s).

The standard approach to learning uses the Baum-
Welch algorithm, a special instance of the EM al-
gorithm (Dempster et al., 1977) which produces (lo-
cally) maximum likelihood (ML) parameters. Such
ML learning of parameters can potentially lead to over-
ﬁtting if the model size is inappropriate for the amount
of data available. This can be partially mitigated us-
ing a more fully Bayesian learning procedure, e.g. using
variational approximations (MacKay, 1997) or Markov
chain Monte Carlo (MCMC) sampling (Scott, 2002).
Such Bayesian approaches also produce estimates of
the marginal probability of data, which can be used to
select for the appropriate model size (or to average over
model sizes if ones desires a more Bayesian analysis).
Such model selection procedures can be computation-
ally expensive since multiple HMMs of diﬀerent sizes
need to be explored.
A new twist on the problem of model selection has
emerged in recent years with the increasing popu-
larity of nonparametric Bayesian models. These are
models of inﬁnite capacity, a ﬁnite portion of which
will be used to model a ﬁnite amount of observed
data. The idea of searching/averaging over the space
of ﬁnite models is replaced with Bayesian inference
over the size of submodel used to explain data. Ex-
amples of successful applications of nonparametric
Bayesian methods include Gaussian Processes (Ras-
mussen & Williams, 2005) for regression and classiﬁ-
cation, Dirichlet Process (DP) mixture models (Es-
cobar & West, 1995; Rasmussen, 2000) for cluster-
ing heterogeneous data and density estimation, Indian
Buﬀet Processes for latent factor analysis (Griﬃths
& Ghahramani, 2006), and deﬁning distributions over
non-trivial combinatorial objects such as trees (Teh
et al., 2008).
The Inﬁnite Hidden Markov Model (iHMM), otherwise
known as the HDP-HMM, (Beal et al., 2002) is a non-

Beam Sampling for the Inﬁnite Hidden Markov Model

parametric Bayesian extension of the HMM with an
inﬁnite number of hidden states. Exact Bayesian in-
ference for the iHMM is intractable. Speciﬁcally, given
a particular setting of the parameters the forward-
backward algorithm cannot be applied since the num-
ber of states K is inﬁnite, while with the parameters
marginalized out all hidden state variables will be cou-
pled and the forward-backward algorithm cannot be
applied either. Currently the only approximate in-
ference algorithm available is Gibbs sampling, where
individual hidden state variables are resampled condi-
tioned on all other variables (Teh et al., 2006). Unfor-
tunately convergence of Gibbs sampling is notoriously
slow in the HMM setting due to the strong dependen-
cies between consecutive time steps often exhibited by
time series data (Scott, 2002).
In this paper we propose a new sampler for the iHMM
called beam sampling. Beam sampling combines two
ideas—slice sampling and dynamic programming—to
sample whole state trajectories eﬃciently. Our ap-
plication of slice sampling (Neal, 2003) is inspired
by (Walker, 2007), who used it to limit the number
of clusters considered when sampling assignment vari-
ables in DP mixtures to a ﬁnite number. We apply
slice sampling to limit to a ﬁnite number the states
considered in each time step of the iHMM, so that dy-
namic programming can be used to sample whole state
trajectories eﬃciently. We call our proposal beam
sampling due to its similarity to beam search, a heuris-
tic procedure for ﬁnding the maximum a posteriori
trajectory given observations in non-linear dynamical
systems. The underlying idea in both is to limit the
search to a small number of states so that a good tra-
jectory can be found using reasonable computational
resources. However, ours is a MCMC sampling method
with guaranteed convergence to the true posterior.
We ﬁrst present a self-contained description of the
iHMM using the Hierarchical Dirichlet process (HDP)
formalism (Teh et al., 2006) in Section 2, followed
by a discussion of Gibbs sampling in Section 3. We
introduce beam sampling in Section 4 and compare
it against Gibbs sampling on both artiﬁcial and real
datasets in Section 5. We ﬁnd that beam sampling
is (1) at least as fast if not faster than Gibbs sam-
pling; (2) more robust than Gibbs sampling as its
performance is not as dependent on initialization and
hyperparameter choice; (3) handles non-conjugacy in
the model more naturally; (4) straightforward to im-
plement. We conclude in Section 6 with a discus-
sion and suggestions for other cases in which beam
sampling might prove useful. All software is avail-
able from http://mlg.eng.cam.ac.uk/jurgen to encour-
age more widespread adoption of the iHMM and the
beam sampler.

2. The Inﬁnite Hidden Markov Model

We start this section by describing the ﬁnite HMM,
then taking the inﬁnite limit to obtain an intuition
for the inﬁnite HMM, followed by a more precise def-
inition. A ﬁnite HMM consists of a hidden state se-
quence s = (s1, s2, . . . , sT ) and a corresponding ob-
servation sequence y = (y1, y2, . . . , yT ). Each state
variable st can take on a ﬁnite number of states, say
1 . . . K. Transitions between states are governed by
Markov dynamics parameterized by the transition ma-
trix π, where πij = p(st = j|st−1 = i), while the ini-
tial state probabilities are π0i = p(s1 = i). For each
state st ∈ {1 . . . K} there is a parameter φst which
parametrizes the observation likelihood for that state:
yt|st ∼ F (φst). Given the parameters {π0, π, φ, K} of
the HMM, the joint distribution over hidden states s
and observations y can be written (with s0 = 0):

T(cid:89)

p(s, y|π0, π, φ, K) =

p(st|st−1)p(yt|st)

t=1

We complete the Bayesian description by specifying
the priors. Let the observation parameters φ be iid
drawn from a prior distribution H. With no fur-
ther prior knowledge on the state sequence, the typical
prior for the transition (and initial) probabilities are
symmetric Dirichlet distributions.
A na¨ıve way to obtain a nonparametric HMM with an
inﬁnite number of states might be to use symmetric
Dirichlet priors over the transition probabilities with
parameter α/K and take K → ∞. Such an approach
has been successfully used to derive DP mixture mod-
els (Rasmussen, 2000) but unfortunately does not work
in the HMM context. The subtle reason is that there
is no coupling across transitions out of diﬀerent states
since the transition probabilities are given indepen-
dent priors (Beal et al., 2002). To introduce coupling
across transitions, one may use a hierarchical Bayesian
formalism where the Dirichlet priors have shared pa-
rameters and given a higher level prior, e.g.

πk ∼ Dirichlet (αβ) ,
β ∼ Dirichlet (γ/K . . . γ/K)

(1)

where πk are transition probabilities out of state k and
β are the shared prior parameters. As K → ∞, the hi-
erarchical prior (1) approaches (with some alterations)
a hierarchical Dirichlet process (Teh et al., 2006).
A hierarchical Dirichlet process (HDP) is a set of
Dirichlet processes (DPs) coupled through a shared
random base measure which is itself drawn from a
Speciﬁcally, each Gk ∼
DP (Teh et al., 2006).
DP(α, G0) with shared base measure G0, which can
be understood as the mean of Gk, and concentration
parameter α > 0, which governs variability around G0,

Beam Sampling for the Inﬁnite Hidden Markov Model

factor

is

the

In order to resample st, we need to compute the prob-
ability p(st|s−t, β, y, α, H) ∝ p(yt|st, s−t, y−t, H) ·
p(st|s−t, β, α).
con-
given s, y and H:
ditional

(cid:82) p(yt|st, φst)p(φst|s−t, y−t, H)dφst. This is easy to

likelihood of yt

The ﬁrst

compute when the base distribution H and likelihood
F from equations (2) and (3) are conjugate. For
the second factor we can use the fact that the hid-
den state sequence is Markov. Let nij be the number
of transitions from state i to state j excluding time
steps t − 1 and t. Let n·i, ni· be the number of tran-
sitions in and out of state i. Finally, let K be the
number of distinct states in s−t. Then we have that1
p(st = k|s−t, β, α) ∝

(nst−1,k + αβk) nk,st+1 +αβst+1
(nst−1,k + αβk) nk,st+1 +1+αβst+1
(nst−1,k + αβk) nk,st+1 +αβst+1

nk·+1+α

nk·+α

nk·+1+α

αβkβst+1

if k ≤ K, k (cid:54)= st−1
if k = st−1 = st+1
if k = st−1 (cid:54)= st+1
if k = K + 1.

For each 1 ≤ t ≤ T we need to compute O(K)
probabilities, hence the Gibbs sampler has an O(T K)
computational complexity. Non-conjugate models can
be handled using more sophisticated sampling tech-
niques. In our experiments below, we used algorithm
8 from (Neal, 2000).
The Gibbs sampler’s success is due to its straightfor-
ward implementation. However, it suﬀers from one
major drawback: sequential and time series data are
likely to be strongly correlated. For example, if we
know the value of a stock at time t then we can be
reasonably sure that it will be similar at time t+1. As
is well known, this is a situation which is far from ideal
for the Gibbs sampler: strong correlations in the hid-
den states will make it unlikely that individual updates
to st can cause large blocks within s to be changed.
We will now introduce the beam sampler which does
not suﬀer from this slow mixing behavior by sampling
the whole sequence s in one go.

4. The Beam Sampler

The forward-backward algorithm does not apply to
the iHMM because the number of states, and hence
the number of potential state trajectories, are inﬁnite.
The idea of beam sampling is to introduce auxiliary
variables u such that conditioned on u the number
of trajectories with positive probability is ﬁnite. Now
dynamic programming can be used to compute the
conditional probabilities of each of these trajectories
and thus sample whole trajectories eﬃciently. These

1Recall that we ignored the ordering of states in β. In
this representation the K distinct states in s are labeled
1 . . . K and K + 1 denotes a new state.

Figure 1. iHMM Graphical Model

can be expressed as follows: G0 =(cid:80)∞
Gk =(cid:80)∞

with small α implying greater variability. The shared
base measure is itself given a DP prior: G0 ∼ DP(γ, H)
with H a global base measure. The stick-breaking con-
struction for HDPs shows that the random measures
k(cid:48)=1 βk(cid:48)δφk(cid:48) and
k(cid:48)=1 πkk(cid:48)δφk(cid:48) , where β ∼ GEM(γ) is the stick-
breaking construction for DPs (Sethuraman, 1994),
πk ∼ DP(α, β), and each φk(cid:48) ∼ H independently.
Identifying each Gk as describing both the transition
from state k to k(cid:48) and the emis-
probabilities πkk(cid:48)
sion distributions parametrized by φk(cid:48), we can now
formally deﬁne the iHMM as follows:
φk ∼ H, (2)
β ∼ GEM(γ),
πk|β ∼ DP(α, β),
yt|st ∼ F (φst). (3)
st|st−1 ∼ Multinomial(πst−1),
The graphical model corresponding to this hierarchical
model is shown in ﬁgure 1. Thus βk(cid:48) is the prior mean
for transition probabilities leading into state k(cid:48), and α
governs the variability around the prior mean. If we ﬁx
β = ( 1
K , 0, 0 . . .) where the ﬁrst K entries are 1
K
and the remaining are 0, then transition probabilities
into state k(cid:48) will be non-zero only if k(cid:48) ∈ {1 . . . K}, and
we recover the Bayesian HMM of (MacKay, 1997).
Finally we place priors over the hyperparameters α
and γ. A common solution, when we do not have
strong beliefs about the hyperparameters, is to use
gamma hyperpriors: α ∼ Gamma(aα, bα) and γ ∼
Gamma(aγ, bγ). (Teh et al., 2006) describe how these
hyperparameters can be sampled eﬃciently, and we
will use this in the experiments to follow.

K . . . 1

3. The Gibbs Sampler

The Gibbs sampler was the ﬁrst sampling algorithm
for the iHMM that converges to the true posterior.
One proposal builds on the direct assignment sampling
scheme for the HDP in (Teh et al., 2006) by marginal-
izing out the hidden variables π, φ from (2), (3) and
ignoring the ordering of states implicit in β. Thus we
only need to sample the hidden trajectory s, the base
DP parameters β and the hyperparameters α, γ. Sam-
pling β, α, γ is exactly the same as for the HDP so we
refer to (Teh et al., 2006) for details.

Beam Sampling for the Inﬁnite Hidden Markov Model

ditional conditioning variables π and φ for clarity):

Figure 2. The auxiliary variable u partitions the probabil-
ity distribution π (vertical bars) into a set of entries less
than u and a set of entries larger than u.

auxiliary variables do not change the marginal distri-
bution over other variables hence MCMC sampling will
converge to the true posterior. This idea of using aux-
iliary variables to limit computation costs is inspired
by (Walker, 2007), who applied it to limit the number
of components in a DP mixture model that need be
considered during sampling.
As opposed to the sampler in the previous section,
the beam sampler does not marginalize out π nor φ.
Speciﬁcally, the beam sampler iteratively samples the
auxiliary variables u, the trajectory s, the transition
probabilities π, the shared DP parameters β and the
hyperparameters α and γ conditioned on all other vari-
ables. In the following, we shall describe in more detail
how to sample each set of variables, as well as how the
auxiliary variables allow dynamic programming to be
carried out over a ﬁnite number of trajectories without
approximations.
for each t we introduce an auxil-
Sampling u:
iary variable ut with conditional distribution ut ∼
Uniform(0, πst−1st) depending on π, st−1 and st.
Sampling s: we sample the whole trajectory s given
the auxiliary variables u and other variables using a
form of forward ﬁltering-backward sampling. The im-
portant observation here is that only trajectories s
with πst−1st ≥ ut for all t will have non-zero probabil-
ity given u. There are only ﬁnitely many such trajec-
tories2 and as a result we can compute the conditional
distribution over all such trajectories eﬃciently using
dynamic programming.
is
First note that the probability density for ut
p(ut|st−1, st, π) =
, where I(C) = 1
if condition C is true and 0 otherwise. We compute
p(st|y1:t, u1:t) for all t as follows (we omitted the ad-
2To see this, note that ut > 0 with probability 1 for each
t, since each πkk(cid:48) > 0 with probability 1. Given the auxil-
iary variable ut, note further that for each possible value of
st−1, ut partitions the set of transition probabilities out of
state st−1 into two sets: a ﬁnite set with πst−1k > ut and
an inﬁnite set with πst−1k < ut, as illustrated in ﬁgure 2.
Thus we can recursively show that for t = 1, 2 . . . T the set
of trajectories s1:t with all πst(cid:48)−1st(cid:48) > ut is ﬁnite.

I(0<ut<πst−1,st )

πst−1,st

p(st|y1:t, u1:t)
∝p(st, ut, yt|y1:t−1, u1:t−1),

p(yt|st)p(ut|st, st−1)p(st|st−1)
p(st−1|y1:t−1, u1:t−1),

st−1

=(cid:88)
=p(yt|st)(cid:88)
=p(yt|st) (cid:88)

st−1

I(ut < πst−1,st)p(st−1|y1:t−1, u1:t−1),

p(st−1|y1:t−1, u1:t−1).

(4)

st−1:ut<πst−1,st

Note that we only need to compute (4) for the ﬁnitely
many st values belonging to some trajectory with
positive probability. Further, although the sum over
st−1 is technically a sum over an inﬁnite number of
terms, the auxiliary variable ut truncates this summa-
tion to the ﬁnitely many st−1’s that satisfy both con-
straints πst−1,st > ut and p(st−1|y1:t−1, u1:t−1) > 0.
Finally, to sample the whole trajectory s, we sam-
ple sT from p(sT|y1:T , u1:T ) and perform a backward
pass where we sample st given the sample for st+1:
p(st|st+1, y1:T , u1:T ) ∝ p(st|y1:t, u1:t)p(st+1|st, ut+1).

these follow directly from the
Sampling π, φ, β:
theory of HDPs (Teh et al., 2006), but we brieﬂy de-
scribe these for completeness.
Let nij be the number of times state i transi-
tions to state j in the trajectory s, where i, j ∈
{1 . . . K}, K is the number of distinct states in s,
and these states have been relabeled 1 . . . K. Merg-
ing the inﬁnitely many states not represented in
the conditional distribution of
s into one state,
k(cid:48)=K+1 πkk(cid:48)) given its Markov blanket

(πk1 . . . πkK,(cid:80)∞
Dirichlet(cid:0)nk1 + αβ1 . . . nkK + αβK, α(cid:80)∞

s, β, α is

(cid:1) ,

i=K+1 βi

To sample β we introduce a further set of auxiliary
variables mij which are independent with conditional
distributions

p(mij = m|s, β, α) ∝ S(nij, m)(αβj)m,

has conditional distribution

where S(·,·) denotes Stirling numbers of the ﬁrst kind.
k(cid:48)=K+1 βk(cid:48))

The shared DP parameter (β1 . . . , βK,(cid:80)∞
where m·k =(cid:80)K

k(cid:48)=1 mk(cid:48)k. (Teh et al., 2006; Antoniak,

Dirichlet (m·1 . . . m·K, γ) ,

1974) gives more details.
Finally, each φk is independent of others conditional on
s, y and their prior distribution H, i.e. p(φ|s, y, H) =

Beam Sampling for the Inﬁnite Hidden Markov Model

Figure 3. iHMM performance on strong negatively corre-
lated data. The top plot shows the error of the Gibbs and
beam sampler for the ﬁrst 1500 iterations averaged over
20 runs. The bottom plot shows the average number of
previous states considered in equation (4) for the ﬁrst 100
iterations of the beam sampler.

(cid:81)
k p(φk|s, y, H). When the base distribution H is
conjugate to the data distribution F each φk can
be sampled eﬃciently. Otherwise we may resort to
Metropolis-Hastings or other approaches. Note that in
the non-conjugate case this is simpler than for Gibbs
sampling. In the experimental section, we describe an
application where the base distribution and likelihood
are non-conjugate.
To conclude our discussion of the beam sampler, it
is useful to point out that there is nothing special
about sampling ut from the uniform distribution on
[0, πst−1,st]: by choosing a distribution over [0, πst,st−1]
with higher mass near smaller values of ut, we will al-
low more trajectories to have positive probability and
hence considered by the forward ﬁltering-backward
sampling algorithm. Although this will typically im-
prove mixing time, it also comes at additional compu-
tational cost. This brings us to the issue of the com-
putational cost of the beam sampler: since for each
timestep and each state assignment we need to sum
over all represented previous states, the worst case
complexity is O(T K 2). However, the sum in (4) is only
over previous states for which the transition probabil-
ity is larger than ut; this means that in practice we
might only need to sum over a few previous states.
In our experiments below, we will give some empirical
evidence for this “average case” behavior. Further, we
have found that the drastically improved mixing of the
beam sampler more than made up for the additional
cost over Gibbs sampling. Finally, although we did not
ﬁnd any advantage doing so, it is certainly possible to
interleave the beam sampler and the Gibbs sampler.

Figure 4. iHMM error on increasing positively correlated
data. The blue curve shows the beam sampler while the red
curve shows the Gibbs sampler performance. The dotted
line show the one standard deviation error bars.

5. Experiments

We evaluate the beam sampler on two artiﬁcial and
two real datasets to illustrate the following properties:
(1) the beam sampler mixes in much fewer iterations
than the Gibbs sampler; (2) the actual complexity per
iteration of the beam sampler is only marginally more
than the Gibbs sampler; (3) the beam sampler mixes
well regardless of strong correlations in the data; (4)
the beam sampler is more robust with respect to vary-
ing initialization and prior distribution; (5) the beam
sampler handles non conjugate models naturally; (6)
the iHMM is a viable alternative to the ﬁnite HMM.
All datasets and a Matlab version of our software are
available at http://mlg.eng.cam.ac.uk/jurgen.

5.1. Artiﬁcial Data
Our ﬁrst experiment compares the performance of the
iHMM on a sequence of length 800 generated by a 4
state HMM. The hidden state sequence was almost
cyclic (1-2-3-4-1-2-3-. . . ) with a 1% probability of self
transition: i.o.w the true distribution of hidden states
is strong negatively correlated. We use a multinomial
output distribution with the following emission matrix

 0.0

0.5

0.6666

0.1666

0.1666

0.3333

0.3333

0.3333

 .

0.5

0.0

0.5

0.5

Next we run the Gibbs and beam sampler 20 times
from a random initialization with every state randomly
chosen between 1 and 20. We test the performance
of both samplers using three diﬀerent hyperparame-
ter settings: (1) vague gamma hyperpriors for α and

05001000150000.20.40.60.81Iterationsp(Error)  Gibbs VagueGibbs StrongGibbs FixedBeam VagueBeam StrongBeam Fixed0204060801000510Iterations# transitions  Beam VagueBeam StrongBeam Fixed010020000.250.50.7510.750010020000.250.50.7510.950010020000.250.50.7510.999010020000.250.50.7510.750010020000.250.50.7510.950010020000.250.50.7510.999Beam Sampling for the Inﬁnite Hidden Markov Model

Figure 5. The 40’th sample of the beam sampler with every state represented by a diﬀerent color on the well-log dataset.

γ (Gamma(1, 1) and Gamma(2, 1) respectively); (2)
strong gamma hyperpriors for α and γ (Gamma(6, 15)
and Gamma(16, 4) respectively); (3) ﬁxed hyperparam-
eters α = 0.4, γ = 3.8. The latter were chosen using
the values the beam and Gibbs samplers converged to.
At every iteration, we greedily compute an assignment
of sample states to true states to maximize overlap and
use the resulting Hamming distance as our error mea-
sure. The top plot in ﬁgure 3 clearly shows that the
beam sampler discovers the underlying structure much
faster than the Gibbs sampler. Also, the beam sam-
pler is insensitive to the prior while the performance
of the Gibbs sampler becomes worse as we strengthen
our prior beliefs. The bottom plot of ﬁgure 3 shows
how many states are summed over in equation (4) av-
eraged per timestep, per state. We ﬁnd that after only
about 20 iterations, the beam sampler on average con-
siders a little more than one state. This implies that
the actual complexity of the beam sampler is closer
to O(T K) rather than the worst case complexity of
O(T K 2). Although this behavior is dependent on the
choice of distribution for the auxiliary variable ut and
the sparsity of the transition matrix, we have veriﬁed
that this behavior is consistent also for larger iHMM’s.
Our second experiment illustrates the performance of
the beam sampler on data generated from HMM’s
with increasing positive correlation between the hid-
den states. We generated sequences of length 4000
from a 4 state HMM with self-transition probabilities
increasing from 0.75 to 0.95 and ﬁnally 0.999. In one
experiment (top plot of ﬁgure 4) we generated nor-
mal distributed observation from an informative out-
put model with means −2.0, 4.0, 1.0,−0.5 and stan-
dard deviation 0.5,
in another experiment (bottom
plot of ﬁgure 4) we generated normal distributed ob-
servations from a less informative output model with
means −1.0, 0.5,−0.5, 0.0 and standard deviation 0.5.
We initialize the experiment as above and set the base
distribution for the state means to be a 0 mean normal
with 2.0 standard deviation. Then, we greedily com-
pute the error compared to ground truth and average
the results over 60 diﬀerent random starting positions.
The top row shows that with an informative prior,
both the Gibbs and beam sampler can reduce the ini-

tial error by at least 50% independent of the correla-
tion between hidden states. When the output model
is less informative however and there is little corre-
lation between the hidden states, the learning prob-
lem is hardest: the lower left plot shows that both
the beam and Gibbs sampler discover structure only
slowly. When the correlation increases, the learning
problem should become easier. However, as the lower
right plot shows, although the beam sampler mixes in-
creasingly well, the Gibbs sampler suﬀers from slow
random walk behavior.

5.2. Well Data
The next experiment illustrates the performance of
the iHMM on a changepoint detection problem. The
data consists of 4050 noisy measurements of nuclear-
response of rock strata obtained via lowering a probe
through a bore-hole. Figure 5 illustrates this datasets.
The data has been previously analyzed in (Ruanaidh
& Fitzgerald, 1996) by eliminating the forty great-
est outliers and running a changepoint detection algo-
rithm with a ﬁxed number of changepoints. This ap-
proach works well as this one-dimensional dataset can
be inspected visually to make a decision on whether
to throw away datapoints and get a rough idea for
the number of changepoints. However, we believe that
with a nonparametric model, we can automatically
adapt the number of changepoints. Moreover, by set-
ting up a noise model with fat tails, we hope to auto-
matically handle the outlier problem.
We model the mean of the nuclear-response for every
segment. First we normalize the data to have zero
mean; then we specify a zero mean normal distribu-
tion for the base distribution H. We choose the vari-
ance of this normal to be the empirical variance of the
dataset. For the output model, we let F correspond
to a Student-t distribution with ν = 1, also known
as the Cauchy distribution. We set the scale parame-
ter for the Cauchy distribution to twice the empirical
standard deviation for the dataset. Since the Cauchy
likelihood is not conjugate with respect to the nor-
mal base distribution, we modiﬁed the Gibbs sampler
based on algorithm 8 in (Neal, 2000). We use the aux-

5001000150020002500300035004000−4−202x 104NMR ResponseMeasurement #Beam Sampling for the Inﬁnite Hidden Markov Model

5.3. Alice in Wonderland
Another application domain for HMMs is the area of
text prediction. One such task is that of predicting
sequences of letters in text taken from Alice’s Adven-
tures in Wonderland. We compare the performance of
a ﬁnite HMM trained using variational Bayes (as de-
scribed in (MacKay, 1997)) with two iHMMs trained
using beam sampling and Gibbs sampling. Both sam-
plers had a burn-in of 1000 iterations and an additional
10000 iterations to collect 50 samples of hidden state
sequences from the posterior (i.e. we sample every 200
iterations).
The training data for each HMM (whether ﬁnite or
inﬁnite) was taken to be a single sequence of 1000
characters from the ﬁrst chapter of the book. There
were 31 diﬀerent observation symbols (26 letters ignor-
ing case plus space and basic punctuation characters).
The test data was taken to be the subsequent 4000
characters from the same chapter. For all ﬁnite HMMs
we analyzed performance on models with the number
of hidden states ranging from 1 to 50. For VB, we
note that the true predictive distribution is intractable
to compute. Therefore, we used the posterior param-
eter distributions to sample 50 candidate parameter
settings, and used these to compute an approximate
predictive log-likelihood. For the iHMMs, we sam-
pled 50 hidden state sequences from the stationary
distribution after convergence and used these samples
to compute an approximate predictive log-likelihood.
For the VB-HMM we set the prior pseudo-counts for
the transition matrix to 4/K across all states and
the prior pseudo-counts for the emission matrix to 0.3
across all symbols. Accordingly, we set the hyperprior
for the iHMMs such that aα = 4 and bα = 1 and
H ∼ Dirichlet (() 0.3,··· 0.3). The results for VB and
the iHMMs were averaged over 50 and 20 independent

Figure 7. Comparing VB-HMM with the iHMM.

Figure 6. The left plots show how frequent two datapoints
were in the same cluster averaged over the ﬁrst 5 samples.
The right plots show how frequently two datapoints were
in the same cluster averaged over the last 30 samples.

iliary variable sampling scheme discussed in (Gelman
et al., 2004) to resample the segment means.
Figure 5 shows the results of one sample from the beam
sampler: the iHMM segments the dataset reasonably
well and robustly handles the outliers. To compare the
Gibbs and beam samplers, we compute 50 samples af-
ter a burnin of 5000 iterations with 1000 iterations in
between each sample. For every pair of datapoints we
compute the probability that they are in the same seg-
ment, averaged over the ﬁrst ﬁve samples (left plots in
ﬁgure 6) and the last thirty samples (right plots in
ﬁgure 6). First, note that after the ﬁrst 10000 itera-
tions, the Gibbs sampler hasn’t discovered any struc-
ture while the beam sampler has. This supports our
claim that the beam sampler mixes faster than the
Gibbs sampler. Moreover, we expect that the Gibbs
sampler will have trouble to reassign the state assign-
ment for whole segments because of slow random walk
behavior. The beam sampler on the other hand re-
samples whole hidden state sequences and should be
able to reassign whole segments more easily. The right
plots of ﬁgure 6 conﬁrm our expectation: a careful in-
spection of both plots shows that the Gibbs sampler
is visually more black-white indicating that either two
datapoints are always in the same cluster or never in
the same cluster; the beam sampler, on the other hand,
has gray areas which indicate that it averages over dif-
ferent assignments of the segments: e.g. the Gibbs plot
(upper right) suggests that the leftmost segment and
rightmost segment are always in the same state, while
the beam sampler plot (bottom right) indicates that
only part of the time, the left and rightmost segments
are in the same state (90% of the time).

1020304050−1.16−1.14−1.12−1.1−1.08−1.06x 104Number of hidden states (K)Predictive Log−likelihood  iHMMsVB−HMMBeam Sampling for the Inﬁnite Hidden Markov Model

runs respectively. The plot includes error bars corre-
sponding to 2 standard deviations.
Figure 7 illustrates the estimated predictive log-
likelihoods for the ﬁnite VB-HMM and the two iHMMs
trained using beam and Gibbs sampling. We ﬁnd that
the iHMMs have superior predictive power when com-
pared to the VB-HMM, even when we select the best
number of hidden states (around K = 16). Both the
iHMMs converged to a posterior distribution over hid-
den state sequences with around 16 states, showing
that nonparametric Bayesian techniques are an eﬀec-
tive way to handle model selection. The ﬁnal perfor-
mance of the Gibbs and beam sampler were not found
to be signiﬁcantly diﬀerent as we set the number of
iterations high enough to ensure that both algorithms
converge. Indeed, the aim of this experiment is not to
compare the performance of individuals iHMM sam-
pling schemes, rather, it is to further illustrate the rel-
ative eﬀectiveness of using models of inﬁnite capacity.

6. Conclusion
In this paper we introduced the beam sampler, a new
inference algorithm for the iHMM that draws inspi-
ration from slice sampling and dynamic programming
to sample whole hidden state trajectories eﬃciently.
We showed that the beam sampler is a more robust
sampling algorithm than the Gibbs sampler. We be-
lieve that the beam sampler is the algorithm of choice
for iHMM inference because it converges faster than
the Gibbs sampler and is straightforward to imple-
ment. Moreover, it conveniently allows us to learn
non-conjugate models. To encourage adoption of the
iHMM as an alternative to HMM learning, we have
made the software and datasets used in this paper
available at http://mlg.eng.cam.ac.uk/jurgen.
The beam sampler idea is ﬂexible enough to do in-
ference for various extensions of the iHMM: our cur-
rent work involves an adaptation of the beam sampler
to an extension of the iHMM that handles inputs, ef-
fectively resulting in a nonparametric generalization
of the input-output HMM (Bengio & Frasconi, 1995).
We believe this is a promising model for nonparamet-
ric Bayesian learning of POMDPs. Another project
currently underway is to use the beam sampler for ef-
ﬁciently learning ﬁnite, but very large hidden Markov
models. Finally, we are exploring the possibilities of
using the embedded HMM construction (Neal et al.,
2004) as an alternative for the beam sampler for eﬃ-
cient inference in the iHMM.

Acknowledgements
We would like to thank the anonymous reviewers for their
helpful comments. JVG is supported by a Microsoft Re-
search PhD scholarship; ZG is also in the Machine Learning
Department, CMU.

References

Antoniak, C. E. (1974). Mixtures of dirichlet processes
with applications to bayesian nonparametric problems.
The Annals of Statistics, 2, 1152–1174.

Beal, M. J., Ghahramani, Z., & Rasmussen, C. E. (2002).

The inﬁnite hidden markov model. NIPS, 14.

Bengio, Y., & Frasconi, P. (1995). An input output hmm

architecture. NIPS, 7.

Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977).
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39, 1–38.

Escobar, M. D., & West, M. (1995). Bayesian density es-
timation and inference using mixtures. Journal of the
American Statistical Association, 90, 577–588.

Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B.
(2004). Bayesian data analysis. CRC Press. 2rev ed
edition.

Griﬃths, T. L., & Ghahramani, Z. (2006). Inﬁnite latent
feature models and the indian buﬀet process. NIPS, 18.

MacKay, D. J. C. (1997). Ensemble learning for hidden
markov models. Technical report, Cavendish Laboratory,
University of Cambridge, 1997.

Neal, R. M. (2000). Markov chain sampling methods for
dirichlet process mixture models. Journal of Computa-
tional and Graphical Statistics, 9, 249–265.

Neal, R. M. (2003). Slice sampling. The Annals of Statis-

tics, 31, 705–741.

Neal, R. M., Beal, M. J., & Roweis, S. T. (2004). Inferring
state sequences for non-linear systems with embedded
hidden markov models. NIPS, 16.

Rabiner, L. R. (1989). A tutorial on hidden markov models
and selected applications inspeech recognition. Proceed-
ings of the IEEE, 77, 257–286.

Rasmussen, C. E. (2000). The inﬁnite gaussian mixture

model. NIPS, 12.

Rasmussen, C. E., & Williams, C. K. I. (2005). Gaussian

processes for machine learning. The MIT Press.

Ruanaidh, J., & Fitzgerald, W. J. (1996). Numerical
bayesian methods applied to signal processing. Springer-
Verlag New York Inc.

Scott, S. L. (2002). Bayesian methods for hidden Markov
models: Recursive computing in the 21st century. Jour-
nal of the American Statistical Association, 97, 337–351.

Sethuraman, J. (1994). A constructive deﬁnition of dirich-

let priors. Statistica Sinica, 4, 639–650.

Teh, Y. W., III, H. D., & Roy, D. (2008). Bayesian ag-

glomerative clustering with coalescents. NIPS, 20.

Teh, Y. W., Jordan, M. I., Beal, M. J., & Blei, D. M.
(2006). Hierarchical dirichlet processes. Journal of the
American Statistical Association, 101, 1566–1581.

Walker, S. G. (2007). Sampling the dirichlet mixture model
with slices. Communications in Statistics - Simulation
and Computation, 36, 45.

