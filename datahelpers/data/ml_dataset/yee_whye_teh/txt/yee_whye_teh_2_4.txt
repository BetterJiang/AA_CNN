3
1
0
2

 
t
c
O
2

 

 
 
]
E
M

.
t
a
t
s
[
 
 

1
v
5
9
5
0

.

0
1
3
1
:
v
i
X
r
a

Statistical Science
2013, Vol. 28, No. 3, 335–359
DOI: 10.1214/13-STS422
c(cid:13) Institute of Mathematical Statistics, 2013

MCMC for Normalized Random Measure
Mixture Models
Stefano Favaro1 and Yee Whye Teh

Abstract. This paper concerns the use of Markov chain Monte Carlo
methods for posterior sampling in Bayesian nonparametric mixture
models with normalized random measure priors. Making use of some
recent posterior characterizations for the class of normalized random
measures, we propose novel Markov chain Monte Carlo methods of both
marginal type and conditional type. The proposed marginal samplers
are generalizations of Neal’s well-regarded Algorithm 8 for Dirichlet
process mixture models, whereas the conditional sampler is a variation
of those recently introduced in the literature. For both the marginal
and conditional methods, we consider as a running example a mix-
ture model with an underlying normalized generalized Gamma process
prior, and describe comparative simulation results demonstrating the
eﬃcacies of the proposed methods.

Key words and phrases: Bayesian nonparametrics, hierarchical mix-
ture model, completely random measure, normalized random measure,
Dirichlet process, normalized generalized Gamma process, MCMC pos-
terior sampling method, marginalized sampler, Algorithm 8, condi-
tional sampler, slice sampling.

1. INTRODUCTION

Mixture models provide a statistical framework
for modeling data where each observation is assumed
to have arisen from one of k groups, with k possibly
unknown, and each group being suitably modeled by
a distribution function from some parametric fam-
ily. The distribution function of each group is re-

Stefano Favaro is Assistant Professor of Statistics,
Department of Economics and Statistics, University of
Torino, C.so Unione Sovietica 218/bis, 10134 Torino,
Italy e-mail: stefano.favaro@unito.it. Yee Whye Teh is
Professor of Statistical Machine Learning, Department
of Statistics, University of Oxford, 1 South Parks Road,
Oxford OX13TG, United Kingdom e-mail:
y.w.teh@stats.ox.ac.uk.

1Also aﬃliated with Collegio Carlo Alberto, Moncalieri,

Italy.

This is an electronic reprint of the original article
published by the Institute of Mathematical Statistics in
Statistical Science, 2013, Vol. 28, No. 3, 335–359. This
reprint diﬀers from the original in pagination and
typographic detail.

ferred to as a component of the mixture model and
is weighted by the relative frequency of the group in
the population. Speciﬁcally, assuming k being ﬁxed,
a collection of observations (Y1, . . . , Yn) is modeled
as independent draws from a mixture distribution
function with k components, that is,

(1.1)

Yi

ind∼

k

Xj=1

˜Jjf (·| ˜Xj),

where f (·| ˜X) is a given parametric family of dis-
tribution functions indexed by a parameter ˜X and
( ˜J1, . . . , ˜Jk) are the mixture proportions constrained
to be nonnegative and sum to unity. A convenient
formulation of the mixture model (1.1) can be stated
in terms of latent allocation random variables, namely,
each observation Yi is assumed to arise from a spe-
ciﬁc but unknown component Zi of the mixture model.
Accordingly, an augmented version of (1.1) can be
written in terms of a collection of latent random
variables (Z1, . . . , Zn), independent and identically
distributed with probability mass function P[Zi =

1

2

S. FAVARO AND Y. W. TEH

j] = ˜Jj , such that the observations are modeled as

(1.2)

Yi|Zi

ind∼ f (·| ˜XZi).

Integrating out the random variables (Z1, . . . , Zn)
then yields (1.1). In a Bayesian setting the formula-
tion of the mixture model (1.2) is completed by spec-
ifying suitable prior distributions for the unknown
quantities that are objects of the inferential analysis:
the parameter ( ˜X1, . . . , ˜Xk) and the vector of pro-
portions ( ˜J1, . . . , ˜Jk). We refer to the monographs
by Titterington et al. [83] and McLachlan and Bas-
ford [55] for accounts on mixture models with a ﬁxed
number of components. Markov chain Monte Carlo
(MCMC) methods for Bayesian analysis of mixture
models with a ﬁxed number of components was pre-
sented in Dielbot and Robert [10].

As regards the general case where the number
of components is unknown, a direct approach has
been considered in Richardson and Green [79], who
modeled the unknown k by mixing over the ﬁxed
k case, and made a fully Bayesian inference using
the reversible jump MCMC methods proposed in
Green [24]. See also Stephens [82] and references
therein for some developments on such an approach,
whereas diﬀerent proposals can be found in the pa-
pers by Mengersen and Roberts [57], Raftery [74]
and Roeder and Wasserman [81]. An early and fruit-
ful approach, still in the context of mixture models
with an unknown number k of components, was pro-
posed in Escobar [11] who treated the problem in a
Bayesian nonparametric setting by means of a prior
distribution based on the Dirichlet process (DP) of
Ferguson [16]. This approach arises as a major de-
velopment of some earlier results in Lo [51] and it is
nowadays the subject of a rich and active literature.
In this paper we deal with mixture models with an
unknown number of components. In particular, we
focus on a Bayesian nonparametric approach with
the speciﬁcation of a class of prior distributions gen-
eralizing the DP prior. In the Bayesian nonparamet-
ric setting the central role is played by a discrete
random probability measure ˜µ deﬁned on a suit-
able measurable space X, an example being the DP,
whose distribution acts as a nonparametric prior.
The basic idea is that since ˜µ is discrete, it can be
written as

˜Jjδ ˜Xj

,

˜µ =Xj≥1

where ( ˜Jj)j≥1 is a sequence of nonnegative random
weights that add up to one and ( ˜Xj)j≥1 is a se-
quence of X-valued random locations independent

of ( ˜Jj)j≥1. Given ˜µ and a collection of continuous
observations (Y1, . . . , Yn), a Bayesian nonparametric
mixture model admits a hierarchical speciﬁcation in
terms of a collection of independent and identically
distributed latent random variables (X1, . . . , Xn).
Formally,

(1.3)

Yi|Xi
Xi|˜µ

ind∼ F (·|Xi),
i.i.d.∼ ˜µ,
˜µ ∼ P,

where P denotes the nonparametric prior distribu-
tion and F (·|Xi) is a probability distribution param-
eterized by the random variable Xi and admitting
a distribution function f (·|Xi). Note that, due to
the discreteness of ˜µ, each random variable Xi will
take on value ˜Xj with probability ˜Jj for each j ≥ 1,
and the hierarchical model (1.3) is equivalent to say-
ing that observations (Y1, . . . , Yn) are independent
and identically distributed according to a probabil-
ity distribution F with random distribution function

(1.4)

f (·) =ZX

f (·|x)˜µ(dx) =Xj≥1

˜Jjf (·| ˜Xj).

This is a mixture of distribution functions with a
countably inﬁnite number of components. The prob-
ability distribution F (·|Xi) is termed the mixture
kernel, whereas the underlying distribution P is
termed the mixing distribution or, alternatively, the
mixing measure. Note that, since ˜µ is discrete, each
pair of the latent random variables (X1, . . . , Xn) will
take on the same value with positive probability,
with this value corresponding to a component of
the mixture model. In this way, the latent random
variables allocate the observations (Y1, . . . , Yn) to a
random number of components, thus naturally pro-
viding a model for the unknown number of compo-
nents. Under the assumption of ˜µ being a Dirichlet
process, the model (1.4) was introduced by Lo [51]
and it is known in Bayesian nonparametrics as the
DP mixture model.

The reason of the success of the Bayesian nonpara-
metric approach in the analysis of mixture models,
as pointed out in the paper by Green and Richard-
son [25], is that it exploits the discreteness of ˜µ, thus
providing a ﬂexible model for clustering items of var-
ious kinds in a hierarchical setting without explic-
itly specifying the number of components. Bayesian
nonparametrics is now the subject of a rich and
active literature spanning applied probability, com-
putational statistics and machine learning. Beyond

MCMC FOR NORMALIZED RANDOM MEASURE MIXTURE MODELS

3

mixture analysis, Bayesian nonparametrics has been
applied to survival analysis by Hjort [29], to feature
allocation models by Griﬃths and Ghahramani [28]
and Broderick et al. [6] and to regression (see the
monograph by Rasmussen and Williams [77]), among
others. The reader is referred to the comprehensive
monograph edited by Hjort et al. [30] for a collec-
tion of reviews on recent developments in Bayesian
nonparametrics.

Several MCMC methods have been proposed for
posterior sampling from the DP mixture model. Early
works exploited the tractable marginalization of ˜µ
with respect to the DP mixing distribution, thus
removing the inﬁnite-dimensional aspect of the in-
ferential problem. The main references in this re-
search area are represented by the sampling meth-
ods originally devised in Escobar [11, 12], MacEach-
ern [52] and Escobar and West [13], and by the sub-
sequent variants proposed in MacEachern [53] and
MacEachern and M¨uller [54]. In Bayesian nonpara-
metrics these MCMC methods are typically referred
to as marginal samplers and, as noted by Ishwaran
and James [31], apply to any mixture model for
which the system of predictive distributions induced
by ˜µ is known explicitly. The reader is referred to
Neal [61] for a detailed overview of marginal sam-
plers for DP mixture models and for some notewor-
thy developments in this direction, such as the well-
known Algorithm 8 which is now a gold standard
against which other methods are compared.

An alternative family of MCMC methods for pos-
terior sampling from the DP mixture model is typ-
ically referred to as conditional samplers and relies
on the simulation from the joint posterior distri-
bution, including sampling of the mixing distribu-
tion ˜µ. These methods do not remove the inﬁnite-
dimensional aspect of the problem and instead focus
on ﬁnding appropriate ways for sampling a ﬁnite but
suﬃcient number of the atoms of ˜µ. Ishwaran and
James [31] proposed the use of a deterministic trun-
cation level by ﬁxing the number of atoms and then
bounding the resulting truncation error introduced;
the same authors also showed how to extend the pro-
posed method to any mixing distribution ˜µ in the
class of the so-called stick-breaking random proba-
bility measures. Alternatively, Muliere and Tardel-
la [58] proposed the use of a random truncation
level that allows one to set in advance the trun-
cation error. The idea of a random truncation has
been recently developed by Papaspiliopoulos and
Roberts [68] who proposed a Metropolis–Hastings

sampling scheme, while Walker [85] proposed the use
of a slice sampling scheme. See also Papaspiliopou-
los [67] and Kalli et al. [39] for further noteworthy
improvements and developments of conditional sam-
plers with random truncation levels.

It is apparent that one can replace the DP mixing
distribution with the distribution of any other dis-
crete random probability measure. Normalized ran-
dom measures (NRMs) form a large class of such
random probability measures. This includes the DP
as a special case, and was ﬁrst proposed as a class of
prior models in Bayesian nonparametrics by Regazz-
ini et al. [78]. See also James [33]. Nieto-Barajas et
al. [64] later proposed using NRMs as the mixing
distribution in (1.4), while Lijoi et al. [46–48] investi-
gated explicit examples of NRMs such as the gener-
alized DP, the normalized σ-stable process, the nor-
malized inverse Gaussian process (NIGP) and the
normalized generalized Gamma process (NGGP).
Various structural properties of the class of NRMs
have been extensively investigated by James [34],
Nieto-Barajas et al. [64], James et al. [35–37] and
Trippa and Favaro [84]. Recently James et al. [36]
described a slightly more general deﬁnition of NRMs
in terms of the normalization of the so-called com-
pletely random measures (CRMs), a class of discrete
random measures ﬁrst introduced by Kingman [40].
We refer to Lijoi and Pr¨unster [49] for a compre-
hensive and stimulating overview of nonparametric
prior models deﬁned within the unifying framework
of CRMs.

In this paper we study MCMC methods of both
marginal and conditional types for posterior sam-
pling from the mixture model (1.4) with a NRM
mixing distribution. We refer to such a model as a
NRM mixture model. Historically, the ﬁrst MCMC
methods for posterior sampling from NRM mixture
models are of the same type as those proposed by
MacEachern [52] and Escobar and West [13] for DP
mixture models: they rely on the system of pre-
dictive distributions induced by the NRM mixing
distribution. See James et al. [36] for details. Typ-
ically these methods can be diﬃcult to implement
and computationally expensive due to the necessary
numerical integrations. To overcome this drawback,
we propose novel MCMC methods of marginal type
for NRM mixture models. Our methods are gener-
alizations of Neal’s celebrated Algorithm 8 [61] to
NRM mixture models, and represent, to the best of
our knowledge, the ﬁrst marginal type samplers for
NRM mixture models that can be eﬃciently imple-

4

S. FAVARO AND Y. W. TEH

mented and do not require numerical integrations.
As opposed to MCMC methods of marginal type,
conditional samplers for NRM mixture models have
been well explored in the recent literature by Nieto-
Barajas and Pr¨unster [63], Griﬃn and Walker [27],
Favaro and Walker [15] and Barrios et al. [2]. Here
we propose some improvements to the existing con-
ditional slice sampler recently introduced by Griﬃn
and Walker [27].

For concreteness, throughout the present paper we
consider as a running example the NGGP mixture
model, namely, a mixture model of the form (1.4)
with the speciﬁcation of a NGGP mixing distribu-
tion. The NGGP is a recently studied NRM gen-
eralizing the DP and featuring appealing theoreti-
cal properties which turns out to be very useful in
the context of mixture modeling. We refer to Pit-
man [70], Lijoi et al. [48, 50] for an account on these
properties with a view toward Bayesian nonpara-
metrics. In particular, the NGGP mixture model has
been investigated in depth by Lijoi et al. [48] who
proposed a comprehensive and comparative study
with the DP mixture model emphasizing the advan-
tages of such a generalization.

The paper is structured as follows. Section 2 intro-
duces NRMs and deﬁnes the induced class of NRM
mixture models. In Section 3 we present the pro-
posed MCMC methods, of both marginal type and
conditional type, for posterior sampling from NRM
mixture models. Section 4 reports on simulation re-
sults comparing the proposed methods on a NRM
mixture model with an underlying NGGP mixing
distribution. A ﬁnal discussion is presented in Sec-
tion 5.

2. NORMALIZED RANDOM MEASURES

We review the class of NRMs with particular em-
phasis on their posterior characterization recently
provided by James et al. [36]. Such a characteriza-
tion will be crucial in Section 3 for devising MCMC
methods for posterior sampling from NRM mixture
models.

2.1 Completely Random Measures

To be self-contained, we start with a description of
CRMs. See the monograph by Kingman [41] and ref-
erences therein for details on such a topic. Let X be a
complete and separable metric space endowed with
the corresponding Borel σ-algebra X . A CRM on
X is a random variable µ taking values on the space
of boundedly ﬁnite measures on (X, X ) and such
that for any collection of disjoint sets A1, . . . , An

in X , with Ai ∩ Aj = ∅ for i 6= j, the random vari-
ables µ(A1), . . . , µ(An) are mutually independent.
Kingman [40] showed that a CRM can be decom-
posed into the sum of three independent compo-
nents: a nonrandom measure, a countable collection
of nonnegative random masses at nonrandom loca-
tions and a countable collection of nonnegative ran-
dom masses at random locations. In this paper we
consider CRMs consisting solely of the third compo-
nent, namely, a collection of random masses (Jj)j≥1
at random locations ( ˜Xj)j≥1, that is,

(2.1)

Jjδ ˜Xj

.

µ =Xj≥1

The distribution of µ can be characterized in terms
of the distribution of the random point set (Jj, ˜Xj)j≥1
as a Poisson random measure on R+ × X with mean
measure ν, which is typically referred to as the L´evy
intensity measure. As an example, Figure 1 demon-
strates a draw of a CRM along with its L´evy inten-
sity measure.

For our purposes we focus on the so-called homo-
geneous CRMs, namely, CRMs characterized by a
L´evy intensity measure ν factorizing as ν(ds, dy) =
ρ(ds)µ0(dy), for a nonnegative measure ρ absolutely
continuous with respect to Lebesgue measure and
a nonatomic probability measure µ0 over (X, X ).
Such a factorization implies the independence be-
tween the random masses (Jj)j≥1 and the random
locations ( ˜Xj )j≥1 in (2.1). Hence, without loss of
generality, the random locations can be assumed to
be independent and identically distributed accord-

Fig. 1. A draw Pj≥1 Jj δ ˜Xj
from a CRM. Each stick de-
notes an atom in the CRM, with mass given by its height Jj
and location given by ˜Xj . Behind the CRM is the density of its
L´evy intensity measure ν. The random point set {(Jj , ˜Xj )}j≥1
is described by a Poisson process with intensity measure given
by the L´evy measure ν.

MCMC FOR NORMALIZED RANDOM MEASURE MIXTURE MODELS

5

ing to the base distribution µ0, while the distribu-
tion of the random masses (Jj)j≥1 is governed by
the L´evy measure ρ: it is distributed according to a
Poisson random measure with intensity ρ.

2.2 Homogeneous Normalized Random

Measures

Homogeneous CRMs provide a fundamental tool
for deﬁning almost surely discrete nonparametric
priors via the so-called normalization approach. Spe-
ciﬁcally, consider a homogeneous CRM µ with L´evy
intensity measure ν(ds, dy) = ρ(ds)µ0(dy) and de-

note by T = µ(X) =Pj≥1 Jj the corresponding total

mass. Then one can deﬁne an almost surely discrete
random probability measure on X as follows:

Other examples of homogeneous NRMs have been
introduced in the recent literature. Notable among
these in terms of both ﬂexibility and suﬃcient math-
ematical tractability is the normalized generalized
Gamma process (NGGP). Such a process, ﬁrst in-
troduced by Pitman [70] and then investigated in
Bayesian nonparametrics by Lijoi et al. [48], is de-
ﬁned by normalizing the so-called generalized Gam-
ma CRM proposed by Brix [5]. Throughout this pa-
per we will consider the NGGP as a running exam-
ple.

Example 2.2 (NGGP). A generalized Gamma
CRM is a homogeneous CRM with L´evy intensity
measure of the form

(2.2)

˜µ =

µ
T

=Xj≥1

˜Jjδ ˜Xj

,

(2.4)

where ( ˜Jj )j≥1 is a sequence of random probabilities
deﬁned by normalizing, with respect to T , the se-
quence of random masses (Jj )j≥1. To ensure that
the normalization in (2.2) is a well-deﬁned opera-
tion, the random variable T has to be positive and
ﬁnite almost surely; this is guaranteed by a well-
known condition on the L´evy measure ρ, that is,

(2.3)

ZR+

ρ(ds) = +∞,

(1 − e−s)ρ(ds) < +∞.

ZR+

The random probability measure ˜µ is known from
James et al. [36] as a homogeneous NRM with L´evy
measure ρ and base distribution µ0. See also Regazz-
ini et al. [78] for an early deﬁnition of NRMs. The
idea of normalizing CRMs, in order to deﬁne almost
surely discrete nonparametric priors, is clearly in-
spired by the seminal paper of Ferguson [16] who
introduced the DP as a normalized Gamma CRM.

Example 2.1 (DP). A Gamma CRM is a ho-
mogeneous CRM with L´evy intensity measure of the
form

ρa(ds)µ0(dy) = as−1e−s dsµ0(dy),

where a > 0. We denote a Gamma CRM by µa and
its total mass by Ta. Note that the L´evy measure ρa
satisﬁes the condition (2.3), thus ensuring that the
NRM

˜µa =

µa
Ta

is a well-deﬁned random probability measure. Specif-
ically, ˜µa is a DP with concentration parameter a
and base distribution µ0.

ρa,σ,τ (ds)µ0(dy)

a

=

Γ(1 − σ)

s−σ−1e−τ s ds µ0(dy),

where a > 0, σ ∈ (0, 1) and τ ≥ 0. We denote a gen-
eralized Gamma CRM by µa,σ,τ and its total mass
by Ta,σ,τ . Note that the L´evy measure ρa,σ,τ satisﬁes
the condition (2.3), thus ensuring that the NRM

˜µa,σ,τ =

µa,σ,τ
Ta,σ,τ

is a well-deﬁned random probability measure. Specif-
ically, ˜µa,σ,τ is a NGGP with parameter (a, σ, τ ) and
base distribution µ0.

The NGGP includes as special cases most of the
discrete random probability measures currently ap-
plied in Bayesian nonparametric mixture modeling.
The DP represents a special case of a NGGP given
by ˜µa,0,1. Further noteworthy examples of NGGPs
include: the normalized σ-stable process, given by
˜µa,σ,0, ﬁrst introduced by Kingman et al. [42] in
relation to optimal storage problems, and the nor-
malized inverse Gaussian process (NIGP), given by
˜µa,1/2,τ , recently investigated by Lijoi et al. [47] in
the context of Bayesian nonparametric mixture mod-
eling. As regards the celebrated two-parameter Pois-
son–Dirichlet process,
introduced by Perman
et al. [69], this is not a NRM. However, it can be
expressed in terms of a suitable mixture of NGGPs.
See Pitman and Yor [72] for details on such a rep-
resentation.

It is worth pointing out that the parameterization
of the L´evy intensity measure (2.4) is diﬀerent from
those proposed in the past by Brix [5], Pitman [70]
and Lijoi et al. [48]. Such a parameterization uses
three parameters rather than two parameters. This
is so that our NGGP can easily encompass all the

6

S. FAVARO AND Y. W. TEH

other NRMs mentioned above. The three-parameter
formulation does not lead to a strict generalization
of the two-parameter formulation since the a and τ
parameters are in fact redundant. Indeed, rescaling
µa,σ,τ by a constant c > 0, which does not aﬀect the
resulting NRM, leads to a generalized Gamma CRM
with parameters (acσ, σ, τ /c).

2.3 Normalized Random Measure Mixture

Models

Given a set of n observations Y = (Y1, . . . , Yn),
a NRM mixture model consists of a corresponding
set of latent random variables X = (X1, . . . , Xn) con-
ditionally independent and identically distributed
given a NRM mixing measure ˜µ. According to the
hierarchical formulation (1.3), a NRM mixture model
can be stated as follows:

(2.5)

ind∼ F (·|Xi),

Yi|Xi
Xi|˜µ i.i.d.∼ ˜µ,

˜µ =

µ
T

,

µ ∼ CRM(ρ, µ0),

where CRM(ρ, µ0) denotes the law of the CRM µ
with L´evy measure ρ and base distribution µ0. The
rest of this section elaborates on some posterior and
marginal characterizations for the NRM mixing mea-
sure ˜µ. These characterizations will be useful in de-
riving the MCMC methods for posterior sampling
from the NRM mixture model (2.5).

Because ˜µ is almost surely discrete, ties may oc-
cur among the latent random variables X, so that
X contains k ≤ n unique values. Hence, an equiv-
alent representation of X can be given in terms of
the random partition on [n] := {1, . . . , n} induced by
the ties and the unique values. Let π be the induced
random partition of [n], that is, a family of random
subsets of [n] such that indices i and j belong to
the same subset (cluster) if and only if Xi = Xj .
For each cluster c ∈ π, we denote the correspond-
ing unique value by X ∗
c . In the context of mixture
modeling, the random partition π describes the as-
signment of observations to the various components,
while the unique value X ∗
c plays the role of the pa-
rameter associated with component c.

The random variables X are a sample from an ex-
changeable sequence directed by ˜µ and, accordingly,
the induced random partition π is also exchange-
able, namely, the probability mass function of π de-
pends only on the number of clusters |π| and the

sizes of the clusters {|c| : c ∈ π}. Such a probability
mass function is known in the literature as the ex-
changeable partition probability function (EPPF).
See the monograph by Pitman [71] and references
therein for details on this topic. The EPPF induced
by the NRM ˜µ has been recently characterized by
James et al. [36] using an auxiliary random variable
U whose conditional distribution, given the total
mass T , coincides with a Gamma distribution with
shape n and inverse scale T . In particular, the joint
conditional distribution of the random variables X
and U , given µ, is

(2.6)

P[π = π,{X ∗

=

1

Γ(n)

c ∈ dxc : c ∈ π}, U ∈ du|µ]
un−1e−T u duYc∈π
µ(dxc)|c|.

The next propositions brieﬂy summarize the poste-
rior characterizations introduced by James et al. [36].
We start by considering the characterization of the
EPPF and the system of predictive distributions in-
duced by a NRM ˜µ. Note that such a characteriza-
tion can be derived from the distribution (2.6) by
means of an application of the so-called Palm for-
mula for CRMs. See, for example, Daley and Vere-
Jones [9].

Proposition 2.1. Let ˜µ be a homogeneous NRM
with L´evy measure ρ and base distribution µ0. The
induced joint distribution of X and U , with ˜µ mar-
ginalized out, is given by

P[π = π,{X ∗

(2.7)

=

1

Γ(n)

c ∈ dxc : c ∈ π}, U ∈ du]
un−1e−ψ(u) duYc∈π

κ|c|(u)µ0(dxc),

where ψ(·) denotes the Laplace exponent of the un-
derlying CRM µ and κm(u) denotes the mth mo-
ment of
the exponentially tilted L´evy measure
e−usρ(ds), that is,

(2.8)

ψ(u) =ZR+
κm(u) =ZR+

(1 − e−us)ρ(ds),

sme−usρ(ds).

In particular, by marginalizing out the auxiliary ran-
dom variable U , the EPPF of π has the following
expression:

P[π = π] =ZR+

1

Γ(n)

un−1e−ψ(u)Yc∈π

κ|c|(u) du,

MCMC FOR NORMALIZED RANDOM MEASURE MIXTURE MODELS

7

while the unique values {X ∗
c : c ∈ π} are independent
and identically distributed according to µ0. Together
these characterize the joint distribution of the latent
variables X. Accordingly,

P[Xn+1 ∈ dx|U, X]
∝ κ1(U )µ0(dx) +Xc∈π

κ|c|+1(U )
κ|c|(U )

δX ∗

c (dx)

is the predictive distribution for a new sample
Xn+1 ∼ ˜µ, given U and X and once ˜µ is marginal-
ized out.

Note that from the probability distribution (2.7)
follows the posterior distribution of U given X, that
is,

(2.9)

P[U ∈ du|X] ∝ un−1e−ψ(u) duYc∈π

κ|c|(u).

The next proposition completes the posterior char-
acterization for NRMs by showing that the poste-
rior distribution of a homogeneuos CRM µ, given X
and U , is still a CRM.

Proposition 2.2. Let ˜µ be a homogeneous NRM
with L´evy measure ρ and base distribution µ0. The
posterior distribution of the underlying homogeneous
CRM µ, given X and U , corresponds to

(2.10)

µ|U, X ∼ µ′ +Xc∈π

J ′
c ,
cδX ∗

where µ′ is a homogeneous CRM with an exponential
tilted L´evy intensity measure of the form

ν′(ds, dy) = e−U sρ(ds)µ0(dy)

and where the random masses {J ′
c : c ∈ π} are in-
dependent of µ′ and among themselves, with condi-
tional distribution

P[J ′

c ∈ ds|U, X] =

1

κ|c|(U )

s|c|e−U sρ(ds).

The posterior distribution of the NRM ˜µ, given X
and U , follows by normalizing the CRM µ|U, X.

We conclude this section by illuminating Propo-
sitions 2.1 and 2.2 via their applications to the DP
and NGGP.

Example 2.3 (DP). An application of Proposi-
tion 2.1 to the L´evy measure of the Gamma CRM

shows that π is independent of U , and its distribu-
tion coincides with

(2.11)

P[π = π|U ] = P[π = π]
Γ(a)a|π|

=

Γ(a + n)Yc∈π

Γ(|c|).

The corresponding predictive distributions are also
independent of U and are of the form
|c|
a + n

Xn+1|U, X ∼

(2.12)

δX ∗
c .

a

a + n

µ0 +Xc∈π

An application of Proposition 2.2 shows that the
posterior distribution of µ, given U and X, corre-
sponds to (2.10) with µ′ a Gamma CRM with L´evy
intensity measure

ν′(ds, dy) = as−1e−s(U +1) ds µ0(dy),

and random masses J ′
c distributed according to a
Gamma distribution with parameter (|c|, U +1). Nor-
malizing the posterior CRM, the resulting posterior
random probability measure ˜µ|U, X does not depend
on the scale U + 1 and is still a DP, with updated
base measure

µn = aµ0 +Xc∈π

|c|δX ∗
c .

The law of the random partition π induced by the
predictive distributions (2.12) is popularly known
as the Chinese restaurant process. The metaphor is
that of a sequence of customers entering a Chinese
restaurant with an inﬁnite number of round tables.
The ﬁrst customer sits at the ﬁrst table, and each
subsequent customer joins a new table with proba-
bility proportional to a, or a table with m previous
customers with probability proportional to m. After
n customers have entered the restaurant, the seat-
ing arrangement of customers around tables corre-
sponds to the partition π, with probabilities given
by (2.11). Relating to X, each table c ∈ π is served a
dish X ∗
c if customer i joined table c,
that is, i ∈ c. See Blackwell and MacQueen [4] for a
ﬁrst characterization of the predictive distributions
(2.12). See also Aldous [1] for details and Ewens [14]
for an early account in population genetics.

c , with Xi = X ∗

Example 2.4 (NGGP). An application of the
formulae (2.8) to the L´evy measure of the general-
ized Gamma CRM leads to

(2.13)

ψ(u) =

κm(u) =

a
σ

((u + τ )σ − τ σ),

a

(u + τ )m−σ

Γ(m − σ)
Γ(1 − σ)

.

8

S. FAVARO AND Y. W. TEH

Fig. 2. Left: prior distribution of the number of clusters with σ = 0.7, τ = 1, a = 0.1, 1 and 10 and n = 1000. With increasing
a the number of clusters increases. Right: distribution of the number of clusters with σ = 0.1, τ = 1, and a = 38.5, 61.5 and
161.8. Values of a were chosen so that the mean number of clusters matches those in the left panel. With a smaller value of
σ both the mean and the variance in the number of clusters decreases, which is why the values of a are increased from the left
panel.

The random partition π and U are not independent
as in the DP, and has a joint distribution given by

P[π = π, U ∈ du]
a|π|un−1

(2.14)

=

Γ(n)(u + τ )n−σ|π| e−(a/σ)((u+τ )σ −τ σ) du
·Yc∈π

Γ(|c| − σ)
Γ(1 − σ)

,

and the corresponding system of predictive distribu-
tions for Xn+1, given U and X, is

Xn+1|U, X

a(U + τ )σ

(2.15)

∼

µ0

a(U + τ )σ + n − σ|π|
|c| − σ
+Xc∈π

a(U + τ )σ + n − σ|π|

δX ∗
c .

Finally, an application of Proposition 2.2 shows that
the posterior distribution of µ, given U and X, cor-
responds to

(2.16)

µ|U, X ∼ µ′ +Xc∈π

J ′
cδX ∗
c ,

where µ′ is a generalized Gamma CRM with pa-
rameters (a, σ, U + τ ) and the random masses J ′
c are
independent among themselves and of µ′, and dis-
tributed according to a Gamma distribution with
parameter (|c| − σ, U + τ ).

Note that the predictive distributions (2.15) pro-
vide a generalization of the Chinese restaurant pro-

cess metaphor for the DP. Conditionally on U , the
probability of the (n + 1)st customer joining a table
with m existing customers is proportional to m− σ,
with σ acting as a discount parameter. Note that the
relative eﬀect of σ is more pronounced for small val-
ues of m, which leads to larger proportions of small
tables with larger σ and power-law behaviors in π.
On the other hand, the probability of joining a new
table is proportional to an increasing function of all
three parameters. Figure 2 shows how the distribu-
tion over the number of clusters is aﬀected by the
parameters, while Figure 3 shows how the distribu-
tion over the number of clusters grows with n for
diﬀerent values of the parameters.

Lijoi et al. [48] provided a detailed comparative
study between the predictive structures of the NGGP
and the DP in the context of mixture modeling. The
advantage of specifying the NGGP mixing distri-
bution with respect to the DP mixing distribution
clearly relies on the availability of the additional pa-
rameter σ. In the DP mixture model the only free
parameter which can be used to tune the distribu-
tion of the number of clusters is the mass parameter
a: the bigger a, the larger the expected number of
clusters. In the NGGP mixture model the parame-
ters a and τ play the same role as the mass parame-
ter a in the DP mixture model. On the other hand,
σ inﬂuences the grouping of the observations into
distinct clusters and can be used to tune the vari-
ance of the number of clusters in the NGGP mix-
ture model: the bigger σ, the larger the variance of
the number of clusters. Further, σ also controls an

MCMC FOR NORMALIZED RANDOM MEASURE MIXTURE MODELS

9

Fig. 3. Mean and standard deviation of the number of clusters as a function of n, on a log–log plot. Left: with parameters
σ = 0.5, τ = 1 and a = 0.1, 1 and 10. Right: with parameters σ = 0.1, 0.5 and 0.9, τ = 1 and a = 1. The growth rate with n
follows a power-law with index σ, while a aﬀects the number of clusters without aﬀecting the power-law behavior.

interesting reinforcement mechanism that tends to
reinforce signiﬁcantly those clusters having higher
frequencies. This turns out to be a very appealing
feature in the context of mixture modeling. We refer
to Lijoi et al. [48] for details on the prior elicitation
for σ to control the reinforcement mechanisms in-
duced by it.

3. MCMC POSTERIOR SAMPLING

METHODS

In this section we develop some novel MCMC sam-
plers of both marginal and conditional type for the
NRM mixture models (2.5). In particular, we con-
sider as a running example the NGGP mixing mea-
sure with parameter (a, σ, τ ) and base distribution µ0.

3.1 Conjugate Marginalized Sampler

We start with the simplest situation, when the
base distribution µ0 is conjugate to the mixture ker-
nel F . In this case both the CRM µ and the cluster
parameters {X ∗
c : c ∈ π} can be marginalized out ef-
ﬁciently, leaving only the partition π and auxiliary
variable U to be sampled. The joint distribution of
π and U is given by (2.7), while the likelihood is

(3.1)

P[Y|π = π] =Yc∈π
where Yc = {Yi : i ∈ c} and
f (Yc) =ZXYi∈c

f (Yi|x)µ0(dx).

Since µ0 is conjugate to F , the integral is assumed
to be available in closed form and eﬃciently evalu-
ated using the suﬃcient statistics of Yc. Moreover,
since both the conditional distribution of π given U
and the likelihood are in product partition form, the
conditional distribution of π given Y and U is also
in a product partition form.

We can update π using a form of Gibbs sampling
whereby the cluster assignment of one data item Yi
is updated at a time. Let π\i be the partition with
i removed. We denote the cluster assignment of Yi
with a variable zi such that zi = c denotes the event
that Yi is assigned to cluster c ∈ π\i, and zi = ∅
denotes the event that it is assigned a new cluster.
In order to update zi, we can use formulae (2.7) and
(3.1) to provide the conditional distribution of zi,
given π\i, Y and U . Speciﬁcally,

(3.2)

P[zi = c|π\i, U, Y]
κ|c|+1(U )
κ|c|(U )
κ1(U )f ({Yi}),

∝


f ({Yi} ∪ Yc)

f (Yc)

,

for c ∈ π\i,
for c = ∅.

f (Yc),

Under the assumption that ˜µ is a NGGP and us-
ing (2.13), the above simpliﬁes to

P[zi = c|π\i, U, Y]
∝(cid:26) (|c| − σ)f (Yi|Yc),
a(U + τ )σf (Yi|∅),

for c ∈ π\i,
for c = ∅,

10

where

f (y|y) =

f ({y} ∪ y)

f (y)

.

We see that the update is a direct generalization of
that for the DP which can be easily recovered by
setting σ = 0. The probability of Yi being assigned
to a cluster is simply proportional to the product of
a conditional prior probability of being assigned to
the cluster and a conditional likelihood associated
with the observation Yi. See MacEachern [52] and
Neal [60] for details on the DP case. In the next sec-
tion we describe the updates for the parameters a, σ
and τ , and for U , before proceeding to the marginal-
ized and conditional samplers in the case when µ0
is not conjugate.

3.1.1 Updates for NGGP parameters and U For U ,
note that given π, U is independent of Y with con-
ditional distribution (2.9). In particular, in the case
of the NGGP, the conditional distribution simpliﬁes
to

un−1

e−(a/σ)((u+τ )σ −τ σ) du.

(u + τ )n−a|π|

P[U ∈ du|π] ∝
A variety of updates can be used here. We have
found that a change of variable V = log(U ) leads
to better behaved algorithms, since the conditional
density fV |π(v) of V given π, that is,

P[V ∈ dv|π] ∝

evn

(ev + τ )n−a|π|

e−(a/σ)((ev +τ )σ−τ σ) dv

= fV |π(v) dv,

is log concave. We use a simple Metropolis–Hastings
update with a Gaussian proposal kernel with mean
V and variance 1/4, although slice sampling by
Neal [62] or, alternatively, adaptive rejection sam-
pling by Gilks and Wild [21] can also be employed.
For the NGGP, we can easily derive the updates
for the parameters a, σ and τ using (2.14) and given
prior speciﬁcations for the parameters. See Lijoi
et al. [48] for a detailed analysis on prior speciﬁca-
tion in the context of Bayesian nonparametric mix-
ture modeling. As regards a, we can simply use a
Gamma prior distribution with parameter (αa, βa).
Then the conditional distribution of a, given σ, τ ,
U and π, is simply a Gamma distribution, that is,

P[da|σ, τ, U, π]
∝ aαa+|π|−1e−a(βa+((U +τ )σ−τ σ)/σ) da.

For τ we can again use a Gamma prior distribu-
tion with parameter (ατ , βτ ). Then the conditional

S. FAVARO AND Y. W. TEH

distribution of τ , given a, σ, U and π, is

P[dτ|a, σ, U, π]
∝ τ ατ −1e−τ βτ e−(a/σ)((U +τ )σ −τ σ)

τ σ|π|(U + τ )n−σ|π|

dτ.

We update τ in its logarithmic domain, using the
same procedure as for U described above. Finally,
for σ we can use a Beta prior distribution with pa-
rameter (ασ, βσ). Then the conditional distribution
of σ, given a, τ , U and π, corresponds to

P[dσ|a, τ, U, π]
∝ σασ−1(1 − σ)βσ−1 e−(a/σ)((U +τ )σ −τ σ)
·Yc∈π

Γ(|c| − σ)
Γ(1 − σ)

τ σ|π|(U + τ )n−σ|π|

dσ.

We can easily update σ using slice sampling by
Neal [62].

3.2 Nonconjugate Marginalized Samplers

The main drawback of the previous algorithm is
the assumption of conjugacy, which limits its appli-
cability since nonconjugate priors are often desirable
in order to increase modeling ﬂexibility. For DP mix-
ture models a number of marginalized algorithms for
the nonconjugate setting have been proposed and in-
vestigated in the literature. The review of Neal [61]
provides a detailed overview along with two novel al-
gorithms. One of these algorithms, the so-called Al-
gorithm 8, is simple to implement, has been demon-
strated to provide excellent mixing speed, and has
a tunable parameter to trade oﬀ computation cost
against speed of convergence.

3.2.1 Generalizing Neal’s Algorithm 8 In this sec-
tion we provide a straightforward generalization of
Neal’s Algorithm 8 to the class of NRM mixture
models with a nonconjugate base distribution. Here,
the cluster parameters X ∗
c cannot be easily marginal-
ized out. Instead we include them into the state of
the MCMC algorithm, so that the state now consists
of the partition π, {X ∗
c : c ∈ π} and the random vari-
able U , and we sample the cluster parameters along
with π and U . Note that the parameters for existing
clusters X ∗
c can be updated with relative ease, using
any MCMC update whose stationary distribution is
the conditional distribution of X ∗
c given everything
else, that is,

P[X ∗

c ∈ dx|π, U, Y] ∝ µ0(dx)Yi∈c

f (Yi|x).

MCMC FOR NORMALIZED RANDOM MEASURE MIXTURE MODELS

11

The diﬃculty with a nonconjugate marginalized sam-
pler is the introduction of new clusters (along with
their parameters) when Gibbs sampling the cluster
assignments. Following Neal [61], we conceptualize
our update in terms of an augmented state with
additional temporarily existing variables, such that
the marginal distribution of the permanent variables
once the temporary ones are integrated out is the
appropriate posterior distribution.

Consider updating the cluster assignment variable
zi given the existing clusters in π\i. We introduce
an augmented space with C empty clusters, with
parameters X e
C that are independent of π\i
and independent and identically distributed accord-
ing to µ0. The state space of zi is augmented as well
to include both existing clusters π\i and the new
ones [C] = {1, . . . , C}, with conditional distribution

1, . . . , X e

P[zi = c ∈ π\i|π\i] ∝

and

κ|c|+1(U )
κ|c|(U )

P[zi = k ∈ [C]|π\i] ∝

κ1(U )

C

,

respectively. Identifying zi being in any of the addi-
tional clusters as assigning Yi to a new cluster, we
see that the total probability for Yi being assigned
to a new cluster is proportional to the ﬁrst moment
κ1(U ), which is the same as in (2.7) and (3.2).

The update can be derived by ﬁrst initializing the
augmentation variables given the current state of
the Markov chain, updating zi, then discarding the
augmentation variables. If Yi is currently assigned to
a cluster which contained another data item, then
zi = c for some c ∈ π\i, and the empty cluster pa-
rameters are simply drawn independently and iden-
tically according to µ0. On the other hand, if Yi
is currently assigned to a cluster containing only it-
self, say, with parameter X ∗
∅, then in the augmented
space zi has to be one of the new clusters, say, zi = k
for some k ∈ [C] with X e
∅. The actual value
of k is unimportant, for convenience we may use
k = 1. The other empty clusters then have parame-
ters drawn independently and identically according
to µ0. We can now update zi by sampling from its
conditional distribution given Yi and the parameters
of all existing and empty clusters. Speciﬁcally,

k = X ∗

(3.3)

P[zi = c|π\i, U, Y]
κ|c|+1(U )
κ|c|(U )
κ1(U )

∝


f (Yi|X ∗
c ),

f (Yi|X e
c ),

C

for c ∈ π\i,

for c ∈ [C].

Under the assumption that ˜µ is a NGGP, (3.3) again
simpliﬁes to

P[zi = c|π\i, U, Y]
∝( (|c| − σ)f (Yi|X ∗
c ),
(U + τ )σf (Yi|X e
c ),

a
C

for c ∈ π\i,
for c ∈ [C].

If the new value of zi is c ∈ [C], this means that
Yi is assigned to a new cluster with parameter X e
c ;
the other empty clusters are discarded to complete
the update. On the other hand, if the new value is
c ∈ π\i, then Yi is assigned to an existing cluster c,
and all empty clusters are discarded. Finally, the
random variable U and any hyperparameters may
be updated using those in Section 3.1.1.

3.2.2 The Reuse algorithm In the above algorithm,
each update to the cluster assignment of an observa-
tion is associated with a set of temporarily existing
variables which has to be generated prior to the up-
date and discarded afterward. As a result, many in-
dependent and identically distributed samples from
the base distribution have to be generated through-
out the MCMC run, and in our experiments this ac-
tually contributes a signiﬁcant portion of the overall
computational cost. We can mitigate this wasteful
generation and discarding of clusters by noting that
after updating the cluster assignment of each obser-
vation, the parameters of any unused empty clus-
ters are in fact already independently and identi-
cally distributed according to the base distribution.
Thus, we can consider reusing them for updating the
next observation. However, note that as a result the
parameters of the empty clusters used in diﬀerent
updates will not be independent, and the justiﬁca-
tion of correctness of Neal’s Algorithm 8 (as Gibbs
sampling in an augmentation scheme) is no longer
valid.

In this section we develop an algorithm that does
reuse new clusters, and show using a diﬀerent tech-
nique that it is valid with stationary distribution
given by the posterior. For the new algorithm, we
instead augment the MCMC state space with a per-
manent set of C empty clusters, so the augmented
state space now consists of the partition π, the la-
tent variable U , the parameters {X ∗
c : c ∈ π} of ex-
isting clusters and the parameters {X e
k : k ∈ [C]} of
the auxiliary empty clusters. Further, we develop the
cluster assignment updates as Metropolis–Hastings
updates instead of Gibbs updates.

In the following we use the superscript ′ in order
to denote variables and values associated with the

12

S. FAVARO AND Y. W. TEH

new proposed state of the Markov chain. Suppose
we wish to update the cluster assignment of obser-
vation Yi. Again we introduce the variable zi, which
takes value c ∈ π\i if Yi is assigned to a cluster con-
taining other observations, and takes values k ∈ [C]
uniformly at random if Yi is assigned to a cluster by
itself. If zi = c ∈ π\i, then the proposal distribution
Q is described by a two-step algorithm:

1. Sample the variable z′

i from the conditional dis-

tribution (3.3) as before.

2a. If z′

2b. If z′

existing cluster c′.

i = c′ ∈ π\i, then we simply assign Yi to the
i = k′ for one of the empty clusters k′ ∈ [C]
with X e
(i) we assign Yi to a newly created cluster with

k′ = x, then:

parameter X ∗′

∅ := x;

(ii) set X e′

k′ := x′ ∼ µ0 with a new draw from the

base distribution.

and

Q[z′

i = k′, X ∗′

∅ ∈ dx0, X e′
zi = c, X e

k′ ∈ dx′ ···|
k′ ∈ dx···]
f (Yi|x)δx(dx0)µ0(dx′).

κ1(U )

C

∝

We have suppressed listing all other variables for
brevity. For the reverse proposal (k′ ⇒ c), the prob-
abilities are, respectively,

P[z′

i = k′, dX ∗′

∅ ∈ dx0, X e′

k′ ∈ dx′ ···]
κ1(U )µ0(dx0)f (Yi|x)µ0(dx′)

1
C

∝

and

Q[zi = c, X e

k′ ∈ dx···|z′
f (Yi|X ∗

κ|c|+1(U )
κ|c|(U )

∝

c )δx0(dx).

i = k′, X ∗′

∅ ∈ dx0 ···]

On the other hand, if Yi is currently assigned to a
cluster all by itself, say, with parameter X ∗
∅ = x0,
then zi will initially take on each value k ∈ [C] uni-
formly with probability 1/C. We start by setting the
value for the kth empty cluster parameter X e
k := x0
(its old value is discarded) and then removing the
singleton cluster that Yi is currently assigned to.
Then the two-step algorithm above is carried out.

It is important to point out that the proposal de-
scribed above is reversible. For example, the reverse
of moving Yi from an existing cluster c to a new clus-
ter with parameter X ∗
∅ = x, where x is the previous
k′ with its new value being a draw x′ from
value of X e
µ0, is exactly the reverse of the proposal moving Yi
from a singleton cluster with parameter X ∗
∅ = x to
the cluster c, while replacing the previous value x′ of
k′ with x. We denote the two proposals as (c ⇒ k′)
X e
and (k′ ⇒ c). Analogously, the reverse of (c ⇒ c′) is
(c′ ⇒ c) and the reverse of (k ⇒ k′) is (k′ ⇒ k).
Note also that the proposals are trans-dimensional
since the number of clusters in the partition π (and
particularly the number of cluster parameters) can
change. See Green [24] and Richardson and Green [79]
for approaches to trans-dimensional MCMC. Fortu-
nately they are dimensionally balanced. In fact, we
can show that the acceptance probability is simply
always one. For example, for the (c ⇒ k′) proposal,
the joint probability of the initial state and the pro-
posal probability are, respectively, proportional to

P[zi = c, X e

k′ ∈ dx···] ∝

κ|c|+1(U )
κ|c|(U )

f (Yi|X ∗

c )µ0(dx)

Note that the normalization constants arising from
the conditional distributions (3.3) for proposals in
both directions are the same, so they can be ignored.
We see that the product of the probabilities for the
(c ⇒ k′) proposal is the same as that for the reverse
(k′ ⇒ c), so the Metropolis–Hastings acceptance ra-
tio is simply one. Similarly, the acceptance ratios of
other proposal pairs are also equal to one.

In addition to updating the cluster assignments of
all observations as above, we also need to update the
parameters of the C empty clusters. We do this by
marginalizing them out before updating U and the
hyperparameters according to Section 3.1.1, and re-
placing them afterward with new independent and
identically distributed draws from the base distri-
bution. Note that the resulting Metropolis–Hastings
updates are very similar to the augmentation scheme
Gibbs updates described in Section 3.2.1. The only
diﬀerence is the way the parameters of the empty
clusters are managed and retained across cluster as-
signment updates of multiple observations.

3.3 Conditional Slice Sampler

In the so-called marginalized samplers the CRM
µ is marginalized out while the latent variables X
representing the partition structure and the cluster
parameters are sampled. In a conditional sampler we
instead alternatively Gibbs sample µ given X and X
given µ. Proposition 2.2 provides the conditional dis-
tribution for µ given X, while the conditional of X
given µ is straightforward. What is not straightfor-
ward is the fact that since µ has an inﬁnite number

MCMC FOR NORMALIZED RANDOM MEASURE MIXTURE MODELS

13

of atoms we cannot explicitly sample all of it on a
computer with ﬁnite resources. Thus, it is necessary
to truncate µ and work only with a ﬁnite number of
atoms.

In this section we will describe a conditional sam-
pler based on a slice sampling strategy for trunca-
tion. See Walker [85] for the slice sampler in DP
mixture models, and Griﬃn and Walker [27] and
Griﬃn et al. [26] for slice samplers in NRM mix-
ture models on which our sampler is based. Recall
from (2.5) that each observation Yi is assigned to a
cluster parametrized by an atom Xi of µ. We aug-
ment the state with an additional slice variable Si,
whose conditional distribution is a Uniform distribu-
tion taking values between 0 and the mass of atom
Xi in µ, that is,

(3.4)

Si|Xi, µ ∼ Uniform(0, µ({Xi})).

Marginalizing out Si, the joint distribution of the
other variables reduces back to the desired poste-
rior distribution. On the other hand, conditioned
on Si, Xi can only take on values corresponding to
atoms in µ with mass at least Si. Since Si is al-
most surely positive, this set of atoms is ﬁnite, and
so Si eﬀectively serves as a truncation level for µ in
the sense that only these ﬁnitely many atoms are
needed when updating Xi. Over the whole data set,
only the (ﬁnitely many) atoms in µ with mass at
least S = mini∈[n] Si > 0 are required when updat-
ing the set of latent variables X given µ and the
slice variables S = {Si : i ∈ [n]}.
The state space of our sampler thus consists of the
latent variables X, the slice variables S, the CRM
µ and the auxiliary variable U introduced in Sec-
tion 2.3. At a high level, our sampler is simply a
Gibbs sampler, iterating among updates to X, U ,
and both µ and S jointly.

First consider updating X. It is easy to see that
conditioned on U , µ and S the Xi’s are mutually
independent. For each i ∈ [n], the conditional prob-
ability of Xi taking on value x ∈ X is proportional
to the product of the probability µ({x})/µ(X) of x
under the NRM ˜µ, the conditional distribution func-
tion f (Yi|x) of observation Yi, and the conditional
density of Si given Xi = x, which is simply 1/µ({x})
when 0 < Si < µ({x}) and 0 otherwise. The resulting
conditional distribution of Xi simpliﬁes to
P[Xi = x|µ, Yi, Si] ∝(cid:26) f (Yi|x),

if Si < µ({x}),
otherwise.

This is a discrete distribution, with positive prob-
ability of Xi = x only when x coincides with the

0,

location of an atom in µ with mass greater than Si.
Note that there almost surely are only a ﬁnite num-
ber of such atoms in µ since Si > 0, so that updating
Xi is computationally feasible.

Now consider updating U . We will perform this
update conditioned only on the partition described
by X, with the random measure µ and the slice vari-
ables S marginalized out. We can also update any
hyperparameters of the CRM and of the base distri-
bution µ0 at this step as well. For example, if ˜µ is a
NGGP, we can update both U and the parameters
(a, σ, τ ) using those described in Section 3.1.1, which
makes these Metropolis-within-Gibbs updates.

Finally, consider updating µ and S jointly. Note
that this update needs to be performed right af-
ter the U update since µ and S were marginalized
out when updating U . The conditional distribution
of µ given U and X is given by Proposition 2.2,
which shows that µ will contain a ﬁnite number of
ﬁxed atoms located at the unique values {X ∗
c : c ∈ π}
among X, and a countably inﬁnite number of ran-
domly located atoms corresponding to the unused
clusters in the NRM mixture model. Given µ, the
slice variables are independent with distributions
given by (3.4); in particular, note that they depend
only on the masses of the ﬁxed atoms of µ. On the
other hand, as noted above, we only need the ran-
dom atoms of µ with masses above the overall trun-
cation level S = mini∈[n] Si. Therefore, a suﬃcient
method for sampling both µ and S is to ﬁrst sample
the ﬁxed atoms of µ, followed by S, and ﬁnally the
random atoms with masses above S.

For the ﬁxed atoms of µ, Proposition 2.2 states
that each of them corresponds to a unique value
among X and that their masses are mutually inde-
pendent and independent from the random atoms.
For each such unique value X ∗
c , c ∈ π, the condi-
tional distribution of its mass J ′
c is

P[J ′

c ∈ ds|U, X] ∝ s|c|e−U sρ(ds),

(3.5)
where |c| is the number of observations allocated to
the cluster c, that is, with Xi = X ∗
c . Under the as-
sumption of ˜µ being a NGGP, the density in (3.5)
simpliﬁes to s|c|−σ−1e−(U +τ )s, a Gamma density. We
also update the locations of the ﬁxed atoms as well
using an acceleration step as in Bush and MacEach-
ern [7]. The conditional distribution function of X ∗
c
is proportional to its prior distribution function times
the likelihoods of observations assigned to the clus-
ter, that is,

P[X ∗

c ∈ dx|Y] ∝ µ0(dx)Yi∈c

f (Yi|x),

14

S. FAVARO AND Y. W. TEH

The method is based on the idea of thinning by
Lewis and Shedler [45], a method to simulate from
a Poisson random measure by ﬁrst proposing points
according to a proposal Poisson random measure
with higher intensity than the desired one. Each
point is then accepted with probability given by
the ratio of intensities under the proposal and de-
sired Poisson random measures. The idea of adap-
tive thinning is that we can propose points itera-
tively from left to right starting at S, and after each
proposed point t the bound wt is used as the inten-
sity of the proposed Poisson random measure from
which the next point is drawn. As t increases, the
bound tightens, so rejections are reduced. Further,
s wt(s′) ds′ < ∞, the iteration will terminate
after a ﬁnite number of points are proposed. Specif-
ically, the sampling scheme is described as follows:

as R ∞

1. set N := ∅, t := S;
2. iterate until termination:

(i) let r be a draw from an Exponential distri-

bution with parameter 1;

(ii) if r > Wt(∞), terminate; else set t′ := W −1
(iii) with probability v′(t′)/wt(t′) accept sample:

t

(r);

set N := N ∪ {t′};

(iv) set t := t′ and continue to next iteration;

3. return N as a draw from the Poisson random

measure with intensity v′ on [S,∞).

The returned N constitutes the set of masses for the
random atoms in µ with masses above the overall
truncation level S.

3.4 Some Remarks

There is a rich literature on conditional sampling
schemes for nonparametric mixture models. In the
DP mixture model case, the use of the stick-breaking
representation for ˜µ, as proposed by Ishwaran and
James [31], Papaspiliopoulos and Roberts [68] and
Walker [85], is very simple since it involves a se-
quence of random variables that are independently
Beta distributed a priori as well as a posteriori con-
ditioned on other variables. However, this simplicity
comes at a cost of slower mixing due to the label-
switching problem discussed in Jasra et al. [38]. Pa-
paspiliopoulos and Roberts [68] noted that while the
likelihood is invariant to the ordering of atoms, the
stick-breaking prior has a weak preference for atoms
to be sorted by decreasing mass, resulting in mul-
tiple modes in the posterior. Then, they proposed
Metropolis–Hastings moves that interchange pairs
of atoms to improve mixing. A more sophisticated

Fig. 4. Adaptive bounds for simulating from a Poisson ran-
dom measure with intensity v′(s).

where i ∈ c indicates indices of those observations
assigned to the cluster c. Note that any ergodic
Markov kernel with the above as its stationary dis-
tribution suﬃces.

Once the ﬁxed atoms are updated, the slice vari-
ables are updated by sampling each Si independently
from its conditional distribution (3.4). Finally, the
random atoms of µ with mass above the overall trun-
cation level S can be sampled using Proposition 2.2.
As we work only with homogeneous CRMs here, the
locations are simply independent and identically dis-
tributed draws from µ0, while their masses are dis-
tributed according to a Poisson random measure on
[S,∞) with an exponentially tilted intensity mea-
sure ρ′(ds) = e−U sρ(ds).
We propose an adaptive thinning approach (see
Ogata [66]) to sample from the Poisson random mea-
sure which is computationally eﬃcient but applies
only to certain classes of intensity measures which
can be adaptively bounded in the following sense.
Let v′(s) be the density of ρ′(ds) with respect to the
Lebesgue measure and assume that for each t ∈ R+
there is a function wt(s) such that wt(t) = v′(t) and
wt(s) ≥ wt′(s) ≥ v′(s) for every s, t′ ≥ t. See Fig-
ure 4. In particular, for the NGGP one has

v′(s) =

a

Γ(1 − σ)

s−1−σe−s(τ +U ),

and we can use the family of adaptive bounds

wt(s) =

a

Γ(1 − σ)

t−1−σe−s(τ +U ),

with the inverse of the integral given by

W −1

t

(r) = t −

1

τ + U

log(cid:18)1 −

r(τ + U )Γ(1 − σ)
at−1−σe−t(τ +U ) (cid:19).

Note that both wt(s) and the inverse of the map
t wt(s′) ds′ are analytically tractable, with

Wt(s) =R s
R ∞
t wt(s′) ds′ < ∞.

MCMC FOR NORMALIZED RANDOM MEASURE MIXTURE MODELS

15

approach that avoids the weak identiﬁability alto-
gether is to use the natural unordered representation
stated in Proposition 2.2. This approach was taken
in Griﬃn and Walker [27], and we used it here as
well.

There are a few alternative methods for sampling
from the Poisson random measure governing the
masses of the random atoms. Griﬃn and Walker [27]
proposed ﬁrst sampling the number of atoms from
a Poisson with rate ρ′([S,∞)), then sampling the
masses independently and identically distributed ac-
cording to a distribution obtained by normalizing ρ′.
Another possibility proposed by Barrios et al. [2]
and Nieto-Barajas and Pr¨unster [63] is to use the
representation proposed by Ferguson and Klass [17],
which involves using the mapping theorem for Pois-
son random measures to sample the masses in order
starting from the largest to the smallest.

Our slice sampler follows Griﬃn and Walker [27]
in introducing a slice variable Si for each observa-
tion i. Another approach described in Griﬃn and
Walker is to introduce a single slice variable Sall for
all observations, with conditional distribution

Sall ∼ Uniform(cid:16)0, min

i∈[n]

µ({Xi})(cid:17).

Griﬃn and Walker [27] found that either method
may work better than the other in diﬀerent situa-
tions. We preferred the method described here, as
it is simpler and the updates for the latent vari-
ables, which form the most time consuming part of
the algorithm, can be trivially parallelized to take
advantage of recent parallel computation hardware
architectures.

Slice samplers have the advantages that they can
technically be exact in the sense that they target
the true posterior distribution. This is opposed to
alternative truncations which introduce approxima-
tions by ignoring atoms with low masses, for exam-
ple, Ishwaran and James [31] and Barrios et al. [2].
However, a diﬃculty with slice samplers is that al-
though the number of random atoms in µ above
the truncation level S is ﬁnite with probability one,
the actual number generated can occasionally be ex-
tremely large, for example, in case of NGGPs when
S is small and σ is large. In our implementation our
program can occasionally terminate as it runs out
of memory. We ﬁx this by introducing an approxi-
mation where we only generate atoms with masses
above 10−8 and only keep a maximum of the 106
atoms with largest masses. Griﬃn and Walker [27]

and Barrios et al. [2] have also made similar approx-
imations. Of course this approximation eﬀectively
nulliﬁes the advantage of slice samplers being ex-
act, though we have found in experiments that the
approximation introduced is minimal.

Comparing the computational requirements of the
proposed marginalized and conditional samplers, we
expect the marginalized samplers to produce chains
with less autocorrelation since they marginalize more
latent variables out. Further, their computational
costs per iteration are controllable and more sta-
ble since each involves introducing a ﬁxed number
of empty clusters. Concluding, while the conditional
sampler is easily parallelizable, the marginalized sam-
plers are not.

4. NUMERICAL ILLUSTRATIONS

In this section we illustrate the algorithms on a
number of data sets: three simple and well-studied
data sets, the galaxy, acidity and the Old Faithful
geyser data sets, as well as a more complex data set
of neuronal spike waveforms. The galaxy data set
consists of the velocities at which 82 galaxies are re-
ceding away from our own and the acidity data set
consists of the log acidity measurements of 155 lakes
in Wisconsin; both are one-dimensional. The geyser
data set is two-dimensional, consisting of 272 dura-
tions of eruptions along with the waiting times since
the last one. The spikes data set2 consists of a to-
tal of 14,802 neuronal spike waveforms recorded us-
ing tetrodes. Each of the four electrodes contributes
28 readings sampled at 32 kHz, so that each wave-
form is 112-dimensional. Prototypical waveforms are
shown in Figure 10. To reduce computation time,
in the following we ﬁrst used PCA to reduce the
data set down to six dimensions, which preserved
approximately 80% of the variance and suﬃcient in-
formation for the mixture model to recover distinct
clusters.

We analyzed the data sets by means of NRM mix-
tures of (multivariate) Gaussian distributions. Let
D be the number of dimensions of the data set. The
base distribution over the Gaussian means and co-
variance matrices is factorized as follows:

µ0(dm, dΣ) = ND(dm; m0, S0)IW D(dΣ; α0, Σ0),

where ND denotes a D-dimensional Gaussian distri-
bution with given mean and covariance matrix and

2We thank G¨or¨ur and Rasmussen [23] for providing us with

the data set.

16

S. FAVARO AND Y. W. TEH

Fig. 5. Visualizing the induced prior on the number of clusters with n = 82 corresponding to the size of the galaxy data set.
Left: histogram of the mean number of clusters. Center: histogram of the standard deviation of the number of clusters. Right:
scatter plot of standard deviation vs mean of the number of clusters. 10,000 draws from the prior for a and σ were used.

IWD denotes an inverse Wishart over D × D posi-
tive deﬁnite matrices with given degree of freedom
and scale matrix.

bution given by

µ0(dm, dΣ)

A number of authors have advocated the use of
weakly informative priors for mixtures of Gaussian
distributions. See Nobile [65], Raftery [75] and Rich-
ardson and Green [79]. We follow the approach ad-
vocated by Richardson and Green [79], generaliz-
ing it to the multivariate setting. In particular, we
assume knowledge of a likely range over which the
data lies, with the range in the ith dimension be-
ing [m0i − si, m0i + si]. We set S0 to be a diagonal
matrix with ith diagonal entry being s2
i so that the
prior over component means is rather ﬂat over the
range. We set α0 = D + 3, and set a hierarchical
prior Σ0 ∼ IWD(β0, γ0S0) where β0 is chosen to be
D − 0.6. These degrees of freedom express the prior
belief that component covariances are generally sim-
ilar without being informative about their absolute
scales. We choose γ0 so that E[Σ] = S0/50, that is,
that the a priori range of each component is approx-

imately √50 ≈ 7 times smaller than the range set by

S0, although the model is not sensitive to this prior
range since Σ0 is random and allowed to adapt to the
data in its posterior. In the one-dimensional setting
this prior reduces to the same one used by Richard-
son and Green. A detailed study of prior speciﬁca-
tions for mixtures of multivariate Gaussian distri-
butions is beyond the scope of this paper and the
interested reader is referred to M¨uller et al. [59] and
Fraley and Raftery [18] for alternative speciﬁcations.
In the one-dimensional setting we also considered
a conjugate prior so that we can compare the sam-
plers with and without component parameters mar-
ginalized out. We use a similar weakly informative
prior in the conjugate case as well, with base distri-

= N1(dm; m0, S0Σ−1

0 Σ)IW1(dΣ; α0, Σ0),

where the one-dimensional inverse Wishart with pa-
rameter (a, s) is simply an inverse gamma with pa-
rameter (a/2, s/2). We used the same α0 and hier-
archical prior for Σ0 as for the nonconjugate prior,
while the a priori expected value for the variance of
m can be seen to be E[S0Σ−1
0 Σ] = S0, which is inde-
pendent of Σ0 and matches the nonconjugate case.
In both cases we updated the Σ0 by Gibbs sampling.
The parameters a and τ of the NGGP are redun-
dant (see Section 2 for details), so we simply set
τ = 1 in the simulations. We place a gamma (1, 1)
prior on a, while σ is given a beta prior with param-
eters (1, 2). We can visualize the induced prior on
the partition structure by drawing samples of a and
σ from their prior and for each sample calculating
the mean and standard deviation of the prior over
the number of clusters. Figure 5 shows the result for
n = 82, corresponding to the size of the galaxy data
set. We see that the prior gives support over a wide
range of values for the mean and standard deviation
of the number of clusters, with higher probability
for the mean number of clusters to be in the region
between 1 and 20.

4.1 One-Dimensional Data Sets: Galaxy and

Acidity

In the conjugate case, we applied both the con-
jugate marginalized sampler of Section 3.1 and the
conditional slice sampler of Section 3.3 (but with
mixture component parameters marginalized out).
To investigate the diﬀerence between marginalizing

MCMC FOR NORMALIZED RANDOM MEASURE MIXTURE MODELS

17

Fig. 6. Visualizations of the posterior distribution of the nonconjugate NGGP mixture model on the galaxy data set. Top-left:
posterior mean and 95% credible interval (pointwise) of the density function. Top-right: co-clustering probabilities, whiskers
at edges denote observations. Bottom: histograms of the posteriors of σ, log(a) and log(β0), respectively.

out the component parameters and not, we also ap-
plied the generalization of Neal’s Algorithm 8 in Sec-
tion 3.2 and the Reuse algorithm of Section 3.2.2,
both with C ∈ {1, 2, 3, 4, 5} and the conditional slice
sampler to the conjugate model (sampling the pa-
rameters instead of marginalizing them out). In the
nonconjugate case we applied the conditional slice
sampler and the two nonconjugate marginalized sam-
plers with C ∈ {1, 2, 3, 4, 5}. For all samplers in both
conjugate and nonconjugate models, the initial
10,000 iterations were discarded as burn-in, followed
by 200,000 iterations, from which we collected 10,000
samples.

Figure 6 shows some aspects of the posterior dis-
tribution on the galaxy data set for the nonconju-
gate model obtained using the conditional slice sam-
pler, while Figure 7 shows the same for the acid-
ity data set. The marginalized samplers produce the
same results, while the posterior for the conjugate
model is similar and not shown. The co-clustering
probabilities are computed as follows: the color at lo-
cation (x, y) indicates the posterior probability that
observations Yi and Yj belong to the same compo-
nents, where Yi is the largest observed value smaller
than min(x, y) and Yj is the smallest observed value

larger than max(x, y). The posterior distribution of
the number of components used is shown in the top
half of Figure 8. The posterior distributions are con-
sistent with those obtained by previous authors, for
example, Richardson and Green [79], Escobar and
West [13], Griﬃn and Walker [27] and Roeder [80].
In Table 1 we compared the samplers in terms of
both their run times (in seconds, excluding time re-
quired to compute predictive probabilities) and their
eﬀective sample sizes (ESSs) of the number of com-
ponents (as computed using the R Coda package).
By marginalizing out the mixture component pa-
rameters, we see that the samplers mix more eﬀec-
tively with higher ESSs. The conditional slice sam-
pler and the nonconjugate marginalized samplers
were eﬀective at handling mixture component pa-
rameters that were sampled instead of marginalized
out, but the ESSs were a little lower, as expected.
Among the marginalized samplers, with increasing
C both the computational costs and the ESS gener-
ally increase, with the computational cost of Neal’s
Algorithm 8 increasing more rapidly, as expected.

While the conditional slice sampler is typically
faster than the marginalized samplers, they also pro-
duce lower ESSs. An important diﬀerence between

18

S. FAVARO AND Y. W. TEH

Fig. 7. Visualizations of the posterior distribution of the nonconjugate NGGP mixture model on the acidity data set. Top-left:
posterior mean and 95% credible interval (pointwise) of the density function. Top-right: co-clustering probabilities, whiskers
at edges denote observations. Bottom row: histograms of the posterior of σ, log(a) and log(β0), respectively.

Fig. 8. Top: Distribution of the number of components used, for the nonconjugate NGGP mixture model for the galaxy
(left) and the acidity (right) data sets, respectively. Bottom: distribution of the number of empty clusters instantiated by the
conditional slice sampler at each iteration (on the logarithmic scale) for the galaxy (left) and the acidity (right) data sets.

MCMC FOR NORMALIZED RANDOM MEASURE MIXTURE MODELS

19

Comparison of sampler eﬃciencies on the one-dimensional galaxy and acidity data sets. Each of 10 runs produces 10,000

samples, at intervals of 20 iterations, after an initial burn-in period of 10,000 iterations. Each entry reports the average and
standard error over the 10 runs. In the ﬁrst column, C indicates conjugate prior speciﬁcation, N for nonconjugate, while M

indicates component parameters are marginalized and S means they are sampled

Table 1

Model

CM
CM

CS
CS
CS
CS
CS
CS
CS
CS
CS
CS
CS

NS
NS
NS
NS
NS
NS
NS
NS
NS
NS
NS

Sampler

Runtime (s)

ESS

Runtime (s)

ESS

Galaxy

Acidity

Cond Slice

Marg (C = 1)

Cond Slice

Marg Neal 8 (C = 1)
Marg Neal 8 (C = 2)
Marg Neal 8 (C = 3)
Marg Neal 8 (C = 4)
Marg Neal 8 (C = 5)
Marg Reuse (C = 1)
Marg Reuse (C = 2)
Marg Reuse (C = 3)
Marg Reuse (C = 4)
Marg Reuse (C = 5)

Cond Slice

Marg Neal 8 (C = 1)
Marg Neal 8 (C = 2)
Marg Neal 8 (C = 3)
Marg Neal 8 (C = 4)
Marg Neal 8 (C = 5)
Marg Reuse (C = 1)
Marg Reuse (C = 2)
Marg Reuse (C = 3)
Marg Reuse (C = 4)
Marg Reuse (C = 5)

239.1 ± 4.2
215.7 ± 1.4

133.0 ± 3.2
74.4 ± 0.6
87.9 ± 0.6
101.9 ± 0.7
115.9 ± 0.6
130.0 ± 0.6
64.3 ± 0.3
67.6 ± 0.5
71.3 ± 0.5
74.9 ± 0.5
78.7 ± 0.6

75.5 ± 1.2
65.0 ± 0.5
78.6 ± 0.4
92.5 ± 0.5
106.3 ± 0.5
119.7 ± 0.6
55.2 ± 0.5
58.7 ± 0.5
62.4 ± 0.6
66.1 ± 0.5
69.8 ± 0.6

2004 ± 178
7809 ± 87

1594 ± 117
5815 ± 145
6292 ± 94
6320 ± 137
6283 ± 86
6491 ± 203
4451 ± 79
5554 ± 112
5922 ± 157
6001 ± 101
6131 ± 124

939 ± 92

4313 ± 172
4831 ± 168
4785 ± 97
4849 ± 120
5029 ± 89
3830 ± 103
4286 ± 101
4478 ± 124
4825 ± 63
4755 ± 141

196.5 ± 1.0
395.5 ± 1.7

77.4 ± 0.7
133.3 ± 1.8
163.8 ± 1.5
188.2 ± 1.1
216.6 ± 1.7
243.8 ± 2.0
114.6 ± 2.0
123.1 ± 1.9
128.2 ± 2.2
140.1 ± 1.6
147.7 ± 1.5

50.9 ± 0.5
110.9 ± 0.8
139.2 ± 1.8
162.7 ± 0.9
187.6 ± 1.1
215.4 ± 1.3
91.3 ± 0.9
98.1 ± 0.9
105.1 ± 0.9
112.3 ± 1.0
121.0 ± 1.8

910 ± 142
5236 ± 181

1099 ± 49
4175 ± 85
4052 ± 158
4241 ± 99
4266 ± 122
4453 ± 123
3751 ± 65
4475 ± 110
4439 ± 158
4543 ± 108
4585 ± 116

949 ± 70
4144 ± 64
4290 ± 125
4368 ± 72
4234 ± 142
4144 ± 213
4007 ± 122
4192 ± 138
4260 ± 136
4191 ± 139
4186 ± 121

the slice sampler and the marginalized samplers is
that the slice sampler we proposed uses one slice
variable per observation, so typically a signiﬁcantly
smaller number of components are considered at each
update of the cluster assignment variables, and thus
the algorithm is faster and has lower ESSs. If a single
slice variable is used instead, as proposed in Griﬃn
and Walker [27], or if a nonslice conditional sampler
like Barrios et al. [2] is used, then all instantiated
components will be considered at each update. This
can result in not only higher ESSs but also higher
computational overheads since the number of empty
components introduced can be very large. The bot-
tom half of Figure 8 shows the distribution of the
number of empty components for one of the runs
for the nonconjugate case (on the logarithmic scale).
The mean numbers of empty components are 76.6
and 31.4 for the galaxy and acidity data sets, respec-
tively. Other runs and the conjugate case are simi-
lar and not shown. For comparison, the top panels of

Figure 8 show the posterior distribution of the num-
ber of nonempty components, which are smaller. As
a further note, we have found that the truncation of
the slice variables at 10−8 described in Section 3.3 is
essential to the program working properly, as other-
wise it will sometimes generate far too many atoms,
causing the program to run out of memory. Table 2
shows the number of times the truncation came into
eﬀect during each MCMC run. We did not ﬁnd cases
in which the 106 limit on the number of atoms was
reached among these runs.

4.2 Multidimensional Data Sets: Geyser and

Spikes

We have also explored the eﬃcacies of the algo-
rithms on the geyser and spikes data sets. For the
spikes data set we reduced the size of the data set
by randomly selecting 500 spike waveforms to re-
duce the overall computation time for the exper-
iments. In preliminary experiments this does not

20

S. FAVARO AND Y. W. TEH

aﬀect the qualitative conclusions drawn from the
results. We did not include the generalization of
Neal’s Algorithm 8 in these experiments, as we have
found in initial explorations that it took signiﬁcantly
more computation time without producing substan-
tially higher ESSs than the Reuse algorithm. The
setups of the experiments are similar as for the one-
dimensional setting, with each algorithm produc-
ing ten independent runs, each consisting of 10,000
burn-in iterations followed by 10,000 samples col-
lected at intervals of 20 iterations. In addition to
C = 1, . . . , 5, we also explored higher values of C =
10, 15 and 20.

The run times and ESSs are reported in Table 3.
The trends observed for the one-dimensional setting
hold here as well: that the slice sampler is faster but
produces lower ESSs, and that with increasing C
the marginalized sampler produces higher ESSs at
higher computational costs. As expected, the algo-
rithms mix more slowly on the higher-dimensional
spikes data set, with signiﬁcantly lower ESSs. For
the spikes data set the nonconjugate marginalized
samplers with higher values of C have signiﬁcantly

Table 2

Average number of times the slice threshold S was less than

the 10−8 truncation level over the 10 conditional slice

sampling runs. The total number of iterations of each run is
210,000. Each entry reports the average and standard error

over 10 runs

Model

Galaxy

Acidity

Geyser

Spikes

CM
CS
NS

4476 ± 440
4597 ± 385
3712 ± 222

6143 ± 1148
4385 ± 394
8017 ± 1180

–
–

–
–

15,180 ± 980

5621 ± 475

higher ESSs. In fact, they had better ESSs per unit
of run time than for lower values of C or for the slice
sampler. This contrasts with the other simpler data
sets, where lower values of C worked very well, prob-
ably because the additional complexity of higher C
values was not needed. Figure 9 shows the poste-
rior distributions over the number of clusters, log(a)
and σ.

Finally, we illustrate the clustering structure among

spike waveforms discovered by the NGGP mixture
model. 2000 spike waveforms were selected at ran-
dom from the data set and the Reuse algorithm
with C = 20 is run as before, with 10,000 burn-in
iterations followed by 10,000 samples collected ev-
ery 20 iterations. We use co-clustering probabilities
to summarize the clustering structure. For each pair
(i, j) of spikes let pij be the (estimated) posterior
probability that the two spikes were assigned to the
same cluster. We use average linkage to organize the
spikes into a hierarchy, where the distance between
spikes i and j is deﬁned to be 1 − pij. This is then
used to reorder the co-clustering matrix. The hier-
archy and reordered matrix are shown on the upper
panels of Figure 10. We see that most spikes belong
to six large clusters, two of which have signiﬁcant
overlap and merged into one, while a subset of wave-
forms formed smaller clusters which may or may not
overlap with other clusters. In the bottom panels of
Figure 10 we visualize the various clusters found by
thresholding the hierarchy at 0.95 and ignoring clus-
ters of size less than 10.

We can interpret the clusters found here in the
context of spike sorting, an important process in
experimental neuroscience of detecting spikes from
neural recordings and determining the neuron cor-

Comparison of sampler eﬃciencies on the geyser (2D) and spikes (6D) data sets. Each of 10 runs produces 10,000 samples,
at intervals of 20 iterations, after an initial burn-in period of 10,000 iterations. Each entry reports the average and standard

error over the 10 runs

Table 3

Model

NS
NS
NS
NS
NS
NS
NS
NS
NS

Sampler

Cond Slice

Marg Reuse (C = 1)
Marg Reuse (C = 2)
Marg Reuse (C = 3)
Marg Reuse (C = 4)
Marg Reuse (C = 5)
Marg Reuse (C = 10)
Marg Reuse (C = 15)
Marg Reuse (C = 20)

Geyser

Spikes

Runtime (s)

ESS

Runtime (s)

ESS

142.6 ± 1.1
208.0 ± 1.3
225.3 ± 1.4
241.5 ± 1.3
257.7 ± 1.7
274.8 ± 1.7
356.3 ± 2.5
446.6 ± 4.9
550.4 ± 3.5

574 ± 36

2770 ± 209
3236 ± 73
3148 ± 71
3291 ± 145
3144 ± 70
3080 ± 135
3312 ± 154
3336 ± 109

732.6 ± 8.1
1120.3 ± 8.8
1164.5 ± 5.4
1204.1 ± 7.3
1238.5 ± 7.8
1291.8 ± 7.9
1513.8 ± 11.9
1746.3 ± 10.7
1944.0 ± 14.7

17.1 ± 2.3
35.7 ± 2.4
46.9 ± 2.9
57.0 ± 3.9
61.4 ± 3.3
69.8 ± 4.9
90.8 ± 5.6
95.9 ± 4.2
114.5 ± 8.4

MCMC FOR NORMALIZED RANDOM MEASURE MIXTURE MODELS

21

Fig. 9. Histograms of the posterior distribution of σ, log(a) and the number of clusters, respectively, for the spikes data.

Fig. 10. Top: hierarchical organization of spike waveforms obtained by average linkage and the corresponding reordered
co-clustering matrix. Bottom: clusters found by thresholding at 0.95. Each panel consists of four subpanels, each corresponding
to the waveforms recorded by an electrode. Each waveform in the cluster is plotted in light grey and their mean in dark grey.

22

S. FAVARO AND Y. W. TEH

responding to each spike from the shape of its wave-
form (as well as the number of neurons) using a va-
riety of manual or automated clustering techniques,
with each cluster interpreted as a unique neuron.
See Quiroga [73] and Lewicki [44] for reviews of spike
sorting methods, and also G¨or¨ur and Rasmussen [23],
Wood and Black [86] and Gasthaus et al. [20] for
Bayesian nonparametric mixture modeling ap-
proaches to spike sorting. We ﬁnd that the 5 largest
clusters found (1, 4, 6, 7 and 8) all correspond to
well-deﬁned waveforms with distinctive shapes, and
expect each of 1, 4, 6 and 8 to correspond to a sin-
gle neuron. Spikes in cluster 5 have similar wave-
forms, as 6 and the two clusters are in fact merged
at a threshold of 0.99, though spikes in 5 lacked re-
fractory periods; they may either correspond to the
same or distinct neurons. Clusters 2 and 3 consist
of outliers, false detections or waveforms formed by
the superposition of two consecutive spikes. We note
that a number of waveforms in other clusters are also
superpositions as well. Finally, analyzing the two
subclusters of 7, we see that although their shapes
are very similar, the waveforms in the ﬁrst two sub-
panels of 7a seem to be slightly smaller than those
in 7b, though it is unclear if the subclustering is due
to two neurons or is an artefact of the mixture com-
ponents not being ﬂexible enough to capture spike
waveform variability.

The approach taken here is simply to use an ag-
glomerative linkage algorithm to help us visualize
and explore the posterior over partition structures
under the mixture model. An alternative approach
is to summarize the posterior using a single parti-
tion, for example, using the maximum a posteriori
partition or one that minimizes the posterior expec-
tation of a loss function like Binder’s loss. The issue
of how best to analyze and interpret the posterior
partition structure of Bayesian models for clustering
is still an open question and beyond the scope of this
paper. We refer the interested reader to Binder [3],
Medvedovic and Sivaganesan [56], Dahl [8], Lau and
Green [43], Fritsch and Ickstadt [19] and Rasmussen
et al. [76] for classical and recent eﬀorts in this re-
gard.

5. DISCUSSION

NRMs provide a large class of ﬂexible nonpara-
metric priors beyond the standard DP, but their
more common use is currently hindered by a lack
of understanding and of good algorithms for pos-
terior simulation. This work provides a review of
NRMs for easy access to the extensive literature, as

well as novel algorithms for posterior simulation that
are eﬃcient and easy to use. We will also provide
open source Java software implementing all four al-
gorithms described in Section 3 so that others might
more easily explore them.

All the samplers proposed in this paper are ba-
sic samplers that make changes to the cluster as-
signment of one observation at a time. Samplers
that make more complex changes, for example, those
based on split-merge Metropolis–Hastings moves by
Jain and Neal [32], can be signiﬁcantly more eﬃcient
at exploring multiple posterior modes. Such sam-
plers can be derived in both marginalized and con-
ditional forms, using the characterizations reviewed
in this paper, and are an interesting avenue of future
research. Beyond the algorithms described in Sec-
tion 3, there are many variants possible with both
marginalized and conditional samplers for NRM mix-
ture models. While conditional samplers have been
well explored in the literature, ours are the ﬁrst
tractable marginalized samplers for mixture mod-
els with a homogeneous NRM prior. In addition,
a number of samplers based on the system of pre-
dictive distributions of NRMs have been proposed
by James et al. [36] and by Lijoi et al. [46–48], but
these sampling methods can be computationally ex-
pensive in the nonconjugate setting due to numerical
integrations needed for computing the probabilities
associated to new clusters, and convergence is slow,
requiring additional acceleration steps. See, for ex-
ample, Bush and MacEachern [7] for details.

A random probability measure that is in popular
use but conspicuously not within the class of NRMs
is the two-parameter Poisson–Dirichlet process (oth-
erwise known as the Pitman–Yor process) by Per-
man et al. [69]. See also Pitman and Yor [72] and
Ishwaran and James [31] for details. It is instead in
an even larger class known as the Poisson–Kingman
processes introduced by Pitman [70], which are ob-
tained by allowing the total mass of the otherwise
completely random measure underlying the NRM
to have a diﬀerent distribution. Poisson–Kingman
processes represent the largest known class of ran-
dom probability measures that are still mathemati-
cally tractable. In addition to NRMs, they also in-
clude random probability measures induced by the
so-called Gibbs type exchangeable random partitions
introduced by Gnedin and Pitman [22]. The marginal-
ized and conditional samplers we have developed
may be extended to the Poisson–Kingman processes
as well.

MCMC FOR NORMALIZED RANDOM MEASURE MIXTURE MODELS

23

Throughout this paper we have used the NGGP
as a running example to illustrate the various prop-
erties and formulae, because of its tractability and
because it includes many well-known NRMs as ex-
amples. It has been shown by Lijoi et al. [50] that
the NGGP is the only NRM that is also of Gibbs
type. Beyond the NGGP, the formulae derived tend
to become intractable and require numerical inte-
grations. A notable exception is the class of NRMs
whose L´evy intensity measure are mixtures of those
for the generalized Gamma CRM, ﬁrst proposed by
Trippa and Favaro [84] who also showed that they
form a dense subclass of the NRMs. It is straight-
forward to extend the algorithms and the software
derived in this paper to this larger class.

As a ﬁnal remark, the study of random probabil-
ity measures underpins a large body of work span-
ning probability, statistics, combinatorics and math-
ematical genetics. They also form the core of many
Bayesian nonparametric models that are increasingly
popular in applied statistics and machine learning.
By expanding the class of tractable random proba-
bility measures beyond the DP to NRMs, we hope
that our work will increase both the range and ﬂex-
ibility of the models in use now and in the future.

ACKNOWLEDGMENTS

The authors are grateful to the Editor, an Asso-
ciate Editor and two anonymous referees for their
constructive comments and suggestions. This work
was supported by the European Research Council
(ERC) through StG “N-BNP” 306406.

REFERENCES

[1] Aldous, D. J. (1985). Exchangeability and related top-
ics. In ´Ecole D’´et´e de Probabilit´es de Saint-Flour,
XIII—1983. Lecture Notes in Math. 1117 1–198.
Springer, Berlin. MR0883646

[2] Barrios, E., Lijoi, A., Nieto-Barajas, L. E. and
Pr¨uenster, I. (2012). Modeling with normal-
ized random measure mixture models. Unpublished
manuscript.
[3] Binder, D. A.

(1978). Bayesian cluster analysis.

Biometrika 65 31–38. MR0501592

[4] Blackwell, D. and MacQueen, J. B. (1973). Fer-
guson distributions via P´olya urn schemes. Ann.
Statist. 1 353–355. MR0362614

[5] Brix, A. (1999). Generalized gamma measures and shot-
noise Cox processes. Adv. in Appl. Probab. 31 929–
953. MR1747450

[6] Broderick, T., Jordan, M. I. and Pitman, J. (2012).
Clusters and features from combinatorial stochastic
processes. Available at arXiv:1206.5862 [math.ST].

[7] Bush, C. A. and MacEachern, S. N. (1996). A semi-
parametric Bayesian model for randomised block
designs. Biometrika 83 275–285.

[8] Dahl, D. B. (2006). Model-based clustering for expres-
sion data via a Dirichlet process mixture model. In
Bayesian Inference for Gene Expression and Pro-
teomics (K. Do, P. M¨uller and M. Vannucci,
eds.). Cambridge Univ. Press, Cambridge.

[9] Daley, D. J. and Vere-Jones, D. (2002). An Intro-
duction to the Theory of Point Processes. Springer,
New York. MR0950166

[10] Diebolt, J. and Robert, C. P. (1994). Estimation of
ﬁnite mixture distributions through Bayesian sam-
pling. J. R. Stat. Soc. Ser. B Stat. Methodol. 56
363–375. MR1281940

[11] Escobar, M. D. (1988). Estimating the means of several
normal populations by nonparametric estimation of
the distribution of the means. Ph.D. thesis, Yale
Univ. MR2637324

[12] Escobar, M. D. (1994). Estimating normal means with
a Dirichlet process prior. J. Amer. Statist. Assoc.
89 268–277. MR1266299

[13] Escobar, M. D. and West, M. (1995). Bayesian
density estimation and inference using mixtures.
J. Amer. Statist. Assoc. 90 577–588. MR1340510

[14] Ewens, W. J. (1972). The sampling theory of selectively
neutral alleles. Theoret. Population Biology 3 87–
112; erratum, ibid. 3 (1972), 240, 376. MR0325177
[15] Favaro, S. and Walker, S. G. (2013). Slice sampling σ-
stable Poisson–Kingman mixture models. J. Com-
put. Graph. Statist. To appear.

[16] Ferguson, T. S. (1973). A Bayesian analysis of some
nonparametric problems. Ann. Statist. 1 209–230.
MR0350949

[17] Ferguson, T. S. and Klass, M. J. (1972). A represen-
tation of independent increment processes without
Gaussian components. Ann. Math. Statist. 43 1634–
1643. MR0373022

[18] Fraley, C. and Raftery, A. E. (2007). Bayesian
regularization for normal mixture estimation and
model-based clustering. J. Classiﬁcation 24 155–
181. MR2415725

[19] Fritsch, A. and Ickstadt, K. (2009). Improved crite-
ria for clustering based on the posterior similarity
matrix. Bayesian Anal. 4 367–391. MR2507368

[20] Gasthaus, J., Wood, F., G¨or¨ur, D. and Teh, Y. W.
(2009). Dependent Dirichlet process spike sorting.
In Advances in Neural Information Processing Sys-
tems 21 497–504.

[21] Gilks, W. R. and Wild, P. (1992). Adaptive rejection
sampling for Gibbs sampling. Appl. Statist. 41 337–
348.

[22] Gnedin, A. and Pitman, J. (2006). Exchangeable Gibbs
partitions and Stirling triangles. J. Math. Sci. 138
5674–5684.

[23] G¨or¨ur, D., Rasmussen, C. E., Tolias, A. S., Sinz, F.
and Logothetis, N. K. (2004). Modelling spikes
with mixtures of factor analysers. In Proceedings of
the Conference of the German Association for Pat-
tern Recognition (DAGM).

24

S. FAVARO AND Y. W. TEH

[24] Green, P. J. (1995). Reversible jump Markov chain
Monte Carlo computation and Bayesian model de-
termination. Biometrika 82 711–732. MR1380810

[25] Green, P. J. and Richardson, S. (2001). Modelling
heterogeneity with and without the Dirichlet pro-
cess. Scand. J. Stat. 28 355–375. MR1842255

[26] Griffin, J. E., Kolossiatis, M. and Steel, M. F. J.
(2013). Comparing distributions using dependent
normalized random measure mixtures. J. R. Stat.
Soc. Ser. B Stat. Methodol. 75 499–529.

[27] Griffin, J. E. and Walker, S. G. (2011). Pos-
terior simulation of normalized random measure
mixtures. J. Comput. Graph. Statist. 20 241–259.
MR2816547

[28] Griffiths, T. L. and Ghahramani, Z. (2011). The In-
dian buﬀet process: An introduction and review.
J. Mach. Learn. Res. 12 1185–1224. MR2804598

[29] Hjort, N. L. (1990). Nonparametric Bayes estimators
based on beta processes in models for life history
data. Ann. Statist. 18 1259–1294. MR1062708

[30] Hjort, N. L., Holmes, C., M¨uller, P. and Wal-
ker, S. G., eds. (2010). Bayesian Nonparamet-
rics. Cambridge Series in Statistical and Probabilis-
tic Mathematics 28. Cambridge Univ. Press, Cam-
bridge. MR2722987

[31] Ishwaran, H. and James, L. F. (2001). Gibbs sampling
methods for stick-breaking priors. J. Amer. Statist.
Assoc. 96 161–173. MR1952729

[32] Jain, S. and Neal, R. M. (2000). A split-merge Markov
chain Monte Carlo procedure for the Dirichlet pro-
cess mixture model. Unpublished manuscript.

[33] James, L. F. (2002). Poisson process partition cal-
culus with applications
to exchangeable mod-
els and Bayesian nonparametrics. Available at
arXiv:math/0205093v1.

[34] James, L. F. (2003). A simple proof of the almost sure
discreteness of a class of random measures. Statist.
Probab. Lett. 65 363–368. MR2039881

[35] James, L. F., Lijoi, A. and Pr¨unster, I. (2006). Con-
jugacy as a distinctive feature of the Dirichlet pro-
cess. Scand. J. Stat. 33 105–120. MR2255112

[36] James, L. F., Lijoi, A. and Pr¨unster, I. (2009). Poste-
rior analysis for normalized random measures with
independent increments. Scand. J. Stat. 36 76–97.
MR2508332

[37] James, L. F., Lijoi, A. and Pr¨unster, I. (2010).
On the posterior distribution of classes of random
means. Bernoulli 16 155–180. MR2648753

[38] Jasra, A., Holmes, C. C. and Stephens, D. A. (2005).
Markov chain Monte Carlo methods and the label
switching problem in Bayesian mixture modeling.
Statist. Sci. 20 50–67. MR2182987

[39] Kalli, M., Griffin, J. E. and Walker, S. G. (2011).
Slice sampling mixture models. Stat. Comput. 21
93–105. MR2746606

[40] Kingman, J. F. C. (1967). Completely random mea-

sures. Paciﬁc J. Math. 21 59–78. MR0210185

[41] Kingman, J. F. C. (1993). Poisson Processes. Oxford
Studies in Probability 3. Clarendon Press, Oxford.
MR1207584

[42] Kingman, J. F. C., Taylor, S. J., Hawkes, A. G.,
Walker, A. M., Cox, D. R., Smith, A. F. M.,
Hill, B. M., Burville, P. J. and Leonard, T.
(1975). Random discrete distribution. J. R. Stat.
Soc. Ser. B Stat. Methodol. 37 1–22. MR0368264

[43] Lau, J. W. and Green, P. J. (2007). Bayesian model-
based clustering procedures. J. Comput. Graph.
Statist. 16 526–558. MR2351079

[44] Lewicki, M. S. (1998). A review of methods for spike
sorting: The detection and classiﬁcation of neural
action potentials. Network 9 53–78.

[45] Lewis, P. A. W. and Shedler, G. S. (1979). Simulation
of nonhomogeneous Poisson processes by thinning.
Naval Res. Logist. Quart. 26 403–413. MR0546120
[46] Lijoi, A., Mena, R. H. and Pr¨unster, I. (2005).
Bayesian nonparametric analysis for a generalized
Dirichlet process prior. Stat. Inference Stoch. Pro-
cess. 8 283–309. MR2177315

[47] Lijoi, A., Mena, R. H. and Pr¨unster, I. (2005). Hier-
archical mixture modeling with normalized inverse-
Gaussian priors. J. Amer. Statist. Assoc. 100 1278–
1291. MR2236441

[48] Lijoi, A., Mena, R. H. and Pr¨unster, I. (2007).
Controlling the reinforcement in Bayesian non-
parametric mixture models. J. R. Stat. Soc. Ser.
B Stat. Methodol. 69 715–740. MR2370077

[49] Lijoi, A. and Pr¨unster, I. (2010). Models beyond
the Dirichlet process. In Bayesian Nonparametrics
(N. L. Hjort, C. C. Holmes, P. M¨uller and
S. G. Walker, eds.) 80–136. Cambridge Univ.
Press, Cambridge. MR2730661

[50] Lijoi, A., Pr¨unster, I. and Walker, S. G. (2008). In-
vestigating nonparametric priors with Gibbs struc-
ture. Statist. Sinica 18 1653–1668. MR2469329

[51] Lo, A. Y. (1984). On a class of Bayesian nonparamet-
ric estimates. I. Density estimates. Ann. Statist. 12
351–357. MR0733519

[52] MacEachern, S. N. (1994). Estimating normal means
with a conjugate style Dirichlet process prior.
Comm. Statist. Simulation Comput. 23 727–741.
MR1293996

[53] MacEachern, S. N. (1998). Computational methods
for mixture of Dirichlet process models. In Prac-
tical Nonparametric and Semiparametric Bayesian
Statistics (D. Dey, P. M¨uller and D. Sinha,
eds.). Lecture Notes in Statist. 133 23–43. Springer,
New York. MR1630074

[54] MacEachern, S. N. and M¨uller, P. (1998). Estimat-
ing mixture of Dirichlet process models. J. Comput.
Graph. Statist. 7 223–238.

[55] McLachlan, G. J. and Basford, K. E. (1988). Mix-
ture Models: Inference and Applications to Clus-
tering. Statistics: Textbooks and Monographs 84.
Dekker, New York. MR0926484

[56] Medvedovic, M.

and Sivaganesan, S.

(2002).
Bayesian inﬁnite mixture model based clustering
of gene expression proﬁles. Bioinformatics 18 1194–
1206.

[57] Mengersen, K. L. and Robert, C. P.

(1996).
Testing for mixtures: A Bayesian entropic ap-

MCMC FOR NORMALIZED RANDOM MEASURE MIXTURE MODELS

25

proach. In Bayesian Statistics, 5 (Alicante, 1994)
(J. O. Berger, J. M. Bernardo, A. P. Dawid,
D. V. Lindley and A. F. M. Smith, eds.) 255–276.
Oxford Univ. Press, New York. MR1425410

[58] Muliere, P. and Tardella, L. (1998). Approximating
distributions of random functionals of Ferguson–
Dirichlet priors. Canad. J. Statist. 26 283–297.
MR1648431

[59] M¨uller, P., Erkanli, A. and West, M. (1996).
Bayesian curve ﬁtting using multivariate normal
mixtures. Biometrika 83 67–79. MR1399156

[60] Neal, R. M. (1992). Bayesian mixture modeling. In
Proceedings of the 11th International Workshop on
Maximum Entropy and Bayesian Methods of Statis-
tical Analysis, Seattle. Kluwer, Dordrecht.

[61] Neal, R. M. (2000). Markov chain sampling methods
for Dirichlet process mixture models. J. Comput.
Graph. Statist. 9 249–265. MR1823804

[62] Neal, R. M. (2003). Slice sampling. Ann. Statist. 31

705–767. MR1994729

[64] Nieto-Barajas, L. E., Pr¨unster,

[63] Nieto-Barajas, L. E. and Pr¨unster, I. (2009). A sen-
sitivity analysis for Bayesian nonparametric density
estimators. Statist. Sinica 19 685–705. MR2514182
I. and Wal-
ker, S. G. (2004). Normalized random mea-
sures driven by increasing additive processes. Ann.
Statist. 32 2343–2360. MR2153987

[65] Nobile, A. (1994). Bayesian analysis of ﬁnite mixture
distributions. Ph.D. thesis, Carnegie Mellon Univ.
MR2692049

[66] Ogata, Y. (1981). On Lewis’ simulation method for
Point processes. IEEE Trans. Inform. Theory 27
23–31.

[67] Papaspiliopoulos, O. (2008). A note on posterior sam-
pling from Dirichlet mixture models. Working Pa-
per 20, Centre for Research in Statistical Method-
ology, Univ. Warwick.

[68] Papaspiliopoulos, O. and Roberts, G. O. (2008).
Retrospective Markov chain Monte Carlo meth-
ods
for Dirichlet process hierarchical models.
Biometrika 95 169–186. MR2409721

[69] Perman, M., Pitman, J. and Yor, M. (1992). Size-
biased sampling of Poisson point processes and ex-
cursions. Probab. Theory Related Fields 92 21–39.
MR1156448

[70] Pitman, J. (2003). Poisson–Kingman partitions. In
Statistics and Science: A Festschrift for Terry Speed
(D.R. Goldstein, ed.). Institute of Mathematical
Statistics Lecture Notes—Monograph Series 40 1–
34. IMS, Beachwood, OH. MR2004330

[71] Pitman, J. (2006). Combinatorial Stochastic Processes.
Lecture Notes in Math. 1875. Springer, Berlin.
MR2245368

[72] Pitman, J. and Yor, M. (1997). The two-parameter
Poisson–Dirichlet distribution derived from a

stable subordinator. Ann. Probab. 25 855–900.
MR1434129

[73] Quiroga, R. Q. (2007). Spike sorting. Scholarpedia 2

3583.

[74] Raftery, A. E. (1996). Hypothesis testing and model
selection. In Markov Chain Monte Carlo in Practice
(W. R. Gilks, S. Richardson and D. J. Spiegel-
halter, eds.). Chapman & Hall, London.

[75] Raftery, A. E. (1996). Hypothesis testing and model
selection via posterior
In Markov
Chain Monte Carlo in Practice (W. R. Gilks,
S. Richardson and D. J. Spiegelhalter, eds.).
Chapman & Hall, London.

simulation.

[76] Rasmussen, C. E., De la Cruz, B. J., Ghahra-
mani, Z. and Wild, D. L. (2009). Modeling and vi-
sualizing uncertainty in gene expression clusters us-
ing Dirichlet process mixtures. IEEE/ACM Trans.
Comput. Biol. and Bioinform. 6 615–628.

[77] Rasmussen, C. E. and Williams, C. K. I. (2006).
Gaussian Processes for Machine Learning. MIT
Press, Cambridge, MA. MR2514435

[78] Regazzini, E., Lijoi, A. and Pr¨unster, I. (2003).
Distributional results for means of normalized ran-
dom measures with independent increments. Ann.
Statist. 31 560–585. MR1983542

[79] Richardson, S. and Green, P. J. (1997). On Bayesian
analysis of mixtures with an unknown number of
components. J. R. Stat. Soc. Ser. B Stat. Methodol.
59 731–792. MR1483213

[80] Roeder, K. (1994). A graphical technique for deter-
mining the number of components in a mixture
of normals. J. Amer. Statist. Assoc. 89 487–495.
MR1294074

[81] Roeder, K. and Wasserman, L. (1997). Practical
Bayesian density estimation using mixtures of
normals. J. Amer. Statist. Assoc. 92 894–902.
MR1482121

[82] Stephens, M. (2000). Bayesian analysis of mixture mod-
els with an unknown number of components—an al-
ternative to reversible jump methods. Ann. Statist.
28 40–74. MR1762903

[83] Titterington, D. M., Smith, A. F. M. and Makov,
U. E. (1985). Statistical Analysis of Finite Mixture
Distributions. Wiley, Chichester. MR0838090

[84] Trippa, L. and Favaro, S. (2012). A class of normalized
random measures with an exact predictive sampling
scheme. Scand. J. Stat. 39 444–460. MR2971631

[85] Walker, S. G. (2007). Sampling the Dirichlet mixture
model with slices. Comm. Statist. Simulation Com-
put. 36 45–54. MR2370888

[86] Wood, F. and Black, M. J. (2008). A nonparamet-
ric Bayesian alternative to spike sorting. Journal of
Neuroscience Methods 173 1–12.

