IEEE Copyright Statement: 
 
Copyright © [2004] IEEE.   Reprinted from IEEE Transactions on Automatic Control, Special 
Issue on Networked Control Systems.  September 2004.   
 
This material is posted here with permission of the IEEE. Such permission of the IEEE does not 
in any way imply IEEE endorsement of any of Carnegie Mellon University's products or services.  
Internal or personal use of this material is permitted.  However, permission to reprint/republish 
this material for advertising or promotional purposes or for creating new collective works for 
resale or redistribution must be obtained from the IEEE by writing to pubs-permissions@ieee.org. 
 
By choosing to view this document, you agree to all provisions of the copyright laws protecting it. 
 

1

Kalman Filtering with Intermittent

Observations

Bruno Sinopoli, Luca Schenato, Massimo Franceschetti,

Kameshwar Poolla, Michael I. Jordan, Shankar S. Sastry

Department of Electrical Engineering and Computer Sciences

University of California at Berkeley

{sinopoli, massimof, lusche, sastry}@eecs.berkeley.edu

poolla@me.berkeley.edu, jordan@cs.berkeley.edu

Abstract

Motivated by navigation and tracking applications within sensor networks, we consider the prob-

lem of performing Kalman ﬁltering with intermittent observations. When data travel along unreliable

communication channels in a large, wireless, multi-hop sensor network, the effect of communication

delays and loss of information in the control loop cannot be neglected. We address this problem starting

from the discrete Kalman ﬁltering formulation, and modelling the arrival of the observation as a random

process. We study the statistical convergence properties of the estimation error covariance, showing the

existence of a critical value for the arrival rate of the observations, beyond which a transition to an

unbounded state error covariance occurs. We also give upper and lower bounds on this expected state

error covariance.

This research is partially supported by DARPA under grant F33615-01-C-1895.

DRAFT

2

I. INTRODUCTION

Advances in VLSI and MEMS technology have boosted the development of micro sensor

integrated systems. Such systems combine computing, storage, radio technology, and energy

source on a single chip [1] [2]. When distributed over a wide area, networks of sensors can

perform a variety of tasks that range from environmental monitoring and military surveillance,

to navigation and control of a moving vehicle [3] [4] [5]. A common feature of these systems

is the presence of signiﬁcant communication delays and data loss across the network. From the

point of view of control theory, signiﬁcant delay is equivalent to loss, as data needs to arrive

to its destination in time to be used for control. In short, communication and control become

tightly coupled such that the two issues cannot be addressed independently.

Consider, for example, the problem of navigating a vehicle based on the estimate from a sensor

web of its current position and velocity. The measurements underlying this estimate can be lost

or delayed due to the unreliability of the wireless links. What is the amount of data loss that the

control loop can tolerate to reliably perform the navigation task? Can communication protocols

be designed to satisfy this constraint? At Berkeley, we have faced these kind of questions in

building sensor networks for pursuit evasion games as part of the Network of Embedded Systems

Technology (NEST) project [2]. Practical advances in the design of these systems are described

in [6]. The goal of this paper is to examine some control-theoretic implications of using sensor

networks for control. These require a generalization of classical control techniques that explicitly

take into account the stochastic nature of the communication channel.

In our setting, the sensor network provides observed data that are used to estimate the state of a

controlled system, and this estimate is then used for control. We study the effect of data losses due

to the unreliability of the network links. We generalize the most ubiquitous recursive estimation

technique in control—the discrete Kalman ﬁlter [7]—modelling the arrival of an observation

as a random process whose parameters are related to the characteristics of the communication

channel, see Figure 1. We characterize the statistical convergence of the expected estimation

error covariance in this setting.

The classical theory relies on several assumptions that guarantee convergence of the Kalman

DRAFT

3

Fig. 1. Overview of the system. We study the statistical convergence of the expected estimation error covariance of the discrete

Kalman ﬁlter, where the observation, travelling over an unreliable communication channel, can be lost at each time step with
probability 1 − λ.

ﬁlter. Consider the following discrete time linear dynamical system:

xt+1 = Axt + wt

yt = Cxt + vt,

(1)
where xt ∈ (cid:60)n is the state vector, yt ∈ (cid:60)m the output vector, wt ∈ (cid:60)p and vt ∈ (cid:60)m are Gaussian
random vectors with zero mean and covariance matrices Q ≥ 0 and R > 0, respectively. wt
is independent of ws for s < t. Assume that the initial state, x0, is also a Gaussian vector of
zero mean and covariance Σ0. Under the hypothesis of stabilizability of the pair (A, Q) and
detectability of the pair (A, C), the estimation error covariance of the Kalman ﬁlter converges

to a unique value from any initial condition [8].

These assumptions have been relaxed in various ways [8]. Extended Kalman ﬁltering attempts

to cope with nonlinearities in the model; particle ﬁltering is also appropriate for nonlinear

DRAFT

SystemKalman FilterMz-1Mz-1+++-4

models and additionally does not require the noise model to be Gaussian. Recently, more

general observation processes have been studied. In particular, in [9], [10] the case in which

observations are randomly spaced in time according to a Poisson process has been studied, where

the underlying dynamics evolve in continuous time. These authors showed the existence of a

lower bound on the arrival rate of the observations below which it is possible to maintain the

estimation error covariance below a ﬁxed value, with high probability. The results were restricted

to scalar SISO systems.

We approach a similar problem within the framework of discrete time, and provide results

for general n-dimensional MIMO systems. In particular, we consider a discrete-time system in

which the arrival of an observation is a Bernoulli process with parameter 0 < λ < 1, and,

rather than asking for the estimation error covariance to be bounded with high probability, we

study the asymptotic behavior (in time) of its average. Our main contribution is to show that,

depending on the eigenvalues of the matrix A, and on the structure of the matrix C, there exists

a critical value λc, such that if the probability of arrival of an observation at time t is λ > λc,
then the expectation of the estimation error covariance is always ﬁnite (provided that the usual
stabilizability and detectability hypotheses are satisﬁed). If λ ≤ λc, then the expectation of the
estimation error covariance tends to inﬁnity. We give explicit upper and lower bounds on λc,
and show that they are tight in some special cases.

Philosophically this result can be seen as another manifestation of the well known uncertainty

threshold principle [11], [12]. This principle states that optimum long-range control of a dy-

namical system with uncertainty parameters is possible if and only if the uncertainty does not

exceed a given threshold. The uncertainty is modelled as white noise scalar sequences acting

on the system and control matrices. In our case, the result is for optimal estimation, rather than

optimal control, and the uncertainty is due to the random arrival of the observation, with the

randomness arising from losses in the network.

Studies on ﬁltering with intermittent observations can be tracked back to Nahi [13] and Hadidi

[14]. More recently, this problem has been studied using jump linear systems (JLS) [15]. JLS

are stochastic hybrid systems characterized by linear dynamics and discrete regime transitions

modelled as Markov chains. In the work of Costa et al. [16] and Nilsson et al. [17], [18], the

Kalman ﬁlter with missing observations is modelled as a JLS switching between two discrete

regimes: an open loop conﬁguration and a closed loop one. Following this approach, these authors

DRAFT

5

obtain convergence criteria for the expected estimation error covariance. However, they restrict

their formulation to the steady state case, where the Kalman gain is constant, and they do not

assume to know the switching sequence. The resulting process is wide sense stationary [19], and

this makes the exact computation of the transition probability and state error covariance possible.

Other work on optimal, constant gain ﬁltering was done by Wang et al. [20], who included the

presence of system parameters uncertainty besides missing observations, and Smith et al. [21],

who considered multiple ﬁlters fusion. Instead, we consider the general case of time varying

Kalman gain. In presence of missing observations, this ﬁlter has a smaller linear minimum mean

square error (LMMSE) than its static counterpart, as it is detailed in Section II.

The general case of time-varying Kalman ﬁlter with intermittent observations was also studied

by Fortmann et al. [22], who derived stochastic equations for the state covariance error. However,

they do not statistically characterize its convergence and provide only numerical evidence of the

transition to instability, leaving a formal characterization of this as an open problem, which is

addressed in this paper. A somewhat different formulation was considered in [23], where the

observations arrival have a bounded delay.

Finally, we point out that our analysis can also be viewed as an instance of Expectation-

Maximization (EM) theory. EM is a general framework for doing Maximum Likelihood esti-

mation in missing-data models [24]. Lauritzen [25] shows how EM can be used for general

graphical models. In our case, however, the graph structure is a function of the missing data, as

there is one graph for each pattern of missing data.

The paper is organized as follows. In section II we formalize the problem of Kalman ﬁltering

with intermittent observations. In section III we provide upper and lower bounds on the expected

estimation error covariance of the Kalman ﬁlter, and ﬁnd the conditions on the observation arrival

probability λ for which the upper bound converges to a ﬁxed point, and for which the lower

bound diverges. Section IV describes some special cases and gives an intuitive understanding of

the results. In section V we compare our approach to previous ones [17] based on jump linear

systems. Finally, in section VI, we state our conclusions and give directions for future work.

II. PROBLEM FORMULATION

Consider the canonical state estimation problem. We deﬁne the arrival of the observation at

time t as a binary random variable γt, with probability distribution pγt(1) = λt, and with γt

DRAFT

independent of γs if t (cid:54)= s. The output noise vt is deﬁned in the following way:

 N (0, R)

N (0, σ2I)

p(vt|γt) =

: γt = 1

: γt = 0,

for some σ2 . Therefore, the variance of the observation at time t is R if γt is 1, and σ2I
otherwise. In reality the absence of observation corresponds to the limiting case of σ → ∞. Our
approach is to re-derive the Kalman ﬁlter equations using a “dummy” observation with a given
variance when the real observation does not arrive, and then take the limit as σ → ∞.

First let us deﬁne:

ˆxt|t

Pt|t

ˆxt+1|t

Pt+1|t

ˆyt+1|t

∆= E[xt|yt, γt]
∆= E[(xt − ˆx)(xt − ˆx)(cid:48)|yt, γt]
∆= E[xt+1|yt, γt+1]
∆= E[(xt+1 − ˆxt+1)(xt+1 − ˆxt+1)(cid:48)|yt, γt+1]
∆= E[yt+1|yt, γt+1],

∆= [y0, . . . , yt](cid:48) and γt

(6)
∆= [γ0, . . . , γt](cid:48). Using the Dirac delta

where we have deﬁned the vectors yt
δ(·) we have:

E[(yt+1 − ˆyt+1|t)(xt+1 − ˆxt+1|t)(cid:48)|yt, γt+1] = CPt+1|t

E[(yt+1 − ˆyt+1|t)(yt+1 − ˆyt+1|t)(cid:48)|yt, γt+1] = CPt+1|tC(cid:48) + δ(γt+1 − 1)R + δ(γt+1)σ2I,

and it follows that the random variables xt+1 and yt+1, conditioned on the output yt and on the
arrivals γt+1, are jointly gaussian with mean

E[xt+1, yt+1|yt, γt+1] =

and covariance

COV (xt+1, yt+1|yt, γt+1) =

 Pt+1|t

 ˆxt+1|t

C ˆxt+1|t

 ,

Pt+1|tC(cid:48)

CPt+1|t CPt+1|tC(cid:48) + δ(γt+1 − 1)R + δ(γt+1)σ2I

6

(2)

(3)

(4)

(5)

(7)

(8)

 .

(9)

DRAFT

Hence, the Kalman ﬁlter equations are modiﬁed as follows:

ˆxt+1|t = Aˆxt|t
Pt+1|t = APt|tA(cid:48) + Q

ˆxt+1|t+1 = ˆxt+1|t + Pt+1|tC(cid:48)(CPt+1|tC(cid:48) + δ(γt+1 − 1)R + δ(γt+1)σ2I)−1(yt+1 − C ˆxt+1|t)
Pt+1|t+1 = Pt+1|t − Pt+1|tC(cid:48)(CPt+1|tC(cid:48) + δ(γt+1 − 1)R + δ(γt+1)σ2I)−1CPt+1|t.

7

(10)

(11)

(12)

(13)

Taking the limit as σ → ∞, the update equations (12) and (13) can be rewritten as follows:

ˆxt+1|t+1 = ˆxt+1|t + γt+1Pt+1|tC(cid:48)(CPt+1|tC(cid:48) + R)−1(yt+1 − C ˆxt+1|t)
Pt+1|t+1 = Pt+1|t − γt+1Pt+1|tC(cid:48)(CPt+1|tC(cid:48) + R)−1CPt+1|t.

(14)

(15)

Note that performing this limit corresponds exactly to propagating the previous state when

there is no observation update available at time t. We also point out the main difference from

the standard Kalman ﬁlter formulation: Both ˆxt+1|t+1 and Pt+1|t+1 are now random variables,
being a function of γt+1, which is itself random.

It is important to stress that Equations (14)-(15) give the minimum state error variance ﬁlter

given the observations {yt} and their arrival sequence{γt}, i.e.
t = E[xt|yn, . . . , y1; γn, . . . , γ1]. As a consequence, the ﬁlter proposed in this paper is neces-
ˆxtm
sarily time-varying and stochastic since it depends on the arrival sequence. The ﬁlters that have

been proposed so far using JLS theory [16][18] give the minimum state error variance ﬁlters
assuming that only the observations {yt} are available, i.e. ˆxJLS
t = E[xt|yn, . . . , y1]. Therefore,
the ﬁlter given by Equations (14)-(15) gives a better performance than its JLS counterparts, since

it exploits additional information regarding the arrival sequence.

Given the new formulation, we now study the Riccati equation of the state error covariance

matrix in the speciﬁc case when the arrival process of the observation is time-independent, i.e.

λt = λ for all time. This will allow us to provide deterministic upper and lower bounds on
its expectation. We then characterize the convergence of these upper and lower bounds, as a

function of the arrival probability λ of the observation.

DRAFT

III. CONVERGENCE CONDITIONS AND TRANSITION TO INSTABILITY

It is easy to verify that the modiﬁed Kalman ﬁlter formulation in Equations (11) and (15) can

be rewritten as follows:

Pt+1 = APtA(cid:48) + Q − γt APtC(cid:48)(CPtC(cid:48) + R)−1CPtA(cid:48),

(16)

8

where we use the simpliﬁed notation Pt = Pt|t−1. Since the sequence {γt}∞
modiﬁed Kalman ﬁlter iteration is stochastic and cannot be determined off-line. Therefore, only

is random, the

0

statistical properties can be deduced. In this section we show the existence of a critical value

λc for the arrival probability of the observation update, such that for λ > λc the mean state
covariance E[Pt] is bounded for all initial conditions, and for λ ≤ λc the mean state covariance
diverges for some initial condition. We also ﬁnd a lower bound λ, and upper bound λ, for the
critical probability λc, i.e., λ ≤ λc ≤ λ. The lower bound is expressed in closed form; the upper
bound is the solution of a linear matrix inequality (LMI). In some special cases the two bounds

coincide, giving a tight estimate. Finally, we present numerical algorithms to compute a lower
bound ¯S, and upper bound ¯V , for limt→∞ E[Pt], when it is bounded.

First, we deﬁne the modiﬁed algebraic Riccati equation (MARE) for the Kalman ﬁlter with

intermittent observations as follows,

gλ(X) = AXA(cid:48) + Q − λ AXC(cid:48)(CXC(cid:48) + R)−1CXA(cid:48).

(17)

Our results derive from two principal facts: the ﬁrst is that concavity of the modiﬁed algebraic

Riccati equation for our ﬁlter with intermittent observations allows use of Jensen’s inequality to

ﬁnd an upper bound on the mean state covariance; the second is that all the operators we use to

estimate upper and lower bounds are monotonically increasing, therefore if a ﬁxed point exists,

it is also stable.

We formally state all main results in form of theorems. Omitted proofs appear in the Appendix.

The ﬁrst theorem expresses convergence properties of the MARE.
Theorem 1. Consider the operator φ(K, X) = (1 − λ)(AXA(cid:48) + Q) + λ(F XF (cid:48) + V ), where
F = A + KC, V = Q + KRK(cid:48). Suppose there exists a matrix ˜K and a positive deﬁnite matrix
˜P such that

˜P > 0 and

˜P > φ( ˜K, ˜P )

DRAFT

Then,

(a) for any initial condition P0 ≥ 0, the MARE converges, and the limit is independent of
the initial condition:

9

lim
t→∞ Pt = lim

t→∞ gt

λ(P0) = P

(b) P is the unique positive semideﬁnite ﬁxed point of the MARE.

The next theorem states the existence of a sharp transition.

Theorem 2. If (A, Q 1
a λc ∈ [0, 1) such that

2 ) is controllable, (A, C) is detectable, and A is unstable, then there exists

t→∞ E[Pt] = +∞ for 0 ≤ λ ≤ λc and some initial condition P0 ≥ 0
lim
E[Pt] ≤ MP0 ∀t
for λc < λ ≤ 1 and any initial condition P0 ≥ 0

(18)

(19)

where MP0 > 0 depends on the initial condition P0 ≥ 01.

The next theorem gives upper and lower bounds for the critical probability λc.

Theorem 3. Let

λ = arginfλ[∃ ˆS | ˆS = (1 − λ)A ˆSA(cid:48) + Q] = 1 − 1
α2
λ = arginfλ[∃( ˆK, ˆX) | ˆX > φ( ˆK, ˆX)]
where α = maxi |σi| and σi are the eigenvalues of A. Then

λ ≤ λc ≤ λ.

(20)

(21)

(22)

Finally, the following theorem gives an estimate of the limit of the mean covariance matrix

E[Pt], when this is bounded.

Theorem 4. Assume that (A, Q 1

2 ) is controllable, (A, C) is detectable and λ > λ, where λ is

deﬁned in Theorem 4. Then

0 ≤ St ≤ E[Pt] ≤ Vt

∀ E[P0] ≥ 0

(23)

1We use the notation limt→∞ At = +∞ when the sequence At ≥ 0 is not bounded; i.e., there is no matrix M ≥ 0 such

that At ≤ M,∀t.

DRAFT

10

where limt→∞ St = ¯S and limt→∞ Vt = ¯V , where ¯S and ¯V are solution of the respective
algebraic equations ¯S = (1 − λ)A ¯SA(cid:48) + Q and ¯V = gλ( ¯V )

The previous theorems give lower and upper bounds for both the critical probability λc and
for the mean error covariance E[Pt]. The lower bound λ is expressed in closed form. We now
resort to numerical algorithms for the computation of the remaining bounds λ, ¯S and ¯V .

The computation of the upper bound λ can be reformulated as the iteration of an LMI feasibility

problem. To establish this we need the following theorem:

Theorem 5. If (A, Q 1

2 ) is controllable and (A, C) is detectable, then the following statements

are equivalent:

(a) ∃ ¯X such that
(b) ∃ ¯K, ¯X > 0
(c) ∃ ¯Z and 0 < ¯Y ≤ I such that

¯X > gλ( ¯X)

such that

¯X > φ( ¯K, ¯X)

Ψλ(Y, Z) =

√

Y

λ(A(cid:48)Y + C(cid:48)Z(cid:48))
√

1 − λA(cid:48)Y



√

λ(Y A + ZC)

√

1 − λY A

Y

0

0

Y

 > 0.

Proof:

(a)=⇒(b) If ¯X > gλ( ¯X) exists, then ¯X > 0 by Lemma 1(g). Let ¯K = K ¯X. Then
¯X > gλ( ¯X) = φ( ¯K, ¯X) which proves the statement. (b)=⇒(a) Clearly ¯X > φ( ¯K, ¯X) ≥ gλ( ¯X)
which proves the statement. (b)⇐⇒(c) Let F = A + KC, then:

X > (1 − λ)AXA(cid:48) + λF XF (cid:48) + Q + λKRK(cid:48)

is equivalent to

where we used the Schur complement decomposition and the fact that X − (1 − λ)AXA(cid:48) ≥
λF XF (cid:48) + Q + λKRK(cid:48) ≥ Q > 0. Using one more time the Schur complement decomposition
on the ﬁrst element of the matrix we obtain

λF
X−1

√

λF (cid:48)

 X − (1 − λ)AXA(cid:48) √


X
√
λF (cid:48)
1 − λA(cid:48)

√

√

λF
X−1
0

Θ =

 > 0,
 > 0.

√

1 − λA
0
X−1

DRAFT

11

This is equivalent to

Λ =

=

 Θ

 X−1 0 0

I 0

0

 > 0

0

0 I

√

λX−1F
X−1
0

√

1 − λX−1A

0
X−1

0

0

0 I

I 0

 X−1 0 0



√

√

X−1
√
λF (cid:48)X−1
1 − λA(cid:48)X−1

 > 0.
 > 0.

Let us consider the change of variable Y = X−1 > 0 and Z = X−1K, in which case the
previous LMI is equivalent to:

√

√

1 − λY A

Ψ(Y, Z) =

Y

λ(Y A + ZC)

λ(A(cid:48)Y + C(cid:48)Z(cid:48))
√

1 − λA(cid:48)Y

Y

0

0

Y

Since Ψ(αY, αK) = αΨ(Y, K), then Y can be restricted to Y ≤ I, which completes the theorem.

Combining theorems 3 and 5 we immediately have the following corollary

Corollary 1. The upper bound λ is given by the solution of the following optimization problem,

λ = argminλΨ(Y, Z) > 0,

0 ≤ Y ≤ I.

This is a quasi-convex optimization problem in the variables (λ, Y, Z) and the solution can

be obtained by iterating LMI feasibility problems and using bisection for the variable λ.

The lower bound ¯S for the mean covariance matrix can be easily obtained via standard
Lyapunov Equation solvers. The upper bound ¯V can be found by iterating the MARE or by
solving a semideﬁnite programming (SDP) problem as shown in the following.

Theorem 6. If λ > λ, then the matrix ¯V = gλ(V ) is given by:

(a) ¯V = limt→∞ Vt; Vt+1 = gλVt where V0 ≥ 0

DRAFT

(b)

argmaxV

subject to

T race(V )

 AV A(cid:48) − V

√

√

λAV C(cid:48)
λCV A(cid:48) CV C(cid:48) + R

12

 ≥ 0,

V ≥ 0

Proof:

(a) It follows directly from Theorem 1.

(b) It can be obtained by using the Schur complement decomposition on the equation V ≤
gλ(V ). Clearly the solution ¯V = gλ( ¯V ) belongs to the feasible set of the optimization problem.
We now show that the solution of the optimization problem is the ﬁxed point of the MARE.
Suppose it is not, i.e., ˆV solves the optimization problem but ˆV (cid:54)= gλ( ˆV ). Since ˆV is a feasible
ˆˆV . However, this implies that T race( ˆV ) <
point of the optimization problem, then ˆV < gλ( ˆV ) =
ˆˆV ), which contradicts the hypothesis of optimality of matrix ˆV . Therefore ˆV = gλ( ˆV )
T race(

and this concludes the theorem.

IV. SPECIAL CASES AND EXAMPLES

In this section we present some special cases in which upper and lower bounds on the critical
value λc coincide and give some examples. From Theorem 1, it follows that if there exists a ˜K
such that F is the zero matrix, then the convergence condition of the MARE is for λ > λc =
1 − 1/α2, where α = maxi |σi|, and σi are the eigenvalues of A.

• C is invertible. In this case a choice of K = −AC−1 makes F = 0. Note that the scalar
case also falls under this category. Figure (1) shows a plot of the steady state of the upper

and lower bounds versus λ in the scalar case. The discrete time LTI system used in this
simulation has A = −1.25, C = 1, with vt and wt having zero mean and variance R = 2.5
and Q = 1, respectively. For this system we have λc = 0.36. The transition clearly appears
in the ﬁgure, where we see that the steady state value of both upper and lower bound tends

to inﬁnity as λ approaches λc. The dashed line shows the lower bound, the solid line the
upper bound, and the dash-dot line shows the asymptote.

• A has a single unstable eigenvalue. In this case, regardless of the dimension of C (and

as long as the pair (A, C) is detectable), we can use Kalman decomposition to bring to

zero the unstable part of F and thereby obtain tight bounds. Figure (2) shows a plot for

DRAFT

13

Fig. 2. Example of transition to instability in the scalar case. The dashed line shows the asymptotic value of the lower bound
( ¯S), the solid line the asymptotic value of the upper bound ( ¯V ), and the dash-dot line shows the asymptote (λc).

 1.25 1

0

.9

, C =

0

7

(cid:179)

(cid:180)

1 0 2

the system A =

0

0 .60

with vt and wt having zero mean and variance R = 2.5 and Q = 20 ∗ I3x3, respectively.
This time, the asymptotic value for trace of upper and lower bound is plotted versus λ.

Once again λc = 0.36.

In general F cannot always be made zero and we have shown that while a lower bound on

λc can be written in closed form, an upper bound on λc is the result of a LMI. Figure (4) shows
an example where upper and lower bounds have different convergence conditions. The system

 1.25

, C =

(cid:179)

0

(cid:180)

used for this simulation is A =
with vt and wt having zero mean and variance R = 2.5 and Q = 20 ∗ I2x2, respectively.

1 1

1.1

1

Finally, in Figure (5) we report results of another experiment, plotting the state estimation error

for the scalar system used above at two similar values of λ, one being below and one above the
critical value. We note a dramatic change in the error at λc ≈ 0.36. The ﬁgure on the left shows

DRAFT

00.10.20.30.40.50.60.70.80.910102030405060708090100S, VλSpecial case: C is invertibleVSλc 14

Fig. 3. Example of transition to instability with a single unstable eigenvalue in the MIMO case. The dashed line shows the
asymptotic value of the trace of lower bound ( ¯S), the solid line the asymptotic value of trace of the upper bound ( ¯V ), and the
dash-dot line shows the asymptote (λc).

Fig. 4. Transition to instability in the general case, with arbitrary A and C. In this case lower and upper bounds do not have

the same asymptote.

DRAFT

00.10.20.30.40.50.60.70.80.9100.511.522.533.544.55x 106Special case: one unstable eigenvalueTr(V)Tr(S)Tr(S), Tr(V)λλc 00.10.20.30.40.50.60.70.80.91051015x 104General caseTr(V)Tr(S)Tr(S), Tr(V)λ λ λ15

Fig. 5. Estimation error for λ below (left) and above (right) the critical value

the estimation error with λ = 0.3. The ﬁgure on the right shows the estimation error for the same

system evolution with λ = 0.4. In the ﬁrst case the estimation error grows dramatically, making

it practically useless for control purposes. In the second case, a small increase in λ reduces the

estimation error by approximately three orders of magnitude.

V. STATIC VERSUS DYNAMIC KALMAN GAIN

In this section we compare the performance of ﬁltering with static and dynamic gain for

a scalar discrete system. For the static estimator we follow the jump linear system approach

of [17]. The scalar static estimator case has been also worked out in [26].

Consider the dynamic state estimator

t + γtK d

t (yt − ˆyt)
t+1 = Aˆxd
ˆxd
t = APtC(cid:48)(CPtC(cid:48) + R)−1
K d
Pt+1 = APtA(cid:48) + Q − γtAPtC(cid:48)(CPtC(cid:48) + R)−1CPtA(cid:48)

(24)

DRAFT

0100200300400500−3−2−10123x 1050100200300400500−1000−800−600−400−20002004006008001000tkEstimation Error: λ = 0.15tk Estimation Error: λ = 0.1where the Kalman gain K d
t

is time-varying. Also consider the static state estimator

ˆxs
t+1 = Aˆxd

t + γtKs(yt − ˆyt)

16

(25)

where the estimator gain Ks is constant. If no data arrives, i.e. γt = 0, both estimators simply
propagate the state estimate of the previous time-step.

The performance of the dynamic state estimator (24) has been analyzed in the previous

sections. The performance of static state estimator (25), instead, can be readily obtained using
∆=
t+1, we obtain the dynamics

jump linear system theory [15] [17]. To do so, let us consider the estimator error es
xt+1 − ˆxs
of the estimation error:

t+1. Substituting Equations (1) for xt+1 and (25) for ˆxs

t+1

t+1 = (A − γtKsC)es
es

t + vt + γtKswt.

(26)

Using the same notation of Chapter 6 in Nilsson [17], where he considers the general system:

zk+1 = Φ(rk)zk + Γ(rk)ek,

the system (26) can be seen as jump linear system switching between two states rk ∈ {1, 2}
given by:

Φ(1) = A − KsC

Γ(1) = [1 Ks]

Φ(2) = A

Γ(2) = [1 0],

k] = Re, the transition probability matrix Qπ and the steady

where the noise covariance E[eke(cid:48)
state probability distribution π∞ are given by:

 Q 0

0 R

 Qπ =

 λ 1 − λ

λ 1 − λ

Re =

(cid:105)

λ 1 − λ

.

 π∞ =
(cid:181)

1 − 1
A2

(cid:104)

(cid:182)

1 −(cid:161)

1
1 − KsC

A

(cid:162)2

Following the methodology proposed in Nilsson [17] is possible to show that the system above
is mean square stable, i.e. limt→∞ E[(es

t ] = 0 if and only if the transition probability is

t )(cid:48)es

λ < λs =

.

(27)

If the system is mean square stable, the steady state error covariance P s∞ = limt→∞ E[es
given by:

P s∞ =

1 − λ(A − KsC)2 − (1 − λ)A2 .

Q + K 2

s R

t )(cid:48)] is

t (es

(28)

DRAFT

Calculations to obtain Equations (27) and (28) are tedious but straightforward, therefore they

are omitted.

17

It is immediately evident that the critical transition probability λs of the estimator (25) using
a static gain is always greater then the critical transition probability λc of the estimator (24)
which adopts a dynamic gain, in fact

1 −(cid:161)

(cid:162)2

A

1
1 − KsC
and the two probabilities are equal only when Ks = A
C .

λs = λc

A natural choice for the static estimator gain Ks is the steady state Kalman gain KSS of the
closed loop system (r = 1), which is always different from A
C . For the scalar system considered
in the previous section, where A = −1.5, C = 1, Q = 1, R = 2.5, this is given by KSS = −0.70,
C = −1.25. In the special case
while the gain for largest mean square stability range is Ks = A
when the arrival probability is known, another natural choice for the estimator gain K is obtained
by substituting the error covariance solution of ¯P = gλ( ¯P ) into the equation for the Kalman
ﬁlter gain Kλ = A ¯P C(cid:48)(C ¯P C(cid:48) + R)−1. For example, assuming λ = 0.6, then ¯P = 7.32 and
Kλ = −0.93. Figure 6 shows all of these cases, comparing them with the upper bound on the
state error covariance ¯V of the dynamic estimator (24) that can be computed as indicated in
Theorem 6. The steady state error covariance of the static predictor for the three different gains
is always greater then our upper bound ¯V . This is not surprising, since the dynamic estimator is
optimal over all possible estimators as shown in Section II. Note that the static predictor with

static gain Kλ (designed for λ = 0.6) achieves the same state error covariance predicted by our
upper bound for the optimal dynamic ﬁlter when λ = 0.6. However, the empirical error state

covariance is on average better then the static ﬁlter, as shown in Figure 7. This is to be expected,

since the solution of MARE gives only an upper bound of the true expected state covariance of

the time-varying ﬁlter. Moreover, it is worth stressing that if the arrival probability is different

from the one used to design the static gain, the performance of the static ﬁlter will degrade

considerably, while the time-varying ﬁlter will still perform optimally and does not require

knowledge of λ. From this example, it seems that the upper bound for the dynamic estimator
¯V gives en estimate of the minimum steady state covariance that can be achieved with a static
estimator for any given arrival probability if the static gain Ks is chosen optimally. Then the
MARE could be used to ﬁnd the minimum steady state covariance and then the corresponding

DRAFT

18

Fig. 6. Error covariance bound ¯V for dynamic predictor obtained from our theory and steady state error covariance for three
natural static predictors obtained from JLS theory.

steady state modiﬁed Kalman gain, thus providing an useful tool for optimal static estimator

design. Future work will explore this possibility.

VI. CONCLUSIONS

In this paper we have presented an analysis of Kalman ﬁltering in the setting of intermittent

observations. We have shown how the expected estimation error covariance depends on the

tradeoff between loss probability and the system dynamics. Such a result is useful to the system

designer who must assess the relationship between the dynamics of the system whose state is

to be estimated and the reliability of the communication channel through which that system is

measured.

Our motivating application is a distributed sensor network that collects observations and sends

them to one or more central units that are responsible for estimation and control. For example,

in a pursuit evasion game in which mobile pursuers perform their control actions based on the

current estimate of the positions of both pursuers and evaders, the sensing capability of each

pursuer is generally limited, and an embedded sensor network is essential for providing a larger

DRAFT

00.10.20.30.40.50.60.70.80.9102468101214161820λState Error Covariance PVKλK=A/CKSS19

Fig. 7.

Empirical state error covariance of our time-varying ﬁlter and the linear mimimum mean square error estimator

(LMMSEE) [16] obtained by using the optimal static kalman gain Kλ. The curves are obtained by averaging 10000 Monte

Carlo simulations for t = 1, . . . , 300, with the values of the input noise (vt, wt) and the arrival sequence γt generated randomly.

Both ﬁlters were compared under the same conditions.

overall view of the terrain. The results that we have presented here can aid the designer of the

sensor network in the choice of the number and disposition of the sensors.

This application also suggests a number of interesting directions for further work. For example,

although we have assumed independent Bernoulli probabilities for the observation events, in the

sensor network there will generally be temporal and spatial sources of variability that lead to

correlations among these events. While it is possible to compute posterior state estimates in such

a setting, it would be of interest to see if a priori bounds of the kind that we have obtained here

can be obtained in this case. Similarly, in many situations there may be correlations between

the states and the observation events; for example, such correlations will arise in the pursuit

evasion game when the evaders move near the boundaries of the sensor network. Finally, the

sensor network setting also suggests the use of smoothing algorithms in addition to the ﬁltering

algorithms that have been our focus here. In particular, we may be willing to tolerate a small

amount of additional delay to wait for the arrival of a sensor measurement, if that measurement

is expected to provide a signiﬁcant reduction in uncertainty. Thus we would expect that the

DRAFT

05010015020025030044.555.566.577.588.59TimeEmpirical state error covarince   (r.m.s.)2time−varying filterLMMSE filter (Kλ)20

tradeoff that we have studied here between loss probability and the system dynamics should

also be modulated in interesting ways by the delay due to smoothing.

We also remark that the assumption of modelling the arrival of observations as a bernoulli

i.i.d. process can be clearly improved upon. For example, one can imagine situations where some

of the sensing is done locally and therefore measurements are available at all sampling times,

while measurements taken at distant locations are available at irregular intervals. This would

translate in different dropping rates for different channels. We have focused to providing a basic

result upon which more sophisticated models can be built and analyzed.

VII. ACKNOWLEDGEMENT

The authors are grateful to the anonymous reviewer for the comments that helped improving

the quality of the manuscript.

VIII. APPENDIX A

In order to give complete proofs of our main theorems, we need to prove some preliminary

lemmas. The ﬁrst one shows some useful properties of the MARE.

Lemma 1. Let the operator

φ(K, X) = (1 − λ)(AXA(cid:48) + Q) + λ(F XF (cid:48) + V )
(29)
V = Q + KRK(cid:48). Assume X ∈ S = {S ∈ Rn×n|S ≥ 0}, R > 0, Q ≥ 0,

where F = A + KC,
and (A, Q 1

2 ) is controllable. Then the following facts are true:

−1, gλ(X) = φ(KX, X)

(a) With KX = −AXC(cid:48) (CXC(cid:48) + R)
(b) gλ(X) = minK φ(K, X) ≤ φ(K, X)∀K
(c) If X ≤ Y , then gλ(X) ≤ gλ(Y )
(d) If λ1 ≤ λ2 then gλ1(X) ≥ gλ2(X)
(e) If α ∈ [0, 1], then gλ(αX + (1 − α)Y ) ≥ αgλ(X) + (1 − α)gλ(Y )
(f) gλ(X) ≥ (1 − λ)AXA(cid:48) + Q
(g) If ¯X ≥ gλ( ¯X), then ¯X > 0
(h) If X is a random variable, then (1 − λ)AE[X]A(cid:48) + Q ≤ E[gλ(X)] ≤ gλ(E[X])

DRAFT

21

Proof:

(a) Deﬁne FX = A + KXC, and observe that

FXXC(cid:48) + KXR = (A + KXC)XC(cid:48) + KXR = AXC(cid:48) + KX(CXC(cid:48) + R) = 0.

Next, we have

gλ(X) = (1 − λ)(AXA(cid:48) + Q) + λ(AXA(cid:48) + Q − AXC(cid:48) (CXC(cid:48) + R)

−1 CXA(cid:48))

= (1 − λ)(AXA(cid:48) + Q) + λ(AXA(cid:48) + Q + KXCXA(cid:48))
= (1 − λ)(AXA(cid:48) + Q) + λ(FXXA(cid:48) + Q)
= (1 − λ)(AXA(cid:48) + Q) + λ(FXXA(cid:48) + Q) + (FXXC(cid:48) + KXR)K(cid:48)

= φ(KX, X)

(b) Let ψ(K, X) = (A + KC)X(A + KC)(cid:48) + KRK(cid:48) + Q. Note that

argminKφ(K, X) = argminKF XF (cid:48) + V = argminKψ(X, K).

Since X, R ≥ 0, φ(K, X) is quadratic and convex in the variable K, therefore the minimizer
can be found by solving ∂ψ(K,X)

∂K = 0, which gives:

2(A + KC)XC(cid:48) + 2KR = 0 =⇒ K = −AXC(cid:48) (CXC(cid:48) + R)

−1 .

Since the minimizer corresponds to KX deﬁned above, the fact follows from fact (1)

(c) Note that φ(K, X) is afﬁne in X. Suppose X ≤ Y . Then

gλ(X) = φ(KX, X) ≤ φ(KY , X) ≤ φ(KY , Y ) = gλ(Y ).

This completes the proof.

(d) Note that AXC(cid:48)(CXC(cid:48) + R)−1CXA ≥ 0. Then

gλ1(X) = AXA(cid:48) + Q − λ1 AXC(cid:48)(CXC(cid:48) + R)−1CXA

≥ AXA(cid:48) + Q − λ2 AXC(cid:48)(CXC(cid:48) + R)−1CXA = gλ2(X)

(e) Let Z = αX + (1 − α)Y where α ∈ [0, 1]. Then we have

gλ(Z) = φ(KZ, Z)

= α(A + KZ C)X(A + KZ C)(cid:48) + (1 − α)(A + KZ C)Y (A + KZ C)(cid:48)+

+(α + 1 − α)(KZ R K(cid:48)

Z + Q)
= αφ(KZ, X) + (1 − α)φ(KZ, Y )
≥ αφ(KX, X) + (1 − α)φ(KY , Y )
= αgλ(X) + (1 − α)gλ(Y ).

(30)

DRAFT

22

(f) Note that FXXF (cid:48)

X ≥ 0 and KRK(cid:48) ≥ 0 for all K and X. Then
gλ1(X) = φ(KX, X) = (1 − λ)(AXA(cid:48) + Q) + λ(FXXF (cid:48)

X + KXRK(cid:48)

X + Q)

≥ (1 − λ)(AXA(cid:48) + Q) + λQ = (1 − λ)AXA(cid:48) + Q.

(g) From fact (f) follows that ¯X ≥ gλ1( ¯X) ≥ (1 − λ)A ¯XA(cid:48) + Q. Let ˆX such that ˆX =
(1 − λ)A ˆXA(cid:48) + Q. Such ˆX must clearly exist. Therefore ¯X − ˆX ≥ (1 − λ)A( ¯X − ˆX)A(cid:48) ≥ 0.
1 − λA.
Moreover the matrix ˆX solves the Lyapunov Equation ˆX = ˜A ˆX ˜A(cid:48) + Q where ˜A =
Since ( ˜A, Q 1

2 ) is detectable, it follows that ˆX > 0 and so ¯X > 0, which proves the fact.

√

(h) Using fact (f) and linearity of expectation we have

E[gλ(X)] ≥ E[(1 − λ)AXA(cid:48) + Q] = (1 − λ)AE[X]A(cid:48) + Q,

fact (e) implies that the operator gλ() is concave, therefore by Jensen’s Inequality we have
E[gλ(X)] ≤ gλ(E[X]).

Lemma 2. Let Xt+1 = h(Xt) and Yt+1 = h(Yt). If h(X) is a monotonically increasing function
then:

X1 ≥ X0 ⇒ Xt+1 ≥ Xt, ∀t ≥ 0
X1 ≤ X0 ⇒ Xt+1 ≤ Xt, ∀t ≥ 0
∀t ≥ 0
X0 ≤ Y0 ⇒ Xt ≤ Yt,

Proof: This lemma can be readily proved by induction. It is true for t = 0, since X1 ≥ X0
by deﬁnition. Now assume that Xt+1 ≥ Xt, then Xt+2 = h(Xt+1) ≥ h(Xt+1) = Xt+1 because
of monotonicity of h(·). The proof for the other two cases is analogous.

It is important to note that while in the scalar case X ∈ R either h(X) ≤ X or h(X) ≥ X;
in the matrix case X ∈ Rn×n, it is not generally true that either h(X) ≥ X or h(X) ≤ X.
This is the source of the major technical difﬁculty for the proof of convergence of sequences in
higher dimensions. In this case convergence of a sequence {Xt}∞
other sequences, {Yt}∞
these two sequences converge to the same point.

is obtained by ﬁnding two
that bound Xt, i.e., Yt ≤ Xt ≤ Zt,∀t, and then by showing that

0 ,{Zt}∞

0

0

The next two Lemmas show that when the MARE has a solution ¯P , this solution is also
stable, i.e., every sequence based on the difference Riccati equation Pt+1 = gλ(Pt) converges to
¯P for all initial positive semideﬁnite conditions P ≥ 0.

DRAFT

Lemma 3. Deﬁne the linear operator

L(Y ) = (1 − λ)(AY A(cid:48)) + λ(F Y F (cid:48))

Suppose there exists Y > 0 such that Y > L(Y ).

23

(a) For all W ≥ 0,

(b) Let U ≥ 0 and consider the linear system

k→∞Lk(W ) = 0

lim

Yk+1 = L(Yk) + U initialized at Y0.

Then, the sequence Yk is bounded.
Proof: (a) First observe that 0 ≤ L(Y ) for all 0 ≤ Y . Also, X ≤ Y implies L(X) ≤ L(Y ).

Choose 0 ≤ r < 1 such that L(Y ) < rY . Choose 0 ≤ m such that W ≤ mY . Then,

0 ≤ Lk(W ) ≤ mLk(Y ) < mrkY

The assertion follows when we take the limit r → ∞, on noticing that 0 ≤ r < 1.

(b) The solution of the linear iteration is

Yk = Lk(Y0) +

Lt(U )

(cid:33)

Y

t=0

k−1(cid:88)
k−1(cid:88)
(cid:182)

t=0
mU
1 − r
mU
1 − r

(cid:182)

Y

Y

(cid:195)
(cid:181)
(cid:181)

≤

≤

≤

mY0rk +

mY0 +

mY0rk +

mU rt

proving the claim.

Lemma 4. Consider the operator φ(K, X) deﬁned in Equation (29). Suppose there exists a

matrix K and a positive deﬁnite matrix P such that

P > 0 and P > φ(K, P ).

Then, for any P0, the sequence Pt = gt
P0 such that

λ(P0) is bounded, i.e. there exists MP0 ≥ 0 dependent of

Pt ≤ M for all

t.

DRAFT

24

Proof: First deﬁne the matrices F = A + KC and consider the linear operator

L(Y ) = (1 − λ)(AY A(cid:48)) + λ(F Y F

(cid:48)

)

Observe that

P > φ(K, P ) = L(P ) + Q + λKRK

(cid:48) ≥ L(P ).

Thus, L meets the condition of Lemma 3. Finally, using fact (b) in Lemma 1 we have

Pt+1 = gλ(Pt) ≤ φ(K, Pt) = LPt + Q + λKRK

(cid:48)

= L(Pt) + U.

Since U = λKRK

(cid:48)

+ Q ≥ 0, using Lemma 3, we conclude that the sequence Pt is bounded.

We are now ready to give proofs for Theorems 1-4.

A. Proof of Theorem 1

(a) We ﬁrst show that the modiﬁed Riccati difference equation initialized at Q0 = 0 converges.

Let Qk = gk

λ(0). Note that 0 = Q0 ≤ Q1. It follows from Lemma 1(c) that

Q1 = gλ(Q0) ≤ gλ(Q1) = Q2.

A simple inductive argument establishes that

0 = Q0 ≤ Q1 ≤ Q2 ≤ ··· ≤ MQ0.

Here, we have used Lemma 4 to bound the trajectory. We now have a monotone non-decreasing

sequence of matrices bounded above. It is a simple matter to show that the sequence converges,

i.e.

Also, we see that P is a ﬁxed point of the modiﬁed Riccati iteration:

lim
k→∞ Qk = P .

P = gλ(P ),

which establishes that it is a positive semi-deﬁnite solution of the MARE.

Next, we show that the Riccati iteration initialized at R0 ≥ P also converges, and to the same

limit P . First deﬁne the matrices

K = −AP C(cid:48)(cid:161)

CP C(cid:48) + R

(cid:162)−1

, F = A + KC

DRAFT

25

and consider the linear operator

ˆL(Y ) = (1 − λ)(AY A(cid:48)) + λ(F Y F

(cid:48)

).

Observe that

P = gλ(P ) = L(P ) + Q + KRK

(cid:48)

> ˆL(P ).

Thus, ˆL meets the condition of Lemma 3. Consequently, for all Y ≥ 0,

Now suppose R0 ≥ P . Then,

ˆLk(Y ) = 0.

lim
k→∞

R1 = gλ(R0) ≥ gλ(P ) = P .

A simple inductive argument establishes that

Rk ≥ P for all k.

Observe that

0 ≤ (Rk+1 − P ) = gλ(Rk) − gλ(P )

= φ(KRk, Rk) − φ(KP , P )
≤ φ(KP , Rk) − φ(KP , P )
= (1 − λ)A(Rk − P )A(cid:48) + λFP (Rk − P )F (cid:48)
= ˆL(Rk − P ).

P

Then, 0 ≤ limk→∞(Rk+1 − P ) ≤ 0, proving the claim.

We now establish that the Riccati iteration converges to P for all initial conditions P0 ≥ 0.
Deﬁne Q0 = 0 and R0 = P0 + P . Consider three Riccati iterations, initialized at Q0, P0, and
R0. Note that

Q0 ≤ P0 ≤ R0.

It then follows from Lemma 2 that

Qk ≤ Pk ≤ Rk

for all k.

DRAFT

26

We have already established that the Riccati equations Pk and Rk converge to P . As a result,
we have

k→∞ Pk ≤ lim

k→∞ Qk ≤ lim

P = lim

k→∞ Rk = P ,

proving the claim.

(b) Finally, we establish that the MARE has a unique positive semi-deﬁnite solution. To this
end, consider ˆP = gλ( ˆP ) and the Riccati iteration initialized at P0 = ˆP . This yields the constant
sequence

ˆP , ˆP ,···

However, we have shown that every Riccati iteration converges to P . Thus P = ˆP .

B. Proof of Theorem 2

First we note that the two cases expressed by the theorem are indeed possible. If λ = 1 the

modiﬁed Riccati difference equation reduces to the standard Riccati difference equation, which

is known to converge to a ﬁxed point, under the theorem’s hypotheses. Hence, the covariance
matrix is always bounded in this case, for any initial condition P0 ≥ 0. If λ = 0 then we reduce
to open loop prediction, and if the matrix A is unstable, then the covariance matrix diverges
for some initial condition P0 ≥ 0. Next, we show the existence of a single point of transition
between the two cases. Fix a 0 < λ1 ≤ 1 such that Eλ1[Pt] is bounded for any initial condition
P0 ≥ 0. Then, for any λ2 ≥ λ1 Eλ2[Pt] is also bounded for all P0 ≥ 0. In fact we have

Eλ1[Pt+1] = Eλ1[APtA(cid:48) + Q − γt+1APtC(cid:48)(CPtC(cid:48) + R)−1CPtA]

= E[APtA(cid:48) + Q − λ1APtC(cid:48)(CPtC(cid:48) + R)−1CPtA]
= E[gλ1(Pt)]
≥ E[gλ2(Pt)]
= Eλ2[Pt+1],

where we exploited fact (d) of Lemma 1 to write the above inequality . We can now choose

λc = {inf λ∗ : λ > λ∗ ⇒ Eλ[Pt]is bounded, for all P0 ≥ 0},

completing the proof.

DRAFT

27

C. Proof of Theorem 3

√

Deﬁne the Lyapunov operator m(X) = ˜AX ˜A(cid:48)+Q where ˜A =

2 ) is control-
2 ) is controllable. Therefore, it is well known that ˆS = m( ˆS) has a unique strictly
1 − λ maxi |σi(A)| < 1,
α2 . If maxi |σi( ˜A)| ≥ 1 it is also a well known fact that there
2 ) is

lable, also ( ˜A, Q 1
positive deﬁnite solution ˆS > 0 if and only if maxi |σi( ˜A)| < 1, i.e.
from which follows λ = 1 − 1
is no positive semideﬁnite ﬁxed point to the Lyapunov equation ˆS = m( ˆS), since ( ˜A, Q 1
controllable.

1 − λA. If (A, Q 1

√

Let us consider the difference equation St+1 = m(St), S0 = 0. It is clear that S0 = 0 ≤
Q = S1. Since the operator m() is monotonic increasing, by Lemma 2 it follows that the
sequence {St}∞
is monotonically increasing, i.e. St+1 ≥ St for all t. If λ < λ this sequence
does not converge to a ﬁnite matrix ¯S, otherwise by continuity of the operator m we would
have ¯S = m( ¯S), which is not possible. Since it is easy to show that a monotonically increasing
sequence St that does not converge is also unbounded, then we have

0

Let us consider now the mean covariance matrix E[Pt] initialized at E[P0] ≥ 0. Clearly

0 = S0 ≤ E[P0]. Moreover it is also true

t→∞ St = ∞.

lim

St ≤ E[Pt] =⇒ St+1 = (1 − λ)AStA(cid:48) + Q ≤ (1 − λ)AE[Pt]A(cid:48) + Q ≤ E[gλ(Pt)] = E[Pt+1],

where we used fact (h) from Lemma 1. By induction, it is easy to show that

St ≤ E[Pt] ∀t, ∀E[P0] ≥ 0 =⇒ lim

t→∞ E[Pt] ≥ lim

t→∞ St = ∞.

This implies that for any initial condition E[Pt] is unbounded for any λ < λ, therefore λ ≤ λc,
which proves the ﬁrst part of the Theorem.

Now consider the sequence Vt+1 = gλ(Vt), V0 = E[P0] ≥ 0. Clearly

E[Pt] ≤ Vt =⇒ E[Pt+1] = E[gλ(Pt)] ≤ gλ(E[Pt]) ≤ [gλ(Vt)] = Vt+1,

where we used facts (c) and (h) from Lemma 1. Then a simple induction argument shows
that Vt ≥ E[Pt] for all t. Let us consider the case λ > λ, therefore there exists ˆX such that
ˆX ≥ gλ( ˆX). By Lemma 1(g) ¯X > 0, therefore all hypotheses of Lemma 3 are satisﬁed, which
implies that

E[Pt] ≤ Vt ≤ MV0 ∀t.

This shows that λc ≤ λ and concludes the proof of the Theorem.

DRAFT

28

D. Proof of Theorem 4

Let consider the sequences St+1 = (1 − λ)AStA(cid:48) + Q, S0 = 0 and Vt+1 = gλ(Vt), V0 =

E[P0] ≥ 0. Using the same induction arguments in Theorem 3 it is easy to show that

St ≤ E[Pt] ≤ Vt ∀t.

From Theorem 1 follows that limt→∞ Vt = ¯V , where ¯V = gλV . As shown before the sequence St
is monotonically increasing. Also it is bounded since St ≤ Vt ≤ M. Therefore limt→∞ St = ¯S,
and by continuity ¯S = (1 − λ)A ¯SA(cid:48) + Q, which is a Lyapunov equation. Since
1 − λA is
stable and (A, Q 1
2 ) is controllable, then the solution of the Lyapunov equation is strictly positive
deﬁnite, i.e. ¯S > 0. Adding all the results together we get

√

0 < ¯S = lim

t→∞ Vt = ¯V ,

t→∞ St ≤ lim

t→∞ E[Pt] ≤ lim

which concludes the proof.

DRAFT

29

REFERENCES

[1] Smart dust project home page. http://robotics.eecs.berkeley.edu/ pister/SmartDust/.

[2] NEST project at Berkeley home page. http://webs.cs.berkeley.edu/nest-index.html.

[3] Seismic sensor research at berkeley, home page. http://www.berkeley.edu/news/media/releases /2001/12/13 snsor.html.

[4] P. Varaiya, “Smart cars on smart roads: Problems of control,” IEEE Transactions on Automatic Control, vol. 38(2), February

1993.

[5] J. Lygeros, D. N. Godbole, and S. S. Sastry, “Veriﬁed hybrid controllers for automated vehicles,” IEEE Transactions on

Automatic Control, vol. 43(4), 1998.

[6] B. Sinopoli, C. Sharp, S. Schaffert, L. Schenato, and S. Sastry, “Distributed control applications within sensor networks,”

IEEE Proceedings Special Issue on Distributed Sensor Networks, November 2003.

[7] R. E. Kalman, “A new approach to linear ﬁltering and prediction problems,” Transactions of the ASME - Journal of Basic

Engineering on Automatic Control, vol. 82(D), pp. 35–45, 1960.

[8] P. S. Maybeck, Stochastic models, estimation, and control, ser. Mathematics in Science and Engineering, 1979, vol. 141.

[9] M. Micheli and M. I. Jordan, “Random sampling of a continiuous-time stochastic dynamical system,” in Proceedings of

15th International Symposium on the Mathematical Theory of Networks and Systems (MTNS), University of Notre Dame,

South Bend, Indiana, August 2002.

[10] M. Micheli, “Random sampling of a continuous-time stochastic dynamical system: Analysis, state estimation, applications,”

Master’s Thesis, University of California at Berkeley, Deparment of Electrical Engineering, 2001.

[11] M. Athans, R. Ku, and S. B. Gershwin, “The uncertainty threshold principle, some fundamental limitations of optimal

decision making under dynamic uncertainty,” IEEE Transactions on Automatic Control, vol. 22(3), pp. 491–495, June

1977.

[12] R. Ku and M. Athans, “Further results on the uncertainty threshold principle,” IEEE Transactions on Automatic Control,

vol. 22(5), pp. 491–495, October 1977.

[13] N. Nahi, “Optimal recursive estimation with uncertain observation,” IEEE Transaction on Information Theory, vol. 15,

no. 4, pp. 457–462, 1969.

[14] M. Hadidi and S. Schwartz, “Linear recursive state estimators under uncertain observations,” IEEE Transaction on

Information Theory, vol. 24, no. 6, pp. 944–948, 1979.

[15] M. Mariton, Jump Linear Systems in Automatic Control. Marcel Dekker, 1990.

[16] O. Costa, “Stationary ﬁlter for linear minimum mean square error estimator of discrete-time markovian jump systems,”

Transactions on Automatic Control, vol. 48, no. 7, pp. 1351–1356, 2002.

[17] J. Nilsson, “Real-time control systems with delays,” Ph.D. dissertation, Department of Automatic Control, Lund Institute

of Technology, 1998.

[18] J. Nilsson, B. Bernhardsson, and B. Wittenmark, “Stochastic analysis and control of real-time systems with random time

delays.” [Online]. Available: citeseer.nj.nec.com/101333.html

[19] Q. Ling and M. Lemmon, “Soft real-time scheduling of networked control systems with dropouts governed by a markov

chain,” in American Control Conference, June 2003, denver, CO.

[20] Y. Wang, D. Ho, and X. Liu, “Variance-constrained ﬁltering for uncertain stochastic systems with missing measurements,”

IEEE. Trans. Automat. Control, vol. 48, no. 7, pp. 1254–1258, 2003.

[21] S. Smith and P. Seiler, “Estimation with lossy measurements: jump estimators for jump systems,” IEEE. Trans. Automat.

Control, vol. 48, no. 12, pp. 2163–2171, 2003.

DRAFT

30

[22] T. Fortmann, Y. Bar-Shalom, M. Scheffe, and S. Gelfand, “Detection thresholds for tracking in clutter-a connection between

estimation and signal processing,” IEEE Transactions on Automatic Control, vol. AC-30, no. 3, pp. 221–228, March 1985.

[23] A. Matveev and A. Savkin, “The problem of state estimation via asynchronous communication channels with irregular

transmission times,” IEEE Transaction on Automatic Control, vol. 48, no. 4, pp. 670–676, 2003.

[24] R. H. Shumway and D. S. Stoffer, Time Series Analysis and Its Applications. Springer Verlag, March 2000.

[25] S. Lauritzen, Graphical Models. Clarendon Press, 1996.

[26] C. N. Hadjicostis and R. Touri, “Feedback control utilizing packet dropping network links,” in Proceedings of the 41st

IEEE Conference on Decision and Control, Las Vegas, NV, Dec 2002, invited.

DRAFT

