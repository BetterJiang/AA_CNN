Convexity,Classiﬁcation,andRiskBoundsPeterL.BARTLETT,MichaelI.JORDAN,andJonD.MCAULIFFEManyoftheclassiﬁcationalgorithmsdevelopedinthemachinelearningliterature,includingthesupportvectormachineandboosting,canbeviewedasminimumcontrastmethodsthatminimizeaconvexsurrogateofthe0–1lossfunction.Theconvexitymakesthesealgorithmscomputationallyefﬁcient.Theuseofasurrogate,however,hasstatisticalconsequencesthatmustbebalancedagainstthecomputationalvirtuesofconvexity.Tostudytheseissues,weprovideageneralquantitativerelationshipbetweentheriskasassessedusingthe0–1lossandtheriskasassessedusinganynonnegativesurrogatelossfunction.Weshowthatthisrelationshipgivesnontrivialupperboundsonexcessriskundertheweakestpossibleconditiononthelossfunction—thatitsatisﬁesapointwiseformofFisherconsistencyforclassiﬁcation.Therelationshipisbasedonasimplevariationaltransformationofthelossfunctionthatiseasytocomputeinmanyapplications.Wealsopresentareﬁnedversionofthisresultinthecaseoflownoise,andshowthatinthiscase,strictlyconvexlossfunctionsleadtofasterratesofconvergenceoftheriskthanwouldbeimpliedbystandarduniformconvergencearguments.Finally,wepresentapplicationsofourresultstotheestimationofconvergenceratesinfunctionclassesthatarescaledconvexhullsofaﬁnite-dimensionalbaseclass,withavarietyofcommonlyusedlossfunctions.KEYWORDS:Boosting;Convexoptimization;Empiricalprocesstheory;Machinelearning;Rademachercomplexity;Supportvectormachine.1.INTRODUCTIONConvexityhasbecomeanincreasinglyimportantthemeinappliedmathematicsandengineering,havingtakenonapromi-nentroleakintothatplayedbylinearityformanydecades.Buildingonthediscoveryofefﬁcientalgorithmsforlinearprograms,researchersinconvexoptimizationtheoryhavede-velopedcomputationallytractablemethodsforlargeclassesofconvexprograms(NesterovandNemirovskii1994).Manyﬁeldsinwhichoptimalityprinciplesformthecoreconceptualstructurehavebeenchangedsigniﬁcantlybytheintroductionofthesenewtechniques(BoydandVandenberghe2004).Convexityarisesinmanyguisesinstatisticsaswell,no-tablyinpropertiesassociatedwiththeexponentialfamilyofdistributions(Brown1986).Butonlyrecentlyhasthesystem-aticexploitationofthealgorithmicconsequencesofconvex-itybeguninstatistics.Oneappliedareainwhichthistrendhasbeenmostsalientismachinelearning,wherethefocushasbeenonlarge-scalestatisticalmodels,forwhichcompu-tationalefﬁciencyisanimperative.Manyofthemostpromi-nentmethodsstudiedinmachinelearningmakesigniﬁcantuseofconvexity;inparticular,supportvectormachines(Boser,Guyon,andVapnik1992;CortesandVapnik1995;CristianiniandShawe-Taylor2000;SchölkopfandSmola2002),boost-ing(FreundandSchapire1997;Collins,Schapire,andSinger2002;LebanonandLafferty2002),andvariationalinferenceforgraphicalmodels(Jordan,Ghahramani,Jaakkola,andSaul1999)areallbaseddirectlyonideasfromconvexoptimization.Thesemethodshavehadsigniﬁcantpracticalsuccessesinsuchappliedareasasbioinformatics,informationmanagement,andsignalprocessing(Federetal.2004;Joachims2002;Schölkopf,Tsuda,andVert2003).Ifalgorithmsfromconvexoptimizationaretocontinuetomakeinroadsintostatisticaltheoryandpractice,weneedtoun-PeterL.BartlettisProfessor(E-mail:bartlett@stat.berkeley.edu)andMichaelI.JordanisProfessor(E-mail:jordan@stat.berkeley.edu),DepartmentofStatisticsandtheComputerScienceDivision,andJonD.McAuliffewasaGraduateStudent(E-mail:jon@stat.berkeley.edu),DepartmentofStatistics,UniversityofCalifornia,Berkeley,CA94720,whenthisworkwasperformed.HeisnowAssistantProfessor,DepartmentofStatistics,UniversityofPennsyl-vania,Philadelphia,PA19104.ThisworkwassupportedbyNationalScienceFoundationgrants0412995and0434383andMURIgrantDAAD190210383fromtheARO.TheauthorsthankGillesBlanchard,OlivierBousquet,PascalMassart,RonMeir,ShaharMendelson,MartinWainwright,andBinYuforhelpfuldiscussions.derstandthesealgorithmsnotonlyfromacomputationalstand-point,butalsointermsoftheirstatisticalproperties.Whatarethestatisticalconsequencesofchoosingmodelsandestima-tionproceduressoastoexploitthecomputationaladvantagesofconvexity?Inthisarticlewestudythisquestioninthecontextofdis-criminantanalysis,atopicreferredtoasclassiﬁcationinthemachinelearningﬁeld.Weconsiderthesettinginwhichaco-variatevectorX∈XistobeclassiﬁedaccordingtoabinaryresponseY∈{−1,1}.Thegoalistochooseadiscriminantfunctionf:X→R,fromaclassoffunctionsF,suchthatthesignoff(X)isanaccuratepredictionofYunderanunknownjointmeasurePon(X,Y).Wefocuson0–1loss;thus,let-ting(cid:1)(α)denoteanindicatorfunctionthatis1ifα≤0and0otherwise,wewishtochoosef∈FthatminimizestheriskR(f)=E(cid:1)(Yf(X))=P(Y(cid:4)=sign(f(X))).GivenasampleDn=((X1,Y1),...,(Xn,Yn)),itisnaturaltoconsiderestimationproceduresbasedonminimizingthesampleaverageoftheloss,ˆR(f)=1n(cid:1)ni=1(cid:1)(Yif(Xi)).Asiswellknown,however,suchaprocedureiscomputationallyin-tractableformanynontrivialclassesoffunctions(see,e.g.,Arora,Babai,Stern,andSweedyk1997).Indeed,thelossfunc-tion(cid:1)(Yf(X))isnonconvexinits(scalar)argument,andal-thoughnotaproof,thissuggestsasourceofthedifﬁculty.Moreover,itsuggeststhatwemightbaseatractableestima-tionprocedureonminimizationofaconvexsurrogateφ(α)fortheloss.Inparticular,ifFconsistsoffunctionsthatarelin-earinaparametervectorθ,thentheexpectationofφ(Yf(X))isconvexinθ(byconvexityofφandlinearityofexpectation).Givenaconvexparameterspace,weobtainaconvexprogramandcanexploitthemethodsofconvexoptimization.Awidevarietyofclassiﬁcationmethodsarebasedonthistactic;inpar-ticular,Figure1showsthe(upper-bounding)convexsurrogatesassociatedwiththesupportvectormachine(CortesandVapnik1995),AdaBoost(FreundandSchapire1997),andlogisticre-gression(Friedman,Hastie,andTibshirani2000).Inthema-chinelearningliterature,theseconvexity-basedmethodshavelargelydisplacedearliernonconvexmethods,suchasneuralnetworks.©2006AmericanStatisticalAssociationJournaloftheAmericanStatisticalAssociationMarch2006,Vol.101,No.473,TheoryandMethodsDOI10.1198/016214505000000907138Bartlett,Jordan,andMcAuliffe:Convexity,Classiﬁcation,andRiskBounds139Figure1.APlotofthe0–1LossFunctionandSurrogatesCorre-spondingtoVariousPracticalClassiﬁers(0–1;exponential;hinge;logistic;truncatedquadratic).Thesefunctionsareplottedasafunctionofthemarginα=yf(x).Notethataclassiﬁcationerrorismadeifandonlyifthemarginisnegative;thusthe0–1lossisastepfunctionthatisequalto1fornegativevaluesoftheabscissa.Thecurvelabeled“logistic”isthenegativelog-likelihood,orscaledde-viance,underalogisticregressionmodel,“hinge”isthepiecewise-linearlossusedinthesupportvectormachine,and“exponential”istheexpo-nentiallossusedbytheAdaBoostalgorithm.Thedevianceisscaledsoastomajorizethe0–1loss;seeLemma4.Abasicstatisticalunderstandingoftheconvexity-basedset-tinghasbeguntoemerge.Inparticular,whenappropriateregularizationconditionsareimposed,itispossibletodemon-stratetheBayes-riskconsistencyofmethodsbasedonminimiz-ingconvexsurrogatesfor0–1loss.LugosiandVayatis(2004)providedsucharesultundertheassumptionthatthesurro-gateφisdifferentiable,monotoneandstrictlyconvexandsat-isﬁesφ(0)=1.ThishandlesallofthecasesshowninFigure1exceptthesupportvectormachine.Steinwart(2005)demon-stratedconsistencyforthesupportvectormachineaswell,inageneralsettingwhereFistakentobeareproducingkernelHilbertspaceandφisassumedtobecontinuous.OtherresultsonBayes-riskconsistencyhavebeenpresentedbyBreiman(2004),Jiang(2004),MannorandMeir(2001),andMannor,Meir,andZhang(2002).ConsistencyresultsprovidereassurancethatoptimizingasurrogatedoesnotultimatelyhinderthesearchforafunctionthatachievestheBayesrisk,andthusallowsuchasearchtopro-ceedwithinthescopeofcomputationallyefﬁcientalgorithms.Thereis,however,anadditionalmotivationforworkingwithsurrogatesof0–1lossbeyondthecomputationalimperative.Minimizingthesampleaverageofanappropriatelybehavedlossfunctionhasaregularizingeffect;itispossibletoobtainuniformupperboundsontheriskofafunctionthatminimizestheempiricalaverageofthelossφ,evenforclassesthataresorichthatnosuchupperboundsarepossibleforthemini-mizeroftheempiricalaverageofthe0–1loss.Indeed,anum-berofsuchresultshavebeenobtainedforfunctionclasseswithinﬁniteVapnik–Chervonenkis(VC)dimension(Bartlett1998,Shawe-Taylor,Bartlett,Williamson,andAnthony1998),suchasthefunctionclassesusedbyAdaBoost(see,e.g.,Schapire,Freund,Bartlett,andLee1998;KoltchinskiiandPanchenko2002).Theseupperboundsprovideguidanceformodelselec-tionandinparticularhelpguidedata-dependentchoicesofreg-ularizationparameters.Tocarrythisagendafurther,weneedtoﬁndgeneralquantita-tiverelationshipsbetweentheapproximationandestimationer-rorsassociatedwithφandthoseassociatedwith0–1loss.ThispointwasemphasizedbyZhang(2004),whopresentedseveralexamplesofsuchrelationships.HerewesimplifyandextendZhang’sresults,developingageneralmethodologyforﬁndingquantitativerelationshipsbetweentheriskassociatedwithφandtheriskassociatedwith0–1loss.Inparticular,letR(f)de-notetheriskbasedon0–1loss,andletR∗=inffR(f)denotetheBayesrisk.Similarly,letRφ(f)=Eφ(Yf(X))becalledthe“φ-risk,”andletR∗φ=inffRφ(f)denotethe“optimalφ-risk.”Weshowthatforallmeasurablef,ψ(cid:2)R(f)−R∗(cid:3)≤Rφ(f)−R∗φ,(1)foranondecreasingfunctionψ:[0,1]→[0,∞).Moreover,wepresentageneralvariationalrepresentationofψintermsofφ,andshowthatthisfunctionistheoptimalupperboundoftheform(1),inthesensethatanyotherfunctionthatsatisﬁes(1)forallmeasurablefiseverywherenolargerthanψ.Thisresultsuggeststhatifψiswell-behaved,thenminimiza-tionofRφ(f)mayprovideareasonablesurrogateforminimiza-tionofR(f).Moreover,theresultprovidesaquantitativewaytotransferassessmentsofstatisticalerrorintermsof“excessφ-risk,”Rφ(f)−R∗φ,intoassessmentsoferrorintermsof“ex-cessrisk,”R(f)−R∗.Althoughourprincipalgoalistounderstandtheimplica-tionsofconvexityinclassiﬁcation,wedonotimposeacon-vexityassumptiononφattheoutset.Indeed,whereassuchconditionsasconvexity,continuity,anddifferentiabilityofφareeasytoverifyandhavenaturalrelationshipstooptimizationprocedures,itisnotimmediatelyobvioushowtorelatesuchconditionstotheirstatisticalconsequences.Thusweconsidertheweakestpossibleconditiononφ:thatitis“classiﬁcation-calibrated,”whichisessentiallyapointwiseformofFishercon-sistencyforclassiﬁcation(Lin2004).Inparticular,ifwedeﬁneη(x)=P(Y=1|X=x),thenφisclassiﬁcation-calibratedif,forxsuchthatη(x)(cid:4)=1/2,everyminimizerf∗ofthecondi-tionalexpectationE[φ(Yf∗(X))|X=x]hasthesamesignastheBayesdecisionrule,sign(2η(x)−1).Weshowthattheupperbound(1)onexcessriskintermsofexcessφ-riskisnontriv-ialpreciselywhenφisclassiﬁcation-calibrated.Obviously,nosuchboundispossiblewhenφisnotclassiﬁcation-calibrated.Thedifﬁcultyofapatternclassiﬁcationproblemiscloselyre-latedtothebehavioroftheposteriorprobabilityη(X).Inmanypracticalproblems,itisreasonabletoassumethatformostX,η(X)isnottoocloseto1/2.MammenandTsybakov(1999)introducedanelegantformulationofsuchanassumption,andTsybakov(2004)consideredtheconvergencerateoftheriskofafunctionthatminimizesempiricalriskoversomeﬁxedclassF.Heshowedthatundertheassumptionoflownoise,theriskconvergessurprisinglyquicklytotheminimumover140JournaloftheAmericanStatisticalAssociation,March2006theclass.Iftheminimumriskisnonzero,thenwemightexpectaconvergenceratenofasterthan1/√n.However,underTsy-bakov’sassumption,theconvergenceratecanbeasfastas1/n.Weshowthatminimizingtheempiricalφ-riskalsoleadstosurprisinglyfastconvergenceratesunderthisassumption.Inparticular,ifφisuniformlyconvex,thentheminimizeroftheempiricalφ-riskhasφ-riskthatconvergesquicklytoitsoptimalvalue,andthenoiseassumptionallowsanimprovementintherelationshipbetweenexcessφ-riskandexcessrisk.Theseresultssuggestageneralinterpretationofpatternclas-siﬁcationmethodsinvolvingaconvexcontrastfunction.Itiscommontoviewtheexcessriskasacombinationofanestima-tiontermandanapproximationterm,R(f)−R∗=(cid:4)R(f)−infg∈FR(g)(cid:5)+(cid:4)infg∈FR(g)−R∗(cid:5).However,choosingafunctionwithrisknear-minimaloveraclassF—thatis,ﬁndinganfforwhichtheestimationtermaboveiscloseto0—is,inaminimaxsetting,equivalenttotheproblemofminimizingempiricalrisk,andhenceiscomputa-tionallyinfeasiblefortypicalclassesFofinterest.Indeed,forclassestypicallyusedbyboostingandkernelmethods,thees-timationterminthisexpressiondoesnotconvergeto0fortheminimizeroftheempiricalrisk.Incontrast,wecanalsosplittheupperboundonexcessriskintoanestimationtermandanapproximationterm,ψ(cid:2)R(f)−R∗(cid:3)≤Rφ(f)−R∗φ=(cid:4)Rφ(f)−infg∈FRφ(g)(cid:5)+(cid:4)infg∈FRφ(g)−R∗φ(cid:5).Often,itispossibletominimizeφ-riskefﬁciently.Thus,al-thoughﬁndinganfwithnear-minimalriskmightbecompu-tationallyinfeasible,ﬁndinganfforwhichthisupperboundonriskisnear-minimalcanbefeasible.Thearticleisorganizedasfollows.Section2presentsbasicdeﬁnitionsandastatementandproofof(1).Italsointroducestheconvexityassumption,andshowshowitsimpliﬁesthecom-putationofψ.Section3presentsareﬁnedversionofourmainresultinthesettingoflownoise.Section4presentsboundsontherateofconvergenceoftheφ-riskoftheempiricalminimizerforstrictlyconvexφ,anddescribesapplicationsoftheseresultstoconvexfunctionclasses,suchasthoseusedbyAdaBoost.Italsodescribessimulationsthatillustratethetheoreticalre-sults.Section5presentsourconclusions.ProofsofallofourresultsarepresentedeitherinthemaintextorinAppendixA.2.RELATINGEXCESSRISKTOEXCESSφ-RISKTherearethreesourcesoferrortoconsiderinastatisticalanalysisofclassiﬁcationproblems:theclassicalestimationer-rorduetoﬁnitesamplesize,theclassicalapproximationerrorduetothesizeofthefunctionspaceF,andanadditionalsourceofapproximationerrorduetotheuseofasurrogateinplaceofthe0–1lossfunction.Itisthislastsourceoferrorthatisourfo-cusinthissection.Thus,throughoutthissection,weworkwithpopulationexpectationsandassumethatFisthesetofallmea-surablefunctions.Thisallowsustoignoreerrorsduetothesizeofthesampleandthesizeofthefunctionspace,andfocusontheerrorduetotheuseofasurrogateforthe0–1lossfunction.Wefollowthetraditionintheclassiﬁcationliteratureandre-fertothefunctionφasalossfunction,becauseitisafunctionthatistobeminimizedtoobtainadiscriminant.Moreprecisely,φ(Yf(X))isgenerallyreferredtoasa“margin-basedlossfunc-tion,”wherethequantityYf(X)isknownasthe“margin.”(Itisworthnotingthatmargin-basedlossfunctionsareratherdiffer-entfromdistancemetrics,apointthatweexploreinApp.B.)Thisambiguityintheuseof“loss”willnotconfuse;inpar-ticular,wewillbecarefultodistinguishtherisk,whichisanexpectationover0–1loss,fromthe“φ-risk,”whichisanex-pectationoverφ.Ourgoalinthissectionistorelatethesetwoquantities.2.1SetupLet(X×{−1,1},G⊗2{−1,1},P)beaprobabilityspace.LetXbetheidentityfunctiononX,andletYbetheidentityfunc-tionon{−1,1},sothatPisthedistributionof(X,Y);thatis,forA∈G⊗2{−1,1},P((X,Y)∈A)=P(A).LetPXon(X,G)bethemarginaldistributionofX,andletη:X→[0,1]beameasurablefunctionsuchthatη(X)isaversionofP(Y=1|X).Throughoutthissection,fisunderstoodasameasurablemap-pingfromXintoR.Deﬁnethe{0,1}-risk,orjustrisk,offasR(f)=P(cid:2)sign(f(X))(cid:4)=Y(cid:3),wheresign(α)=1forα>0and−1otherwise.[Theparticularchoiceofthevalueofsign(0)isnotimportant,butweneedtoﬁxsomevaluein{±1}forthedeﬁnitionsthatfollow.]BasedonaniidsampleDn=((X1,Y1),...,(Xn,Yn)),wewanttochooseafunctionfnwithsmallrisk.DeﬁnetheBayesrisk,R∗=inffR(f),wheretheinﬁmumisoverallmeasurablef.Thenanyfsatisfyingsign(f(X))=sign(η(X)−1/2)a.s.on{η(X)(cid:4)=1/2}hasR(f)=R∗.Fixafunctionφ:R→[0,∞).Deﬁnetheφ-riskoffasRφ(f)=Eφ(Yf(X)).LetFbeaclassoffunctionsf:X→R.Letfn=ˆfφbeafunctioninFthatminimizestheempiricalexpectationofφ(Yf(X)),ˆRφ(f)=ˆEφ(Yf(X))=1nn(cid:6)i=1φ(Yif(Xi)).Thuswetreatφasspecifyingacontrastfunctionthatistobeminimizedindeterminingthediscriminantfunctionfn.2.2BasicConditionsontheLossFunctionDeﬁnetheconditionalφ-risk,E(cid:2)φ(Yf(X))|X=x(cid:3)=η(x)φ(f(x))+(1−η(x))φ(−f(x))a.e.(x).Itisusefultothinkoftheconditionalφ-riskintermsofagenericconditionalprobabilityη∈[0,1]andagenericclas-siﬁervalueα∈R.Toexpressthisviewpoint,weintroducethegenericconditionalφ-risk,Cη(α)=ηφ(α)+(1−η)φ(−α).Thenotationsuppressesthedependenceonφ.Thegenericcon-ditionalφ-riskcoincideswiththeconditionalφ-riskoffatBartlett,Jordan,andMcAuliffe:Convexity,Classiﬁcation,andRiskBounds141x∈Xifwetakeη=η(x)andα=f(x).Herevaryingαinthegenericformulationcorrespondstovaryingfintheoriginalformulation,forﬁxedx.Asausefulillustrationforthedeﬁni-tionsthatfollow,considerasingletondomainX={x0}.Min-imizingtheφ-riskcorrespondstochoosingf(x0)tominimizeCη(x0)(f(x0)).Forη∈[0,1],deﬁnetheoptimalconditionalφ-risk,H(η)=infα∈RCη(α)=infα∈R(cid:2)ηφ(α)+(1−η)φ(−α)(cid:3).Thentheoptimalφ-risksatisﬁesR∗φ:=inffRφ(f)=EH(η(X)),wheretheinﬁmumisovermeasurablefunctions.IftheinﬁmuminthedeﬁnitionofH(η)isuniquelyattainedforsomeα,wecandeﬁneα∗:[0,1]→Rbyα∗(η)=argminα∈RCη(α).Inthatcase,wedeﬁnef∗φ:X→R,uptoPX-nullsets,byf∗φ(x)=argminα∈RE(cid:2)φ(Yα)|X=x(cid:3)=α∗(η(x))andthenRφ(f∗φ)=EH(η(X))=R∗φ.Forη∈[0,1],deﬁneH−(η)=infα:α(2η−1)≤0Cη(α).Thisistheoptimalvalueoftheconditionalφ-risk,undertheconstraintthatthesignoftheargumentαdisagreeswiththatof2η−1.Wenowturntothebasicconditionthatweimposeonφ.ThisconditiongeneralizestherequirementthattheminimizerofCη(α)(ifitexists)hasthecorrectsign.ThisisaminimalconditionthatcanbeviewedasapointwiseformofFishercon-sistencyforclassiﬁcation.Deﬁnition1.Wesaythatφisclassiﬁcation-calibratedif,foranyη(cid:4)=1/2,H−(η)>H(η).ConsideragainasingletondomainX={x0}.Minimizingφ-riskcorrespondstochoosingf(x0)tominimizeCη(x0)(f(x0)).Theclassiﬁcation-calibratedconditionrequiresthataddingtheconstraintthatf(x0)hastheincorrectsignalwaysleadstoastrictlylargerφ-risk.Example1(Exponentialloss).Considerthelossfunctionφ(α)=exp(−α)usedbyAdaBoost.Figure2(a)showsφ(α),φ(−α),andthegenericconditionalφ-riskCη(α)forη=.3andη=.7.InthiscaseφisstrictlyconvexonR,andhenceCη(α)isalsostrictlyconvexonR,foreveryη.SoCηiseitherminimalatauniquestationarypointorattainsnominimum.Indeed,ifη=0,thenCη(α)→0asα→−∞;ifη=1,thenCη(α)→0asα→∞.ThuswehaveH(0)=H(1)=0forexponentialloss.Forη∈(0,1),solvingforthestationarypointyieldsthe(a)(b)Figure2.ExponentialLoss.(a)φ(α)(),itsreﬂectionφ(−α)(),andtwodifferentconvexcombinationsofthesefunctions[C.3(α);C.7(α)],forη=.3andη=.7.NotethattheminimaofthesecombinationsarethevaluesH(η),andtheminimizingargu-mentsarethevaluesα∗(η).(b)H(η)()andα∗(η)()plottedasafunctionofη,andtheψ-transformψ(θ)().uniqueminimizerα∗(η)=12log(cid:7)η1−η(cid:8).WemaythensimplifytheidentityH(η)=Cη(α∗(η))toobtainH(η)=2(cid:9)η(1−η).Notethatthisexpressionisalsocorrectforηequalto0or1.Figure2(b)showsthegraphsofα∗andHovertheinterval[0,1].ItiseasytocheckthatH−(η)≡exp(0)=1,andthisisstrictlygreaterthan2√η(1−η)whenη(cid:4)=1/2,sotheexponentiallossisclassiﬁcation-calibrated.2.3Theψ-TransformandtheRelationshipBetweenExcessRisksWebeginbydeﬁningafunctionaltransformofthelossfunc-tion.ThenTheorem1showsthatthistransformgivesoptimalboundsonexcessriskintermsofexcessφ-risk.Deﬁnition2.Wedeﬁnetheψ-transformofalossfunc-tionasfollows.Givenφ:R→[0,∞),deﬁnethefunctionψ:[−1,1]→[0,∞)byψ=˜ψ∗∗,where˜ψ(θ)=H−(cid:7)1+θ2(cid:8)−H(cid:7)1+θ2(cid:8),andg∗∗:[−1,1]→RistheFenchel–Legendrebiconjugateofg:[−1,1]→R,whichischaracterizedbyepig∗∗=coepig.142JournaloftheAmericanStatisticalAssociation,March2006HerecoSistheclosureoftheconvexhullofthesetS,andepigistheepigraphofthefunctiong,thatis,theset{(x,t):x∈[0,1],g(x)≤t}.Thenonnegativityofψisestab-lishedinLemma2,part7.Recallthatgisconvexifandonlyifepigisaconvexset,andgisclosed(epigisaclosedset)ifandonlyifgislowersemicontinuous(Rockafellar1997).ByLemma2,part5,˜ψiscontinuous,so,infact,theclosureoperationinDeﬁnition2isvacuous.Wethushavethatψissimplythefunctionalconvexhullof˜ψ(alsoknownasthegreatestconvexminorantof˜ψ),ψ=co˜ψ.Thisisequivalenttotheepigraphconvexhullconditionofthedeﬁnition;thatis,ψisthelargestconvexlowerboundon˜ψ.Thisimpliesthatψ=˜ψifandonlyif˜ψisconvex;seeExam-ple9foralossfunctionwhere˜ψisnotconvex.Theimportanceoftheψ-transformisshownbythefollowingtheorem.Theorem1.1.Foranynonnegativelossfunctionφ,anymeasurablef:X→R,andanyprobabilitydistributiononX×{±1},ψ(cid:2)R(f)−R∗(cid:3)≤Rφ(f)−R∗φ.2.Supposethat|X|≥2.Foranynonnegativelossfunc-tionφ,any>0,andanyθ∈[0,1],thereisaprobabilitydistributiononX×{±1}andafunctionf:X→RsuchthatR(f)−R∗=θandψ(θ)≤Rφ(f)−R∗φ≤ψ(θ)+.3.Thefollowingconditionsareequivalent:a.φisclassiﬁcation-calibrated.b.Foranysequence(θi)in[0,1],ψ(θi)→0ifandonlyifθi→0.c.Foreverysequenceofmeasurablefunctionsfi:X→RandeveryprobabilitydistributiononX×{±1},Rφ(fi)→R∗φimpliesthatR(fi)→R∗.Herewementionthatclassiﬁcation-calibrationimpliesψisinvertibleon[0,1],sointhatcaseitismeaningfultowritetheupperboundonexcessriskinTheorem1,part1asψ−1(Rφ(f)−R∗φ).InvertibilityfollowsfromconvexityofψtogetherwithLemma2,parts6,8,and9.Zhang(2004)hasgivenacomparisontheoremlikeparts1and3bofthistheoremforconvexφthatsatisfycertainconditions.Theseconditionsimplyanassumptionontherateofgrowth(andconvexity)of˜ψ.LugosiandVayatis(2004)showedthatalimitingresultlikepart3choldsforstrictlycon-vex,differentiable,monotonicφ.Thefollowingtheoremshowsthatifφisconvex,classiﬁcation-calibrationisequivalenttoasimplederivativeconditiononφat0.Clearly,theconclusionsofTheorem1holdunderweakerconditionsthanthoseassumedbyZhang(2004)orLugosiandVayatis(2004).Steinwart(2005)hasshownthatifφiscontinuousandclassiﬁcation-calibrated,thenRφ(fi)→R∗φimpliesthatR(fi)→R∗.Theo-rem1showsthatwemayobtainamorequantitativestatementoftherelationshipbetweentheseexcessrisks,underweakerconditions.Itisusefultonotethatwhenφisconvex,classiﬁcation-calibrationisequivalenttoaconditiononthederivativeofφat0,andinthatcasetheψ-transformtakesasimpliﬁedform.Theorem2.1.Letφbeconvex.Thenφisclassiﬁcation-calibratedifandonlyifitisdifferentiableat0andφ(cid:11)(0)<0.2.Ifφisconvexandclassiﬁcation-calibrated,thenψ(θ)=φ(0)−H(cid:7)1+θ2(cid:8).Intheremainderofthissectionwepresenttwoprelimi-narylemmasandthenpresentaproofofTheorem1.NotethatSection3presentsseveralexamplesofcalculationsoftheψ-transform;somereadersmaywanttovisitthatsectionﬁrstbeforeproceedingtotheproof.Thefollowingelementarylemmawillbeusefulthroughoutthearticle.Lemma1.Supposethatg:R→Risconvexandg(0)=0.Then1.Forallλ∈[0,1]andx∈R,g(λx)≤λg(x).2.Forallx>0,0≤y≤x,g(y)≤yxg(x).3.g(x)/xisincreasingon(0,∞).Proof.Forpart1,g(λx)=g(λx+(1−λ)0)≤λg(x)+(1−λ)g(0)=λg(x).Toseepart2,putλ=y/xin1.Forpart3,rewritepart2asg(y)/y≤g(x)/x.Lemma2.Foranynonnegativelossfunctionφ,thefunctionsH,H−,andψhavethefollowingproperties:1.HandH−aresymmetricabout1/2andψissymmetricabout0.Forallη∈[0,1],H(η)=H(1−η),H−(η)=H−(1−η),andψ(η)=ψ(−η).2.Hisconcaveand,for0≤η≤1,itsatisﬁesH(η)≤H(cid:7)12(cid:8)=H−(cid:7)12(cid:8).3.Ifφisclassiﬁcation-calibrated,thenH(η)<H(1/2)forallη(cid:4)=1/2.4.H−isconcaveon[0,1/2]andon[1/2,1],and,for0≤η≤1,itsatisﬁesH−(η)≥H(η).5.HandH−arecontinuouson[0,1].6.ψand˜ψarecontinuouson[−1,1].7.ψisnonnegativeandminimalat0.8.ψ(0)=0.9.Thefollowingstatementsareequivalent:a.φisclassiﬁcation-calibrated.b.ψ(θ)>0forallθ∈(0,1].Bartlett,Jordan,andMcAuliffe:Convexity,Classiﬁcation,andRiskBounds143FortheproofseeAppendixA.ProofofTheorem1.Forpart1,itisstraightforwardtoshowthatR(f)−R∗=R(f)−R(η−1/2)=E(cid:2)1(cid:10)sign(f(X))(cid:4)=sign(η(X)−1/2)(cid:11)|2η(X)−1|(cid:3),where1[]is1ifthepredicateistrueand0otherwise(see,e.g.,Devroye,Györﬁ,andLugosi1996).WecanapplyJensen’sinequality,becauseψisconvexbydeﬁnition,andthefactthatψ(0)=0(Lemma2,part8)toshowthatψ(cid:2)R(f)−R∗(cid:3)≤Eψ(cid:2)1(cid:10)sign(f(X))(cid:4)=sign(η(X)−1/2)(cid:11)|2η(X)−1|(cid:3)=E(cid:2)1(cid:10)sign(f(X))(cid:4)=sign(η(X)−1/2)(cid:11)ψ(cid:2)|2η(X)−1|(cid:3)(cid:3).Now,fromthedeﬁnitionofψ,weknowthatψ(θ)≤˜ψ(θ),sowehaveψ(cid:2)R(f)−R∗(cid:3)≤E(cid:2)1(cid:10)sign(f(X))(cid:4)=sign(η(X)−1/2)(cid:11)˜ψ(cid:2)|2η(X)−1|(cid:3)(cid:3)=E(cid:2)1(cid:10)sign(f(X))(cid:4)=sign(η(X)−1/2)(cid:11)×(cid:2)H−(η(X))−H(η(X))(cid:3)(cid:3)=E(cid:4)1(cid:10)sign(f(X))(cid:4)=sign(η(X)−1/2)(cid:11)×(cid:4)infα:α(2η(X)−1)≤0Cη(X)(α)−H(η(X))(cid:5)(cid:5)≤E(cid:2)Cη(X)(f(X))−H(η(X))(cid:3)=Rφ(f)−R∗φ,wherewehaveusedthefactthatforanyx,andinparticularwhensign(f(x))=sign(η(x)−1/2),wehaveCη(x)(f(x))≥H(η(x)).Forpart2,theﬁrstinequalityisfrompart1.Forthesecondinequality,ﬁx>0andθ∈[0,1].Fromthedeﬁnitionofψ,wecanchooseγ,α1,α2∈[0,1],forwhichθ=γα1+(1−γ)α2andψ(θ)≥γ˜ψ(α1)+(1−γ)˜ψ(α2)−/2.Choosedistinctx1,x2∈X,andchoosePXsuchthatPX{x1}=γ,PX{x2}=1−γ,η(x1)=(1+α1)/2,andη(x2)=(1+α2)/2.FromthedeﬁnitionofH−,wecanchoosef:X→Rsuchthatf(x1)≤0,f(x2)≤0,Cη(x1)(f(x1))≤H−(η(x1))+/2,andCη(x2)(f(x2))≤H−(η(x2))+/2.ThenwehaveRφ(f)−R∗φ=Eφ(Yf(X))−infgEφ(Yg(X))=γ(cid:2)Cη(x1)(f(x1))−H(η(x1))(cid:3)+(1−γ)(cid:2)Cη(x2)(f(x2))−H(η(x2))(cid:3)≤γ(cid:2)H−(η(x1))−H(η(x1))(cid:3)+(1−γ)(cid:2)H−(η(x2))−H(η(x2))(cid:3)+/2=γ˜ψ(α1)+(1−γ)˜ψ(α2)+/2≤ψ(θ)+.Furthermore,becausesign(f(x1))=sign(f(x2))=−1butη(x1),η(x2)≥1/2,R(f)−R∗=E|2η(X)−1|=γ(2η(x1)−1)+(1−γ)(2η(x2)−1)=θ.Forpart3,ﬁrstnotethatforanyφ,ψiscontinuouson[0,1]andψ(0)=0byLemma2,parts6and8,andhenceθi→0impliesthatψ(θi)→0.Thuswecanreplacecondition3bby3b(cid:11).Foranysequence(θi)in[0,1],ψ(θi)→0impliesthatθi→0.Toseethatpart(3a)implies3b(cid:11),letφbeclassiﬁcation-calibrated,andlet(θi)beasequencethatdoesnotconvergeto0.Deﬁnec=limsupθi>0,andpasstoasubsequencewithlimθi=c.Thenlimψ(θi)=ψ(c)bycontinuity,andψ(c)>0byclassiﬁcation-calibration(Lemma2,part9).Thus,fortheoriginalsequence(θi),weseelimsupψ(θi)>0,sowecannothaveψ(θi)→0.Toseethatpart3b(cid:11)implies3c,supposethatRφ(fi)→R∗φ.Bypart1,ψ(R(fi)−R∗)→0,andpart3b(cid:11)impliesthatR(fi)→R∗.Finally,toseethatpart3cimpliespart3a,supposethatφisnotclassiﬁcation-calibrated.Bydeﬁnition,wecanchooseη(cid:4)=1/2andasequenceα1,α2,...suchthatsign(αi(η−1/2))=−1butCη(αi)→H(η).Fixx∈Xandchoosetheprobabilitydis-tributionPsothatPX{x}=1andP(Y=1|X=x)=η.Deﬁneasequenceoffunctionsfi:X→Rforwhichfi(x)=αi.ThenlimR(fi)>R∗,andthisistrueforanyinﬁnitesubsequence.ButCη(αi)→H(η)impliesthatRφ(fi)→R∗φ.2.4ExamplesInthissectionwepresentseveralexamplesofthecomputa-tionoftheψ-transform.Example2(Exponentialloss).Becauseφ(α)=exp(−α)isconvex,differentiable,anddecreasing,Theorem2,part1im-pliesthatitisclassiﬁcation-calibrated,aswehaveseen.WealsonotedthatH(η)=2√η(1−η).FromTheorem2,part2,ψ(θ)=1−(cid:9)1−θ2.Figure2(b)showsthegraphofψovertheinterval[0,1].(FromLemma2,part1,ψ(θ)=ψ(−θ)foranyψandanyθ∈[−1,1].)Example3(Truncatedquadraticloss).Nowconsiderφ(α)=(max{1−α,0})2,asdepictedtogetherwithφ(−α),C.3(α),andC.7(α)inFigure3(a).Thisfunctionisconvex,differentiable,anddecreasingatzero,andthusisclassiﬁcation-calibrated.Ifη=0,thenitisclearthatanyα∈(−∞,−1]makesCη(α)vanish.Similarly,anyα∈[1,∞)makestheconditionalφ-riskvanishwhenη=1.Butwhen0<η<1,Cηisstrictlyconvexwitha(unique)stationarypoint,andsolvingforityieldsα∗(η)=2η−1.(2)Notethat,althoughα∗isinprincipleundeﬁnedat0and1,wecouldchoosetoﬁxα∗(0)=−1andα∗(1)=1,whicharevalidsettings.Thiswouldextend(2)toallof[0,1].144JournaloftheAmericanStatisticalAssociation,March2006(a)(b)Figure3.TruncatedQuadraticLoss.(a)φ(α);φ(−α);C.3(α);C.7(α).(b)α∗(η);H(η);Ψ(θ).AsinExample1,wemaysimplifytheidentityH(η)=Cη(α∗(η))for0<η<1toobtainH(η)=4η(1−η),whichisalsocorrectforη=0and1,asnoted.Thus,ψ(θ)=θ2.Figure3(b)showsα∗,H,andψ.Example4(Hingeloss).Herewetakeφ(α)=max{1−α,0},whichisshowninFigure4(a)alongwithφ(−α),C.3(α),andC.7(α).Again,φisconvexanddifferentiableat0andhasnegativederivativeat0,soitisclassiﬁcation-calibrated.Bydi-rectconsiderationofthepiecewise-linearformofCη(α),itiseasytoseethatforη=0,eachα≤−1makesCη(α)van-ish,justasinExample3.Thesameholdsforα≥1whenη=1.Nowforη∈(0,1),weseethatCηdecreasesstrictlyon(−∞,−1]andincreasesstrictlyon[1,∞).Thusanyminimamustliein[−1,1].ButCηislinearon[−1,1],sothemini-mummustbeattainedat1forη>1/2,−1forη<1/2,andanywherein[−1,1]forη=1/2.Wehavearguedthatα∗(η)=sign(η−1/2)(3)forallη∈(0,1)otherthan1/2.Because(3)yieldsvalidmin-imaat0,1/2,and1aswell,wecouldchoosetoextendittotheentireunitinterval.Regardless,asimpledirectveriﬁcationasinthepreviousexamplesshowsH(η)=2min{η,1−η}for0≤η≤1,andsoψ(θ)=|θ|.Wepresentα∗,H,andψinFigure4(b).(a)(b)Figure4.HingeLoss.(a)φ(α);φ(−α);C.3(α);C.7(α).(b)α∗(η);H(η);Ψ(θ).Example5(Distance-weighteddiscrimination).MarronandTodd(2002)introducedthedistance-weighteddiscriminationmethodforhigh-dimensional,small-sample-sizeproblems.Thismethodchoosesanelementoftheunitballinarepro-ducingkernelHilbertspacetominimizeacertaincriterion.Itisstraightforwardtoshowthatthiscriterionisanempiricalφ-risk,forthelossfunctionφ(α)=1αifα≥γ1α(cid:7)2−αγ(cid:8)otherwise,whereγisapositiveconstant.Notethatφisconvex,differen-tiable,decreasing,andhenceclassiﬁcation-calibrated.ItiseasytoverifythatH(η)=1η(1+2min{η,1−η}),andhenceψ(θ)=|θ|γ.Example6(ARC–X4).Breiman(1999)proposedARC–X4,aboostingalgorithmbasedontheconvexcostfunctionφ(α)=|1−α|5.Moregenerally,considerthefunctionφ(α)=|1−α|pforp>1.Thisisconvexandhasφ(cid:11)(0)<0,soitisclassiﬁcation-calibrated.Furthermore,itiseasytoverifythatforη∈(0,1),α∗(η)=η1/(p−1)−(1−η)1/(p−1)η1/(p−1)+(1−η)1/(p−1),andsoH(η)=2pη(1−η)((1−η)1/(p−1)+η1/(p−1))p−1Bartlett,Jordan,andMcAuliffe:Convexity,Classiﬁcation,andRiskBounds145andψ(θ)=φ(0)−H(cid:7)1−θ2(cid:8)=1−2p−1(1−θ2)((1−θ)1/(p−1)+(1+θ)1/(p−1))p−1.Example7(Sigmoidloss).Weconcludebyexamininganonconvexlossfunction.Letφ(α)=1−tanh(kα)forsomeﬁxedk>0.Figure5(a)depictsφ(α)withk=1,aswellasφ(−α),C.3(α),andC.7(α).Usingthefactthattanhisanoddfunction,wecanrewritetheconditionalφ-riskasCη(α)=1+(1−2η)tanh(kα).(4)Fromthisexpression,twofactsareclear.First,whenη=1/2,everyαminimizesCη(α),becauseitisidentically1.Second,whenη(cid:4)=1/2,Cη(α)attainsnominimum,becausetanhhasnomaximalorminimalvalueonR.Henceα∗isnotdeﬁnedforanyη.Inspecting(4),for0≤η<1/2weobtainH(η)=2ηbylet-tingα→−∞.Analogously,whenα→∞,wegetH(η)=2(1−η)for1/2<η≤1.ThuswehaveH(η)=2min{η,1−η},0≤η≤1.BecauseH−((1+θ)/2)≡φ(0)=1,wehave˜ψ(θ)=|θ|,andconvexitygivesψ=˜ψ.WepresentHandψinFig-ure5(b).Finally,theforegoingconsiderationsimplythatsig-moidlossisclassiﬁcation-calibrated,providedthatwenotethatthedeﬁnitionofclassiﬁcation-calibrationrequiresnothingwhenη=1/2.(a)(b)Figure5.SigmoidLoss.(a)φ(α);φ(−α);C.3(α);C.7(α).(b)H(η);Ψ(θ).Thefollowingexampleillustratesthedifﬁcultieswithnon-differentiabilityat0,evenifφisdecreasingandstrictlyconvex.Example8.Considerφ(α)=(cid:16)e−2αifα≤0e−αotherwise.Thenφisstrictlyconvexanddecreasing,butnotclassiﬁcation-calibrated.Toseethis,notethatηφ(α)+(1−η)φ(−α)=(cid:16)ηe−2α+(1−η)eαifα≤0ηe−α+(1−η)e2αotherwise.(5)Takingderivativesandsettingto0showsthat(5)isminimizedontheset{α≤0}atα=min(cid:7)0,13ln2η1−η(cid:8).Thus,ifη<1/2and2η≥1−η(i.e.,1/3≤η<1/2),thentheoptimalαisatleast0.2.5FurtherAnalysisofψItisinterestingtoconsiderwhatpropertiesofconvexcostfunctionsdeterminetheoptimalboundψonexcessriskintermsofexcessφ-risk.Thefollowinglemmashowsthataﬂat-terfunctionφleadstoabetterboundψ.Themeasureofcur-vaturethatweconsiderinvolvestheBregmandivergenceofφat0.Ifφisconvexandclassiﬁcation-calibrated,thenitisdif-ferentiableat0,andwecandeﬁnetheBregmandivergenceofφat0,dφ(0,α)=φ(α)−(cid:2)φ(0)+αφ(cid:11)(0)(cid:3).Weconsiderasymmetrized,normalizedversionoftheBreg-mandivergenceat0,forα>0,ξ(α)=dφ(0,α)+dφ(0,−α)−φ(cid:11)(0)α.BecauseφisconvexonR,bothφandξarecontinuous,sowecandeﬁneξ−1(θ)=inf{α:ξ(α)=θ}.Lemma3.Forconvex,classiﬁcation-calibratedφ,ψ(θ)≥−φ(cid:11)(0)θ2ξ−1(cid:7)θ2(cid:8).Noticethataslowerincreaseofξ(i.e.,alesscurvedφ)givesbetterboundsonR(f)−R∗intermsofRφ(f)−R∗φ.2.6GeneralLossFunctionsAlloftheclassiﬁcationproceduresmentionedinearliersectionsusesurrogatelossfunctionsthateitherareupperboundson0–1lossorcanbetransformedintoupperboundsthroughapositivescalingfactor.Thisisnotacoincidence;asthenextlemmaestablishes,itmustbepossibletoscaleanyclassiﬁcation-calibratedφintosuchamajorant.Lemma4.Ifφ:R→[0,∞)isclassiﬁcation-calibrated,thenthereisaγ>0suchthatγφ(α)≥1[α≤0]forallα∈R.146JournaloftheAmericanStatisticalAssociation,March2006(a)(b)Figure6.TheLossFunctionofExample9(a)andtheCorresponding(nonconvex)˜ψ(b).Thedottedlinesdepictthegraphsforthetwolinearfunctionsofwhich˜ψisapointwiseminimum.Wehaveseenthatforconvexφ,thefunction˜ψisconvex,andsoψ=˜ψ.Thefollowingexampleshowsthat,ingeneral,wecannotavoidcomputingtheconvexlowerboundψ.Example9.Considerthefollowing(classiﬁcation-calibra-ted)lossfunction;seeFigure6(a):φ(α)=4ifα≤0,α(cid:4)=−13ifα=−12ifα=10ifα>0,α(cid:4)=1.ItiseasytocheckthatH−(η)=(cid:16)min{4η,2+η}ifη≥1/2min{4(1−η),3−η}ifη<1/2,andthatH(η)=4min{η,1−η}.ThusH−(η)−H(η)=(cid:16)min{8η−4,5η−2}ifη≥1/2min{4−8η,3−5η}ifη<1/2,so˜ψ(θ)=min(cid:16)4θ,12(5θ+1)(cid:17).Thisfunction,illustratedinFigure6(b),isnotconvex;infact,itisconcave.Thusψ(cid:4)=˜ψ.3.TIGHTERBOUNDSUNDERLOW–NOISECONDITIONSPredictingtheoptimalclasslabelisdifﬁcultinregionswhereη(X)iscloseto1/2,becausetheinformationprovidedbythelabelsismostnoisythere.Inmanypracticalpatternclassiﬁ-cationproblems,itisreasonabletoassumethattheposteriorprobabilityη(X)isunlikelytobeverycloseto1/2.Henceitisimportanttounderstandhowpatternclassiﬁcationmethodsper-formunderthese“low-noise”conditions.Toquantifythenotionoflownoise,considerthefollowingtwopropertiesofaprob-abilitydistributiononX×{±1},introducedbyMammenandTsybakov(1999)andTsybakov(2004):Mβ.Forsomecandall>0,Pr(cid:7)0<(cid:18)(cid:18)(cid:18)(cid:18)η(X)−12(cid:18)(cid:18)(cid:18)(cid:18)≤(cid:8)≤cβ.Nα.Forsomecandallmeasurablef:X→{±1},Pr(cid:2)f(X)(η(X)−1/2)<0(cid:3)≤c(cid:2)R(f)−R∗(cid:3)α.(6)Theseconditionsareequivalent.Lemma5.For0≤β<∞,aprobabilitydistributionsatis-ﬁesMβiffitsatisﬁesNβ/(1+β).Furthermore,M∞isequivalenttoN1,becausePr(cid:7)0<(cid:18)(cid:18)(cid:18)(cid:18)η(X)−12(cid:18)(cid:18)(cid:18)(cid:18)<12c(cid:8)=0(7)iff,forallmeasurablef:X→{±1},Pr(cid:2)f(X)(η(X)−1/2)<0(cid:3)≤c(cid:2)R(f)−R∗(cid:3).(8)Inwhatfollows,wesaythatPhasnoiseexponentα≥0ifitsatisﬁesNα.RecallthatR(f)−R∗=E(cid:2)1(cid:10)f(X)(cid:4)=sign(η(X)−1/2)(cid:11)|2η(X)−1|(cid:3)=E(cid:2)1(cid:10)f(X)(η(X)−1/2)<0(cid:11)|2η(X)−1|(cid:3)≤PX(cid:2)f(X)(η(X)−1/2)<0(cid:3),(9)Bartlett,Jordan,andMcAuliffe:Convexity,Classiﬁcation,andRiskBounds147whichimpliesthatα≤1.Ifα=0,thenthisimposesnocon-straintonthenoise.Takec=1toseethateveryprobabilitymeasuresatisﬁesN1.Thefollowingtheoremshowsthatiftheprobabilitydistrib-utionissuchthatη(X)isunlikelytobecloseto1/2,thentheboundontheexcessriskintermsoftheexcessφ-riskisim-proved.Incaseswhereψisstrictlyconvex,suchastheexpo-nential,quadratic,andlogisticlossfunctions,thisimpliesthatperformanceimprovesinthepresenceofafavorablenoiseex-ponent,withoutknowledgeofthenoiseexponent.Theorem3.SupposethatPhasnoiseexponent0<α≤1,andthatφisclassiﬁcation-calibrated.Thenthereisac>0suchthatforanyf:X→R,c(cid:2)R(f)−R∗(cid:3)αψ(cid:7)(R(f)−R∗)1−α2c(cid:8)≤Rφ(f)−R∗φ.Furthermore,thisnevergivesaworseratethantheresultofTheorem1,because(cid:2)R(f)−R∗(cid:3)αψ(cid:7)(R(f)−R∗)1−α2c(cid:8)≥ψ(cid:7)R(f)−R∗2c(cid:8).Proof.Recallingthedeﬁnitionoflownoisein(6),ﬁxc>0suchthatforeveryf:X→R,PX(cid:2)sign(f(X))(cid:4)=sign(η(X)−1/2)(cid:3)≤c(cid:2)R(f)−R∗(cid:3)α.Weapproximatetheerrorintegralseparatelyoveraregionwithhighnoiseandovertheremainderoftheinputspace.Towardthisend,ﬁx>0(thenoisethreshold),andnotethatR(f)−R∗=E(cid:2)1(cid:10)sign(f(X))(cid:4)=sign(η(X)−1/2)(cid:11)|2η(X)−1|(cid:3)=E(cid:2)1(cid:10)|2η(X)−1|<(cid:11)×1(cid:10)sign(f(X))(cid:4)=sign(η(X)−1/2)(cid:11)|2η(X)−1|(cid:3)+E(cid:2)1(cid:10)|2η(X)−1|≥(cid:11)×1(cid:10)sign(f(X))(cid:4)=sign(η(X)−1/2)(cid:11)|2η(X)−1|(cid:3)≤c(R(f)−R∗)α+E(cid:2)1(cid:10)|2η(X)−1|≥(cid:11)×1(cid:10)sign(f(X))(cid:4)=sign(η(X)−1/2)(cid:11)|2η(X)−1|(cid:3).Now,foranyx,1(cid:10)|2η(x)−1|≥(cid:11)|2η(x)−1|≤ψ()ψ(cid:2)|2η(x)−1|(cid:3).(10)Indeed,when|2η(x)−1|<,(10)followsfromthefactthatψisnonnegative(Lemma2,parts2,8,and9),andwhen|2η(x)−1|≥,(10)followsfromLemma1,part2.Thus,usingthesameargumentasintheproofofTheorem1,R(f)−R∗≤c(cid:2)R(f)−R∗(cid:3)α+ψ()E(cid:2)1(cid:10)sign(f(X))(cid:4)=sign(η(X)−1/2)(cid:11)×ψ(cid:2)|2η(X)−1|(cid:3)(cid:3)≤c(cid:2)R(f)−R∗(cid:3)α+ψ()(cid:2)Rφ(f)−R∗φ(cid:3),andhence(cid:7)R(f)−R∗−c(cid:2)R(f)−R∗(cid:3)α(cid:8)ψ()≤Rφ(f)−R∗φ.Choosing=12c(cid:2)R(f)−R∗(cid:3)1−αandsubstitutinggivestheﬁrstinequality.[WecanassumethatR(f)−R∗>0,becausetheinequalityistrivialotherwise.]Thesecondinequalityfollowsfromthefactthatψ(θ)/θisnondecreasing,whichweknowfromLemma1,part3.4.ESTIMATIONRATESInprevioussectionsweshowedthattheexcessrisk,R(f)−R∗,canbeboundedintermsoftheexcessφ-risk,Rφ(f)−R∗φ.Inthissectionwegiveboundsontheexcessφ-risk.Combinedwithourearlierresults,theseleadtoboundsontheexcessrisk.WefocusonmethodsthatchooseafunctionfromaclassFtominimizetheempiricalφ-risk,ˆRφ(f)=ˆEφ(Yf(X))=1nn(cid:6)i=1φ(Yif(Xi)).Letˆfdenotetheminimizeroftheempiricalφ-risk.Weareinterestedintheconvergenceofˆf’sexcessφ-risk,Rφ(ˆf)−R∗φ.Wecansplitthisexcessφ-riskintoanestimationerrortermandanapproximationerrorterm,Rφ(ˆf)−R∗φ=(cid:4)Rφ(ˆf)−inff∈FRφ(f)(cid:5)+(cid:4)inff∈FRφ(f)−R∗φ(cid:5).Wefocusontheﬁrstterm,theestimationerrorterm.Weassumethroughoutthatsomef∗∈Fachievestheinﬁmum,Rφ(f∗)=inff∈FRφ(f).ThesimplestwaytoboundRφ(ˆf)−Rφ(f∗)istouseauni-formconvergenceargument;ifsupf∈F|ˆRφ(f)−Rφ(f)|≤n,(11)thenRφ(ˆf)−Rφ(f∗)=(cid:2)Rφ(ˆf)−ˆRφ(ˆf)(cid:3)+(cid:2)ˆRφ(ˆf)−ˆRφ(f∗)(cid:3)+(cid:2)ˆRφ(f∗)−Rφ(f∗)(cid:3)≤2n+(cid:2)ˆRφ(ˆf)−ˆRφ(f∗)(cid:3)≤2n,becauseˆfminimizesˆRφ.Butthisapproachcangivethewrongrate.Forexample,foranontrivialclassF,theexpectationofthesupremumoftheempiricalprocessin(11)candecreasenofasterthan1/√n.ButifFisasmallclass(e.g.,ifitisasub-setofaﬁnite-dimensionallinearclass)andRφ(f∗)=0,thenRφ(ˆf)shoulddecreaseaslogn/n.Lee,Bartlett,andWilliamson(1996)showedthatbetterratesthanthosethatfollowfromtheuniformconvergenceargumentcanbeobtainedforthequadraticlossφ(α)=(1−α)2ifFisconvex,evenifRφ(f∗)>0.Inparticular,becausethequadratic148JournaloftheAmericanStatisticalAssociation,March2006lossfunctionisstrictlyconvex,itispossibletoboundthevari-anceoftheexcessloss(i.e.,thedifferencebetweenthelossofafunctionfandthatoftheoptimalf∗)intermsofitsexpectation.Becausethevariancedecreasesasweapproachtheoptimalf∗,theriskoftheempiricalminimizerconvergesmorequicklytotheoptimalriskthanthesimpleuniformconvergenceresultswouldsuggest.Mendelson(2002)improvedthisresultandex-tendeditfrompredictioninL2(PX)topredictioninLp(PX)forothervaluesofp.Theproofusedtheideaofthemodulusofconvexityofanorm.Inthissectionweusethisideatogiveasimplerproofofamoregeneralboundwhenthelossfunctionsatisﬁesastrictconvexitycondition,andweobtainriskbounds.Themodulusofconvexityofanarbitrarystrictlyconvexfunction(ratherthananorm)isakeynotioninformulatingourresults.RecallthatapseudometricdonasetSsatisﬁesalloftheaxiomsofametric,exceptthattherecanbea(cid:4)=bwithd(a,b)=0.Deﬁnition3(Modulusofconvexity).Givenapseudomet-ricddeﬁnedonaconvexsubsetSofavectorspace,andaconvexfunctionf:S→R,themodulusofconvexityoffwithrespecttodisthefunctionδ:[0,∞)→[0,∞]satisfyingδ()=inf(cid:16)f(x1)+f(x2)2−f(cid:7)x1+x22(cid:8):x1,x2∈S,d(x1,x2)≥(cid:17).Ifδ()>0forall>0,thenwesaythatfisstrictlyconvexwithrespecttod.Forexample,forS=R,ddenotingtheEuclideandistance,andf(α)=α2,themodulusofconvexityisδ()=2/4.ForS=[−a,a]andthesamemetric,f(α)=eαhasmodulusofconvexitye−a((1+e)/2−e/2)=e−a2/8+o(2).WeconsiderlossfunctionsφthatalsosatisfyaLipschitzconditionwithrespecttoapseudometricdonR;wesaythatφ:R→RisLipschitzwithrespecttod,withconstantL,ifforalla,b∈R,|φ(a)−φ(b)|≤L·d(a,b).Notethatifdisametricandφisconvex,thenφnecessar-ilysatisﬁesaLipschitzconditiononanycompactsubsetofR(Rockafellar1997).AssumptionA.Thelossfunctionφ:R→RandtheclassFofrealfunctionsonXsatisfythefollowingconditions.ForsomepseudometricdonR,thereareconstantsL,c,r,andB,suchthatthefollowingconditionsobtain:A.1.φisclassiﬁcation-calibrated.A.2.φisLipschitzwithconstantL,withrespecttod.A.3.φisconvexwithmodulusofconvexityδ()≥crwithrespecttod.A.4.Fisconvex.A.5.Forallf∈F,x1,x2∈X,andy1,y2∈Y,d(y1f(x1),y2f(x2))≤B.DeﬁnetheexcesslossclassgFasgF={gf:f∈F}=(cid:19)(x,y)(cid:12)→φ(yf(x))−φ(yf∗(x)):f∈F(cid:20),wheref∗=argminf∈FEφ(Yf(X)).NoticethatfunctionsingFcantakenegativevalues,buttheyallhavenonnegativeexpectation.Weareinterestedinboundsontheexcessφ-risk,Rφ(ˆf)−R∗φ,whereˆfistheminimizeroftheempiricalφ-risk.Thisisequivalenttotheexpectationofgˆf,wheregˆfistheele-mentofthelossclasswithminimalsampleaverage.Inthefollowingtheorem,weexploittheconcentrationofmeasurephenomenontogiveaboundontheexcessφ-risk.Astandarduniformconvergenceargument,describedatthebe-ginningofthissection,couldproceedbyconsideringthesupre-mumoftheempiricalprocessindexedbythelossclass,Esup{Eg−ˆEg:g∈gF}.Thiscorrespondstoconsideringthemaximaldeviationbetweenexpectationsandsampleaveragesoverthelossclass.Instead,weuseanapproachintroducedbyBartlettandMendelson(2005)(seealsoMassart2000b;KoltchinskiiandPanchenko2000;Mendelson2002;LugosiandWegkamp2004;Bartlett,Bousquet,andMendelson2005).Wedividetheexcesslossclassintosubsetsofdifferentexpectation,{g∈gF:Eg=},andconsiderthesupremaoftheempiricalprocessesindexedbysuchsubsets,ξgF()=Esup{Eg−ˆEg:g∈gF,Eg=}.(NotethatthefunctionξgFdependsonthesamplesizen,butwesimplifythenotationbyomittingthisdependence.)ForstrictlyconvexLipschitzφandconvexF,thevarianceofeachexcesslossfunctionisboundedintermsofitsexpecta-tion,whichallowsustoreplacethemaximaldeviationoverthewholeclassbythemaximaldeviationoverasmallsubsetoftheclass:thosefunctionswithexpectation∗n,where∗nistheﬁxedpointofthemap(cid:12)→ξgF().Theorem4.SupposethatthelossfunctionφandthefunctionclassFsatisfyAssumptionA.ThenthereisaconstantKsuchthat,withprobabilityatleast1−δ,theminimizerˆf∈Foftheempiricalφ-risksatisﬁesRφ(ˆf)≤inff∈FRφ(f)+n,wheren=Kmax(cid:16)∗n,(cid:7)crL2ln(1/δ)n(cid:8)1/(2−β),BLln(1/δ)n(cid:17),∗n≥ξgF(∗n),cr=(cid:16)(2c)−2/rifr≥2(2c)−1B2−rotherwise,andβ=min(cid:7)1,2r(cid:8).Thusthereisaconstantc(cid:11)suchthatforanyprobabilitydistri-butionPonX×Ywithnoiseexponentα,withprobabilityatleast1−δ,c(cid:11)(cid:2)R(ˆf)−R∗(cid:3)αψ(cid:7)(R(ˆf)−R∗)1−α2c(cid:11)(cid:8)≤n+inff∈FRφ(f)−R∗φ.Itisinstructivetoconsiderthevariouscomponentsoftheclassiﬁcationriskinthisbound.Theestimationerror,n,in-creasesasthecomplexityoftheclassFincreasesandde-Bartlett,Jordan,andMcAuliffe:Convexity,Classiﬁcation,andRiskBounds149creasesasthesamplesizeincreases.Theapproximationerror,inff∈FRφ(f)−R∗φ,isexpressedintermsoftheφ-risk.Itde-creasesastheclassFincreases.Finally,usingtheconvexsur-rogateφinplaceofthe0–1lossaffectstheboundthroughtherateofgrowthofthefunctionofR(ˆf)−R∗thatappearsontheleftside.Therateofdecreaseofclassiﬁcationriskimprovesasthenoiseexponentincreases.Considertheimpactontheboundofthemodulusofcon-vexityofthelossfunction.Forﬂatterlossfunctions,wheretheexponentofthemodulusofconvexityr>2,theratecanbenobetterthann−1/(2−2/r)=n−r/(2(r−1)),whichapproachesn−1/2asrgetslarge.Formorecurvedφ,withr≤2,theratecanbeasgoodasn−1.Incontrast,wehaveseenthatamorecurvedφleadstoaworseψ.But,ifthenoiseexponentisα=1,thentheboundisoptimizedbyamorecurvedφ,withr≥2.Shen,Tseng,Zhang,andWong(2003)showedthatfastratesarealsopossibleunderthelow-noiseassumptionforapar-ticularnonconvexφ.Inthatcase,however,minimizationofempiricalφ-riskrequirestheuseofheuristics,becausetheoptimizationproblemcannotbesolvedefﬁciently.Intheremainderofthissection,wepresentaproofofTheo-rem4.Thisproofhastwokeyingredients,whichwecaptureinapairoflemmas.Theﬁrstlemmashowsthatifthevarianceofanexcesslossfunctionisboundedintermsofitsexpectation,thenwecanobtainfasterratesthanwouldbeimpliedbytheuniformconvergencebounds.Thesecondlemmapresentssim-pleconditionsonthelossfunctionthatensurethatthisvarianceboundissatisﬁedforconvexfunctionclasses.Lemma6.ConsideraclassFoffunctionsf:X→Rwithsupf∈F(cid:13)f(cid:13)∞≤B.LetPbeaprobabilitydistributiononX,andsupposethattherearec≥1and0<β≤1suchthat,forallf∈F,Ef2(X)≤c(Ef)β.(12)Fix0<α,<1.Supposethatifsomef∈FhasˆEf≤αandEf≥,thensomef(cid:11)∈FhasˆEf(cid:11)≤αandEf=.Thenwithprobabilityatleast1−e−x,anyf∈FsatisﬁesˆEf≤α⇒Ef≤,providedthat≥max(cid:16)∗,(cid:7)9cKx(1−α)2n(cid:8)1/(2−β),4KBx(1−α)n(cid:17),whereKisanabsoluteconstantand∗≥61−αξF(∗).Asanaside,noticethatassumingthatthedistributionhasnoiseexponentαcanleadtoaconditionoftheform(12).Toseethis,letf∗betheBayesdecisionruleandconsidertheclassoffunctions{αgf:f∈F,α∈[0,1]},wheregf(x,y)=(cid:1)(f(x),y)−(cid:1)(f∗(x),y)and(cid:1)isthe0–1loss.ThentheconditionPX(cid:2)f(X)(cid:4)=f∗(X)(cid:3)≤c(cid:2)E(cid:1)(f(X),Y)−E(cid:1)(f∗(X),Y)(cid:3)αcanberewrittenasEg2f(X,Y)≤c(Egf(X,Y))α.ThuswecanobtainaversionofTsybakov’sresultforsmallfunctionclassesfromLemma6:IftheBayesdecisionrulef∗isinF,thenthefunctionˆfthatminimizesempiricalriskhasˆEgˆf=ˆR(f)−ˆR(f∗)≤0,andsowithhighprobabilityhasEgˆf=R(f)−R∗≤undertheconditionsofthetheorem.IfFisaVCclass,thenwehave≤clogn/nforsomeconstantc,whichissurprisinglyfastwhenR∗>0.ThesecondingredientintheproofofTheorem4isthefol-lowinglemma,whichgivesconditionsthatensureavarianceboundofthekindrequiredforthepreviouslemma[condi-tion(12)].ForapseudometricdonRandaprobabilitydis-tributiononX,wecandeﬁneapseudometric˜donthesetofuniformlyboundedrealfunctionsonX,˜d(f,g)=(cid:2)Ed(cid:2)f(X),g(X)(cid:3)2(cid:3)1/2.IfdistheusualmetriconR,then˜distheL2(P)pseudometric.Lemma7.ConsideraconvexclassFofreal-valuedfunc-tionsdeﬁnedonX,aconvexlossfunction(cid:1):R→R,andapseudometricdonR.Supposethat(cid:1)satisﬁesthefollowingconditions:1.(cid:1)isLipschitzwithrespecttod,withconstantL,foralla,b∈R,|(cid:1)(a)−(cid:1)(b)|≤Ld(a,b).2.R(f)=E(cid:1)(f)isastrictlyconvexfunctionalwithrespecttothepseudometric˜d,withmodulusofconvexity˜δ,˜δ()=inf(cid:16)R(f)+R(g)2−R(cid:7)f+g2(cid:8):˜d(f,g)≥(cid:17).Supposethatf∗satisﬁesR(f∗)=inff∈FR(f),anddeﬁnegf(x)=(cid:1)(f(x))−(cid:1)(f∗(x)).ThenEgf≥2˜δ(˜d(f,f∗))≥2˜δ(cid:7)√Eg2fL(cid:8).Weapplythelemmatoaclassoffunctionsoftheform(x,y)(cid:12)→yf(x),withthelossfunction(cid:1)=φ.(Thelemmacanbetriviallyextendedtoalossfunction(cid:1):R×Y→Rthatsat-isﬁesaLipschitzconstraintuniformlyoverY.)Inourapplication,thefollowingresultimpliesthatwecanestimatethemodulusofconvexityofRφwithrespecttothepseudometric˜difwehavesomeinformationaboutthemodulusofconvexityofφwithrespecttothepseudometricd.Lemma8.Supposethataconvexfunction(cid:1):R→RhasmodulusofconvexityδwithrespecttoapseudometricdonR,andthatforsomeﬁxedc,r>0,every>0satisﬁesδ()≥cr.Thenforfunctionsf:X→Rsatisfyingsupx1,x2d(f(x1),f(x2))=B,themodulusofconvexity˜δofR(f)=E(cid:1)(f)withrespecttothepseudometric˜dsatisﬁes˜δ()≥crmax{2,r},wherecr=cifr≥2andcr=cBr−2otherwise.150JournaloftheAmericanStatisticalAssociation,March2006Itisalsopossibletoproveaconverseresult,thatthemod-ulusofconvexityofφisatleasttheinﬁmumoverprobabilitydistributionsofthemodulusofconvexityofR.[Toseethis,wechooseaprobabilitydistributionconcentratedonthex∈Xwheref1(x)andf2(x)achievetheinﬁmuminthedeﬁnitionofthemodulusofconvexity.]ProofofTheorem4.Considertheclass{gf:f∈F}where,foreachf∈F,gf(x,y)=φ(yf(x))−φ(yf∗(x)),andwheref∗∈FminimizesRφ(f)=Eφ(Yf(X)).ApplyingLemma8,weseethatthefunctionalR(f)=Eφ(f),deﬁnedforfunctions(x,y)(cid:12)→yf(x),hasmodulusofconvexity˜δ()≥crmax{2,r},wherecr=cifr≥2andcr=cBr−2otherwise.FromLemma7,Egf≥2cr(cid:7)√Eg2fL(cid:8)max{2,r},whichisequivalenttoEg2f≤c(cid:11)rL2(Egf)min{1,2/r}withc(cid:11)r=(cid:16)(2c)−2/rifr≥2(2c)−1B2−rotherwise.ToapplyLemma6totheclass{gf:f∈F},weneedtocheckthecondition.SupposethatgfhasˆEgf≤αandEgf≥.Then,bytheconvexityofFandthecontinuityofφ,somef(cid:11)=γf+(1−γ)f∗∈F,for0≤γ≤1hasEgf=.Jensen’sinequalityshowsthatˆEgf=ˆEφ(cid:2)Y(cid:2)γf(X)+(1−γ)f∗(X)(cid:3)(cid:3)−ˆEφ(Yf∗(X))≤γ(cid:2)ˆEφ(Yf(x))−ˆEφ(Yf∗(X))(cid:3)≤α.ApplyingLemma6wehave,withprobabilityatleast1−e−x,thatanygfwithˆEgf≤/2alsohasEgf≤,providedthat≥max(cid:16)∗,(cid:7)36c(cid:11)rL2Kxn(cid:8)1/(2−min{1,2/r}),16KBLxn(cid:17),where∗≥12ξgF(∗).Inparticular,ifˆf∈Fminimizesempir-icalrisk,thenˆEgˆf=ˆRφ(ˆf)−ˆRφ(f∗)≤0<2,andhenceEgˆf≤.CombiningwithTheorem3showsthatforsomec(cid:11),c(cid:11)(cid:2)R(ˆf)−R∗(cid:3)αψ(cid:7)(R(ˆf)−R∗)1−α2c(cid:11)(cid:8)≤Rφ(ˆf)−R∗φ=Rφ(ˆf)−Rφ(f∗)+Rφ(f∗)−R∗φ≤+Rφ(f∗)−R∗φ.4.1ExamplesWeconsiderfourlossfunctionsthatsatisfytherequirementsforthefastconvergencerates:theexponentiallossfunctionusedinAdaBoost,thedeviancefunctioncorrespondingtolo-gisticregression,thequadraticlossfunction,andthetruncatedquadraticlossfunction;seeTable1.Thesefunctionsareillus-tratedinFigures1and3.Weusethepseudometricdφ(a,b)=inf(cid:19)|a−α|+|β−b|:φconstanton(min{α,β},max{α,β})(cid:20).Forallfunctionsexceptthetruncatedquadraticlossfunction,thiscorrespondstothestandardmetriconR,dφ(a,b)=|a−b|.Inallcases,dφ(a,b)≤|a−b|,butforthetruncatedquadratic,dφignoresdifferencestotherightof1.ItiseasytocalculatetheLipschitzconstantandmodulusofconvexityforeachoftheselossfunctions.TheseparametersaregiveninTable1.Inthefollowingresult,weconsiderthefunctionclassusedbyalgorithmssuchasAdaBoost:theclassoflinearcombinationsofclassiﬁersfromaﬁxedbaseclass.WeassumethatthisbaseclasshasﬁniteVCdimension,andconstrainthesizeoftheclassbyrestrictingthe(cid:1)1normofthelinearparameters.IfGistheVCclass,thenwewriteF=Babsconv(G)forsomeconstantB,whereBabsconv(G)=(cid:21)m(cid:6)i=1αigi:m∈N,αi∈R,gi∈G,(cid:13)α(cid:13)1=B(cid:22).Theorem5.Letφ:R→Rbeaconvexlossfunction.Sup-posethatontheinterval[−B,B],φisLipschitzwithcon-stantLBandhasmodulusofconvexityδ()=aB2(bothwithrespecttothepseudometricd).ForanyprobabilitydistributionPonX×Ythathasnoiseexponentα,thereisaconstantc(cid:11)forwhichthefollowingistrue.Foriiddata(X1,Y1),...,(Xn,Yn),letˆf∈Fbethemin-imizeroftheempiricalφ-risk,Rφ(f)=ˆEφ(Yf(X)).SupposethatF=Babsconv(G),whereG⊆{±1}XhasVCdimensionVand∗n≥BLBmax(cid:16)(cid:7)LBaBB(cid:8)1/(V+1),1(cid:17)n−(V+2)/(2(V+1)).Then,withprobabilityatleast1−δ,R(ˆf)≤R∗+c(cid:11)(cid:7)∗n+LB(LB/(2aB)+B)ln(1/δ)n+inff∈FRφ(f)−R∗φ(cid:8)1/(2−α).Table1.FourConvexLossFunctionsDeﬁnedonRφ(α)LBδ()Exponentiale−αeBe−B2/8Logisticln(1+e−2α)2e−2B2/4Quadratic(1−α)22(B+1)2/4Truncatedquadratic(max{0,1−α})22(B+1)2/4NOTE:Ontheinterval[−B,B],eachhastheindicatedLipschitzconstantLBandmodulusofconvexityδ()withrespecttodφ.Allhaveaquadraticmodulusofconvexity.Bartlett,Jordan,andMcAuliffe:Convexity,Classiﬁcation,andRiskBounds1514.2SimulationsThissectiondescribesasetofsimulationsthatillustratetheperformanceoftheexcessriskboundbasedontheψ-transform,aswellasthetheoreticalexcessφ-riskratesobtainedfromThe-orem4.WetookX=[−1,1]10asourcovariatespace,withPXequaltotheuniformdistributiononX.Fortheconditionaldistributionη(x),weusedmembersofaparameterizedfamilybasedonthelogisticfunction,ηq(x)=P(Y=1|x)=σ(cid:2)Csign(x1)|x1|q(cid:3),q>0,whereσ(u)=1/(1+exp(−u)).Varyingqresultsindifferentnoiseexponentsfortheconditionaldistribution;itisstraight-forwardtoseethatPX(0<|2ηq(X)−1|<)<(4/C)1/q1/q,soLemma5impliesthatηqhasnoiseexponent1/(q+1).WechosetheconstantCsothatηq(−1)=1/4andηq(1)=3/4,forallq.Themargin-basedlossfunctionsinoursimulationsalsocamefromaone-dimensionalfamily,indexedbyp>1,φp(α)=(p−1)(2/p)p/(p−1)−2α+|α|p.Theleadingconstantensuresthatφpisnonnegativeforallα∈[−1,1].Forp>1,φpisconvexwithanegativeﬁrstderiv-ativeat0,soTheorem2,part1tellsusthatitisclassiﬁcation-calibrated.Differentchoicesofpleadtodifferentvaluesforthemodulusofconvexityexponentofφp,because(φ()+φ(−))/2−φ(0)=pforpositive.Wetookasafamilyofreal-valuedclassiﬁerstheconvexhullofthecoordinatefunctions,F=co{x(cid:12)→βi(x)=xi:i=1,...,10}.Thuseachf∈Fhastheformf(u)=λ(cid:17)uforsomeλ≥0,λ(cid:17)1≤1.Wesimulateddatasetsofseveralsizesnbetween10and10,000,usingvariousvaluesofpandq,asde-tailedlater.Foreachchoiceofn,p,andq,weperformed25rep-etitionsofthefollowingprocedure.First,wegeneratedadatasetaccordingto(PX,ηq(x))andfoundtheempiricalriskmini-mizerˆfnoverF,throughaconstrainedconvexoptimization.Thenwecomputedthe0–1riskofˆfn,approximatingtherele-vantintegralwithadaptivenumericalquadrature.SubtractingtheBayesriskforthechosendistribution(dependingonq),alsoapproximatedusingquadrature,gaveustheexcess0–1riskofˆfn.Finally,wecarriedoutasimilarcomputationtodeterminetheexcessφ-riskofˆfn.Weillustratethebehavioroftheupperboundonexcess0–1riskobtainedfromtheψ-transformusingthesesimulationre-sults.AroutinecalculationalongthelinesoftheexamplesinSection2.3showsthatψ(θ)=(p−1)(2θ/p)p/(p−1).Weap-pealtoTheorem1toobtaintheinequality(p−1)(cid:7)2(R(f)−R∗)p(cid:8)p/(p−1)≤Rφ(f)−R∗φ.Solvingthisinequalityforexcess0–1riskgivesanupperboundasafunctionofexcessφ-risk,R(f)−R∗≤p2(cid:7)Rφ(f)−R∗φp−1(cid:8)(p−1)/p.Weveriﬁedthat,asexpected,everyexcess0–1riskinoursim-ulationsobeyedtheupperbounddeterminedbyitscorrespond-ingexcessφ-risk,acrossallvaluesofpandq.Wealsousedthesimulationstoillustratethetheoreticalratesofconvergenceforexcessφ-riskimpliedbyTheorem4.FortheclassFunderconsideration,thecenteredempiricalprocessξF()usedinthetheoremcanbepointwiseupper-boundedusingalocalRademacheraveragesymmetrization,whichinturnisboundedbytheDudleyentropyintegral.ThederivationcloselyfollowstheproofofTheorem5(seealsoBartlettetal.2005).Thesecalculationsrevealthatasuitableupperboundon∗inTheorem4isc(d/n)log(nL/d),withd=10thedi-mensionofFandcauniversalconstant.Thus,withprobabilityatleast1−e−x,wehavetheexcessφ-riskboundRφ(ˆf)−Rφ(f∗)≤cmax(cid:16)(cid:7)crL2xn(cid:8)1/(2−min{1,2/p}),BLxn,dnlog(cid:7)nLd(cid:8)(cid:17),recallingthatpisthemodulusofconvexityexponentforφp(α).Treatingthelogarithmicfactorasapproximatelyconstant,wethereforeexpectarateofordern−1for1<p<2andofordern−1/(2−2/p)forp≥2.Figure7presentsthesimulationresultsforp∈{1.5,2,3.5},allwithq=1.Resultswithq∈{1.5,2,2.5,3}aresimilar,andindeedthetheoreticalratesdonotvarywithq.Thesolidlinesarenaturalcubicsplineﬁts,onthelog–logscale,tothesam-plesizeandexcessφ-riskfromeachsimulation.Theslopeofeachdashedlineisthetheoreticalrateexponentimpliedbythebound:−1.0forp=1.5and2and−.7forp=3.5.Astheplotsreveal,theagreementwiththeorywhenp=1.5and2isextremelygoodforlargeenoughn.Althoughthematchwhenp=3.5islessexact,thesimulatedresultsappearcompatiblewiththetheoreticalratetowithinthenoisetolerance.5.CONCLUSIONSWehavefocusedontherelationshipbetweenpropertiesofanonnegativemargin-basedlossfunctionφandthestatisticalperformanceoftheclassiﬁerthat,basedonaniidtrainingset,minimizesempiricalφ-riskoveraclassoffunctions.Weﬁrstderivedauniversalupperboundonthepopulationmisclassiﬁ-cationriskofanythresholdedmeasurableclassiﬁerintermsofitscorrespondingpopulationφ-risk.Theboundisgovernedbytheψ-transform,aconvexiﬁedvariationaltransformofφ.Itisthetightestpossibleupperbounduniformoverallprobabilitydistributionsandmeasurablefunctionsinthissetting.Usingthisupperbound,wecharacterizedtheclassoflossfunctionsthatguaranteethateveryφ-riskconsistentclassiﬁersequenceisalsoBayes-riskconsistentunderanypopulationdistribution.Hereφ-riskconsistencydenotessequentialcon-vergenceofpopulationφ-riskstothesmallestpossibleφ-riskofanymeasurableclassiﬁer.Thecharacteristicpropertyofsuchaφ,whichwetermclassiﬁcation-calibration,isakindofpointwiseFisherconsistencyfortheconditionalφ-riskateachx∈X.Thenecessityofclassiﬁcation-calibrationisapparent;thesufﬁciencyunderscoresitsfundamentalimportanceinelab-oratingthestatisticalbehavioroflarge-marginclassiﬁers.Forthespecialcaseofconvexφ,whichiswidespreadinpracticalapplications,wedemonstratedthatclassiﬁcation-calibrationisequivalenttotheexistenceandstrictnegativityoftheﬁrstderivativeofφat0,aconditionthatisreadilyveriﬁableinmostpracticalexamples.Inaddition,theconvexiﬁcationstep152JournaloftheAmericanStatisticalAssociation,March2006(a)(b)(c)Figure7.RatePlotsfor(a)p=1.5,(b)p=2,and(c)p=3.5.Eachpanelshowssimulatedexcessφ-riskonthelogscaleversussimulatedsamplesizeonthelogscale,forthechoiceofpgivenattop.Wetookq=1ineachcase.Naturalcubicsplineﬁtsappearassolidlines.Thedashedlinedepictstheslopecorrespondingtothetheoreticalrateforthechosenp.(Theverticalpositionofthedashedlineisnotinformative.)intheψ-transformisvacuousforconvexφ,whichsimpliﬁesthederivationofclosedforms.Underthelow-noiseassumptionofMammenandTsybakov(1999)andTsybakov(2004),wesharpenedouroriginalupperbound.Wefoundthatempiricalφ-riskminimizationyieldscon-vergenceofφ-risktothatofthebest-performingfunctioninFasthesamplesizegrows.Forstrictlyconvexφ,theconvergenceratecanbefasterthanthatimpliedbystandarduniformcon-vergencearguments,dependingonthestrictnessofconvexityofφandthecomplexityofF.Combinedwiththelow-noisecondition,wesawthatthisimpliesfastratesofconvergenceofthemisclassiﬁcationrisktoitsoptimalvalue.Simulationscon-ﬁrmtheconvergenceratesofφ-riskpredictedbythetheory,foralinearclassandaparticularprobabilitydistribution.Simula-tionsalsoshowthattherelationshipbetweenexcessφ-riskandexcessriskcloselyfollowsthatpredictedbythetheory.Twoimportantissuesthatwehavenottreatedaretheapprox-imationerrorforpopulationφ-riskrelativetoF,andalgorith-micconsiderationsintheminimizationofempiricalφ-risk.Inthesettingofscaledconvexhullsofabaseclass,someapprox-imationresultshavebeengivenbyBreiman(2004),Mannoretal.(2002),andLugosiandVayatis(2004).Regardingthenu-mericaloptimizationtodetermineˆf,ZhangandYu(2005)gavenovelboundsontheconvergencerateforgenericforwardstage-wiseadditivemodeling(seealsoZhang2003).Theseauthorsfocusedonoptimizationofaconvexriskfunctionalovertheentirelinearhullofabaseclass,withregularizationenforcedbyanearlystoppingrule.APPENDIXA:PROOFSProofofLemma2Theproofofpart1isimmediatefromthedeﬁnitions.Forpart2,concavityfollowsbecauseHisaninﬁmumofconcave(afﬁne)func-tionsofη.Now,becauseHisconcaveandsymmetricabout1/2,H(1/2)=H((1/2)η+(1/2)(1−η))≥(1/2)H(η)+(1/2)H(1−η)=H(η).ThusHismaximalat1/2.ToseethatH(1/2)=H−(1/2),notethatα(2η−1)≤0forallαwhenη=1/2.Toprovepart3,assumethatthereisanη(cid:4)=1/2withH(η)=H(1/2).Fixasequenceα1,α2,...forwhichlimi→∞C1/2(αi)=H(1/2).Bytheassumption,liminfi→∞(cid:2)ηφ(αi)+(1−η)φ(−αi)(cid:3)≥H(η)=H(1/2)=limi→∞φ(αi)+φ(−αi)2.(A.1)Rearranging,wehave(η−1/2)liminfi→∞(cid:2)φ(αi)−φ(−αi)(cid:3)≥0.BecauseH(1−η)=H(η),thesameargumentshowsthat(η−1/2)liminfi→∞(cid:2)φ(−αi)−φ(αi)(cid:3)≥0.Itfollowsthatlimi→∞(cid:2)φ(αi)−φ(−αi)(cid:3)=0,sothatalloftheexpressionsin(A.1)areequal.Hence,H(η)=limi→∞Cη(αi)=limi→∞Cη(−αi),whichimpliesthatH(η)=H−(η).ThusifH(η)=H(1/2),thenφisnotclassiﬁcation-calibrated.Forpart4,H−isconcaveon[0,1/2]bythesameargumentasfortheconcavityofH.(Notethatwhenη<1/2,H−isaninﬁmumoverasetofconcavefunctions,butinthiscasewhenη>1/2,itisaninﬁmumoveradifferentsetofconcavefunctions.)TheinequalityH−≥Hfollowsfromthedeﬁnitions.Forpart5,ﬁrstnotethattheconcavityofHimpliesthatitiscon-tinuousontherelativeinteriorofitsdomain,thatis,(0,1).Thus,toshowthatHiscontinuous[0,1],itsufﬁces(bysymmetry)toshowthatitisleft-continuousat1.Because[0,1]islocallysimplicialinthesenseofRockafellar(1997),histheorem10.2giveslowersemi-continuityofHat1(equivalently,uppersemicontinuityoftheconvexfunction−Hat1).ToseeuppersemicontinuityofHat1,ﬁxany>0andchooseαsuchthatφ(α)≤H(1)+/2.Thenforanyηbetween1−/(2φ(−α))and1,wehaveH(η)≤Cη(α)≤H(1)+.Becausethisistrueforany,limsupη→1H(η)≤H(1),whichisup-persemicontinuity.ThusHisleft-continuousat1.Thesameargu-mentshowsthatH−iscontinuouson(0,1/2)and(1/2,1)andisBartlett,Jordan,andMcAuliffe:Convexity,Classiﬁcation,andRiskBounds153left-continuousat1/2and1.SymmetryimpliesthatH−iscontinuousontheclosedinterval[0,1].Thecontinuityof˜ψisnowimmediate.Toseepart6,observethatψisaclosedconvexfunctionwithlocallysimplicialdomain[−1,1],sothatitscontinuityfollowsbyonceagainapplyingtheorem10.2ofRockafellar(1997).Itfollowsimmediatelyfromparts2and4that˜ψisnonnegativeandminimalat0.Becauseepiψistheconvexhullofepi˜ψ(i.e.,thesetofallconvexcombinationsofpointsinepi˜ψ),weseethatψisalsononnegativeandminimalat0,whichispart7.Part8followsimmediatelyfrom2.Toprovepart9,supposeﬁrstthatφisclassiﬁcation-calibrated.Thenforallθ∈(0,1],˜ψ(θ)>0.Buteverypointinepiψisaconvexcom-binationofpointsinepi˜ψ,soif(θ,0)∈epiψ,thenwecanonlyhaveθ=0.Henceforθ∈(0,1],pointsinepiψoftheform(θ,c)musthavec>0,andclosureof˜ψnowimpliesψ(θ)>0.Fortheconverse,notethatifφisnotclassiﬁcation-calibrated,thensomeθ>0has˜ψ(θ)=0,andsoψ(θ)=0.ProofofTheorem2Recallthatasubgradientofφatα∈Risanyvaluemα∈Rsuchthatφ(x)≥φ(α)+mα(x−α)forallx.Toprovepart1,ﬁxaconvexfunctionφ.(⇒)Becauseφisconvex,wecanﬁndsubgradientsg1≥g2suchthatforallα,φ(α)≥g1α+φ(0)andφ(α)≥g2α+φ(0).Thenwehaveηφ(α)+(1−η)φ(−α)≥η(cid:2)g1α+φ(0)(cid:3)+(1−η)(cid:2)−g2α+φ(0)(cid:3)=(cid:2)ηg1−(1−η)g2(cid:3)α+φ(0)(A.2)=(cid:7)12(g1−g2)+(g1+g2)(cid:7)η−12(cid:8)(cid:8)α+φ(0).(A.3)Becauseφisclassiﬁcation-calibrated,forη>1/2,wecanexpressH(η)asinfα>0[ηφ(α)+(1−η)φ(−α)].If(A.3)weregreaterthanφ(0)foreveryα>0,thenitwouldfollowthatforη>1/2,H(η)≥φ(0)≥H(1/2),which,byLemma2,part3,wouldbeacontradiction.Wenowshowthatg1>g2impliesthiscontradiction.Indeed,wecanchoose12<η<12+g1−g22|g1+g2|toshowthat|(η−1/2)(g1+g2)|<(g1−g2)/2,so(A.3)isgreaterthanφ(0)forallα>0.Thusifφisclassiﬁcation-calibrated,thenwemusthaveg1=g2,whichimpliesthatφisdifferentiableat0.Toseethatwemustalsohaveφ(cid:11)(0)<0,notethatfrom(A.2),wehaveηφ(α)+(1−η)φ(−α)≥(2η−1)φ(cid:11)(0)α+φ(0).Butforanyη>1/2andα>0,ifφ(cid:11)(0)≥0,thenthisexpressionisatleastφ(0).Thusifφisclassiﬁcation-calibrated,thenwemusthaveφ(cid:11)(0)<0.(⇐)Supposethatφisdifferentiableat0andhasφ(cid:11)(0)<0.ThenthefunctionCη(α)=ηφ(α)+(1−η)φ(−α)hasC(cid:11)η(0)=(2η−1)φ(cid:11)(0).Forη>1/2,thisisnegative.ItfollowsfromtheconvexityofφthatCη(α)isminimizedbysomeα∗∈(0,∞].Toseethis,notethatforsomeα0>0,wehaveCη(α0)≤Cη(0)+α0C(cid:11)η(0)/2.Buttheconvexityofφ,andhenceofCη,impliesthatforallα,Cη(α)≥Cη(0)+αC(cid:11)η(0).Inparticular,ifα≤α0/4,thenCη(α)≥Cη(0)+α04C(cid:11)η(0)>Cη(0)+α02C(cid:11)η(0)≥Cη(α0).Similarly,forη<1/2,theoptimalαisnegative.Thismeansthatφisclassiﬁcation-calibrated.Fortheproofofpart2,notethatpart1impliesthatφisdifferen-tiableat0andφ(cid:11)(0)<0,andsoφ(0)≥H−(η)=infα:α(η−1/2)≤0(cid:2)ηφ(α)+(1−η)φ(−α)(cid:3)≥infα:α(η−1/2)≤0(cid:2)η(cid:2)φ(0)+φ(cid:11)(0)α(cid:3)+(1−η)(cid:2)φ(0)−φ(cid:11)(0)α(cid:3)(cid:3)=φ(0)+infα:α(η−1/2)≤0(cid:2)(2η−1)φ(cid:11)(0)α(cid:3)=φ(0).ThusH−(η)=φ(0).TheconcavityofH(Lemma2,part2impliesthat˜ψ=H−(η)−H(η)=φ(0)−H(η)isconvex,whichimpliesthatψ=˜ψ.ProofofLemma3Fromtheconvexityofφ,wehaveψ(θ)=H(cid:7)12(cid:8)−H(cid:7)1+θ2(cid:8)=φ(0)−infα>0(cid:7)1+θ2φ(α)+1−θ2φ(−α)(cid:8)=supα>0(cid:7)−θφ(cid:11)(0)α+1+θ2(cid:2)φ(0)−φ(α)+αφ(cid:11)(0)(cid:3)+1−θ2(cid:2)φ(0)−φ(−α)−αφ(cid:11)(0)(cid:3)(cid:8)=supα>0(cid:7)−θφ(cid:11)(0)α−1+θ2dφ(0,α)−1−θ2dφ(0,−α)(cid:8)≥supα>0(cid:2)−θφ(cid:11)(0)α−dφ(0,α)−dφ(0,−α)(cid:3)=supα>0(θ−ξ(α))(−φ(cid:11)(0)α)≥(cid:2)θ−ξ(ξ−1(θ/2))(cid:3)(cid:2)−φ(cid:11)(0)ξ−1(θ/2)(cid:3)=−φ(cid:11)(0)θ2ξ−1(cid:7)θ2(cid:8),wheretheﬁrstinequalityusedthefactthatforallα∈[0,1]andalla,b>0,αa+(1−α)b≤a+b.ProofofLemma4Proceedingbycontrapositive,supposethatnosuchγexists.Be-causeφ(α)≥1[α≤0]on(0,∞),thenwemusthaveinfα≤0φ(α)=0.Butφ(α)=C1(α),andhence0=infα≤0C1(α)=H−(1)≥H(1)≥0.ThusH−(1)=H(1),soφisnotclassiﬁcation-calibrated.154JournaloftheAmericanStatisticalAssociation,March2006ProofofLemma5WeﬁrstshowthatNαimpliesMα/(1−α).ConsiderthesetS={x:0<|η(x)−1/2|≤},andletfbesuchthatS={x:f(x)(η(x)−1/2)<0}.ThenNαimpliesthatPr(S)≥(cid:23)S(cid:18)(cid:18)(cid:18)(cid:18)η(x)−12(cid:18)(cid:18)(cid:18)(cid:18)dPX(x)=12(cid:2)R(f)−R∗(cid:3)≥12(cid:7)1cPr(S)(cid:8)1/α.RearrangingshowsthatPr(S)≤(2)α/(1−α)c1/(1−α),andhencethedistributionsatisﬁesMα/(1−α).ToseethatMβimpliesNβ/(1+β),weﬁx>0andf:X→{±1},deﬁneS={x:f(x)(η(x)−1/2)<0},andwriteR(f)−R∗=E(cid:2)1[X∈S]|2η(X)−1|(cid:3)=2(cid:23)S(cid:18)(cid:18)(cid:18)(cid:18)η(x)−12(cid:18)(cid:18)(cid:18)(cid:18)dPX(x)≥2(cid:23)S1(cid:24)(cid:18)(cid:18)(cid:18)(cid:18)η(x)−12(cid:18)(cid:18)(cid:18)(cid:18)>(cid:25)dPX(x)=2(cid:7)Pr(S)−(cid:23)S1(cid:24)0<(cid:18)(cid:18)(cid:18)(cid:18)η(x)−12(cid:18)(cid:18)(cid:18)(cid:18)≤(cid:25)dPX(x)(cid:8)≥2(cid:2)Pr(S)−cβ(cid:3).With=(Pr(S)/(c(1+β)))1/β,thisshowsthatR(f)−R∗≥2βc1/β(1+β)(β+1)/β(Pr(S))(β+1)/β,andhencethedistributionsatisﬁesNβ/(β+1).Nowconsiderthesecondpartofthelemma.Foranymeasurablef:X→{±1},(8)isequivalenttoPr(Af)≤c(cid:23)Af|2η(x)−1|dPX(x)⇐⇒(cid:23)Af1cdPX(x)≤(cid:23)Af|2η(x)−1|dPX(x),(A.4)whereAf={x:f(x)(η(x)−1/2)<0}.NotethatAfrangesoverallmeasurablesubsetsof{x:|η(x)−1/2|>0},sothat(A.4)istrueforallsuchAfiffPr(cid:7)0<|2η(X)−1|<1c(cid:8)=0,whichis(7).ProofofLemma6TheproofofLemma6usestechniquesduetoBartlettandMendelson(2005),whichbuiltontheworkofMassart(2000b),KoltchinskiiandPanchenko(2000),Mendelson(2002),LugosiandWegkamp(2004),andBartlettetal.(2005).Weusethefollowingcon-centrationinequality,whichisareﬁnement,duetoRio(2001)andKlein(2002),ofaresultofMassart(2000a),followingTalagrand(1994)andLedoux(2001).ThebestestimatesontheconstantsareduetoBousquet(2002).LemmaA.1.ThereisanabsoluteconstantKforwhichthefol-lowingholds.LetGbeaclassoffunctionsdeﬁnedonXwithsupg∈G(cid:13)g(cid:13)∞≤b.SupposethatPisaprobabilitydistributionsuchthatforeveryg∈G,Eg=0.LetX1,...,XnbeindependentrandomvariablesdistributedaccordingtoPandsetσ2=supg∈Gvarg.DeﬁneZ=supg∈G1nn(cid:6)i=1g(Xi).Then,foreveryx>0andeveryρ>0,Pr(cid:16)Z≥(1+ρ)EZ+σ(cid:26)Kxn+K(1+ρ−1)bxn(cid:17)≤e−x.ToproveLemma6,fromtheconditiononF,wehavePr{∃f∈F:ˆEf≤α,Ef≥}≤Pr{∃f∈F:ˆEf≤α,Ef=}=Pr(cid:19)sup{Ef−ˆEf:f∈F,Ef=}≥(1−α)(cid:20).WeboundthisprobabilityusingLemmaA.1,withρ=1andG={Ef−f:f∈F,Ef=}.ThisshowsthatPr{∃f∈F:ˆEf≤α,Ef≥}≤Pr{Z≥(1−α)}≤e−x,providedthat2EZ≤(1−α)3,(cid:27)cβKxn≤(1−α)3,and4KBxn≤(1−α)3.(Wehaveusedthefactthatsupf∈F(cid:13)f(cid:13)∞≤Bimpliesthatsupg∈G(cid:13)g(cid:13)∞≤2B.)ObservingthatEZ=ξF(),andrearranginggivestheresult.ProofofLemma7Theproofproceedsintwosteps.TheLipschitzconditionallowsustorelateEg2fto˜d(f,f∗),andthemodulusofconvexitycondition,togetherwiththeconvexityofF,relatesthistoEgf.WehaveEg2f=E(cid:2)(cid:1)(f(X))−(cid:1)(f∗(X))(cid:3)2≤E(cid:2)Ld(cid:2)f(X),f∗(X)(cid:3)(cid:3)2=L2(˜d(f,f∗))2.(A.5)Fromthedeﬁnitionofthemodulusofconvexity,R(f)+R(f∗)2≥R(cid:7)f+f∗2(cid:8)+˜δ(˜d(f,f∗))≥R(f∗)+˜δ(˜d(f,f∗)),wheretheoptimalityoff∗intheconvexsetFimpliesthesecondinequality.RearranginggivesEgf=R(f)−R(f∗)≥2˜δ(˜d(f,f∗)).Combiningwith(A.5)givestheresult.Bartlett,Jordan,andMcAuliffe:Convexity,Classiﬁcation,andRiskBounds155ProofofLemma8Fixfunctionsf1,f2:X→Rwith˜d(f1,f2)=(cid:9)Ed2(f1(X),f2(X))≥.WehaveR(f1)+R(f2)2−R(cid:7)f1+f22(cid:8)=E(cid:7)(cid:1)(f1(X))+(cid:1)(f2(X))2−(cid:1)(cid:7)f1(X)+f2(X)2(cid:8)(cid:8)≥E(cid:2)δ(cid:2)d(cid:2)f1(X),f2(X)(cid:3)(cid:3)(cid:3)≥cEdr(cid:2)f1(X),f2(X)(cid:3)=cE(cid:2)d2(cid:2)f1(X),f2(X)(cid:3)(cid:3)r/2.Whenthefunctionξ(a)=ar/2isconvex(i.e.,whenr≥2),Jensen’sinequalityshowsthatR(f1)+R(f2)2−R(cid:7)f1+f22(cid:8)≥cr.Otherwise,weusethefollowingconvexlowerboundonξ:[0,B2]→[0,Br]:ξ(a)=ar/2≥BraB2,whichfollowsfrom(theconcaveanalogof)Lemma1,part2.ThisimpliesthatR(f1)+R(f2)2−R(cid:7)f1+f22(cid:8)≥cBr−22.ProofofTheorem5ItisclearthatFisconvexandsatisﬁestheconditionsofTheorem4.Thattheoremimpliesthat,withprobabilityatleast1−δ,(cid:2)R(ˆf)−R∗(cid:3)2−α≤c(cid:11)(cid:4)n+inff∈FRφ(f)−R∗φ(cid:5),providedthatn≥Kmax(cid:16)∗n,L2Bln(1/δ)2aBn,BLBln(1/δ)n(cid:17),where∗n≥ξgF(∗n).Itremainstoprovesuitableupperboundsfor∗n.Byaclassicalsymmetrizationinequality(see,e.g.,VanderVaartandWellner1996),wecanupperboundξgFintermsoflocalRademacheraverages,ξgF()=Esup{Egf−ˆEgf:f∈F,Egf=}≤2Esup(cid:21)1nn(cid:6)i=1σigf(Xi,Yi):f∈F,Egf=(cid:22),wheretheexpectationsareoverthesample(X1,Y1),...,(Xn,Yn)andtheindependentuniform(Rademacher)randomvariablesσi∈{±1}.TheLedouxandTalagrand(1991)contractioninequalityandLemma7implythatξgF()≤4LEsup(cid:21)1nn(cid:6)i=1σidφ(cid:2)Yif(Xi),Yif∗(Xi)(cid:3):f∈F,Egf=(cid:22)≤4LEsup(cid:21)1nn(cid:6)i=1σidφ(cid:2)Yif(Xi),Yif∗(Xi)(cid:3):f∈F,˜dφ(f,f∗)2≤2aB(cid:22)=4LEsup(cid:21)1nn(cid:6)i=1σif(Xi,Yi):f∈Fφ,Ef2≤2aB(cid:22),whereFφ=(cid:19)(x,y)(cid:12)→dφ(cid:2)yf(x),yf∗(x)(cid:3):f∈F(cid:20).OneapproachtoapproximatingtheselocalRademacheraveragesisthroughinformationabouttherateofgrowthofcoveringnumbersoftheclass.ForsomesubsetAofapseudometricspace(S,d),letN(,A,d)denotethecardinalityofthesmallest-coverofA,thatis,thesmallestsetˆA⊂Sforwhicheverya∈Ahassomeˆa∈ˆAwithd(a,ˆa)≤.UsingDudley’sentropyintegral(Dudley1999),Mendelson(2002)showedthefollowingresult.SupposethatFisasetof[−1,1]-valuedfunctionsonXandthatthereareγ>0and0<p<2forwhichsupPN(cid:2),F,L2(P)(cid:3)≤γ−p,wherethesupremumisoverallprobabilitydistributionsPonX.ThenforsomeconstantCγ,p(whichdependsonlyonγandp),1nEsup(cid:21)n(cid:6)i=1σif(Xi):f∈F,Ef2≤(cid:22)≤Cγ,pmax(cid:19)n−2/(2+p),n−1/2(2−p)/4(cid:20).Becausedφ(a,b)≤|a−b|,any-coverof{f−f∗:f∈F}isan-coverofFφ,sothatN(,Fφ,L2(P))≤N(,F,L2(P)).Nowfortheclassabsconv(G)withdVC(G)=d,wehavesupPN(cid:2),absconv(G),L2(P)(cid:3)≤Cd−2d/(d+2);(see,e.g.,VanderVaartandWellner1996).ApplyingMendelson’sresultshowsthat1nEsup(cid:21)n(cid:6)i=1σif(Xi):f∈Babsconv(G),Ef2≤(cid:22)≤Cdmax(cid:19)Bn−(d+2)/(2d+2),Bd/(d+2)n−1/21/(d+2)(cid:20).Solvingfor∗n≥ξgF(∗n)showsthatitsufﬁcestochoose∗n=C(cid:11)dBLBmax(cid:16)(cid:7)LBaBB(cid:8)1/(d+1),1(cid:17)n−(d+2)/(2d+2)forsomeconstantC(cid:11)dthatdependsonlyond.APPENDIXB:LOSS,RISK,ANDDISTANCEWecouldconstrueRφastheriskunderalossfunction(cid:1)φ:R×{±1}→[0,∞)deﬁnedby(cid:1)φ(ˆy,y)=φ(ˆyy).Thefollowingresultes-tablishesthatlossfunctionsofthisformarefundamentallyunlikedis-tancemetrics.LemmaB.1.Supposethat(cid:1)φ:R2→[0,∞)hastheform(cid:1)φ(x,y)=φ(xy)forsomeφ:R→[0,∞).Thenthefollowingresultshold:1.(cid:1)φisnotadistancemetriconR.2.(cid:1)φisapseudometriconRiffφ≡0,inwhichcase(cid:1)φassignsdistance0toeverypairofreals.Proof.Byhypothesis,(cid:1)φisnonnegativeandsymmetric.Anotherrequirementofadistancemetricisdeﬁniteness;forallx,y∈R,x=y⇐⇒(cid:1)φ(x,y)=0.(B.1)Butwemaywriteanyz∈(0,∞)intwodifferentways,as√z√zand,forexample,√2z√(1/2)z.Satisfying(B.1)requiresφ(z)=0intheformercaseandφ(z)>0inthelattercase,animpossibility.Thisprovespart1.Toprovepart2,recallthatapseudometricrelaxes(B.1)tothere-quirementx=y⇒(cid:1)φ(x,y)=0.(B.2)156JournaloftheAmericanStatisticalAssociation,March2006Becauseeachz≥0hastheformxyforx=y=√z,(B.2)amountstothenecessaryconditionthatφ≡0on[0,∞).Theﬁnalrequirementon(cid:1)φisthetriangleinequality,whichintermsofφbecomesφ(xz)≤φ(xy)+φ(yz)forallx,y,z∈R.(B.3)Becauseφmustvanishon[0,∞),takingy=0in(B.3)showsthatonlythezerofunctioncan(anddoes)satisfytheconstraint.[ReceivedApril2003.RevisedJune2005.]REFERENCESArora,S.,Babai,L.,Stern,J.,andSweedyk,Z.(1997),“TheHardnessofApproximateOptimainLattices,Codes,andSystemsofLinearEquations,”JournalofComputerandSystemSciences,54,317–331.Bartlett,P.L.(1998),“TheSampleComplexityofPatternClassiﬁcationWithNeuralNetworks:TheSizeoftheWeightsIsMoreImportantThantheSizeoftheNetwork,”IEEETransactionsonInformationTheory,44,525–536.Bartlett,P.L.,Bousquet,O.,andMendelson,S.(2005),“LocalRademacherComplexities,”TheAnnalsofStatistics,33,1497–1537.Bartlett,P.L.,andMendelson,S.(2005),“EmpiricalMinimization,”Probabil-ityTheoryandRelatedFields,toappear.Boser,B.E.,Guyon,I.M.,andVapnik,V.N.(1992),“ATrainingAlgorithmforOptimalMarginClassiﬁers,”inProceedingsofthe5thAnnualWorkshoponComputationalLearningTheory,NewYork:ACMPress,pp.144–152.Bousquet,O.(2002),“ABennettConcentrationInequalityandItsApplicationtoSupremaofEmpiricalProcesses,”ComptesRendusdel’AcadémiedesSci-ences,SérieI,334,495–500.Boyd,S.,andVandenberghe,L.(2004),ConvexOptimization,Cambridge,U.K.:CambridgeUniversityPress.Breiman,L.(1999),“PredictionGamesandArcingAlgorithms,”NeuralCom-putation,11,1493–1517.(2004),“PopulationTheoryforBoostingEnsembles,”TheAnnalsofStatistics,32,1–11.Brown,L.D.(1986),FundamentalsofStatisticalExponentialFamilies,Hay-ward,CA:InstituteofMathematicalStatistics.Collins,M.,Schapire,R.E.,andSinger,Y.(2002),“LogisticRegression,Ad-aboostandBregmanDistances,”MachineLearning,48,253–285.Cortes,C.,andVapnik,V.(1995),“Support-VectorNetworks,”MachineLearn-ing,20,273–297.Cristianini,N.,andShawe-Taylor,J.(2000),AnIntroductiontoSupportVectorMethods,Cambridge,U.K.:CambridgeUniversityPress.Devroye,L.,Györﬁ,L.,andLugosi,G.(1996),AProbabilisticTheoryofPat-ternRecognition,NewYork:Springer-Verlag.Dudley,R.M.(1999),UniformCentralLimitTheorems,Cambridge,U.K.:CambridgeUniversityPress.Feder,M.,Figueiredo,M.A.T.,Hero,A.O.,Lee,C.-H.,Loeliger,H.-A.,Nowak,R.,Singer,A.C.,andYu,B.(2004),“SpecialIssueonMachineLearningMethodsinSignalProcessing,”IEEETransactionsonSignalProcessing,52,2152.Freund,Y.,andSchapire,R.E.(1997),“ADecision-TheoreticGeneralizationofOn-LineLearningandanApplicationtoBoosting,”JournalofComputerandSystemSciences,55,119–139.Friedman,J.,Hastie,T.,andTibshirani,R.(2000),“AdditiveLogisticRegres-sion:AStatisticalViewofBoosting,”TheAnnalsofStatistics,28,337–374.Jiang,W.(2004),“ProcessConsistencyforAdaboost,”TheAnnalsofStatistics,32,13–29.Joachims,T.(2002),LearningtoClassifyTextUsingSupportVectorMachines,Dordrecht:KluwerAcademic.Jordan,M.I.,Ghahramani,Z.,Jaakkola,T.S.,andSaul,L.K.(1999),“Intro-ductiontoVariationalMethodsforGraphicalModels,”MachineLearning,37,183–233.Klein,T.(2002),“UneInégalitédeConcentrationàGauchePourlesProces-susEmpiriques”[ALeftConcentrationInequalityforEmpiricalProcesses],ComptesRendusdel’AcadémiedesSciences,SérieI,334,501–504.Koltchinskii,V.I.,andPanchenko,D.(2000),“RademacherProcessesandBoundingtheRiskofFunctionLearning,”inHighDimensionalProbabil-ityII,Vol.47,eds.E.Giné,D.M.Mason,andJ.A.Wellner,Boston,MA:Birkhäuser,pp.443–459.(2002),“EmpiricalMarginDistributionsandBoundingtheGeneral-izationErrorofCombinedClassiﬁers,”TheAnnalsofStatistics,30,1–50.Lebanon,G.,andLafferty,J.(2002),“BoostingandMaximumLikelihoodforExponentialModels,”inAdvancesinNeuralInformationProcessingSys-tems14,eds.T.Dietterich,S.Becker,andZ.Ghahramani,Cambridge,MA:MITPress,pp.447–454.Ledoux,M.(2001),TheConcentrationofMeasurePhenomenon,Providence,RI:AmericanMathematicalSociety.Ledoux,M.,andTalagrand,M.(1991),ProbabilityinBanachSpaces:IsoperimetryandProcesses,NewYork:Springer-Verlag.Lee,W.S.,Bartlett,P.L.,andWilliamson,R.C.(1996),“EfﬁcientAgnosticLearningofNeuralNetworksWithBoundedFan-in,”IEEETransactionsonInformationTheory,42,2118–2132.Lin,Y.(2004),“ANoteonMargin-BasedLossFunctionsinClassiﬁcation,”StatisticsandProbabilityLetters,68,73–82.Lugosi,G.,andVayatis,N.(2004),“OntheBayesRiskConsistencyofRegu-larizedBoostingMethods,”TheAnnalsofStatistics,32,30–55.Lugosi,G.,andWegkamp,M.(2004),“ComplexityRegularizationviaLocal-izedRandomPenalties,”TheAnnalsofStatistics,32,1679–1697.Mammen,E.,andTsybakov,A.B.(1999),“SmoothDiscriminationAnalysis,”TheAnnalsofStatistics,27,1808–1829.Mannor,S.,andMeir,R.(2001),“GeometricBoundsforGeneralizationinBoosting,”inProceedingsoftheFourteenthAnnualConferenceonCompu-tationalLearningTheory,eds.D.HelmboldandR.Williamson,Springer-Verlag,pp.461–472.Mannor,S.,Meir,R.,andZhang,T.(2002),“TheConsistencyofGreedyAlgo-rithmsforClassiﬁcation,”inProceedingsoftheAnnualConferenceonCom-putationalLearningTheory,eds.J.KivinenandR.Sloan,Springer-Verlag,pp.319–333.Marron,J.S.,andTodd,M.(2002),“Distance-WeightedDiscrimination,”Tech-nicalReport1339,CornellUniversity,SchoolofOperationsResearchandIndustrialEngineering.Massart,P.(2000a),“AbouttheConstantsinTalagrand’sConcentrationIn-equalityforEmpiricalProcesses,”TheAnnalsofProbability,28,863–884.(2000b),“SomeApplicationsofConcentrationInequalitiestoStatis-tics,”AnnalesdelaFacultédesSciencesdeToulouse,IX,245–303.Mendelson,S.(2002),“ImprovingtheSampleComplexityUsingGlobalData,”IEEETransactionsonInformationTheory,48,1977–1991.Nesterov,Y.,andNemirovskii,A.(1994),Interior-PointPolynomialAlgorithmsinConvexProgramming,Philadelphia:SIAMPublications.Rio,E.(2001),“InégalitésdeConcentrationPourlesProcessusEmpiriquesdeClassesdeParties”[ConcentrationInequalitiesforSet-IndexedEmpiricalProcesses],ProbabilityTheoryandRelatedFields,119,163–175.Rockafellar,R.T.(1997),ConvexAnalysis,Princeton,NJ:PrincetonUniversityPress.Schapire,R.E.,Freund,Y.,Bartlett,P.,andLee,W.S.(1998),“BoostingtheMargin:ANewExplanationfortheEffectivenessofVotingMethods,”TheAnnalsofStatistics,26,1651–1686.Schölkopf,B.,andSmola,A.(2002),LearningWithKernels,Cambridge,MA:MITPress.Schölkopf,B.,Tsuda,K.,andVert,J.-P.(2003),KernelMethodsinComputa-tionalBiology,Cambridge,MA:MITPress.Shawe-Taylor,J.,Bartlett,P.L.,Williamson,R.C.,andAnthony,M.(1998),“StructuralRiskMinimizationOverData-DependentHierarchies,”IEEETransactionsonInformationTheory,44,1926–1940.Shen,X.,Tseng,G.C.,Zhang,X.,andWong,W.H.(2003),“OnPsi-Learning,”JournaloftheAmericanStatisticalAssociation,98,724–734.Steinwart,I.(2005),“ConsistencyofSupportVectorMachinesandOtherReg-ularizedKernelClassiﬁers,”IEEETransactionsonInformationTheory,51,128–142.Talagrand,M.(1994),“SharperBoundsforGaussianandEmpiricalProcesses,”TheAnnalsofProbability,22,28–76.Tsybakov,A.(2004),“OptimalAggregationofClassiﬁersinStatisticalLearn-ing,”TheAnnalsofStatistics,32,135–166.VanderVaart,A.W.,andWellner,J.A.(1996),WeakConvergenceandEmpir-icalProcesses,NewYork:Springer-Verlag.Zhang,T.(2003),“SequentialGreedyApproximationforCertainConvexOpti-mizationProblems,”IEEETransactionsonInformationTheory,49,682–691.(2004),“StatisticalBehaviorandConsistencyofClassiﬁcationMeth-odsBasedonConvexRiskMinimization,”TheAnnalsofStatistics,32,56–85.Zhang,T.,andYu,B.(2005),“BoostingWithEarlyStopping:ConvergenceandConsistency,”TheAnnalsofStatistics,33,1538–1579.