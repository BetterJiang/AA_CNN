Convex and Semi-Nonnegative Matrix Factorizations

Chris Ding

Tao Li

Lawrence Berkeley National Laboratory

School of Computer Science

University of California

Berkeley, CA 94720, USA

chqding@lbl.gov

Florida International University

Miami, FL 33199, USA

taoli@cs.fiu.edu

Department of Electrical Engineering and Computer Science

Michael I. Jordan

Department of Statistics
University of California

Berkeley, CA 94720, USA
jordan@cs.berkeley.edu

November 22, 2006

Abstract

We present several new variations on the theme of nonnegative matrix factorization
(NMF). Considering factorizations of the form X = F GT , we focus on algorithms in
which G is restricted to contain nonnegative entries, but allow the data matrix X to have
mixed signs, thus extending the applicable range of NMF methods. We also consider
algorithms in which the basis vectors of F are constrained to be convex combinations of
the data points. This is used for a kernel extension of NMF. We provide algorithms for
computing these new factorizations and we provide supporting theoretical analysis. We
also analyze the relationships between our algorithms and clustering algorithms, and
consider the implications for sparseness of solutions. Finally, we present experimental
results that explore the properties of these new methods.

1

Introduction

Matrix factorization is a unifying theme in numerical linear algebra. A wide variety of matrix
factorization algorithms have been developed over many decades, providing a numerical
platform for matrix operations such as solving linear systems, spectral decomposition, and
subspace identi(cid:12)cation. Some of these algorithms have also proven useful in statistical data
analysis, most notably the singular value decomposition (SVD), which underlies principal
component analysis (PCA).

Recent work in machine learning has focused on matrix factorizations that directly
target some of the special features of statistical data analysis. In particular, nonnegative
matrix factorization (NMF) (Lee and Seung, 1999, 2001) focuses on the analysis of data
matrices whose elements are nonnegative, a common occurrence in data sets derived from

1

text and images. Moreover, NMF yields nonnegative factors, which can be advantageous
from the point of view of interpretability.

The scope of research on NMF has grown rapidly in recent years. NMF has been shown
to be useful in a variety of applied settings, including environmetrics (Paatero and Tapper,
1994), chemometrics (Xie et al., 1999), pattern recognition (Li et al., 2001), multimedia data
analysis (Cooper and Foote, 2002), text mining (Xu et al., 2003; Pauca et al., 2004) and
DNA gene expression analysis (Brunet et al., 2004). Algorithmic extensions of NMF have
been developed to accommodate a variety of objective functions (Dhillon and Sra, 2005;
Ding et al., 2006) and a variety of data analysis problems, including classi(cid:12)cation (Sha
et al., 2003) and collaborative (cid:12)ltering (Srebro et al., 2005). A number of studies have
focused on further developing computational methodologies for NMF (Hoyer, 2004; Berry
et al., 2006; Li and Ma, 2004). Finally, researchers have begun to explore some of the
relationships between matrix factorizations and K-means clustering (Ding et al., 2005); as
we emphasize in the current paper, this relationship has implications for the interpretability
of matrix factors.

Our goal in this paper is to expand the repertoire of nonnegative matrix factorization.
Our focus is on algorithms that constrain the matrix factors; we do not require the data
matrix to be similarly constrained. In particular, we develop NMF-like algorithms that yield
nonnegative factors but do not require that the data matrix be nonnegative. This extends
the range of application of NMF ideas. Moreover, by focusing on constraints on the matrix
factors, we are able to strengthen the connections between NMF and K-means clustering.
Note in particular that the result of a K-means clustering run can be written as a matrix
factorization X = F GT , where X is the data matrix, F contains the cluster centroids,
and G contains the cluster membership indicators. Although F typically has entries with
both positive and negative signs, G is nonnegative. This motivates us to propose general
factorizations in which G is restricted to be nonnegative and F is unconstrained. We also
consider algorithms that constrain F ; in particular, restricting the columns of F to be
convex combinations of data points in X we obtain a matrix factorization that can be
interpreted in terms of weighted cluster centroids.

The paper is organized as follows. In Section 2 we present the new matrix factorizations
and in Section 3 we present algorithms for computing these factorizations. Section 4 provides
a theoretical analysis which provides insights into the sparseness of matrix factors for a
convex variant of NMF. In Section 5.1 we show that a convex variant of NMF has the
advantage that it is readily kernelized. In Section 6 we present comparative experiments
that show that constraining the F factors to be convex combinations of input data enhances
their interpretability. We also present experiments that compare the performance of the
NMF variants to K-means clustering, where we assess the extent to which the imposition
of constraints that aim to enhance interpretability leads to poorer clustering performance.
Finally, we present our conclusions in Section 7.

2

2 Semi-NMF and Convex-NMF

Let the input data matrix X = (x1; : : : ; xn) contain a collection of n data vectors as columns.
We consider factorizations of the form:

X (cid:25) F GT ;

(1)

where X 2 Rp(cid:2)n; F 2 Rp(cid:2)k and G 2 Rn(cid:2)k. For example, the SVD can be written in this
form. In the case of the SVD, there are no restrictions on the signs of F and G; moreover,
the data matrix X is also unconstrained. NMF can also be written in this form, where the
data matrix X is assumed to be nonnegative, as are the factors F and G. We now consider
some additional examples.

2.1 Semi-NMF

When the data matrix is unconstrained (i.e., it may have mixed signs), we consider a
factorization that we refer to as Semi-NMF, in which we restrict G to be nonnegative while
placing no restriction on the signs of F .

We can motivate Semi-NMF from the perspective of clustering. Suppose we do a K-
means clustering on X and obtain cluster centroids F = (f1; : : : ; fk). Let G denote the
cluster indicators: i.e., gik = 1 if xi belongs to cluster ck; gik = 0 otherwise. We can write
the K-means clustering objective function as

JK-means =

n

K

Xi=1

Xk=1

gikjjxi (cid:0) fkjj2 = jjX (cid:0) F GTjj2:

In this paper, jjvjj denotes the L2 norm of a vector v and jjAjj denotes the Frobenius
norm of a matrix A. We see that the K-means clustering objective can be alternatively
viewed as an objective function for matrix approximation. Moreover, this approximation
will generally be tighter if we relax the optimization by allowing gij to range over values in
(0; 1), or values in (0;1). This yields the Semi-NMF matrix factorization.

2.2 Convex-NMF

While in NMF and Semi-NMF there are no constraints on the basis vectors F = (f1;(cid:1)(cid:1)(cid:1) ; fk),
for reasons of interpretability it may be useful to impose the constraint that the vectors
de(cid:12)ning F lie within the column space of X:

f‘ = w1‘x1 + (cid:1)(cid:1)(cid:1) + wn‘xn = Xw‘; or F = XW:

(2)

Moreover, again for reasons of interpretability, we may wish to restrict ourselves to convex
combinations of the columns of X. This constraint has the advantage that we could interpret
the columns f‘ as weighted sums of certain data points; in particular, these columns would
capture a notion of centroids. We refer to this restricted form of the F factor as Convex-
NMF. Convex-NMF applies to both nonnegative and mixed-sign data matrices. As we will
see, Convex-NMF has an interesting property: the factors W and G both tend to be very
sparse.

3

Lee and Seung (1997) considered a model in which the F factors were restricted to the
unit interval; i.e., 0 (cid:20) Fik (cid:20) 1. This so-called convex coding does not require the fk to be
nonnegative linear combinations of input data vectors and thus in general do not capture
the notion of cluster centroid. Indeed, the emphasis in Lee and Seung (1997) and in Lee
and Seung (1999) is the parts-of-whole encoding provided by NMF, not the relationship of
nonnegative factorizations to vector quantization.

To summarize our development thus far, let us write the di(cid:11)erent factorizations as

follows:

SVD:

NMF:

Semi-NMF:

Convex-NMF:

(cid:6)

X(cid:6) (cid:25) F(cid:6)GT
X+ (cid:25) F+GT
X(cid:6) (cid:25) F(cid:6)GT
X(cid:6) (cid:25) X(cid:6)W+GT
+;

+

+

(3)

(4)

(5)

(6)

where the subscripts are intended to suggest the constraints imposed by the di(cid:11)erent fac-
torizations.

Before turning to a presentation of algorithms for computing Semi-NMF and Convex-
NMF factorizations and supporting theoretical analysis, we provide an illustrative example.

2.3 An Illustration

Consider the following data matrix:

X =

7:1

5:0

5:2

1:8
6:9
1:6
8:3

1:3
4:8
8:0
1:5
3:9 (cid:0)5:5 (cid:0)8:5 (cid:0)3:9 (cid:0)5:5
6:5
8:2 (cid:0)7:2 (cid:0)8:7 (cid:0)7:9 (cid:0)5:2
7:4
3:8
4:7
(cid:0)7:3 (cid:0)1:8 (cid:0)2:1
6:2

6:4
2:7

7:5
6:8

3:2
4:8

0
BBBB@

:

1
CCCCA

One can see that the (cid:12)rst three columns should be clustered together and the last four
columns should form another cluster. The computed basis vectors F for the di(cid:11)erent matrix
factorizations are as follows:

Fsvd =

; Fsemi =

; Fcnvx =

(cid:0)0:41
0:50
0:35
0:21
0:66
0:32
(cid:0)0:28
0:72
(cid:0)0:43 (cid:0)0:28

0
BBBB@

1
CCCCA

0:05
0:27
0:40 (cid:0)0:40
0:70 (cid:0)0:72
0:08
0:30
(cid:0)0:51
0:49

0
BBBB@

1
CCCCA

0:31
0:53
0:42 (cid:0)0:30
0:56 (cid:0)0:57
0:41
0:49
(cid:0)0:41
0:36

0
BBBB@

;

1
CCCCA

and the cluster centroids obtained from K-means clustering are given by the columns of
the following matrix:

We have rescaled all column vectors so that their L2-norm is one for purposes of comparison.

CKmeans =

0:29
0:52
0:45 (cid:0)0:32
0:59 (cid:0)0:60
0:36
0:46
(cid:0)0:41
0:37

0
BBBB@

:

1
CCCCA

4

One can see that Fcnvx is close to CKmeans:

jjFcnvx (cid:0) CKmeansjj = 0:08. Fsemi deviates
substantially from CKmeans: jjFsemi (cid:0) CKmeansjj = 0:53. Two of the elements in Fsemi are par-
ticularly far from those in CKmeans: (Fsemi)1;1 = 0:05 and (Fsemi)4;2 = 0:08. Thus restrictions
on F can have large e(cid:11)ects on subspace factorization. Convex-NMF gives F factors that
are closer to cluster centroids, validating our expectation that this factorization produces
centroid-like factors. More examples are given in Figure 1.

Let us now consider the matrix factor G for the three factorizations:

GT

GT

GT

0:01

0:50 0:60 0:43

0:30 (cid:0)0:12

svd =(cid:18)0:25 0:05 0:22 (cid:0):45 (cid:0):44 (cid:0):46 (cid:0):52
0:31(cid:19) ;
semi =(cid:18)0:61 0:89 0:54 0:77 0:14 0:36 0:84
0:12 0:53 0:11 1:03 0:60 0:77 1:16(cid:19) ;
0:31 0:27 0:30 0:36(cid:19) :
cnvx =(cid:18)0:31 0:31 0:29 0:02

0:06

0

0

0

0:02

0

Again we see that both Semi-NMF and Convex-NMF tend to give results that correspond
to the K-means clustering: In each of the (cid:12)rst three columns, the values in the upper rows
are larger than the lower rows, while in each of the last four columns the upper rows are
smaller than the lower rows. Note, however, that Convex-NMF gives sharper indicators of
the clustering.

Finally, computing the residual values, we have jjX(cid:0)F GTjj = 0:27940; 0:27944; 0:30877;
for SVD, Semi-NMF and Convex-NMF, respectively. We see that the enhanced inter-
pretability provided by Semi-NMF is not accompanied by a degradation in approximation
accuracy relative to the SVD. The more highly constrained Convex-NMF involves a modest
degradation in accuracy.

We now turn to a presentation of algorithms for computing the two new factorizations,

together with theoretical results establishing convergence of these algorithms.

3 Algorithms and analysis

In this section we provide algorithms and accompanying analysis for the NMF factorizations
that we presented in the previous section.

3.1 Algorithm for Semi-NMF

We compute the Semi-NMF factorization via an iterative updating algorithm that alterna-
tively updates F and G:

(S0) Initialize G. Do a K-means clustering. This gives cluster indicators G: Gik = 1 if xi
belongs to cluster k. Otherwise, Gik = 0. Add a small constant (we use the value 0:2 in
practice) to all elements of G.

(S1) Update F (while (cid:12)xing G) using the rule

F = XG(GT G)(cid:0)1:

(7)

5

Note GT G is a k (cid:2) k positive semide(cid:12)nite matrix. The inversion of this small matrix is
trivial.

(S2) Update G (while (cid:12)xing F ) using

Gik   Giks (X T F )+

(X T F )(cid:0)

ik + [G(F T F )(cid:0)]ik
ik + [G(F T F )+]ik

;

where we separate the positive and negative parts of a matrix A as

A+
ik = (jAikj + Aik)=2;

A(cid:0)
ik = (jAikj (cid:0) Aik)=2:

(8)

(9)

The computational complexity for Semi-NMF is of order m(pnk + nk2) for Step (S1) and
of order m(npk + kp2 + n2k) for Eq.(8), where m (cid:24) 100 is the number of iterations to
convergence.

Theorem 1 (A) Fixing F , the residual jjX (cid:0) F GTjj2 decreases monotonically (i.e., it is
non-increasing) under the update rule for G. (B) Fixing G, the update rule for F gives the
optimal solution to minF jjX (cid:0) F Gjj2.
Proof. We (cid:12)rst prove part (B). The objective function that we minimize is the following
sum of squared residuals:

J = jjX (cid:0) F GTjj2 = Tr (X T X (cid:0) 2X T F GT + GF T F GT ):

(10)

Fixing G, the solution for F is obtained by computing dJ=dF = (cid:0)2XG + 2F GT G = 0:
This gives the solution F = XG(GT G)(cid:0)1.
To prove part (A), we now (cid:12)x F and solve for G while imposing the restriction G (cid:21) 0.
This is an constrained optimization problem. We present two results: (1) We show that at
convergence, the limiting solution of the update rule of Eq. (8) satis(cid:12)es the KKT condition.
This is established in Proposition 1 below. This proves the correctness of the limiting
solution. (2) We show that the iteration of the update rule of Eq. (8) converges. This is
established in Proposition 2 below.
u{
Proposition 1 The limiting solution of the update rule in Eq. (8) satis(cid:12)es the KKT con-
dition.

Proof. We introduce the Lagrangian function

L(G) = Tr ((cid:0)2X T F GT + GF T F GT (cid:0) (cid:12)GT );

(11)

where the Lagrangian multipliers (cid:12)ij enforce nonnegative constraints, Gij (cid:21) 0. The zero
gradient condition gives @L
@G = (cid:0)2X T F +2GF T F(cid:0)(cid:12) = 0: From the complementary slackness
condition, we obtain
((cid:0)2X T F + 2GF T F )ikGik = (cid:12)ikGik = 0:
(12)

This is a (cid:12)xed point equation that the solution must satisfy at convergence.

6

It is easy to see that the limiting solution of the update rule of Eq. (8) satis(cid:12)es the (cid:12)xed

point equation. At convergence, G(1) = G(t+1) = G(t) = G; i.e.,

Gik = Giks (F T X)+

(F T X)(cid:0)

ik + [G(F T F )(cid:0)]ik
ik + [G(F T F )+]ik

:

(13)

Note F T F = (F T F )+ (cid:0) (F T F )(cid:0); F T X = (F T X)+ (cid:0) (F T X)(cid:0): Thus Eq. (13) reduces to

(cid:0)(cid:0)2X T F + 2GF T F(cid:1)ik G2

ik = 0:

(14)

Eq. (14) is identical to Eq. (12). Both equations require either of the two factors are zero
or both are zero. The (cid:12)rst factor in both equations are identical. For the second factor Gik
or G2
ik = 0, and vice versa. Thus if Eq. (12) holds, Eq. (14) also holds
and vice versa.
u{
Next we prove the convergence of the iterative update algorithm. We need to state two

ik, if Gik = 0 then G2

propositions that are used in the proof of convergence.

Proposition 2 The residual of Eq. (10) is monotonically decreasing (non-increasing) un-
der the update given in Eq. (8) for (cid:12)xed F .

Proof. We write J(H) as

J(H) = Tr((cid:0)2H T B+ + 2H T B(cid:0) + H T A+H (cid:0) H T A(cid:0)H)

(15)

where A = F T F , B = F T X, and H = G.

We use the auxiliary function approach (Lee and Seung, 2001). A function Z(H; ~H) is

called an auxiliary function of J(H) if it is satis(cid:12)es

for any H; ~H. De(cid:12)ne

Z(H; ~H) (cid:21) J(H); Z(H; H) = J(H);

H (t+1) = arg min

Z(H; H (t)):

H

(16)

(17)

By construction, we have J(H (t)) = Z(H (t); H (t)) (cid:21) Z(H (t+1); H (t)) (cid:21) J(H (t+1)): Thus
J(H (t)) is monotonic decreasing (non-increasing). The key is to (cid:12)nd appropriate Z(H; ~H).
According to Proposition 3, Z(H; H 0) de(cid:12)ned in Eq. (18) is an auxiliary function of J and
its minima is given by Eq. (19). According to Eq. (17), H (t+1)   H and H (t)   H 0, and
substitute A = F T F , B = F T X, and H = G, we recover Eq. (8).
u{
Proposition 3 Given the objective function J de(cid:12)ned as in Eq.(15), where all matrices
are nonnegative, the following function

Z(H; H 0) = (cid:0)Xik
+Xik

2B+

ikH 0

ik(1 + log

(A+H 0)ikH 2
ik

H 0
ik

Hik
H 0
ik

B(cid:0)
ik

) +Xik

H 2

ik + H 02
H 0
ik

ik

A(cid:0)

k‘H 0

ikH 0

i‘(1 + log

(cid:0)Xik‘

7

HikHi‘
H 0
ikH 0
i‘

)

(18)

is an auxiliary function for J(H); i.e., it satis(cid:12)es the requirements J(H) (cid:20) Z(H; H 0) and
J(H) = Z(H; H). Furthermore, it is a convex function in H and its global minima is

Hik = arg min

H

Proof. The function J(H) is

Z(H; H 0) = H 0

iks B+

B(cid:0)

ik + (A(cid:0)H 0)ik
ik + (A+H 0)ik

:

(19)

J(H) = Tr((cid:0)2H T B+ + 2H T B(cid:0) + H T A+H (cid:0) H T A(cid:0)H):

(20)

We (cid:12)nd upper bounds for each of the two positive terms, and lower bounds for each of
the two negative terms. For the third term in J(H), using Proposition 5 and setting
A   I; B   A+, we obtain a bound

Tr(H T A+H) (cid:20)Xik

(A+H 0)ikH 2
ik

H 0
ik

:

The second term of J(H) is bounded by

Tr(H T B(cid:0)) =Xik

HikB(cid:0)

ik (cid:20)Xik

B(cid:0)
ik

H 2

ik + H 02
2H 0
ik

ik

;

using the inequality a (cid:20) (a2 + b2)=2b, which holds for any a; b > 0.
which holds for any z > 0, and obtain

To obtain lower bounds for the two remaining terms, we use the inequality z (cid:21) 1 + logz,

and

Hik
ik (cid:21) 1 + log
H 0

Hik
H 0
ik

;

HikHi‘
ikH 0
H 0

i‘ (cid:21) 1 + log

HikHi‘
ikH 0
H 0
i‘

:

(21)

(22)

From Eq. (21), the (cid:12)rst term in J(H) is bounded by

Tr(H T B+) =Xik

B+

ikHik (cid:21)Xik

B+

ikH 0

ik(1 + log

Hik
H 0
ik

):

From Eq. (22), the last term in J(H) is bounded by

Tr(HA(cid:0)H T ) (cid:21)Xik‘

A(cid:0)

k‘H 0

ikH 0

i‘(1 + log

HikHi‘
H 0
ikH 0
i‘

):

Collecting all bounds, we obtain Z(H; H 0) as in Eq. (18). Obviously, J(H) (cid:20) Z(H; H 0) and
J(H) = Z(H; H).

To (cid:12)nd the minima of Z(H; H 0), we take

@Z(H; H 0)

@Hik

= (cid:0)2B+

ik

H 0
ik
Hik

+ 2B(cid:0)
ik

Hik
H 0
ik

+

2(A

+

H 0)ikHik
H 0
ik

(cid:0) 2

(A(cid:0)H 0)ikH 0
ik

Hik

:

(23)

8

The Hessian matrix containing the second derivatives

@2Z(H; H 0)
@Hik@Hj‘

= (cid:14)ij(cid:14)k‘Yik

is a diagonal matrix with positive entries

Yik =

4[(B+)ik + (A(cid:0)H 0)ik]H 0
ik

H 2
ik

+ 2

B(cid:0)

ik + (A+H 0)ik

H 0
ik

Thus Z(H; H 0) is a convex function of H. Therefore, we obtain the global minima by setting
@Z(H; H 0)=@Hik = 0 in Eq. (23) and solving for H. Rearranging, we obtain Eq.(19).
u{
+ ; with A and

Proposition 4 For any matrices A 2 Rn(cid:2)n
B symmetric, the following inequality holds:

+ ; B 2 Rk(cid:2)k

+ ; S 2 Rn(cid:2)k

+ ; S0 2 Rn(cid:2)k

n

k

Xi=1

Xp=1

(AS0B)ipS2
ip

S0
ip

(cid:21) Tr(ST ASB):

(24)

Proof. Let Sip = S0
side and the right-hand side can be written as

ipuip. Using an explicit index, the di(cid:11)erence (cid:1) between the left-hand

(cid:1) =

n

k

Xi;j=1

Xp;q=1

AijS0

jqBqpS0

ip(u2

ip (cid:0) uipujq):

Because A and B are symmetric, this is equal to

(cid:1) =

n

k

Xi;j=1

Xp;q=1

AijS0

jqBqpS0

ip(

u2
ip + u2
jq

2

(cid:0) uipujq) =

1
2

n

k

Xi;j=1

Xp;q=1

AijS0

jqBqpS0

ip(u2

ip (cid:0) u2

jq)2 (cid:21) 0:

u{
In the special case in which B = I and S is a column vector, this result reduces to a result
due to Lee and Seung (2001).

3.2 Algorithm for Convex-NMF

(C0) Initialize W and G. There are two methods.

We describe the algorithm for computing the Convex-NMF factorization when X has mixed
sign (denoted as X(cid:6)). When X is nonnegative, the algorithm is simpli(cid:12)ed in a natural way.
(1) Fresh start. Do a K-means
clustering. Let the obtained cluster indicators be H = (h1;(cid:1)(cid:1)(cid:1) ; hk), Hik = f0; 1g. Then set
G(0) = H + 0:2E, where E is a matrix of all 1’s. The cluster centroids can be computed
as fk = Xhk=nk, or F = XHD(cid:0)1
n , where Dn = diag(n1;(cid:1)(cid:1)(cid:1) ; nk). Thus W = HD(cid:0)1
n .
We smooth W and set W (0) = (H + 0:2E)D(cid:0)1
(2) Suppose we already have a NMF
n .
or Semi-NMF solution. In this case G is known and we set G(0) = G + 0:2E. We solve
X = XW GT for W . This leads to W = G(GT G)(cid:0)1. Since W must be nonnegative, we set

9

W (0) = W + + 0:2EhW +i; where hAi = Pij jAijj=jjAjj0 and where jjAjj0 is the number of

nonzero elements in A.

Then update G+ and W+ alternatively until convergence as follows:

(C1) Update G+ using

Gik   Giks [(X T X)+W ]ik + [GW T (X T X)(cid:0)W ]ik

[(X T X)(cid:0)W ]ik + [GW T (X T X)+W ]ik

:

This can be derived in a manner similar to Eq. (8), replacing F by XW ;

(C2) Update W+ using

Wik   Wiks [(X T X)+G]ik + [(X T X)(cid:0)W GT G]ik

[(X T X)(cid:0)G]ik + [(X T X)+W GT G]ik

:

(25)

(26)

The computational complexity for convex-NMF is of order n2p + m(2n2k + nk2) for
Eq.(25) and is of order m(2n2k + 2nk2) for Eq.(26), where m (cid:24) 100 is the number of
iterations to convergence. These are matrix multiplications and can be computed e(cid:14)ciently
on most computers.

The correctness and convergence of the algorithm are addressed in the following:

Theorem 2 Fixing G, under the update rule for W of Eq. (26), (A) the residual jjX (cid:0)
XW GTjj2 decreases monotonically (non-increasing), and (B) the solution converges to a
KKT (cid:12)xed point.

The proof of part (B) is given by Proposition 5, which ensures the correctness of the
solution. The proof of part (A) is given by Proposition 6, which ensures the convergence of
the algorithm.

Proposition 5 The limiting solution of update rule of Eq. (26) satis(cid:12)es the KKT condition.

Proof. We minimize

J2 = jjX (cid:0) XW GTjj2 = Tr (X T X (cid:0) 2GT X T XW + W T X T XW GT G);

where X 2 Rp(cid:2)n; W 2 Rn(cid:2)k
as in Semi-NMF. We focus on the minimization with respect to W ; that is, we minimize

+ : The minimization with respect to G is the same

+ ; G 2 Rk(cid:2)n

J(W ) = Tr ((cid:0)2GT X T XW + W T X T XW GT G):

We can easily obtain the KKT complementarity condition
((cid:0)X T XG + X T XW GT G)ikWik = 0:

Next we can show that the limiting solution of the update rule of Eq. (26) satis(cid:12)es

((cid:0)X T XG + X T XW GT G)ikW 2

ik = 0:

(27)

(28)

(29)

These two equations are identical for the same reasons that Eq. (14) is identical to Eq. (12).
Thus the limiting solution of the update rule satis(cid:12)es the KKT (cid:12)xed point condition.
u{

10

Proposition 6 The residual, Eq.(27), decreases monotonically (it is non-increasing). Thus
the algorithm converges.

Proof. We write J(W ) as

J(H) = Tr ((cid:0)2H T B+ + 2H T B(cid:0) + H T A+HC (cid:0) H T A(cid:0)HC);

(30)

where B = X T XG, A = X T X, C = GT G; H = W . J(H) di(cid:11)ers from J(H) of Eq.(20)
in that the last two terms has four matrix factors instead of three. Following the proof of
Proposition 3, with the aid of Proposition 4, we can prove that the following function

Z(H; H 0) =

(cid:0)Xik
+Xik

2B+

ikH 0

ik(1 + log

+

(A

H 0C)ikH 2
ik

H 0
ik

Hik
H 0
ik

B(cid:0)
ik

) +Xik

H 2

ik + H 02
H 0
ik

ik

A(cid:0)

ijH 0

jkCk‘H 0

i‘(1 + log

(cid:0)Xijk‘

HjkHi‘
jkH 0
H 0
i‘

)

(31)

is an auxiliary function of J(H), and furthermore, Z(H; H 0) is a convex function of H and
its global minima are

Hik = arg min

H

= H 0

iks B+

B(cid:0)

ik + (A(cid:0)H 0C)ik
ik + (A+H 0C)ik

:

(32)

From its minima and setting H (t+1)   H and H (t)   H 0, we recover Eq. (26), letting
B+ = (X T X)
u{

G, B(cid:0) = (X T X)(cid:0)G, A = X T X, C = GT G and H = W .

+

4 Sparsity of Convex-NMF

In the original presentation of NMF, Lee and Seung (1999) emphasized the desideratum of
sparsity. For example, in the case of image data, it was hoped that NMF factors would cor-
respond to a coherent part of the original image, for example a nose or an eye; these would
be sparse factors in which most of the components would be zero. Further experiments have
shown, however, that NMF factors are not necessarily sparse, and sparsi(cid:12)cation schemes
have been developed on top of NMF (Hoyer, 2004; Li et al., 2001). Parts-of-whole represen-
tations are not necessarily recovered by NMF, but conditions for obtaining parts-of-whole
representations have been discussed (Donoho and Stodden, 2004). See also D’Aspremont
et al. (2006) Zou et al. (2006), and Zhang et al. (2004) for related literatures on sparse
factorizations in the context of PCA.

Interestingly, the Convex-NMF factors W and G are naturally sparse. We provide
theoretical support for this assertion in this section, and provide additional experimental
support in Section 6. (Sparseness can also be seen in the illustrative example presented in
Section 2.3).

We (cid:12)rst note that Convex-NMF can be reformulated as

min

W;G(cid:21)0Xk

kjjvT
(cid:27)2

k (I (cid:0) W GT )jj2;

s.t. W 2 Rn(cid:2)k

+ ; G 2 Rk(cid:2)n
+ ;

(33)

11

where we use the SVD of X = U (cid:6)V T and thus have X T X = Pk (cid:27)2
jjX (cid:0) XW GTjj2 = Tr (I (cid:0) GW T )X T X(I (cid:0) W GT ) = Pk (cid:27)2

k . Therefore
k (I (cid:0) W GT )jj2: We now
claim that (a) this optimization will produce a sparse solution for W and G, and (b) The
more slowly the (cid:27)k decrease, the sparser the solution.

kjjvT

kvkvT

This second part of our argument is captured in a Lemma:

Lemma 1 The solution of the following optimization problem

W;G(cid:21)0jjI (cid:0) W GTjj2;
min

s.t. W; G 2 Rn(cid:2)K
+ ;

is given by W = G = any K columns of (e1 (cid:1)(cid:1)(cid:1) eK), where ek is a basis vector: (ek)i6=k =
0; (ek)i=k = 1:
Proof. We prove (cid:12)rst for a slightly more general case. Let D = diag(d1;(cid:1)(cid:1)(cid:1) ; dn) be
a diagonal matrix and d1 > d2 > (cid:1)(cid:1)(cid:1) > dn > 0. The Lemma holds if we replace I
by D and W = G = (pd1e1 (cid:1)(cid:1)(cid:1)pd1eK).
1 The proof follows the fact that we have a
unique spectral expansion Dek = dkek and D = Pn
k . Now we take the limit:
d1 = (cid:1)(cid:1)(cid:1) = dn = 1. The spectral expansion is not unique: I = Pn
k for any
orthogonal basis (u1;(cid:1)(cid:1)(cid:1) ; un) = U . However, due the nonnegativity constraint, (e1 (cid:1)(cid:1)(cid:1) en)
u{
is the only viable basis. Thus W = G = any K columns of (e1 (cid:1)(cid:1)(cid:1) en).
The main point from Lemma 9 is that the solution to minW;G jjI (cid:0) W GTjj2 are the
sparsest possible rank-K matrices W; G. Now returning to our characterization of Convex-
NMF of Eq.(33), we can write

k=1 dkekeT

k=1 ukuT

jjI (cid:0) W GTjj2 =Xk

jjeT

k (I (cid:0) W GT )jj2:

Comparing to Convex-NMF case, we see the projection of (I (cid:0) W GT ) onto the principal
components has more weight while the projection of (I(cid:0)W GT ) onto the non-principal com-
ponents has less weight. Thus we conclude that sparsity is enforced heavily in the principal
component subspace and lightly in the non-principal component subspace. Overall, Lemma
9 provides a basis for concluding that Convex-NMF tends to yield sparse solutions.

A more intuitive understanding of the source of the sparsity can be obtained by counting
parameters. Note in particular that Semi-NMF is based on Nparam = kp + kn parameters
whereas Convex-NMF is based on Nparam = 2kn parameters. Considering the usual case
n > p (i.e., the number of data points is more than the data dimension), Convex-NMF
has more parameters than Semi-NMF. But we know that Convex-NMF is a special case of
Semi-NMF. The resolution of these two contradicting facts is that some of the parameters
in Convex-NMF must be zero.

5 Additional remarks

Convex-NMF stands out for its interpretability and its sparsity properties.
In this sec-
tion we consider two additional interesting aspects of Convex-NMF and we also consider

1In NMF, the freedom of a diagonal rescaling is always present: Let E = (e1 (cid:1) (cid:1) (cid:1) eK). Our choice of
W = G = EpD can be written in di(cid:11)erent ways W GT = (EpD)(EpD)T = (ED(cid:11))(E T D1(cid:0)(cid:11))T , where
(cid:0)1 < (cid:11) < 1.

12

the relationship of all of the NMF-like factorizations that we have developed to K-means
clustering.

5.1 Kernel-NMF

Consider a mapping xi ! (cid:30)(xi), or X ! (cid:30)(X) = ((cid:30)(x1);(cid:1)(cid:1)(cid:1) ; (cid:30)(xn)). A standard NMF or
Semi-NMF factorization (cid:30)(X) (cid:25) F GT would be di(cid:14)cult to compute since F and G depend
explicitly on the mapping function (cid:30)((cid:1)). However, Convex-NMF provides an appealing
resolution of this problem:

(cid:30)(X) (cid:25) (cid:30)(X)W GT :
Indeed, it is easy to see that the minimization objective

jj(cid:30)(X) (cid:0) (cid:30)(X)W GTjj2 = Tr[(cid:30)(X)T (cid:30)(X) (cid:0) 2GT (cid:30)T (X)(cid:30)(X)W + W T (cid:30)T (X)(cid:30)(X)W GT G]
depends only on the kernel K = (cid:30)T (X)(cid:30)(X). In fact, the update rules for Convex-NMF
presented in Eqs.(26) and (25) depend on X T X only. Thus it is possible to \kernelize"
Convex-NMF in a manner analogous to the kernelization of PCA and K-means .

5.2 Cluster-NMF

In Convex-NMF, we require the columns of F to be convex combinations of input data.
Suppose now that we interpret the entries of G as posterior cluster probabilities. In this
case the cluster centroids can be computed as fk = Xgk=nk, or F = XGD(cid:0)1
n , where
Dn = diag(n1;(cid:1)(cid:1)(cid:1) ; nk). The extra degree of freedom for F is not necessary. Therefore,
the pair of desiderata: (1) F encodes centroids, and (2) G encodes posterior probabilities
motivates a factorization X (cid:25) XGD(cid:0)1

n GT . We can absorb D(cid:0)1

n into G and solve for

Cluster-NMF : X (cid:25) XG+GT
+:

(34)

We call this factorization Cluster-NMF because the degree of freedom in this factorization
is the cluster indicator G, as in a standard clustering problem. The objective function is
J = kX (cid:0) XGGTk2.

5.3 Relation to relaxed K-means clustering

NMF, Semi-NMF, Convex-NMF, Cluster-NMF and Kernel-NMF all have K-means clus-
tering interpretations when the factor G is orthogonal (GT G = I). Orthogonality and
nonnegativity together imply that each row of G has only one nonnegative element; i.e., G
is a bona (cid:12)de cluster indicator. This relationship to clustering is made more precise in the
following theorem.

Theorem 3 G-orthogonal NMF, Semi-NMF, Convex-NMF, Cluster-NMF and Kernel-NMF
are all relaxations of K-means clustering.

Proof. For NMF, Semi-NMF and Convex-NMF, we (cid:12)rst eliminate F . The objective
is J = kX (cid:0) F GTk2 = Tr(X T X (cid:0) 2X T F GT + F F T ): Setting @J=@F = 0, we obtain
F = XG: Thus we obtain J = Tr(X T X (cid:0) GT X T XG). For Cluster-NMF, we obtain the

13

same result directly: J = kX (cid:0) XGGTk2 = Tr(X T X (cid:0) GT X T XG): For Kernel-NMF,
we have J = k(cid:30)(X) (cid:0) (cid:30)(X)W GTk2 = Tr(K (cid:0) GT KW + W T KW ), where K is the kernel.
Setting @J=@W = 0, we have KG = KW . Thus J = Tr(X T X (cid:0) GT KG). In all (cid:12)ve of these
cases, the (cid:12)rst terms are constant and do not a(cid:11)ect the minimization. The minimization
problem thus becomes maxGT G=I Tr(GT KG); where K is either a linear kernel X T X or
h(cid:30)(X); (cid:30)(X)i. It is known that this is identical to (kernel-) K-means clustering (Zha et al.,
u{
2002).
In the de(cid:12)nitions of NMF, Semi-NMF, Convex-NMF, Cluster-NMF and Kernel-NMF,
G is not restricted to be orthogonal; these NMF variants are soft versions of K-means
clustering.

6 Experiments

We (cid:12)rst present the results of an experiment on synthetic data which aims to verify that
Convex-NMF can yield factorizations that are close to cluster centroids. We then present
experimental results for real data comparing K-means clustering and the various factor-
izations.

6.1 Synthetic dataset

One main theme of our work is that the Convex-NMF variants may provide subspace fac-
torizations that have more interpretable factors than those obtained by other NMF variants
(or PCA). In particular, we expect that in some cases the factor F will be interpretable
as containing cluster representatives (centroids) and G will be interpretable as encoding
cluster indicators. We begin with a simple investigation of this hypothesis. In Figure 1,
we randomly generate four two-dimensional datasets with three clusters each. Computing
both the Semi-NMF and Convex-NMF factorizations, we display the resulting F factors.
We see that the Semi-NMF factors (denoted Fsemi in the (cid:12)gure) tend to lie distant from
the cluster centroids. On the other hand, the Convex-NMF factors (denoted Fcnvx) almost
always lie within the clusters.

6.2 Real life datasets

We conducted experiments on the following datasets: Ionosphere and Wave from the UCI
repository, the document datasets URCS, WebkB4, Reuters (using a subset of the data
collection which includes the 10 most frequent categories), WebAce and a dataset which
contains 1367 log messages collected from several di(cid:11)erent machines with di(cid:11)erent operating
systems at the School of Computer Science at Florida International University. The log
messages are grouped into 9 categories: con(cid:12)guration, connection, create, dependency, other,
report, request, start, and stop. Stop words were removed using a standard stop list. The
top 1000 words were selected based on frequencies.

Table 1 summarizes the datasets and presents our experimental results. These results

are averages over 10 runs for each dataset and algorithm.

We compute clustering accuracy using the known class labels. This is done as follows:
The confusion matrix is (cid:12)rst computed. The columns and rows are then reordered so as to

14

6

4

2

0

−2

−4

−6
−6

4

3

2

1

0

−1

−2

−3

−4

−5
−1

−4

−2

0

2

4

6

0

1

2

3

4

5

6

6

5

4

3

2

1

0

−1

−2

−3
−2

6

4

2

0

−2

−4

−6

−8
−7

−1

0

1

2

3

4

5

−6

−5

−4

−3

−2

−1

0

1

2

3

Figure 1: Four random datasets, each with 3 clusters. \C" are Fsemi factors and \(cid:3)" are
Fcnvx factors.

maximize the sum of the diagonal. We take this sum as a measure of the accuracy: it rep-
resents the percentage of data points correctly clustered under the optimized permutation.
To measure the sparsity of G in the experiments, we compute the average of each column
of G and set all elements below 0.001 times the average to zero. We report the number
of the remaining nonzero elements as a percentage of the total number of elements. (Thus
small values of this measure correspond to large sparsity).

A consequence of the sparsity of G is that the rows of G tend to become close to
orthogonal. This indicates a hard clustering (if we view G as encoding posterior probabilities
for clustering). We compute the normalized orthogonality, (GT G)nm = D(cid:0)1=2(GT G)D(cid:0)1=2,
where D = diag(GT G). Thus diag[(GT G)nm] = I. We report the average of the o(cid:11)-diagonal
elements in (GT G)nm as the quantity \Orthogonality" in the table.

From the experimental results, we observe the following: (1) All of the matrix fac-
torization models are better than K-means on all of the datasets. This is our principal
empirical result. It indicates that the NMF family is competitive with K-means for the
purposes of clustering.
(2) On most of the nonnegative datasets, NMF gives somewhat
better accuracy than Semi-NMF and Convex-NMF (with WebKb4 the exception). The

15

Table 1: Dataset descriptions and results.

Reuters

URCS WebKB4

data sign
# instance
# class

+

2900
10

+
476
4

K-means
NMF
Semi-NMF
Convex-NMF

Clustering Accuracy
0.4448
0.4947
0.4867
0.4789

0.4250
0.5713
0.5628
0.5340

+

4199

4

0.3888
0.4218
0.4378
0.4358

Log WebAce
+

+

1367

9

2340
20

Ionosphere Wave
(cid:6)
5000
2

(cid:6)
351
2

0.6876
0.7805
0.7385
0.7257

0.4001
0.4761
0.4162
0.4086

0.4217

0.5018

-

0.5947
0.5470

-

0.5896
0.5738

Sparsity (percentage of nonzeros)

Semi-NMF
Convex-NMF

0.9720
0.6152

0.9688
0.6448

0.9993
0.5976

0.9104
0.5070

0.9543
0.6427

0.8177
0.4986

0.9747
0.4861

Orthogonality

Semi-NMF
Convex-NMF

0.6578
0.1979

0.5527
0.1948

0.7785
0.1146

0.5924
0.4815

0.7253
0.5072

0.9069
0.1604

0.5461
0.2793

di(cid:11)erences are modest, however, suggesting that the more highly-constrained Semi-NMF
and Convex-NMF may be worthwhile options if interpretability is viewed as a goal of a
data analysis. (3) On the datasets containing both positive and negative values (where
NMF is not applicable) the Semi-NMF results are better in terms of accuracy than the
Convex-NMF results. (3) In general, Convex-NMF solutions are sparse, while Semi-NMF
solutions are not. (4) Convex-NMF solutions are generally signi(cid:12)cantly more orthogonal
than Semi-NMF solutions.

6.3 Shifting mixed-sign data to nonnegative

While our algorithms apply directly to mixed-sign data, it is also possible to consider
shifting mixed-sign data to be nonnegative by adding the smallest constant so all entries
are nonnegative. We performed experiments on data shifted in this way for the Wave and
Ionosphere data. For Wave, the accuracy decreases to 0.503 from 0.590 for Semi-NMF
and decreases to 0.5297 from 0.5738 for Convex-NMF. The sparsity increases to 0.586 from
0.498 for Convex-NMF. For Ionosphere, the accuracy decreases to 0.647 from 0.729 for
Semi-NMF and decreases to 0.618 from 0.6877 for Convex-NMF. The sparsity increases
to 0.829 from 0.498 for Convex-NMF. In short, the shifting approach does not appear to
provide a satisfactory alternative.

6.4 Flexibity of NMF

A general conclusion is that NMF almost always performs better than K-means in terms
of clustering accuracy while providing a matrix approximation. We believe this is due to
the (cid:13)exibility of matrix factorization as compared to the rigid spherical clusters that the
K-means clustering objective function attempts to capture. When the data distribution is
far from a spherical clustering, NMF may have advantages. Figure 2 gives an example.

16

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.015

0.01

0.005

0

−0.005

−0.01

−0.015

0

20

40

60

80

100

120

140

160

180

200

Figure 2: A dataset with 2 clusters in 3D. Top Left: clusters obtained using K-means, as
indicated by either \r " or \(cid:3)". Top Right: clusters obtained using NMF. Bottom: The
di(cid:11)erence g2(i) (cid:0) g1(i); i = 1;(cid:1)(cid:1)(cid:1) ; 200, \r" for those mis-clustered points, and \(cid:3) " for
correctly-clustered points.

The dataset consists of two parallel rods in 3D space containing 200 data points. The
two central axes of the rods are 0.3 apart and they have diameter 0.1 and length 1. As
seen in the (cid:12)gure, K-means gives a poor clustering, while NMF yields a good clustering.
The bottom panel of Figure 2 shows the di(cid:11)erences in the columns of G (each column is

with random initialization.

normalized to Pi gk(i) = 1). The mis-clustered points have small di(cid:11)erences. NMF starts

Finally, note that NMF is initialized randomly for the di(cid:11)erent runs. We investigated
the stability of the solution over multiple runs and found that NMF converges to solutions F
and G that are very similar across runs; moreover, the resulting discretized cluster indicators
were identical.

7 Conclusions

We have presented a number of new nonnegative matrix factorizations. We have provided
algorithms for these factorizations and theoretical analysis of convergence of these algo-
rithms. The ability of these algorithms to deal with mixed-sign data makes them useful for
many applications, particularly given that covariance matrices are often centered.

17

Semi-NMF o(cid:11)ers a low-dimensional representation of data points which lends itself to
a convenient clustering interpretation. Convex-NMF further restricts the basis vectors to
be convex combinations of data points, providing a notion of cluster centroids for the basis.
We also brie(cid:13)y discussed additional NMF algorithms|Kernel-NMF and Cluster-NMF|
that are further specializations of Convex-NMF.

We also showed that the NMF variants can be viewed as relaxations of K-means clus-
tering, thus providing a closer tie between NMF and clustering than has been present in
the literature to date. Moreover, our empirical results showed that the NMF algorithms
all outperform K-means clustering on all of the datasets that we investigated in terms of
clustering accuracy. We view these results as indicating that the NMF family is worthy
of further investigation. We view Semi-NMF and Convex-NMF as particularly worthy of
further investigation, given their native capability for handling mixed-sign data and their
particularly direct connections to clustering.

Acknowledgments

We acknowledge the support for this project from the U.S. Department of Energy, O(cid:14)ce of
Science, under contract DE-AC02-05CH11231. This is LBNL Tech Report 60428.

References

M. Berry, M. Browne, A. Langville, P. Pauca, and R. Plemmons. Algorithms and appli-
cations for approximate nonnegative matrix factorization. To Appear in Computational
Statistics and Data Analysis, 2006.

J.-P. Brunet, P. Tamayo, T.R. Golub, and J.P. Mesirov. Metagenes and molecular pattern
discovery using matrix factorization. Proc. Nat’l Academy of Sciences USA, 102(12):
4164{4169, 2004.

M. Cooper and J. Foote. Summarizing video using non-negative similarity matrix factor-

ization. In Proc. IEEE Workshop on Multimedia Signal Processing, pages 25{28, 2002.

A. D’Aspremont, L. El Ghaoui, M. I. Jordan, and G. R. G. Lanckriet. A direct formulation

for sparse PCA using semide(cid:12)nite programming. to appear in SIAM Review, 2006.

I. Dhillon and S. Sra. Generalized nonnegative matrix approximations with Bregman di-
vergences. In Advances in Neural Information Processing Systems 17. Cambridge, MA:
MIT Press, 2005.

C. Ding, X. He, and H.D. Simon. On the equivalence of nonnegative matrix factorization

and spectral clustering. Proc. SIAM Data Mining Conf, 2005.

C. Ding, T. Li, and W. Peng. Nonnegative matrix factorization and probabilistic latent
semantic indexing: Equivalence, chi-square statistic, and a hybrid method. Proc. National
Conf. Arti(cid:12)cial Intelligence, 2006.

18

D. Donoho and V. Stodden. When does non-negative matrix factorization give a correct
decomposition into parts? In Advances in Neural Information Processing Systems 16.
Cambridge, MA: MIT Press, 2004.

P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. J. Machine

Learning Research, 5:1457{1469, 2004.

D.D. Lee and H. S. Seung. Unsupervised learning by convex and conic coding. In Advances

in Neural Information Processing Systems 9. Cambridge, MA: MIT Press, 1997.

D.D. Lee and H. S. Seung. Learning the parts of objects by non-negative matrix factoriza-

tion. Nature, 401:788{791, 1999.

D.D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In Advances

in Neural Information Processing Systems 13. Cambridge, MA: MIT Press, 2001.

S.Z. Li, X. Hou, H. Zhang, and Q. Cheng. Learning spatially localized, parts-based represen-
tation. In Proc. IEEE Conf. Computer Vision and Pattern Recognition, pages 207{212,
2001.

T. Li and S. Ma. IFD: Iterative feature and data clustering. In Proc. SIAM Int’l Conf. on

Data Mining (SDM 2004), pages 472{476, 2004.

P. Paatero and U. Tapper. Positive matrix factorization: A non-negative factor model with

optimal utilization of error estimates of data values. Environmetrics, 5:111{126, 1994.

V. P. Pauca, F. Shahnaz, M.W. Berry, and R.J. Plemmons. Text mining using non-negative

matrix factorization. In Proc. SIAM Int’l conf on Data Mining, pages 452{456, 2004.

F. Sha, L. K. Saul, and D. D. Lee. Multiplicative updates for nonnegative quadratic pro-
In Advances in Neural Information Processing

gramming in support vector machines.
Systems 15. Cambridge, MA: MIT Press, 2003.

N. Srebro, J. Rennie, and T. Jaakkola. Maximum margin matrix factorization. In Advances

in Neural Information Processing Systems. Cambridge, MA: MIT Press, 2005.

Y.-L. Xie, P.K. Hopke, and P. Paatero. Positive matrix factorization applied to a curve

resolution problem. Journal of Chemometrics, 12(6):357{364, 1999.

W. Xu, X. Liu, and Y. Gong. Document clustering based on non-negative matrix factor-

ization. In Proc. ACM Conf. Research Development in IR, pages 267{273, 2003.

H. Zha, C. Ding, M. Gu, X. He, and H.D. Simon. Spectral relaxation for K-means clustering.
In Advances in Neural Information Processing Systems 14. Cambridge, MA: MIT Press,
2002.

Z. Zhang, H. Zha, and H.D. Simon. Low-rank approximations with sparse factors ii: Penal-
ized methods with discrete newton-like iterations. SIAM J. Matrix Analysis Applications,
25:901{920, 2004.

H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. J. Computa-

tional and Graphical Statistics, 15:265{286, 2006.

19

All in-text references underlined in blue are linked to publications on ResearchGate, letting you access and read them immediately.

